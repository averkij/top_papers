{
    "paper_title": "Wait, We Don't Need to \"Wait\"! Removing Thinking Tokens Improves Reasoning Efficiency",
    "authors": [
        "Chenlong Wang",
        "Yuanning Feng",
        "Dongping Chen",
        "Zhaoyang Chu",
        "Ranjay Krishna",
        "Tianyi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large reasoning models have enabled complex, step-by-step reasoning but often introduce significant overthinking, resulting in verbose and redundant outputs that hinder efficiency. In this study, we examine whether explicit self-reflection, signaled by tokens such as \"Wait\" and \"Hmm\", is necessary for advanced reasoning. We propose NoWait, a simple yet effective approach that disables explicit self-reflection by suppressing these tokens during inference. Extensive experiments on ten benchmarks across textual, visual, and video reasoning tasks show that NoWait reduces chain-of-thought trajectory length by up to 27%-51% in five R1-style model series, without compromising model utility. NoWait thus offers a plug-and-play solution for efficient and utility-preserving multimodal reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 3 4 3 8 0 . 6 0 5 2 : r Wait, We Dont Need to Wait! Removing Thinking Tokens Improves Reasoning Efficiency Chenlong Wang1, Yuanning Feng1, Dongping Chen1, Zhaoyang Chu2, Ranjay Krishna3, Tianyi Zhou1 1University of Maryland, 2University College London, 3University of Washington"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in large reasoning models have enabled complex, step-by-step reasoning but often introduce significant overthinking, resulting in verbose and redundant outputs that hinder efficiency. In this study, we examine whether explicit self-reflection, signaled by tokens such as Wait and Hmm, is necessary for advanced reasoning. We propose NOWAIT, simple yet effective approach that disables explicit self-reflection by suppressing these tokens during inference. Extensive experiments on ten benchmarks across textual, visual, and video reasoning tasks show that NOWAIT reduces chain-of-thought trajectory length by up to 27%51% in five R1-style model series, without compromising model utility. NOWAIT thus offers plug-and-play solution for efficient and utility-preserving multimodal reasoning."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large reasoning models (LRMs), exemplified by DeepSeek-R1 (Guo et al., 2025), have shown that complex reasoning abilities can be effectively elicited through simple rule-based reinforcement learning (Team, 2025; Qwen, 2025; Abdin et al., 2025; Xia et al., 2025). These models produce explicit, step-by-step reasoning through long chain-of-thought (CoT) trajectories (Yang et al., 2025a; Ma et al., 2025a) before arriving at final answers. This capability is believed to be accompanied by the emergence of the Aha Moment phenomenon (Chen et al., 2025b; Yang et al., 2025b), in which the model begins to rethink problems and self-reflect on its reasoning trajectory with anthropomorphic expressions such as Wait, Hmm, or Alternatively. This was firstly achieved on R1-style language reasoning models and has been extended to vision-language models (VLMs) (Team, 2024; Team et al., 2025), Corresponding Author. Project Leader. 1 enabling multimodal reasoning on images (Zhang et al., 2025b; Shen et al., 2025a; Huang et al., 2025; Zhou et al., 2025) and videos (Feng et al., 2025; Team, 2024; Team et al., 2025). Despite the effectiveness of long CoT reasoning with self-reflection, the overthinking problem has emerged (Chen et al., 2024a; Cuadron et al., 2025; Chen et al., 2024b; Wu et al., 2025; Sui et al., 2025). It is characterized by excessively verbose reasoning and redundant thought steps, often extending over thousands of tokens, resulting in significant computational overhead and high reasoning latency. Such inefficiencies hinder the practical deployment of R1-style reasoning models in applications with limited computational resources. Although numerous efforts have been devoted to efficient reasoning, many existing approaches require additional training, either through reinforcement learning (RL) with length-based rewards (Aggarwal and Welleck, 2025; Liao et al., 2025; Luo et al., 2025) or fine-tuning on variable-length CoT trajectories (Ma et al., 2025b; Munkhbat et al., 2025). On the other hand, several training-free approaches have been proposed to mitigate overthinking by reducing token usage during inference. However, they often compromise the overall model utility (Ma et al., 2025a) or have only demonstrated effectiveness on distilled reasoning models (Yang et al., 2025c,a; Xu et al., 2025). In this study, we investigate the impact of excessive self-reflection during the reasoning process and question whether explicit self-reflection, signaled by Wait-like tokens, is really necessary for advanced reasoning. To this end, we propose NOWAIT, simple yet effective training-free approach that disables explicit self-reflection in R1style reasoning models, significantly reducing token usage while maintaining overall model utility. As illustrated in Figure 1, we directly intervene in the inference process by identifying specific keyword tokens (e.g., Wait, Hmm, and AlternaFigure 1: Illustrative pipeline for NOWAIT. We introduce NOWAIT, simple yet effective approach that suppresses the generation of reflection keywords (e.g., Wait and Hmm) during inference. NOWAIT reduces chain-of-thought trajectory length by up to 27%-51% across textual, visual, and video reasoning tasks. tively) that indicate explicit self-reflection and suppressing their generation. Specifically, we achieve this by proactively adjusting the logits of these tokens to negative values during decoding, thereby steering the model toward selecting alternative tokens to continue the reasoning process. Comprehensive experiments show that NOWAIT achieves strong performance on ten benchmarks spanning ❶ textual reasoning (AMC 2023 (AIMO, 2024), AIME 2024, AIME 2025 (MAA Committees), GQPA-D (Rein et al., 2024)), ❷ visual reasoning (MMMU (Yue et al., 2024a), MMMU-Pro (Yue et al., 2024b), MathVista (Lu et al., 2024), EMMA-mini (Hao et al., 2025)), and ❸ video reasoning (MMVU (Zhao et al., 2025), VSI-Bench (Yang et al., 2024)). When integrated into five R1-style model series, including QwQ (Qwen, 2025), Phi4 (Abdin et al., 2025), Qwen3 (Team, 2025), Kimi-VL (Team et al., 2025), QvQ (Team, 2024), NOWAIT reduces CoT trajectory length by up to 27%-51% across different modalities. NOWAIT serves as plug-and-play solution for improving reasoning efficiency while preserving overall model utility."
        },
        {
            "title": "2 Preliminaries",
            "content": "Reasoning Model Generation Patterns. Reasoning models structure their output using thinking delimiters (i.e., <think> and <think>), dividing the response into two main components: the CoT trajectory detailing the reasoning process and the final answer summarizing overall thoughts. Within the generated CoTs, models employ complex reasoning strategies, such as forward thinking, backtracking, and self-reflection. Notably, large reasoning models often continue to reason even after obtaining an initial result, performing additional validation steps. Accordingly, we define each segment of reasoning as thinking chunk. Each thinking chunk is associated with an intermediate answer r. Formally, thinking chunk can be represented as pair (chunki, ri), where chunki is the reasoning text and ri is the intermediate answer from chunki derived from chunki. Thus, complete CoT can be structured as follows: CoT = {(chunki, ai)}n i=1 . (1) The final response is the combination of the CoT trajectory and concise reasoning summary: Response = (CoT, summary) . (2) Self-Reflection within Reasoning Models. As stated above, single CoT can contain multiple reasoning chunks. The transitions between these chunks are often marked by specific keywords, such as Wait, Alternatively, or Hmm. Models tend to switch their reasoning approaches in subsequent steps, often to verify previous results or explore alternative paths. However, this mechanism can sometimes lead to unproductive overthinking, causing models to repeatedly enter new reasoning steps and engage in redundant and unnecessary validation loops. In this study, we introduce NOWAIT, simple yet effective method for efficient reasoning 2 by intervening in the generation of these keywords. This method alters models self-reflection strategies and can be generalized to various modalities."
        },
        {
            "title": "3 Removing Thinking Pattern is Better",
            "content": "In this section, we propose NOWAIT, simple yet effective method, that improves the reasoning efficiency while maintaining acceptable model utility. We first expand the method details in Section 3.1, and introduce the experimental setup in Section 3.2. We then report the experiment results on textual reasoning in Section 3.3. Additionally, we conduct the comparison experiment in Section 3.4 and further analyze the generalization across different modalities ( visual reasoning and video reasoning ) in Section 3.5. 3.1 Method NOWAIT functions as an inference-time intervention. It directly prevents the model from generating the specific tokens associated with self-reflection. Our method involves three main stages: Initialize Reflection Keywords List. We begin by identifying the initial reflection keywords, such as Wait, Alternatively, and Hmm. To empirically establish the list, we conduct 32 independent runs of the QwQ-32B (Qwen, 2025) on AIME 2025 (MAA Committees). Using nn as delimiters, we identify the 15 most frequent monolingual words as our identified keywords = {ki}."
        },
        {
            "title": "Keyword List for Suppressing",
            "content": "wait, alternatively, hmm, but, however, alternative, another, check, double-check, oh, maybe, verify, other, again, now, ah, any Specific Token-Level Keyword List. Secondly, for each target model α, we expand the initial keyword list into specific token-level list, Kα. For instance, the variants of wait include wait, Wait, Wait, .wait and WAIT. We achieve this by iterating through the overall vocabulary Vα and identifying all variant tokens whose textual representation contains any keyword from as substring. Specifically, we define that, is_substr(x, y) = rue when is the substring of y. This process can be formulated as follows: Kα = {v Vαks K, s.t.is_substr(ks, v)} We further manually filter keywords that are not reasonable (i.e., Ohio for oh). Suppressing Keywords Generation. During the inference, we leverage logit processor to prohibit models from generating keywords. For any keyword Kα, its corresponding logit is set to large negative value. This effectively makes these reflection-associated tokens, ensuring they are highly unlikely to be sampled by models. By surgically preventing the generation of these targeted reflection-associated tokens, NOWAIT aims to streamline the LRMs reasoning pathways. This targeted intervention is designed to enhance inference efficiency, reducing both latency and token costs, without requiring any modification to the models underlying architecture or weights. 3.2 Experimental Setup Model & Benchmark. To comprehensively evaluate the effectiveness of NOWAIT, we conduct experiments on the open-source models across different modalities and parameter scales. For the textual reasoning task, we assess reinforcement learning (RL) based models, including QwQ-32B (Qwen, 2025), Phi4-ReasoningPlus (Abdin et al., 2025), and Qwen3-32B (Team, 2025) on math reasoning benchmarks, AIME 2024, AIME 2025 (MAA Committees), and AMC 2023 (AI-MO, 2024). For the visual reasoning task, our experiments cover the state-of-the-art RL-based visual reasoning models, Kimi-VL-A3B-Thinking (Team et al., 2025) and QvQ-72B-Preview (Team, 2024) and evaluate on MMMU-Pro (Yue et al., 2024b), MMMU (Yue et al., 2024a), MathVista (Lu et al., 2024) and EMMA-mini (Hao et al., 2025) For the video reasoning task, we select QvQ72B-Preview and evaluate on VSI-Bench (Yang et al., 2024) and MMVU (Zhao et al., 2025). Metrics. The goal of NOWAIT is to preserve the models reasoning accuracy while substantially diminishing the number of generated tokens during inference. Performance is assessed using two key metrics: ❶ Accuracy (ACC): This measures the correctness of the models final output. ❷ Generation Length (LEN) quantifies the average number of tokens generated by the model per problem instance, calculated over independent runs. Baselines. To compare the latency of NOWAIT, we use both the models original performance and the 3 Table 1: Experiment results on Textual Reasoning Tasks. Strategy AMC 2023 ACC LEN AIME 2024 LEN ACC AIME 2025 LEN ACC QwQ-32B Original 91.25 NoThink 72.50 NOWAIT 95.50 +4.25 5267 -30% 71.33 -2.00 73.33 46.67 7542 4265 14142 7980 11907 -16% 68.00 +1.33 10548 -31% 15240 8167 66.67 40. Phi4-Resoning-Plus Original 90.00 NoThink 80.83 NOWAIT 96.00 +6.00 4524 -28% 69.33 -0.67 70.00 34.67 6366 3805 15161 6200 11185 -26% 62.67 +3.34 12490 -23% 16257 59.33 31.33 Qwen3-32B Original 97.50 NoThink 59.50 NOWAIT 96.67 -0.83 6424 1240 5560 -13% 83.33 +2.00 10732 -16% 64.44 -2.67 12720 2511 66.67 20. 81.33 25.33 14987 2165 12930 -14% NoThink strategy (Ma et al., 2025a) as baselines. Both NOWAIT and NoThink share similar rationale, aiming to intervene in the models reasoning process. However, while NOWAIT operates at the token level by prohibiting the output of specific tokens, NoThink removes the entire reasoning process by prompt engineering. By including these baselines, we can better analyze the models performance under different intervention approaches. Experiment Details. For each evaluated benchmark, we conduct five independent runs. Except for the Qwen3 series, we infer without chat templates on open-ended problems and leverage the same prompt template for multiple-choice problems (see Appendix C). Because of the different thinking patterns, we apply chat templates for the Qwen3 model inference. In baseline and NOWAIT experiments, we set maximum token limit of 32,768 tokens per instance. If models generation reaches this limit before finishing CoT generation, that instance is considered incorrect, and the generation length is 32,768 tokens. If not, we will extract the final answer from the generated CoT and judge the correctness. This policy ensures that models failing to complete their response within the budget are appropriately penalized in Accuracy metric. For NoThink strategy (Ma et al., 2025a), we set token budget of 10,000. Details of the token budget for NoThink can be found in Section B.1."
        },
        {
            "title": "3.3 LRMs can be Efficient without “WAIT”",
            "content": "Table 1 presents comprehensive quantitative overview of our NOWAITs performance on various textual reasoning tasks, evaluated across different LRMs with diverse model structures and parameter scales. Our method NOWAIT consistently and significantly reduces the output length while maintaining the reasoning accuracy. Model Architectures Generalization. Notably, when integrated with QwQ-32B, NOWAIT improves accuracy on AMC 2023 by 4.25 percentage points, while reducing output length to just 70% of the baseline. With another model architecture, Phi4-Reasoning-Plus, our method achieves an even greater improvement of 6.00 percentage points, alongside 28% reduction in token generation. Additionally, Qwen3-32B also benefits from our approach, reducing output length by 13% with only marginal decrease in reasoning accuracy. These results demonstrate that our method NOWAIT consistently enhances efficiency across diverse model architectures. This consistency suggests fundamental similarity in the reasoning patterns and redundancy present in different models, underscoring the broad applicability of our approach. Reasoning Difficulty Analysis. We tested our method on mathematical reasoning benchmarks spanning various difficulty levels (AMC 2023 < AIME 2024 < AIME 2025). The experimental statistics demonstrated strong generalization 4 Table 2: Comparison Experiments across Multiple Efficient Reasoning Methods. We use QwQ-32BPreview for experiments. Strategy AIME 2024 ACC LEN ACC LEN AMC 2023 Baseline 42.00 8979 82.50 4143 Token-Budget 46.67 82.50 3636 O1-Pruner 33.33 4289 77.50 2399 NOWAIT 42.00 86.00 3396 across these levels: All tested models achieved comparable reductions in token usage regardless of task difficulty. Crucially, NOWAIT enabled models to maintain or even improve performance on more challenging tasks. For instance, QwQ-32B achieved 1.33% point increase on the challenging AIME 2025 benchmark, while reducing token usage by 31%, which is comparable to its performance on the college-level AMC 2023. Qwen332B consistently reduced output length by 14% to 16% across all three math benchmarks, while Phi4Reasoning-Plus showed similar gains and reductions from 23% to 28%. On the non-mathematical GPQA-Diamond task, models showed slight performance decrease compared to the math reasoning benchmarks, but still maintained efficiency, with an overall 11.67% reduction in token usage. These consistent efficiency gains and stable performance across diverse models and varying tasks suggest that, despite the architecture and scale, LRMs exhibit similar inherent redundancy in their reasoning processes. NOWAIT effectively prunes this redundancy, demonstrating that substantial efficiency improvements can be achieved simply by suppressing the keywords generation, without the need for complex explicit waiting mechanisms."
        },
        {
            "title": "3.4 Comparison Analysis",
            "content": "Comparison Experiment. We further compare with existing efficient reasoning techniques, including prompt-based training-free technique, TokenBudget (Han et al., 2024), and training-based technique, O1-Pruner (Luo et al., 2025), using QwQ32B-Preview (Qwen, 2025) on AIME 2024 and AMC 2023. All inference is conducted without chat templates to ensure fairness. As shown in Table 2, NOWAIT exhibits more significant generation length curtailment compared to Token-Budget. Although Token-Budget shows promising results on base models, such as GPT4o, its effectiveness does not generalize to current LRMs(Deepseek-R1 (Guo et al., 2025), QwQ32B (Qwen, 2025)). These reasoning models are less sensitive to the prompt design, resulting in less efficiency. O1-Pruner, while effective at reducing token usage, incurs severe performance degradation on QwQ-32B-Preview. In contrast, NOWAIT does not require additional training or data, but instead guides models to strike an effective balance between output length and reasoning accuracy, achieving spontaneous trade-off. LRM Cannot Skip Thinking. As shown in Table 1, Qwen3-32B, model specifically trained for non-thinking patterns, exhibits notable reductions in token usage. However, for other models (QwQ-32B and Phi4-Reasoning-Plus) without non-thinking pattern training, NoThink (Ma et al., 2025a), prompt-based method, fails to thoroughly skip the generation of reasoning steps. While NoThink does reduce the generation length, the evaluated model can still generate the thinking process and demonstrate serious compromise in accuracy. This failure indicates that the presence of explicit \"thinking\" tokens (<think> and <think>) can influence models output, but is insufficient to precisely control models reasoning strategy. Our method NOWAIT operates on similar premise by targeting key reasoning-related tokens, but achieves significant efficiency improvements with better maintain on reasoning accuracy."
        },
        {
            "title": "3.5 Efficient Multimodal Reasoning",
            "content": "In this section, we propose efficient multimodal reasoning. We assess NOWAIT on visual reasoning models using image and video reasoning benchmarks. As shown in Table 3 and Table 4, visual reasoning models exhibit more exciting outcomes. Severe Verbosity on Multimodal Reasoning. Although Kimi-VL-A3B-Thinking generates an average of only 2,000 tokens across four image reasoning benchmarks, significantly fewer than that in math reasoning tasks, our method NOWAIT further reduces the generation length by an average of 49%, with only modest overall accuracy drop of 3.42 percentage points. similar trend is observed with QvQ-72B-Preview, which achieves up to 30% reduction in token usage, accompanied by only slight decrease in accuracy (ranging from 0.11% to 4.00%). For video reasoning tasks, QvQ-72B5 Table 3: Experiment results on Visual Reasoning Tasks. Strategy MMMU-Pro LEN ACC MMMU MathVista ACC LEN ACC LEN EMMA-mini LEN ACC Kimi-VL-A3B-Thinking Original 61.27 NOWAIT 58.73 -2.54 1457 -51% 55.20 -1.80 1746 -40% 69.40 -2.10 1045 -43% 27.50 -7.25 2269 -60% 57. 71.50 34.75 2929 1822 2975 Original 65.77 66.85 NOWAIT 63.79 -1.98 1659 -21% 66.74 -0.11 1571 -21% 70.92 -2.62 939 -30% 73.54 1338 1977 2094 32.00 28.00 -4.00 1554 -26% 2097 QvQ-72B-Preview Table 4: Experiment Results on Video Reasoning Tasks. We use QvQ-72B-Preview for experiments. Strategy MMVU VSI-Bench ACC LEN ACC LEN"
        },
        {
            "title": "Original\nNOWAIT",
            "content": "64.10 1734 62.20 1260 22.51 1280 22.57 1020 Performance -1.90 -27% +0.06 -20% Preview also demonstrates substantial reductions in output length while maintaining comparable accuracy. Similar to textual reasoning tasks, these results reveal the same challenging problems that significant portion of generated tokens are either redundant or contribute little to the final reasoning. Existing multimodal reasoning models still suffer from severe reasoning inefficiency. Reinforcement Learning is Less Efficient. We further evaluate various RL-based reasoning models across varying benchmarks and modalities. While generation of intellectual reasoning models confirms the effectiveness of the RL algorithm in advanced reasoning capabilities, the efficiency of the optimal policy derived from the RL algorithm is still disappointing. The model learns reasoning policy from training and begins to spontaneously reflect reasoning processes during inference. However, these algorithms fail to effectively teach models when reflection is truly necessary. As result, these models often adopt lower threshold for self-reflection, leading to unnecessary verification steps and less efficient reasoning. Our method suppresses the generation of reflection keywords, raising the threshold of self-reflection, and making it more efficient and necessary."
        },
        {
            "title": "4 Discussion",
            "content": "In this section, we first discuss the effectiveness of our method NOWAIT in Section 4.1 by case study and the robustness of the model while applying NOWAIT in Section 4.2. Additionally, we conduct an empirical experiment to analyze the difference between RL-based models and distill models based on NOWAIT in Section 4.3."
        },
        {
            "title": "4.1 Why does NOWAIT Work?",
            "content": "As we discussed in Table 3.4, thinking tokens (<think> and <think>) cannot thoroughly control models actions. Similarly, can banning keywords completely remove self-reflection from CoTs? If not, why NOWAIT result in more efficient reasoning? To answer this, we conduct case study to analyze the effectiveness of our method. More Efficient Self-Reflection Mechanism. NOWAIT does not prohibit models from selfreflection. However, this method guides models to skip the unnecessary waiting reasoning. To illustrate this, we select an example from Qwen332B on AMC 2023 benchmark. The NOWAIT CoT (see Figure 12) is noticeably shorter than the original CoT (see Figure 9). Specifically, the NOWAIT CoT reserves its self-reflection for two clear points: ❶ it notes the extraneous root and instantly discards it after factoring, ❷ it quickly verifies both original equations with the numeric solution. By contrast, the original CoT continually interjects let me check again, and perhaps another way, leading to 5 derivations of essentially the same algebra. In short, the first approach builds more concise reasoning process with necessary checks to ensure correctness, whereas the second strategy prefers to pause to highlight every minor thought, making the logic scattered and less efficient. 6 Figure 2: One Case Study From QvQ-72B-Preview on MMVU. NOWAIT CoT is more straightforward than the original CoT, without unnecessary self-reflection and verbosity. Concise and Straightforward Reasoning. Figure 8 presents an example from QvQ-72B-Preview on MMVU. The original CoT contains six instances of self-reflection, resulting in excessive token usage and disorganized reasoning process. In contrast, the NOWAIT CoT exhibits more streamlined and coherent approach. The model analyzes the video in detail, using series of time-sequence cues such as starts, After that, At one point, As the video progresses and Towards the end of the video. With fewer self-reflections, the NOWAIT CoT organizes its reasoning more logically and systematically, whereas the original CoT appears fragmented and less focused, always generating new reasoning branch by Wait. Ultimately, the NOWAIT derives the final answer directly from its detailed analysis. Unlike the original policy, NOWAIT encourages the model to connect observations to conclusions more directly, reducing unnecessary speculation and making the reasoning process more concise and straightforward."
        },
        {
            "title": "4.2 A Closer Look at RL Models Performance",
            "content": "For textual reasoning tasks, our evaluation primarily focuses on the math problems. As we discussed in Table 3.3, NOWAIT yields consistent experimental outcomes across math benchmarks of varying difficulty levels. For multimodal reasoning tasks, Figure 3 shows the accuracy of the QvQ72B-Preview on MMMU across wide range of 7 Figure 4: Accuracy Degradation across Qwen3 Seires Models on Math Reasoning Benchmarks. racy degradation relative to the RL-based model on the simpler AMC 2023. This performance gap extends significantly as problem difficulty increases, surpassing 5-percentage-point drop on AIME 2024 and dramatically exceeding 12 percentage points on more challenging AIME 2025. This sharp performance degradation among distilled models, in contrast to the stable performance of the RL-based model, demonstrates their higher sensitivity to reflection keywords. Given that the supervised fine-tune (SFT) directly injects new knowledge into models, the CoT structures are crucial for advanced reasoning. Simply removing these keywords, however, severely disrupts the inherent CoT structure, restricting distilled models from exhibiting full reasoning capabilities. Especially on more challenging reasoning problems, distill models fail to effectively conduct validation, suffering from substantial underthinking."
        },
        {
            "title": "5 Conclusion",
            "content": "This work demonstrates that explicit self-reflection, signaled by tokens such as Wait and Hmm, is not essential for advanced reasoning in R1-style models. By suppressing these tokens during inference, the proposed NOWAIT approach effectively reduces overthinking and shortens chain-of-thought trajectories without compromising overall model utility. Extensive experiments across diverse models and benchmarks in textual, visual, and video reasoning tasks demonstrate that NOWAIT serves as an efficient and utility-preserving solution for multimodal reasoning, offering new insights for the lightweight deployment of large reasoning models. Figure 3: Accuracy Radar Map on MMMU for QvQ72B-Preview. fields. crucial observation highlights remarkably small accuracy divergence between the baseline and NOWAIT in almost all tested disciplines. Despite the potential intervention introduced by NOWAIT, the models performance remains closely aligned with the baseline across diverse academic and professional subjects. This minimal degradation strongly indicates the robustness of the QvQ72B-Preview when applying NOWAIT, highlighting generalization capability across varying areas."
        },
        {
            "title": "4.3 Distilled Models Cannot Reasoning",
            "content": "without Wait Recent studies (Yue et al., 2025) underscore the significant differences between reasoning models based on reinforcement learning (RL) and those trained through distillation. To better understand these differences, we further evaluate the effectiveness of NOWAIT across Qwen3 series, including an RL-based model (Qwen3-32B) and several distilled models (Qwen3-4B/8B/14B). Figure 4 illustrates the accuracy degradation for models using NOWAIT, where higher score indicates more pronounced decline. The selected math reasoning benchmarks differ in difficulty, ordered as follows: AMC 2023 < AIME 2024 < AIME 2025. While the RL-based models maintain consistent performance across these benchmarks, distilled models exhibit distinct trend of increasing accuracy degradation as difficulty rises. Specifically, distilled models show similar accu-"
        },
        {
            "title": "Acknowledgment",
            "content": "Many thanks to Yao Wan and Jieyu Zhang for their invaluable support and comments."
        },
        {
            "title": "Limitation",
            "content": "In this study, we introduce NOWAIT, simple yet effective method for efficient reasoning on all modalities. We conduct experiments across large range of models across various benchmarks to validate the effectiveness of our method. Although the promising results, we acknowledge that existing benchmarks cannot comprehensively exhibit the reasoning capabilities of models from all aspects."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, and 1 others. 2025. Phi-4-reasoning technical report. arXiv preprint arXiv:2504.21318. Pranjal Aggarwal and Sean Welleck. 2025. L1: Controlling how long reasoning model thinks arXiv preprint with reinforcement learning. arXiv:2503.04697. AI-MO. 2024. AMC 2023. https://huggingface. co/datasets/AI-MO/aimo-validation-amc. Accessed: 2024-05-20. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. 2024. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. 2025a. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, and 1 others. 2024a. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, and 1 others. 2024b. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, and 1 others. 2024c. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271. Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, and 1 others. 2025b. An empirical study on eliciting and improving r1-like reasoning models. arXiv preprint arXiv:2503.04548. Jeffrey Cheng and Benjamin Van Durme. 2024. Compressed chain of thought: Efficient reasoning arXiv preprint through dense representations. arXiv:2412.13171. Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, and 1 others. 2025. The danger of overthinking: Examining the reasoning-action dilemma in agentic tasks. arXiv preprint arXiv:2502.08235. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. 2025. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776. Google. 2025. Gemini 2.5 Pro, Generative AI on Vertex AI. https://cloud.google.com/vertex-ai/ generative-ai/docs/models/gemini/2-5-pro. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. 2024. Token-budget-aware llm reasoning. arXiv preprint arXiv:2412.18547. Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. 2025. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. 2025. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749. Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, and Caiming Xiong. 2025. Reward-guided speculative decoding for efficient llm reasoning. arXiv preprint arXiv:2501.19324. 9 Kevin Lin, Charlie Snell, Yu Wang, Charles Packer, Sarah Wooders, Ion Stoica, and Joseph Gonzalez. 2025. Sleep-time compute: Beyond inference scaling at test-time. arXiv preprint arXiv:2504.13171. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. 2024. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In Proceedings of International Conference on Learning Representations (ICLR). Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. 2025. O1-pruner: Lengthharmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570. Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. 2025a. Reasoning models can be effective without thinking. arXiv preprint arXiv:2504.09858. Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. 2025b. Cot-valve: Lengthcompressible chain-of-thought tuning. arXiv preprint arXiv:2502.09601. MAA Committees. Aime problems and solutions. https://artofproblemsolving.com/wiki/ index.php/AIME_Problems_and_Solutions. Accessed: 2024-05-20. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393. Tergel Munkhbat, Namgyu Ho, Seo Hyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun. 2025. Selftraining elicits concise reasoning in large language models. arXiv preprint arXiv:2502.20122. OpenAI. 2024. GPT-4o System Card. Preprint, arXiv:2410.21276. OpenAI. 2024. Introducing OpenAI o1. https:// openai.com/o1/. Qwen. 2025. QwQ-32B: Embracing the Power of Reinforcement Learning. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In Proceedings of First Conference on Language Modeling. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, and 1 others. 2025a. Vlm-r1: stable and generalizable r1style large vision-language model. arXiv preprint arXiv:2504.07615. Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, and Shiguo Lian. 2025b. Dast: Difficulty-adaptive slowthinking for large reasoning models. arXiv preprint arXiv:2503.04472. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Hanjie Chen, Xia Hu, and 1 others. 2025. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419. Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. 2024. Fast best-of-n decoding via speculative rejection. arXiv preprint arXiv:2410.20290. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, and 1 others. 2025. Kimi-vl technical report. arXiv preprint arXiv:2504.07491. Qwen Team. 2024. QVQ: To See the World with Wisdom. Qwen Team. 2025. Qwen3: Think Deeper, Act Faster. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the Advances in Neural Information Processing Systems. Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, and Yisen Wang. 2025. When more is less: Understanding chain-of-thought length in llms. arXiv preprint arXiv:2502.07266. Shyam Sundhar Ramesh, Yifan Hu, Iason Chaimalas, Viraj Mehta, Pier Giuseppe Sessa, Haitham Bou Ammar, and Ilija Bogunovic. 2024. Group robust preference optimization in reward-free rlhf. Advances in Neural Information Processing Systems, 37:37100 37137. Bingquan Xia, Bowen Shen, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu, Jiebao Xiao, Jinhao Dong, Liang Zhao, and 1 others. 2025. Mimo: Unlocking the reasoning potential of language model arXiv preprint from pretraining to posttraining. arXiv:2505.07608. 10 Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. 2025. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600. Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Zheng Lin, Li Cao, and Weiping Wang. 2025a. Dynamic early exit in reasoning models. arXiv preprint arXiv:2504.15895. Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. 2024. Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces. arXiv preprint arXiv:2412.14171. Shu Yang, Junchao Wu, Xin Chen, Yunze Xiao, Xinyi Yang, Derek Wong, and Di Wang. 2025b. Unfrom external obserderstanding aha moments: arXiv preprint vations to internal mechanisms. arXiv:2504.02956. Wang Yang, Xiang Yue, Vipin Chaudhary, and Xiaotian Han. 2025c. Speculative thinking: Enhancing small-model reasoning with large model guidance at inference time. arXiv preprint arXiv:2504.12329. Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. 2024. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, and 3 others. 2024a. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of Conference on Computer Vision and Pattern Recognition. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. 2024b. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. 2025. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837. Anqi Zhang, Yulin Chen, Jane Pan, Chen Zhao, Aurojit Panda, Jinyang Li, and He He. 2025a. Reasoning models know when theyre right: Probing hidden states for self-verification. arXiv preprint arXiv:2504.05419. Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. 2025b. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937. Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, Chuhan Li, Junyang Song, and 1 others. 2025. Mmvu: Measuring expert-level multiarXiv preprint discipline video understanding. arXiv:2501.12380. Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. 2025. R1zeros\" aha moment\" in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132."
        },
        {
            "title": "A Related Work",
            "content": "Large Reasoning Model The pursuit of advanced reasoning capabilities in Large Language Models (LLMs) (OpenAI, 2024) has spurred significant research, particularly focusing on strategies that scale computation (Chen et al., 2024c; Snell et al., 2024) or refine the generation process during inference. Prior studies apply fundamental techniques like Chain-of-Thought (CoT) prompting (Wei et al., 2022), guiding the model to think step by step, or integrate Process Reward Models(PRMs), external verifiers, and search-guided decoding (Brown et al., 2024) to aggregate multiple reasoning paths and enhance final answer accuracy. These efforts have culminated in new generation of powerful Large Reasoning Models (LRMs), such as ChatGPT-O1 (OpenAI, 2024), Deepseek-R1 (Guo et al., 2025), QwQ (Qwen, 2025), Gemini2.5 (Google, 2025), which enable to spontaneous generation of extensive CoT sequences involving forward thinking, backtracking, and verification steps. Within the open-source domain, models derive reasoning abilities from diverse training paradigms, primarily through reinforcement learning (RL) (Guo et al., 2025; Ramesh et al., 2024; Muennighoff et al., 2025) on reasoning tasks or distillation (Guo et al., 2025; Yu et al., 2024) on high-quality CoT data produced from RLbased models. Recent works (Yue et al., 2025) have analyzed the difference between the two types of models. In this study, we include RL-based models for further exploration, underscoring the defects of RL-triggered reasoning capabilities. Efficient Reasoning While elaborating reasoning processes like long CoT demonstrates enhanced performance on reasoning tasks, the associated verbosity presents significant efficiency challenge (Chen et al., 2024a). The generation of extensive intermediate steps substantially increase inference latency and computational cost, hindering practical deployment in real-world applications. 11 Consequently, considerable body of work explores methods for efficient reasoning, aiming to reduce the length of reasoning traces without compromising accuracy. Some techniques continue to train models for CoT optimization (Aggarwal and Welleck, 2025; Luo et al., 2025; Shen et al., 2025b), such as applying RL with length-based reward design (Sun et al., 2024; Liao et al., 2025; Luo et al., 2025; Aggarwal and Welleck, 2025), or fine-tuning with variable-length CoT data (Han et al., 2024; Yu et al., 2024; Munkhbat et al., 2025). Other methods conform training-free strategy, applying dynamic reasoning paradigms during inference (Yang et al., 2025a; Zhang et al., 2025a; Wu et al., 2025; Lin et al., 2025) or leveraging prompts to guide efficient reasoning (Cheng and Van Durme, 2024; Xu et al., 2025; Han et al., 2024; Ma et al., 2025a). While existing studies are effective in cutting down the token usage, our study provides new insight to rethink the internal mechanism of efficient reasoning and propose efficient multimodal reasoning. Self-Reflection & Overthinking Parallel to enhancing reasoning capabilities and efficiency, recent studies analyze the intricacies of the generated thought processes. Within these generated sequences, an interesting phenomenon occurs - moments marked by keywords like \"wait\" and \"hmm\", which we term Aha Moment (Guo et al., 2025; Liu et al., 2025). These moments seemingly indicate capability for self-reflection (Chen et al., 2025a), allowing models to reassess their reasoning path and verify their CoT before concluding. Prior studies (Yang et al., 2025b; Zhang et al., 2025a) have begun to characterize these moments and probe the latent states to explore the potential mechanisms behind such spontaneous self-reflection. However, the frequent occurrence of these keywords can also lead to significant Overthinking (Chen et al., 2024a; Sui et al., 2025), where the model continues reflecting even after reaching correct intermediate or final conclusions. Building on the initial characterizations from previous work, our study takes further step to evaluate the functional effectiveness of these spontaneously generated Aha Moments, directly addressing whether they are essential contributors to the reasoning outcomes or potentially represent form of inefficient behavioral mimicry."
        },
        {
            "title": "B Baseline Implementation Details",
            "content": "Our experiments include three baselines, NoThinking (Ma et al., 2025a), TokenBudget (Han et al., 2024), and O1-Pruner (Luo et al., 2025). In this section, we will systematically introduce the implementation details of these techniques. B.1 NoThinking The core idea of NoThinking is to leverage prompts, guide reasoning models to skip the reasoning processes, and directly generate final response. For models that have not been post-trained for nonreasoning mode, such as QwQ-32B and Phi4Reasoning-Plus, we apply the prompt template as follows: Prompt Template for NoThinking {Question} <think> Okay, think have finished thinking. <think> We then adopt budget forcing technique specifically for NoThinking. Different from the token budget we apply for normal inference and NOWAIT, we set the token budget to 10,000 and forced models to generate Final Answer when the model reaches the token budget. B.2 Token-Budget We apply the TALE-EP strategy, prompt-based method. This method consists of two steps: ❶ Directly answering the reasoning model: Prompt Template for TALE-EP Task: Analyze the given question and estimate the minimum number of tokens required to generate complete and accurate response. Please give the response by strictly following this format: [[budget]], for example, Budget: [[12]]. ❷ We include token budget in the prompt to guide models to think efficiently. Table 5: Prompt Template Applied for Token Budget."
        },
        {
            "title": "Vanilla CoT",
            "content": "Lets think step by step:"
        },
        {
            "title": "Token Budget",
            "content": "Lets think step by step and use less than {budget} tokens: 12 B.3 O1-Pruner O1-Pruner is an effective post-training method. We select released model trained on QwQ-32BPreview by O1-Pruner. This model can be accessed via Hugging Face."
        },
        {
            "title": "C Prompts",
            "content": "Prompt Template for Multiple-Choice Question {Question} Choices: A. option B. option ... Choose the correct answer from the choices above. Output format: [ANSWER: \"<answer>\"] If the answer is A, output [ANSWER: \"A\"] Benchmark & Models D.1 Textual QA In this paper, we evaluate range of mathematics competition benchmarks designed to assess the mathematical reasoning abilities of models, including AIME2024, AIME2025, AMC2023. We have also evaluated GPQA-Diamond (Rein et al., 2024), challenging benchmark spanning biology, physics, and chemistry. The detailed information about these benchmarks is as follows: AIME2024: benchmark derived from the 2024 American Invitational Mathematics Examination (AIME), challenging mathematics competition aimed at high school students in the U.S., designed specifically to evaluate the advanced mathematical reasoning abilities of AI models. It consists of complex problems covering algebra, geometry, combinatorics, and number theory, each requiring integer solutions ranging from 0 to 999. Models are tested on their ability to perform multi-step reasoning, provide accurate step-by-step explanations, and derive correct final answers. AIME2025: Like AIME2024, the AIME2025 benchmark is based on the 2025 American Invitational Mathematics Examination (AIME), an advanced and highly respected mathematics competition aimed at high school students in the United States, intended specifically for evaluating the mathematical reasoning and problem-solving capabilities of AI models. AMC2023: benchmark derived from the 2023 American Mathematics Competitions (AMC), specifically designed to evaluate the mathematical reasoning abilities of AI models. It consists of 40 questions, covering various mathematical topics such as algebra, geometry, number theory, and combinatorics. GPQA-Diamond (Rein et al., 2024): subset of the GPQA dataset, specifically designed to assess the reasoning capabilities of advanced AI systems and highly knowledgeable humans on extremely difficult, domain-expert-level questions in biology, physics, and chemistry. The \"Diamond\" subset is the hardest subset of the benchmark, which is intended to facilitate research on reasoning models. We evaluated and measured these models on the above benchmarks: QwQ-32B (Qwen, 2025): large-scale language model designed to achieve robust performance across wide range of natural language processing tasks. Developed with 32 billion parameters, QwQ32B leverages advanced architecture and training techniques to enhance understanding, generation, and reasoning in general and specialized domains. Phi4-Reasoning-Plus (Abdin et al., 2025): Built on Phi-4 Base, it is an advanced language model specifically designed to excel in complex reasoning and problem-solving tasks across multiple domains, demonstrating strong performance in textual data. Qwen3-32B(Team, 2025): state-of-the-art large language model developed by Alibaba Cloud, featuring 32 billion parameters and designed to deliver high performance across broad spectrum of language understanding, text generation and reasoning tasks. QwQ-32B-Preview (Qwen, 2025): An experimental large language model developed by Alibaba, designed to advance AI reasoning capabilities. With 32.5 billion parameters and 13 We evaluated and measured these models on the above benchmarks: Kimi-VL-A3B-Thinking (Team et al., 2025): An efficient open-source vision-language model (VLM) built on Mixture-of-Experts (MoE) architecture, designed to deliver advanced multimodal reasoning, robust longcontext understanding, maths problem solving as well as strong agent capabilities. QvQ-72B-Preview (Team, 2024): QVQ-72Bpreview is an open-source, large-scale multimodal reasoning model built on top of Qwen2VL-72B, achieving remarkable performance on challenging benchmarks. In this part of the experiment, the image recognition and reasoning capability of this model has been tested. D.3 Video QA Furthermore, we conduct experiment on video benchmarks, whose name and details is listed as follows: MMVU (Zhao et al., 2025): comprehensive dataset designed to evaluate the capabilities of AI models in understanding and reasoning over expert-level, domain-specific videos. Each example is meticulously crafted using textbook-guided annotation process, ensuring that questions require both visual comprehension and the application of domain-specific knowledge. Whats unique to MMVU is the inclusion of expert-annotated reasoning rationales and relevant domain knowledge for each question, which largely facilitates the fine-grained analysis of model performance. VSI-Bench (Yang et al., 2024): pioneering dataset, designed to evaluate the visual-spatial reasoning capabilities of multimodal large language models (MLLMs). It comprises many question-answer pairs derived from egocentric videos that are sourced from public indoor 3D scene reconstruction datasets, aiming to provide comprehensive benchmark for testing and improving the spatial reasoning abilities of multimodel large language models, moving beyond traditional static image evaluations. 32,768-token context window, it is specifically tested on benchmark AIME2024 and AMC2023 to compare with other methods. D.2 Visual QA Additionally, we incorporate evaluations on the multimodal benchmarks including MMMU-Pro (Yue et al., 2024b), MMMU (Yue et al., 2024a), Math-Vista (Lu et al., 2024) and EMMA-mini (Hao et al., 2025) to further explore the models capabilities across diverse reasoning and multimodal tasks. Here is the detailed information: MMMU (Yue et al., 2024a): multimodal evaluation benchmark specifically designed to test the capabilities of AI models on collegelevel tasks that require both advanced subject knowledge and deliberate reasoning across broad range of academic disciplines. MMMU-Pro (Yue et al., 2024b): MMMUPro is an enhanced evaluation benchmark designed to rigorously test the true understanding and reasoning capabilities of multimodal AI models. Building on the original MMMU benchmark, it forces models to simultaneously process and integrate visual and textual information, simulating real-world scenarios that require human-like cognitive skills. Math-Vista (Lu et al., 2024) : comprehensive benchmark specifically designed to evaluate and challenge the mathematical reasoning abilities of large language and multimodal models within visual contexts. It requires models to perform deep, fine-grained visual understanding and complex compositional reasoning across diverse mathematical tasks. EMMA-mini (Hao et al., 2025): specialized benchmark designed to rigorously assess the ability of Multimodal Large Language Models (MLLMs) to perform integrated, organic reasoning over both text and imagesan essential aspect of human intelligence. Unlike existing benchmarks that often focus on textbased reasoning or superficial visual cues, EMMA-mini presents tasks spanning mathematics, physics, chemistry, and coding, all of which require genuine cross-modal reasoning that cannot be solved by independently analyzing text or images alone. 14 Additional Experiment Results & Case Study Table 6: Complete Experiment results of NOWAIT on Qwen3 Series Models and Other Distill Models. Strategy AMC 2023 AIME AIME 2025 GPQA-D ACC LEN ACC LEN ACC LEN ACC LEN Original NoThink NOWAIT 97.50 59.50 96."
        },
        {
            "title": "Original\nNoThink\nNOWAIT",
            "content": "96.25 69.50 95."
        },
        {
            "title": "Original\nNoThink\nNOWAIT",
            "content": "97.50 66.50 94."
        },
        {
            "title": "Original\nNoThink\nNOWAIT",
            "content": "93.75 70.00 95.50 6424 1240 5560 6677 1749 4714 8513 1760 5251 8125 2236 4523 Qwen3-32B 81.33 25.33 83.33 12720 2511 10732 66.67 20.00 64.44 14987 2165 12930 69.19 50.50 63.13 Qwen3-14B 78.67 33.33 73.33 77.33 28.89 72.67 70.00 33.33 65.33 14217 3559 10919 78.00 26.67 61.33 14765 3171 59.59 38.59 54.75 Qwen3-8B 14142 3362 10963 74.61 25.56 60.00 16094 3719 13674 57.07 32.93 51. Qwen3-4B 13488 4068 10358 70.00 23.00 56.67 18086 4656 12213 53.54 27.27 76.26 Llama-Nemontron-Nano-8B-v"
        },
        {
            "title": "Original\nNoThink\nNOWAIT",
            "content": "71.50 39.00 72."
        },
        {
            "title": "Original\nNoThink\nNOWAIT",
            "content": "73.00 30.00 72.00 4535 2982 3690 4796 2552 4315 39.33 6.6 30.67 7371 2185 4865 44.67 14.67 33. 11798 3677 7271 54.1 30.30 42.83 Deepseek-R1-Distill-Qwen-7B 34.67 18.67 26.67 7755 3895 7247 40.00 10.00 31. 13767 2724 8236 49.10 21.00 40.91 5613 605 4788 4633 1001 3889 5904 1271 4735 5965 1288 5071 3109 3754 3809 1112 3672 15 (a) Accuracy Radar Map for QvQ-72B-Preview on VSIBench (b) Accuracy Radar Map for Kimi-VL-A3B-Thinking-7B on MMMU Figure 6: CoT Example from QvQ-72B-Preview on MMVU 2023. 17 Figure 7: CoT Example from QvQ-72B-Preview on MMVU 2023. 18 Figure 8: CoT Example from QvQ-72B-Preview applied NOWAIT on MMVU 2023. Figure 9: CoT Example from Qwen3-32B on AMC 2023. 20 Figure 10: CoT Example from Qwen3-32B on AMC 2023. 21 Figure 11: CoT Example from Qwen3-32B on AMC 2023. Figure 12: CoT Example from Qwen3-32B applied NOWAIT on AMC 2023. 23 Figure 13: CoT Example from Qwen3-32B applied NOWAIT on AMC 2023."
        }
    ],
    "affiliations": [
        "University College London",
        "University of Maryland",
        "University of Washington"
    ]
}