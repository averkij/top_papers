{
    "paper_title": "CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation",
    "authors": [
        "Zhongyuan Peng",
        "Caijun Xu",
        "Changyi Xiao",
        "Shibo Hong",
        "Eli Zhang",
        "Stephen Huang",
        "Yixin Cao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . [ 1 0 6 6 1 0 . 2 0 6 2 : r CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation Zhongyuan Peng1,3, Caijun Xu1,2, Changyi Xiao1, Shibo Hong1, Eli Zhang3, Stephen Huang3, Yixin Cao1, 1Fudan University, 2Shanghai Innovation Institute, 3M-A-P Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) benefit substantially from training on challenging competitionlevel questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of models ability to generate valid, high-difficulty questions. Then, we develop CoDiQGenerator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research. Date: February 2, 2026 Correspondence: yxcao@fudan.edu.cn, zhangge.eli@bytedance.com Project Page: https://github.com/ALEX-nlp/CoDiQ"
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of Large Reasoning Models (LRMs) has demonstrated remarkable capabilities in complex reasoning, with recent works achieving impressive performance on challenging benchmarks across mathematics and coding [22, 23, 38]. crucial factor driving these improvements is the availability of high-quality training and evaluation data that truly stress reasoning, yet such data are scarce. Importantly, much like scientific discovery, finding the right difficult questions can be as critical as solving them. As difficulty rises, reliable problem construction demands expert knowledge and careful validation, making purely human-driven pipelines expensive and hard to scale. In this paper, we aim to scale the synthesis of high-difficulty questions while keeping them well-posed and solvable. Recent research has explored various approaches for mathematics and programming, ranging from human-in-the-loop methodologies [26] and adversarial generation [33] to iterative evolutionary strategies [9, 1 22, 38]. However, pushing difficulty at scale faces three major challenges. First, there is generator capacity ceiling, where model typically struggles to generate questions substantially harder than what it can reliably reason about, leading to stalled progress. Second, the solvabilitycomplexity trade-off implies that forcing complexity often breaks logical consistency, producing fake hard but unsolvable or ill-defined questions. Finally, the difficulty definition and control. Since difficulty is neither directly observable nor standardized, make it harder becomes uncontrollable without measurable surrogate, rendering curriculum-style training brittle. To address these challenges, we propose CoDiQ (Controllable Difficult Question Generation), framework that introduces test-time scaling into question generation and systematically scales difficulty through three key innovations. First, we design six Difficulty-Enhancement Strategies and train the CoDiQ-Generator via Reinforcement Learning to synthesize questions beyond zero-shot baselines. Second, we develop the CoDiQ Pipeline, an iterative framework with hybrid verification to ensure logical consistency while increasing complexity. Third, we establish relative difficulty paradigm through LLM-based ranking and ValueNetwork that quantifies difficulty via continuous scores for precise level grouping. Based on CoDiQ, we construct CoDiQ-Corpus, comprising 44K competition-grade math and coding question sequences. Human evaluation and experiments confirm that our method yields high-quality data that significantly enhances downstream reasoning performance. Our key contributions are: Difficulty-Enhancement Strategies. We propose six systematic strategies that guide LLMs to inject difficult elements into question generation, enabling the synthesis of high-difficulty questions that surpass zero-shot generation baselines. Test-Time Scaling Tendency for Difficulty. We identify scaling tendency linking test-time compute to question difficulty, characterizing the upper bound of models capacity to produce valid, high-difficulty questions. CoDiQ-Corpus. We construct dataset of 44K competition-grade mathematical and coding questions based on our CoDiQ-Generator. Experiments demonstrate that training on CoDiQ-Corpus significantly enhances the reasoning capabilities of large reasoning models compared to existing baselines. We will open-source CoDiQ-Corpus, CoDiQ-Generator, and all implementations to support future research."
        },
        {
            "title": "2 Related Works",
            "content": "Generating difficult yet valid questions is increasingly recognized as key lever for scaling reasoning progress: it expands the training distribution beyond scarce human-curated problems, continuously provides frontier-level supervision, and provides controlled way of generalization testing under increasing difficulty [12, 21, 29]. As result, recent research has devoted substantial effort to synthesizing competition-level problems with both intellectual challenge and formal correctness guarantees. Prompt-based and agentic synthesis pipelines. One dominant paradigm treats hard-problem creation as prompt-driven or agentic workflow: the system bootstraps from seed problems, concepts, or human-authored solution structures, then iteratively refines candidates with self-critique, filtering, and verification signals to ensure well-posedness [20, 31, 39]. On the math side, PromptCoT [38] drives generation with concept sampling and structured design cues, explicitly inducing expert-like problem-construction rationales, and then applies rejection sampling to retain coherent, high-difficulty instances. CogAtom [5] instead decomposes human solutions into reusable cognitive atoms, constructs an atom graph, and synthesizes new problems via constrained recombination, enabling systematic exploration of compositional design space. For programming tasks, reliability is even more salient: valid instance requires not only statement but also precise I/O specifications, meaningful constraints, and anti-shortcut test suites. AutoCode [40] exemplifies closed-loop setter pipeline that jointly generates problem statements and reference solutions, and filters under-specified or ill-posed tasks via automated test generation and cross-verification. Overall, these approaches are effective but often depend on complex multi-step orchestration and heavy post-hoc filtering to maintain validity. 2 Training generators for difficult questions. complementary line of work focuses on training dedicated generators to amortize the cost of multi-step agentic flow, enabling large-scale difficult-problem synthesis at low marginal cost [7, 15, 34]. For example, ScaleQuest [10] unlocks question-generation capability via Question Fine-Tuning and Question Preference Optimization to align generation toward solvability and difficulty. ScaleDiff [24] first identifies hard instances efficiently , then trains specialized generator on the hard subset to expand the upper tail. Overall, generator-training methods scale well, but common limitations remain: difficulty controls are often coarse, and validity still depends heavily on post-hoc filtering or human verification. In contrast to prior synthesis pipelines and generator-training methods, our approach centers on test-time scaling as core mechanism for fine-grained difficulty control under verifiable solvability: we explicitly scale instance difficulty at inference time while enforcing correctness via automated verification, rather than relying on filtering. This enables systematic frontier tracking of hard-yet-solvable questions while keeping validity constraints intact and controllable at scale."
        },
        {
            "title": "3.1 Overview",
            "content": "Our method aims to endow LRMs with scalable testtime question generation capability by enabling them to synthesize progressively challenging yet valid questions. To achieve this, we first introduce six DifficultyEnhancement Strategies (3.2) that explicitly guide LRMs toward difficulty-scaling reasoning and hard question construction. These strategies are instantiated within the CoDiQ Pipeline (3.3), which integrates two verification modulesdifficulty estimation (3.3.1) and solvability verification (3.3.2)to jointly regulate both difficulty and validity. Leveraging this pipeline, we construct CoDiQ-Bench to systematically benchmark models question-generation performance under unified evaluation framework. Then, we develop specialized CoDiQ-Generator through reinforcement learning (3.5), utilizing pipeline-derived feedback signals to further enhance the difficulty and reliability of synthesized questions. Finally, we construct CoDiQ-Corpus, dataset of 44K competition-grade mathematical and coding questions based on our CoDiQ-Generator. The detailed statistics are provided in Appendix E, and the distribution is shown in Figure 1."
        },
        {
            "title": "3.2 Difficulty-Enhancement Strategies",
            "content": "Figure 1 Distribution of CoDiQ-Corpus Dataset To systematically scale problem difficulty beyond naive prompting (e.g., make this harder), we design six Difficulty-Enhancement Strategies (detailed in Appendix L) that serve as explicit cognitive scaffolds for LLMs. These strategiesDimensionality & Constraints, Mathematical Abstraction, Inverse & Constructive, State Explosion, Theorem Disguise, and Edge Case & Rigor Engineeringguide the model to inject algorithmic difficulty systematically, ensuring difficulty arises from reasoning depth rather than superficial modifications."
        },
        {
            "title": "3.3 CoDiQ Pipeline",
            "content": "Building upon the difficulty injection strategies (3.2), we introduce the CoDiQ Pipeline (Algorithm 1), which systematically scales difficulty through iterative refinement. The pipeline implements an evolutionary loop where seed question Q0 progressively evolves into harder variants {Q1, . . . , Qn} over up to Tmax = 8 rounds. At each iteration, the model is prompted with Can you make it more difficult? to trigger deeper reasoning. 3 To ensure generation quality, the pipeline incorporates two core validation modules: Difficulty Estimation (3.3.1) and Solvability Verification (3.3.2). The process terminates under strict stopping rules (3.3.3)."
        },
        {
            "title": "3.3.1 Difficulty Estimation",
            "content": "To strictly enforce the monotonic difficulty constraint, we require robust mechanism to detect difficulty regression. Since CoDiQ targets the frontier of model capabilities, standard absolute scoring suffers from saturation effectswhere models assign uniformly high scores to challenging queries rendering direct comparison ineffective. Consequently, we adopt relative difficulty paradigm comprising two complementary approaches: explicit LLMs-Ranking (3.3.1) to discern comparative hardness, and implicit ValueNetwork Scoring (3.3.1) to capture internal uncertainty. Finally, we normalize these discrete rankings (3.3.1) to eliminate granularity bias. Algorithm 1 CoDiQ Iterative Pipeline 1: Input: Seed Question Q0, Max Rounds Tmax 2: Output: Evolved Questions 3: Initialize , d0 Difficulty(Q0) 4: for = 1 to Tmax do 5: Qi LLM(Qi1) 6: 7: 8: 9: 10: {Qi} 11: end for 12: return di Difficulty(Qi) if Valid(Qi) = False or di < di1 then break end if LLMs-Ranking. To adaptively allocate the reasoning budget, we utilize Doubao-Seed-1.8 [25] for listwise difficulty estimation. Given batch of queries = {q1, . . . , qn}, the model ranks them by perceived difficulty following the prompt template in Appendix I. To mitigate positional bias, we apply stochastic shuffling τ (Q) before ranking. The model outputs structured JSON results, from which we extract ranked indices to map computation budgets K, allocating more samples to harder queries. ValueNetwork Scoring. To efficiently estimate question difficulty, we extend the hidden-representation-based approach of [41] by analyzing the models reasoning trajectory. We employ Qwen3-8B to extract hidden states across sampling window of up to 4,096 tokens. To capture the critical early stages of reasoning, we implement quadratic sampling strategy(Eq.4) that allocates higher density to the onset of generation. These representations are fed into lightweight MLP trained via binary cross-entropy to predict the probability of correctness {0, 1} across mixture of standard and competition-level benchmarks [8, 16, 17, 19]. This approach demonstrates strong capability in distinguishing problem difficulty(See AppendixC.5). At inference, the predicted probability serves as proxy for LLM-perceived difficulty, where lower scores indicate higher difficulty. Detailed implementation is provided in AppendixC. Difficulty Normalization. To convert the discrete grouped rankings from 3.3.1 and 3.3.1 into continuous scores, we apply linear scaling. Given difficulty groups = {g1, . . . , gG} ordered from easiest to hardest, the normalized difficulty for question qi in group gj is: di = 1 1 , {1, . . . , G}. (1) This maps discrete rankings to [0, 1], where di serves as the scaling factor for adaptive computation allocation. 3.3.2 Solvability Verification While difficulty estimation ensures monotonic complexity growth, it does not guarantee logical validity. To prevent invalid or unsolvable instances, we utilize Qwen3-32B [35] to verify the solvability of generated instances. Following the template in Appendix J, the model generates responses in JSON format, from which we extract the solvability status and confidence score. Only instances verified as solvable with high confidence are retained."
        },
        {
            "title": "3.3.3 Termination Criteria",
            "content": "To maintain the integrity of the question trajectory, the pipeline enforces strict stopping rules. The iterative process terminates immediately if: (1) Non-Monotonic Difficulty, where the generated question Qi has lower difficulty score compared to its predecessors; or (2) Unsolvability, where the candidate Qi is flagged as invalid. Upon termination at step i, the invalid candidate is discarded, and the pipeline yields the sequence {Q1, . . . , Qi1}. See Appendix for case study and Appendix for failure type analysis."
        },
        {
            "title": "3.4 CoDiQ-Bench",
            "content": "Table 1 Dataset statistics of CoDiQ-Bench. Statistics Number #Questions - math - code 200 100 100 Question Tokens Length - max/min/avg 726/9/128.2 To systematically evaluate the question generation capability of LRMs, we first construct CoDiQ-Bench, curated dataset comprising 200 carefully selected cases across coding and mathematical domains  (Table1) . For coding tasks, we randomly sample 50 cases each from CodeAlpaca_20K (general programming) and LeetCodeDataset (algorithmic challenges). For mathematical reasoning, we sample 50 cases each from GSM8K (grade school questions) and MATH12K (mathematical question-solving). We intentionally focus on relatively simple questions to establish baseline benchmark, with detailed selection criteria regarding solvability and quality provided in AppendixD."
        },
        {
            "title": "3.5 CoDiQ-Generator",
            "content": "To further enhance the CoDiQ Pipelines capacity for generating high-difficulty, high-quality questions, we develop CoDiQ-Generator via reinforcement learning. By directly optimizing the models question-setting behavior through targeted reward signals, we aim to improve both the validity and difficulty scaling of synthesized problems. 3.5.1 RL Data Construction We construct our Reinforcement Learning dataset, DRL, by capturing the critical failure modes of Qwen3-8B within the CoDiQ Pipeline (Section 3.3). Rather than maximizing absolute difficulty, we target the models specific capability boundary [37]. We identify evolutionary trajectories where the model successfully generates valid questions for rounds 1 through 1 but fails at round (e.g., due to unsolvability or difficulty stagnation). These boundary instances are collected to form training pairs, effectively converting the models breaking point into precise learning signal. To ensure broad domain coverage, we initialize the pipeline with seed questions (Q0) drawn from diverse established benchmarks. For mathematics, we sample from Math12k [14], GSM8K [8], SVAMP [6], and ASDiv [36]. For code generation, we utilize Code Alpaca [4], LeetCodeDataset [32], MBPP [2], and DS-1000 [18]. After filtering for the specific boundary conditions described above, the final dataset DRL comprises 1,173 high-quality samples. 3.5.2 RL Training Paradigm Reinforcement Learning Optimization (RL). The recent success of R1-style methods has demonstrated the effectiveness of online RL using discrete, rule-based rewards [27]. In our pipeline, Qwen3-8B [35] is further refined using reinforcement learning signals derived from solvability confidence, difficulty progression, and question validity checks. Based on the dataset described in 3.5.1, we apply rule-based RL approach to optimize the models judgment reasoning capability. Specifically, we utilize the GRPO algorithm [27] within the VeRL reinforcement learning framework [28]. To ensure smooth optimization, we design difficulty-aware reward function that balances validity guarantees with progressive difficulty scaling. Given confidence lower bound conf = max(0.5, confidence(x)) and difficulty change (D) = di di1 [1, 1] (2) for iteration {1, . . . , R}, where di is computed via Eq. (1) and denotes the maximum number of evolution rounds: = 0, 0.6 conf, 0.2 conf + 0.8 (0.8 + 0.2 (D)), if invalid if (D) = 0 if (D) > (3) where invalid cases include unsolvable questions, repetitive outputs, or negative difficulty changes ((D) < 0)."
        },
        {
            "title": "4.1 Experimental Setup\nBaselines To evaluate the effectiveness of our CoDiQ Prompt and CoDiQ-Generator, we compare against\nmodels with inherent test-time scaling capabilities that support extended reasoning. These baseline models\ninclude flagship models (GLM-4.6 [11]) and smaller-parameter models (GPT-OSS-20B [1], GLM-Z1-9B-\n0414 [11], and the Qwen3 series [35]: Qwen3-0.4B, Qwen3-1.7B, Qwen3-4B, Qwen3-8B, Qwen3-14B, Qwen3-\n32B). All these models utilize the CoDiQ Pipeline described in Section 3.3 for generation.",
            "content": "Evaluation Metrics. We employ two metrics to quantify problem difficulty: (1) DS-LLM: Difficulty score estimated by the Doubao-Seed-1.8 [25] model (details in Section 3.3.1). (2) DS-VN: Difficulty score derived from the ValueNetwork (VN) (details in Section 3.3.1). Both scores are normalized to the range [0, 1] and reported as percentages (0%-100%), where higher values indicate greater difficulty. All reported scores are averaged across questions in CoDiQ-Bench."
        },
        {
            "title": "4.2 Main Results",
            "content": "4.2.1 Maximum Solvable Difficulty To evaluate the question generation capability of Large Reasoning Models (LRMs) within our proposed framework, and to identify the optimal Generator for the subsequent synthesis of difficult questions, we conduct comparative analysis. Specifically, we instantiate distinct LRMs as the backbone Generator within the CoDiQ Pipeline and assess the difficulty of the questions they generate on CoDiQ-Bench. Effectiveness of CoDiQ Prompt. We first evaluate the efficacy of the CoDiQ Prompt in eliciting deep reasoning for difficulty synthesis. As detailed in Table 2, the application of the CoDiQ Prompt induces substantial expansion in reasoning token usage across all evaluated architectures. This significant increase in test-time computation suggests that the prompt successfully triggers extended reasoning trajectories, enabling models to construct more intricate constraints and logic. Consequently, the majority of baseline models exhibit marked improvement in the difficulty of generated questions when conditioned on our prompt. 6 Table 2 Performance of different Long-CoT models on CoDiQ-Bench. Group rankings based on the highest difficulty of solvable questions generated across 8 rounds without difficulty degradation on CoDiQ-Bench. The best, the second-best and the third-best scores for each indicator are shown in box , bold and underlined, respectively."
        },
        {
            "title": "Model",
            "content": "Rounds Tokens DR-LLM DR-VN DR(AVG) GPT-OSS-20B GLM-4.6 Qwen3-32B Qwen3-8B GLM-Z1-9B-0414 Qwen3-14B Qwen3-4B Qwen3-1.7B Qwen3-0.6B GLM-4.6 GPT-OSS-20B Qwen3-32B Qwen3-14B Qwen3-4B Qwen3-8B GLM-Z1-9B-0414 Qwen3-1.7B Qwen3-0.6B CoDiQ-Gen-8B Direct Prompt 5528.2 3385.8 1239.3 1130.5 1229.8 2076.4 1419.7 844.5 314. 68.5 71.2 50.6 39.2 48.8 45.9 36.8 25.6 17.2 CoDiQ Prompt(ours) 7143.8 8057.3 4893.6 5281.9 4422.3 4155.6 3638.3 2975.7 2052.7 73.2 63.8 63.0 53.9 49.1 49.8 54.7 32.3 22. 2.9 2.8 2.3 3.4 2.7 3.1 4.2 3.3 2.4 2.7 2.1 2.2 2.6 2.8 2.4 1.7 1.4 1.0 CoDiQ Generator(ours) 58.9 7499.6 3. 74.4 65.8 54.8 59.6 43.7 44.4 40.4 37.1 35.0 83.3 61.5 46.5 44.2 42.7 41.9 30.0 37.3 29.2 71.5 68.5 52.7 49.4 46.3 45.2 38.6 31.4 26.1 78.3 62.7 54.8 49.1 45.9 45.8 42.4 34.8 25.8 58.1 58. Superiority of CoDiQ-Generator. Notably, our CoDiQ-Gen-8B outperforms the significantly larger Qwen332B in generating high-complexity instances. We attribute this performance gain to the Reinforcement Learning alignment described in Section 3.5.1. By optimizing for solvability and difficulty progression, the RL training enables CoDiQ-Generator to maintain high validity rates across iterative evolution. This stability allows the model to sustain the generation pipeline for greater number of roundsexceeding the iteration depth of baseline modelsthereby accumulating complexity monotonically without premature termination due to unsolvability. 4.2.2 Difficulty Metrics Comparison To further verify the number of tokens consumed by LRMs can estimate question difficulty, we highlight the positive correlation between token volume and difficulty rankings shown in Figure 2. We further validated this relationship by analyzing the correlation between token consumption and our established metrics (DR-LLM and DR-VN), yielding Pearson coefficients of = 0.8299 (p < 0.001) and = 0.8545 (p < 0.001), respectively. These results confirm that computational cost serves as reliable proxy for difficulty, provided that the problem complexity remains within the evaluators capability and consistent scaling method is applied."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "7 Figure 2 Question Difficulty Scaling on CoDiQ-Bench. Scatter plot showing the relationship between average reasoning tokens and difficulty ranking (DR-AVG) for models using CoDiQ Prompt. Each point represents model, demonstrating the positive correlation between increased reasoning computation and generated problem difficulty. Figure 3 Model Performance on CoDiQ-Bench Across Token Budgets. Average difficulty rank (%) of three model variants (Qwen3-8B with Direct Prompt, Qwen38B with CoDiQ Prompt, and CoDiQ-Gen-8B) under different token budget constraints (8k, 16k, 32k). Higher scores indicate better performance in handling difficult questions. Table 3 Performance of different Long-CoT models on CoDiQ-Bench. Group rankings based on the highest difficulty of questions generated across 8 rounds on CoDiQ-Bench. The best, the second-best and the third-best scores for each indicator are shown in box , bold and underlined, respectively."
        },
        {
            "title": "Tokens",
            "content": "DR-LLM DR-VN DR(AVG) Qwen3-8B Qwen3-14B Qwen3-32B Qwen3-8B Qwen3-14B Qwen3-32B 6.0 5.6 5.7 5.8 5.6 5."
        },
        {
            "title": "Direct Prompt",
            "content": "2439.7 4927.4 4124.9 33.5 45.6 65.3 CoDiQ Prompt(ours) 53.5 58.6 7282.2 9590.2 9762. 74.6 CoDiQ Generator(ours) 39.1 55.6 47.5 53.3 63.1 65.0 36.3 50.6 56. 53.4 60.9 69.8 CoDiQ-Gen-8B 5.9 12591.6 52. 72.2 62.4 4.3.1 Upper Bound of Difficulty Generation In 4.2.1, we evaluated the maximum solvable difficulty under the constraint of maintaining solution validity. However, this solvability requirement inherently limits the difficulty ceiling, as highly complex questions may not be unsolvable per se, but rather beyond the current models capability to generate valid solutions. To explore the theoretical upper bound of difficulty synthesisindependent of solution generation constraintswe conduct an ablation study by removing the solvability verification module from the CoDiQ Pipeline. The results indicate that incorporating the CoDiQ Prompt significantly elevates the difficulty ceiling across backbone models compared to standard prompting. Notably, despite having fewer parameters, our CoDiQGen-8B generates questions with difficulty upper bound that surpasses that of Qwen3-14B. This suggests that our specialized tuning and prompting strategy effectively unlocks the potential for synthesizing highly complex logical structures, even in smaller architectures."
        },
        {
            "title": "4.3.2 Impact of Max Token Budget",
            "content": "We further examine the efficiency of difficulty scaling relative to computational cost. Figure 3 illustrates the maximum difficulty of solvable questions generated by the CoDiQ Pipeline under strict constraints on accumulated token usage. To simulate resource-constrained environments, we enforce strict cumulative token budget that encompasses both generation and verification phases. If the total token consumption exceeds the threshold during an iteration, that round is discarded, and the system reports the highest-difficulty valid problem from the preceding rounds. The comparative analysis reveals that CoDiQ-Gen-8B exhibits distinct advantage across all token budget thresholds, consistently yielding higher difficulty scores than baseline models. Furthermore, we observe that Qwen3-8B utilizing the CoDiQ Prompt achieves superior performance compared to its direct prompt counterpart. This performance gap validates the effectiveness of our CoDiQ methodology in leveraging computational resources to maximize question difficulty while maintaining solvability."
        },
        {
            "title": "4.4 Scaling Tendency Analysis",
            "content": "Figure 4 Question Difficulty Scaling on CoDiQ-Bench. Normalized average difficulty ranking of questions generated by different Long-CoT models across 8 rounds. Higher rankings indicate higher question difficulty and better model performance. Figure 5 Question Solvability Scaling on CoDiQBench. Solvable rate of questions generated by different Long-CoT models across 8 rounds. Higher indicates indicate better question quality. The preceding analyses established the performance ceilings of different LRMs, identifying both their maximum solvable difficulty ( 4.2.1) and their theoretical upper bounds ( 4.3.1). However, these metrics represent static endpoints. To understand how these models arrive at such complexity, we now shift to fine-grained analysis of the generation dynamics. In this section, we track the scaling tendencies of difficulty and solvability relative to reasoning computation within specific model groups (More results are provided in Appendix F). 4.4.1 Difficulty Scaling We analyze problem complexity evolution across 8 generation rounds in Figure 4. Compared to the Direct Prompt, the CoDiQ Prompt significantly stimulates deeper reasoning, resulting in marked increase in token consumption. While consistent upward difficulty trajectory is observed across most models, large-parameter models tend to saturate in later rounds. We attribute this plateau to the substantial token consumption, which likely approaches the upper bound of either the models generation capacity or the difficulty evaluators limit. Furthermore, this analysis corroborates the findings in Section 4.2.2 from single-model perspective, reinforcing the conclusion that token volume serves as robust indicator of difficulty."
        },
        {
            "title": "4.4.2 Solvability Scaling",
            "content": "We examine how solvability rates degrade with increasing difficulty (Figure 5). This degradation reveals fundamental trade-off between problem difficulty and validity. Three key findings emerge: Robustness of SOTA Models: Flagship models (e.g., GLM-4.6) maintain high solvability across all difficulty levels, demonstrating well-balanced generation-verification capabilities. Over-Reasoning Pitfall: Smaller models experience validity collapse under CoDiQ, as they generate complexity beyond their reasoning capacity. Efficacy of RL Alignment: CoDiQ-Gen-8B breaks this degradation pattern through RL, successfully decoupling difficulty scaling from validity loss."
        },
        {
            "title": "4.5 Effectiveness of CoDiQ-Corpus",
            "content": "To comprehensively assess the value of this corpus, we conduct multi-dimensional evaluation focusing on difficulty(Section4.5.1), quality(Section4.5.2), and training effectiveness(Section4.5.3). We first demonstrate that CoDiQ-Corpus significantly surpasses existing competition-grade benchmarks in problem hardness. Subsequently, we verify the logical soundness and solvability of the generated problems through rigorous human evaluation. Finally, we validate the practical utility of the corpus by employing it in curriculum learning framework, demonstrating its capability to drive continuous improvements in reasoning models. 4.5.1 Difficulty Comparison To validate the elevated difficulty of CoDiQ-Corpus, we randomly sample 300 questions from each dataset, including CoDiQ-Corpus, AIME [30], NuminaMath-1.5 [17], LiveCodeBench [16], and Code-Contests [19], and compare them using the ranking methodology in Section 4.1. As shown in Table 4, our CoDiQ-Corpus demonstrates significantly higher difficulty than existing competition-level datasets. Table 4 Datasets Difficulty Comparison. The best, the second-best and the third-best scores for each indicator are shown in box , bold and underlined, respectively. Table 5 Model Performance Comparison. The best, the second-best and the third-best scores for each indicator are shown in box , bold and underlined, respectively. Dataset DR-LLM DR-VN DR(AVG) Model MATH-500 AIME 2024 Baselines AIME(1983-2024) NuminaMath-1.5 LiveCodeBench Code-Contests 57.9 27.5 39.4 47. 45.1 32.0 45.2 41.0 51.5 29.8 42.3 44.1 CoDiQ Dataset(ours) CoDiQ-Corpus 91. 82.8 87.1 Qwen3-4B Qwen3-RL-4B Baselines 94.4 95.2 63.1 64.3 Curriculum Learning Models(ours) 65. CoDiQ-L1-4B CoDiQ-L2-4B CoDiQ-L3-4B 96.0 94.8 96.0 66.7 70.6 4.5.2 Human Quality Assessment To verify the reliability of our CoDiQ-Corpus and CoDiQ Pipeline, we conducted human evaluation on = 200 stratified samples from accepted CoDiQ-Corpus and rejected cases. Three PhD experts independently assessed Clarity, Completeness, and Reasoning Validity (Appendix G), achieving substantial agreement (Fleiss κ = 0.76). Results show 82% precision for accepted instances and 90% NPV for rejected cases. Notably, error analysis on the false negatives (valid problems incorrectly rejected) empirically reveals the Verifier Paradox : these instances were logically sound but exceeded the verifiers reasoning horizon, causing the model to misclassify them as unsolvable rather than hard. This confirms that our pipelines upper bound is currently capped by the verifiers capability."
        },
        {
            "title": "4.5.3 Training Effectiveness Validation",
            "content": "Reinforcement Learning Validation via Curriculum. distinct advantage of CoDiQ lies in its inherent controllability. By adjusting the token budget, it generates question sequences of progressive difficulty, naturally facilitating curriculum learning strategy [3] that aligns with the models evolving capabilities. Leveraging this, we implement multi-stage reinforcement learning paradigm by sequentially training models CoDiQ-Li-4B (i {1, 2, 3}), where each stage utilizes dataset subset of increasing difficulty. Rewards are derived by prompting Qwen3-32B to evaluate response quality via weighted aggregation (details in Appendix H). We compare our approach against vanilla Qwen-4B and Qwen3-RL-4B, baseline trained via standard RL on original datasets without stratification. Evaluation results on MATH-500 and AIME 2024  (Table 5)  demonstrate that our budget-controlled curriculum framework significantly enhances performance compared to standard training paradigms, thereby validating the effectiveness and utility of our CoDiQ-Corpus."
        },
        {
            "title": "5 Conclusion & Limitations",
            "content": "We presented CoDiQ, principled framework for synthesizing verifiable, high-difficulty reasoning problems at scale. By addressing the generator capacity ceiling through test-time scaling and mitigating \"fake hard\" instances via hybrid verification pipeline, we successfully trained the CoDiQ-Generator using reinforcement learning. The resulting CoDiQ-Corpus features budget-driven difficulty stratification, and its effective application in curriculum learning validates the methods superiority. We open-source our pipeline to facilitate future research into scaling laws and automated curriculum learning. However, we acknowledge certain limitations. Our scope is currently restricted to English math/code tasks, and the verification cost limits real-time use. Most critically, our pipeline faces the Verifier Paradox : relying on fixed-capacity verifier creates an epistemic ceiling, where valid problems exceeding the verifiers capabilities are at risk of being discarded as unsolvable. Future work must address this scalable oversight challenge."
        },
        {
            "title": "Impact Statement",
            "content": "Our work provides foundational framework for scaling the difficulty of synthetic reasoning data while maintaining logical validity. By decoupling problem complexity from human curation, this research facilitates the development of more robust reasoning capabilities in AI systems across mathematical and programming domains. While this enables rapid progress in model performance, it also underscores the importance of integrating strict solvability constraints to prevent the degradation of data quality in automated training loops."
        },
        {
            "title": "References",
            "content": "[1] Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. [2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [3] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 4148, 2009. [4] Sahil Chaudhary. Code alpaca: An instruction-following llama model for code generation. https://github.com/ sahil280114/codealpaca, 2023. [5] Zhuofan Chen, Jiyuan He, Yichi Zhang, Xing Hu, Haoxing Wen, Jun Bai, and Wenge Rong. Cogatom: From cognitive atoms to olympiad-level mathematical reasoning in large language models, 2025. URL https://arxiv. org/abs/2509.17318. [6] ChilleD. Svamp dataset, 2024. [7] Bryan Christ, Jonathan Kropko, and Thomas Hartvigsen. Mathwell: Generating educational math word problems using teacher annotations, 2024. URL https://arxiv.org/abs/2402.15861. [8] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [9] Yuyang Ding, Xinyu Shi, Juntao Li, Qiaoming Zhu, Min Zhang, et al. Unleashing reasoning capability of llms via scalable question synthesis from scratch. arXiv preprint arXiv:2410.18693, 2024. [10] Yuyang Ding, Xinyu Shi, Xiaobo Liang, Juntao Li, Zhaopeng Tu, Qiaoming Zhu, and Min Zhang. Unleashing llm reasoning capability via scalable question synthesis from scratch, 2025. URL https://arxiv.org/abs/2410.18693. [11] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. [12] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. URL https: //arxiv.org/abs/2402.14008. [13] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with apps. NeurIPS, 2021. [14] hiyouga. Math12k dataset, 2025. [15] Hanxu Hu, Xingxing Zhang, Jannis Vamvas, Rico Sennrich, and Furu Wei. Quest: Incentivizing llms to generate difficult problems, 2025. URL https://arxiv.org/abs/2510.17715. [16] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [17] Lewis Tunstall Jia LI, Edward Beeching et al. Numinamath tir, 2024. [18] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen tau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: natural and reliable benchmark for data science code generation. ArXiv, abs/2211.11501, 2022. [19] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378 (6624):10921097, 2022. [20] Haoxiong Liu, Yifan Zhang, Yifan Luo, and Andrew Chi-Chih Yao. Augmenting math word problems via iterative question composing, 2024. URL https://arxiv.org/abs/2401.09003. [21] Thang Luong, Dawsen Hwang, Hoang H. Nguyen, Golnaz Ghiasi, Yuri Chervonyi, Insuk Seo, Junsu Kim, Garrett Bingham, Jonathan Lee, Swaroop Mishra, Alex Zhai, Clara Huiyi Hu, Henryk Michalewski, Jimin Kim, Jeonghyun Ahn, Junhwi Bae, Xingyou Song, Trieu H. Trinh, Quoc V. Le, and Junehyuk Jung. Towards robust mathematical reasoning, 2025. URL https://arxiv.org/abs/2511.01846. [22] Chaitanya Manem, Pratik Prabhanjan Brahma, Prakamya Mishra, Zicheng Liu, and Emad Barsoum. Sandmath: Using llms to generate novel, difficult and useful mathematics questions and answers. arXiv preprint arXiv:2507.20527, 2025. [23] Qizhi Pei, Zhuoshi Pan, Honglin Lin, Xin Gao, Yu Li, Zinan Tang, Conghui He, Rui Yan, and Lijun Wu. Scalediff: Scaling difficult problems for advanced mathematical reasoning. arXiv preprint arXiv:2509.21070, 2025. [24] Qizhi Pei, Zhuoshi Pan, Honglin Lin, Xin Gao, Yu Li, Zinan Tang, Conghui He, Rui Yan, and Lijun Wu. Scalediff: Scaling difficult problems for advanced mathematical reasoning, 2025. URL https://arxiv.org/abs/2509.21070. [25] Bytedance Seed. Seed1. 8 model card: Towards generalized real-world agency. 2025. [26] Vedant Shah, Dingli Yu, Kaifeng Lyu, Simon Park, Jiatong Yu, Yinghui He, Nan Rosemary Ke, Michael Mozer, Yoshua Bengio, Sanjeev Arora, et al. Ai-assisted generation of difficult math questions. arXiv preprint arXiv:2407.21009, 2024. [27] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [28] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pages 12791297, 2025. [29] Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, Lei Fang, and Ji-Rong Wen. Challenging the boundaries of reasoning: An olympiad-level math benchmark for large language models, 2025. URL https://arxiv.org/abs/2503.21380. [30] Hemish Veeraboina. Aime problem set 1983-2024, 2023. URL https://www.kaggle.com/datasets/ hemishveeraboina/aime-problem-set-1983-2024. [31] Shengbo Wang, Mingwei Liu, Zike Li, Anji Li, Yanlin Wang, Xin Peng, and Zibin Zheng. Evolmatheval: Towards evolvable benchmarks for mathematical reasoning via evolutionary testing, 2025. URL https://arxiv.org/abs/ 2508.13003. [32] Yunhui Xia, Wei Shen, Yan Wang, Jason Klein Liu, Huifeng Sun, Siyue Wu, Jian Hu, and Xiaolong Xu. Leetcodedataset: temporal dataset for robust evaluation and efficient training of code llms. arXiv preprint arXiv:2504.14655, 2025. [33] Roy Xie, Chengxuan Huang, Junlin Wang, and Bhuwan Dhingra. Adversarial math word problem generation. arXiv preprint arXiv:2402.17916, 2024. [34] Roy Xie, Chengxuan Huang, Junlin Wang, and Bhuwan Dhingra. Adversarial math word problem generation, 2024. URL https://arxiv.org/abs/2402.17916. [35] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [36] yimingzhang. asdiv dataset, 2025. [37] Charlie Zhang, Graham Neubig, and Xiang Yue. On the interplay of pre-training, mid-training, and rl on reasoning language models. arXiv preprint arXiv:2512.07783, 2025. [38] Xueliang Zhao, Wei Wu, Jian Guan, Zhuocheng Gong, and Lingpeng Kong. Promptcot 2.0: Scaling prompt synthesis for large language model reasoning. arXiv preprint arXiv:2509.19894, 2025. 13 [39] Xinyue Zheng, Haowei Lin, Shaofei Cai, Zilong Zheng, and Yitao Liang. Unicode: framework for generating high quality competitive coding problems, 2025. URL https://arxiv.org/abs/2510.17868. [40] Shang Zhou, Zihan Zheng, Kaiyuan Liu, Zeyu Shen, Zerui Cheng, Zexing Chen, Hansen He, Jianzhu Yao, Huanzhi Mao, Qiuyang Mang, Tianfu Fu, Beichen Li, Dongruixuan Li, Wenhao Chai, Zhuang Liu, Aleksandra Korolova, Peter Henderson, Natasha Jaques, Pramod Viswanath, Saining Xie, and Jingbo Shang. Autocode: Llms as problem setters for competitive programming, 2025. URL https://arxiv.org/abs/2510.12803. [41] Yubo Zhu, Dongrui Liu, Zecheng Lin, Wei Tong, Sheng Zhong, and Jing Shao. The llm already knows: Estimating llm-perceived question difficulty via hidden representations. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 11601176, 2025."
        },
        {
            "title": "Appendix",
            "content": "A CoDiQ Pipeline: Case Study We demonstrate the CoDiQ pipeline through complete workflow from an initial easy problem to iterative difficulty escalation, illustrating both successful upgrades and failure modes. Each generated problem undergoes solvability verification (Appendix J) and difficulty assessment (Appendix I)."
        },
        {
            "title": "Initial Problem",
            "content": "A.1 Problem Statement: Count subsequences with an odd sum from array nums, returning the result modulo 109 + 7. Example: For nums = [1,1,1], the answer is 4 (subsequences from positions: {0}, {1}, {2}, {0, 1, 2}, all with odd sums). Solution: Simple DP tracking sum parity in O(n) time. A.2 Round 1: Controlled Escalation A.2.1 Upgraded Problem Count non-empty subsequences satisfying three simultaneous conditions: 1. Sum is odd 2. Length is even 3. Sum mod3 = Difficulty Enhancement: The upgrade introduces multi-dimensional state tracking, expanding the DP state space from 2 (sum parity) to 2 2 3 = 12 states (sum parity, length parity, sum mod 3). A.2.2 Verification Solvability Score: 0.90 Time Complexity: O(12n) 1.2 106 operations for = 105 (feasible) Solution Density: 8% of subsequences satisfy all conditions (non-trivial) Solvability: PASS Difficulty: INCREASED A.3 Round 2: Further Escalation A.3.1 Upgraded Problem Count subsequences satisfying five conditions: 1. Sum is odd 2. Length is even 3. Sum mod3 = 1 4. Sum mod5 = 2 5. Length mod4 = 2 15 Mathematical Simplification: By the Chinese Remainder Theorem (CRT), conditions 1, 3, and 4 can be unified: sum (mod 2), sum 1 (mod 3), sum 2 (mod 5) sum 7 (mod 30) The effective state space becomes 30 4 = 120 states. A.3.2 Verification Solvability Score: 0.85 Time Complexity: O(120n) 1.2 107 operations for = 105 (acceptable) Solution Density: 0.83% (still non-trivial) Solvability: PASS Difficulty: UNCHANGED A.4 Round 3: Over-Escalation Failure A.4.1 Upgraded Problem Count subsequences satisfying six conditions: 1. Sum is odd 2. Sum mod3 = 1 3. Sum mod5 = 2 4. Sum mod7 = 4 5. Sum mod11 = 6 6. Length mod8 = 2 (which ensures even length) By CRT, conditions 15 unify to sum (mod 2310) for some constant c, yielding state space of 2310 8 = 18,480 states. A.4.2 Verification Solvability Score: 0.65 Solvability: FAIL Difficulty: INCREASED Failure Analysis: 1. Computational Infeasibility Time complexity: O(18,480n) 1.8 109 operations for = 105 Exceeds practical competitive programming limits (typically 108109 operations within time constraints) 2. Solution Space Collapse (Critical Issue) While constraints are mathematically consistent via CRT, they create an extremely sparse solution space Probability that random subsequence satisfies all conditions: 1 2310 1 8 = 18,480 Expected number of valid subsequences: 2n 18,480 For 14: 214 18,480 0.89 < 1 Practical impact: For typical inputs with small to moderate n, the answer is almost always 0, making the problem vacuously trivial Pipeline Termination: The pipeline correctly terminates at Round 3, discarding Q3 and outputting {Q0, Q1, Q2}. Despite the increased theoretical difficulty, the problem becomes unsolvable due to computational infeasibility and solution space collapse, demonstrating the effectiveness of solvability verification in preventing quality degradation. CoDiQ Pipeline: Failure Type Analysis To systematically understand the failure modes of the CoDiQ pipeline, we conduct comprehensive clustering analysis on the collected failure reasons. Our analysis follows three-stage hierarchical approach: initial K-means clustering, keyword extraction, and hierarchical merging with manual refinement. B.1 Clustering Methodology Stage 1: K-means Pre-clustering. We first apply K-means clustering to the failure reason descriptions to obtain an initial partitioning of the data. This pre-clustering step reduces computational complexity and provides coarse-grained grouping of similar failure patterns. Stage 2: Keyword Extraction. For each cluster obtained from K-means, we extract representative keywords using TF-IDF weighting. These keywords serve as semantic signatures that characterize the dominant failure patterns within each cluster, facilitating interpretability and subsequent hierarchical analysis. Stage 3: Hierarchical Clustering and Manual Refinement. We then perform hierarchical clustering on the cluster centroids, leveraging the extracted keywords to compute semantic similarity. Finally, we manually merge related clusters and consolidate small clusters (containing fewer than predefined threshold of samples) with their semantically nearest neighbors. This hybrid approach balances computational efficiency with semantic coherence. B.2 Failure Category Distribution Table 6 categorizes the identified failure modes. The analysis reveals two dominant distinct failure dynamics: validity breaches (Unsolvable) and complexity degradation (Difficulty Decreased). Unsolvable Scenarios. The majority of pipeline failures stem from fundamental deficits in problem formulation. Specifically, Definition & Information Missing combined with Constraints & Logic Conflicts collectively account for the lions share of unsolvable cases. This indicates that the primary challenge lies not in parsing or formatting (which constitute negligible fraction), but in the models capacity to maintain semantic consistency and logical completeness during generation. Difficulty Preservation. critical observation is the prevalence of the Difficulty Decreased category (N = 12, 916). In these instances, the generated problems remain solvable but fail to meet the intended cognitive demand. The high frequency of Constraint Simplification and Numerical Range Reduction suggests model tendency towards \"safe\" or simplified generative paths, inadvertently pruning the solution space or removing key logical hurdles required for high-quality mathematical reasoning."
        },
        {
            "title": "C ValueNetwork Training Detail",
            "content": "C.1 Dataset split We compiled labeled dataset by selecting samples from standard benchmarks [8, 32] for the Easy class and competition-level datasets [13, 17] for the Hard class. We maintained an easy-to-hard ratio of 2:3 to prioritize the identification of challenging samples. We partition the compiled dataset into an 85:15 train-test split to ensure robust evaluation. 17 Table 6 Distribution of Failure Cases in CoDiQ Pipeline Failure Type Unsolvable Failure Subtype Definition & Information Missing Constraints & Logic Conflicts Computationally Infeasible Implementation Details Missing No Suitable Algorithm/Structure Overly Complex Requires Specific Capability Parsing & Rule Ambiguity Number-Theoretic Constraints Other Difficulty Decreased Constraint Simplification Numerical Range Reduction Key Condition Removal Solution Space Narrowing Structural Simplification Other Count 8,630 8,142 1,948 1,926 1,285 1,104 726 611 566 1,184 3,245 2,890 2,654 1,987 1,456 684 C.2 Training Data C.2.1 Input Features For training data, we employ Qwen3-8B (in non-thinking mode) to capture generation dynamics. We define sampling window from the last token of the question extending to min(4096, Lr) generated tokens. Within this window, we apply quadratic sampling strategy to select hidden states (K = 10 for windows > 1024, else = 8) at positions: (cid:36) pi = (cid:18) (cid:19)2(cid:37) 1 , = 0, 1, . . . , 1. (4) C.2.2 Output Labels This strategy allocates higher sampling density to the onset of generation, capturing critical information for establishing the reasoning path. To mitigate stochasticity, scores are averaged over 5 independent passes. For each question, we generate response using Qwen3-8B and assign binary label {0, 1} based on the final answers correctness. The input features are extracted via the quadratic sampling strategy (Eq. 4) applied to the first 4096 tokens. C.3 Network Architecture The Value Network is implemented as lightweight Multi-Layer Perceptron (MLP) designed to project high-dimensional hidden states (din = 4096) to scalar correctness score. The architecture consists of an initial projection layer, Layer Normalization, GELU activation, and final regression head. This setup allows the network to minimize the discrepancy with the correctness label via weighted binary cross-entropy objective, effectively estimating the likelihood of successful generation solely from the reasoning dynamics captured in the early stages. 18 Table 7 Configuration and performance evaluation of the Value Network. Table 8 Hyperparameter settings. Hyperparameter Value Input Dim (din) Hidden Dim Batch Size Learning Rate Weight Decay Dropout Optimizer Scheduler Max Epochs Split 4096 512 512 1 104 1 102 0.3 AdamW StepLR (γ = 0.8) 30 85% / 15% Table 9 Performance on held-out test set. Metric Accuracy Precision Recall F1 Score ROC-AUC PR-AUC Value 72.52% 54.21% 95.62% 69.20% 84.84% 65.77% C.4 Training Configuration The model is trained using the AdamW optimizer with step learning rate scheduler. To address class imbalance, we apply positive class weight in the loss function, dynamically calculated as the ratio of negative to positive samples. Complete hyperparameter settings are listed in Table 8. C.5 Performance Evaluation We evaluate the trained Value Network on the held-out test set (15% split). As shown in Table 9, the model achieves an ROC-AUC of 84.84%, demonstrating robust discriminative power in distinguishing correct reasoning paths from incorrect ones despite the challenging nature of the dataset. It is worth noting that our training strategy prioritizes identifying all potential correct answers. This is reflected in the high Recall of 95.62%, which ensures that the Value Network successfully preserves valid reasoning paths. While this focus on coverage results in moderate Precision (54.21%) due to the trade-off inherent in class-weighted training, the high ROC-AUC indicates that the predicted scores effectively rank correct generations higher, making the model reliable for difficulty estimation and filtering. CoDiQ-Bench Selection Criteria To ensure the quality and reliability of our benchmark, we establish three primary criteria for data selection: Solvability: We verify that each problem is well-defined and admits at least one valid solution, ensuring the benchmarks validity and fairness. Difficulty Level: We assess whether the difficulty level is appropriate for the intended evaluation purpose, maintaining balanced distribution across different complexity levels. Quality Assessment: We conduct rigorous quality checks to ensure that all selected problems meet acceptable standards in terms of clarity, correctness, and relevance. Statistics of CoDiQ-Corpus We employ CoDiQ-Gen-8B following the CoDiQ Pipeline (Section 3.3) to transform eight diverse mathematical and programming datasets into the more challenging CoDiQ-Corpus, which comprises approximately 44,453 question sequences with progressive difficulty from easy to hard. The detailed distribution is presented in Table 10. 19 Table 10 Dataset statistics of CoDiQ-Corpus. Dataset Question Tokens Length AVG Round Category Sequences Math12K [14] GSM8K [8] SVAMP [6] ASDiv [36] CodeAlpaca20K [4] LeetCodeDataset [32] MBPP [2] DS-1000 [18] Minimum Maximum Average 7,829 6,896 3,992 4,703 7,174 4,365 3,440 4,138 995.4 1,093.7 971.3 1,013.1 1,106.1 1,281.0 1,000.4 1,240.7 38 52 172 55 70 254 52 Total 38 7,829 1,073.0 4.7 4.5 3.3 4.7 3.8 3.8 3.4 3.2 4. Math Math Math Math Code Code Code Code - 11,764 8,685 804 1,480 17,845 2,027 876 972 44,"
        },
        {
            "title": "F Scaling Tendency Analysis Details",
            "content": "This section presents the complete scaling tendency analysis with all evaluated models. Figure 6 shows the full results of difficulty and solvability scaling across 8 generation rounds for all Long-CoT models under both Direct Prompt and CoDiQ Prompt settings. The complete results reveal consistent scaling patterns across all models: (1) increased reasoning computation correlates with higher problem difficulty, and (2) trade-off exists between difficulty and solvability, with larger models maintaining better balance between the two metrics. CoDiQ-Corpus Quality Criteria We establish rigorous criteria to assess the quality and solvability of problems in CoDiQ-Corpus. Three PhD-level domain experts independently evaluate 300 randomly sampled problems following these standardized guidelines: G."
        },
        {
            "title": "Information Completeness",
            "content": "Sufficient Parameters: All necessary numerical values, variables, and constraints are explicitly provided. Clear Objectives: The problem goal is unambiguous and well-defined. Complete Context: No truncation or missing problem statements. G.2 Logical Consistency Non-contradictory Conditions: All given constraints are mutually consistent. Valid Premises: For logical problems, premises are sufficient to support conclusions. Feasible Solutions: The problem admits at least one valid solution path. G.3 Problem Well-definedness Determinable Answer: The answer can be uniquely determined or bounded within reasonable range. Appropriate Scope: The problem complexity matches its stated domain and difficulty level. Standard Formulation: Follows conventional mathematical or logical notation. Figure 6 Complete Scaling Analysis on CoDiQ-Bench. Normalized average difficulty ranking (top row) and solvable rate (bottom row) of questions generated by all evaluated Long-CoT models across 8 rounds, using Direct Prompt (left) and CoDiQ Prompt (right). Higher rankings indicate higher question difficulty; higher rates indicate better question quality. G.4 Evaluation Protocol Each expert assigns binary solvability label (solvable/unsolvable) with confidence scores. problem is marked as solvable only when at least two experts agree. Disagreements are resolved through discussion. The inter-annotator agreement (Fleiss κ) reaches 0.78, indicating substantial consensus."
        },
        {
            "title": "H Curriculum learning Detail",
            "content": "H.1 Training Data Selection for Curriculum Learning To validate the effectiveness of CoDiQ-Corpus for curriculum learning, we carefully select question sequences with progressive difficulty structures. Specifically, we sample 480 question sequences from CoDiQ-Corpus where each sequence length 3, forming the curriculum learning dataset: Dcurriculum = {Sn}480 n=1, Sn 3 (5) For each sequence Sn = {qn with increasing complexity: 0 , qn 1 , qn 2 , . . . , qn Sn1} with progressive difficulty, we construct three training stages Level 1 (L1): Contains all initial questions qn difficulty progression. from each sequence, representing the starting point of each 21 Level 2 (L2): Randomly samples one question from intermediate positions {qn 1 , qn 2 , . . . , qn Sn2} for each sequence, capturing mid-stage complexity. Level 3 (L3): Contains all final questions qn level within each progression. Sn1 from each sequence, representing the highest difficulty Formally, the data selection strategy is defined as: 0 Sn Dcurriculum} L1 = {qn L2 = {random({qn L3 = {qn }Sn2 i=1 Sn1 Sn Dcurriculum} ) Sn Dcurriculum} (6) (7) (8) This design ensures clear difficulty progression: Difficulty(L1) < Difficulty(L2) < Difficulty(L3). The sample distribution across levels follows the ratio L1 : L2 : L3 = 2 : 2 : 1, achieved by duplicating L1 and L2 during training to balance exposure to different difficulty levels. This ratio is designed to provide sufficient foundational learning before progressing to more challenging problems, following curriculum learning principles [3]. For the baseline model Qwen3-RL-4B, we use the original untransformed datasets (before applying the CoDiQ Pipeline) as training data, maintaining the same total number of training samples to ensure fair comparison. This allows us to isolate the impact of progressive difficulty transformation on model performance. Training Schedule: Models are trained sequentially through three stages: 1. CoDiQ-L1-4B: Trained on L1 (starting level) 2. CoDiQ-L2-4B: Initialized from CoDiQ-L1-4B, further trained on L2 (intermediate level) 3. CoDiQ-L3-4B: Initialized from CoDiQ-L2-4B, further trained on L3 (advanced level) H.2 Reward Signal Design We design multi-dimensional reward function to evaluate answer quality by prompting Qwen3-32B as an expert evaluator. The reward signal [0, 1] is computed based on four key dimensions: Evaluation Dimensions: Problem Resolution (spr): Measures how completely the answer addresses all aspects of the question (0.0-1.0). Reasoning Correctness (src): Evaluates the correctness and coherence of the reasoning process (0.0-1.0). Information Completeness (sic): Assesses whether all necessary information, steps, and explanations are included (0.0-1.0). Accuracy (sacc): Measures factual correctness, calculation accuracy, and conceptual clarity (0.0-1.0). The reward function aggregates these dimensions with carefully tuned weights optimized for high-difficulty mathematical reasoning tasks: = wpr spr + wrc src + wic sic + wacc sacc where the default weights are set as: wpr = 0.20, wrc = 0.35, wic = 0.25, wacc = 0.20 (9) (10) This configuration emphasizes reasoning quality (35%) and information completeness (25%), which are critical for complex problem-solving. The evaluation prompt instructs Qwen3-32B to assess each dimension independently using continuous scores and return results in JSON format. Special handling is applied for 22 edge cases, such as correctly identifying unsolvable problems, which receives high problem resolution scores (0.8-1.0) despite not providing numerical solution. To ensure evaluation quality, we implement automatic validation of the returned scores, retry mechanisms (up to 3 attempts), and text truncation to handle long inputs (max 4096 tokens for questions, 16384 tokens for answers). The confidence score returned by the evaluator helps identify uncertain assessments for potential manual review. I"
        },
        {
            "title": "Instruction for LLMs Ranking",
            "content": "You are an expert in assessing question difficulty. Evaluate questions based on: 1. Knowledge Complexity: Number and depth of concepts required 2. Cognitive Load: Reasoning levels and abstract thinking needed 3. Computational Complexity: Steps and calculations involved 4. Traps and Common Mistakes: Hidden pitfalls in the question 5. Integration Skills: Cross-domain knowledge application required Your task is to group questions by difficulty level and sort groups from easiest to hardest. Important: Questions with the SAME difficulty level should be grouped together. Analyze each question carefully and return them grouped by difficulty level. Output format requirements: Return ONLY valid JSON object with TWO fields: result: list of lists (groups), each containing question indices of the SAME difficulty level reason: list of strings, where reason[i] explains why questions in result[i] share the same difficulty Groups in both arrays should be ordered from easiest to hardest The length of \"result\" and \"reason\" arrays MUST be identical Use 0-based indexing matching the input order Example output format: { \" result \": [[1 , 3] , [0] , [2 , 4]] , \" reason \": [ \" Both require only basic arithmetic operations with single - step reasoning \" , \" Multi - step algebraic manipulation with intermediate concepts \" , \" Complex integration of advanced concepts and non - obvious strategies \" ] } This means: Questions 1 and 3 are easiest (Group 0) - they both involve only basic arithmetic and single-step reasoning 23 Question 0 is medium difficulty (Group 1) - it requires multi-step algebraic manipulation with intermediate concepts Questions 2 and 4 are hardest (Group 2) - they both demand complex integration of advanced concepts and non-obvious strategies Important: Each reasoning string should explain the COMMON difficulty characteristics that unite all questions in the corresponding group Ensure reason[i] corresponds to result[i] for all groups Please group the following questions by difficulty level and sort groups from easiest to hardest: {questions} Return the result as JSON object with format: { } \" result \": [[ indices of easiest group ] , [ indices of next group ] , ...] , \" reason \": [\" reasoning for group 0\" , \" reasoning for group 1\" , ...]"
        },
        {
            "title": "J Instructions for Solvability Check",
            "content": "Instructions for Solvability Check You are an expert in analyzing mathematical and logical problems. Your task is to determine whether given question is solvable. question is considered SOLVABLE if: 1. It provides all necessary information and conditions 2. The problem is well-defined with clear objectives 3. It has determinable answer (even if complex) 4. The constraints are consistent (not contradictory) question is considered UNSOLVABLE if: 1. Missing critical information or parameters 2. Contains contradictory conditions 3. The problem statement is ambiguous or unclear 4. Asks for information that cannot be determined from given data 5. The question is incomplete or truncated Important Guidelines: Be strict but reasonable in your judgment Consider if reasonable person could solve the problem with the given information For mathematical problems, check if all necessary values are provided For logical problems, verify if the premises are sufficient for the conclusion Output format requirements: Return ONLY valid JSON object 24 Must have exactly these fields: \"solvable\": boolean (true/false) \"confidence\": number (0.0-1.0, your confidence in the judgment) \"reason\": string (brief explanation in English, max 200 characters) \"missing_info\": list of strings (what information is missing, empty list if solvable) Example Outputs: {\" solvable \": true , \" confidence \": 0.95 , \" reason \": \" All necessary parameters provided , problem is well - defined \" , \" missing_info \": []} {\" solvable \": false , \" confidence \": 0.85 , \" reason \": \" Missing the radius value needed to calculate circle area \" , \" missing_info \": [\" radius \"]}"
        },
        {
            "title": "K Instruction for Direct Prompt",
            "content": "Instruction for Direct Prompt # Problem Difficulty Upgrade Generator ## Task Description You are an expert competitive programming problem creator. Your task is to take given problem and create significantly more challenging, competition-level version. ## Input Original Problem: {original_problem} ## Output Format Return ONLY the new upgraded problem, nothing else. [Your upgraded competitive programming problem here]"
        },
        {
            "title": "L Instruction for CoDiQ Prompt",
            "content": "Instruction for CoDiQ Prompt # Problem Difficulty Upgrade Generator ## Task Description You are an expert competitive programming problem creator. Your task is to take given problem and create significantly more challenging, competition-level version by strategically adding difficulty elements that test deeper understanding and more complex reasoning. ## Design Standards (Mandatory Quality Check) To ensure the upgraded problem is competition-worthy, you must strictly adhere to these principles: 25 1. Deep Synthesis: The difficulty element must naturally intertwine with the original logic. The solution should feel like single cohesive challenge, not \"patchwork\". 2. Multi-Step Reasoning: The solution must require 2-3 non-trivial intermediate logical jumps. The solver must derive lemmas or intermediate states before applying standard algorithms. 3. No Trivial Upgrades: Avoid simply increasing to 105 if the logic remains O(N ). The upgrade must force change in complexity class (e.g., from Greedy to Flow, from Simulation to Matrix Exponentiation). 4. Disguise & Abstraction: (If applicable) Hide the core theorem or data structure behind unique story or abstract mathematical setting. Never explicitly name the required algorithm. ## Difficulty Elements Library (Select 1-2 distinct elements) ### Category A: Dimensionality & Constraints Best for: Array/Sequence/Tree problems with simple naive solutions (e.g., O(N ), O(N 2), or O(N 3)). Avoid when: Original problem already requires logarithmic or sublinear complexity. Description: Explode the data scale or dimensionality to invalidate simple simulation or brute force. Core Strategy: 1. Identify the naive complexity (e.g., O(N ), O(N 2), or O(N 3)). 2. Impose constraints that compel superior complexity class (e.g., O(log ) or O(N log )). 3. Introduce dynamic updates, higher-dimensional spaces, or multiple query types to break linear scans. Examples: [ { \" original \": \" Given an array of size ( N1000) , find the sum of elements in range [L , ].\" , \" upgrade \": \" Given an array of size ( N10^5) , handle ( M10^5) operations : 1. Update range [L , ] by adding . 2. Query sum of range [L , ]. ( Requires Segment Tree with Lazy Propagation ) \" \" original \": \" Given grid , find the shortest path from (0 ,0) to (R , ) avoiding obstacles .\" , \" upgrade \": \" Given grid where obstacles appear and disappear at specific time intervals modulo . Find the shortest path . ( Requires BFS in State Space (x , , time % ) ) \" \" original \": \" Find the maximum value in an array .\" , \" upgrade \": \" Given tree with nodes ( N10^5) , support path updates ( add value to all nodes on path - ) and path maximum queries . ( Requires Heavy - Light Decomposition ) \" } , { } , { } , { 26 \" original \": \" Given set of points , find the two closest points .\" , \" upgrade \": \" Given set of points in 3 space , find the size of the largest subset where every pair has Manhattan distance > . ( Requires Coordinate Transformation + Data Structures ) \" } , { \" original \": \" Check if string contains pattern .\" , \" upgrade \": \" Given text and patterns . Support dynamic insertion of new patterns and query if any pattern appears in . ( Requires Aho - Corasick Automaton or Suffix Structures ) \" } ] ### Category B: Mathematical Abstraction Best for: Problems that can be reframed into mathematical structures, e.g., simulation or iterative problems with clear patterns. Avoid when: The problem is already focused on advanced specialized theorems or complex data structures. Description: Transform procedural or descriptive problem into formal model, e.g., using number theory, combinatorics, or game theory. Core Strategy: 1. Increase constraints to push beyond computational limits (e.g., 1018), making simple iteration or simulation impossible. 2. Force the discovery of underlying structures, such as closed-form formulas, recurrence relations, or invariant properties. 3. Introduce formal constraints (e.g., modular arithmetic, coordinate systems) that require rigorous mathematical modeling. Anti-pattern: Simply making large without ensuring mathematical insight exists is not valid. Examples: [ { } , { } , \" original \": \" Simulate process where bacteria double every hour . Find count at hour ( N50) .\" , \" upgrade \": \" Bacteria have complex growth rule ( ) = * (n -1) + * (n -2) . Find count at hour ( N10^18) modulo 10^9+7. ( Requires Matrix Exponentiation ) \" \" original \": \" Given items , in how many ways can you pick items ?\" , \" upgrade \": \" Given items with specific color constraints , calculate the number of ways to pick items modulo 10^9+7 where is up to 10^9. ( Requires Lucas Theorem or Generating Functions ) \" 27 { } , { } , { \" original \": \" Two players take turns removing 1 -3 stones . Who wins ?\" , \" upgrade \": \" Played on graph with N10^5 nodes . token moves along edges . player loses if they cannot move . The graph has cycles . ( Requires Game Theory on Graphs / Sprague - Grundy with loop handling ) \" \" original \": \" Calculate the Greatest Common Divisor ( GCD ) of two numbers .\" , \" upgrade \": \" Calculate the sum of GCD (i , ) for all 1 , where N10^7. ( Requires Euler Totient Function / Mobius Inversion ) \" \" original \": \" Find the area of polygon given integer coordinates .\" , \" upgrade \": \" Given lines in the plane , find the area of their union region accurately . Handle parallel and concurrent lines . ( Requires Integration logic or Green Theorem application ) \" } ] ### Category C: Inverse & Constructive Best for: Problems with well-defined algorithms and clear \"input algorithm output\" flow. Avoid when: The original problems core challenge is already in the \"design/construction\" phase rather than \"computation/processing\" (i.e., problems that lack standard algorithm to reverse). Description: Instead of asking for the result of process, ask for the input that produces specific result. Core Strategy: 1. Reversethe problem direction: from \"Given X, find Y\" to \"Construct such that holds\". 2. Require understanding of structural properties (e.g., what makes graph have specific flow?). 3. Add multiple constraints to make construction non-trivial. Examples: [ { } , { \" original \": \" Given graph , find the shortest path from to .\" , \" upgrade \": \" Construct graph with vertices and edges such that the shortest path from 1 to is exactly , and the MST weight is exactly .\" \" original \": \" Sort an array using QuickSort .\" , \" upgrade \": \" Construct permutation of size that causes standard QuickSort implementation ( with first element as pivot ) to hit its 28 } , { } , { } , { worst - case ( ^2) time complexity .\" \" original \": \" Given binary tree , print its pre - order traversal .\" , \" upgrade \": \" Given the pre - order and post - order traversals , reconstruct all possible binary trees . Determine if the solution is unique or count how many such trees exist .\" \" original \": \" Check if string is palindrome .\" , \" upgrade \": \" Construct string of length containing exactly distinct palindromic substrings . Prove that no such string exists if exceeds certain bound .\" \" original \": \" Find the maximum flow in network .\" , \" upgrade \": \" Given desired max flow value , construct network with minimum edges that achieves this flow , subject to capacity constraints on each edge .\" } ] ### Category D: State Explosion Best for: Problems with simple, polynomial DP states that can be enriched. Avoid when: The original state space is already exponential (e.g., TSP); adding dimensions would make it computationally infeasible. Description: Add complex dependencies or history requirements that necessitate advanced Dynamic Programming or Network Flow by expanding the state space. Core Strategy: 1. Redefine the state: move from simple states (e.g., dp[i]) to composite, multi-dimensional states (e.g., adding an exponential mask for sets or polynomial remainder for constraints). 2. Introduce constraints that depend on past choices or specific history (e.g., \"cannot visit node visited steps ago,\" requiring sliding window or history state). 3. Add multiple orthogonal restrictions (e.g., count, parity, or modularity) that must be tracked simultaneously. Anti-pattern: Simply adding variables that dont interact. The new dimensions must fundamentally change the recurrence logic or transition dependencies. Examples: [ { \" original \": \" Climb stairs taking 1 or 2 steps . How many ways ?\" , \" upgrade \": \" Cover 3N grid with 12 dominoes . How many ways modulo 10^9+7? ( Requires Broken Profile DP / Bitmask DP to track cross - 29 } , { } , { } , { } , { section state ) \" \" original \": \" Knapsack Problem : Max value with weight limit .\" , \" upgrade \": \" Knapsack on Tree : Each node has value / weight . Max value by selecting nodes such that no two selected nodes are adjacent , and total weight . ( Requires Tree DP + Knapsack dimensions ) \" \" original \": \" Longest Increasing Subsequence in an array .\" , \" upgrade \": \" Count the number of permutations of length that have Longest Increasing Subsequence of length exactly . ( Requires DP with Young Tableaux or RSK correspondence ) \" \" original \": \" Find min cost to traverse grid .\" , \" upgrade \": \" Find min cost to traverse grid with batteries to jump obstacles ; recharge depends on grid value modulo , and cells must be unlocked in order . ( State : position + battery_count + mod_state + lock_mask ) \" \" original \": \" Edit Distance between two strings .\" , \" upgrade \": \" Given strings and , find the number of strings of length such that EditDistance (A , ) and EditDistance (B , ) . ( Requires DP on DP / Automaton DP ) \" } ] ### Category E: Theorem Disguise Best for: Problems that can map to classic high-level algorithms but appear in unrelated or abstract domains. Avoid when: The original problem explicitly mentions the algorithm or data structure. Description: Hide sophisticated algorithmic core behind narrative or alternative mathematical structure that misleads intuition. Core Strategy: 1. Map the problem to well-known non-trivial algorithm (e.g., Network Flow, Linear Basis, Generating Functions, or Advanced DS). 2. Remove all technical terminology and explicit constraints that hint at the solution. 3. Create \"Red Herring\" narrative that suggests an intuitive but suboptimal approach (e.g., Greedy, simple DP, or naive Simulation). 4. Ensure the bridge between the surface problem and the hidden theorem requires deep structural insight. Examples: [ { } , { } , { } , { } , { \" original \": \" Find the maximum number of meetings one can attend .\" , \" upgrade \": \" Given set of intervals , color them with minimum colors so no overlapping intervals share color . Some intervals are VIP and must use specific colors . ( Disguised as Greedy , actually Interval Graph Coloring / Dilworth Theorem with constraints ) \" \" original \": \" Can we partition an array into two equal sum subsets ?\" , \" upgrade \": \" Given graph , determine if it is bipartite . If not , remove minimum edges to make it bipartite while preserving connectivity . ( Disguised Graph Theory , actually Odd Cycle Transversal / Max Cut variant ) \" \" original \": \" Assign tasks to workers to minimize total time .\" , \" upgrade \": \" workers and tasks . Each pair ( worker , task ) has cost . Some workers conflict and cannot work simultaneously . Minimize total cost . ( Disguised Minimum Cost Maximum Flow with conflict resolution ) \" \" original \": \" Find the maximum number of compatible intervals .\" , \" upgrade \": \" Given circle with chords represented by their endpoints . Find the maximum number of non - intersecting chords . ( Disguised DP on Circle Graph or Catalan - related structure ) \" \" original \": \" Find number in an array that appears more than /2 times .\" , \" upgrade \": \" Given stream of elements , find all elements appearing more than / times using ( ) space and one pass . Handle up to 10^3. ( Disguised Boyer - Moore Voting generalized / Misra - Gries Algorithm ) \" } ] ### Category F: Edge Case & Rigor Engineering Best for: Problems with simple logic but tricky implementation details. Avoid when: The original problem is already highly technical or implementation-heavy. Description: Focus on logical pitfalls, geometric precision, tricky edge cases, or requiring formal proof for correctness. Core Strategy: 31 1. Target weaknesses of standard data types (e.g., float precision, integer overflow, empty sets). 2. Introduce degenerate cases (e.g., disconnected graphs, collinear points, zero denominators). 3. Require strict mathematical proofs (like \"Exchange Arguments\") to justify strategy where intuition fails. 4. Add precision constraints (e.g., \"answer accurate to 109\") or handle extreme ranges. Examples: [ { \" original \": \" Given coordinates of 3 points , calculate the area of the triangle .\" , \" upgrade \": \" Given ( N10^5) lines in plane . Find the area of the finite region formed by their intersection . Handle parallel lines , lines at infinity , and precision errors . ( Requires Half - plane Intersection with strict boundary handling and epsilon comparison ) \" \" original \": \" Find if there is path from to in graph .\" , \" upgrade \": \" Find if reaches . The graph may contain self - loops , multiple edges , and consists of up to 10^5 disconnected components . Answer 10^5 online queries . Handle the case where = and the case where or doesn exist . ( Tests Edge Cases : Connectivity , Disjoint Set Union with path compression , query validation ) \" \" original \": \" Divide by .\" , \" upgrade \": \" Divide by where and are strings representing integers up to 10^1000. Handle =0 , negative signs , leading zeros , and output the quotient and remainder . Prove your division algorithm correctness . ( Requires BigInteger Implementation + comprehensive Edge Case handling ) \" \" original \": \" Greedy : Always pick the largest coin available to make change .\" , \" upgrade \": \" Given non - standard coin system ( . . , 1 , 3 , 4) , find the smallest value where the greedy approach fails to find the minimum number of coins . Prove that for all values below , greedy is optimal . ( Requires finding the Edge Case of the algorithm itself + Exchange Argument ) \" \" original \": \" Sort an array of tasks by duration .\" , \" upgrade \": \" Given tasks with processing time and decay rates . Each task loses value at different rate while waiting . Find } , { } , { } , { } , { 32 processing order to minimize total decay . Prove your sorting strategy using an Exchange Argument . ( Requires formal proof to derive the correct comparator ; simple sorting is wrong ) \" } ] ## Construction Protocol (Internal Thinking Process) 1. Analyze Original: Identify the naive solution and its complexity. 2. Select Category: Choose 1-2 categories from the library above that best fit the problems potential. 3. Apply Core Strategy: Use the \"Core Strategy\" defined in your selected category to redesign the problem constraints and objectives. 4. Review: Check against the \"Design Standards\". Does it require multi-step reasoning? Is the theorem disguised? 5. Final Output: Write the problem statement clearly using standard CP formatting. ## Input Original Problem: {original_problem} ## Output Format Return ONLY the new upgraded problem, nothing else. [Your upgraded competitive programming problem here]"
        }
    ],
    "affiliations": [
        "Fudan University",
        "M-A-P",
        "Shanghai Innovation Institute"
    ]
}