{
    "paper_title": "Block Cascading: Training Free Acceleration of Block-Causal Video Models",
    "authors": [
        "Hmrishav Bandyopadhyay",
        "Nikhil Pinnaparaju",
        "Rahim Entezari",
        "Jim Scott",
        "Yi-Zhe Song",
        "Varun Jampani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/"
        },
        {
            "title": "Start",
            "content": "Block Cascading: Training Free Acceleration of Block-Causal Video Models Hmrishav Bandyopadhyay1,2 Nikhil Pinnaparaju1 Rahim Entezari1 Jim Scott1 Yi-Zhe Song2 Varun Jampani1 2SketchX, University of Surrey 1Stability AI 5 2 0 2 5 2 ] . [ 1 6 2 4 0 2 . 1 1 5 2 : r Figure 1. We propose Block Cascading (right), where future blocks of frames are cascaded with current ones to improve generation speed. Without requiring any fine-tuning, Block Cascading can improve inference FPS by upto 2.79x (right) in block-causal pipelines without negatively affecting quality for short, long, and interactive video generation (left). All FPS is reported using the same python environment with Flash Attention 3 [31] on H100 GPUs. Most prompts are condensed in all figures, with full prompts available in Supplementary PDF and all videos available in Supplementary zip. Abstract Block-causal video generation faces stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve 2 acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of 200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal loss in generation pipelines demonstrate no significant quality when switching from block-causal to Block Cascading pipelines for inference. 1. Introduction Diffusion-based block-causal video generation pipelines denoise videos auto-regressively for faster, streamable generation. However, generation speed decreases significantly with model scale [16, 39]. Small-scale diffusion models (1.3B parameters) barely reach 16 FPS [14], while highquality large-scale models (14B parameters [1]) crawl at 4.5 FPS on H100 GPUs, making them impractical for any interactive use. This speed-quality trade-off forces users to choose between responsive but lower-quality small models, or impressive but unusably slow large models. We revisit this trade-off, arguing that beyond model complexity it stems from rigid scheduling with strict causal dependencies. To overcome this, we introduce parallelised video generation with Block Cascading, unlocking faster inference with multiple GPUs and smarter scheduling. The key intuition driving our work is that future frames need not rely on fully denoised past frames to begin generation. Current block-causal models like CausVid [51], SelfForcing [14], and LongLive [47] enforce strict sequential dependencies: block Bi+1 must wait for block Bi to completely denoise from t=1000 to t=0 before starting. This rigid ordering allows the denoising of only one block at time in causal manner, limiting opportunities to parallelise generation with multiple GPUs. We demonstrate that this strict dependency is overly conservative, especially since many of these block-causal models are adapted from bidirectional pre-trained networks [39] that naturally accommo1 date noisy context. Empirically, we observe that conditioning Bi+1 on partially denoised outputs from Bi at intermediate timesteps (e.g. t=750) can yield visual quality comparable to using fully denoised blocks. To build on this, we propose Block Cascading to overlap denoising of multiple blocks for enabling parallel generation across GPUs. Instead of sequential processing (B1 completes, then B2 starts, then B3 starts), we cascade block generation: when B1 reaches an intermediate timestep, we use its features as noisy context to start denoising B2. Similarly, B3 begins denoising once B2 reaches this intermediate checkpoint. This transforms the generation pipeline from sequential to parallel where multiple blocks denoise simultaneously at different stages, significantly reducing inference time. In effect, this reduces the waiting time for future blocks, which are already partially generated when the current block is decoded and viewed. Further, by cascading current and future blocks together, we can leverage bidirectional attention amongst these blocks to potentially improve generation quality. Pre-generation of future blocks helps exploit their noisy context during prompt switches for interactive video generation. In block-causal pipelines, context switches have to be accompanied by expensive KV-recaching, where past KV is re-cached using new prompts, causing 200ms+ latency spikes which break interactivity. With Block Cascading, we instead switch context by simply changing the text prompt as future blocks demonstrate gradual adaptation to new context based on their current noise level. This allows for seamless context switching without blocky artifacts or abrupt changes in scenes and subjects. Finally, we note that our multi-GPU pipeline exploits temporal parallelism: each GPU processes different temporal segment of the video. This approach is fundamentally distinct from data parallelism (processing independent samples) or model parallelism (splitting layers). However, sub-linear scaling (2.79 FPS on 5 GPUs with 14B model) is expected due to VAE decoding after generation and overhead from KV communication across GPUs for self-attention. Optimizations targeting these factors, such as linear attention, quantization, or smaller VAEs can be applied on top of block-cascaded models for additional gains. Moreover, VAE decoding can be moved to separate GPU to overlap decoding with next-block denoising. We do not discuss these optimizations in detail as they are not unique to block cascading. Our contributions are: (i) We identify that strict causal dependencies in video generation are unnecessary, as noisy intermediate states provide sufficient context for generation; (ii) We propose Block Cascading, training-free parallelization strategy that works universally across different block-causal models; (iii) We achieve practical inference speed with 5 GPUs: 30 FPS for 1.3B models and 12.5 FPS for 14B models, while maintaining quality comparable to baselines; (iv) We demonstrate prompt switching without latency spikes in interactive applications, previously impossible with causal models. Across short [14], long [47], and interactive [47] video generation, and even across model scales [1], Block Cascading consistently delivers significant speed-up (average 2 with 5 GPUs, 10% on single GPU) without any fine-tuning. 2. Related works 2.1. Timestep Distillation Timestep Distillation [4, 17, 49, 50, 57] of diffusion models [12] compresses their diffusion ODE trajectory [34] for speeding up iterative denoising from noise to data. This setup typically involves using multi-step teacher model to train few-step student by comparing their outputs along different points of the trajectory. Progressive distillation [17, 30] directly compares teacher and student outputs for same inputs in short skips of ODE trajectory. Advanced approaches like DMD [49, 50] and SiD [58] use scores from teacher and student models to align the distributions of multi-step teacher and few-step student. Recent works can perform aggressive ODE compression [6], making it possible to generate in as few as one step [4, 50], generally by supplementing distribution matching with adversarial techniques [4, 17] and progressive distillation [50] ideas. We perform few-step inference with timestep-distilled models, to reduce the complexity of Block Cascading inference. 2.2. Auto-Regressive Video Generation Early Autoregressive models [37, 46] have been predominantly used in conjunction with adversarial training [33, 37, 42] for sequential frame-by-frame video generation. With the advancement of diffusion-based generative models for image generation [10, 12, 29], recent works in video generation [7, 16, 22, 39, 40, 48] have adopted diffusion models to generate all video frames at once with full bidirectional attention across frames. These bidirectional models [16, 39, 48] can generate high quality videos but are slow and bounded by quadratic attention scaling [41] with respect to number of generated frames. To improve speed in video generation pipelines, recent works [18, 23, 5456] attempt to reduce number of sampling steps to directly reduce generation latency through timestep distillation. In parallel, several works re-introduce auto-regressive generative modelling of videos [11, 14, 19, 21, 44, 47, 51] by distilling bidirectional models [14, 51] or training auto-regressive pipelines from scratch [5, 36]. Fast, causal video generative pipelines set the stage for interactive and controllable generation [32, 47, 52], letting users inject control and action prompts that can influence future frames. In this work, we relax causality restrictions in distilled autoregressive models to speed up inference by parallel process2 ing of future blocks. 2.3. KV Caching Autoregressive generation of videos [14, 47] can be accelerated during inference with KV caching, where Key-Value (KV) pairs of previous frames are retained for efficient attention computation [38]. Since information from previous frames provides auto-regressive models with context for current frame generation, KV caching [28] helps prevent recomputing these features by simply storing them. KV caching was originally introduced in LLMs [2, 9, 13, 20, 38, 45] and has been since adapted to video generation [11, 14, 47, 51] to improve generation efficiency. While KV caching reduces KV computation for attention, the attention unit itself scales at quadratic rate [41] with KV size, requiring eviction of older cache elements time to time to support longer generation. Evicting salient features, however, can cause issues like drifting [53] where the model forgets key context from previous time-frames. To reduce attention complexity without drifting, both LLMs [35, 45] and video generation approaches [14, 21, 47] use some form of KV sink [35], where key features from earlier frames are retained. In this paper, we share KV features from parallel blocks to compute self-attention, reducing the requirement of an external KV cache during inference. 3. Methodology 3.1. Background and Overview Diffusion based multi-step generative models [16, 39, 48] have predominantly used bidirectional attention for high quality video generation at the cost of extra compute and generation time. Recent few-step video generation works like CausVid [51] and Self-Forcing [14] reduce generation time by distilling multi-step to few-step diffusion and by modelling auto-regressive generation on top of bidirectional models [39]. Adapting bidirectional pipelines like Wan2.1[39] for auto-regressive causal generation is non-trivial as it requires training to accommodate for the fundamental distribution shift where current frames xi are modelled through previous frames x<i only. This is usually in the form of regressionbased objective including points on the teacher and student trajectories where the student is causal network. This student is further trained as few-step model with timestepdistillation objective to catch up to the multi-step bidirectional teacher. CausVid trains from Wan2.1 [39] by matching distributions of the Wan2.1 teacher (preal) and the causal few-step student (pfake) as: DKL(pfakepreal) = Expfake (cid:16) log (cid:17) preal(x) pfake(x) (1) for samples generated by the student. This KL divergence being intractable, the DMD objective allows representing 3 the gradient of KL as difference of scores [25, 34, 43, 58] from the teacher and the student model as: θLDMD = Expfake (cid:16) (sreal(x) sfake(x)) (cid:17) dGθ dθ (2) where Gθ corresponds to the student network, and sfake and sreal are the teacher and student scoring functions respectively. In practice, we use the teacher network for sreal and proxy sfake trained to adapt to the changing distribution of the student model Gθ. To provide the proxy model enough room for catching up to the generator, it is trained for iterations at every generator update. Block-causal video generation: To balance quality and speed, CausVid [51], Self-Forcing [14] and other recent works [21, 47] employ block-causal (rather than strictly latent frames are temporally causal) pipeline. bunched into blocks Bi t={f : K}, which are protn cessed in causal (i.e. B0 B1 B2 . . .) fashion. Internally each block with its group of frames uses bidirectional attention to generate video segments. During inference, blocks are rolled out sequentially, and denoised from t=1000 to t=0. In here, CausVid [51] and Self-Forcing [14] introduce efficient causal video generation through KV caching [26], where future blocks can depend on pre-computed Key-Value (KV) features of previous blocks during self-attention. This prevents recomputing KV features for obtaining context from previous blocks which have been denoised. To generate long videos and improve interactive control in generated frames, LongLive [47] introduces KV-recaching where stored KV is recalculated for new prompts. This form of KV caching where future blocks depend on clean KV from previous blocks requires future blocks to wait for current block to finish denoising. Further in the event of KVrecache, all previous cache has to be discarded and recomputed - causing drop in inference FPS. In this work, we propose to remove this stall by denoising noisy future blocks Bi+1 with noisy context in current block Bi t1. In other words, we speculatively roll forward with noisy cache consisting of partially denoised KV features to get started with video generation in future blocks. 3.2. Noisy Caching , j+2 t={f , j+1 For Block-wise Causal generation of videos, blocks } with frames and > 0 are deBi noised using previously denoised blocks B<i 0 . In multistep diffusion environment, the block Bi has to wait till all previous blocks B<i are denoised to t=0 for using their 0 clean KV features in block-causal self-attention. However, block-causal distilled models [14, 47, 51] are often trained from bidirectional teachers [39] that use noisy features from all frames with bidirectional attention. This allows us to exploit the underlying bidirectional pre-training of causal disFigure 2. Block Cascading Inference: Our inference pipeline reduces dependency on clean, denoised blocks for future block generation. For example in here (center), we denoise block B1 t2 ={f 3, 4, 5} jointly using bidirectional attention instead of waiting for B0 t3 (left) [14, 47, 51]. By reducing dependency on previous blocks, we can free up the inference pipeline and allow parallel processing of multiple blocks (right) to improve generation speed significantly. t3 ={f 3, 4, 5} and tc to denoise B1 tilled models to kickstart frame generation in future blocks even when current block is not completely denoised. Specifically, we start denoising block Bi at t=T using noisy cache from blocks {Bik k}i k=1 instead of waiting for blocks B<i to finish denoising. Again, at t=t0, the previous blocks B<i have already finished denoising, but we retain context for Bi t0 , through another forward pass, using Bi1 tc where tc=0 corresponds to the cache timestep. This is similar to how previous works [14, 51] construct KV cache from denoised blocks with an extra forward pass. Forward pass for block Bi t0 thus requires attending to KV from {Bik k=1 in addition to its own for self-attention. tc }i In practice however, it is not computationally feasible to record noisy KV for all previous blocks B<i on =50 step diffusion model like Wan2.1[39], even if just for inference. Working with limited window size of blocks, this has complexity of O(N ), with DotProduct Attention complexity of O(M ) for KV size of and fixed number of frames per block. Since complexity linearly scales with number of steps, we can directly reduce the complexity of this operation using distillation techniques [4, 49, 50] that reduce inference timesteps to improve feasibility. We use 4 steps to denoise from =1000 (i.e. {t3, t2, t1, t0}) and an additional step tc=0 to improve video consistency across different blocks with cleaner cache from previous frames. Bi Naive implementation of this 4 step semi-causal denoising brings us to another sequential trap where we denoise the current block Bi t1 and then start denoising the next block Bi+1 t1 sequentially. This kickstarts future block denoising but slows down current block, resulting in slow inference. Concurrent work Rolling-Forcing [21] proposes to stitch the blocks together and jointly denoise all frames, following previous research using KV from Bi on robust training for causal video models [3]. Fundamentally different to Rolling Forcing [21] and Diffusion Forcing [3], we propose denoising of each block individually and parallelly through mini-batching, following individual block denoising in CausVid [51] and Self-Forcing [14]. We construct mini-batches where previous and current blocks are batched together with shared attention. (see Fig. 2). 3.3. Block Cascading Mini-batching multiple blocks together can overburden single GPU and reduce generation FPS (see Fig. 5a). To reduce the computation burden of our pipeline on single GPU, we process batches on different GPUs and share KV features for computing self-attention. Using fixed window size of , we batch together blocks that depend on each other for denoising and can share KV during denoising. For denoising each mini-batch, we share KV features of each block with other blocks being denoised at each self attention layer. This is done by creating global shared KV pool which can be referred to by all blocks, enabling continuity in generated frames. By nature, this KV sharing mimics rolling KV cache in block-causal generation pipelines [14, 51] that allows generation of long videos without running out of memory with fixed window sizes. Maximum parallelisation can be attained with window-size equal to number of denoising steps ( with tc) where each block depends on previous blocks cleaner timestep. Increasing window size beyond this requires maintaining an add on KV cache from previous blocks (similar to [14]). While smaller window sizes are possible by restricting attention window during self-attention, we find they can impact generation as later blocks do not get access to clean latents at t=tc. better way to obtain smaller windows is through reduced parallelism (see Sec. 4.2). We note that generating long videos with fixed win4 dow often results in drifting artifacts [53]. Following recent works [47], we reduce drifting in long video generation by using the first block as sink token, that stays in an external KV cache. Algorithm 1 Multi-GPU Inference with Block Cascading Require: Generator Gθ, Timesteps = {t3, t2, t1, t0, tc} Require: Total Frames , Block Size S, GPUs {g1, . . . , gG}, Scheduler Number of blocks Noisy latents, size Output latents Primary block index Timestep index in Cascade depth {(xb+i, [τ i])} on gi Create Batch ψ, VAE Decoder 1: Block computation: 2: M/S 3: Initialize: 4: (0, I) 5: 6: 0 7: τ 0 8: 0 9: while 0 do 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: end if 24: 25: end while 26: return D(O) end if else (cid:83)min(d,G1) i=0 0 , . . . , ˆxgd {ˆxg0 gj for each ˆx do } Gθ(B) if [τ j] > t0 then ϵ (0, I) X[b + j] Ψ(ˆx else if [τ j] = t0 then O[b + j] ˆx gj end for if block completed tc then + 1; 1 if + min(d + 1, 1); τ τ + 1 gj , ϵ, [τ + 1]) VAE decode For streaming, decode here 3.5. Interactive Control pattern on surf-board in Fig. 6. These artifacts are noticeable in video models as frame-jumps, where fine-grained details are not aligned across frames. We find that artifacts are prominent particularly in early frames where the model does not have enough context from clean latents (i.e. B<i tc ). For example, during denoising B1 using KV features from B0, B1 observes clean features from B0 tc only during its last time-step, during denoising B1 t0 . This creates occasional distortion from context mismatch between previously generated context B1 tc . Consequently, we find these distortions missing in ablative experiments with lower parallelism (see Sec. 4.2) where previously generated context (e.g. B1 t2 ) is still too noisy for conflict. To fix these artifacts and generate smooth videos, we exploit bidirectional pre-training and use full bidirectional attention across the current cascade. Bidirectional attention here helps smooth out inconsistencies by naturally aligning the features of both current and future frames during video generation (see Fig. 6). Since bidirectional attention uses more context to denoise every block, it can even surpass the block-causal baseline in quality at times (see Fig. 8). t0 and new cache context B0 To allow interactive control over video generation, current block-causal denoising pipelines [47] perform KVrecaching, recomputing past KV cache with new prompts for influencing next block generation. While starting denoising at t=1000, current blocks are heavily influenced by clean past cache, and without KV-recaching, can completely ignore new instructions. However, KV-recaching heavily drops inference FPS as all previous blocks in the attention window have to be recached before current block can start generating (see Fig. 5c). We demonstrate that KV-recaching can be avoided with our Block Cascading pipeline since we adapt to changing context easily with new text prompts. Specifically during context switch, future blocks are at various stages of denoising while the first block is denoised and is entering cache state Bi tc. t0 Directly injecting new context here allows bidirectional attention to handle the context switch where different frames in the cascade are affected with the new context based on their current noise levels. Similar to KV-recaching, our first block is being cached with new context in Bi tc and t0 provides clean KV features for all future blocks. We find that in practice, videos generated with bidirectional attention over our cascade can blend context much smoother than KV-recaching (more in Supplementary) and are preferred by users (see Fig. 8). Bi Bi 4. Experiments Figure 3. Expensive KV-recaching: KV-recaching can result in FPS drop in interactive generation as previous generated and stored cache has to be recached using new context. We skip KVrecaching as our KV is auto-recomputed using new context. 3.4. Training Free Adaptation Naive causal denoising with Block Cascading can cause artifacts, specifically in fine-grained features like colour and We conduct experiments by adapting causal video models without training. Specifically, we adapt Self-Forcing [14], LongLive [47] and Krea [1], obtaining an inference FPS Figure 4. Qualitative comparisons: Comparing our bidirectional inference pipeline with corresponding original inference pipelines boost of 14.05, 12.66 and 8.07 respectively with multiple GPUs. We choose these particular models for holistic assessment over different inference domains like short, long, and interactive video generation across different model capacities. For all inference configurations, we use window size of 7 blocks, and specifically for LongLive, sink size of 1 block where all blocks have 3 frames each. LongLives static sink can corrupt dynamic videos, so for interactive generation, we generate our samples without sink while LongLive refreshes sink during KV-recaching. For context change in interactive generation, we simply change the text prompt without incurring any artifacts, which helps us to prevent FPS drop from expensive KV recaching (see Sec. 3.3). We compare quality of videos generated with our pipeline v/s the original inference pipelines with qualitative  (Fig. 4)  and quantitative (Sec. 4.4, Tab. 1) analysis and comprehensive user study  (Fig. 8)  . We compare our videos with those generated with (i) Block-causal video generation pipelines: CausVid [51], Self-Forcing [14], LongLive [47], SkyReels-V2 [5] and MAGI-1 [36] and (ii) Bidirectional Video generation pipelines: Rolling Forcing [21], FastVideo [54] and Wan2.1 [39]. CausVid [51] learns block-causal video generative model with the DMD [50] objective in Eq. (2). Self-Forcing [14] builds on CausVid, reducing mismatch between training and inference pipelines for block-causal 6 (a) Ablative config. in Single GPU environment (b) Ablative config. in Multi GPU environment (c) FPS drop from KV-recaching Figure 5. Instantaneous FPS: We measure FPS as time taken to denoise particular block of 3 latent frames (12 video frames). FPS fluctuates from changing attention window sizes during block-causal denoising and number of parallel blocks in Block Cascading. generation by simulating the inference environment during training. LongLive [47] addresses drifting by training with permanent attention sink that retains specific context irrespective of causal window size. Separate from these models that are fine-tuned from Wan2.1[39], SkyReels-V2 [5] and MAGI-1 [36] train block-causal pipelines from scratch. Fully bidirectional FastVideo [54] distils from Wan2.1 [39] using sparse attention strategy. Rolling-Forcing [21] implements bidirectional attention on top of block-causal generations strategies and distils from Wan2.1 [39] using rolling bidirectional attention window. 4.1. FPS Analysis In Block Cascading, FPS initially starts low while blocks are being loaded in the cascade, then stabilises as the cascade is full, and finally goes up towards the end (Fig. 5b and Fig. 5a) as blocks are evicted. In contrast, causal video generation pipelines have initial blocks with smaller attention windows that are faster to generate than later blocks with larger windows. For our experiments, we want to define Streaming FPS as the FPS when the attention window is full and new blocks are being generated and decoded with the VAE. This definition reflects the steadystate performance that user would experience during long, auto-regressive video generation. We report Streaming FPS in Tab. 1 and in Fig. 5 as the average FPS across 8th and 9th blocks during generation of 13 blocks with 7-block attention window. Additionally, we plot instantaneous FPS observed during generation to illustrate FPS changes (Fig. 5a and Fig. 5b) and to highlight FPS drops in interactive video generation ( Fig. 5c). 4.2. Ablative Studies For ablation experiments, we analyse the degree of parallelisation attainable with different forms of Block Cascading, using Self-Forcing [14] pre-trained checkpoint as our baseline. We define different parallelization forms based on timesteps for cascading, from {t3, t2, t1, t0, tc}, denoting Pi for i-timestep skip by using Bj tci for denoising Bj+1 . We start with baseline self-forcing as P1, where blocks are generated sequentially and cached features from t0 7 Figure 6. Bidirectional Inference: Causal attention in fully parallelised generation (P4) can yield artifacts. These can be fixed by using bidirectional attention with same fully parallelised pipeline. Figure 7. Cascading Types: Different types of cascading leads to different levels of parallelism. We conduct user study to analyse performance across differently cascaded pipelines. At P1, the pipeline reduces to block-causal pipelines like Self-Forcing [14] t0 t0 tc are used for denoising B>i Bi . To reduce dependency on tc, we allow block Bi+1 to denoise using cache from t3 in P2 which cascades blocks Bi+1 and Bi. Next, we Bi move block Bi+1 further up (see Fig. 7), denoising with tc and Bi+2 t2 for two block cascade and with Bi block Bi t0 for three block cascade at t=t2. Finally, we use our default configuration P4, which is five block cascade (e.g. with {B0 , B4 }). We observe from Fig. 5a tc t0 and Fig. 5b that causal generation from P4 has the highest FPS both in single (+2.9 FPS) and multi GPU (+14.5 FPS) environments. However, causal generation can yield artifacts (see Fig. 6) in early frames with limited clean con- , B2 t2 , B3 , B1 t3 4.4. Quantitative Analysis We use VBench [15] for quantitative analysis with prompts re-written using Qwen2.5-7B-Instruct [27] following other methods [14]. All samples for evaluation with VBench in Tab. 1 use re-written prompts for video generation. In addition to brief summary in Tab. 1, we include complete table with all VBench scores in Supplementary. From Tab. 1, inference with our denoising pipeline closely follows scores of the original pipelines without significant drop in quality. During inference, our model is parallelised across 5 GPUs, providing us an avg. boost of 2 in FPS. Table 1. Video quality benchmark: We report VBench [15] scores for all competitors. FPS for other methods reported with 1xH100, FPS for ours reported on 5xH100s Streaming FPS () Total Score () Quality Score () Semantic Score () Model Wan2.1 [39] FastVideo [54] SkyReels-V2 [5] MAGI-1 [36] CausVid [51] Rolling Forcing [21] Self-Forcing [14] Ours (w Self-Forcing ckpt) LongLive [47] Ours (w LongLive ckpt) - - 0.49 0.19 16.31 17.80 16.31 30.36 15.60 30.46 0.8381 0.8313 0.8267 0.7918 0.8502 0. 0.8440 0.8353 0.8335 0.8230 0.8471 0.8647 0.8470 0.8204 0.8555 0.8358 0.8498 0.8435 0.8378 0. Krea-14B [1] Ours (w Krea-14B ckpt) 0.8471 0.8532 Numbers taken from [14], Bidirectional pipelines cannot stream. 5. Limitations 0.8437 0.8492 4.52 12.59 0.8019 0.6978 0.7453 0.6774 0.8290 0. 0.8206 0.8024 0.8162 0.8223 0.8304 0.8332 We note that window size in pre-training configuration can be limitation for Block Cascading when full parallelism is desired for optimal FPS boost. Empirically, we notice slightly higher drifting in some samples during inference with 7-block window, while using checkpoint that was trained with 4-block window [47]. Nevertheless, the degree of parallelism can be reduced ( e.g. with P3 or P2) for generation with strict small window models at some cost to inference speed. We also note sublinear scaling with GPUs in multi-GPU environments, making Block Cascading ideal for single video generation but poor alternative for generating large batches of videos. The latter case is better parallelised with distributed sampling. 6. Conclusion In conclusion, Block Cascading parallelises inference in block-causal video pipelines without requiring any retraining or fine-tuning. Extensive user studies demonstrate that Block Cascading during inference does not degrade video quality in pre-trained causal pipelines. This allows us to boost streaming inference speed for 1.3B video models to 30 FPS and 14B models to 12.5 FPS using multiple GPUs, supporting downstream applications like interactive and controllable video generation. Deploying large-scale video models like Krea-14B for dynamic 8 Figure 8. User Study: We conduct user study to analyse video quality from our cascaded bidirectional pipeline against original inference pipelines of other models under different configurations. text. In single GPU setup, switching to P3 solves this as cleaner context is available for later frames to reduce artifacts  (Fig. 7)  . In multi GPU environment, causal and bidirectional attention have marginal difference in FPS, and the best option is to use Bidirectional P4 that yields high quality with high generation FPS. For analysing video quality with different ablative environments, we perform user studies (see Sec. 4.3) observing better performance with bidirectional P4 and causal P2 and P3, compared to causal P4. 4.3. User Study To assess degradation in generated videos, we perform user studies where we show users videos generated by Self-Forcing [14], LongLive [47], and Krea [1] inference pipelines and those generated by ours. With the same seed and prompt, our method produces videos that closely match those generated by the vanilla inference pipeline (see Fig. 4). Therefore, we do not report prompts in our study, except in interactive video analysis where prompts are essential for illustrating context changes. The instruction provided is to choose the video that has higher quality given two competing videos - with each video reviewed by three annotators. The options available to users are left, right, both and none. We include more details of the user study in supplementary. For Self-Forcing, Krea and our ablative configurations (Sec. 4.2 and Fig. 7), we generate 418 short videos from each method with prompts sampled from VBench [15] extended prompts [14]. We evaluate LongLive on 100 long videos with prompts sampled from MovieGen [24] extended prompts and 20 interactive videos with prompts from [47] and Gemini 2.5 Pro [8]. Both long videos and interactive videos are generated to total length of 120 latent frames (30 seconds) while short videos are generated to 21 latent frames (5 seconds). From results of this user study in Fig. 8, we note that users have hard time distinguishing our pipeline from original pipeline when compared with Self-Forcing or LongLive. We find our pipeline is preferred in interactive video generation with LongLive and short video generation with Krea, potentially from additional context from bidirectional attention. generation and live-viewing bridges the gap between high quality video generation and real-time inference."
        },
        {
            "title": "References",
            "content": "[1] Krea AI. Krea realtime 14b: Real-time video generation. 2025. 1, 2, 5, 8 [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020. 3 [3] Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. NeurIPS, 2024. 4 [4] Dar-Yen Chen, Hmrishav Bandyopadhyay, Kai Zou, and YiZhe Song. Nitrofusion: High-fidelity single-step diffusion through dynamic adversarial training. In CVPR, 2024. 2, 4 [5] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Skyreels-v2: Zheng Chen, Chengcheng Ma, et al. arXiv preprint Infinite-length film generative model. arXiv:2504.13074, 2025. 2, 6, 7, 8 [6] Junsong Chen, Shuchen Xue, Yuyang Zhao, Jincheng Yu, Sayak Paul, Junyu Chen, Han Cai, Enze Xie, and Song Han. Sana-sprint: One-step diffusion with continuous-time consistency distillation. arXiv preprint arXiv:2503.09641, 2025. [7] Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, et al. Sana-video: Efficient video generation with block linear diffusion transformer. arXiv preprint arXiv:2509.24695, 2025. 2 [8] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 8 [9] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond fixed-length context. arXiv preprint arXiv:1901.02860, 2019. 3 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 2 [11] Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, and Jun Xiao. Vid-gpt: Introducing gpt-style autoregressive generation in video diffusion models. arXiv preprint arXiv:2406.10981, 2024. 2, 3 [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 2 [13] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Yakun Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. NeurIPS, 2024. 3 [14] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the traintest gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 1, 2, 3, 4, 5, 6, 7, 8 [15] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. 8 [16] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 1, 2, 3 [17] Shanchuan Lin, Anran Wang, and Xiao Yang. SdxlProgressive adversarial diffusion distillation. lightning: arXiv preprint arXiv:2402.13929, 2024. 2 [18] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial post-training for one-step video generation. arXiv preprint arXiv:2501.08316, 2025. 2 [19] Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, and Lu Jiang. Autoregressive adversarial post-training for real-time interactive video generation. arXiv preprint arXiv:2506.09350, 2025. 2 [20] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023. 3 [21] Kunhao Liu, Wenbo Hu, Jiale Xu, Ying Shan, and Shijian Lu. Rolling forcing: Autoregressive long video diffusion in real time. arXiv preprint arXiv:2509.25161, 2025. 2, 3, 4, 6, 7, [22] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. 2 [23] Xiaofeng Mao, Zhengkai Jiang, Fu-Yun Wang, Jiangning Zhang, Hao Chen, Mingmin Chi, Yabiao Wang, and Wenhan Luo. Osv: One step is enough for high-quality image to video generation. In CVPR, 2025. 2 [24] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 8 [25] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 3 [26] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. MLSys, 2023. 3 9 [27] Yang Qwen, Baosong Yang, Zhang, Hui, Zheng, Yu, Chengpeng Li, Liu, Huang, Wei, et al. Qwen2. 5 technical report. arXiv preprint, 2024. [28] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019. 3 [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2 [30] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. 2 [31] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. NeurIPS, 2024. 1 [32] Joonghyuk Shin, Zhengqi Li, Richard Zhang, Jun-Yan Zhu, Jaesik Park, Eli Schechtman, and Xun Huang. Motionstream: Real-time video generation with interactive motion controls. arXiv preprint arXiv:2511.01266, 2025. 2 [33] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: continuous video generator with the price, image quality and perks of stylegan2. In CVPR, 2022. [34] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2, 3 [35] Zunhai Su and Kehong Yuan. Kvsink: Understanding and enhancing the preservation of attention sinks in kv cache arXiv preprint arXiv:2508.04257, quantization for llms. 2025. 3 [36] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. 2, 6, 7, 8 [37] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris Metaxas, and Sergey Tulyakov. good image generator is what you need for high-resolution video synthesis. arXiv preprint arXiv:2104.15069, 2021. 2 [38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017. 3 [39] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1, 2, 3, 4, 6, 7, 8 [40] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 2 [41] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. 2, [42] Yuhan Wang, Liming Jiang, and Chen Change Loy. Styleinv: temporal style modulated inversion network for unconditional video generation. In ICCV, 2023. 2 [43] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In NeurIPS, 2023. 3 [44] Wenming Weng, Ruoyu Feng, Yanhui Wang, Qi Dai, Chunyu Wang, Dacheng Yin, Zhiyuan Zhao, Kai Qiu, Jianmin Bao, Yuhui Yuan, et al. Art-v: Auto-regressive text-tovideo generation with diffusion models. In CVPR, 2024. 2 [45] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. 3 [46] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. 2 [47] Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, et al. Longlive: Real-time interactive long video generation. arXiv preprint arXiv:2509.22622, 2025. 1, 2, 3, 4, 5, 6, 7, [48] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3 [49] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. In NeurIPS, 2024. 2, 4 [50] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024. 2, 4, 6 [51] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In CVPR, 2025. 1, 2, 3, 4, 6, 8 [52] Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Gamefactory: Creating new games with generative interactive videos. arXiv preprint arXiv:2501.08325, 2025. 2 [53] Lvmin Zhang, Shengqu Cai, Muyang Li, Gordon Wetzstein, and Maneesh Agrawala. Frame context packing and drift prevention in next-frame-prediction video diffusion models. In NeurIPS, 2025. 3, [54] Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. Vsa: Faster video diffusion with trainable sparse attention. arXiv preprint arXiv:2505.13389, 2025. 2, 6, 7, 8 [55] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhengzhong Liu, and Hao Zhang. Fast video generation with sliding tile attention. arXiv preprint arXiv:2502.04507, 2025. [56] Zhixing Zhang, Yanyu Li, Yushu Wu, Anil Kag, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Dimitris Metaxas, Sergey Tulyakov, et al. Sf-v: Single forward video generation model. NeurIPS, 2024. 2 10 [57] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, and Tat-Jen Cham. Trajectory consistency distillation: Improved latent consistency distillation by semi-linear consistency function with trajectory mapping. arXiv preprint arXiv:2402.19159, 2024. 2 [58] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In ICML, 2024. 2,"
        }
    ],
    "affiliations": [
        "SketchX, University of Surrey",
        "Stability AI"
    ]
}