{
    "paper_title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement",
    "authors": [
        "Yihe Deng",
        "Hritik Bansal",
        "Fan Yin",
        "Nanyun Peng",
        "Wei Wang",
        "Kai-Wei Chang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex reasoning abilities in large language models (LLMs), including sophisticated behaviors such as self-verification and self-correction, can be achieved by RL with verifiable rewards and significantly improves model performance on challenging tasks such as AIME. Motivated by these findings, our study investigates whether similar reasoning capabilities can be successfully integrated into large vision-language models (LVLMs) and assesses their impact on challenging multimodal reasoning tasks. We consider an approach that iteratively leverages supervised fine-tuning (SFT) on lightweight training data and Reinforcement Learning (RL) to further improve model generalization. Initially, reasoning capabilities were distilled from pure-text R1 models by generating reasoning steps using high-quality captions of the images sourced from diverse visual datasets. Subsequently, iterative RL training further enhance reasoning skills, with each iteration's RL-improved model generating refined SFT datasets for the next round. This iterative process yielded OpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on challenging benchmarks such as MathVista, MathVerse, and MathVision, demonstrating the potential of our strategy for robust vision-language reasoning. The code, model and data are held at https://github.com/yihedeng9/OpenVLThinker."
        },
        {
            "title": "Start",
            "content": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement Yihe Deng1, Hritik Bansal1, Fan Yin1, Nanyun Peng1, Wei Wang1 and Kai-Wei Chang1 1University of California, Los Angeles Abstract: Recent advancements demonstrated by DeepSeek-R1 have shown that complex reasoning abilities in large language models (LLMs), including sophisticated behaviors such as self-verification and self-correction, can be achieved by RL with verifiable rewards and significantly improves model performance on challenging tasks such as AIME. Motivated by these findings, our study investigates whether similar reasoning capabilities can be successfully integrated into large vision-language models (LVLMs) and assesses their impact on challenging multimodal reasoning tasks. We consider an approach that iteratively leverages supervised fine-tuning (SFT) on lightweight training data and Reinforcement Learning (RL) to further improve model generalization. Initially, reasoning capabilities were distilled from pure-text R1 models by generating reasoning steps using high-quality captions of the images sourced from diverse visual datasets. Subsequently, iterative RL training further enhance reasoning skills, with each iterations RL-improved model generating refined SFT datasets for the next round. This iterative process yielded OpenVLThinker, LVLM exhibiting consistently improved reasoning performance on challenging benchmarks such as MathVista, MathVerse, and MathVision, demonstrating the potential of our strategy for robust vision-language reasoning. The code, model and data are held at https: //github.com/yihedeng9/OpenVLThinker. 5 2 0 2 1 ] . [ 1 2 5 3 7 1 . 3 0 5 2 : r Figure 1: Performance (accuracy) of OpenVLThinker-7B on multi-modal reasoning benchmarks. 1. Introduction LLMs have displayed remarkable progress in multi-step reasoning, most notably exemplified by OpenAIs O1 model [20] and O3-mini. Follow-up studies have sought to replicate and extend these complex chain-of-thought abilities [19, 37, 43, 61, 62], revealing that targeted post-training methods can further improve model performance on challenging tasks. Among these, the recently open-sourced DeepSeekCorresponding author(s): yihedeng@cs.ucla.edu; project website: notion. Work in progress. OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement Figure 2: Demonstration of the training process toward OpenVLThinker-7B. We obtain the SFT data for iteration 1 via text-based R1 model that only receives the question and the generated image caption. Subsequently, we apply SFT and GRPO iteratively to leverage new reasoning data from the previous iteration and achieve self-improvement. We also evolve the data sources to progressively include more challenging questions over iterations. R1 [13] has shown that reinforcement learning with verifiable rewards is particularly effective at eliciting nuanced self-verification and self-correction reasoning from the LLM, thereby effectively enhancing the overall reliability of mathematical or logical reasoning chains. While these advances predominantly focus on text-only models, the potential of incorporating similarly complex reasoning strategies within large vision-language models (LVLMs) has remained relatively underexplored. Modern LVLMs merge open-source foundation LLMs [9, 50, 51, 56] with text-aligned image encoders [24, 44], enabling them to process visual contexts and produce outputs upon instructions [2, 7, 10, 11, 26, 27]. These LVLMs have broad vision-language pretraining and strong visual instruction-following ability, but lack complex multi-step reasoning abilities like O1 and R1 models. Meanwhile, explicitly training these models to harness sophisticated CoT reasoning is much less developed than their text-only counterparts [14, 17, 38, 49, 55, 65, 66]. Very recently, concurrent works have begun to investigate how to endow LVLMs with R1-like reasoning capabilities via distillation from text-only reasoning models [5, 18, 23, 57, 67], illustrating that advanced reasoning has great potential to contribute to improved performance on vision-language tasks. In this paper, we present one of the first explorations aimed at enhancing complex reasoning capabilities in LVLMs and introduce OpenVLThinker-7B, model trained through iterative self-improvement via alternating SFT and RL. Importantly, we consider the role of SFT primarily as to guide the model in obtaining an initial reasoning structure, while RL serves as the main driver for significant performance gains and improved generalization. We thus employ warm-start approach, utilizing SFT to establish the initial reasoning framework, rather than directly applying RL from scratch. We distill R1-style reasoning from existing text-only models into the vision-language context using textual image descriptions. This image-to-text conversion inevitably introduces information loss, resulting in comparatively weaker SFT dataset. Nevertheless, since the primary purpose of SFT in our method is to establish reasoning structure, we proceed from this adequate starting point. Next, we incrementally refine the model through RL and specifically Group Relative Policy Optimization (GRPO) [47]. The improved reasoning capability is then leveraged to generate the reasoning and answer for the SFT training data in the next iteration that 2 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement trains the model from scratch (Qwen2.5-VL), creating loop that enhances the LVLMs complex reasoning behavior and benchmark performance. We also evolve the data sources to progressively include more challenging questions over iterations, from including mostly simpler questions such as TabMWP [34] to more difficult questions such as Geometry3K [30]. We summarize our method pipeline in Figure 2. We demonstrate the effectiveness of our approach on challenging multimodal reasoning tasks, including MathVista [33], MathVerse [64] and MathVision [53], which require both precise image grounding and robust multi-step reasoning. Our experiments show that (1) the distilled R1 reasoning structure transfers effectively from text-only to vision-language domains and (2) iterative RL can refine and stabilize these reasoning pathways, ultimately surpassing the baseline performance of standard LVLMs. Beyond highlighting the synergy of SFT and RL for complex reasoning, our findings contribute early evidence toward the promise of bridging R1-style reasoning into multimodal contexts. Assistant: <think> Okay, so have this triangle XYZ with sides labeled as 9x, 4x + 5, and 6x + 3. The problem is asking me to find the length of side XY, which is 4x + 5. First, remember that in triangle, all sides are equal because its an equilateral triangle. That means all sides are the same length. So, set the expressions for the sides equal to each other. start with the sides: 9x = 4x + 5 and 9x = 6x + 3. Let me solve the first equation, 9x = 4x + 5. subtract 4x from both sides: 9x - 4x = 5, which simplifies to 5x = 5. Then, divide both sides by 5: = 1. Now, substitute = 1 into the expression for side XY, which is 4x + 5. So, 4(1) + 5 = 4 + 5 = 9. double-check my work. The other sides are 9x and 6x + 3. Substituting = 1, 9x becomes 9(1) = 9, and 6x + 3 becomes 6(1) + 3 = 9. Both sides are equal to 9, so the triangle is equilateral with all sides equal to 9. </think> The length of side XY is 9. <answer>D</answer> Figure 3: An example reasoning trace of OpenVLThinker. It shows desirable reasoning behavior including self-reflection and self-verification, such as double-check my work. Corresponding image and question are shown in Figure 9. 3 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement Figure 4: Iterative performance improvement of our model on MathVista. We note that SFT-Iter(i) is always fine-tuned from the base model Qwen2.5-VL-7B, with its training data generated from GRPOIter(i-1). GRPO-Iter(i) is obtained by applying GRPO to train SFT-Iter(i). 2. Related Work 2.1. Complex Chain-of-Thought Reasoning Since the introduction of OpenAIs O1 model [20], researchers have shown strong interest in reproducing and enhancing the complex reasoning capabilities of LLMs [19, 37, 43, 61, 62], partly due to its superior performance on mathematical benchmarks. Guo et al. [13] introduce the open-source DeepSeek-R1 model and investigate how RL with verifiable rewards can promote advanced chain-of-thought reasoning and reflective behaviors. This development inspired line of research focused on open-source reproduction [16, 29, 35, 63] and the analysis of such complex reasoning in mathematical problem solving [6, 54, 58, 59]. In parallel, several recent studies have similarly explored the effects of test-time scaling on encouraging more complex model reasoning behaviors [12, 28, 39, 46]. 2.2. Vision-Language Reasoning Model Recent advancements in large vision-language models (LVLMs) stem from open-source LLMs [9, 50, 51, 56] and text-aligned image encoders [24, 44]. Integrating these components has enabled LVLMs to follow diverse visual instructions and generate meaningful responses [2, 7, 10, 11, 26, 27]. Parallel to the model development, researchers have also been interested in eliciting reasoning chains from LVLMs via prompting [17, 38, 65, 66] or fine-tuning [14, 49, 55]. Concurrent work. Following the introduction and open-sourcing of DeepSeek-R1, there have been several concurrent early efforts aiming to equip LVLMs with R1-style reasoning capabilities [18, 23, 36, 42, 57, 67]. Our work aligns with these early studies, and we contribute our initial results and insights to support and further advance community exploration in this direction. Notably, we propose an iterative self-improving approach toward such multi-modal reasoning. OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement 3. Preliminaries ùëó=1 ùëùùúÉ(ùë¶ùëóx, ùë¶1, . . . , ùë¶ùëó1). An LLM is defined by probability distribution ùëùùúÉ, parameterized by model weights ùúÉ. Given prompt sequence = [ùë•1, . . . , ùë•ùëõ], the model generates response sequence = [ùë¶1, . . . , ùë¶ùëö], where ùë•ùëñ and ùë¶ùëó represent individual tokens. The response is sampled from the conditional distribution ùëùùúÉ(x), factorized as ùëùùúÉ(yx) = ùëö Supervised Fine-Tuning (SFT). We usally use SFT to specialize LLMs for particular task or domain. This process updates the model parameters ùúÉ by providing example responses of desired behavior to the input instructions. Concretely, Given dataset ùíü = {(x(ùëñ), y(ùëñ))}ùëÅ , where x(ùëñ) is the prompt sequence and y(ùëñ) is the desired response sequence. We update ùúÉ to maximize the likelihood of producing y(ùëñ) x(ùëñ)). By minimizing the loss, the model learns to given x(ùëñ). Formally, ‚ÑíSFT(ùúÉ) = ùëÅ produce responses more aligned with the labeled examples. ùëñ=1 log ùëùùúÉ (y(ùëñ) ùëñ=1 Reinforcement Learning Fine-tuning (RFT). RL approaches fine-tune LLMs via human preferences modeled under the Bradley-Terry model [1, 8, 40, 47]: ùëù(yùë§ yùëô x) = ùúé(ùëü(x, yùë§) ùëü(x, yùëô)), where yùë§ and yùëô denote preferred and dispreferred responses, respectively, and ùúé(ùë°) = 1/(1 + ùëíùë°) is the sigmoid function. The common RL objective under the Bradley-Terry assumption of the reward model ùëü(x, y) is thus [ Ex,yùëùùúÉ [ùëü(x, y)] ùõΩ Ex [KL(ùëùùúÉ(x)ùëùref (x))]] , max ùúÉ where ùõΩ > 0 is the KL penalty coefficient. Under this framework, Shao et al. [47] introduced Group Relative Policy Optimization (GRPO) by sampling group of response trajectories {oùëñ}ùê∫ from the old policy model ùúÉold for each query x, with the objective as maximizing: ùëñ=1 [ 1 ùê∫ ùê∫ ùëñ= 1 oùëñ oùëñ ùë°=1 min ( ùëùùúÉ(ùëúùëñ,ùë° x, oùëñ,<ùë°) ùëùùúÉold(ùëúùëñ,ùë° x, oùëñ,<ùë°) ùê¥ùëñ,ùë°, clip ( ùëùùúÉ(ùëúùëñ,ùë° x, oùëñ,<ùë°) ùëùùúÉold(ùëúùëñ,ùë° x, oùëñ,<ùë°) , 1 ùúñ, 1 + ùúñ ) )] ùê¥ùëñ,ùë° [ ùõΩ DKL ùëùùúÉ ùëùùúÉref ] , (1) [ùúãùúÉ where ùúñ > 0 is hyperparameter bounding the clipping range, ùõΩ > 0 balances the KL-penalty term ] against the advantage-weighted policy update, and ùúÉold is the old policy model. Here, the DKL advantage ùê¥ùëñ,ùë° = ùëüùëñ = (ùëüùëñ mean(ùëü))/std(ùëü) is set as the normalized reward at group level. ùúãref 4. OpenVLThinker: Iterative Self-improvement via SFT and RL We first present the evaluation results of our model OpenVLThinker-7B. Our evaluation employs exact matching and grader function from MathRuler1. We use the same inference hyperparameter as suggested by Qwen and recovered Qwen2.5-VL-7Bs reported results on MathVista. With three iterations of SFT and RL conducted on small amount of data, we observed effective R1-style reasoning and consistent performance gains across several visual reasoning benchmarks including MathVista [33], MathVerse [64] and MathVision [53]. Detailed comparison results are provided in Table 1. As concurrent models remain largely in active development, we exclude these models in our preliminary comparisons. Our evaluation uses the inference hyperparameters as suggested by Qwen and detailed in Table 6. 1https://github.com/hiyouga/MathRuler OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement In this section, we introduce our iterative approach to enhancing complex reasoning capabilities in OpenVLThinker-7B with SFT and RL. Our method consists of three main phases: (1) constructing reasoning-rich datasets through image captioning and distillation from text-based models, (2) using SFT to guide the base model in learning structured reasoning, and (3) applying RL-based exploration to improve generalization. Steps (2) and (3) are conducted iteratively, enabling continuous self-improvement. After each iteration, the RL-fine-tuned model serves as an enhanced source of verified reasoning demonstrations. These improved demonstrations are then used to train the base model through another round of SFT. Additionally, we progressively incorporate more challenging datasets throughout successive iterations, as summarized in Table 2. Table 1: Evaluation results across multi-modal reasoning benchmarks including MathVista, MathVerse and MathVision. We include the reported performance of GPT-4o as reference. OpenVLThinker-7B consistently and effectively improves upon the performance of Qwen2.5-VL-7B, surpassing or matching the performance of GPT-4o. Benchmark GPT-4o (reported) Qwen2.5-VL-7B OpenVLThinker-7B MathVista MathVerse MathVision (testmini) MathVision (full) 63.8 50.2 - 30.4 68.5 46.8 27.6 24.0 70.2 47.9 29.6 25.3 Table 2: Data sources and amounts for different iterations. We progressively evolve the data so that more challenging data are included in later iterations. Method Data Sources Data Amount SFT-Iter1 GRPO-Iter1 Geometry3K, GEOS, FigureQA, CLEVR, and SuperCLEVR FigureQA, GEOS, Geometry3K, TabMWP, VizWiz, and AI2D SFT-Iter2 FigureQA, GEOS, Geometry3K, TabMWP, VizWiz, AI2D, CLEVR, SuperCLEVR, IconQA, MapQA, and ScienceQA GRPO-Iter2 Geometry3K, GEOS, FigureQA, CLEVR, and SuperCLEVR SFT-Iter3 FigureQA, GEOS, Geometry3K, TabMWP, VizWiz, AI2D, CLEVR, SuperCLEVR, IconQA, MapQA, and ScienceQA GRPO-Iter3 Geometry3K (larger proportion), GEOS, FigureQA, CLEVR, and SuperCLEVR (larger proportion) 25k 5k 5k 5k 5k 5k 4.1. Captioning and Text-based Reasoning Distillation We curate collection of visually grounded tasks, including FigureQA [22], GEOS [45], Geometry3K [30], TabMWP [34], VizWiz [3], and AI2D [15], which provide approximately 59.2k image-question-answer triples. These data sources encompass wide variety of real-world visual tasks. To generate descriptions for each image, we employ captioning model, Qwen2.5-VL-3B-Instruct, which is prompted to produce textual captions as detailed as possible. Subsequently, text-based R1 reasoning model, DeepSeek-R1Distill-14B, is prompted with captions as substitute to images to generate ùëò = 4 candidate reasoning OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement chains. We select the shortest reasoning path with the correct final answer based on verification with both exact match and the grade function grade_answer() from MathRuler. The generated reasoning length distribution is shown in Figure 10. We verify the models final answer with ground-truth answer. We further observe that we obtain fewer reasoning examples with correct answers from challenging tasks like Geometry3K, but more from simpler datasets like TabMWP, as shown in Figure 11. This could be partially due to the information loss during the image-to-text conversion. 4.2. SFT to Follow Reasoning Structure We initialize our experiments with the base model as Qwen2.5-VL-7B-Instruct, which has broad visionlanguage pretraining but lacks complex multi-step reasoning abilities. We refer to the initial SFT phase as SFT-Iter1. In this phase, the model is trained on (image, question, reasoning) triplets derived from the data verification procedure and learns to produce the distilled reasoning traces and final answer in structured format. We monitor performance on the MathVista [33] benchmark to evaluate progress. We observe that the raw reasoning traces could be excessively verbose or repetitive, likely due to information loss during the image-to-caption conversion process. Consequently, the reasoning behaviors after SFT become longer and even more repetitive, often exhibiting unhelpful self-reflections. An example can be found in Appendix C. To mitigate this issue, we experiment with two filtering strategies. First, we remove samples containing reasoning traces exceeding 500 words. Second, and ultimately adopted, we truncate reflections by identifying and splitting on specific keywords (e.g., Wait, But wait, But the question) and discarding subsequent segments while retaining the final answer, so that we try to prevent the model from learning repeated reflections that significantly degrades its performance. This strategy resulted in 25k shorter examples for training. In Table 3, we compare the performance of model SFTed on our original data against our processed data. We note that although our data cleaning process substantially reduced instances of the \"aha moment\" behavior from the SFT dataset, such behaviors still occur in our SFT-Iter1 model. Nevertheless, their frequency is considerably more appropriate compared to training with unfiltered SFT data. Below, we present an example of an \"aha moment\" that persists in the SFT-Iter1 model despite removing all occurrences of the keyword But wait in the SFT dataset.2 The corresponding image for this example is provided in Figure 8. Table 3: Performance on the MathVista benchmark comparing different SFT data-filtering strategies."
        },
        {
            "title": "Model Variant",
            "content": "Accuracy (%) Qwen2.5-VL-7B (baseline) Vanilla SFT data (unfiltered) Filtered by length (<500 words) Removed reflections (SFT-Iter1) 68.5 48.4 55.0 62.5 Table 3 shows that unfiltered data severely degraded performance to 48.4%. Restricting the text to under 500 words improved accuracy to 55.0%, while removing repetitive self-reflections further raised it to 62.5%. Consequently, the Removed reflections variant served as our dataset for SFT-Iter1. We start from decent reasoning model and iteratively leverage SFT and RL with small amount of data to improve model performance. 2We note that the self-reflective behavior has been observed with text-based Qwen models without distillation as well [29, 63]. OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement \"But wait\" at SFT-Iter1 with correct answer Instruction Move the ruler to measure the length of the line to the nearest centimeter. The line is about (_) centimeters long. SFT-Iter1 <think> Okay, so need to figure out the length of the line in the image using the ruler provided. Let me start by understanding the image. The image shows wooden ruler with measurements from 0 cm to 10 cm. The line starts at the 1 cm mark and ends just before the 5 cm mark. Hmm, so the line starts at 1 cm and goes up to just before 5 cm. That means the line is 4 cm long because from 1 cm to 5 cm is 4 cm. But wait, the question is asking for the length to the nearest centimeter. Since the line ends just before 5 cm, its not exactly 4 cm, but its close. So, rounding to the nearest whole number, it would be 4 cm. </think> The line starts at 1 cm and ends just before 5 cm, which is 4 cm long. Rounding to the nearest centimeter gives 4 cm. <answer>4</answer> The Role of SFT. Given the observed performance decline and substantial increase in response length when applying SFT to unfiltered data, we hypothesize that SFT primarily establishes reasoning structures and effectively constrains the search space for RL during subsequent training. Meanwhile, RL training itself should be viewed as the significant contributor to performance gains. While recent studies have explored cold-starting RL training, SFT provides an advantageous warm-start, reducing the need for incorporating format rewards or special prompts that could potentially induce reward hacking. Consequently, we opted for smaller, carefully processed SFT dataset. Additionally, we report in Table 4 the evaluation results on MathVista for concurrent work, R1-Onevision3 [57], which use SFT with approximately 110k visual reasoning data. Under the same evaluation, we similarly observed performance decline for R1-Onevision. Table 4: Accuracy (%) on MathVista. While our evaluation recovered the reported performance of Qwen2.5-VL-7B, the performance of SFT-ed model R1-Onevision with 119k SFT data exhibited similar performance decline as our SFT-Iter1 under the same evaluation. Qwen2.5-VL-7B R1-Onevision-7B SFT-Iter"
        },
        {
            "title": "MathVista",
            "content": "68.5 61.5 62.5 4.3. Reinforcement Learning with GRPO After completing SFT-Iter1, we applied GRPO to further improve the model. For this stage, we drew upon smaller but more challenging subset of 5k examples from Geometry3K, GEOS, FigureQA, CLEVR [21], 3https://huggingface.co/Fancy-MLLM/R1-Onevision-7B 8 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement and SuperCLEVR [25], for which we werent able to obtain much correct SFT data via captioning and text-based reasoning. We considered simple binary reward function (0/1) based on the correctness of the final answer compared to ground truth. No additional template-based or formatting rewards were used, as SFT had already enabled the model with structured reasoning style. During training, in Figure 5, we observed correlation between longer reasoning outputs and improved accuracy on held-out test set, while verbose self-reflections were markedly less frequent than the SFT model trained with unfiltered data. On MathVista, GRPO-Iter1 improved the accuracy from 62.5% (SFT-Iter1) to 65.6%, suggesting that RL can effectively improve the models reasoning capability once the basic reasoning structure is established. (a) Average response length (b) Test score Figure 5: Training progress with GRPO at iteration 1. We report the test accuracy on held-out validation set from the same distribution as the training data. 4.4. Iterative Refinements The model obtained from our first iteration (GRPO-Iter1) now serves as better source of reasoning demonstrations, having acquired complex reasoning capabilities while more reliable on processing visual inputs as compared to image-text conversion. This motivates us to adopt an iterative self-improvement strategy, which is proven to be effective in iterative SFT [48] and iterative DPO schemes [41, 60]. Following the initial RL phase, we generated new set of higher-quality reasoning traces by sampling from the GRPO-Iter1 model. This yielded another 5k examples drawn from more diverse sources, including FigureQA, GEOS, Geometry3K, TabMWP, VizWiz, AI2D, CLEVR, SuperCLEVR, IconQA [31], MapQA [4], and ScienceQA [32]. We referred to the subsequent SFT round on these data as SFT-Iter2. Empirically, SFT-Iter2 with less and higher-quality data (5k) further reduced the base models performance decline on MathVista, reaching 66.1%. We then repeated the GRPO process on SFT-Iter2, denoted GRPO-Iter2, using the same 5k curated examples from the previous RL stage. Notably in Figure 6, although the average response length decreased in this second RL round, the accuracy on MathVista increased to 69.4%, surpassing the original baseline of 68.5%. This confirms that iterative cycles of supervised fine-tuning and reward-based optimization can progressively enhance performance. We summarize the models performance on MathVista across iterations in Figure 4. 9 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement (a) Average response length (b) Test score Figure 6: Training progress with GRPO at iteration 2. We report the test accuracy on held-out validation set from the same distribution as the training data. 5. Conclusion In this work, we introduced OpenVLThinker-7B, LVLM enhanced through an iterative self-improving process combining SFT and RL to enable complex CoT reasoning. Our results demonstrate that integrating R1-style reasoning into LVLMs effectively boosts their multimodal reasoning performance across benchmarks. This initial result highlights the promising potential for future exploration of complex reasoning in vision-language contexts."
        },
        {
            "title": "References",
            "content": "[1] Arash Ahmadian, Chris Cremer, Matthias Gall√©, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet √úst√ºn, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Jeffrey Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, pages 333342, 2010. [4] Shuaichen Chang, David Palzer, Jialin Li, Eric Fosler-Lussier, and Ningchuan Xiao. Mapqa: dataset for question answering on choropleth maps. arXiv preprint arXiv:2211.08545, 2022. [5] Hardy Chen, Haoqin Tu, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Vlthinking: An r1-derived visual instruction tuning dataset for thinkable lvlms. https://github. com/UCSC-VLAA/VL-Thinking, 2025. [6] Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen. An empirical study on eliciting and improving r1-like reasoning models, 2025. URL https://arxiv.org/ abs/2503.04548. 10 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement [7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024. [8] Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. [9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [10] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. [11] Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, Wenqi Shao, Chao Xu, Conghui He, Junjun He, Hao Shao, Pan Lu, Hongsheng Li, and Yu Qiao. Sphinx-x: Scaling data and parameters for family of multi-modal large language models. In International Conference on Machine Learning (ICML), 2024. [12] Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: recurrent depth approach, 2025. URL https://arxiv.org/abs/2502.05171. [13] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [14] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. arXiv preprint arXiv:2412.05237, 2024. [15] Tuomo Hiippala, Malihe Alikhani, Jonas Haverinen, Timo Kalliokoski, Evanfiya Logacheva, Serafina Orekhova, Aino Tuomainen, Matthew Stone, and John A. Bateman. Ai2d-rst: multimodal corpus of 1000 primary school science diagrams. Language Resources and Evaluation, 55(3): 661688, December 2020. ISSN 1574-0218. doi: 10.1007/s10579-020-09517-1. URL http: //dx.doi.org/10.1007/s10579-020-09517-1. [16] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. Openreasoner-zero: An open source approach to scaling reinforcement learning on the base model, 2025. [17] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. [18] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models, 2025. URL https://arxiv.org/abs/2503.06749. 11 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement [19] Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. O1 replication journey part 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson?, 2024. URL https://arxiv.org/abs/2411. 16489. [20] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [21] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29012910, 2017. [22] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, √Åkos K√°d√°r, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300, 2017. [23] Sicong Leng, Jing Wang, Jiaxi Li, Hao Zhang, Zhiqiang Hu, Boqiang Zhang, Hang Zhang, Yuming Jiang, Xin Li, Deli Zhao, Fan Wang, Yu Rong, Aixin Sun, and Shijian Lu. Mmr1: Advancing the frontiers of multimodal reasoning. https://github.com/LengSicong/MMR1, 2025. [24] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1096510975, 2022. [25] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan Yuille. Super-clevr: virtual benchmark to diagnose domain robustness in visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1496314973, 2023. [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems (NeurIPS), 36, 2023. [27] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llavanext: Improved reasoning, ocr, and world knowledge, January 2024. URL https://llava-vl. github.io/blog/2024-01-30-llava-next/. [28] Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling, 2025. URL https://arxiv.org/abs/2502.06703. [29] Zichen Liu, Changyu Chen, Wenjun Li, Tianyu Pang, Chao Du, and Min Lin. There may not be aha moment in r1-zero-like training pilot study, 2025. Notion Blog. [30] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. 12 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement [31] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021. [32] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. [33] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [34] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=DHyHRBwJUTN. [35] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. [36] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [37] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems, 2024. URL https://arxiv.org/abs/2412.09413. [38] Debjyoti Mondal, Suraj Modi, Subhadarshi Panda, Rituraj Singh, and Godawari Sudhakar Rao. Kam-cot: Knowledge augmented multimodal chain-of-thoughts reasoning. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 1879818806, 2024. [39] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand√®s, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. [40] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. [41] Richard Yuanzhe Pang, Weizhe Yuan, He He, Kyunghyun Cho, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. Advances in Neural Information Processing Systems, 37:116617116637, 2024. [42] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. 13 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement [43] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, and Pengfei Liu. O1 replication journey: strategic progress report part 1, 2024. URL https://arxiv.org/abs/2410.18982. [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), pages 87488763. PMLR, 2021. [45] Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. Solving geometry problems: Combining text and diagram interpretation. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 14661476, 2015. [46] Amrith Setlur, Nived Rajaraman, Sergey Levine, and Aviral Kumar. Scaling test-time compute without verification or rl is suboptimal, 2025. URL https://arxiv.org/abs/2502.12118. [47] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [48] Avi Singh, John Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter Liu, James Harrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023. [49] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamav-o1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. [50] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [51] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [52] Rohan Wadhawan, Hritik Bansal, Kai-Wei Chang, and Nanyun Peng. Contextual: Evaluating contextsensitive text-rich visual reasoning in large multimodal models. arXiv preprint arXiv:2401.13311, 2024. [53] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. [54] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning, 2025. [55] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. 14 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement [56] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [57] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, and Wei Chen. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization, 2025. URL https://arxiv.org/abs/ 2503.10615. [58] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning, 2025. [59] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chainof-thought reasoning in llms, 2025. [60] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models, 2024. URL https://arxiv.org/abs/2401. 10020. [61] Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self-training via process reward guided tree search. Advances in Neural Information Processing Systems, 37:6473564772, 2025. [62] Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, et al. Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning. arXiv preprint arXiv:2410.02884, 2024. [63] Hanning Zhang, Jiarui Yao, Chenlu Ye, Wei Xiong, and Tong Zhang. Online-dpo-r1: Unlocking effective reasoning without the ppo overhead, 2025. Notion Blog. [64] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [65] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. [66] Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. Ddcot: Duty-distinct chain-ofthought prompting for multimodal reasoning in language models. Advances in Neural Information Processing Systems, 36:51685191, 2023. [67] Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1zeros\" aha moment\" in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132, 2025. 15 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement A. Additional Empirical Study Does Complex Reasoning Matter for VQA? We additionally investigated whether complex, multi-step reasoning provides significant performance gains over standard (non-R1) reasoning in visual tasks. In this study, we use the ConTextual [52] validation set of 100 VQA examples, aiming to disentangle the roles of image grounding and textual reasoning. As similar to our first distillation process, we separately employ vision-language model for caption generation and pure-text model for reasoning. The image description generated by the captioning model is then fed into one of two text-based models: DeepSeek-R1-Distill-14B (an R1-style reasoner) or Qwen2.5-14B-Instruct (a standard instruction-tuned model). This setup allows us to isolate the impact of R1 reasoning from the effects of the underlying vision encoder. We further explore how different levels of caption quality influence final accuracy by comparing two caption generators, LLaVA-v1.6-34B and GPT-4o. Additionally, we vary the number of sampled reasoning paths (ùëò = 1, 2, 4) and compute pass@ùëò accuracies for each condition. As baseline, we include direct QA outputs from LLaVA-v1.6-34B without any intermediate text description (i.e., the model sees images directly). Figure 7 summarizes these results. In our experiments, we find that R1-style reasoning provides consistent benefits: (1) R1 reasoning outperforms standard methods. When provided with identical captioned inputs, DeepSeek-R1-Distill-14B achieves higher accuracy than Qwen2.5-14B-Instruct. Moreover, its performance can match (or even surpass) the direct QA accuracy of its own captioning model (LLaVA-v1.6-34B), despite potential information loss from translating the image into text. Increasing the number of sampled reasoning chains (ùëò = 2 (2) Sampling benefits complex reasoners. or ùëò = 4) leads to larger performance gains for R1 models than for standard Qwen models, indicating that the multi-step reasoning approach can more effectively converge on correct solutions when multiple hypotheses are explored. (3) Image grounding quality matters. We observe that richer and more precise captions significantly enhance final VQA accuracy. When captions are more detailed (e.g., from GPT-4o), the improvements from complex reasoning are especially pronounced. Caption Refinement via Feedback: Limited Effectiveness. We also investigated whether one-round feedback loop could improve the quality of captions and thus final VQA performance. Concretely, DeepSeek-R1-Distill-14B was prompted to list missing or ambiguous details in the initial captions generated by LLaVA-v1.6-34B. The captioning model then re-generated refined description incorporating this feedback. Table 5 shows that the refined captions did not produce major accuracy improvements, suggesting that single feedback pass is insufficient for significantly enriching image descriptions. Overall, although the idea of iterative caption refinement has intuitive appeal, our preliminary tests suggest that more elaborate or repeated feedback cycles might be necessary to achieve substantial gains. 16 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement Figure 7: Pass@k accuracy of different reasoning models based on captions generated with different vision-language LLMs. Table 5: VQA accuracy after single round of caption refinement. While pass@4 increases slightly, pass@1 and pass@2 remain largely unchanged. Caption Type pass@1 pass@2 pass@"
        },
        {
            "title": "Original\nRefined",
            "content": "33 29 37 35 44 46 Even so, the primary finding remains that R1-style reasoning robustly boosts performance relative to standard instruction-tuned reasoning, underscoring the importance of multi-step logic in VQA tasks. B. Experiment Details We thank LLaMA-Factory4 and EasyR15 for open-sourcing the training framework that we used for SFT and GRPO. In Table 7 and 8, we detail the hyperparameters that we used for SFT, GRPO and inference. We further lay out the prompts we used for generating image captions. Table 6: Inference hyperparameters. max_new_tokens top_p top_k temperature repetition_penalty 2048 0.001 1 0.01 1.0 4https://github.com/hiyouga/LLaMA-Factory 5https://github.com/hiyouga/EasyR1 17 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement Table 7: Supervised fine-tuning hyperparameters. Table 8: GRPO hyperparameters. Data type Learning rate Global batch size Scheduler Warmup ratio Num train epochs Image max pixels bf16 5e-7 32 Cosine 0.1 1 262144 Rollout batch size Global batch size Max grad norm Data type Learning rate Weight decay Warmup ratio Rollout temperature 512 128 1.0 bf16 1e-6 1e-2 0.0 1."
        },
        {
            "title": "Prompt for image description generation",
            "content": "Please provide detailed description of this image that could help another AI model understand it completely. Be specific and comprehensive while maintaining natural language flow. Figure 8: The image to the visual reasoning task that we showed in section 4.2 Figure 9: The image to the demonstrated reasoning example in section 1. Question: What is the length of side XY? Choices: (A) 4 (B) 5 (C) 6 (D) 9. 18 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement Figure 10: The reasoning length distribution of DeepSeek-R1-Distill-Qwen-14B given the image descriptions and questions from VQA datasets. Figure 11: Pass@4 accuracy of DeepSeek-R1-Distill-14B, which takes image descriptions and questions as inputs, on the six data sources we selected for SFT-Iter1. 19 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement C. Output Examples Repetitive reasoning example after SFT without filtering But the question is asking for the route and endpoint. So, the route would be the highway number, and the endpoint would be the destination. Wait, but the signs are arranged vertically. So, the top signs are the routes, and the bottom ones are the destinations. So, if youre at the junction and youre facing the signs, you can see both. So, the route is the highway number, and the endpoint is the destination. But the question is phrased as \"the route and endpoint reached by continuing in straight line.\" So, if youre at the junction and youre facing the signs, you can see both the routes and the destinations. So, the route would be the highway number, and the endpoint would be the destination. Wait, but the instruction is bit ambiguous. It says \"continuing in straight line,\" which might mean following the route indicated by the sign. So, if youre at the junction and youre facing the signs, you can see both the routes and the destinations. So, the route is the highway number, and the endpoint is the destination. But the question is asking for the route and endpoint. So, the route would be the highway number, and the endpoint would be the destination. Wait, but the signs are arranged vertically. So, the top signs are the routes, and the bottom ones are the destinations. So, if youre at the junction and youre facing the signs, you can see both. So, the route is the highway number, and the endpoint is the destination. But the question is phrased as \"the route and endpoint reached by continuing in straight line.\" So, if youre at the junction and youre facing the signs, you can see both the routes and the destinations. So, the route would be the highway number, and the endpoint would be the destination. 20 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement Model Output at GRPO-Iter1 Gradient Step 3 user If the parallelogram is cut along the dashed line, what is the length of the resulting rectangles longer side? assistant <think> Okay, so Ive got this image of parallelogram with some measurements. The image says the top base is 21 cm, the bottom base is 17 cm, and the height is 8 cm labeled at the bottom base. Theres dashed line inside the parallelogram from the top right corner to the central point on the left base, making right angle with the left base. </think> The dashed line is median, which splits the parallelogram into two congruent triangles. When the parallelogram is cut along this line, each resulting trapezoid has longer side of 21 cm, as the median connects opposite sides. Since the dotted line is perpendicular, it forms rectangle along the base, making the longer side 21 cm. <answer>21 cm</answer> ground truth 21 cm 21 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement Model Output at GRPO-Iter1 Gradient Step 5 user How many non-cube objects are there after removing all brown cubes? assistant <think> Okay, so need to figure out how many non-cube objects there are after removing all brown cubes. Let me start by un[0/1833]ing the image provided. The image has five objects on plain, light surface. They are: 1. small, shiny golden cube on the left. 2. larger, metallic green sphere near the center-left. 3. smaller, brown cube near the center-right. 4. Another green sphere, slightly smaller than the metallic one, on the right. 5. teal, cylindrical object in the foreground. First, the question is about non-cube objects after removing all brown cubes. So, need to identify which objects are not cubes initially and then subtract the brown cube since its being removed. The cubes are the small golden one and the smaller brown one. Removing the brown cube means were taking it out of the count. Now, the non-cube objects are: - The large metallic green sphere. - The smaller green sphere. - The teal cylinder. So, after removing the brown cube, were left with three objects. Therefore, the number of non-cube objects is three. </think> The image contains five objects: one small golden cube, one large green metallic sphere, one small brown cube, another small green sphere, and teal cylinder. The question asks how many non-cube objects remain after removing all brown cubes. Initially, the non-cube objects are the two spheres (one green metallic and one green) and the teal cylinder. The two cubes (golden and brown) are removed, leaving the three non-cube objects. <answer>3</answer> ground truth 3 22 OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement Model Output at GRPO-Iter1 Gradient Step 45 user Does Forest Green have the highest peak value? assistant <think> Okay, so need to figure out if Forest Green has the highest peak value in the graph described. Let me start by understanding the information given. The graph has seven lines, each with different colors: Orange Red, Coral, Forest Green, Tomato, Red, Aqua, and Crimson. The y-axis ranges from about 68 to 69. The x-axis goes from 0 to 100, in intervals of 20. Looking at the image, each line starts at point on the y-axis and then either increases, decreases, or fluctuates. The task is to determine if Forest Green has the highest peak. First, Ill go through each line one by one to see their starting points and trends. 1. **Orange Red**: Starts around 68 and decreases gradually. So, its peak is at the start, which is around 68. 2. **Coral**: Starts around 68 and decreases slightly. So, at x=0, its also around 68. 3. **Forest Green**: Starts at 69 and increases slightly. So, its peak is higher than the others mentioned so far. 4. **Tomato**: Starts at 68 and increases gradually. So, it peaks around 69, same as Forest Green. 5. **Red**: Starts at 68 and increases sharply, reaching above 69. So, Red is one of the highest. 6. **Aqua**: Starts at 68 and increases slightly. So, around 69. 7. **Crimson**: Starts at 68 and increases slightly. Same as Aqua. </think> The Forest Green line starts at 69 and increases slightly, meaning its peak value is 69. However, the Red line starts at 68 and increases sharply to value exceeding 69. Therefore, Red has the highest peak value. <answer>No</answer> ground truth No"
        }
    ],
    "affiliations": [
        "University of California, Los Angeles"
    ]
}