{
    "paper_title": "Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding",
    "authors": [
        "Yueqian Wang",
        "Xiaojun Meng",
        "Yuxuan Wang",
        "Jianxin Liang",
        "Qun Liu",
        "Dongyan Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-modal multi-party conversation (MMC) is a less studied yet important topic of research due to that it well fits real-world scenarios and thus potentially has more widely-used applications. Compared with the traditional multi-modal conversations, MMC requires stronger character-centered understanding abilities as there are many interlocutors appearing in both the visual and textual context. To facilitate the study of this problem, we present Friends-MMC in this paper, an MMC dataset that contains 24,000+ unique utterances paired with video context. To explore the character-centered understanding of the dialogue, we also annotate the speaker of each utterance, the names and bounding bboxes of faces that appear in the video. Based on this Friends-MMC dataset, we further study two fundamental MMC tasks: conversation speaker identification and conversation response prediction, both of which have the multi-party nature with the video or image as visual context. For conversation speaker identification, we demonstrate the inefficiencies of existing methods such as pre-trained models, and propose a simple yet effective baseline method that leverages an optimization solver to utilize the context of two modalities to achieve better performance. For conversation response prediction, we fine-tune generative dialogue models on Friend-MMC, and analyze the benefits of speaker information. The code and dataset is publicly available at https://github.com/yellow-binary-tree/Friends-MMC and thus we call for more attention on modeling speaker information when understanding conversations."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 2 ] . [ 1 5 9 2 7 1 . 2 1 4 2 : r Friends-MMC: Dataset for Multi-modal Multi-party Conversation Understanding Yueqian Wang1, Xiaojun Meng2, Yuxuan Wang3, Jianxin Liang1, Qun Liu2, Dongyan Zhao1,4 * 1Wangxuan Institute of Computer Technology, Peking University 2Huawei Noahs Ark Lab 3Beijing Institute for General Artificial Intelligence 4National Key Laboratory of General Artificial Intelligence {wangyueqian,liangjx,zhaodongyan}@pku.edu.cn, {xiaojun.meng, qun.liu}@huawei.com, wangyuxuan1@bigai.ai Abstract Multi-modal multi-party conversation (MMC) is less studied yet important topic of research due to that it well fits real-world scenarios and thus potentially has more widelyused applications. Compared with the traditional multi-modal conversations, MMC requires stronger character-centered understanding abilities as there are many interlocutors appearing in both the visual and textual context. To facilitate the study of this problem, we present Friends-MMC in this paper, an MMC dataset that contains 24,000+ unique utterances paired with video context. To explore the character-centered understanding of the dialogue, we also annotate the speaker of each utterance, the names and bounding bboxes of faces that appear in the video. Based on this Friends-MMC dataset, we further study two fundamental MMC tasks: conversation speaker identification and conversation response prediction, both of which have the multi-party nature with the video or image as visual context. For conversation speaker identification, we demonstrate the inefficiencies of existing methods such as pre-trained models, and propose simple yet effective baseline method that leverages an optimization solver to utilize the context of two modalities to achieve better performance. For conversation response prediction, we fine-tune generative dialogue models on Friend-MMC, and analyze the benefits of speaker information. The code and dataset is publicly available and thus we call for more attention on modeling speaker information when understanding conversations. Datasets https://github.com/yellow-binary-tree/Friends-MMC Introduction Multi-modal dialogue systems have attracted extensive attention in recent studies (Zang et al. 2021; Zheng et al. 2022; Feng et al. 2023; Zhu et al. 2023; Liu et al. 2023; Li et al. 2023a; Maaz et al. 2023; Li et al. 2023b), especially with the rapid development of multi-modal large language models. However, there are two main deficiencies of existing works: (1) As most multi-modal datasets are collected from human annotations, LLM generated contents or chat history from social media, these dialogues are mostly presented in *Corresponding Author Copyright 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. question-answer format (AlAmri et al. 2019) between human and system, instead of among several human interlocutors; (2) the speakers are bystanders (Das et al. 2016) and discuss the given visual content such as an image or video, instead of really being situated in the context. In contrast, in real-world face-to-face conversations, the interlocutors are situated in the visual & audio context 1, which means that the conversation and the visual context influence each other rather than interlocutors discussing about given fixed visual context. Also, there can be more than 2 interlocutors involved, which means that modelling speaker information is crucial for comprehending the conversation. Understanding face-to-face conversations is an important foundation for achieving embodied artificial intelligence (Zhou et al. 2023; Xu et al. 2023; Zhang et al. 2023). Therefore, we emphasize that multi-modal multi-party conversation is very important field for real industrial applications yet less studied topic. To overcome the shortcomings of existing work on multimodal conversation, in this paper we extend its scopes and propose new field of research: multi-modal multi-party conversation (MMC). Compared with traditional multimodal conversations, MMC differs in the following aspects: (1) The conversation is not only between user and an assistant, but among an arbitrary number of interlocutors, therefore the information of the speaker should be explicitly modeled; (2) Instead of chatting on images / videos as bystanders, the interlocutors are situated within the visual context, or in other words, the visual context provides rich information about the interlocutors, such as their identities, expressions and actions. To foster the study of MMC, we build Friends-MMC, multi-modal multi-party conversation dataset collected from the famous TV series Friends. 2 An overview of Friends1In this work, either images or videos can be used as the visual context. When videos are used, the audios are also included. To avoid cluttering, we keep using visual context to refer to videos and omit & audio in the rest of this paper. 2We have investigated the copyright issue of releasing our dataset, it can be released and used for non-commercial purposes. In fact, previous work (Poria et al. 2018) also created dataset collected from TV Series Friends, and the authors have already released raw videos in their homepage. Figure 1: An example of multi-modal multi-party conversation. The task of conversation speaker identification is to infer the dotted arrows pointing from characters to utterances, and the task of conversation response prediction is to infer the last utterance in the dotted rectangular. Only one frame of the video is shown as the visual context to avoid clutter. MMC is shown in Figure 1. session in Friends-MMC consists of several turns, each paired with video clip as visual context, in which the face tracks are detected and classified by character. Compared to the existing multi-modal dialogue and multi-party conversation datasets, our proposed Friends-MMC have some traits worth emphasizing: a) Modalities of available data are multiple, including: textual information of each utterance3, visual & audio context in forms of frames and videos, face bounding boxes and face character names. Utilizing all of these modalities can be challenging for existing models; b) Conversations are taken from daily life such as TV series, which are more natural and diverse compared to existing multi-party conversation datasets (Ouchi and Tsuboi 2016; Hu et al. 2019) that are mostly collected from chats on the topic of computers. c) Reasoning can be very complex. For example, in terms of conversation speaker identification, the speaker may not appear in its corresponding video or frame. Therefore, the preceding or succeeding textual and visual context, as well as their temporal relations, should be taken into account, which is quite difficult to solve for even humans in our experiments. We focus on two fundamental MMC tasks: Conversation Speaker Identification and Conversation Response Prediction. The goal of conversation speaker identification is to link the speaker of every turn to the faces in the corresponding visual context. It requires system to not only infer the speaker of each turn from the textual content, but also understand the visual context in which the dialogue happens. Existing works (Gu et al. 2021; Meng, Mou, and Jin 2018) of speaker identification mostly focus on text-only multi-party conversation, where only the speaker of the last one or few turns should be predicted given the speaker label of previous turns. Our paper introduces the multi-modal information of the interlocutors to make this task more challenging and better conform to real-world scenarios where all 3In this paper we use the term turn to denote all contexts of single turn of speaking, including utterance (i.e., textual content), visual & audio context, face information and speaker information. We use the term utterance to denote the textual content only. speaker labels are not available in the dialogue history. To accomplish this task, we present our task-specific baseline method including three modality-specific modules which is further introduced later in the Conversation Speaker Identification Section. With our task-specific designs, this system can achieve competitive accuracy on conversation speaker identification, far exceeding recent multi-modal pre-trained models, while takes small amount of computation and enjoys high degree of flexibility. The other task is conversation response prediction, one of the most popular tasks for dialogue modelling. It requires system to predict the last utterance with respect to the context. Compared with existing response generation task in multi-modal dialogues, the context of MMC is very heterogeneous in modality. We fine-tune text-only and multi-modal dialogue models on Friends-MMC with different sources of speaker information, to validate that the speaker information is critical to the response prediction of MMCs. In other words, the ability of correctly identifying the conversation speaker can benefit response prediction, which is an issue that has been ignored in existing multimodal dialogue researches where two speakers simply take turns to speak to each other. In summary, our contributions are three-fold: (1) We formally propose multi-modal multi-party conversation, new and valuable field of research, and study two sub-tasks of it: conversation speaker identification and conversation response prediction; (2) We build and release Friends-MMC, dataset to facilitate the study of multi-modal multi-party conversation; (3) We design baseline for conversation speaker identification, validate its performance on FriendsMMC, and analyze the benefits of speaker information on conversation response prediction. Friends-MMC Dataset In this section, we describe the dataset collection and annotation procedure for constructing the Friends-MMC dataset, which covers all the 220 episodes from 10 seasons of the TV show Friends. The reasons we use Friends are: (1) it is sitcom series, which has numerous conversations that contain diverse topics of daily life; (2) Though having as many as 220 episodes, it has relatively small number of main characters, which is convenient for automatic face labelling and data cleaning; and (3) Its easy to get publicly available resources like high-quality subtitles that are often manually revised and paired perfectly with the video by large group of TV fans, which greatly reduces manual labour during the data construction process as well as guarantees the data quality. The content and speaker of each turn are extracted from transcripts and subtitles4. Faces and their character names in each frame are detected and labelled automatically for the train set (Season 1, 2, and from 4 to 10), and manually for the test set (Season 3) to ensure its accuracy. Construction Process Figure 2 shows the overall construction process of the dataset. Now we introduce every step in details: Video Preprocessing. We crop clip from the video according to the start and end timestamp in the subtitle. We use an off-the-shelf face detector (Zhang et al. 2017) to detect faces for each frame in the clip. Following (Kalogeiton and Zisserman 2020), we merge the faces in adjacent frames into face tracks and thus remove the faces that are not in any track to clean out false positive faces. Face Prototype Construction. C1C (Kalogeiton and Zisserman 2020) is dataset with human-labelled face tracks for season 3 of Friends. We choose set of 18 main characters, manually select 20 faces in different viewing angles per character, and encode them using Facenet-512 (Schroff, Kalenichenko, and Philbin 2015) to get face feature vectors in the embedding space of Facenet-512 as prototypes for each character. Automatic Face Labelling. We automatically label the detected faces tracks with character names by finding their nearest neighbour in the mentioned embedding space. In particular, for each detected face track, we encode each frame in the track with Facenet-512 and calculate the cosine similarity between their feature and all prototypes. If the mean value of the largest 5 cosine similarities is greater than threshold = 0.6 (which is set to maximize the validation accuracy described in the following paragraph), we label this face track with the corresponding character name, otherwise we think this face does not belong to any of the main characters and discard it. To validate the accuracy of automatic face labelling, we use the same process to detect and label faces for season 3 and compare the results with human-annotated ones from C1C. The verification follows the rule that if the IoU of bounding boxes of an automatically labelled face and human-annotated face is greater than 0.5, we identify them as pair of identical faces. Given this threshold, 95% of all pairs of identical faces are labelled with correct names, which verifies the effectiveness of our automatic face labelling method. 4https://my-subs.co/showlistsubtitles-610-friends https://fangj.github.io/friends Test & Test-noisy Set. For the test set, we directly use the human-annotated faces in C1C to guarantee the accuracy of face labelling, thus serving as high-quality ground-truths for this test set. Moreover, in order to align with the fact of imperfect face recognition in real-world scenarios and be consistent with the train set, we also create more challenging test-noisy set by randomly removing 20% labelled face tracks. Image Frame Selection. As many recent multi-modal pre-trained models accept images instead of videos as input, we also provide an alternative option of visual context where each turn is paired with only 1 frame. In such case, we select the frame with the most detected faces from each video clip, as well as its face bounding boxes and face names, as the paired visual context of this turn. Session Selection with Sliding Windows. We use sliding window of size to select adjacent turns as dialogue session if the following conditions are met: (1) all speakers are from the main characters; (2) the time intervals between all adjacent turns are shorter than 8 seconds, which is heuristic rule to prevent selecting turns from different scenes. Therefore, we use = {5, 8} to create two types of sessions with different context lengths (5 turns and 8 turns). Note that different dialogue sessions may contain the same turn, as it belongs to different contexts and thus the preceding or succeeding textual and visual contents differ. Dataset Statistics Dataset statistics are shown in Table 3. Apart from the basic statistics, we also count the proportion of speakers whose faces are not detected in the current clip or frame, or even not appear in all the clips or frames within this entire session. As table 3 shows, the test-noisy set includes significantly larger number of speakers not in the current clips (or frames for image as input) than the test set. Therefore, this case is more challenging for speaker identification task, as the candidate model needs to really understand the conversation and find out more clues from the context rather than only the current clips or frames to infer who is the real speaker."
        },
        {
            "title": "Conversation Speaker Identification",
            "content": "Task Introduction Conversation speaker identification requires models to identify the speaker of each turn given the textual and visual context. Existing works on speaker identification mostly focuses on the text-only multi-party conversation. It often asks models to predict the speaker of the last few turns, given the dialogue history and the speaker of previous turns. However, in real-world scenarios, speaker labels are usually either available for all turns or not available for any turn. To simulate it, when we extend the task to multi-modality, we provide the basic visual information of interlocutors so that the speaker of each turn can be predicted. However, too many decisive clues for identifying the speaker, such as voice characteristics and facial movements, can also be provided by the video and audio. This will cause Figure 2: An overview of the construction process of Friends-MMC dataset. 5 turns train test 13584 2017 21092 3069 18.87 20.28 2.85 2.83 3.12 2.41 2.71 2.31 1.03 13.43 0.17 6.13 2.20 1.61 6.52 % speakers not in current frame 24.05 1.01 9.53 # sessions # unique turns # words in utterance # speakers in each session # face tracks per clip avg. secs per face track % speakers not in current clip % speakers not in all clips # faces per frame % speakers not in all frames 8 turns test test-noisy train 8730 1325 16990 2480 18.71 20.42 3.47 3.43 3.14 2.39 2.74 2.30 1.10 13.51 0.14 5.57 2.21 1.60 6.42 24.15 0.42 7.45 2017 3069 20.28 2.85 2.50 2.72 19.26 1.13 1.76 25.64 3.32 test-noisy 1325 2480 20.42 3.47 2.52 2.73 18.93 0.44 1.78 25.30 1.37 Table 1: Dataset Statistics of Friends-MMC. We provide train set, test set and more challenging test-noisy set. models to tend to only rely on the rich video information and ignore the dialogue context, which deviates from our original motivation of promoting researches on conversation systems. Therefore, we also propose an alternative setting: providing only one frame and no audio as the visual context. In this setting, the basic information of interlocutors such as identities, expressions, and the scene they are in, can also be provided by this frame, and it leaks less decisive clues and shortcuts for directly identifying the speaker. Baseline Method To effectively utilize all the modalities of the proposed benchmark dataset, including visual, audio, textual and face tracks, we propose baseline method, which consists of three modules: 1) visual model M1 to recognize speaking faces, 2) text model M2 to analyse multi-speaker relations based on dialogue contexts, and 3) quadratic binary optimization problem solver to combine their results and thus identify the speaker of each turn. This modular design makes our system enjoy high degree of flexibility, as one can easily change the visual model or the text model with alternative ones when different contextual information (e.g., using image or video as visual input) is provided. Figure 3 shows the overview of our proposed method, and we introduce each module in the following paragraphs. Visual Model We use visual model to predict the probability of each face belonging to the current speaker individually: pf ace = M1(f ace) (0, 1). There are two different visual contexts: 1) If the provided visual context is an image frame, we use an inception model (Szegedy et al. 2014) pre-trained on VGGFace2 (Cao et al. 2017) and finetuned on the train set of Friends-MMC as the visual model M1, and ace is an image of the face cropped using bounding box b. When fine-tuning on Friends-MMC, the speaking label of face yf ace is set to 1 if the character name of this face is identical to the speaker name y, and 0 otherwise: yf ace = 1[c = y]. We use the cross-entropy classification loss as the training objective. 2) If the provided visual context is video, we use TalkNet (Tao et al. 2021), state-ofthe-art active speaker detection model, as the visual model M1, and ace is video of the face cropped using bounding box sequence b, accompanied by the audio of the same period. Note that using TalkNet in zero-shot manner is good enough, we try to fine-tune it on the train set of FriendsMMC as like the CNN model, but it does not result in better performance. Text Model We use DeBERTa-v3 (He, Gao, and Chen 2021) first trained on Ubuntu Dialogue Corpus (Lowe et al. 2015) and then fine-tuned on the train set of Friends-MMC as M2 to predict whether every two utterances in dialogue session are spoken by the same speaker. The intuitive reason behind it is that for some utterances, it is hard to identify the speaker solely by feeding the video or frame into M1. We thus try to conjecture its speaker by finding whether it likely shares the same speaker with another utterance, for which we have confidences or prior knowledge to infer its speaker. In addition, M2 helps to achieve relatively global optimum speaker inference by considering all turns together. Given dialogue session consists of utterances, we prepend an <bos> token to each utterance as the input of M2 as like: <bos>u1 <bos>um. We use the last hidden state of each <bos> hi as the representation of each utterance, and use head layer to calculate the similarity of every two representations: pij sim = σ(W2GeLU(W1[hi; hj; hi hj] + b1) + b2) where i, = 1, , m, and (W1, b1, W2, b2) are learnable parameters. σ is the sigmoid activation function, and pij sim (0, 1) is scalar that denotes the probability of two utterances spoken by the same person. The loss function is defined as: LM2 = SE(psim, ysim) + SE(psim, pT sim) where ysim {0, 1}mm is the ground truth label of whether any two utterances are from the same speaker, and SE denotes mean squared error loss. Quadratic Binary Optimization Problem Solver This module aims to integrate the outputs of both visual (M1) and textual (M2) models for speaker identification. In particular, we design two matrices using the outputs of these two models, and address it as the quadratic binary optimization. For each dialogue session, we first obtain candidate speaker set by recording all faces appeared in every frame / video: = {c1, , cl}. We construct vision reward matrix Rlm of selecting character ci as the speaker of the turn uj. If the face of ci appears in the frame / video vj, bij is set to the probability of that face as speaking face predicted by M1, otherwise bij = 0. Apparently, can only express those situations that the speaker appears in the corresponding frame / video. To address those problems, we construct another text reward matrix Rmm to reveal the reward for assigning the same speaker to two turns ui and uj. We first use M2 described in the previous subsection and get similarity matrix psim. However, if we simply use this similarity matrix psim as the reward matrix A, since all elements in psim are larger than 0, the optimization solver tends to assign the same speaker to every turn to get the maximum rewards. To avoid which, we subtract the similarity matrix with the mean value of its elements as the reward matrix, i.e., = psim mean(psim) As Figure 3 shows, with and in hand, the task of multi-modal multi-party speaker identification can be represented by quadratic binary optimization problem: Maximize (X) = (1 α)X AX + αXB s.t. {0, 1}ml, (cid:88) j= Xij = 1, = 1, 2, . . . , where α is hyperparameter to control the weight of two rewards and is selected according to the performance on validation set held-out from the train set. We use 0.8 for frame as visual context, 0.7 for video as visual context, and 0.2 when ground truth labels of the text model are provided (M 2 ). By now, this problem can be easily solved using optimization problem solvers like (Gurobi Optimization, LLC Figure 3: Model overview of the three modules in different colors: the visual (M1) is yellow, the textual (M2) is green, and the optimization solver taking vision and text reward matrix as input is blue. 2023), which adaptively makes decisions based on the output of M1 and M2. The reason we use an optimization solver instead of an end-to-end pre-trained model is that this task of identifying speakers still remains challenging to use the general attention mechanism of pre-trained models to fuse different modalities. Experiment Results Implementations We conduct experiments in five settings with information of different modalities used: (1) image (frame) only; (2) video only; (3) text only; (4) image and text; and (5) video and text. In image only setting (1), we use the model M1 CNN to predict one face from all detected faces from the frame as the speaker. If there are no faces, we randomly choose character from the candidate speaker list. The video only (2) is almost the same as (1) but uses M1 TalkNet to predict one face from the video. In the text only setting (3), we compare M2(DeBERTa-v3) with 3-shot GPT 3.5 using its in-context learning ability to perform this task. For M2, although it is good at judging whether two utterances are said by the same speaker, it is not trained to identify the speaker for single utterance. Therefore, it can only make guesses according to the relations between sentences. GPT 3.5, however, possesses some ability to understand the candidate speaker list and identifying names from utterances, so it can make full use of the textual context to achieve more accurate reasoning. In image-text setting (4), the jointed M1 (CNN) + M2 model is used together with quadratic binary optimization solver, and we also try to replace the output of M2 with ground truth labels (denoted by 2 ) to explore bottlenecks and possible improvement directions. We also test GPT4-o (OpenAI 2024) under this setting. Video-text setting (5) is almost the same as (4), but again CNN is replaced by TalkNet for M1. We also fine-tune three popular and powerful multi-modal pre-trained models: Violet (Fu et al. 2021), LLaVA (Liu et al. 2023) and Emu (Sun et al. 2023). As LLaVA only accepts one image at time during pre-training, we also use one single turn (one utterance and one frame) during fine-tuning and testing, neglecting information from adjacent turns. Violet and Emu, on the other hand, are pre-trained 0 random (std.dev.) 5 turns 8 turns noisy noisy 31.82 32.61 28.54 29.03 (0.25) (0.47) (0.49) (0.27) M2 M1(CNN) M1(TalkNet) 72.88 63.72 72.90 62.51 80.89 70.91 81.00 70. 33.24 33.85 29.09 29.33 37.21 37.24 33.35 32.81 Frame Only 1 Video Only 2 Text Only 3 4 GPT 3.5 (3-shot) Use image and text modality 5 Violet 6 7 8 M1(CNN) + M2 9 M1(CNN) + 2 10 GPT-4o (0-shot) 11 Human Use video and text modality 12 M1(TalkNet) + M2 83.21 74.12 83.60 75.00 13 M1(TalkNet) + 2 90.88 83.09 95.10 89.69 32.66 33.16 27.73 28.86 LLaVA v1.5-13B 46.30 42.39 45.73 41.41 61.76 58.23 60.96 56.46 75.81 68.61 74.53 67.21 84.90 78.01 90.80 83.93 66.36 65.60 63.64 61.02 82.25 Emu-14B 84.49 - - Table 2: Accuracy on the test and test-noisy set of FriendsMMC. M1 and M2 denote the visual and textual model of our baseline method, respectively. For M1, we use CNN or TalkNet to take image or video as input. indicates that we use ground truths instead of textual model outputs (M2) to serve as upper bounds. with videos or image-text interleaved data, so we can directly use all 5 or 8 turns as input. To check how much useful information can be provided by using frame as the visual context, we also report the human performance of this task. we randomly sample 80 dialogue sessions from each (5 turns / 8 turns) test set, provide dialogue contents, frames, face bounding boxes & names to participants, and ask them to select speaker for each turn from the candidate speaker set (i.e., the characters that appear in all frames). This process requires intensive efforts from humans, according to their post-interview, as the task of selecting speakers requires careful observation and thorough understanding of the dialogue contents. We thus only perform the human studies on the test set, since we believe the human performance on the test-noisy set should be apparently worse. See appendix for details of all the baselines mentioned above. Main Results Results can be found in Table 2: (1) visual context acquired by the vision model M1, including which face appears in the frame and looks like speaking face, serves as the most critical clues, shown by the performance of M1 (line 1, 2). We can conclude that this speaker identification task is still vision dominant, especially when videos are provided. (2) Speaker relations acquired by the text model M2 also play vital supporting role to bring significant improvement of 2% 5% from M1 to M1 + M2 (line 8 vs. line 1, line 12 vs. line 2). The textual context benefits this task not only by providing dialogue contents, but also aims for more real scenarios where the speaker does not appear in the scene. Whats more, the benefit is greater when the paired visual model is more accurate. It makes sense that one has to accurately identify speakers of some turns before it is able to identify other turns using this speaker relation information. (3) Directly fine-tuning multi-modal pretrained model (line 5, 6, 7) or using proprietary multi-modal models (line 10) is not good option despite we have tried different models with various parameter sizes. We believe the key aspects that are essential to solve this problem remain difficult to be understood by these candidate models, as this task differs lot from the original training objectives. It also may be due to the reason that this speaker identification task is difficult to be formatted as the proper and shorten input of the model and thus to be easily learned. (4) Comparing the line 8-9 and 12-13, our current method of fine-tuning M2 model to predict whether two utterances are spoken by the same speaker still has lot of room for improvement. This strong upper bound result also evidences that our designed objective for the textual module is meaningful. The influence of the hyper-parameter alpha to the results is listed in the appendix."
        },
        {
            "title": "Conversation Response Prediction",
            "content": "Task Introduction Conversation response prediction aims to predict the textual content of the last turn given the visual context of all turns and the textual content of all previous turns. In this study, we demonstrate that speaker information is critical to the response prediction task of multi-modal multi-party conversations. Though predicting the next utterance in multi-modal dialogues has been widely studied topic, none of the previous work focuses on the specific nature of multi-party dialogues, i.e., taking the speaker information into account. We hypothesize that the potential advantages come from speaker information in two aspects: (1) The model may learn the speaking style of each speaker. When the speaker information like name is available, it can be used as global speaking style indicator for generating better responses; (2) Speaker information can be local context indicator to infer the response is from which persons point of view, i.e., should be consistent with which utterance in the local conversation. To validate these two aspects, we constructed 3 test sets: (a) test set with random speaker names: speakers of all utterances are randomly assigned. Here the potential advantages of (1) and (2) are both broken. (b) test set with random history speaker names: similar to (a) but we keep the speaker name of the utterance we need to predict unchanged. Here only the potential advantage of (2) is broken. (c) test set with shuffled names: we replace all names in both speaker information and utterance content with another name according to predefined random shuffle mapping, e.g., replace Ross with Joey, and Joey with Chandler. Here only the potential advantage of (1) is broken. Note that in all cases the speaker information of the train set is not modified. Our experiments are in the following settings: train and test without speaker name; train with ground truth speaker information and test with ground truth or random or shuffled or our identified speaker name (M1 + M2). Note that Model Llama2-7B Emu-14B Speaker No Random Random History Shuffled Ground truth M1(CNN) + M2 5 turns 8 turns 36.98 30.69 43.32 31.23 43.40 31.63 48.60 35.20 49.36 36.89 45.81 34.16 M1(TalkNet) + M2 34.56 46.64 31.09 30.49 31.55 29.35 31.25 29.45 35.17 33.02 36.30 34.06 33.89 31.98 M1(TalkNet) + M2 32.97 34.64 No Random Random History Shuffled Ground truth M1(CNN) + M2 Table 3: Accuracy of conversation response prediction by selecting one from set of ten utterances as candidates. when using our M2 model, the last utterance to predict is removed from the input, i.e., the speaker of top 1 turns is predicted using both visual and textual context, and the speaker of the last turn is predicted using visual context. In addition, we finetune Emu-14B as dialogue model with visual input, and Llama2-7B as dialogue model without visual input. As mentioned in (Wang, Wang, and Zhao 2023), it is often hard to predict the next utterance solely by the dialogue history, hence the text generation metrics (e.g., rouge) may not be very valid when evaluating on our movie dialogue dataset. Therefore, following (Wang, Wang, and Zhao 2023), we build response selection task to evaluate models instead of using generative metrics. We randomly select 9 other utterances from the test set as negative responses, and ask the model to select the one with the lowest perplexity from totally 10 (plus ground truth) candidates as output. Experiment Results As shown in Table 3, regardless of which pre-trained model is used, or the length of dialogue context, in all cases adding speaker information always improves the performance. Comparing to using the random speakers and random history speakers, the performance with shuffled speakers is much closer to the one with ground truth speakers. Therefore, in terms of our hypothesis on the two potential advantages that come from speaker information, this result indicates that speaker information mainly works by indicating the local context instead of serving as global speaking style indicator. Overall, our baseline method of speaker identification also benefits the response prediction task as it achieves significant better performance than the ones with no or random speakers."
        },
        {
            "title": "Related Works",
            "content": "Multi-party Conversations Multi-party conversations (MPC), as opposed to two-party conversations, is more practical and challenging scenario which studies conversations that involve more than two interlocutors. Research on MPC often includes three subtopics: speaker prediction, utterance prediction, and addressee prediction. (Ouchi and Tsuboi 2016; Lowe et al. 2015) built MPC datasets from Ubuntu technical dialogues and proposed baseline models for MPC tasks. Recent studies on MPC usually train and evaluate models jointly on those three objectives. (Gu et al. 2021) propose MPC-BERT, which fine-tunes BERT (Devlin et al. 2019) on several selfsupervised tasks, and achieve state-of-the-art results on the above MPC tasks. (Su and Zhou 2022) identify speakers of utterances by clustering them with pairwise relations encoded by dialogue content encoder. GIFT (Gu et al. 2023) revises the model structure of transformer encoders to make the self-attention layer be aware of the information flow of MPC. Details regarding MPC can be found in these recent surveys (Gu, Tao, and Ling 2022; Ganesh, Palmer, and Kann 2023). Multi-Modal Dialogue Datasets There have been number of works on building multi-modal dialogue datasets (Das et al. 2016; Mostafazadeh et al. 2017; Shuster et al. 2020; AlAmri et al. 2019; Zheng et al. 2022; Zang et al. 2021; Feng et al. 2023). Despite the diversity in modality (image or video) and the position of the visual information in the dialogue history, all above datasets are limited as the interlocutors are outside the visual contexts rather than situated inside them, and only includes two interlocutors instead of being multi-party. Dialogue in movie/TV series is typical data source of multi-party conversation with situated visual context. Recent large-scale movie dialogue datasets include OpenViDial (Meng et al. 2020; Wang et al. 2021) and VSTAR (Wang et al. 2023). However, these datasets do not have any kind of speaker information, which hinders deeper-level understanding of the conversation. Perhaps the dataset most similar to ours is MELD (Poria et al. 2018), which is also speaker-aware multi-modal multi-party dialogue dataset collected from Friends but focuses only on emotion recognition, and does not annotate faces in the visual context. Conclusion We work on new field of research, multi-modal multi-party conversation (MMC), to bridge the gap between existing researches on multi-modal dialogue and real-world applications such as face-to-face conversations or meetings. We build Friends-MMC, an MMC dataset in which each utterance is paired with video context, speaker, face bounding box and face name annotation. We propose two new tasks of MMC, namely conversation speaker identification and conversation response prediction, and design baseline to solve both tasks. Our future directions include collecting more diverse data rather than movies, and exploring ways to make better use of speaker information for response generation. Acknowledgments The authors thank the kind suggestions and support from AI Data Technology Laboratory of Huawei Noahs Ark Lab. References AlAmri, H.; Cartillier, V.; Das, A.; Wang, J.; Lee, S.; Anderson, P.; Essa, I.; Parikh, D.; Batra, D.; Cherian, A.; Marks, T. K.; and Hori, C. 2019. Audio Visual Scene-Aware Dialog. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 75507559. Cao, Q.; Shen, L.; Xie, W.; Parkhi, O. M.; and Zisserman, A. 2017. VGGFace2: Dataset for Recognising Faces across Pose and Age. 2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018), 6774. Das, A.; Kottur, S.; Gupta, K.; Singh, A.; Yadav, D.; Moura, J. M. F.; Parikh, D.; and Batra, D. 2016. Visual Dialog. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 10801089. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In North American Chapter of the Association for Computational Linguistics. Feng, J.; Sun, Q.; Xu, C.; Zhao, P.; Yang, Y.; Tao, C.; Zhao, D.; and Lin, Q. 2023. MMDialog: Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation. In Annual Meeting of the Association for Computational Linguistics. Fu, T.-J.; Li, L.; Gan, Z.; Lin, K.; Wang, W. Y.; Wang, L.; and Liu, Z. 2021. VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling. ArXiv, abs/2111.12681. Ganesh, A.; Palmer, M.; and Kann, K. 2023. Survey of Challenges and Methods in the Computational Modeling of Multi-Party Dialog. Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023). Gu, J.-C.; Ling, Z.; LIU, Q.; Liu, C.; and Hu, G. 2023. GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding. In Annual Meeting of the Association for Computational Linguistics. Gu, J.-C.; Tao, C.; and Ling, Z. 2022. Who Says What to Whom: Survey of Multi-Party Conversations. In International Joint Conference on Artificial Intelligence. Gu, J.-C.; Tao, C.; Ling, Z.; Xu, C.; Geng, X.; and Jiang, D. 2021. MPC-BERT: Pre-Trained Language Model for Multi-Party Conversation Understanding. In Annual Meeting of the Association for Computational Linguistics. Gurobi Optimization, LLC. 2023. Gurobi Optimizer Reference Manual. DeBERHe, P.; Gao, TaV3: Improving DeBERTa using ELECTRA-Style PreTraining with Gradient-Disentangled Embedding Sharing. arXiv:2111.09543. Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Hu, W.; Chan, Z.; Liu, B.; Zhao, D.; Ma, J.; and Yan, R. 2019. GSN: Graph-Structured Network for Multi-Party In International Joint Conference on Artificial Dialogues. Intelligence. Kalogeiton, V. S.; and Zisserman, A. 2020. Constrained Video Face Clustering using1NN Relations. In British Machine Vision Conference. J.; and Chen, W. 2021. Li, K.; He, Y.; Wang, Y.; Li, Y.; Wang, W.; Luo, P.; Wang, Y.; Wang, L.; and Qiao, Y. 2023a. VideoChat: Chat-Centric Video Understanding. ArXiv, abs/2305.06355. Li, K.; Wang, Y.; He, Y.; Li, Y.; Wang, Y.; Liu, Y.; Wang, Z.; Xu, J.; Chen, G.; Luo, P.; Wang, L.; and Qiao, Y. 2023b. MVBench: Comprehensive Multi-modal Video Understanding Benchmark. ArXiv, abs/2311.17005. Liu, H.; Li, C.; Wu, Q.; and Lee, Y. J. 2023. Visual Instruction Tuning. ArXiv, abs/2304.08485. Lowe, R.; Pow, N.; Serban, I.; and Pineau, J. 2015. The Ubuntu Dialogue Corpus: Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems. In Koller, A.; Skantze, G.; Jurcicek, F.; Araki, M.; and Rose, C. P., eds., Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 285294. Prague, Czech Republic: Association for Computational Linguistics. Maaz, M.; Rasheed, H. A.; Khan, S. H.; and Khan, F. S. 2023. Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models. ArXiv, abs/2306.05424. Meng, Y.; Wang, S.; Han, Q.; Sun, X.; Wu, F.; Yan, R.; and Li, J. 2020. OpenViDial: Large-Scale, Open-Domain Dialogue Dataset with Visual Contexts. ArXiv, abs/2012.15015. Meng, Z.; Mou, L.; and Jin, Z. 2018. Towards Neural Speaker Modeling in Multi-Party Conversation: The Task, Dataset, and Models. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). Mostafazadeh, N.; Brockett, C.; Dolan, W. B.; Galley, M.; Gao, J.; Spithourakis, G. P.; and Vanderwende, L. 2017. Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation. ArXiv, abs/1701.08251. OpenAI. GPT-4o. https://openai.com/index/hello-gpt-4o/. Accessed: 2024-1210. Ouchi, H.; and Tsuboi, Y. 2016. Addressee and Response In Conference on Selection for Multi-Party Conversation. Empirical Methods in Natural Language Processing. Poria, S.; Hazarika, D.; Majumder, N.; Naik, G.; Cambria, E.; and Mihalcea, R. 2018. MELD: Multimodal MultiParty Dataset for Emotion Recognition in Conversations. ArXiv, abs/1810.02508. Schroff, F.; Kalenichenko, D.; and Philbin, J. 2015. FaceNet: unified embedding for face recognition and clustering. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 815823. Shuster, K.; Humeau, S.; Bordes, A.; and Weston, J. 2020. Image-Chat: Engaging Grounded Conversations. In Annual Meeting of the Association for Computational Linguistics. Su, Z.; and Zhou, Q. 2022. Speaker Clustering in Textual Dialogue with Pairwise Utterance Relation and Cross-corpus Dialogue Act Supervision. In Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022, 734744. International Committee on Computational Linguistics. 2024. Hello standing with Advanced Large Language Models. ArXiv, abs/2304.10592. Sun, Q.; Yu, Q.; Cui, Y.; Zhang, F.; Zhang, X.; Wang, Y.; Gao, H.; Liu, J.; Huang, T.; and Wang, X. 2023. Generative Pretraining in Multimodality. CoRR, abs/2307.05222. Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P.; Reed, S. E.; Anguelov, D.; Erhan, D.; Vanhoucke, V.; and Rabinovich, A. 2014. Going deeper with convolutions. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 19. Tao, R.; Pan, Z.; Das, R. K.; Qian, X.; Shou, M. Z.; and Li, H. 2021. Is Someone Speaking?: Exploring Long-term Temporal Features for Audio-visual Active Speaker Detection. Proceedings of the 29th ACM International Conference on Multimedia. Wang, S.; Meng, Y.; Li, X.; Sun, X.; Ouyang, R.; and Li, J. 2021. OpenViDial 2.0: Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts. ArXiv, abs/2109.12761. Wang, Y.; Wang, Y.; and Zhao, D. 2023. Overview of the NLPCC 2023 Shared Task 10: Learn to Watch TV: Multimodal Dialogue Understanding and Response Generation. In 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12-15, 2023, Proceedings, Part III, volume 14304, 412419. Wang, Y.; Zheng, Z.; Zhao, X.; Li, J.; Wang, Y.; and Zhao, D. 2023. VSTAR: Video-grounded Dialogue Dataset for Situated Semantic Understanding with Scene and Topic Transitions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, 50365048. Association for Computational Linguistics. Xu, Y.; Wang, S.; Li, P.; Luo, F.; Wang, X.; Liu, W.; and Liu, Y. 2023. Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf. ArXiv, abs/2309.04658. Zang, X.; Liu, L.; Wang, M.; Song, Y.; Zhang, H.; and Chen, J. 2021. PhotoChat: Human-Human Dialogue Dataset With Photo Sharing Behavior For Joint Image-Text Modeling. ArXiv, abs/2108.01453. Zhang, H.; Du, W.; Shan, J.; Zhou, Q.; Du, Y.; Tenenbaum, J. B.; Shu, T.; and Gan, C. 2023. Building Cooperative Embodied Agents Modularly with Large Language Models. CoRR, abs/2307.02485. Zhang, S.; Zhu, X.; Lei, Z.; Shi, H.; Wang, X.; and Li, S. 2017. S3FD: Single Shot Scale-Invariant Face Detector. 2017 IEEE International Conference on Computer Vision (ICCV), 192201. Zheng, Y.; Chen, G.; Liu, X.; and Lin, K. W. 2022. MMChat: Multi-Modal Chat Dataset on Social Media. ArXiv, abs/2108.07154. Zhou, P.; Madaan, A.; Potharaju, S. P.; Gupta, A.; McKee, K. R.; Holtzman, A.; Pujara, J.; Ren, X.; Mishra, S.; Nematzadeh, A.; Upadhyay, S.; and Faruqui, M. 2023. How FaR Are Large Language Models From Agents with Theoryof-Mind? ArXiv, abs/2310.03051. Zhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M. 2023. MiniGPT-4: Enhancing Vision-Language Under-"
        },
        {
            "title": "Appendix",
            "content": "Analysis of Reward Weights α in Conversation Speaker Identification Figure 4 shows the change of accuracy with respect to α, key hyper-parameter which controls the weight of the scores provided by the visual model M1 and the text model M2. Note that when α = 0, the task reduces to using only the text model M2, and when α = 1, the task reduces to using only the visual model M1. In considerable range of α values, introducing the results of M2 improves the overall accuracy, compared with using M1 only. It verifies that textual contexts certainly contribute to this speaker identification task."
        },
        {
            "title": "Details of Baseline Methods of Conversation\nSpeaker Identification",
            "content": "GPT 3.5 We use in-context learning to perform 3-shot inference with GPT 3.5. Instruction, input and expected target we use is as follow: You are listening to conversation among group of people. You will be provided with name list and the content of conversation, and need to guess which people in the name list speaks each turn of the conversation. Answer one name for each turn in the dialogue, [num turns] comma-seperated names in all. Name list: [candidate 1], [candidate 2], . . . Conversation (one utterance per line): [utterance1] [utterance2] . . . Answer: [speaker 1], [speaker 2], . . . [several more examples] [candidate 2], list: [candidate 1], Name . . . Conversation (one utterance per line): [utterance1] [utterance2] . . . Answer: [speaker 1], [speaker 2], . . . If GPT 3.5 generates more than [num turns] names, we only keep the first [num turns] names as its predictions. If GPT 3.5 generates less than [num turns] names, or generates names not in the candidate list, we pad its prediction / replace the name not in the candidates list with names randomly selected from the candidate list. GPT-4o We evaluate the performance of GPT-4o in zero-shot manner. The prompt we use is as follow: You are listening to conversation among group of people. Their names are: [candidate 1], [candidate 2], . . . , which is labeled on the frames. The visual context of the conversation is also provided in [num turns] images, one image per turn. You need to guess which people in the name list said each sentence. Answer one name for each turn in the dialogue, for example, Alice, Bob, Alice, Carol. The conversation is (one turn per line): [utterance1] [utterance2] . . . Answer: [speaker 1], [speaker 2], . . . Violet We fine-tune all the parameters of Violet. The input format is: [patches of all images] [candidate 1] [candidate 2] ... [CLS] [utterance 1] [CLS] [utterance 2] ... and calculate the cosine similarity between the representation of each utterance (i.e., the last hidden state of the [CLS] before it) and each speaker (i.e., the last hidden state of the candidate speaker name [candidate x]). Emu We use LoRA (Hu et al. 2022) as an paramter-efficient method to fine-tune Emu-14B. As Emu was pre-trained with interleaving image-text data, we also use interleaving visual context and text context as its input. The input format is as follow: Choose the name of the speaker for the each of the [num turns] turns given the dialog history, frames, and faces in the frames. Image of turn 1: [image 1] Content of turn 1: [utterance 1] Faces in turn 1: [bbox 1] [name 1], [bbox 2] [name 2], ... Image of turn 2: ... Speakers of the above [num turns] frames: [speaker 1], [speaker 2], ... LLaVA We use LoRA (Hu et al. 2022) as an paramter-efficient method to fine-tune LLaVA-v1.5-13B. As LLaVA is pretrained with inputs with single frame at time, we also use only 1 turn at time as its input. The input format is as follow: [image] Choose the name of [utterance] the speaker for this sentence: (a) Test set performance on 5 turns dataset. (b) Test set performance on 8 turns dataset. Figure 4: The change of accuracy with respect to α. The dotted horizontal line shows the performance of only using the visual model. Given the names and bounding boxes of the faces for the people that appear in the visual context where the dialog takes place: [bbox 1] [face 1], [bbox 2] [face 2], ... [speaker] Human Performance Here we provide instructions given to participants for the human performance experiment: The two folders each contain 20 pieces of data, with 5 or 8 rounds of conversation Each piece of data contains 5 (or 8) screenshots of TV series. In each screenshot, the green word indicates the content of the line, and the red word indicates the name of the person appearing in the screenshot There is line of blue characters at the bottom, which is collection of all the red characters above. Your task is to select the person from the blue name for each of the 5 (8) lines that is most likely to be the speaker of the line. Note that the speaker may not appear in the corresponding screenshot, so you may need to make comprehensive judgment based on the context information. If you feel that you cant judge accurately, guess an answer, guessing is also an important ability of humans. There are also two files, 5 turns-pred.txt and 8 turnspred.txt in the package. The two files now have row each, representing the results of my annotation of 19 .jpg this piece of data. You need to add 20 lines to each of these two files in the same format to indicate the results of your annotation of 60.jpg-79 .jpg these 20 pieces of data. That is, first enter number to indicate which data you are marking, and then enter 5 (8) personal names to represent speakers for each line, separated by spaces. If you find it too cumbersome to enter the entire persons name, you can just enter the first two letters. For example, ro for ross, ch for chandler, etc."
        }
    ],
    "affiliations": [
        "Beijing Institute for General Artificial Intelligence",
        "Huawei Noahs Ark Lab",
        "National Key Laboratory of General Artificial Intelligence",
        "Wangxuan Institute of Computer Technology, Peking University"
    ]
}