{
    "paper_title": "Bridging Supervised Learning and Reinforcement Learning in Math Reasoning",
    "authors": [
        "Huayu Chen",
        "Kaiwen Zheng",
        "Qinsheng Zhang",
        "Ganqu Cui",
        "Yin Cui",
        "Haotian Ye",
        "Tsung-Yi Lin",
        "Ming-Yu Liu",
        "Jun Zhu",
        "Haoxiang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning (RL) has played a central role in the recent surge of LLMs' math abilities by enabling self-improvement through binary verifier signals. In contrast, Supervised Learning (SL) is rarely considered for such verification-driven training, largely due to its heavy reliance on reference answers and inability to reflect on mistakes. In this work, we challenge the prevailing notion that self-improvement is exclusive to RL and propose Negative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to reflect on their failures and improve autonomously with no external teachers. In online training, instead of throwing away self-generated negative answers, NFT constructs an implicit negative policy to model them. This implicit policy is parameterized with the same positive LLM we target to optimize on positive data, enabling direct policy optimization on all LLMs' generations. We conduct experiments on 7B and 32B models in math reasoning tasks. Results consistently show that through the additional leverage of negative feedback, NFT significantly improves over SL baselines like Rejection sampling Fine-Tuning, matching or even surpassing leading RL algorithms like GRPO and DAPO. Furthermore, we demonstrate that NFT and GRPO are actually equivalent in strict-on-policy training, even though they originate from entirely different theoretical foundations. Our experiments and theoretical findings bridge the gap between SL and RL methods in binary-feedback learning systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 6 1 1 8 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Bridging Supervised Learning and Reinforcement\nLearning in Math Reasoning",
            "content": "Huayu Chen1,2 Kaiwen Zheng1,2 Qinsheng Zhang2 Ganqu Cui1 Yin Cui2 Haotian Ye2, Tsung-Yi Lin2 Ming-Yu Liu2 Jun Zhu1 Haoxiang Wang2 1Tsinghua University 2NVIDIA 3Stanford University https://research.nvidia.com/labs/dir/Negative-aware-Fine-Tuning"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning (RL) has played central role in the recent surge of LLMs math abilities by enabling self-improvement through binary verifier signals. In contrast, Supervised Learning (SL) is rarely considered for such verification-driven training, largely due to its heavy reliance on reference answers and inability to reflect on mistakes. In this work, we challenge the prevailing notion that selfimprovement is exclusive to RL and propose Negative-aware Fine-Tuning (NFT) supervised approach that enables LLMs to reflect on their failures and improve autonomously with no external teachers. In online training, instead of throwing away self-generated negative answers, NFT constructs an implicit negative policy to model them. This implicit policy is parameterized with the same positive LLM we target to optimize on positive data, enabling direct policy optimization on all LLMs generations. We conduct experiments on 7B and 32B models in math reasoning tasks. Results consistently show that through the additional leverage of negative feedback, NFT significantly improves over SL baselines like rejection sampling fine-tuning, matching or even surpassing leading RL algorithms like GRPO and DAPO. Furthermore, we demonstrate that NFT and GRPO are actually equivalent in strict-on-policy training, even though they originate from entirely different theoretical foundations. Our experiments and theoretical findings bridge the gap between SL and RL methods in binary-feedback learning systems."
        },
        {
            "title": "Introduction",
            "content": "The recent surge in math reasoning abilities of Large Language Models (LLMs) is largely driven by fundamental shift in their learning paradigm, from imitation to self-improvement [14, 56, 36, 9]. Instead of relying on reference answers supplied by human annotators or stronger models [29, 35], the new paradigm requires only question dataset with binary verifier to judge the correctness of self-generated answers. By reflecting on their own generation, LLMs can improve autonomously. This approach not only eliminates the need for costly data annotation but also removes competence ceilings imposed by external teachers, offering promising path toward general intelligence [14, 34]. Reinforcement Learning (RL) appears to be natural fit for such verification-driven training. Specific algorithms like PPO [40] and GRPO [41] are explicitly designed to maximize reward signals, which can conveniently take the form of binary verifier outcome. In contrast, Supervised Learning (SL) is rarely considered for realizing self-improvement. common view holds that SL is inherently designed to mimic external teachers by memorizing the positive training data, rendering it unsuitable for self-reflective learning from negative mistakes [11]. Corresponding author. Preprint. Figure 1: spectrum of online algorithms for LLM fine-tuning. NFT bridges reinforcement learning and supervised learning methods through the leverage of negative feedback via supervision. In this work, we challenge the prevailing notion that self-improvement is exclusive to RL, and demonstrate it can be similarly achieved within the supervised learning paradigm. We start with simple SL baseline: Rejection sampling Fine-Tuning (RFT) [61, 15]. At each iteration, an LLM generates answers to questions. verifier helps reject all negative answers. The remaining positive ones are compiled into dataset to fine-tune the LLM itself in supervised manner. RFT has been demonstrated effective by various works [3, 32, 58, 43, 46, 53]. However, it prevents any learning from negative feedback. LLMs are encouraged to reinforce what they already perform well, rather than reflect on their mistakes an ability we believe critical for achieving general intelligence. To overcome this limitation, we propose Negative-aware Fine-Tuning (NFT), an online learning algorithm that enables LLMs to learn from their negative generations (Sec. 3). Like RFT, NFT fine-tunes positive LLM on positive answers via supervision. Crucially, instead of throwing away negative answers, NFT also constructs an implicit negative policy to model them. This implicit policy is parameterized with the same positive LLM we target to optimize on positive data, enabling direct policy optimization on all LLMs generations (Figure 2). NFT has minimal memory overhead, as only single model is maintained throughout training. To understand the connection between NFT and RL approaches, we conduct an in-depth comparison between NFT and GRPO (Sec. 4). Surprisingly, we find the two methods are actually equivalent in strict-on-policy training, despite that they originate from entirely different theoretical frameworks (Figure 1). Notably, the advantage normalization characteristic of GRPO is already implicitly reflected in NFTs loss function. Their main difference arises in off-policy settings, regarding different strategies for clipping model gradients when the learned policy deviates from the old policy. These observations suggest strong connection between SL and RL in binary-feedback learning systems. We evaluate NFT on 7B and 32B Qwen models and report two key findings: 1. Supervised Learning alone can significantly enhance LLMs math reasoning with no external teachers. NFT matches or even surpasses state-of-the-art RL algorithms like GRPO [41] and DAPO [57]. 2. The performance gap between SL and RL in online training largely stems from SLs past inability to leverage negative feedback, rather than any inherent superiority of RL. Through the additional leverage of negative data, NFT substantially bridges the performance gap between SL and leading RL algorithms."
        },
        {
            "title": "2 Background",
            "content": "2.1 Maximum Likelihood vs. Policy Gradient Supervised Learning essentially aims to learn model πθ(aq) to fit the data distribution π(aq). This can be realized by employing the maximum-likelihood objective: max θ Eaπ(aq) log πθ(aq) min θ DKL [π(aq)πθ(aq)] . In LLM fine-tuning, usually means the question prompt, and the answer. To perform Maximumlikelihood training, we need dataset = {q, π(aq)} to draw training samples from. Reinforcement Learning, by contrast, maximizes pre-defined reward r(q, a) for πθ(aq): max θ J(θ) := Eaπθ(aq)r(q, a). Directly back-propagating through J(θ) is non-trivial, as r() can be an arbitrary scalar function whose gradient is unknown. Luckily, θJ(θ) can be estimated, making policy optimization feasible. θJ(θ) = Eaπθ(aq)θ [r(q, a) log πθ(aq)] . (1) 2 Figure 2: Illustration of the NFT algorithm. Data Collection: An LLM π generates answers to set of math questions. Generation results are split into two sub-datasets based on their answer correctness. Policy Optimization: By constructing an implicit policy for modeling negative data, NFT enables direct policy optimization on both positive and negative answers via maximum-likelihood training. Eq. 1 is known as the Policy Gradient (PG) or REINFORCE algorithm [51, 44]. In sequential decision-making problems such as language reasoning, can be interpreted as the token decision for each step t, and r(q, a) can be replaced with advantage functions A(q, a) [39, 10]."
        },
        {
            "title": "2.2 Math Reasoning RL: From Policy Gradient to GRPO",
            "content": "Eq. 1 requires training to be on-policy, where πθ can only be updated single time after data collection. To break this limitation, importance sampling can be applied [40]. Suppose the policy for collecting the RL dataset is denoted as πold, we have θJ(θ) = Eaπold(aq) (cid:20) πθ(aq) πold(aq) r(q, a)θ log πθ(aq) (cid:21) = Eaπold(aq) [r(q, a)θRθ(q, a)] , (2) πold(aq) is the likelihood Ratio between two policies. where Rθ(q, a) := πθ(aq) In math reasoning tasks, PPO [40] and subsequent GRPO [41] algorithms further apply gradient clipping to constrain the distance between πθ and πold: (cid:88) (cid:88) (cid:104) (cid:105) Rt θ(q, a) ˆAq,a, clip(Rt θ(q, a), 1 ϵ, 1 + ϵ) ˆAq,a LGRPO(θ) = min (3) , q,aπold θ(q, a) := πθ(atq,a<t) πold(atq,a<t) , and ˆAq,a is the estimated advantage value. Note that we have where Rt dropped some auxiliary loss terms, such as KL and entropy regularization, as they have been pointed out to be unnecessary by recent studies like DAPO [57]. GRPO proposes an efficient and effective way to estimate ˆAq,a. Collect answers a1:K and their binary reward r1:K {0, 1} for each question. The advantage is defined to be the normalized reward: ˆAq,a := (cid:2)r(q, a) mean{r1:K}(cid:3) / std{r1:K}. Later studies [31] suggest removing the std term from Eq. 4, and keeping mean normalization only. (4)"
        },
        {
            "title": "3 Method",
            "content": "3.1 Problem Setup Dataset. Given set of math questions {q1:N }, pretrained LLM π(aq), and verifier for judging the correctness. In every iteration, we generate dataset = {q, a1:K π, r1:K}1:N , where {0, 1} is the correctness label, and the number of answers collected for each question. Dataset π can be split into two subsets: D+ and D. D+ contains all positive answers, and contains the rest negative ones. We denote the underlying answer distribution of D+ as π+(q). Learning Target. We want to optimize the old policy π into new policy π+ π+(aq) can be formalized using Bayes Rule: θ π+. The target π+(aq) := π(aq, r=1) = π(aq)p(r=1q, a) π(aq)p(r=1q, a) (cid:80) , (5) 3 Figure 3: Left: Policy Splitting. The generation policy can be split into positive policy and negative policy, and re-expressed as their linear combination. Right: Policy Improvement. By iteratively optimizing towards its positive split, an LLM policy π0 can improve continuously. where means all possible language space for a. Discussion. An obvious solution for learning π+ is to fine-tune solely on correct answers (D+) and discard totally (RFT) [15, 53]. However, this approach prevents the model from any learning on its negative feedback (D). We posit that the ability to reflect on ones own failures is not merely desirable, but central to general intelligence, marking shift from pure imitation to selfreflective learning. Though traditionally viewed as distinctive strength of RL [11, 62], we ask: Can self-reflective improvement be similarly achieved within the SL paradigm? 3.2 Direct Optimization of Language Models with Negative Answers In this section, we discuss how to leverage negative data to directly optimize π+ θ . Despite seemingly impossible at first thought, we find the target policy π+ and the negative policy π are tightly coupled, making feasible training π+ θ directly from D. First, we formalize the definition of the negative policy π similar to Eq. 5 π(aq) := π(aq, r=0) = π(aq)[1 p(r=1q, a)] π(aq)[1 p(r=1q, a)] (cid:80) . (6) Combining Eq. 5 and Eq. 6, we make key observation that rqπ+(aq) + [1 rq] π(aq) = π(aq), (7) where rq := (cid:80) π(aq)p(r=1q, a) = p(r=1q) is the correctness rate of LLM π over question q. In practice, rq mean{r1:K} can be estimated using the Monte Carlo rewards in dataset D. Implicit negative policy. Eq. 7 reveals tight coupling between π+ and π (Figure 3). Given that we already have π as the pretrained LLM and rq is estimable, learning π from negative data should, in principle, shape the target policy π+ θ , in manner analogous to SL on positive data. To realize this idea, we construct an implicit negative policy, denoted π target policy π+ θ using the relationship in Eq. 7: θ , by re-parameterizing the π θ (aq) := π(aq) rqπ+ θ (aq) 1 rq . θ on negative answers directly leads to optimizing the underlying positive LLM π+ Thus, training π (Figure 2). We have the following guarantee: Theorem 3.1 (Policy Optimization with Negative Answers). Consider the maximum-likelihood objective for training the implicit negative policy π θ : θ max θ Ep(q)π(aq) (cid:2)log π θ (aq)(cid:3) min θ (cid:20) E(q,a)D log π(aq) rqπ+ θ (aq) 1 rq (cid:21) (8) Assuming unlimited data and model capacity, the optimal solution for solving Eq. 8 is q, : π+ θ (aq) = π+(aq) Proof in Appendix A. Theorem 3.1 demonstrates the feasibility of policy optimization on negative data only. To further utilize positive data, we additionally conduct supervised training on D+: LNFT (a,q,r)D(θ) = (cid:20) log (cid:21) π+ θ (aq) π(aq) + (1 r) log 1 rq π+ θ (aq) π(aq) 1 rq (9) Generate answers a1:K and verify their correctness r1:K Calculate correctness rate rq = mean{r1:K } and token-level likelihood {π(atq, a<t)1:a}1:K {q, rq, a1:K , r1:K , π1:K } If 0 < rq < 1 // Data Collection // Prompt Filtering for each sampled prompt do Return stop_gradient[max(x, ϵ) x] + Algorithm 1 Negative-aware Fine-Tuning (NFT) 1: Input: Language model π, prompt set q1:N , verifier r(). 2: Def max_v(x, ϵ): 3: 4: for each iteration do 5: 6: 7: 8: 9: 10: 11: end for Initialize π+ for each mini batch {q, a, r, rq, πt} in do θ (atq,a<t) π(atq,a<t) , θ π θ(q, a) = π+ Rt If = 0: Rt Rt 12: 13: 14: 15: 16: 17: 18: 19: end for end for π π+ θ(q, a) = (1 rq Rt θ(q, a) = max_v[Rt (cid:80) log Rt θ(q, a))/(1 rq) θ(q, a), ϵ] θ(q, a) (Eq. 10) θ θ + λθ θ , start the next iteration //Straight-through Max Operator //Clip Value while Keeping Gradient // Positive Likelihood Ratio // Implicit Negative Likelihood Ratio // Clip Negative Likelihood Ratio // Maximum Likelihood Training Note that we have subtracted baseline term log π(aq) from the loss. Since this term is unrelated to θ, it does not affect the loss gradient and thus the optimal solution. π(aq) is the old data likelihood before optimizer update. At the start of training, we have π+ θ = π such that Lθ (a,q,r) = 0. We name our method Negative-aware Fine-Tuning (NFT) as it enables the additional leverage of negative data for policy optimization compared with RFT. NFT is memory-efficient. In practice, we keep only single model copy in memory. The old policy likelihood π(aq) can be pre-computed during data generation. 3.3 Practical Algorithm We introduce several improvements over Eq. and propose practical objective of NFT: LNFT (θ) = (cid:88) q,a,r (cid:88) ω(q) (cid:20) log Rt θ(q, a) + (1 r) log max_v( 1 ˆrq Rt 1 ˆrq θ(q, a) (cid:21) , ϵ) (10) where Rt θ(q, a) = π+ θ (atq, a<t) π(atq, a<t) , and ˆrq = 1 (cid:88) aq r(q, a). Pseudo code is in Algorithm 1. Below, we explain our key design choices. Token-level loss. Eq. essentially deals with sequence data, where answer likelihood π(aq) = (cid:81) π(atq, a<t) is heavily correlated with answer length. This introduces high variance in gradient estimation and causes numerical instabilities during training. Following existing approaches [40, 31, 57], we view each token decision as an individual unit and sum up their loss in Eq. 10. Clipping negative likelihood ratio. The negative loss calculation in Eq. 10 involves logarithm whose argument must remain positive, imposing (1 ˆrqRt θ is unoptimized, this requirement may not be satisfied, potentially leading to training collapse. We therefore enforce minimum value of ϵ > 0 for the negative likelihood ratio. To preserve gradient flow after clipping, we further apply straight-through gradient estimation [4, 47]. Implementation detail is in Algorithm 1. θ)/(1 ˆrq) > 0. When Rt Prompt weighting. To focus training on more informative instances, we weight each prompt by ω(q) and assign higher importance to hard questions with low correctness rate rq. An ablation study is posted in Sec. 5.4. This design also helps align NFT with RL algorithms like GRPO. We discuss the details in Sec. 4."
        },
        {
            "title": "4 Understanding the Gap between NFT and GRPO",
            "content": "Despite originating from entirely different theoretical foundations, NFT and GRPO exhibit significant similarities. Notably, we find GRPO and NFT are equivalent in on-policy training. To understand this, we calculate and compare their loss gradients: Proposition 4.1 (Algorithm Gradient Comparision). Suppose there are ˆrqK positive answers and (1 ˆrq)K negative ones for given question (a) GRPO Gradient: Consider only binary reward {0, 1} in Eq. 3, GRPO loss gradient satisfies θ(q, a) < 1 + ϵ(cid:3)+(1r)A θ(q, a) > 1 ϵ(cid:3)(cid:111) θLGRPO I (cid:2)Rt I(cid:2)Rt (θ) = θRt (cid:88) (cid:110) rA+ where A+ = (cid:113) 1ˆrq ˆrq and = (cid:113) ˆrq 1ˆrq are respectively normalized advantages for answers. (b) NFT Gradient: Let ω(q) = (cid:112)(1 ˆrq)/ˆrq, NFT loss gradient satisfies θLNFT (θ) = (cid:88) (cid:110) rA+ 1 θ(q, a) Rt + (1 r)A max (cid:2) 1 ˆrq Rt 1 ˆrq θ(q, a) , ϵ(cid:3)1(cid:111) θRt θ(q, a). θ(q, a), (11) All proofs are provided in Appendix A. Comparing Eq. 11 and Eq. 12, the only difference between GRPO and NFT is their strategy for clipping model gradients when training data becomes off-policy (Figure 4). GRPO simply zeros out the gradient when the learned policy πθ shifts far away from the old policy π, while NFT takes softer decay schedule. Surprisingly, we find GRPO and NFT to be equivalent when training is totally on-policy, despite their distinctively different derivations: (12) Figure 4: Gradient weight for NFT and GRPO. Proposition 4.2 (On-policy Gradient Equivalence). Following the set up of Proposition 4.1 and let ϵ 1, GRPO and NFT loss gradient are equivalent in on-policy training: (θ) = θLGRPO θ(q, a) = 1 = θLNFT (θ) Rt Implicit group normalization. Proposition 4.1 shows the normalized advantage term is implicitly present within NFTs loss function. This partially justifies the Group Normalization design choice for GRPO, which was initially introduced only as an empirical technique [41]. In Appendix A, we further demonstrate that by adjusting ω(q) = 1 ˆrq, NFT also aligns with Dr. GRPO [31]. Our findings suggest strong connection between RL and SL frameworks in binary reward settings."
        },
        {
            "title": "5 Experiments",
            "content": "We seek to answer the following questions through our experiments. 1. How does NFT perform in comparison with existing RL algorithms such as GRPO? (Sec. 5.2) 2. How does negative data contribute to NFTs performance gain? (Sec. 5.3) 3. Which empirical design choices are important to the effectiveness of NFT? (Sec. 5.4) 5.1 Experiment Setups Training. We perform online fine-tuning on Qwen2.5-Math-7B [56] and Qwen2.5-32B [55] to enhance their math abilities without relying on external teachers. The training dataset is the publicly available DAPO-Math-17k [57], which consists solely of math questions paired with ground-truth answers in integer form. During training, all models are fine-tuned for approximately 5,000 gradient steps with batch size of 512. We fix the generation temperature to 1.0. Evaluation. We evaluate models on six validation benchmarks and report their average accuracy: AIME 2024, AIME 2025, AMC 2023 [27], MATH500 [20], OlympiadBench [19], and Minerva Figure 5: Comparison of the released NFT-7B with other zero-style math models of Qwen series. Table 1: NFT performs competitively compared with other algorithms. We report avg@32 for AIME24, AIME25, and AMC23 and avg@1 for others. Numbers within 1 % of the max are bolded. Model AIME24 MATH500 AIME25 AMC23 Olympiad Minerva Average Qwen2.5-Math-7B Preference fine-tuning + DPO Reinforcement fine-tuning + GRPO + Dr. GRPO + DAPO Supervised fine-tuning + RFT + NFT Qwen2.5-32B + DAPO + RFT + NFT 13.3 29. 30.2 31.8 33.1 33.7 32.0 4.1 44.1 29.9 37.8 69.0 79.8 80.4 83.4 81. 79.8 83.2 68.6 89.2 86.2 88.4 5.5 13.8 17.1 15.7 18.7 13.4 18. 1.0 33.4 19.1 31.5 45.8 83.2 79.5 80.2 85.0 79.7 88.5 45.0 90.9 92.4 93. 34.7 48.0 51.8 49.6 49.9 44.3 47.3 31.1 54.1 45.3 55.0 21. 39.0 38.2 38.2 39.3 38.6 40.8 27.9 47.5 44.1 48.9 31.6 48. 49.5 49.8 51.2 48.3 51.7 29.6 59.9 52.8 59.2 Math [26]. Validation is conducted using top-p value of 0.7. Validation temperature is 1.0 for 7B models and 0.6 for 32B models. We use math-verify [24] as the verifier for training validation, and simpleRL verifier [63] for final evaluation. Baseline methods. We compare against set of online fine-tuning algorithms, including Iterative DPO [38, 52, 18], GRPO [41], Dr. GRPO [31], DAPO [57], and RFT [15, 61]. DAPO and RFT are highlighted below. Details for other algorithms are in Appendix B. DAPO is variant of GRPO that has achieved state-of-the-art AIME performance on 32B models. Our NFT implementation is adapted from the official DAPO codebase, based on the VeRL framework [42]. NFT inherits most of DAPOs hyperparameters and design choices, including dynamic data sampling, token-level loss normalization, and no KL regularization. RFT is simple but effective SL baseline that only fine-tunes LLMs on positive answers and throws away negative data. In our implementation, the main difference between RFT and NFT is that RFT zeros out negative data loss and keeps constant prompt weight ω(q) = 1 during training. 5.2 NFT Performance Evaluation Model comparison. By applying NFT to Qwen2.5-Math-7B, we release NFT-7B-Zero (Figure 5). NFT-7B-Zero achieves competitive performance on all benchmarks compared to other zero-style 7B math models [13, 31, 53, 52]. This provides strong empirical evidence for the effectiveness of the NFT algorithm and demonstrates that SL alone can enable effective self-improvement in math tasks. Algorithm comparison. To isolate the contribution of the algorithm itself, we benchmarked various online algorithms using identical training data, infrastructure, and general hyperparameters  (Table 1)  . Results show that NFT matches or even surpasses state-of-the-art methods such as DAPO. Figure 6 and 11 present training curves across multiple runs. NFT exhibits convergence speed and final performance on par with DAPO, further supporting its stability. 7 Figure 6: Training and validation accuracy curves for 7B experiments. We conducted 3-4 random and independent experiments for each algorithm and report their mean std results. Figure 7: Average accuracy across 6 benchmarks for 32B experiments. More curves in Appendix C. 5.3 Benefits of Negative Data Negative feedback enhances performance and exploration. Table 1 shows that NFT consistently outperforms RFT by clear margin, highlighting the benefit of incorporating negative feedback during training. Notably, we observe clear divergence in training dynamics between RFT and NFT. Across both 7B and 32B model settings, RFT tends to reduce entropy over time, whereas NFT and RL methods like DAPO encourage increasing entropy (Figure 8). This behavior suggests more active exploration [57], potentially leading to the performance gap between NFT and RFT. Figure 8: Entropy curves for 7B and 32B runs. Negative Feedback becomes increasingly important in larger models. The performance gap between RFT and NFT widens over training in 32B experiments (Figure 11), while the trend is less obvious for 7B. Similarly, the DeepSeek-R1 report [14] also notes RL offers greater benefits over SFT in larger models. potential explanation could be the increasing importance of negative data. RFT remains strong baseline. Although surpassed by numerous algorithms, RFT still deserves attention due to its extreme simplicity. In 32B settings (Figure 11), learning from positive data (RFT) contributes to 80% of the total gain achieved by our best-performing model, while negative data only accounts for the remaining 20%. These findings echo recent studies [62, 53, 30, 65, 50], which suggest RL primarily amplifies existing capabilities in large models rather than fostering new skills. How to exploit negative feedback remains an open challenge with heavy potential. 8 5.4 Ingredients Behind NFTs Effectiveness We discuss two empirical design choices that notably help NFT achieve strong performance. Prioritize harder questions. We find that assigning higher weights to difficult questions with low answer correctness rate ˆrq can enhance model performance. We achieve this mainly by selecting ω(q) in Eq. 10 from 3 choices: (1) ω(q) = 1. (2) ω(q) = 1 rq, which aligns NFT with Dr. GRPO in on-policy training (Sec. 4). (3) ω(q) = (cid:112)(1 rq)/rq, which aligns NFT with GRPO. Figure 9 visualizes different ω(q), and their effect for NFT and RFT. We find choices (2) and (3) perform similarly, both outperforming constant weighting choice (1). Avoid overpenalizing mistakes. The clip value ϵ of NFT (Eq. 10) sets an upper bound on the penalty weight when the likelihood ratio Rt θ for negative answers increases. When ϵ is small (e.g., near zero), the algorithm empirically assigns high penalties to rising likelihoods of incorrect answers (Figure 10). However, our experiments show that overly aggressive penalization with ϵ 0 degrades overall performance. We thus adopt default setting of ϵ = 1.0."
        },
        {
            "title": "6 Related Works",
            "content": "Figure 9: Effect of prompt weighting. Figure 10: Effect of negative ratio clip value ϵ. Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the frontier of LLM reasoning [14, 36, 45, 9]. Compared with previous RL practices that rely on strong reward models [49, 59, 64] to simulate human feedback [37, 10, 13], RLVR turns to ground truth verifier for providing reliable binary supervision [25, 41]. Moreover, unlike preference-based learning algorithms such as DPO [38, 5, 2, 16, 48, 6, 21, 54], RLVR does not require paired preference data, rendering it more flexible and memory-efficient. Despite the demonstrated effectiveness of RL algorithms in verification-driven training [28, 1, 22, 57, 12, 60], recent studies suggest that supervised learning (SL) may also suffice for achieving self-improvement in LLMs [15, 53]. Our method further addresses SLs inability to incorporate negative feedback [23], bridging both the theoretical and the performance gap between the two fields, and can be easily adapted to other language paradigms such as masked LMs [17, 67, 33]. key design of NFT involves implicit policy modeling for direct policy optimization. This design, emphasizing direct optimization via implicitly defined models, shares conceptual similarities with some existing approaches. In preference-based training, DPO [38] introduces an implicit reward model parameterized by the policy network to allow optimizing policies directly. Recent visual modeling efforts also leverage implicit conditional or residual models parameterized by generation networks to avoid guided sampling [8, 7] or enhance quality [66]."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduce Negative-aware Fine-Tuning (NFT), supervised approach that enables LLMs to learn from their own negative generations. In online training, NFT substantially improves upon supervised learning baselines through the additional leverage of negative feedback, achieving performance comparable to leading RL algorithms like GRPO. Notably, we unveiled theoretical equivalence between NFT and GRPO under strict-on-policy conditions, despite their disparate theoretical foundations. These findings highlight the robust capability of supervised learning for verification-driven self-improvement and significantly bridge the conceptual and practical gap between SL and RL paradigms in binary-feedback learning systems."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Wei Xiong, Zekun Hao, Yuxuan Tong, Lifan Yuan, Jiashu Xu, and Chang Zhou for the insightful discussion."
        },
        {
            "title": "References",
            "content": "[1] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. [2] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In AISTATS, 2024. [3] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. [4] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. [5] Tianchi Cai, Xierui Song, Jiyan Jiang, Fei Teng, Jinjie Gu, and Guannan Zhang. Ulma: Unified language model alignment with demonstration and point-wise human preference. arXiv preprint arXiv:2312.02554, 2023. [6] Huayu Chen, Guande He, Lifan Yuan, Ganqu Cui, Hang Su, and Jun Zhu. Noise contrastive alignment of language models with explicit rewards. In NeurIPS, 2024. [7] Huayu Chen, Kai Jiang, Kaiwen Zheng, Jianfei Chen, Hang Su, and Jun Zhu. Visual generation without guidance. In ICML, 2025. [8] Huayu Chen, Hang Su, Peize Sun, and Jun Zhu. Toward guidance-free ar visual generation via condition contrastive alignment. In ICLR, 2025. [9] Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acereason-nemotron: Advancing math and code reasoning through reinforcement learning. arXiv preprint, 2025. [10] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In NeurIPS, 2017. [11] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. [12] Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. Gpg: simple and strong reinforcement learning baseline for model reasoning. arXiv preprint arXiv:2504.02546, 2025. [13] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. [14] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [15] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. TMLR, 2023. [16] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. [17] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. arXiv preprint arXiv:1904.09324, 2019. [18] Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792, 2024. 10 [19] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [20] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [21] Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model. arXiv preprint arXiv:2403.07691, 2024. [22] Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. [23] Ermo Hua, Biqing Qi, Kaiyan Zhang, Yue Yu, Ning Ding, Xingtai Lv, Kai Tian, and Bowen Zhou. Intuitive fine-tuning: Towards simplifying alignment into single process. arXiv preprint arXiv:2405.11870, 2024. [24] Hynek Kydlíˇcek. Math-verify: Math verification library, 2024. [25] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. T\" ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. [26] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. In NeurIPS, 2022. [27] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 2024. [28] Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. Remax: simple, effective, and efficient reinforcement learning method for aligning large language models. arXiv preprint arXiv:2310.10505, 2023. [29] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [30] Zichen Liu, Changyu Chen, Wenjun Li, Tianyu Pang, Chao Du, and Min Lin. There may not be aha moment in r1-zero-like traininga pilot study, 2025. [31] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [32] Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir Radev, and Jianfeng Gao. Learning math reasoning from self-sampled correct and partiallycorrect solutions. In ICLR, 2023. [33] Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024. [34] NVIDIA. Cosmos-reason1: From physical common sense to embodied reasoning. arXiv preprint arXiv:2503.15558, 2025. [35] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [36] OpenAI. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [37] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In NeurIPS, 2022. [38] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. 11 [39] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, 2015. [40] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [41] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [42] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [43] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023. [44] Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In NeurIPS, 1999. [45] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [46] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [47] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017. [48] Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints. arXiv preprint arXiv:2309.16240, 2023. [49] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. [50] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025. [51] Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 1992. [52] Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. arXiv preprint arXiv:2312.11456, 2023. [53] Wei Xiong, Jiarui Yao, Yuhui Xu, Bo Pang, Lei Wang, Doyen Sahoo, Junnan Li, Nan Jiang, Tong Zhang, Caiming Xiong, et al. minimalist approach to llm reasoning: from rejection sampling to reinforce. arXiv preprint arXiv:2504.11343, 2025. [54] Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others: Iterative preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682, 2023. [55] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [56] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [57] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [58] Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback. In NeurIPS, 2023. 12 [59] Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981, 2024. [60] Yurun Yuan, Fan Chen, Zeyu Jia, Alexander Rakhlin, and Tengyang Xie. Trajectory bellman residual minimization: simple value-based method for llm reasoning, 2025. [61] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023. [62] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. [63] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. [64] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. [65] Rosie Zhao, Alexandru Meterez, Sham Kakade, Cengiz Pehlevan, Samy Jelassi, and Eran Malach. Echo chamber: Rl post-training amplifies behaviors learned in pretraining. arXiv preprint arXiv:2504.07912, 2025. [66] Kaiwen Zheng, Yongxin Chen, Huayu Chen, Guande He, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Direct discriminative optimization: Your likelihood-based visual generative model is secretly gan discriminator. In ICML, 2025. [67] Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024."
        },
        {
            "title": "A Proof of Theorems",
            "content": "Theorem A.1 (Policy Optimization with Negative Answers). Consider the maximum-likelihood objective for training the implicit negative policy π θ : max θ Ep(q)π(aq) (cid:2)log π θ (aq)(cid:3) min θ (cid:20) E(q,a)D log π(aq) rqπ+ θ (aq) (cid:21) 1 rq Assuming unlimited data and model capacity, the optimal solution for solving Eq. 8 is q, : π+ θ (aq) = π+(aq) Proof. The proof is quite straightforward. First, we show that maximum-likelihood training leads to the optimal solution π θ (aq) = π(aq). max θ max θ Ep(q)π(aq) Ep(q)π(aq) min θ Ep(q)DKL (cid:2)log π (cid:2)log π (cid:2)π(aq)π θ (aq)(cid:3) θ (aq) log π(aq)(cid:3) θ (aq)(cid:3) Since DKL (cid:2)π(aq)π θ (aq)(cid:3) 0. The equality holds if and only if : π θ = π. We thus have Next, we prove π+ θ = π+. q, : π θ (aq) = π(aq). (13) Note that that during training, π θ is only an implicit policy defined by π+ π(aq) rqπ+ θ (aq) π θ (aq) := . 1 rq θ through On the other hand, the negative data distribution π has the same relationship with π+ by Eq. 7. π(aq) := π(aq) rqπ+(aq) 1 rq . We have ensured 0 < rq < 1 during training, combining these observations and Eq. 13, we have q, : π+ θ (aq) = π+(aq) Proposition A.2 (Algorithm Gradient Comparision). Suppose there are ˆrqK positive answers and (1 ˆrq)K negative ones for given question (a) GRPO Gradient: Consider only binary reward {0, 1} in Eq. 3, GRPO loss gradient satisfies θLGRPO (θ) = (cid:88) (cid:110) rA+ (cid:2)Rt θ(q, a) < 1 + ϵ(cid:3)+(1r)A I(cid:2)Rt θ(q, a) > 1 ϵ(cid:3)(cid:111) θRt θ(q, a), where A+ = (cid:113) 1ˆrq ˆrq and q = (cid:113) ˆrq 1ˆrq are respectively normalized advantages for answers. (b) NFT Gradient: Let ω(q) = (cid:112)(1 ˆrq)/ˆrq, NFT loss gradient satisfies θLNFT (θ) = (cid:88) (cid:110) rA+ 1 θ(q, a) Rt + (1 r)A max (cid:2) 1 ˆrq Rt 1 ˆrq θ(q, a) , ϵ(cid:3)1(cid:111) θRt θ(q, a). Proof. (a) GRPO Gradient: We first copy down the GRPO loss definition from Eq. 3. (cid:105) θ(q, a) ˆAq,a, clip(Rt θ(q, a), 1 ϵ, 1 + ϵ) ˆAq,a . (cid:88) (cid:104) Rt min LGRPO (θ) = q,a,t 14 The normalized advantage can be estimated as ˆAq,a := (cid:2)r(q, a) mean{r1:K}(cid:3) / std{r1:K} where mean{r1:K} = and std{r1:K} ="
        },
        {
            "title": "1\nK\n(cid:114) 1\nK",
            "content": "[ˆrqK 1 + (1 ˆrq)K 0] = ˆrq [ˆrqK (1 ˆrq)2 + (1 ˆrq)K (0 ˆrq)2] = (cid:113) ˆrq(1 ˆrq). When is positive answer, r(q, a) = 1. We have A+ = (cid:113) 1ˆrq ˆrq > 0."
        },
        {
            "title": "LGRPO",
            "content": "D+ (θ) = (cid:88) q,a+,t min (cid:2)Rt θ(q, a+), 1 + ϵ(cid:3) ˆAq,a+ θLGRPO D+ (θ) = (cid:88) q,a+,t (cid:2)Rt A+ θ(q, a+) < 1 + ϵ(cid:3). (14) When is negative answer, r(q, a) = 0. We have = (cid:113) ˆrq 1ˆrq < 0. LGRPO (θ) = (cid:88) q,a,t max (cid:2)Rt θ(q, a), 1 ϵ(cid:3) ˆAq,a θLGRPO (θ) = (cid:88) q,a,t (cid:2)Rt θ(q, a) > 1 ϵ(cid:3). (15) Combining Eq. 14 and Eq. 15, we have θLGRPO (θ) = (cid:88) (cid:110) rA+ (cid:2)Rt θ(q, a) < 1 + ϵ(cid:3)+(1r)A I(cid:2)Rt θ(q, a) > 1 ϵ(cid:3)(cid:111) θRt θ(q, a), (a) NFT Gradient: We copy down the NFT loss definition from Eq. 10. LNFT (θ) = (cid:88) q,a,t ω(q) (cid:20) log Rt θ(q, a) + (1 r) log max_v( 1 ˆrq Rt 1 ˆrq θ(q, a) (cid:21) , ϵ) When is positive answer, r(q, a) = 1. LNFT D+ (θ) = (cid:88) q,a+,t ω(q) log Rt θ(q, a) (cid:115) 1 ˆrq ˆrq log Rt θ(q, a) A+ log Rt θ(q, a+) (cid:88) = = q,a+,t (cid:88) q,a+,t θLNFT D+ = (cid:88) q,a+,t A+ 1 θ(q, a+) Rt θRt θ(q, a+) (16) When is negative answer, r(q, a) = 0. LNFT (θ) = (cid:88) q,a,t (cid:20) ω(q) log max_v( 1 ˆrq Rt θ(q, a) 1 ˆrq (cid:21) , ϵ) (cid:88) = q,a,t (cid:115) 1 ˆrq ˆrq (cid:20) log max_v( 1 ˆrq Rt θ(q, a) 1 ˆrq (cid:21) , ϵ) θLNFT = (cid:115) (cid:88) q,a,t (cid:20) max( 1 ˆrq ˆrq 1 ˆrq Rt θ(q, a) 1 ˆrq , ϵ)1 ˆrq 1 ˆrq θRt θ(q, a) (cid:21) = (cid:88) q,a,t (cid:88) q,a,t = (cid:115) (cid:20) max( ˆrq 1 ˆrq 1 ˆrq Rt θ(q, a) 1 ˆrq , ϵ)1 θRt θ(q, a) (cid:21) (cid:20) max( q 1 ˆrq Rt θ(q, a) 1 ˆrq , ϵ)1 θRt θ(q, a) (cid:21) (17) Combining Eq. 16 and Eq. 17, we have θLNFT (θ) = (cid:88) (cid:110) rA+ 1 θ(q, a) Rt + (1 r)A max (cid:2) 1 ˆrq Rt 1 ˆrq θ(q, a) , ϵ(cid:3)1(cid:111) θRt θ(q, a). Remark A.3 (Dr. GRPO). The main practical difference between Dr. GRPO [31] and GRPO is that Dr. GRPO removes the std normalization term when estimating group-normalized advantages. Following Proposition 4.1, we simply need to set ω(q) = 1 ˆrq instead of ω(q) = with the Dr. GRPO loss function. Proposition A.4 (On-policy Gradient Equivalence). Following the set up of Proposition 4.1 and let ϵ 1, GRPO and NFT loss gradient are equivalent in on-policy training: (cid:113) 1ˆrq ˆrq to align Rt θ(q, a) = 1 = θLNFT (θ) = θLGRPO (θ) Proof. The proof is simple. When on-policy, Rt Regarding positive answers a+, GRPO gradient (Eq. 14) and NFT gradient (Eq. 16) both become θ(q, a) = 1. θLGRPO D+ (θ) = θLNFT D+ (θ) = A+ θRt θ(q, a+). Regarding negative answers a, GRPO gradient (Eq. 15) and NFT gradient (Eq. 17) both become θLGRPO (θ) = θLNFT (θ) = θRt θ(q, a)."
        },
        {
            "title": "B Experiment Details",
            "content": "General training setup. All algorithms are implemented based on the official DAPO codebase within the VeRL framework. We use learning rate of 1e-6 with linear warm-up schedule across all experiments. At each rollout step, we generate 16 answers for each of 512 sampled questions, then split the data into 16 mini-batches and train the policy network for 16 gradient steps. Models are trained for 320 rollout steps, totaling over 5,000 gradient steps. Unless otherwise specified, we follow DAPOs default design choices, including dynamic data sampling, token-level loss normalization, and no KL regularization. For 7B training, we restrict context lengths to 4K and use 64 NVIDIA H100 GPUs. For 32B training, we restrict context lengths to 32K (DAPO) or 16K (others), and use 128-256 NVIDIA H100 GPUs. DAPO. We adopt faithful implementation of the official DAPO codebase, keeping all hyperparameters untoched. NFT. Compared with DAPO, NFT modifies the context length to 16K for 32B training. We find this does not significantly affect performance, but noticeably reduces data collection time. Another difference is that we remove the overlong reward shaping technique of DAPO to ensure binary reward outcome. In our setting, truncated answers are treated as negative, which sufficiently discourages overlong answers. The negative ratio clipping parameter is set to ϵ = 1.0, and the prompt weight is defined as ω(q) = 1 rq. RFT zeros out negative data loss in NFR implementation and keeps constant prompt weight ω(q) = 1 during training. 16 GRPO does not adopt the dynamic sampling technique proposed by DAPO. Rather, it simply leverages all data for training, even though the gradient for all-positive or all-negative questions should be zeroed out automatically by the algorithm itself [57]. Other DAPO-related techniques are kept, such as setting positive clipping parameter to higher value ϵ + = 0.28. Since GRPO requires less time for sampling data, we train GRPO models for 580+ rollout steps instead of 320 steps, which roughly takes the same training time compared with DAPO experiments. Dr. GRPO is modified from our GRPO implementation. The only difference is the removal of the std normalization when computing group-normalized advantages. Iterative DPO. Comparing DPO with other RL algorithms in head-to-head fashion is difficult because DPO requires paired data to calculate the training objective, while we sample 16 unpaired answers for each question. To solve this problem, we take the implementation of InfoNCA [6], variation of the DPO algorithm that can handle > 2 responses per question:"
        },
        {
            "title": "LInfoNCA",
            "content": "(q,a1:K ,r1:K )D(θ) = (cid:34) (cid:88) k=1 r(k) i=1 r(i) (cid:80)K log eβRθ(q,ak) i=1 eβRθ(q,ai) (cid:80)K (cid:35) InfoNCA is guaranteed to become DPO algorithm in = 2 settings. We ablate β {0.1, 0.01, 0.02} and report the best averaged validation result. We find InfoNCA training to be unstable and add an SFT regularization term to the original loss function for stabilizing the training process. Validation details. Validation is performed with top-p value of 0.7. The temperature is set to 1.0 for 7B models and 0.6 for 32B models, with context lengths matching the training configuration. We use math-verify [24] as the verifier during training validation, and simpleRL [63] for final evaluation. The DAPO-17k benchmark consists solely of training questions whose ground truth answers are integers and includes both prefix and lastfix for each question. Accordingly, for validation on AIME and AMC questions, we adapt the prompt format to match the training pattern. For other benchmarks with non-integer answers, question prompts remain unmodified."
        },
        {
            "title": "C Additional Experiment Results",
            "content": "Figure 11: Training and validation accuracy curves for 32B experiments."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "Stanford University",
        "Tsinghua University"
    ]
}