{
    "paper_title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images",
    "authors": [
        "Boyang Deng",
        "Songyou Peng",
        "Kyle Genova",
        "Gordon Wetzstein",
        "Noah Snavely",
        "Leonidas Guibas",
        "Thomas Funkhouser"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes (\"trends\") across a city over a certain period. Unlike previous visual analyses, our analysis answers open-ended queries (e.g., \"what are the frequent types of changes in the city?\") without any predetermined target subjects or training labels. These properties cast prior learning-based or unsupervised visual analysis tools unsuitable. We identify MLLMs as a novel tool for their open-ended semantic understanding capabilities. Yet, our datasets are four orders of magnitude too large for an MLLM to ingest as context. So we introduce a bottom-up procedure that decomposes the massive visual analysis problem into more tractable sub-problems. We carefully design MLLM-based solutions to each sub-problem. During experiments and ablation studies with our system, we find it significantly outperforms baselines and is able to discover interesting trends from images captured in large cities (e.g., \"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more results and interactive demos at https://boyangdeng.com/visual-chronicles."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 7 2 7 8 0 . 4 0 5 2 : r Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images Boyang Deng1 Songyou Peng2, Kyle Genova2, Gordon Wetzstein1 Noah Snavely2 1Stanford University Leonidas Guibas1, Thomas Funkhouser2 2Google DeepMind Figure 1. Visual Trend Discovery from Massive-Scale Collections of Images. We introduce Visual Chronicles, novel approach that uses Multimodal LLMs (MLLMs) to make discoveries from massive collections of images over time. We analyze millions of images collected over more than 10 years by Google Street View. As seen above, our method discovers both expected and surprising visual trends in San Franciscoe.g. outdoor dining areas were added to many storefronts, and miles of overpass was painted blue, both with visual evidence."
        },
        {
            "title": "Abstract",
            "content": "We present system using Multimodal LLMs (MLLMs) to analyze large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes (trends) across city over certain period. Unlike previous visual analyses, our analysis answers open-ended queries (e.g., what are the frequent types of changes in the city?) without any predetermined target subjects or training labels. These properties cast prior learning-based or unsupervised visual analysis tools unsuitable. We identify MLLMs as novel tool for their open-ended semantic understanding capabilities. Yet, our datasets are four orders of magnitude too large for an MLLM to injest as context. So we introduce bottom-up procedure that decomposes the massive visual analysis problem into more tractable sub-problems. We carefully design MLLMbased solutions to each sub-problem. During experiments and ablation studies with our system, we find it significantly outperforms baselines and is able to discover interesting trends from images captured in large cities (e.g., addition of outdoor dining,, overpass was painted blue, etc.). See more results and interactive demos at our project page. 1. Introduction Searching for semantic patterns and correlations in image databases is long-standing problem in computer vision with applications in urban planning, visual analytics, and other fields. Classical research on this problem includes tracking changes in images within specific domains over time (e.g., Century of Portraits [15]) and finding salient patterns of objects in images (e.g., estimating demographics from vehicle sightings [14]). However, these methods have only been demonstrated for predetermined specific target subjects (e.g., faces [15] or cars [14]); and often require labeled training data (e.g., labeled car attributes [14]). In this paper, we study how to make open-ended discoveries about frequently occurring changes (trends) across * Equal contributions. Ordered decided by random number generator. 1 city within given time period, using visual analysis of massive collection of images without required labeled training data. We provide study using Google Street View as data source [3]. Street View provides tens of millions of posed and timestamped images covering large cities with repeated captures spanning decade. Our goal is to answer open-vocabulary queries about how city has changed over time. In contrast to prior work, our queries are open-ended, and not limited to predetermined list of subject categories. Additionally, the data doesnt come with labeled answers to our queries, and manual labeling at scale is infeasible because the nature of the queries are unknown priori. The new properties of our analysis make it difficult problem. Any pretrained subject-specific recognition model, e.g. car detector, is insufficient for open-ended queries. Meanwhile, the lack of labels prevents us from training tailored learning-based model as in prior work [14]. Training-free image analysis techniques, from image features [11] to unsupervised remote sensing [40], have also been used in prior work. However, our goal is to find semantically interesting insights. This demands holistic semantic understanding of images, e.g., understanding that shop is closed from visual evidence, which these prior techniques fail to offer. Therefore, we develop new, scalable, training-free solution to open-ended visual analysis, leveraging the considerable scene understanding capabilities of Multimodal Large Language Models (MLLMs). Our solution is motivated by key observationreadymade MLLMs are very effective at open-ended semantic analyses of small collection of images. They can detect semantically salient temporal changes with high accuracy in 20-40 images. However, straightforward application of MLLMs does not solve our problem due to the massive scale of the data. For instance, simply asking an MLLM to name the trends without using images would fail, because we empirically find they mainly produce abstract answers such as economical growth. Alternatively, we could feed all images as context to an MLLM, and ask it to find the trends. However, this does not work due to the inability of MLLMs to handle massive context. Thus, we design novel bottom-up hierarchical system to decompose the problem into smaller subtasks: detecting and summarizing local visual changes followed by finding global trends among these local changes. For each subtask, we propose an effective MLLM-based solution. To the best of our knowledge, our system marks the first use of MLLMs for the analysis of database of more than million images. Experiments with our proposed system reveal several interesting insights. According to results of tests with manually labeled data produced for this paper, we find that our system discovers visual trends more accurately than prior solutions or trivial use of MLLMs. In experiments with 20 million Street View images from each of two major cities (NYC and SF), it finds several interesting trends that would be difficult to discover without detailed visual analyses (see Fig. 1). Overall, our research contributions are as follows: We study novel open-ended visual analysis problem on massive-scale dataset without labels. We identify MLLMs as the critical tool for such unprecedented analyses, and design novel system for massivescale analyses using MLLMs, bypassing the context and efficiency limitations. We present experiments using our system in two major cities yielding unexpected and informative findings. These contributions together signal new paradigm of computer vision studiesadvancing from previous task-specific supervised training or supervised fine-tuning, to carefully designed use of MLLMs in system like ours, for complicated large-scale tasks without labels. 2. Related Work 2.1. Large-Scale Image Analysis General Analysis. Analyzing large-scale visual datasuch as datasets composed of millions of imagescan unveil important characteristics of the data that cannot be observed from just few examples. Prior work has applied such analytical study to various visual data collections, leading to interesting discoveries, including distinctive visual styles [11, 21, 47], recurring visual patterns in art [17, 46], dataset biases and irregularities [4, 24, 51], and socioeconomic insights such as economic outcomes [19], urban neighborhood perceptions [12], and demographic compositions [14]. Among these works, imagery linked to GIS [3] has been popular data source [11, 12, 14, 19, 38], closely related to the data in our study. While these methods have made significant contributions, they largely rely on imagedriven analysis, e.g., clustering of visual elements [11, 38], interconnecting images through shared visual structure [16], learning visual prototypes [53], average images [15, 62], color diagrams [51], image recognition models [12, 14, 19], or analysis-by-synthesis approaches with image generation models [6, 9, 10, 47]. These methods primarily analyze visual information without considering temporal change. Temporal Analysis. Building upon the foundation of largescale image analysis, other methods examine visual data over time to uncover changes, trends, and evolving patterns [15, 18, 26, 3638, 41, 42, 56, 62]. For example, Martin-Brualla et al. [36, 37] mined time-lapse videos from internet photos to visualize landmark transformations, revealing patterns of construction, decay, and seasonal variations. Naik et al. [42] predicted urban change by analyzing sequential street view imagery, identifying factors that influence physical transformations in cities. Ginosar et al. [15] analyzed century of American high school yearbook photos to study the evolution of facial expressions and fashion trends, demonstrating how temporal analysis can provide sociolog2 isfactory in our task. One concurrent work, TeDeSC [35], applies VisDiffs framework to semantic clustering tasks in relatively small datasets, organizing images based on text descriptions to find groups. Another recent work [61] introduces general analytical framework based on VLMs, utilizing creative language predicates as interpretable hypotheses. This approach, however, does not directly apply to our task due to the intractably large hypothesis space and the inability of LLMs to effectively predict them. Additionally, completing their analysis requires labels for regression, equivalent to annotating is an image in trend?, which is extremely challenging in our context. 3. Massive-Scale Image Analysis with MLLMs 3.1. Task: Open-Ended Urban Imagery Analysis Our work uses visual analysis to make open-ended discoveries from massive collections of images without labels. We use two image collections (New York City and San Francisco) provided by Google Street View with permission, each with 20M posed and timestamped images captured at street level from 2011 to 2023. Our objective is to identify and describe trends of visual changes over time in these image collections, with interpretable text descriptions accompanied by visually grounded evidence. By visual change, we refer to semantic change event that occurred at specific imaged location between two discrete times (corresponding to times when that location was imaged). This query is open-ended because the event can be of any nature and happen to any kind of subject. visual trend is group of similar changes that have occurred at least times across different locations within certain period, where is hyper-parameter determining the minimum frequency for trend to be considered significant. Moreover, we desire the most detailed and specific insights we can find that still occur at least times, to gain deeper understanding of subtle urban transformations. 3.2. Using MLLMs for Massive Image Analysis We aim to leverage the powerful analytical capabilities of MLLMs at large scales. However, the naıve approach of feeding all the data to MLLMs does not work; it is not possible to feed tens of millions of images into an MLLM due to context size limitations (e.g., Gemini-1.5 Pro [45]s limit is 2 million tokens, which corresponds to at most 8k images). Instead, we decompose massive dataset queries into set of local queries that can be answered independently by an MLLM. Then, we combine the answers to those local queries into an answer about the whole dataset. This bottomup approach is appropriate because there are natural ways to decompose an image dataset into small sets of images related to individual visual changes. During early experiments, we discovered that MLLMs are very effective (see Sec. 4.2) at Figure 2. Using MLLMs for Visual Change Detection. Given set of images captured from nearby views at different timestamps (top row), we use an MLLM as visual analyst to detect changes. ical insights. These methods either require specific, predetermined target categories [15] or labelled training data [42], whereas our method leverages MLLMs for open-ended analyses without the need for labeled data. Our work can capture temporal and contextual changes more effectively, defining new direction for future image analysis. Note that separate line of research focuses on analyzing video data, targeting much smaller time scales and relying on the continuity between frames [4, 23, 27, 28, 33, 52, 58]. In contrast, we focus on temporal analysis of non-continuous temporal image collections. 2.2. Vision-Language Models in Visual Analysis The evolution of VLMs has significantly advanced visual analysis. Early models, such as image captioning systems [22, 54], were limited to generating descriptions for individual images, restricting analyses to small-scale tasks like captioning differences between pairs of images [5, 25, 43, 57]. Models like CLIP [44], ALIGN [20], and their variants [8, 59, 63] have also been used in various visual analysis tasks. However, as VLM encoders without generative capabilities, they fall short for tasks requiring detailed image descriptions and reasoning. The advent of large-scale VLMs, from Flamingo [2], PaLI [7], and BLIP-2 [31], to powerful new models like Gemini [49, 50] and ChatGPT [1], affords new opportunities for visual analysis. These models integrate the expansive understanding of large language models with visual inputs, enabling sophisticated visual reasoning and generation. While LLMs have been employed in large-scale text analyses [32, 48, 60], large-scale visual analyses using large VLMs remain scarce. recent method, VisDiff [13], uses MLLMs to find differences between datasets of few thousand images, but does not scale to our problem due to dataset size and differing objectives. Also, their verification step is largely based on CLIP embeddings that prove to be unsat3 Algorithm 1 Using MLLMs for Trend Discovery Require: Trend T, set of changes C, parameters k, Ensure: Is real trend or not? 1: Encode and each ci using TextEmb 2: Compute distances di between and each ci 3: Select nearest ci based on di as set Ck 4: Use MLLM to determine if each cj Ck belongs to 5: Let Npos be the number of positive responses 6: if Npos then 7: 8: else 9: return Trend is positive return Trend is negative describing subtle changes within temporal image sequence captured from nearly the same perspective (e.g., those shown in Fig. 2). MLLMs not only detect small changes within such sequences, but also effectively ignore many common trivial changes (variations in time of day, season, etc.) and focus instead more on consistent semantic content that might be of interest to person. They also identify where in the sequence the visual changes occur in addition to producing detailed text descriptions of them (e.g., the corner store gets new for sale sign on the window between the third and fourth image). With this key insight regarding the capabilities of MLLMs, we design two-step analysis system consisting of local change detection followed by trend discovery. In both steps, we harness the analytical power of MLLMs, and design careful solutions to effectively process and analyze the data. The following sections describe each step in detail. 3.3. Using MLLMs for Visual Change Detection In the first step of our system, we detect local visual changes by analyzing small sets of images captured from approximately the same viewpoint at different times (Fig. 2 top row, see data processing details in Supplementary). Since the images are time-stamped, we can sort them into chronological order at each location. We feed every sorted image sequence into an MLLM and craft query that, in essence, asks: 1) what changes have occurred, and 2) from which images these changes were detected. The output for the first question provides text description for each detected visual change, while the second grounds these detections to specific visual evidence and timestamps. Please refer to the supplement for more details about image sequence preparation, MLLM prompting, and more example model outputs. 3.4. Using MLLMs for Visual Trend Discovery In the second step, we aim to analyze the set of local visual changes produced by local change detection to discover frequent visual trends occurring throughout city. Specifically, we look for trends where similar changes occur at least times at different locations in the same time period. At first glance, one may consider simple solution of presenting all local change descriptions to an MLLM and asking for trends. However, this is impractical for massive city-scale datasets. For instance, from collection of 20 million images for one city, the first step of the system produces over 3 million local changes. Analyzing them all at once would exceed the context window size of current MLLMs. To address this issue, we designed two-step process that 1) produces visual trend proposals, and then 2) verifies which proposed trends are supported by at least visual changes. The first proposal step is relatively simple. We embed the text description for every visual change into vector space and use Canopy Clustering [39] with tolerant loose threshold to find potential cluster proposals. The second verification step is more challenging. An easy and efficient approach would be to assign each visual change to trend proposal based on its proximity in an embedding space. However, we find that this approach performs poorly because distances in embedding spaces are not distinctive enough to capture the subtle similarities and differences between visual changes required for our application. Image embeddings may encode both change-relevant and changeirrelevant content, and text embeddings may fail to capture subtle wording differences (e.g., adding not) or different conceptual granularities (e.g., storefront changed its business and pizza shop changed into cafe are difficult to cluster because they represent different levels of specificity). Moreover, there is no universal embedding distance threshold to classify whether change belongs to trend, making it difficult to achieve consistent accuracy. An alternative, albeit impractical, approach would be to feed every detected local change and every proposed trend to an MLLM and ask it whether the change fits within the trend (yes or no). This approach is compelling because we find that MLLMs provide remarkably high accuracy for this small task. However, the downside of this approach is efficiency. Verifying 200 trend proposals exhaustively against 3 million changes using MLLMs would take more than one year. To address this issue, we propose hybrid algorithm which combines the efficiency of text embeddings with the precision of MLLMs, as shown in Alg. 1. Since we only need to find at least changes that belong to trend to confirm it, we first use text embeddings to sort all changes based on their similarity to the trend proposal. Next, we use the MLLM to verify only the top > nearest neighbors. If positive matches are found among these k, the trend is confirmed. We set = 1500 for = 500 in our experiments, reducing the MLLM inference cost by 2000 and making the process computationally feasible. 4. Evaluations In this section, we evaluate Visual Chronicles against alternatives as well as design choices made in Visual Chronicles. We use Gemini-1.5 Pro [45] as the MLLM throughout the paper, for its impressive long-context multimodal analytical capability. We collect small set of data for evaluation. HoG [11] C-Hist [51] R-Sensing [40] CLIP [44] NV-Emb [29] Gemini [45] AP 16.44% 16.76% 18.51% 26.52% 23.75% 76.56% Table 1. Evaluating Visual Change Detection. Gemini detects visual changes with significantly higher average precision (AP) than baselines based on image features (HoG, C-Hist), remote sensing (R-Sensing), and semantic embeddings (CLIP, NV-Emb). 4.1. Comparisons to Trivial Use of MLLMs 4.2. Local Change Detection Evaluation We first investigate how well our system works in comparison to naıve solutions using MLLMs trivially. Can an MLLM predict similar visual trends without looking at any images? In the first experiment, we investigate how the visual trends produced by our proposed method compare to those produced directly by an MLLM when prompted only with text (i.e., without providing any images as context). We acknowledge that this baseline is not apples-to-apples since our method can access lots of extra data and use significantly more compute. However, it addresses an important question: Are MLLMs already knowledgeable enough to list visual trends on their own? To perform this comparison, we asked Gemini-1.5 Pro and GPT-4o to predict visual trends over the last decade using various prompt strategies. We found that MLLMs answers are usually abstract, e.g. Increased focus on sustainability, and general without intriguing details, e.g. store closures. Please see complete answers in Supplementary. In contrast, our method identifies specific and subtle trends, e.g. addition of solar panels in Fig. 4, more nuanced, e.g. closures of grocery stories in Fig. 5, and sometimes unexpected trends, e.g. an overpass painted blue in Fig. 1, all supported by visual evidence. These visual trends provide more detailed and verifiable understanding of changes in the city than the broad abstract ones provided by the MLLM on its own. Can an MLLM process all the images in massive collection in one query? In the second experiment, we investigate the limits of using MLLMs to predict visual trends with single query using set of images as context. For this test, ideally, wed provide the entire massive image collection of millions of images as context to the MLLM, and then ask what are the visual trends? However, this is not possible, since MLLMs have limits on the number of tokens that can be provided as context (8K images for the MLLM with largest limit [45]). So, instead we performed tests with randomly sampled subsets of 8K images out of 20M in NYC as context, and take the union of answers from all subsets as the final result. The results however are not satisfactorydue to the severe under-sampling of images in each batch, only the extremely frequent trends can be found, e.g. the addition and removal of scaffoldings in NYC which our system also identifies. Additionally, this solution only finds less than 10 trends throughout whole city even after union of multiple sub-samples. See complete answers in Supplementary. Next, we quantitatively evaluate the design choices made in Visual Chronicles, starting from local change detection. To understand how good Visual Chronicles is at local change detection, we collect dataset of 200 locations with 3036 images in total. For each location, we manually label the changes, including between which 2 consecutive images each change happens and the description of changes. We compare our method with prior unsupervised image analysis tools, including image features such as HoG features [11] and Color Histograms (C-Hist) [51], and semantic features such as CLIP [44] and NV-Emb [29]. These methods first featurize every image in the image sequence for each location and measures the feature distance between consecutive images as an indicator for potential change. For NV-Emb, we use Gemini [45] to caption each image and embed the caption text. Additionally, we compare ours with an unsupervised remote sensing method (R-Sensing) [40]. Note that other learning-based approaches [14] requires labelled training data, hence are inapplicable. To have fair comparison between MLLMs and baselines that only predict the change likelihood between two images, we use classification metric, Average Precision (AP). For any consecutive image pairs in our test set, our human labels give us binary ground truth on whether there is change. We then compute the AP on this binary classification task for all methods. Note that if there are multiple changes between pair of images, we only count the pair once. Additionally, to further measure MLLMs performance, we also evaluate the precision of its prediction by manually labelling if each detected change is true, that is if the text is actually describing change and the change align with the corresponding evidence images, as well as its recall by manually labelling if each human labelled change is covered in MLLMs answers. The results in Tab. 1 show that the MLLM (Gemini) is significantly better than the two methods based on embedding distances i.e., by more than 50 AP percentage points. Overall, we find that the MLLM is remarkably good at this task its precision is 81.34% and its recall is 89.87%. 4.3. Trend Discovery Evaluation Trend discovery is not trivial for MLLMs. We first evaluate trivial solution of feeding all change descriptions to MLLMs and asking for trends, to the extent this is possible. Current context limits can support at most 50K changes at"
        },
        {
            "title": "Random",
            "content": "CLIP NV-Emb"
        },
        {
            "title": "Gemini",
            "content": ""
        },
        {
            "title": "MLLM",
            "content": "47.70% 54.78% 73.13% 86.63% Table 2. Evaluating Trend Discovery. The MLLM discovers trends with significantly higher average precision (AP) than baselines based on embedding distances, but is not scalable due to expensive inference."
        },
        {
            "title": "Method",
            "content": "AllTrue NV-Emb RandMLLM Hybrid (Ours) Acc.@50 Acc.@100 Acc.@200 72.7% 54.1% 28.9% 77.9% 69.6% 81.8% 31.8% 49.9% 74.9% 93.9% 94.6% 98.3% Table 3. Evaluating Change-Trend Classfiers. AllTrue is predicting positive for all. Our hybrid algorithm is significantly more accurate at verifying change-trend pairs than scalable alternatives. once, out of 3M changes per city (less than 2%). We therefore subsample sets of 50K changes and take the union over multiple sets, but the answers are not satisfactory. Similar to feeding subsampled image collections to MLLMs, only very few trends can be identified and they are dominated by extremely frequent trends, e.g. scaffolding and construction. See the supplementary for more details. MLLMs are good at verifying trends, but are not scalable. Next, we test MLLM trend verification capabilities. We sample collection of 50 cluster proposals each with 40 change candidates to verify. This makes 2000 trend-change pairs for the binary classification task does this change belong to this trend?. We use human labellers to manually label these pairs. To ensure more balanced label distribution, we over-label more randomly sampled pairs and subsample aforementioned 2000 pairs to balance positives and negatives. We compare MLLMs with SOTA image embedding (CLIP) and text embedding (NV-Emb), as well as trivial solution that produces uniformly random binary predictions. As in the previous test, we use average precision as the metric. Our results in Tab. 2 show that the MLLM (Gemini) significantly outperforms the methods based on embeddings, by margin more than 13%. However, MLLMs are expensive in inference time. Even with 64 MLLMs running in parallel, it would take 380 days to verify 200 trend proposals against 3M changes (0.6B queries at 3.5s/query/MLLM), while embedding approaches can finish in less than day with 64 A100 GPUs. Our hybrid solution is both accurate and scalable. At last, we verify performance of our hybrid solution. We know that our solution reduces the required MLLM inference cost by 2000, making it sufficiently fast to verify trends, e.g. 4.6 hours to verify 200 trends with the same 64 parallel MLLMs. So, the remaining question is whether it can discover trends accurately. To test this, we sample 1000 trend proposals, each with 5000 changes to verify against. We use an MLLM to exhaustively verify every trend-change pair, 5M in total, and label each trend proposal as true or false, based on whether the MLLM confirmed at least changes for it (shown as @N in Tab. 3). We then test our hybrid method against with this labeled set of trends to measure how accurately it classifies them as true or false, and report the binary classification accuracy. We compare our hybrid solution with: (1) trivial approach that always predicts true, (2) pure text embedding approach (NV-Emb) where we use threshold to binarize trend-change similarity and use the binarised label to decide if trend proposal is true. We report the best accuracy among 1000 grid-searched thresholds, and (3) Randomly selecting changes to verify (RandMLLM). Unlike our text embedding sorted verification, this solution randomly picks the same amount of changes as our solution to verify for each trend proposal. In Tab. 3, we find that, compared to other scalable alternatives, our hybrid solution is significantly more accurate. It does not degrade much the accuracy from exhaustive MLLM inference while being much more efficient. For the hyper-parameter in our solution, we use = 3N and ablate the choice of in Supplementary. 5. Applications In this section, we apply our system to study visual trends spanning decade in two major US cities, San Francisco (SF) and New York City (NYC). For each city, we collect 20M Street View images, captured from 2011 to 2023. For NYC, the images are sampled from 8.8M different locations, with an average of 23 images per location. For SF, we sample from 9.5M locations, with an average of 21 images per location. Local change detection yields 2.9M change entries in NYC and 3.6M in SF. For each city, we verify the most detailed 500 trend proposals. 5.1. What are the trends of the decade? We first show the results of an open-ended attempt to discover what changes were frequently observed over decade? in each city. Unlike most prior work, these results are produced without any preconditioning. That is, no external input was provided to guide the system to find specific types of changes, to avoid finding commonly occurring trivial changes (time of day, seasons, etc.), and to avoid semantically irrelevant changes (e.g., presence of distractors). The notion of what types of trends are worth reporting is determined by the MLLM during our local change detection and trend discovery processes. Fig. 3 and Fig. 4 show sampling of the visual trends found in NYC and SF, respectively (more appear in the Supplementary). For each trend, we can provide hundreds of image pairs as supporting visual evidence. 6 Figure 3. Trends of the Decade in New York City. We discover interesting trends of visual change in NYC, including 745 new security cameras, 509 new fences around parking lots, and 519 red ADA warning pads on sidewalks. For each example, the top row shows images before change, bottom row shows after, and markers appear on the map where the changes occurred. Figure 4. Trends of the Decade in San Francisco. In San Francisco, we discover 1504 new solar panels (mostly in residential areas), 751 bus lanes conversions (mostly along few major roads), and 1799 new bike racks (mostly near downtown). Yet, we still must be careful when drawing conclusions. For example, in Fig. 4, it appears many new solar panels were added near the raised freeway. In reality, it may be that roof-mounted solar panels are more likely to be visible in an image from raised freeway than from ground level. We must be careful to account for spatial and temporal image sampling biases when reviewing the results. Despite this, we believe our system can be useful for discovering what changes happened, and where they happened, in an evidencedriven way that previously would not have been possible without an enormous amount of human effort. 5.2. What happened in SF since 2020? Our system also supports searching for visual trends regarding specific sub-period of interest. To demonstrate, we showcase time-conditioned search for the range 2020-0101 to 2022-12-31. Here, we filter to changes that happened strictly within this period, i.e., both images (before and after) are captured within this period, and we rerun only the trend discovery step. To encourage the system to find trends that might be unique to post-2020, we use simple heuristic to sort trend proposals by their potential frequency differences between post-2020 years (20202022) and pre-2020 years (20172019). We conduct this study on data from SF. Outdoor dining. One notable trend is The storefront added tables and chairs outside, indicating the addition of outdoor dining, as shown in Fig. 1. We observe this occurrence 1,482 times in 2020-2022 and 668 times in 20172019. An Overpass Painted Blue. To our surprise, Fig. 1 shows the visual trend The support of the overpass was painted blue., observed 481 times since 2020 (5 times before 2020). An internet search reveals the likely explanation San Francisco settled on Coronado Blue for its $31 million paint 7 Figure 5. NYC Retail Visual Trends. By conditioning on retail store has changed, we are able to discover visual trends related to the opening and closing of types of retail stores in New York. job of the Central Freeway (SF Standard reporting). 5.3. Trends related to retail stores in NYC Visual Chronicles also supports searches for trends relevant to specific subject. We provide an example study of retail stores in NYC. Similar to Sec. 5.2, we can re-use the local changes detected in Sec. 5.1, and this time filter them by semantic relevance. To compute semantic relevance to retail stores, we first embed the sentence, retail store has changed. Then, similar to our hybrid trend verification solution, we pick the nearest 500K changes in NYC and use an MLLM to check that they are indeed changes relevant to retail stores. After filtering, we run the trend discovery stage, except we verify 150 trends uniformly across trends of varying word count (rather than the lengthiest trends), hoping to find variety of trends about retail stores. Bakeries and Juice Shops. Seen in Fig. 5, one type of trend that arises within this search is the opening of various shops, including There is bakery newly opened at the storefront and juice shop opened at the storefront. Groceries and Bank Branches. Opposite to the opening of certain stores, the closing of certain stores also happened in 20112023  (Fig. 5)  . Among our trends, we find that many grocery stores and bank branches closed in NYC. 5.4. Non-Temporal Query: Unusual Things Our system also supports non-temporal open-ended queries. As one example, we investigated searching for unusual things that actually happen lot in city. To do so, we simply changed the first step of our system to send just single 8 Figure 6. Unusual Things on the Streets of NYC. Beyond temporal queries, our system can also be applied to analyzing individual images, for non-temporal open-ended queries. Here we show our findings from the individual image query What are the unusual things in the image?. We find there are many unusual large, abstract sculptures in NYC. image to the MLLM and ask it what are the unusual things in the image?. The MLLM then produces text describing the local unusual things, which will be processed by the second trend discovery step without change. Applying this process to large set of images in NYC yields cluster of images showing large, abstract sculpture  (Fig. 6)  , among other interesting clusters. 6. Conclusions, Limitations, and Future Work This paper explores how to use MLLMs to analyze massive image collection without labels to find trends with openended semantic queries. In addition to introducing this novel task, we carefully design novel system effectively leveraging the strong capabilities of MLLMs for large scale analyses. Experiments show the proposed system is able to discover fascinating trends automatically. This paper is the first to analyze image datasets with millions of images using MLLMs, and so it should be considered an initial proof-of-concept, rather than an exhaustive analysis of all possible design choices. Accordingly, there are many limitations. One key avenue for future work is to integrate MLLM analysis over large-scale, distributed imagery into robust statistical framework that takes into account biases, e.g., due to uneven sampling of imagery and inaccuracies or bias in MLLM results our work is scalable way to generate hypotheses from vast amounts of visual data, but the methods we devise could also be developed into tools that allow for more rigorous conclusions, grounded in evidence. Another fruitful direction would be to apply our methods to other kinds of large visual datasets, including video (e.g., archival news footage), and to other kinds of queries, like find interesting styles. In any case, we are likely near the beginning of long line of research related to using MLLMs for analyzing massive image collections, and this paper takes small step in that direction. Acknowledgements. Thanks to Jiahui Lei, Anh Thai, Jiapeng Tang, Linyi Jin, Luming Tang, Rundi Wu, Ian Huang, Colton Stearns, Francis Engelman, Manu Gopakumar, Suyeon Choi, Haley So, Richard Tucker, Abhijit Kundu, Jonathan Barron, Glenn Entis, and David for their comments and constructive discusSalesin, sions; to Abhijit Kundu, William Freeman, and John Quintero for helping review our draft; G.W. was in part supported by Google, Samsung, and Stanford HAI. B.D. was in part supported by Qualcomm Innovation Fellowship."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. NeurIPS, 2022. 3 [3] Dragomir Anguelov, Carole Dulong, Daniel Filip, Christian Frueh, Stephane Lafon, Richard Lyon, Abhijit Ogale, Luc Vincent, and Josh Weaver. Google street view: Capturing the world at street level. Computer, 2010. 2 [4] Oren Boiman and Michal Irani. Detecting irregularities in images and in video. IJCV, 2007. 2, 3 [5] Shizhen Chang and Pedram Ghamisi. Changes to captions: An attentive network for remote sensing change captioning. IEEE TIP, 2023. 3 [6] Eric Ming Chen, Jin Sun, Apoorv Khandelwal, Dani Lischinski, Noah Snavely, and Hadar Averbuch-Elor. Whats in decade? transforming faces through time. In Computer Graphics Forum, 2023. [7] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: jointlyscaled multilingual language-image model. In ICLR, 2023. 3 [8] Gabriela Csurka, Tyler Hayes, Diane Larlus, and Riccardo Volpi. What could go wrong? discovering and describing failure modes in computer vision. arXiv preprint arXiv:2408.04471, 2024. 3 [9] Theophile Dalens, Mathieu Aubry, and Josef Sivic. Bilinear image translation for temporal analysis of photo collections. IEEE TPAMI, 2019. 2 [10] Boyang Deng, Richard Tucker, Zhengqi Li, Leonidas Guibas, Noah Snavely, and Gordon Wetzstein. Streetscapes: Largescale consistent street view generation using autoregressive In SIGGRAPH 2024 Conference Papers, video diffusion. 2024. 2 [11] Carl Doersch, Saurabh Singh, Abhinav Gupta, Josef Sivic, and Alexei Efros. What makes paris look like paris? Communications of the ACM, 2015. 2, 5 [12] Abhimanyu Dubey, Nikhil Naik, Devi Parikh, Ramesh Raskar, and Cesar Hidalgo. Deep learning the city: Quantifying urban perception at global scale. In ECCV, 2016. 2 [13] Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph Gonzalez, and Serena Yeung-Levy. Describing differences in image sets with natural language. In CVPR, 2024. [14] Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Lieberman Aiden, and Li Fei-Fei. Using deep learning and google street view to estimate the demographic makeup of neighborhoods across the united states. Proceedings of the National Academy of Sciences, 2017. 1, 2, 5 [15] Shiry Ginosar, Kate Rakelly, Sarah Sachs, Brian Yin, and Alexei Efros. century of portraits: visual historical record of american high school yearbooks. In CVPR Workshops, 2015. 1, 2, 3 [16] Kyle Heath, Natasha Gelfand, Maks Ovsjanikov, Mridul Aanjaneya, and Leonidas Guibas. Image webs: Computing and exploiting connectivity in image collections. In CVPR, 2010. 2 [17] Phillip Isola, Devi Parikh, Antonio Torralba, and Aude Oliva. Understanding the intrinsic memorability of images. NeurIPS, 2011. 2 [18] Phillip Isola, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. What makes an image memorable? In CVPR, 2011. 2 [19] Neal Jean, Marshall Burke, Michael Xie, Matthew Davis, David Lobell, and Stefano Ermon. Combining satellite imagery and machine learning to predict poverty. Science, 2016. 2 [20] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021. 3 [21] Sergey Karayev, Matthew Trentacoste, Helen Han, Aseem Agarwala, Trevor Darrell, Aaron Hertzmann, and Holger Winnemoeller. Recognizing image style. arXiv preprint arXiv:1311.3715, 2013. 2 [22] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015. [23] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In CVPR, 2014. 3 [24] Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz, Alexei Efros, and Antonio Torralba. Undoing the damage of dataset bias. In ECCV, 2012. 2 [25] Hoeseong Kim, Jongseok Kim, Hyungseok Lee, Hyunsung Park, and Gunhee Kim. Agnostic change captioning with cycle consistency. In ICCV, 2021. 3 [26] Pierre-Yves Laffont, Zhile Ren, Xiaofeng Tao, Chao Qian, and James Hays. Transient attributes for high-level understanding and editing of outdoor scenes. ACM TOG, 2014. 2 9 [27] Ivan Laptev. On space-time interest points. IJCV, 2005. 3 [28] Ivan Laptev and Tony Lindeberg. Interest point detection and scale selection in space-time. In International Conference on Scale-Space Theories in Computer Vision, 2003. [29] Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvembed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024. 5 [30] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrievalaugmented generation for knowledge-intensive nlp tasks. NeurIPS, 2020. 6 [31] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 3 [32] Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Scott Smith, Yian Yin, et al. Can large language models provide useful feedback on research papers? largescale empirical analysis. NEJM AI, 2024. 3 [33] Tony Lindeberg. On automatic selection of temporal scales In International workshop on in time-causal scale-space. algebraic frames for the perception-action cycle, 1997. 3 [34] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. [35] Mingxuan Liu, Zhun Zhong, Jun Li, Gianni Franchi, Subhankar Roy, and Elisa Ricci. Organizing unstructured imarXiv preprint age collections using natural language. arXiv:2410.05217, 2024. 3 [36] Ricardo Martin-Brualla, David Gallup, and Steven Seitz. 3d time-lapse reconstruction from internet photos. In ICCV, 2015. 2 [37] Ricardo Martin-Brualla, David Gallup, and Steven Seitz. Time-lapse mining from internet photos. ACM TOG, 2015. 2 [38] Kevin Matzen, Kavita Bala, and Noah Snavely. StreetStyle: Exploring world-wide clothing styles from millions of photos. arXiv preprint arXiv:1706.01869, 2017. 2 [39] Andrew McCallum, Kamal Nigam, and Lyle Ungar. Efficient clustering of high-dimensional data sets with application to reference matching. In ACM SIGKDD international conference on Knowledge discovery and data mining, 2000. 4 [40] Farid Melgani, Gabriele Moser, and Sebastiano Serpico. Unsupervised change-detection methods for remote-sensing images. Optical Engineering, 41(12):32883297, 2002. 2, 5 [41] Nikhil Naik, Jade Philipoom, Ramesh Raskar, and Cesar Hidalgo. Streetscore-predicting the perceived safety of one million streetscapes. In CVPR workshops, 2014. 2 [42] Nikhil Naik, Scott Duke Kominers, Ramesh Raskar, Edward Glaeser, and Cesar Hidalgo. Computer vision uncovers predictors of physical urban change. Proceedings of the National Academy of Sciences, 2017. 2, [43] Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. In ICCV, 2019. 3 [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 3, 5 [45] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 3, 5 [46] Xi Shen, Alexei Efros, and Mathieu Aubry. Discovering visual patterns in art collections with spatially-consistent feature learning. In CVPR, 2019. 2 [47] Ioannis Siglidis, Aleksander Holynski, A. Alexei Efros, Mathieu Aubry, and Shiry Ginosar. Diffusion models as data mining tools. In ECCV, 2024. [48] Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, and Jianfeng Gao. Rethinking interpretability arXiv preprint in the era of large language models. arXiv:2402.01761, 2024. 3 [49] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 3 [50] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 3 [51] Antonio Torralba and Alexei Efros. Unbiased look at dataset bias. In CVPR, 2011. 2, 5 [52] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. closer look at spatiotemporal convolutions for action recognition. In CVPR, 2018. 3 [53] Nanne Van Noord. Prototype-based dataset comparison. In ICCV, 2023. 2 [54] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: neural image caption generator. In CVPR, 2015. 3 [55] Wikipedia contributors. the free encyclopedia. Hudson yards (development) https : / / en . Wikipedia, wikipedia.org/w/index.php?title=Hudson_ Yards_(development)&oldid=1247695752, 2024. [Online; accessed 21-November-2024]. 1 [56] Michael Xie, Neal Jean, Marshall Burke, David Lobell, and Stefano Ermon. Transfer learning from deep features for remote sensing and poverty mapping. In AAAI, 2016. 2 [57] Linli Yao, Weiying Wang, and Qin Jin. Image difference captioning with pre-training and contrastive learning. In AAAI, 2022. [58] Lihi Zelnik-Manor and Michal Irani. Event-based analysis of video. In CVPR, 2001. 3 [59] Zeliang Zhang, Mingqian Feng, Zhiheng Li, and Chenliang Xu. Discover and mitigate multiple biased subgroups in image classifiers. In CVPR, 2024. 3 10 [60] Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. Goal driven discovery of distributional differences via language descriptions. NeurIPS, 2023. 3 [61] Ruiqi Zhong, Heng Wang, Dan Klein, and Jacob Steinhardt. Explaining datasets in words: Statistical models with natural language parameters. arXiv preprint arXiv:2409.08466, 2024. 3 [62] Jun-Yan Zhu, Yong Jae Lee, and Alexei Efros. Averageexplorer: Interactive exploration and alignment of visual data collections. ACM TOG, 2014. [63] Zhiying Zhu, Weixin Liang, and James Zou. Gsclip: framework for explaining distribution shifts in natural language. arXiv preprint arXiv:2206.15007, 2022. 3 11 Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 8. Many Graffiti Observed, 20202022. In SF since 2020, many new graffiti are observed. quick search on the Internet reveals relevant reports in local news and local policy change. The enforcement has since resumed. 2020. Such changes were spotted 3152 times post-2020 and 1150 times pre-2020. As reported by ABC News, during COVID San Francisco paused enforcement of local regulations requiring building owners to remove graffiti promptly. Yet, this is an example of why we must be careful when drawing conclusions from this data, as at least two hypotheses seem compatible: 1) graffiti tagging became more popular, or 2) graffiti was removed less frequently, increasing its likelihood of being observed in our data. 7.3. Additional Trend Results More Images of Shown Trends. In previous sections, we have shown example trends discovered in Visual Chronicles, each evidenced by pair of images. In Fig. 9, we further provide an additional pair of images illustrated visual evidence for each of these previously shown trends, demonstrating the variation in the visual data. More Trends in Images. In addition, we show in Fig. 10 more trends found using Visual Chronicles along with accompanying visual evidence. Such trends include specific canopied outdoor dining trend in NYC, new zebra crosswalk painted on the street, an added under-bridge wooden plank, an added green bike lane, and the opening of new cafes. More Trends in Text. In previous figures, we have shown all trends with visual evidence. For visualization purpose, the shown text is shortened descriptions of the exact trend found in Visual Chronicles. In Fig. 11 and Tab. 4, we first show list of complete text for all the trends we have shown. Moreover, we show an addition set of found trends also in complete text descriptions. Figure 7. Where are new buildings in NYC? We plot all locations where we observe the trend, lot now has multi-story building constructed on it, in NYC. We find notable clustering in Brooklyn, Long Island City, and Hudson Yards. In this supplementary, we provide more application results including an example of non-temporal query and spatial insight (Sec. 7), more details about our data, system, and evaluations (Sec. 8), more evaluations including ablations and complete results of MLLM baselines on the hyper-parameters of Visual Chronicles (Sec. 9), and potential societal impacts of Visual Chronicles (Sec. 11). 7. Extended Application Results 7.1. Where are new buildings in NYC? Aside from the analysis for specific temporal period of interest (Sec. 5.2) or semantic subject of interest (Sec. 5.3), Visual Chronicles can also reveal insights about the spatial domain, e.g. by visualising the spatial distribution of trend. In particular, in this study, we showcase the spatial distribution of one of the trends of the decade in NYC (Sec. 5.1), namely lot now has multi-story building constructed on it. As shown in Fig. 7, we find that more new buildings are observed in Brooklyn and Long Island City, compared to sparser, scattered points in Manhattan. Additionally, the well-known Hudson Yards development [55] appears as rare spatial cluster in Manhattan. 7.2. Many Graffiti Observed Since 2020 As shown in Fig. 8, our system also identified the trend New graffiti were added to the wall as significant trend since Figure 9. More Images of Shown Trends. Figure 10. More Trends in Images. 8. Extended Details 8.1. Computation Costs. Our change detection step takes < 24 hours for analysing 20M images from 1 city. As in L457, our aggregation step takes 4.6 hours for 200 trends. Both steps use 64 MLLMs running in parallel. The total cost of MLLM inference is < 10K$, well within the budget of commercial city-scale analysis projects. 8.2. Data Preprocessing To produce the sets of local image sequences for the local change detection step of our system, we first randomly sample set of seed locations from the dataset. For each seed location, we then sample view direction thats likely to have buildings in view, based on the map. Finally, for each panorama centered within 1.8 meters of the seed location, we reproject the panorama to the sampled view direction as pinhole image and add the resulting image to the set. For city-scale dataset with 20M images (e.g., NYC), this process results in manageable set of 10 to 40 images in each set at 900K sampled locations. 8.3. Dataset Coverage We collect our data from two major US cities, San Francisco and New York City. In particular, we sample locations from selected region of each city, as shown in Fig. 12. Note that the regions are selected to represent busy, dense regions of the city for our study of trends. They do not cover the whole city by the definition of range of city administration. Yet, within the selected regions we already collect 20M images for our massive scale study. 8.4. Local Change Detection Details Data Preparation. For the first step, local change detection, in Visual Chronicles, we first sample set of locations from 2 Figure 11. More Trends in Text. Fig. 1 Fig. 3 Fig. 4 Fig. Fig. 7 Fig. 8 Fig. 10 Trend Description (1483) The storefront added tables and chairs outside. (481) The support of the overpass was painted blue. (745) Security cameras became visible on the light poles. (509) The parking lot in front of the building now has fence enclosing it. (519) The crosswalk had its marking changed from white to red. (1504) Solar panels were added to the roof of one of the buildings in the background. (751) The street has been reconfigured from standard two-way configuration to configuration with dedicated bus lane. (1799) Bike racks were added in front of building. (318) juice shop opened at the storefront. (512) There is bakery newly opened at the storefront. (1614) The bank branch at the storefront closed. (741) The grocery store on the street closed. (1693) lot now has multi-story building constructed on it. (3152) New graffiti were added to the wall. (706) An outdoor seating area with canopy was added in front of the buildings. (1646) The crosswalk changed from having solid white painted lines to zebra-striped painted lines. (577) Portions of the underside of the overpass structure are covered with wooden planks, where it was primarily metal beams and supports before. (754) green bike lane was added to the street in front of building. (1978) The storefront changed into cafe. Table 4. Full Texts for Trends in Various Figures. the city of interest. Since the street view imagery we use is sampled from capture vehicles, we naturally have all the captured locations scattered on the street instead of inside of buildings. We hence directly sample set of locations from the capture locations. To avoid redundancy caused by sampling nearby points, we implement Non-Maximum Suppression (NMS). Specifically, we first calculate the number of nearby images (as defined in Sec. 3.3) for each location. Then we rank all locations by number of neighbors in deceasing oreder, and apply NMS to select non-redundant locations. After selecting all locations, we collect local neighboring images following the process outlined in Sec. 3.3. Note that we filter out locations that have fewer than 10 local images at the end of this preparation step. Detection Prompting. To detect local changes using MLLMs, we essentially feed sequence of images, in chronological order, to the MLLM and ask it what changes it detects and from which images. We use Gemini-1.5 Pro (Gemini) as the MLLM here. We find that such prompts already work surprisingly well on Gemini. Gemini would describe significant temporal semantic changes it notices, including changes minute in visual scale such as the tiny security camera in Fig. 3. At the same time, it often voluntarily ignores semantically insignificant changes such as illumination changes or camera displacement changes (as our camera poses are not perfectly aligned). Yet, through our empirical study, we find few error patterns of such simple prompting. Among them, the notable ones include: 1) Gemini may hal3 derived trends text will be candidates for trend verification. Verification Prompting. We develop detailed prompt promoting MLLM (in our system, Gemini) to verify if change belongs to trend proposal, shown in Fig. 19. In particular, we leverage the in-context learning ability of MLLMs, by including small set of examples in the prompt. We empirically find that such examples improve the verification quality. 8.6. Evaluation Details For our evaluation in Sec. 4.2 and Sec. 4.3, we sample the test set from our data collection from New York City. For the evaluation of change detection in Tab. 1, we randomly sample 200 locations in New York City and ask human labeller to describe list of changes for each location and from which 2 consecutive images the changes were found, as the ground truth changes. Given such labels, we can derive if theres change between any 2 consecutive images, which is the ground truth we use to calculate the Average Precision in Tab. 1. In our evaluation of trend discovery in Tab. 2, we aim to collect 50 clusters each with 40 candidate changes to verify. However, naıvely randomly sampling such collection would lead to mostly negative labels, that is, the change does not belong to the trend. To better balance the test labels, we over-sample 75 clusters, with selection heuristic of enough nearby neighbours, i.e. more than changes in the pool with text embedding distance smaller than 0.25. Thereafter, to sample balanced change candidates, we over-label change changes until we collect at least 20 positive labels for the cluster, i.e. 50% positive rate. We also apply early-stop, i.e. setting the maximum number of changes to label for each cluster, to avoid excessive labelling for one cluster. After labelling all 75 clusters, we pick 50 clusters with the most balanced test labels as our final test set. For Tab. 3, since NV-Emb is predicting continuous distance values, we grid-search 1000 thresholds to binarize its predictions and pick the highest accuracy to report. 9. Extended Evaluations 9.1. Validity of Evaluation Data Our analysis is conducted on massive-scale datasets. Its impractical to annotate even 1% of the 20M images in the datasets. We instead designed an evaluation set large enough to provide comparisons of different methods with low variance, with conclusions generalizable to larger-scale applications. To verify this, we computed the standard deviation of metrics using 1K random 75%-subsets of the evaluation set, and found them to be 1% in AP for both subtasks. 9.2. Ablation Studies We conducted extensive ablation studies evaluating design choices of our system, including MLLMs for change deFigure 12. Data Coverage. We show the region we sample our locations from in both cities in red. Note that our data collection does not coverage the whole region of administration for both cities. lucinate changes that can not be certainly seen in images. 2) The answers occasionally include changes that are caused by camera wiggles, normally in the periphery of the frame. 3) Sometimes Gemini would mistake color changes caused by illumination change as permanent change, e.g. change from dark color in shadow to very bright color under the sun. 4) Very rarely, Gemini may mistake the foreground object with the background, e.g. mistake the sign and text on the car in front of building as text and sign on the building. To address these, we articulate more extensive list of instructions in the prompt shown in Fig. 17. We include all the error patterns we empirically find in this prompt. Self-Critic. Despite the careful prompting, we still find Gemini making mistakes such as hallucination in some of the answers. Inspired by OpenAI o1, we implement simple Self-Critic strategy by feeding the visual evidence and the detected change back to Gemini and ask it to verify that the change is significant and that it can be found in the visual evidence. We find such simple verification step greatly improves the precision of local change detection, as shown in Sec. 9.4. 8.5. Trend Discovery Details Trend Proposal. In the second step of our system, we derive trend proposals from detected local changes. This step has 2 parts. First, we design prompt that make MLLMs deduct potential proposals from detected change, as shown in Fig. 18. Then, we implement standard Canopy Clustering [39] using text embedding distance and set the tight distance threshold to 0.15 and the loss distance threshold to 0.2. Because our verification needs at least nearest neighbours in cluster, we filter clusters from the Canopy Clustering that has less than elements. After the filtering, each cluster center becomes trend proposal, and all its"
        },
        {
            "title": "Change Detection Trend Discovery",
            "content": "LLAVA [34] Gemini [45] 64.31% 76.56% 81.11% 86.63% Table 5. Ablation on Different MLLMs in AP. The smaller MLLM, LLAVA, still performs decently on both sub-problems, while being slightly worse than more powerful Gemini. Without Self-Critic With Self-Critic"
        },
        {
            "title": "Precision\nRecall",
            "content": "57.83% 91.50% 81.74% 89.87% Table 6. Ablation on Self-Critic in Local Change Detection. The Self-Critic strategy (Sec. 8.4) significantly improves the precision of local change detection."
        },
        {
            "title": "Method",
            "content": "k = 2N = 3N = 4N = 5N Acc.@50 Acc.@100 Acc.@200 88.6% 91.4% 95.7% 93.9% 94.6% 98.3% 95.9% 96.1% 99.2% 97.2% 97.1% 99.5% Table 7. Ablation of Hyperparameters of Our Hybrid Solution in Trend Discovery. is the number of nearest neighbours our hybrid solution verify using MLLMs. is the frequency threshold of trend. We choose = 3N for all our experiments for its balance between accuracy and efficiency. tection (Sec. 4.2 & Tab. 1), prompting strategy in change detection (Sec. 9.4 & Tab. 6), MLLMs for trend discovery (Sec. 4.3 & Tab. 2), hybrid solutions for scaling (Sec. 4.3 & Tab. 3), its hyperparameters (Sec. 9.5 & Tab. 7), and alternative MLLMs (Sec. 9.3 & Tab. 5). 9.3. Alternative MLLMs We investigate the impact of using different MLLMs on the effectiveness of our system. To this end, we replace Gemini with smaller open-sourced MLLM, LLAVA [34], and evaluate its performance on both sub-problems in our system. We find that it still produces reasonably accurate analysis while being slightly worse than Gemini. 9.4. Self-Critic Prompting in Change Detection As mentioned in Sec. 8.4, we find that applying round of self-critic to local change detection greatly improves the precision of the detection. In Tab. 6, we quantitatively evaluate the impact of self-critic. We find that it boosts the precision by significant margin of more than 33% while not degrading much on the recall. 9.5. Hyperparameters in Trend Discovery Our hybrid solution to trend discovery requires setting hyper-parameter k, i.e. the number of nearest changes for which we use MLLMs to verify regarding trend proposal. In Tab. 7, we show an ablation study of varying the value of k, as multiples of the trend threshold . We find that as increases, the accuracy goes up as expected, which comes at cost of more MLLM inferences and slower verification. However, = 3N can already produce satisfying accuracy (> 90% across the board). Hence, we choose it as good balance between accuracy and speed. 9.6. MLLMs Without Images Complete Answers from MLLMs. As mentioned in Sec. 4.1, we find MLLMs answers without looking at any images unsatisfactory. Such answers are often abstract and lack intriguing details. In Fig. 13, we illustrate the complete answers from MLLMs to the prompting questions, What are the trends of changes happened in [NYC / SF] that we can see visually in 2011-2023?. Here we use Gemini-1.5 Pro for queries for both cities. Alternative Prompts and MLLMs. We also vary the wording of the prompts in search for potentially better alternatives, such as replacing changes ... that we can see visually with visual changes or simply changes. Yet, we empirically find the results not varying much in quality, except we see more abstract trends, e.g. Increased focus on sustainability, when using simply changes. Additionally, we show in Fig. 14 the results from an alternative MLLM, namely GPT4o. We find that GPT-4o focuses more on unique changes, such as construction of numerous super-tall buildings ... along 57th Street ... known as Billionaires Row., but misses more frequent trending changes such as bike lane expansions, which both Gemini and Visual Chronicles identify as significant trend. We also notice that to answer the query, GPT-4o would search online and retrieve relevant information, both image and text, in the answering process, and even output few images in the output, as shown in Fig. 14. Yet these images are not strictly all visual evidence for the discovered trends, unlike in Visual Chronicles. 9.7. MLLMs Processing All Images In Sec. 4.1, we discussed the context size as MLLMs major limitation. Due to such constraint, it is infeasible to feed all our images to MLLMs. Instead, in Fig. 15, we sub-sample the maximum subset of our images that can be fed into MLLMs, ranging from 7K to 8K images because we feed complete image sequences to MLLMs and such sequences vary in lengths. Specifically, we sample 4 such subsets and take the union of the answers as the final answer shown in Fig. 15. Note that we only sample 4 subsets for increasing the number of subsets mainly creates duplicate trends. 9.8. MLLMs Processing All Changes As mentioned in Sec. 4.3, trend discovery is not trivial to MLLMs for simply feeding all change descriptions to MLLMs is not adequate. In Fig. 16, we show the complete results of feeding as many changes as possible to MLLMs and 5 11. Potential Societal Impacts To our best knowledge, Visual Chronicles marks the first proof-of-concept of using MLLMs to analyze millions of images. In the context of searching for trends of visual changes in the urban environment, our studies show that system like ours could potentially facilitate studies in urban planning, civil engineering, or public policy, by automatically providing insights from massive scale dataset. Nevertheless, as we mentioned in our discussion on our results, its vital to keep aware of the potential bias in our data, hence being cautious in drawing conclusions that demand statistical soundness. Particularly, careful uses of systems of this kind are critical in battling misinformation. Our focus on providing verifiable visual evidence for any trend proposed by our system can be seen as part of such efforts. ask for trends among them. Here we sample 50K changes for each batch to feed to MLLMs, from our pool of 3M changes. We sample 4 batches in total and take the union of answers from each batch as the final answer shown in Fig. 16. Note that we only sample 4 batches because in our observations, more batches mostly create redundancy in found trends. 9.9. Hybrid Trend Discovery Method and RAG Our hybrid solution to trend discovery bears some connections to Retrieval-Augmented-Generation (RAG [30]). Particularly, both use embeddings to filter relevant examples from database. However, ours is fundamental different from RAG in few aspects: 1) Our goal is different from RAG. We aim to find salient patterns of examples in the database, whereas RAG retrieves examples to assist question answering. 2) The scale of relevant examples is different. In our task, we mark thousands of examples in the database as relevant, while RAG normally select several most relevant examples. 3) We use relevant examples differently from RAG. RAG often combines all relevant examples together as context to LLM or MLLMs. In contrast, we process each relevant example independently to verify visual trend proposals. 10. Limitations and Failure Cases As described in the Conclusion section of the main paper, our approach has limitations that lead to failure cases and topics for future work. The following provides few examples. The proposed approach is natural when the input image collection can be decomposed into small sets for MLLM analysis. We utilized image poses to create sets of images at nearby viewpoints from which local changes can be detected. However, this method would not work well for detecting all types of trends, including ones that involve global changes or ones that cannot be observed in images. We rely upon simple prompts to guide the MLLM to produce descriptions of local changes that we expect people would find interesting. However, other types of changes might be of interest in different applications (e.g., to find new dirt on the sidewalk). That would require prompt engineering to get the MLLM to detect certain types of changes further study is required to discover how readily good prompts can be found. Our formulation produces sets of visual changes that happened multiple times at different locations, but does not reason about how often the reverse change happened, and so it sometimes reports changes that are the result of transient activities (e.g., the addition of Halloween decorations), which are not in fact long-term trends. Similarly, our system does not measure the fraction of times change happened for the same starting state, which could be valuable for studying certain types of trends. Adding these capabilities is an interesting topic for future studies. 6 Prompt: What are the trends of changes happened in New York City that we can see visually in 2011-2023? MLLM (Gemini 1.5) Answers: Visually noticeable changes in New York City between 2011 and 2023 include: Infrastructure & Development: Rise of Supertalls: The proliferation of extremely tall, slender skyscrapers, particularly in Midtown Manhattan (e.g., 432 Park Avenue, 111 West 57th Street) and Downtown Brooklyn, dramatically altered the skyline. Hudson Yards Development: The massive Hudson Yards project on Manhattans west side, with its distinctive Vessel structure and surrounding high-rises, became completely new neighborhood, visually impacting the cityscape. Second Avenue Subway: The long-awaited Second Avenue Subway extension finally opened its first phase, visibly changing the streetscape along Second Avenue on the Upper East Side with new station entrances. Brooklyn Waterfront Transformation: Continued development along the Brooklyn waterfront, from Williamsburg to DUMBO to Greenpoint, resulted in new parks, residential buildings, and commercial spaces, reshaping the view from Manhattan. Bike Lane Expansion: significant increase in dedicated bike lanes throughout the city is visible change, altering street markings and traffic patterns. Social & Cultural: Increased Outdoor Dining: The COVID-19 pandemic led to boom in outdoor dining structures, from simple sheds to elaborate setups, significantly changing the look and feel of many streets. While some have been removed, many remain and have become more permanent fixture. Public Art Installations: Larger and more frequent public art installations have become more common in parks and public spaces, adding visual interest to the city. Retail Landscape Changes: The rise of online shopping and the pandemic impacted brick-and-mortar stores, resulting in visible vacancies and changes in storefront usage in some areas. Greening Initiatives: Efforts to increase green space, including park expansions and rooftop gardens, are slowly but surely adding more visible greenery to the city. Less Visible but Noteworthy: Demographic Shifts: While not immediately visually obvious, changes in demographics (gentrification, population growth/decline in certain neighborhoods) influence the types of businesses, public services, and overall aesthetic of different areas over time. Security Measures: Increased security measures after 9/11 and other events, though not always visually prominent, have subtly altered public spaces and infrastructure. To truly appreciate these changes, comparing photos of specific locations from 2011 and 2023 can be particularly revealing. Google Street Views historical imagery feature is useful tool for this purpose. Youll see the transformation of specific streetscapes and neighborhoods unfold before your eyes. Prompt: What are the trends of changes happened in San Francisco that we can see visually in 2011-2023? MLLM (Gemini 1.5) Answers: Visually noticeable changes in San Francisco between 2011 and 2023 include: Increased Construction and Development: High-rise buildings: The skyline has grown denser, particularly in areas like South of Market (SoMa), Mission Bay, and Transbay. Many new residential and commercial high-rises have been constructed. Crane activity: While fluctuating, crane activity has been generally high throughout this period, visible sign of ongoing construction. Infilling: Vacant lots and older, smaller buildings have been replaced by larger, denser structures in many neighborhoods. Transportation Changes: Bike lanes and infrastructure: significant increase in protected and dedicated bike lanes is visible throughout the city, reflecting the push for alternative transportation. Transit changes: While the Central Subway finally opened in this period, other visual changes are less noticeable from street level. However, increasing numbers of electric buses might be seen. Scooter proliferation (and subsequent regulation): From around 2017-2019, dockless scooters flooded the city, dramatically changing the streetscape, before regulations brought more order. Neighborhood Transformations: Mid-Market revitalization (and struggles): The area around Twitters headquarters has seen noticeable changes, with new businesses and offices, but also persistent challenges with homelessness and public safety. Mission Bay development: This formerly industrial area has transformed into bustling neighborhood with new housing, UCSF facilities, and the Chase Center. Gentrification and displacement: While not always directly visible, the effects of gentrification can manifest in changes to storefronts, the types of businesses present, and the overall aesthetic of neighborhood. Other Visible Changes: Increased homelessness: Sadly, significant rise in visible homelessness has been major change in this period, with tent encampments and individuals on the streets becoming more prevalent. Retail vacancies: Particularly after the pandemic, an increase in empty storefronts, especially in downtown areas, has been noticeable change. Parklets and outdoor dining: Accelerated by the pandemic, parklets and outdoor dining spaces became more common, altering the look and feel of many commercial streets. Art installations and murals: San Franciscos vibrant street art scene continues to evolve, with new murals and installations regularly appearing and disappearing. To actually see these changes, you would need to compare imagery from the two periods: Google Street View: Offers historical record of street-level views, allowing for direct comparisons over time. Satellite imagery: Services like Google Earth can show broader changes in building density and land use. Photo archives: Local historical societies, libraries, and online photo collections can provide visual documentation of past cityscapes. Documentaries and news footage: Videos can offer dynamic perspective on how the city has changed over time. By comparing imagery and exploring these different sources, you can gain clearer visual understanding of San Franciscos transformation between 2011 and 2023. Figure 13. Full Gemini 1.5 Predictions Without Images (Accessed time: Nov. 2024). Prompt: What are the trends of changes happened in New York City that we can see visually in 2011-2023? MLLM (GPT-4o) Answers: Between 2011 and 2023, New York City underwent significant visual transformations across its skyline, infrastructure, and public spaces. Key trends include: 1. Skyline Evolution: Rise of Super-Tall Skyscrapers: The city saw the construction of numerous super-tall buildings, particularly along 57th Street, known as Billionaires Row. Notable additions include 432 Park Avenue and Central Park Tower, which have redefined the citys silhouette. 2. Infrastructure and Public Space Enhancements: Pedestrianization Initiatives: Broadway underwent significant changes, with sections transformed into pedestrian plazas to enhance walkability and reduce vehicular traffic. Notably, areas in Times Square and Herald Square were closed to cars, creating more space for pedestrians. Reopening of Public Spaces: After 15 years, 15,000-square-foot greenspace under the Brooklyn Bridge, part of The Arches area, reopened in November 2024, providing much-needed greenery and recreational space for the community. 3. Adaptive Reuse of Buildings: Office-to-Residential Conversions: In response to changing work habits and increased office vacancies, developers have been converting obsolete office buildings into residential apartments, particularly in areas like the Financial District. This trend aims to address housing shortages and revitalize underused spaces. 4. Preservation and Restoration Efforts: Neon Signage Conservation: Efforts to preserve iconic neon signs have intensified, with restorations of landmarks like Radio City Music Hall and the Essex House hotel, reflecting desire to maintain the citys historical visual elements amid modernization. 5. Sustainable Urban Design: Green Infrastructure Projects: The city has embraced sustainable development, with projects like the transformation of Fifth Avenue into more pedestrian-friendly boulevard, featuring expanded sidewalks, seating, and greenery, aiming to enhance urban vitality and environmental sustainability. These developments collectively highlight New York Citys dynamic evolution, balancing modernization with preservation and sustainability. Figure 14. GPT-4o Predictions Without Images. (Accessed time: Nov. 2024) Prompt: have some image sequences, each captured from one location over time. They are sorted in chronological order. [. . . Image Sequences Containing 7K-8K Images. . . ] Now you, as brilliant analyst, need to tell me what are the trends of temporal changes in these sequences. Describe each trend in line in the format of [Before] [After] (Evidence ID: [IDX],[IDX]). Replace [Before] with descriptions for before the trend, replace [After] with descriptions for after the trend, and replace [IDX] with an evidence image sequence index for the trend. For each trend, you need to tell me at least 500 evidence. MLLM (Gemini 1.5) Answers: Construction site with partially built structure Completed building. One business Different business One business Vacancy/Lease available Older signage/awning Newer signage/awning No bike lane markings Bike lane markings added One advertisement Different advertisement One business storefront Different business storefront Older storefronts/signage New storefronts/signage Bare trees/winter scenery Leafy trees/summer scenery Building under scaffolding Building with no scaffolding Brownstone building at street corner Brownstone building with an awning and signage One storefront/business signage Different storefront/business signage/closed down Smaller plants/Bare trees Larger plants/Leafy trees/Bare Trees Older graffiti New graffiti/graffiti removal Building exterior or area without scaffolding/construction equipment Building exterior or area with scaffolding/construction equipment Figure 15. MLLMs Processing All Images. 8 Prompt: Here are 50K text descriptions, each describing change happened in the city. [. . . Contents of 50K Changes . . . ] Now you, as brilliant analyst, need to tell me what are the trends among these changes. Describe each trend in line in the format of [Before] [After] (Evidence ID: [IDX],[IDX]). Replace [Before] with descriptions for before the trend, replace [After] with descriptions for after the trend, and replace [IDX] with an evidence change index for the trend. For each trend, you need to tell me at least 500 evidence. MLLM (Gemini 1.5) Answers: There is no scaffolding on the building There is scaffolding on the building. The scaffolding is present. The scaffolding is gone. The crosswalk markings are solid white lines The crosswalk markings are thick white stripes. The building is covered in scaffolding The scaffolding is removed. The building has [Business name] sign The [Business name] sign is replaced. The storefront is occupied by business The storefront is vacant. The storefront is occupied by one business The storefront is occupied by another business. The trees on the street is thin The trees grow taller and thicker. Scaffolding is erected in front of the building The scaffolding is removed, revealing the buildings facade. The billboard has an advertisement The advertisement changed. business with [Business name] sign The [Business name] sign is gone. There were no bike lanes There is new bike lane. Entrance to the building has [Color] door The entrance to the building has [Color] door. Green scaffolding covers the building The scaffolding is gone. Figure 16. MLLMs Processing All Changes. 9 Prompt: Below are pictures of the same location taken over time. This is image No. 2, taken in June 2021. This is image No. 3, taken in May 2022. ... ... You, as brilliant role, need to describe what have changed over time in these images in details and also tell me from what images you found the changes. Please format the answer strictly in the following format, one change line: Start: [Start DESCRIPTION] End: [End DESCRIPTION] (happened after image No.[X]). Format Instructions: Replace [Start DESCRIPTION] with detailed descriptions of the start of the change; Replace [End DESCRIPTION] with detailed descriptions of the end of the change; Replace [X] with the index of the image where the change happened after. That said, this number should come from the image No.[X] descriptions above. There are few guidelines that you should follow when answering the question: DO NOT HALLUCINATE. You should stick to the changes that can be clearly and certainly seen in the images. IGNORE CHANGES IN THE PERIPHERAL. Because the viewpoint may wiggle, ignore the changes that happened very close the the peripheral of images because they usually can not be confirmed. IGNORE TINY UNRECOGNIZABLE TEXT. If certain text in the image is too small to be clearly recognized, such text content should not be part of change because it can not be confirmed. IGNORE CHANGES CAUSED BY OCCULUSIONS. There will be occlusions, like trucks or cars parked in front of some subjects in images. If subject appear or disappear due to such occlusions, it is not change. TRACK THE SAME SUBJECT. When detect changes, please be sure to describe the change happen to the same subject. DO NOT MISTAKE FOREGROUND AND BACKGROUND. Do not mistake the foreground subject, such street sign, with attachments to the background subject, like signs on the building behind the street sign. IGNORE CHANGES CAUSED BY ILLUMINATION CHANGES. Ignore subtle color changes caused by illumination changes, like colors become darker because of shadows, you can get the clue of illumination from the time of day, i.e. the time after taken at in the prompt. ONE EVENT CHANGE. For each detected change, it should focus on one event only, not multiple events. For example, the change Start: The storefront of the building has black awning and red truck parked in front of it. End: The storefront of the building has red awining now and the truck is gone should be 2 changes one on the awning and the other on the truck. CHANGES SHOULD BE SIGNIFICANT. This means the description after Start: should describe clearly different status of THE SAME SUBJECT from the description after End:. For example, the change Start: The building is free of scaffolding. End: There is no scaffolding on the building. is not significant change because both Start and End are the same status of the same subject. Now, please tell me changes that you can find in the images. Make sure you tell me ALL THE CORRECT CHANGES, following the above guidelines. MLLM (Gemini 1.5) Answers: . . . Start: The support beams of the overpass were in pale green color. End: The support beams were painted into bright blue color. (Happened after image No. 2) . . . Figure 17. Local Change Detection Prompt. 10 Prompt: have text description of change that happened in city. For example: Before: The Ginger Mans Bar did not have any Bar for Lease sign. After: The Ginger Mans Bar put up Bar for Lease sign on the storefront window. As you can see, in the description, there will be the status before the change and the status after the change. Your job, as brilliant analyst, is to derive more abstract versions of the same change by removing details. Formulate the answer strictly in the following list format: There are [N] levels of details on where the change happened: p[X]. [Description of one level] Meanwhile, there are [M] levels of details on the change itself: c[X]. [Description of one level] [More descriptions of the reasoning]. Answer: (reason) [A derived abstract version]. Here are few examples: . . . [Examples] . . . Here is the change you need to derive abstract versions for: Before: The single door to the right of the main entrance has sign above it that reads 151. After: The sign above the single door is blank. Now, please tell me the abstract versions of this change, following the above instructions. MLLM (Gemini 1.5) Answers: Derivation: There are 3 levels of details on where the change happened: p1. The single door to the right of the main entrance. (reason: the original very specific place). p2. The single door. (reason: more general location description without mentioning its to the right of the main entrance). p3. door. (reason: the most general location description). Meanwhile there are 2 levels of details on the change itself: c1. The sign above it changing from reading 151 to being blank. (reason: the original very specific change.) c2. The sign above it changing from reading 151 to something else. (reason: more general without specifying the sign being blank.) c3. The sign above it changing. (reason: the most general change, without even specifying how the sign changed). Answer: (p1 + c1) The sign above the single door to the right of the main entrance changed from reading 151 to being blank. (p1 + c2) The sign above the single door to the right of the main entrance changed from reading 151 to something else. (p1 + c3) The sign above the single door to the right of the main entrance changed. (p2 + c1) The sign above the single door changed from reading 151 to being blank. (p2 + c2) The sign above the single door changed from reading 151 to something else. (p2 + c3) The sign above the single door changed. (p3 + c1) The sign above the door changed from reading 151 to being blank. (p3 + c2) The sign above the door changed from reading 151 to something else. (p3 + c3) The sign above the door changed. Figure 18. Trend Proposal Prompt. 11 Prompt: have text description of change that happened in city. For example: Before: The Ginger Mans Bar did not have any Bar for Lease sign. After : The Ginger Mans Bar put up Bar for Lease sign on the storefront window. As you can see, in the description, there will be the status before the change and the status after the change. also have test description of group of similar changes that happened. For example: storefront put up For Lease sign on its window. Your job, as brilliant analyst, is to decide if the given change belongs to the given group, for yes and for no, and why. Formulate the answer strictly in the following format: Answer: [Y/N]. Reason: [Description of the reasoning]. For the example above, the answer would be: Answer: Y. Reason: The given change describes bar putting up Bar for Lease sign. We can remove the details of the location being specific Ginger Mans Bar but only keep it as storefront, Meanwhile, we can keep the event of putting up the sign the same mostly intact, only removing the detail of bar, i.e. Bar for Lease to For Lease, to align with storefront. Then, we can arrive at the group description. Therefore, the given change belongs to the given group. Here are few more examples: . . . [Examples] . . . Here is the change for you: Before: The storefront on the corner is GNC store. After: The storefront is an OPT store. Here is the group for you: juice shop opened at the storefront. MLLM (Gemini 1.5) Answers: Answer: N. Reason: The group description specifies change to juice shop. The provided change describes change from GNC store to an OPT store. While both describe storefront changes, the specific type of store in the group description (juice shop) is not reflected in the change description. We cannot generalize OPT store to juice shop without adding information. Figure 19. Trend Verification Prompt."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "Stanford University"
    ]
}