{
    "paper_title": "LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models",
    "authors": [
        "Haiwen Huang",
        "Anpei Chen",
        "Volodymyr Havrylov",
        "Andreas Geiger",
        "Dan Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved impressive results on various downstream tasks, but their limited feature resolution hampers performance in applications requiring pixel-level understanding. Feature upsampling offers a promising direction to address this challenge. In this work, we identify two critical factors for enhancing feature upsampling: the upsampler architecture and the training objective. For the upsampler architecture, we introduce a coordinate-based cross-attention transformer that integrates the high-resolution images with coordinates and low-resolution VFM features to generate sharp, high-quality features. For the training objective, we propose constructing high-resolution pseudo-groundtruth features by leveraging class-agnostic masks and self-distillation. Our approach effectively captures fine-grained details and adapts flexibly to various input and feature resolutions. Through experiments, we demonstrate that our approach significantly outperforms existing feature upsampling techniques across various downstream tasks. Our code is released at https://github.com/andrehuang/loftup."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 2 3 0 4 1 . 4 0 5 2 : r LoftUp: Learning Coordinate-Based Feature Upsampler for Vision Foundation Models Haiwen Huang1,2 Anpei Chen1,2 Volodymyr Havrylov1 1University of Tubingen Andreas Geiger1,2 2 Tubingen AI Center Dan Zhang"
        },
        {
            "title": "Abstract",
            "content": "Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved impressive results on various downstream tasks, but their limited feature resolution hampers performance in applications requiring pixel-level understanding. Feature upsampling offers promising direction to address this challenge. In this work, we identify two critical factors for enhancing feature upsampling: the upsampler architecture and the training objective. For the upsampler architecture, we introduce coordinate-based crossattention transformer that integrates the high-resolution images with coordinates and low-resolution VFM features to generate sharp, high-quality features. For the training objective, we propose constructing high-resolution pseudogroundtruth features by leveraging class-agnostic masks and self-distillation. Our approach effectively captures fine-grained details and adapts flexibly to various input and feature resolutions. Through experiments, we demonstrate that our approach significantly outperforms existing feature upsampling techniques across various downstream tasks. Our code is released at https://github.com/ andrehuang/loftup. 1. Introduction High-quality pretrained representations from Vision Foundation Models (VFMs) have become standard for wide range of computer vision tasks [10, 16, 21, 37, 41, 43, 45, 54, 56, 62, 63]. However, because of the patrification or aggressive pooling operations in VFMs, the output features are typically 16 or even more times smaller in spatial resolution than the input images, limiting their utility for tasks that require fine-grained, pixel-level understanding. To address the challenge of limited feature resolution in VFMs, one straightforward approach is to use larger image inputs to obtain higher-resolution features. However, processing high-resolution inputs incurs quadratic increase in computational cost and can introduce severe artifacts if the VFMs are not trained for such resolutions. Moreover, Semantic Seg. 61.11 LoftUp (ours) Max(FeatUp, LiFT) Backbone only"
        },
        {
            "title": "Depth Estimation",
            "content": "88.71 91.35 53.76 Interactive Seg. 78.49 65. 69.83 44.30 72."
        },
        {
            "title": "Normal Estimation",
            "content": "26."
        },
        {
            "title": "27.82\nOpen-Voc Seg.",
            "content": "Video Object Seg. (Object Tracking) 60.25 Figure 1. LoftUp improves significantly across various tasks over the VFM backbone (DINOv2-S [37]) and current SoTA feature upsampling performance (FeatUp [12] and LiFT [52]). See experiment details in Sec. 5. training or fine-tuning VFMs on high-resolution images demands substantial computational resources and meticulous tuning [37, 43, 45, 55]. An alternative strategy is to train task-specific decoders that leverage multi-layer intermediate features to upsample VFM outputs [3, 16, 24, 27, 62]. Yet, this approach often comes with significant training costs and necessitates retraining the decoder for each new application. In addition, many downstream tasks lack sufficient data needed to fine-tune high-quality decoder. More recently, FeatUp [12] and LiFT [52] have independently introduced task-agnostic feature upsamplers trained with general reconstruction losses, demonstrating that such methods can substantially enhance VFM performance across variety of tasks. By upsampling VFM features and pairing them with lightweight task-specific decoder, these approaches provide promising alternative that avoids heavy, task-specific training while achieving more generalizable solution. Nonetheless, as shown in Fig. 1, current feature upsamplers still fall short of reaching optimal performance. (a) Original image (b) Low res (c) Bilinear (d) Resize-conv (e) LIIF [2] (f) LiFT [52] (g) FeatUp [12] (h) LoftUp (Ours) Figure 2. Comparison of features from upsamplers. Backbone is DINOv2-S/14 [37]. In this work, we systematically explore the design space of feature upsamplers and identify two critical components: the upsampler architecture and the training objective. The architecture determines the capacity of the upsampler to learn effectively, while the training objective defines the upper performance limit. By optimizing both elements, our approach achieves substantially stronger results than previous state-of-the-art methods. Regarding the upsampler architecture, to capture highresolution details while avoiding the cumulative artifacts from multiple layers of interpolation or deconvolution, we propose simple coordinate-based transformer that directly predicts high-resolution features for each pixel. Specifically, our model takes image coordinates and RGB values as inputs and performs cross-attention with the lowresolution VFM feature map. This facilitates fine-grained, content-aware mapping from coordinates to high-resolution features, effectively bypassing the constraints imposed by fixed local kernels and standard upsampling layers. Moreover, unlike the implicit approach in FeatUp [12], which requires test-time optimization, our method learns feature upsampling directly from the training dataset and generalizes to diverse scenes without additional test-time adjustments. For the training objective, the primary challenge is the absence of groundtruth high-resolution feature annotations. Although downstream task labels such as depth or masks can be used, this approach risks compromising the taskagnostic nature of the upsampler and hinders its ability to generalize to unseen tasks. Due to this challenge, previous task-agnostic feature upsampling works, FeatUp [12] and LiFT [52], both compute the training loss at low resolution. In this work, we address these limitations by constructing pseudo-groundtruth (pseudo-GT) features directly at the input image resolution. Specifically, we leverage class-agnostic masks generated by off-the-shelf segmentation foundation models [21, 45] to ensure that the pseudoGT accurately reflects the underlying geometry and delineates object boundaries. We further refine the pseudo-GT using self-distillation strategy that reduces noise and artifacts. This high-resolution pseudo-GT enables loss computation at high resolution, empowering the upsampler to learn fine-grained details. In Fig. 2, we qualitatively show that LoftUp yields markedly sharper and more detailed features than alternative upsampling methods. To further demonstrate the versatility and effectiveness of our approach, we evaluate it on range of downstream tasks such as semantic segmentation, depth estimation, and video objectschannen2025siglip2. As shown in Fig. 1, our approach leads to performance gains of 1020% over previous SoTA upsamplers for most tasks, and an impressive nearly 50% improvement on video object segmentation [39]. Furthermore, thanks to its coordinatebased design, our upsampler adapts seamlessly to various input and feature resolutions, catering to the diverse requirements of downstream applications. Overall, with less than 20% increase in parameters compared to the original foundation models, our feature upsampler offers taskagnostic, lightweight, and plug-and-play enhancement that significantly boosts VFM backbones across multiple tasks. 2. Related Work Feature upsampling refers to increasing the spatial resolution of feature map. In this work, our goal is to increase the feature resolution to the original image resolution, that is, full resolution. Traditional non-learnable methods include various ways of interpolation [7, 33] and imageadaptive filtering such as joint bilateral filtering (JBU) [22] and guided image filtering [15]. In modern deep learning, previous work has proposed various architectureand downstream-task-specific feature upsamplers. For example, Index Networks [29] and A2U [5] are effective on image matting, but fall short in other tasks. PointRend [20] proposes point-rendering method specifically for upsampling segmentation output. And CARAFE [57], SAPA [31], and FADE [30] are proposed specifically for encoder-decoder architectures. More recently, with the success of vision foundation models such as DINOv2 [37] and CLIP [41], there is trend for feature upsamplers to be downstreamtask-agnostic so that they can be used with the VFM backbone together in various applications [12, 52]. Our work falls into this category. With this task-agnostic goal in mind, we further explain the architecture and training objective design as follows. Architecture for Feature Upsamplers. Traditional upsampler architectures rely on multiple layers of interpolation or deconvolution to transform low-resolution features into higher resolutions. Examples include JBU [22], standard deconvolution [8, 35, 49], resize-convolution [36] and U-Net-style upsampling modules [47, 52]. However, the multi-layer design inevitably leads to error accumulation, resulting in increased blurriness as the resolution increases. In this work, inspired by coordinate-based methods in 3D reconstruction [25, 53, 60, 61], we adopt coordinatebased approach and view feature upsampling as mapping from high-resolution coordinates to high-resolution features. This effectively bypasses the limitations of standard upsampling layers. Previously, FeatUp also proposed coordinate-based network (MLP) for feature upsampling [12]. However, their approach requires per-image optimization and is therefore not scalable. Another related work, LIIF [2], also employs an MLP to parameterize highresolution outputs but is limited to local feature interactions. In contrast, our method does not need test-time optimization and enables global interactions between image inputs and low-resolution features through cross-attention mechanism, leading to stronger upsampling performance. Task-agnostic Training Objective for Feature Upsampling. Due to the absence of groundtruth high-resolution features, it is challenge to create high-quality training objective for task-agnostic feature upsampling. FeatUp [12] and LiFT [52] were the first two works to propose taskagnostic training pipeline for feature upsamplers. However, their training is at low resolution, leaving the highresolution features severely under-constrained. We will explain them in more detail Sec. 4. In this paper, we propose self-distillation approach to generate full-resolution pseudo-GT, which is then used to supervise feature upsampling training at full resolution. This approach fully unFigure 3. Architecture of LoftUp. Our coordinate-based network with cross-attention mechanism effectively integrates the fine-grained details from image RGB values and semantically-rich low-res features to produce high-resolution feature maps. locks the potential of the feature upsampler to capture finegrained details in high-resolution images. 3. Coordinate-Based Feature Representation Prior works typically address feature upsampling as gradual resolution enhancement process, where features are progressively lifted to higher resolution. In contrast, we adopt coordinate-based representation of the high-resolution features and view feature upsampling as mapping from pixel coordinate (x, y) to the high-resolution features at that pixel. Specifically, we propose cross-attention mechanism to effectively incorporate the low-resolution features and the high-resolution image inputs with coordinates. As shown in Fig. 3, following prior work on 3D coordinate-based reconstruction [34, 61], our model encodes full-resolution coordinates with sinusoidal positional embeddings and concatenates them with RGB values. convolutional layer then projects this combined input into the feature dimension. Next, an L-block cross-attention transformer uses these high-resolution features as queries and low-resolution VFM features as keys and values, producing the final high-resolution feature map. Our cross-attention design enables high-frequency details to interact globally with semantically rich representations, facilitating global-content-aware upsampling. This overcomes the limitations of fixed or locally predicted kernels in prior feature upsamplers. As illsustrated in Fig. 2, multi-layer upsamplers such as resize-conv [36], LiFT (UNet) [52], and FeatUp (a modified JBU) [12] tend to produce blurry outputs with artifacts. Additionally, while LIIF [2] employs coordinate-based design, its reliance on local interactions limits its upsampling quality. In contrast, our LoftUp accurately captures object boundaries and produces fine-grained feature maps with minimal artifacts. Figure 4. Our two-stage LoftUp training approach. Stage 1 trains an upsampler with class-agnostic masks to refine bicubic-upsampled features. Stage 2 employs self-distillation, initializing teacher and student upsamplers from Stage 1s pre-trained model. All VFM image inputs share the resolution (H ). For visual clarity, the VFM block is omitted from Stage 2s teacher branch. implicit Per-image map"
        },
        {
            "title": "Depth",
            "content": "Metrics High-Res Loss Task-Agnostic Geometry Fidelity Free of Noise/Artifacts Scalability (LiFT) features 2x Mask-Bicubic Self-Distilled COCO Seg. (IoU) Cityscapes Seg. (IoU) 56.02 44.82 59.08 46.05 52.90 35.85 59.87 50. 61.11 53.10 Table 1. Comparison of different pseudo-GT choices. Furthermore, our coordinate-based approach allows flexible generation of feature maps at arbitrary resolutions by adjusting the input coordinate resolutionunlike traditional upsampling methods, which are restricted to fixed scaling factors due to their multi-layer structures. Finally, we note that while attention mechanisms can be computationally expensive, our cross-attention remains relatively efficient because it processes much smaller set of lowresolution tokens as keys and values, rather than the progressively higher-resolution feature maps as in multi-layer upsampler architectures. In fact, as our experiments (Tab. 7) later show, our upsamplers inference speed is comparable to bilinear upsampling. 4. Training Objective While the architecture of the upsampler establishes its capacity, the training objective ultimately sets the performance ceiling. central challenge for training taskagnostic feature upsampler is the absence of full-resolution groundtruth features. As result, previous work has either relied on proxy tasks or constructed pseudo-GT at lower resolutions. For instance, FeatUp [12] employs multiview reconstruction task: the predicted high-resolution features, ˆFHR, are first transformed via an affine mapping t, then downsampled, and finally compared against the lowresolution features extracted from images that go through the same transformation, i.e., LFeatUp = D(cid:0)f (t(I)), σ(t( ˆFHR)(cid:1). In contrast, LiFT [52] directly uses features from 2 larger images I2 as pseudo-GT, constraining the upsampler to predict 2 upsampled features ˆF2 via LLiFT = D(cid:0) ˆF2, (I2)(cid:1). However, since both LFeatUp and LLiFT are at relatively low resolutions (only 1/16 or 1/8 of the target resolution), they provide only weak supervisory signals for capturing finegrained details inherent in high-resolution outputs, potentially leaving the upsampled features under-constrained. In this work, we employ self-distillation strategy to generate high-quality pseudo-GT for supervising fullresolution features, as shown in Fig. 4. First, we train an upsampler using high-resolution class-agnostic masks, which emphasize sharp boundaries and geometric awareness. In the second stage, we enhance training through selfdistillation. Specifically, we first initialize teacher and student upsampler both from the trained upsampler in Stage 1. Then, the teacher processes high-resolution image crops, and its outputs serve as supervision for the corresponding student upsampler outputs. By distilling more detailed and accurate feature maps from the teacher, Stage 2 further reduces noise and enhances sharpness in the students outputs. Overall, by constructing high-quality pseudo-GT fea- (a) Original image (b) Per-image optimized (c) 2x features (LiFT) (d) Mask-Bicubic (Stage 1) (e) Self-Distilled (Stage 2) Figure 5. Visualization of different pseudo-GT. Both Mask-Bicubic and Self-Distilled are proposed by our work. We set α = 0.8 (in Eq. (1)) to balance sharp boundaries from masks and fine-grained details from high-res features. ture maps at full-resolution, our approach enables the upsampler to capture more detailed structures and boundaries, ultimately pushing the limits of feature upsampling. 4.1. Stage 1: Training with class-agnostic masks Low-resolution features and their upsampled features using existing upsamplers are often too blurry and noisy to be used as pseudo-GT. In contrast, Segment Anything Model (SAM) [21] produces full-resolution, class-agnostic masks that capture fine-grained details such as small object parts and boundaries. Previous work has shown that training with these masks leads to strong task generalization [21, 4446]. These qualities make them well suited for refining feature maps to reduce artifacts and noise while promoting smoothness in homogeneous regions. To leverage these masks, we first upsample lowresolution feature map to full resolution via bicubic interpolation, yielding FBicubic. Then, for each mask = {m1, m2, ..., mN }, we compute the mean feature, FBicubic[m], and blend it with the original features, yielding mask-refined feature map at pixels within mask m: FMask-Bicubic[m] = αFBicubic[m]+(1α)FBicubic[m], (1) where α [0, 1] controls the degree of mask refinement. We then supervise the upsampler using the loss: LMask-Bicubic = ˆFHR FMask-Bicubic2, (2) which encourages high-resolution outputs with homogeneous features within each mask region, thereby leveraging the rich structural information from the class-agnostic masks. Note while these masks can also refine features from more complicated upsampling methods such as FeatUp [12] implicit Per-image map"
        },
        {
            "title": "Depth",
            "content": "Metrics High-Res Loss Task-Agnostic Geometry Fidelity Free of Noise/Artifacts Scalability (LiFT) features 2x Mask-Bicubic Self-Distilled COCO Seg. (IoU) Cityscapes Seg. (IoU) 56.02 44.82 59.08 46.05 52.90 35.85 59.87 50. 61.11 53.10 Table 2. Comparison of different pseudo-GT choices. or JBU [22], we observe that simple bicubic upsampling already yields strong results, making it the preferred choice for Stage 1 training. 4.2. Stage 2: Training with self-distillation We further enhance our upsampler training using selfdistillation. To distill high-quality features, we adopt teacher-student dual-branch design. Both the teacher and student upsamplers are initialized with the upsampler trained in Stage 1. The teacher is updated via an Exponential Moving Average (EMA) during training, ensuring stable and progressively improving target. For the student branch, each image is resized to fixed resolution RHW (224px or 336px), matching the VFMs input requirements. For the teacher branch, the original image is resized to larger size IHR RtHtW with [2, 4]. The models then process on crop from IHR that also matches the VFMs resolution, i.e., crop(IHR) RHW . Note our original images, sourced from the SA1B Semantic Seg."
        },
        {
            "title": "Method",
            "content": "COCO mIoU CS mIoU RMSE Recall RMSE Recall Low-res Bilinear LiFT FeatUp LoftUp 51.21 56.15 53.35 56.30 61.11 36.54 44.79 35.80 44.19 53.10 0.1071 0.1132 0.1078 0.1092 0.0921 89.08 87.68 88.71 88.57 91. 32.29 32.27 32.31 32.25 30.79 69.56 70.03 69.78 69.83 72.76 Table 3. Comparison of feature upsamplers across tasks of semantics segmentation, depth estimation, and normal estimation. For each metric, we boldface the best performance and underline the second best. Video Obj. Seg. Open-Voc Seg. Interactive Seg."
        },
        {
            "title": "F Mean",
            "content": "J & Mean COCO [26] CS [4] ADE [64] GrabCut [48] Berkeley [32] DAVIS [38] Low-res Bilinear LiFT FeatUp LoftUp 42.05 42.62 47.68 45.70 58.72 31.27 33.90 36.78 42.90 61.79 36.66 38.26 42.23 44.30 60.25 25.70 25.78 25.96 26.61 27. 34.48 34.56 35.39 35.01 38.82 19.50 19.53 19.50 19.99 21.29 64.90 65.04 29.66 65.89 78.49 55.77 55.83 31.99 56.67 65.24 52.82 54.26 41.14 55.03 67.31 Table 4. Comparison of feature upsamplers across tasks of video object segmentation, zero-shot open-vocabulary segmentation, and interactive segmentation. Following previous works [16, 17, 21, 23, 27, 52], we report Mean, Mean, and their average for video object segmentation, mIoU for open-vocabulary segementation, and IoU@1 Click for interactive segmentation. dataset [21], have minimum resolution of 1500px on the shortest side and is thus larger than tH tW . While higher yields more fine-grained teacher outputs, it also lowers the supervision resolution on the student (H/t W/t), reducing supervision effectiveness. In practice, we find [2, 4] strikes good balance. The teachers output (cid:0)crop(IHR)(cid:1) RHW , is subsequently on this crop, fteacher downsampled via σ() to match the resolution of the corresponding student output, crop(cid:0)fstudent(I)(cid:1) RH/tW/t, and used as pseudo GT for the student. Formally, the selfdistillation loss is defined as: LSelf-Distilled = (cid:16) (cid:0)fteacher(crop(IHR)(cid:1), crop(cid:0)fstudent(I)(cid:1)(cid:17) , σ where denotes discrepancy function between features. In our experiments, we adopt the affinity matrix loss [40, 58, 59], which consistently outperforms the standard L2 loss. This self-distillation strategy leverages the teachers ability to handle an easier taskgenerating high-quality features for cropped regionsthus providing more reliable pseudo-GT to guide the student. Furthermore, to capitalize on the sharp boundary priors provided by the class-agnostic masks, we apply the same mask refinement described in Eq. (1) to the teachers features. For simplicity, we denote this refined output as FSelf-Distilled. As shown in Fig. 5, FSelf-Distilled reduces the blurry artifacts in FMask-Bicubic and captures the underlying geometry much better. Further Discussion on Pseudo-GT Choices. Earlier in this section, we identified low-resolution loss as bottleneck in prior training objectives. Here, we expand on that discussion by addressing additional key factors uncovered during our pseudo-GT exploration. In particular, Tab. 2 compares our approach with three alternative pseudo-GT choices: (1) depth maps, (2) per-image optimized high-resolution features from FeatUp-Implicit [12], and (3) features from 2 larger inputs, as in LiFT [52]. First, pseudo-GT should be task-agnostic to ensure generalizability across downstream tasks. For example, although depth maps contain fine-grained geometric details, they lack semantic information, potentially limiting the upsamplers ability to maintain global semantic consistency. Furthermore, pseudo-GT must capture image geometry accurately, maintain sharp boundaries and clear color transitions, and minimize artifacts and noise. As shown in Fig. 5, per-image optimized features exhibit halo artifacts around objects and speckle noise in smooth regions, while 2 features introduces significant mosaic artifacts. FMask-Bicubic still suffers from blurriness and blobby artifacts inherited from bicubic upsampling. Among these, only FSelf-Distilled meets these requirements, demonstrating the effectiveness of our self-distillation framework. Finally, scalability is another crucial factor. Per-image optimization is computationally expensive, requiring 1 minute per HR feature map on an A100 GPU and 74MB for storage equating to 35 GPU days and 3.5TB storage for just 50K images. In contrast, other methods generate pseudo-GT in real time with single forward pass, taking less than 0.1 seconds per image. Overall, we find that FSelf-Distilled best satisfies these requirements and delivers the strongest performance. Architecture COCO Cityscapes Depth Rec. Normal Rec. 5. Experiments In this section, we compare LoftUp with other upsampling approaches on various tasks and conduct in-depth analysis on the strengths of LoftUp. By default, we use the DINOv2S/14 model [37] as VFM. Experiments with CLIP [41] and RADIO [43] show that DINOv2 produces the best downstream performance. Detailed results for CLIP and RADIO are provided in the supplements. Following previous works [12, 52], we resize input images to resolution of 224 and upsample the VFM features to match the input resolution (14 for DINOv2-S/14). All upsamplers are trained on 1M-image subset of the SA1B dataset [21] and additional implementation details are available in the supplements. 5.1. Cross-Task Comparison We compare LoftUp with two most recent task-agnostic feature upsamplers: FeatUp (JBU) [12] and LiFT [52], along with bilinear upsampling as baseline. As demonstrated in Tab. 3 and Tab. 4, LoftUp consistently outperforms all alternatives on six downstream tasks. Since we expect high-resolution features to enhance finegrained scene understanding, we first evaluate them on semantic segmentation following [12, 13]. We train linear projection on upsampled features to predict coarse classes in COCO-Stuff [1] and Cityscapes [4]. LoftUp achieves 7.3% and 15.6% relative improvements over previous best methods. As expected, Cityscapes benefits more from feature upsampling due to its high number of small objects. To assess the upsamplers generalizability to geometry prediction, we follow [10] and evaluate depth and normal estimation using lightweight DPT decoder head [42] on the NAVI dataset [18]. Among the compared methods, LoftUp is the only upsampling method that significantly outperforms the low-res baseline, achieving 2.5% and 4.4% relative improvements in recall. We further evaluate the temporal consistency of upsampled features through video object segmentation on DAVIS2017 [39], which requires consistent object tracking across video frames. Following prior works [17, 52], we use feature affinity maps to track objects across frames and report Mean (mIoU, reflecting region similarity), Mean (mean F-score, reflecting contour accuracy) and their average. LoftUp achieves significant performance improvements, with 39.6% increase in Mean and 97.6% increase in Mean over the low-resolution baseline, demonstrating strong ability to identify object boundaries. Finally, to assess the flexibility of usage across modalities like text and click embeddings, we evaluate on zeroshot open-vocabulary segmentation with ProxyCLIP [23] where DINOv2 features adjust CLIP features spatially for Low-res Resize-conv LIIF FeatUp-JBU LoftUp 51.21 56.05 52.24 57.90 61.11 36.54 45.94 40.36 44.83 53.10 89.08 88.38 79.85 89.38 91.35 69.56 65.59 46.06 68.83 72. Table 5. Architecture comparison on semantic segmentation and depth and normal estimation. All upsamplers are trained using LoftUp training objective. better vision-language alignment, and on interactive segmentation using modified SimpleClick [27] where visual features are incorporated with click embeddings for segmentation. Results show that LoftUp is the only method to achieve consistent, significant improvements over the lowres and bilinear baselines, demonstrating its adaptability to multimodal alignment. 5.2. More Analysis of LoftUp Design Our pseudo-GT provides strong training signal. In Fig. 5 and Tab. 2, we already discuss and compare different pseudo-GT choices both qualitatively and quantitatively by training LoftUp architecture using different pseudo-GTs. In Fig. 7, we further show that our pseudo-GT can be used to enhance other upsamplers such as resize-conv and FeatUp (JBU), demonstrating broad applicability of the strong, full-resolution supervision signal of our pseudo-GT alone. Our architecture significantly outperforms other alternatives when training under the same LoftUp training objective as in Sec. 4. As shown in Tab. 5, we compare LoftUp with resize-conv, LIIF, and FeatUp-JBU. We demonstrate that while other methods may bias towards certain tasks, LoftUp achieves the strongest performance across all of the tasks, indicating its stronger capacity to leverage the highquality pseudo-GT. LoftUp benefits from the long-range, content-aware image-feature interaction thanks to the cross-attention mechanism. Instead of using local kernel as in most previous feature upsampling works, our cross-attention mechanism effectively allows global attention and content-aware upsampling. In Fig. 8, we visualize the attended region of pixel in high-res coordinate (in cross) in the corresponding low-resolution features (in dots) where the density of dots reflects the values of attention maps. As shown, the attended regions often exhibit semantically similar patterns across the image. Consequently, our upsampler leverages relevant information from the entire image, enhancing global semantic consistency and refining details at object boundaries. LoftUp supports arbitrary upsampling scales via its coordinate-based design. As shown in Tab. 6, LoftUp maintains strong performance across varying input and feature resolutions. Here, input res refers to the image reso- (a) Original image (b) Bilinear (c) LiFT (d) FeatUp (e) LoftUp (f) Groundtruth Figure 6. Visualization of predictions examples on semantic segmentation on COCO-Stuff [1] and depth estimation on NAVI [18]. We provide additional visualization in the supplements."
        },
        {
            "title": "Feature res COCO Cityscapes",
            "content": "Low-res 2x-large input 4x-large input"
        },
        {
            "title": "LoftUp",
            "content": "224 448 896 224 448 16 32 64 56 112 224 448 56 112 224 51.21 55.42 56.79 59.87 60.75 61.11 60.61 60.16 61.50 61.78 62.26 36.54 50.29 58.45 48.20 51.04 53.10 52.29 51.57 56.46 58.55 61. Table 6. LoftUp maintains strong performance with diverse input sizes and upsampling scales. curs nearly 4 the computational and memory cost. This highlights LoftUps efficiency and adaptability, making it well-suited for diverse scaling needs in downstream tasks. LoftUp also enjoys relatively high efficiency. In Tab. 7, we compare the parameter size and inference time of each upsampler. Interestingly, while LoftUp has slightly more parameters than most alternatives (still less than 20% of the VFM), it is the second fastest upsampler after LiFT, which only does 2x upsampling. This efficiency arises because LoftUp directly maps coordinates to features, bypassing multiple intermediate upsampling layers used by other methods. Moreover, compared to the coordinatebased FeatUp-Implicit that requires per-image optimization, LoftUp is both significantly faster and more effective. 6. Conclusion In this work, we systematically explored upsampler architectures and training objectives for feature upsampling and introduced LoftUp, coordinate-based crossattention transformer upsampler trained with our proposed self-distilled high-resolution pseudo-GT. Our experiments demonstrate that LoftUp generates sharp, fineFigure 7. LoftUp pseudo-GT improves different upsamplers. Figure 8. Visualization of attended regions (shown as dots) in the low-resolution features corresponding to high-resolution pixel (marked as cross). The density of dots represents the values in the attention map. LoftUp leverages relevant information from the global feature map to upsample features at each pixel. lution fed into the VFM, while feature res denotes the final feature map size generated by LoftUp. Notably, LoftUp outperforms the original VFM backbone even when the latter processes images at 2 resolutionan approach that in-"
        },
        {
            "title": "COCO Cityscapes",
            "content": "Params (M) Infer time (s/img)"
        },
        {
            "title": "Bilinear",
            "content": "Resize-conv LIIF LiFT FeatUp-JBU FeatUp-Implicit LoftUp 56.15 56.05 52.24 53.35 57.90 51.12 61.11 44.79 45.94 40.36 35.80 44.83 44.81 53.10 22. 27.5 (+5.4) 24.0 (+1.9) 23.3 (+1.2) 22.3 (+0.2) 22.5 (+0.4) 26.4 (+4.3) 0.0604 0.0922 0.1757 0.0773 0.1213 54.302 0.0893 Table 7. Comparison of efficiency for different upsampler architectures. Inference time is computed on single A100 GPU. grained feature maps, enables global content-aware imagefeature interaction, supports arbitrary upsampling scales, and achieves fast inference. Notably, LoftUp outperforms existing methods across six diverse downstream tasks. We hope our work inspires further research in feature upsampling and contributes to the vision community by enhancing VFMs for wide range of downstream applications."
        },
        {
            "title": "References",
            "content": "[1] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. CocoIn CVPR, pages stuff: Thing and stuff classes in context. 12091218, 2018. 7, 8, 12, 15 [2] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local implicit image In Proceedings of the IEEE/CVF conference on function. computer vision and pattern recognition, pages 86288638, 2021. 2, 3 [3] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. 2022. 1 [4] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 6, 7, 12 [5] Yutong Dai, Hao Lu, and Chunhua Shen. Learning affinityaware upsampling for deep image matting. In CVPR, pages 68416850, 2021. 3 [6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. [7] Claude Duchon. Lanczos filtering in one and two dimensions. Journal of Applied Meteorology (1962-1982), pages 10161022, 1979. 2 [8] Vincent Dumoulin and Francesco Visin. guide to conarXiv preprint volution arithmetic for deep learning. arXiv:1603.07285, 2016. 3 [9] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from single image using multi-scale deep network. NeurIPS, 27, 2014. 12 [10] Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the 3d awareness of visual foundation models. In CVPR, pages 2179521806, 2024. 1, 7, [11] David Fouhey, Wajahat Hussain, Abhinav Gupta, and Martial Hebert. Single image 3d without single 3d image. In ICCV, pages 10531061, 2015. 12 [12] Stephanie Fu, Mark Hamilton, Laura E. Brandt, Axel Feldmann, Zhoutong Zhang, and William T. Freeman. Featup: model-agnostic framework for features at any resolution. In ICLR, 2024. 1, 2, 3, 4, 5, 6, 7, 12 [13] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William Freeman. Unsupervised semantic segmentation by distilling feature correspondences. arXiv preprint arXiv:2203.08414, 2022. 7, 12 [14] Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. In ICCV, pages 991998. IEEE, 2011. 13 [15] Kaiming He, Jian Sun, and Xiaoou Tang. Guided image filtering. PAMI, volume=35, number=6, pages=13971409, year=2012, publisher=IEEE. 3 [16] Haiwen Huang, Songyou Peng, Dan Zhang, and Andreas Geiger. Renovating names in open-vocabulary segmentaIn The Thirty-eighth Annual Conference tion benchmarks. on Neural Information Processing Systems, 2024. 1, 6 [17] Allan Jabri, Andrew Owens, and Alexei Efros. Space-time correspondence as contrastive random walk. NeurIPS, 33: 1954519560, 2020. 6, 7, 12 [18] Varun Jampani, Kevis-Kokitsi Maninis, Andreas Engelhardt, Arjun Karpur, Karen Truong, Kyle Sargent, Stefan Popov, Andre Araujo, Ricardo Martin-Brualla, Kaushal Patel, Daniel Vlasic, Vittorio Ferrari, Ameesh Makadia, Ce Liu, Yuanzhen Li, and Howard Zhou. NAVI: Categoryagnostic image collections with high-quality 3d shape and pose annotations. In NeurIPS, 2023. 7, 8, 12, [19] Diederik Kingma and Jimmy Ba. Adam: method for arXiv preprint arXiv:1412.6980, stochastic optimization. 2014. 13 [20] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross GirIn Image segmentation as rendering. shick. Pointrend: CVPR, pages 97999808, 2020. 3 [21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, pages 40154026, 2023. 1, 2, 5, 6, 7, 12, [22] Johannes Kopf, Michael Cohen, Dani Lischinski, and Matt Uyttendaele. Joint bilateral upsampling. ACM TOG, 26(3): 96es, 2007. 3, 5 [23] Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang. Proxyclip: Proxy attention improves clip for open-vocabulary segmentation. In ECCV, 2024. 6, 7, 12 [24] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. In European conference on computer vision, pages 280296. Springer, 2022. 1 [25] Kai-En Lin, Yen-Chen Lin, Wei-Sheng Lai, Tsung-Yi Lin, Yi-Chang Shih, and Ravi Ramamoorthi. Vision transformer for nerf-based view synthesis from single input image. pages 806815, 2023. 3 [26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740755. Springer, 2014. 6, 12 [27] Qin Liu, Zhenlin Xu, Gedas Bertasius, and Marc Niethammer. Simpleclick: Interactive image segmentation with simple vision transformers. In ICCV, pages 2229022300, 2023. 1, 6, 7, 12, [28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 12 [29] Hao Lu, Yutong Dai, Chunhua Shen, and Songcen Xu. Index networks. IEEE TPAMI, 44(1):242255, 2020. 3 [30] Hao Lu, Wenze Liu, Hongtao Fu, and Zhiguo Cao. Fade: Fusing the assets of decoder and encoder for task-agnostic upsampling. In ECCV, pages 231247. Springer, 2022. 3 [31] Hao Lu, Wenze Liu, Zixuan Ye, Hongtao Fu, Yuliang Liu, and Zhiguo Cao. Sapa: Similarity-aware point affiliation for feature upsampling. NeurIPS, 35:2088920901, 2022. 3 [32] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. database of human segmented natural images and its application to evaluating segmentation algorithms and In ICCV, pages 416423. measuring ecological statistics. IEEE, 2001. 6, [33] Sky McKinley and Megan Levine. Cubic spline interpolation. College of the Redwoods, 45(1):10491060, 1998. 2 [34] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 3, 13 [35] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for semantic segmentation. In ICCV, 2015. 3 [36] Augustus Odena, Vincent Dumoulin, and Chris Olah. Deconvolution and checkerboard artifacts. Distill, 1(10):e3, 2016. 3 [37] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. 2024. Featured Certification. 1, 2, 3, 7, 12, [38] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In CVPR, pages 724732, 2016. 6, 13 [39] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv:1704.00675, 2017. 2, 7, 12, 16 [40] Gilles Puy, Spyros Gidaris, Alexandre Boulch, Oriane Simeoni, Corentin Sautier, Patrick Perez, Andrei Bursuc, and Renaud Marlet. Three pillars improving vision foundation model distillation for lidar. In CVPR, pages 2151921529, 2024. 6 [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. pages 87488763. PmLR, 2021. 1, 3, 7, 12, 13 [42] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. ViIn ICCV, pages sion transformers for dense prediction. 1217912188, 2021. 7 [43] Mike Ranzinger, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. Am-radio: Agglomerative vision foundation model reduce all domains into one. In CVPR, pages 12490 12500, 2024. 1, 7, 13 [44] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In CVPR, pages 1300913018, 2024. [45] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 1, 2 [46] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 5 [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 3 [48] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. grabcut interactive foreground extraction using iterated graph cuts. ACM TOG, 23(3):309314, 2004. 6, 13 [49] Wenzhe Shi, Jose Caballero, Lucas Theis, Ferenc Huszar, Andrew Aitken, Christian Ledig, and Zehan Wang. Is the deconvolution layer the same as convolutional layer? arXiv preprint arXiv:1609.07009, 2016. 3 [50] Konstantin Sofiiuk, Olga Barinova, and Anton Konushin. In ICCV, Adaptis: Adaptive instance selection network. pages 73557363, 2019. 13 [51] Konstantin Sofiiuk, Ilya Petrov, and Anton Konushin. Reviving iterative training with mask guidance for interactive segmentation. In ICIP, pages 31413145. IEEE, 2022. 13 [52] Saksham Suri, Matthew Walmer, Kamal Gupta, and Abhinav Shrivastava. Lift: surprisingly simple lightweight feature In ECCV, pages 110 transform for dense vit descriptors. 128. Springer, 2024. 1, 2, 3, 4, 6, 7, 12 [53] Stanislaw Szymanowicz, Chrisitian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. In CVPR, pages 1020810217, 2024. 3 [54] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. 1 [55] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. [56] Narek Tumanyan, Assaf Singer, Shai Bagon, and Tali Dekel. Dino-tracker: Taming dino for self-supervised point tracking in single video. In ECCV, pages 367385. Springer, 2024. 1 [57] Jiaqi Wang, Kai Chen, Rui Xu, Ziwei Liu, Chen Change Loy, and Dahua Lin. Carafe: Content-aware reassembly of features. In ICCV, pages 30073016, 2019. 3 [58] Yangtao Wang, Xi Shen, Shell Xu Hu, Yuan Yuan, James Crowley, and Dominique Vaufreydaz. Self-supervised transformers for unsupervised object discovery using normalized cut. In CVPR, pages 1454314553, 2022. 6 [59] Monika Wysoczanska, Oriane Simeoni, Michael Ramamonjisoa, Andrei Bursuc, Tomasz Trzcinski, and Patrick Perez. Clip-dinoiser: Teaching clip few dino tricks for openIn ECCV, pages 320 vocabulary semantic segmentation. 337. Springer, 2024. 6 [60] Jianglong Ye, Naiyan Wang, and Xiaolong Wang. Featurenerf: Learning generalizable nerfs by distilling foundation models. In ICCV, pages 89628973, 2023. 3, 13 [61] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. 2021 ieee. In CVPR, 2020. 3, [62] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and LiangChieh Chen. Convolutions die hard: Open-vocabulary segmentation with single frozen convolutional clip. Advances in Neural Information Processing Systems, 36:3221532234, 2023. 1 [63] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, pages 1197511986, 2023. 1 [64] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Scene parsing through Barriuso, and Antonio Torralba. ade20k dataset. In CVPR, pages 633641, 2017. 6, 12 LoftUp: Learning Coordinate-Based Feature Upsampler for Vision Foundation Models"
        },
        {
            "title": "Supplementary Material",
            "content": "This supplementary material to the main paper LoftUp: earning Coordinate-Based Feature Upsampler for Vision Foundation Models is structured as follows: In Appendix A, we explain more implementation details of LoftUp training and the downstream tasks. In Appendix B, we show more quantitative results of LoftUp with CLIP and RADIO backbones and ablate the architecture and dataset size choices of training LoftUp. In Appendix C, we provide more visualization of upsampled features, prediction results on various tasks, pseudoGT, and cross-attention regions. A. More Implementation Details A.1. Training details of LoftUp Our LoftUp upsampler is 2-block cross-attention transformer that incorporates high-res image inputs with coordinates using an additional convolutional layer and low-res VFM features as keys and values in the cross-attention layers. Each transformer block consists of 1 cross-attention layer and 1 feedforward layer as in ViT [6]. To train LoftUp, we use batch size of 8 and AdamW [28] optimizer with learning rate of 1e-3 in Stage 1 and 1e-4 in Stage 2 for more stable improvement during self-distillation. In Stage 2, we (cid:1), and take 2 random crops per image to construct crop(cid:0)IHR update the teacher upsamplers weights every 10 steps using the EMA of the student upsampler with decay factor of 0.99. In both stages, we use α = 0.8 for mask refinement when constructing pseudo-GT to balance sharp boundaries from masks and the fine-grained details from high-resolution features within each mask region. For all upsamplers, including our compared ones, we train for 1 epoch on 1M-image subset of SA1B dataset [21]. A.2. Task setups Semantic segmentation. Following [12, 13], we perform semantic segmentation on coarse classes in COCO-Stuff [1] (27 classes) and Cityscapes [4] (19 classes) and report mean Intersection-over-Union (mIoU) for each dataset. We train linear decoder layer on upsampled features with batch size of 8 and AdamW optimizer [28] with learning rate of 1e-4 for 10 epochs. Depth and normal estimation. Following [10], we evaluate depth and normal estimation using NAVI dataset [18] and train DPT decoder head with 7 convolutional layers on top of the VFM features. We use batch size of 8 and AdamW optimizer [28] with learning rate of 5e-4 for 10 epochs. Following prior works [911], we report the rootmean-squared prediction error (RMSE) for both tasks and recall at δ3 for scale-invariant depth estimation and at 30 for normal estimation. Here δ3 is computed as the number of pixels whose ratio of depth prediction to groundtruth is less than 1.253: δ3(dpr, dgt) ="
        },
        {
            "title": "1\nN",
            "content": "(cid:88) jN max (cid:33) (cid:32) dpr dgt , dgt dpr < 1.253, where dpr is predicted depth and dgt is groundtruth depth. Video object segmentation. This task involves propagating an object segmentation mask across video frames, given the ground truth mask for the first frame. Following prior evaluation protocols [17, 52], we compute dense feature affinity maps between frames to track objects. Performance is assessed using three metrics: Mean, Mean, and & Mean. Specifically, Mean denotes the average Intersection-over-Union (IoU) between predicted segmentations and groundtruth masks, while Mean represents the average F-score, measuring contour accuracy via precision and recall against groundtruth boundaries.We evaluate our method on the DAVIS validation set [39], popular benchmark for video object segmentation. The dataset comprises 30 videos of varying lengths, each containing between 1 and 4 objects. Zero-shot Open-Vocabulary Segmentation. We incorporate upsampled VFM features into ProxyCLIP [23], stateof-the-art method for zero-shot open-vocabulary segmentation (OVSeg), and evaluate on three popular OVSeg benchmarks: COCO [26], Cityscapes [4], and ADE20K [64]. ProxyCLIP enhances CLIP features by leveraging spatial feature correspondence from VFMs as proxy attention, effectively inheriting the strong local consistency of VFMs while retaining CLIPs remarkable zero-shot transferability. Due to the high computational cost of proxy attention, we perform upsampling to 8 for all upsampling methods. We use CLIP ViT-B/16 [41] as the CLIP backbone, DINOv2S/14 [37] as the proxy VFM, and set the input resolution to 336px, matching the resolution of the CLIP backbone. Interactive Segmentation. We adapt the SimpleClick [27] architecture to evaluate upsampled features. Specifically, we use frozen VFM backbone and train single-layer click encoder that directly adds to the image patch embedding, along with three-layer convolutional decoder head on top of the upsampled features for interactive segmentation. For training, we follow prior works [27, 51] and use the SBD dataset [14] to train for 20 epochs with the normalized focal loss [50, 51]. We employ the Adam optimizer [19] with learning rate of 5e-5 and batch size of 8. For evaluation, following common practice [21, 27, 51], we sample the first click point as the farthest point from the object boundary, and report the mean IoU of the predicted segmentation masks with the groundtruth, denoted as IoU@1 Click. We report results on three popular interactive segmentation benchmarks: GrabCut [48], Berkeley [32], and DAVIS [38]. B. More Quantitative Results"
        },
        {
            "title": "Image conv",
            "content": "# blocks # Train data COCO Cityscapes no learnable learnable Sine Sine"
        },
        {
            "title": "Sine\nSine\nSine",
            "content": "(a) (b) (c) 1x1 1x1 3x3 1x1 3x3 3x3 3x3 3x3 3x3 3x3 3x 2 2 2 2 2 1 2 3 2 2 2 50k 50k 50k 50k 50k 50k 50k 50k 50k 200k 1M 56.46 57.89 58.40 58.10 58.65 57.94 58.65 58.35 58.65 59.40 59.87 43.13 46.06 47.35 47.24 48.63 48.13 48.63 48.32 48.63 49.84 50. Table B.3. Ablation of architecture and training data size choices. Upsamplers are trained using only Stage 1 loss for convenience."
        },
        {
            "title": "Resolution Upsampler COCO Cityscapes",
            "content": ""
        },
        {
            "title": "NA\nBilinear\nFeatUp\nLoftUp",
            "content": "40.30 47.12 52.08 52.58 42.14 48.32 52.55 53.87 30.79 39.84 33.50 44.66 37.36 45.58 40.00 50.14 Table B.1. Comparison of feature upsampers when VFM is CLIP-B/16 [41]."
        },
        {
            "title": "Resolution Upsampler COCO Cityscapes",
            "content": ""
        },
        {
            "title": "NA\nBilinear\nFeatUp\nLoftUp",
            "content": "51.00 56.77 56.59 58.36 58.94 62.29 62.18 63.55 34.42 43.09 42.23 46.98 49.30 57.42 56.58 60.83 Table B.2. Comparison of feature upsampers when VFM is RADIOv2.5-B [43]. Upsampling CLIP and RADIO. In Tab. B.1 and Tab. B.2, we compare LoftUp with FeatUp and bilinear upsampling baseline using CLIP [41] and RADIO [43] as VFM backbones. The upsamplers are trained following the same procedure as on the DINOv2 backbone and evaluated at resolutions 224 and 448. As with DINOv2, LoftUp consistently outperforms all baselines when using CLIP and RADIO as VFM backbone, demonstrating the general applicability of our approach across different VFMs. Ablation on the architecture and training data size. In Tab. B.3, we conduct an ablation study on both the architecture components of LoftUp and the training data size. For convenience, the upsamplers are trained using only the Stage 1 training objective. Specifically, in experiment (a), we demonstrate that employing sinusoidal positional encoding for the low-resolution featurescombined with 3x3 convolutional layer to process the high-resolution coordinates and image inputsyields improved performance. This result is in line with prior work showing that sinusoidal positional encodings excel in coordinate-based methods [34, 60, 61] and that stronger image processing layers help better integrate high-resolution information. In experiment (b), we observe that two blocks of the cross-attention transformer are sufficient for optimal feature upsampling, with performance saturating at greater depths. Finally, in experiment (c), we find that training with larger dataset improves performance, although the benefits begin to diminish as the dataset size increases. Consequently, we select 1M-subset of the SA1B dataset [21] to achieve the best balance among data diversity, model performance, and training time. C. More Visualization We further provide more visualization examples of upsampled features of various methods in Fig. C.1, more prediction examples in semantic segmentation, depth estimation, and video object segmentation in Fig. C.2 and Fig. C.3, more examples of different pseduo-GT in Fig. C.4, and more examples of the attended regions of high-resolution pixel in Fig. C.5. (a) Original image (b) Low res (c) Bilinear (d) Resize-conv (e) LIIF (f) LiFT (g) FeatUp (h) LoftUp(Ours) Figure C.1. More visualization of features from various upsamplers. Backbone is DINOv2-S/14 [37]. (a) Original image (b) Bilinear (c) LiFT (d) FeatUp (e) LoftUp (f) Groundtruth Figure C.2. More visualization of predictions examples on semantic segmentation on COCO-Stuff [1] and depth estimation on NAVI [18]. (a) Original image (b) Bilinear (c) LiFT (d) FeatUp (e) LoftUp (f) Groundtruth Figure C.3. Visualization of prediction examples of video object segmentation on the DAVIS 2017 dataset [39]. Each image displays its corresponding frame number in the top right corner. The groundtruth segmentation for the 0-th frame is provided, and dense feature affinity maps are employed to propagate its segmentation labels to subsequent frames. We can see that LoftUp outperforms all the other baselines in accurately tracking the objects across the frames. (a) Original image (b) Per-image optimized (c) 2x features (LiFT) (d) Mask-Bicubic (e) Self-Distilled Figure C.4. More visualization of different pseudo-GT. Figure C.5. Visualization of attended region (in dots) in the low-res features of high-res pixel (in cross). The density of dots reflects the value of the attention map. LoftUp is able to use relevant information across the global feature map for upsampling features at each pixel."
        }
    ],
    "affiliations": [
        "Tubingen AI Center",
        "University of Tubingen"
    ]
}