{
    "paper_title": "STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion",
    "authors": [
        "Shui-Hsiang Hsu",
        "Tsung-Hsiang Chou",
        "Chen-Jui Yu",
        "Yao-Chung Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment particularly challenging. Recent methods such as QGpT attempt to enrich table semantics by generating synthetic queries, yet they still rely on coarse partial-table sampling and simple fusion strategies, which limit semantic diversity and hinder effective query-table alignment. We propose STAR (Semantic Table Representation), a lightweight framework that improves semantic table representation through semantic clustering and weighted fusion. STAR first applies header-aware K-means clustering to group semantically similar rows and selects representative centroid instances to construct a diverse partial table. It then generates cluster-specific synthetic queries to comprehensively cover the table's semantic space. Finally, STAR employs weighted fusion strategies to integrate table and query embeddings, enabling fine-grained semantic alignment. This design enables STAR to capture complementary information from structured and textual sources, improving the expressiveness of table representations. Experiments on five benchmarks show that STAR achieves consistently higher Recall than QGpT on all datasets, demonstrating the effectiveness of semantic clustering and adaptive weighted fusion for robust table representation. Our code is available at https://github.com/adsl135789/STAR."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 2 ] . [ 1 0 6 8 5 1 . 1 0 6 2 : r STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion Shui-Hsiang Hsu National Chung Hsing University Smart Sustainable New Agriculture Research Center (SMARTer) Taichung, Taiwan g113056055@smail.nchu.edu.tw Tsung-Hsiang Chou National Chung Hsing University Smart Sustainable New Agriculture Research Center (SMARTer) Taichung, Taiwan yumeow0122@smail.nchu.edu.tw Chen-Jui Yu National Chung Hsing University Smart Sustainable New Agriculture Research Center (SMARTer) Taichung, Taiwan rui0828@smail.nchu.edu.tw Yao-Chung Fan National Chung Hsing University Smart Sustainable New Agriculture Research Center (SMARTer) Taichung, Taiwan yfan@nchu.edu.tw Abstract Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment particularly challenging. Recent methods such as QGpT attempt to enrich table semantics by generating synthetic queries, yet they still rely on coarse partial-table sampling and simple fusion strategies, which limit semantic diversity and hinder effective querytable alignment. We propose STAR (Semantic Table Representation), lightweight framework that improves semantic table representation through semantic clustering and weighted fusion. STAR first applies headeraware K-means clustering to group semantically similar rows and selects representative centroid instances to construct diverse partial table. It then generates cluster-specific synthetic queries to comprehensively cover the tables semantic space. Finally, STAR employs weighted fusion strategies to integrate table and query embeddings, enabling fine-grained semantic alignment. This design enables STAR to capture complementary information from structured and textual sources, improving the expressiveness of table representations. Experiments on five benchmarks show that STAR achieves consistently higher Recall than QGpT on all datasets, demonstrating the effectiveness of semantic clustering and adaptive weighted fusion for robust table representation. Our code is available at https://github.com/adsl135789/STAR. CCS Concepts Information systems Document representation. Keywords Table Retrieval, Semantic Representation, Clustering Methods, Adaptive Fusion, Large Language Models"
        },
        {
            "title": "1 Introduction\nTable retrieval aims to identify the most relevant tables from a\ncorpus given a natural language query. However, a substantial\nsemantic gap exists between unstructured queries and structured",
            "content": "tables [3], limiting the effectiveness of dense retrieval methods. In practice, serialized tables frequently exceed encoder token limits, making querytable semantic matching more challenging. To address the semantic gap and long table scenarios, recent studies have attempted to address these issues through semantic augmentation. QGpT [7] adopts two-stage design: First, it selects the first ğ‘˜ rows of the table to form partial table, which serves as an approximation of the entire table under the token length restriction. Then, it uses an LLM to generate synthetic queries for the partial table, extending the structured content into natural language for easier semantic matching with queries. However, QGpT has two main limitations: (1) Heuristic instance selection: it uses simple top-ğ‘˜ strategy that lacks representativeness for the entire table; (2) Coarse representation fusion: it concatenates the partial table and synthetic queries into single sequence, making it difficult to control their relative contributions to the final table representation. To address these issues, we propose STAR (Semantic TAble Representation), lightweight framework for table retrieval that improves table representations without modifying the underlying retriever architecture. STAR captures table semantics by selecting representative instances and augmenting tables with synthetic queries, which are then integrated through fusion mechanism. Experimental results on multiple benchmarks demonstrate the effectiveness of the proposed approach. Concretely, our contributions are: We introduce STAR, lightweight framework for table retrieval that improves semantic table representations without modifying the retriever architecture. We propose clustering-based approach to select representative table content, improving semantic coverage beyond heuristic row sampling. We present fusion strategy that integrates tables with synthetic queries for improved querytable alignment. Experiments on multiple benchmark datasets demonstrate that STAR consistently outperforms baseline methods. Shui-Hsiang Hsu, Tsung-Hsiang Chou, Chen-Jui Yu, and Yao-Chung Fan Figure 1: Overview of the STAR framework compared to the QGpT baseline. STAR improves representation via two stages: (1) replacing top-ğ‘˜ sampling with Header-aware Clustering for diverse instance selection and query generation, and (2) replacing simple concatenation with Weighted Fusion to explicitly model the importance of structured data and synthetic queries. Different colors denote different semantic row clusters."
        },
        {
            "title": "2 Related Works\nEarly table retrieval methods primarily relied on sparse retrieval\ntechniques [11], using lexical matching to compute relevance but\nstruggling to capture deeper semantic relationships. With the ad-\nvancement of deep learning, dense retrieval methods [5] leveraging\nvector representations of text emerged for semantic matching and\nhave gradually been applied to table retrieval tasks. To better han-\ndle the structural characteristics of tables, some studies proposed\nstructure-aware encoding methods [2, 13], while recent research\nhas shifted towards query generation for semantic enhancement,\nusing generative models to produce pseudo-queries for improving\nretrieval performance [8, 12]. In the field of table retrieval, QGpT\n[7] is the first to systematically apply LLMs to generate synthetic\nqueries, enriching table semantic representations.",
            "content": "However, QGpT employs simple top-k sampling strategy, lacking semantic diversity, and directly concatenates tables and queries, failing to model the importance of different information sources at fine-grained level. Previous research has also highlighted that heterogeneous signals from tables and text should be modeled separately before fusion, rather than treated as simple sequence [14]. This paper ensures instance diversity through header-aware clustering [4] and introduces weighted fusion strategy for fine-grained semantic integration."
        },
        {
            "title": "3 Methodology\nGiven a table T with a header H and ğ‘› instances {r1, r2, . . . , rğ‘› },\nand a user query ğ‘, the goal of table retrieval is to retrieve the",
            "content": "most relevant tables from corpus based on ğ‘. The STAR framework consists of two stages: (1) Semantic Clustering and Query Generation (SCQG), and (2) Weighted Fusion (WF)."
        },
        {
            "title": "3.1.1 Header-aware K-means Clustering. Clustering solely\non instances ignores the table schema, potentially misgrouping\ninstances with different meanings. We thus integrate header se-\nmantics to capture the global context. Specifically, we first use a\npretrained encoder to encode the header H and each instance rğ‘– ,\nobtaining the header embedding eH and instance embedding erğ‘– :",
            "content": "eH = Encoder(H ) erğ‘– = Encoder(rğ‘– ) (1) (2) We then perform weighted fusion of the header embedding eH and the instance embeddings erğ‘– to form the header-aware instance embedding: eğ‘– = ğ›¼ eH + (1 ğ›¼) erğ‘– (3) where ğ›¼ is hyperparameter controlling the relative importance of the header and the instance, combining both instance semantics and header structural information. Next, we perform K-means clustering on the embeddings {e1, e2, . . . , eğ‘› } and group them into ğ‘˜ clusters {ğ¶1, ğ¶2, . . . , ğ¶ğ‘˜ }. For STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion each cluster ğ¶ ğ‘— , we select the instance closest to the cluster center as the representative: ğ‘— = arg min rğ‘– ğ¶ ğ‘— where ğ ğ‘— is the center of cluster ğ¶ ğ‘— . The resulting partial table Tğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘ğ‘™ = {r ğ‘˜ } preserves both semantic diversity and 1 representativeness. eğ‘– ğ ğ‘— 2 , . . . , , 2 (4)"
        },
        {
            "title": "3.1.2 Clustering-Guided Query Generation. For each cluster\nğ¶ ğ‘— , we construct a clustered table Tğ‘— = {H, r | r âˆˆ ğ¶ ğ‘— } and generate\na synthetic query ğ‘ ğ‘— using a large language model (LLM):",
            "content": "ğ‘ ğ‘— = LLM(Prompt(Tğ‘— )) (5) Thus, for each table , we generate ğ‘˜ synthetic queries {ğ‘1, ğ‘2, . . . , ğ‘ğ‘˜ }, each corresponding to specific semantic cluster. Compared to QGpTs top-k sampling strategy for generating queries from partial table, our cluster-specific generation ensures that the queries cover the diverse aspects represented by the tables different semantic clusters, acting as semantic bridge between the user query and the table."
        },
        {
            "title": "3.2 Weighted Fusion\nAs shown in Figure 1 (Stage 2), QGpT adopts a simple concate-\nnation strategy, where the partial table and synthetic queries are\nconcatenated and passed into a single encoder to obtain a unified\nrepresentation. This approach mixes different sources of informa-\ntion without modeling the semantic importance of each component.\nIn contrast, we propose a weighted fusion strategy, where the table\nand queries are encoded separately and then fused with weights.",
            "content": "Specifically, we first encode the partial table and the concatenation of all synthetic queries: etable = Encoder(Tğ‘ğ‘ğ‘Ÿğ‘¡ğ‘–ğ‘ğ‘™ ) equeries = Encoder(ğ‘1 ğ‘2 ğ‘ğ‘˜ ) (6) (7) The final table representation is obtained through weighted fusion: eT = ğ‘¤ğ‘¡ etable + ğ‘¤ğ‘ equeries (8) where ğ‘¤ğ‘¡ and ğ‘¤ğ‘ are the weights for the table and queries, respectively, and ğ‘¤ğ‘¡ + ğ‘¤ğ‘ = 1. We propose two weight allocation strategies:"
        },
        {
            "title": "3.2.2 Dynamic Weight Fusion (DWF). In contrast, this method\ndynamically adapts the weights based on the semantic similarity\nbetween the table and queries. We calculate the cosine similarity:\nğ‘  = cos(etable, equeries)",
            "content": "(9) The weights are then determined by the similarity: ğ‘¤ğ‘ = ğ›½ ğ‘ , ğ‘¤ğ‘¡ = 1 ğ‘¤ğ‘ (10) where ğ›½ is scaling factor that controls the impact of similarity changes on the weights."
        },
        {
            "title": "4.1.2 Baselines. We compare with QGpT[7], which generates\nsynthetic queries using LLM for table retrieval.",
            "content": "Implementation. To ensure fair comparison, we follow 4.1.3 the original experimental setup of QGpT. We use BGE-M3 as the encoder and Llama 3.1 8B-Instruct to generate synthetic queries. The number of clusters ğ‘˜ is set to 10, so each table generates 10 queries. In header-aware K-means clustering, we set ğ›¼ = 0.2. To explore the impact of different weight configurations, we test multiple values of ğœ† for Fixed Weight Fusion, with ğœ† {0.1, 0.2, . . . , 0.9}; for Dynamic Weight Fusion, we set ğ›½ = 0.5. All experiments are evaluated using Recall@K (ğ¾ {1, 5, 10}) as the evaluation metric."
        },
        {
            "title": "4.2 Main Experimental Results\nTable 1 presents a comparison of STAR and QGpT across five\ndatasets, along with the performance of different weighted fusion\nstrategies. We can observe the following points:",
            "content": "STAR significantly outperforms QGpT. The STAR framework outperforms QGpT under all fusion strategies. For example, with DWF, STARs average R@1 improves by 6.39 percentage points, and R@5 also increases by 6.06 percentage points. This shows that the semantic clustering and weighted fusion strategies proposed in STAR are effective at capturing the semantics of tables and improving retrieval performance. Different weights in FWF reflect dataset characteristics. Analyzing the performance of different weights (ğœ†) in FWF, we observe that different datasets have varying preferences for query weight. The Mimo (ch) and Mimo (en) datasets perform best when ğœ† is between 0.4 and 0.5. This is likely because the Mimo tables originate from real-world Excel files, which have higher structural complexity. In this case, synthetic queries serve as more effective semantic bridge between user queries and complex tables. In contrast, the OTTQA, FetaQA, and E2E-WTQ datasets achieve the best FWF performance at lower values of ğœ† (e.g., 0.3 or 0.4). These datasets contain simpler table structures, where the tables inherent semantics are already clear. Thus, giving higher weight to the table itself leads to better retrieval performance. DWF demonstrates strong adaptability and potential. DWF demonstrates strong adaptability and runs robustly across different datasets. DWF performs best on OTTQA, FetaQA, and E2EWTQ, while also achieving competitive results on the both Mimo datasets. This indicates that DWF can effectively balance the semantic sources of different tables through dynamic weighting without requiring manual tuning. In the future, this could be replaced with more powerful learnable module for end-to-end adaptive weighting. Table 1: Performance comparison on five datasets with different fusion strategies. Bold: best, underlined: second-best. Method Mimo (ch) Mimo (en) OTTQA FetaQA E2E-WTQ Avg. R@ R@5 R@10 R@1 R@5 R@10 R@ R@5 R@10 R@1 R@5 R@10 R@ R@5 R@10 R@1 R@5 R@10 Shui-Hsiang Hsu, Tsung-Hsiang Chou, Chen-Jui Yu, and Yao-Chung Fan 49.81 71.06 77.23 QGpT 50.66 STAR w/ FWF (ğœ† : weight of query embeddings) 53.56 56.89 58.40 58.79 59.10 58.79 56.53 55.75 54.97 ğœ† = 0.1 ğœ† = 0.2 ğœ† = 0.3 ğœ† = 0.4 ğœ† = 0.5 ğœ† = 0.6 ğœ† = 0.7 ğœ† = 0.8 ğœ† = 0.9 69.06 71.79 72.16 71.95 71.49 71.26 69.52 68.66 67. 47.14 49.58 51.36 52.26 51.56 50.18 48.94 47.12 45.32 76.99 77.54 78.08 77.73 76.82 77.39 77.10 76.26 74.84 72.35 80.80 51.45 78. 86.68 33.95 50.87 57.86 41.49 65. 72.61 45.47 67.88 75.04 74.43 76.14 76.98 77.56 77.30 77.17 76.09 75.00 73.21 80.93 81.92 82.50 82.66 83.26 83.02 83.02 81.34 80. 52.21 53.52 53.84 53.88 53.79 53.16 51.90 50.32 48.46 78.77 80.08 80.17 79.95 79.81 78.55 77.60 76.15 74.25 86.86 87.53 88.17 88.53 87.58 86.86 85.46 84.28 82.70 34.55 35.40 36.00 35.90 35.75 34.90 34.10 32.65 31.60 53.37 54.22 54.92 55.02 54.37 54.12 52.92 50.72 48.88 62.01 62.51 62.21 62.11 61.76 61.16 60.51 59.11 57. 55.60 56.85 58.51 56.85 56.43 54.77 54.77 53.94 52.70 85.48 85.06 85.89 85.06 83.40 82.16 82.16 81.33 80.08 89.63 89.63 90.04 90.04 89.63 89.63 87.14 85.89 85.48 48.61 50.45 51.62 51.54 51.33 50.36 49.25 47.96 46.61 72.22 73.46 74.02 73.91 73.27 72.65 71.66 70.37 68.76 79.28 79.83 80.20 80.21 79.81 79.61 78.65 77.38 76. STAR w/ DWF 51.58 72.15 77.99 58.89 77. 82.89 54.07 79.99 88.08 36.25 54. 62.21 58.51 85.06 90.06 51.86 73. 80."
        },
        {
            "title": "5 Analysis\n5.1 Ablation Study of STAR Components\nTo verify the effectiveness of each component of STAR, we con-\nducted ablation experiments. Table 2 shows the average Recall\nmetrics across five datasets. Specifically, we evaluate the following\nfour model variants:",
            "content": "STAR (full): Full model with SCQG and Dynamic Weight Fusion. w/o SCQG: SCQG removed; tables are represented using top-ğ‘˜ row sampling and direct query generation. w/o WF: Removing the weighted fusion mechanism and encoding tables and synthetic queries via simple concatenation. w/o Header-aware: Removing header information from clustering and performing K-means using only instance embeddings. Based on these variants, the experimental results reveal the following observations: Importance of the SCQG stage. Among all components, SCQG contributes the most to overall performance. Removing SCQG leads to substantial drop in average R@1 from 51.86% to 47.07% (4.79), indicating that semantic clustering plays critical role in selecting representative instances and enabling diverse, cluster-aware query generation. Effect of Weighted Fusion. Compared to SCQG, weighted fusion provides complementary but smaller gain. When replacing weighted fusion with simple concatenation (w/o WF), the average R@1 decreases by 2.78 points, suggesting that explicitly modeling the relative importance of structured tables and synthetic queries is beneficial for fine-grained semantic alignment. Role of Header-aware Clustering. Incorporating header semantics yields consistent improvement.Removing header information from clustering results in 1.29-point drop in R@1, indicating that header-aware embeddings help stabilize clustering quality by providing global schema context, even when instance semantics dominate. Table 2: Ablation Study of STAR Components (avg. across five datasets)."
        },
        {
            "title": "Method",
            "content": "STAR (full) R@1 51.86 R@5 73.94 R@ 80.25 w/o SCQG w/o WF w/o Header-aware 47.07 (-4.79) 49.08 (-2.78) 50.57 (-1.29) 68.43 (-5.51) 69.69 (-4.25) 73.63 (-0.31) 75.88 (-4.37) 77.02 (-3.23) 79.92 (-0.33)"
        },
        {
            "title": "6 Conclusion\nThis paper addresses the heuristic sampling and coarse fusion is-\nsues in existing table retrieval methods by proposing the STAR\nframework. STAR improves table representations through two\nstages: Semantic Clustering and Query Generation, which applies\nheader-aware K-means clustering to select diverse instances and\ngenerate cluster-specific synthetic queries, and Weighted Fusion,\nwhich enables fine-grained integration of structured tables and\nsynthetic queries. Experiments on five benchmark datasets show\nthat STAR consistently outperforms QGpT, achieving an average\nR@1 improvement of 6.39 percentage points across all datasets.\nThese results demonstrate the effectiveness of combining semantic\nclustering with adaptive fusion for robust table representation.",
            "content": "Despite its consistent improvements, STAR has several limitations. First, the clustering-based design relies on informative table headers; tables with vague or missing headers may benefit less from header-aware clustering. Second, generating cluster-specific synthetic queries introduces additional computational overhead compared to QGpT, which may affect efficiency in large-scale retrieval settings. Future work will explore more efficient instance selection strategies, such as lightweight clustering or adaptive row sampling, to reduce computational cost. In addition, we plan to investigate endto-end learnable weighting mechanisms to automatically optimize fusion weights based on table and query characteristics. STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion"
        },
        {
            "title": "Prompt",
            "content": "You are an expert at analyzing tables and generating diverse, natural query that could be answered using the table data. ### Input Table: {clustered_table} ### Your Task: Generate **one query** based on the actual content and structure of this table. ### Query Types (choose the most appropriate): - Numerical: \"What is the average Sales in Region X?\" - List: \"List all Products with Price above 100\" - Count: \"How many Orders have Status = shipped?\" - Select: \"Which Employee has the highest Revenue?\" ### Important Requirements: - Use **natural, conversational language** - Make the query **specific to the actual content** in the table - Reference real values, names, or entities that appear in the table when possible - fact-verification entity-specific and temporal queries - For reasoning-oriented tables, include multi-step or conditional queries - **Language code: {lang}**: Generate the query in this language tables, style focus"
        },
        {
            "title": "For",
            "content": "on ### Output Format (JSON only): { \"query\": \"your query here\" } Generate the query now: Acknowledgments This research was supported (in part) by NSTC 114-2634-F-005002 - project Smart Sustainable New Agriculture Research Center (SMARTer). [2] References [1] Wenhu Chen, Ming-Wei Chang, Eva Schlinger, William Yang Wang, and William W. Cohen. 2021. Open Question Answering over Tables and Text. In International Conference on Learning Representations. Jonathan Herzig, Pawel Krzysztof Nowak, Thomas Mueller, Francesco Piccinno, and Julian Eisenschlos. 2020. TaPas: Weakly Supervised Table Parsing via Pre-training. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 43204333. Junjie Huang, Wanjun Zhong, Qian Liu, Ming Gong, Daxin Jiang, and Nan Duan. 2022. Mixed-modality Representation Learning and Pre-training for Joint Table-and-Text Retrieval in OpenQA. In EMNLP (Findings). [3] [4] Woojun Jung and Susik Yoon. 2025. HAETAE: In-domain Table Pretraining with Header Anchoring. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval. [5] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 67696781. [6] Zheng Li, Yang Du, Mao Zheng, and Mingyang Song. 2025. MiMoTable: Multiscale Spreadsheet Benchmark with Meta Operations for Table Reasoning. In Proceedings of the 31st International Conference on Computational Linguistics. [7] Hsing-Ping Liang, Che-Wei Chang, and Yao-Chung Fan. 2025. Improving Table Retrieval with Question Generation from Partial Tables. In Proceedings of the 4th Table Representation Learning Workshop. 217228. [8] Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2021. Generation-Augmented Retrieval for OpenDomain Question Answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 4089 4100. [9] Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech KryÅ›ciÅ„ski, Hailey Schoelkopf, Riley Kong, Xiangru Tang, Mutethia Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob Cunningham, Caiming Xiong, Dragomir Radev, and Dragomir Radev. 2022. FeTaQA: Free-form Table Question Answering. Transactions of the Association for Computational Linguistics 10 (2022), 3549. [10] Feifei Pan, Mustafa Canim, Michael Glass, Alfio Gliozzo, and Peter Fox. 2021. CLTR: An End-to-End, Transformer-Based System for Cell-Level Table Retrieval and Table Question Answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. 202209. [11] Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends in Information Retrieval 3 (2009), 333389. [12] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022. GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 23452360. [13] Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. 2020. TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 84138426. [14] Vicky Zayats, Kristina Toutanova, and Mari Ostendorf. 2021. Representations for Question Answering from Documents with Tables and Text. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. Prompt for Pseudo Query Generation The following prompt is used to generate pseudo query for each clustered table. For each cluster ğ¶ ğ‘— , the clustered table Tğ‘— is formatted and passed to the LLM with this prompt to generate one synthetic query ğ‘ ğ‘— ."
        }
    ],
    "affiliations": [
        "National Chung Hsing University",
        "Smart Sustainable New Agriculture Research Center (SMARTer)"
    ]
}