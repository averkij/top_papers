{
    "paper_title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
    "authors": [
        "Karthik Dharmarajan",
        "Wenlong Huang",
        "Jiajun Wu",
        "Li Fei-Fei",
        "Ruohan Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/."
        },
        {
            "title": "Start",
            "content": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow Karthik Dharmarajan, Wenlong Huang, Jiajun Wu, Li Fei-Fei*, Ruohan Zhang* Stanford University 5 2 0 2 1 3 ] . [ 1 6 6 7 4 2 . 2 1 5 2 : r Fig. 1: Dream2Flow leverages off-the-shelf video generation models to produce videos of the task being performed in the same scene of the robot. Dream2Flow then extracts 3D object flow from the motion in the video, allowing for downstream planning and execution with robot across wide variety of tasks. Abstract Generative video modeling has emerged as compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categoriesincluding rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/. I. INTRODUCTION Robotic manipulation in the open world could greatly benefit from visual world models that predict how an environment would evolve given an agents interactions. Recent advances in generative video modeling have produced systems capable of zero-shot synthesizing minute-long, highfidelity clips of physical interactions in pixel space, conditioned on an unseen initial image and an open-ended task instruction [1]. Such video models implicitly capture intuitive physics and rich priors of object properties and interactions, making them compelling for open-world manipulation settings where robot is tasked to complete novel tasks in unseen environments through partial observations. Despite their promise, it remains unclear what role such models should serve in robot manipulation system. Most frontier video generators produce the best interaction clips with human embodiment. This reflects where supervision is most abundant as human interactions are significantly more *Equal Advising. Correspondence: Wenlong Huang broadly documented than those of robots. But this becomes challenge for using them in robotic manipulation due to the embodiment gap and hence the different action spaces. We propose extracting actionable signals from their visual predictions of human interactions, which will then be enacted by robot. This essentially separates the state changes in the real world from the actuators that realize those changes. Our proposed method, Dream2Flow, employs 3D object flow as an intermediate interface that bridges high-level video simulation with low-level robot actions. Dream2Flow works because, despite occasional visual artifacts, state-of-the-art video generation models often predict physically plausible object motions that align with the task intent in open-world manipulation tasks. Then, given generated videos, instead of trying to directly mimic the human motions for completing given task, we focus on reconstructing and reproducing the object flows in 3D. the video model The problem is thus reduced to object trajectory tracking: the robots job is to manipulate the object to closely follow imagined. This the generated flow that approach cleanly separates what needs to happen (i.e., state changes in an environment) from how particular embodiment achieves it with respect to its kinematic and dynamic constraints (i.e., actions). Importantly, it seamlessly interfaces with both motion planners and sensorimotor policies the extracted object motion in 3D serves as the tracking goal for trajectory optimization or reinforcement learning policy, which then yields sequence of low-level robot joint commands. Leveraging off-the-shelf models and tools, we demonstrate an autonomous pipeline that 1) generates text-conditioned video of plausible interaction [1], 2) obtains 3D object flow by performing depth estimation and point tracking using vision foundation models [2, 3], and 3) synthesizes robot actions that realize this flow using trajectory optimization and reinforcement learning. Notably, this design enjoys number of desirable benefits in manipulation tasks. By first leveraging video modelspre-trained on large corpora of human activitiesto interpret and ground open-ended language commands in visual predictions, our system inherits scalable mechanism for task specification. These predictions are then distilled into reconstructed 3D object flow, representation that naturally captures diverse object interactions spanning rigid, articulated, deformable, and granular objects. Together, this synergy enables an end-to-end pipeline that performs open-world manipulation directly from visual perception and language, without task-specific data or training. In summary, our key contributions are: We propose 3D object flow as an interface for adapting off-the-shelf video generation models for open-world manipulation by formulating it as an object trajectory tracking problem. We demonstrate its effectiveness by implementing the approach in both simulated and real domains, which performs diverse tasks given only RGB-D observations and language instructions in zero-shot manner. We examine the properties of 3D object flow by comparing it with alternative intermediate representations and by studying its key design choices as well as generalization properties. II. RELATED WORKS A. Task Specification in Manipulation Specifying tasks for the wide range of manipulation problems spans symbolic, learning-based, outcome-driven, and object-centric interfaces. Classical approaches encode goals and constraints with symbolic formalisms such as logics, or optimize cost-augmented PDDL and temporal formulations [46]. Learning-based systems specify tasks often through language and perception, mapping instructions to actions via language-conditioned visuomotor policies and visionlanguageaction models [711]. Outcomebased specification sets goals by example observations, e.g., image goals with goal-conditioned policies [1217], and some works incorporate force targets [18]. Object-centric alternatives rely on descriptors or keypoints to capture taskrelevant structure [19]. Recently, foundation models enable higher-level interfaces that compile intent into actionable specifications via code [20], 3D value maps akin to potential fields [21], keypoint relations [22], or affordance maps [23]. B. 2D/3D Flow in Robotics Dense motion fieldsoptical flow, point tracks, and 3D scene/object flowprovide an embodiment-agnostic, midlevel interface for manipulation [24, 25]. In scene-centric formulations, policies parameterize or condition on motion in 2D or 3D to decide actions, with point/keypoint interfaces unifying perception and action and with action-flow improving precision [2632]. In object-centric formulations, desired object motion is specified independently of embodiment and then converted into actions via policy inference, planning, and optimization [3338]. When actions are absent, retargeting with predicted tracks and dense correspondences offers practical bridge across embodiments [3941]. Advances in perception and tracking such as RGB-D motion-based segmentation, 3D scene flow in point clouds, zero-shot monocular and ToF-based scene flow, rigid-motion learning, refractive flow for transparent objects, deformable and articulated reconstruction, and structured scene representations [4252] make the interface reliable. Flow-derived supervision further supports learning, and video generation supplies plausible visual rollouts for planning and imitation [5356]. Our approach follows the object-centric path by reconstructing 3D object flow from language-conditioned generations and tracking it under embodiment constraints, complementing other flow-conditioned policy representations [26, 28, 3335]. C. Video Models for Robotics Recent work increasingly integrates video models across robotic tasks in various ways [57, 58]. They can serve as auxiliary training objectives [5964], as reward models [65 67], as policies [68, 69], or as simulator for the environments [70, 71]. Notably, predictive modeling in robotics can leverage video frame prediction as form of world model. By simulating future visual observations, these models can enable visual planning and manipulation by anticipating how the environment will evolve. For example, video generative model can simulate long-horizon task outcomes, effectively acting as visual planner [72]. Video models in this way can also serve as as dynamics models [69, 7377]. Another notable direction is that video generation can directly provide new training data for robot learning. Several recent works devise frameworks to imagine new trajectories in the form of videos and use them to train or finetune policies, particularly for imitation learning [78, 79]. III. METHOD introduce Herein, we the problem formulation of Dream2Flow in Sec. III-A. Leveraging 3D object flow as an interface, we subsequently discuss how to extract 3D object flow from video generations in Sec. III-B and how to plan actions with 3D object flow for manipulation in Sec. III-C. A. Problem Formulation Given task instruction ℓ, an initial RGB-D observation (I0 RHW3, D0 RHW ), and known camera projection Π (intrinsics and extrinsics to the robot frame), our goal is to output an action sequence u0:H1 that accomplishes the task by following an object motion inferred from generated video. We make no assumption about specific action parameterization: may represent motion primitives, end-effector poses, or low-level controls. Extracting 3D Object Flow. From (I0, ℓ), an image-tovideo model produces frames {Vt}T t=1, and video-depth estimator provides per-frame depth sequence {Zt}T t=1. Using binary mask of the task-relevant object, and the projection Π, we lift masked image points with Z1:T to obtain an object-centric 3D trajectory P1:T RT n3 in the robot frame; we refer to P1:T as the 3D object flow. Action Inference with 3D Object Flow. We represent state as the task-relevant object and the robot: xt = (xobj , rt), where xobj Rn3 are object points and rt denotes the robot state. Let be dynamics model and ˆxt+1 = (ˆxt, ut) with ˆx0 = x0. At each planning step t, we use time-aligned target Pt Rn3 derived from the video object flow (e.g., via uniform time-warping or nearest-shape matching). We formulate action inference as an optimization problem: min {utU } H1 (cid:88) t=0 λtask (cid:0)ˆxobj , Pt (cid:1) + λcontrol(ˆxt, ut) s.t. ˆxt+1 = (ˆxt, ut), ˆx0 = x0, λtask (cid:0)ˆxobj , Pt (cid:1) = (cid:88) i=1 (cid:13) (cid:13)ˆxobj [i] Pt[i](cid:13) 2 2, (cid:13) Section III-C instantiates and for different domains. B. Extracting 3D Object Flows from Videos Video Generation: Given the task language instruction ℓ and an RGB image of the workspace without the robot visible I0 RHW 3, Dream2Flow uses an off-the-shelf image-tovideo generation model to produce an RGB video {Vt}T t=1 with Vt RHW 3, showing the task being performed. We do not include the robot in the initial frame or mention the robot in the text prompt, as we empirically find that current image-to-video generation models not specifically finetuned on robotics data tend to produce physically implausible finegrained interactions, and consequently have worse object trajectories (for details see Appendix A). Video Depth Estimation: Given the generated video, Dream2Flow leverages SpatialTrackerV2 [80, 81] to estimate per-frame depth { Zt}T t=1, Zt RHW . Due to the scale-shift ambiguity of monocular video, we compute global (s, b) by aligning the first frame to the initial depth D0 from the robot, and obtain calibrated depths Zt = Zt + b. 3D Object Flow Extraction: 3D object flow aims to produce 3D trajectories P1:T RT n3 with visibilities {0, 1}T for the task-relevant object. We first localize the relevant object using Grounding DINO [82] to produce bounding box from (I0, ℓ), and then use the box to prompt SAM 2 [2] for binary mask. From the masked region at t=1, we sample pixels and track them across the video with CoTracker3 [3] to obtain 2D trajectories ct and visibilities vt . Visible points are lifted to 3D using the calibrated depths and camera intrinsics/extrinsics, producing P1:T as in Sec. III-A. C. Action Inference with 3D Object Flow tasks Simulated Push-T Domain. For involving non-prehensile manipulation, such as the Push-T task, Dream2Flow uses push skill primitive parameterized with the start push position on flat table (cx, cy), unit push direction (cx, cy), and the distance of the push d. In this setting, we learn forward dynamics model that takes as input feature-augmented particles xt RN 14 of the whole scene and produces ˆxt+1, the delta of positions of each point for the next timestep. The features associated with each point consist of the position, RGB color, and normal vector as determined from camera observations along with the push parameters (as further elaborated in Appendix B). To optimize the 3D object flow following cost with the learned dynamics model, we use random-shooting, where push skill parameters are randomly sampled such that all pushes will make contact with the object of interest at different points and directions. Then, we select the push skill parameter out of ones that has the least cost according to the predicted point positions from the dynamics model. For determining which timestep from the video should be used in the cost function, we find the timestep where the points of the relevant object in the trajectory are closest to the current observed points with further details in Appendix C. Real-World Domain. We use absolute end-effector poses as the action space and rigid-grasp dynamics model for the real robot domain. With this combination, we first proceed to grasp the desired part from the relevant object, and then use the dynamics model and point-flow following objective to move the end-effector such that the grasped part moves in fashion similar to the video. We use AnyGrasp [83] to propose candidate grasps on the object of interest, but these Fig. 2: An overview of Dream2Flow. Given task instruction and an initial RGB-D observation, an image-to-video model synthesizes video frames conditioned on the instruction. We additionally obtain object masks, video depth, and point tracking from vision foundation models, which are used to reconstruct 3D object flow. Finally, robot policy generates executable actions that track the 3D object flow using trajectory optimization or reinforcement learning. grasps may not lie exactly on the desired part. We select the grasp closest to the thumb as detected from the video by HaMer [84], as we observe that in generated videos, the hand tends to interact with the relevant part of an object, such as the handle. The rigid-grasp dynamics model assumes that the grasped part is rigid; consequently, the prediction of point positions to the next timestep can be described by sequence of rigid transformations for the grasped subset, while nongrasped points remain unchanged. To produce trajectory of end-effector poses, Dream2Flow directly optimizes the objective using PyRoki [85], incorporating pose smoothness and reachability costs as λcontrol in addition to the 3D objectflow following cost λtask as in Appendix F. Simulated Door Opening Domain. For the Door Opening task, we use reinforcement learning to learn sensimotor policy which moves the object according to the 3D object flow with SAC [86]. This approach can be viewed as using the simulator as dynamics model for compiling the optimization process prescribed in Eq. III-A into parametric policy in an offline fashion, using 3D object flow as the reward function. Depending on the embodiment used, the action space consists of delta end-effector poses and delta joint angles for the gripper or dexterous hand. The reward function used consists of one term which encourages the end-effector to move toward the current mean object particle position along with another term for encouraging matching , where is the timestep with the the 3D object flow, closest particle positions to the 3D object flow and tend is the last timestep of the object motion (further details in Appendix H). By training policies with such an object-centric reward, we observe different strategies emerge to accomplish the same object motion across embodiments, including quadruped manipulators, humanoids with dexterous hands, and fixed-base arms with parallel grippers. tend IV. EXPERIMENTS"
        },
        {
            "title": "We seek to answer",
            "content": "the following research questions through our experiments: Q1: What are properties of 3D object flow when used as an interface to bridge videos and robot control? Q2: How does Dream2Flow perform compared to alternative interfaces? Q3: How effective is 3D object flow as reward for learning sensimotor policies? Q4: How does the choice of video model affect Dream2Flow in simulation and in real-world tasks? Q5: How does the choice of dynamics model affect the performance of Dream2Flow? A. Tasks For evaluation, we consider several tasks involving different types of objects and manipulation strategies  (Fig. 3)  . 1) Push-T: We develop simulated Push-T task in OmniGibson [87], where T-shaped block is placed with random position and yaw angle on the wooden platform. It is success if the T-block ends up within 2cm translation and 15 degrees rotation from the goal position, where the T-shaped block is in the center of the board facing forwards. task consists of fake piece of bread and green bowl placed randomly on the workspace. trial is success if the bread is inside the bowl at the end. 2) Put Bread in Bowl: The Put Bread in Bowl 3) Open Oven: In the Open Oven task, the toaster oven is randomly placed in semi-circular arc with the orientation towards the base of the robot. trial is considered success if the opening angle is at least 60 degrees. 4) Cover Bowl: The Cover Bowl task starts with folded scarf placed at random position and orientation on the workspace, with blue bowl placed next to it. The trial is success if the scarf is covering at least 25% of the top of the bowl after the robot finishes execution. 5) Open Door: In the Open Door task from Robosuite [88], door is placed at random positions and orientations on top of table. Rotating the handle and pulling the door open by at least 17 without timing out is success. B. Properties of 3D Object Flow as Video-Control Interface We run Dream2Flow on the simulated Push-T task with Wan2.1 [89] as the video generation model. For this task only, we allow prompting with goal image of the T-block in the center, similar to the example final state in Fig. 3, as this task requires accurate movement. We consider 10 different initial states with the same final state. We then proceed to run 10 trials for each initial configuration on different seeds to properly evaluate performance due to the stochastic nature of random shooting, yielding 100 total trials. The result Fig. 5: Multiple tasks in the same scene. With different language goals, Dream2Flow adapts object-flow targets to produce distinct behaviors in the same environment. appears in Table IV. We note that in this task, 6 generated videos included substantial morphing of the T-block, which consequently ruined the tracking and downstream execution. We evaluate Dream2Flows ability to perform manipulation with different types of objects with Veo 3 [90] as the video generation model in the real world for 10 trials per task, and report the results in Table I. To assess Dream2Flows generalization to different object instances, backgrounds, and viewing angles, we conduct an additional five trials each for six different scenarios, as shown in Fig. 4. Compared to the reference setting, with exception of putting large piece of bread, there is not significant drop off in performance, suggesting that Dream2Flow inherits some generalization through the use of video generation models. We additionally show that Dream2Flow can perform different tasks from the same scene in Fig. 5 due to the ability of video generation models to follow different task instructions given the same input image. We have additional case studies demonstrating that 3D object flow can be used for downstream manipulation in in-the-wild settings for tasks such as pulling chair, opening drawer, sweeping pasta, and recycling can, as shown in Fig. 1 and Appendix G. For all 60 trials of Dream2Flow executed in the real world, we provide breakdown of failures in Fig. 7. In the 12 video generation failures, for half the time, the generated video either morphs an object in an implausible way or hallucinates new objects, causing tracking to unreasonably fail or making the robot move an object to an incorrect 3D location. The four flow extraction failures occurred because of severe rotations or objects temporarily going out of view of the camera, leading to tracks with no visibility in the end. The four robot execution failures occurred in the Bowl Covering task, where the robot either did not grasp at the correct point or did not move enough. C. How does Dream2Flow perform compared to alternative interfaces? For the three tasks in the real world, we consider the following alternative interfaces related to extracting object Fig. 3: Evaluation Tasks. (a) The initial states of each task used in the evaluation trials. (b) For one of the initial states, the corresponding final state after the robot performs the task. For Push-T only, the desired final state is the same for all initial states. Fig. 4: Robustness evaluations. Relative performance across instance, background, and task variations, showing Dream2Flow remains robust under various different settings. Task AVDC RIGVID Dream2Flow Reward Type Franka Spot GR1 Bread in Bowl Open Oven Cover Bowl 7/10 0/10 2/10 6/10 6/10 1/10 8/10 8/10 3/10 TABLE I: Comparisons of intermediate representations on real robot. Dream2Flow outperforms AVDC and RIGVID across three tasks by following 3D object flow rather than rigid transforms alone. trajectories from videos: 1) AVDC: AVDC [40] leverages generated videos by computing dense optical flow between frames to track points on rigid object. Then, using the initial depth and the point correspondences, it solves for sequence of rigid transforms of the relevant object, allowing for trajectory playback after grasp has occurred. In our implementation, we utilize the video depth corresponding to the tracks from the optical flow, and optimize for rigid transforms relative to the initial frame, as we empirically found that to be less noisy than the original optimization procedure. 2) RIGVID: RIGVID [56] uses 6D object pose tracking to generate rigid object trajectory from generated video. Since it is ill-defined to have such pose for deformable objects, we do not use 6D pose tracker, but instead adapt their approach by solving for rigid pose transformation between the initial 3D points and visible 3D points from our 3D Object Flow computation in the same manner as AVDC. We present the results in Table I. While AVDC does reasonable job at tracking the bread, the dense optical flow does not keep up with the motion of the oven, resulting in insufficient motion. Under certain circumstances, there are only few visible points for RIGVID and AVDC, making transform estimation noisy. Attempting to follow the noisy transform trajectories leads to execution failures or optimization instability. Dream2Flow is less affected by this issue, because there is typically not heavy cost when most of the points are occluded, allowing the planned endeffector poses to smoothly move between areas of high point visibility. The Cover Bowl task remains challenge for AVDC and RIGVID, as in addition to video generation and tracking failures, the transform estimates are incorrect since the points are now on deformable object. Object State 3D Object Flow 99/100 100/100 99/100 100/100 96/100 94/100 TABLE II: Comparison of policies trained using different rewards. The policies trained using the 3D object flow reward perform comparably to those trained with the object state reward across different embodiments. Fig. 6: Rollouts from policies trained using 3D object flow as reward. Different embodiments such as the (a) Panda, (b) Spot, or (c) GR1 use different strategies to open the door. The Spot is able move its base for better reachability while the GR1 uses the area between its fingers and palm to pull for better stability. the Spot is able to move its base for better reachability and kinematic range, and the GR1 uses the area between the fingers and palm for pulling the door as opposed to individual fingers for better stability. E. How does the choice of video model affect Dream2Flow in simulation and in real-world tasks? Video Generation Model Push-T Open Oven Wan2.1 [89] Kling 2.1 Veo 3 [90] 52/100 31/100 - 2/10 4/10 8/10 D. How effective is 3D object flow as reward for learning sensimotor policies? The 3D object flow extracted by Dream2Flow can be used in an RL reward for training policies across different embodiments. We evaluate SAC [86] policies trained using handcrafted object state reward from Robosuite and 3D object flow rewards on Franka Panda, Spot with floating base, and GR1 with the right arm only across 100 random door positions, and report the results in Table II with corresponding episode visualizations in Figure 6. The policies trained with the object state and 3D object flow reward have comparable performance across all the embodiments. The learned strategies are different between the embodiments, as TABLE III: Effect of video generator. Veo 3 excels on real-world domains such as Open Oven, while Wan 2.1 performs better on simulated domains such as Push-T. To evaluate how well different video generation models capture physically plausible object trajectories, we run Dream2Flow on the simulated Push-T task and the real Open Oven task using three models: Wan2.1 [89], Kling 2.1, and Veo 3 [90]. The results are shown in Table III. Note that there are no results for Veo 3 on Push-T because at the time of evaluation, Veo 3 did not support prompting with goal image. For the Push-T task, Kling 2.1 had more videos that had substantial morphing, throwing off the tracking, resulting in Dynamics Model Type Success Rate Pose Heuristic Particle 12/100 17/100 52/ TABLE IV: Dynamics model ablation. Particle dynamics substantially outperform pose and heuristic models, highlighting the importance of per-point predictions. Fig. 7: Failure breakdown on real-robot experiments. Common causes include video artifacts (object morphing, object hallucination), tracking errors, and grasp selection mismatches. more downstream failures. For the Open Oven task, Wan2.1 tends to produce more videos with substantial camera motion, which violates the still camera assumption. Additionally, both Kling 2.1 and Wan2.1 produce videos where the direction of articulation is incorrect, such as revolving around the wrong axis, leading to far more failures than Veo 3. F. How does the choice of dynamics model affect the performance of Dream2Flow? For the Push-T task, we compare the effect of different dynamics models used for planning. In addition to the particle-based dynamics model, we consider another learned dynamics model which takes in the block pose and push skill parameters and predicts the delta pose of the T-block, trained on the same data as the particle based dynamics model, as well as heuristic dynamics model, which translates the points of the T-block in the direction and amount of the push without any rotation. We present the results in Table IV, and find that the particle representation for this dynamics model is crucial to ensure success, as despite having the same 3D object flow guidance, the pose and heuristic based dynamics models could not sufficiently account for the rotation needed. V. CONCLUSION We presented Dream2Flow, simple, general interface that turns text-conditioned video predictions into executable robotic actions by reconstructing and tracking 3D object flow. This decouples what should happen in the world (task-relevant object motion and state change) from how particular embodiment realizes it under kinematic, dynamic, and morphology constraints. Built entirely from off-the-shelf video generation and perception tools, Dream2Flow solves open-world manipulation tasks in simulation and on real robots across rigid, articulated, deformable, and granular objects using only RGB-D observations and language, without task-specific demonstrations. Experiments show consistent gains over trajectory baselines derived from dense optical flow or rigid pose transforms, robustness to variations in instances, backgrounds, and viewpoints, and the importance of both the upstream video model and downstream dynamics choice (with particle dynamics proving most reliable). failure analysis highlights current bottlenecks, such as video artifacts (morphing, hallucinations), occlusion-induced tracking dropouts, and grasp selection mismatches as concrete directions for improvement. Overall, our results indicate that 3D object flow is scalable bridge from open-ended video generation to robot control in unstructured environments."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work is in part supported by the Stanford Institute for Human-Centered AI (HAI), the Schmidt Futures Senior Fellows grant, ONR MURI N00014-21-1-2801, ONR MURI N00014-22-1-2740."
        },
        {
            "title": "REFERENCES",
            "content": "[1] T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman et al., Video generation models as world simulators, OpenAI Blog, vol. 1, no. 8, p. 1, 2024. [2] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. Radle, C. Rolland, L. Gustafson, E. Mintun, J. Pan, K. V. Alwala, N. Carion, C.-Y. Wu, R. Girshick, P. Dollar, and C. Feichtenhofer, Sam 2: Segment anything in images and videos, arXiv preprint arXiv:2408.00714, 2024. [Online]. Available: https://arxiv.org/abs/2408.00714 [3] N. Karaev, I. Makarov, J. Wang, N. Neverova, A. Vedaldi, and C. Rupprecht, CoTracker3: Simpler and better point tracking by pseudo-labelling real videos, arxiv, 2024. [4] C. R. Garrett, R. Chitnis, R. Holladay, B. Kim, T. Silver, L. P. Kaelbling, and T. Lozano-Perez, Integrated task and motion planning, Annual review of control, robotics, and autonomous systems, vol. 4, no. 1, pp. 265293, 2021. [5] Z. Zhao, S. Cheng, Y. Ding, Z. Zhou, S. Zhang, D. Xu, and Y. Zhao, survey of optimization-based task and motion planning: From classical to learning approaches, IEEE/ASME Transactions on Mechatronics, 2024. [6] M. Toussaint, Logic-geometric programming: An optimization-based approach to combined task and motion planning. in IJCAI, 2015, pp. 19301936. [7] M. Shridhar, L. Manuelli, and D. Fox, Cliport: What and where pathways for robotic manipulation, in Conference on robot learning. PMLR, 2022, pp. 894906. [8] , Perceiver-actor: multi-task transformer for robotic manipulation, in Conference on Robot Learning. PMLR, 2023, pp. 785799. [9] B. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid et al., Rt-2: Vision-language-action models transfer web knowledge to robotic control, in Conference on Robot Learning. PMLR, 2023, pp. 21652183. [10] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter et al., pi0: visionlanguage-action flow model for general robot control, arXiv preprint arXiv:2410.24164, 2024. [11] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. P. Foster, P. R. Sanketi, Q. Vuong et al., Openvla: An open-source vision-language-action model, in Conference on Robot Learning. PMLR, 2025, pp. 26792713. [12] C. Lynch, M. Khansari, T. Xiao, V. Kumar, J. Tompson, S. Levine, and P. Sermanet, Learning latent plans from play, in Conference on robot learning. Pmlr, 2020, pp. 11131132. [13] F. Ebert, C. Finn, S. Dasari, A. Xie, A. Lee, and S. Levine, Visual foresight: Model-based deep reinforcement learning for vision-based robotic control, arXiv preprint arXiv:1812.00568, 2018. [14] K. Black, M. Nakamoto, P. Atreya, H. Walke, C. Finn, A. Kumar, and S. Levine, Zero-shot robotic manipulation with pretrained imageediting diffusion models, arXiv preprint arXiv:2310.10639, 2023. [15] A. Xie, A. Singh, S. Levine, and C. Finn, Few-shot goal inference for visuomotor learning and planning, in Conference on Robot Learning. PMLR, 2018, pp. 4052. [16] P. Sharma, D. Pathak, and A. Gupta, Third-person visual imitation learning via decoupled hierarchical controller, Advances in Neural Information Processing Systems, vol. 32, 2019. [17] R. Mendonca, O. Rybkin, K. Daniilidis, D. Hafner, and D. Pathak, Discovering and achieving goals via world models, Advances in Neural Information Processing Systems, vol. 34, pp. 24 37924 391, 2021. [18] A. Adeniji, Z. Chen, V. Liu, V. Pattabiraman, R. Bhirangi, S. Haldar, P. Abbeel, and L. Pinto, Feel the force: Contact-driven learning from humans, arXiv preprint arXiv:2506.01944, 2025. [19] A. Simeonov, Y. Du, A. Tagliasacchi, J. B. Tenenbaum, A. Rodriguez, P. Agrawal, and V. Sitzmann, Neural descriptor fields: Se (3)- equivariant object representations for manipulation, in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 63946400. [20] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng, Code as policies: Language model programs for embodied control, in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 94939500. [21] W. Huang, C. Wang, R. Zhang, Y. Li, J. Wu, and L. Fei-Fei, Voxposer: Composable 3d value maps for robotic manipulation with language models, Proceedings of Machine Learning Research, vol. 229, 2023. [22] W. Huang, C. Wang, Y. Li, R. Zhang, and L. Fei-Fei, Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation, in Conference on Robot Learning. PMLR, 2025, pp. 45734602. [23] Y. Tang, W. Huang, Y. Wang, C. Li, R. Yuan, R. Zhang, J. Wu, and L. Fei-Fei, Uad: Unsupervised affordance distillation for generalization in robotic manipulation, arXiv preprint arXiv:2506.09284, 2025. [24] M. Xu, Z. Xu, Y. Xu, C. Chi, G. Wetzstein, M. Veloso, and S. Song, Flow as the cross-domain manipulation interface, in Conference on Robot Learning. PMLR, 2025, pp. 24752499. [25] C. Yuan, C. Wen, T. Zhang, and Y. Gao, General flow as foundation learning, in Conference on Robot affordance for scalable robot Learning. PMLR, 2025, pp. 15411566. [26] T. Weng, S. Bajracharya, Y. Wang, K. Agrawal, and D. Held, Fabricflownet: Bimanual cloth manipulation with flow-based policy, arXiv preprint arXiv:2111.05623, 2021. [27] A. Goyal, A. Mousavian, C. Paxton, Y.-W. Chao, B. Okorn, J. Deng, and D. Fox, Ifor: Iterative flow minimization for robotic object rearrangement, arXiv preprint arXiv:2202.00732, 2022. [28] D. Seita, Y. Wang, S. J. Shetty, E. Y. Li, Z. Erickson, and D. Held, Toolflownet: Robotic manipulation with tools via predicting tool flow from point clouds, arXiv preprint arXiv:2211.09006, 2022. [29] T. Chen, Y. Mu, Z. Liang, Z. Chen, S. Peng, Q. Chen, M. Xu, R. Hu, H. Zhang, X. Li, and P. Luo, G3flow: Generative 3d semantic flow for pose-aware and generalizable object manipulation, arXiv preprint arXiv:2411.18369, 2024. [30] S. Wang, J. You, Y. Hu, J. Li, and Y. Gao, Skil: Semantic keypoint imitation learning for generalizable data-efficient manipulation, in Robotics: Science and Systems (RSS), 2025. [31] S. Haldar and L. Pinto, Point policy: Unifying observations and actions with key points for robot manipulation, arXiv preprint arXiv:2502.20391, 2025. [32] S. Guo, X. Liang, J. Lin, Y. Zhuang, L. Lin, and X. Liang, Actionsink: Toward precise robot manipulation with dynamic integration of action flow, arXiv preprint arXiv:2508.03218, 2025. [33] B. Eisner, H. Zhang, and D. Held, Flowbot3d: Learning 3d articulation flow to manipulate articulated objects, in Robotics: Science and Systems (RSS), 2022. [34] C. Gao, H. Zhang, Z. Xu, Z. Cai, and L. Shao, Flip: Flow-centric generative planning as general-purpose manipulation world model, arXiv preprint arXiv:2412.08261, 2024. [35] J. Guo, X. Ma, Y. Wang, M. Yang, H. Liu, and Q. Li, Flowdreamer: rgb-d world model with flow-based motion representations for robot manipulation, arXiv preprint arXiv:2505.10075, 2025. [36] H. Zhi, P. Chen, S. Zhou, Y. Dong, Q. Wu, L. Han, and M. Tan, 3dflowaction: Learning cross-embodiment manipulation from 3d flow world model, arXiv preprint arXiv:2506.06199, 2025. [37] Y. He and Q. Nie, Manitrend: Bridging future generation and action prediction with 3d flow for robotic manipulation, arXiv preprint arXiv:2502.10028, 2025. [38] Z.-H. Yin, S. Yang, and P. Abbeel, Object-centric 3d motion field for robot learning from human videos, arXiv preprint arXiv:2506.04227, 2025. [39] H. Bharadhwaj, R. Mottaghi, A. Gupta, and S. Tulsiani, Track2act: Predicting point tracks from internet videos enables generalizable robot manipulation, in European Conference on Computer Vision. Springer, 2024, pp. 306324. [40] P.-C. Ko, J. Mao, Y. Du, S.-H. Sun, and J. B. Tenenbaum, Learning to act from actionless videos through dense correspondences, arXiv preprint arXiv:2310.08576, 2023. [41] Y. Chen, P. Li, Y. Huang, J. Yang, K. Chen, and L. Wang, Ec-flow: Enabling versatile robotic manipulation from action-unlabeled videos via embodiment-centric flow, arXiv preprint arXiv:2507.06224, 2025. [42] L. Shao, P. Shah, V. Dwaracherla, and J. Bohg, Motion-based object segmentation based on dense rgb-d scene flow, arXiv preprint arXiv:1804.05195, 2018. [43] X. Liu, C. R. Qi, and L. J. Guibas, Flownet3d: Learning scene flow in 3d point clouds, arXiv preprint arXiv:1806.01411, 2018. [44] Zero-shot monocular scene flow estimation in the wild, CVPR 2025, CVF Open Access, 2025. [45] Zeromsf: Zero-shot monocular scene flow in the wild, NVIDIA Research Project Page, 2025. [46] J. Sander, G. Caroleo, A. Albini, and P. Maiolino, Estimating scene flow in robot surroundings with distributed miniaturized time-of-flight sensors, arXiv preprint arXiv:2504.02439, 2025. [47] A. Byravan and D. Fox, Se3-nets: Learning rigid body motion using deep neural networks, arXiv preprint arXiv:1606.02378, 2016. [48] T. Tang, J. Liu, J. Zhang, H. Fu, W. Xu, and C. Lu, Rftrans: Leveraging refractive flow of transparent objects for surface normal estimation and manipulation, arXiv preprint arXiv:2311.12398, 2023. [49] B. P. Duisterhof, Z. Mandi, Y. Yao, J.-W. Liu, J. Seidenschwarz, M. Z. Shou, D. Ramanan, S. Song, S. Birchfield, B. Wen, and J. Ichnowski, Deformgs: Scene flow in highly deformable scenes for deformable object manipulation, arXiv preprint arXiv:2312.00583, 2024. [50] J. Kerr, C. M. Kim, M. Wu, B. Yi, Q. Wang, K. Goldberg, and A. Kanazawa, Robot see robot do: Imitating articulated object manipulation with monocular 4d reconstruction, arXiv preprint arXiv:2409.18121, 2024. [51] H. Jiang, B. Huang, R. Wu, Z. Li, S. Garg, H. Nayyeri, S. Wang, and Y. Li, Roboexp: Action-conditioned scene graph via interactive exploration for robotic manipulation, arXiv preprint arXiv:2402.15487, 2024. [52] O. Shorinwa, J. Tucker, A. Smith, A. Swann, T. Chen, R. Firoozi, M. Kennedy III, and M. Schwager, Splat-mover: Multi-stage, openvocabulary robotic manipulation via editable gaussian splatting, arXiv preprint arXiv:2405.04378, 2024. [53] I. Guzey, Y. Dai, G. Savva, R. Bhirangi, and L. Pinto, Bridging the human to robot dexterity gap through object-oriented rewards, in 2025 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2025, pp. 33443351. [54] K. Yu, S. Zhang, H. Soora, F. Huang, H. Huang, P. Tokekar, and R. Gao, Genflowrl: Shaping rewards with generative objectlearning, arXiv preprint centric flow in visual reinforcement arXiv:2508.11049, 2025. [55] H. Bharadhwaj, D. Dwibedi, A. Gupta, S. Tulsiani, C. Doersch, T. Xiao, D. Shah, F. Xia et al., Gen2act: Human video generation in novel scenarios enables generalizable robot manipulation, arXiv preprint arXiv:2409.16283, 2024. [56] S. Patel, S. Mohan, H. Mai, U. Jain, S. Lazebnik, and Y. Li, Robotic manipulation by imitating generated videos without physical demonstrations, arXiv preprint arXiv:2507.00990, 2025. [57] S. Yang, J. Walker, J. Parker-Holder, Y. Du, J. Bruce, A. Barreto, P. Abbeel, and D. Schuurmans, Video as the new language for realworld decision making, arXiv preprint arXiv:2402.17139, 2024. [58] R. McCarthy, D. C. Tan, D. Schmidt, F. Acero, N. Herr, Y. Du, T. G. Thuruthel, and Z. Li, Towards generalist robot learning from internet video: survey, Journal of Artificial Intelligence Research, vol. 83, 2025. grounded pre-training for open-set object detection, arXiv preprint arXiv:2303.05499, 2023. [83] H.-S. Fang, C. Wang, H. Fang, M. Gou, J. Liu, H. Yan, W. Liu, Y. Xie, and C. Lu, Anygrasp: Robust and efficient grasp perception in spatial and temporal domains, IEEE Transactions on Robotics (T-RO), 2023. [84] G. Pavlakos, D. Shan, I. Radosavovic, A. Kanazawa, D. Fouhey, and J. Malik, Reconstructing hands in 3D with transformers, in CVPR, 2024. [85] C. M. Kim*, B. Yi*, H. Choi, Y. Ma, K. Goldberg, and A. Kanazawa, Pyroki: modular toolkit for robot kinematic optimization, in 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025. [Online]. Available: https://arxiv.org/abs/2505.03728 [86] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor, in Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, vol. 80. PMLR, 1015 Jul 2018, pp. 18611870. [87] C. Li, R. Zhang, J. Wong, C. Gokmen, S. Srivastava, and et al., Behavior-1k: human-centered, embodied ai benchmark with 1,000 everyday activities and realistic simulation, arXiv preprint arXiv:2403.09227, 2024. [88] Y. Zhu, J. Wong, A. Mandlekar, R. Martın-Martın, A. Joshi, K. Lin, S. Nasiriany, and Y. Zhu, robosuite: modular simulation learning, in arXiv preprint framework and benchmark for robot arXiv:2009.12293, 2020. [89] T. Wan, A. Wang, B. Ai, B. Wen, C. Mao, and et al., Wan: Open and advanced large-scale video generative models, arXiv preprint arXiv:2503.20314, 2025. [90] Google DeepMind, [Online]. Available: https://storage.googleapis.com/deepmind-media/veo/ Veo-3-Tech-Report.pdf Veo-3 Technical Report, 2025. [91] X. Wu, L. Jiang, P.-S. Wang, Z. Liu, X. Liu, Y. Qiao, W. Ouyang, T. He, and H. Zhao, Point transformer v3: Simpler, faster, stronger, in CVPR, 2024. [92] E. Coumans and Y. Bai, Pybullet, python module for physics simulation for games, robotics and machine learning, http://pybullet.org, 20162021. [93] Y. Zhu, A. Joshi, P. Stone, and Y. Zhu, Viola: Imitation learning for vision-based manipulation with object proposal priors, arXiv preprint arXiv:2210.11339, 2022. [59] H. Wu, Y. Jing, C. Cheang, G. Chen, J. Xu, X. Li, M. Liu, H. Li, and T. Kong, Unleashing large-scale video generative pre-training for visual robot manipulation, arXiv preprint arXiv:2312.13139, 2023. [60] Y. Seo, K. Lee, S. L. James, and P. Abbeel, Reinforcement learning with action-free pre-training from videos, in International Conference on Machine Learning. PMLR, 2022, pp. 19 56119 579. [61] J. Wu, H. Ma, C. Deng, and M. Long, Pre-training contextualized world models with in-the-wild videos for reinforcement learning, Advances in Neural Information Processing Systems, vol. 36, pp. 39 71939 743, 2023. [62] J. Yang, B. Liu, J. Fu, B. Pan, G. Wu, and L. Wang, Spatiotemporal predictive pre-training for robotic motor control, arXiv preprint arXiv:2403.05304, 2024. [63] Y. Hu, Y. Guo, P. Wang, X. Chen, Y.-J. Wang, J. Zhang, K. Sreenath, C. Lu, and J. Chen, Video prediction policy: generalist robot policy with predictive visual representations, arXiv preprint arXiv:2412.14803, 2024. [64] S. Li, Y. Gao, D. Sadigh, and S. Song, Unified video action model, arXiv preprint arXiv:2503.00200, 2025. [65] T. Huang, G. Jiang, Y. Ze, and H. Xu, Diffusion reward: Learning rewards via conditional video diffusion, in European Conference on Computer Vision. Springer, 2024, pp. 478495. [66] A. Escontrela, A. Adeniji, W. Yan, A. Jain, X. B. Peng, K. Goldberg, Y. Lee, D. Hafner, and P. Abbeel, Video prediction models as rewards for reinforcement learning, Advances in Neural Information Processing Systems, vol. 36, pp. 68 76068 783, 2023. [67] A. S. Chen, S. Nair, and C. Finn, Learning generalizable robotic reward functions from in-the-wild human videos, arXiv preprint arXiv:2103.16817, 2021. [68] A. Ajay, S. Han, Y. Du, S. Li, A. Gupta, T. Jaakkola, J. Tenenbaum, L. Kaelbling, A. Srivastava, and P. Agrawal, Compositional foundation models for hierarchical planning, Advances in Neural Information Processing Systems, vol. 36, pp. 22 30422 325, 2023. [69] Y. Du, S. Yang, B. Dai, H. Dai, O. Nachum, J. Tenenbaum, D. Schuurmans, and P. Abbeel, Learning universal policies via text-guided video generation, Advances in neural information processing systems, vol. 36, pp. 91569172, 2023. [70] D. Valevski, Y. Leviathan, M. Arar, and S. Fruchter, Diffusion models are real-time game engines, arXiv preprint arXiv:2408.14837, 2024. [71] J. Bruce, M. D. Dennis, A. Edwards, J. Parker-Holder, Y. Shi, E. Hughes, M. Lai, A. Mavalankar, R. Steigerwald, C. Apps et al., Genie: Generative interactive environments, in Forty-first International Conference on Machine Learning, 2024. [72] Y. Luo and Y. Du, Grounding video models to actions through goal conditioned exploration, arXiv preprint arXiv:2411.07223, 2024. [73] M. Yang, Y. Du, K. Ghasemipour, J. Tompson, D. Schuurmans, and P. Abbeel, Learning interactive real-world simulators, arXiv preprint arXiv:2310.06114, vol. 1, no. 2, p. 6, 2023. [74] S. Zhou, Y. Du, J. Chen, Y. Li, D.-Y. Yeung, and C. Gan, Robodreamer: Learning compositional world models for robot imagination, arXiv preprint arXiv:2404.12377, 2024. [75] O. Rybkin, K. Pertsch, K. G. Derpanis, K. Daniilidis, and A. Jaegle, Learning what you can do before doing anything, arXiv preprint arXiv:1806.09655, 2018. [76] R. Mendonca, S. Bahl, and D. Pathak, Structured world models from human videos, arXiv preprint arXiv:2308.10901, 2023. [77] H. Che, X. He, Q. Liu, C. Jin, and H. Chen, Gamegen-x: Interactive open-world game video generation, arXiv preprint arXiv:2411.00769, 2024. [78] J. Jang, S. Ye, Z. Lin, J. Xiang, J. Bjorck, Y. Fang, F. Hu, S. Huang, K. Kundalia, Y.-C. Lin et al., Dreamgen: Unlocking generalization in robot learning through neural trajectories, arXiv e-prints, pp. arXiv 2505, 2025. [79] J. Liang, R. Liu, E. Ozguroglu, S. Sudhakar, A. Dave, P. Tokmakov, S. Song, and C. Vondrick, Dreamitate: Real-world visuomotor policy learning via video generation, arXiv preprint arXiv:2406.16862, 2024. [80] Y. Xiao, J. Wang, N. Xue, N. Karaev, I. Makarov, B. Kang, X. Zhu, H. Bao, Y. Shen, and X. Zhou, Spatialtrackerv2: 3d point tracking made easy, in ICCV, 2025. [81] L. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, and H. Zhao, Depth anything v2, Advances in Neural Information Processing Systems, vol. 37, pp. 21 87521 911, 2024. [82] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang, H. Su, J. Zhu et al., Grounding dino: Marrying dino with"
        },
        {
            "title": "APPENDIX",
            "content": "A. Video Generation Prompts For all real world tasks, the prompt to each video generation model consists of the initial RGB observation from one camera, as well as language instruction of the form:"
        },
        {
            "title": "Real World Task Language Prompt",
            "content": "<TASK> by one hand. The camera holds still pose, not zooming in or out. Values of <TASK>: Put Bread in Bowl: The bread is grabbed and placed into the green bowl Open Oven: The toaster oven is opened Cover Bowl: The scarf is lifted by corner and directly dragged over the blue bowl in one smooth motion without flinging the cloth or any dynamic / fast motions Pull Out Chair: The chair is pulled out straight from under the table to the right by grabbing the middle Open Drawer: The partially opened drawer is opened all the way out Sweep Pasta: The brush with green handle moves left to right to push the pasta into the compost bin Recycle Can: The can is grabbed and dropped into the recycling bin For the robustness evaluations, the word bread is replaced with donut or long piece of bread for the different object instances, but otherwise the prompt remains the same. Since Dream2Flow leverages the position of hand in the video to help select the grasp on the object, the by one hand phrase must be included. Additionally, the camera holds still pose must be included to increase the probability that the generated videos do not have substantial camera motion, as the depth estimation pipeline assumes still camera. Push-T Task Language Prompt The T-shaped block slides and rotates smoothly to the center of the wooden platform. Fig. 8: Example Push-T image prompt. To generate sufficiently accurate motions for the Push-T task, the visual component of the prompt includes both start and end frame. For Push-T, since the task success requires accurate positions and to provide better hint as to where the block should go, we also provide goal image of the T-block, as shown in Figure 8. At the time of evaluation, Veo 3 did not Fig. 9: Particle dynamics model. The particle dynamics model primarily composed of Point Transformer V3 backbone used in the Push-T task takes as input set of featureaugmented particles and outputs delta position predictions for all particles in the scene. have the ability to take in an end frame, and hence was not considered for Push-T experiments."
        },
        {
            "title": "Open Door Task Language Prompt",
            "content": "The door opens all the way by itself to the right. The camera holds still pose, not zooming in or out."
        },
        {
            "title": "The Open Door task prompt does not include a hand to",
            "content": "perform the action as it is in simulated environment. All prompts discussed in this section are identical for each video generation model evaluated. For Kling 2.1, relevance value of 0.7 and negative prompt of fast motion, morphing, camera motion are added. B. Particle Dynamics Model The particle dynamics model used in the Push-T task takes as input feature-augmented particles xt RN 14, consisting of the position, RGB value, normal vector, and push parameters and produces ˆxt+1, the delta of positions of each point for the next timestep, as shown in Figure 9. The architecture of the particle dynamics model is small Point Transformer V3 [91] backbone surrounded by MLPs to project between the desired input and output sizes. To get the input set of particles from the scene, there are 4 virtual cameras around the workspace, whose RGB-D observations are combined into single point cloud. Then, points outside of the bounds of the wooden platform that the T-shaped block is on are discarded. Finally, this point cloud is voxel downsampled by randomly keeping only 1 particle per each 1.5cm sided cube. The same push parameter is appended to each particles features. To train the particle dynamics model, we collect 500 transitions of random pushing actions. In this setting, we track particle positions before and after the push by using all objects poses from the simulator for efficiency, but in practice this can also be tracked with CoTrackerV3 [3] and depth. C. Push-T Planning Details To optimize the particle trajectory following cost with the learned particle based dynamics model, Dream2Flow replans after every single push with random shooting until the T-block is within the specified tolerance to the goal or maximum number of pushes have been executed. In this setting, at each replanning step, push skill parameters are randomly sampled such that all pushes will make contact with the object of interest at different points and directions. Then, Dream2Flow selects the push skill parameter out of those ones that has the least cost according to the predicted particle positions from the dynamics model with respect to subgoal of particle positions (may not necessarily be the final goal position). While the T-block is being pushed, Dream2Flow uses the online version of CoTrackerV3 [3] to track its motion, and once the push is completed, the tracked points are lifted into 3D, forming the updated particles of the T-block. We find that while parts of the T-block may be occluded during pushing action, CoTrackerV3 [3] tends to recover visibility of previously occluded points when the gripper lifts up. Since Dream2Flow tracks particles for the T-block while the particle dynamics model takes in downsampled particles of the entire scene, Dream2Flow performs nearest neighbor matching to find correspondences between the currently tracked particles and the feature-augmented particles which are the input to the dynamics model. With these correspondences, and the predicted delta positions of these corresponding particles, Dream2Flow computes the predicted particle positions of the T-block as ˆxt+1 = ˆxt + ˆxt+1. For determining which timestep from the video should be used in the cost function as subgoal, Dream2Flow first finds the timestep where the tracked particles are closest to the particles in the 3D object flow, as single push can encompass the motion of many timesteps. Dream2Flow then chooses the particles from the timestep min(t + L, tend) as the next subgoal for planning, where is fixed look ahead time amount, and tend is the final timestep of the video. In our experiments, we used = 20. We choose to use intermediate subgoals to be the target for random shooting as opposed to only the final particle positions of the T-block, as for cases involving substantial rotation, only having the final positions can result in the T-block have small translation error but large rotation error, eventually lead to timeouts. Fig. 10: Grasp selection with thumb position. Dream2Flow selects grasp closest to the position of the thumb in the video, if such grasp exists within 2cm. The red sphere indicates the thumb position predicted by HaMer, with the red grasp being the selected one. D. Grasp Selection Dream2Flow uses AnyGrasp [83] to propose set of up to 40 top-down grasps after applying the mask to the object of interest. Up to 20 grasps come from point cloud in the coordinate frame of the camera, and another 20 grasps come from transforming those points into the coordinate frame of virtual camera looking straight down in the center of the workspace. We added this addition virtual camera so that some more vertical grasps could be proposed. While for rigid objects consisting of one part, such as piece of bread, it is typically fine to grasp at any stable position, for articulated objects, the part that moves needs to be grasped instead. To perform grasp selection in such cases, we exploit video generation models ability to synthesize plausible hand-object interactions, typically where the hand first grabs the relevant part of the object and then proceeds to move it. Dream2Flow uses HaMer [84] to detect the position of the hand, and in particular the thumb. If the predicted thumb position comes within 2cm of proposed grasp, then such grasp at the earliest timestep is chosen. An example of grasp selection with this method is shown in Figure 10. It is possible that there are no such grasps detected, either because the grasp planner proposes grasps that are not near the hand in the video or the detections from HaMer [84] are not accurate enough. In such cases, Dream2Flow defaults to heuristic of selecting the closest grasp to the points on the movable part of the rigid object (obtaining points on the movable part is described in the next section). E. Movable Part Flow Filtering for Rigid-Grasp Dynamics Model Since the rigid-grasp dynamics model assumes that points that are grasped move with the end-effector, it is necessary to identify what points are part of the movable object. To determine this, we employ the heuristic that individual flows Fig. 11: In-the-Wild Task Rollouts. The Franka successfully pulls out chair, opens partially opened drawer, sweeps pasta into the compost bin, and recycles an aluminum can. that move at least 1 pixel on average per timestep are part of the movable flow. We empirically chose the 1 pixel threshold, as we noticed that points tracked on the stationary parts of articulated objects such as the oven had low averages of how many pixels they moved. For the 2D tracks associated with the movable part, it may be possible that for certain intermediate frames they are 1 pixel off, making the individual tracks belong to the background or floor when lifted into 3D, causing part of the 3D object flow to be dramatically incorrect, leading to downstream execution failures (such as trying to push the toaster ovens door in the direction of the hinge). To remedy this issue, we utilize SAM 2 [2] with positive point prompts in the initial frame for the movable part and negative point prompts for the non-movable part, and use the part mask over each consecutive frame to constrain the 2D flow (if any tracked point is outside of the mask, it is considered to be invalid). F. Real World Planning Details The optimization problem with rigid-grasp assumption in the real world follows the same formulation as in Sec. III-A, where we optimize robot joint angles R7 to minimize: H1 (cid:88) t= λtask (cid:0)ˆxobj , Pt (cid:1) + λcontrol(ˆxt, ut) (1) The control cost λcontrol(ˆxt, ut) is expanded into three components: λcontrol(ˆxt, ut) = wrCr(qt) + wsCs(qt, qt1) + wmCm(qt) (2) where: Cr(qt): Reachability cost penalizing joint configurations outside the robots workspace Cs(qt, qt1): Pose smoothness cost measuring the difference between consecutive end-effector poses after running forward kinematics Cm(qt): Manipulability cost encouraging configurations with good manipulability The weight parameters are set to wr = 100, ws = 1, and wm = 0.01 to encourage the robot to make smooth motions within its joint limits. The task cost λtask uses weight of wf = 10 and follows the same formulation as in Sec. IIIA. The optimized end-effector poses after running forward kinematics are then fit with B-spline and sampled such that each sampled pose is at least 1cm away from the previous and next pose and/or has rotation difference of at least 20 degrees. To execute this planned trajectory, we use the IK solver from PyBullet [92] to get target joint angles and then joint impedance controller from Deoxys [93] commands the Franka to reach those positions. G. In-the-Wild Tasks For In-the-Wild tasks, we consider: 1) Pull Out Chair: In the Pull Out Chair task, black rolling chair is placed underneath table. trial is considered success if the chair is moved at least 5cm horizontally from its initial position. The reason for the limited motion requirement is that in certain configurations, it is difficult for the Franka robot to push the chair. In the Sweep Pasta task, 2) Open Drawer: The Open Drawer task involves opening partially opened drawer out to at least 90% of its full possible extension. 3) Sweep Pasta: there is brush with green handle placed against wooden support structure, along with four pieces of dried pasta next to compost bin. The objective is for the robot to grasp the handle of the brush and use the brush to push the pasta into the compost bin. If all pieces of pasta are inside of the compost bin, it is considered success. 4) Recycle Can: In the Recycle Can task, an aluminum can is placed in between recycling and trash bin while upright. trial is successful if the can is inside of the recycling bin. See Fig. 11 for rollouts for each of these in-the-wild tasks. H. Open Door Reinforcement Learning Details For the Open Door task, we utilize Soft Actor-Critic (SAC) [86] to train sensorimotor policies across three different embodiments: Franka Panda, Spot robot, and GR1 humanoid arm. The policies are trained to follow the 3D object flow extracted from generated videos, which serves as reward signal. 1) Hyperparameters: The hyperparameters used for SAC training are detailed in Table V. These values were consistent across all reward types and embodiments with exception of the GR1, which uses 10000 training iterations due to the larger action space. I. Video Generation Failures Hyperparameter Learning rate Discount factor (γ) Batch size Buffer size Target update rate (τ ) Target network update freq Hidden layers Hidden units per layer Training iterations Episode horizon Value 3 104 0.99 256 106 0.005 1 2 256 5000 500 TABLE V: SAC Hyperparameters for Open Door Task. 2) Reward Functions: We compare policies trained with two different reward formulations: handcrafted object state reward and 3D object flow reward. Object State Reward: The handcrafted reward Rstate consists of reaching component rreach and handle rotation component rrot: rreach = 0.25 (1 tanh(10 dgripper, handle)) (cid:19) (cid:18) rrot = clip 0.25 , 0.25, 0.25 θhandle 0.5π (3) (4) where dgripper, handle is the L2 distance between the robots end-effector and the door handle, and θhandle is the rotation angle of the handle. The total reward is rreach + rrot, unless the door hinge angle θhinge > 0.3 radians, in which case completion reward of 1.0 is returned. 3D Object Flow Reward: The 3D object flow reward Rflow leverages the reference trajectory P1:T extracted from the video. It consists of particle tracking term rparticle and an end-effector alignment term ree: rparticle = 0.75 tend (5) where is the index of the closest timestep in the reference trajectory P1:T based on the current object particle positions ˆxobj in the door frame: = argmin t{1...tend} 1 (cid:88) ˆxobj [i] Pt[i] (6) i=1 The end-effector term ree encourages the robot to stay near the object particles: ree = 0.25 (1 tanh(10 eedoor x2)) (7) where eedoor is the end-effector position in the door body frame and = 1 [i] is the mean position of the object particles. The total reward is Rflow = rparticle + ree. i=1 ˆxobj (cid:80)n Instead of tracking the mean particle position with CoTrackerV3, we proceed to use simpler approach of transforming the initial particles as the door angle changes for better computation efficiency. We thus consider the door joint angle to be part of the state xt. Fig. 12: Video Generation Failure Examples. (a) The bread undergoes object morphing, where it turns into stack of crackers, causing the downstream tracking to fail. (b) Another green bowl appears due to model hallucination, causing downstream execution failure where the bread is dropped onto the surface of the workspace instead of the original green bowl. From the generated videos, we observe that there are two common failure modes: morphing and hallucination. Object morphing occurs when an existing object in the scene dramatically changes shape to something else, such as another object or an object with significantly different geometric properties than what it should be. Hallucination occurs when new object (previously non-existent) appears in the scene. We show examples of morphing and hallucination for the Put Bread task in Fig. 12. J. Limitations Dream2Flow has several limitations. First, it relies on rigid-grasp assumption for real world manipulation, limiting the types of tasks that can be performed. While this work shows that particle dynamics model can be used for other types of tasks such as non-prehensile pushing, training and scaling particle dynamics model for the real world is non-trivial and can be considered for future work. Another limitation is that the total processing time to get 3D object flow depending on the video generation model is between 3 and 11 minutes, which limits its usability, with the main bottleneck being the video generation. Since Dream2Flow relies upon one angle in generated video, it cannot handle heavy occlusions gracefully, such as when the human hand covers the majority of small object. Future work may consider methods which can deal with such occlusions better, such as 3D point trackers [80] or full 4D representations."
        }
    ],
    "affiliations": [
        "Stanford University"
    ]
}