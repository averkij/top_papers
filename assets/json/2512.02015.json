{
    "paper_title": "Generative Video Motion Editing with 3D Point Tracks",
    "authors": [
        "Yao-Chih Lee",
        "Zhoutong Zhang",
        "Jiahui Huang",
        "Jui-Hsien Wang",
        "Joon-Young Lee",
        "Jia-Bin Huang",
        "Eli Shechtman",
        "Zhengqi Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing."
        },
        {
            "title": "Start",
            "content": "Generative Video Motion Editing with 3D Point Tracks Yao-Chih Lee1,3, Joon-Young Lee1 Zhoutong Zhang2 Jia-Bin Huang3 Jiahui Huang1 Eli Shechtman1 Jui-Hsien Wang1 Zhengqi Li 1Adobe Research 2Adobe 3University of Maryland College Park https://edit-by-track.github.io 5 2 0 2 ] . [ 1 5 1 0 2 0 . 2 1 5 2 : r Figure 1. Edit-by-Track. Our novel framework enables precise video motion editing via 3D point tracks. By specifying desired 3D trajectories, users can seamlessly control joint camera and object motion, remove objects, and transfer motion between videos."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Camera and object motions are central to videos narrative. However, precisely editing these captured motions remains significant challenge, especially under complex object movements. Current motion-controlled image-tovideo (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning video generation model on source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing. Video motion, complex blend of scene dynamics and camera movement, conveys rich visual information and supports storytelling. Nonetheless, precisely editing the motion of given videowhether correcting camera trajectories or synthesizing different motions for an objectremains challenging, particularly when handling both camera motion and complex object movements with occlusions. Existing video editing methods typically address only one side of this problem. Object-centric approaches [67, 71] enable simple object motion editing (e.g., shifting and resizing) using bounding boxes or 3D meshes, but lack control over camera viewpoints. Conversely, recent video diffusion models [3, 84, 127] address camera motion manipulation for videos. However, these methods are designed to preserve the scenes object motion identically by training on synchronous, multi-view data. Hence, they inherently fall short when attempting to edit object motion (Fig. 2a). In this paper, we present, Edit-by-Track, unified framework for manipulating both camera and object motion in video. Inspired by recent track-conditioned image-tovideo (I2V) generation models [24, 27], we adopt point trajectories [30, 86, 129] as general motion representation *Work done while Yao-Chih was an intern at Adobe Research. 1 to guide the editing of both types of motions. Nevertheless, existing track-conditioned I2V models are unsuitable for video editing since they generate motion from single frame, neglect the remaining frames, and thus lose the original scene context (Fig. 2b). Hence, we propose motioncontrolled video-to-video (V2V) model that leverages the full input video and its corresponding 3D point tracks, establishes sparse correspondences between the source and target motions, and enables precise motion editing while faithfully preserving the original scene context. Editing 3D motion in video often exposes previously unseen regions. To address this challenge, we leverage the strong generative prior of pretrained text-to-video (T2V) diffusion model [98] and further fine-tune it using new 3D-track conditioner tailored for video editing. To effectively encode 3D tracks, our 3D-track conditioner uses cross-attention to perform learnable sampling-andsplatting process that adaptively gathers visual context from the input video and projects it into the target frame space with 3D awareness, while remaining robust to noisy inputs. Training V2V diffusion model requires paired videos with specific camera and object motion manipulations, along with 3D point-track annotationsdata that is difficult to obtain in real-world settings. We address this challenge with two-stage fine-tuning strategy. The first stage uses synthetic video pairs of animated humans with groundtruth 3D tracks to bootstrap motion control from pretrained T2V model. To bridge the domain gap, the second stage further fine-tunes the model on diverse real-world video pairs constructed by sampling non-contiguous clips from monocular videos. Our model achieves precise, 3Daware control over both camera and object motion, enabling wide range of editing capabilities, including manipulating object depth and handling occlusions  (Fig. 1)  . Extensive experiments demonstrate that our method substantially outperforms state-of-the-art approaches in editing both camera motion and object dynamics, while preserving the coherence of the input context throughout the edited videos. In summary, our contributions include: (i) robust video motion editing model based on novel 3D track conditioner, (ii) new two-stage training pipeline that enables model to learn precise 3D motion control for real video clips, and (iii) we show our system enables versatile motion editing effects that cannot be done by recent work. 2. Related Work Motion-controlled video generation. Recent advances in text-to-video (T2V) and image-to-video (I2V) models [1, 6, 8, 11, 28, 29, 33, 50, 73, 81, 98, 119, 121] have laid the foundation for controlled video synthesis. growing body of work [9, 12, 24, 27, 31, 57, 76, 88, 100, 105, 109, 112, 114, 118, 120, 125, 135] integrates explicit moFigure 2. Limitations of existing methods. We demonstrate joint camera and object motion editing on an input video (first row)changing both the camera viewpoint and the persons falling locationusing frames warped by the edited motion as reference (second row). The prior camera-controlled V2V approach [84] inpaints from the warped input video but fails to correct secondary effects (e.g., splashes) caused by the edited object motion. The track-conditioned I2V method [99] loses input scene context by conditioning only on the first frame. In contrast, our approach edits both camera and object motion while preserving the input context and maintaining coherent causal effects (third row). tion conditions, often through adapters such as ControlNet [131], to control the camera and scene dynamics in the generated videos. For effective motion control, the choice of motion representation is crucial. For camera motion, [31, 112] directly encode and inject camera pose parameters into video models. For object motion, [57, 70, 120] leverage 2D bounding boxes to guide the object motion. Alternatively, several approaches [12, 24, 27, 87, 99, 112, 114, 118, 120, 125, 134] adopt point tracks as unified representation for both camera and object motion, where background point movements correspond to camera motion and foreground points to object motion. Such point-track conditioning can rely on few points (e.g., < 10) for user-friendly input [12, 87, 102, 112, 125, 134] or denser, hundreds of points for fine-grained control [24, 27, 39]. Recent studies [12, 27, 102] use 3D point tracks for 3D-aware motion synthesis. However, these I2V models [12, 24, 27, 99] are not wellsuited for video editing tasksthey use only the first frame of an input video as reference, omitting subsequent frames and losing the context of the whole dynamic scene. In contrast, our V2V framework conditions on the entire input video, preserving the full context of the original scene. Camera-controlled video-to-video synthesis. Editing camera viewpoints of an input video, often known as novel view synthesis, is longstanding research topic. Previous 2 Figure 3. Edit-by-Track framework. Given video Vsrc, we estimate camera poses and 3D tracks using off-the-shelf models [117, 129]. Users then edit the poses and tracks to form pairs of source and target screen-projected tracks, which serve as the motion condition. To preserve the original visual context, the source video is encoded by VAE and patchifier into source tokens νsrc, which are concatenated with noisy target tokens νtgt. For motion control, our 3D track conditioner  (Fig. 4)  processes the track pairs into tokens [τsrc, τtgt], which are added to the paired video tokens [νsrc, νtgt]. These motion-conditioned tokens are then fed into DiT blocks to denoise νtgt. reconstruction-based methods [21, 54, 56, 58, 59, 68, 78, 89, 106, 115] lack generative priors to hallucinate unseen areas, limiting the effective range of viewpoint changes. Recent studies leverage generative models to synthesize complete content. Multi-view diffusion models [2, 4, 90, 101, 113, 137] input video to generate multi-view videos, which can be lifted into 4D Gaussian Splatting field [2, 90, 101, 113]. Inpainting-based methods [40, 72, 84, 126128] warp input pixels to the target views, then employ video diffusion models to fill the missing pixels. Instead, ReCamMaster [3] uses V2V model that directly takes the input video and target camera poses to generate target videos. However, these methods focus only on camera viewpoint changes, neglecting object motion editing. While inpainting approaches may handle warped video of edited objects, they inherently struggle to synthesize plausible outputs, as the models are trained on synchronous video pairs [3, 96]. In contrast, our method learns from asynchronous video pairs to enable joint camera and object motion editing. Video editing. Existing studies have made remarkable progress in editing tasks, including object replacement/appearance editing [5, 10, 15, 20, 22, 25, 34, 36, 38, 41, 43, 44, 47, 48, 52, 61, 62, 6466, 77, 82, 123], object insertion [41, 66, 94, 124], and object removal [41, 55, 66]. Nevertheless, these methods alter appearance or composition while preserving the original motion in output videos. Recent works have explored aspects of motion or shape editing in videos, but each addresses only subset of the broader problem. Drag-based approaches [16, 92] enable 3 simple shape deformation but struggle with complex motion editing. Human motion editing methods [93, 138] condition on human skeleton poses and thus do not generalize to general objects. Object-centric methods manipulate object position or shape using bounding boxes [71], 3D meshes [67], or assume static objects [51], but lack control over camera or fine-grained object motion. GS-DiT [7] and ReVideo [74] use video diffusion models to refine coarse video draft, generated from pseudo-Gaussian Field or masked background with first-frame edit, respectively. However, these methods remain sensitive to the rendering quality of the coarse input [7] and still lose the full scene context of the original video [74]. In contrast, we introduce streamlined V2V framework for fine-grained, joint camera and object motion editing. Our method directly uses the entire unmasked input video and the pair of input and target 3D tracks to generate the desired video. Video pose, depth, and 3D track estimation. Recent progress in video depth and pose estimation [35, 60, 103, 107, 110, 130, 133] enable 3D point trackers [14, 19, 75, 116, 117, 122, 129] to lift 2D tracks [13, 17, 45, 46, 49, 104] into 3D space. Building on these advances, our method utilizes 3D pose and track estimations to enable usercontrollable editing of both camera and 3D object motion. 3. Method Our goal is to generate target video Vtgt from an input source video Vsrc that accurately reflects userW 3 RF specified motion editing. To achieve fine-grained control over both the camera viewpoint and scene dynamics, we leverage unified motion representation based on 3D point tracks. We first estimate the source videos per-frame cam3 Psrc and 3D point tracks era parameters using off-the-shelf video-depth-pose and 3D tracking models [117, 129]. The user then defines the target motion by editing the source camera/object motion ( Tsrc) to Psrc, Ttgt). Subseobtain the desired target parameters, ( Ptgt, quently, the source video and the corresponding source and target motion parameters are input to our V2V framework to synthesize the target video  (Fig. 3)  . Tsrc RF Since 3D motion editing inevitably requires synthesizing unseen content, we leverage the strong generative prior of pretrained T2V generation model [98] to fine-tune our V2V model with 3D track conditioner (Sec. 3.1). major challenge lies in the scarcity of ideal, annotated video pairs for motion manipulation. To address this, we adopt two-stage fine-tuning approach (Sec. 3.2): We first train the model on synthetic data to establish the core motion control capability, followed by fine-tuning on real-world videos to significantly enhance generalizability. Finally, we detail how our trained model enables various editing tasks (Sec. 3.3). 3.1. Track-Conditioned Video-to-Video Model Rf hw Our Edit-by-Track is built upon the pretrained T2V model, Wan-2.1 [98], transformer-based video diffusion model (DiT) trained with the Rectified Flow objective [18] to generate 81-frame videos. The T2V generation begins by patchifying noisy video latent at step t, zt tgt, into video d. These tokens are fed into series of tokens νtgt DiT blocks, conditioned by encoded text tokens, and then unpathchfied back to the denoised latent, zt tgt . Throughout an iterative denoising process, the final clean latent, z0 tgt is decoded back to the target RGB video, Vtgt by the VAE decoder. Here, , and stand for the downsampled temporal and spatial dimensions, and is the token dimension. For video editing, our V2V model is conditioned on source video, Vsrc, which is encoded by VAE encoder to latent, zsrc, and patchified into source video tokens, νsrc. These tokens are concatenated with the noisy target video d, to serve as the video condiR2f hw tokens, [νsrc, νtgt] tion for the DiT blocks to generate the target video. 3D track conditioning. Many existing motion-controlled approaches [24, 27, 102, 114] explicitly represent motion in 2D screen space before feeding it into the model, aligning it with the target video frames. While this provides direct guidance, it relies on hand-crafted 2D-screen-space representations, which become difficult to handle for large number of 3D tracks where occlusions frequently occur. In contrast, recent works [3, 120] directly encode motion values into features without explicit screen-space alignment. While these models can handle the motion signals more 4 , proj tgt ) 3D track conditioner. Given track pairs, Figure 4. (T proj 2f 3, consisting of 2D screen coordisrc nates xy and normalized disparity z, the conditioner first uses cross-attention to adaptively sample per-track context from source video tokens νsrc. second cross-attention then splats the sampled tokens, τsampled , via the source and target tracks into the re2f hwd aligned spective frame spaces, yielding [τsrc, τtgt] with the paired video tokens [νsrc, νtgt]. We use different token orientations to represent the corresponding dimensions. src src , proj tgt ) First, we project adaptively, they may suffer from imprecise control and scale ambiguity. To combine the advantages of both motion encoding approaches, we propose learnable mechanism that adaptively encodes 3D tracks to 2D screen-aligned tokens. the source and target 3D tracks, (Tsrc, Ttgt), using respective camera parameters, (Psrc, Ptgt), to the 2D screen coordinate, as (T proj 2F 3, where the projected tracks values are normalized to range [0, 1] in disparity space. Consequently, the projected 3D tracks represent relative camera motion through the coordinates in the screen space. We then temporally downsample 2f 3 via nearest-neighbor these projected 3D tracks to sampling and employ our 3D track conditioner to transform the 3D track coordinates to track tokens, [τsrc, τtgt] R2f hw to align with the video tokens, [νsrc, νtgt]  (Fig. 4)  . The core function of the 3D track conditioner is to use the projected tracks to sample visual context from the source video tokens Vsrc, and splat the context back to the source and target frame spaces to establish source-target correspondence. similar approach, TrajAttn [118], employs nearest-neighbor sampling and splatting in hidden states with 2D tracks for I2V generation. However, this direct sampling is not robust to noisy and frequently occluded 3D tracks. Instead, our method, inspired by Tracktention [53], uses coordinate-based cross-attention for sampling and splatting. This provides an adaptive mechanism to encode variable number of 3D tracks in 2D space. For the cross attention, we first apply positional encoding to map the 3D tracks xyz into token embeddings, yielding ρxyz d. Then, to transfer visual context, we { use the source coordinate embedding ρxyz src as the query to Rf } src,tgt for sample per-frame source video tokens νsrc each track, followed by Transformer [97] blocks that aggregate temporal information within each track:"
        },
        {
            "title": "Rf hw",
            "content": "τsampled src = Transformer (Attn (ρxyz src , , νsrc)) , (1) hwd, denotes the positional where the attention key, encoding of xy-grid coordinates with = 0. } { src src,tgt"
        },
        {
            "title": "Rf N",
            "content": ", yielding [τsrc, τtgt] The sampled tokens, τsampled d, which carry visual context from the source video, are then splatted back to both source and target video spaces using the source and target coordinate embedding ρxyz R2f hw each branch is achieved by another cross-attention: (cid:17) (cid:16) d. In reverse to the sampling step, the splatting of , ρxyz { Both cross-attention sampling and splatting adaptively retrieve values from corresponding 3D coordinates, facilitating the robustness to noisy point tracks. Additionally, while Tracktention [53] injects an attention bias from input tracks to guide the attention operations, we omit this bias term as we found that it is sensitive to noisy tracks. , τsampled src = Attn src,tgt src,tgt (2) τ { } } . While the splatting assigns values to xy locations, the depth information is not explicitly splatted, which limits 3D awareness. Thus, we additionally apply positional encoding d, which are added to tracks values, as σz {src,tgt} to the sampled tokens τsampled before the splatting branches. src Figure 5. Training data. (a) Our model is first fine-tuned on the synthetic data with ground-truth point tracks to learn motion control. Each video pair shares the same objects and background scenes but differs in object actions and camera motions. (b) We continue fine-tuning on real data by sampling two non-contiguous clips from monocular video, leveraging its natural motion to scalably simulate joint camera and object motion changes. [τsrc, τtgt] Lastly, the obtained track token pairs, R2f hw d, are element-wise added to the video tokens [νsrc, νtgt], then concatenated to feed to the DiT blocks for motion control. Notably, we do not use visibility labels from 3D track estimation, as visibility of edited tracks becomes ambiguous after 3D manipulation. Instead, all source tracks are used, regardless of occlusion, allowing the model to implicitly reason about visibility and occlusion. 3.2. Two-Stage Training Our V2V model is fine-tuned from pretrained T2V model with an added 3D track conditioner initialized from scratch. The ideal training data for this task would consist of largescale video pairs depicting the same object and background context but with different camera and object motions, accompanied by precise 3D track pairs. As such data is exceptionally rare in the real world, we employ two-stage fine-tuning approach to address the scarcity, using both synthetic data and real monocular videos. Stage 1: Synthetic data bootstrapping. To establish the first stage utilizes synmotion control capability, thetic dataset of video pairs from the same scene with varied motions and ground-truth 3D track pairs. Scenes are generated in Blender using dynamic Mixamo human assets [37] for the foreground and Kubric assets [26] for the background. base scene is initialized with randomly selected human characters and background texture, and then Figure 6. Joint camera and object motion editing. Our method enables the editing of camera and/or object motion using edited camera poses and 3D point tracks (visualized in corner insets). rendered into multiple clips using varied human animation scripts and camera trajectories. Ground-truth 3D tracks are subsequently extracted from the mesh vertices [136]. During training, two clips are randomly selected from base scene to form training pair (Fig. 5a). Stage 2: Real data fine-tuning. We bridge the syntheticto-real domain gap by fine-tuning on real video pairs. Given the scarcity of real-world videos with controlled variations in camera and object motion, we construct such pairs directly from monocular videosa process that scales efficiently. Each pair is formed by randomly sampling two noncontiguous clips (with temporal gap of 1-5 seconds) from single dynamic video, allowing the clips to naturally cap5 Figure 7. Additional applications. Our model leverages flexibly manipulated 3D tracks for diverse editing tasks. (a) We achieve complex multi-dancer synchronization by transferring human poses via SMPL-X [79]. The point tracks also enables (b) shape deformation for general objects, while (c) object removal and (d) duplication are accomplished by moving tracks off-screen or repeating them, respectively. for object-level track editing. Some estimated background tracks may exhibit temporal jitter in static scenes, leading to slight distortion in certain viewpoint editing tasks (e.g., stationary views). Hence, we can optionally fix background points as static using their segmentation labels in this step. Application 1: Joint camera and object motion editing. Since 3D point track estimation disentangles scene-object motion from camera motion in video, users can independently edit their desired camera and/or object motion. For example, user can manipulate only the 3D tracks to change object movement while preserving the original camera motion, or vice versa. Fig. 6 shows that our model effectively handles edited viewpoints and tracks, generating the desired video outputs, even under unrealistic editing scenarios. Application 2: Human motion transfer. We leverage the desirable properties of the SMPL-X [79] representation to enable human motion transfer. First, we estimate SMLP-X parameters [108] for humans in the input video. Motion is then transferred by swapping the local pose parameters (θ) from source human, while keeping the targets shape parameters (β) and global pose. Finally, we reconstruct mesh vertices from the original and edited SMPL-X models to obtain corresponding point tracks. Our model effectively handles these tracks to achieve complex human motion transfer like synchronizing multiple dancers movements (Fig. 7a), extending prior work on 2D-layer-based retiming [69]. Application 3: Shape deformation. Our method also supports non-rigid shape deformation for general moving objects. In practice, users select groups of points using 2D bounding boxes, transform these groups. The unselected points are then interpolated via an operation similar to Linear Blending Skinning. Fig. 6b shows this application with body-shape transformation of walking dog. Application 4: Object removal and duplication. The point track inputs support additional applications, such as object removal and duplication (Fig. 7c and d). Object removal is achieved by moving the target objects points out of the frame boundaries. Notably, our approach removes objects while simultaneously supporting viewpoint changes, capability not present in existing inpaintingbased methods [41, 55]. For object duplication, we repliFigure 8. Handling partial tracks. By specifying only the body motion (moving right) via bounding box and removing leg tracks, our model synthesizes correct leg motion without explicit controls on the legs. Background tracks are hidden for clarity. ture changes in both camera and object motion (Fig. 5b). For track correspondence, we employ 3D tracking models [117, 129] to estimate 2K 3D tracks across the full video. We sample 24K dynamic videos from an internal stock video dataset, focusing on those in which dynamic objects are consistently tracked. To counteract the smooth camera motion and similar view directions often present in this collection, we add small portion of data from the DL3DV Dataset [63], which provides static scenes with large, arbitrary camera movements. Additionally, small collection of object-removal video pairs [55] is integrated to specifically enhance the capability of object-level manipulation. = 384 Training details. We employ LoRA (rank=64) fine-tuning for our V2V model, initializing from the pretrained Wan2.1T2V-1.3B model [98]. The training is conducted at resolution of 672 with learning rate of 4 and total batch size of 64 on 16 A100 GPUs. 10 1 During training, we use random sample of tracks, with the sample size varying between [500, 1000]. The two-stage fine-tuning process comprised 4K iterations for Stage 1 and 8K iterations for Stage 2. More details can be found in SM. 3.3. Inference We demonstrate that our Edit-by-Track supports wide range of video editing tasks during inference. We strongly encourage readers to view our video results in our webpage. Point track preprocessing. For test video, we begin by using SAM2 [83] to obtain foreground object masks, and 3D tracking models [117, 129] estimate camera parameters, depths, and 3D tracks across all frames. We then leverage the segmentation masks to label point tracks by object 6 Table 1. Quantitative comparison on joint camera and object motion on DyCheck [23]. We report full-frame and masked (covisible areas only [23]) metrics, averaged across 12 scenes. The best and second-best scores are highlighted. Some methods use ground-truth (GT) information for their inputs. GT 1st frame denotes using the first frame of the target GT video. Methods marked with use the estimated flow to the GT video to warp the input. TrajAttn [118] takes warped video input using the extension of NVS-Solver [126]. Method Base model Type Privilege GT information TrajAttn [118] DaS [27] PaC [12] ATI [99] GEN3C [84] GEN3C [84]* TrajCrafter [127] TrajCrafter [127]* SVD [8] CogVX [121] SD1.5 [85] Wan [98] I2V + track I2V + track I2V + track I2V + track V2V + inpaint Cosmos [1] Cosmos [1] V2V + inpaint CogVX [121] V2V + inpaint CogVX [121] V2V + inpaint GT 1st frame GT 1st frame GT 1st frame GT 1st frame None GT 1st frame + flow to GT video None Flow to GT video ReVideo [74] ReVideo [74]* TrajAttn [118]+ [126]* SVD [8] SVD [8] SVD [8] IV2V + track + inpaint GT 1st frame IV2V + track + inpaint GT 1st frame + flow to GT video IV2V + track + inpaint GT 1st frame + flow to GT video Edit-by-Track (Ours) Wan [98] V2V + track None PSNR 12.16 13.69 12.78 13.67 12.85 13.61 12.08 11.98 13.11 13.18 13.94 14.80 Full frame SSIM .345 .390 .371 .371 .398 .406 .383 .376 .407 .417 .416 .424 LPIPS .592 .563 .544 .468 .567 .517 .585 .593 .649 .644 .549 .406 mPSNR 12.25 13.75 12.53 14.18 13.49 13.98 13.10 12.82 13.05 13.18 14.94 15.99 Masked mSSIM .697 .724 .737 .711 .727 .740 .726 .724 .727 .730 .741 .747 mLPIPS .416 .391 .385 .312 .373 .339 .381 .383 .443 .434 .351 .247 Table 2. Quantitative comparison on in-the-wild videos. We compare with track-conditioned methods on test set of 100 videos randomly sampled from MiraData [42]. We report PSNR, SSIM, LPIPS, FVD for visual quality, and EPE for track control. Method Base model # params PSNR ReVideo [74] TrajAttn [118] DaS [27] PaC [12] ATI [99] Ours SVD [8] SVD [8] CogVX [121] SD1.5 [85] Wan [98] Wan [98] 1.5B 1.5B 5B 0.9B 14B 1.3B 17.10 17.59 18.15 17.51 19.07 19.55 SSIM .599 .617 .599 .581 .635 . LPIPS .388 .359 .315 .336 .244 .236 FVD 438.50 396.40 393.32 345.29 268.80 306.44 EPE 24.00 26.54 17.92 31.23 11.44 6.12 cate both source and target tracks of an object to establish new correspondence, where the new duplicate can also be manipulated by different 3D transformations. Handling partial track input. While 3D tracks offer precise control, they can be unintuitive for novice users to manage multiple tracks. Our model mitigates this by supporting partial point tracks, avoiding detailed track editing. For example  (Fig. 8)  , user can use bounding boxes to select the points on goats body and specify key target motion (e.g., simple shift) while omitting the legs. The model then synthesizes plausible leg motion without manual specification. 384 video in 4.5 minutes on an A100 GPU, using 50 sampling steps and text classifier-free guidance (CFG) [32] scale of 5. Our model processes an 81-frame, 672 4. Experiments Baseline. We conduct extensive comparisons with existing motion-controlled video diffusion methods, categorized into three groups, as no prior V2V method directly supports joint camera and object motion editing. (i) Trackconditioned I2V methods [12, 27, 99, 118] generate videos from single image and motion tracks. We input the ground-truth first frame instead of the original video. (ii) Camera-controlled V2V methods [84, 127] are designed solely for viewpoint manipulation by inpainting warped target view. To test their limits, we supply them with input videos warped by joint camera and object motion. (iii) ReVideo [74] and TrajAttn [118] with an NVS-Solver [126] 7 extension perform motion editing by inpainting masked video, guided by first frame and motion tracks. The baselines [12, 74, 99] use few point tracks; thus, we adopt their respective methods to select subset from our input tracks. Evaluation method. Due to the lack of ground-truth real video pairs for joint camera and object motion manipulation, we simulate the in/output pair by selecting two noncontiguous clips from monocular video to define the target motion change. We use widely adopted metrics, PSNR, SSIM [111], and LPIPS [132] to measure the difference between the (pseudo-)ground-truth and generated videos. 4.1. Evaluation on the DyCheck iPhone Dataset DyCheck [23] provides synchronized multi-view videos with depth and camera-pose annotations. These sequences are captured in dynamic scenes using one moving camera and two stationary cameras, enabling us to validate manipulations of camera and object motion, either jointly or independently. Our primary task, joint camera and object motion control, is tested by extracting two clips from the moving-camera video, which naturally contains both motion types (12 scenes). The evaluation of disjoint motion control is provided in SM. We report PSNR, SSIM, LPIPS [132], and the respective masked scores [23], which evaluate only the co-visible areas between input and output. Table 1 shows the quantitative results for the joint motion manipulation task. Track-conditioned I2V methods [12, 27, 99, 118], despite using the ground-truth first frame for motion animation, lack the full scene context from the input video, leading to notable hallucinations and obInpainting-based camera-controlled V2V ject distortions. methods [84, 127] struggled to synthesize the target object motion when given the original point-cloud-warped video. While we attempted to improve their performance by providing warped video derived from estimated optical flow [91] between the ground-truth and input frames, large viewpoint changes often led to flow estimation failures, preFigure 9. Visual comparisons on video editing. We edit DAVIS [80] video with 3D object rotation, using the target motion-warped as reference. I2V methods [27, 99] lose context outside of the input first frame (corner insets). GEN3C [84] inputs the warped video but fails to correct the shadow of the edited object (red arrow). See SM for additional in-the-wild results. venting overall score improvement. Finally, although ReVideo [74] and extended TrajAttn [118] use the groundtruth first frame and partial background video, they lack full spatiotemporal context of the input video. In contrast, our method outperforms prior approaches on all metrics by jointly managing object and viewpoint changes while preserving original context (See SM for visual comparisons). 4.2. Quantitative Evaluation on In-the-Wild Videos We compare against track-conditioned methods [12, 27, 74, 99, 118] on test set of 100 monocular videos randomly sampled from MiraData [42] to form in/output pairs with joint motion changes. In Table 2, we also evaluate overall visual quality using the Frechet Video Distance (FVD) [95] and motion control using the End-Point Error (EPE), which measures the L2 distance between estimated 2D tracks [46] in the generated versus ground-truth video. ATI [99] achieves low FVD score by using the groundtruth first frame and massive 14B base model. However, it fails to preserve the full context of the input video or adhere to the input tracks. In contrast, our 1.3B model achieves superior overall results and the best track control by better leveraging spatiotemporal information from the input. 4.3. Comparisons on Video Editing We compare our method against baseline approaches on 10 real-world editing examples. These edits involve modifying camera and/or object motion, with object manipulation performed on 3D tracks and depth-unprojected point clouds. The edited point cloud, reprojected to target views, provides the warped input for GEN3C [84]. Additionally, we ensure that the first frame remains unedited to allow I2V methods [27, 99] to generate target motion from the first framea constraint our method does not assume. As shown in Fig. 9, I2V methods [27, 99] suffer from severe visual context loss, and GEN3C [84] struggles with object editing, leaving noticeable shadows at the original location. Full video results and quantitative human evaluation are provided in SM. Table 3. Ablation study on 3D track conditioning. Our method (bottom) adaptively handles 3D tracks using cross-attentional sampling/splatting and injects depth embeddings for 3D-aware control. Ablation configurations are detailed in Sec. 4.4. Sampling method 2D/3D Inject depth tracks Naıve Cross-attn Cross-attn Cross-attn 2D 2D 3D 3D PSNR 13.42 13.88 14.82 14.80 DyCheck SSIM .377 .407 .423 .424 LPIPS .489 .415 . .406 PSNR 16.69 18.88 19.07 19.55 In-the-wild videos LPIPS SSIM .569 .650 .647 .657 .335 .249 . .236 FVD 413.84 305.93 335.85 EPE 16.18 7.03 7.44 306. 6.12 Table 4. Ablation study on training scheme. Our method (bottom) first learns track control on synthetic data, then fine-tunes on real data for generalizability, outperforming all ablated settings. Syn Real data data Two stage PSNR 9.61 10.62 13. 14.80 DyCheck SSIM .286 .311 .370 .424 LPIPS .706 .669 .483 .406 PSNR 11.92 12.68 18. 19.55 In-the-wild videos LPIPS SSIM .431 .462 . .657 .562 .568 .258 .236 FVD 623.32 594.35 347.95 EPE 24.64 63.98 6.93 306.44 6.12 4.4. Ablation Study 3D track conditioning. In Table 3, we first compare our adaptive cross-attention sampling and splatting with naıve baseline that employs fixed Gaussian kernel (used as bias term in Tracktention [53]) for direct sampling and splatting, making it more sensitive to the noisy tracks than our adaptive approach. Moreover, 3D track inputs offer crucial depth cues that 2D inputs lack, improving performance on large viewpoint changes on DyCheck [23]. Finally, injecting tracks embeddings σz into sampled tokens enhances 3D-aware control, reducing the track control error (EPE). Training scheme. Table 4 validates our two-stage strategy using both synthetic and real data. We find that training solely on synthetic data results in weak generalization, whereas training only on real data leads to poor track control. Lastly, our two-stage strategy, which first trains on synthetic data, then fine-tunes on real data, outperforms singlestage training on mixed synthetic-real dataset. 5. Discussion and Conclusion We proposed novel video editing framework that enables joint manipulation of 3D viewpoints, object motion, and shape deformation, tasks challenging for existing methods. Despite its effectiveness, our approach still has few limitations. For example, it may struggle when point tracks are densely clustered, especially for small objects, hindering accurate visual context extraction and motion control. It may also fail to synthesize complex physical phenomena arising from the edited motions (see SM for failure cases), which reflects the limited physical grounding of current generative priors. We believe these limitations can be alleviated by advances in physically grounded generative models and data scaling, which our scalable fine-tuning on real monocular videos is positioned to leverage. Our method enables versatile controllable video motion editing, bridging the gap between user intent and complex video synthesis. 8 Acknowledgements. We are grateful for the valuable feedback and insightful discussions provided by Yihong Sun, Linyi Jin, Yiran Xu, Quynh Phung, Dekel Galor, Chun-Hao Paul Huang, Tianyu (Steve) Wang, Ilya Chugunov, Jiawen Chen, Marc Levoy, Wei-Chiu Ma, Ting-Hsuan Liao, Hadi Alzayer, Yi-Ting Chen, Vinayak Gupta, Yu-Hsiang Huang, and Shu-Jung Han."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world founarXiv preprint dation model platform for physical ai. arXiv:2501.03575, 2025. 2, 7, 5 [2] Sherwin Bahmani, Tianchang Shen, Jiawei Ren, Jiahui Huang, Yifeng Jiang, Haithem Turki, Andrea Tagliasacchi, David Lindell, Zan Gojcic, Sanja Fidler, et al. Lyra: Generative 3d scene reconstruction via video diffusion model self-distillation. arXiv preprint arXiv:2509.19296, 2025. 3 [3] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. In ICCV, 2025. 1, 3, 4, 6 [4] Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, and Di Zhang. Syncammaster: Synchronizing multi-camera video generation from diverse viewpoints. In ICLR, 2025. 3 [5] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-driven layered image and video editing. In ECCV, 2022. 3 [6] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: space-time difIn SIGGRAPH Asia, fusion model for video generation. 2024. 2 [7] Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, FuYun Wang, and Hongsheng Li. Gs-dit: Advancing video generation with pseudo 4d gaussian fields through efficient dense 3d point tracking. In CVPR, 2025. 3 [8] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 7, [9] Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, et al. Go-with-the-flow: Motioncontrollable video diffusion models using real-time warped noise. In CVPR, 2025. 2 [10] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistency-aware diffusion video editing. In CVPR, 2023. 3 [11] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, 9 Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 2 [12] Yingjie Chen, Yifang Men, Yuan Yao, Miaomiao Cui, and Liefeng Bo. Perception-as-control: Fine-grained controllable image animation with 3d-aware motion representation. In ICCV, 2025. 2, 7, 8, 5, [13] Seokju Cho, Jiahui Huang, Jisu Nam, Honggyu An, Seungryong Kim, and Joon-Young Lee. Local all-pair correspondence for point tracking. In ECCV, 2024. 3 [14] Seokju Cho, Jiahui Huang, Seungryong Kim, and JoonYoung Lee. Seurat: From moving points to depth. In CVPR, 2025. 3 [15] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. Flatten: optical flow-guided attention for consistent text-to-video editing. In ICLR, 2024. 3 [16] Yufan Deng, Ruida Wang, Yuhao Zhang, Yu-Wing Tai, and Chi-Keung Tang. Dragvideo: Interactive drag-style video editing. In ECCV, 2024. 3 [17] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. Tapir: Tracking any point with per-frame initialization and temporal refinement. In ICCV, 2023. 3 [18] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [19] Haiwen Feng*, Junyi Zhang*, Qianqian Wang, Yufei Ye, Pengcheng Yu, Michael J. Black, Trevor Darrell, and Angjoo Kanazawa. St4rtrack: Simultaneous 4d reconstruction and tracking in the world. In ICCV, 2025. 3 [20] Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan, Jianmin Bao, Chong Luo, Zhibo Chen, and Baining Guo. Ccedit: Creative and controllable video editing via diffusion models. In CVPR, 2024. 3 [21] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic monocular video. In ICCV, 2021. 3 [22] Chenjian Gao, Lihe Ding, Xin Cai, Zhanpeng Huang, Zibin Wang, and Tianfan Xue. Lora-edit: Controllable first-frame-guided video editing via mask-aware lora finetuning. arXiv preprint arXiv:2506.10082, 2025. 3 [23] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dynamic view synthesis: reality check. In NeurIPS, 2022. 7, 8, 5, 6 [24] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Yusuf Aytar, Michael Rubinstein, Chen Sun, et al. Motion prompting: Controlling video generation with motion trajectories. In CVPR, 2025. 1, 2, [25] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. In ICLR, 2023. 3 [26] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, et al. Kubric: scalable dataset generator. In CVPR, 2022. 5, 1 [27] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, et al. Diffusion as shader: 3d-aware video diffusion for versatile video generation control. In SIGGRAPH, 2025. 1, 2, 4, 7, 8, 5, 6 [28] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. In ICLR, 2024. 2 [29] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models. In ECCV, 2024. 2 [30] Adam Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In ECCV, 2022. [31] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: EnIn abling camera control for text-to-video generation. ICLR, 2025. 2 [32] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 7 [33] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-tovideo generation via transformers. In ICLR, 2023. 2 [34] Jiahui Huang, Leonid Sigal, Kwang Moo Yi, Oliver Wang, and Joon-Young Lee. Inve: Interactive neural video editing. arXiv preprint arXiv:2307.07663, 2023. 3 [35] Jiahui Huang, Qunjie Zhou, Hesam Rabeti, Aleksandr Korovko, Huan Ling, Xuanchi Ren, Tianchang Shen, Jun Gao, Dmitry Slepichev, Chen-Hsuan Lin, et al. Vipe: Video pose engine for 3d geometric perception. arXiv preprint arXiv:2508.10934, 2025. 3 [36] Yi Huang, Wei Xiong, He Zhang, Chaoqi Chen, Jianzhuang Liu, Mingfu Yan, and Shifeng Chen. Dive: Taming dino for subject-driven video editing. In ICCV, 2025. 3 [37] Adobe Systems Inc. Mixamo. https://www.mixamo. com/. Accessed: 2025-10-28. 5, [38] Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. Vmc: Video motion customization using temporal attention adaption for text-to-video diffusion models. In CVPR, 2024. 3 [39] Hyeonho Jeong, Chun-Hao Huang, Jong Chul Ye, Niloy Mitra, and Duygu Ceylan. Track4gen: Teaching video diffusion models to track points improves video generation. In CVPR, 2025. 2 [40] Hyeonho Jeong, Suhyeon Lee, and Jong Chul Ye. Reanglea-video: 4d video generation as video-to-video translation. In ICCV, 2025. 3 [41] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. In ICCV, 2025. 3, 6 [42] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. In NeurIPS, 2024. 7, 8, 3, 4 [43] Manuel Kansy, Jacek Naruniec, Christopher Schroers, Markus Gross, and Romann Weber. Reenact anything: Semantic video motion transfer using motion-textual inversion. In SIGGRAPH, 2025. [44] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James Rehg, and Pinar Yanardag. Rave: Randomized noise shuffling for fast and consistent video editing with diffusion models. In CVPR, 2024. 3 [45] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In ECCV, 2024. 3 [46] Nikita Karaev, Yuri Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudolabelling real videos. In ICCV, 2025. 3, 8 [47] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Layered neural atlases for consistent video editing. ACM TOG, 2021. 3 [48] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. In CVPR, 2023. 3 [49] In`es Hyeonsu Kim, Seokju Cho, Jiahui Huang, Jung Yi, Joon-Young Lee, and Seungryong Kim. Exploring In CVPR, temporally-aware features for point tracking. 2025. 3 [50] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [51] Juil Koo, Paul Guerrero, Chun-Hao Huang, Duygu Ceylan, and Minhyuk Sung. Videohandles: Editing 3d object compositions in videos using video generative priors. In CVPR, 2025. 3 [52] Max Ku, Cong Wei, Weiming Ren, Harry Yang, and Wenhu Chen. Anyv2v: tuning-free framework for any video-tovideo editing tasks. TMLR, 2024. 3 [53] Zihang Lai and Andrea Vedaldi. Tracktention: Leveraging point tracking to attend videos faster and better. In CVPR, 2025. 4, 5, 8 [54] Yao-Chih Lee, Zhoutong Zhang, Kevin Blackburn-Matzen, Simon Niklaus, Jianming Zhang, Jia-Bin Huang, and Feng Liu. Fast view synthesis of casual videos with soup-ofplanes. In ECCV, 2024. 3 [55] Yao-Chih Lee, Erika Lu, Sarah Rumbley, Michal Geyer, Jia-Bin Huang, Tali Dekel, and Forrester Cole. Generative omnimatte: Learning to decompose video into layers. In CVPR, 2025. 3, 6, 2 [56] Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian fusion 10 from casual videos via 4d motion scaffolds. In CVPR, 2025. 3 [57] Quanhao Li, Zhen Xing, Rui Wang, Hui Zhang, Qi Dai, and Zuxuan Wu. Magicmotion: Controllable video generation with dense-to-sparse trajectory guidance. In ICCV, 2025. 2 [58] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In CVPR, 2021. 3 [59] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar: Neural dynamic image-based rendering. In CVPR, 2023. 3 [60] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast and robust structure and motion from casual dynamic videos. In CVPR, 2025. 3 [61] Feng Liang, Bichen Wu, Jialiang Wang, Licheng Yu, Kunpeng Li, Yinan Zhao, Ishan Misra, Jia-Bin Huang, Peizhao Zhang, Peter Vajda, et al. Flowvid: Taming imperfect optical flows for consistent video-to-video synthesis. In CVPR, 2024. [62] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu, and Jiashi Feng. Magicedit: High-fidelity arXiv preprint and temporally coherent video editing. arXiv:2308.14749, 2023. 3 [63] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In CVPR, 2024. 6, 2 [64] Jia-Wei Liu, Yan-Pei Cao, Jay Zhangjie Wu, Weijia Mao, Yuchao Gu, Rui Zhao, Jussi Keppo, Ying Shan, and Mike Zheng Shou. Dynvideo-e: Harnessing dynamic nerf for large-scale motion-and view-change human-centric video editing. In CVPR, 2024. 3 [65] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. In CVPR, 2024. [66] Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Joon-Young Lee, Yijun Li, Bei Yu, Zhe Lin, Soo Ye Kim, et al. Generative video propagation. In CVPR, 2025. 3 [67] Yuhao Liu, Tengfei Wang, Fang Liu, Zhenwei Wang, and Rynson WH Lau. Shape-for-motion: Precise and consistent video editing with 3d proxy. In SIGGRAPH Asia, 2025. 1, [68] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1323, 2023. 3 [69] Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew Zisserman, David Salesin, William Freeman, and Michael Rubinstein. Layered neural rendering for retiming people in video. In SIGGRAPH Asia, 2020. 6 [70] Wan-Duo Kurt Ma, John Lewis, and Bastiaan Kleijn. Trailblazer: Trajectory control for diffusion-based video generation. In SIGGRAPH Asia, 2024. 2 [71] Yue Ma, Xiaodong Cun, Sen Liang, Jinbo Xing, Yingqing He, Chenyang Qi, Siran Chen, and Qifeng Chen. Magicstick: Controllable video editing via control handle transformations. In WACV, 2025. 1, 3 [72] Yue Ma, Kunyu Feng, Xinhua Zhang, Hongyu Liu, David Junhao Zhang, Jinbo Xing, Yinhan Zhang, Ayden Yang, Zeyu Wang, and Qifeng Chen. Follow-your-creation: Empowering 4d creation through video inpainting. arXiv preprint arXiv:2506.04590, 2025. 3 [73] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. Snap video: Scaled spatiotemporal transformers for text-to-video synthesis. In CVPR, 2024. [74] Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. Revideo: Remake video with motion and content control. In NeurIPS, 2024. 3, 7, 8, 5, 6 [75] Tuan Duc Ngo, Peiye Zhuang, Chuang Gan, Evangelos Kalogerakis, Sergey Tulyakov, Hsin-Ying Lee, and Chaoyang Wang. Delta: Dense efficient long-range 3d tracking for any video. In ICLR, 2025. 3 [76] Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, and Yinqiang Zheng. Mofa-video: Controllable image animation via generative motion field adaptions in frozen image-to-video diffusion model. In ECCV, 2024. 2 [77] Wenqi Ouyang, Yi Dong, Lei Yang, Jianlou Si, and XinI2vedit: First-frame-guided video editing via In SIGGRAPH Asia, gang Pan. image-to-video diffusion models. 2024. 3 [78] Keunhong Park, Utkarsh Sinha, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Steven Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In ICCV, 2021. 3 [79] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael Black. Expressive body capture: 3d hands, face, and body from single image. In CVPR, 2019. [80] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In CVPR, 2016. 8 [81] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 2 [82] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In CVPR, 2023. 3 [83] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 6, 3 11 [84] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Muller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video generation with precise camera control. In CVPR, 2025. 1, 2, 3, 7, 8, 5, [85] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 7, 5 [86] Peter Sand and Seth Teller. Particle video: Long-range motion estimation using point trajectories. IJCV, 2008. 1 [87] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In SIGGRAPH, 2024. 2 [88] Xincheng Shuai, Henghui Ding, Zhenyuan Qin, Hao Luo, Xingjun Ma, and Dacheng Tao. Free-form motion control: synthetic video generation dataset with controllable camera and object motions. In ICCV, 2025. 2 [89] Colton Stearns, Adam Harley, Mikaela Uy, Florian Dubost, Federico Tombari, Gordon Wetzstein, and Leonidas Guibas. Dynamic gaussian marbles for novel view synthesis of casual monocular videos. In SIGGRAPH Asia, 2024. 3 [90] Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. In ICCV, 2025. 3 [91] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In ECCV, 2020. 7 [92] Yao Teng, Enze Xie, Yue Wu, Haoyu Han, Zhenguo Li, and Xihui Liu. Drag-a-video: Non-rigid video editing with point-based interaction. arXiv preprint arXiv:2312.02936, 2023. 3 [93] Shuyuan Tu, Qi Dai, Zhi-Qi Cheng, Han Hu, Xintong Han, Zuxuan Wu, and Yu-Gang Jiang. Motioneditor: Editing video motion via content-aware diffusion. In CVPR, 2024. 3 [94] Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, and Hengshuang Zhao. Videoanydoor: High-fidelity video object insertion with precise motion control. In SIGGRAPH, 2025. 3 [95] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 8 [96] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, and Carl Vondrick. Generative camera dolly: Extreme monocular dynamic novel view synthesis. In ECCV, 2024. [97] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017. 5 [98] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 4, 6, 7, 5 [99] Angtian Wang, Haibin Huang, Jacob Zhiyuan Fang, Yiding Yang, and Chongyang Ma. Ati: Any trajectory instruction for controllable video generation. arXiv preprint arXiv:2505.22944, 2025. 2, 7, 8, 5, 6 [100] Boyang Wang, Xuweiyi Chen, Matheus Gadelha, and Zezhou Cheng. Frame in-n-out: Unbounded controllable image-to-video generation. In NeurIPS, 2025. 2 [101] Chaoyang Wang, Peiye Zhuang, Tuan Duc Ngo, Willi Menapace, Aliaksandr Siarohin, Michael Vasilkovsky, Ivan Skorokhodov, Sergey Tulyakov, Peter Wonka, and HsinYing Lee. 4real-video: Learning generalizable photorealistic 4d video diffusion. In CVPR, 2025. 3 [102] Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, and Limin Wang. Levitor: 3d trajectory oriented image-to-video synthesis. In CVPR, 2025. 2, [103] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In CVPR, 2025. 3 [104] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, and Noah Snavely. Tracking everything everywhere all at once. In ICCV, 2023. 3 [105] Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai. Cinemaster: 3d-aware and controllable framework for cinematic text-to-video generation. In SIGGRAPH, 2025. 2 [106] Qianqian Wang, Vickie Ye, Hang Gao, Weijia Zeng, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. In ICCV, 2025. 3 [107] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In CVPR, 2025. 3 [108] Yufu Wang, Yu Sun, Priyanka Patel, Kostas Daniilidis, Michael Black, and Muhammed Kocabas. Prompthmr: Promptable human mesh recovery. In CVPR, 2025. 6 [109] Yuang Wang, Chao Wen, Haoyu Guo, Sida Peng, Minghan Qin, Hujun Bao, Xiaowei Zhou, and Ruizhen Hu. Precise action-to-video generation through visual action prompts. arXiv preprint arXiv:2508.13104, 2025. 2 [110] Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, and Tong He. Pi3: Permutation-equivariant visual geometry learning. arXiv preprint arXiv:2507.13347, 2025. 3 [111] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE TIP, 2004. [112] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. 12 [128] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. IEEE TPAMI, 2025. 3 [129] Bowei Zhang, Lei Ke, Adam Harley, and Katerina Fragkiadaki. Tapip3d: Tracking any point in persistent 3d geometry. In NeurIPS, 2025. 1, 3, 4, 6 [130] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. In ICLR, 2025. 3 [131] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. ICCV, 2023. 2 [132] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 7 [133] Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Rubinstein, Noah Snavely, and William Freeman. Structure and motion from casual videos. In ECCV, 2022. 3 [134] Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generation. In CVPR, 2025. 2 [135] Zhongwei Zhang, Fuchen Long, Zhaofan Qiu, Yingwei Pan, Wu Liu, Ting Yao, and Tao Mei. Motionpro: precise motion controller for image-to-video generation. In CVPR, 2025. 2 [136] Yang Zheng, Adam Harley, Bokui Shen, Gordon Wetzstein, and Leonidas Guibas. Pointodyssey: large-scale In ICCV, synthetic dataset for long-term point tracking. 2023. 5, 1 [137] Jensen Jinghao Zhou, Hang Gao, Vikram Voleti, Aaryaman Vasishta, Chun-Han Yao, Mark Boss, Philip Torr, Christian Rupprecht, and Varun Jampani. Stable virtual camera: Generative view synthesis with diffusion models. arXiv preprint arXiv:2503.14489, 2025. [138] Yi Zuo, Lingling Li, Licheng Jiao, Fang Liu, Xu Liu, Wenping Ma, Shuyuan Yang, and Yuwei Guo. Edit-yourmotion: Space-time diffusion decoupling learning for video motion editing. arXiv preprint arXiv:2405.04496, 2024. 3 Motionctrl: unified and flexible motion controller for video generation. In SIGGRAPH, 2024. 2 [113] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan Barron, and Aleksander Holynski. Cat4d: Create anything in 4d with multi-view video diffusion models. In CVPR, 2025. 3 [114] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In ECCV, 2024. 2, 4 [115] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiance fields for free-viewpoint video. In CVPR, 2021. 3 [116] Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: Tracking any 2d pixels in 3d space. In CVPR, 2024. 3 [117] Yuxi Xiao, Jianyuan Wang, Nan Xue, Nikita Karaev, Yuri Makarov, Bingyi Kang, Xing Zhu, Hujun Bao, Yujun Shen, and Xiaowei Zhou. Spatialtrackerv2: 3d point tracking made easy. In ICCV, 2025. 3, 4, 6 [118] Zeqi Xiao, Wenqi Ouyang, Yifan Zhou, Shuai Yang, Lei Yang, Jianlou Si, and Xingang Pan. Trajectory attention for fine-grained video motion control. In ICLR, 2025. 2, 4, 7, 8, 5, [119] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In ECCV, 2024. 2 [120] Jinbo Xing, Long Mai, Cusuh Ham, Jiahui Huang, Aniruddha Mahapatra, Chi-Wing Fu, Tien-Tsin Wong, and Feng Liu. Motioncanvas: Cinematic shot design with controllable image-to-video generation. In SIGGRAPH, 2025. 2, 4 [121] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-tovideo diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 7, 5 [122] David Yifan Yao, Albert Zhai, and Shenlong Wang. Uni4d: Unifying visual foundation models for 4d modeling from single video. In CVPR, 2025. 3 [123] Danah Yatim, Rafail Fridman, Omer Bar-Tal, Yoni Kasten, and Tali Dekel. Space-time diffusion features for zero-shot text-driven motion transfer. In CVPR, 2024. 3 [124] Danah Yatim, Rafail Fridman, Omer Bar-Tal, and Tali Dekel. Dynvfx: Augmenting real videos with dynamic content. arXiv preprint arXiv:2502.03621, 2025. [125] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. 2 [126] Meng You, Zhiyu Zhu, Hui Liu, and Junhui Hou. Nvssolver: Video diffusion model as zero-shot novel view synthesizer. In ICLR, 2024. 3, 7, 5, 6 [127] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocular videos via diffusion models. In ICCV, 2025. 1, 7, 5,"
        },
        {
            "title": "Appendix",
            "content": "We elaborate our method details (Sec. A), model analysis (Sec. B), and additional baseline comparisons (Sec. C). We also strongly encourage readers to view our webpage for full video results."
        },
        {
            "title": "Contents",
            "content": "1. Introduction 2. Related Work 3. Method 3.1. Track-Conditioned Video-to-Video Model . . . . . . 3.2. Two-Stage Training . . . . . . 3.3. Inference . . . . . . . . . . . . . . . . . . . . . . 4. Experiments 4.1. Evaluation on the DyCheck iPhone Dataset . 4.2. Quantitative Evaluation on In-the-Wild Videos . . . . 4.3. Comparisons on Video Editing . . . . . . 4.4. Ablation Study . . . . . . . . . . . . . . 5. Discussion and Conclusion A. Method Details . . . . . . . . . . A.1. Model and Training Details . . . . A.2. Synthetic Data Generation . A.3. Real Data Curation . . . . . . A.4. Track Perturbation and Data Augmentation . A.5. Existence Label for Object Removal . . . . . A.6. Video Pose and 3D Point Track Estimation . . . . . A.7. Editing and Inference Details . . . . . . . . . . . . . . . . B. Model Analysis B.1. Robustness to Sparse Point Tracks . B.2. Robustness to Noisy Point Tracks . . . B.3. Effects of Text Prompts . B.4. Effects of Random Seeds . . . B.5. Failure Cases . . . . . . . . . . . . . . . . . . C. Additional Comparisons . . C.1. Evaluation on DyCheck . C.2. User Study on Video Editing . . C.3. Comparison with ReCamMaster . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 3 4 5 6 7 7 8 8 8 8 1 1 1 2 2 2 3 3 4 4 4 4 4 4 5 5 6 A. Method Details A.1. Model and Training Details We fine-tune our track-conditioned V2V model by using LoRA on the pre-trained DiT blocks and the 3D trackconditioner, which is initialized from scratch. The LoRA is applied to the MLPs in all attention modules of the DiT 1 blocks, including the projection heads for query, key, value, and the feed-forward layers. For our 3D track conditioner, we use cross-attention module for sampling, followed by two self-attention Transformer blocks to aggregate temporal information within each track. Finally, the paired splatting branches use shared-weight cross-attention model to project the processed tokens back to corresponding frame spaces. For both sampling and splatting cross-attention modules, we feed the positional embedding of track coordinates to both MLPs of query and key. The attention value is directly applied by the attention weight from the softmax operation without passing through an MLP beforehand. We use sinusoidal positional encoding to map each tracks four input values (xyz coordinates and an existence label , see Sec. A.5) to 128-dimensional embedding, matching the dimension of single attention head (12 heads total). { 0, 1 } The two-stage fine-tuning takes total of 60 hours on 16 A100-80GB GPUs with total batch size of 64. The first stage, using synthetic data, takes 4,000 iterations ( 20 hours), and the second stage, using real data, takes 8,000 40 hours). Our 3D track conditioner comprises iterations ( 45M parameters, and the LoRA adapters for the DiT blocks account for total of 87M parameters. A.2. Synthetic Data Generation To train our model to learn motion control, we generated synthetic dataset of video pairs. The core principle of this dataset is that each pair shares the same objects and background scene but features different object actions and camera movements. This design allows the model to isolate motion changes from appearance. For the background scenes, we create backgrounds by applying textures from the Kubric [26] dataset onto large, dome-shaped mesh that envelops the scene. For the foreground dynamic objects, we exploit the Mixamo human animation asset library [37]. We gathered 25 unique human characters and 64 distinct action animations, enabling large combinatorial space of motions and appearances. Our generation process starts by defining base scene, which consists of randomly selected background texture and random number [1, 4] of chosen human characters. For each base scene, we then generate four distinct video clips, each rendered with different combination of camera movements and human actions. The ground-truth 3D point tracks are then extracted from the mesh vertices [136] of the rendered scenes. Notably, we randomly designate some foreground objects as background dynamics. These specific objects perform the same action across all four clips, and we intentionally do not extract their point tracks. This strategy aims to train the model to preserve the motion of objects when tracks are not specified. We generated total of 500 base scenes, resulting in synthetic dataset of 2,000 video clips (four clips per scene). The training process randomly selects two clips from base scene as the training input and ground truth each time. A.3. Real Data Curation Our training dataset for Stage 2 fine-tuning is built from several sources. The primary component consists of 24K real monocular videos curated from large internal video dataset. To obtain clean data suitable for training, we first apply filtering process to remove videos that contain dynamic, cluttered objects, such as crowds on streets, flocks of birds, and fast-moving traffic. These 24K selected videos are then preprocessed to estimate camera pose and 3D tracks, as detailed in Sec. A.6. We observed that while this internal dataset provides diverse motion of scene dynamics, its camera motion is often smooth and linear. This property limits the models ability to learn from large and arbitrary viewpoint changes. To complement these videos and address this limitation, we augment the training set with the DL3DV [63] dataset, which specifically contains static-scene videos with large viewpoint changes. Lastly, we integrate the object-effect-removal training pairs from Gen-Omnimatte [55] (comprising total of 46 real-video pairs) into our training set. This enhances the models ability to remove objects while preserving the remaining scene. During training, we set the sampling ratios for the final data mixture to 85% for our dynamic monocular videos, 10% for DL3DV [63], and 5% for GenOmnimatte [55]. A.4. Track Perturbation and Data Augmentation For Stage 2 fine-tuning on real monocular videos, we apply data augmentations to the video frames and 3D tracks to better match the testing distribution. primary challenge is that the source tracks, Tsrc, estimated from monocular video are often inaccurate and noisy, particularly in their depth component. These inaccuracies are then propagated and Ttgt. frequently amplified when edited into target tracks, For example, when applying 3D rotation, point in Tsrc with an incorrect depth value will be transformed to an incorrect 3D position, resulting in significant visual artifacts. To address this, we apply several point track perturbations directly to the target tracks ( Ttgt) during fine-tuning to improve the models robustness to the inevitable noise during 3D track editing. Perturbing along epipolar line: To simulate noise from inaccurate depth, we perturb up to 10% of the target tracks. We use the estimated camera poses ( Ptgt) to transform the target 3D tracks into the source cameras coordinate system. We then apply random jitter along the viewing ray, which ensures the same 2D projection but perturbs the depth component. Finally, these jitPsrc, tered target 3D tracks are reprojected back into the target video frames, which simulates 2D misalignments along the epipolar line caused by depth errors. Perturbing by random homography: Since jittering along epipolar lines can be limited in videos with small viewpoint changes, we apply an additional random homography perturbation. We begin by sampling subset of 3D tracks (up to 10%) to be perturbed. We then randomly designate one frame to serve as an anchor. Following this, series of per-frame homography matrices is obtained using randomly chosen four tracks within the subset, each matrix defining random transformation for its corresponding frame relative to the anchor frame. These per-frame homographies are then applied to all tracks in each frame of the selected subset to simulate more complex and noisy tracks. Linear motion: To further simulate motion noise, we select up to 10% of the target tracks and apply linear motion drift. This involves adding consistent 2D velocity vector to each selected track in the 2D frame space, causing it to move uniformly in specific direction throughout the video. These track perturbations enhance the models robustness to noisy 3D tracks during editing, and we provide detailed analysis of this effectiveness in Sec. B.2. In addition to 3D track perturbation, we also perform data augmentation on the two non-contiguous clips sampled from monocular video. Source frame dropout: With simple camera motion in training pair, the model may learn to over-rely on only the first and last source frames while ignoring intermediate content. To prevent this, we randomly zero out (mask) up to 50% of the source video frames, encouraging the model to utilize the visual context from the entire input video. Small clip overlap: While most training pairs are sampled from non-contiguous clips, we ensure that 5% of pairs have some temporal overlap (up to 50% of frames). This small subset of overlapping clips, when combined with our target track jittering, improves the models ability to robustly preserve input details. Horizontal flipping: Since the source and target clips sampled from single monocular video can be visually similar, we introduce further data variance by horizontally flipping the target clip and its corresponding tracks with 50% probability. These data augmentations enhance the models robustness, ensuring it generalizes from our monocular training data to more complex editing scenarios during inference. A.5. Existence Label for Object Removal We introduce an existence label in the track inputs, designed specifically to control the object-removal task. By default, 2 Figure 10. Analysis on the sparsity of track inputs. We evaluate End-Point Error (EPE) using 100 in-the-wild videos from MiraData [42] to test performance with different numbers of point tracks. All evaluations are run on the same final model, which was trained once using random number of tracks between 500 and 1000. for non-removal tasks, this label is set to 1 for all tracks. To specify removal, we set the label to 0 for the target tracks Ttgt of an object to be removed. During training, we utilize the object-removal data from Gen-Omnimatte [55], either by setting the existence label to 0 for the target tracks, or by moving the tracks off-screen in the target video. At inference time, removing an object involves both setting the existence label of its associated tracks to 0 and moving those tracks off-screen. We emphasize again that this existence label is separate from the visibility labels obtained from the 3D track estimation. Our method does not use visibility labels for occlusion handling since 3D motion editing can introduce the ambiguity of visibility. Therefore, the model is trained to handle all tracks regardless of their visibility, enabling it to reason about depth order and occlusion for target video edits. A.6. Video Pose and 3D Point Track Estimation We leverage the recent advancements in video-depthpose [35, 60, 103, 110] and 3D tracking methods [60] to estimate the required 3D tracks and camera parameters, including intrinsics and extrinsics, for our framework. While some 3D methods may further apply optimization after feed-forward models to refine the 3D estimates, such as MegaSAM [60] for consistent depth, or SpatialTrackerV2 [117] for 3D tracks, these steps can be time-consuming. To speed up pose and track estimation, we use the fine-tuned VGGT [103] model in SpatialTrackerV2 [117] to estimate video depth and pose, and then employ TAPIP3D [129] to obtain 3D point tracks without optimizations. For 400-frame training video, we sample 2000 points across 17 uniformly spaced keyframes with the same interval to ensure most video content is tracked. To ensure Figure 11. Robustness to noisy track inputs. Our final model, trained with track perturbation (Sec. A.4), is compared to an ablated model trained without it. Both are tested with varying levels of Gaussian noise on the target tracks. Our model handles 4-pixel noise with only 1.26-pixel increase in error, demonstrating its robustness. detailed foreground movements are tracked, we employ foreground-biased sampling strategy when masks are available. This involves densely sampling points on foreground objects while sparsely sampling the background. The mask generation process is adapted for different phases: during training, we automatically generate masks by applying semantic segmentation to extract likely-dynamic classes; at test time, users interactively provide the foreground masks using SAM2 [83]. This 3D preprocessing takes approximately 4 minutes for 400-frame video on an A100-80GB GPU. A.7. Editing and Inference Details For given test video, we first run SAM2 [83] to extract foreground object masks before performing video pose and 3D track estimation (Sec. A.6). These masks serve two purposes. First, they allow us to associate point tracks with their corresponding objects (Sec. 3.3, main paper). Second, they are used to densely sample points on the foreground objects during the 3D track estimation, which is crucial for capturing detailed motions, such as those of arms and legs. For all our demonstrated examples, the editing process exploits Python script to apply 3D transformations to the 3D tracks and camera poses. For more specific tasks, such as Shape deformation for body parts or applying Partial track inputs (e.g., removing legs tracks to avoid detailed leg motion specification), we select keyframe and use 2D bounding box in the 2D frame space to select the corresponding subset of 3D points for editing. For basic 3D track edits, we can also optionally generate preview video 3 by editing the depth-unprojected per-frame point clouds and warping them to the target viewpoints before running the full model. Note that the preview video will not be input to our model. The 3D track and viewpoint editing process itself is very fast, typically taking less than 1 second for an 81-frame video without preview warping. The time increases to around 30 to 60 seconds if preview video is rendered, as that process additionally requires editing the per-frame point clouds. In the future, we plan to develop 3D GUI editor to make viewpoint and 3D object motion editing more accessible to general users. Once the poses and tracks are edited, we input random sample of 1,000 tracks to the track-conditioned V2V model. The model uses these tracks and text prompt describing the target video to edit the video with the specified motion. For consistency, we use fixed random seed of 0 for all demonstrated examples and evaluations. B. Model Analysis B.1. Robustness to Sparse Point Tracks Our model is trained using random number of point tracks, ranging from 500 to 1000. We tested the trained models robustness to track sparsity during inference by measuring the End-Point Error (EPE) on 100 in-the-wild videos from MiraData [42], with results shown in Fig. 10. While the model fails with extremely sparse inputs (e.g., = 32), due to few correspondences, it may still achieve reasonable performance with 256 tracks. Visual results are available in the webpage. B.2. Robustness to Noisy Point Tracks To account for potential noise and inaccuracies in estimated 3D tracks and camera poses, we test the robustness of our model to perturbed track inputs. This experiment involves adding varying amounts of Gaussian noise to the target point tracks. We evaluate the End-Point Error (EPE) on 100 MiraData [42] videos, with results shown in Fig. 11. Our model, trained with track perturbation augmentation (Sec. A.4), is robust to handle approximately 4 pixels of noise while incurring only 1.26-pixel increase in error. In contrast, an ablated model trained without this augmentation is less robust, and its End-Point Error (EPE) increases more rapidly as noise levels rise. We provide video results in the webpage, demonstrating the performance of our final model under various noise levels. B.3. Effects of Text Prompts Our model utilizes 3D tracks for precise motion control, while text prompts provide supplementary context, helping to generate unseen regions from novel viewpoints  (Fig. 12)  or specific, motion-dependent effects. Please see the videos in our project webpage. Figure 12. Effects of text prompts. The text prompt serves as supplementary context to help generate unseen regions revealed in the novel viewpoints. As shown in the example, the right-half regions without tracks overlayed are the unseen regions. Figure 13. Effects of different random seeds. Random seeds introduce generation variations, especially in newly revealed areas (red arrows). Please note that all other examples and evaluations utilize the same fixed seed (0) for consistency. B.4. Effects of Random Seeds Different random seeds produce slight variations in the generated videos, particularly in areas that are unseen from the input and revealed by the target motion  (Fig. 13)  . Note that for consistency, all our results and evaluations use fixed seed = 0, except for Fig. 13. B.5. Failure Cases While our model demonstrates strong capability in joint camera and object motion control, it still exhibits few limitations  (Fig. 14)  . First, applying large motion changes (e.g., 270 front-flipping) to small objects that are tracked by noisy, densely-clustered points can be challenging. In such cases, the model may struggle to accurately transfer visual context or precisely apply the motion condition, leading to distortion artifacts (Fig. 14a). Second, although the model can synthesize plausible secondary effects associated with edited objectssuch as water splashes (Fig. 2, main paper) or shadows (Fig. 9, main paper), it may fail to handle more complex physical phenomena (e.g., liquid dynamics) that arise from the target motion. For instance, in Fig. 14b, the model fails to synthesize the mixing of coffee and milk when the pouring action is redirected into the other milk bottle. 4 Figure 14. Limitations. Our model shows limitations in two main areas: (a) small objects with large motion changes (e.g., 270 frontflip) can suffer from distortion when their tracks are densely-clustered and noisy; and (b) complex, motion-dependent physical effects (e.g., liquid dynamics) are not synthesized correctly, such as the failure to blend coffee and milk. Table 5. Inference runtime and resolution. The table reports runtime on single A100-80GB GPU, the number of parameters in the base models, and the inference resolutions for our method and all baselines. Method Base model # params Temporal length ReVideo [74] TrajAttn [118] TrajAttn [118]+ [126] DaS [27] PaC [12] ATI [99] GEN3C [84] TrajCrafter [127] Ours SVD [8] SVD [8] SVD [8] CogVX [121] SD1.5 [85] Wan [98] Cosmos [1] CogVX [121] Wan [98] 1.5B 1.5B 1.5B 5B 0.9B 14B 7B 5B 1.3B 14 25 25 49 16 81 121 49 C. Additional Comparisons Spatial resolution Runtime 1344 1024 1024 720 768 832 1280 672 672 768 576 576 480 512 480 704 384 (min) 1.7 2.0 6.5 4.4 1.4 14.0 12.7 3.2 4.5 In this section, we present additional quantitative comparisons with existing methods. We also report the details for inference runtime and output resolutions in Table 5. We highly encourage our readers to view our project webpage for full video comparisons of motion editing on in-the-wild videos. C.1. Evaluation on DyCheck DyCheck [23] provides synchronized multi-view videos with depth and camera-pose annotations, captured by one moving camera and two stationary cameras. The multi-view videos in dynamic scene enable us to validate manipulations of camera and object motion, either jointly or independently. We derive three distinct evaluation scenarios: Joint camera and object motion: evaluated by extracting two non-overlapping clips from each moving-camera video, which naturally contains both motion types (12 scenes). 5 Table 6. Quantitative comparison on camera motion only. We evaluate camera motion only control using the synchronized multiview video pairs in the DyCheck Dataset [23]. Method GEN3C [84] TrajCrafter [127] Ours PSNR 13.03 13.41 13.75 SSIM .307 .304 .302 LPIPS .544 .551 .481 mPSNR 14.64 14.97 15.00 mSSIM .723 .729 .721 mLPIPS .382 .358 .316 Table 7. Quantitative comparison on object motion only. We compare our method with track-conditioned I2V methods on the object motion only task (with static camera) on DyCheck [23]. Note that the baseline I2V methods directly use the ground-truth first frame as input, while our method operates without this privileged input. Method PSNR ReVideo [74] TrajAttn [118] DaS [27] PaC [12] ATI [99] Ours 16.22 15.01 17.15 16.63 16.79 18.11 SSIM .595 .415 .600 .559 .652 .629 LPIPS .395 .370 .265 .328 .263 .261 mPSNR 18.86 17.13 20.20 19.42 20.11 21.03 mSSIM .768 .594 .766 .729 .826 .808 mLPIPS .268 .244 .137 .218 .128 .149 Camera motion only: evaluated using synchronized pair, where the moving-camera video serves as input and the corresponding fixed-view video as ground truth (5 scenes). Object motion only: evaluated by extracting two nonoverlapping clips from fixed-view video (4 scenes). For the joint-motion and object-motion-only scenarios, we require training pairs consisting of two non-contiguous clips (81 frames each) from the same video. Because the Input Ground truth Ours DaS [27] PaC [12] ATI [99] GEN3C* [84] TrajCrafter* [127] ReVideo* [74] TrajAttn* [118] Figure 15. Visual comparison of joint motion editing on DyCheck [23]. Our method handles large, joint camera and object motion changes, aligning with the (pseudo) ground-truth video. Methods marked with use the flow estimation between input and ground-truth videos to warp the input. The results of TrajAttn are generated with the extension of NVS-Solver [126] to take the warped video input. Figure 16. Human perceptual evaluation. We present the preference percentages from 42 subjects in comparison with representative methods, including track-conditioned I2V, DaS [27] and ATI [99], and inpainting-based V2V, GEN3C [84]. provided multi-view dataset contains videos of varying durations, we select only those with sufficient length to extract such pairs. The resulting clips have an average temporal gap of 99 frames (minimum 17, median 77, maximum 222). For fair evaluation, we first crop all videos to landscape orientation. All methods are evaluated at resolution of 672 384. To accommodate baselines that generate fewer than 81 frames, we apply temporal stride (2 or 3) to the input video, ensuring access to the full video content in single pass. We then compute error metrics only on the corresponding subsampled output frames. Note that ReVideo [74] is an exception because it outputs only 14 frames. For this method, we provide the 81-frame video subsampled with temporal stride of 3 (yielding 27-frame input). We then run the model twice on consecutive segments of this input and concatenate the results to obtain 27-frame output video for evaluation. For our primary task of joint camera and object motion control, we present the quantitative comparisons in the main paper, and the visual comparison in Fig. 15. Table 6 presents the quantitative comparisons on the cameramotion-only task. Our approach performs comparably to dedicated camera-controlled V2V methods [84, 127], which rely on dense depth warping. Furthermore, in the objectmotion-only task  (Table 7)  , while tracked-conditioned methods [12, 27, 74, 99, 118] use the ground-truth first frame, our approach achieves overall better scores due to its effective use of context from the input video. C.2. User Study on Video Editing We conduct human perceptual evaluation with 42 subjects using the Two-Alternative Forced Choice (2AFC) method to assess 10 real-world motion editing cases, including camera and/or object motion editing. Subjects assessed output quality based on three critical aspects: (i) alignment with desired motion, (ii) preservation of input context, and (iii) perceived visual quality. As reported in Fig. 16, our method consistently shows higher preference over the representative baselines, DaS [27], ATI [99], and GEN3C [84] across all three key aspects of the video motion editing task. C.3. Comparison with ReCamMaster ReCamMaster [3] is camera-controlled video-to-video diffusion model that uses target camera extrinsics to edit viewpoints. While it shares the same Wan2.1-T2V-1.3B base model as our method, its design is fundamentally different. First, ReCamMaster conditions only on the target extrinsics and ignores the source camera parameters of the input video. This design choice relies on key assumption: the input and target output videos must share the same 6 In contrast, our method does not have this first frame. same-first-frame requirement. Our training pairs are constructed from two non-contiguous clips from monocular video, where the first frames of the clips are different by design. Second, although directly inputting camera extrinsics is straightforward, this approach can suffer from scale ambiguity, which limits accurate camera-motion control. Our method avoids this issue by representing camera motion using screen-projected point tracks, enabling more flexible and precise control."
        }
    ],
    "affiliations": [
        "Adobe",
        "Adobe Research",
        "University of Maryland College Park"
    ]
}