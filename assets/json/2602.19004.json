{
    "paper_title": "MoBind: Motion Binding for Fine-Grained IMU-Video Pose Alignment",
    "authors": [
        "Duc Duy Nguyen",
        "Tat-Jun Chin",
        "Minh Hoai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We aim to learn a joint representation between inertial measurement unit (IMU) signals and 2D pose sequences extracted from video, enabling accurate cross-modal retrieval, temporal synchronization, subject and body-part localization, and action recognition. To this end, we introduce MoBind, a hierarchical contrastive learning framework designed to address three challenges: (1) filtering out irrelevant visual background, (2) modeling structured multi-sensor IMU configurations, and (3) achieving fine-grained, sub-second temporal alignment. To isolate motion-relevant cues, MoBind aligns IMU signals with skeletal motion sequences rather than raw pixels. We further decompose full-body motion into local body-part trajectories, pairing each with its corresponding IMU to enable semantically grounded multi-sensor alignment. To capture detailed temporal correspondence, MoBind employs a hierarchical contrastive strategy that first aligns token-level temporal segments, then fuses local (body-part) alignment with global (body-wide) motion aggregation. Evaluated on mRi, TotalCapture, and EgoHumans, MoBind consistently outperforms strong baselines across all four tasks, demonstrating robust fine-grained temporal alignment while preserving coarse semantic consistency across modalities. Code is available at https://github.com/bbvisual/ MoBind."
        },
        {
            "title": "Start",
            "content": "MoBind: Motion Binding for Fine-Grained IMUVideo Pose Alignment Australian Institute for Machine Learning, Adelaide University, Adelaide, SA 5000, Australia"
        },
        {
            "title": "Duc Duy Nguyen",
            "content": "Tat-Jun Chin Minh Hoai 6 2 0 2 2 2 ] . [ 1 4 0 0 9 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We aim to learn joint representation between inertial measurement unit (IMU) signals and 2D pose sequences extracted from video, enabling accurate cross-modal retrieval, temporal synchronization, subject and body-part localization, and action recognition. To this end, we introduce MoBind, hierarchical contrastive learning framework designed to address three challenges: (1) filtering out irrelevant visual background, (2) modeling structured multi-sensor IMU configurations, and (3) achieving finegrained, sub-second temporal alignment. To isolate motionrelevant cues, MoBind aligns IMU signals with skeletal motion sequences rather than raw pixels. We further decompose full-body motion into local body-part trajectories, pairing each with its corresponding IMU to enable semantically grounded multi-sensor alignment. To capture detailed temporal correspondence, MoBind employs hierarchical contrastive strategy that first aligns token-level temporal segments, then fuses local (body-part) alignment with global (body-wide) motion aggregation. Evaluated on mRi, TotalCapture, and EgoHumans, MoBind consistently outperforms strong baselines across all four tasks, demonstrating robust fine-grained temporal alignment while preserving coarse semantic consistency across modalities. Code is available at https://github.com/bbvisual/ MoBind. 1. Introduction Understanding human motion is critical for wide range of applications, including action recognition [40, 41], sports performance analysis [16, 2830, 43], and rehabilitation monitoring [3, 24]. However, human motion is often subtle or hard to sense from single modality. This limitation can be mitigated by integrating signals from complementary sensing sources, such as video recordings and IMUs. Video can offer rich spatial and semantic information but is sensitive to occlusion, viewpoint changes, and limited frame rates, while IMUs provide precise, temporally dense motion signals but lack visual context, making the captured motion difficult to interpret. To fully leverage the strengths Figure 1. Proposed framework for motion binding between IMUs and 2D pose sequence from video. Contrastive learning is applied at both the local space, aligning each IMU with its corresponding body-part, and the global space, aligning full-body representations. This representation supports several downstream tasks, including cross-modal retrieval, temporal synchronization, subject and body parts localization, and human action recognition. of both modalities, it is essential to develop joint representation that establishes meaningful correspondence between themboth at the coarse-grained action category level and the fine-grained, sub-second temporal alignment. Such joint representation that aligns IMU signals and video-based body motion would unlock several important capabilities. First, it enables IMU-Video temporal synchronization without cumbersome, explicit calibration procedures. Traditional calibration often requires global timestamps, trigger pulses, or manual alignmentmethods that can be technically demanding and error-prone, especially for non-expert users. learned joint representation enables synchronization based purely on the content of the signals, making it more accessible to collect and use multimodal data. This is important because multimodal data is only truly useful when the different streams are temporally aligned. Second, it supports cross-modal retrieval, where one modality can be used to query relevant information from database in the other. This is especially valuable in privacy-sensitive scenarios where synchronized video data may be unavailable or restricted, yet similar examples can still be retrieved from the database for visualization and en1 hanced understanding. Third, it facilitates spatial localization, i.e., associating each IMU with the correct person in the video, which in turn stabilizes tracking in multi-person scenes, and identities can be maintained under occlusions or re-entries. Together, these capabilities make multimodal datasets more reliable to collect, curate, and analyze beyond controlled lab settings. In practice, much recent work on IMUvision coupling targets human activity recognition (HAR) by learning shared embedding space with contrastive objectives [9, 12, 27, 35, 44]. Typically, each clip or window is projected to single global embedding, and training relies on matched/mismatched pairs. This design might excel at coarse semantic discrimination (e.g., action categories) but overlooks fine-grained temporal structure: segments that differ only by phase shifts, short lags, or repetition boundaries (i.e., distinct cycles of the same action) collapse to nearby codes. Consequently, the resulting representations become insensitive to true temporal synchrony, limitation that hinders calibration-free temporal synchronization, subsecond cross-modal retrieval, and spatial localization. This limitation motivates us to develop joint representation that explicitly models the fine-grained temporal dynamics between IMU and video while keeping the coarse semantics. In searching for an effective approach to fine-grained IMUvideo alignment, we found no prior work that directly targets this setting. We therefore first examined joint audiovideo representation learning designed for sub-second alignment [1, 5, 11, 22], but these techniques do not transfer well to the IMUvideo setting. Unlike audio, which often correlates with multiple visual instances in the scene and provides scene-level cues, IMU signals are localized and strictly motion-centric, making most visual background irrelevant. Moreover, IMUs are commonly deployed in multisensor configurations, each attached to different body part, naively concatenating these signals fails to capture their spatial and temporal specificity. Lastly, synchronization cues in audiovideo data exist in multiple formssome are momentary and localized (e.g., door closures, hand claps), while others are more continuous and span broader contexts (e.g., background music aligned with scene changes) [17]. Both types are often informative for alignment. In contrast, human motion tends to exhibit continuous and highly repetitive patterns (e.g., walking cycles), producing abundant but highly similar synchronization cues. This repetition can lead to ambiguous alignments, particularly when distinct motion segments appear nearly identical. Taken together, these factors limit the direct applicability of audiovideo synchronization techniques and highlight the need for tailored strategies that model sensor-specific dynamics and their alignment with motion-derived visual cues. In this paper, we introduce contrastive learning framework illustrated in Fig. 1, that addresses key limitations of prior works. To focus on motion-relevant cues and reduce irrelevant visual background, we learn joint representation between IMU signals and video-derived skeletal motion sequences instead of raw pixels. To support multi-sensor IMU setups, we decompose the extracted skeleton into local body-parts and align each with its corresponding IMU signal, enabling structured, semantically grounded association. To achieve fine-grained temporal alignment, we use hierarchical objective: local term aligns each IMUpart pair on short segments (sub-second synchrony), and global term aggregates local features into full-body, multi-IMU embeddings. To retain action-level semantics, we introduce Masked Token Prediction (MTP) auxiliary task, optimized jointly with the contrastive loss. Evaluated on mRi [2], TotalCapture [38], and EgoHumans [20], our method consistently outperforms competing approaches across crossmodal retrieval, temporal synchronization, subject/bodypart localization, and action recognition. 2. Related Work 2.1. Multi-modal contrastive learning In the context of contrastive learning, most IMU work has focused on improving encoders for human activity recognition (HAR) [8, 9, 14, 42, 44]. However, following the success of CLIP [31] in visionlanguage modeling, attention has shifted toward learning semantically meaningful cross-modal representations involving IMU signals. IMU2CLIP [27] and ImageBind [12] align IMU features with the CLIP space, typically via egocentric video, to enable IMU and video retrieval. UniMTS [45] maps synthetic IMU from motion data also into the CLIP space to pair with textual description, enabling zero-shot HAR, while DeSPITE [23] builds joint embedding across LiDAR, skeletons, and IMU, optionally incorporating CLIP text when annotations exist, enabling cross-modal retrieval. Although these approaches share the goal of learning joint representations, they generally operate at global (clip-level), which fails to preserve fine-grained temporal alignment. As our experiments indicate, such formulations lack the precision required for sub-second cross-modal synchronization. 2.2. Temporal synchronization Temporal synchronization across modalities has been extensively studied in the audio-visual (AV) domain [4 6, 11, 26, 33]. An early approach [33] leveraged statistical correlations between MFCCs from audio and mouth appearance in video using correlation to solve AV synchronization. SyncNet [5] was among the first to introduce deep contrastive framework that aligns speech and mouth motion at clipand frame-level granularity, and DiVAS [11] extended this line with transformer-based model that is robust to varying frame rates. More re2 Figure 2. Overview of the proposed MoBind. The framework first encodes each IMU stream together with the motion of its corresponding body part, yielding token-level and local-level representations per sensor. These local representations are then aggregated across sensors to form global-level embeddings. The contrastive objective applies at all three levels. In addition, Masked Token Prediction (MTP) module is used only during training to preserve coarse semantic structure, preventing the model from over-focusing on fine-grained alignment. cent AV methods [17, 18] replace pure contrastive objectives by tokenizing audio and video and formulating synchronization as temporal offset classification. Inspired by these two paradigmscontrastive alignment and offset classificationwe instantiate two strong baselines for IMU-video synchronization by aligning IMU signals with video-extracted skeletal motion. Despite the rich AV literature, IMU-video synchronization remains underexplored. The most closely related work is SyncWISE [46], which estimates temporal offsets by computing correlations between IMU signals and optical-flow derivatives. In contrast, our work provides fully automatic, learning-based solution that avoids manual gestures or ad-hoc heuristics, making synchronization practical in real-world settings. 2.3. IMU-to-Subject Association The task of associating an IMU with the correct individual in video has been previously explored. This task is conceptually similar to active speaker detection in audio-visual research [5, 11, 32, 36], whose goal is to link audio sources to corresponding visual entities. VIT [15] formulates the association as graph-labeling task, assigning IMU identities to visual tracklets based on orientation consistency. VIPL [34] learns shared visual-inertial feature space using contrastive loss to match persons wearable-IMU motion with their video motion. Vi-Fi [25] introduces recurrent multimodal network that estimates affinity matrices between camera tracks and IMU/device IDs. In contrast to these methods, our approach addresses both spatial association (who is carrying the IMU) and temporal synchronization within unified learning framework. Moreover, by leveraging the locality of IMU signals, our model also infers body-part attribution-determining not only who is wearing given IMU, but also where on the body it is worn. 3. MoBind This section presents MoBind, an end-to-end framework for learning joint representations between wearable IMU signals and video-based human motion  (Fig. 2)  . From the video, skeletal joint sequences are extracted, while raw IMU streams from body-mounted sensors are simultaneously processed. Modality-specific modules produce representations trained with contrastive objective to align synchronized IMUvideo pairs and separate mismatched ones. Unlike prior work that compresses inputs into single global vector, MoBind preserves spatial and temporal structure by capturing body-part motion in local representations, aggregating them into global one, and modeling token-level temporal dynamics. This hierarchical design can capture fine-grained motion details. However, an exclusive fine-grained focus can under-represent coarse semantics beneficial for HAR. To address this, we add Masked Token Prediction (MTP) module tailored to IMU inputs, encouraging embeddings to retain high-level semantics while emphasizing temporal granularity. Together, hierarchical alignment and MTP enable the model to learn localized motion patterns and fine-grained dynamics, yielding robust cross-modal alignment and improved HAR performance. 3.1. Modality-Specific Modules IMU Module. The input to the IMU module consists of IMU signals, each captured from sensor mounted on limb of the human body. Let RF denote the data from specific IMU, where is the number of frames and is the number of sensor channels. We design our IMU encoder Eimu as combination of 1D convolutional blocks followed by Transformer layer, which encodes into sequence of temporal tokens as follows. stack of 1D convolutional layers is first applied to 3 X, preserving the temporal resolution and producing an intermediate feature map ˆX RF D. This sequence is then divided into non-overlapping temporal patches ˆX1, . . . , ˆXT Rf D, where = F/f . Each patch is flattened and linearly projected into D-dimensional vector, forming the input for the Transformer layer. The output of the Transformer is sequence of temporal tokens = [Z1, . . . , ZT ] where Zt RD. The local representation RD for the individual IMU sequence is then obtained by applying mean pooling across the temporal dimension of the token sequence. Let {Zn}N n=1 denote the per-sensor local representations, Zn RD. We aggregate them by concatenation followed by normalizationMLP block: = MLP(cid:0)LayerNorm(cid:0)Zcat (cid:1)(cid:1) RD . We refer to MLPLayerNorm as the aggregator block, and as the resulting global representation. RF 2J Pose Module. The sequence of 2D skeletal joint coordinates is first extracted from an input video. Given the known mounting positions of the IMU sensors, we decompose the full-body motion sequence into part-specific segments Xpart , each corresponding to subset of joints. Using the same architectural design as the IMU stream, we employ body-part encoder Epart, composed of 1D convolutional layers followed by Transformer. For each body part, the encoder outputs sequence of temporal tokens of dimensions, which encodes the dynamics of the body parts motion. Similar to the IMU stream, these temporal tokens are averaged to produce local representation for the entire sequence. To derive global pose representation Gpart, we concatenate the local vectors and pass them through an aggregator block. While the number of frames in the video and IMU streams may differ, we ensure that the number of temporal tokens are the same across the two modalities. 3.2. Hierarchical Contrastive Alignment with its counterpart Zpart To learn fine-grained cross-modal alignment, we adopt contrastive learning in hierarchical scheme: (i) tokenlevel alignment, which matches individual temporal tokens across modalities to promote fine-grained correspondence along time (i.e., aligning Zimu ); (ii) local-level alignment, where each IMU sensor is aligned with the motion of its corresponding body part by conpart trasting ; and (iii) global-level alignment, where the aggregated IMU representation Gimu is aligned with the global skeletal representation Gpart. Before applying the contrastive objective, representations from both modalities are projected into shared embedding space using modality-specific linear projection layers. and imu batch of paired samples (indexed by = 1, . . . , K) as: LAB global = 1 (cid:88) i= log exp (cid:0)s (cid:0)GA,i, GB,i(cid:1) /τ (cid:1) j=1 exp (s (GA,i, GB,j) /τ ) (cid:80)K , LAB local = 1 (cid:88) i=1 log (cid:16) (cid:16) A,i exp , B,i(cid:17) (cid:17) (cid:80)K j=1 exp (cid:16) (cid:16) Z A,i , /τ B,j(cid:17) (cid:17) , /τ where s(, ) denotes cosine similarity, and τ is learnable temperature parameter. Similarity, the token-level contrastive from modality to modality is defined as: (cid:17) (cid:16) (cid:17) (cid:88) (cid:88) i= t=1 (cid:16) exp log (cid:80)T j=1 exp , ZB,i ZA,i (cid:16) (cid:16) ZA,i , ZB,i /τ (cid:17) (cid:17) , /τ LAB token = 1 Hence, the contrastive learning objective considering bidirectional alignment between the IMU and Pose modalities, is defined as: (cid:16) (cid:16) (cid:16) Lglobal = Llocal = Ltoken = Limupart global + Lpartimu Limupart + Lpartimu local local global Limupart token + Lpartimu token (cid:17) (cid:17) (cid:17) /2, /2, /2, Lalign = λgLglobal + λlLlocal + λtLtoken, where λg, λl and λt are weighting coefficients that balance the contributions to the contrastive loss. For brevity, we omit the IMU sensor index in the local-level and tokenlevel formulations to keep the notation uncluttered. 3.3. Masked Token Prediction (MTP) While the hierarchical contrastive objectives capture finegrained IMUpose synchrony, they can under-represent the action-level semantics that are useful for downstream action recognition tasks. To complement alignment with semantics-preserving signal, we introduce Masked Token Prediction (MTP) auxiliary task that runs in parallel to the alignment branch and is applied to the IMU stream. Let Zimu RN D denote the IMU temporal tokens for sensors and timesteps before local pooling; for brevity, we drop the imu superscript. We sample mask set {1, . . . , } {1, . . . , } with = αN and replace the selected tokens with learnable query vector qmask, yielding the masked sequence Zmask. lightweight Transformer Dmtp takes the masked sequence Zmask as input and uses the unmasked context to predict the missing tokens, Zpred for (n, t) M. MTP loss is the mean-squared error over masked positions: n,t = (cid:2)MLP(cid:0)Dmtp(Zmask)(cid:1) (cid:3) n,t Lmtp = 1 (cid:88) (n,t)M (cid:13) (cid:13)Zpred (cid:13) n,t Zn,t (cid:13) 2 (cid:13) (cid:13) 2 (1) We adopt InfoNCE loss [39] to define global and local contrastive objectives from modality to modality for We jointly optimize MTP with the alignment objective using scalar weight λmtp: = Lalign + λmtp Lmtp. 4 mRi TotalCapture EgoHumans Method IMUVideo VideoIMU IMUVideo VideoIMU IMUVideo VideoIMU R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 IMU2CLIP 0.67 0.57 DeSPITE 0.77 SyncNet 0.94 MoBind 0.88 0.85 0.94 0.99 0.92 0.91 0.97 1.00 0.38 0.32 0.75 0. 0.69 0.70 0.92 0.99 0.81 0.82 0.95 0.99 0.06 0.03 0.51 0.87 0.20 0.15 0.87 0.96 0.33 0.24 0.94 0.98 0.07 0.03 0.54 0. 0.17 0.11 0.86 0.91 0.27 0.25 0.93 0.97 0.29 0.54 0.74 0.83 0.51 0.73 0.80 0.93 0.59 0.80 0.93 0.96 0.29 0.54 0.71 0. 0.51 0.74 0.89 0.93 0.60 0.80 0.93 0.95 Table 1. Cross-modal retrieval performance on the mRi, TotalCapture and EgoHumans datasets. We compare our method against prior contrastive learning baselines in both retrieval directions: IMUVideo and VideoIMU. Our method consistently outperforms all others across all ranks, demonstrating strong alignment between modalities. This superior performance is particularly critical for downstream tasks that rely on accurate similarity scores between embedded features. 4. Experiments MoBind provides joint representation that supports range of downstream tasks: cross-modal retrieval, temporal synchronization, subject and body-part localization, and action recognition. We first introduce the evaluation datasets, thenfor each downstream taskdetail the setup, metrics, baselines, and experiment results. Finally, we present ablation studies that quantify the contribution of key design choices and evaluate robustness to real-world conditions via simulated sensor dropouts. 4.1. Datasets & Training We perform experiments on three multimodal datasets: mRi [2], TotalCapture [38], and EgoHuman [20], where EgoHumans contains multi-person scenes. MoBind is trained with fixed 5 windows, and the same window length is used for retrieval, localization, and action recognition. To reflect realistic deployment conditions, we use estimated 2D keypoints throughout all experiments: pose sequences are predicted with MMPose software [7] using RTMPose [19]. We adopt subject-wise splits for mRi and TotalCapture, and non-overlapping scene split for EgoHumans. Using 5 input window, each segment yields = 25 temporal tokens. We use 256-dimensional embeddings for both local (D) and global (D) representations. Loss weights are λg = 1.0, λl = 1.0, λt = 0.5, and λmtp = 0.3. The MTP auxiliary task uses masking ratio α = 0.75. MoBind is trained with Adam optimizer [21] (learning rate 1104, batch size 1356) using early stopping based on the R@1 retrieval score on the validation set, with patience of 500 epochs. Training runs on single NVIDIA GeForce RTX 5090 and takes about 2.5 hours per run. More details can be found in the supplementary material. 4.2. Cross-Modal Retrieval Experiment setup. Given short segment from one modality, the goal is to retrieve the corresponding moment in the other. For each batch of IMUvideo data, we first extract 2D pose sequences from the videos, then embed all IMU and pose segments, and compute cosine similarity scores for every queryreference pair. These scores form similarity matrix, where the entry at row and column represents the similarity between query IMU and reference pose sequence j. For IMU-to-Video retrieval, we sort each querys scores in descending order to rank pose references, with higher scores indicating better matches. The process is analogous for Video-to-IMU retrieval, where pose segments act as queries and IMU segments as references. Baselines. We compare MoBind against strong contrastive IMU baselines: IMU2CLIP [27], DeSPITE [23], and SyncNet [5] (originally for audiovisual synchronization, here adapted to the IMUpose setting). We adopt multipositive contrastive mechanism, where single pose sequence can correspond to multiple IMU sensors, and the final IMU embedding is obtained by averaging their representations. All experiment results are reported under this setting. The concatenated variant and additional implementation details for MoBind and the baselines are provided in the supplementary material. We evaluate bidirectional retrieval (IMUVideo and VideoIMU) using the Recall@k metrics, with {1, 5, 10}. Results. Table 1 compares retrieval on mRi, TotalCapture, and EgoHumans. MoBind consistently outperforms all baselines in both retrieval directions, with especially large gains on mRi and TotalCapture. On mRi, among the R@1 errors, the share whose top-1 impostor belongs to the same action class is 0.79 (IMU2CLIP), 0.76 (DeSPITE), and 0.75 (SyncNet). This confusion indicates that these methods compress sequences into single global vector that primarily encodes coarse semantics, useful for action recognition but insufficient for instance-level alignment. On TotalCapture, the effect is starker: the median hard-negative margin (cosine similarity of the true match minus that of the hardest impostor) is -0.14, -0.11, and -0.14 for IMU2CLIP, DeSPITE, and SyncNet, respectively, whereas MoBind attains +0.10. Thus, for the baselines, the hardest impostor is typically more similar than the true match, while MoBind maintains positive gap. These findings are consistent with the quantitative gains and indicate that global-only embeddings struggle in TotalCaptures highly dynamic scenes, where 5 Method mRi TotalCapture EgoHumans MAE Acc MAE Acc MAE Acc Random guess SyncWISE IMU2CLIP DeSPITE SyncNet IMUSync MoBind 10.72 3.31 1.17 1.42 0.89 0.72 0.47 0.00 0.04 0.70 0.63 0.76 0.75 0.88 10.06 4.07 2.32 2.38 1.85 0.96 0.05 0.00 0.02 0.13 0.16 0.21 0.71 0. 10.18 3.68 3.13 2.58 2.93 1.01 0.04 0.01 0.02 0.44 0.58 0.39 0.82 1.00 Table 2. Synchronization results on three datasets. All models are evaluated on 20-second videos with random temporal offsets sampled uniformly from [7, 7] seconds. Top-k retrieval is performed with = 5. Our method significantly outperforms baselines in both mean absolute error (MAE, in seconds) and accuracy (Acc, within 200ms tolerance) across all datasets. Figure 4. Per-action synchronization accuracy on EgoHumans (left) and mRi (right). MoBind achieves sub-50ms error on all EgoHumans actions and under 1s on all mRi actions, despite the challenges posed by repetitive movements and near-duplicate segments. Results confirm MoBinds robustness across diverse motion types and environments. Baselines. In addition to the contrastive baselines, and inspired by recent learning-based audiovideo synchronization methods [17, 18], we design an offset-classification baseline, which we call IMUSync. We also include SyncWISE [46], correlation-based method. Performance is reported using mean absolute error (MAE) in seconds and accuracy within 200ms tolerance. Results. Table 2 demonstrates MoBinds ability to correct temporal misalignment between video and IMU streams. We perform bidirectional retrieval with k=5 using 20 clips, partitioned into overlapping 5 segments with 0.2 stride (yielding 76 segments per clip). This setup captures fine-grained temporal correspondences, moving beyond coarse retrieval to enable sub-second alignment. MoBind consistently achieves accurate synchronization across diverse real-world scenarios, including rehabilitation settings (mRi), dynamic actor-driven motions (TotalCapture), and complex multi-person activities (EgoHumans). Per-action synchronization results are visualized in Fig. 4, showing MoBinds robust offset estimation across wide range of action categories. On EgoHumans, the synchroFigure 3. IMUVideo retrieval results on mRi (left) and EgoHumans (right). Each example shows the query IMU signal, its corresponding ground-truth video segment, and the top three retrieved video segments. Our method successfully retrieves the ground-truth segment, and the other top-ranked results are also visually similar to the ground truth, demonstrating robust crossmodal alignment. whole-body motion overwhelms limb-level cues, hindering alignment to the worn IMUs. By first aligning at the local body-part level and then aggregating globally, MoBind preserves local information and remains robust. Qualitative IMU-Video retrieval examples are shown in Fig. 3. 4.3. Temporal Synchronization , Xpart Experiment Setup. Given paired IMU-video sequences, the goal of this task is to estimate the temporal offset between the two modalities. For this experiment, we construct test set of IMU-video sequences of 20s in duration and simulate misalignment by introducing random lags uniformly sampled from [7, 7]s between the modalities. We first extract the pose sequence from the video and then divide each modality into overlapping temporal windows, each with the length corresponding to duration of the temporal windows used for training MoBind (5s), resulting in paired segments (Ximu ). Each segment is passed through MoBind to obtain modality-specific feature embeddings (Gimu ). Next, we compute pairwise similarity matrix RN where each entry Dp,q is the cosine similarity between the p-th IMU window and the q-th pose window. For each IMU window p, we retrieve the top-k most similar pose windows based on Dp,q, and vice versa. Each matched pair (p, q) yields an offset vote: pq = with the associated similarity score Dp,q used as the votes weight. All offset votes from both retrieval directions are aggregated into weighted histogram, where each offset bin accumulates the similarity scores of matching pairs that fall into that bin. The final estimated temporal offset ˆδ is obtained as the bin with the highest total weight: , Gpart ˆδ = arg max (cid:88) Dp,q (p,q) : qp= 6 Figure 5. Examples of body-part localization on EgoHumans. Each column shows the query IMU (top) and the predicted body part with the highest similarity score (bottom), demonstrating accurate identification of sensor placement. Method IMU2CLIP DeSPITE SyncNet VIPL MoBind Acc F1-score 0.9446 0.9430 0.9759 0.9743 0.9749 0.9748 0.9014 0.8933 0.9812 0. Table 3. IMU-to-person identification in multi-person scenes from EgoHumans. This experiment evaluates who wears the IMU sensor, and MoBind achieves the highest accuracy and F1 score. nization error remains below 50ms for all actions. On mRi, while performance is slightly lower due to the presence of repetitive exercises that produce many near-duplicate segments and hard negatives, MoBind still maintains errors under one second for all categories. Importantly, MoBind supports synchronization of IMU and video sequences of arbitrary lengths. Longer input durations yield higher accuracy: on the challenging mRi dataset where many action classes involve repetitive movements, synchronization accuracy reaches 97% with 2minute clips and achieves perfect alignment (100%) with 3-minute clips. 4.4. Subject and Body-part Localization Experiment setup. Given synchronized IMU segment and multi-person video, we identify the IMU wearer by comparing the IMUs global embedding to each persons global pose embedding generated by MoBind, using cosine similarity. The person with the highest similarity is then assigned as the IMU wearer. For body-part association, we apply the same approach using local (body-part) embeddings: we compare the IMUs local embedding with all candidate body-part embeddings across individuals and assign the pair with the highest similarity. Results. Table 3 shows MoBinds person localization performance on EgoHumans, achieving the highest accuracy and F1-score compared to baselines, including VIPL [34]. MoBind incorporates local contrastive learning in addition to global alignment, enabling it to identify not only the person wearing an IMU sensor but also the specific body part to which the sensor is attached. As shown in Fig. 5, given query signal from specific IMU, our method correctly assigns it to the corresponding body part of the subject in the video. Across datasets, MoBind achieves bodypart localization accuracies of 0.81 (mRi), 0.57 (TotalCapture), and 0.63 (EgoHumans). Similair to synchronize, we Figure 6. Results for the challenging combined task of temporal synchronization and spatial localization. The query 20s IMU signal from the left wrist precedes the video by 6.8 s. MoBind accurately recovers this offset using weighted histogram and simultaneously localizes the signal to the correct body part, demonstrating robustness to dynamic and highly repetitive motion. Method UniMTS ImageBIND EVI-MAE Primus IMU2CLIP DeSPITE SyncNet MoBind mRi TotalCapture Finetune 1-NN Finetune 1-NN 0.95 0.95 0.54 0.82 0.79 0.79 0.97 0.98 0.36 0.40 0.43 0.80 0.85 0.87 0.85 0.87 0.66 0.72 0.63 0.57 0.60 0.50 0.68 0.72 0.53 0.47 0.66 0.67 0.68 0.53 0.63 0. Table 4. Human activity recognition results under finetuning and 1-NN setting on the mRi and TotalCapture. For consistency, fine-tuning results are reported at the 5th epoch. can also improve this accuracy with longer input window. Moreover, this capability naturally extends to the combined task of joint spatial grounding and temporal synchronization, as illustrated in Fig. 6. MoBind not only detects temporal misalignment but also correctly grounds the query IMU to the left wrist, demonstrating the frameworks flexibility and practical utility. 4.5. Human Action Recognition Experiment Setup. Beyond fine-grained temporal alignment, we evaluate the semantic quality of MoBind by applying it to human action recognition. We consider two classifiers: (1) 1-Nearest Neighbor (1-NN) [10, 13, 37], where each action is predicted based on the cosine similarity to the nearest labeled example in the representation space; (2) Fine-tuned classifier, where linear classification head is trained jointly with the IMU encoder. We benchmark MoBind against the baseline representations used throughout this work, as well as several recent state-of-the-art IMU encoders: UniMTS [45], ImageBind [12], EVI-MAE [44], and Primus [9]. These addiCombinations Retrieval Sync. local token MAE Acc Locali zation 0.34 0.77 0.94 0.31 0.78 0.92 0.74 0.81 0.49 0.86 0.47 0.88 0.22 0.75 0. global Table 5. Ablation studies on contrastive objectives on mRi. Results show consistent gains across all tasks as each contrastive level is added, highlighting the effectiveness of the hierarchical design. tional encoders are designed exclusively for IMU signals and are suitable for human activity recognition, but not for cross-modal synchronization or retrieval. Therefore, they were not included in the previous experiments. Results. The results are shown in Table 4. MoBind achieves the best overall performance across all settings, confirming the effectiveness of its learned representations. These results demonstrate that the embedding space not only supports fine-grained cross-modal alignment but also preserves higher-level semantic structure, enabling strong class-level discriminability. The improvement further highlights the importance of the MTP auxiliary task, which regularizes the model by preventing overfitting to alignment cues and helps retain action semantics critical for recognition tasks. 4.6. Ablation Study Multi-level Contrastive Objectives. Table 5 reports an ablation study of the alignment objectives and their impact on R@1 retrieval, synchronization, and body-part localization. We propose using contrastive losses at multiple levelstoken, single-sensor, and multi-sensorand find that each contributes meaningfully to the models overall performance. This validates our hierarchical design: fine-grained objectives capture sub-second, limb-specific cues, while the global objective aggregates full-body motion patterns, providing complementary benefits. Masked Token Prediction. Table 6 evaluates the impact of the Masked Token Prediction (MTP) auxiliary task on action recognition under both fine-tuning and 1-NN settings. Adding MTP helps MoBind preserve class-level semantics, yielding consistent gains across all settings and nearly 20% improvement on TotalCapture for both classifiers. This highlights the regularizing effect of MTP in retaining semantic information beneficial for downstream action recognition, while still maintaining MoBinds strong fine-grained temporal alignment performance. Robustness to Sensor Failure. MoBinds modular design enables it to operate effectively with any number of IMU sensors, maintaining functionality even when some sensors become unavailable. To prepare for and evaluate such failure scenarios, we train single model using complete sensor data augmented with random sensor dropouts. At test time, Figure 7. Robustness to Sensor Failure. Retrieval performance (R@1 and R@5) under different sensor availability conditions. R@k measures the percentage of queries for which the groundtruth video appears within the top-k retrieved results, given the representation computed from subset of IMU sensors. In general, using more IMUs provides more complete motion representation and thus improves retrieval accuracy. MoBind remains highly effective even when some sensors are unavailable, demonstrating strong performance under partial sensor input and highlighting its robustness for real-world deployment. mRi TotalCapture Finetune 1-NN Finetune 1-NN MoBind w/o MTP MoBind 0.97 0.98 0.76 0.86 0.55 0.72 0.53 0.71 Table 6. Effect of Masked Token Prediction on model performance. The MTP significantly improves action recognition on both datasets, demonstrating its importance for retaining actionlevel semantics. we simulate sensor failure by randomly masking out subsets of sensors and measuring IMU-to-video retrieval accuracy. As shown in Fig. 7, while performance naturally degrades with fewer sensors, it remains strong and continues to outperform or match baselines that rely on full sensor input. This highlights the strength of our design, which enables robust operation under partial sensor availabilityan essential feature for real-world deployment. 5. Conclusion MoBind is hierarchical contrastive framework for aligning IMU signals with video-based skeletal motion. It addresses prior limitations by (i) focusing IMUs on motion-relevant pose cues instead of raw pixels, (ii) modeling multi-sensor structure by aligning each body part with its corresponding IMU, and (iii) enforcing alignment at token, local, and global levels. This representation supports several downstream tasks and achieves state-of-the-art IMUvideo alignment on mRi, TotalCapture, and EgoHumans while retaining class semantics via the MTP auxiliary task. Acknowledgment: This work was funded by the Australian Institute for Machine Learning (Adelaide University) and the Centre for Augmented Reasoning, an initiative by the Department of Education, Australian Government."
        },
        {
            "title": "References",
            "content": "[1] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, and Andrew Zisserman. Self-supervised learning of audio-visual objects from video. In Proceedings of the European Conference on Computer Vision, 2020. 2 [2] Sizhe An, Yin Li, and Umit Ogras. mRI: Multi-modal 3d human pose estimation dataset using mmwave, RGB-d, and inertial sensors. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. 2, 5 [3] Md Atiqur Rahman Ahad, Anindya Das Antar, and Omar Shahid. Vision-based action understanding for assistive healthcare: short review. 2019. 1 [4] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. Audio-visual In Proceedings of the British synchronisation in the wild. Machine Vision Conference, 2021. 2 [5] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Asian Conference on Computer Vision Workshop, 2016. 2, 3, 5 [6] Soo-Whan Chung, Joon Son Chung, and Hong-Goo Kang. Perfect match: Improved cross-modal embeddings for audiovisual synchronisation. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2019. [7] MMPose Contributors. Openmmlab pose estimation toolbox and benchmark. https://github.com/openmmlab/mmpose, 2020. 5 [8] Gaole Dai, Huatao Xu, Hyungjun Yoon, Mo Li, Rui Tan, and Sung-Ju Lee. Contrastsense: Domain-invariant contrastive learning for in-the-wild wearable sensing. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(4), 2024. 2 [9] Arnav Das, Chi Ian Tang, Fahim Kawsar, and Mohammad Malekzadeh. Primus: Pretraining imu encoders with In Proceedings of the IEEE multimodal self-supervision. International Conference on Acoustics, Speech and Signal Processing, 2025. 2, 7 [10] Jianfeng Dong, Shengkai Sun, Zhonglin Liu, Shujie Chen, Baolong Liu, and Xun Wang. Hierarchical contrast for unsupervised skeleton-based action representation learning. In Proceedings of AAAI Conference on Artificial Intelligence, 2023. 7 [11] Clara Fernandez-Labrador, Mertcan Akcay, Eitan Abecassis, Joan Massich, and Christopher Schroers. Divas: Video and In Proaudio synchronization with dynamic frame rates. ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. 2, 3 [12] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. 2, [13] Tianyu Guo, Hong Liu, Zhan Chen, Mengyuan Liu, Tao Wang, and Runwei Ding. Contrastive learning from extremely augmented skeleton sequences for self-supervised action recognition. In Proceedings of AAAI Conference on Artificial Intelligence, 2022. 7 [14] Harish Haresamudram, Irfan Essa, and Thomas Plotz. Contrastive predictive coding for human activity recognition. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 5(2), 2021. 2 [15] Roberto Henschel, Timo von Marcard, and Bodo Rosenhahn. Simultaneous identification and tracking of multiple people using video and imus. 2019. 3 [16] Yifeng Huang, Duc Duy Nguyen, Lam Nguyen, Cuong Pham, and Minh Hoai. Count what you want: Exemplar identification and few-shot counting of human actions in the wild. In Proceedings of AAAI Conference on Artificial Intelligence, 2024. 1 [17] V. Iashin, W. Xie, E. Rahtu, and A. Zisserman. Sparse in space and time: Audio-visual synchronisation with trainable selectors. In Proceedings of the British Machine Vision Conference, 2022. 2, 3, [18] V. Iashin, W. Xie, E. Rahtu, and A. Zisserman. Synchformer: Efficient synchronization from sparse cues. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2024. 3, 6 [19] Tao Jiang, Peng Lu, Li Zhang, Ningsheng Ma, Rui Han, Chengqi Lyu, Yining Li, and Kai Chen. Rtmpose: Real-time multi-person pose estimation based on mmpose, 2023. 5 [20] Rawal Khirodkar, Aayush Bansal, Lingni Ma, Richard Newcombe, Minh Vo, and Kris Kitani. Ego-humans: An egoIn Proceedings of the centric 3d multi-human benchmark. International Conference on Computer Vision, 2023. 2, 5 [21] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In Proceedings of International Conference on Learning and Representation, 2015. 5 [22] Bruno Korbar, Du Tran, and Lorenzo Torresani. Cooperative learning of audio and video models from self-supervised synchronization. In Advances in Neural Information Processing Systems, 2018. 2 [23] Thomas Kreutz, Max Muhlhauser, and Alejandro Sanchez Guinea. Despite: Exploring contrastive deep skeletonpointcloud-imu-text embeddings for advanced point cloud human activity understanding. In Proceedings of the International Conference on Computer Vision, 2025. 2, 5 [24] Ying Li, Chenxi Wang, Yu Cao, Benyuan Liu, Joanna Tan, and Yan Luo. Human pose estimation based in-home lower In International Joint Conferbody rehabilitation system. ence on Neural Networks, 2020. 1 [25] Hansi Liu, Abrar Alali, Mohamed Ibrahim, Bryan Bo Cao, Nicholas Meegan, Hongyu Li, Marco Gruteser, Shubham Jain, Kristin Dana, Ashwin Ashok, Bin Cheng, and Hongsheng Lu. Vi-fi: Associating moving subjects across vision In ACM/IEEE International Conferand wireless sensors. ence on Information Processing in Sensor Networks, 2022. 3 [26] Etienne Marcheret, Gerasimos Potamianos, Josef Vopicka, and Vaibhava Goel. Detecting audio-visual synchrony using deep neural networks. In Interspeech, 2015. [27] Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Alireza Dirafzoon, Aparajita Saraf, Amy Bearman, and Babak 9 tual benefits. In Proceedings of the International Conference on Automatic Face and Gesture Recognition, 2018. 1 [41] Boyu Wang, Lihan Huang, and Minh Hoai. Active vision for In Proceedings of the early recognition of human actions. IEEE Conference on Computer Vision and Pattern Recognition, 2020. 1 [42] Huatao Xu, Pengfei Zhou, Rui Tan, Mo Li, and Guobin Shen. Limu-bert: Unleashing the potential of unlabeled data for imu sensing applications. In Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems, 2021. 2 [43] Jinglin Xu, Sibo Yin, Guohao Zhao, Zishuo Wang, and Yuxin Peng. Fineparser: fine-grained spatio-temporal action parser for human-centric action quality assessment. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 1 [44] Mingfang Zhang, Yifei Huang, Ruicong Liu, and Yoichi Sato. Masked video and body-worn imu autoencoder for egocentric action recognition. In Proceedings of the European Conference on Computer Vision, 2025. 2, [45] Xiyuan Zhang, Diyan Teng, Ranak Roy Chowdhury, Shuheng Li, Dezhi Hong, Rajesh K. Gupta, and Jingbo Shang. UniMTS: Unified pre-training for motion time series. In Advances in Neural Information Processing Systems, 2024. 2, 7 [46] Yun C. Zhang, Shibo Zhang, Miao Liu, Elyse Daly, Samuel Battalio, Santosh Kumar, Bonnie Spring, James M. Rehg, and Nabil Alshurafa. Syncwise: Window induced shift estimation for synchronization of video and accelerometry from wearable sensors. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 4(3), 2020. 3, 6 Damavandi. Imu2clip: Multimodal contrastive learning for imu motion sensors from egocentric videos and text. arXiv preprint arXiv:2210.14395, 2022. 2, 5 [28] Dan Morris, T. Scott Saponas, Andrew Guillory, and Ilya Kelner. Recofit: using wearable sensor to find, recognize, and count repetitive exercises. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 2014. 1 [29] Duc Duy Nguyen, Lam Thanh Nguyen, Yifeng Huang, Cuong Pham, and Minh Hoai. Class-agnostic repetitive action counting using wearable devices. IEEE Transactions on Pattern Analysis and Machine Intelligence, 47(6), 2025. [30] Paritosh Parmar and Brendan Tran Morris. What and how well you performed? multitask learning approach to action quality assessment. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019. 1 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning, 2021. [32] Muhammad Shahid, Cigdem Beyan, and Vittorio Murino. Svvad: Visual voice activity detection by motion segmentation. In Proceedings of the IEEE Workshop on Applications of Computer Vision, 2021. 3 [33] Malcolm Slaney and Michele Covell. Facesync: linear operator for measuring synchronization of video facial images and audio tracks. In Advances in Neural Information Processing Systems, 2000. 2 [34] Xi Sun, Xinshuo Weng, and Kris Kitani. When we first met: Visual-inertial person localization for co-robot rendezvous. In Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems, 2020. 3, 7 [35] Shuhan Tan, Tushar Nagarajan, and Kristen Grauman. Egodistill: Egocentric head motion distillation for efficient In Advances in Neural Information video understanding. Processing Systems, 2023. 2 [36] Ruijie Tao, Zexu Pan, Rohan Kumar Das, Xinyuan Qian, Mike Zheng Shou, and Haizhou Li. Is someone speaking? exploring long-term temporal features for audio-visual active speaker detection. In Proceedings of the ACM International Conference on Multimedia, 2021. 3 [37] Fida Mohammad Thoker, Hazel Doughty, and Cees G. M. Snoek. Skeleton-contrastive 3d action representation learning. In Proceedings of the 29th ACM International Conference on Multimedia, 2021. [38] Matt Trumble, Andrew Gilbert, Charles Malleson, Adrian Hilton, and John Collomosse. Total capture: 3d human pose estimation fusing video and inertial sensors. In Proceedings of the British Machine Vision Conference, 2017. 2, 5 [39] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. ArXiv, 2018. 4 [40] Boyu Wang and Minh Hoai. Predicting body movement and recognizing actions: an integrated framework for mu-"
        }
    ],
    "affiliations": [
        "Australian Institute for Machine Learning, Adelaide University, Adelaide, SA 5000, Australia"
    ]
}