{
    "paper_title": "UMoE: Unifying Attention and FFN with Shared Experts",
    "authors": [
        "Yuanhang Yang",
        "Chaozheng Wang",
        "Jing Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify the MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, revealing an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 0 6 2 7 0 . 5 0 5 2 : r UMoE: Unifying Attention and FFN with Shared Experts Yuanhang Yang1, Chaozheng Wang2, Jing Li3 1Institute of Science Tokyo, Tokyo, Japan 2The Chinese University of Hong Kong, Hong Kong, China 3Hong Kong Polytechnic University, Hong Kong, China yang.y.ea2c@m.isct.ac.jp czwang23@cse.cuhk.edu.hk jing-amelia.li@polyu.edu.hk"
        },
        {
            "title": "Abstract",
            "content": "Sparse Mixture of Experts (MoE) architectures have emerged as promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify the MoE designs in attention and FFN layers by introducing novel reformulation of the attention mechanism, revealing an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components."
        },
        {
            "title": "Introduction",
            "content": "Scaling plays crucial role in advancing the capabilities of large language models [1, 2, 3]. However, this scaling advantage comes with substantial computational costs, making continued scaling increasingly impractical. Sparse Mixture-of-Experts (MoE) architectures have emerged as promising solution by selectively activating only subset of model parameterstermed expertsfor each input [4, 5, 6, 7]. This approach effectively decouples model size from computational cost, enabling efficient scaling with minimal overhead. Recent work has demonstrated the effectiveness of MoE in Transformer architectures [8, 9, 10, 5, 11], particularly when applied to feed-forward neural network (FFN) layers. Building on this success, several studies have explored extending MoE to attention layers [12, 13, 14], indicating potential for performance gains through attention scaling. Despite the potential, we find that existing MoE attention layers demonstrate suboptimal performance compared to FFN-MoE approaches, when provided with similar computational and parametric budgets. This performance gap challenges the practical utility of attention-MoE architectures, as parameters allocated to scaling attention layers might be more effectively utilized for scaling FFNs instead. We identify two distinctions between attention-MoE and FFN-MoE implementations that likely account for the observed performance differential: (1) the different expert design between attention and FFN layers, and (2) attention-MoEs necessity to compromise the expressiveness of vanilla attention mechanisms to accommodate sparse computation [12]. Motivated by these observations, we investigate compelling question: can we reformulate attention to reveal an underlying structure compatible with the same expert design as FFN layers, without sacrificing the expressive power of Preprint. Under review. Figure 1: Illustration of UMoE layer, which incorporates MoE into both FFN and attention modules with shared experts. The primary distinction between attention-MoE and FFN-MoE lies in an additional token mixing operation. Figure 2: Two formulations of the multi-head attention mechanism. (a) Vanilla attention interleaves mixing operations with value and output projections. (b) Pre-mixing attention performs token mixing prior to projections. the attention mechanism? This is challenging question due to the inherent complexity of attention mechanisms, including multiple projections and softmax calculations, which fundamentally differ from the straightforward two-matrix multiplication pattern of FFNs. To bridge this structural gap, we reformulate the attention mechanism to reveal its underlying FFN-like structure. Our reformulation decomposes attention into two sequential operations: token mixing and token-wise expert processing. The token-wise expert processing, consisting of two consecutive matrix multiplications, can be implemented as an FFN with small intermediate size. This implementation naturally aligns with recent advances in fine-grained FFN expert design [6, 15, 3], enabling unified expert architectures and parameter sharing across both attention and FFN layers. Based on this insight, we introduce UMoE, unified MoE architecture that abstracts Transformer layers into three fundamental components: experts, token mixing operations, and routers, as shown in Fig. 1. The experts, implemented as standard two-layer FFNs, serve as the primary components for token processing and knowledge storage. The token mixing operations facilitate contextual information exchange through weighted summation of tokens. Routers are employed to dynamically dispatch tokens to the most relevant experts to enable sparse computation. In UMoE, the distinction between the FFN and attention layers lies solely in the expert inputs: FFN layers process tokens independently, while attention layers process tokens simultaneously through weighted summation. This unified design not only simplifies the architecture but also enables parameter-efficient scaling through expert sharing between attention and FFN components. To evaluate the effectiveness of UMoE, we conduct extensive experiments across various model sizes and tasks, including pre-training and zero-shot evaluations. With the reformulated attention mechanism, the attention-based MoE layers of UMoE match or exceed the performance of previous FFN-based MoE layers. Moreover, by sharing parameters across attention and FFN modules, UMoE achieves superior performance in fully MoE architectures while maintaining the same parameter count. We also present detailed routing analysis of UMoE, revealing expert specialization patterns across modules, with higher-ranked experts demonstrating interpretable attention patterns. Our code is available at https://github.com/ysngki/UMoE."
        },
        {
            "title": "2 Related Work",
            "content": "Sparse Mixture-of-Experts (MoE). Sparse Mixture-of-Experts (MoE) models have gained increasing attention for their ability to scale model capacity while maintaining computational effi2 ciency [4, 9, 10, 5, 11]. The core component of these models is the sparsely activated MoE sub-layer, which selectively activates different parameter subsets for different inputs. In recent Transformerbased implementations, MoE architectures primarily replace feed-forward network (FFN) layers with MoE sub-layers. Each MoE layer consists of collection of experts, denoted as {Ei}N i=1, where each expert Ei is implemented as an FFN. Tokens are routed to subset of experts through routing mechanism, with the top-k router [4] being the most prevalent approach. Despite advances in routing mechanisms [16, 17, 18, 19, 15], the top-k router remains widely adopted due to its simplicity and robust performance [9]. For given token Rd, where is the hidden dimension, and trainable weight matrix Wr RN d, the top-k router computes the probability distribution over experts as: = softmax(Wrx). (1) The set of top-k experts is then selected based on p, where = k. Each expert processes the token independently and the final output of the MoE layer is computed as the weighted combination of these experts outputs: = (cid:88) iT piEi(x), (2) where each expert is implemented as an FFN with two matrices and non-linear activation function. MoE for Attention. Several recent approaches have explored extending the MoE paradigm to attention layers in Transformers [12, 13], with primary focus on expert design. Since attention layers lack the consecutive matrix multiplication pattern found in FFNs, these approaches necessitate expert designs that differ from FFN-MoE models. The Mixture-of-Attention (MoA) [12] propose to conceptualize individual attention heads as experts, scaling attention layers by increasing the number of attention heads. However, introducing sparsity into attention layers presents significant challenge: query vectors computed by specific expert (or head) require corresponding key and value vectors from the same expert, necessitating identical expert activation across all tokens. To address this constraint, MoA implements distinct query and output projections per head while maintaining shared key and value projections across attention heads. SwitchHead [13] presents an alternative approach to implementing the MoE paradigm in attention layers. Rather than treating entire attention heads as experts, SwitchHead designates individual projection matrices within heads as experts. straightforward implementation maintains four separate MoE sub-layers per head for query, key, value, and output projections. While scaling all projections yields performance improvements, empirical results show that value and output projections benefit most significantly from scaling. In contrast to these approaches, UMoE unifies attention-MoE and FFN-MoE through novel reformulation of the multi-head attention mechanism, enabling the shared expert design and parameters across both attention and FFN layers. Other Related Work. Several studies have explored connections between MoE and attention from different perspectives. MoH [20] proposes using MoE for pruning attention heads in LLMs by continuing pre-training with routing function. During inference, certain output projections (Wo), viewed as experts, are selectively skipped based on routing decisions. Taking different approach, MH-MoE [21] incorporates concepts from multi-head attention to enhance FFN-based MoE models. Instead of routing original input tokens to experts, MH-MoE decomposes each token into multiple low-dimensional sub-tokens, which are then processed in parallel by diverse sets of experts."
        },
        {
            "title": "3 Method",
            "content": "The attention mechanism is the core of Transformers [22], processing token embeddings to capture contextual relationships. However, its structure differs from FFN layers, which complicates the unification of MoE designs across both modules. In this section, we present two alternative formulations of attention, pre-mixing and post-mixing, that reveal an inherent FFN-like structure within attention layers. Based on these formulations, we introduce novel MoE architecture, UMoE. 3 def UMoELayer (x , ) : # : [1 , ] , : [n , ] # ## Attention MoE indices , probs = TopKRouter ( ) # Assign token to Experts residual_x = . copy () = @ W_k for , in zip ( indices , probs ) : = @ W_q [ ] # and ( the token embeddings ) are shared across experts . = Attention ( =q , =K , = ) residual_x += * Experts [ ]( ) = residual_x # ## FFN MoE indices , probs = TopKRouter ( ) # Assign token to Experts residual_x = . copy () for , in zip ( indices , probs ) : residual_x += * Experts [ ]( ) return residual_x Figure 3: Implementation details of UMoE layer. The input consists of sequence containing token embeddings and representing the final token embedding. For simplicity, this implementation focuses on computing the output for the last token. 3.1 Formulations of Attention Preliminaries. Consider sequence of token embeddings Rnd, where is the sequence length and is the embedding dimension. In multi-head attention, each token attends to all other tokens in the sequence through query, key, and value projections. For single token (e.g., the last token in the sequence for simplicity), its attention output is computed as: = xWq, = XWk, = XWv, (cid:18) qK dk where Wq, Wk Rddk and Wv Rddv are learnable matrices, respectively, and Rn is the attention weight. To enhance representation capacity, this process is repeated times in parallel, and the outputs are combined: = softmax , = aV, (cid:19) (4) (3) where Wo Rhdvd projects the concatenated outputs back to the original dimension d. = [o1; o2; ; oh]Wo, (5) Pre-Mixing Formulation. While multi-head attention is typically expressed using concatenation, it can be equivalently expressed as sum of per-head outputs, which helps reveal its connection to Rdvd along the feature dimension, we FFN layers. By decomposing Wo into small matrices Wi can express the output as: = (cid:88) i=1 oiWi = = (cid:88) (aiXWi v)Wi i=1 (cid:88) (aiX)(Wi vWi o). (6) (7) This reformulation provides two distinct interpretations, as shown in Fig. 2: i=1 Eq. 6: The conventional view where value vectors are first aggregated then projected back into the hidden space with an output projection. Eq. 7: new interpretation where token embeddings are first aggregated into contextualized representations, i.e., weighted averages of all tokens, before being processed by the value (Wi o) projections. We term this formulation as pre-mixing attention. v) and output (Wi 4 While both interpretations yield same outputs, the pre-mixing formulation enables the grouping of Wo and Wv. This grouping reveals that pre-mixing attention exhibits two-layer structure analogous to FFN modules, which can be implemented as linear FFN with no activation function. Post-Mixing Formulation. Alternatively, we can rearrange the computation as: = (cid:88) i=1 ai(XWi vWi o). (8) In this formulation, token embeddings are transformed by two successive projections independently for each token, before being aggregated using the attention weights. 3.2 UMoE By grouping Wv and Wo, both pre-mixing and post-mixing attention can be naturally interpreted as MoE architecture, aligning with established FFN-MoE practices. Using pre-mixing attention as an example, let the expert E(x) := xWvWo. The multi-head attention can then be reformulated as: = (cid:88) i=1 Ei(aiX). (9) By increasing the number of experts and introducing routing mechanism, such as top-k router, we derive MoE architecture, denoted as UMoE-Att. The output of UMoE-Att layer is: = (cid:88) iT piEi(aiX), where is the set of activated experts. (10) Referring to Eq. 2, we observe that the primary distinction between FFN-MoE layers and UMoE-Att layers lies in their expert inputs: FFN experts operate on individual token embeddings x, while attention experts process weighted combinations of all token embeddings. This reveals relationship: FFN-MoE layers can be interpreted as specialized case of pre-mixing attention layers where the attention matrix is constrained to an identity matrix, limiting each token to self-attention only. Fully MoE Architecture. Both the experts in UMoE-Att and the FFN layers of Transformer consist of two consecutive matrices. While attention layer experts utilize relatively small intermediate size (dv), FFN layers typically employ larger dimensions. Recent advances in FFN-MoE models suggest the efficacy of using FFN layers with reduced intermediate sizes as experts [6, 15, 3]. This insight enables the direct adoption of experts in attention layers for FFN layers, resulting in fully MoE architecture, denoted as UMoE. Fig. 3 presents the pseudo-code of UMoE layer, where the MoE paradigm is applied to both FFN and attention layers using shared expert set. Notably, to facilitate parameter sharing, experts are implemented as two-layer FFNs with an intermediate size of dv and incorporate non-linear activation function between matrix multiplications. Pre-mixing Implementation. The token mixing operation in pre-mixing attention is weighted summation over token embeddings, which can be implemented as vanilla attention, accepting Q, K, matrices as input and producing an output matrix. Each token generates distinct query vectors for different experts, while values (input token embeddings) and their associated keys are shared across experts. To generate expert-dependent queries for input tokens, each expert requires an additional query projection matrix. To mitigate the parameter count disparity with existing MoE models, where experts typically comprise two matrices, we employ low-rank matrices [23] for query projection within UMoE experts. For given token x, the query for expert is computed as: qi = xWq + xWi aWi b, (11) where the first term is shared across all experts, while the second term is expert-specific with unique parameters, Wi Rrdk , for each expert. Rdr and Wi 5 Putting It All Together. As illustrated in Fig. 1, UMoE integrates three key components: (1) experts implemented as fine-grained FFNs with dual low-rank query projection matrices, (2) pre-mixing attention mechanism utilizing shared keys and values across experts, and (3) the top-k router for expert selection. It is noteworthy that while MoA [12] also shares keys and values across experts, the values of MoA are the results after applying value linear transformation to the input token embeddings. In contrast, the values of UMoE directly refer to the input token embeddings. 3.3 Discussion Vanilla Attention vs Pre-mixing Attention. Vanilla attention and pre-mixing attention differ in terms of KV cache requirements and computational complexity. During inference, vanilla attention requires caching multiple keys and values per token, whereas pre-mixing attention requires only one key and token embedding per token. Regarding the computation, while vanilla attention performs weighted summation over low-dimensional value vectors, pre-mixing attention operates on input token embeddings, introducing modest increase in computational complexity. This modest increase, however, becomes increasingly negligible as models scale to larger dimensions, effectively amortizing the additional computational overhead. detailed comparative analysis is presented in Table 8 (A.1). Additionally, the abstract formulation of UMoE opens avenues for future research to explore more computationally efficient token mixing alternatives, such as linear attention mechanisms [24, 25]. Pre-mixing Attention vs Post-mixing Attention. As illustrated in Fig. 4, post-mixing attention processes individual tokens through experts prior to mixing. The architectural distinction between pre-mixing and post-mixing variants represents different perspectives on token-parameter interactions in attention layers. Recent interpretability studies have drawn parallels between the two-matrix multiplication pattern of FFNs and associative memory modules, where value neurons, i.e. columns of the second matrix in FFNs, are retrieved by inputs [26, 27, 28]. Within this framework, pre-mixing attention leverages token mixing to generate contextualized inputs for precise retrieval. In contrast, post-mixing attention can be conceptualized as an ensemble of independent retrievals executed by preceding tokens. Our preliminary experiments (A.2) demonstrates significant performance advantage of pre-mixing attention over its post-mixing counterpart. This observation suggests that generating contextualized inputs for token-parameter interactions more effectively aligns with the principles of attention mechanisms. Figure 4: Post-Mixing Attention."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Setup Datasets. We conduct language modeling pretraining on two datasets: FineWeb-Edu 100B [29] and Wikitext-103 [30]. FineWeb-Edu has shown superior data efficiency when evaluated on knowledgeintensive benchmarks. Wikitext-103, consisting of approximately 100M tokens, is smaller corpus that has been widely adopted in previous studies [16, 13]. We apply the LLaMA tokenizer [31] with 32K vocabulary size to both datasets. The zero-shot performance of models trained on FineWeb-Edu is evaluated using the lm-evaluation-harness framework [32]. Baselines. We compare UMoE against three categories of baselines: dense models, FFN-MoE models with fine-grained experts [8, 3], and attention-MoE models (specifically MoA [12] and SwitchHead [13]). Attention-MoE models are configured with identical expert parameters1. We implement all MoE models with fixed expert per layer, following recommendations for optimal model performance [8, 33]. UMoE adopts the pre-mixing attention mechanism, which consistently outperforms post-mixing variants. Experimental Setup. UMoE is implemented as decoder-only Transformer with rotary position embedding [34], following deepseek-MoE [6]. We evaluate two model configurations: The base 1SwitchHead represents an exception, as it requires the number of experts to be divisible by the number of attention heads. 6 Table 1: Comparison of Dense and Sparse Mixture-of-Experts (MoE) Models for Language Modeling. In MoE models, denotes experts per layer with size in #Total columns, while in #Active columns indicates the number of experts activated per token. Gray entries in #Total columns indicate shared parameters between attention and FFN modules in UMoE models. UMoE-Att refers to UMoE variants with MoE only applied to attention modules. Model Params Attention FFN PPL () #Total #Active #Total #Active Fineweb Wikitext Base Models Dense Fine-grained FFN-MoE MoA SwitchHead UMoE-Att UMoE Large Models Dense Fine-grained FFN-MoE MoA SwitchHead UMoE-Att UMoE 768 768 134 535 525 192116 533 192116 547 192116 540 192 1.1 3.8 3.6 3.7 3.8 3.6 2048 2048 51257 51260 51257 51264 768 768 1924 1924 1924 1924 2048 2048 5124 5124 5124 5124 3072 192128 3072 3072 3072 192128 5632 51264 5632 5632 5632 512 3072 19216 3072 3072 3072 19216 5632 51211 5632 5632 5632 51211 25.79 21.19 22.28 22.91 20.81 20.44 17.53 16.09 16.72 16.48 16.03 15.95 30.41 27.94 27.57 29.47 27.45 26.67 25.46 25.47 25.14 27.24 25.53 25. MACs 525 530 486 542 611 616 4.59 4.61 3.99 4.62 4.73 4.75 models comprise 12 layers with hidden size of 768, while the large models consist of 24 layers with hidden size of 2048. These configurations yield dense models with 134M and 1.1B parameters, respectively. MoE variants replace all attention or FFN layers with MoE layers. Due to computational constraints, unless otherwise specified, models are pretrained on 50B tokens from FineWeb-Edu with batch size of 1024. For Wikitext-103, following Csordás et al. [13], models are trained for 100k steps, though models typically overfit within 20k steps. Detailed hyperparameters are provided in A.3. 4.2 Comparison with Baselines Results. From Table 1, we observe that UMoE shows consistent superiority across different model sizes and datasets. In the base model regime, UMoE achieves the best performance. Notably, the attention-only variant of UMoE exhibits substantial improvements over previous attention-based approaches. Even without parameter sharing, UMoE-Att establishes itself as compelling alternative to traditional FFN-based MoE models. The parameter sharing mechanism between attention and FFN modules further enhances the effectiveness without increasing the total parameter count. Despite equalizing the number of activated experts across all baselines, we observe subtle computational discrepancies due to different attention layer implementations, as measured by MACs (multiplication accumulation operation). We additionally perform MAC-matched comparison by increasing the number of activated experts of baseline models. Table 2 shows that even under comparable computational constraints, UMoE method achieves the lowest perplexity. In larger-scale models, UMoE maintains its competitive advantage. While MoA shows marginally better performance on Wikitext-103, this result may not fully reflect model capabilities given the relatively small size of Wikitext-103 (100M tokens) compared to the model scale. Following established practice [16], we further report validation perplexity. Fig. 5 shows that UMoE demonstrates faster convergence and lower validation perplexity compared to baselines, indicating enhanced modeling capabilities. This superior performance translates to downstream tasks, with Table 4 showing UMoE consistently achieving the highest average zero-shot accuracy across diverse tasks. Efficiency. Following Zhang et al. [12], Jin et al. [20], we employ MACs2 as an efficiency metric, as it remains independent of hardware implementations. As shown in Table 1, the pre-mixing attention introduces modest computational overhead, resulting in approximately 1.17 slowdown for base models. However, this slowdown becomes increasingly negligible as models scale up; in large models, 2MACs is measured using the DeepSpeed Flops Profiler. 7 Table 2: MAC-matched comparison for base models by increasing the number of activated experts of baseline models. Table 3: Parameter sharing strategies. indicates shared components between modules indicates separate components. while Model MACs Active Params PPL () Fineweb Wikitext FFN-MoE MoA SwitchHead UMoE-Att UMoE 617 621 649 611 616 768 + 19222 1928 + 3072 19212 + 3072 1924 + 3072 1924 + 19216 20.80 22.00 21.57 20.81 20.44 27.39 27.63 28.13 27.45 26.67 Component UMoE Fixed Experts Router # Params 540 536 540 537 PPL 22.82 23. 23.05 23.02 Table 4: Zero-shot accuracy on downstream tasks. The best score is marked in bold. Model Params HellaSwag PIQA ARC-E ARC-C RACE Lambada MMLU Wino Avg. Base Models Dense MoA SwitchHead FFN-MoE UMoE (Att) UMoE Large Models Dense MoA SwitchHead FFN-MoE UMoE (Att) UMoE 134 525 533 535 547 540 1.1 3.6 3.7 3.8 3.8 3.6 33.58 37.82 37.19 39.69 40.72 41.28 48.45 50.61 51.90 52.74 53.20 53.17 62.35 65.58 66.12 66.43 67.36 66.65 69.26 70.28 70.83 71.52 71.44 72.47 46.09 51.34 50.55 52.95 51.77 51. 58.85 61.47 62.34 64.23 63.30 64.23 24.74 26.19 26.59 26.71 27.82 29.01 32.17 33.22 33.69 35.67 34.39 35.75 27.75 28.83 28.14 29.76 29.76 28.71 33.11 32.38 33.27 33.30 32.82 32.44 19.97 22.33 21.73 23.46 23.66 23. 31.75 33.15 33.66 34.00 34.78 35.32 24.8 25.1 25.2 25.3 25.9 26.6 27.4 28.6 28.8 29.2 29.3 30.4 49.8 50.7 50.9 52.1 52.5 52.6 53.8 54.7 55.7 56.3 57.4 56.9 36.14 38.49 38.30 39.55 39.94 40. 44.35 45.55 46.27 47.12 47.08 47.58 UMoE introduces only 1.03 slowdown compared to the dense baseline. This favorable scaling behavior arises from the different growth rates in computational complexity: expert processing scales quadratically with hidden dimension, while token aggregation in attention layers scales linearly. 4.3 Ablations We conducted ablation experiments using base models trained on FineWeb-Edu with 20B tokens. Parameter Sharing Analysis. We investigated various sharing strategies across FFN and attention layers for fixed experts [6, 33] and routers. As shown in Table 3, all configurations achieved comparable perplexity. Our default configuration, which employs separate fixed experts and routers across FFN and attention layers, yielded the optimal perplexity. Expert Allocation. As suggested in Section 3.2, FFN-MoE layers can be interpreted as specialized case of pre-mixing attention layers with an identity matrix as attention matrix. This interpretation raises question: does UMoE perform better when allocating more experts to attention layers rather than FFN layers? According to Table 5, we observe an trend when gradually shifting expert allocation from FFN to attention modules while maintaining total activated expert size of 20. The model achieves its best perplexity when all experts are allocated to attention layers. This finding provides empirical evidence supporting our theoretical interpretation that FFN layers function as specialized form of attention, with the attention mechanism exhibiting greater expressiveness. However, increasing the number of attention experts introduces substantial computational overhead due to token mixing Figure 5: Best valid PPL (top) and training loss (bottom) on Wikitext. 8 Table 5: Impact of expert allocation between Attention and FFN layers (total experts = 20). Model # Expert Attention FFN UMoE 4 8 12 16 20 16 12 8 4 0 PPL 22.82 22.63 22.44 22.50 21. Table 6: Effect of Activation Functions in Expert Modules. indicates experts with activation functions while indicates experts without activation functions. Model UMoE UMoE (Att) Act. Function PPL 22.82 24.43 23.37 23.99 Table 7: Top tokens for selected experts in the last attention and FFN layer of UMoE. Expert ID Top Tokens in Attention Layer Top Tokens in FFN Layer 3 10 46 _Each , This , Every , Each , _This _Film , _video , _lab , _film , _Video The , _The , _the , the , _Our :) , . , %. , .\" , .\" This , _This , Every , _Each , _Another Tag , _Font , _ISBN , _a , _his , _my , Your , _Your _relatively , _extremely , _a , _very , _Very twitter , _DNS operations. Future research could explore efficient attention alternatives within the attention MoE framework. Activation Function. Table 6 presents our investigation into the impact of activation functions in UMoE. The results demonstrate that incorporating activation functions between matrix multiplications within experts consistently improves model performance, reinforcing the crucial role of non-linearity in deep learning architectures. Notably, while the removal of activation functions reduces the experts to pure linear transformations in both FFN and attention modules, UMoE remains trainable. We attribute this robustness to the preserved non-linearity from token mixing operations and layer normalization. Nevertheless, the consistent performance degradation underscores the importance of activation functions in model expressiveness, particularly in the context of shared expert architectures. 4.4 Expert Specialization Table 7 presents the routing patterns in the final layer of UMoE, where experts are shared between attention and FFN modules while maintaining distinct routers. Notably, certain token categories consistently route to the same experts across both modules, as evidenced by experts 3 and 46. Expert 3 consistently processes determiners, while expert 46 specializes in demonstrative pronouns. The analysis also reveals divergent specialization patterns that highlight the complexity of shared expert architectures. notable example is expert 64, which exhibits distinct specializations: processing consecutive punctuation marks in the attention layer while handling degree adverbs in the FFN layer. This phenomenon suggests that shared experts can develop multiple specializations, potentially leading to more efficient parameter utilization. However, it also raises important questions about potential knowledge conflicts within individual experts, indicating promising directions for future research in routing mechanism design for shared expert architectures. We also provide an analysis on the attention maps of UMoE in A.4, which confirms that higher-ranked experts show more focused attention distributions on relevant tokens compared to lower-ranked ones."
        },
        {
            "title": "5 Conclusion",
            "content": "The paper proposes UMoE, novel architecture that unifies MoE designs for attention and FFN layers. The key insight is reformulation of the attention mechanism that allows the value and output projections to be grouped into FFN-like experts. This unification enables parameter sharing across attention and FFN layers, resulting in fully MoE architecture that improves performance without introducing additional parameters. The paper presents extensive experiments demonstrating UMoE 9 superiority over existing MoE architectures in terms of perplexity on language modeling datasets and accuracy on zero-shot tasks. As for future work, we are looking at replacing the token mixing mechanism of UMoE with more efficient alternatives to enable scaling up the number of activated experts in attention layers. In addition, we are also interested in investigating architectures that unify attention and FFN into single layer, given our finding that FFN layers function as specialized case of attention with reduced expressiveness."
        },
        {
            "title": "References",
            "content": "[1] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361. [2] Elias Frantar, Carlos Riquelme Ruiz, Neil Houlsby, Dan Alistarh, and Utku Evci. Scaling laws for sparsely-connected foundation models. In The Twelfth International Conference on Learning Representations, 2024. [3] Jan Ludziejewski, Jakub Krajewski, Kamil Adamczewski, Maciej Pióro, Michał Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Król, Tomasz Odrzygózdz, Piotr Sankowski, Marek Cygan, and Sebastian Jaszczur. Scaling laws for fine-grained mixture of experts. In Proceedings of the 41st International Conference on Machine Learning, volume 235, pages 3327033288. PMLR, 2127 Jul 2024. [4] Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-ofexperts layer. In International Conference on Learning Representations, 2017. [5] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: scaling to trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res., 23(1), January 2022. ISSN 1532-4435. [6] Damai Dai, Chengqi Deng, Chenggang Zhao, R.x. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y.k. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. DeepSeekMoE: Towards ultimate expert specializaIn Proceedings of the 62nd Annual Meeting tion in mixture-of-experts language models. of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1280 1297, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.70. URL https://aclanthology.org/2024.acl-long.70/. [7] Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. Openmoe: an early effort on open mixture-of-experts language models. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. [8] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, 10 Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2024. [9] Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, and Hannaneh Hajishirzi. Olmoe: Open mixture-of-experts language models. CoRR, abs/2409.02060, 2024. [10] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts. CoRR, abs/2401.04088, 2024. URL https: //doi.org/10.48550/arXiv.2401.04088. [11] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022. [12] Xiaofeng Zhang, Yikang Shen, Zeyu Huang, Jie Zhou, Wenge Rong, and Zhang Xiong. Mixture of attention heads: Selecting attention heads per token. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 41504162. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLP-MAIN.278. URL https://doi.org/10.18653/v1/ 2022.emnlp-main.278. [13] Róbert Csordás, Piotr Piekos, Kazuki Irie, and Jürgen Schmidhuber. Switchhead: Accelerating transformers with mixture-of-experts attention. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id= 80SSl69GAz. [14] Xiaoye Qu, Daize Dong, Xuyang Hu, Tong Zhu, Weigao Sun, and Yu Cheng. Llama-moe v2: Exploring sparsity of llama from perspective of mixture-of-experts with post-training. CoRR, abs/2411.15708, 2024. [15] Yuanhang Yang, Shiyi Qi, Wenchao Gu, Chaozheng Wang, Cuiyun Gao, and Zenglin Xu. XMoE: Sparse models with fine-grained and adaptive expert selection. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1166411674. Association for Computational Linguistics, 2024. doi: 10.18653/v1/2024.findings-acl.694. [16] Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large sparse models. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/ forum?id=lMgDDWb1ULW. [17] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. BASE layers: Simplifying training of large, sparse models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 62656274. PMLR, 2021. URL http: //proceedings.mlr.press/v139/lewis21a.html. [18] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M. Dai, Zhifeng Chen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing. In Advances in Neural Information Processing Systems, 2022. URL https://openreview. net/forum?id=jdJo1HIVinI. 11 [19] Quzhe Huang, Zhenwei An, Nan Zhuang, Mingxu Tao, Chen Zhang, Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Songfang Huang, and Yansong Feng. Harder task needs more experts: Dynamic routing in MoE models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1288312895, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology. org/2024.acl-long.696/. [20] Peng Jin, Bo Zhu, Li Yuan, and Shuicheng Yan. Moh: Multi-head attention as mixture-ofhead attention. CoRR, abs/2410.11842, 2024. doi: 10.48550/ARXIV.2410.11842. URL https://doi.org/10.48550/arXiv.2410.11842. [21] Xun Wu, Shaohan Huang, Wenhui Wang, Shuming Ma, Li Dong, and Furu Wei. Multi-head mixture-of-experts. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=dyZ8GJZjtX. [22] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, In Advances in Neural Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Information Processing Systems. Curran Associates, Inc., 2017. [23] Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=nZeVKeeFYf9. [24] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=y8Rm4VNRPH. [25] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. CoRR, abs/2312.00752, 2023. URL https://doi.org/10.48550/arXiv.2312.00752. [26] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers In Proceedings of the 2021 Conference on Empirical Methods are key-value memories. in Natural Language Processing. Association for Computational Linguistics, 2021. URL https://aclanthology.org/2021.emnlp-main.446/. [27] Zeyu Liu, Tim Dettmers, Xi Lin, Veselin Stoyanov, and Xian Li. Towards unified view of sparse feed-forward network in pretraining large language model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1503815061, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.930. URL https://aclanthology.org/2023.emnlp-main.930/. [28] Zeping Yu and Sophia Ananiadou. Neuron-level knowledge attribution in large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 32673280. Association for Computational Linguistics, 2024. doi: 10.18653/v1/2024. emnlp-main.191. URL https://aclanthology.org/2024.emnlp-main.191/. [29] Guilherme Penedo, Hynek Kydlícek, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale. CoRR, abs/2406.17557, 2024. URL https://doi.org/10. 48550/arXiv.2406.17557. [30] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [31] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang 12 Table 8: Computational complexity analysis of vanilla attention and pre-mixing attention mechanisms. Entries in gray denote identical complexity terms between the two mechanisms. Here, denotes sequence length, represents hidden dimension, indicates the number of attention heads (activated experts), dk and dv are key and value dimensions respectively, and is the rank of query projection matrices in the pre-mixing attention. Operation"
        },
        {
            "title": "Vanilla",
            "content": "Pre-mixing Output Projection O(N dv h) O(N dv h) Value Projection O(N dk h) Key Projection O(N dk h) O(N dk + (dk + d) h) Query Projection O(N 2 dk h) O(N 2 dv h) O(N dv h) O(N dv h) O(N dk d)"
        },
        {
            "title": "QK Multiplication\nWeighted Sum",
            "content": "O(N 2 dk h) O(N 2 h) Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. URL https://doi.org/10.48550/arXiv.2307.09288. [32] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/ 12608602. [33] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-ofexperts inference and training to power next-generation AI scale. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 1833218346. PMLR, 2022. [34] Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568: 127063, 2024. URL https://doi.org/10.1016/j.neucom.2023.127063."
        },
        {
            "title": "A Technical Appendices and Supplementary Material",
            "content": "A.1 Complexity Table 8 presents detailed computational complexity comparison between vanilla attention and premixing attention mechanisms. The key distinctions lies in two operations: key projection and weighted sum computation. In key projection, pre-mixing attention achieves lower complexity. However, this efficiency is partially offset in the weighted sum operation, where the complexity increases due to the full dimensional mixing. It is worth mentioning that the computational complexity of weighted sum grows linearly with the hidden dimension, while the FFN computation grows quadratically. The computational overhead of weighted sum becomes less significant as model size increases. A.2 Post-mixing vs Pre-mixing 6 presents the pseudo-code of UMoE layer based on post-mixing attention. We conducted preliminary experiments on Wikitext-103 and FineWeb-Edu datasets. As illustrated in 7, MoE models incorporating pre-mixing attention demonstrate substantially superior performance compared to their post-mixing counterparts. As discussed in 3.1, these two reformulations of the attention mechanism are mathematically equivalent, owing to the absence of non-linear transformations within the matrix chain multiplication def PostMixingMoE ( ) : # : [n , ] # ## Attention MoE # Independent Expert Processing all_token_out puts = [] all_token_keys = [] for this_token in : indices , probs = TopKRouter ( this_token ) for , in zip ( indices , probs ) : = * Experts [ ]( this_token ) all_t oken_outputs . append ( ) = W_k [ ]( this_token ) all_token_keys . append ( ) # Mixing = ( @ W_q ) . unsuqeeze (1) # [n , 1 , d_q ] = all_token_keys . reshape (n , , d_q ) = all_token_o utputs . reshape (n , , d_v ) attn_output = Attention (Q , , ) # [n , 1 , d_v ] # ## FFN MoE ... Figure 6: Implementation of UMoE layer based on post-mixing attention. is sequence of token embeddings. In the attention layer, tokens are processed by their top-k experts independently. The output embeddings of all tokens are aggregated according to the attention weights. in attention heads. However, when grouping the value and output projections and implementing them as an FFN with non-linear activation function, these formulations yield distinct outputs in the MoE layers. 8 illustrates MoE models implemented based on these two reformulations. Both approaches can be interpreted as extensions of conventional FFN-based MoE models. Specifically, the pre-mixing approach enhances FFN-based MoE models by contextualizing the inputs, while post-mixing attention enables MoE layers to incorporate other tokens outputs in generating the final output. Future research could explore the synergistic combination of postand pre-mixing approaches to fully leverage contextual information. Figure 7: Loss curves of UMoE with attention MoE layers implemented on post-mixing and premixing attention, respectively. Models are trained on Wikitext-103 (left) and FineWeb-Edu (right). 14 Figure 8: Two implementations of UMoE based on pre-mixing and post-mixing attention, respectively. Table 9: Hyperparameters of Base Models. MoA and SwitchHead use the same hyperparameters as UMoE-Att. Dense FFN-MoE UMoE-Att UMoE Hyperparameter Context Length Number of Layers Hidden Size Attention Heads FFN Size Query (Key) Dimension Value Dimension Query Lora Rank 1024 12 768 4 3072 512 192 Number of MoE layers Expert Size Experts per MoE Layer FFN Experts per Token Attention Experts per Token - 1024 12 768 4 192 16 512 192 12 192 128 16 1024 12 768 3072 512 16 12 192 116 4 768 12 768 4 192 16 512 16 12 192 128 16 4 A.3 Hyperparameters 9 and 10 give the parameters used for based and large models, respectively. MoA and SwitchHead utilize identical parameters as UMoE-Att, excluding the low-rank query projections. 11 details the hyperparameters used during training. For the Wikitext-103 dataset, we adopt the same hyperparameter configuration as Csordás et al. [13]. A.4 Attention Analysis We analyze the attention patterns in UMoE by visualizing expert-specific attention maps. While UMoE (Large) contains 64 experts per layer across 24 layers, we focus on the top 8 experts (ranked by 15 Table 10: Hyperparameters of Large Models. MoA and SwitchHead use the same hyperparameters as UMoE-Att."
        },
        {
            "title": "Dense",
            "content": "FFN-MoE UMoE-Att UMoE"
        },
        {
            "title": "Context Length\nNumber of Layers\nHidden Size\nAttention Heads",
            "content": "FFN Size Query (Key) Dimension Value Dimension Query Lora Rank 1024 24 2048 4 5632 512"
        },
        {
            "title": "Number of MoE layers\nExpert Size\nExperts per MoE Layer\nFFN Experts per Token\nAttention Experts per Token",
            "content": "- 1024 24 2048 4 512 11 512 512 24 512 64 11 1024 24 2048 4 5632 512 24 512 57 4 1024 24 2048 4 512 11 512 36 24 512 64 11 4 Table 11: Training Hyperparameters on FineWeb-Edu and Wikitext-103. Hyperparameter FineWeb-Edu Wikitext Global Batch Size Learning Rate Training Steps LR Scheduler Warmup Ratio GPU 1024 4e-4 50000 cosine 0.05 H100 64 2.5e-4 100000 cosine 0.05 H100 router scores) to maintain tractability. Each expert utilizes its own query projection matrix, allowing us to compute attention maps regardless of activation status. To investigate attention behavior, we examine two inputs: Context: William Shakespeare wrote the famous play Romeo and Juliet in the late 16th century. Question: Who wrote Romeo and Juliet? The Answer is\" Context: Tokyo is the capital city of Japan and has population of over 37 million people in its metropolitan area. Question: What is the capital of Japan? The Answer is\" UMoE successfully predicted the correct answers for both inputs, even after removing the context. For the final token in each input, we collected attention maps from the top 8 experts across all layers. 9 presents attention maps for the final token prediction, revealing distinct patterns between higher and lower-ranked experts. Higher-ranked experts demonstrate more focused attention distributions that align with task requirements. For instance, in the Shakespeare question, Expert_0 and Expert_32 show pronounced attention weights on task-critical tokens \"who wrote\". Similarly, for the Tokyo question, the top expert exhibits sophisticated attention patterns by focusing on key contextual elements like \"Japan\" and \"capital\". These observations suggest that the routing mechanism effectively identifies experts capable of extracting task-relevant information through specialized attention patterns. To create comprehensive visualization, we aggregated attention maps of all layers by summing them, as shown in 10. The values on the left (e.g., E0: 3.83) represent the sum of router scores received by experts at each rank position across all layers. The accumulated patterns reveal that higher-ranked experts (particularly E0 and E1) maintain more targeted attention distributions focused on question-relevant tokens, while lower-ranked experts tend to focus heavily on the initial token, 16 (a) (b) Figure 9: Representative attention maps. The heatmaps show the attention weights of the last token produced by top 8 experts, ranked by their router scores. (a) Attention patterns for the Shakespeare question, where higher-ranked experts (e.g., Expert_0, Expert_32) demonstrate focused attention on question-relevant tokens. (b) Attention patterns for the Tokyo question, showing similar task-specific attention concentration among top experts. Notably, we observe minimal attention paid to answer tokens present in the context. This phenomenon aligns with the conceptualization of experts (two consecutive matrices) as key-value memory modules, where input serves as query. In other words, the output of attention layers is the composition of values, i.e., columns of the second matrix, in the activated experts, rather than token embeddings. This suggests that the token mixing should focus on building appropriate query for accurate compositions. Therefore, the last token should pay attention to the tokens relevant to the answer token, rather than the answer itself. 17 (a) (b) Figure 10: Layer-wise accumulated attention weights across the model. The values on the left (e.g., E0: 3.83) represent the sum of router scores for experts. Higher-ranked experts (E0, E1) consistently show more focused attention distributions compared to lower-ranked experts."
        }
    ],
    "affiliations": [
        "Hong Kong Polytechnic University, Hong Kong, China",
        "Institute of Science Tokyo, Tokyo, Japan",
        "The Chinese University of Hong Kong, Hong Kong, China"
    ]
}