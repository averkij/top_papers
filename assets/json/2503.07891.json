{
    "paper_title": "Gemini Embedding: Generalizable Embeddings from Gemini",
    "authors": [
        "Jinhyuk Lee",
        "Feiyang Chen",
        "Sahil Dua",
        "Daniel Cer",
        "Madhuri Shanbhogue",
        "Iftekhar Naim",
        "Gustavo Hern√°ndez √Åbrego",
        "Zhe Li",
        "Kaifeng Chen",
        "Henrique Schechter Vera",
        "Xiaoqi Ren",
        "Shanfeng Zhang",
        "Daniel Salz",
        "Michael Boratko",
        "Jay Han",
        "Blair Chen",
        "Shuo Huang",
        "Vikram Rao",
        "Paul Suganthan",
        "Feng Han",
        "Andreas Doumanoglou",
        "Nithi Gupta",
        "Fedor Moiseev",
        "Cathy Yip",
        "Aashi Jain",
        "Simon Baumgartner",
        "Shahrokh Shahi",
        "Frank Palma Gomez",
        "Sandeep Mariserla",
        "Min Choi",
        "Parashar Shah",
        "Sonam Goenka",
        "Ke Chen",
        "Ye Xia",
        "Koert Chen",
        "Sai Meher Karthik Duddu",
        "Yichang Chen",
        "Trevor Walker",
        "Wenlei Zhou",
        "Rakesh Ghiya",
        "Zach Gleicher",
        "Karan Gill",
        "Zhe Dong",
        "Mojtaba Seyedhosseini",
        "Yunhsuan Sung",
        "Raphael Hoffmann",
        "Tom Duerig"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this report, we introduce Gemini Embedding, a state-of-the-art embedding model leveraging the power of Gemini, Google's most capable large language model. Capitalizing on Gemini's inherent multilingual and code understanding capabilities, Gemini Embedding produces highly generalizable embeddings for text spanning numerous languages and textual modalities. The representations generated by Gemini Embedding can be precomputed and applied to a variety of downstream tasks including classification, similarity, clustering, ranking, and retrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark (MMTEB), which includes over one hundred tasks across 250+ languages, Gemini Embedding substantially outperforms prior state-of-the-art models, demonstrating considerable improvements in embedding quality. Achieving state-of-the-art performance across MMTEB's multilingual, English, and code benchmarks, our unified model demonstrates strong capabilities across a broad selection of tasks and surpasses specialized domain-specific models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 1 9 8 7 0 . 3 0 5 2 : r Gemini Embedding: Generalizable Embeddings from Gemini Jinhyuk Lee*, Feiyang Chen*, Sahil Dua*, Daniel Cer*, Madhuri Shanbhogue*, Iftekhar Naim , Gustavo Hern√°ndez √Åbrego , Zhe Li , Kaifeng Chen , Henrique Schechter Vera , Xiaoqi Ren , Shanfeng Zhang , Daniel Salz , Michael Boratko , Jay Han , Blair Chen , Shuo Huang , Vikram Rao , Paul Suganthan , Feng Han , Andreas Doumanoglou , Nithi Gupta , Fedor Moiseev , Cathy Yip , Aashi Jain , Simon Baumgartner , Shahrokh Shahi , Frank Palma Gomez , Sandeep Mariserla , Min Choi , Parashar Shah , Sonam Goenka , Ke Chen , Ye Xia , Koert Chen , Sai Meher Karthik Duddu , Yichang Chen , Trevor Walker , Wenlei Zhou , Rakesh Ghiya , Zach Gleicher , Karan Gill , Zhe Dong , Mojtaba Seyedhosseini , Yunhsuan Sung , Raphael Hoffmann and Tom Duerig Gemini Embedding Team, Google1 In this report, we introduce Gemini Embedding, state-of-the-art embedding model leveraging the power of Gemini, Googles most capable large language model. Capitalizing on Geminis inherent multilingual and code understanding capabilities, Gemini Embedding produces highly generalizable embeddings for text spanning numerous languages and textual modalities. The representations generated by Gemini Embedding can be precomputed and applied to variety of downstream tasks including classification, similarity, clustering, ranking, and retrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark (MMTEB), which includes over one hundred tasks across 250+ languages, Gemini Embedding substantially outperforms prior state-of-the-art models, demonstrating considerable improvements in embedding quality. Achieving state-of-the-art performance across MMTEBs multilingual, English, and code benchmarks, our unified model demonstrates strong capabilities across broad selection of tasks and surpasses specialized domain-specific models. 1. Introduction Embedding models, which transform inputs into dense vector representations, are pivotal for capturing semantic information across various domains and modalities. Text embedding models represent words and sentences as vectors, strategically positioning semantically similar texts in close proximity within the embedding space (Gao et al., 2021; Le and Mikolov, 2014; Reimers and Gurevych, 2019). Recent research has focused on developing general-purpose embedding models capable of excelling in diverse downstream tasks, including information retrieval, clustering, and classification (Cer et al., 2018; Muennighoff et al., 2023). Leveraging their vast pre-training knowledge, large language models (LLMs) have emerged as promising avenue for constructing such general-purpose embedding models, with the potential to significantly enhance performance across broad spectrum of applications (Anil et al., 2023a,b; Brown et al., 2020). The integration of LLMs has revolutionized the development of high-quality embedding models through two primary approaches. Firstly, LLMs have been employed to refine training datasets by generating higher quality examples. Techniques such as hard negative mining (Lee et al., 2024) and synthetic data generation (Dai et al., 2022; Wang et al., 2023) enable the distillation of LLM knowledge into smaller, more efficient embedding models, leading to substantial performance gains. Secondly, recognizing that the embedding model parameters are frequently initialized from language models (Devlin et al., 2019; Karpukhin et al., 2020), researchers have explored leveraging LLM parameters directly for initialization (Ni et al., 2021). While this approach introduces increased 1See Contributions and Acknowledgments section. Equal contributions. 2025 Google DeepMind. All rights reserved Gemini Embedding: Generalizable Embeddings from Gemini Gemini Embedding Embedding Gecko gte-Qwen2multilingual-e5Cohere-embed7B-instruct large-instruct multilingual-v3.0 3-large text-embeddingMTEB(Multilingual) (Enevoldsen et al., 2025) Mean (Task) Mean (Type) 68.32 59.64 79.32 71.84 54.99 5. - Bitext Mining - Classification - Clustering - Inst. Retrieval - Multilabel Class. 29.16 83.64 - Pair Class. 65.72 - Reranking 67.71 - Retrieval 79.40 - STS 73.30 67.67 74.66 MTEB(Eng, v2) (Enevoldsen et al., 2025) Mean (Task) Mean (Type) MTEB(Code) (Enevoldsen et al., 2025) XOR-Retrieve (Asai et al., 2021) XTREME-UP (Ruder et al., 2023) Commercial Use 62.13 54.32 70.73 64.64 48.47 4.08 22.80 81.14 61.22 59.68 76.11 69.53 64. 65.40 62.51 56.00 73.92 61.55 53.36 4.94 25.48 85.13 65.55 60.08 73.98 70.72 65.77 63.23 55.17 80.13 64.94 51.54 -0.40 22.91 80.86 62.61 57.12 76. 65.53 61.21 56.41 57.94 90.42 65.67 N/A N/A 64.33 34.97 17.39 18.68 (cid:33) (cid:33) 61.10 53.31 70.50 62.95 47.61 -1.89 22.74 79.88 64.07 59.16 74.80 66.01 61.43 51.94 N/A N/A (cid:33) 58.92 51.48 62.17 60.27 47.49 -2.68 22.03 79.17 63.89 59.27 71.68 66.43 62.15 58. 68.76 18.80 (cid:33) Table 1 Comparison of embedding models on Massive Multilingual Embedding Benchmark: MTEB(Multilingual), MTEB(Eng, v2), and MTEB(Code). We also show results on XOR-Retrieve and XTREME-UP. For MTEBs, we report task and type mean performances. We report MRR@10 for XTREME-UP and Recall@5kt for XOR-Retrieve. : Averaged over seven code tasks available for all models. : For Gecko Embedding (Lee et al., 2024), we evaluate text-embedding-004 on MTEB(Eng, v2), text-embedding-005 on MTEB(Code), and text-multilingual-embedding-002 on others. computational demands compared to traditional embedding models, empirical evidence suggests that utilizing strong LLMs for initialization can yield significantly superior performance (Lee et al., 2025; Neelakantan et al., 2022; Wang et al., 2023). In this work, we introduce Gemini Embedding,2 novel embedding model initialized from the powerful Gemini large language model (Anil et al., 2023a; Team, 2024). Leveraging Geminis diverse capabilities, we train Gemini Embedding on comprehensive suite of embedding tasks. To construct high-quality, heterogeneous training dataset, we employ Gemini for several critical data curation steps: filtering low-quality examples, determining relevant positive and negative passages for retrieval, and generating rich synthetic datasets. This curated dataset facilitates training with contrastive learning objective, enabling Gemini Embedding to learn robust semantic representations. Building upon the success of Gecko (Lee et al., 2024), we incorporate task prompts and pre-finetuning stage to enhance performance. Finally, we utilize Model Soup (Wortsman et al., 2022), simple yet effective parameter averaging technique, to combine multiple fine-tuned checkpoints, yielding superior final embedding model. To rigorously assess the capabilities of Gemini Embedding, we conduct extensive evaluations across diverse spectrum of tasks and languages. We primarily utilize the Massive Multilingual Text Embedding Benchmark (MMTEB) (Enevoldsen et al., 2025), comprehensive test suite encompassing over 100 embedding evaluation tasks across more than 250 languages, to provide thorough evaluation. Notably, Gemini Embedding achieves state-of-the-art performance on MTEB(Multilingual), significantly surpassing the previous best models. Gemini Embedding achieves first-place ranking 2Our model is available at https://ai.google.dev/gemini-api/docs/embeddings. 2 Gemini Embedding: Generalizable Embeddings from Gemini on the public leaderboard based on Borda rank,3 as well as on mean score averaged over tasks where it attains score of 68.32, substantial +5.09 improvement over the second-best model, multilingual-e5-large-instruct. Furthermore, it achieves the highest task-type mean of 59.64, +3.64 improvement over gte-Qwen2-7B-instruct. As summarized in Table 1, Gemini Embedding establishes new state-of-the-art on multiple other benchmarks such as XOR-Retrieve (Asai et al., 2021) for crosslingual retrieval. Remarkably, our findings demonstrate that Gemini Embedding exhibits exceptional performance not only in high-resource languages like English but also in numerous low-resource languages, such as Macedonian. We provide detailed ablation study to elucidate the key factors contributing to Gemini Embeddings superior performance, offering insights into its effectiveness. 2. Related Work Text Embedding Models Text embeddings are fundamental for wide array of downstream natural language processing tasks, including semantic similarity, information retrieval, clustering, and classification. Prior models, such as Universal Sentence Encoder (Cer et al., 2018) and Sentence T5 (Ni et al., 2022), have aimed to provide general-purpose embeddings capable of handling diverse applications. However, empirical studies have revealed limitations in their ability to generalize effectively across varied tasks and domains, highlighting the need for more robust and adaptable embedding models. This has motivated the creation of comprehensive benchmarks like MTEB (Enevoldsen et al., 2025; Muennighoff et al., 2023), which emphasize novel task and domain generalization. LLMs for Embedding Data Generation Synthetic query generation (Bonifacio et al., 2022; Dai et al., 2022; Jeronymo et al., 2023; Nogueira et al., 2019) for given documents or passages has proven highly effective for creating diverse training data for embedding models. Lee et al. (2024) showed that the seed passage from which synthetic query was generated may not be the best positive passage for that query and proposed an LLM-based approach to find better positive and negative passages. Wang et al. (2023) scaled up synthetic data generation over nearly one hundred languages and hundreds of thousands of tasks by prompting LLMs to first generate diverse pool of candidate tasks and then generate data as (query, positive, hard negative) triplets conditioned on specific tasks in the pool. LLMs as Embedding Models Pre-trained LLM encoders with bidirectional attention, such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020), have been very popular as backbones for embedding models. DPR (Karpukhin et al., 2020), Contriever (Izacard et al., 2022), SentenceBERT (Reimers and Gurevych, 2019), Language-agnostic BERT Sentence Embedding (LaBSE) (Feng et al., 2022), Sentence-T5 (Ni et al., 2021), GTR (Ni et al., 2021), and E5 (Wang et al., 2022) are some of the notable ones. Neelakantan et al. (2022) initialized embedding models from decoder-only GPT-3 (Brown et al., 2020) and adapted it for embeddings via continued contrastive pre-training. They have drastically scaled their embedding model up to 175 billion parameters, demonstrating scaling gains from pre-trained LLM backbones. Several recent embedding models such as E5-Mistral (Wang et al., 2023), SFR-Mistral (Meng et al., 2024), BGE-ICL (Li et al., 2024), and NV-Embed (Lee et al., 2025) have been initialized from the Mistral-7B (Jiang et al., 2023) backbone and then further adapted as embedding models. These models generally outperform the BERT or T5 based models, showing the benefits of initializing from pre-trained LLMs. However, their reliance on extensive in-domain training datasets has resulted in overfitting to specific benchmarks (Enevoldsen et al., 2025). 3https://huggingface.co/spaces/mteb/leaderboard; March 10th, 2025. Gemini Embedding: Generalizable Embeddings from Gemini Figure 1 Gemini Embedding represents text as dense vectors where semantically similar text inputs are mapped to vectors near one another in the vector space. Currently it supports more than 100+ languages, and its embeddings can be used for various tasks such as retrieval and classification. 3. Gemini Embedding In this section we provide technical details of the Gemini Embedding model in terms of the model architecture, the objective function, and the training recipe. 3.1. Model Architecture The Gemini Embedding model is built to create holistic representations of inputs for diverse downstream tasks, including retrieval, clustering, classification, and ranking by leveraging the power of Gemini. The embedding model is initialized from Gemini and further refined. This allows Gemini Embedding to build representations on top of the vast knowledge already present in Geminis parameters. In this sense, initializing the embedding model from Gemini can be seen as the \"pre-training\" of the Gemini Embedding model. An input sequence of ùêø tokens is processed by M, transformer with bidirectional attention initialized from Gemini, producing sequence of token embeddings Tembed = (T) ‚ÑùùêøùëëM , where ùëëM is the model dimension. To generate single embedding representing all the information in the input, pooler is applied, Pembed = (Tembed) ‚ÑùùëëM . Prior research (Suganthan et al., 2025) has demonstrated that simple pooling strategies can be effective in model adaptation. Therefore we have chosen mean pooling, and simply average the token embeddings along the sequence axis. Finally, randomly initialized linear projection is applied to scale the embedding to the target dimension, = (Pembed) ‚Ñùùëë, where ùëë is the output embedding dimension. 3.2. Training Objective The Gemini Embedding model was trained with noise-contrastive estimation (NCE) loss with inbatch negatives. The exact loss differs slightly depending on the stage of training. In general, training example includes query ùëûùëñ, positive target ùëù+ ùëñ and (optionally) hard negative target ùëù ùëñ . Each example also has prescribed task string ùë°, for example \"question answering\" or \"fact checking\", describing the nature of the task. The query and passages are embedded as vectors in ‚Ñùùëë: qùëñ = ùëì (mean_pool(M (ùë° ùëûùëñ))), ùëñ = ùëì (mean_pool(M ( ùëù ùëñ )). (1) 4 Gemini Embedding: Generalizable Embeddings from Gemini Given batch of size ùêµ the loss applied to these embeddings is as follows: = 1 ùêµ ùêµ ùëñ=1 log ùëísim(qùëñ,p ùëó )/ùúè + (cid:205)ùêµ ùëñ )/ùúè ùëísim(qùëñ,p+ ùëó=1 mask(ùëñ, ùëó)ùëísim(qùëñ,p+ ùëó )/ùúè where sim(x, y) = xy/xy is cosine similarity, and mask(ùëñ, ùëó) = (cid:40) 0 if ùëûùëñ = ùëû ùëó or ùëù+ 1 otherwise. ùëñ = ùëù+ ùëó , (2) (3) This masking term is particularly relevant for classification tasks, where the number of targets (labels) is small. The first term in the denominator is omitted if no hard negative is provided. In contrast with Gecko (Lee et al., 2024), we omit the same-tower negatives (Moiseev et al., 2023) from the loss, as we find this decreases performance for most tasks due to the potential of false negatives. In order to support different dimensions of embeddings with single model, we adapt the above loss using MRL (Kusupati et al., 2022), which adapts the loss above into ùëò separate losses across ùëò overlapping sub-dimensions of the embedding (e.g. multi-loss training with one loss for the first 768 embedding dimensions, another for the first 1,536 dimensions, and so on). Gemini Embedding provides ùëë = 3, 072 dimensional embeddings, with the MRL support on 768 and 1,536 dimensions. 3.3. Recipe Initializing the embedding model from the Gemini parameters is good starting point that leverages the language model power. This initialization can be considered \"pre-training\" of the embedding model. However, in order to truly capture the generalization capabilities of initialization, we found it beneficial to leverage two-stage training pipeline. Pre-finetuning First, the model is \"pre-finetuned\" on large number of potentially noisy (query, target) pairs, omitting the hard-negative term from the loss function. We find it beneficial to use large batch size, as the primary objective is to adapt the parameters from autoregressive generation to encoding. The larger batch size also provides more stable gradient, mitigating the impact of noise in this phase of training. Due to the larger size of the pre-finetuning dataset, pre-finetuning is performed for substantially greater number of steps compared to fine-tuning. Finetuning Next, the model is fine-tuned on large mixture of task-specific datasets which contain (query, target, hard negative target) triples. For this phase of training we found it beneficial to use smaller batch sizes (e.g., less than 1024), and furthermore limit each batch to single dataset, as distinguishing given positive target from in-batch targets from the same task provides greater signal than discerning (say) retrieval target from classification label. We perform grid search of various training hyperparameters, including the inclusion and exclusion of components of the mixture, to obtain candidate checkpoints. Model Soup To obtain additional generalization performance, we averaged the parameters obtained from individual fine-tuning runs. We experimented with different combinations of parameters, including averaging checkpoints from the same training run (Izmailov et al., 2018), from different training runs (Wortsman et al., 2022), as well as various weighted averages. The final set of ingredient checkpoints were obtained through combination of intentional data variation as well as manual checkpoint selection and experimentation. Gemini Embedding: Generalizable Embeddings from Gemini 4. Datasets Our training data mixture contains diverse multilingual embedding tasks as well as code retrieval tasks. Gemini is used in three different ways to improve the quality of our data: synthetic data generation, data filtering, and hard negative mining. 4.1. Training Data Mixture Pre-finetuning Our pre-finetuning stage aims to maximize the exposure of diverse training datasets to Gemini Embedding models. We leverage billion-scale web corpus and used title and passage pairs as input and positive target pairs, similar to some prior work (Lee et al., 2024; Neelakantan et al., 2022). Despite being very simple, this technique is consistently found to be effective even when the embedding model is initialized from an LLM. Fine-tuning For fine-tuning, we prepare three different mixtures aiming for task diversity, language diversity, and coding capability. For task diversity, we use subset of academic datasets used by Gecko (Lee et al., 2024) as well as several synthetic datasets introduced in 4.2. Unlike existing models on the classic MTEB (Muennighoff et al., 2023), we excluded many in-domain MTEB datasets, which improved the performance only on their own test split mainly due to train-test leakage or dataset bias. The training mixture rate was decided based on fine-grained grid search, initialized from the optimal number of training steps to converge on each training dataset. 4.2. Improving Data Quality with Gemini Synthetic Data Generation Recent embedding evaluation benchmarks such as MMTEB (Enevoldsen et al., 2025) contain many different tasks other than retrieval. We diversify and improve our training mixture by adding synthetically generated datasets for two task types: retrieval and classification. For retrieval, we extended our prior work on synthetic data generation using Gemini enhanced adaptations of FRet (Lee et al., 2024) and SWIM-IR (Thakur et al., 2024). Using few-shot prompting, we first use Gemini to generate synthetic queries for web passages followed by Gemini auto-rater to filter lower-quality examples (e.g., unrealistic search queries). For classification, we generate synthetic counterfactual, sentiment, and review classification datasets in English. To increase the quality of these synthetic datasets we developed multi-stage prompting strategies, such as conditioning on synthetic user, product, or movie generations in hierarchical manner and sampling from the tail of longer lists of generations, as diversity naturally increases with generation length. Data Filtering Our training data mixture includes many human-annotated datasets. We noticed that many retrieval datasets have quality issues of incorrect positive or negative targets for query. We use Gemini to filter such bad examples. Based on our few-shot prompting for data quality assessment, we remove low quality examples. Hard Negative Mining standard technique when training embedding models is to mine \"hard negatives,\" i.e. targets which are semantically similar to true positive target but do not answer the query (Reddi et al., 2019). We mine hard negatives for our retrieval datasets using Gemini. We first train Gemini-initialized embedding model without using any hard negatives. Based on this initial embedding model, we retrieve top ùëò nearest neighbors for each query. Each nearest neighbor is then scored by Gemini along with the query. We follow Lee et al. (2024) and employ two different prompting strategiesgraded classification and query likelihoodcombining the scores with Reciprocal Rank Fusion (RRF) (Cormack et al., 2009). We found that the lowest-scoring nearest neighbors, (the ùëò-th neighbor after being sorted by Gemini scores) serve as the best hard negatives. 6 Gemini Embedding: Generalizable Embeddings from Gemini Model Name Gemini Embedding Linq-Embed-Mistral gte-Qwen2-7B-instruct multilingual-e5-large-instruct SFR-Embedding-Mistral GritLM-7B text-multilingual-embedding-002 GritLM-8x7B e5-mistral-7b-instruct Cohere-embed-multilingual-v3.0 gte-Qwen2-1.5B-instruct bilingual-embedding-large Mean (Task) Mean (Type) Bitext Mining Rank 1 2 3 4 5 6 7 8 9 10 11 12 68.3 59.6 61.5 62.5 63.2 60.9 60.9 62.1 60.5 60.3 61.1 59.5 60. 54.2 56.0 55.2 54.0 53.8 54.3 53.4 53.2 53.3 52.8 53.0 79.3 70.3 73.9 80.1 70.0 70.5 70.7 68.2 70.6 70.5 62.5 73.6 Class. Clus. 71.8 55. 62.2 61.6 64.9 60.0 61.8 64.6 61.6 60.3 63.0 58.3 62.8 51.3 53.4 51.5 52.6 50.5 48.5 50.9 51.4 47.6 52.6 47.2 Inst. Retrieval Multi. Class. Pair. Class. 5. 0.9 4.9 -0.4 0.2 3.5 4.1 2.4 -0.6 -1.9 0.7 -3.0 29.2 24.8 25.5 22.9 24.6 22.8 22.8 24.4 22.2 22.7 24.0 22.4 83.6 80.4 85.1 80.9 80.3 79.9 81.1 79.7 81.1 79.9 81.6 79.8 Rerank. Retrieval STS 65.6 67.7 79.4 64.4 65.6 62.6 64.2 63.8 61.2 62.6 63.8 64.1 62.6 61.4 58.7 60.1 57.1 59.4 58.3 59.7 57.5 55.8 59.2 60.8 55. 74.9 74.0 76.8 74.8 73.3 76.1 73.2 74.0 74.8 71.6 77.8 Table 2 Performance of top leaderboard models on MTEB(Multilingual). 5. Evaluation Gemini Embedding is assessed on comprehensive collection of task types, domains, languages, and language pairs (e.g., Hindi queries retrieving English content) using benchmark evaluations from the Massive Multilingual Text Embedding Benchmark, MMTEB (Enevoldsen et al., 2025), and the cross-lingual benchmarks XTREME-UP (Ruder et al., 2023) and XOR-Retrieve (Asai et al., 2021). 5.1. Benchmarks and Tasks MMTEB consists of large collection of individual evaluation tasks covering 250+ languages and 10 task types: Bitext Mining, Classification, Clustering, Instruction Retrieval, Multilabel Classification, Pair Classification, Reranking, Retrieval, STS, and Summarization. Our MMTEB evaluations include 164 individual evaluation tasks consisting of 132 evaluation tasks for MTEB(Multilingual), 41 tasks for MTEB(Eng, v2), and 12 code retrieval tasks for MTEB(Code). Notably, MTEB(Multilingual) contains 250+ languages. XOR-Retrieve and XTREME-UP provide cross-lingual retrieval evaluations, with XOR-Retrieve pairing English passages with retrieval queries in 7 different languages and XTREME-UP similarly pairing English passages with queries in 20 underrepresented Indo-European languages. 5.2. Overall Performance Gemini Embeddings overall performance along with that of other top performing models is presented in Table 1 on the following evaluations: three benchmarks from MMTEB, MTEB(Multilingual), MTEB(Eng, v2), MTEB(Code); and the two cross-lingual benchmarks XOR-Retrieve and XTREME-UP. Gemini Embedding establishes new state-of-the-art in performance, achieving the highest overall performance on the MTEB(Multilingual) leaderboard (March 10th, 2025) with substantial performance lead over all previous top performing models on each of the overall metrics summarizing aggregate performance across tasks: Task Mean (equal weighting of all tasks): 68.32, Task Type Mean (equal weighting of all task types): 59.64, and Borda rank #1 (official leaderboard ranking metric). Gemini Embeddings performance advantage is not limited to just MTEB(Multilingual). Within single unified model and shared embedding space, Gemini Embeddings capabilities allow it to achieve: (i) #1 ranking on MTEB(Multilingual), (ii) #1 ranking on MTEB(Eng, v2), (iii) #1 ranking on MTEB(Code), and (iv) excellent cross-lingual retrieval on XOR-Retrieve and XTERME-UP, advancing the state-of-the-art for general-purpose embeddings as cross-lingual representations. 7 Gemini Embedding: Generalizable Embeddings from Gemini Model Name Gemini Embedding Linq-Embed-Mistral jasper_en_vision_language_v1 SFR-Embedding-Mistral NV-Embed-v2 text-embedding-005 (Gecko) text-embedding-004 (Gecko) gte-Qwen2-7B-instruct e5-mistral-7b-instruct stella_en_400M_v5 stella_en_1.5B_v5 gte-Qwen2-1.5B-instruct Mean (Task) Mean (Type) Rank Class. Clus. 1 2 3 4 5 6 7 8 9 10 11 12 73.3 67.7 90.1 59. 69.8 71.4 69.3 69.8 69.6 69.5 70.7 68.0 69.4 69.4 67.2 65.3 66.7 64.9 65.0 64.8 64.8 65.8 64.0 64.8 65.3 63.3 83.0 90.3 80.5 87.2 86.0 86.0 88.5 79.9 88.3 89.4 85.8 54.1 60.5 54.9 47.7 51.9 51.5 59.0 51.4 57.7 57.1 53.5 Pair. Class. 87. 88.4 88.1 88.6 88.7 87.6 87.7 85.9 88.4 87.2 88.0 87.5 Rerank. Retrieval STS Summ. 48.6 49.4 50.0 50.2 49.6 48.8 48.5 50.5 49.8 49.6 50.2 49. 64.4 85.3 38.3 60.1 56.1 59.3 62.8 58.8 59.1 58.1 57.6 52.7 52.4 50.3 84.7 84.4 84.8 83.8 85.2 84.8 82.7 84.3 83.9 83.3 82.5 37.3 37.2 36.3 35.2 35.1 36.1 35.7 36.6 34.5 36.9 33. Table 3 Performance of top leaderboard models on MTEB(Eng, v2). Model Name Gemini Embedding inf-retriever-v1-1.5b text-embedding-005 (Gecko) voyage-code-3 NV-Embed-v2 voyage-3 GritLM-7B KaLM-emb.-mling.-mini-v1 text-embedding-3-large NV-Embed-v1 SFR-Embedding-Mistral Linq-Embed-Mistral Rank Mean All Mean -COIR AppsR. COIR CESR CSNCCR CSNR CTOC CTODL CQA 1 2 3 4 5 6 7 8 9 10 11 12 75.5 74. 93.8 81.1 81.6 62.9 63.3 - - - - - - - - - 60.6 65.4 - 59.4 67.3 62.4 57.4 59.0 57.7 56.7 57.5 38.9 91.3 93.6 29.1 73.0 35.1 46.8 28.4 30.3 26.1 30. 78.6 48.4 89.4 - - - - - - - - 67.2 54.4 - 74.0 75.6 74.6 60.0 71.1 70.8 68.8 70.6 84.7 75.5 55.7 90.1 68.8 77.9 86.7 59.5 73.2 65.1 64.5 64.5 91.3 90.9 87.2 94.0 86.6 92.3 86.7 88.0 90.5 85.8 86.7 87. 89.5 85.0 82.8 95.0 89.1 89.9 89.2 79.9 84.3 85.1 83.5 84.9 31.5 33.8 34.4 38.6 33.4 33.9 33.0 34.0 34.2 33.1 32.9 32.8 50.2 33.1 52.2 34.5 34.8 28.7 31.2 33.6 31.0 33.4 34.3 32. Table 4 Performance of top leaderboard models on MTEB(Code). MTEB(Multilingual) leaderboard In Table 2, Gemini Embedding is compared with top-ranked models from MTEB(Multilingual). Achieving the highest Borda rank and excellent overall performance across task types, Gemini Embedding particularly excels at Classification (+9.6), Clustering (+3.7) and Retrieval (+9.0) compared to the second-best model. MTEB(Eng, v2) leaderboard Comparing with top-ranked MTEB(Eng, v2) leaderboard models in Table 3, Gemini Embedding achieves the highest Borda rank and great overall performance across task types, with particularly striking performance improvements on Classification (+7.1), Clustering (+5.3), and Retrieval (+4.3) compared to the second-best model. MTEB(Code) leaderboard The eight tasks present on the MTEB(code) leaderboard, which excludes the four additional MTEB(code) tasks CodeFeedbackMT, CodeFeedbackST, StackOverflowQA, and SyntheticText2SQL, are shown in Table 4. Only few models, including both Gemini Embedding and Googles Gecko model, have been submitted to the MTEB(Code) leaderboard with evaluations over all tasks. On the MTEB(Code) leaderboard, Gemini Embedding once again achieves the highest Borda rank and mean performance across all eight evaluation tasks. Since the majority of other top models on MTEB(Code) are missing COIRCodeSearchNetRetrieval (COIR), we also report the mean performance over the seven remaining tasks, Mean -COIR. Gemini Embedding still achieves the best mean performance over the seven Mean -COIR evaluation tasks. 8 Gemini Embedding: Generalizable Embeddings from Gemini Average as bho brx gbm gom gu hi hne kn mai ml mni mr mwr or pa ps sa ta ur Gemini Embedding Gecko i18n Embedding 64.3 35.0 39.2 voyage-3-large Linq-Embed-Mistral 24.6 multiling.-e5-large-instr. 18.7 17.4 gte-Qwen2-7B-instruct 18.8 text-embedding-3-large 69.2 66.4 25.7 64.9 65.5 70.3 69.1 68.3 69.5 68.4 70.8 44.4 68.8 66.5 65.8 69.5 61.9 68.1 68.6 64.8 37.0 31.9 39.7 42.9 33. 35.9 40.5 26.0 37.4 46.3 42. 41.6 44.1 45.5 41.5 40.7 19. 40.9 9.4 3.8 34.3 23.8 21.2 14.7 18.2 44.8 38.1 21.9 22.7 28.8 7.9 8.6 1.5 5.4 3. 46.6 37.0 19.3 23.0 28.4 27.1 21.7 8.7 7.0 11.1 46.7 11.6 13.9 19.1 14.6 54.3 44.2 30.6 30.4 40.4 45.3 39.7 22.6 19.1 29.3 41.5 21.7 24.2 16.2 17. 48.3 38.5 24.0 25.9 31.1 45.3 10.2 8.6 21.7 15.6 19.2 14.7 6.3 7.2 2.9 45.5 31.4 23.0 23.8 25.5 47.9 36.2 19.8 24.0 28.7 32.3 10.7 17.3 11.3 8. 48.4 8.3 24.5 19.2 11.3 26.8 13.8 15.9 11.0 6.8 40.0 37.7 19.1 21.1 26.6 36.0 14.3 22.9 9.7 6.0 45.6 29.3 28.2 15.5 22.0 Table 5 Performance of top multilingual models on XTREME-UP (MRR@10). Figure 2 Gemini Embedding supports cross-lingual retrieval where different languages can be used for queries and passages. We show two examples from XTREME-UP showing the strong cross-lingual retrieval capability of Gemini Embedding. Despite Assamese being relatively low-resource language and the Hindi query having typo, the Gemini Embedding model correctly understood the key entities and the contexts in the queries and retrieved the correct passages. XTREME UP The performance of Gemini Embedding along with the top-performing multilingual models on XTREME-UP cross-lingual retrieval is presented in Table 5. XTREME-UP requires mapping queries in 20 underrepresented languages to English passages. Gemini Embedding demonstrates remarkable improvement in cross-lingual retrieval with its general-purpose embeddings. 5.3. Qualitative Examples In Figure 2, we show examples from XTREME-UP that show the cross-lingual retrieval capability of Gemini Embedding. The two queries are given in Assamese and Hindi, and the task is to retrieve relevant English passages that contain the answers. Each query without any translation is encoded and the highest-scoring English passages are retrieved using cosine similarity. Gemini Embedding found the right passages showcasing its strong capability on multilingual and cross-lingual tasks. 9 Gemini Embedding: Generalizable Embeddings from Gemini MTEB(Multilingual) MTEB(Eng, v2) MTEB(Code) XOR-Retrieve XTREME-UP 68.32 73.28 74.66 90.42 64. 48.89 30.55 66.75 58.24 60.20 50.99 28.17 72.77 61.88 62.25 46.18 9.86 58.68 58.75 72. 76.64 - 85.70 89.00 82.16 21.22 - 49.34 65.06 34.74 Gemini Embedding Pre-Finetuning Pre-finetuning Only No Training Fine-tuning Mixtures English Only (Diverse Task) Multilingual Only (Retrieval) Code Only (Retrieval) Table 6 Results using different training mixtures for MTEBs (task mean), XTREME-UP (MRR@10), and XOR-Retrieve (Recall@5kt). Using Gemini foundation, the English Only mixture is able to achieve good performance on MTEB(Multilingual), MTEB(Eng, v2) and XOR-Retrieve. Multilingual fine-tuning helps the most on the long-tail languages in XTREME-UP. Ablations exclude model souping. Average AmazonCounterfactual AmazonPolarity AmazonReviews Emotion w/o Synthetic w/ Synthetic 57.57 75.17 (+17.6) Gecko Embedding 66.78 Gemini Embedding 76.09 65.43 91.30 66.52 92.70 67.29 96.51 97.28 96. 48.84 57.00 51.24 59.30 48.70 55.90 52.09 56.27 Table 7 Results on MTEB classification using synthetic datasets. Self-training on Gemini generated training data dramatically improves model performance, +17.6. Ablation models exclude souping. Gecko training mixtures include training sets provided by several classification tasks from Huggingface. 6. Ablation Study To better understand how Gemini Embedding achieves great performance across many different tasks and languages, we provide systematic analysis of our training recipe. 6.1. Does Gemini Embedding Generalize to Multilingual Tasks? In Table 6, we show how Gemini Embedding can generalize over different languages and tasks. In the middle rows, we show our models performance before fine-tuning: no training and pre-finetuning only. Pre-finetuning greatly improves the performance across multiple benchmarks. The bottom rows show the effect of further fine-tuning the pre-finetuned checkpoints. We find that training on the English-only mixture still achieves very strong performance on MTEB(Multilingual) where the evaluations are mostly zero-shot. Remarkably, even when training our model on the English-only mixture, we are able to outperform the top embedding models on XTREME-UP.4 This shows Gemini Embedding can generalize over different languages even if its training mixture contains only single language. On the other hand, our multilingual-only mixture consists of only retrieval datasets but not other task types such as classification. Its lower score indicates that task diversity matters more than language diversity for fine-tuning in Gemini Embedding. 4+10.1 MMR@10 for English-only fine-tuning in Table 6 vs. the top performing non-Gemini model in Table 5 10 Gemini Embedding: Generalizable Embeddings from Gemini Average ar bn de en es fa fi fr hi id ja ko ru sw te th yo zh w/o Filtering w/ Filtering 59.8 63.7 (+3.9) Gecko Embedding 56.2 Gemini Embedding 70.1 74.8 71.5 71.9 85.0 51.7 74.2 74.7 52.9 54.7 47.3 55.5 74.7 49.5 59.3 47.1 61.9 63.3 64.8 76.0 75.0 75.0 83.3 57.0 46.0 59.4 34. 51.3 73.6 46.5 62.6 55.1 54. 56.9 44.5 69.7 66.8 66.7 64.3 50.7 78.3 79.0 59.8 58.7 57.0 60.9 78.0 55.6 65.4 54.3 75.1 68.9 73.4 81.0 80.5 80.8 88.8 65. 52.6 54.0 44.7 49.2 67.4 57. 55.1 45.3 74.5 55.0 66.5 49. 48.5 45.1 65.2 Table 8 Results on filtering the MIRACL datasets. We show that proper filtering of retrieval datasets using LLMs can greatly improve the performance. Figure 3 Results on retrieval datasets with different number of hard negatives. We show that our hard negatives are mostly useful. 6.2. How Does Gemini Improve Data Quality? Synthetic Data Generation We show the effectiveness of our multi-stage prompting strategy to create diverse, realistic synthetic classification datasets in Table 7. Note that these are zero-shot synthetic datasets, so no actual examples from the original datasets were used when prompting Gemini. Training on our synthetic classification datasets greatly improves the performance on all datasets. We find that the performance with synthetic datasets can match the performance of indomain datasets (e.g. Gecko on AmazonPolarity), and our multi-stage prompting strategy even allows for controllable generation, raising the possibility of reducing bias compared to real data. Data Filtering We use Gemini to filter retrieval datasets. We test filtering the MIRACL (Zhang et al., 2023) training datasets, which contain retrieval datasets in 18 different languages, and measure the impact of training on the filtered dataset. Table 8 shows that filtered results consistently show better results across different languages showing only minor drops for some languages. As demonstrated in Table 6, our English mixture helps to improve the quality on multilingual tasks, making Gemini Embedding the best in Table 8 as well. Hard Negative Mining We examine the quality of our hard negatives selected by Gemini. As demonstrated in Figure 3, incorporating hard negatives generally enhances our models retrieval performance across the four datasets. However, excessive hard negatives often led to overfitting, causing performance degradation for retrieval tasks. Future work will explore regularization techniques and better hard negative sampling strategies to address overfitting. 7. Future Work Beyond the text embedding capabilities described here, we will explore extending the embedding capabilities for other modalities like image, video, and audio. We want to leverage the powerful multimodal capabilities of Gemini to make the Gemini Embedding model comprehensive (Jiang et al., 2024) in terms of representing different combinations of modalities together in single embedding space. This will require curating multi-modal data tasks suitable for learning generalizable representations. We will also explore training recipes that will balance the performance of single model across different uni-modal and multi-modal capabilities. Gemini Embedding: Generalizable Embeddings from Gemini 8. Conclusion Gemini Embedding is unified, general-purpose, and highly-capable embedding model that capitalizes on the strong capabilities of Gemini to advance the state-of-the-art in representation learning. Building on an excellent foundation provided by Geminis multilingual and code understanding capabilities, Gemini Embedding generates versatile encoding of model inputs into representations with wide range of capabilities over many languages, domains, and task types including: classification, similarity search, clustering, ranking, and retrieval. Gemini Embedding both adapts the capabilities of Gemini to representation learning and uses Gemini itself to generate many of the training sets for this adaptation. The resulting representations benefit from the underlying capabilities of Gemini itself while also being efficient to precompute, cache, and re-use them. Efficiently cacheable and reusable representations unlock the ability to apply the power of Gemini in new compute and latency-sensitive settings. Rigorous evaluations provided by the Massive Multilingual Text Embedding Benchmark (MMTEB) reveal substantial gains over previous top-performing models advancing the state-of-the-art in performance on multilingual, English, and code evaluations. Beyond strong overall performance, Gemini Embedding particularly excels at classification, clustering and retrieval tasks. The advanced versatile and unified capabilities provided by Gemini Embedding and the ability to precompute representations enables the power of Gemini to be leveraged more broadly by both researchers and developers alike."
        },
        {
            "title": "References",
            "content": "R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023a. R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023b. A. Asai, J. Kasai, J. H. Clark, K. Lee, E. Choi, and H. Hajishirzi. Xor qa: Cross-lingual open-retrieval question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 547564, 2021. L. Bonifacio, H. Abonizio, M. Fadaee, and R. Nogueira. Inpars: Unsupervised dataset generation for information retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 23872392, 2022. T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. D. Cer, Y. Yang, S.-y. Kong, N. Hua, N. Limtiaco, R. S. John, N. Constant, M. Guajardo-Cespedes, S. Yuan, C. Tar, et al. Universal sentence encoder for english. In Proceedings of the 2018 conference on empirical methods in natural language processing: system demonstrations, pages 169174, 2018. G. V. Cormack, C. L. Clarke, and S. Buettcher. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 758759, 2009. 12 Gemini Embedding: Generalizable Embeddings from Gemini Z. Dai, V. Y. Zhao, J. Ma, Y. Luan, J. Ni, J. Lu, A. Bakalov, K. Guu, K. B. Hall, and M.-W. Chang. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755, 2022. J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 41714186. Association for Computational Linguistics, 2019. K. Enevoldsen, I. Chung, I. Kerboua, M. Kardos, A. Mathur, D. Stap, J. Gala, W. Siblini, D. Krzemi≈Ñski, G. I. Winata, et al. Mmteb: Massive multilingual text embedding benchmark. arXiv preprint arXiv:2502.13595, 2025. F. Feng, Y. Yang, D. Cer, N. Arivazhagan, and W. Wang. Language-agnostic BERT sentence embedding. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), May 2022. URL https: //aclanthology.org/2022.acl-long.62/. T. Gao, X. Yao, and D. Chen. Simcse: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 68946910, 2021. G. Izacard, M. Caron, L. Hosseini, S. Riedel, P. Bojanowski, A. Joulin, and E. Grave. Unsupervised dense information retrieval with contrastive learning. Transactions on Machine Learning Research, 2022. P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. G. Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407, 2018. V. Jeronymo, L. Bonifacio, H. Abonizio, M. Fadaee, R. Lotufo, J. Zavrel, and R. Nogueira. Inpars-v2: Large language models as efficient dataset generators for information retrieval. arXiv preprint arXiv:2301.01820, 2023. A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310. 06825. Z. Jiang, R. Meng, X. Yang, S. Yavuz, Y. Zhou, and W. Chen. Vlm2vec: Training vision-language models for massive multimodal embedding tasks. arXiv preprint arXiv:2410.05160, 2024. V. Karpukhin, B. Oƒüuz, S. Min, P. Lewis, L. Y. Wu, S. Edunov, D. Chen, and W. tau Yih. Dense passage retrieval for open-domain question answering. ArXiv, abs/2004.04906, 2020. A. Kusupati, G. Bhatt, A. Rege, M. Wallingford, A. Sinha, V. Ramanujan, W. Howard-Snyder, K. Chen, S. Kakade, P. Jain, et al. Matryoshka representation learning. Advances in Neural Information Processing Systems, 35:3023330249, 2022. Q. Le and T. Mikolov. Distributed representations of sentences and documents. In International conference on machine learning, pages 11881196. PMLR, 2014. C. Lee, R. Roy, M. Xu, J. Raiman, M. Shoeybi, B. Catanzaro, and W. Ping. Nv-embed: Improved techniques for training llms as generalist embedding models. ArXiv, 2025. URL https://arxiv. org/abs/2405.17428. Gemini Embedding: Generalizable Embeddings from Gemini J. Lee, Z. Dai, X. Ren, B. Chen, D. Cer, J. R. Cole, K. Hui, M. Boratko, R. Kapadia, W. Ding, Y. Luan, S. M. K. Duddu, G. H. Abrego, W. Shi, N. Gupta, A. Kusupati, P. Jain, S. R. Jonnalagadda, M.-W. Chang, and I. Naim. Gecko: Versatile text embeddings distilled from large language models. arXiv preprint arXiv:2403.20327, 2024. C. Li, M. Qin, S. Xiao, J. Chen, K. Luo, Y. Shao, D. Lian, and Z. Liu. Making text embedders few-shot learners. ArXiv, 2024. URL https://arxiv.org/abs/2409.15700. R. Meng, Y. Liu, S. R. Joty, C. Xiong, Y. Zhou, and S. Yavuz. Sfrembedding-mistral: enhance text retrieval with transfer learning. Salesforce AI Research Blog, 3:6, 2024. F. Moiseev, G. H. Abrego, P. Dornbach, I. Zitouni, E. Alfonseca, and Z. Dong. Samtone: Improving contrastive loss for dual encoder retrieval models with same tower negatives. arXiv preprint arXiv:2306.02516, 2023. N. Muennighoff, N. Tazi, L. Magne, and N. Reimers. Mteb: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 20062029, 2023. A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. Tezak, J. W. Kim, C. Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005, 2022. J. Ni, C. Qu, J. Lu, Z. Dai, G. H. Abrego, J. Ma, V. Zhao, Y. Luan, K. B. Hall, M.-W. Chang, and Y. Yang. Large dual encoders are generalizable retrievers. In Conference on Empirical Methods in Natural Language Processing, 2021. J. Ni, G. H. Abrego, N. Constant, J. Ma, K. Hall, D. Cer, and Y. Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Findings of the Association for Computational Linguistics: ACL 2022, pages 18641874, 2022. R. Nogueira, W. Yang, J. Lin, and K. Cho. Document expansion by query prediction. arXiv preprint arXiv:1904.08375, 2019. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):54855551, 2020. S. J. Reddi, S. Kale, F. Yu, D. Holtmann-Rice, J. Chen, and S. Kumar. Stochastic negative mining for learning with large output spaces. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 19401949. PMLR, 2019. N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, 2019. S. Ruder, J. H. Clark, A. Gutkin, M. Kale, M. Ma, M. Nicosia, S. Rijhwani, P. Riley, J.-M. Sarr, X. Wang, et al. Xtreme-up: user-centric scarce-data benchmark for under-represented languages. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 18561884, 2023. P. Suganthan, F. Moiseev, L. Yan, J. Wu, J. Ni, J. Han, I. Zitouni, E. Alfonseca, X. Wang, and Z. Dong. Adapting decoder-based language models for diverse encoder downstream tasks, 2025. URL https://arxiv.org/abs/2503.02656. 14 Gemini Embedding: Generalizable Embeddings from Gemini G. Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv.org/abs/2403.05530. N. Thakur, J. Ni, G. Hernandez Abrego, J. Wieting, J. Lin, and D. Cer. Leveraging LLMs for synthesizing training data across many languages in multilingual dense retrieval. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), June 2024. URL https://aclanthology.org/ 2024.naacl-long.426/. L. Wang, N. Yang, X. Huang, B. Jiao, L. Yang, D. Jiang, R. Majumder, and F. Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022. L. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368, 2023. M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International conference on machine learning, pages 2396523998. PMLR, 2022. X. Zhang, N. Thakur, O. Ogundepo, E. Kamalloo, D. Alfonso-Hermelo, X. Li, Q. Liu, M. Rezagholizadeh, and J. Lin. Miracl: multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics, 11:11141131, 2023. Gemini Embedding: Generalizable Embeddings from Gemini 9. Full Results Task Name Performance Task Name Performance AILAStatutes AfriSentiClassification AlloProfClusteringS2S.v2 Alloprof Reranking AmazonCounterfactualClassification ArXivHierarchicalClusteringP2P ArXivHierarchicalClusteringS2S ArguAna ArmenianParaphrasePC BUCC.v2 BelebeleRetrieval BibleNLPBitextMining BigPatentClustering.v2 BiorxivClusteringP2P.v2 BornholmBitextMining BrazilianToxicTweetsClassification BulgarianStoreReviewSentimentClassfication CEDRClassification CLSClusteringP2P.v2 CSFDSKMovieReviewSentimentClassification CTKFactsNLI CataloniaTweetClassification Core17InstructionRetrieval CovidRetrieval CyrillicTurkicLangClassification CzechProductReviewSentimentClassification DBpediaClassification DalajClassification DiaBlaBitextMining EstonianValenceClassification FaroeseSTS FilipinoShopeeReviewsClassification FinParaSTS FinancialPhrasebankClassification FloresBitextMining GermanSTSBenchmark GreekLegalCodeClassification GujaratiNewsClassification HALClusteringS2S.v2 HagridRetrieval IN22GenBitextMining IndicCrosslingualSTS IndicGenBenchFloresBitextMining IndicLangClassification IndonesianIdClickbaitClassification IsiZuluNewsClassification ItaCaseholdClassification JSICK KorHateSpeechMLClassification KorSarcasmClassification KurdishSentimentClassification LEMBPasskeyRetrieval LegalBenchCorporateLobbying MIRACLRetrievalHardNegatives MLQARetrieval MacedonianTweetSentimentClassification MalteseNewsClassification MasakhaNEWSClassification MasakhaNEWSClusteringS2S MassiveIntentClassification MedrxivClusteringP2P.v2 MultiEURLEXMultilabelClassification MultiHateClassification NTREXBitextMining NepaliNewsClassification News21InstructionRetrieval 48.77 53.56 56.36 81.77 88.20 64.92 63.84 86.44 96.89 98.99 90.73 20.72 38.06 53.86 51.69 28.02 78.13 57.42 42.68 49.38 87.59 54.51 7.69 79.13 95.30 68.16 94.76 50.47 87.23 53.52 86.12 48.45 28.60 88.64 83.71 88.09 43.76 92.05 32.00 99.31 93.75 62.87 96.77 87.69 67.00 40.53 73.30 84.99 17.69 60.51 86.39 38.50 95.98 70.42 84.16 71.83 37.38 83.55 57.45 81.92 47.16 5.28 72.47 93.64 98.14 10.26 NollySentiBitextMining NordicLangClassification NorwegianCourtsBitextMining NusaParagraphEmotionClassification NusaTranslationBitextMining NusaX-senti NusaXBitextMining OdiaNewsClassification OpusparcusPC PAC PawsXPairClassification PlscClusteringP2P.v2 PoemSentimentClassification PolEmo2.0-OUT PpcPC PunjabiNewsClassification RTE3 Robust04InstructionRetrieval RomaniBibleClustering RuBQReranking SCIDOCS SIB200ClusteringS2S SICK-R SNLHierarchicalClusteringP2P STS12 STS13 STS14 STS15 STS17 STS22.v2 STSB STSBenchmark STSES ScalaClassification SemRel24STS SentimentAnalysisHindi SinhalaNewsClassification SiswatiNewsClassification SlovakMovieReviewSentimentClassification SpartQA SprintDuplicateQuestions StackExchangeClustering.v2 StackOverflowQA StatcanDialogueDatasetRetrieRetrieval SwahiliNewsClassification SwednClusteringP2P SwissJudgementClassification T2Reranking TERRa TRECCOVID Tatoeba TempReasonL1 ToxicConversationsClassification TswanaNewsClassification TweetTopicSingleClassification TwitterHjerneRetrieval TwitterURLCorpus VoyageMMarcoReranking WebLINXCandidatesReranking WikiCitiesClustering WikiClusteringP2P.v2 WikipediaRerankingMultilingual WikipediaRetrievalMultilingual WinoGrande XNLI indonli 68.71 85.97 93.42 56.38 77.52 80.31 82.52 91.84 96.62 71.68 59.99 74.31 59.66 77.53 95.50 82.61 89.55 -2.41 43.22 73.84 25.15 41.74 82.75 61.41 81.55 89.89 85.41 90.44 88.58 71.69 85.50 89.08 81.75 51.85 73.14 76.06 82.29 62.38 90.35 10.30 96.90 92.07 96.71 51.11 66.05 45.84 57.86 67.95 63.92 86.32 81.97 2.96 88.75 53.37 71.11 98.02 87.05 66.73 10.97 91.63 28.23 92.24 94.20 60.52 85.26 60.69 Table 9 Full results of Gemini Embedding on MTEB(Multilingual). Gemini Embedding: Generalizable Embeddings from Gemini Task Name Performance AmazonCounterfactualClassification ArXivHierarchicalClusteringP2P ArXivHierarchicalClusteringS2S ArguAna AskUbuntuDupQuestions BIOSSES Banking77Classification BiorxivClusteringP2P.v2 CQADupstackGamingRetrieval CQADupstackUnixRetrieval ClimateFEVERHardNegatives FEVERHardNegatives FiQA2018 HotpotQAHardNegatives ImdbClassification MTOPDomainClassification MassiveIntentClassification MassiveScenarioClassification MedrxivClusteringP2P.v2 MedrxivClusteringS2S.v2 MindSmallReranking SCIDOCS SICK-R STS12 STS13 STS14 STS15 STS17 STS22.v2 STSBenchmark SprintDuplicateQuestions StackExchangeClustering.v2 StackExchangeClusteringP2P.v2 SummEvalSummarization.v2 TRECCOVID Touche2020Retrieval.v3 ToxicConversationsClassification TweetSentimentExtractionClassification TwentyNewsgroupsClustering.v2 TwitterSemEval2015 TwitterURLCorpus 92.69 64.92 63.84 86.44 64.24 88.97 94.27 53.86 70.68 53.69 31.06 88.98 61.78 87.01 94.98 99.27 88.46 92.08 47.16 45.01 32.95 24.04 82.75 81.55 89.89 85.41 90.44 91.61 68.37 89.08 96.90 92.07 50.91 38.28 86.32 52.39 88.75 69.88 57.37 79.17 87.05 Task Name Performance AppsRetrieval COIRCodeSearchNetRetrieval CodeEditSearchRetrieval CodeFeedbackMT CodeFeedbackST CodeSearchNetCCRetrieval CodeSearchNetRetrieval CodeTransOceanContest CodeTransOceanDL CosQA StackOverflowQA SyntheticText2SQL 93.75 81.06 81.61 56.28 85.33 84.69 91.33 89.53 31.47 50.24 95.92 69.96 Table 10 Full results of Gemini Embedding on MTEB(Eng, v2) (left) and MTEB(Code) (right). Language Performance ar bn fi ja ko ru te 91.26 94.08 89.17 86.31 89.82 88.61 93.70 Language Performance as bho brx gbm gom gu hi hne kn mai ml mni mr mwr or pa ps sa ta ur 69.25 66.38 25.66 64.87 65.54 70.26 69.06 68.33 69.54 68.39 70.82 44.44 68.77 66.49 65.77 69.55 61.90 68.09 68.57 64.85 Table 11 Full results of Gemini Embedding on XOR-Retrieve (left) and XTREME-UP (right). Gemini Embedding: Generalizable Embeddings from Gemini 10. Contributions and Acknowledgments Leadership Zach Gleicher Karan Gill Zhe Dong Mojtaba Seyedhosseini Yunhsuan Sung Raphael Hoffmann Tom Duerig Core Contributors (: equal contributions) Jinhyuk Lee Feiyang Chen Sahil Dua Daniel Cer Madhuri Shanbhogue Iftekhar Naim Gustavo Hern√°ndez √Åbrego Zhe Li Kaifeng Chen Henrique Schechter Vera Xiaoqi Ren Shanfeng Zhang Daniel Salz Michael Boratko Jay Han Blair Chen Shuo Huang Vikram Rao"
        },
        {
            "title": "Contributors\nPaul Suganthan\nFeng Han\nAndreas Doumanoglou\nNithi Gupta\nFedor Moiseev\nCathy Yip\nAashi Jain\nSimon Baumgartner\nShahrokh Shahi\nFrank Palma Gomez\nSandeep Mariserla\nMin Choi\nParashar Shah\nSonam Goenka\nKe Chen\nYe Xia\nKoert Chen\nSai Meher Karthik Duddu\nYichang Chen\nTrevor Walker\nWenlei Zhou\nRakesh Ghiya",
            "content": "18 Gemini Embedding: Generalizable Embeddings from Gemini Acknowledgement Anthony Chen, Slav Petrov, Ben Hora, Andrew McCallum, Manzil Zaheer, Lakshman Yagati, Fernando Pereira, Tania Bedrax-Weiss, Nicholas Monath, Enrique Alfonseca, Xinyang Yi, Lichan Hong, Andrew Lee, Lisa Patel, Ayla Karmali, Aditya Kusupati, Andrew Forbes, Scott Crowell, Srini Narayanan, Sean Nakamoto, Roopal Garg, Golnaz Farhadi, Ye Tian, Hongxiang Gu, Huijie Feng, Jiameng Fan, Pelin Dogan Sch√∂nberger, Grzegorz Makosa, M√°rio Lipovsk√Ω, Peter Ralbovsky, Istv√°n Gy√ºrki, Yi-Ting Chen, Zhongli Ding, Tanmaya Dabral, Ariel Fuxman, Chun-Ta Lu, Stein Xudong Lin, Yi Luan, Howard Zhou, Michael Kwong, Ting Liu"
        }
    ],
    "affiliations": [
        "Google"
    ]
}