{
    "paper_title": "Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference",
    "authors": [
        "Xiangwei Shen",
        "Zhimin Li",
        "Zhantao Yang",
        "Shiyi Zhang",
        "Yingfang Zhang",
        "Donghao Li",
        "Chunyu Wang",
        "Qinglin Lu",
        "Yansong Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x."
        },
        {
            "title": "Start",
            "content": "Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference Xiangwei Shen1,2,, Zhimin Li1,, Zhantao Yang1, Shiyi Zhang3, Yingfang Zhang1, Donghao Li1, Chunyu Wang1, Qinglin Lu1, Yansong Tang3, 1Hunyuan, Tencent 2School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen 3Shenzhen International Graduate School, Tsinghua University 5 2 0 2 ] . [ 2 2 4 9 6 0 . 9 0 5 2 : r Figure 1. Images generated by FLUX.1-dev finetuned through our Semantic Relative Preference Optimization (SRPO) Our method substantially improves upon the baseline model, achieving superior photorealism and enhanced fine-grained detail while maintaining remarkable training efficiency-converging in just 10 minutes using 32 NVIDIA H20 GPUs."
        },
        {
            "title": "Abstract",
            "content": "Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, method that predefines * Equal contribution. Corresponding author. noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x. 1. Introduction Online reinforcement learning (Online-RL) [5, 24, 25, 33] methods that perform direct gradient update through differentiable rewards have demonstrated substantial potential to align diffusion models with human preferences. Compared to policy-based approaches [4, 7, 8, 19, 30, 34], these methods use analytical gradients rather than policy gradients, allowing more efficient fitting of reward preferences. However, they face two serious challenges. First, they restrict optimization to only few diffusion steps, making them more susceptible to reward hacking, phenomenon where models achieve high reward scores for low-quality images [5, 6, 14, 19, 23, 34]. Second, they lack an online mechanism to adjust rewards and require costly offline preparations before RL to tune for desired aesthetic qualities such as photorealism or precise lighting effects. The first limitation stems from the conventional process of aligning the generation progress with reward model. Existing methods typically backpropagate gradients through standard multistep sampler [11, 28], such as DDIM. However, these frameworks are not only computationally expensive but also prone to severe optimization instability, such as gradient explosion. This issue becomes particularly acute when backpropagating gradients through the long computational graphs of early diffusion timesteps, forcing these methods to restrict optimization to the later stages of the trajectory. Nevertheless, this narrow focus on late-stage timesteps makes the model prone to overfitting the reward, as demonstrated in our experiment (see Fig. 7). This overfitting manifests as reward hacking, leading models to exploit known biases in popular reward models. For instance, HPSv2 [32] develops preference for reddish tones, PickScore [12] for purple images and ImageReward [33] for overexposed regions. Previous work [3, 15] has also found that these models tend to prefer smoothed images with low-detail. To address this limitation, our method first injects predefined noise into the clean image, enabling the model to directly interpolate back to the original from any given timestep. The second challenge is the absence of mechanisms for online reward adjustment to accommodate the evolving needs of real-world scenarios. To achieve superior visual quality, both the research community and industry often perform offline adjustments before RL. For example, contemporaneous works such as ICTHP [3] and Flux.1 Krea [15] have shown that existing reward models tend to ICTHP adfavor images with low aesthetic complexity. dresses this issue by collecting large, high-quality dataset to fine-tune the reward model, while other works such as DRaFT [5] and DanceGRPO [34] search for suitable reward systems to modulate image attributes such as brightness and saturation. In contrast, we propose treating rewards as text-conditional signals, enabling online adjustment through prompt augmentation without the need for additional data. To further mitigate reward hacking, we regularize the reward signal by using the relative difference between conditional reward pairs, defined by predefined positive and negative keywords applied to the same sample, as the objective function. This approach effectively filters out information irrelevant to semantic guidance. Consequently, we introduce reinforcement learning framework, Semantic Relative Preference Optimization (SRPO), built upon Direct-Align. In our experiments, we first leverage SRPO to adjust standard reward models to two critical but often overimage realism and texture detail. Next, looked aspects: we rigorously compare SRPO with several state-of-theart online RL-based methods on FLUX.1.dev, including ReFL [33], DRaFT [5], DanceGRPO [34], across diverse set of evaluation metrics such as Aesthetic predictor 2.5 [1], Pickscore [12], ImageReward [33], GenEval [9], and human assessments. Remarkably, our approach demonstrates substantial improvement in human evaluation metrics. Specifically, compared to the baseline FLUX.1.dev [13] model, our method achieves an approximate 3.7-fold increase in perceived realism and 3.1-fold improvement in aesthetic quality. Finally, we emphasize the efficiency of our approach. By applying SRPO to the FLUX.1.dev and training for only 10 minutes on HPDv2 dataset [32], our method enables the model to surpass the performance of the latest version of FLUX.1.Krea [15] on the HPDv2 benchmark. In summary, the key contributions are as follows: Mitigating Reward Hacking: The proposed framework effectively mitigates reward hacking. Specifically, it removes the limitation of previous methods that could only train on the later steps of the diffusion process. Furthermore, we introduce Semantic Relative Preference mechanism, which regularizes the reward signal by evaluating each sample with both positive and negative prompt conditional preference. Online Reward Adjustment: We reformulate reward signals as text-conditioned preferences, which enables dynamic control of the reward model via user-provided prompt augmentation. This approach reduces the reliance on reward-system or reward-model fine-tuning, thereby facilitating more fine-grained adaptation to downstream task requirements. State-of-the-Art Performance: Extensive evaluations demonstrate that our approach achieves state-of-the-art results. Breakthrough in Efficiency: Our method significantly enhances the realism of large-scale flow matching models without requiring additional data, achieving convergence within just 10 minutes of training. Figure 2. Method Overview. The SRPO contains two key elements: Direct-Align, and single reward model that derives both rewards and penalties from positive and negative prompts. The pipeline of Direct-Align consists of four stages: (0) generate/load an image for training; (1) inject noise into image; (2) perform one-step denoise/inversion; (3) recover image. 2. Related Work Optimization on Diffusion Timesteps. Recent advances [2, 6, 16, 21] have demonstrated that diffusion models [11, 28, 29] and flow matching methods [18, 20] can be unified under continuous-time SDE/ODE framework, where images are generated through progressive trajectory, with the early stages modeling the low-frequency structure and later steps refining high-frequency details such as texture and color. Recent studies [17, 37] suggest that optimizing early timesteps improves training efficiency and generation quality. However, standard direct backpropagation with reward approaches [5, 24, 25, 33] struggle with early stage optimization due to excessive noise that corrupts reward gradients. To address this, we propose novel sampling strategy that recovers clean images from highly noisy inputs in single step, enabling effective reward-based optimization even at early diffusion stages. Refining Reward Models for Human Preferences. central challenge in aligning diffusion models with human preferences is reward hacking, which often arises from mismatch between existing reward models and genuine human preferences. This discrepancy can be attributed to two primary factors. First, modeling inherently subjective human aesthetics is significant challenge, as illustrated by the low inter annotator agreement in previous reports [22, 33]: 65.7% for the ImageReward test set [33] and 59.7% for HPDv2 [32]. Second, current reward models [12, 27, 32, 33] are typically trained on limited criteria and outdated model generations, capturing preferences only at coarse granularity learned from their training data like Fielidy and Text-to-image alignment in ImageReward [33], and often require offline adjustment before RL to align with more advanced generative architectures and higher aesthetic demands. For example, ICTHP [3] highlights the bias of the reward models toward low detail and low aesthetic images, while HPSv3 [22] addresses this by training the rewards with advanced models and real images, and MPS [36] introduces more fine-grained criteria for training. In contrast, our work focuses on how the reward signal is utilized within the RL process, employing text-conditional preference to align reward attribution with targeted attributes and filter out non-essential biases. This endows our method with robust generalization and provides different rewards, significantly improving the visual quality of the latest FLUX.1.dev model using standard rewards like HPSv2 without requiring advanced or specifically fine-tuned alternatives. 3. Method This work introduces novel Online-RL learning framework for text-to-image generation. Section 3.1 first identifies key limitation in current direct backpropagation approaches and introduces an improved reinforcement learning algorithm that proposes new optimization pipeline to addressing this constraints. Subsequently, we analyze existing reward feedback mechanisms and an online reward adjustment method. Section 3.2 then presents our reward formulation specifically designed for RL optimization. 3.1. Direct-Align Limitations of Existing Approaches. Existing direct backpropagation algorithms optimize diffusion models by maximizing reward functions evaluated on generated samples. Current approaches [5, 24, 25, 33] typically employ twostage process: (1) sampling noise without gradients to obtain an intermediate state xk, followed by (2) differentiable prediction is conducted to produce an image. This enables gradients from the reward signal to be backpropagated through the image generation process. The final objectives of these methods can be categorized into two types: Draft-like: = R(sample(xt, c)) ReFL-like: = R( xt σtϵθ(xt, t, c) αt ) (1) (2) DRaFT [5] performs regular noise sampling throughout the process, including the final few steps and even the last step, as multistep sampling leads to significant computational cost and unstable training when the number of steps exceeds five, as reported in the original work. Similarly, ReFL [33] also opts for later value of before performing one-step prediction to obtain x0, as the one-step prediction tends to lose accuracy at early timestep. Both methods restrict the reinforcement learning process to the later stages of sampling. Single-Step Image Recovery. To address the limitation mentioned above, an accurate single-step prediction is essential. Our key insight is inspired by the forward formula in diffusion models, which suggests that clean image can be reconstructed directly from an intermediate noisy image and Gaussian noise as shown in Eq. (4). Building on this insight, we propose method that begins by injecting groundtruth Gaussian noise prior into an image, placing it at specific timestep to initiate optimization. key advantage of this approach is the existence of closed-form solution, derived from Eq. 4, which can directly recover the clean image from this noisy state. This analytical solution obviates the need for iterative sampling, thus avoiding its common pitfalls, such as gradient explosion, while preserving high accuracy even at early high-noise timesteps (see Fig. 3). xt = αtx0 + σtϵgt x0 = xt σtϵgt αt (3) (4) As shown in Eqs. 2-5, our method employs ground truth vectors to denoise the majority of diffusion chains, thereby mitigating the accumulation of errors introduced by model predictions. This strategy facilitates more accurate reward assignment during the early stages of the process. Figure 3. Comparison on one-step prediction at early timestep The values 0.075 and 0.025 denote the weight of the model prediction term used for method, respectively. The earliest 5% represent state with 95% noise from an unshifted timestep. By constructing Gaussian prior, our one-step sampling method achieves highquality results at early timesteps, even when the input image is highly noised. Our Reward Aggregation Framework. framework  (Fig. 2)  generates clean images x0 and injects noise in single step. For enhanced stability, we perform multiple noise injections to produce sequence of images {xk, . . . , xkn} from the same x0. Subsequently, we apply denoising and recovery processes to each image in the sequence, allowing for the computation of intermediate rewards. These rewards are then aggregated using decaying discount factor through gradient accumulation, which helps mitigate reward hacking at later timesteps. r(xt) = λ(t) (cid:80)kn r(xi ϵθ(xi, i, c), c) (6) 3.2. Semantic-Relative Preference Optimization Semantic Guided Preference. Modern Online-RL for textto-image generation employs reward models to evaluate output quality and guide optimization. These models typically combine an image encoder fimg and text encoder ftxt to compute similarity, following the CLIP architecture [26]: r(x) = RM (x, p) fimg(x)T ftxt(p) (7) In our experiments, we observe that the reward can be interpreted as an image-dependent function parameterized by text embedding denoted as C. Crucially, we find that strategically augmenting the prompts with magic control words denoted as pc can steer the reward characteristics by modifying the semantic embedding, therefore we propose the Semantic Guided Preference (SGP) that shifts reward preference by text condition. = r( xt σtϵθ(xt, t, c) (σt σ)ϵ αt ) (5) rSGP (x) = RM (x, (pc, p)) fimg(x)T C(pc,p) (8) Although this approach enables controlled preference, it still inherits the original reward models biases. To address this limitation, we further propose the Semantic-Relative Preference mechanism. Semantic-Relative Preference. Existing approaches often combine multiple reward models to prevent overfitting to any single preference signal. Although this can balance opposing biases (e.g., using CLIPScores underexposure to offset HPSv2.1s oversaturation tendencies [34]). As shown in Fig. 7, it merely adjusts reward magnitudes rather than aligning optimization directions, resulting in compromised trade-offs rather than true bias mitigation. Based on the insight that reward bias primary originates from the image branch (as the text branch does not backpropagation gradient), we introduce technique to generate pair of opposing reward signals from single image through prompt augmentation, which facilitates the propagation of negative gradients for effective regularization. This approach effectively neutralizes general biases via negative gradients while preserving specific preferences in semantic difference. In our experiments, to balance training efficiency and regularization strength, scaling coefficients can be applied to the positive and negative rewards. Alternatively, the reward formulation can be designed in manner analogous to classifierfree guidance [10]. rSRP (x) = r1 = fimg(x)T (C1 C2) rCF G(x) = fimg(x)T ((1 k) C2 + C1) (9) (10) (11) where C1 represents desired attributes (e.g., realistic) and C2 encodes unwanted features. This formulation explicitly optimizes for target characteristics while penalizing undesirable ones. For implementation, we simply add control phrases to prompts (e.g., <control>. <prompt>) , maintaining the syntactic structure for scoring while ensuring fair comparison with existing methods. Inversion-Based Regularization. Compared to previous methods, which require image reconstruction through model-based prediction and therefore can only optimize along the denoising chain, our proposed Direct-Align approach provides key feature. Specifically, Direct-Align reconstructs the image via predetermined prior constant, thereby decoupling the reconstruction process from the computational graph direction. As result, our method inherently supports optimization in the inversion direction. We simplify the reward formulations for both the denoising (Eq. 12) and inversion (Eq. 13) processes. Consequently, the denoising process performs gradient ascent, thereby aligning the model with the reward-preferred distribution, whereas the inversion process performs gradient descent, which has the opposite effect. r1 = r2 = r2 (cid:18) σtϵθ(xt, t, c) αt (cid:18) + σtϵθ(xt, t, c) αt (cid:19) (cid:19) (12) (13) Empirical analysis indicates that reward hacking predominantly occurs at high-frequency timesteps. By employing the inversion mechanism, we decouple the penalization term and the reward term from Semantic-Relative Preference at different timesteps, thereby enhancing the robustness of the optimization process. 4. Experiments 4.1. Implement Details We evaluate Online-RL algorithms using FLUX.1.dev [13] as our base model, state-of-the-art open-source model, hereafter referred to as FLUX. All methods use HPS [32](short for HPSv2.1) as the reward model and train on the Human Preference Dataset v2 [32], which contains four visual concepts from DiffusionDB [31]. Direct propagation methods are run on 32 NVIDIA H20 GPUs. For DanceGRPO [34], we follow the official FLUX configurations on 16 NVIDIA H20 GPUs. For direct propagation methods, we use 25 sampling steps to maintain gradient accuracy. To avoid oversmoothing and artifacts caused by high CFG scales, we set the CFG scale to 3.5. We use 50 sampling steps during inference to ensure fair comparison with the original FLUX.1.dev. We also compare the latest opensource FLUX.1 release from Krea [15] with our own fine-tuned FLUX.1.dev model. For the Krea version, we use its default configuration (CFG=4.5, 28 sampling steps). 4.2. Evaluation Protocol. Automatic metrics. We assess image quality using established metrics on the HPDv2 benchmark (3,200 prompts). Our evaluation combines four standard measures: Aesthetic Score v2.5 [1, 27], PickScore [12], ImageReward [33], and HPSv2.1 [32], which collectively evaluate aesthetic quality and semantic alignment. Furthermore, we introduce SGP-HPS, which quantifies the difference between score extracted by HPSv2.1 from prompts prefixed with Realistic photo (C1) and CG Render (C2) using HPSv2.1. For comprehensive evaluation, we employ GenEval [9] for semantic alignment and DeQA [35] for degradation. Human Evaluation. We conduct comprehensive human evaluation study comparing generative models using rigorously designed assessment framework. The evaluation involves 10 trained annotators and 3 domain experts to ensure statistical significance and professional validation. Our data set comprises 500 prompts (first 125 prompts from each Method Aes Pick ImageReward HPS SGP-HPS GenEval DeQA Real Aesth Overall Reward Other Metrics Human Eval GPU hours(H20) FLUX 5.867 ReFL 5.903 DRaFT-LV 5.729 DanceGRPO 6.022 6.032 Direct-Align 6.194 SRPO 22.671 22.975 22.932 22.803 23.030 23.040 1.115 1.195 1.178 1.218 1.223 1.118 0.289 0.298 0.296 0.297 0.294 0.289 0.463 0.470 0.458 0.414 0.448 0. 0.678 0.656 0.636 0.585 0.668 0.665 4.292 4.299 4.236 4.353 4.373 4.275 8.2 5.5 8.3 5.3 5.9 38.9 9.8 6.6 9.7 8.5 8.7 40.5 5.3 3.1 4.7 3.7 3.9 29.4 16 24 480 16 5. Table 1. Comparison of Online-RL methods on Reward and Human Evaluation (Excellent Rate) on the HPDv2 Benchmark. * indicates code implement by us. Figure 4. Comparison of human evaluation results for Vanilla FLUX, ReFL, DRaFT LV, DanceGRPO, Direct-Align, and SRPO on the criteria of Realism, Aesthetics, and Overall Preference. SRPO demonstrates significant improvements in Aesthetics and achieves substantial reduction in AIGC artifacts. of the four subcategories in the HPD benchmark). Each prompt was evaluated by five distinct annotators in fully crossed experimental design. The assessment focuses on four critical dimensions of image quality: (1) Text-image alignment (semantic consistency), (2) Realism and artifact presence, (3) Detail complexity and richness, and (4) Aesthetic composition and appeal. Each dimension is rated using four-level ordinal scale: Excellent (fully meets criteria), Good (minor deviations), Pass (moderate issues), and Fail (significant deficiencies). To maintain evaluation reliability, we implement multi-stage quality control process: (1) Experts train and calibrate annotators, (2) Systematic resolution of scoring discrepancies, and (3) Continuous validation of assessment criteria. 4.3. Main Result Automatic Evaluation Results. Our method demonstrates three key advantages when train with HPSv2.1 (Table. 1): (1) immunity to HPSv2.1 score inflation from overfitting, (2) superior performance across multiple reward metrics compared to SOTA methods, and (3) 75 greater training efficiency than DanceGRPO while matching or exceeding all online-RL baselines in image quality. Human Evaluation Results. Our method achieves stateof-the-art (SOTA) performance, as shown in Fig. 4. Methods that directly optimize for reward preferences, including Direct-Align, demonstrate suboptimal performance in terms of realism, even falling short of the baseline FLUX In Fig. 5, we present model due to reward hacking. visual comparison between DanceGRPO and our method. The full set of model visualizations is provided in the Appendix. Although DanceGRPO can improve aesthetic quality and achieve relatively high scores after reinforcement learning, it often introduces undesirable artifacts, e.g., excessive glossiness (row 2, column 1) and pronounced edge highlights (row 2, column 6). To further verify the enhancement in realism, we selected the first 200 prompts from the photo category in the benchmark dataset. We augmented these prompts by prepending realism-related words before the vanilla FLUX input. Fig. 9 (b) shows that the direct generation of our main model significantly outperforms FLUX.1.dev involving lighting and realism-related style words. In contrast, our SRPO substantially improves FLUX across realism, aesthetics, and overall user preference. To the best of our knowledge, this is the first approach to comprehensively enhance realism in large-scale diffusion models, increasing the excellent rate from 8.2% to 38.9% without requiring additional training data. In addition, as shown in Fig. 9 (a), our enhanced FLUX.1.dev through SRPO surpasses the latest open source FLUX.1.krea on the HPDv2 benchmark. 4.4. Comparative Analysis of Reward Models We evaluated our method using three CLIP-based reward models: CLIP ViT-H/14, PickScore, and HPSv2.1, as ilFigure 5. Qualitative Comparison on FLUX, DanceGRPO and SRPO with same seed. Our approach demonstrates superior performance in realism and detail complexity. Figure 6. Cross-reward results of SRPO. lustrated in Fig. 6. Our approach consistently enhances image realism and detail complexity across all models, including CLIP, though improvements with CLIP remain limited due to its lack of human preference alignment. Notably, PickScore demonstrates faster and more stable convergence than HPS, while both yield comparable visual quality. Crucially, no reward hacking is observed in our method, highlighting the effectiveness of Direct-Aligns design ( Fig. 6 (c)) in decoupling optimization from reward-specific biases while preserving alignment with user objectives. Additionally, we validate the generalization of our approach to unimodal rewards (e.g., Aesthetic Score 2.5 [1] ), with further extensions discussed in the Appendix. stages of the denoising process. Optimization timestep. We compare three training intervals using Direct-Align without late-timestep discount and PickScore, as shown in Fig. 7: Early (the first 25% of noise levels), All (the entire training interval), and Late (the last 25% of noise levels). We randomly selected 200 prompts from the HPD test set for human evaluation. Annotators were asked: Do any of these three images show hacking artifacts, such as being too saturated, too smooth, or lacking image details? Mark the worst as hacked. We observe that training exclusively on the late interval leads to significant increase in the hacking rate, likely due to overfitting to PickScores preference for smooth images. When training over the entire interval, the hacking rate remains considerable, as this scheme still includes the late-timestep region. Effectiveness of Direct-Align. The core contribution of Direct-Align is its ability to address the limitations of previous methods that only optimize late timesteps. Direct-Align introduces two key components: early timestep optimization and late-timestep discount. In Fig. 9 (d), we ablate these components in the Direct-Align. As shown in Eq. 2 and Eq. 5, removing early timestep optimization causes the reward structure to resemble ReFL, leading to reduced realism and increased vulnerability to reward hacking, such as oversaturation and visual artifacts. Similarly, removing the λ(t) discount makes the model prone to reward hacking, resulting in oversaturated and unnatural textures. These findings confirm the importance of our approach in overcoming the limitations of late-timestep optimization. Fig. 9 (d) also compares the use of inversion versus the direct construction of the reward as in Eq. 10. Although direct construction yields slightly lower realism and texture complexity than inversion, the results remain competitive. These results highlight the potential of the SRPO reward formulation for future applications in other online RL algorithms that are unable to support inversion or non-differentiable rewards. Fine-Grained Human Preference Optimization. The principal contribution of SRPO is its ability to effectively guide the direction of RL through the manipulation of control words. Through comprehensive experiments involving diverse set of control words, we find that adjectives with higher frequency in the rewards training set, or those that are more readily identified by their underlying vision-language model (VLM) backbone, exhibit significantly stronger controllability. Detailed statistics on highfrequency words in HPDv2 are provided in the Appendix. In Fig. 8, we present simple controls for RL fine-tuning on HPDv2 and HPSv2.1, including brightness adjustment (col 13) and shifting the output distribution to comic or concept art. For rare or unseen styles in the reward training set (e.g., Renaissance), style word must be added at inference for proper generation. Moreover, since our reward is based on image-text similarity, prepending target style prompts Figure 7. Comparison of Optimization Effects of Different timestpe Intervals & Comparison of Reward-System and SRPO on Direct-Align. (1) Hacking Rate: Annotators compare three outputs and select the one that is least detailed or most overprocessed, labeling it as hacking (2) The prompt is young girl riding gray wolf in dark forest. Reward-System can only adjusts scale of rewards, resulting in trade-offs between two rewards effect. In contrast, SRPO penalizes out irrelevant directions from the reward, effectively preventing reward hacking and enhancing image texture. 4.5. Analysis Denoising Efficiency. We compare the final images generated by standard one-step sampling [11] used in previous method [33], which directly utilize model predictions, with those produced by our method at early timesteps. As illustrated in Fig. 3, the standard one-step sampling still exhibit noticeable artifacts throughout significant portion of the denoising process. In contrast, Direct-Align, which primarily relies on ground truth noise for prediction, is able to recover the coarse structure of the image even at the initial 5% of timesteps, and produces results that are nearly indistinguishable from the original image at 25%. Furthermore, we investigate the effect of the proportion of model-predicted steps within the total denoising trajectory (as shown in the two rows from 0.075 to 0.025 in the figure). Our results indicate that shorter proportion of the predicted steps leads to higher final image quality. These findings demonstrate the optimization capability of Direct-Align during the early Figure 8. Visualization of SRPO-controlled results for different style words Figure 9. Overview of experimental results demonstrating the key properties of our SRPO method on the HPDv2 dataset: A: Comparison between FLUX.1.Krea and FLUX.1.dev following the application of our SRPO method. B: Comparison between our main model and vanilla FLUX.1.dev using realism-related recaptioning. C: Illustration of enhanced style control achieved through the incorporation of style-word conditioning. D: Ablation study on the main components of SRPO. to the HPD training set enables RL to increase the presence of desired styles in training images, improving finetuning efficiency. For quantitative evaluation, we conducted user study to compare models before and after training with style words. For user study, we selected the first 200 prompts from the photo category, as these prompts are simple and do not contain explicit style terms. Each prompt was prepended with style word to generate two images for each prompt. Annotators then evaluated each image pair for adherence to the intended style, and in cases of equal style fidelity, overall aesthetics were used as tiebreaker. As illustrated in Fig. 9 (c), our approach enables more effective style control and improves the performance of FLUX on certain styles. However, the degree of improvement depends on the reward models ability to recognize specific style terms; For the Cyberpunk style, although SRPO enhances realism and atmosphere as shown in Fig. 8 (col 8), its relative infrequency in the training data makes it difficult for the reward model to recognize this style, resulting in grid-like artifacts. Consequently,the overall improvement in human evaluation is limited, with most scores comparable to those of the original FLUX. Offline SRPO. In our experiments, we observed that Direct-Align exhibits properties similar to Supervised FineTuning (SFT), demonstrating an ability to fit images from online rollouts. Building on this finding, we replaced the online rollout with offline real-world photographs, which led to another significant improvement in the realism of the FLUX model. To distinguish our method from pure SFT, we test it with raw CLIP reward and human preference aligned reward like PickScore and HPSv2.1. This comparison underscores that our approach is comprehensive reinforcement learning method that integrates both data fitting and human preference signals. Supporting visualizations can be found in the final section of the Appendix. 5. Conclusion In this work, we propose novel reinforcement learning (RL) framework for aligning text-to-image (T2I) models with fine-grained human preferences, enabling fine-grained preference adjustment without the need for fine-tuning reward. Our approach addresses two primary limitations of existing methods. First, we overcome the sampling bottleneck, allowing the RL algorithm to be applied beyond the late-stage generation of clean images. Second, we revisit the design of reward signals to enable more flexible and effective preference modulation. Through comprehensive experimental evaluations, we demonstrate that our method outperforms state-of-the-art (SOTA) approaches in terms of both image realism and alignment with human aesthetic preferences. Compared to DanceGRPO, our framework achieves over 75 improvement in training efficiency. Furthermore, to the best of our knowledge, this is the first work to systematically enhance the realism of large-scale diffusion models. Limitations & Future Work. This work has two main limitations. First, in terms of controllability, our control mechanism and certain control tokens are somewhat outside the domain of the existing reward model, which may result in reduced effectiveness. Second, in terms of interpretability, since our method relies on similarity in the latent space for reinforcement learning, the effects of some control texts may not align with the intended RL direction after being mapped by the encoder. In future work, our aim is to (1) develop more systematic control strategy or incorporate learnable tokens and (2) fine-tune vision language model (VLM) reward that is explicitly responsive to both control words and the prompt system. Additionally, the SRPO framework can be extended to other online reinforcement learning algorithms. We anticipate that these improvements will further enhance the controllability and generalization capabilities of SRPO in practical applications."
        },
        {
            "title": "References",
            "content": "[1] Aesthetic predictor v2.5. https : / / github . com / discus0434/aesthetic-predictor-v2-5, 2025. Accessed: 2025-06-10. 2, 5, 7 [2] Michael Albergo, Nicholas Boffi, and Eric VandenEijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. 3 [3] Ying Ba, Tianyu Zhang, Yalong Bai, Wenyi Mo, Tao Liang, Bing Su, and Ji-Rong Wen. Enhancing reward models for high-quality image generation: Beyond text-image alignment. arXiv preprint arXiv:2507.19002, 2025. 2, 3 [4] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 2 [5] Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. 2, 3, 4 [6] Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, and Ricky TQ Chen. Adjoint matching: Fine-tuning flow and diffusion generative models with memoryless stochastic optimal control. arXiv preprint arXiv:2409.08861, 2024. 2, [7] Ying Fan and Kangwook Lee. Optimizing ddpm sampling with shortcut fine-tuning. arXiv preprint arXiv:2301.13362, 2023. 2 [8] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023. 2 [9] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. 2, 5 [10] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5 [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 3, 8 [12] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36: 3665236663, 2023. 2, 3, 5 [13] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, 5 [14] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning textarXiv preprint to-image models using human feedback. arXiv:2302.12192, 2023. 2 [15] Sangwu Lee, Titus Ebbecke, Erwann Millon, Will Beddow, Le Zhuo, Iker Garcıa-Ferrero, Liam Esparraguera, Mihai Petrescu, Gian Saß, Gabriel Menezes, and Victor Perez. Flux.1 krea [dev]. https://github.com/krea-ai/fluxkrea, 2025. 2, 5 [16] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. 3 [17] Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Mingxi Cheng, Ji Li, and Liang Zheng. Aesthetic post-training diffusion models from generic prefer- [32] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better aligning textIn Proceedings to-image models with human preference. of the IEEE/CVF International Conference on Computer Vision, pages 20962105, 2023. 2, 3, [33] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. 2, 3, 4, 5, 8 [34] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. 2, 5 [35] Zhiyuan You, Xin Cai, Jinjin Gu, Tianfan Xue, and Chao Dong. Teaching large language models to regress accurate image quality scores using score distribution. arXiv preprint arXiv:2501.11561, 2025. 5 [36] Sixian Zhang, Bohan Wang, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Learning multidimensional human preference for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80188027, 2024. 3 [37] Tao Zhang, Cheng Da, Kun Ding, Huan Yang, Kun Jin, Yan Li, Tingting Gao, Di Zhang, Shiming Xiang, and Chunhong Pan. Diffusion model as noise-aware latent reward model for step-level preference optimization. arXiv preprint arXiv:2502.01051, 2025. 3 ences with step-by-step preference optimization. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1319913208, 2025. 3 [18] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [19] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. [20] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3 [21] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. 3 [22] Yuhang Ma, Xiaoshi Wu, Keqiang Sun, and Hongsheng Li. Hpsv3: Towards wide-spectrum human preference score. arXiv preprint arXiv:2508.03789, 2025. 3 [23] Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. arXiv preprint arXiv:2201.03544, 2022. 2 [24] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-to-image diffusion models with reward backpropagation. 2023. 2, 3, 4 [25] Mihir Prabhudesai, Russell Mendonca, Zheyang Qin, Katerina Fragkiadaki, and Deepak Pathak. Video diffusion alignment via reward gradients. arXiv preprint arXiv:2407.08737, 2024. 2, 3, [26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 4 [27] Christoph Schuhmann. https : / / laion.ai/blog/laion-aesthetics/, 2022. Accessed: 2023-11-10. 3, 5 Laion-aesthetics. [28] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2, and Stefano Ermon. arXiv preprint [29] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 3 [30] Yibin Wang, Zhimin Li, Yuhang Zang, Yujie Zhou, Jiazi Bu, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Pref-grpo: Pairwise preference reward-based grpo for staarXiv preprint ble text-to-image reinforcement learning. arXiv:2508.20751, 2025. 2 [31] Zijie Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. Diffusiondb: large-scale prompt gallery dataset for text-toimage generative models. arXiv preprint arXiv:2210.14896, 2022. 5 Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference"
        },
        {
            "title": "Supplementary Material",
            "content": "S1. Extension to Aesthetic Models While SRPO primarily operates on the text branch and thus cannot explicitly control purely image-based aesthetic models, relative reward can still be achieved through data processing techniques. Specifically, we introduce small amounts of noise to the images generated by the model and compute the aesthetic reward for both the noised and the original clean images. This setup naturally forms positive and negative optimization gradients. Although the reward scores for noisy images may not be accurate, they serve to penalize the overall bias in the reward model. Our experiments demonstrate that this approach remains effective in mitigating reward hacking phenomena as show in Fig. S1 Figure S2. Comparison on GRPO and SRPO. Figure S1. Extension to the Aesthetic Model. The first row is trained with Direct-Align using the original Aesthetic Predictor 2.5, while the second row is trained using SRPO with Aesthetic Predictor 2.5. S2. Comparison to GRPO Our approach is inspired by the group relativity mechanism in GRPO. Similar to GRPO, our method first samples clean images without gradients and then injects noise back into the corresponding intermediate to train. However, our method offers several key advantages over GRPO. First, we apply direct propagation on the reward signal, in contrast to the policy optimization used in GRPO; this leads to significantly improved convergence speed. For example, during FLUX training, we observe that methods based on direct propagation yield noticeable image changes within 30 steps, whereas GRPO requires over 100 steps for comparable results. Second, our approach computes semantic-relative advantages, requiring only single sample for each update and relying solely on the original ODE. This eliminates the reliance on the diversity of the generative model or sampler. Third, unlike GRPO, which often necessitates additional KL regularization and reference model to prevent over-optimization, our method directly constrains the optimization by propagating the negative reward signal, thus obviating the need for auxiliary constraints. Figure S3. High-frequency Word Statistics (part) in HPDv2 Training Set. S3. High-frequency Word Statistics in HPDv"
        },
        {
            "title": "Training Set",
            "content": "We found that the effectiveness of our method depends on the reward models ability to perceive control words. the word frequency statistics Here, we briefly present in the HPDv2.1 training set. As discussed in Section Sec. 4.5, painting is the most frequent word and achieves the best experimental results, while the less frequent word Cyberpunk yields weaker enhancement effects. Furthermore, we observed that low-frequency words can benefit from being combined with high-frequency words. For example, the Comic column in our experiment uses combination of anime, comic, and digital painting. Similarly, Renaissance is constructed by combining Renaissance-style and oil painting. S4. Visualization Comparsion Figure S4. Qualitative Comparison on several methods with same seed. Our approach demonstrates superior performance in realism and detail complexity. Criterion Realism&AI artifacts Description Evaluates whether the image looks real and free of AI artifacts compared to other images Subject Clarity and Detail Complexity: Whether the main subject of the image is clear and detailed. textures compared to other images. Image-Text Alignment Measures Image-Text Alignment by grading Aesthetic Quality No need to reference the prompt; evaluate the aesthetic appeal of each image based on composition, lighting, color, etc. Overall Quality Comprehensively evaluate the overall preference for the image. Key Points Whether deformation artifacts appear in the image Whether the text is correct (if the image contains text ) Oily surface or over-saturated colors on objects Abnormal highlights on object edges or unnatural transition to background Whether the objects texture is overly simple or even Scoring: For each key point issue compared to other images, downgrade the rating by one level (e.g., from Excellent to Good, or from Good to Pass). If an obvious issue is present, mark as Failed. Whether there is obvious blurriness in the image. Whether the main subject of the image is intuitively presented (i.e., not blurry). Whether there are any watermarks or garbled text in the image that affect its presentation. Whether the texture of the image is complex, for example, whether the texture of leaves is distinguishable. Whether the lighting and shadows in the image are prominent, and whether the light source is identifiable. Scoring: For each key point issue compared to other images, downgrade the rating by one level (e.g., from Excellent to Good, or from Good to Pass). If an obvious issue is present, mark as Failed. Excellent: Over 90% of the elements match the prompt, and the style is fully consistent. If there is text, it should be fully generated and naturally embedded in the image. Good: 70%90% of the elements match the prompt. Minor errors in the text are allowed. Pass: 50%70% of the elements match the prompt. Most key elements are present, or the image generally looks similar to the prompt at first glance. Failed: Many key elements are missing, or the style does not match the prompt. Excellent: The image has strong atmosphere and is highly visually appealing. Only images that make you want to save them as wallpapers or share them with others qualify for this rating. Good: asstands out pectcomposition, lighting, or colormaking it comfortable to view or eye-catching. least one image in at The Pass: The image has no obvious flaws, but its aesthetic appeal is average. Failed: The image is unattractive or even unpleasant to look at. Excellent: All dimensions are rated as Excellent. Good: At least half of the dimensions are rated as Excellent. Pass: No dimension is rated as Failed Failed: Any dimension is rated as Failed. Figure S5. Qualitative Comparison on offline-SRPO and online-SRPO"
        }
    ],
    "affiliations": [
        "Hunyuan, Tencent",
        "School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen",
        "Shenzhen International Graduate School, Tsinghua University"
    ]
}