{
    "paper_title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams",
    "authors": [
        "Shuai Yuan",
        "Yantai Yang",
        "Xiaotian Yang",
        "Xupeng Zhang",
        "Zhonghao Zhao",
        "Lingming Zhang",
        "Zhipeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT"
        },
        {
            "title": "Start",
            "content": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams Shuai Yuan1 Yantai Yang1,2 Xiaotian Yang1 Xupeng Zhang1 Lingming Zhang Zhipeng Zhang1(cid:66) Zhonghao Zhao1 1AutoLab, School of Artificial Intelligence, Shanghai Jiao Tong University"
        },
        {
            "title": "2 Anyverse Dynamics",
            "content": "6 2 0 2 5 ] . [ 1 1 8 2 2 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinitehorizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, causal visual geometry transformer that operationalizes the concept of rolling memory through bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise training-free, attentionagnostic pruning strategy that intelligently discards obsolete information, effectively rolling the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such system is its performance over truly infinite horizon, capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT 1. Introduction The dense reconstruction of 3D scenes from 2D images constitutes cornerstone problem in geometric vision, serv- (cid:66) Corresponding Author. Figure 1. Paradigm Comparison between previous online and offline 3D geometry understanding and our InfiniteVGGT. ing as the bedrock for critical applications such as augmented reality (AR) [15, 18, 44] and embodied AI [3, 16, 20, 23, 41, 43, 46]. Historically, the domain has been dominated by classical methods rooted in Structure-fromMotion (SfM) [1, 9, 22, 28, 32, 39] and Multi-View Stereo (MVS) [10, 13]. While capable of high-fidelity geometric optimization, these approaches are characterized by fragmented, multi-stage pipelines that are prohibitively slow, and prone to cascading errors. paradigm shift has been catalyzed by the advent of end-to-end deep learning frameworks, which transcend these limitations by holistically inferring 3D structure from raw image data. Models such as DUSt3R [36], VGGT [34], and their derivatives [19, 42] have reshaped the landscape, championing fully data-driven methodologies that achieve globally consistent reconstructions with unprecedented efficiency. As these end-to-end models mature, the contemporary landscape has become defined by fundamental dichotomy between offline batch processing [7, 30, 34] and online streaming paradigms [4, 17, 35, 45]. As illustrated in Fig. 1(a), offline methods masterfully exploit multi-view geometric constraints to achieve superior geometric fidelity, rendering them ideal for short-term reconstructions where data is fully pre-captured. This batch-centric paradigm, however, is fundamentally ill-suited for online applications or unbounded sequences due to its prohibitive GPU memory footprint [30]. Conversely, streaming architectures are conceptually tailored for online scenarios, such as robotics, by processing inputs sequentially to provide immediate perceptual feedback. Their theoretical appeal lies in handling infinite-length scene flows. Yet, this promise is largely unrealized in practice. One paradigm, namely explicit history accumulation frameworks like StreamVGGT [45], betrays its online intent by accumulating unbounded KeyValue (KV) stores (Fig. 1(c)), path that inevitably leads to crippling memory and computational overheads. The other one, namely implicit state compression mechanisms, such as those in CUT3R [35] and TTT3R [4] (Fig. 1(b)), make Faustian bargain, where they compress history into simple RNN hidden state to guarantee bounded resources, but in doing so, discard critical information, thereby exacerbating long-term drift and compromising robustness. Then, question naturally arises that is it possible to selectively retain critical historical information to ensure temporal consistency, while still operating within the bounded resources required for truly online system? The key to escaping this dilemma lies not in more complex model, but in pivotal insight into the nature of the data itself. We observe that in contiguous camera trajectories, minimal viewpoint shifts create massive token-level redundancy within the KV cache. This is not trivial matter, as each frame adds approximately 1,000 tokens, the cache rapidly explodes to scale (O(105) tokens within 100 frames) that necessitates hardware-optimized kernels like FlashAttention just to remain functional. Herein lies fundamental paradox that these kernels achieve their speed by circumventing the materialization of the full O(N 2) attention matrix, yet traditional pruning methods rely on accessing these very weights to gauge token importance. Consequently, the tool required to manage the size of the cache prevents us from intelligently shrinking it. To resolve this impasse, we introduce an elegant solution by leveraging key cosine similarity as an efficient, attention-independent proxy for token importance. This allows us to identify and discard redundant tokens before the costly attention computation, thereby preserving the efficiency of optimized kernels while surgically shrinking the cache and finally paving the way for truly scalable streaming reconstruction. Building upon this, we introduce InfiniteVGGT, which embodies novel rolling memory paradigm for online 3D It avoids the unbounded memgeometry understanding. ory growth inherent in explicit history accumulation frameworks while simultaneously mitigating the information drift that plagues implicit state compression methods. Our rolling memory achieves this by continuously and dynamically refreshing its contents through deeply integrated, multi-level retention strategy. At its foundation, the strategy abandons intuitive and coarse frame-wise deletion, selectively preserving individual tokens to maintain crucial longterm context. This granular process is then governed by dynamic budget that is intelligently structured across the models architecture. It functions layer-wise by assigning unique token budget to each layer, resulting in layer-specific and specialized KV caches. This systematic control system operates without materializing attention weights, ensuring full compatibility with FlashAttention, and ultimately enables system with strictly bounded GPU memory footprint capable of processing infinite sequences. The ultimate test for such system is its performance over truly infinite horizon, capability that has been impossible to rigorously validate due to the lack of continuous, long-term benchmarks. To address this gap, we introduce the Long3D benchmark, which, for the first time, enables rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D scene understanding and reconstruction. Our contributions are threefold: An unbounded memory architecture InfiniteVGGT for continuous 3D geometry understanding, built on novel, dynamic, and interpretable explicit memory system. State-of-the-art performance on long-sequence benchmarks and unique capability for robust, infinite-horizon reconstruction without memory overflow. The Long3D benchmark, new dataset for the rigorous evaluation of long-term performance, addressing critical gap in the field. 2. Related Work Classical Offline and Online Reconstruction. Traditional 3D vision methods fall into two primary paradigms distinguished by their operational constraints, namely offline batch processing and online streaming. The cornerstone of offline reconstruction is Structure-from-Motion (SfM). SfM pipelines [1, 9, 22, 28, 32, 39], epitomized by COLMAP [28], perform global Bundle Adjustment (BA) [14] across all views and points to achieve maximum global accuracy. While computationally demanding, this batch optimization produces highly precise results that subsequently serve as foundation for Multi-View Stereo (MVS) [11, 12, 29, 37] algorithms to generate dense modIn stark contrast, online streaming methods, promiels. nently represented by Simultaneous Localization and Mapping (SLAM), prioritize online performance. These systems incrementally estimate the camera trajectory, employing range of techniques that include feature-based [24], direct [8], and dense [25] approaches. streaming framework capable of infinite-length 3D geometry reconstruction by introducing hierarchical, dynamic rolling memory that preserves long-term dependencies to reduce both drift and catastrophic forgetting. Learning-based Offline 3D Reconstruction. Recent advances in offline 3D reconstruction have seen classical multi-stage pipelines give way to unified, feedforward architectures. Early works like DUSt3R [36] and MASt3R [19] formulate reconstruction as pairwise pointmap regression problem, imposing computationally expensive global alignment stage to aggregate multi-view information. VGGT [34] addresses this by introducing large transformer that jointly predicts camera poses, depth, and feature tracks in single forward pass. More recently, π3 [38] refines VGGT to operate independently of fixed reference frame. However, the input length of such large models remains bottleneck. To extend scalability, VGGTLong [7] decomposes long trajectories into sub-maps at the cost of single-pass simplicity. In different approach, SailRecon [6] enhances scene regression using subset of anchor images to create global neural representation for efficient localization of all other images. Focusing instead on computational efficiency, FastVGGT [30] accelerates the forward process through training-free token merge mechanism. This method exploits attention redundancy to preserve key geometric cues, achieving 4 speedup on 1000frame sequences while reducing drift. Learning-based Online 3D Reconstruction. The early transformer-based methods, such as Spann3R [33] and Point3R [40], pioneered online forward-pass reconstruction using explicit spatial or pointer memory. This paradigm was refined by StreamVGGT [45] and Stream3R [17], which apply causal attention and KV cache to process sequences on-the-fly. However, the reliance on an evergrowing KV cache leads to prohibitive increases in memory and computation, rendering these models impractical for truly long streaming inputs. To circumvent this scaling issue, WinT3R [21] employs sliding-window mechanism to balance reconstruction quality with latency. This design inherently limits the temporal receptive field and can cause drift. Although WinT3R attempts to mitigate this with global camera-token pool, it still falls short of supporting infinite-length reconstruction. Seeking to overcome these limitations, another line of research adopts RNNbased architectures. CUT3R [35], for example, uses continuously updated states to accommodate arbitrary-length image streams. Building on this foundation, TTT3R [4] introduces test-time training rules to improve length generalization, enabling the online processing of thousands of frames. Nevertheless, catastrophic forgetting caused by transitionally compressed memory remains fundamental challenge, limiting the ability of capturing long-range temporal dependencies. To mitigate these limitations, we propose an online 3. Method 3.1. Preliminaries From Offline to Online 3D Reconstruction. The offline model VGGT [34] processes batch of images {Ii RHW 3}N i=1 in single forward pass. It alternately applies frame (Fθ) and global (Gθ) interaction across 24 selfattention layers to jointly estimate set of 3D quantities, (gi, Di, Pi, Ti)N i=1 = ϕ(Fθ (cid:0){Ii}N i=1 (cid:1) , Gθ (cid:0){Ii}N i=1 (cid:1)), (1) where gi R9 represents the camera parameters, Di RHW is the depth map, Pi RHW 3 is the point map, and Ti RHW are point-tracking features. To adapt this architecture for online and streaming usage, models like StreamVGGT [45] substitute the global interaction Gθ with causal temporal attention module Tθ. This allows the model to process frames incrementally. At any given timestep t, the module generates the output for the current frame It by leveraging KV cache, Ct1, that stores the context from all previous frames, (gt, Dt, Pt, Tt) = ϕ(Fθ (It) , Tθ(It, Ct1)) (2) (cid:110) (cid:111)NL (K(l) , (l) ) , where NL is the The KV cache Ct = total number of causal attention layers, is contiguously updated by combining the new keys and values. The cache size grows linearly (O(t)) with the sequence length t. l=1 3.2. Motivation and Analysis As previously discussed, the KV cache, which functions as explicit memory, grows linearly with each new frame, leading to unsustainable memory demands over time. The central challenge is thus to maintain fixed-size rolling memory. This requires an intelligent eviction strategy that preserves valuable information while discarding redundancy. Are Attention Scores Feasible Eviction Criterion? An intuitive approach is to use attention scores as proxy for token importance. This idea is initially compelling because sequential input frames in 3D reconstruction possess high spatio-temporal redundancy due to significant viewpoint overlap. We empirically validate this by extracting the patch-embedded tokens from the backbone of StreamVGGT [45] for adjacent frames, finding their cosine similarity consistently exceed 0.95. This high similarity stems from the DINO [26] backbone being trained as semantic encoder with high invariance to slight changes in viewpoint. It prioritizes what is seen over from where Figure 2. Visualization Results. (a) Attention maps from the current frame to adjacent historical cached frames, demonstrating nearidentical distributions due to minimal viewpoint shifts in online streaming camera motion. (b) PCA embeddings of query (Q) and key (K) vectors for representative layers and heads, revealing clustering and redundancy in the feature space. it is seen, which further confirms the extensive redundancy present in the tokens fed into the subsequent aggregator module. As result, the query frame It often assigns nearidentical attention weights to historical frames that share similar perspective (Fig. 2 (a)). This observation suggests that an attention-based eviction strategy could effectively prune the cache. However, this approach introduces critical computational dilemma. The KV cache in these architectures must scale to hundreds of thousands or even millions of tokens (Ct 105). To manage this scale online, the causal attention mechanism (Tθ) fundamentally depends on hardware-optimized kernels, such as FlashAttention, which mitigate memory bandwidth bottlenecks by never explicitly materializing the full attention matrix. This reliance creates an irreconcilable conflict that any token filtering strategy predicated on attention scores requires the materialization of the full attention weight matrix, which is the very operation that optimized kernels are designed to bypass. Executing this operation would be computationally prohibitive and would negate the low-latency inference essential for streaming systems. We therefore argue that this paradigm is suboptimal and motivate the need for an alternative approach to construct an efficient rolling memory. Key Diversity as Redundancy Proxy. Instead of estimating token salience through attention weights, we measure redundancy in the key space. As illustrated in Fig. 2(b), PCA visualizations of the key and query spaces reveal that queries (qt) from the current frame and cached key vectors (kt1) consistently occupy distinct, nearly orthogonal subspaces across layers. This geometric separation persists over time, confirming that key-space similarity provides stable measure of redundancy. Therefore, distinct keys will be more aligned with the query, and in turn, can be preserved as more salient keys. Building on this, we define the negative cosine similarity as diversity score to quantify this dispersion, hypothesizing that keys are the principal components and provide the most effective mechanism for quantifying redundancy. This metric efficiently captures the dispersion of key representations in feature space and is independent of the current query. Tokens with higher diversity scores correspond to those most dissimilar from the global mean, and are thus retained during cache compression. As result, the cache preserves the most informative subset of tokens while maintaining minimal memory footprint. 3.3. Diversity-aware Rolling Memory Immutable Anchor Token. As illurstrated in Fig. 3, our rolling memory pipeline commences by establishing an immutable set of anchor tokens, defined as the complete KV cache derived from the initial input frame. This design choice is motivated by the architectural foundation of VGGT [34], wherein all subsequent 3D predictions are rigidly aligned to the coordinate system of the first frame, which serves as the canonical global reference. Any alteration or pruning of these initial tokens would irreversibly compromise geometric consistency across the entire reconstruction. Accordingly, we designate the first-frame cache as the immutable anchor set C(l,h) and exclude it from all subsequent compression operations. For any given layer and head h, the total cache C(l,h) is thus partitioned into the anchor set and mutable candidate set, C(l,h) t,cand, which contains all tokens from = 2 onwards. anc Diversity-quantified Token Retention. Then, we apply our retention strategy π exclusively to the candidate set C(l,h) t,cand to retain the most informative tokens. This process is performed independently for each layer and head to account for their heterogeneous redundancy profiles. Our strategy begins by establishing reference vector for each heads key space. This is achieved by computing the mean key µ(l,h). To ensure this metric captures directional variance exclusively, we operate on L2-normalized keys, where ˆki = ki/ki. The mean key is thus the expectation over the set of normalized candidate keys ˆK(l,h) t,cand, µ(l,h) = ˆk ˆK(l,h) t,cand [ˆk] (3) Figure 3. Overview of the InfiniteVGGT, illustrating rolling memory paradigm that prunes KV cache contents to prevent VRAM accumulation over time, employing key cosine similarity and adaptive layer-wise allocation for 3D geometry understanding. Next, we define diversity score sdiv for each individual key ˆki to quantify its dissimilarity from this mean vector. Based on our previous analysis, we employ the negative cosine similarity as our metric, s(l,h) div ( ˆki) = CosSim(µ(l,h), ˆki) (4) This formulation ensures that keys with the lowest cosine similarity to the mean, which represent the most geometrically distinct features, are assigned the highest scores. Consequently, high sdiv score signifies high informational salience, guiding the retention of the most valuable tokens. 3.4. Layer-wise Adaptive Budget Allocation To optimize the KV cache, we introduce an adaptive, layer-wise budget allocation mechanism that assigns nonuniform storage budget to each layer in proportion to its measured information diversity. This strategy is motivated by the observation that informational diversity is unevenly distributed across the model. Our analysis reveals that shallow layers, which amplify subtle inter-frame differences In contrast, for spatial reasoning, exhibit high diversity. both the initial layer, processing low-level statistics like color and brightness, and the deep layers, where representations converge towards holistic semantic understanding, demonstrate significantly less diversity. To implement this principle, we first define layer-wise average diversity div as the mean of all token diversity s(l,h) score sl div within that layer. The budget proportion pl bud for each layer is then calculated via softmax normalization of these scores, pl bud = div/τ ) exp(sl j=1 exp(sj div/τ ) (cid:80)L (5) Figure 4. Long3D Examples. Views and global point clouds of different scenes. where τ is temperature hyperparameter. The total budget for layer is Bl = pl bud Btotal. This budget B(l,h) is then enforced via TopK selection. The final compressed cache Ct is the union of all retained candidate tokens C(l,h) t,cand and the immutable anchor set Canc. 4. Long3D Benchmark To address the critical lack of benchmarks for evaluating continuous, long-term 3D geometry estimation, we propose Long3D. Prior to this work, rigorously assessing systems performance over extended, uninterrupted periods was infeasible, as existing benchmarks are either restricted to short sequences ( 1000 frames) or, like the 7Scenes [31], are merely collections of discontinuous clips, which prevents proper assessment of long-term, uninterrupted performance. Long3D fills this critical void by providing the first framework for evaluating model robustness on truly continuous video streams. In total, our dataset features 5 challenging sequences captured in diverse indoor and outdoor environments, with each individual sequence ranging from about 2,000 to 10,000 frames. Fig. 4 shows Table 1. 3D Reconstruction Results on 7-Scenes [31] and NRGBD [2]."
        },
        {
            "title": "Input",
            "content": "Acc. 7-Scenes Comp. NC Acc."
        },
        {
            "title": "NRGBD",
            "content": "Comp. NC Mean Med. Mean Med. Mean Med. Mean Med. Mean Med. Mean Med. VGGT (Offline) [34] StreamVGGT [45] CUT3R [35] Point3R [40] TTT3R [4] InfiniteVGGT VGGT (Offline) [34] StreamVGGT [45] CUT3R [35] Point3R [40] TTT3R [4] InfiniteVGGT VGGT (Offline) [34] StreamVGGT [45] CUT3R [35] Point3R [40] TTT3R [4] InfiniteVGGT 300 400 500 OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM 0.624 0.135 0.695 0.047 0.673 0.041 0.756 0.040 0.091 0.027 0.025 0.015 0.224 0.076 0.103 0. 0.543 0.563 0.565 0.570 0.012 0.005 0.005 0.005 0.126 0.043 0.045 0.032 0.562 0.596 0.599 0.607 0.032 0.011 0.005 0.005 0.579 0.618 0.608 0. 0.074 0.014 0.025 0.022 0.071 0.029 0.024 0.025 OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM 0.572 0.162 0.685 0.049 0.657 0.052 0.763 0.043 0.114 0.023 0.031 0.016 0.315 0.093 0.140 0.069 0.532 0.559 0.558 0. 0.093 0.026 0.027 0.026 0.032 0.005 0.014 0.005 0.215 0.045 0.070 0.040 0.546 0.589 0.587 0.599 0.050 0.009 0.005 0.005 0.551 0.613 0.599 0. 0.101 0.024 0.058 0.034 OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM 0.582 0.183 0.684 0.063 0.648 0.062 0.746 0.043 0.130 0.026 0.036 0.018 0.326 0.113 0.165 0.080 0.530 0.555 0.552 0.561 0.091 0.031 0.029 0. 0.543 0.583 0.577 0.593 0.033 0.015 0.005 0.005 0.556 0.613 0.594 0.643 0.132 0.037 0.095 0.037 0.042 0.005 0.015 0.008 0.243 0.048 0.084 0. an example of our benchmark. This data was collected using handheld 3D spatial scanner equipped with an IMU, 3D LiDAR (360 horizontal by 59 vertical FOV), and an RGB camera (800 600 at 10 Hz, 90 FOV). For each scene, the data consists of global ground-truth point cloud and the corresponding uninterrupted sequence of RGB images. On our benchmark, we evaluate dense-view streaming reconstruction, where models process the entire image stream to generate complete global point cloud. For evaluation, predicted and ground-truth point clouds are aligned using the Iterative Closest Point (ICP) algorithm, consistent with prior methods [35, 45]. Performance is quantified using three established metrics, including Accuracy (Acc.), Completion (Comp.), Chamfer Distance(CD) and Normal Consistency (NC). 5. Experiments 5.1. Experiments Setup We conduct comprehensive evaluation of our proposed method across three demanding tasks of 3D reconstruction, video depth estimation, and camera pose estimation. Initially, we leverage the longest contiguous sequences from established public datasets, benchmarking InfiniteVGGT leading long-term streaming baselines, namely against CUT3R [35] and TTT3R [4]. Our method is training-free optimization designed to overcome the memory bottlenecks inherent in long-sequence reconstruction. We implement and evaluate this approach on the StreamVGGT [45], concentrating our analysis on long-sequence scenarios where the benefits of our memory-efficient design are most pronounced. On shorter sequences, where the baselines GPU memory is not exceeded, performance differences are negligible (see Sec. 6). Subsequently, we introduce and evaluate on our novel large-scale Long3D benchmark to probe stability across extensively prolonged inputs. All experiments were executed on single NVIDIA A100 GPU. 5.2. 3D Reconstruction Evaluation on 7-Scenes and NRGBD Datasets. Following the previous work [35], we evaluate scene-level 3d reconstruction on 7-Scenes [31] and NRGBD [2] datasets. But unlike the evaluation of extremely sparse-view reconstruction protocol before, for the long-term streaming, we sampled images with stride = 2 in each sequence and use the first 300 to 500 images as input like TTT3R [4]. As shown in Tab. 1, the offline method VGGT [34] and the online method StreamVGGT [45] fail on long sequences input as the memory constraints. As for the other runnable online method, while TTT3R maintains robust performance on the 7-Scenes dataset, its reconstruction capability on NRGBD degrades significantly as the number of input frames increases. Despite affording greater robustness on varied datasets, Point3Rs explicit pointer mechanism [40] suffers from perpetually increasing memory usage, rendering it incompatible with long-sequence reconstruction (evidenced in [4]). Our method InfiniteVGGT exhibits minimal temporal error accumulation as the sequence length increases, allowing it to consistently maintain the state-of-art reconstruction accuracy. Concurrently, its strong performance across varied datasets highlights its high robustness. Evaluation on Long3D Benchmark. As Sec. 4 states, Table 2. 3D Reconstruction Results on Long3D. 5.3. Video Depth Estimation Figure 5. Qualitative Results of 3D Reconstruction. Method Scene/Input CUT3R [35] TTT3R [4] InfiniteVGGT CUT3R [35] TTT3R [4] InfiniteVGGT CUT3R [35] TTT3R [4] InfiniteVGGT CUT3R [35] TTT3R [4] InfiniteVGGT CUT3R [35] TTT3R [4] InfiniteVGGT Classroom Dormitory 4208 Library 4726 Badminton Court 6067 Academic Building 9545 Acc. Comp. NC Mean Med. Mean Med. Mean Med. 0.496 0.396 0.357 1.800 1.965 1.438 1.907 2.175 1.121 2.489 2.791 1. 8.062 7.710 5.733 0.374 0.319 0.298 1.372 1.749 1.159 1.437 1.484 0.821 2.432 2.392 1.555 5.650 5.793 4. 0.085 0.081 0.057 0.404 0.329 0.575 0.193 0.430 0.571 5.802 3.160 1.854 0.673 6.192 1.206 0.036 0.035 0. 0.090 0.100 0.089 0.079 0.095 0.077 5.071 2.673 0.816 0.251 5.159 0.251 0.520 0.530 0.576 0.501 0.515 0. 0.504 0.494 0.508 0.495 0.509 0.510 0.496 0.513 0.495 0.525 0.540 0.612 0.495 0.509 0.538 0.507 0.481 0. 0.483 0.502 0.509 0.491 0.519 0.490 CD 0.291 0.239 0.207 1.102 1.147 1.007 1.050 1.303 0. 4.146 2.975 1.848 4.638 6.951 3.470 we evaluated our method alongside other models capable of processing extended-length inputs on sequences of approximately 2,000, 4,500, 6000 and nearly 10,000 frames on Long3D dataset. The results demonstrate that our approach achieves robust performance across diverse scenes and varying sequence lengths, outperforming existing models like CUT3R [35] and TTT3R [4] on most metrics. More importantly, although temporal drift inevitably accumulates with increasing input frames, our method effectively limits this error propagation compared to baselines. However, we observed that our method underperforms on the mean of Comp. metric compared to these baselines. We identify this as key area for optimization in our future work. Evaluation on Bonn Datasets. Video depth estimation evaluates per-frame depth quality and inter-frame depth consistency. Since most existing datasets only contain limited number of continuous frames, to show the long-term performance, we evaluate InfiniteVGGT on the longest available continuous sequences from Bonn [27]. Specifically, we select continuous sequences ranging from 200 to 500 frames, beginning after the initial 30 frames like TTT3R. As shown in Tab. 3, the performance of InfiniteVGGT is benchmarked against CUT3R [35] and TTT3R [4], showing the effectiveness of our method. 5.4. Ablation Study Crucial Token Selection Policy. We conduct comparative analysis of attention weight-based and cosine similaritybased token selection policies on the 7-Scenes dataset [31]. In addition to evaluating reconstruction quality via chamfer distance (CD), and normal consistency (NC) metrics, chamfer distance is computed as the average of accuracy and completeness. We also profile the per-frame inference time and peak GPU memory consumption. As summarized in Tab. 4, the cosine similarity-based approach yields more accurate point cloud reconstruction. Moreover, standard attention weight-based methods can introduce an additional 120ms of inference latency per frame, while our models Table 3. Video Depth Estimation on Bonn [27]. Table 4. Ablation on Attention and Cosine Similarity Method."
        },
        {
            "title": "Bonn",
            "content": "Abs Rel δ < 1."
        },
        {
            "title": "Method",
            "content": "CD NC Time (s) Peak Memory (GB)"
        },
        {
            "title": "Attention weight\nCosine similarity",
            "content": "0.036 0.032 0.567 0.570 0.288 0.168 17.30 14.49 VGGT (Offline) [34] StreamVGGT [45] CUT3R [35] Point3R [40] TTT3R [4] InfiniteVGGT VGGT (Offline) [34] StreamVGGT [45] CUT3R [35] Point3R [40] TTT3R [4] InfiniteVGGT VGGT (Offline) [34] StreamVGGT [45] CUT3R [35] Point3R [40] TTT3R [4] InfiniteVGGT VGGT (Offline) [34] StreamVGGT [45] CUT3R [35] Point3R [40] TTT3R [4] InfiniteVGGT 200 300 400 OOM OOM 0.072 0.069 0.068 0.063 OOM OOM 0.089 0.081 0.079 0.072 OOM OOM 0.090 0.081 0.078 0.070 OOM OOM 0.084 0.081 0.076 0.069 OOM OOM 0.947 0.954 0.953 0.964 OOM OOM 0.938 0.946 0.949 0. OOM OOM 0.934 0.945 0.951 0.958 OOM OOM 0.939 0.946 0.953 0.960 compatibility with FlashAttention [5] mitigates this bottleneck, enabling significantly faster inference speeds and reduced peak GPU memory footprint. Initial Budget Per-head. We further ablate the budget B(l,h) using the 7-Scenes dataset, comparing the results for 300 and 500 input with stride of 2, where B(l,h) denotes the initial maximum storage capacity for tokens per head in each layer. As shown in Tab. 5, smaller token storage budget B(l,h) significantly degrades the reconstruction quality, while this impact diminishes as the budget increases, eventually becoming negligible. Layer-wise Allocation Mechanism. To demonstrate the effectiveness of our layer-wise allocation mechanism for token selection, we conducted an ablation study on the 7Scenes. The input frames are 500. Given an initial budget B(l,h) 10000, we compare maintaining uniform, fixed storage limit across all layers against our dynamic layer-wise allocation scheme. As shown in Tab. 6, dynamically allocating the budget across layers further improves the resulting point cloud accuracy and normal consistency. Anchor Frame. The VGGT [34] architecture fundamentally depends on the first frame as global reference and establishes the canonical coordinate system for the entire sequence. Given this pivotal role, we posit that applying Table 5. Ablation Study on Initial Budget Per-Head."
        },
        {
            "title": "Input",
            "content": ""
        },
        {
            "title": "Initial Budget",
            "content": "10000 Bl,h Bl,h Bl,h 25000 50000 CD NC CD NC 0.062 0.032 0. 0.565 0.570 0.570 0.075 0.033 0.031 0.555 0.560 0.562 Table 6. Ablation Study on Layer-wise Allocation Mechanism. Acc. Comp. NC"
        },
        {
            "title": "Input",
            "content": "Mean Med. Mean Med. Mean Med. w/o layer-wise allocation w/ layer-wise allocation 500 0.098 0.093 0.058 0.053 0.057 0. 0.008 0.008 0.554 0.555 0.582 0.583 Table 7. Ablation Study on Anchor Frame Mechanism."
        },
        {
            "title": "Method",
            "content": "Acc. Comp. NC Mean Med. Mean Med. Mean Med. w/o anchor frame w/ anchor frame 0.047 0. 0.020 0.015 0.027 0.025 0.006 0.006 0.570 0.570 0.606 0.607 token pruning to the initial state could lead to irreversible information loss. Therefore, we design strategy where the tokens of the first frame are fully retained as an anchor frame, effectively bypassing the diversity-based selection mechanism applied to subsequent frames. To validate the necessity of this design, we conduct an ablation study on the 7-Scenes [31] dataset, using 300-frame inputs sampled with stride of 2, to assess the impact of this anchor frame strategy on 3D reconstruction accuracy. As evidenced by the results in Tab. 7, preserving the complete reference information of the first frame prevents error accumulation and leads to significant improvement in reconstruction quality. 6. Discussion The primary objective of this work is to enable online, infinite-horizon 3D geometry estimation for streaming scenes through novel rolling memory mechanism. Given that our method, InfiniteVGGT, is training-free modification of StreamVGGT [45], its performance on shorter sequences relative to the baseline warrants clarification. We therefore begin by confirming that our approach achieves comparable performance in these less demanding scenarios. On input sequences from 50 to 100 frames, which is range where the baseline operates without memory constraints, our comparison of CD and NC metrics reveals negligible performance differences. As shown in Fig. 6, InfiniteVGGT even achieves slight precision advantage in the NC metric. This advancements arise from our diversity-aware rolling memory mechanism, which refines the models historical context. By preserving more diverse set of informa- [4] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and Anpei Chen. Ttt3r: 3d reconstruction as test-time training. arXiv preprint arXiv:2509.26645, 2025. 2, 3, 6, 7, 8 [5] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS, pages 1634416359, 2022. 8 [6] Junyuan Deng, Heng Li, Tao Xie, Weiqiang Ren, Qian Zhang, Ping Tan, and Xiaoyang Guo. Sail-recon: Large sfm by augmenting scene regression with localization. arXiv preprint arXiv:2508.17972, 2025. 3 [7] Kai Deng, Zexin Ti, Jiawei Xu, Jian Yang, and Jin Xie. Vggtlong: Chunk it, loop it, align it pushing vggts limits on kilometer-scale long rgb sequences, 2025. 2, [8] J. Engel, T. Schops, and D. Cremers. LSD-SLAM: Largescale direct monocular SLAM. In ECCV, 2014. 3 [9] Jan-Michael Frahm, Pierre Fite-Georgel, David Gallup, Tim Johnson, Rahul Raguram, Changchang Wu, Yi-Hung Jen, Enrique Dunn, Brian Clipp, Svetlana Lazebnik, et al. BuildIn ECCV, pages 368381. ing rome on cloudless day. Springer, 2010. 1, 2 [10] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and robust multiview stereopsis. 32(8):13621376, 2009. 1 [11] Yasutaka Furukawa, Carlos Hernandez, et al. Multi-view stereo: tutorial. Foundations and Trends in Computer Graphics and Vision, 9(1-2):1148, 2015. 2 [12] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Massively parallel multiview stereopsis by surface normal diffusion. In ICCV, pages 873881, 2015. 2 [13] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution In CVPR, pages multi-view stereo and stereo matching. 24952504, 2020. [14] Richard Hartley and Andrew Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, 2000. 2 [15] Jin Gyu Hong, Seung Young Noh, Hee Kyung Lee, Won Sik Cheong, and Ju Yong Chang. 3d clothed human reconstruction from sparse multi-view images. In CVPRW, pages 677687, 2024. 1 [16] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 1 [17] Yushi Lan, Yihang Luo, Fangzhou Hong, Shangchen Zhou, Honghua Chen, Zhaoyang Lyu, Shuai Yang, Bo Dai, Chen Change Loy, and Xingang Pan. Stream3r: Scalable sequential 3d reconstruction with causal transformer. arXiv preprint arXiv:2508.10893, 2025. 2, 3 [18] Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds. In CVPR, 2025. 1 [19] Vincent Leroy, Yohann Cabon, and Jerome Revaud. Grounding image matching in 3d with mast3r, 2024. 1, Figure 6. Comparison of 3D Reconstruction. CD and NC metrics on NRGBD [2] dataset. tion from early stages, the mechanism enhances robustness against noisy data encountered as the sequence grows. It also prevents the memory from being saturated by redundant subsequent inputs. For long sequences, these benefits become critical. InfiniteVGGT not only resolves the outof-memory (OOM) errors that plague the baseline but also curtails the accumulation of temporal error. In line with our primary goal of addressing the challenges of long-sequence reconstruction, our evaluation is therefore concentrated on these demanding scenarios. 7. Conclusion InfiniteVGGT, novel We present rolling memory paradigm for streaming 3D geometry understanding that mitigates the trade-off between unbounded memory growth and long-term drift. Our training-free strategy achieves this by identifying memory redundancy via key cosine similarity and applying an adaptive, layer-wise budget allocation. This mechanism, fully compatible with FlashAttention, ensures bounded memory and computational efficiency for online streaming over infinite-horizon sequences. As result, InfiniteVGGT surpasses existing explicitand implicitstate methods in reconstruction accuracy and robustness. We also introduce the Long3D benchmark to support rigorous evaluation of extended-sequence performance."
        },
        {
            "title": "References",
            "content": "[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven Seitz, and Richard Szeliski. Building rome in day. Communications of the ACM, 54 (10):105112, 2011. 1, 2 [2] Dejan Azinovic, Ricardo Martin-Brualla, Dan Goldman, Matthias Nießner, and Justus Thies. Neural rgb-d surface reconstruction. In CVPR, pages 62906301, 2022. 6, 9 [3] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-languageaction flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. 1 [35] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In CVPR, 2025. 2, 3, 6, 7, 8 [36] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. 1, 3 [37] Yuesong Wang, Zhaojie Zeng, Tao Guan, Wei Yang, Zhuo Chen, Wenkai Liu, Luoyuan Xu, and Yawei Luo. Adaptive patch deformation for textureless-resilient multi-view stereo. In CVPR, pages 16211630, 2023. 2 [38] Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, and Tong He. π3: Scalable permutation-equivariant visual geometry learning, 2025. [39] Changchang Wu. Towards linear-time incremental structure from motion. In 2013 International Conference on 3D Vision (3DV), pages 127134. IEEE, 2013. 1, 2 [40] Yuqi Wu, Wenzhao Zheng, Jie Zhou, and Jiwen Lu. Point3r: Streaming 3d reconstruction with explicit spatial pointer memory. arXiv preprint arXiv:2507.02863, 2025. 3, 6, 8 [41] Yuqi Wu, Wenzhao Zheng, Sicheng Zuo, Yuanhui Huang, Jie Zhou, and Jiwen Lu. Embodiedocc: Embodied 3d occupancy prediction for vision-based online scene understanding. In ICCV, 2025. 1 [42] Jianing Yang, Alexander Sax, Kevin J. Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In CVPR, 2025. 1 [43] Yantai Yang, Yuhao Wang, Zichen Wen, Luo Zhongwei, Chang Zou, Zhipeng Zhang, Chuan Wen, and Linfeng Zhang. Efficientvla: Training-free acceleration and compression for vision-language-action models. arXiv preprint arXiv:2506.10100, 2025. 1 [44] Shunyuan Zheng, Boyao Zhou, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, and Yebin Liu. Gpsgaussian: Generalizable pixel-wise 3d gaussian splatting for real-time human novel view synthesis. In CVPR, 2024. 1 [45] Dong Zhuo, Wenzhao Zheng, Jiahe Guo, Yuqi Wu, Jie Zhou, and Jiwen Lu. Streaming 4d visual geometry transformer. arXiv preprint arXiv:2507.11539, 2025. 2, 3, 6, 8 [46] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pages 21652183. PMLR, 2023. [20] Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, Xiaofan Wang, Bei Liu, Jianlong Fu, Jianmin Bao, Dong Chen, Yuanchun Shi, Jiaolong Yang, and Baining Guo. Cogact: foundational vision-languageaction model for synergizing cognition and action in robotic manipulation, 2024. 1 [21] Zizun Li, Jianjun Zhou, Yifan Wang, Haoyu Guo, Wenzheng Chang, Yang Zhou, Haoyi Zhu, Junyi Chen, Chunhua Shen, and Tong He. Wint3r: Window-based streaming reconstruction with camera token pool, 2025. 3 [22] Shaohui Liu, Yidan Gao, Tianyi Zhang, Remi Pautrat, Johannes Schonberger, Viktor Larsson, and Marc Pollefeys. Robust incremental structure-from-motion with hybrid features. In ECCV, pages 249269. Springer, 2025. 1, 2 [23] Hidenobu Matsuki, Gwangbin Bae, and Andrew Davison. 4dtam: Non-rigid tracking and mapping via dynamic surface gaussians. In CVPR, 2025. 1 [24] Raul Mur-Artal, J. M. M. Montiel, and Juan D. Tardos. ORBSLAM: versatile and accurate monocular SLAM system. Submitted to IEEE Transaction on Robotics. arXiv preprint arXiv:1502.00956, 2015. 3 [25] Richard A. Newcombe, Steven J. Lovegrove, and Andrew J. Davison. Dtam: Dense tracking and mapping in real-time. In ICCV, pages 23202327, 2011. 3 [26] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [27] Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe Gigu`ere, and Cyrill Stachniss. Refusion: 3d reconstruction in dynamic environments for rgb-d cameras exploiting residuals. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 78557862, 2019. 7, 8 [28] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016. 1, 2 [29] Johannes Schonberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for In ECCV, pages 501518. unstructured multi-view stereo. Springer, 2016. 2 [30] You Shen, Zhipeng Zhang, Yansong Qu, and Liujuan Cao. Fastvggt: Training-free acceleration of visual geometry transformer. arXiv preprint arXiv:2509.02560, 2025. 2, 3 [31] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene coordinate regression forests for camera relocalization in rgb-d images. In CVPR, pages 29302937, 2013. 5, 6, 7, 8 [32] Noah Snavely, Steven Seitz, and Richard Szeliski. Photo In ACM SIGtourism: Exploring photo collections in 3d. GRAPH 2006 Papers, pages 835846, 2006. 1, 2 [33] Hengyi Wang and Lourdes Agapito. 3d reconstruction with spatial memory. arXiv preprint arXiv:2408.16061, 2024. 3 [34] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In CVPR, 2025. 1, 2, 3, 4, 6,"
        }
    ],
    "affiliations": [
        "AutoLab, School of Artificial Intelligence, Shanghai Jiao Tong University"
    ]
}