{
    "paper_title": "DiffusionLane: Diffusion Model for Lane Detection",
    "authors": [
        "Kunyang Zhou",
        "Yeqin Shao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we present a novel diffusion-based model for lane detection, called DiffusionLane, which treats the lane detection task as a denoising diffusion process in the parameter space of the lane. Firstly, we add the Gaussian noise to the parameters (the starting point and the angle) of ground truth lanes to obtain noisy lane anchors, and the model learns to refine the noisy lane anchors in a progressive way to obtain the target lanes. Secondly, we propose a hybrid decoding strategy to address the poor feature representation of the encoder, resulting from the noisy lane anchors. Specifically, we design a hybrid diffusion decoder to combine global-level and local-level decoders for high-quality lane anchors. Then, to improve the feature representation of the encoder, we employ an auxiliary head in the training stage to adopt the learnable lane anchors for enriching the supervision on the encoder. Experimental results on four benchmarks, Carlane, Tusimple, CULane, and LLAMAS, show that DiffusionLane possesses a strong generalization ability and promising detection performance compared to the previous state-of-the-art methods. For example, DiffusionLane with ResNet18 surpasses the existing methods by at least 1\\% accuracy on the domain adaptation dataset Carlane. Besides, DiffusionLane with MobileNetV4 gets 81.32\\% F1 score on CULane, 96.89\\% accuracy on Tusimple with ResNet34, and 97.59\\% F1 score on LLAMAS with ResNet101. Code will be available at https://github.com/zkyntu/UnLanedet."
        },
        {
            "title": "Start",
            "content": "DiffusionLane: Diffusion Model for Lane Detection Kunyang Zhou1,2,Yeqin Shao1* 1School of ZhangJian, Nantong University 2School of Automation, Southeast University {kunyangzhou@seu.edu.cn, hnsyk@ntu.edu.cn} 5 2 0 2 5 2 ] . [ 1 6 3 2 2 2 . 0 1 5 2 : r Abstract In this paper, we present novel diffusion-based model for lane detection, called DiffusionLane, which treats the lane detection task as denoising diffusion process in the parameter space of the lane. Firstly, we add the Gaussian noise to the parameters (the starting point and the angle) of ground truth lanes to obtain noisy lane anchors, and the model learns to refine the noisy lane anchors in progressive way to obtain the target lanes. Secondly, we propose hybrid decoding strategy to address the poor feature representation of the encoder, resulting from the noisy lane anchors. Specifically, we design hybrid diffusion decoder to combine global-level and local-level decoders for high-quality lane anchors. Then, to improve the feature representation of the encoder, we employ an auxiliary head in the training stage to adopt the learnable lane anchors for enriching the supervision on the encoder. Experimental results on four benchmarks, Carlane, Tusimple, CULane, and LLAMAS, show that DiffusionLane possesses strong generalization ability and promising detection performance compared to the previous state-of-the-art methods. For example, DiffusionLane with ResNet18 surpasses the existing methods by at least 1% accuracy on the domain adaptation dataset Carlane. Besides, DiffusionLane with MobileNetV4 gets 81.32% F1 score on CULane, 96.89% accuracy on Tusimple with ResNet34, and 97.59% F1 score on LLAMAS with ResNet101. Code will be available at https://github.com/zkyntu/UnLanedet. Introduction Lane detection is fundamental task in computer vision and autonomous driving, playing crucial role in adaptive cruise control and lane keeping. It aims to predict the location of lanes in the given image. Existing lane detection methods can be divided into three categories: segmentation-based (Pan et al. 2018; Zheng et al. 2021), anchor-based (Zheng et al. 2022; Honda and Uchida 2024; Xiao et al. 2023), and parameter-based methods (Tabelini et al. 2021b; Feng et al. 2022). Among the existing methods, anchor-based methods achieve excellent performance via predefining the high-quality lane anchors, attracting wide attention. Most anchor-based approaches adopt the learnable anchors to fit the distribution of the dataset. Although this way brings high performance on the specific dataset, it suffers from poor generalization ability in the distribution-shift *Correspond author. Figure 1: Diffusion model for lane detection. (a) The diffusion model where denotes the diffusion process and pÎ¸ represents the reverse process. (b) Noise-to-image process from noisy pixels to target image. (c) Our noise-to-lane paradigm from noisy lane anchors to target lanes. scenarios and requires re-training in these scenarios, damaging the convenience of the model. To address this issue, we reformulate the anchor-based methods as denosing diffusion process, i.e., directly predicting lanes from set of random lane anchors. Starting from the random lane anchors without any learnable parameters, we expect to gradually refine the random lane anchors to target lanes. This noise-to-lane approach does not require the learnable lane anchors, avoiding overfitting on the training dataset."
        },
        {
            "title": "Diffusion model",
            "content": "(Song, Meng, and Ermon 2020; Blattmann et al. 2023), prominent class of generative models for image synthesis, generates images through an iterative denoising process. This process is viewed as noise-toimage process. Since the diffusion model displays strong generalization ability, subsequent researches explore the diffusion model in perception tasks such as object detection (Chen et al. 2023a), object tracking (Xie, Wang, and Ma 2024), and image segmentation (Ji et al. 2023). However, to the best of our knowledge, there is no prior work that successfully explores the diffusion model in lane detection. In this paper, we propose DiffusionLane, casting the lane detection task as denosing diffusion paradigm over the space of the starting point and the angle of the lane. As shown in Figure 1, our noise-to-lane paradigm can be analogous to the noise-to-image process, where noise-to-image generates the target image by removing the noise from noisy pixels and noise-to-lane predicts the target lanes by removing the noise from the noisy lane anchors. At the training stage, Gaussian noise is added to the starting point and the angle of the ground truth to obtain the noisy lane anchors. Then, the noisy lane anchors are sent to the RoI pooling (Zheng et al. 2022) to crop the RoI features from the features of the encoder. Finally, the RoI features are fed into the decoder to predict the target lanes without noise. At the inference stage, DiffusionLane samples the random lane anchors from the Gaussian distribution and generates the target lanes by removing the noise in the random lane anchors. However, the quality of the random lane anchors is poor compared to the learnable lane anchors, degrading the feature representation ability of the encoder. In order to solve this issue, we propose hybrid decoding strategy, including the hybrid diffusion decoder and an auxiliary head. The former aims to refine the lane anchors with higher quality by combining the global-level and the local-level decoders, and the latter adopts the learnable lane anchors to provide extra positive supervision, which is only used in the training. Experimental results demonstrate that our DiffusionLane achieves state-of-the-art results on four benchmarks, i.e., Carlane, Tusimple, CULane, and LLAMAS. For instance, DiffusionLane with MobileNetV4 (Qin et al. 2024) gets 81.32% F1 score on CULane, setting new state-of-theart result. Thanks to the random lane anchors and denoising diffusion paradigm, DiffusionLane shows strong generalization ability and does not require re-training on the distribution-shift scenarios. For example, DiffusionLane achieves 86.23% accuracy on MuLane, sub-dataset of the domain adaptation dataset Carlane, surpassing the CLRerNet (Honda and Uchida 2024) by 1.21% accuracy. The main contributions of this paper are as follows. 1. We formulate the lane detection as the denoising diffusion process from the noisy lane anchors to the target lanes. To the best of our knowledge, we are the first to apply the diffusion model to the lane detection task. 2. We propose novel hybrid decoding strategy, including hybrid diffusion decoder and an auxiliary head, to generate high-quality lane anchors and improve the feature representation ability of the encoder. results show that our DiffusionLane achieves the state-of-the-art performance on four benchmarks. Remarkably, DiffusionLane displays strong generalization ability in the distribution-shift scenarios. 3. Experimental"
        },
        {
            "title": "Related Work",
            "content": "Lane Detection Existing lane detection methods can be divided into three categories according to the representation of the lane: segmentation-based methods, anchor-based methods, and parameter-based methods. Segmentation-based methods (Pan et al. 2018; Zhang et al. 2023; Zheng et al. 2021) regard the lane detection task as the segmentation task. SCNN (Pan et al. 2018) proposes message-passing module to capture the spatial dependency. RESA (Zheng et al. 2021) designs real-time feature aggregation module to capture the global and local features while keeping real-time detection. Anchor-based methods refine the predefined lane anchors to regress accurate lanes. UFLD (Qin, Wang, and Li 2020) predicts the lanes by the novel row-wise anchor. Line-CNN (Li et al. 2019) adopts the dense lane anchors and the RoI pooling (Ren et al. 2015) to achieve lane detection. CLRNet (Zheng et al. 2022) proposes learnable lane anchors and progressive lane refinement to detect lanes. Parameterbased methods (Tabelini et al. 2021b; Liu et al. 2021b; Feng et al. 2022) cast the lane detection as parametric modeling task. PolyLaneNet (Tabelini et al. 2021b) views lane as polynomial and predicts the parameters of the polynomial. LSTR (Liu et al. 2021b) adopts the DETR-like architecture and realizes the end-to-end lane detection. Different from the existing methods, DiffusionLane reformulates the lane detection as noise-to-lane process. Diffusion Model Diffusion model shows strong capabilities in the visual generation task (Song and Ermon 2019; Rombach et al. 2022; Zhang, Rao, and Agrawala 2023; Blattmann et al. 2023). Considering the strength of the diffusion model, several works transfer the diffusion model to the visual perception task. DDP (Ji et al. 2023) concatenates the noisy feature map with the feature map outputted from the backbone and performs the semantic segmentation via the diffusion process. DiffusionTrack (Xie, Wang, and Ma 2024) views the object tracking task as the denosing task and develops point setbased diffusion model to facilitate the object tracking. DiffusionDet (Chen et al. 2023a) predicts the bounding boxes from the noisy boxes. LSR-DM (Ruiz et al. 2024) segments the lane graph with the diffusion model from the aerial imagery, not belonging to the traditional lane detection that detects lanes from an on-board camera. Our DiffusionLane is the first attempt to employ the diffusion denoising process in the lane detection task. The main difference between the previous visual diffusion models and DiffusionLane is the diffusion target. Existing methods add noise to the bounding box (Chen et al. 2023a) or the feature map (Ji et al. 2023), while our method diffuses the starting point and the angle of the lane. Besides, we propose hybrid decoding strategy to reduce the influence of random lane anchors. Method In this section, we first introduce the preliminaries on the representation of the lane and the diffusion model. Then, we detail the model architecture, the training stage, and the inference stage. Finally, we describe the proposed hybrid decoding strategy. Preliminaries Lane representation. Following (Zheng et al. 2022), lane is represented as sequence of of equal-spaced 2d points, i.e., = {(x1, y1), (x2, y2), ..., (xN , yN )}, where is the number of points. In this paper, we call the representation the lane anchor. As shown in Figure 2, the coordinate is equally sampled along the image height, i.e., Figure 2: Illustration of lane representation. We take 7 sampling points as the example. yi = YmaxYmin 1 i, here Ymax and Ymin denote the coordinates of the ending point and the starting point of the lane. In this paper, Ymax is the image height, and Ymin is set to fixed value. Accordingly, the coordinates are one-to-one corresponding to the coordinates. DiffusionLane regresses the accurate lanes by refining the noisy lane anchors. The outputs of the model consist of four components: (1) the probabilities of background and foreground. (2) the starting point and the angle of the refined lanes. (3) the length of the refined lanes. (4) offests, i.e., the horizontal distance between sampling points and their ground truth. We first predict the background and foreground probabilities of lane anchor. Then, we obtain the starting point, angle, and length of foreground lane anchor. Finally, we refine the foreground lane anchors by regressing offsets. Diffusion model. Diffusion models (Ho, Jain, and Abbeel 2020; Sohl-Dickstein et al. 2015; Song, Meng, and Ermon 2020; Song and Ermon 2019) are generative likelihoodbased models, drawing inspiration from nonequilibrium thermodynamics (Song and Ermon 2019, 2020). Diffusion model defines two Markovian chains, i.e., diffusion forward chain that adds noise to the image and reverse chain that refines the noise back to the image. Formally, given the distribution of an image x0 q(x0), the diffusion forward process at time is defined as q(xtxt1). During the diffusion forward process, the diffusion model gradually adds the Gaussian noise to the image according to the variance schedule Î²1, ..., Î²T , where denotes the total number of time steps. The diffusion forward process can be written as: q(xtxt1) = (xt; (cid:112)1 Î²txt1, Î²tI) Where is the identity matrix. Benefiting from the Markovian chain, xt can be obtained by giving x0: (1) xt = Î±tx0 + (1 Î±t)Ïµ (2) where Ïµ (0, I) represents the sampled Gaussian vector. Î±t = (cid:81)t s=0(1 Î²s). During the training process, the diffusion model predicts x0 from xt at time t. In the inference, the model reverses the Gaussian noise xT back to x0. Diffusion Model for Lane Detection The overall architecture of DiffusionLane is shown in Figure 3, which is composed of the encoder (image encoder and SAFPN (Xiao et al. 2023)), the hybrid diffusion decoder, and the auxiliary head. The encoder takes the raw image as input and outputs the image features. The hybrid diffusion decoder receives the image features and runs times to obtain the predictions from the noisy lane anchors. Previous methods (Blattmann et al. 2023; Wu et al. 2024) requires multiple runs of the whole model to generate refined samples, while DiffusionLane only applies the decoder in multiple steps on the image features, significantly decreasing the computational burden. The auxiliary head is introduced to improve the feature representation of the encoder by adopting the learnable lane anchors. The detailed description of the hybrid diffusion decoder and the auxiliary head is presented in the hybrid decoding section. Encoder. As depicted in Figure 3, the encoder contains an image encoder and the SAFPN, an improved FPN with large kernel attention. The image encoder is responsible for extracting the high-level features of the input image. The extracted features are fed into SAFPN to generate multi-scale features. Single-scale feature is represented as Mi, [0, 2]. Training In the training stage, we first introduce the diffusion process, i.e., converting the ground truth to noisy lane anchors, and then train the model to reverse the diffusion process. We show the training pseudo-code of our method in Algorithm 1 of the supplementary materials. Ground truth padding. In the existing lane detection benchmarks, the number of lanes varies across images, which is inconvenient for the training. In order to set the number of lanes across images to fixed number Ntrain, we apply the padding operation to the ground truth in each image. Specifically, we pad some extra lane anchors to the ground truth. Common padding strategies are repeating the ground truth and concatenating the random lane anchors. We adopt the concatenating random lane anchors since this strategy works best (see Table 8). Lane corruption. We add the Gaussian noise to the padded ground truth. Existing methods (Xie, Wang, and Ma 2024; Chen et al. 2023a) add the Gaussian noise to the keypoints of the ground truth. However, adding the Gaussian noise to all points of lane anchor leads to heavy computational burden and hard optimization during the diffusion process. We add the Gaussian noise to the starting point and angle of lane anchor since the starting point and the angle determine the coarse location of lane anchor. As presented in Figure 3, given ground true lane, we first extend the coordinates of the starting point and the ending point to Ymin and Ymax to obtain the target lane anchor following (Zheng et al. 2022). Then, we sample Gaussian vector Ïµ R13 and add it to the starting point coordinate X0, Y0 and the angle Î¸0 of the target lane anchor by [Xt, Yt, Î¸t] = Î±t[X0, Y0, Î¸0] + (1 Î±t)Ïµ (3) Noise scale is controlled by Î±t, which is adjusted by the cosine schedule at time step t. Finally, we get the noisy lane anchor according to [Xt, Yt, Î¸t]. Training losses. We adopt focal loss (Lin et al. 2017) as the classification loss, segmentation loss (Qin, Wang, and Li 2020) as the auxiliary loss, smooth L1 loss, angle loss (Su et al. 2024), and Line-IoU loss (Zheng et al. 2022) as the regression loss. The weight of angle loss is 0.02, and the Figure 3: Training pipeline of DiffusionLane. The image encoder and SAFPN (Xiao et al. 2023) extract the multi-scale image features Mi, [0, 2] and feed them into the hybrid diffusion decoder. The hybrid diffusion decoder, containing stack of hybrid diffusion blocks, refines the noisy lanes to target lanes iteratively. Auxiliary head, only used in the training process, improves the feature representation of the encoder via learnable lane anchors. weights of other losses are the same as those in CLRNet. Losses are calculated between Ntrain refined lanes outputted by the hybrid diffusion decoder and the ground truths. SimOTA algorithm (Zheng et al. 2022) is utilized to assign multiple predictions to each ground truth."
        },
        {
            "title": "Inference",
            "content": "The inference process of DiffusionLane reverses the noisy lane anchors to target lanes. Starting from the noisy lane anchors sampled from the Gaussian distribution, DiffusionLane refines the noisy lanes progressively. Pseudo-code of the inference stage is offered in Algorithm 2 of the supplementary materials. Sampling step. The hybrid diffusion decoder runs times to perform lane anchor refinement, and each refinement is called sampling step. Specifically, the random lane anchors at the first sampling step or the refined lane anchors from the last sampling step are fed to the hybrid diffusion decoder to obtain further refined lane anchors. Then, the refined lane anchors are sent to the next sampling for iterative refinement via DDIM (Song, Meng, and Ermon 2020). As done in lane corruption in the training, we sample the starting points and angles from the Gaussian distribution to construct random lane anchors at the first sampling step. Lane anchors resampling. After the refinement in each sampling step, the lane anchors can be divided into two categories: foreground and background. Common practice is to filter the background via setting the confidence threshold. However, the number of foreground lane anchors varies across the image, leading to conflict with ground truth padding in the training. We adopt the lane anchor resampling strategy to solve this conflict. Specifically, we first filter the background lane anchors and then concatenate the foreground lane anchors with the random lane anchors sampled from the Gaussian distribution, which can ensure the feature distribution alignment between the inference and the Figure 4: Visualization of the attention map of the encoder. training. The concatenated lane anchors are sent to the next sampling step. After finishing all sampling steps, we adopt NMS algorithm (Zheng et al. 2022) to remove the duplicate predictions in the foreground lane anchors."
        },
        {
            "title": "Hybrid Decoding",
            "content": "Recent studies (Xiao et al. 2023; Wang et al. 2024) point out that the quality of the lane anchors has an impact on the feature representation of the encoder. DiffusionLane samples lane anchors from the Gaussian distribution and the quality of initial lane anchors is not as good as the learnable lane anchors used in previous methods (Zheng et al. 2022; Honda and Uchida 2024). As shown in Figure 4, the encoder of CLRNet can discriminate the features of the lane better than DiffusionLane. To alleviate the poor representation caused by random lane anchors, we propose novel hybrid decoding strategy, including novel hybrid diffusion decoder and an auxiliary head with learnable lane anchors. Hybrid diffusion decoder. In general, lane detector is equipped with single decoder, while we argue that lane detector can utilize multiple decoders. Although multiple decoders bring an extra computational burden, multiple decoders can integrate the strengths of each decoder and complement each other to generate high-quality lane anchors. Based on this point, we propose novel hybrid diffusion decoder, which combines two decoders at both global and local levels. To be specific, the hybrid diffusion decoder consists of stack of hybrid diffusion blocks and each block corresponds to scale image feature Mi. The architecture of single hybrid diffusion block is shown in Figure 5. Each hybrid diffusion block takes the lane anchors from the previous block and image feature Mi as inputs. The first diffusion block receives the noisy lane anchors. We define the set of RoI features of lane anchors Ps = {pi}Ntrain generated by RoI pooling (Tabelini et al. 2021a). i=0 For the global-level decoder, we adopt the RoIGather module (Zheng et al. 2022) to aggregate the global feature into pi. Since the noise scale has an important impact on the performance of the diffusion model (Chen et al. 2023b), we apply the scale and shift operation (Chen et al. 2023a) to pi. For the local-level decoder, we utilize self-attention module to enhance Ps and then adopt the dynamic convolution (Sun et al. 2021) to fuse the local features into pi. Dynamic convolution enhances the local features by facilitating the interaction between pi and other roi features. Specifically, we take the Ps as the convolution kernels. Thereafter, these kernels are applied to pi via convolution layers. To take full advantage of the potential of each decoder, we view the global-level decoder as the main decoder and the local-level decoder as the auxiliary decoder. The local-level decoder injects the implicit information into the input and output features of RoIGather. In particular, we integrate the output features of self-attention module and dynamic convolution into the input and output features of RoIGather through simple add operation with learnable weights. The output features of the main decoder are used to generate refined lane anchors. Auxiliary head. The poor lane anchors cause sparse supervision on the encoder due to the fewer positive samples, damaging the feature representation of the encoder. To alleviate this issue, we introduce an auxiliary head with learnable lane anchors, e.g., the head of CLRNet or CLRerNet, to enrich the supervision on the encoder. As shown in Figure 3, we send the multi-scale features Mi into the auxiliary head to get the predictions Qi. The auxiliary head computes the supervised targets for positive and negative samples in Qi. The training losses of the auxiliary head are the same as the original head, e.g., CLRNet head or CLRerNet head. The auxiliary head is only used in the training process."
        },
        {
            "title": "Experiments",
            "content": "Datasets We conduct the experiments on four lane detection benchmarks: CULane (Pan et al. 2018), LLAMAS (Behrendt and Soussan 2019), Tusimple (Shirke and Udayakumar 2019), and Carlane (Stuhr, Haselberger, and Gebele 2022). Carlane is dataset for domain adaptative lane detection task, containing three sub-datasets: TuLane, MoLane, and MuLane. Each sub-dataset comprises over 20K training images. Image size is 1280720. LLAMAS is large-scale lane detection datasets with over 100K images. All lanes are annotated with accurate maps. Image size is 1280717. Tusimple is dataset for highway scenario, consisting of 3626 images for training and 2782 images for testing. All images have 1280720 pixels. Figure 5: Architecture of hybrid diffusion block. Table 1: Performance on MoLane and TuLane. Acc denotes the Accuracy and * represents Source-only. Methods MoLane TuLane Acc(%) FP(%) FN(%) Acc(%) FP(%) FN(%) UFLD* (Qin, Wang, and Li 2020) BezierLaneNet* (Feng et al. 2022) CLRNet* (Zheng et al. 2022) DACCA* (Zhou, Feng, and Li 2024) CLRerNet* (Honda and Uchida 2024) DANN (Ganin et al. 2016) DiffusionLane* 88.15 86.78 89.14 86.15 88.54 85.25 91. 34.35 30.15 27.00 38.85 32.26 39.07 24.59 28.45 28.10 26.96 29.50 28.33 36.18 19.32 87.43 86.10 87.92 85.93 87.35 88.74 91.00 34.21 32.99 32.25 34.99 34.56 32.71 26.11 23.48 24.21 20.28 24.22 23.44 21.64 15.77 CULane is widely used dataset for lane detection, which is composed of 88.9K training images, 9.7K images for validation, and 34.7K testing images. Image size is 1640590. Evaluation Metrics We utilize F1 score to measure the performance for CULane and LLAMAS datasets. For Tusimple and Carlane datasets, we adopt accuracy, FP, and FN to evaluate the model performance. More details of the evaluation metrics are provided in the supplementary materials. Implementation Details The total time steps is set to 2. We choose CLRNet head as the auxiliary head. The confidence threshold to distinguish the foreground and background is 0.4. The initial noise scale is set to 2. The number of hybrid diffusion block is 3. More implementation details are in the supplementary materials. Comparison with the State-of-the-art Methods Performance on Carlane. We first compare the domain adaptation ability of different lane detectors. Source-only means model is only accessible to the source domain in the training process. We adopt ResNet18 as the image encoder Table 2: Performance comparison of different models on MuLane. Table 4: Performance comparison of different models on Tusimple. Methods UFLD* (Qin, Wang, and Li 2020) BezierLaneNet* (Feng et al. 2022) CLRNet* (Zheng et al. 2022) CLRerNet* (Honda and Uchida 2024) DACCA* (Zhou, Feng, and Li 2024) DANN (Ganin et al. 2016) DiffusionLane* Acc(%) 79.61 77.23 84.44 85.01 83.28 84.01 86.23 FP(%) 44.78 46.26 39.62 35.52 47.17 38.31 27.85 FN(%) 33.36 35.11 29.82 27.10 55.21 36.30 25.31 Table 3: Performance comparison of different models on LLAMAS. Methods SCNN (Pan et al. 2018) UFLD (Qin, Wang, and Li 2020) GANet (Wang et al. 2022) LaneATT (Tabelini et al. 2021a) CondLaneNet (Liu et al. 2021a) CondLaneNet (Liu et al. 2021a) CLRNet (Zheng et al. 2022) CLRNet (Zheng et al. 2022) GSENet (Zheng et al. 2022) Lane2Seq (segmentation) (Zhou 2024) DiffusionLane DiffusionLane DiffusionLane Image encoder Acc(%) VGG16 ResNet34 ResNet34 ResNet122 ResNet34 ResNet101 ResNet34 ResNet101 ResNet34 ViT-Base ResNet18 ResNet34 ResNet101 96.53 95.86 95.87 96.10 95.37 96.54 96.87 96.83 96.88 96.85 96.84 96.89 96.86 FP(%) 6.17 18.91 1.99 5.64 2.20 2.01 2.27 2.37 2.04 2.01 2.15 2.03 1.97 FN(%) 1.80 3.75 2.64 2.17 3.82 3.50 2.08 2.38 - 2.03 2.10 2.03 2.06 Methods PolyLaneNet (Tabelini et al. 2021b) BezierLaneNet (Feng et al. 2022) LaneATT (Tabelini et al. 2021a) LaneATT (Tabelini et al. 2021a) LaneAF (Abualsaud et al. 2021) CLRNet (Zheng et al. 2022) CLRNet (Zheng et al. 2022) Lane2Seq (segmentation) (Zhou 2024) ViT-Base (Dosovitskiy et al. 2020) DiffusionLane DiffusionLane DiffusionLane Image encoder EfficientNet-B0 (Tan and Le 2019) ResNet34 ResNet34 ResNet122 DLA34 ResNet18 DLA ResNet18 ResNet34 ResNet101 F1(%) 90.20 96.11 94.96 95.17 96.90 96.96 97.16 97.42 97.27 97.36 97.59 for all models. Results are shown in Table 9 and 10. We can see that DiffusionLane achieves the best performance among the existing methods. Compared with CLRerNet, DiffusionLane gains 2.74% (91.28% vs. 88.54%), 3.65% (91.00% vs. 87.35%), and 1.22% (86.23% vs. 85.01%) accuracy improvements on MoLane, TuLane, and MuLane. Compared with the domain adaptation method DANN with UFLD, DiffusionLane obtains the better performance on three datasets. The results manifest the strong generalization ability of DiffusionLane in the domain shift scenarios. We attribute the reason to that the lane anchors of DiffusionLane are sampled from Gaussian distribution, which are domain agnostic. However, the lane anchors in existing methods like CLRNet are learned from the source domain, requiring the data distribution is similar between the target domain and the source domain. Performance on LLAMAS. As shown in Table 11, DiffusionLane increases the F1 score from 97.42% to 97.59% compared to the previous state-of-the-art method Lane2Seq. DiffusionLane with ResNet18 achieves 0.31% (97.27% vs. 96.90%) F1 score improvement compared to CLRNet with ResNet18. The results show the effectiveness of DiffusionLane in multi-lane scenarios (the number of lanes > 4). Performance on Tusimple. We present the results on Tusimple dataset in Table 12. Since the data scale of Tusimple is small, the performance gap between different methods is small. DiffusionLane with ResNet34 achieves the best accuracy of 96.89%. DiffusionLane with ResNet101 decreases FP from 1.99% to 1.97%, establishing new state-of-the-art result for this indicator. Performance on CULane. The benchmark results on CULane dataset are presented in Table 13. It is worth noting that DiffusionLane with MobileNetV4-Hybrid-M reaches new state-of-the-art F1 score of 81.32%. DiffusionLane with DLA34 achieves the better performance than the previous state-of-the-art method CLRerNet (81.18% vs. 81.12%), Figure 6: Visualization results of CLRNet, Lane2Seq, and DiffusionLane on four benchmarks. especially in difficult scenarios such as Dazzle (74.78% vs. 74.41%) and Night (76.79% vs. 76.53%). Compared with CLRNet, DiffusionLane with ResNet34 surpasses it by 0.49% (80.24% vs. 79.73%) F1 score improvement. The results indicate that the random lane anchors can achieve competitive even better performance than the learnable lane anchors with the proposed diffusion paradigm and the hybrid decoding strategy. Besides, compared with the parameterbased method BSNet and Lane2Seq with segmentation format, DiffusionLane also has the performance advantages. Qualitative results. We display the qualitative results in Figure 6. The results show that DiffusionLane can effectively detect lanes in the distribution-shift scenario (Figure 6 (c)) but CLRNet and Lane2Seq does not. Even in the extreme lightning scenario (Figure 6 (a) and (d)), DiffusionLane is able to detect lanes successfully. Ablation Study We conduct the ablation studies on the CULane dataset to evaluate the effectiveness of the key components in our method. We take CLRNet with ResNet34, SAFPN, and angle loss as the baseline. Additional ablation studies are provided in the supplementary materials. Effectiveness of diffusion paradigm. We first ablate the influence of the diffusion paradigm. As shown in Table 6, performance degrades from 79.96% to 74.74% when using random lane anchors, indicating that the quality of the lane anchors affects the performance significantly. Equipped with our diffusion paradigm, F1 score is improved by 3.64% Table 5: Comparison of F1 score on CULane testing set. We only report the false positives for Cross category. Methods SCNN (Pan et al. 2018) RESA (Zheng et al. 2021) AtrousFormer (Yang, Zhang, and Lu 2023) LaneATT (Tabelini et al. 2021a) O2SFormer (Zhou and Zhou 2023) UFLD (Qin, Wang, and Li 2020) CondLaneNet (Liu et al. 2021a) ADNet (Xiao et al. 2023) BezierLaneNet (Feng et al. 2022) BSNet (Chen, Wang, and Liu 2023) Eigenlanes (Jin et al. 2022) Laneformer (Han et al. 2022) CLRNet (Zheng et al. 2022) Lane2Seq (segmentation) (Zhou 2024) CLRerNet (Han et al. 2022) CLRerNet (Han et al. 2022) DiffusionLane DiffusionLane DiffusionLane DiffusionLane Image encoder ResNet50 ResNet50 ResNet34 ResNet122 ResNet50 ResNet34 ResNet34 ResNet34 ResNet18 ResNet34 ResNet50 ResNet50 ResNet34 ViT-Base ResNet34 DLA34 ResNet34 ResNet50 DLA34 MobileNetV4-Hybrid-M Normal 90.60 92.10 92.83 91.74 93.09 90.70 93.38 92.90 90.22 93.75 91.70 91.77 93.49 93.39 93.93 94.02 93.82 93.91 94.06 94.15 Crowded Dazzle 69.70 73.10 75.96 76.16 76.57 70.20 77.14 77.45 71.55 78.01 76.00 75.74 78.06 77.27 79.51 80.20 78.65 79.25 79.94 79.99 58.50 69.20 69.48 69.47 72.25 59.50 71.17 71.71 62.49 76.65 69.80 70.17 74.57 73.45 73.88 74.41 74.39 74.66 74.78 74. Shadow No line Arrow 84.10 43.40 88.30 47.70 88.66 50.15 86.29 50.46 89.50 52.80 85.70 44.40 89.89 51.85 89.90 52.89 84.09 45.30 90.72 54.69 87.70 52.20 87.65 48.73 90.59 54.01 90.53 53.91 90.87 55.55 90.39 56.27 90.88 54.86 90.69 55.32 90.45 56.24 56.45 90.41 66.90 72.80 77.86 76.31 76.56 69.30 79.93 79.11 70.91 79.55 74.10 75.75 79.92 79.69 83.16 83.71 82.18 82.57 83.75 83.80 Curve Night 66.10 64.40 69.90 70.30 73.74 71.14 70.81 64.05 73.85 69.60 66.70 69.50 73.92 73.88 74.78 70.64 68.70 58.98 75.28 73.99 71.80 62.90 71.04 66.33 75.02 72.77 74.96 73.37 76.02 74.45 76.53 74.67 75.79 73.77 76.06 73.61 76.69 74.80 76.87 75.02 Cross 1900 1503 1054 1264 3118 2037 1387 1499 996 1445 1509 19 1216 1129 1088 1161 1119 1054 1233 1133 Total 71.60 75.3 78.08 77.02 77.83 72.30 78.74 78.94 73.67 79.89 77.20 77.06 79.73 79.64 80.76 81.12 80.24 80.68 81.18 81.32 Table 6: Effectiveness of each component in DiffusionLane. RLA dentoes the random lane anchors. RLA Diffusion paradigm Hybrid diffusion decoder Auxiliary head F1(%) 79.96 74.74 78.38 79.46 80.24 (78.38% vs. 74.74%). The result manifests that the proposed diffusion paradigm has good denoising effect. Effectiveness of hybrid diffusion decoder. Table 6 shows that there still exists the quality gap between the lane anchors improved by the diffusion paradigm and the learnable lane anchors. Therefore, we propose the hybrid diffusion decoder to further improve the quality of the lane anchors. We can see that hybrid diffusion decoder gains 1.08% (79.46% vs. 78.38%) improvement in Table 6. The effectiveness of hybrid diffusion decoder shows that 1) the feature refinement of lane anchors can improve the quality of lane anchors; 2) multi-decoders surpasses the single decoder by complementing each other. Effectiveness of the auxiliary head. We further introduce an auxiliary head to improve the representation ability of the encoder. As presented in Table 6, the auxiliary head increases the F1 score from 79.46% to 80.24%. The result suggests that auxiliary positive samples generated by the auxiliary head is conductive to improving the performance via providing extra positive supervision singals. Influence of the time step. The denoising step in the inference stage can be viewed as the iterative denoising. The total time step decides the number of the iterations. As shown in Table 7, DiffusionLane achieves the best performance when = 2. We find that when > 2, the model performance degrades, indicating that foreground lanes are filtered when is large. Besides, the larger time step brings more computational burden. Therefore, we set to 2. Sampling strategy. We compare different sampling strategies in Table 7. We first evaluate DiffusionLane without DDIM, i.e., taking the output of the current step without Table 7: The ablation study on the sampling strategy. DDIM Lane anchors resampling = 1 77.63 78.89 78.39 79.85 = 2 77.35 79.79 79.46 80.24 = 4 76.98 79.15 79.02 79. = 6 76.11 79.08 78.80 79.62 Table 8: The ablation study on the GT padding strategy. F1 (%) Repeating 78.27 Padding Gaussian 80.24 Padding Uniform 79. the reverse chain as input for the next sampling step. Results show that DDIM can effectively improves the model performance. When equipped with lane anchors resampling, model performance gains the further enhancement, showing that DDIM and lane anchors resampling are beneficial. Ground truth padding strategy. We study different padding strategy in Table 8. We consider three strategies: repeating the ground truth, padding the random lane anchors from the Gaussian distribution, and padding the random lane anchors from the uniform distribution. Results show that padding the random lane anchors from the Gaussian distribution works best."
        },
        {
            "title": "Conclusion",
            "content": "In this paper, we propose novel lane detection paradigm, named DiffusionLane, casting the lane detection task as diffusion process from the noisy lane anchors to target lanes. Benefiting from the random lane anchors setting and denoising diffusion paradigm, DiffusionLane shows strong generalization ability, enabling us to adopt the DiffusionLane in distribution-shift scenarios without re-training the model. Moreover, we propose hybrid decoding strategy, including the hybrid diffusion decoder and the auxiliary detection head, to achieve the better lane anchor refinement and feature representation. Experiments on four benchmarks demonstrate that DiffusionLane achieves favorable performance compared to the existing lane detectors. References Abualsaud, H.; Liu, S.; Lu, D. B.; Situ, K.; Rangesh, A.; and Trivedi, M. M. 2021. Laneaf: Robust multi-lane detection with affinity fields. IEEE Robotics and Automation Letters, 6(4): 74777484. Behrendt, K.; and Soussan, R. 2019. Unsupervised labeled lane markers using maps. In Proceedings of the IEEE/CVF international conference on computer vision workshops, 0 0. Blattmann, A.; Dockhorn, T.; Kulal, S.; Mendelevitch, D.; Kilian, M.; Lorenz, D.; Levi, Y.; English, Z.; Voleti, V.; Letts, A.; et al. 2023. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127. Chen, H.; Wang, M.; and Liu, Y. 2023. Bsnet: Lane dearXiv preprint tection via draw b-spline curves nearby. arXiv:2301.06910. Chen, S.; Sun, P.; Song, Y.; and Luo, P. 2023a. Diffusiondet: Diffusion model for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, 1983019843. Chen, T.; Li, L.; Saxena, S.; Hinton, G.; and Fleet, D. J. 2023b. generalist framework for panoptic segmentation of images and videos. In Proceedings of the IEEE/CVF international conference on computer vision, 909919. Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and FeiFei, L. 2009. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, 248255. Ieee. Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Feng, Z.; Guo, S.; Tan, X.; Xu, K.; Wang, M.; and Ma, L. 2022. Rethinking efficient lane detection via curve modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1706217070. Ganin, Y.; Ustinova, E.; Ajakan, H.; Germain, P.; Larochelle, H.; Laviolette, F.; March, M.; and Lempitsky, V. 2016. Domain-adversarial training of neural networks. Journal of machine learning research, 17(59): 135. Han, J.; Deng, X.; Cai, X.; Yang, Z.; Xu, H.; Xu, C.; and Liang, X. 2022. Laneformer: Object-aware row-column transformers for lane detection. In Proceedings of the AAAI conference on artificial intelligence, volume 36, 799807. He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residIn Proceedings of the ual learning for image recognition. IEEE conference on computer vision and pattern recognition, 770778. Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33: 68406851. Honda, H.; and Uchida, Y. 2024. Clrernet: improving confidence of lane detection with laneiou. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, 11761185. Ji, Y.; Chen, Z.; Xie, E.; Hong, L.; Liu, X.; Liu, Z.; Lu, T.; Li, Z.; and Luo, P. 2023. Ddp: Diffusion model for dense visual prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2174121752. Jin, D.; Park, W.; Jeong, S.-G.; Kwon, H.; and Kim, C.-S. 2022. Eigenlanes: Data-driven lane descriptors for strucIn Proceedings of the IEEE/CVF turally diverse lanes. Conference on Computer Vision and Pattern Recognition, 1716317171. Li, X.; Li, J.; Hu, X.; and Yang, J. 2019. Line-cnn: EndIEEE to-end traffic line detection with line proposal unit. Transactions on Intelligent Transportation Systems, 21(1): 248258. Lin, T.-Y.; Goyal, P.; Girshick, R.; He, K.; and Dollar, P. 2017. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, 29802988. Liu, L.; Chen, X.; Zhu, S.; and Tan, P. 2021a. Condlanenet: top-to-down lane detection framework based on conditional convolution. In Proceedings of the IEEE/CVF international conference on computer vision, 37733782. Liu, R.; Yuan, Z.; Liu, T.; and Xiong, Z. 2021b. End-toend lane shape prediction with transformers. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, 36943702. Pan, X.; Shi, J.; Luo, P.; Wang, X.; and Tang, X. 2018. Spatial as deep: Spatial cnn for traffic scene understanding. In Proceedings of the AAAI conference on artificial intelligence, volume 32. Qin, D.; Leichner, C.; Delakis, M.; Fornoni, M.; Luo, S.; Yang, F.; Wang, W.; Banbury, C.; Ye, C.; Akin, B.; et al. 2024. MobileNetV4: universal models for the mobile In European Conference on Computer Vision, ecosystem. 7896. Springer. Qin, Z.; Wang, H.; and Li, X. 2020. Ultra fast structureaware deep lane detection. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXIV 16, 276291. Springer. Ren, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 10684 10695. Ruiz, A.; Melnik, A.; Wang, D.; and Ritter, H. 2024. Lane arXiv segmentation refinement with diffusion models. preprint arXiv:2405.00620. Shirke, S.; and Udayakumar, R. 2019. Lane datasets for lane detection. In 2019 International Conference on Communication and Signal Processing (ICCSP), 07920796. IEEE. Sohl-Dickstein, J.; Weiss, E.; Maheswaranathan, N.; and Deep unsupervised learning using Ganguli, S. 2015. In International confernonequilibrium thermodynamics. ence on machine learning, 22562265. pmlr. Yang, J.; Zhang, L.; and Lu, H. 2023. Lane detection with versatile atrousformer and local semantic guidance. Pattern Recognition, 133: 109053. Yu, F.; Wang, D.; Shelhamer, E.; and Darrell, T. 2018. Deep layer aggregation. In Proceedings of the IEEE conference on computer vision and pattern recognition, 24032412. Zhang, J.-Q.; Duan, H.-B.; Chen, J.-L.; Shamir, A.; and Wang, M. 2023. HoughLaneNet: Lane detection with deep hough transform and dynamic convolution. Computers & Graphics, 116: 8292. Zhang, L.; Rao, A.; and Agrawala, M. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, 38363847. Zheng, T.; Fang, H.; Zhang, Y.; Tang, W.; Yang, Z.; Liu, H.; and Cai, D. 2021. Resa: Recurrent feature-shift aggregator for lane detection. In Proceedings of the AAAI conference on artificial intelligence, volume 35, 35473554. Zheng, T.; Huang, Y.; Liu, Y.; Tang, W.; Yang, Z.; Cai, D.; and He, X. 2022. Clrnet: Cross layer refinement network for lane detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 898907. Zhou, K. 2024. Lane2seq: towards unified lane detection via sequence generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1694416953. Zhou, K.; Feng, Y.; and Li, J. 2024. Unsupervised Domain Adaptive Lane Detection via Contextual Contrast and Aggregation. arXiv preprint arXiv:2407.13328. Zhou, K.; and Zhou, R. 2023. tection with one-to-several transformer. arXiv:2305.00675. End-to-end lane dearXiv preprint Song, J.; Meng, C.; and Ermon, S. 2020. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502. Song, Y.; and Ermon, S. 2019. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32. Song, Y.; and Ermon, S. 2020. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33: 1243812448. Stuhr, B.; Haselberger, J.; and Gebele, J. 2022. Carlane: lane detection benchmark for unsupervised domain adaptation from simulation to multiple real-world domains. Advances in Neural Information Processing Systems, 35: 40464058. Su, J.; Chen, Z.; He, C.; Guan, D.; Cai, C.; Zhou, T.; Wei, J.; Tian, W.; and Xie, Z. 2024. Gsenet: Global semantic enhancement network for lane detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, 1510815116. Sun, P.; Zhang, R.; Jiang, Y.; Kong, T.; Xu, C.; Zhan, W.; Tomizuka, M.; Li, L.; Yuan, Z.; Wang, C.; et al. 2021. Sparse r-cnn: End-to-end object detection with learnable proposals. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1445414463. Tabelini, L.; Berriel, R.; Paixao, T. M.; Badue, C.; De Souza, A. F.; and Oliveira-Santos, T. 2021a. Keep your eyes on the lane: Real-time attention-guided lane detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 294302. Tabelini, L.; Berriel, R.; Paixao, T. M.; Badue, C.; De Souza, A. F.; and Oliveira-Santos, T. 2021b. Polylanenet: Lane estimation via deep polynomial regression. In 2020 25th international conference on pattern recognition (ICPR), 6150 6156. IEEE. Tan, M.; and Le, Q. 2019. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, 61056114. PMLR. Wang, J.; Ma, Y.; Huang, S.; Hui, T.; Wang, F.; Qian, C.; and Zhang, T. 2022. keypoint-based global association network for lane detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 13921401. Wang, S.; Liu, J.; Cao, X.; Song, Z.; and Sun, K. 2024. Polar R-CNN: End-to-End Lane Detection with Fewer Anchors. arXiv preprint arXiv:2411.01499. Wu, J.; Fu, R.; Fang, H.; Zhang, Y.; Yang, Y.; Xiong, H.; Liu, H.; and Xu, Y. 2024. Medsegdiff: Medical image segmentation with diffusion probabilistic model. In Medical Imaging with Deep Learning, 16231639. PMLR. Xiao, L.; Li, X.; Yang, S.; and Yang, W. 2023. Adnet: Lane shape prediction via anchor decomposition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 64046413. Xie, F.; Wang, Z.; and Ma, C. 2024. Diffusiontrack: Point set diffusion model for visual object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1911319124. Suppmentary Material for DiffusionLane: Diffusion Model for Lane Detection Abstract In this suppmentary material, we first provide the pseudocodes of the training and the inference stage. Then, we supplemet the additional experiments, including the evaluation metrics, additional implementation details, and additional ablation studies. Finally, we describe the limitation and broader impact."
        },
        {
            "title": "Method",
            "content": "Pseudo-codes of training and inference stage The pseudo-codes of the training and inference stage 2. In Algorithm 1, are provided in Algorithm 1 and alpha cumprod(t) = (cid:81)t In Algorithm 2, linespace means generating evenly spaced values. i=1 Î±i."
        },
        {
            "title": "Additional Experiments",
            "content": "Evaluation metrics For CULane and LLAMAS dataset, we utilize F1 score to measure the performance: F1 = 2P recisionRecall , where recision+Recall recision = P +F and Recall = P +F . , , and represent the true positive rate, the false positive rate, and the false negative rate, respectively. (cid:80) (cid:80) For Tusimple and Carlane datasets, we adopt accuracy, FP, and FN to evaluate the model performance. Accuracy is clip Cclip , Cclip denotes the numdefined as Accuracy = clip Sclip ber of accurately predicted lane points and Sclip represents the total number of lane points of clip. lane point is treated as correct point if its distance is smaller than the threshold tpc = 20 cos(ayl) , here aul is the angle of the corresponding ground truth. Additional implementation details We select the ResNet (He et al. 2016), DLA (Yu et al. 2018), and MobileNetV4-Hybrid-M (Qin et al. 2024) as the image encoder and all image encoders are initialized with pretrained weights on ImageNet1K (Deng et al. 2009). We train DiffusionLane using AdamW optimizer with learning rate 0.0003. Cosine schedule is adopted to adjust the learning rate. All models are trained on single 3090 GPU with 24 GB memory and batch size is 20. The training epoches are set to 70,25,20,20 for Tusimple, CULane, LLAMAS, and Carlane datasets, respectively. The number of lanes Ntrain in the ground truth padding is 800. We set Ymin to 160, 270, 300, and 160 for Tusimple, CULane, LLAMAS, and Carlane. Additional ablation studies In this section, we provide the additional ablation studies. If not specified, all supplemeted experiments are conducted on CULane and image encoder is ResNet34. Table 9: The ablation study on the noise scale. F1 (%) 0.1 77.35 0.5 78.69 1 79.42 2 80.24 3 80. Noise scale. We first ablate the influence of the nosie scale and the results are presented on Table 9. We can see that the model achieves the optimal performance when the noise scale is 2, which is higher than the image generation task (noise scale is 1) (Song, Meng, and Ermon 2020) and segmentation task (noise scale is 0.1) (Ji et al. 2023). Table 10: The ablation study on the number of lane anchors Ntrain. F1 (%) 192 76.11 384 77.53 600 78. 800 80.24 1000 80.31 The number of lane anchors. We study the number of lane anchors Ntrain. Results in Table 10 show that DiffusionLane prefers high Ntrain (800 lane anchors). We explain that if Ntrain is small, the lane anchors around the target lanes are sparse due to the uniform distribution. This leads to hard optimization during the training compared to the learnable lane anchors. However, when Ntrain is large enough, the computational burden increases. Considering the performance and the efficiency, we set Ntrain to 800. Table 11: The ablation study on FPS. 192 88 384 600 65 800 52"
        },
        {
            "title": "FPS",
            "content": "Analysis on the FPS. Table 11 presents the study on FPS. We test te inference speed on single 2080Ti GPU without TensorRT. It can be observed that DiffusionLane does not have the advantages in the inference speed. Reasons may be that DiffusionLane requires the multiple runs on the decoder, which affects the inference speed. We think this is limitation of DiffusionLane and our ongoing work is improving the inference speed. Transferring hybrid decoding to other lane detectors. Table 12 presents the results on transferring the hybrid decoding technology to the existing anchor-based lane detectors. Results demonstrate that our hybrid decoding can brings the consistent improvement on the existing anchorbased methods, indicating that hybrid decoding technology can help optimize the parameters of the learnable lane anchors. Diffusion target. We select the starting point coordinate and the angle of the lane anchor as the diffusion target. An Table 12: Transferring the hybrid decoding technology to the existing lane detectors. Methods CLRNet (Zheng et al. 2022) CLRNet+hybrid decoding CLRerNet (Honda and Uchida 2024) CLRerNet+hybrid decoding ADNet (Xiao et al. 2023) ADNet+hybrid decoding Image encoder ResNet34 ResNet34 ResNet34 ResNet34 ResNet34 ResNet F1(%) 79.73 80.58 80.67 81.11 78.94 79.79 Table 13: Comparison of different diffusion targets."
        },
        {
            "title": "Diffusion target\nStarting point and angle\nAll lane points\nAll lane points",
            "content": "F1(%) Training epoches 80.24 78.95 80.21 25 25 40 alternative diffusion paradigm is adding the Gaussian noise to all points of lane anchor. We compare the above two methods in Table 13. The results show that adopting all lane points as the diffusion target require more training epoches than the starting point and the angle, indicating that selecting all lane points as the diffusion target leads to hard optimization. Besides, diffusing all lane points brings extra computational burden. Therefore, we adopt the starting point and the angle as the diffusion target."
        },
        {
            "title": "Limitation and Broader Impact",
            "content": "Limitation As discussed in the Table 11, one major limitation of DiffusionLane is the inference speed. Reasons lie in two folds: 1) dense lane anchors. 2) multiple runs of the decoder. In the further, we will focus on developing decoding technology with high efficiency. Broader Impact From the results presented by DiffusionLane, we believe the diffusion model is new possible model for lane detection due to its random lane anchors setting and denoising diffusion paradigm, showing strong generalization ability on the distribution-shift scenarios without retraining the model. We hope our method can serve as simple yet effective baseline for anchor-based methods and inspire designing more high-performance and efficient architecture. def train_loss(images, gt_lanes): Algorithm 1: Training 1 2 3 4 5 6 7 \"\"\" images: [B, H, W, 3] gt_lanes: [B, *, 76] # B: batch # N_train: number of lane anchors # 76=72+4, where 72 is the number of offsets and 4 represent the x,y coordinates of the starting point, the angle, and the length 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 36 37 38 39 40 41 42 \"\"\" # Encode image features feats = encoder(images) # Pad gt_lanes to N_train pb = pad_boxes(gt_lanes) # padded boxes: [B, N_train, 76] # Construct the target lane anchors pb = gt_to_anchor(pb) # [B, N_train, 76] # select the starting point and the angle as the diffusion target diff_target = pd[:,:,:3] # Normalize diff_target = (diff_target * 2 - 1) * scale # Corrupt gt_lanes = randint(0, T) eps = normal(mean=0, std=1) # noise: [B, # time step N_train, 3] pb_crpt = sqrt( diff_target + alpha_cumprod(t)) * sqrt(1 - alpha_cumprod(t)) * eps # construct noisy lane anchors pb_crpt = point_to_anchor(pb_crpt) # [B, N_train, 76] # Predict pb_pred = hybrid_diffusion_decoder(pb_crpt, feats, t) # Compute loss loss = compute_loss(pb_pred, gt_lanes) loss_aux = aux_head(pb_pred, gt_lanes) loss = loss + loss_aux return loss # Encode image features feats = encoder(images) \"\"\" images: [B, H, W, 3] # steps: number of sample steps # T: number of time steps \"\"\" Algorithm 2: DiffusionLane Sampling 1 def infer(images, steps, T): 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # construct noisy lane anchors pb_t = point_to_anchor(pb_t) -1)] # sample the starting point and angle # pb_t: [B, N_train, 3] pb_t = normal(mean=0, std=1) # uniform sample step size times = reversed(linespace(-1, T, steps)) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, 22 23 24 25 27 28 29 30 31 32 33 34 time_pairs = list(zip(times[:-1], times[1:]) for t_now, t_next in zip(time_pairs): # Predict pb_0 from pb_t pb_pred = hybrid_diffusion_decoder(pb_t, feats, t_now) # Estimate pb_t at t_next pb_t = ddim_step(pb_t, pb_pred, t_now, t_next) # Lane anchor resampling pb_t = lane_anchor_resampling(pb_t) return pb_pred"
        }
    ],
    "affiliations": [
        "School of Automation, Southeast University",
        "School of ZhangJian, Nantong University"
    ]
}