{
    "paper_title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning",
    "authors": [
        "Jinyang Wu",
        "Guocheng Zhai",
        "Ruihan Jin",
        "Jiahao Yuan",
        "Yuhao Shen",
        "Shuai Zhang",
        "Zhengqi Wen",
        "Jianhua Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) \\textbf{training-free cluster-based routing} that exploits empirical priors for domain-specific alignment, and (2) \\textbf{RL-based multi-step routing} that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools."
        },
        {
            "title": "Start",
            "content": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning Jinyang Wu1*, Guocheng Zhai1*, Ruihan Jin1*, Jiahao Yuan3, Yuhao Shen2, Shuai Zhang1, Zhengqi Wen1, Jianhua Tao1 1Tsinghua University, 2Zhejiang University, 3East China Normal University wu-jy23@mails.tsinghua.edu.cn 6 2 0 2 7 ] . [ 1 2 7 8 3 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes high-dimensional optimization challenge. Existing approaches often rely on single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), dualpath framework for dynamic tool usage in crossdomain complex reasoning. ATLAS operates via dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both indistribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have evolved from static problem solvers into collaborative reasoning engines through adaptive integration with external tools. These tools range from symbolic reasoning modules (Feng et al., 2025a) to real-time information retrieval APIs (Ma et al., 2025), significantly extending LLMs operational capabilities. As this LLM-tool ecosystem evolves, the synergy from multiple candidates increasingly surpasses the potential of either routing in model swarms (Yue et al., * Equal Contribution. Project Lead. Corresponding Authors. 1 Figure 1: Comparison of different LLM inference paradigms. While routing (efficiency) and RL (performance optimization) present promising approach, dynamic tool usage still faces significant challenges. 2025b) or tool augmentation (Dong et al., 2025) alone, highlighting the critical need for identifying the optimal model-tool combination. Recent advances have focused on different aspects of these reasoning engines separately. For tool usage, existing frameworks (Kong et al., 2024; Wu et al., 2024) improve performance through task planning, yet relying on fixed logic that cannot dynamically adapt to different model capabilities or task requirements. For LLM routing, methods like ZOOTER (Lu et al., 2024) and RouterDC (Chen et al., 2024) optimize model selection through reward-guided learning and dual contrastive learning. Likewise, frameworks such as HybridLLM (Ding et al., 2024) and RouteLLM (Ong et al., 2024) combine strong and weak models for cost efficiency. However, these routing methods treat models as isolated execution units and fail to incorporate external tools, which could significantly enhance task performance. For reinforcement learning (RL), methods such as RLHF (Ouyang et al., 2022) and PPO (Schulman et al., 2017) are explored to optimize reasoning capabilities in LLMs. RLAIF (Lee et al., 2023) and DPO (Rafailov et al., 2023) bypass explicit reward modeling, streamFigure 2: Performance comparison on in-distribution and out-of-distribution settings. Our ATLAS method consistently outperforms all baselines across diverse datasets, demonstrating superior generalization capability. lining preference learning. Additionally, RouterR1 (Zhang et al., 2025a) allows models to deliberate internally before invoking auxiliary models. Recent works (Chen et al., 2025; Jin et al., 2025a; Feng et al., 2025a) apply RL to tool usage, but miss the opportunity to integrate both models and tools to fully harness their combined strengths. As shown in Figure 1, existing methods neglect the dynamic interplay of tool usage, LLM routing and RL, thus falling short especially when faced with the emerging diversity of LLMs and tools. This fundamental limitation manifests in three key challenges: (1) Failure to leverage model-tool synergies: LLM routing methods focus solely on model selection without integrating external tools, limiting their potential to enhance task performance; (2) Rigid invocation and limited flexibility: Existing tool usage methods rely on fixed, preconfigured invocation logic that hinders adaptability and scalability, preventing reasoning engines from dynamically optimizing model-tool combinations in open-domain tasks; (3) Isolated optimization of RL: Even advanced RL approaches focus on optimizing individual components in isolation, missing opportunities to jointly leverage modeltool synergies for complex reasoning. To address these challenges, we propose ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), generalizable framework that dynamically orchestrates optimal model-tool combinations. Our approach employs dual-path approach to bridge the gap between empirical knowledge and open-domain reasoning. We firstly introduce training-free cluster-based routing that efficiently selects model-tool pairs by leveraging domain-specific expertise within semantic embedding space. This approach exploits historical performance patterns for rapid, accurate routing in familiar domains. For generalized scenarios where explicit priors are absent, we utilize RL-based multi-step routing that iteratively explores the model-tool combinations for superior execution paths. This bifurcated design effectively resolves the scalability challenges in high-dimensional search spaces while ensuring robustness. We conduct experiments on 15 benchmarks to evaluate the proposed ATLAS in both indistribution and out-of-distribution settings. Empirical results shown in Figure 2 reveal that ATLAS achieves superior performance across diverse tasks, which demonstrates its effectiveness as new paradigm for tool-augmented reasoning agents. Our primary contributions are as follows: We introduce ATLAS, generalizable agentic framework that explicitly optimizes heterogeneous synergies between diverse LLMs and tools, enabling dynamic and adaptive tool invocation for complex reasoning tasks. We propose dual-path design that handles both domain-specific and open-domain tasks: (1) training-free cluster-based routing for efficient selection using domain expertise, and (2) RL-driven multi-step routing for generalizing across unfamiliar tasks via iterative exploration. Experiments across 9 tasks and 15 benchmarks show that ATLAS outperforms top-performing closed-source LLMs and powerful routing methods on multi-domain tasks and exhibits robust adaptability in multi-modal scenarios."
        },
        {
            "title": "2 Related Work",
            "content": "Query-based LLM Routing. As the landscape of LLMs continues to evolve, query-based routing has become crucial in reasoning engines for balancing performance and computational efficiency by dynamically selecting the most appropriate model for each query. Early approaches rely on rewardguided (Lu et al., 2024) and contrastive learning strategies (Chen et al., 2024) to improve routing 2 Figure 3: Overview of Adaptive Tool-LLM Alignment and Synergistic Invocation (ATLAS). The framework operates via dual-path approach: (1) Training-free Cluster-based Routing; and (2) RL-driven Multi-step Routing. accuracy. Existing methods balance computational cost with performance through query-level orchestration (Ding et al., 2024; Ong et al., 2024; Zhang et al., 2025b), model cascading (Chen et al., 2023), adaptive selection (Feng et al., 2025b; Wang et al., 2025; Jin et al., 2025b), and budget allocation (Mei et al., 2025). Further, the integration of routing within reasoning frameworks (Yue et al., 2025b; Pan et al., 2025) enhances the performance boundaries. However, these existing methods often treat LLMs as isolated execution units, neglecting the synergies between specific model capabilities and external tool interfaces. Our approach addresses this gap by jointly optimizing model-tool combinations, enabling more adaptive, scalable, and effective reasoning engine capable of dynamically integrating the strengths of both models and tools. Reinforcement Learning for LLM. Reinforcement learning (RL) has been widely applied to optimize LLMs for aligning with complex human preferences and improving reasoning tasks. The paradigm has evolved from reward-model-based approaches like RLHF (Ouyang et al., 2022) and PPO (Schulman et al., 2017) to more efficient frameworks such as DPO (Rafailov et al., 2023), which bypass explicit reward modeling to streamline preference learning. RL has also been applied to optimize routing decisions, with approaches like Router-R1 (Zhang et al., 2025a) allowing models to deliberate internally before invoking auxiliary models. Recent works (Chen et al., 2025; Jin et al., 2025a; Feng et al., 2025a) investigate the application of RL for tool usage. While these methods reveal the potential of RL in optimizing reasoning trajectories, they primarily focus on single-model or single-tool optimization, overlooking the large potential for combined synergies. ATLAS extends this by employing RL to jointly optimize modeltool combinations, enabling more adaptive and efficient reasoning."
        },
        {
            "title": "3 Methodology",
            "content": "We present the ATLAS framework (Figure 3), which combines two-tier strategy: Training-free Cluster-based Routing ( 3.1) to enable quick decision-making; and RL-driven Multi-step Routing ( 3.2) to handle more complex open-domain tasks that require iterative model-tool interactions. 3.1 Training-Free Cluster-Based Routing We hypothesize that the optimal model-tool combination is query-dependent and exhibits semantic locality. Consequently, the empirical strategy approximates the optimal routing function : by leveraging historical metadata. We define the search space as the Cartesian product = , where = {m1, . . . , mM } denotes the set of candidate LLMs and = {t1, . . . , tT } represents the available tools. Given Qtrain = {qi}N i=1 denote the training queries set, we map each query qi into Ddimensional latent manifold using pre-trained encoder: vi = E(qi) RD. To capture the semantic task distribution, we partition the embedding space into disjoint clusters {Ck}K k=1 by minimizing the inertia: 3.2 RL-Driven Multi-Step Routing min {µk}K k=1 (cid:88) (cid:88) k=1 viCk vi µk2, (1) where µk represents the semantic centroid of cluster Ck. The clustering process effectively groups queries with similar reasoning requirements and tool affinities. We derive empirical statistics from the training observations for each modal-tool pair (m, t) within the cluster Ck. The empirical accuracy is defined as the success rate of the pair (m, t) on the cluster Ck: (cid:100)Acc (m,t) = 1 Ck (cid:88) qiCk 1(cid:2)(m, t) solves qi (cid:3), (2) where 1[] denotes the indicator function. Simultaneously, we model the operational cost to account for resource consumption, which is computed based on the average token throughput observed during the profiling phase: (cid:91)Cost (m,t) = (m,t) in (m,t) in + (m,t) out (m,t) out , (3) where Nin and Nout represent the mean input and output token counts for the cluster, while Pin and Pout denote their respective unit prices. To facilitate flexible trade-off between reasoning performance and inference cost, we define cluster-level utility score Uk(m, t) as: Uk(m, t) = (1α) (cid:100)Acc (m,t) α (cid:91)Cost (m,t) , (4) where α [0, 1] is hyperparameter that balances the performance-cost trade-off. At inference time, the framework performs lowlatency orchestration by projecting novel query qj into the latent manifold vj = E(qj). The routing is executed via proximal cluster lookup, where the query is assigned to = arg mink vj µk. Subsequently, the system retrieves the optimal modeltool pair for execution: (m, t) = arg max (m,t)S Uk(m, t). (5) By caching heterogeneous synergies within the embedding space, this empirical strategy enables real-time, cost-aware tool invocation with constanttime complexity relative to the number of clusters. 4 While the empirical strategy excels in low-latency routing, it is inherently limited by its reliance on single-shot decision. To address complex tasks that demand multi-round reasoning and iterative modeltool interactions, we introduce an RL-driven strategy that instantiates the router as an autonomous agent capable of interleaving internal reasoning with external invocation. We model this process as sequential decision task over maximum horizon Tmax. For given query qj, the agent maintains an evolving state st = {qj, Ct}, where Ct represents the accumulated context of previous reasoning trajectories and tool outputs. At each step t, the policy πθ samples an action at from the augmented action space A, comprising two types of operations: (1) Internal Reasoning (think), where the agent performs local chain-of-thought processing to decompose complex queries or synthesize intermediate results; and (2) Dynamic Routing (route), where the agent selects specific model-tool pair (m, t) from the routing pool to gather external observations ot. This iterative loop ensures that the agent can adaptively refine its search space based on real-time feedback from the environment until an answer is extracted or the maximum step limit is reached. To optimize this decision-making process, we train the policy π using Proximal Policy Optimization (PPO) (Schulman et al., 2017), which maximizes the following regularized objective: (cid:20) max π EqD,τ π rϕ(q, τ ) β log π(τ q; P) πref(τ q; P) (cid:21) , (6) where τ is the interaction trajectory, πref is reference policy to ensure training stability, and β is the KL-regularization coefficient. We design the reward function rϕ as composite of three finely-tuned rule-based signals (detailed in Appendix A.2) including format reward (Rfmt), outcome reward (Rout) and model selection reward (Rsel), bridging the gap between structured execution and task correctness, formally: Format Reward (Rfmt): signal enforces structural integrity by penalizing trajectories that deviate from the predefined format and tool-invocation syntax. Outcome Reward (Rout): binary signal that directly aligns the policy with task correctness. Model Selection Reward (Rsel): penaltybased signal guides the agent toward optimal efficiency by penalizing the selection of suboptimal models. The final reward is computed as: rϕ = Rfmt + γRout + ξRsel, (7) where γ and ξ are hyperparameters. This framework facilitates autonomous orchestration, as the model learns to assess the sufficiency of its internal state before invoking external resources. By decoupling routing logic via the Rsel signal, ATLAS internalizes the fundamental alignment between domains and tool utilization rather than memorizing rigid model-tool mappings. This design ensures that the routing policy captures the essential characteristics of expertise distribution, remaining robust and generalizable even as the available tools and models evolve in dynamic environments."
        },
        {
            "title": "4 Experiments",
            "content": "This section presents comprehensive evaluation of ATLAS, covering main results across multidomain benchmarks ( 4.2), multi-modal visual reasoning ( 4.3), model-tool pool extensions ( 4.4), and further analysis on reasoning boundaries, model-tool alignment preferences, and RL convergence dynamics ( 4.5). 4.1 Experimental Settings Models Selection. To evaluate ATLASs generalization across model architectures and scales, we select six heterogeneous open-source LLMs: Qwen2.5-7B-Instruct (Yang et al., 2024a), Llama3.1-8B-Instruct (Dubey et al., 2024), InternLM38B-Instruct (Cai et al., 2024), DeepSeek-R1-DistillQwen-7B (Guo et al., 2025), Qwen2.5-Coder-7BInstruct (Hui et al., 2024), and multi-modal LLM Qwen3-8B-VL-Instruct (Yang et al., 2025). This diverse selection allows us to observe how different models synergize with specific external tools. Tool Definition. We introduce two tool sets for textual and visual reasoning: computation; and (4) Process Reward Model (PRM) for scoring and ranking model outputs. Multi-modal Tools: (1) Qwen3-Chart for chart data extraction; (2) Qwen3-Counting for enumerating objects in images; (3) Qwen3Geo for parsing geometric properties and performing post-hoc self-verification of geometric proofs; and (4) Hunyuan-OCR (Team et al., 2025) for text extraction from images. The first three tools use Qwen3-8B-VL with taskspecific prompts, due to the underperformance of most existing specialized tools. Benchmarks and Baselines. We evaluate on multi-domain tasks: (1) mathematical reasoning: AIME2024 (MAA, 2024), AIME2025 (MAA, 2025), AMC (Lightman et al., 2023); (2) code generation: HumanEval (Chen, 2021), MBPP (Austin et al., 2021); (3) arithmetic reasoning: Calculator (Wu et al., 2025b); (4) commonsense reasoning: NQ (Kwiatkowski et al., 2019), WebQ (Berant et al., 2013); (5) logical reasoning: LogiQA2 (Liu et al., 2023); (6) scientific reasoning: GPQA (Rein et al., 2024). Furthermore, we extend our evaluations to multi-modal benchmarks, including ChartQA (Masry et al., 2022), Geometry3K (Lu et al., 2021), TallyQA (Acharya et al., 2019), CountBench (Paiss et al., 2023), and TableVQA (Kim et al., 2024). We use accuracy as the primary metric. Baselines include Zeroshot/Few-shot Router, Random Router, RouterDC (Chen et al., 2024), MLPRouter (Hu et al., 2024), and BertRouter (Ong et al., 2024). Details are provided in Appendix B. Implementation Details. For RL experiments, we adopt Qwen2.5-3B-Instruct as the policy model for model-tool selection. The policy is optimized with batch size of 32 for 250 training steps, and the learning rate is set to 1 106. More details are provided in Appendix B.4. 4.2 Main Results Table 1 presents comprehensive evaluation of our framework against various routing baselines across in-distribution and out-of-distribution tasks. Foundation Tools: This set includes four essential tools: (1) Code Interpreter, Python execution environment for algorithmic and logical verification; (2) Web Search for retrieving real-time open-domain information; (3) Calculator for high-precision numerical In-Distribution Performance 4.2.1 Under the in-distribution setting, where training data for all tasks is accessible, ATLAS(cluster) achieves 63.5% average accuracy, surpassing the strongest baseline RouterDC by 10.1%. This advantage is pronounced on rigorous mathematical 5 Table 1: Performance comparison across diverse tasks and domains. In-Distribution: All datasets have training data available, so evaluation is in-distribution. Out-of-Distribution: Models are trained only on Calc., NQ, and MBPP (in-distribution, marked as ), then evaluated on all datasets (out-of-distribution for AIME24, AIME25, AMC, HumanEval, WebQ, LQA2, and GPQA). Zero-shot Router uses direct prompting without examples, while Few-shot Router uses prompting with examples. The best results are highlighted in bold. Method Gemini2.5-Pro GPT-5 GPT-4.1 GPT-4o ZS Router FS Router Random Router RouterDC MLPRouter BertRouter ATLAS (cluster) RouterDC MLPRouter BertRouter ATLAS (cluster) ATLAS (RL) Math Reasoning Code Arith. Common. Logic Sci. AIME24 AIME25 AMC Human. MBPP Calc. NQ WebQ LQA2 GPQA 92.0 93.3 46.7 13.3 13.3 23.3 6. 40.0 26.7 30.0 43.3 13.3 13.3 6.7 13.3 43.3 86.7 94.6 33.3 6.7 6.7 13.3 3.3 23.3 10.0 13.3 40.0 3.3 3.3 6.7 3.3 33. Closed-Source Models 81.5 93.4 92.1 85.4 83.7 98.4 57.7 82.6 Training-free Baselines 53.0 68.9 37.8 64.2 64.7 52. 64.7 82.9 62.0 58.1 55.7 47.2 40.2 In-Distribution Performance 80.5 76.2 75.4 91.5 77.7 68.7 72.1 83.6 74.9 48.2 77.1 83. 62.5 97.5 82.5 45.8 32.5 40.0 15.0 62.5 45.0 45.0 82.5 Out-of-Distribution Performance 70.8 54.6 67.0 83.3 81.6 78.7 67.7 79.0 83.6 81.8 79.2 75.0 78.7 91.5 85. 47.5 32.5 40.0 47.5 67.5 59.2 59.3 54.5 59.4 29.2 27.3 25.3 41.2 32.1 38.9 43.8 40.1 37.3 38.9 43.8 44.1 63.5 61.5 61.5 63. 39.2 35.8 32.1 47.6 40.4 50.4 53.6 50.8 43.7 51.4 51.4 52.2 78.9 83.8 78.2 72.9 45.3 40.8 49.2 47.2 41.2 47.1 66. 50.4 38.9 40.3 45.6 62.7 84.0 85.7 62.1 44.4 24.6 25.9 30.6 39.1 34.8 36.6 46.4 28.6 26.8 27.7 29.0 42.0 Avg. 75.6 85.0 63.0 53.1 36.4 38.7 29.3 53.4 42.3 48.6 63.5 46.3 39.3 43.6 49.2 59.4 reasoning: ATLAS achieves 40.0% on AIME25 and 82.5% on AMC (+16.7% and +20.0% over RouterDC). Notably, ATLAS(cluster) exceeds GPT-4o (53.1%) and approaches GPT-4.1 (63.0%), demonstrating that strategic modeltool orchestration enables reasoning engine of smaller-scale models to rival larger proprietary systems. This performance stems from exploiting rich empirical priors through semantic embedding. By mapping queries into structured clusters and caching historical performance patterns, the framework achieves near-optimal task-configuration In contrast, supervised routers like alignment. BertRouter and MLPRouter struggle with nonlinear decision boundaries in heterogeneous modeltool spaces. Their classification-based selection fails to capture nuanced synergies from domainspecific pairings, resulting in suboptimal routing. 4.2.2 Generalization Scenarios When facing out-of-distribution (OOD) challenges, ATLAS(cluster) suffers significant degradation (e.g., dropping from 40.0% to 3.3% on AIME25) as well as other baselines, whereas ATLAS(RL) maintains an average accuracy of 59.4% with 10.2% higher than ATLAS(cluster) (49.2%) and 13.1% higher than RouterDC (46.3%). The gap is most striking in mathematical reasoning: on AIME24 and AIME25, ATLAS(RL) sustains 43.3% and 33.3% accuracy, respectively, while the clustering method achieves only 13.3% and 3.3% (a 10 difference). This indicates that the RL path learns transferable collaborative decision principles rather than taskspecific mappings. ATLAS(RL) autonomously explores effective trajectories through multi-faceted reward signals, learning generalizable patterns of model-tool synergies: when to invoke symbolic tools for verification or route to reasoning-specialized models rather than memorizing task-specific mappings. This enables robust transfer, maintaining competitive performance on unfamiliar tasks like AIME24 (43.3%) and GPQA (42.0%), approaching or exceeding GPT-4o despite using only 7B and 8B models. These results confirm that RL-driven component provides essential generalization capability, effectively bridging established domain expertise and unseen reasoning challenges. 4.3 Multi-modal Tool Orchestration To evaluate ATLAS on multi-modal tasks, we benchmark it against single-tool baselines across five visual understanding and reasoning datasets, as shown in Figure 4. ATLAS achieves an average ac6 Table 2: Performance evaluation under dynamic routing pool extensions. denotes results after integrating domain-specialized models (Llama-3.1-8B-UltraMedical, Qwen2.5-Math-7B-Instruct) and an Outcome Reward Model into the routing pool. marks in-domain benchmarks; all others are out-of-domain. Best results are in bold. Method ZS Router ZS Router FS Router FS Router RandomRouter RandomRouter BertRouter BertRouter ATLAS (RL) ATLAS (RL) Math Reasoning Code Arith. Common. Logic Sci. AIME24 AIME25 AMC Human. MBPP Calc. NQ WebQ LQA2 GPQA 13.3 20.0 23.3 26. 6.7 3.3 26.7 33.3 43.3 50.0 6.7 13.3 13.3 16.7 3.3 3. 16.7 20.0 33.3 40.0 32.5 37.5 40.0 47.5 15.0 17.5 42.5 50. 67.5 70.0 53.0 52.4 68.9 70.7 37.8 35.4 76.8 75.0 85.4 84. 64.2 63.1 64.7 63.8 52.6 52.0 72.6 73.0 81.8 81.8 55.7 55. 47.2 46.5 40.2 41.3 62.7 61.3 81.6 82.4 29.2 28.7 27.3 25. 25.3 22.7 35.4 36.2 44.1 45.3 39.2 38.9 35.8 36.2 32.1 31. 49.8 50.1 52.2 52.8 45.3 45.9 40.8 41.7 49.2 49.8 52.5 53. 62.7 64.8 24.6 25.7 25.9 25.0 30.6 30.1 33.3 32.4 42.0 45. Avg. 36.4 38.0 38.7 40.0 29.3 28.7 46.9 48.4 59.4 61. reasoning, Qwen2.5-Math-7B-Instruct (Yang et al., 2024b) for mathematical problem-solving, and an Outcome Reward Model for solution verification. Notably, our policy is trained exclusively on the original pool of 5 models and 4 tools; the newly added components are introduced only at inference time without any retraining. This extension substantially increases the combinatorial search space, posing more challenging routing problem. As shown in Table 2, ATLAS(RL) exhibits strong adaptability, improving from 59.4% to 61.7% (+2.3%) after pool extension. Gains are most pronounced on mathematical benchmarks: AIME24 (+6.7%) and AIME25 (+6.7%), confirming effective utilization of the newly added mathspecialized model and verification tool. In contrast, baseline methods show limited or inconsistent responses: BertRouter gains only +1.5%, while RandomRouter degrades due to the expanded search space. This disparity arises because classifier-based routers learn fixed decision boundaries that become misaligned with new candidates, whereas ATLAS learns transferable routing principles through RL exploration, enabling seamless integration of new components without retraining. 4.5 Discussion Figure 4: Performance comparison of ATLAS against single-tool baselines across multi-modal benchmarks. None denotes direct reasoning without any tools. ATLAS achieves the highest accuracy. curacy of 68.9% through dynamic tool invocation, outperforming the strongest single-tool baseline by 4.3%. Notably, ATLAS surpasses all individual tool in each task category. For example, exceeding the best single tool (Qwen3-Chart, 83.0%) on ChartQA and overcoming the performance limitations of single tools (e.g., Qwen3-Chart only achieves 50.2% on Geometry3K). This reveals that adaptive model-tool routing effectively integrates internal reasoning with external tool augmentation, thus establishing strong effectiveness on complex multi-modal tasks. Detailed results are provided in Appendix C.2. 4.4 Generalization Toward Dynamic Model-Tool Synergy practical orchestration framework must accommodate an evolving ecosystem where new models and tools are continuously introduced. To evaluate this extensibility, we expand the routing pool with three additional components: Llama-3.1-8BUltraMedical (Zhang et al., 2024) for biomedical Evaluation on Reasoning Capacity Boundary. Inspired by (Yue et al., 2025a), we implement the pass@k metric to measure the reasoning capacity boundary of ATLAS(RL), where pass@k equals 1 if at least one of sampled outputs passes verification. As shown in Table 3, RL training yields an absolute improvement of +23.0% in pass@1 accuracy (from 36.4% to 59.4%), demonstrating 7 Table 3: Reasoning capacity boundary analysis of ATLAS(RL). We report the pass@k metrics across diverse benchmarks to evaluate the exploration (k = 1) and the potential reasoning upper bound (k = 16). Math Reasoning Code Arith. Common. Logic Sci. AIME AIME25 AMC Human. MBPP Calc. NQ WebQ LQA2 GPQA w/o w/o 13.3 43.3 +30.0 16.7 50.0 +33. 6.7 33.3 +26.6 13.3 36.7 +23.4 Pass@1 Results with/without ATLAS RL Training 32.5 67.5 +35.0 53.0 85.4 +32.4 64.2 81.8 +17. 55.7 81.6 +25.9 29.2 44.1 +14.9 39.2 52.2 +13.0 Pass@16 Results with/without ATLAS RL Training 40.0 75.0 +35.0 73.1 89.6 +16. 73.9 84.5 +10.6 70.6 83.3 +12.7 36.8 46.9 +10.1 48.8 54.9 +6.1 45.3 62.7 +17.4 47.0 64.4 +17. 24.6 42.0 +17.4 27.2 45.8 +18.6 Avg. 36.4 59.4 +23.0 44.7 63.1 +18.4 (a) Average LLM API Calls (b) Reward Convergence (c) Entropy Loss Figure 5: Analysis of LLM API call count and ATLAS(RL) training dynamics. significantly optimized exploration efficiency. At pass@16, the upper bound reaches 63.1% (+3.7%), indicating that ATLAS(RL) already operates near its reasoning capacity ceiling, efficiently converging to optimal solutions without requiring extensive sampling. The trained model maintains substantial advantages across all tasks even at pass@16, with gains ranging from +6.1% to +35.0%, confirming that RL training effectively enhances agentic reasoning potential. Analysis of LLM API Call Count. As illustrated in Figure 5a, ATLAS exhibits highly task-adaptive invocation patterns. For challenging reasoningintensive tasks like AIME25 and GPQA, API calls increase significantly as the RL policy allocates higher computational budgets through multi-round routing and verification. Conversely, for straightforward retrieval tasks like WebQ and NQ, call counts remain minimal. This differentiated distribution confirms that ATLAS balances reasoning performance and inference cost, effectively suppressing redundant invocations where simpler models or fewer rounds suffice. Analysis of RL Training Convergence. We validate the RL-driven routing policys stability through reward and entropy evolution during training. Figure 5b shows that incorporating model selection reward (Rsel) yields faster convergence to higher plateau compared to the baseline, guiding the agent toward higher-yield decision regions. Figure 5c demonstrates that the ATLAS configuration achieves much sharper reduction in entropy compared to the ablation group, reaching lower terminal value. This indicates that the router successfully transitions from stochastic exploration to deterministic, high-confidence decision-making state, ensuring both the robustness and predictability of the routing process. More Discussion. Due to space, we include more discussions in Appendix, including detailed multimodal results (C.2), test-time scaling (C.3), analysis of model-tool preferences (C.4), ablation on reward design (C.5), and sensitivity analysis (C.6)."
        },
        {
            "title": "5 Conclusion",
            "content": "We present ATLAS, generalizable framework for dynamic model-tool alignment through dual-path architecture: training-free cluster-based routing for domain-specific efficiency and RL-driven exploration for open-domain adaptability. ATLAS rivals or exceeds powerful closed-source models across diverse benchmarks, demonstrating paradigm shift from model-centric scaling to ecosystemcentric orchestration. Experimental results show 8 that strategic coordination of heterogeneous modeltool combinations unlocks superior reasoning while maintaining efficiency. As model and tool ecosystems continue to evolve, such orchestration reasoning systems will become essential for nextgeneration autonomous agents that address complex real-world challenges."
        },
        {
            "title": "Limitations",
            "content": "While ATLAS demonstrates strong performance across diverse benchmarks, several limitations warrant discussion. First, our current evaluation focuses primarily on text-based and visual reasoning tasks; extending to other modalities (e.g., audio, video) remains unexplored. Second, our framework assumes reliable API access to candidate models and tools-network latency or service unavailability in real-world deployments may impact performance. We plan to investigate more lightweight policy architectures and robust fallback mechanisms in future work."
        },
        {
            "title": "Ethical Considerations",
            "content": "All datasets, models, and tools utilized in this work are derived from publicly available resources with proper citations, involving no private or sensitive information. ATLAS consists of two components: training-free cluster-based router and an RL-trained policy model that learns to orchestrate existing LLMs and tools. While the policy model is trained to make routing decisions, the underlying candidate models and tools remain unmodified. Consequently, our framework inherits the potential biases, safety limitations, and ethical concerns present in these constituent components. ATLAS itself does not introduce new harmful capabilities beyond those already existing in the routing pool. We recommend that practitioners carefully evaluate all candidate models and tools for compliance with ethical guidelines, and apply appropriate safety measures when deploying ATLAS in realworld applications."
        },
        {
            "title": "References",
            "content": "Manoj Acharya, Kushal Kafle, and Christopher Kanan. 2019. Tallyqa: Answering complex counting quesIn Proceedings of the AAAI conference on tions. artificial intelligence, volume 33, pages 80768084. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 15331544, Seattle, Washington, USA. Association for Computational Linguistics. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. 2024. Internlm2 technical report. arXiv preprint arXiv:2403.17297. Guoxin Chen, Zhong Zhang, Xin Cong, Fangda Guo, Yesai Wu, Yankai Lin, Wenzheng Feng, and Yasheng Wang. 2025. Learning evolving tools for large language models. In The Thirteenth International Conference on Learning Representations. Lingjiao Chen, Matei Zaharia, and James Zou. 2023. Frugalgpt: How to use large language models while reducing cost and improving performance. Transactions on Machine Learning Research. Mark Chen. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Shuhao Chen, Weisen Jiang, Baijiong Lin, James Kwok, and Yu Zhang. 2024. Routerdc: Query-based router by dual contrastive learning for assembling large language models. Advances in Neural Information Processing Systems, 37:6630566328. Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Rühle, Laks V. S. Lakshmanan, and Ahmed Hassan Awadallah. 2024. Hybrid LLM: Cost-efficient and quality-aware query routing. In The Twelfth International Conference on Learning Representations. Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. 2025. Tool-star: Empowering llm-brained multi-tool reasoner via reinforcement learning. arXiv preprint arXiv:2505.16410. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv e-prints, pages arXiv2407. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. 2025a. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536. Tao Feng, Yanzhen Shen, and Jiaxuan You. 2025b. Graphrouter: graph-based router for LLM selections. In The Thirteenth International Conference on Learning Representations. 9 Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing. In The Eleventh International Conference on Learning Representations. Qitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang, Benjamin Keigwin, Gaurav Ranganath, Kurt Keutzer, and Shriyash Kaustubh Upadhyay. 2024. Routerbench: benchmark for multi-LLM routing system. In Agentic Markets Workshop at ICML 2024. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025a. Search-r1: Training LLMs to reason and leverage search engines with reinforcement learning. In Second Conference on Language Modeling. Ruihan Jin, Pengpeng Shao, Zhengqi Wen, Jinyang Wu, Mingkuan Feng, Shuai Zhang, and Jianhua Tao. 2025b. Radialrouter: Structured representation for efficient and robust large language models routing. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1458714600, Suzhou, China. Association for Computational Linguistics. Yoonsik Kim, Moonbin Yim, and Ka Yeon Song. 2024. Tablevqa-bench: visual question answering benchmark on multiple table domains. arXiv preprint arXiv:2404.19205. Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao, Shi Shiwei, Xiaoru Hu, Hangyu Mao, Ziyue Li, Xingyu Zeng, et al. 2024. Tptu-v2: Boosting task planning and tool usage of large language model-based agents in real-world industry systems. In Proceedings of the 2024 conference on empirical methods in natural language processing: industry track, pages 371385. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453 466. Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, and Yue Zhang. 2023. Logiqa 2.0an improved dataset for logical reasoning in natural language understanding. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31:29472962. Keming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2024. Routing to the expert: Efficient reward-guided ensemble of large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 19641974. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. 2021. Inter-GPS: Interpretable geometry problem solving with formal language and symbolic reasoning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6774 6786, Online. Association for Computational Linguistics. Zexiong Ma, Chao Peng, Qunhong Zeng, Pengfei Gao, Yanzhen Zou, and Bing Xie. 2025. Tool-integrated reinforcement learning for repo deep search. arXiv preprint arXiv:2508.03012. MAA. 2024. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2024. MAA. 2025. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2025. Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the association for computational linguistics: ACL 2022, pages 2263 2279. Kai Mei, Wujiang Xu, Shuhang Lin, and Yongfeng Zhang. 2025. Omnirouter: Budget and performance controllable multi-llm routing. arXiv preprint arXiv:2502.20576. Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. 2023. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267. Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. 2024. Routellm: Learning to route llms from preference data. In The Thirteenth International Conference on Learning Representations. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel. 2023. Teaching clip to count to ten. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 31703180. Zhihong Pan, Kai Zhang, Yuze Zhao, and Yupeng Han. 2025. Route to reason: Adaptive routing for llm and reasoning strategy selection. arXiv preprint arXiv:2505.19435. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741. Dinesh Reddy and Sudeep Pillai. 2025. Orion: unified visual agent for multimodal perception, advanced visual reasoning and execution. arXiv preprint arXiv:2511.14210. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Hunyuan Vision Team, Pengyuan Lyu, Xingyu Wan, Gengluo Li, Shangpin Peng, Weinong Wang, Liang Wu, Huawen Shen, Yu Zhou, Canhui Tang, et al. 2025. Hunyuanocr technical report. arXiv preprint arXiv:2511.19575. Chenxu Wang, Hao Li, Yiqun Zhang, Linyao Chen, Jianhao Chen, Ping Jian, Peng Ye, Qiaosheng Zhang, and Shuyue Hu. 2025. Icl-router: In-context learned model representations for llm routing. arXiv preprint arXiv:2510.09719. Jinyang Wu, Mingkuan Feng, Shuai Zhang, Fangrui Lv, Ruihan Jin, Feihu Che, Zengqi Wen, and Jianhua Tao. 2025a. Boosting multimodal reasoning with automated structured thinking. arXiv preprint arXiv:2502.02339. Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Kaidi Cao, Vassilis Ioannidis, Karthik Subbian, Jure Leskovec, and James Zou. 2024. Avatar: Optimizing llm agents for tool usage via contrastive reasoning. Advances in Neural Information Processing Systems, 37:2598126010. Wenxun Wu, Yuanyang Li, Guhan Chen, Linyue Wang, and Hongyang Chen. 2025b. Tool-augmented policy optimization: Synergizing reasoning and adaptive tool use with reinforcement learning. arXiv preprint arXiv:2510.07038. Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin Li. 2024. Large multimodal agents: survey. arXiv preprint arXiv:2402.15116. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024a. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. 2024b. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. 2025a. Does reinforcement learning really incentivize reasoning capacity in LLMs beyond the base model? In The Thirty-ninth Annual Conference on Neural Information Processing Systems. Yanwei Yue, Guibin Zhang, Boyang Liu, Guancheng Wan, Kun Wang, Dawei Cheng, and Yiyan Qi. 2025b. MasRouter: Learning to route LLMs for multi-agent systems. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1554915572, Vienna, Austria. Association for Computational Linguistics. Haozhen Zhang, Tao Feng, and Jiaxuan You. 2025a. Router-r1: Teaching llms multi-round routing and aggregation via reinforcement learning. In The Thirtyninth Annual Conference on Neural Information Processing Systems. Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui, Biqing Qi, Xuekai Zhu, Xingtai Lv, Hu Jinfang, Zhiyuan Liu, and Bowen Zhou. 2024. Ultramedical: Building specialized generalists in biomedicine. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Yiqun Zhang, Hao Li, Chenxu Wang, Linyao Chen, Qiaosheng Zhang, Peng Ye, Shi Feng, Daling Wang, Zhen Wang, Xinrun Wang, Jia Xu, Lei Bai, Wanli Ouyang, and Shuyue Hu. 2025b. The avengers: simple recipe for uniting smaller language models to challenge proprietary giants. arXiv preprint arXiv:2505.19797. 11 Richard Zhuang, Tianhao Wu, Zhaojin Wen, Andrew Li, Jiantao Jiao, and Kannan Ramchandran. 2025. EmbedLLM: Learning compact representations of large language models. In The Thirteenth International Conference on Learning Representations. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, Biqing Qi, Youbang Sun, Zhiyuan Ma, Lifan Yuan, Ning Ding, and Bowen Zhou. 2025. TTRL: Test-time reinforcement learning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems."
        },
        {
            "title": "Contents",
            "content": "A Details of Methodology tions . A.1 Complete Algorithm Implementa- . . . . A.2 Detailed Specification of Reward . . . . Signals . . . . . . . . . . . . . . . . . . . . . . Additional Experimental Details . . . . . B.1 Datasets . . B.2 Baselines . . . B.3 Evaluation Details. B.4 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . More Results and Analysis C.1 Details Main Results . . . C.2 Detailed Multimodal Results . . . C.3 Test-Time Scaling Results . . . C.4 Analysis of Model-Tool Alignment . . . C.5 Ablation Study on Reward Compo- . . . . C.6 Sensitivity Analysis on Cluster . . . . Preferences Number nents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Case Study Additional Discussion E.1 Distinguishing ATLAS from Prior Routing and Tool Usage Methods E.2 Discussion on RL Reward Design E.3 When to Use Cluster-Based vs. . . RL-Based Routing . . . . . . 12 12 13 13 16 16 16 18 18 18 18 18 19 20 20 20 21"
        },
        {
            "title": "A Details of Methodology",
            "content": "A.1 Complete Algorithm Implementations We provide detailed implementations about training-free cluster-based routing (Algorithm 1), and RL-driven Multi-step Routing (Algorithm 2). A.2 Detailed Specification of Reward Signals To bridge the gap between structured interaction and task-specific accuracy, ATLAS employs composite reward function rϕ = Rfmt + γRout + ξRsel. This section provides the formal definitions and criteria for each reward component. Format Reward (Rfmt) The format reward ensures that the RL agent adheres to the predefined syntactic protocols, which is essential for stable parsing and environment interaction. Rfmt is set to 0 if all the following conditions are satisfied, and 1 otherwise: Tag Integrity: All XML-style tags (e.g., <think>, <route>, and <answer>) must be correctly opened and closed in nested or sequential manner. Invocation Syntax: Tool calls within the search block must strictly follow the format Model-Name@@Tool-Name:Input. Furthermore, the specified model and tool names must exist within the active routing pool P. Mandatory Reasoning: trajecThe least one complete tory must contain at <think>...</think> ensure to block internal deliberation before an action or answer. Uniqueness of Response: The trajectory must conclude with exactly one <answer>...</answer> block. Execution Consistency: To maintain the integrity of the multi-step interaction, the number of search calls initiated by the agent must strictly match the number of information blocks returned by the environment. Outcome Reward (Rout) The outcome reward serves as the primary signal for task success. It is binary indicator evaluated upon the completion of the trajectory: Rout = (cid:40) 1, 0, if the answer yj is correct, otherwise. (8) Algorithm 1 Training-free Cluster-based Routing Input: Test query qj, cluster centroids {µ1, . . . , µK}, historical performance statistics Stats[k][(m, t)], performance-cost trade-off parameter α; Output: Optimal model-tool pair (m, t) and the generated response yj; // Step 1: Query Representation 1: vj Embed(qj) // Step 2: Semantic Clustering 2: arg mink{1,...,K} dist(vj, µk) // Step 3: Dynamic Selection 3: if Stats[k] is not empty then 4: for each candidate pair (m, t) do Project query into the latent embedding manifold Find the nearest semantic cluster centroid (m,t) (1 α) Accuracy(Stats[k]) α Cost(Stats[k]) 5: 6: end for (m, t) arg max(m,t) (m,t) 7: 8: else 9: 10: end if 11: yj Execute(m, t, qj) 12: return yj (m, t) FallbackStrategy(qj) Select the optimal combination Handle out-of-distribution queries Invoke the selected model with the specific tool Model Selection Reward (Rsel) To encourage the agent to select the most efficient and capable expert for given domain, we introduce an alignmentbased penalty. The optimal model for each task is pre-determined as follows: For the MBPP dataset, the optimal model is defined as Qwen2.5-Coder-7B-Instruct. For the Calculator and NQ datasets, the optimal model is identified via an offline evaluation where GPT-4o judges the best-performing candidate from the pool for each specific query. The reward is then formulated to penalize suboptimal invocations: Rsel = (cid:40) 0, 0.15, if select the optimal model, otherwise. (9)"
        },
        {
            "title": "B Additional Experimental Details",
            "content": "B.1 Datasets The datasets utilized in this paper are summarized in Table 4. Below, we provide detailed descriptions of each benchmark to illustrate the diverse reasoning capabilities required by our framework. Mathematical Reasoning. AIME 2024 & AIME 2025 (MAA, 2024, 2025): The American Invitational Mathematics Examination (AIME) is prestigious 15-question, 3-hour test designed for highperforming high school students. We evaluate on the 2024 and 2025 editions, each containing 30 problems that demand advanced problemsolving skills, strategic thinking, and precise numerical computation. AMC (Lightman et al., 2023): The American Mathematics Competitions (AMC) consist of multiple-choice problems ranging from elementary to intermediate difficulty. Our evaluation set includes 40 problems that assess fundamental mathematical reasoning and computational proficiency. Arithmetic Reasoning. Calculator (Wu et al., 2025b): benchmark containing 1,000 complex arithmetic problems requiring precise numerical computation. These problems test the models ability to recognize when external calculation tools are necessary and to correctly formulate and interpret computational results, evaluating the integration of reasoning and tool invocation. Code Generation. HumanEval (Chen, 2021): This benchmark comprises 164 hand-crafted programming prob13 Algorithm 2 RL-driven Multi-step Routing Input: Query qj, policy πθ, reference πref, pool P, parameters Tmax, θ, β, γ, ξ; Output: Response yj and trajectory τ ; // Step 1: Initialization 1: τ , C0 , s0 {qj, C0} // Step 2: Multi-step Reasoning Loop 2: for = 0 to Tmax 1 do 3: at πθ( st, P) if at = think then 4: 5: 6: 7: 8: 9: 10: 16: ot πθ.Reasoning(st) else ot Execute(m, ttool, st) end if Ct+1 Ct {at, ot}, st+1 {qj, Ct+1} τ τ {(st, at, ot)} if at contains Final Answer then break 11: 12: end for 13: yj ParseAnswer(τ ) // Step 3: Policy Update (Training Mode) 14: if training_mode then 15: rϕ(τ ) Rfmt + γRout + ξRsel Lθ Update θ via PPO update rule: θLθ (cid:104) rϕ(τ ) log πθ(τ ) β log πθ(τ ) πref(τ ) 17: 18: end if 19: return (yj, τ ) Action at {think, route(m, ttool)} Internal reasoning Dynamic routing and tool invocation Answer extraction (cid:105) lems designed to evaluate code synthesis capabilities. Each problem includes function signature, docstring, body, and unit tests. Solutions require understanding natural language specifications and generating functionally correct Python code. MBPP (Austin et al., 2021): The Mostly Basic Programming Problems (MBPP) dataset contains 974 crowd-sourced Python programming problems designed for entry-level programmers. Problems are described in natural language and require generating short Python functions, typically 1-10 lines of code. This benchmark tests basic programming constructs including loops, conditionals, and string manipulation. Commonsense Reasoning. Natural Questions (NQ) (Kwiatkowski et al., 2019): question-answering dataset containing real user queries issued to Google Search. Questions span diverse topics and require retrieving and synthesizing information from Wikipedia articles. This benchmark evaluates knowledge-intensive reasoning and information retrieval capabilities. Web Questions (WebQ) (Berant et al., 2013): dataset of 1,000 questions designed to test knowledge-based question answering. Questions are sourced from web search queries and require retrieving factual information from knowledge bases, evaluating the models ability to access and reason over external knowledge sources. Logical Reasoning. LogiQA2 (Liu et al., 2023): An improved version of LogiQA containing 1,572 multiplechoice logical reasoning problems. Questions are sourced from standardized exams and require identifying logical relationships, drawing inferences, and evaluating argument structures. This benchmark tests formal reasoning capabilities including deductive, inductive, and abductive reasoning. Scientific Reasoning. 14 Table 4: Detailed information on the datasets and test set sizes used in our experiments. Category Dataset #Test Samples Mathematical Reasoning Code Generation AIME 2024 (MAA, 2024) AIME 2025 (MAA, 2025) AMC (Lightman et al., 2023) HumanEval (Chen, 2021) MBPP (Austin et al., 2021) Arithmetic Reasoning Calculator (Calc.) (Wu et al., 2025b) Commonsense Reasoning Natural Questions (NQ) (Kwiatkowski et al., 2019) Web Question (WebQ) (Berant et al., 2013) Logical Reasoning LogiQA2 (Liu et al., 2023) Scientific Reasoning GPQA (Rein et al., 2024) Multi-modal Perception and Reasoning ChartQA (Masry et al., 2022) Geometry3K (Lu et al., 2021) TallyQA (Acharya et al., 2019) CountBench (Paiss et al., 2023) TableVQA (Kim et al., 2024) 30 30 40 164 974 1000 1200 1000 448 500 601 498 491 500 GPQA (Rein et al., 2024): The GraduateLevel Google-Proof Q&A benchmark consists of 448 multiple-choice questions across biology, physics, and chemistry, written by domain experts with PhD-level knowledge. Questions are designed to be difficult even for experts and require deep domain understanding beyond simple fact retrieval. Multi-modal Reasoning. ChartQA (Masry et al., 2022): visual question-answering benchmark containing questions about various chart types (bar charts, line graphs, pie charts). Questions require extracting quantitative information from visual representations and performing numerical reasoning, testing the integration of visual perception and mathematical computation. Geometry3K (Lu et al., 2021): comprehensive geometry problem-solving dataset comprising multiple problems with diagram annotations. Problems involve diverse geometric concepts including angles, areas, perimeters, and spatial relationships. This benchmark evaluates visual-geometric reasoning and the ability to apply mathematical principles to diagrammatic representations. TallyQA (Acharya et al., 2019): visual counting dataset containing complex counting questions across diverse real-world images. Questions range from simple object counting to complex scenarios requiring spatial reasoning and selective attention. This benchmark tests finegrained visual perception and numerical reasoning capabilities. CountBench (Paiss et al., 2023): specialized counting benchmark with questions designed to evaluate precise object enumeration in images. Unlike traditional counting tasks, CountBench emphasizes accuracy on challenging cases involving occlusions, similar objects, and cluttered scenes, requiring robust visual understanding. TableVQA (Kim et al., 2024): visual question-answering benchmark containing questions about tables across multiple domains. Questions require understanding table structures, extracting relevant information, and performing reasoning over tabular data, evaluating the integration of visual perception and structured data comprehension. These diverse benchmarks collectively assess the frameworks ability to dynamically select optimal model-tool combinations across varying task requirements, ranging from symbolic mathematical reasoning to multi-modal visual understanding. B.2 Baselines In our experiments, we compare the proposed methods against six baseline approaches. Below, we provide detailed descriptions of each baselines. Zero-shot (ZS) Router: baseline that directly prompts base LLM to select the most suitable candidate model-tool combination from the available pool without prior examples. Few-shot (FS) Router: An extension of the zero-shot approach that incorporates several in-context examples to provide the base LLM with task-specific demonstrations and routing guidance. Random Router: stochastic baseline that selects candidate model-tool combination uniformly at random from the candidate pool for each query. RouterDC (Chen et al., 2024): routing framework based on dual contrastive learning that maps queries and model-tool combinations into shared embedding space. It utilizes sample-LLM and sample-sample contrastive losses to optimize query-model alignment and selects the optimal combination via cosine similarity. MLPRouter (Hu et al., 2024): classification-based framework that trains an MLP for each model-tool combination. Each MLP predicts the success probability of its corresponding combination, and the one with the highest output is selected. BertRouter (Ong et al., 2024): router utilizing pre-trained mDeBERTaV3-base encoder (He et al., 2021) with an integrated classification head to predict the accuracy of model-tool pairings, following selection logic similar to MLPRouter. B.3 Evaluation Details. Our experiments employ two evaluation protocols: In-Distribution (ID), where each dataset has its own training split, and Out-of-Distribution (OOD), where models are trained exclusively on three datasets (Calculator, NQ, MBPP) and evaluated on all ten benchmarks, making AIME24, AIME25, AMC, HumanEval, WebQ, LogiQA2, and GPQA fully out-of-domain. For cluster-based 16 routing in OOD settings, semantic clusters and performance statistics are derived solely from the three training datasets; test queries from unseen domains are assigned to the nearest cluster based on semantic similarity, without accessing any OOD test set information. This design reflects realistic domainspecific scenarios but inevitably suffers from cluster misalignment on unfamiliar tasks (49.2% OOD vs. 63.5% ID, Table 1). In contrast, RL-based routing learns transferable patterns (when to invoke symbolic tools or defer to specialized models) that generalize beyond training distributions, achieving 59.4% OOD accuracy. Importantly, no test set information is leaked: all routing decisions rely purely on query embeddings and training domain statistics, ensuring evaluation integrity and demonstrating that gains stem from our dual-path architectures complementary strengths. B.4 Implementation Details Hyperparameters for Cluster-based Routing. We set the number of cluster centers to 8 and employ the KMeans algorithm with the following hyperparameters: the cluster centers are initialized using the k-means++ method to accelerate convergence; the algorithm is allowed up to 1000 iterations per run; the number of initializations is set to automatic selection, the hyperparameter α in Equation 4 is set to 0.5; and the Elkan variant of KMeans is used for computational efficiency. Hyperparameters for RL Training. We train the policy model (Qwen2.5-3B-Instruct) using PPO with generalized advantage estimation (GAE). The training and validation batch sizes are both set to 24. The maximum prompt length is 4096 tokens, while the maximum response length is set to 3000 tokens. To control context growth, the maximum lengths for the observations are set to 2048 tokens each, and the maximum number of interaction turns is limited to 4. The actor is optimized with learning rate of 1 106, while the critic uses learning rate of 1105. The PPO mini-batch size and micro-batch size for the actor are set to 12 and 6, respectively. The KL-divergence coefficient is fixed to 0.001. During rollout, we use temperature of 1.0. For the reward weights in rϕ = Rfmt + γRout + ξRsel, we assign γ = ξ = 1. All experiments are conducted for 250 total training steps. We also provide the RL system prompt in Figure 6. Tool Details. We provide the system prompt for three special multimodal tools in Figure 7-9. ReTable 5: Extended performance comparison across diverse tasks and domains. In-Distribution: All datasets have training data available, so evaluation is in-distribution. Out-of-Distribution: Models are trained only on Calc., NQ, and MBPP (in-distribution, marked as ), then evaluated on all datasets (out-of-distribution for AIME24, AIME25, AMC, HumanEval, WebQ, LQA2, and GPQA). Zero-shot Router uses direct prompting without examples, while Few-shot Router uses prompting with examples. The best results are highlighted in bold. Method Gemini2.5-Pro Gemini2.5-Flash GPT-5 GPT-4.1 GPT-4o ZS Router FS Router Random Router RouterDC GraphRouter EmbedLLM MLPRouter BertRouter ATLAS (cluster) RouterDC GraphRouter EmbedLLM MLPRouter BertRouter ATLAS (cluster) ATLAS (RL) Math Reasoning Code Arith. Common. Logic Sci. AIME24 AIME25 AMC Human. MBPP Calc. NQ WebQ LQA2 GPQA 92.0 88.0 93.3 46.7 13.3 13.3 23.3 6. 40.0 30.0 23.3 26.7 30.0 43.3 13.3 16.7 13.3 13.3 6.7 13.3 43.3 86.7 78.0 94.6 33.3 6.7 6.7 13.3 3.3 23.3 16.7 13.3 10.0 13.3 40.0 3.3 3.3 3.3 3.3 6.7 3.3 33. Closed-Source Models 81.5 80.5 93.4 92.1 85.4 83.7 82.6 98.4 57.7 82.6 Training-free Baselines 53.0 68.9 37.8 64.2 64.7 52. 64.7 58.9 82.9 62.0 58.1 55.7 47.2 40.2 In-Distribution Performance 80.5 78.7 75.6 76.2 75.4 91.5 77.7 75.0 72.0 68.7 72.1 83.6 74.9 72.3 76.7 48.2 77.1 83. 62.5 72.5 97.5 82.5 45.8 32.5 40.0 15.0 62.5 50.0 45.0 45.0 45.0 82.5 Out-of-Distribution Performance 70.8 71.2 79.1 54.6 67.0 83.3 81.6 78.7 73.4 73.0 67.7 79.0 83.6 81.8 79.2 76.2 79.9 75.0 78.7 91.5 85. 47.5 42.5 45.0 32.5 40.0 47.5 67.5 59.2 54.9 59.3 54.5 59.4 29.2 27.3 25.3 41.2 37.5 36.8 32.1 38.9 43.8 40.1 36.5 41.4 37.3 38.9 43.8 44.1 63.5 61.3 61.5 61.5 63. 39.2 35.8 32.1 47.6 49.6 48.3 40.4 50.4 53.6 50.8 49.3 50.2 43.7 51.4 51.4 52.2 78.9 74.6 83.8 78.2 72.9 45.3 40.8 49.2 47.2 45.8 51.4 41.2 47.1 66. 50.4 47.2 51.5 38.9 40.3 45.6 62.7 84.0 78.3 85.7 62.1 44.4 24.6 25.9 30.6 39.1 37.1 35.0 34.8 36.6 46.4 28.6 27.7 31.7 26.8 27.7 29.0 42.0 Avg. 75.6 73.0 85.0 63.0 53.1 36.4 38.7 29.3 53.4 49.3 47.7 42.3 48.6 63.5 46.3 44.4 46.8 39.3 43.6 49.2 59.4 garding text-based tools, the Code Interpreter executes Python code, returns the execution results, indicates whether the execution was successful, and reports error locations and underlying causes in case of failures. The Web Search tool leverages the official Google Custom Search API to retrieve the three most relevant search result snippets. Search results are obtained by sending HTTP GET requests to the API (https://www.googleapis. com/customsearch/v1) with the required parameters, including the API key, search engine ID, query string, and the number of top results to return. The Calculator parses the model output in functioncall format to extract the mathematical expression, computation type, and precision requirements, and then computes and returns the result using appropriate functions from the sympy library. The Process Reward Model (PRM) runs five model outputs in parallel, evaluates the segmented outputs using reward model, and selects the output with the highest average score as the final result. In this work, we use the off-the-shelf Qwen2.5-Math-PRM-7B1. 1Qwen/Qwen2.5-Math-PRM-7B Baseline Details. The original baselines perform routing among multiple models. When evaluating these baselines, we replace the models in the code with modeltool combinations, thereby enabling the baselines to route over both models and tools simultaneously. For example, when reproducing EmbedLLM (Zhuang et al., 2025), we substitute the model names in the training data with the modeltool combination names, and replace the model performance with the empirically measured performance of the modeltool combinations. Apart from these modifications, all training and evaluation procedures strictly follow the official open-source implementations, ensuring that the reported results faithfully reflect the true performance of the baseline methods. When evaluating closedsource models, we use exactly the same evaluation code and prompt templates (including the use of CoT reasoning and fixed answer formats) as those used for other baselines and our proposed method, ensuring strictly fair comparison and convincing final results. 17 Table 6: Performance comparison of ATLAS against single-tool baselines across multi-modal benchmarks. The framework dynamically routes queries among multi-modal tools using Qwen3-VL-8B-Instruct as the backbone. None represents direct reasoning without any tools. The best results are highlighted in bold. Tool Chart Understanding Math Reasoning Object Enumeration ChartQA TableVQA Geometry3K TallyQA CountBench None (Direct Reasoning) +OCR +Qwen3-Chart +Qwen3-Counting +Qwen3-Geo ATLAS (ours) vs. None 81.2 79.6 83.0 80.0 75.8 84.0 +2.8 64.5 67.0 62.4 64.8 62.4 68.2 +3.7 57.9 57.4 50.2 54.4 58.7 65.6 +7. 23.9 27.9 32.5 34.1 32.9 36.7 +12.8 84.1 83.9 87.8 89.6 87.8 90.2 +6.1 Avg. 62.3 63.2 63.2 64.6 63. 68.9 +6.6 Computing Details. All experiments are conducted on eight NVIDIA A100-80GB GPUs. Table 7: Performance scaling with Self-Consistency (SC) across different sample sizes."
        },
        {
            "title": "C More Results and Analysis",
            "content": "C.1 Details Main Results We provide extended comparisons in Table 5, incorporating additional closed-source models (e.g., Gemini-2.5-Flash) and routing baselines including GraphRouter (Feng et al., 2025b) and EmbedLLM (Zhuang et al., 2025). C.2 Detailed Multimodal Results Visual perception, comprehension, and reasoning are crucial capabilities for autonomous agents (Xie et al., 2024; Wu et al., 2025a; Reddy and Pillai, 2025). We conduct multimodal extension experiments as described in Section 4.3, with detailed orchestration results provided in Table 6. Our evaluation spans diverse visual reasoning tasks: chart understanding (ChartQA (Masry et al., 2022), TableVQA (Kim et al., 2024)), math reasoning (Geometry3K (Lu et al., 2021)), and object enumeration (TallyQA (Acharya et al., 2019), CountBench (Paiss et al., 2023)). To ensure fair comparison, all configurations, including the baseline (direct inference with Qwen38B-VL), single-tool baselines, and ATLAS, share the same foundational model (Qwen3-8B-VL) and identical evaluation protocols. The only modification across settings is the inclusion of toolinvocation instructions in the system prompt, which guide the model on when and how to invoke specific tools (e.g., chart parsing, object counting, geometric reasoning). Crucially, the core reasoning capacity and model parameters remain unchanged, ensuring that observed performance gains stem from adaptive tool orchestration rather than modellevel differences or prompt engineering artifacts. Dataset Pass@1 SC@4 SC@8 SC@16 AIME24 AIME25 AMC Calc. GPQA 43.3 40.0 82.5 83.3 46. 63.3 43.3 92.5 83.5 53.6 66.7 46.7 95.0 84.7 57.1 70.0 50.0 97.5 86.9 59.4 The results demonstrate that ATLAS consistently outperforms single-tool baselines across all categories, validating the effectiveness of dynamic tool orchestration in multimodal scenarios. We plan to explore more backbones in future work. C.3 Test-Time Scaling Results We analyze the scalability of our approach by increasing the self-consistency (SC) sample count on several representative benchmarks. As illustrated in Table 7, performance across almost all datasets shows positive correlation with the number of samples (k). For example, on the AIME24 benchmark, SC@16 yields significant improvement from 43.3% to 70.0%. Similar findings are also observed on other tasks, such as commonsense reasoning and scientific reasoning. These results demonstrate that the ensemble of model-tool combinations provides more robust candidate pool for majority voting. C.4 Analysis of Model-Tool Alignment Preferences Table 8 illustrates the strategic alignment between specific models and tools across diverse benchmarks. In deterministic domains such as coding (HumanEval) and advanced mathematics (AIME), ATLAS exhibits clear convergence, selecting specialized pairings like Qwen2.5-Coder-7B with 18 Table 8: Distribution of dominant model-tool combinations across diverse benchmarks. Dominant combination indicates the most frequently selected model-tool pair by our framework for each specific dataset. Dataset Dominant Combination ATLAS(Cluster) Dominant Combination ATLAS(RL) AIME24 AIME25 AMC Human. MBPP Calc. NQ WebQ LQA2 GPQA DeepSeek.-7B@PRM DeepSeek.-7B@PRM DeepSeek.-7B@PRM Coder-7B@Python Coder-7B@Python Qwen2.5-7B@Calc. Llama3.1-8B@Search Llama3.1-8B@Search InternLM3-8B@Search DeepSeek.-7B@Python 100.0% 100.0% 100.0% 100.0% 100.0% 100.0% 92.8% 98.8% 99.7% 80.4% DeepSeek.-7B@PRM DeepSeek.-7B@PRM DeepSeek.-7B@PRM Coder-7B@Python Coder-7B@Python Qwen2.5-7B@Calc. Llama3.1-8B@Search Llama3.1-8B@Search InternLM3-8B@Search DeepSeek.-7B@Python 100.0% 100.0% 91.7% 100.0% 100.0% 95.8% 99.0% 100.0% 56.4% 95.5% Python or DeepSeek-R1 with PRM in nearly 100% of cases. This high degree of consistency confirms the frameworks ability to internalize the performance advantages of domain-specific modules. In contrast, knowledge-intensive tasks (NQ, MedQA) trigger transition toward retrievalaugmented configurations, primarily utilizing Llama-3.1-8B with Web-Search. For more complex, broad-spectrum benchmarks like LQA2, the selection distribution becomes significantly more granular, with the dominant combination of ATLAS(RL) accounting for only 56.4%. This shift demonstrates that ATLAS avoids rigid heuristics, instead employing flexible orchestration strategy that adapts to the specific nuances and difficulty of each query. C.5 Ablation Study on Reward Components To investigate individual reward contributions, we train the RL policy without Rsel or Rfmt while keeping other signals intact. This addresses concerns about potential circularity from GPT-4o judgments in Rsel and validates the necessity of format enforcement. As shown in Table 9, removing Rsel causes modest 3.1% degradation (59.4% 56.3%), with notable drops on mathematical reasoning (AIME24/25: 6.6% each). However, the policy still substantially outperforms all baselines, including RouterDC (46.3%) and EmbedLLM (46.8%). The retained performance (56.3% vs. 36.4% for zero-shot) confirms that ATLAS learns effective routing independently through Rfmt and Rout, without requiring external model judgments. This validates Rsel as an efficiency-oriented auxiliary signal rather than necessary component. In contrast, removing Rfmt leads to more substantial 6.1% degradation (59.4% 53.3%), with significant drops on mathematical reasoning (AIME24: 10.0%, AMC: 12.5%) and code generation (HumanEval: 7.4%). This reveals that format enforcement is critical for maintaining structured interaction patternsproper tool syntax and reasoning-action sequencingwhich form the foundation for multi-step orchestration. Without Rfmt, the policy produces malformed tool calls that propagate failures throughout reasoning trajectories. These results validate our design: Rfmt and Rout constitute essential signals, while Rsel provides optional efficiency guidance. C.6 Sensitivity Analysis on Cluster Number To evaluate the robustness of cluster-based routing to the choice of cluster granularity, we conduct sensitivity analysis by varying the number of clusters {4, 8, 16} while keeping all other hyperparameters fixed. As shown in Table 10, the optimal performance is achieved at = 8 with 63.5% average accuracy, representing an 11.6% improvement over = 4 (51.9%) and modest 0.7% gain over = 16 (62.8%). The substantial performance drop at = 4 suggests that overly coarse clustering fails to capture fine-grained task distinctions, leading to suboptimal model-tool alignments, particularly evident on code generation (HumanEval: 43.3% vs. 91.5%) where diverse programming patterns require more specialized routing. Conversely, increasing to = 16 yields diminishing returns, as excessively fine-grained clusters may suffer from data sparsity within each partition, resulting in less reliable performance statistics. These results demonstrate that moderate cluster granularity (K = 8) strikes an effective balance between semantic specificity and statistical robustness, though the framework remains reasonably stable across range of values (62.8%63.5% for {8, 16}), indicating limited sensitivity to this hyperparameter in practical deployments. 19 Table 9: Ablation study on reward components. We evaluate the impact of removing Rsel (model selection reward) and Rfmt (format reward) on out-of-distribution performance. Models are trained on Calc., NQ, and MBPP (), then evaluated on all datasets. The best results are highlighted in bold. Method ZS Router FS Router Random Router RouterDC GraphRouter EmbedLLM MLPRouter BertRouter ATLAS (RL) w/o Rsel ATLAS (RL) w/o Rfmt Math Reasoning Code Arith. Common. Logic Sci. AIME24 AIME25 AMC Human. MBPP Calc. NQ WebQ LQA2 GPQA 13.3 23.3 6.7 13.3 16.7 13.3 13.3 6.7 43.3 36.7 -6.6 43.3 33.3 -10.0 6.7 13.3 3.3 3.3 3.3 3.3 3.3 6. 33.3 26.7 -6.6 33.3 26.7 -6.6 Training-free Baselines 53.0 68.9 37.8 64.2 64.7 52.6 55.7 47.2 40. Training-based Baselines 79.2 76.2 79.9 75.0 78.7 85.4 82.3 -3.1 85.4 78.0 -7.4 78.7 73.4 73.0 67.7 79.0 81.8 80.6 -1. 81.8 75.4 -6.4 70.8 71.2 79.1 54.6 67.0 81.6 79.1 -2.5 81.6 78.3 -3.3 32.5 40.0 15.0 47.5 42.5 45.0 32.5 40. 67.5 65.0 -2.5 67.5 55.0 -12.5 29.2 27.3 25.3 40.1 36.5 41.4 37.3 38.9 44.1 41.3 -2.8 44.1 41.6 -2. 39.2 35.8 32.1 50.8 49.3 50.2 43.7 51.4 52.2 48.3 -3.9 52.2 48.0 -4.2 45.3 40.8 49.2 50.4 47.2 51.5 38.9 40. 62.7 62.9 +0.2 62.7 58.2 -4.5 24.6 25.9 30.6 28.6 27.7 31.7 26.8 27.7 42.0 40.6 -1.4 42.0 38.4 -3. Avg. 36.4 38.7 29.3 46.3 44.4 46.8 39.3 43.6 59.4 56.3 -3.1 59.4 53.3 -6.1 Table 10: Sensitivity analysis on cluster number K. Performance across different cluster granularities in clusterbased routing. All datasets have training data available (in-distribution setting). #Cluster Math Reasoning Code Arith. Common. Logic Sci. AIME24 AIME25 AMC Human. MBPP Calc. NQ WebQ LQA2 GPQA 4 8 16 36.7 43.3 40.0 30.0 40.0 40. 75.0 82.5 82.5 43.3 91.5 90.9 71.5 83.6 82.9 79.1 83.3 82.3 28.8 43.8 44.1 48.5 53.6 53. 66.8 66.8 66.7 39.6 46.4 45.3 Avg. 51.9 63.5 62."
        },
        {
            "title": "D Case Study",
            "content": "We provide some representative examples in Figures 1014 to illustrate how ATLAS dynamically orchestrates model-tool combinations across diverse reasoning tasks. Adaptive Multi-turn Reasoning. Figure 10 demonstrates ATLASs capacity for self-correction through iterative exploration. When addressing logical reasoning problem, the policy initially selects Qwen2.5-7B with web search to verify its hypothesis (option C), but upon receiving contradictory feedback, it re-evaluates the alternatives and routes to InternLM3-8B for second verification. This multi-turn deliberation ultimately leads to the correct answer (option D), showcasing the frameworks ability to recover from suboptimal initial decisions through adaptive re-routing. Task-Aware Model-Tool Alignment and Selection. Figures 1114 highlight how ATLAS aligns model-tool pairs with task-specific requirements. For arithmetic computation (Figure 11), the policy directly invokes the calculator tool without unnecessary reasoning steps. For factual retrieval (Figure 12), it routes to Llama-3.1-8B with web search, recognizing the need for external knowledge. Code generation tasks (Figure 13) are delegated to the specialized Qwen2.5-Coder model with Python execution. For challenging mathematical problems (Figure 14), ATLAS combines DeepSeek-7B with PRM for rigorous verification. These examples collectively demonstrate that ATLAS has internalized meaningful associations between task categories and optimal model-tool configurations, rather than relying on rigid heuristics."
        },
        {
            "title": "E Additional Discussion",
            "content": "E.1 Distinguishing ATLAS from Prior Routing and Tool Usage Methods While ATLAS employs established techniques such as semantic clustering for query representation and PPO for policy optimization, its contribution extends beyond the individual components to address fundamental gap in existing literature: the joint 20 optimization of heterogeneous model-tool combinations. Prior routing methods (Chen et al., 2024; Ong et al., 2024; Lu et al., 2024) focus exclusively on model selection, treating LLMs as isolated execution units without considering external tool augmentation. Conversely, tool usage frameworks (Feng et al., 2025a; Wu et al., 2025b) rely on fixed invocation logic that cannot dynamically adapt to different model capabilities. ATLAS unifies these two paradigms by explicitly modeling the Cartesian product space = and learning task-aware alignments within this joint space. The technical novelty of ATLAS manifests in three key aspects. First, the dual-path architecture strategically combines training-free clusterbased routing for exploiting domain-specific priors with RL-driven exploration for generalizing to unfamiliar tasks, achieving complementary strengths across distribution shifts  (Table 1)  . Second, our composite reward structure (Rfmt + γRout + ξRsel) decouples execution correctness from routing efficiency through the Rsel signal, enabling the policy to internalize transferable expertise distribution rather than memorizing task-specific mappings, which is evidenced by robust generalization to expanded model-tool pools without retraining (Section 4.4). Third, our controlled experiments ensure that all configurations share identical backbone models and evaluation protocols, with only routing mechanisms differing (Section B.4), thereby isolating the contribution of orchestration strategies from confounding factors such as model capacity or prompt engineering. The consistent performance gains across 15 benchmarks, including out-of-distribution settings (+13.1% over baselines) and multi-modal tasks (+4.3%), demonstrate that ATLAS captures generalizable principles for adaptive model-tool coordination. E.2 Discussion on RL Reward Design Our composite reward function rϕ = Rfmt + γRout + ξRsel balances structured execution, task correctness, and routing efficiency. Regarding potential concerns about Rout that require groundtruth labels, we note that test-time reinforcement learning remains effective in label-scarce scenarios through alternative supervision signals: majority voting across sampled trajectories has proven effective as pseudo-labeling (Zuo et al., 2025). Future extensions of ATLAS could integrate such selfverification mechanisms to further reduce reliance on explicit supervision. 21 Regarding Rsel, which penalizes suboptimal model selections based on offline evaluation (Equation 9), potential concern is whether this introduces evaluator bias or test-time information leakage. However, Rsel encodes domain priors from offline profiling, such as code tasks benefit from specialized models or retrieval tasks favor search-augmented models, which practitioners naturally possess and use to initialize routing systems. Critically, it does not leak test-time information but rather provides consistent training targets to guide efficiency-aware exploration. The low weight ξRsel = 0.15 (vs. γRout = 1.0 for Rout) ensures routing efficiency serves as an auxiliary signal without overriding correctness. Our ablation (Figure 5b-5c) shows that Rsel accelerates convergence and reduces entropy, while generalization to expanded model pools without prior annotations  (Table 2)  shows that the policy learns transferable routing principles that aligns task characteristics with model capabilities, rather than memorizing specific mappings. E.3 When to Use Cluster-Based vs. RL-Based Routing The choice between cluster-based and RL-based routing depends on data availability and generalization requirements. When domain-specific training data is accessible, such as historical queryanswer pairs in enterprise QA systems, clusterbased routing offers simple and efficient solution. It achieves strong in-domain performance (63.5% average accuracy, Table 1) with zero training cost by leveraging semantic clustering and historical statistics, making it ideal for rapid deployment in well-defined domains. Conversely, when the reasoning engine must handle diverse, unfamiliar tasks where domain priors are unavailable, such as general-purpose assistants facing unpredictable queries, RL-based routing provides superior generalization. It learns transferable patterns of when to invoke tools or defer to specialized models, maintaining robust OOD performance (59.4% vs. 49.2% for cluster-based) at the cost of upfront training. In practice, practitioners can adopt hybrid strategy: using cluster-based routing as the default for efficiency while reserving RL-based routing for critical queries or new domains, thereby balancing simplicity with adaptability. Figure 6: System prompt for ATLAS RL Experiments. 22 Figure 7: System prompt for Qwen3-Chart Tool. 23 Figure 8: System prompt for Qwen3-Counting Tool. Figure 9: System prompt for Qwen3-Counting Tool. 25 Figure 10: Example 1 on the LQA2 dataset. 26 Figure 11: Example 2 on the Calculator dataset. Figure 12: Example 3 on the WebQ dataset. 27 Figure 13: Example 4 on the MBPP dataset. 28 Figure 14: Example 5 on the AIME dataset."
        }
    ],
    "affiliations": [
        "East China Normal University",
        "Tsinghua University",
        "Zhejiang University"
    ]
}