{
    "paper_title": "3D-Fixup: Advancing Photo Editing with 3D Priors",
    "authors": [
        "Yen-Chi Cheng",
        "Krishna Kumar Singh",
        "Jae Shin Yoon",
        "Alex Schwing",
        "Liangyan Gui",
        "Matheus Gadelha",
        "Paul Guerrero",
        "Nanxuan Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite significant advances in modeling image priors via diffusion models, 3D-aware image editing remains challenging, in part because the object is only specified via a single image. To tackle this challenge, we propose 3D-Fixup, a new framework for editing 2D images guided by learned 3D priors. The framework supports difficult editing situations such as object translation and 3D rotation. To achieve this, we leverage a training-based approach that harnesses the generative power of diffusion models. As video data naturally encodes real-world physical dynamics, we turn to video data for generating training data pairs, i.e., a source and a target frame. Rather than relying solely on a single trained model to infer transformations between source and target frames, we incorporate 3D guidance from an Image-to-3D model, which bridges this challenging task by explicitly projecting 2D information into 3D space. We design a data generation pipeline to ensure high-quality 3D guidance throughout training. Results show that by integrating these 3D priors, 3D-Fixup effectively supports complex, identity coherent 3D-aware edits, achieving high-quality results and advancing the application of diffusion models in realistic image manipulation. The code is provided at https://3dfixup.github.io/"
        },
        {
            "title": "Start",
            "content": "3D-Fixup: Advancing Photo Editing with 3D Priors YEN-CHI CHENG, University of Illinois Urbana-Champaign, USA KRISHNA KUMAR SINGH, Adobe Research, USA JAE SHIN YOON, Adobe Research, USA ALEXANDER SCHWING, University of Illinois Urbana-Champaign, USA LIANG-YAN GUI, University of Illinois Urbana-Champaign, USA MATHEUS GADELHA, Adobe Research, USA PAUL GUERRERO, Adobe Research, UK NANXUAN ZHAO, Adobe Research, USA 5 2 0 2 5 1 ] . [ 1 6 6 5 0 1 . 5 0 5 2 : r Fig. 1. 3D-aware photo editing. Given source image with user-specified 3D transformations, our model generates new image that follows the users edit while preserving the input identity. The 3D edit is visualized via the transformation between the original mesh (pink) and the edited mesh (cyan). Work was done while Yen-Chi was an intern at Adobe Research. Authors Contact Information: Yen-Chi Cheng, yenchic3@illinois.edu, University of Illinois Urbana-Champaign, Urbana, Illinois, USA; Krishna Kumar Singh, krishsin@ adobe.com, Adobe Research, San Jose, California, USA; Jae Shin Yoon, jaeyoon@adobe. com, Adobe Research, San Jose, California, USA; Alexander Schwing, aschwing@ illinois.edu, University of Illinois Urbana-Champaign, Urbana, Illinois, USA; Liang-Yan Gui, lgui@illinois.edu, University of Illinois Urbana-Champaign, Urbana, Illinois, USA; Matheus Gadelha, gadelha@adobe.com, Adobe Research, San Jose, California, USA; Paul Guerrero, guerrero@adobe.com, Adobe Research, London, UK; Nanxuan Zhao, nanxuanz@adobe.com, Adobe Research, San Jose, California, USA. Despite significant advances in modeling image priors via diffusion models, 3D-aware image editing remains challenging, in part because the object is only specified via single image. To tackle this challenge, we propose 3DFixup, new framework for editing 2D images guided by learned 3D priors. The framework supports difficult editing situations such as object translation and 3D rotation. To achieve this, we leverage training-based approach that harnesses the generative power of diffusion models. As video data naturally encodes real-world physical dynamics, we turn to video data for generating training data pairs, i.e., source and target frame. Rather than relying solely on single trained model to infer transformations between source Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM XXXX-XXXX/2025/5-ART https://doi.org/10.1145/nnnnnnn.nnnnnnn , Vol. 1, No. 1, Article . Publication date: May 2025. 2 Yen-Chi Cheng, Krishna Kumar Singh, Jae Shin Yoon, Alexander Schwing, Liang-Yan Gui, Matheus Gadelha, Paul Guerrero, and Nanxuan Zhao and target frames, we incorporate 3D guidance from an Image-to-3D model, which bridges this challenging task by explicitly projecting 2D information into 3D space. We design data generation pipeline to ensure high-quality 3D guidance throughout training. Results show that by integrating these 3D priors, 3D-Fixup effectively supports complex, identity coherent 3Daware edits, achieving high-quality results and advancing the application of diffusion models in realistic image manipulation. The code is provided at https://3dfixup.github.io/. CCS Concepts: Computing methodologies Computer vision. Additional Key Words and Phrases: Image editing, 3D, Diffusion Model ACM Reference Format: Yen-Chi Cheng, Krishna Kumar Singh, Jae Shin Yoon, Alexander Schwing, Liang-Yan Gui, Matheus Gadelha, Paul Guerrero, and Nanxuan Zhao. 2025. 3D-Fixup: Advancing Photo Editing with 3D Priors. 1, 1 (May 2025), 10 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn"
        },
        {
            "title": "Introduction",
            "content": "Generative image editing is growing research field that promises intuitive edits of objects in images, even if information about these objects or the scene that contains them is incomplete. For example, moving an object from one side of the image to the other, or rotating it as shown in Figure 1, may require knowledge about lighting, shadows, and occluded parts of the scene that are not available in the image. For these edits, generative models can hallucinate the missing information to obtain plausible result. Current generative editing methods focus on appearance edits or 2D edits of image patches. However the objects we typically manipulate in images are projections of 3D objects. Some natural edits for 3D objects, like outof-plane rotations or 3D translations, are thus not possible in most current approaches. However, 3D editing is crucial in applications such as e-commerce, where 3D object may need to be shown from multiple angles, or digital media production, where it gives artists the ability to re-configure or relight 3D scene shown in an image, without having to explicitly reconstruct the entire 3D scene, simplifying the creative process. By developing such 3D editing method on natural images, we aim to bridge the gap between 2D and 3D workflows, making realistic edits more accessible for real-world applications. The current challenge for 3D editing of objects in images lies in maintaining consistent object appearance across different angles and lighting conditions, which is essential for creating seamless edits. Existing approaches have attempted to address these issues using either optimization or feed-forward deep net techniques. Optimizationbased approaches, such as Image Sculpting [Yenphraphai et al. 2024], begin by constructing rough 3D shape of the object, followed by directly editing the 3D shape, and finally performing refinement to obtain the final edits. While this method achieves high-quality results, it is computationally intensive and slow, limiting its practical applications. In contrast, feed-forward approaches like 3DIT [Michel et al. 2024] and Magic Fixup [Alzayer et al. 2024] leverage conditional diffusion models to guide the editing process, making them relatively fast and efficient. However, these methods are primarily limited by their dependence on 2D data and synthetic training sets, which either lack depth and spatial understanding or real-world data understanding. Besides, the reliance on text prompts restricts , Vol. 1, No. 1, Article . Publication date: May 2025. the precision and granularity of the user control, often leading to outputs that may diverge from the users intention. To address these challenges, we propose feed-forward method that utilizes real-world video data enriched with 3D priors, allowing for realistic 3D-aware editing of objects in natural images. We design novel data generation pipeline to overcome the challenge of collecting large-scale 3D-aware image editing datasets in real-world scenarios. Our pipeline generates training data by leveraging 3D transformations estimated between frames in video, and combines those with the priors obtained from an image-to-3D model. This intermediate 3D guidance serves as crucial bridge, enabling the model to learn 3D-aware editing without requiring explicit 3D annotations for every frame. By utilizing both the dynamic information from videos and the structural insights provided by 3D priors, our approach captures real-world physical dynamics while facilitating fine-grained control over edits. This innovative design allows the model to generalize effectively to natural scenes, bridging the gap between synthetic and real-world applications. In Figure 1, we show some 3D-aware edits performed by our approach which allows fine-grained 3D user control while preserving object identity. Our contributions are threefold: (1) we develop novel data pipeline for generating 3D-aware training data from real-world videos, bridging 2D inputs with 3D editing capabilities; (2) we design an efficient feed-forward model that performs precise 3D editing on natural images using this 3D guidance; and (3) we conduct extensive evaluations, demonstrating that our method achieves realistic 3D edits and outperforms state-of-the-art approaches."
        },
        {
            "title": "2 Related Work\n2.1",
            "content": "3D-Aware Generative Image Editing 3D-aware image editing methods provide 3D control for image objects while maintaining consistency w.r.t. object identity, pose, and lighting during editing. Object3DIT [Michel et al. 2024] is one of the earliest 3D-aware editing methods, directly operating on 3D transformation parameters as condition in feed-forward deep-net architecture. The method is however limited by its small synthetic training set, reducing its generality and the precision of its edits. Diffusion Handles [Pandey et al. 2024] and GeoDiffuser [Sajnani et al. 2024] are recent approaches that are general and precise, but use inference-time optimization to align the output to particular edit, limiting robustness and inference speed. The space of supported 3D transformations is somewhat limited since those techniques make no attempt in explicitly reasoning about unseen parts of objects. In contrast, Image Sculpting [Yenphraphai et al. 2024] leverages off-the-shelf single-view reconstruction models to enable impressive 3D-aware editing results, but also requires computationally demanding inference-time optimization. Unlike prior approaches, our work focuses on fine-tuning large image diffusion models specifically for this task. This allows our method to exhibit remarkable robustness to challenging editing operations that could not be performed with existing baselines (see Figure 8). Additionally, no inference-time optimization is needed, allowing for fast evaluation."
        },
        {
            "title": "2.2 Generative Image Editing",
            "content": "Drag-based methods have emerged as prominent paradigm for interactive image editing, offering users precise control over object movement and transformation. Early methods like DragGAN [Pan et al. 2023] utilized GANs for point-to-point dragging but faced challenges with generalization and editing quality. More recent methods, such as DragDiffusion [Mou et al. 2023], InstantDrag [Shin et al. 2024], and EasyDrag [Hou et al. 2024], extend this concept to diffusion models, leveraging fine-tuned or reference-guided approaches to enhance photorealism. DragonDiffusion [Mou et al. 2023] stands out by avoiding fine-tuning and employing energy functions with visual cross-attention, enabling diverse editing tasks within and across images. DiffEditor [Mou et al. 2024] and DiffUHaul [Avrahami et al. 2024] further refine drag-style editing, addressing challenges like entanglement and enhancing consistency in dragging results. For articulated object interactions, DragAPart [Li et al. 2025] focuses on part-level motion understanding, allowing edits like opening drawers or repositioning parts. In contrast, generative editing methods that do not rely on drag-based interactions offer alternative workflows for tasks like object insertion, removal, and repositioning. ObjectDrop [Winter et al. 2024] models the effects of objects on scenes using counterfactual supervision, enabling realistic object manipulation. Meanwhile, SEELE [Wang et al. 2024a] formulates subject repositioning as prompt-guided inpainting task, preserving image fidelity while offering precise spatial control. Magic Fixup [Alzayer et al. 2024] employs diffusion models to transform coarsely edited images into photorealistic outputs, leveraging video data to learn how objects adapt under various conditions. Similarly, ObjectStitch [Song et al. 2023] and IMPRINT [Song et al. 2024] focus on object compositing while preserving identity and harmonizing with the background, making them valuable for realistic image manipulation. However, unlike our approach, none of the methods in this paragraph benefit from 3D-aware prior or provide controls to support 3D-aware edits like out-of-plane rotations or 3D translations. 2.3 Image-to-video with motion control Image-to-video methods with motion control [Bahmani et al. 2025; Guo et al. 2025; Shi et al. 2024; Wang et al. 2024b] are related to generative image editing to some extent, as any generated frame could be taken as an edited image. However, edits are limited to motions that are plausible in video and, to our best knowledge, none of the methods provide 3D control."
        },
        {
            "title": "3 Approach",
            "content": "Our goal is 3D editing (e.g., out-of-plane rotation and translation) of chosen object within an image. Existing 3D editing approaches that use inference-time optimization [Pandey et al. 2024; Yenphraphai et al. 2024] suffer from excessive inference times, making them impractical in real-world applications. In contrast, feedforward approaches [Michel et al. 2024] suffer from lack of high-quality training data, limiting generality and control. We propose feedforward 3D editing method that offers precise control and good generality. The editing workflow is illustrated in Figure 2. As shown, the 3D edits we consider include out-of-plane rotations and translations 3D-Fixup: Advancing Photo Editing with 3D Priors 3 Fig. 2. Inference pipeline. We assume editing instructions (possibly converted from text prompts) are in the form of 3D operations like rotation and translation. Given mask indicating the object to be edited, we first perform image-to-3D [Xu et al. 2024] to reconstruct the mesh. We then apply the users desired 3D edit to obtain the 3D guidance. Here the 3D edit is visualized as the transformation between original mesh (pink wire-frame) and the edited mesh (cyan wire-frame). Finally, the model outputs the 3D aware editing result. that change the perspective of the object. Formally, given source image 洧냪src, the user selects the object to be modified. The selection is represented via the mask 洧src, which we use to obtain rough 3D reconstruction. The user then performs 3D edit of the rough 3D reconstruction. Upon rendering the modified 3D reconstruction we obtain the 3D guidance 洧냪guide R洧냩 洧녥 3, which is used to generate the desired editing result. To provide the necessary supervision for training the feedforward model, i.e., to obtain source image 洧냪src, guidance image 洧냪guide, and ground truth target image 洧냪tgt, we construct new dataset derived from videos. For this, we develop the data processing pipeline that we describe in Section 3.1. As videos naturally capture 3D motion as well as variations in lighting and background conditions, they offer rich source of real-world data. By integrating this dataset into the training process, our method learns to handle complex 3D transformations while ensuring photorealism and maintaining the fidelity of the edited subject. Using this dataset, we fine-tune pretrained diffusion model conditioned on 1) the edited guidance image 洧냪guide, and 2) the source image 洧냪src. Importantly, unlike prior 2D editing methods, the guidance image is obtained by 3D-transforming full 3D reconstruction of the chosen object in the image. We describe architecture and training setup of this model in Section 3.2."
        },
        {
            "title": "3.1 Constructing the Dataset from Videos",
            "content": "Given video, we create data pairs by sampling two frames, source image 洧냪src and target image 洧냪tgt. We use both images to compute guidance image 洧냪guide. Figure 3 provides an overview of our data processing pipeline. We discuss details next. Flow for sampling the source and target image. We compute optical flow across all frames in given video. If the accumulated flow across the entire video is too small, we discard the video. If the accumulated flow exceeds threshold, we sample two frames from the video clip which we refer to as 洧냪src and 洧냪tgt. Obtaining mask for the main object. We use Grounded-SAM [Ren et al. 2024] to obtain object masks and filter cases with privacy, aesthetic, or insignificant 3D transformation issues. Notice that Grounded-SAM struggles to detect occluded or unusually shaped instances, which are rare. To automatically identify the main object, , Vol. 1, No. 1, Article . Publication date: May 2025. Yen-Chi Cheng, Krishna Kumar Singh, Jae Shin Yoon, Alexander Schwing, Liang-Yan Gui, Matheus Gadelha, Paul Guerrero, and Nanxuan Zhao Fig. 3. Data pipeline: Overview. Given video, we sample two frames, the source frame 洧냪src and the target frame 洧냪tgt, using optical flow as cue: we discard videos where the flow indicates little motion through the entire clip. Using Image-to-3D methods, we reconstruct mesh for the desired object for both frames. We then estimate the 3D transformation (see Figure 4) between the source frame mesh and the target frame mesh. Availability of the transformation enables two ways to create the training data: (1) in Transform Source, we paste the rendering of the transformed source mesh onto the target frame; (2) in Transform Target, we paste the rendering of the target mesh onto the source frame. Data examples are shown in Figure 5. Fig. 4. Data pipeline: Estimation of the 3D transformation T. We estimate the 3D transformation by leveraging correspondences between the source frame and the target frame. Given frames between the source frame and the target frame, we first perform tracking to obtain corresponding points. We then initialize the parameters for the 3D transformation and use an optimization to improve T: (1) We unproject the 2D correspondences on the source frame to 3D pointclouds and apply the current to transform points to the target image; (2) we project points back to 2D and compare via an L2 loss with the 2D correspondences of the target frame. we define an instance score that prioritizes centrally located objects that occupy significant portions of the frame. The score is the weighted sum of inverted border score 洧녡洧녪 and area score 洧녡洧녩 across the video. They are calculated by 洧녡洧녪 = 1 洧녷inst border 洧녷border , 洧녡洧녩 = 洧녷inst 洧녷image , (1) where 洧녷border is the number of border pixels in the whole image, 洧녷inst border is the number of instance pixels which touch the border, 洧녷inst is the total number of pixels of the instance, and 洧녷image is the number of pixels of the whole image. The instance score is 洧녡inst = 0.6 洧녡洧녪 + 0.4 洧녡洧녩. We select the highest-scoring instance. Image to 3D reconstruction. Given the source image 洧냪src and the target image 洧냪tgt, we perform image to 3D reconstruction using InstantMesh [Xu et al. 2024]. Specifically, given the foreground masks 洧src R洧냩 洧녥 and 洧tgt R洧냩 洧녥 , we first crop the foreground , Vol. 1, No. 1, Article . Publication date: May 2025. Fig. 5. Examples of the training data. Given video, we use the steps described in Figure 3 to obtain the training data, i.e., the source image 洧냪src and the target image 洧냪tgt. The guidance image is obtained via the developed data pipeline. The mask has three values: 0 indicates the hole created by the coarse edit and the model needs to inpaint by looking at the details of the reference; 0.5 refers to the rendering of the object; and 1.0 denotes the original background. object and pad the image to square while ensuring that the object is centered. InstantMesh then employs Zero123++ [Shi et al. 2023] to generate multiview images. Subsequently, InstantMesh operates on the multiview images to compute the reconstructed meshes 洧녡src and 洧녡tgt for the source and target images. Estimating 3D transformation with tracking. To obtain coarse edit in an automated manner, we use the object meshes for the source and target image to estimate 3D transformation. This process is illustrated in Figure 4. Concretely, to estimate the 3D transformation between the object in the source image 洧냪src and the target image 洧냪tgt, we first compute 洧녜 correspondences 洧녷src and 洧녷tgt along with their visibility maps 洧녺src and 洧녺tgt, using SpaTracker [Xiao 3D-Fixup: Advancing Photo Editing with 3D Priors 5 where  is the projection operation: (P, K) = (cid:2)K 0(cid:3) P. (7) The optimization iteratively adjusts until convergence. Eq. 6 assumes to be rigid transformations. However, for non-rigid transformations, Eq. 6 finds close rigid approximation. During training, the model learns from non-rigid transformation as ground truth while using rigid approximation in guidance 洧냪guide. This discrepancy is often desirable, as it encourages the model to adapt non-rigidly, ensuring the edited object fits naturally into its new context. For example, when rotating the dog in Fig. 1 or the horse in Fig. 9, subtle posture adjustments, such as foot placement, help the resulting scene remain plausible. Creating the training data. With the optimized transformation computed, we have two settings to obtain the guidance image and the editing mask. We refer to the first setting as Transform Source (TS): the estimated 3D transformation is applied to the source mesh 洧녡src. The transformed mesh is rendered and pasted onto the target image 洧냪tgt based on the target mask 洧tgt to form the guidance image 洧냪洧녢 洧녡 guide. The editing mask 洧洧녢 洧녡 guide has 1.0 for the static background, 0.5 for the rendered regions, and 0.0 for the holes created by cropped the object in the 洧냪tgt. We refer to the second setting as Transform Target (TT): we transform and render the target mesh 洧녡tgt onto the source frame 洧냪src to obtain the guidance image 洧냪洧녢洧녢 guide. The background is warped based on the flow computed between the source frame and the target frame following Alzayer et al. [Alzayer et al. 2024]. 洧洧녢洧녢 guide is similar to 洧洧녢 洧녡 guide for the background (1.0) and rendered regions (0.5), while the holes (0.0) indicated the warped regions. Thus, the final training tuple for Transform Source and Transform Target are (洧냪source, 洧냪洧녢 洧녡 , 洧냪target) respectively. We sample data pair randomly from TT or TS when training the model. Both the editing masks 洧洧녢 洧녡 guide guide the model to inpaint missing regions (0.0), enhance 3D-transformed areas (0.5), and preserve intact content (1.0). Examples of the collected data are illustrated in Figure 5. , 洧냪target), and (洧냪source, 洧냪洧녢洧녢 guide and 洧洧녢洧녢 , 洧洧녢洧녢 , 洧洧녢 洧녡 guide guide guide guide Our dataset is sourced from 8 million licensed videos. We discard videos exceeding 500 frames due to high compute costs, reducing the dataset to 2 million. After filtering cases depicting humans (privacy and aesthetics issues), illustrations and drone videos (insignificant 3D transformations), and videos without detected entities, we retain 375k clips. Further motion-based flow filtering reduces this to 50k videos. Note that the proposed data pipeline is general and applicable to any video dataset. Processing each video takes 95.7s and requires 14.41GB of memory on single A100 GPU. The pipeline includes flow filtering, mask extraction, image-to-3D, tracking, and 3D estimation, with videos averaging 50500 frames. 3.2 3D Editing with Diffusion Model Our diffusion model aims to generate realistic images that complete the 3D edit specified by the guidance image 洧냪guide. Hence, the structure of the image should be preserved as outlined in the guidance. For regions indicated by the mask 洧guide, the model performs the following operations: inpainting missing regions (洧guide = 0.0), , Vol. 1, No. 1, Article . Publication date: May 2025. Fig. 6. Overview of the training pipeline. We develop conditional diffusion model for 3D-aware image editing. It consists of two networks: 洧녭gen and 洧녭detail. During training, given the inputstarget frame 洧냪tgt, 3D guidance 洧냪guide, mask 洧guide, and detail feature 洧냧洧노 洧녭gen learns the reverse diffusion process to predict the noise 洧랬 and reconstruct 洧냪tgt. To better preserve identity and fine-grained details from the source image 洧냪src, 洧녭detail takes as input the source image 洧냪src, its noisy counterpart 洧냪洧노 , and the mask 洧guide, and extracts detail features 洧냧洧노 . We apply cross-attention between 洧냧洧노 and the intermediate features of 洧녭gen to incorporate content and details from 洧냪src during the reverse diffusion process. et al. 2024]. The input to the tracking model is the video segment containing the two frames 洧냪src and 洧냪tgt, along with the source mask 洧src. Depth maps 洧냥src and 洧냥tgt are rendered from the meshes 洧녡src and 洧녡tgt, respectively. Using the depth maps and correspondences, we backproject the 2D points into 3D space: Psrc = 1 (洧녷src, 洧냥src, K), Ptgt = 1 (洧녷tgt, 洧냥tgt, K), (3) where 1 denotes the unprojection operation and R33 is the intrinsic camera matrix. The 3D points R3 are calculated as: (2) = 洧냥 (洧녷) K1 , (4) 洧녷洧논 洧녷洧녽 1 where 洧냥 (洧녷) is the depth at pixel 洧녷 = (洧녷洧논 , 洧녷洧녽). The translation component of is initialized by calculating the centroid offset between the two point clouds: = ctgt csrc, = 1 洧녜 洧녜 洧녰=1 P洧녰 . (5) To initialize the rotation, coarse grid search is performed jointly over the 洧녦 , 洧녧 , and 洧녨 axes, using step size of 10 within the range [0, 360]. The transformation is optimized by minimizing the re-projection loss: Lreproj = 洧녜 洧녰=1 洧녷tgt,洧녰 (TPsrc,洧녰, K)2 2 , (6) Yen-Chi Cheng, Krishna Kumar Singh, Jae Shin Yoon, Alexander Schwing, Liang-Yan Gui, Matheus Gadelha, Paul Guerrero, and Nanxuan Zhao modifying ambiguous regions (洧guide = 0.5), and preserving content and identity in confident regions (洧guide = 1.0). An overview of our architecture is provided in Figure 6. We base our architecture on MagicFixup [Alzayer et al. 2024] and adopt two networks in our pipeline: generator for generating the output image 洧녭gen and an extractor for extracting details 洧녭detail from the source image 洧냪src. We use those networks in diffusion process which operates as follows: the diffusion forward process progressively adds Gaussian noise to an image, yielding sequence of intermediate states: 洧논洧노 ( 洧띺洧노 洧논洧노 1, (1 洧띺洧노 )I), (8) which gradually resemble standard Gaussian as diffusion time 洧노 increases towards final timestep 洧녢 . Here 洧띺洧노 defines the noise schedule at diffusion time 洧노. The model learns the reverse process: standard Gaussian input 洧논洧녢 (0, I) is gradually denoised toward intermediate states 洧논洧노 before eventually arriving at the final estimated image 洧논0: 洧논洧노 1 = 洧녭gen (洧논洧노 , 洧냪guide, 洧guide, 洧냧洧노 , 洧노; 洧랚 ). (9) At each denoising step 洧노, the model is conditioned on the guidance 洧냪guide, mask 洧guide, and the features 洧냧洧노 extracted by the extractor 洧녭detail. We initialize the process from noisy version of the guidance image, i.e., we use 洧논洧녢 = 洧띺洧녢 洧냪guide + 1 洧띺洧녢 洧랬, (10) where 洧랬 (0, I) and 洧띺洧녢 = (cid:206)洧녢 洧띺洧 . This initialization ensures 洧=1 alignment with the guidance while bridging the gap between training and inference domains. The extractor 洧녭detail operates on the source image 洧냪src for referencing and with the goal to preserve finegrained details and object identity. To ensure compatibility with the diffusion process, we add noise to the source image: 洧냪洧노 = 洧띺洧노 洧냪src + 1 洧띺洧노 洧랬, 洧랬 (0, I), 洧띺洧노 = 洧노 (cid:214) 洧=1 洧띺洧 . (11) 洧냧洧노 = [洧녭 洧노 , . . . , 洧녭 洧녵 洧노 ] = 洧녭detail ([洧냪洧노 , 洧냪src, 洧guide]; 洧노), From this noisy source image, the extractor computes the feature (12) for each self-attention block. Here 洧녵 is the number of attention blocks, and [] denotes concatenation along the channel dimension. These features are injected into the model through cross-attention layers, enabling details preservation from the source image to outputs during synthesis. For each layer 洧녰, at step 洧노, the features 洧냧洧노 extracted by the extractor 洧녭detail serve as keys 洧 and values 洧녤 , while the features of the generator 洧녭gen [洧녮1 洧노 ] act as queries 洧녟. Formally, we have 洧노 , . . . , 洧녮洧녵 洧냢洧녰 洧노 = softmax( 洧노 洧쮫롐 洧녟洧녰 洧노 洧녬 ), and 洧냨洧녰 洧노 = 洧냢洧녰 洧노洧녤 洧녰 洧노 , (13) 洧노 , 洧쮫롐 洧노 , and 洧녤 洧녰 where 洧녟洧녰 洧노 are query, key, and value projections of the respective features. This mechanism ensures that fine details from 洧냪src are faithfully transferred to the synthesized output. During inference, user instructions (e.g., text prompts, drags on 3D objects) are converted into 3D transformations (out-of-plane rotations and translations). Using InstantMesh [Xu et al. 2024], we perform image-to-3D reconstruction to generate 3D mesh of the subject. Applying to the mesh, we obtain the guidance for editing. , Vol. 1, No. 1, Article . Publication date: May 2025. The model uses this guidance 洧냪guide along with the mask 洧guide to produce the final output. Figure 2 illustrates this framework. 3."
        },
        {
            "title": "Implementation details",
            "content": "For fair comparisons, following the state-of-the-art Magic-Fixup [Alzayer et al. 2024], we train our model starting from pretrained weights of Stable Diffusion 1.4. Training samples are drawn from data settingsTT, TS, MFwith probabilities (0.35, 0.35, 0.3), where MF is sampling from Magic Fixups data. To encourage identity preservation, we drop the conditioning on 洧냪src with 0.2 probability, forcing the model to rely on 洧냪srcs context. We train the model with batch size of 8, using AdamW [Loshchilov 2017] and learning rate of 1e5 on 8 NVIDIA A100 GPUs for about two days. We use linear diffusion noise schedule, with 洧띺1 = 0.9999, 洧띺洧녢 = 0.98, and 洧녢 = 1000. We use DDIM for sampling with 100 steps during inference time. The images were all cropped to 512 512 for training."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate the proposed method both qualitatively and quantitatively on set of edits. For this, we curated set of user edits to show the use cases of the model in practical applications. We also created test dataset which contains large 3D transformations to validate the proposed method. Dataset. We use stock video as our dataset for training and testing. The training data consists of around 50k data points which contains common objects with motions in the scene. We randomly sample diverse scenes and objects while maintaining reasonable scale when constructing the test set. Baseline. To validate the effectiveness of the proposed method, we compare to seven baselines: Magic Fixup [Alzayer et al. 2024], Object 3DIT [Michel et al. 2024], Zero-1-to-3 [Liu et al. 2023], InstantDrag [Shin et al. 2024], MOFA-Video [Niu et al. 2024], Blended LDM [Avrahami et al. 2023], and finally Instruct-pix2pix [Brooks et al. 2023]. Please refer to the supplementary materials for details regarding the baselines. Metrics. For quantitative evaluation, we use LPIPS [Zhang et al. 2018] and FID [Heusel et al. 2017] metrics. LPIPS measures the perceptual similarity to assess fidelity to the ground truth using neural network such as AlexNet [Krizhevsky et al. 2012] or VGGNet [Simonyan and Zisserman 2014]. FID (Fr칠chet Inception Distance) evaluates the realism of generated images by comparing their distribution to that of real data. 4.1 Comparison with Baselines We compare the proposed method with several state-of-the-art image-editing methods, which operate on different types of conditions, such as the 3D transform, drag (point correspondence), and text prompt. The results are shown in Figure 7. Similar to our approach, 3DIT [Michel et al. 2024] and Zero123 [Liu et al. 2023] use the 3D transform as condition. However, 3DIT fails to generate plausible results because of the domain gap between its synthetic training data and real-world images, while Zero123 struggles with identity preservation. For the dragging-based methods InstantDrag [Shin et al. 2024] and MOFA-Video [Niu et al. 2024], we 3D-Fixup: Advancing Photo Editing with 3D Priors 7 Fig. 7. Comparison with baselines. We compare several state of the art baselines with different kinds of conditions, such as 3D transforms, drags, inpainting masks, and text prompts. We can see that none of the baselines accurately follow the target 3D transform while preserving identity. Baselines that directly use 3D transforms suffer from lack of good training data, and using other types of conditions makes it hard to unambiguously specify the 3D transform. Table 1. Quantitative comparison to baselines. We compare to the baselines using the LPIPS and FID metrics. The result shows that the 3D editing of the proposed method is closer to the ground truth and real distribution. Table 2. Quantitative results for training with different sets of training data. Data setting LPIPS FID Model LPIPS FID (5k) FID (30k) Magic Fixup [Alzayer et al. 2024] 3DIT [Michel et al. 2024] Zero123 [Liu et al. 2023] Instruct-pix2pix [Brooks et al. 2023] InsantDrag [Shin et al. 2024] Blended LDM [Avrahami et al. 2023] MOFA-Video [Niu et al. 2024] 0.5776 174.6926 0.4493 145.3389 0.6803 202.2304 0.7532 231.7562 0.4810 163.6477 0.5012 185.0291 0.3283 149.1247 Ours 0.2397 132.1145 27.1542 23.8392 44.8570 73.3829 34.6370 41.3255 20.9583 13. use the known correspondence between the source and transformed mesh to define an input drag. We find that drags are too ambiguous to clearly define 3D transform and both methods struggle to interpret larger drags, such as the rotation of the goldfish or the shoes. Blended LDM [Avrahami et al. 2023] takes the guidance image and the mask as inputs and adopts Blended Diffusion [Avrahami et al. 2022] to refine the coarse edit, which does not preserve identity. Finally, Instruct-pix2pix [Brooks et al. 2023] is instructed by text prompt, but suffers from its ambiguity. In contrast, our proposed method can generate high-quality edits for large 3D transformations while preserving identity. We also present comparison with Magic Fixup in Figure 9. The results demonstrate that our proposed method achieves more realistic images, benefiting from 3D-transformation-based guidance. For example, our method effectively handles pose changes, such as adjusting the cameras viewing direction or modifying the poses of subjects, as shown in the horse, jaguar, and cake examples. In contrast, Magic Fixup struggles with such edits. 4.2 3D Editing with Continuous Rotations We also demonstrate that the proposed method can handle extensive 3D edits on common objects, as illustrated in Figure 8. In each scenario, we progressively increase the rotation from 0 to 180 degrees along the y-axis, applying it to the reconstructed mesh to generate the 3D-transformation-based guidance image. The results show that our method successfully deals with out-of-plane 3D rotation edits, from minor adjustments to substantial transformations, highlighting the models 3D-awareness during editing. 151.6589 145.3312 Transform source + Transform Target + MF 0.2397 132. Transform source Transform source + MF 0.3874 0.3321 Table 3. Ablation study of conditioning. We evaluate the effect of different conditioning mechanisms with two mask configurations: (0.0,0.5,1.0) v.s. (0.0,1.0), and the impact of dropout of image features. Configurations Mask (0.0, 1.0) Without dropout Ours FID 17. 21.2870 13."
        },
        {
            "title": "4.3 Quantitative Comparison of 3D Editing",
            "content": "We compute metrics to evaluate the performance of methods as shown in Tab. 1. LPIPS is calculated for each model by measuring the similarity between its outputs and the ground-truth images. The final LPIPS score is obtained as the mean value across all pairs. FID assesses the realism of the generated results by comparing the distribution of the generated images to that of real video frames. The results demonstrate that the proposed method produces outputs that are highly realistic and align well with the real data as indicated by lower FID and LPIPS values. In terms of the detail preservation, we address the challenging task of hallucinating novel views and missing parts based on 洧냪guide. Since it relies only on single-view image to generate unseen regions, preserving identity and fine details is inherently difficult. However, LPIPS in Table 1 shows that 3D-Fixup achieves better detail preservation than prior methods. Finally, we also compare the runtime with Image-sculpting [Yenphraphai et al. 2024], an optimization-based method for image editing. The runtime is 877洧 per sample, while ours can achieve 2洧 for 50 DDIM steps."
        },
        {
            "title": "4.4 Ablation Study of Data Setting and Conditioning",
            "content": "We evaluate the importance of each data setting in Tab. 2. Using the setting with all the data yields the best performance in terms of FID and LPIPS. We also evaluate the effect of different conditioning mechanisms in Tab. 3. First, we consider different mask settings: (0.0, 0.5, 1.0) vs. (0.0, 1.0). Then we study the impact of dropout of image features for cross-attention. The results suggest that our conditioning outperforms other configurations. , Vol. 1, No. 1, Article . Publication date: May 2025. 8 Yen-Chi Cheng, Krishna Kumar Singh, Jae Shin Yoon, Alexander Schwing, Liang-Yan Gui, Matheus Gadelha, Paul Guerrero, and Nanxuan Zhao"
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced 3D-Fixup, workflow that tackles the problem of 3Daware image editing. To make 3D-aware image editing efficient, we adopt feedforward method. To train such model, data is crucial. Since collecting data with corresponding 3D edits is time-consuming and expensive, we developed an automatic framework to collect suitable data from real-world videos. The resulting method bridges the gap between 2D image editing and 3D transformations, enabling realistic edits that preserve content identity while maintaining fidelity across various perspectives. We demonstrated the effectiveness of our approach through extensive experiments, showcasing its ability to handle large out-ofplane rotations and translations, as well as challenging scenarios involving significant pose changes. Quantitative evaluations using LPIPS and FID metrics validate the realism and accuracy of our edits, outperforming state-of-the-art baselines such as Magic Fixup. We found that intricate details, such as sprinkles on donuts or textures on clothing, are sometimes not preserved well, likely due to image encoder limitations. Additionally, 3D-Fixup produces suboptimal results when 洧냪guide is of low quality. This occurs when the image-to-3D step performs poorly due to occlusion, incompleteness, or suboptimally detected mask, which can be mitigated by outpainting masks/objects. Future work may explore extending the framework to handle more complex scenes with multiple objects and refining the underlying 3D priors to enhance generalization across diverse datasets."
        },
        {
            "title": "Acknowledgments",
            "content": "Work supported in part by NSF grants 2008387, 2045586, 2106825, MRI 1725729, NIFA award 2020-67021-32799, and the Amazon-Illinois Center on AI for Interactive Conversational Experiences."
        },
        {
            "title": "References",
            "content": "Hadi Alzayer, Zhihao Xia, Xuaner Zhang, Eli Shechtman, Jia-Bin Huang, and Michael Gharbi. 2024. Magic Fixup: Streamlining Photo Editing by Watching Dynamic Videos. arXiv preprint arXiv:2403.13044 (2024). Omri Avrahami, Ohad Fried, and Dani Lischinski. 2023. Blended latent diffusion. ACM transactions on graphics (TOG) 42, 4 (2023), 111. Omri Avrahami, Rinon Gal, Gal Chechik, Ohad Fried, Dani Lischinski, Arash Vahdat, and Weili Nie. 2024. DiffUHaul: Training-Free Method for Object Dragging in Images. arXiv preprint arXiv:2406.01594 (2024). Omri Avrahami, Dani Lischinski, and Ohad Fried. 2022. Blended diffusion for text-driven editing of natural images. In CVPR. Sherwin Bahmani, Xian Liu, Wang Yifan, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, et al. 2025. Tc4d: Trajectory-conditioned text-to-4d generation. In European Conference on Computer Vision. Springer, 5372. Tim Brooks, Aleksander Holynski, and Alexei Efros. 2023. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1839218402. Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. 2025. Sparsectrl: Adding sparse controls to text-to-video diffusion models. In European Conference on Computer Vision. Springer, 330348. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems 30 (2017). Xingzhong Hou, Boxiao Liu, Yi Zhang, Jihao Liu, Yu Liu, and Haihang You. 2024. EasyDrag: Efficient Point-based Manipulation on Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 84048413. Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. 2012. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems 25 (2012). Ruining Li, Chuanxia Zheng, Christian Rupprecht, and Andrea Vedaldi. 2025. Dragapart: Learning part-level motion prior for articulated objects. In European Conference , Vol. 1, No. 1, Article . Publication date: May 2025. on Computer Vision. Springer, 165183. Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. 2023. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision. 92989309. Loshchilov. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017). Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Aniruddha Kembhavi, and Tanmay Gupta. 2024. Object 3dit: Language-guided 3d-aware image editing. Advances in Neural Information Processing Systems 36 (2024). Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. 2023. Dragondiffusion: Enabling drag-style manipulation on diffusion models. arXiv preprint arXiv:2307.02421 (2023). Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. 2024. Diffeditor: Boosting accuracy and flexibility on diffusion-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 84888497. Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, and Yinqiang Zheng. 2024. Mofa-video: Controllable image animation via generative motion field adaptions in frozen image-to-video diffusion model. In European Conference on Computer Vision. Springer, 111128. Xingang Pan, Ayush Tewari, Thomas Leimk칲hler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. 2023. Drag your gan: Interactive point-based manipulation on the generative image manifold. In ACM SIGGRAPH 2023 Conference Proceedings. 111. Karran Pandey, Paul Guerrero, Metheus Gadelha, Yannick Hold-Geoffroy, Karan Singh, and Niloy J. Mitra. 2024. Diffusion Handles: Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D. CVPR (2024). Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. 2024. Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks. arXiv:2401.14159 [cs.CV] Rahul Sajnani, Jeroen Vanbaar, Jie Min, Kapil Katyal, and Srinath Sridhar. GeoDiffuser: Geometry-Based Image Editing with Diffusion Models. 2024. arXiv:2404.14403 [cs.CV] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. 2023. Zero123++: Single Image to Consistent Multi-view Diffusion Base Model. arXiv:2310.15110 [cs.CV] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. 2024. Motioni2v: Consistent and controllable image-to-video generation with explicit motion modeling. In ACM SIGGRAPH 2024 Conference Papers. 111. Joonghyuk Shin, Daehyeon Choi, and Jaesik Park. 2024. InstantDrag: Improving Interactivity in Drag-based Image Editing. In SIGGRAPH Asia 2024 Conference Papers. 110. Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014). Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and Daniel Aliaga. 2023. Objectstitch: Object compositing with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1831018319. Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, He Zhang, Wei Xiong, and Daniel Aliaga. 2024. Imprint: Generative object compositing by learning identity-preserving representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 80488058. Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. 2024b. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint arXiv:2402.01566 (2024). Yikai Wang, Chenjie Cao, Ke Fan, Qiaole Dong, Yifan Li, Xiangyang Xue, and Yanwei Fu. 2024a. Repositioning the Subject within Image. arXiv preprint arXiv:2401.16861 (2024). Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. 2024. ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object Removal and Insertion. arXiv preprint arXiv:2403.18818 (2024). Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. 2024. SpatialTracker: Tracking Any 2D Pixels in 3D Space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2040620417. Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. 2024. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191 (2024). Jiraphon Yenphraphai, Xichen Pan, Sainan Liu, Daniele Panozzo, and Saining Xie. 2024. Image sculpting: Precise object editing with 3d geometry control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 42414251. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition. 586595. 3D-Fixup: Advancing Photo Editing with 3D Priors 9 Fig. 8. 3D editing with continuous rotations. We demonstrate that the proposed method can handle extensive 3D edits on common objects. In each scenario, we progressively increase the rotation from 0 to 180 degrees along the y-axis, applying it to the reconstructed mesh to generate the 3D-transformation-based image guidance. The results show that our method can handle large out-of-plane 3D rotations during editing. , Vol. 1, No. 1, Article . Publication date: May 2025. 10 Yen-Chi Cheng, Krishna Kumar Singh, Jae Shin Yoon, Alexander Schwing, Liang-Yan Gui, Matheus Gadelha, Paul Guerrero, and Nanxuan Zhao Fig. 9. Comparison with Magic Fixup. The results demonstrate that the proposed method achieves more realistic outputs by leveraging the 3D-transformationbased guidance. For instance, our method effectively handles pose changes, such as adjusting the cameras viewing direction for the cakes and jaguar, or modifying the poses of the horse and parrot. , Vol. 1, No. 1, Article . Publication date: May 2025."
        }
    ],
    "affiliations": [
        "Adobe Research, UK",
        "Adobe Research, USA",
        "University of Illinois Urbana-Champaign, USA"
    ]
}