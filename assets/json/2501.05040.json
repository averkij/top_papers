{
    "paper_title": "SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution",
    "authors": [
        "Chengxing Xie",
        "Bowen Li",
        "Chang Gao",
        "He Du",
        "Wai Lam",
        "Difan Zou",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated remarkable proficiency across a variety of complex tasks. One significant application of LLMs is in tackling software engineering challenges, particularly in resolving real-world tasks on GitHub by fixing code based on the issues reported by the users. However, many current approaches rely on proprietary LLMs, which limits reproducibility, accessibility, and transparency. The critical components of LLMs for addressing software engineering issues and how their capabilities can be effectively enhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a novel open-source LLM designed to effectively and efficiently resolve GitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval module and a code editing module. The retrieval module employs BM25 along with a lightweight LLM model to achieve coarse-to-fine file retrieval. Subsequently, the code editing module utilizes the other LLM model to generate patches for the identified files. Then, to mitigate the lack of publicly available datasets, we compile an extensive dataset that includes 110K GitHub issues along with their corresponding patches, and train the two modules of SWE-Fixer separately. We assess our approach on the SWE-Bench Lite and Verified benchmarks, achieving state-of-the-art performance among open-source models with scores of 23.3% and 30.2%, respectively. These outcomes highlight the efficacy of our approach. We will make our model, dataset, and code publicly available at https://github.com/InternLM/SWE-Fixer."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 0 4 0 5 0 . 1 0 5 2 : r SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution Chengxing Xie 1, 2 Bowen Li 1 Chang Gao 1, 3 He Du1 Wai Lam3 Difan Zou4 Kai Chen1 1Shanghai AI Laboratory 2Xidian University 4The University of Hong Kong 3The Chinese University of Hong Kong <xiechengxing34@gmail.com> <libowen.ne@gmail.com> <chenkai@pjlab.org.cn>"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated remarkable proficiency across variety of complex tasks. One significant application of LLMs is in tackling software engineering challenges, particularly in resolving real-world tasks on GitHub by fixing code based on the issues reported by the users. However, many current approaches rely on proprietary LLMs, which limits reproducibility, accessibility, and transparency. The critical components of LLMs for addressing software engineering issues and how their capabilities can be effectively enhanced remain unclear. To address these challenges, we introduce SWE-Fixer, novel open-source LLM designed to effectively and efficiently resolve GitHub issues. SWE-Fixer comprises two essential modules: code file retrieval module and code editing module. The retrieval module employs BM25 along with lightweight LLM model to achieve coarse-to-fine file retrieval. Subsequently, the code editing module utilizes the other LLM model to generate patches for the identified files. Then, to mitigate the lack of publicly available datasets, we compile an extensive dataset that includes 110K GitHub issues along with their corresponding patches, and train the two modules of SWE-Fixer separately. We assess our approach on the SWE-Bench Lite and Verified benchmarks, achieving state-of-theart performance among open-source models with scores of 23.3% and 30.2%, respectively. These outcomes highlight the efficacy of our approach. We will make our model, dataset, and code publicly available at https://github.com/InternLM/SWE-Fixer."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable progress in automating software engineering tasks beyond simple function-level code generation, such as HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). prominent example is SWE-Bench (Jimenez et al., 2023), benchmark where LLMs are tasked with resolving real-world GitHub issues by generating test-passing solutions using provided codebases. Current approaches to resolving the Github issues in SWE-Bench can be broadly categorized into two main paradigms: agent and pipeline. Agent-based systems rely on LLMs dynamically determining the next action, allowing them to autonomously explore codebase and resolve issues. In contrast, pipeline-based methods guide LLMs through series of predefined steps, such as initially identifying the defective files and subsequently editing them to resolve the issue. Despite significant advancements in agent and pipeline-based frameworks, most existing solutions depend on powerful proprietary models such as GPT-4o and Claude-3.5Sonnet. While these solutions are effective, they are often costly and lack of transparency, preventing deeper understanding and further improvements in the problem-solving ability of LLMs. This motivates us to develop an open-source LLM to resolve the Github issues effectively and efficiently. Equal Contribution. Corresponding author. There are also two options to train open-source LLMs: agent-based frameworks and pipeline-based methods. However, training open-source models in an agent-based framework presents several challenges. First, compared with frontier closed-source models such as GPT-4o, existing open-source models lack the robust agent capabilities necessary for self-directed decision-making, long-term planning, and effective tool use, limiting their potential for substantial improvements. Second, constructing training data for agent-based methods often requires access to real execution environment, which can be difficult to set up. Even with such an environment, the performance of the most advanced models is still not satisfactory to resolve real-world issues, making trajectory collection both expensive and inefficient. In contrast, pipeline-based methods simplify data construction and model training by explicitly defining each subtask. Notably, the Agentless framework (Xia et al., 2024) demonstrates competitive performance on the GitHub issue resolution task using proprietary models. However, Agentless employs complex, multi-stage design that artificially organizes problem-solving procedures into pipeline. For instance, in fault localization, the framework utilizes an intricate procedure that includes LLM-filtered dense retrieval, LLM-based file retrieval, class and function localization, and line localization. While this process may benefit cutting-edge closed-source LLMs, it poses significant challenges for training open-source LLMs. The difficulty lies in preparing training data for these steps and conducting effective model training at scale. To address these challenges, we introduce SWE-Fixer, simple yet effective pipeline-based approach for training open-source models to resolve Github issues. Unlike Agentless, which employs complex pipeline, SWE-Fixer simplifies the process by reducing the number of reasoning steps, making it more cost-efficient while retaining effectiveness. Our method decomposes the task into two core subtasks: code file retrieval and code editing. For the retrieval task, we use coarse-to-fine strategy combining BM25 for initial file retrieval and model to identify the defective files from the BM25 results. Once the defective files are identified, the editing model generates code patch to resolve the issue, trained using chain-of-thought (CoT) data. The streamlined pipeline design facilitates easier training data construction and efficient inference, eliminating the need for complex manipulations. We curate large-scale dataset comprising 110K instances for the training of both retrieval and editing tasks. To ensure data quality, we apply rigorous filtering techniques, making the training set both extensive and reliable. For model implementation, we fine-tune the Qwen2.5 (Qwen et al., 2024) base series models, which include 7B code retriever and 72B code editor. SWE-Fixer achieves state-of-the-art performance among open-source models. We evaluate SWE-Fixer on both SWE-Bench Lite and Verified, achieving 23.3% on SWE-Bench Lite and 30.2% on SWE-Bench Verified. Among open-source model-based approaches, SWE-Fixer achieves the highest Best@1 performance. Notably, compared to frameworks built on proprietary models, SWE-Fixer outperforms several methods on both benchmarks, including those based on GPT-4, GPT-4o, and Claude-3-Opus. We also conduct comprehensive study of training configurations for both tasks, providing insights into optimizing future systems. Additionally, the trained retriever and editor have the potential to serve as components in agent-based systems, enhancing the efficiency and effectiveness of existing agent-based approaches. Our contributions can be summarized as follows: State-of-the-art performance: We propose novel pipeline-based method that leverages open-source models, achieving state-of-the-art Best@1 performance on SWE-Bench Lite and Verified compared to other open-source model-based approaches. Large-scale dataset curation: We curate large-scale dataset comprising 110K instances with rigorous filtering techniques to ensure both the extensiveness and reliability of the training data. Comprehensive Analysis: We provide an in-depth analysis of the data configurations for each subtask, enabling optimized task performance."
        },
        {
            "title": "2 Related Works",
            "content": "Code Large Language Models The use of large language models (LLMs) for coding tasks has garnered substantial attention recently. Numerous LLMs, trained on extensive datasets of code, have been introduced, including closed-source models such as GPT-4 (Achiam et al., 2023), Gemini (Team et al., 2023), and Claude3.5 (Anthropic, 2024) and open-source models such as Code-LLaMA (Roziere et al., 2023), Mixtral (Jiang et al., 2024), Deepseek-Coder Zhu et al. (2024), and Qwen-Coder Hui et al. (2024). Beyond these general2 Figure 1: The pipeline of SWE-Fixer. Our framework contains two components: code file retrieval and code editing. The two frames of BM25 and Retriever indicate our coarse-to-fine retrieval method. The editor frame represents the code editing task. purpose code models, significant efforts have been made to tackle specific software engineering challenges with specialized LLMs. For instance, several studies have introduced strategies to improve program generation (Wang et al., 2024a; Huang et al., 2023; Zheng et al., 2023; Chen et al., 2024; Liu et al., 2024a) or enhance program repair (Lin et al., 2024; Jiang et al., 2023; Xia et al., 2022). In the domain of code retrieval, researchers have developed advanced techniques for refining code representations (Bui et al., 2021; Li et al., 2024; Martinez-Gil, 2024; Saieva et al., 2023) and improving fault retrieval (Qin et al., 2024; Du et al., 2024). LLMs for GitHub Issue Solving In the field of software engineering, one critical task has gained considerable attention: resolving repository issues using LLMs. This task involves providing LLMs with specific code issue alongside the corresponding codebase and expecting the model to generate solution. widely recognized benchmark for this task is SWE-Bench (Jimenez et al., 2023), which consists of 2,294 issues meticulously curated from 12 real-world, high-quality GitHub repositories. Various agent-based approaches, including SWE-agent (Yang et al., 2024), Autocoderover (Zhang et al., 2024), OpenHands (Wang et al., 2024b), and Moatless Tools (Albert Örwall, 2024), built on proprietary models, have been proposed to enhance the performance of LLMs on SWE-Bench. These methods equip LLMs with various tools, allowing them to autonomously explore the codebase and resolve issues. SWE-SynInfer (Ma et al., 2024) and SWE-Gym (Pan et al., 2024) attempt to train open-source models in an agent-based framework to tackle these challenges. Additionally, pipeline-based methods, such as Agentless (Xia et al., 2024), have been introduced for this task, showing promising results compared to state-of-the-art agent-based methods. Agentless shares similar philosophy with our SWE-Fixer but differs significantly in its approach. Overall, Agentless features highly sophisticated architectural design that leverages powerful proprietary models, this complexity poses challenges for training open-source models. In the retrieval stage, Agentless employs multi-step process, including LLM-prompted file retrieval, LLM-filtered dense retrieval, and multi-level fine-grained retrieval, ranging from class and function levels to individual lines. However, attempts to replicate such intricate cascaded steps has proven ineffective for open-source model training (see Section 5.4). To address this, SWE-Fixer adopts simpler yet effective two-step retrieval approach, combining BM25 for coarse retrieval with trained retrieval model to further identify defective files. This streamlined method is not only cost-efficient but also demonstrates strong performance in our experiments. Similarly, Agentless introduces significant complexity in the editing stage by requiring the LLM to generate patches, reproduce tests, and select test files, resulting in heavy and demanding pipeline that is difficult for open-source LLMs to learn and execute. In contrast, SWE-Fixer simplifies the editing task by generating patches using only retrieved file content and incorporating chain-of-thought (CoT) mechanism to enhance performance. Ultimately, compared to Agentless, SWE-Fixer offers simpler and more robust design that facilitates easier training data construction, is better suited for finetuning open-source models, and delivers impressive results on SWE-Bench."
        },
        {
            "title": "3 SWE-Fixer",
            "content": "The SWE-Fixer framework is designed to efficiently address real-world GitHub issues by generating code patches that specify the necessary modifications to repositorys codebase. We divide this task into two 3 subtasks: code file retrieval and code editing. The code file retrieval subtask identifies which files require modification, while the code editing subtask implements the necessary changes. As shown in Figure 1, we employ simple pipeline approach to optimize open-source models for each subtask. The following sections provide detailed overview of these components within the SWE-Fixer framework. 3.1 Code File Retrieval To efficiently identify relevant files for modification in repository, we adopt coarse-to-fine strategy. First, we use BM25 (Robertson et al., 2009) to retrieve the 30 most relevant files to the issue, treating the issue description as query and the code files as documents. Next, we finetune language model to further pinpoint which files require modification from the BM25-retrieved files. We chose BM25 over the dense retrieval method used in the Agentless (Xia et al., 2024) and Moatless Tools (Albert Örwall, 2024) because it provides lightweight, scalable and robust approach for initial file retrieval, especially when the number of files in repository is large. The subsequent language model refinement step enhances accuracy by narrowing down the selection to the most relevant files. Given the large potential context of all files, we take inspiration from the Agentless framework and use file documentations (skeletons) as input. file documentation includes module docstrings, class headers, class methods, and function signatures, retaining only the signatures of class methods and the first and last five lines of functions (see example in Appendix B). This approach significantly reduces context size while preserving the essential information for effective file-level retrieval. 3.2 Code Editing The code editing task involves generating patch to resolve an issue based on the relevant files. During training, we utilize gold defective files, i.e., those within the patches. For inference, we use retrieved files. To provide sufficient context, we include the content of all retrieved files, even though only small portion of the content in these files would be modified. Including additional context helps the model better understand the issue and apply the necessary changes effectively. To assist the model in generating valid code modifications, we define structured output that includes three key components (see Appendix C). The first component is the file path, which specifies the location of the file to be modified. The second is the original code block, which includes the specific code snippet to be edited along with its line numbers. The third component is the modified code block, which represents the final modification result but excludes line numbers as the new line numbers are difficult for the model to calculate. The input includes complete file content with line numbers. This approach ensures that the model can efficiently identify the lines to edit and produce valid modifications. In contrast, the standard patch format3 requires line number calculations in the output hunk, which adds complexity. Our structured output simplifies model training and can be transformed to patches automatically for evaluation."
        },
        {
            "title": "4 Model Training",
            "content": "4.1 Structured Instruction Tuning We adopt structured approach, JsonTuning (Gao et al., 2023), to train our models, enhancing the overall pipeline performance. As shown in Figure 2, the input JSON structure includes task input elements, task instructions, and output control information, while the output JSON structure consists of task output elements. For the code file retrieval task, input elements include issues and file documentations, and output elements include files to be edited. For the code editing task, input elements include issues and file content, and output elements include reasoning processes, original code snippets, and modified code snippets. The employment of JsonTuning offers several advantages: (1) By incorporating explicit task structures into the input and output representations during instruction tuning, JsonTuning enhances the models comprehension of critical task elements and their interrelations, thereby improving generalization capabilities. (2) Given the inherently structured nature of code, JsonTuning efficiently leverages this structured information. (3) JsonTuning provides 3https://git-scm.com/docs/diff-generate-patch 4 (a) The code file retrieval task. Figure 2: Structured representation of the retrieval/editing tasks inputs and outputs. (b) The code editing task. structured output that is more robust compared to generating patches, which often involve stricter syntax and pose greater challenges for model interpretation. Post-processing To ensure that the model-generated outputs meet the task requirements, we implement post-processing procedure with unified resampling strategy for both the retrieval and code editing tasks to ensure output validity. The sampling temperature is initially set to 0 for deterministic outputs. If the output fails or is invalid, the temperature is adjusted to 0.7 for further attempts. To prevent endless retries, the maximum number of attempts is set to 5. For the retrieval task, outputs are checked for JSON syntax validation, which means, triggering resampling if invalid. For the code editing task, we similarly require the model-generated results to initially pass JSON validation. Once validated, the model-generated modifications are applied to the original file. If the original code snippet cannot be located, the result is deemed invalid. Additionally, if the modified code fails to pass syntax checks after replacement, it is also considered invalid. This post-processing procedure effectively improves the correctness of the model-generated outputs in terms of format and content, ensuring that they better meet the requirements of real-world applications. 4.2 Training Data In line with the data collection methodology outlined in SWE-Bench , we gather high-quality issues, pull requests, and codebases from GitHub repositories for training purposes. Subsequently, we apply filtering process to enhance data quality and eliminate overly complex training examples. 4.2.1 Data Collection Repository Collection We use Github REST API to crawl list of initial repositories. We select Python repositories that have more than 100 pull requests (PRs). In the official SWE-Bench code base, an instance extraction script is used to crawl SWE-Bench-style instances, each corresponding to an issue with PR and set of potential unit tests. We find that this script relies on string match 4 to identify valid issue PR pairs. Therefore, it has large chance to miss out many instances. We instead rely on GitHub events5 to conduct the 4https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/ linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword 5https://docs.github.com/en/rest/using-the-rest-api/issue-event-types?apiVersion= 2022-11-28 crawling job. Eventually, for the raw data, we collect 2.3K repositories and 331K instances, where repositories within SWE-Bench have been excluded. (a) (b) (c) Figure 3: We sample subset of the crawled raw data to analyze the statistics on real-world data. (a) The histogram of the number of edited files. (b) The distribution of the number of edited code lines. (c) The histogram shows the number of edited code hunks. Data Statistics As shown in Figure 3a, most real-world instances involve small number of edited files. Specifically, 54.7% of instances involve modifications to only one file, and nearly 80% involve no more than three files. Figure 3b shows the distribution of modified lines, with 73.7% of instances involving modifications to 099 lines and over 85% involving up to 200 lines. Similarly, Figure 3c reveals that instances involving more modification regions (hunks) occur less frequently. Data Filtering To address the challenges posed by the messy nature of real-world data, we apply instancelevel data filtering to ensure higher data quality. For the sake of efficiency, we sample subset of 140K instances, parse the oracle code patches and discard instances whose patches cannot be parsed. Then we filter out instances where the number of edited files (excluding test files) is more than three. This decision is based on our observation in the data statistics and editing more than three files introduces excessive complexity, hindering effective model training. This process results in train set of 110K instances, SWE-Fixer-Train-110K. 4.2.2 Chain-of-Thought Data Construction The data collected from real-world scenarios typically includes only the specific codebase associated with an issue and the corresponding gold patches, without capturing the intermediate reasoning behind these patches. To address this gap, we construct Chain-of-Thought (CoT) data for the code editing task. Previous research (Wei et al., 2022; Wang et al., 2022) has shown that CoT can significantly improve model performance on tasks requiring reasoning. straightforward approach to generating CoT data involves prompting teacher model to produce the reasoning process and then performing rejection sampling. However, this approach presents several challenges. Due to the complexity of the task, even advanced proprietary models struggle to achieve high accuracy when performing code editing independently. Additionally, without access to real code execution environment, we are unable to verify the patches generated by the teacher model through execution, making standard rejection sampling infeasible. To address these challenges, we adopt methodology inspired by the rationalization approach in Zelikman et al. (2022). Specifically, we use the gold patches as part of the input to guide the teacher model in generating both the reasoning process and the corresponding patches. The model is tasked with producing reasoning chain and code patch as if it were unaware of the correct answer. Detailed prompts can be found in Appendix A. We use GPT-4o for the generation, and the resulting reasoning chains are generally coherent and sound."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup We finetune the Qwen2.5 (Qwen et al., 2024) base series models to implement SWE-Fixer, which consists of 7B code retriever and 72B code editor. The training is conducted on 96 Nvidia A800 GPUs using the xtuner-lite framework (Contributors, 2023), with global batch size of 96 and 64K context window. We evaluate SWE-Fixer on two datasets: SWE-Bench Lite and SWE-Bench Verified. SWE-Bench Lite, part of the official SWE-Bench benchmark (Jimenez et al., 2023), consists of 300 selected instances optimized for efficient evaluation. SWE-Bench Verified, proposed by OpenAI, is human-validated subset that offers more reliable evaluation. Each instance includes real-world GitHub issue description paired with its corresponding codebase. Model-generated code patches are assessed using developer-written unit tests, and accuracy is calculated as the percentage of instances successfully resolved. 5.2 Dataset Preparation For the code file retrieval task, we filter out those where the gold defective files do not appear in the top 30 retrieved files, or where the total length exceeds the 64K context window limit. This results in 80K valid training instances for the retrieval task, denoted as SWE-Fixer-Retrieval-80K. For the code editing task, due to computational resource constraints and API limits, we sample subset of 70K instances, referred to as SWE-Fixer-Editing-70K, and construct CoT data for this subset, SWE-Fixer-Editing-70K-CoT. To explore optimal data formats and training strategies, we further sample 200 repositories, each contributing 50 instances (10K instances total), to create smaller dataset, SWE-Fixer-Train-10K, for the ablation study (see Section 5.4). 5.3 Main Results Table 1 presents the primary results on SWE-Bench Lite and SWE-Bench Verified. We categorize the results into two groups: the first uses open-source frameworks based on proprietary models, while the second involves approaches built entirely on open-source frameworks and models.6 We begin by comparing SWE-Fixer with open-source frameworks based on proprietary models. Remarkably, our approach outperforms several methods across both SWE-Bench Lite and Verified, especially those based on GPT-4, GPT-4o, and Claude-3-Opus. The exceptions are SWE-SynInfer (Ma et al., 2024) and Agentless (Xia et al., 2024) using GPT-4o. This is encouraging, as these baselines typically involve complex agent frameworks, specifically designed for the SWE-Bench task, and rely on advanced proprietary models. Our results suggest that SWE-Fixer offers promising, cost-effective alternative to proprietary model-based frameworks. We also note that Claude-3.5-Sonnet excels in coding tasks, delivering consistent improvements over all open-source methods, where our approach still lags behind such systems. Among the approaches based on open-source models, our SWE-Fixer framework achieves the highest Best@1 performance. SWE-Gym-32B (Pan et al., 2024) achieves higher Best@8 results with its specially trained 32B verifier, representing promising direction for the task by enabling reinforcement learning in executable environments. However, SWE-Gyms reliance on substantial human effort for environment construction limits its scalability. SWE-Fixer outperforms SWE-SynInfer, an agent-based method. While SWE-SynInfer uses finetuned 72B model and requires up to eight reasoning steps, SWE-Fixer provides lightweight solution with only two steps (7B retriever and 72B editor). This approach not only significantly reduces inference costs but also achieves 1.3% higher accuracy on SWE-Bench Lite compared to SWE-SynInfer. These observations suggest that our approach is simple, effective, and cost-efficient solution, particularly in the absence of automatic environment construction. 6Closed-source methods are omitted from Table 1, as the lack of technical transparency limits the discussion of meaningful insights. 7 Table 1: Performance of various models/methods on SWE-Bench Lite and Verified. : Finetuned open-source models. : All results are reported as Pass@1 or Best@1 in this table, except for SWE-Gym, which uses specially trained 32B verifier. Method Model Type Verified Lite Open-source Methods w/ Proprietary Models GPT-4 RAG (Jimenez et al., 2023) Claude-3-Opus RAG (Jimenez et al., 2023) Claude-3-Opus SWE-agent (Yang et al., 2024) GPT-4o SWE-agent (Yang et al., 2024) GPT-4o AppMap Navie (AppMap, 2024) GPT-4 AutoCodeRover (Zhang et al., 2024) SWE-agent + RepoGraph (Ouyang et al., 2024) GPT-4o AutoCodeRover + RepoGraph (Ouyang et al., 2024) GPT-4 GPT-4o OpenHands (Wang et al., 2024b) GPT-4o AutoCodeRover (Zhang et al., 2024) GPT-4o SWE-SynInfer (Ma et al., 2024) Claude-3.5-Sonnet SWE-agent (Yang et al., 2024) GPT-4o Agentless (Xia et al., 2024) Claude-3.5-Sonnet-20241022 Moatless Tools (Albert Örwall, 2024) Claude-3.5-Sonnet-20241022 AutoCodeRover-v2.0 (Zhang et al., 2024) Claude-3.5-Sonnet-20241022 Agentless (Xia et al., 2024) Claude-3.5-Sonnet-20241022 OpenHands (Wang et al., 2024b) Open-source Methods w/ Open-source Models RAG (Jimenez et al., 2023) AutoCodeRover (Liu et al., 2024b) SWE-Gym (Best@1) (Pan et al., 2024) SWE-SynInfer (Ma et al., 2024) SWE-Fixer (Ours) SWE-Gym (Best@8 w/ Verifier) (Pan et al., 2024) SWE-Llama-13B Qwen2-72B-Instruct SWE-Gym-32B Lingma-SWE-GPT-72B SWE-Fixer-72B SWE-Gym-32B Pipeline Pipeline Agent Agent Pipeline Agent Agent Agent Agent Agent Agent Agent Pipeline Agent Agent Pipeline Agent Pipeline Agent Agent Agent Pipeline Agent 2.8 7.0 18.2 23.0 26.2 - - - - 28.8 31.8 33.6 38.8 - - 50.8 53.0 1.2 - - 30.2 30.2 - 2.7 4.3 11.7 18.3 21.7 19.0 20.3 21.3 22.0 22.0 20.7 23.0 32.0 38.3 46.2 40.7 41.7 1.0 9.3 19.1 22.0 23.3 26.0 5.4 Ablation Study 5.4.1 Code File Retrieval Table 2: Ablation study of the code file retrieval task on SWE-Bench Lite. The table compares precision and recall across different methods and training datasets, using Qwen2.5-7B as the base model. : The default setting utilizes 64K context window, which includes the readme file and BM25-retrieved file documentation limited to 30 files. : Fills the entire 64K context window with file documentation, which may exceed the default limit of 30 files. : Trains with additional 100K editing data without CoT, which is sampled from SWE-Fixer-Train-110K. Method BM25 Top-3 BM25 Top-30 Default setting - Remove readme - Remove 30 files limit - 32K context limit - Add file content Training Dataset BM25 Baseline - - Training on Limited Data SWE-Fixer-Train-10K SWE-Fixer-Train-10K SWE-Fixer-Train-10K SWE-Fixer-Train-10K SWE-Fixer-Train-10K Training on Full Data Precision(%) Recall(%) 18.9 2.9 56.7 86.7 65.4 65.2 ( 0.2) 64.9 ( 0.5) 64.3 ( 1.1) 59.7 ( 5.7) 67.3 66.7 ( 0.6) 68.7 ( 1.4) 64.7 ( 2.6) 60.3 ( 7.0) Default setting Mix training SWE-Fixer-Retrieval-80K SWE-Fixer-Retrieval-80K + 100K Editing Data 68.5 ( 3.1) 69.4 ( 4.0) 69.0 ( 1.7) 69.7 ( 2.4) 8 As shown in Table 2, we conduct series of ablation experiments on the code file retrieval task to evaluate the impact of various input configurations on model performance. The default setup employs 64K context window that includes the readme file along with BM25-retrieved file documentation, limited to maximum of 30 files. The model is trained to predict list of files requiring modification. Irrelevant information adversely affects performance. Providing relevant input context is critical in the code file retrieval task. Including unnecessary details, such as the entire file content, leads to decline in performance since the retrieval task does not require such fine-grained code information. Moreover, increasing the number of input files to fully occupy the context window improves recall but reduces precision, which offers no performance gains for the overall pipeline, as shown in Table 4, while also demanding more computational resources. Conversely, incorporating relevant information, such as the readme file, improves model performance. Smaller context windows reduce effectiveness. smaller context window (32K) restricts the number of input files, which may lead to the exclusion of target defective files. This limitation significantly lowers recall within the input context, ultimately resulting in poorer overall performance compared to model using larger 64K context window. Larger datasets improve model performance. Expanding the training dataset size from 10K to 80K significantly boosts the models performance. Additionally, incorporating data from both the retrieval and editing tasks during training further enhances retrieval performance. This improvement highlights the beneficial influence of editing task data on the retrieval task. 5.4.2 Code Editing Table 3: Ablation study of the code editing task on SWE-Bench Lite with gold defective code files as input using Qwen2.5-72B and the SWE-Fixer-Train-10K. Cls&Func refers to classes and functions. : The default setting uses only file content with line numbers. : Contains only Cls&Func content, without file content. Method Default setting - Only Cls&Func Content - Add Cls&Func Name - Add readme - Remove Line Number Resolve (%) 20.0 18.0 ( 2.0) 22.0 ( 2.0) 19.0 ( 1.0) 14.0 ( 6.0) Table 4: Impact of different retrieval training methods of the 7B model on the overall pipeline performance of SWEBench Lite, using the 72B editor model. Both the retrieval and editor models are trained on SWE-Fixer-Train-10K. : Fills the entire 64K context window, which may exceed the default limit of 30 files. : In this setup, the retriever is trained to additionally retrieve class and function names, where the 72B editor model is also specifically trained to incorporate the input change. Method Pre/Recall(%) Resolve(%) Default setting - Remove 30 file limit - Also retrieve Cls&Func 65.4/67.3 64.9/68.7 54.0/56.0 16.3 16.0 ( 0.3) 15.7 ( 0.6) As shown in Table 3, we also conduct detailed ablation study on the code editing task to evaluate the impact of different data configurations. All experiments use gold input (oracle defective files). The default configuration includes complete file content with line numbers, providing better contextual understanding and precise localization. Enhanced location information improves performance. Including line numbers acts as an anchor, helping LLMs locate and edit specific code snippets more effectively, which improves performance. Adding oracle class and function names that require editing also enhances performance by providing additional locationspecific details. However, this additional information increases the complexity of retrieval tasks and reduces the overall pipeline performance given retrieved class and function names (see Table 4). Redundant or insufficient information reduces performance. The readme file, while useful in retrieval tasks, introduces high-level abstract information that is irrelevant to the editing task. Editing tasks require detailed understanding of flawed code, and the inclusion of readme information detracts from this, resulting in lower performance. Insufficient information, such as input limited to class and function content, also negatively impacts performance. This suggests that class and function-only inputs omit essential context needed for effective editing."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we present SWE-Fixer, an open-source framework designed to efficiently and effectively address real-world software engineering challenges using finetuned open-source models. SWE-Fixer adopts pipelinebased approach, dividing the task into two subtasks: code file retrieval and code editing, requiring only two steps to generate the final results. To support model training, we curate large-scale, real-world dataset and construct task-specific training data for both subtasks. Our method demonstrates impressive performance on SWE-Bench Lite and Verified, achieving the highest Best@1 performance among open-source model-based approaches, while maintaining low inference steps and minimal computational overhead. Notably, SWE-Fixer outperforms several methods based on proprietary models, including those leveraging GPT-4, GPT-4o, and Claude-3-Opus. By offering simple yet effective approach to training models for software engineering tasks, SWE-Fixer lowers barriers for the community and fosters further innovation in this domain."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Albert Örwall. moatless-tools. https://github.com/aorwall/moatless-tools, 2024. Anthropic. Claude 3.5 sonnet. https://www.anthropic.com/news/claude-3-5-sonnet, 2024. AppMap. Appmap navie. appmap-navie-swe-bench-leader/, 2024. https://appmap.io/blog/2024/06/20/ Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Nghi DQ Bui, Yijun Yu, and Lingxiao Jiang. Self-supervised contrastive learning for code retrieval and summarization via semantic-preserving transformations. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 511521, 2021. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. Mouxiang Chen, Hao Tian, Zhongxin Liu, Xiaoxue Ren, and Jianling Sun. Jumpcoder: Go beyond autoregressive coder via online modification. arXiv preprint arXiv:2401.07870, 2024. XTuner Contributors. Xtuner: toolkit for efficiently fine-tuning llm. https://github.com/InternLM/ xtuner, 2023. Xiaohu Du, Ming Wen, Jiahao Zhu, Zifan Xie, Bin Ji, Huijun Liu, Xuanhua Shi, and Hai Jin. Generalization-enhanced code vulnerability detection via multi-task instruction fine-tuning. arXiv preprint arXiv:2406.03718, 2024. Chang Gao, Wenxuan Zhang, Guizhen Chen, and Wai Lam. Jsontuning: Towards generalizable, robust, and controllable instruction tuning. arXiv preprint arXiv:2310.02953, 2023. 10 Baizhou Huang, Shuai Lu, Weizhu Chen, Xiaojun Wan, and Nan Duan. Enhancing large language models in coding through multi-perspective self-consistency. arXiv preprint arXiv:2309.17272, 2023. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Nan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. Impact of code language models on automated program repair. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pp. 14301442. IEEE, 2023. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Haochen Li, Xin Zhou, and Zhiqi Shen. Rewriting the code: simple method for large language model augmented code search. arXiv preprint arXiv:2401.04514, 2024. Bo Lin, Shangwen Wang, Ming Wen, Liqian Chen, and Xiaoguang Mao. One size does not fit all: Multigranularity patch generation for better automated program repair. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, pp. 15541566, 2024. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36, 2024a. Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Fei Wang, Michael Shieh, and Wenmeng Zhou. Codexgraph: Bridging large language models and code repositories via code graph databases. arXiv preprint arXiv:2408.03910, 2024b. Yingwei Ma, Rongyu Cao, Yongchang Cao, Yue Zhang, Jue Chen, Yibo Liu, Yuchen Liu, Binhua Li, Fei Huang, and Yongbin Li. Lingma swe-gpt: An open development-process-centric language model for automated software improvement. arXiv preprint arXiv:2411.00622, 2024. Jorge Martinez-Gil. Improving source code similarity detection through graphcodebert and integration of additional features. arXiv preprint arXiv:2408.08903, 2024. Siru Ouyang, Wenhao Yu, Kaixin Ma, Zilin Xiao, Zhihan Zhang, Mengzhao Jia, Jiawei Han, Hongming Zhang, and Dong Yu. Repograph: Enhancing ai software engineering with repository-level code graph. arXiv preprint arXiv:2410.14684, 2024. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with SWE-Gym, 2024. Yihao Qin, Shangwen Wang, Yiling Lou, Jinhao Dong, Kaixin Wang, Xiaoling Li, and Xiaoguang Mao. Agentfl: Scaling llm-based fault localization to project-level context. arXiv preprint arXiv:2403.16362, 2024. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2024. URL https://arxiv.org/abs/2412.15115. Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389, 2009. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. Anthony Saieva, Saikat Chakraborty, and Gail Kaiser. Reinforest: Reinforcing semantic code similarity for cross-lingual code search models. arXiv preprint arXiv:2305.03843, 2023. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, and Hugh Zhang. Planning in natural language improves llm search for code generation. arXiv preprint arXiv:2409.03733, 2024a. Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Opendevin: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024b. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. Practical program repair in the era of large pre-trained language models. arXiv preprint arXiv:2210.14179, 2022. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489, 2024. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. arXiv preprint arXiv:2405.15793, 2024. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, pp. 15921604, 2024. Lin Zheng, Jianbo Yuan, Zhi Zhang, Hongxia Yang, and Lingpeng Kong. Self-infilling code generation. In Forty-first International Conference on Machine Learning, 2023. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024."
        },
        {
            "title": "A CoT Generation Prompt",
            "content": "Cot Generation System Prompt You are an expert software engineer and seasoned code reviewer, specializing in bug localization and code optimization within real-world code repositories. Your role is to meticulously analyze code and provide clear, logical reasoning to guide the resolution of issues within the codebase. In this role, you focus on the precision and effectiveness of the problem-solving process. Your expertise includes understanding complex code structures and accurately mapping issues to the specific parts of the code requiring modification. You excel at breaking down the reasoning process into coherent, easy-to-follow steps that lead to efficient and accurate code fixes. In this task, we are training model to generate code modifications for resolving issues within realworld codebases. For this, we have the issue description, the codebase, and the corresponding oracle code modifications. Your task is to generate detailed reasoning to aid in collecting high-quality training data. Your reasoning process must be thorough, evidence-based, and strictly adhere to the provided issue. Although oracle code modifications are available, **you should simulate reasoning independentlyas if you are identifying the necessary files and changes without prior knowledge of those modifications**. Avoid statements like \"The edited code makes sense because. . . \" that imply direct knowledge of the oracle modifications. 13 Cot Generation User Prompt # Issue Statement: {problem_statement} # File Content to be Modified: You are provided with the files that require modification to resolve the issue. This includes the full file content. You should identify the code snippets to be modified based on the issue and the file content. {content} # Oracle Code Modifications: {target} # Task Objective: Your objective is to develop clear and logical reasoning process that guides the modification of the code snippets based on the issue. The reasoning should explain the relationship between the issue and each code snippet, and why the modifications are necessary. # Reasoning Process Guidelines: The reasoning process should generally include the following steps. You may adjust these steps as needed for clarity and accuracy: 1. **Issue Analysis**: - Begin by **clearly articulating the issue**. Provide comprehensive explanation of why this issue is significant, highlighting the specific challenges or obstacles that must be addressed. Identify the key requirements or objectives necessary for resolving the issue, ensuring that all aspects of the issue are thoroughly examined and understood. 2. **Task Decomposition*: - Break down the overall issue into **smaller, manageable sub-tasks**. Explain the purpose of each sub-task and its significance in solving the issue. Ensure that sub-tasks are logically ordered and clearly connected. 3. **Code Localization and Editing**: - First, for each sub-task, identify the relevant **code snippet** by providing the file path and referring to the specific part of the code related to that sub-task. Next, give detailed explanation of how this code snippet is connected to the sub-task, explain how the code should be edited to resolve the issue and justify why these changes are necessary. Finally, provide the edited code based on the explanation. - Ensure that the final output for this part MATCHES the provided oracle modifications EXACTLY. # General Requirements: 1. **Clear and Evidence-Based Reasoning**: Provide clear and precise reasoning for each step, strictly based on the provided issue and code without inferring information not explicitly stated. 2. **Comprehensive and Concise**: Address all relevant aspects of the issue comprehensively while being concise. Justify the exclusion of any sections that are not relevant. 3. **Detailed Guidance**: Ensure the reasoning steps are detailed enough to allow someone unfamiliar with the solution to infer and implement the necessary code modifications. 4. **Faithfulness**: Ensure that your final output for the code modifications MATCHES the provided oracle modifications EXACTLY. 5. **Neutral Perspective**: Approach the issue as if you do not know the correct answer in advance. Avoid language that implies prior knowledge of the correct modifications. # Format Requirements: 1. **File path**: Always mention the file path when referring to code snippet (including class or function names). 2. **Reasoning Process Format**: Use markdown to present your reasoning process. Clearly define each step and ensure logical connections between them. 3. **Code Snippet**: You must include **line numbers** when referring to the original code for context and outputing code_snippet_to_be_modified. However, do **not include line numbers** in your editing suggestions. Please ensure your response is clearly formatted and provides enough detail to justify why each code section was selected for modification and how it should be edited."
        },
        {
            "title": "B File Documentation",
            "content": "Figure 4: Example of file documentation. The documentation includes the relative file path and module docstring (if available). It also contains class names, their associated docstrings, and all method names. For functions, only the name and the first/last five lines of code are included."
        },
        {
            "title": "C Edit Output Patch Example",
            "content": ""
        },
        {
            "title": "D Case Study",
            "content": "We perform an in-depth analysis of several representative special cases and identify key issues in the models predictions. In some instances, the models output deviates from the standard code patch but still provides reasonable edits based on the retrieved results, ultimately resolving the problem effectively. For instance, in the case of psf__requests-2674, the standard solution identifies the issue in the file requests/adapter.py, while our model retrieves requests/expectation.py. According to the standard evaluation, the models prediction would be considered incorrect. However, the patch generated during the editing stage successfully resolves the underlying issue. This illustrates that, despite differing from the standard solution, the models approach is both valid and effective. This observation highlights critical limitation of evaluation methods that rely solely on rule-based matching. Such methods fail to account for the diversity of valid solutions, potentially overlooking unique and effective editing strategies. To address this, future research should prioritize the development of real code execution environments to evaluate repair effectiveness through actual execution, rather than relying exclusively on static comparisons with standard answers."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "The Chinese University of Hong Kong",
        "The University of Hong Kong",
        "Xidian University"
    ]
}