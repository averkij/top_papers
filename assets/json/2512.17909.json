{
    "paper_title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
    "authors": [
        "Shilong Zhang",
        "He Zhang",
        "Zhifei Zhang",
        "Chongjian Ge",
        "Shuchen Xue",
        "Shaoteng Liu",
        "Mengwei Ren",
        "Soo Ye Kim",
        "Yuqian Zhou",
        "Qing Liu",
        "Daniil Pakhomov",
        "Kai Zhang",
        "Zhe Lin",
        "Ping Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 9 0 9 7 1 . 2 1 5 2 : r Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing Shilong Zhang1 He Zhang2 Zhifei Zhang2 Chongjian Ge2 Shuchen Xue3 Shaoteng Liu2 Mengwei Ren2 Soo Ye Kim2 Yuqian Zhou2 Qing Liu2 Daniil Pakhomov2 Kai Zhang2 Zhe Lin2 Ping Luo 1The University of Hong Kong 2Adobe Research 3University of Chinese Academy of Sciences Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoders inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce semanticpixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into highly compact representation (96 channels with 16 16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components. Date: December 20th, 2025 Project Page: https://jshilong.github.io/PS-VAE-PAGE/"
        },
        {
            "title": "1 Introduction",
            "content": "Representation encoders trained via self-supervision (Caron et al., 2021; Oquab et al., 2023; Siméoni et al., 2025; He et al., 2019, 2022) or contrastive learning (Radford et al., 2021; Tschannen et al., 2025; Bolya et al., 2025) have established themselves as the cornerstone of visual understanding. They produce highly discriminative, semantic-rich features that generalize exceptionally well, enabling efficient adaptation to downstream tasks with limited data (Liu et al., 2023, 2024a). From dense prediction tasks to complex reasoning in Large Vision Language Models, these encoders have become the universal bedrock of visual analysis. Yet, despite this pervasive dominance, these powerful representations have yet to conquer the generative domain. Instead, state-of-the-art generative systems predominantly rely on Variational Autoencoder (VAE) (Kingma and Welling, 2013), which operates on low-level, compact latents trained with pixel reconstruction objective. These VAE latents lack the high-level semantic structure of representation encoders, forcing diffusion models to learn visual concepts from scratch and necessitating massive computational resources (Esser et al., 2024). To bridge this divide and achieve the long-sought goal of unifying perception and generation, natural question arises: can we migrate the generative modeling space from VAE latents to the representation-encoder space, enabling diffusion models to directly benefit from the representation encoders discriminative and semantically structured features? Recent work, RAE (Zheng et al., 2025), offers pioneering answer to this question. By redesigning the DiT architecture to handle high-dimensional features, it successfully enables generation within the representation space, achieving impressive results on the class-conditional ImageNet benchmark (Russakovsky et al., 2015). However, the efficacy of 1 this paradigm does not easily translate to open-world applications. When extended to practical text-to-image synthesis and complex instruction-based editing tasks, RAE exhibits significant performance limitations compared to mature VAE-based baselines, as highlighted in Figure 1. To uncover the root causes of this degradation, we analyze the behavior of the representation space both experimentally and theoretically in Section 3.2, identifying two key issues: insufficient compact regularization of representation features, leading to off-manifold latent generation, together with weak pixel-level reconstruction, which prevents the generator from learning accurate geometry and texture. The first issue is that generation over representation features is performed in an unconstrained space without compact regularization, leading to mismatch between the high dimensionality of representation features and their much lower intrinsic information content. Training on such redundant high-dimensional space makes the diffusion model prone to producing off-manifold latents1, ultimately leading to inaccurate and structurally distorted objects. We verify this phenomenon by visualizing off-manifold outliers through toy fitting experiment and theoretical analysis (Section 3.2 and Figure 3). This observation motivates us to regularize the generative space: we propose S-VAE, which maps the frozen representation features into compact, KLregularized latent space (Rombach et al., 2022) via semantic autoencoder. This constraint effectively eliminates off-manifold outliers, ensuring that generated latents remain within the valid decoding manifold and thereby improving generation performance (as shown by the purple line in Figure 1). Figure 1 Reconstruction and generation performance across different generation spaces. Compared to vanilla VAE, RAE improves generation coverage speed but quickly saturates due to its unconstrained semantic space and weak reconstruction. To address this, we project RAE features into compact 96-channel latent space with semantic reconstruction objective, forming S-VAE, which mitigates off-manifold issues and improves generation performance. Finally, PS-VAE further augments the semantic latent space with pixel-level reconstruction, enriching structural and texture details and achieving superior performance in both reconstruction and generation. The second issue arises from the training objective of representation encoders, which focuses on producing sufficiently discriminative features for understanding rather than preserving detailed structure and fine-grained visual information required for generation. Consequently, even within the regularized S-VAE space, the model struggles to synthesize realistic fine-grained textures and precise small-scale structures. To address this, we unfreeze the encoder and jointly optimize it with pixel-level reconstruction loss on the input image and semantic reconstruction loss defined on the outputs of the original frozen pretrained encoder. This encourages the encoder to maintain fine-grained details during the computation of strong semantic representations, yielding our final PixelSemantic VAE (PS-VAE Figure 1). Specifically, we instantiate PS-VAE with 96-channel latent design based on DINOv2 (Oquab et al., 2023). Compared to vanilla VAEs such as MAR-VAE (Li et al., 2024), this architecture achieves state-of-the-art reconstruction quality, significantly improving rFID (0.534 0.203), PSNR (26.18 28.79), and SSIM (0.715 0.817). It also outperforms MAR-VAE in text-to-image generation, exhibiting faster convergence and superior final performance (GenEval (Ghosh et al., 2023): 75.8 76.6; DPG-Bench (Hu et al., 2024): 83.2 83.6). Most notably, on the challenging instructionbased image editing taskrequiring both accurate image understanding and faithful instruction executionPS-VAE delivers substantial improvement, boosting the editing reward from 0.06 to 0.22. We also validate our method on SigLIP2 (Tschannen et al., 2025), which is used in Bagel (Deng et al., 2025), observing consistent generation behavior. Importantly, the fine-tuned encoder retains strong understanding ability without any LLM fine-tuning, highlighting the potential of our approach for unifying the encoder for both understanding and generation. In summary, our contributions are: Conceptual insight. We are the first to show that standard representation encoders are not directly suitable for text-to-image generation or instruction-based editing. Through comprehensive analysis, we identify two fundamental issues: off-manifold generation arising from unconstrained feature spaces and poor pixel reconstruction fidelity 1We define off-manifold latents as features falling into undefined/OOD regions where image decoding becomes unreliable. 2 inherent to discriminative pre-training. Methodological design. We propose principled approach that transforms the original unconstrained feature space into compact generative latent space using semantic autoencoder, and further enriches fine-grained structural and textural details through pixel-reconstruction objective. The resulting 96-channel latent space, built on DINOv2-B, achieves state-of-the-art performance in both reconstruction and generation, and generalizes effectively to SigLIP2, supporting its potential as unified encoder for understanding and generation. Unified evaluation. We establish standardized evaluation pipeline for fair and controlled comparison across different generation space designs on text-to-image and image-editing tasks, and empirically demonstrate the superior performance of PS-VAE under this framework."
        },
        {
            "title": "2 Related work\nRepresentation Encoders for Visual Understanding Representation learning is foundational to modern visual\nunderstanding. By mapping raw image data into a discriminative feature space, pre-trained encoders (such as DINO (Caron\net al., 2021; Oquab et al., 2023; Siméoni et al., 2025), SigLIP (Zhai et al., 2023; Tschannen et al., 2025), and Perception\nEncoder (Bolya et al., 2025)) enable a wide array of downstream tasks, including classification, object detection,\nsegmentation, and Vision-Language multi-modality modeling (Liu et al., 2023; Tong et al., 2024). These powerful\nencoders are typically obtained through two major paradigms: self-supervised learning (Chen et al., 2020; He et al.,\n2019; Grill et al., 2020; Caron et al., 2021; Oquab et al., 2023; Siméoni et al., 2025) and image–text contrastive\nlearning (Radford et al., 2021; Zhai et al., 2023; Bolya et al., 2025). Critically, since these representations are optimized\nmainly for discrimination, they effectively act as lossy compressors, discarding high-frequency pixel details that are\nnon-essential for semantic understanding but vital for accurate synthesis. Furthermore, the resulting feature space is\ntypically high-dimensional and unconstrained (non-regularized). This lack of generative regularization makes them\nsusceptible to the off-manifold generation issue when used directly as a diffusion target (as shown by our analysis\nin Section 1), which prevents their straightforward adaptation to generative modeling tasks.",
            "content": "VAEs for Visual Generation Variational Autoencoders (VAEs) (Kingma and Welling, 2013) are fundamental components of Latent Diffusion Models (LDMs) (Rombach et al., 2022), primarily serving to reduce the computational cost of high-resolution generation. However, VAEs are trained mainly on pixel-reconstruction objective, which often yields latent space focused predominantly on low-level structural details rather than high-level semantic concepts. This forces the subsequent diffusion model to learn complex visual concepts from scratch, necessitating massive computational resources. While early LDM work evaluated VAEs almost exclusively via reconstruction fidelity (Rombach et al., 2022), more recent studies have recognized that the topological properties of the latent space are crucial for robust generation. This realization has prompted efforts to introduce explicit regularization (Kouzelis et al., 2025; Skorokhodov et al., 2025; Yao et al., 2025), often via KL-divergence constraints, to encourage better latent utilization and stability. Our work extends this idea by explicitly designing compact, KL-regularized latent space that is directly informed by high-level semantics, thus combining the generative stability of VAEs with the discriminative power of foundation models. Unifying Feature Spaces for Generation and Understanding Recent efforts to bridge the gap between discriminative and generative feature spaces generally follow two distinct strategies. The first focuses on aligning standard VAE latents with representation encoders through representation-alignment objectives, treating the encoder as soft semantic constraint (e.g., (Yao et al., 2025; Xu et al., 2025)). The second strategy, which is more closely related to this work, aims to construct generative feature space directly from representation encoders (Chen et al., 2025; Lu et al., 2025; (Chen et al., 2025; Lu et al., 2025; Yue et al., 2025) do Yue et al., 2025; Shi et al., 2025; Zheng et al., 2025). not explicitly require the latent space to fully preserve the original semantic structure of the representations. More closely related to our study, SVG (Shi et al., 2025) and RAE (Zheng et al., 2025) propose diffusing directly over raw, unconstrained, high-dimensional semantic features. While this approach captures powerful semantics, we identify two critical failures preventing its widespread application: insufficient compact regularization of representation features, leading to off-manifold latent generation, together with weak pixel-level reconstruction, which prevents the generator from learning accurate geometry and texture. Thus, we propose PS-VAE, which introduces principled intermediate step: creating compact, KL-regularized semantic latent space and then applying joint pixel-reconstruction objective to enrich it with high-fidelity details. This design achieves superior performance in tasks requiring both semantic understanding and precise structural control, e.g. instruction-based image editing. Related work in the autoregressive paradigm (Ma et al., 2025; Song et al., 2025; Lin et al., 2025; Han et al., 2025) is discussed in the Supplementary Material, as it follows fundamentally different modeling approach. 3 Figure 2 Visualization comparison between RAE and VAE. (a) RAE shows noticeable gap in reconstruction performance compared to VAE. Benefiting from its rich semantic representation, RAE demonstrates stronger prompt-following ability in image editing tasks that require understanding the input image (b). However, its poor reconstruction quality limits practical usability, as it fails to preserve fine-grained and consistent details from the input image. Counterintuitively, in text-to-image generation, RAE exhibits severe structural and texture artifacts and substantially lags behind VAE (c), with performance gap far larger than that observed in reconstruction."
        },
        {
            "title": "3.1 Overview",
            "content": "In this section, we begin by providing detailed analysis of the reconstruction and generation behavior of RAE (Zheng et al., 2025). We theoretically and experimentally validate that its extremely high-dimensional feature space makes the diffusion model prone to generating off-manifold latent features that lie outside the training support of the pixel decoder. Furthermore, RAEs inherently poor reconstruction quality hinders the generative model from learning accurate object structures and fine-grained textures, making it unsuitable for high-fidelity tasks such as instruction-based editing. To address these fundamental challenges, we subsequently introduce our step-wise strategy for preparing the representation encoder for generation by mapping both pixels and representations into unified, compact latent space. Finally, we present Deep-Fusion architecture for text-to-image generation and image editing, establishing fair benchmarking framework for different generative latent spaces. Unless otherwise specified, we follow the settings of RAE (Zheng et al., 2025) and use DINOv2-B (Oquab et al., 2023) encoder for feature extraction."
        },
        {
            "title": "3.2 Analysis of RAE",
            "content": "Comparison: RAE vs. VAE To analyze the reconstruction and generation behavior of RAE (Zheng et al., 2025), we conduct series of visualization and benchmarking experiments, comparing it with vanilla VAE (Li et al., 2024) using the same 16 16 spatial compression. We first compare the reconstruction performance. As shown in Figure 2(a), RAE exhibits notable shortfall in reconstruction quality compared to VAE, often introducing artifacts in regions such as faces and text. This aligns with the quantitative findings in Table 4: although RAE achieves comparable rFID, its SSIM and PSNR are significantly lower. This behavior is expected, as the DINOv2 encoder used in RAE is trained with purely discriminative objective and does not explicitly optimize for reconstruction. To further validate their impact on generation, we conduct both text-to-image generation and image editing tasks (All equipped with wide DDT head as in RAE (Wang et al., 2025; Zheng et al., 2025)). Notably, in text-to-image generation, despite faster coverage (as shown in Figure 1) enabled by its strong semantic feature space, RAE still suffers from severe structural and texture artifacts (see Figure 2(c)) and substantially underperforms VAE, resulting in much poorer performance on benchmarks such as GenEval (Ghosh et al., 2023). For image editing tasks, RAE demonstrates superior capacity for prompt-following in 4 Figure 3 Off-manifold behavior varies significantly with feature dimensionality. We construct 2D PS-shaped distribution and embed it into an 8D ambient space, yielding two learning settings with intrinsic dimension 2 and ambient dimension 8. (a) The 8D setting produces substantially more off-manifold samples than the intrinsic 2D space. (b) We measure the mean nearest-neighbor distance of the top 5% tail samples and observe that samples generated in 8D deviate much farther from the data manifold, indicating stronger off-manifold drift. edits that necessitate semantic comprehension of the input image (see Figure 2(b)), but its poor reconstruction quality limits detail preservation. In conclusion, we empirically observed that RAE exhibits inferior reconstruction quality compared to VAE, limiting its ability to preserve fine-grained details in both image generation and editing. Conversely, its semantically rich latent space facilitates faster coverage in text-to-image generation and superior prompt-following in editing tasks. While these results largely align with expectations, we noted one counterintuitive finding: objects generated based on RAE space suffer from severe artifacts compared to VAE, the severity of which far exceeds what would be predicted by the reconstruction gap alone. Analysis of Off-Manifold Behavior in RAE The structural and texture artifacts in RAE far exceed typical reconstruction errors, suggesting fundamental problem in the generation process. We hypothesize that these artifacts arise because the diffusion model, trained on the high-dimensional RAE feature space, generates off-manifold samples. These samples reside outside the training distribution of the pixel decoder, leading to the sub-optimal decoded results. We trace the root cause to the discrepancy between the high dimensionality of DINOv2 features and their lower intrinsic information content. To rigorously analyze the difficulty of learning low-dimensional intrinsic manifold embedded in high-dimensional space, we model the generative dynamics of an h-dimensional feature space containing an l-dimensional manifold (h > l). Let Rl denote the latent data and = Qz Rh denote the observed data, where Rhl is column-orthonormal mapping (QQ = Il). The forward diffusion processes for and are coupled: xt = (1 t)x0 + tϵh implies that the projected variable zt = Qxt follows zt = (1 t)z0 + tϵl, where ϵl = Qϵh. Beyond the coupling of the forward processes, the optimal denoising objectives are strictly related. We denote the optimal velocity estimators for the intrinsic and embedded processes as vz,θ and vx,θ, respectively. These are defined as the expected velocity targets given the noisy states: vz,θ(zt) = E[ϵl z0zt], vx,θ(xt) = E[ϵh x0xt] By projecting the signal in the embedded high dimensions onto the data manifold and its orthogonal complement, we can express the high-dimensional estimator vx,θ purely in terms of the low-dimensional estimator vz,θ plus residual term: vx,θ(xt) = Qvz,θ(Qxt) + 1 (I QQ)xt (1) Equation 1 reveals fundamental disparity in learning difficulty. The first term represents the generative flow along the intrinsic manifold. While this component is intrinsically low-dimensional, the model operating in the ambient space must implicitly learn the projection (Q) and embedding (Q) operations to resolve it. This imposes significant burden of manifold discoveryidentifying the sparse data subspace within the vast high-dimensional ambient spacea challenge that is entirely bypassed when diffusing directly in the intrinsic latent coordinates. The second term consists purely of Gaussian noise in the orthogonal subspace, which forces the network to learn an identity-like mapping that conveys no semantic information, resulting in inefficient use of model capacity. This also 5 helps explain the behavior observed in RAE (Zheng et al., 2025): when the model dimensionality is smaller than the input dimensionality, the network struggles to fit even single example. High-dimensional Gaussian noise is full-rank and cannot be compressed without loss, which forces the model to allocate sufficient capacity to transmit noise. As result, an intrinsic information bottleneck emerges when the model width is smaller than the ambient feature dimension. This also explains why the wide DDT-Head design (Wang et al., 2025; Zheng et al., 2025), which incorporates long skip connection from the input x, substantially improves the performance of RAE. To validate this theoretical analysis, we investigate diffusion training in high-dimensional feature space (h = 8) that implicitly contains lower-dimensional intrinsic manifold (l = 2). We construct ground-truth 2D PS-shaped distribution and embed it into R8 via linear isometric mapping = Qz, where R82 has orthonormal columns. In this setup, acts as the linear decoder defining the manifold. We then train identical 256-channel MLP-based diffusion models separately on the intrinsic 2D data and the embedded 8D data. For evaluation, we project the generated 8D samples back to the 2D plane using the linear encoder (noting that QQ = I2, which perfectly recovers on-manifold data). As shown in Figure 3, learning in the 8D ambient space results in slower convergence and degradation in sample quality. Nearest-neighbor distance evaluation against the ground truth reveals that the top 5% tail samples from the 8D model deviate significantly from the true manifold. Despite sharing the same intrinsic geometry, the unconstrained high-dimensional representation amplifies off-manifold behavior. This confirms that discovering and training on the intrinsic low-dimensional isomorphic distribution is essential for stabilizing diffusion training and eliminating generation artifacts."
        },
        {
            "title": "3.3 Make Representation Encoders Ready",
            "content": "Building on our analysis of RAE (Zheng et al., 2025), we first address the off-manifold problem, identified as the primary limitation. Subsequently, we enhance reconstruction fidelity to improve the text-to-image performance and enable detail-sensitive applications such as image editing. The overall training framework of PS-VAE is illustrated in Figure 5. Given an input image Iinput RHW 3, we first ˆH ˆW dh using pretrained Representation Encoder, e.g. DINOv2-B (Oquab extract semantic feature map et al., 2023). As discussed in Section 3.2, is high-dimensional, unconstrained representation, where its dimension dh = 768. To address the off-manifold problem, we introduce semantic VAE (S-VAE) that maps the high-dimensional unconstrained feah to compact latent space fl ˆH ˆW dl via an ture space encoder Es. Here, dl = 96 (dl dh). Then, another semantic decoder Ds is adopted to reconstruct the latent back to the original feature . Both the semantic encoder Es and decoder Ds are optimized with semantic reconstruction loss Ls, which combines an ℓ2 loss and cosine similarity loss on features, while the latent is further regularized by KullbackLeibler divergence loss LKL following (Rombach et al., 2022). The encoder and decoder share symmetric design with three Transformer blocks inherited from the representation encoder and an MLP projection layer for dimensionality adjustment. For the evaluation of S-VAE, we additionally train pixel decoder that reconstructs the output image Ioutput from the detached semantic latent fl.detach() via the pixel reconstruction loss LP (The pixel reconstruction loss LP follows (Rombach et al., 2022).). diffusion model is further trained on this semantic VAE (S-VAE) latent space. As shown in Figure 4 and Table 4, both visual quality and quantitative results are substantially improved, despite slight performance drop in reconstruction fidelity. This result confirms that the primary limitation lies in the off-manifold issue rather than reconstruction quality. Figure 4 Visual comparison of generated examples across progressively improved latent spaces (RAE S-VAE PS-VAE). Artifacts are gradually reduced, with step-by-step improvements in texture and structure. To enhance image reconstruction without compromising the semantic structure of the latent space, we unfreeze the representation encoder during pixel decoder training. By removing the detach operation in fl, we enable gradients to propagate from the pixel decoder back to the encoder. To preserve the pretrained semantic representations during this 6 Figure 5 Compact latent space construction for preserving semantic structure and fine-grained details We first regularize the unconstrained representation-encoder feature space by freezing the encoder and training semantic VAE using only the Ls and Lkl; during this stage, the pixel decoder is trained on the detached semantic latent with pixel reconstruction loss LP. After semantic reconstruction converges, we unfreeze all components and allow the pixel decoder to backpropagate the gradient into the encoder, ensuring that the representation encoder captures fine-grained details of the input image. optimization, we enforce semantic reconstruction loss on KL loss and pixel-reconstruction loss. After this training stage, we obtain our Pixel-Semantic VAE (PS-VAE). relative to the original encoder, while retaining the and As demonstrated in Figure 4 and Table 4, this strategy significantly improves the reconstruction quality of the representation encoder while preserving its semantic structure. This enables the generation model to learn fine-grained geometry and texture, while the well-preserved semantics ensure fast coverage of text-to-image pretraining and strong instruction-following ability for image editing(as shown in Figure 7). As result, both visual quality and quantitative performance are consistently enhanced for text-to-image generation and editing."
        },
        {
            "title": "3.4 Generation Architecture",
            "content": "Unified models for generation and understanding are being actively explored. Owing to its strong semantic representation and high-fidelity reconstruction, PS-VAE has strong potential to serve as unified encoder in such frameworks. For these reasons, we adopt deep-fusion architecture as our generation paradigm. To investigate which deep-fusion architecture yields superior performance, we first conduct preliminary ablation study on three popular deep-fusion designs for generation, as illustrated in Figure 6. (a) LlamaFusion (Shi et al., 2024), which freezes all language blocks and adds parallel image blocks with identical architecture; (b) Bagel-style models (Deng et al., 2025), which unfreeze both text and image branches to improve multimodal alignment; and (c) Transfusion (Zhou et al., 2025), which processes image and text tokens jointly using fully shared transformer blocks. Figure 6 Block comparison of three deep-fusion architectures. Table 1 GenEval scores of Deep-Fusion architectures. We evaluate the three deep-fusion architectures using VAVAE (Li et al., 2024) (32-channel latent, stride-16, patch size 1). All fusion blocks are initialized from Qwen2.5-0.5B (Bai et al., 2023). We apply 2D positional encoding to the VAE features and inject timestep embeddings into their initial hidden states before feeding them into the LLM backbone, following Bagel (Deng et al., 2025). For text-to-image, we concatenate text embeddings with the noisy image latent. Text tokens use causal mask, while noisy image latent uses full attention mask. For image editing, we concatenate the clean latents of input images, the instruction text embeddings, and the noisy latents. We apply full attention mask to the clean and noisy latents, while employing causal mask for the instruction texts. LlamaFusion Bagel Transfusion Transfusion + Wide DDT Head Params (M) GenEval 0.524 0.763 0.752 857 857 500 Model 0.762 7 Results in Table. 1 indicate that LlamaFusion exhibits clear bottleneck, likely due to its frozen language branch being unable to adapt to text-to-image generation. Compared to the Transfusion-style block design, the Bagel-style design improves performance by 1.1 but increases parameters by 71%. Since we only evaluate text-to-image performance across different feature spaces and do not consider preserving language modeling capability, we adopt the TransFusion-style block as our core fusion architecture for better parameter efficiency. With the fusion block fixed, we incorporate the wide DDT head (Wang et al., 2025) from RAE (Zheng et al., 2025), which enhances generation quality in high-channel feature spaces(as we analyzed in Section 3.2). We validate its effectiveness through consistent gains across multiple VAEs. As shown in Table 1, the head improves VAVAE (32-channel, stride-16, patch size 1) from 75.2 to 76.2. We observe similar improvements for Flux-VAE (Labs, 2024) (16-channel, stride-8, patch size 2), which increases from 63.7 to 68.04, and MAR-VAE (16-channel, stride-16, patch size 1), which rises from 72.6 to 75.75. Given these consistent results, we adopt the wide DDT head as standard component."
        },
        {
            "title": "4 Experiments",
            "content": "In this subsection, we outline our training and inference pipelines, followed by the evaluation protocols for reconstruction, text-to-image generation, and instruction-based image editing. We then present performance results across varying feature spaces to demonstrate the effectiveness of PS-VAE. Subsequently, we analyze the scaling behavior of our 96and 32-channel variants, showing that larger generation models effectively leverage the rich semantic and pixel-level details preserved in high-channel latent spaces. Finally, we extend our framework by replacing the DINOv2 (Oquab et al., 2023) encoder with SigLIP2 (Tschannen et al., 2025), highlighting the latters potential as unified encoder for both visual understanding and generative modeling."
        },
        {
            "title": "4.1 Training and Evaluation Details",
            "content": "Reconstruction. To ensure fair comparison with prior work, we train our reconstruction models exclusively on ImageNet-1K (Russakovsky et al., 2015), though we note that future work could benefit from larger, more diverse datasets. Input images are resized and center-cropped to 224 224. Using patch size of 14 results in sequence length of 16 16, making the computationin terms of both FLOPs and runtimesignificantly more efficient than the VAE-style encoders used in Latent Diffusion Models (LDM) (Rombach et al., 2022). Our pixel decoder adopts the LDM architecture (Rombach et al., 2022) and reconstructs images at resolution of 256 256. We evaluate performance using rFID, SSIM, PSNR, and LPIPS on the ImageNet-1K validation set. Models are trained with batch size of 96 and learning rate of 104. We employ two-stage training strategy: first, we freeze the foundation model and train only the semantic encoder and decoder, with the pixel decoder trained on detached semantic latents to prevent interference with semantic compression. In the second stage, we unfreeze all components, allowing gradients from the pixel decoder to backpropagate to both the foundation model and the semantic encoder. The loss weights for Ls and Lp are set to 1 and 0.1, respectively. Further details are provided in the Supplementary Material. Text-to-Image Generation. We utilize CC12M-LLaVA-NeXT (Changpinyo et al., 2021; Emporium, 2024) for training, which comprises 10.9 million images with detailed long-form captions (Liu et al., 2024a). Images are resized and center-cropped to 256 256. We evaluate performance using GenEval (Ghosh et al., 2023) and DPG-Bench (Hu et al., 2024). GenEval relies on object detection, making it highly sensitive to structure and texturefactors closely tied to human perceptual preference. If generated objects exhibit geometric inaccuracies or distorted textures, the detector may fail to classify them correctly or produce duplicate detections. As result, scores can be lower even when the textimage semantic alignment appears correct at glance. Conversely, DPG-Bench employs visionlanguage model as judge, prioritizing high-level alignment over fine-grained details. This complementarity allows us to better interpret trade-offs between structural fidelity and semantic alignment. We train with batch size of approximately 730, learning rate of 104, and apply EMA with decay of 0.9999. Training for 200K iterations ensures convergence across various generative feature spaces. For GenEval, we use the rewritten long-prompt version from Bagel (Deng et al., 2025), consistent with our long-caption training data. Variations in patch size and channel dimensionality along the sequence length alter the signal-to-noise ratio (SNR) during interpolation between noise and latents. To maintain consistent SNR weighting across feature spaces, we apply shifted timestep = 1+(shift_factor1)t , where is sampled from Logit-Normal distribution (Esser et al., 2024; Zheng et al., 2025). Since the sequence length is fixed across feature spaces, the shift factor depends only on the latent shift_factort 8 Table 2 Comparison of reconstruction and generation performance. The best results are shown in bold and the second-best are underlined. Flux-VAE (stride 8) is listed for reference, and all other results correspond to the feature space with stride 16. Method rFID () PSNR () LPIPS () SSIM () GenEval () DPG-Bench () Editing Reward () Flux-VAE (Labs, 2024) 0.175 32. 0.044 0.912 MAR-VAE (Li et al., 2024) VAVAE (Yao et al., 2025) RAE (Zheng et al., 2025) PS-VAE32c PS-VAE96c 0.534 0.279 0.619 0.584 0. 26.18 27.71 19.20 24.53 28.79 0.135 0.097 0.254 0.168 0.085 0.715 0.779 0.436 0.662 0. 68.04 75.75 76.16 71.27 76.22 76.56 78.98 83.19 82.45 81.72 84.25 83. -0.271 0.056 0.227 0.059 0.274 0.222 Figure 7 Coverage curves for generation and editing tasks across different feature spaces. By jointly providing rich semantics and state-of-the-art reconstruction fidelity, PS-VAE consistently outperforms semantic-only RAE and pixel-only VAEs across both generation and editing benchmarks. The strong semantic structure and well-regularized latent space of PS-VAE enable significantly faster convergence during text-to-image training, while also facilitating better image understanding and, consequently, stronger instruction-following in image editing. Meanwhile, higher reconstruction fidelity leads to more realistic structures and textures in text-to-image generation, improved detail consistency between the edited and input images during editing, and thus better overall performance in both tasks. channel dimension Cvae and patch size Pvae: shift_factor = , where Cbase = 16 and Pbase = 1. For instance, Flux (Labs, 2024) (C = 16, = 2) yields shift factor of 2, while DINOv2-B (C = 768, = 1) yields approximately 6.93. We conduct ablation studies to confirm that these calculated values are reasonable. For Flux, factor of 2 yields better GenEval score compared to factors of 1 and 3, while for RAE, factor of 6.93 performs best compared to 6 and 8. We therefore apply this rule to all feature-space and channel-number ablation experiments. During inference, we use 50-step Euler sampling with timestep shift of 3 and classifier-free guidance scale of 6.5. base CvaeP 2 vae CbaseP 2 Instruction Editing. We utilize the OmniEdit dataset (Wei et al., 2024), which contains 1.2 million imageediting pairs spanning seven editing categories: object replacement, object removal, object addition, attribute modification, background swap, environment change, and style transfer. We resume the text-to-image checkpoint as the initialization for the editing training. For evaluation, we adopt the corresponding editing subtasks from GEdit-Bench (Liu et al., 2025) (Background Change, Color Alteration, Material Modification, Style Transfer, Subject Addition, Subject Removal, Subject Replacement, and Tone Transformation), using their provided input images and prompts. Results are evaluated using EditingReward (Wu et al., 2025), state-of-the-art image editing scoring model that assesses both visual consistency and instruction adherence. This setup ensures reliable and reproducible evaluation. Training and inference settings match the text-to-image configuration, with models trained for 50k iterations and the best-performing checkpoints reported."
        },
        {
            "title": "4.2 Reconstruction and Generation Performance of Different Feature Space",
            "content": "As shown in Table 2, our 96-channel PS-VAE96c achieves the highest reconstruction quality among all stride-16 VAEs, trailing only Flux-VAE, which benefits from finer stride of 8. In generation and editing tasks, both PS-VAE32c and PS-VAE96c significantly outperform RAE with the same training budget. Specifically, PS-VAE32c achieves top performance on DPG-Bench (Hu et al., 2024) and Editing Reward (Wu et al., 2025), ranking second on GenEval (Ghosh et al., 2023). Meanwhile, PS-VAE96c leads on GenEval and ranks second and third on DPG-Bench and Editing Reward, respectively, maintaining clear advantage over the RAE baseline. Besides, benefiting from well-constrained and semantically rich latent space, PS-VAE converges faster than RAE and other VAEs (see Figure 7). Furthermore, 9 Figure 8 Editing visual examples of models trained on different feature spaces. As shown in (a), both PS-VAE and RAE exhibit reasonable visual grounding, correctly identifying the elderly man and the background wall in (a). However, RAEs performance is strongly limited by its weak reconstruction ability, resulting in inconsistent details with the input image (a,b,c). In contrast, benefiting from strong semantic alignment and high-fidelity reconstruction, PS-VAE achieves accurate instruction following while preserving consistent visual details between the input and edited images, such as the human face in (a,b,c). enhanced detail fidelity enables PS-VAE to surpass standard VAE performance, highlighting the distinct advantage of the pixelsemantic constrained latent space. Meanwhile, we observed in Table 2 and Figure 8 that VAEs trained solely on pixel reconstruction objectives (e.g., MAR-VAE and Flux-VAE) exhibit significantly lower prompt-following capabilities than models with semantically structured latent spaces, e.g. PS-VAE32c, PS-VAE96c, and VAVAE. We hypothesize that instruction-based editing couples two subtasks: semantic comprehension of the input latent and faithful generation based on the prompt. Consequently, semantically organized latent space facilitates source image interpretation and improves instruction adherence. This aligns with findings from Bagel (Deng et al., 2025), where the injection of SigLIP2 (Tschannen et al., 2025) features similarly enhances performance. We further verify this observation by the clear drop in editing performance when the semantic reconstruction loss is removed from our training pipeline (denoted as P-VAE), decreasing from 0.22 for PS-VAE to 0.04, as shown in Table 4. Besides, we also observed that RAEs editing performance is constrained by weak reconstruction capabilities, resulting in visual inconsistencies relative to the input, while PS-VAE effectively balances semantic understanding with fine-grained detail preservation. Thats to say, PS-VAE retains the strong instruction-following capability of RAE (see Figure 8.a) while achieving superior consistency in fine-grained regions such as facial features (see Figure 8.a,b,c)."
        },
        {
            "title": "4.3 Scaling Behavior of Generative Models across PS-VAE Channel Dimensions",
            "content": "As shown in Figure 7 and Table 2, both PS-VAE32c and PS-VAE96c achieve state-of-the-art generation performance. While PS-VAE96c provides better reconstruction quality, it slightly underperforms PS-VAE32c in generation metrics, likely due to limited model capacity when modeling excessive fine-grained details. We further examine whether increasing 10 Figure 9 Text-to-image generation examples using PS-VAE96c. Despite being trained only at 256 256 resolution, the semantically structured and detail-preserving latent space enables the generator to accurately follow complex text prompts, yielding images with correct structures, fine-grained textures, precise text rendering, realistic portraits, and flexible compositions of abstract concepts. Prompts are simplified for visualization; full prompts can be found in the Supplementary Material. Figure 10 Scaling behavior of 653M (dashed) and 1708M (solid) models under different PS-VAE channel sizes (32c/96c) on (a) GenEval, (b) DPG-Bench, and (c) Editing Reward. model capacity can mitigate this effect. To assess whether the 96-channel variant offers higher performance ceiling, we scale the generation backbone from Qwen-0.5B to Qwen-1.5B (Bai et al., 2023) under fixed training setting, guided by the scaling observations in (Esser et al., 2024). This revealed distinct scaling behaviors as shown in Figure 10: PS-VAE96c exhibits consistent improvements across all tasks, with GenEval rising from 76.56 to 78.14, DPG-Bench from 83.62 to 84.09, and Editing Reward increasing significantly from 0.222 to 0.285. In contrast, PS-VAE32c demonstrates diminishing performance, showing only marginal gains on GenEval (77.07 to 77.67) and slight degradation on both DPG-Bench (84.25 vs. 84.10) and Editing Reward (0.274 vs. 0.228). These results indicate that higher-channel latent spaces possess superior scaling properties and higher upper bounds when paired with larger generative model. Investigating the correspondence between high-channel latent spaces and large-scale generation backbones represents promising direction for future research. Finally, we fine-tune Qwen-3B (Bai et al., 2023) with PS-VAE96c for 600k iterations, including 200k iterations under the same training setting and an additional 400k iterations on high-quality internal dataset. Benefiting from the strong semantic alignment and high-fidelity reconstruction of our latent space, the generator produces images with accurate text rendering, high-quality portraits, and flexible compositions of complex concepts (see Figure 9). Notably, it achieves realistic textures and prompt-following capabilities while trained solely at 256 256 resolution. Extending to higher resolutions will further amplify these capabilities, which we leave as direction for future exploration. 11 Table 3 Comparison of reconstruction and generation performance between DINOv2 and SigLIP2. Method rFID () PSNR () LPIPS () SSIM () GenEval () DPG-Bench () Editing Reward () PS-VAE96c(DINOv2-B) PS-VAE96c(SigLIP2-so400m/14) 0.203 0.222 28.79 28.14 0.085 0.096 0.817 0.795 76.56 77. 83.62 83.33 0.222 0."
        },
        {
            "title": "4.4 PS-VAE with SigLIP2: A Unified Encoder for Understanding and Generation",
            "content": "While SigLIP (Zhai et al., 2023; Tschannen et al., 2025) is widely adopted as vision encoder for multimodal understanding, we investigate whether our method enables SigLIP2 to function as unified encoder for both understanding and generation. Specifically, we utilize the SigLIP2-so400m/14 encoder from Bagel (Deng et al., 2025), training it under settings identical to DINOv2 for fair comparison. The only modification is the weighting ratio between the semantic loss Ls and the pixel reconstruction loss Lp, which is adjusted from 0.1:1 to 0.05:1. We observe that under equal loss weighting, the reconstruction quality of SigLIP2 saturates earlier than DINOv2, likely due to its more high-level and abstract representations. Generation Performance. As shown in Table 3, PS-VAE96c(DINOv2) and PS-VAE96c(SigLIP2) achieve comparable reconstruction quality with similar rFID, PSNR, LPIPS, and SSIM scores. In generation tasks, PS-VAE96c(SigLIP2) attains slightly higher GenEval score, while PS-VAE96c(DINOv2) performs better on DPG-Bench and Editing Reward. Overall, both encoders demonstrate similar generation capabilities, confirming the robust transferability of our method across different pretrained foundation models. Understanding Performance. To evaluate whether optimizing for reconstruction degrades semantic representations, we integrate our fine-tuned encoder into the original Bagel (Deng et al., 2025) pipeline, replacing the original encoder while keeping the LLM parameters frozen. On standard benchmarks, we observe negligible performance degradation in the zero-shot settings. For example, MME-P (Fu et al., 2023) scores decrease slightly from 1685 to 1652, and VBench (Liu et al., 2024b) drops marginally from 85.0 to 84.7. These results validate that our proposed PS-VAE preserves the core semantic capabilities of the original SigLIP2. It is worth noting that these results are obtained without any fine-tuning of the LLM parameters. We hypothesize that jointly training with the LLM could further enable the model to surpass the original baseline, which is left as future extension, as the fine-tuned encoder preserves all visual information while maintaining well-structured semantic representation. Overall, these results suggest that SigLIP2, optimized via PS-VAE, holds the potential as unified encoder for future understanding and generation architectures."
        },
        {
            "title": "5.1 Evolution from RAE to PS-VAE",
            "content": "We conduct comprehensive ablation study to analyze how reconstruction and generation performance evolve as we progressively extend RAE to S-VAE and finally to PS-VAE. The results are summarized in Table 4, where MAR-VAE (Li et al., 2024) is included as performance reference. From RAE to S-VAE: S-VAE is mainly trained with the semantic reconstruction loss LS and the KL loss LKL to compact and regularize the feature space. Compared to RAE, S-VAE yields substantial improvements in generation and editing performance (GenEval: 71.373.7, DPG-Bench: 81.783.6, Editing Reward: 0.060.12; see Table 4), despite marked degradation in reconstruction quality (PSNR: 19.217.78, SSIM: 0.4360.390). This suggests that RAEs primary limitation stems not from reconstruction fidelity, but from the off-manifold nature of its high-dimensional semantic features, which S-VAE effectively mitigates. Besides, we also compare S-VAE with MAR-VAE (Li et al., 2024). Results indicate that while S-VAE trails MAR-VAE in pixel-level reconstruction, its robust semantic representations allow it to outperform MAR-VAE on the semantics-oriented DPG-Bench (Hu et al., 2024) and in instruction-based editing. However, S-VAE underperforms on GenEval (Ghosh et al., 2023). Since GenEval relies on object detectors that are sensitive to texture and structure, the degraded pixel fidelity of S-VAE prevents the generation model from learning realistic textures and fine-grained structural details, leading to detection failures. This underscores the necessity of accurate reconstruction in VAEs for learning fine-grained object details. The Proposed PS-VAE: By integrating fine-grained detail supervision while maintaining semantic structure, our PS-VAE 12 Table 4 Evolving the Representation Autoencoder (RAE) into our PixelSemantic Variational Autoencoder (PS-VAE). Semantic regularization is essential for alleviating off-manifold behavior and improving generation and instruction-based editing performance (RAE S-VAE), while enriching pixel details without disrupting semantic structure yields PS-VAE with the best overall performance. In contrast, pixel-only VAE (P-VAE) trained with only pixel reconstruction loss exhibits degraded semantic quality, as indicated by linear probing, along with inferior DPG-Bench and editing performance, despite achieving better pixel-level reconstruction. This highlights the importance of semantic structure in the latent space. Method rFID () PSNR () LPIPS () SSIM () Linear Probe () GenEval () DPG () Editing Reward () MAR-VAE RAE S-VAE P-VAE PS-VAE 0.534 0.619 1.407 0.398 0.203 26.18 19.20 17.78 29. 28.79 0.135 0.254 0.296 0.073 0.085 0.715 0.436 0.390 0. 0.817 5.4/15.1 83.0/96.6 81.1/95.7 11.8/26.7 79.5/94.8 75.7 71.3 73.7 75. 76.6 83.2 81.7 83.6 82.1 83.6 0.06 0.06 0.12 0. 0.22 #C rFID () PSNR () LPIPS () SSIM () 32 0.584 48 0.475 64 0.423 80 0.292 0.203 96 112 0.159 256 0.156 24.53 25.43 26.65 27.38 28.79 30.51 30.30 0.168 0.147 0.124 0.107 0.085 0.064 0.065 0.662 0.697 0.744 0.772 0.817 0.865 0. Figure 11 Channel ablation of PS-VAE. Left: reconstruction metrics versus channel dimensionality. Middle/Right: convergence curves on GenEval and DPG-Bench. As channel dimensionality increases, convergence becomes slightly slower; GenEval and DPG-Bench remain stable from 32c to 96c, while DPG-Bench drops beyond 96c, suggesting an intrinsic latent dimensionality of approximately 96 channels under our training setup. Further increasing the channel dimension primarily captures high-frequency details, which may consume model capacity and interfere with semantic learning. recovers high-frequency details without sacrificing semantic coherence. The 96-channel PS-VAE achieves state-of-the-art reconstruction, surpassing MAR-VAE on GenEval (+0.9) while retaining the strong DPG-Bench performance of S-VAE. In instruction-based editing, this enhanced visual fidelity significantly improves imageedit consistency, nearly doubling the editing reward (0.120.22). The Role of Semantic Structure (P-VAE): We isolate the specific contribution of the semantic structure of latent space by training Pixel-VAE (P-VAE) using solely the pixel reconstruction objective (LP ). Linear probing indicates that its semantic quality regresses to the level of MAR-VAE. Consequently, DPG performance declines (83.682.6) and editing reward drops sharply (0.220.04). This confirms that explicit semantic regularization within the latent space is indispensable for text alignment and instruction following in both generation and editing tasks."
        },
        {
            "title": "5.2 Ablation on Latent Channel of PS-VAE",
            "content": "To find the optimal latent space dimensionality of PS-VAE, we conduct grid search (from 32 to 256) on the channel number. As shown in Figure 11, reconstruction performance saturates at 112 channels, with further increases in width yielding negligible gains. As for generation performance, increasing latent dimensionality slows convergence (see Figure 11). However, final performance remains comparable between 32 and 96 channels on both GenEval and DPG-Bench. turning point occurs at 112 channels, where the DPG-Bench score drops noticeably by approximately 0.6 points. This observation suggests that, under our training setup, the intrinsic latent dimensionality required to jointly preserve semantic structure and pixel-level fidelity is around 96 channels. Further increasing the channel capacity mainly introduces high-frequency details that consume additional model capacity, which might hinder semantic alignment, ultimately degrading generation performance."
        },
        {
            "title": "5.3 Ablation on Encoder and Decoder Architectures",
            "content": "As shown in Table 5, projecting an unconstrained representation space into compact, KL-regularized latent manifold (Kingma and Welling, 2013) need non-trivial computational overhead. For example, shallow 2-layer MLP is insufficient to preserve rich semantic features within 96-channel latent space, as evidenced by substantial drop in 13 Table 5 Comparison of reconstruction and generation performance across different PS-VAE design variants. 2L-MLP S-VAE denotes an S-VAE in which both the semantic encoder and the semantic decoder are implemented as 2-layer MLPs. DPixel on DSemantic indicates that the pixel decoder is attached to the semantic decoder features during the final detail-enrichment training stage."
        },
        {
            "title": "Method",
            "content": "rFID () PSNR () LPIPS () SSIM () Linear () GenEval () DPG-Bench () PS-VAE 2L-MLP S-VAE DPixel on DSemantic 0.214 0.205 0.193 28.63 28.94 29. 0.087 0.082 0.077 0.813 0.812 0.840 79.5 / 94.8 44.5 / 64.5 80.4 / 95.4 76.6 70.3 74.4 83.6 83.1 83.6 linear probing accuracy. This suggests that limited mapping capacity prevents the semantic VAE from fully capturing the intrinsic dimensionality of the presentation feature, leaving the effective intrinsic dimensionality below 96. Under the KL regularization constraint, this further induces posterior collapse, which ultimately degrades generation performance. We also explore an architecturally symmetric design that directly feeds the reconstructed semantic features into the pixel decoder for final image reconstruction. While this approach improves reconstruction quality, we observe degradation in GenEval performance. We hypothesize that this stems from gradient interference, as both semantic and pixel reconstruction objectives are backpropagated through shared Transformer pathway. Alternatively, this architecture may require distinct re-balancing of loss weights to function effectively. We leave deeper investigation of these dynamics to future work."
        },
        {
            "title": "5.4 Directly Enriching High-Dimensional Features Fails",
            "content": "An alternative strategy to improve pixel reconstruction involves training the pixel decoder directly on the original high-dimensional feature space, while maintaining the semantic reconstruction loss with frozen DINOv2 encoder. As shown in Table 6, while this approach yields rapid improvements in reconstruction quality, it leads to significant degradation in generation performance. The generated images exhibit severe structural artifacts and incoherent textures(as shown in Figure 12.a). Table 6 High-dimensional enrichment causes shortcut reconstruction. RAE-HD greatly improves reconstruction metrics but harms generation, indicating loss of semantic structure and using shortcut reconstruction. Method rFID PSNR LPIPS SSIM GenEval RAE 0.619 RAE-HD 0.193 19.20 33.10 0.254 0. 0.436 0.916 71.3 60.2 We attribute this failure to the inherent difficulty of constraining high-dimensional latent space. Even with semanticpreserving losses, the model can exploit shortcut solutions by relying on sparse subset of channels for reconstruction, without inducing meaningful changes in feature distances in high-dimensional spaces, where distance metrics tend to become less informative. We verify this shortcut behavior by showing that retraining pixel decoder using only the 32 selected channels (out of 768) with the largest deviations between the fine-tuned encoder and the frozen DINOv2 features is sufficient to achieve strong reconstruction performance (as shown in Figure 12.b). This indicates that constraining detail enrichment to compact, semantically regularized latent space is essential, thereby validating the core design of our PS-VAE. Figure 12 Directly enriching details in high-dimensional space leads to severe generation artifacts (a). We further verify that this behavior arises from reconstruction shortcuts in high-dimensional feature spaces (b)."
        },
        {
            "title": "5.5 Conclusions",
            "content": "In this work, we show that powerful representation encoders, despite their strong discriminative ability, are not directly suitable as generative spaces due to unconstrained feature distributions and insufficient image reconstruction fidelity. Through systematic analysis, we identify off-manifold generation and poor reconstruction as the two key bottlenecks limiting their performance in text-to-image generation and instruction-based editing. To address this, we propose PixelSemantic VAE (PS-VAE) that maps representation features and pixel details into compact, KL-regularized latent space by properly finetuning pre-trained representation encoders under both pixel and semantic reconstruction objectives. As result, PS-VAE achieves state-of-the-art performance in reconstruction, generation, and image editing. We believe this work offers practical pathway toward unifying visual understanding and generation within single encoder."
        },
        {
            "title": "References",
            "content": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, Junke Wang, Marco Monteiro, Hu Xu, Shiyu Dong, Nikhila Ravi, Daniel Li, Piotr Dollár, and Christoph Feichtenhofer. Perception encoder: The best visual embeddings are not at the output of the network. arXiv, 2025. Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 35583568, 2021. Bowei Chen, Sai Bi, Hao Tan, He Zhang, Tianyuan Zhang, Zhengqi Li, Yuanjun Xiong, Jianming Zhang, and Kai Zhang. Aligning visual foundation encoders to tokenizers for diffusion models. arXiv preprint arXiv:2509.25162, 2025. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Caption Emporium. conceptual-captions-cc12m-llavanext. https://huggingface.co/datasets/CaptionEmporium/ conceptual-captions-cc12m-llavanext, 2024. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent: new approach to self-supervised learning, 2020. Jiaming Han, Hao Chen, Yang Zhao, Hanyu Wang, Qi Zhao, Ziyan Yang, Hao He, Xiangyu Yue, and Lu Jiang. Vision as dialect: Unifying visual understanding and generation via text-aligned representations. arXiv preprint arXiv:2506.18898, 2025. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1600016009, 2022. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis. Eq-vae: Equivariance regularized latent space for improved generative image modeling. arXiv preprint arXiv:2502.09509, 2025. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. Haokun Lin, Teng Wang, Yixiao Ge, Yuying Ge, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun, and Ying Shan. Toklip: Marry visual tokens to clip for multimodal comprehension and generation. arXiv preprint arXiv:2505.05422, 2025. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 15 Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. https://llava-vl.github.io/blog/2024-01-30-llava-next/. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024b. Jiasen Lu, Liangchen Song, Mingze Xu, Byeongjoo Ahn, Yanjun Wang, Chen Chen, Afshin Dehghan, and Yinfei Yang. Atoken: unified tokenizer for vision. arXiv preprint arXiv:2509.14476, 2025. Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3): 211252, 2015. Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan, Xiaoshi Wu, Xintao Wang, Pengfei Wan, Jie Zhou, and Jiwen Lu. Latent diffusion model without variational autoencoder. arXiv preprint arXiv:2510.15301, 2025. Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Lmfusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. Oriane Siméoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothée Darcet, Théo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Hervé Jégou, Patrick Labatut, and Piotr Bojanowski. DINOv3, 2025. Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li, Rameen Abdal, Sergey Tulyakov, and Aliaksandr Siarohin. Improving the diffusability of autoencoders. arXiv preprint arXiv:2502.14831, 2025. Wei Song, Yuran Wang, Zijia Song, Yadong Li, Haoze Sun, Weipeng Chen, Zenan Zhou, Jianhua Xu, Jiaqi Wang, and Kaicheng Yu. Dualtoken: Towards unifying visual understanding and generation with dual visual vocabularies. arXiv preprint arXiv:2503.14324, 2025. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, vision-centric exploration of multimodal llms. and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 8731087356. Curran Associates, Inc., 2024. doi: 10.52202/079017-2771. https://proceedings.neurips.cc/paper_files/paper/2024/file/ 9ee3a664ccfeabc0da16ac6f1f1cfe59-Paper-Conference.pdf. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Hénaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features, 2025. Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. Ddt: Decoupled diffusion transformer. arXiv preprint arXiv:2504.05741, 2025. Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In The Thirteenth International Conference on Learning Representations, 2024. Keming Wu, Sicong Jiang, Max Ku, Ping Nie, Minghao Liu, and Wenhu Chen. Editreward: human-aligned reward model for instruction-guided image editing. arXiv preprint arXiv:2509.26346, 2025. Wanghan Xu, Xiaoyu Yue, Zidong Wang, Yao Teng, Wenlong Zhang, Xihui Liu, Luping Zhou, Wanli Ouyang, and Lei Bai. Exploring representation-aligned latent space for better generation. arXiv preprint arXiv:2502.00359, 2025. Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. Zhengrong Yue, Haiyu Zhang, Xiangyu Zeng, Boyu Chen, Chenting Wang, Shaobin Zhuang, Lu Dong, KunPeng Du, Yi Wang, Limin Wang, et al. Uniflow: unified pixel flow tokenizer for visual understanding and generation. arXiv preprint arXiv:2510.10575, 2025. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023. Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. Chunting Zhou, LILI YU, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. In The Thirteenth International Conference on Learning Representations, 2025. https://openreview.net/forum?id=SI2hI0frk6."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "The University of Hong Kong",
        "University of Chinese Academy of Sciences"
    ]
}