{
    "paper_title": "Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning",
    "authors": [
        "Ali Taghibakhshi",
        "Sharath Turuvekere Sreenivas",
        "Saurav Muralidharan",
        "Marcin Chochowski",
        "Yashaswi Karnati",
        "Raviraj Joshi",
        "Ameya Sunil Mahabaleshwarkar",
        "Zijia Chen",
        "Yoshi Suhara",
        "Oluwatobi Olabiyi",
        "Daniel Korzekwa",
        "Mostofa Patwary",
        "Mohammad Shoeybi",
        "Jan Kautz",
        "Bryan Catanzaro",
        "Ashwath Aithal",
        "Nima Tajbakhsh",
        "Pavlo Molchanov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Hybrid LLM architectures that combine Attention and State Space Models (SSMs) achieve state-of-the-art accuracy and runtime performance. Recent work has demonstrated that applying compression and distillation to Attention-only models yields smaller, more accurate models at a fraction of the training cost. In this work, we explore the effectiveness of compressing Hybrid architectures. We introduce a novel group-aware pruning strategy that preserves the structural integrity of SSM blocks and their sequence modeling capabilities. Furthermore, we demonstrate the necessity of such SSM pruning to achieve improved accuracy and inference speed compared to traditional approaches. Our compression recipe combines SSM, FFN, embedding dimension, and layer pruning, followed by knowledge distillation-based retraining, similar to the MINITRON technique. Using this approach, we compress the Nemotron-H 8B Hybrid model down to 4B parameters with up to 40x fewer training tokens. The resulting model surpasses the accuracy of similarly-sized models while achieving 2x faster inference, significantly advancing the Pareto frontier."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 9 0 4 1 1 . 4 0 5 2 : r 2025-4-16 Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning Ali Taghibakhshi*, Sharath Turuvekere Sreenivas*, Saurav Muralidharan*, Marcin Chochowski*, Yashaswi Karnati*, Raviraj Joshi, Ameya Sunil Mahabaleshwarkar, Zijia Chen, Yoshi Suhara, Oluwatobi Olabiyi, Daniel Korzekwa, Mostofa Patwary, Mohammad Shoeybi, Jan Kautz, Bryan Catanzaro, Ashwath Aithal, Nima Tajbakhsh, Pavlo Molchanov Abstract: Hybrid LLM architectures that combine Attention and State Space Models (SSMs) achieve stateof-the-art accuracy and runtime performance. Recent work has demonstrated that applying compression and distillation to Attention-only models yields smaller, more accurate models at fraction of the training cost. In this work, we explore the effectiveness of compressing Hybrid architectures. We introduce novel group-aware pruning strategy that preserves the structural integrity of SSM blocks and their sequence modeling capabilities. Furthermore, we demonstrate the necessity of such SSM pruning to achieve improved accuracy and inference speed compared to traditional approaches. Our compression recipe combines SSM, FFN, embedding dimension, and layer pruning, followed by knowledge distillation-based retraining, similar to the MINITRON technique. Using this approach, we compress the Nemotron-H 8B Hybrid model down to 4B parameters with up to 40x fewer training tokens. The resulting model surpasses the accuracy of similarly-sized models while achieving 2x faster inference, significantly advancing the Pareto frontier."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in language modeling have led to the development of hybrid architectures that combine Transformer layers [1] with State Space Models (SSMs) [2, 3]. These hybrid models leverage the complementary strengths of both approaches: Transformers excel at capturing global dependencies through self-attention mechanisms, while SSMs provide efficient sequence processing with 𝑂(𝑁 ) scaling during training and 𝑂(1) cache size during inference. Mamba [2, 3] in particular is popular SSM designed for efficient sequence modeling with linear-time complexity and support for long contexts and is often the preferred choice for non-attention layers in hybrid architectures. Despite their improved efficiency, many hybrid LLMs remain incredibly large, often spanning billions of parameters - this motivates the need for efficiently creating smaller hybrid models suitable for deployment in various resource-constrained environments. Model pruningthe removal of redundant parameters while preserving accuracyhas recently emerged as promising approach for compressing LLMs. In particular, methods that combine structured pruning (i.e., pruning of entire parameter blocks such as neurons, attention heads, etc.) with knowledge distillation [4] have proven effective at simultaneously reducing model memory footprint while improving runtime performance and accuracy [5]. While pruning techniques have been extensively studied for Transformer architectures [5, 6, 7], their application to hybrid models remains significantly underexplored. Some early work on Mamba and SSM pruning includes Mamba-Shredder [8], which removes the entire state space module from the Mamba layers, leaving only linear projections and convolution layer. In concurrent study, Ghattas et al. [9] proposed method for pruning Mamba architectures by focusing on three aspects: state space dimension reduction, Mamba head dimension pruning, and Mamba head merging. To the best of our knowledge, no existing work on SSM/Mamba pruning presents holistic compression strategy that simultaneously combines various aspects of SSM pruning with the pruning of other network components such as FFN neurons, embedding channels, and network depth; we believe such an approach is essential for obtaining the best combination of runtime performance and model accuracy. In this paper, we introduce novel pruning method for Mamba architectures that compresses multiple dimensions (Mamba heads, head channels). We also present unified pruning recipe that combines Mamba pruning with FFN, embedding dimension, and layer pruning to maximize accuracy and runtime performance. This paper makes the following key contributions: Introduces group-aware pruning method for Mamba layers that preserves SSM block structure and sequence modeling capabilities. Presents novel hybrid pruning recipe that effectively combines Mamba pruning with the pruning of other network components such as FFN neu- * Equal contribution. 2025 NVIDIA. All rights reserved. Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning Figure 1 Comparison of Nemotron-H 4B model accuracy w.r.t. inference throughput (left), and training budget for the base model (right) to similarly-sized community models. Inference throughput is measured at an input and output sequence length of 65536 and 1024, respectively. rons, embedding channels and layers. Presents findings on the sensitivity of Mamba to pruning, along with block components accuracy-throughput trade-offs when combined with pruning of other network components. Utilizes the proposed hybrid pruning recipe to compress the Nemotron-H 8B model to 4B parameters through pruning and knowledge distillation. The resulting model requires up to 40x fewer training tokens compared to others in the same size range. It also achieves state-of-the-art accuracy on benchmarks, along with 2x speedup in throughput compared to models of similar size, significantly pushing the Pareto frontier."
        },
        {
            "title": "Background",
            "content": "State Space Models (SSMs). SSMs are class of sequence models that process inputs through hidden states evolving over time [3]. The general form of an SSM is given by: ℎ𝑡 = 𝐴ℎ𝑡1 + 𝐵𝑥𝑡 𝑦𝑡 = 𝐶 ℎ𝑡 + 𝐷𝑥𝑡 (1) (2) Here, ℎ𝑡 represents the hidden state, 𝑥𝑡 the input, 𝑦𝑡 the output, and 𝐴, 𝐵, 𝐶, and 𝐷 are parameter matrices. The above equations describe linear time-invariant (LTI) SSMs, where the parameters remain constant across timesteps. The Mamba architecture [3] introduced selective SSM variant with time-varying parameters: ℎ𝑡 = 𝐴𝑡ℎ𝑡1 + 𝐵𝑡𝑥𝑡 𝑦𝑡 = 𝐶 𝑡 ℎ𝑡 + 𝐷𝑡𝑥𝑡 (3) (4) This selective mechanism allows the model to adapt dynamically to the input sequence, improving performance on complex tasks. Mamba2 [3] builds upon the selective SSM framework and introduces several enhancements to improve efficiency and scalability. It leverages the Structured State Space Duality (SSD), which connects SSMs and attention mechanisms through semi-separable matrix representations. This duality enables Mamba2 to combine the linear efficiency of SSMs with hardware-friendly quadratic computations typical of attention models. SSM-Transformer Hybrid Model architectures combine State Space Models (SSMs) and Transformers to leverage complementary strengths: SSMs enable linear-scaling long-sequence processing, while transformers provide contextual reasoning. Recent implementations demonstrate this synergyNemotronH [10] is family of hybrid Mamba2/Transformer architectures that replaces 92% of attention layers with constant-memory Mamba2 [3] blocks, achieving state-of-the-art accuracy while delivering up to 3x higher inference throughput compared to pure Transformers. Jamba [11] incorporates mixture-ofexperts (MoE) modules, and cuts KV cache sizes 8, supporting 256K-token contexts. Zamba [12] further enhances parameter efficiency through shared global attention and low-rank projections, maintaining performance with minimal resources. These archi2 Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning tectures demonstrate three key advantages over pure Transformer architectures: (1) drastically reduced KV cache requirements enabling memory-efficient longcontext processing, and (2) increased throughput via SSM-based sequence modeling. By balancing SSMs computational efficiency with Transformers expressivity, hybrid models address critical limitations in pure Transformers approaches for large-scale sequence tasks. Model Pruning. Weight pruning is powerful and well-known technique for reducing model size [5, 13, 14]. In particular, structured pruning removes blocks of nonzero elements at once from model weights, making it easier to realize actual hardware speedups; examples of structured pruning techniques include neuron, attention head, convolutional filter, and depth pruning [5, 15, 16, 17, 18, 19, 20, 21]. In most recent work, pruning is typically divided into three phases: (1) importance estimation, (2) model trimming, and (3) accuracy recovery. Here, importance estimation computes the importance or sensitivity of various network components (attention heads, layers, etc.). These components are then sorted in decreasing order of importance, following which the corresponding weight matrices are reshaped (trimmed). The pruned model typically loses lot of accuracy in this process, which is then recovered using continued training. Recent work [5] has demonstrated that knowledge distillation [4] can be an effective alternative to traditional fine-tuning for accuracy recovery."
        },
        {
            "title": "Methodology",
            "content": "We start the pruning procedure by computing the importance or sensitivity of each network component; namely, Mamba heads and head channels, FFN neurons, embedding channels, and layers. To keep this phase lightweight, we adopt purely activation-based strategy (requiring only forward propagation passes) for computing importance scores, similar to Minitron [5]. Once scores are computed, we sort the corresponding network components in decreasing order of importance while following any additional implementation constraints (discussed in more detail in the following subsection). We then prune away the network components with the lowest scores. Finally, the pruned model is distilled using the teacher model to obtain the final pruned model. The full procedure is illustrated in Figure 2. forward pass of Mamba layer. The Mamba layer processes input through five distinct projection matrices 𝑊𝑧, 𝑊𝑥, 𝑊𝐵, 𝑊𝐶, and 𝑊𝑑𝑡 , following layer normalization. These projections generate intermediate matrices 1: 𝑧 = 𝑊𝑧(LN(𝑋)), 𝑊𝑧 R𝑑𝑒(𝑚ℎ𝑚𝑑) 𝑥 = 𝑊𝑥(LN(𝑋)), 𝑊𝑥 R𝑑𝑒(𝑚ℎ𝑚𝑑) 𝐵 = 𝑊𝐵(LN(𝑋)), 𝑊𝐵 R𝑑𝑒(𝑔𝑑𝑠) 𝐶 = 𝑊𝐶(LN(𝑋)), 𝑊𝐶 R𝑑𝑒(𝑔𝑑𝑠) 𝑑𝑡 = 𝑊𝑑𝑡 (LN(𝑋)), 𝑊𝐶 R𝑑𝑒𝑚ℎ (5) (6) (7) (8) (9) Where 𝑋 is the layer input and LN denotes layer normalization. 𝑑𝑒 is model embedding dimension (AKA hidden dimension), 𝑔 is the number of Mamba groups, 𝑑𝑠 is the SSM state dimension, 𝑚ℎ is number of Mamba heads, and 𝑚𝑑 is Mamba head channels. The matrices 𝑥, 𝐵, and 𝐶 undergo causal convolution before participating in the selective state space model (SSM) updates: ^𝑥 = conv1d(𝑥) ^𝐵 = conv1d(𝐵) ^𝐶 = conv1d(𝐶) 𝑦 = SSM(^𝑥, ^𝐵, ^𝐶, 𝐴, 𝐷, 𝑑𝑡) (10) (11) (12) (13) Here, 𝐴, 𝐷 R𝑚ℎ are SSM learnable parameters corresponding to state transition and direct feed through, respectively (see Equations 3 and 4). The SSM output is fed into gated normalization layer, which is then followed by output projection, 𝑊𝑂 R(𝑚ℎ𝑚𝑑)𝑑𝑒: 𝑦 = 𝑊𝑂(RMSNorm(𝑦, 𝑧)) (14) Group-Aware Head Permutation Constraints Pruning requires scoring, sorting, and trimming neurons or heads of each layer, as shown in Figure 2. The FFN and embedding activations are permutation equivariant, i.e. for permutation operator 𝒫, FFN or embedding layer 𝐿, and activation 𝒜, and input 𝑋 we have: 𝐿(𝑋) = 𝒜 = 𝒫(𝐿)(𝑋) = 𝒫(𝒜). (15) Mamba Pruning We now describe the importance estimation and pruning of Mamba layers in more detail. To understand the pruning procedure better, we first dive into the However, Mamba layers and activations are not permutation equivariant. As shown in Figure 3, the 1We factor out the sequence length and batch size to simplify our description; the analysis remains valid without them. 3 Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning Figure 2 Overview of pruning and distillation for hybrid architectures. Starting from pretrained LLM, we first evaluate the importance of Mamba heads and channels, FFN neurons, and embedding channels. We then rank them, trim the least important neurons, and distill the knowledge from the original LLM to the pruned model. Attention layers are not pruned since they amount to only 8% of the total number of layers. 𝐵𝑡𝑥𝑡 operation from Eq. 3 involves reshaping 𝐵 into 𝐵 R𝑔𝑑𝑠 , and broadcasting it across 𝑥 R(𝑚ℎ𝑚𝑑). This broadcasting creates group-specific interaction patterns that constrain our pruning approach. As result, permuting heads across groups would alter the 𝐵𝑡𝑥𝑡 broadcast pattern, violating Eq. 3s group-wise computation as shown by: 𝐵𝑡𝑥𝑡 = (𝐵𝒫(𝑥𝑡))𝒫 𝑇 (16) Therefore, when sorting Mamba heads using activation scores, we must preserve Mambas group structure. Let 𝒢𝑔 {1, ..., 𝑚ℎ} denote the set of heads belonging to group 𝑔. Any permutation 𝒫 of heads must satisfy: 𝒫(ℎ) 𝒢𝑔 ℎ 𝒢𝑔. (17) In other words, Mamba heads and activations are permutation equivariant only for the permutation operators defined in constraint 17. Head Channel Consistency. similar constraint for permuting Mamba head channels applies. For head channel pruning, we maintain consistency across all heads through shared ranking. The state tensor ℎ R𝑚ℎ𝑚𝑑𝑑𝑠 requires channel-wise permutations 𝒫𝑑 to satisfy: 𝒫𝑑(ℎ𝑖,𝑗,𝑘) = 𝒫𝑑(ℎ𝑖,𝑗,𝑘) 𝑖, 𝑖 {1, ..., 𝑚ℎ} (18) meaning each channel index 𝑘 is either preserved or pruned uniformly across all heads. Scoring and Ranking Methodology. The Mamba head and head channel ranking follows nested scoring procedure: 1. Head Channel Scoring: For each head channel 𝑑 {1, ..., 𝑚𝑑}, we compute aggregate importance 4 Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning Figure 3 Mamba group structure visualization showing broadcasting and original 𝐵𝑡𝑥𝑡 computation. Colors represent distinct entries. The Figure illustrates how only within-group head permutations can preserve SSM semantics. As counter example, if H3 and H8 were to be swapped, the resulting 𝐵𝑡𝑥𝑡 would NOT be any permutation of the original (no permutation) 𝐵𝑡𝑥𝑡. scores: 𝑠 = 𝐿𝑁 (𝑋)(𝑊𝑥)𝑇 𝑠𝑑 = 𝑠:,𝑑2 (19) (20) 3. Group-Constrained Ranking: Within each Mamba group 𝒢𝑔, sort heads by their scores: ℛ𝑔 = argsort (𝑓ℎ) ℎ𝒢𝑔 (23) 𝐵,𝐿 where the aggregation is over 𝐿, the sequence length, and 𝐵, the batch size. Aggregation metric used along 𝐿 and 𝐵 dimensions are mean and 𝐿2, respectively, following Minitron [5]. 𝑠 R(𝑚ℎ𝑚𝑑) contains raw activation scores, and s:,𝑑 denotes the 𝑑-th column across all heads. We then select the top-𝑘𝑑 channels: 𝒟top = topk (𝑠𝑑, 𝑘 = 𝑘𝑑) (21) 𝑑{1,...,𝑚𝑑} 2. Head Scoring: Using the pruned channels 𝒟top, compute head importance scores: 𝑓ℎ = sℎ,𝒟top 2 ℎ {1, ..., 𝑚ℎ} (22) The final head ranking ℛ is the concatenation of group-wise rankings: ℛ = 𝐺 𝑔=1 ℛ𝑔[1 : 𝑘𝑔] (24) where 𝑘𝑔 is the target head count per group and denotes ordered concatenation. The following algorithm provides concise walkthrough on how to obtain mamba head and head channel rankings: Require: Activation scores R𝑚ℎ𝑚𝑑 , target channels 𝑘𝑑, target heads per group {𝑘𝑔}𝐺 𝑔=1 5 Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning Ensure: Head ranking ℛ, channel ranking 𝒟top 1: Compute channel scores: 𝑠𝑑 s:,𝑑2 𝑑 2: 𝒟top top-𝑘𝑑 indices of {𝑠𝑑} 3: Compute head scores: 𝑓ℎ 4: for 𝑔 1 to 𝐺 do ℛ𝑔 argsort-descending({𝑓ℎℎ 𝒢𝑔}) 5: ℛsel 6: 7: end for 8: ℛ 𝐺 𝑔 first 𝑘𝑔 elements of ℛ𝑔 2 ℎ sℎ,𝒟top 𝑔=1 ℛsel 𝑔 After obtaining the Mamba heads and head channel neurons to keep, we trim the corresponding matrices: 𝑊 𝑊 [ℛ], for 𝑊 {𝑊𝑥, 𝑊𝑧, 𝑊𝑂, 𝑊𝐴, 𝑊𝐷, 𝑊𝑑𝑡, conv1d} (25)"
        },
        {
            "title": "FFN and Embedding Pruning",
            "content": "For FFN and embedding channels, we compute importance scores using activation-based metrics. Similar to the approach in structured pruning of transformers [5], we examine the activations produced by the FFN and LayerNorm layers to determine which neurons and embedding channels contribute least to the models performance. For the 𝑖-th neuron in feed-forward layer, we compute its importance score as: 𝐹 (𝑖) neuron = 𝐵,𝐿 𝑋(𝑊 𝑖 )𝑇 (26) where 𝑊 𝑖 refers to the 𝑖-th row of the weight matrix 1 𝑊1 in the first linear projection of the FFN, 𝑋 is the input to the FFN layer, and denotes aggregation along the batch and sequence dimensions. 𝐵,𝐿"
        },
        {
            "title": "FLAP Importance for Hybrid Models",
            "content": "FLAP [22] is retraining-free structured pruning technique designed to measure the recoverability of models output feature map upon removing specific columns from weight matrices. FLAP quantifies the fluctuation of each input feature relative to baseline using calibration data. Specifically, the FLAP importance score for column is computed as the product of the squared norm of the column weights and the sample variance of the corresponding input features across calibration samples. We extend FLAP to the SSM layers in hybrid architectures by applying the metric to the activations serving as inputs to the output projection (OutProj) matrix. Here, we compute the FLAP importance by assessing the variance in activations input to the OutProj matrix, weighted by the squared norms of the respective columns of the OutProj weights. Mathematically, the extended FLAP importance metric for given column 𝑗 of weight matrix 𝑊 in SSM layers can be defined as: 𝑆𝑗 = 𝑊𝑗2 Var(𝑋𝑗) where 𝑊𝑗2 denotes the squared norm of the column weights and Var(𝑋𝑗) represents the variance of the activations input to the output projection matrix of SSM layer across calibration samples. We use the above-computed metric to rank different heads within each group and remove the corresponding rows in the input projection matrix, the corresponding channels in the SSM convolution kernel, corresponding rows in the 𝐴 and 𝐷 matrices of SSM, as well as trimming the corresponding columns in the output projection matrix. Similarly, for the 𝑖-th embedding channel, we compute: Depth Pruning 𝐹 (𝑖) emb = 𝐵,𝐿 LN(𝑋)𝑖 (27) where LN(𝑋)𝑖 represents the 𝑖-th dimension of the layer-normalized input. The embedding channel scores are computed across all layers that utilize the embedding channel, including FFN, Mamba and Attention projection layers, and LayerNorm components. Aggregation metric used along 𝐿 and 𝐵 dimensions are mean and 𝐿2, respectively, for both embedding Equations 26. After computing these scores, we sort them in descending order and keep the top-k neurons and embedding channels based on the target compression ratio, pruning those with the lowest importance scores. We explored depth pruning by analyzing layer importance using Kullback-Leibler divergence (KLD) between logits from model with specific layer removed and the full model. This importance estimation was averaged over small random subset of 256 samples to account for sample variability. Figure 4 shows the average importance scores for each layer in the Nemotron-H 8B Base model, with green, blue, and red dotted lines representing selfattention, FFN, and Mamba layers. As seen in previous work [10], the most important layers are concentrated at the models start and end. Interestingly, the first attention layer is among the least important, while other attention layers are more critical than neighboring layers. saw-like pattern emerges 6 Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning where MLP layers are more important than adjacent Mamba layers in the middle of the network, though this reverses in the models critical regions. We experimented by pruning the least important layers (4, 8, 12, 16, and 26 layers), followed by distillation with 126B tokens. While core-knowledge benchmarks remained largely unaffected, tasks like math and coding showed significant performance degradation (Figure 5)."
        },
        {
            "title": "Architecture Search",
            "content": "Our compression strategy explores multiple axes within the 4B parameter budget through combinatorial pruning. Our search space includes depth reduction (removing 4-26 layers from the original 52-layer architecture) combined with width pruning of embedding channels (3072-4096), FFN dimension (998421504), Mamba heads (64-128), and Mamba head channels (32-64). This multi-axis search space generated over hundred candidate architectures meeting the parameter constraints. Our search procedure follows these steps: (1) compute the zero-shot validation loss for all candidates on 1024 calibration samples, (2) select the top architectures (22 in this study) with the best loss values and perform lightweight knowledge distillation (KD) on them with 3.8B tokens, using the original 8B model as the teacher, and (3) select the top architecture candidate from step (2), using throughput and latency measurements for breaking ties, and perform extended knowledge distillation with 380𝐵 tokens to obtain the final model (see Table 4). We note that step (2) is critical for getting reliable ranking of architectural candidates, as also noted in prior work [5]. Accuracy Recovery with Knowledge Distillation (KD) To recover the accuracy lost due to pruning, the model undergoes continued training. Recent work has demonstrated that distilling knowledge [4] from the original model to the pruned model outperforms conventional fine-tuning [23, 6]; we thus adopt logitbased distillation for continued training, employing forward KL divergence (FKLD) loss exclusively during the accuracy recovery phase. 𝜏 exp( 𝑥𝑖 The output probability distribution of an LLM is computed as: 𝑝(𝑥𝑖, 𝜏 ) = for given token 𝑥𝑖 ) , where 𝜏 is the softmax temperature exp( 𝑥𝑗 𝑉 𝑗=1 and 𝑉 is the vocabulary size. Logit-based KD loss across the sequence of all output tokens is represented 𝐿 as: 𝐿logits = 1 (𝑥, 𝜏 )); here, FKLD(𝑝𝑘 𝑡 𝐿 (𝑥, 𝜏 ) represent the teacher and stu- (𝑥, 𝜏 ) and 𝑝𝑘 𝑝𝑘 𝑠 𝑡 (𝑥, 𝜏 ), 𝑝𝑘 𝑠 𝑘=1 ) 𝜏 dent probability distributions on the 𝑘𝑡ℎ token, respectively, and 𝐿 represents the sequence length."
        },
        {
            "title": "Experiments and Results",
            "content": "# 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25* 8B parent Layers Emb FFN Heads Head Channel LM Val Loss Relative Throughput 52 52 52 52 52 52 44 44 52 52 52 52 52 52 52 48 48 52 52 44 48 48 52 26 52 3072 3072 3328 3072 3072 3072 3072 3584 3072 3072 3072 3072 3072 3072 3072 3072 3328 3072 3072 3328 3072 3328 3072 4096 4096 12288 10752 9984 12288 12288 13056 14592 10752 11520 13056 13824 12288 13056 13056 14592 12288 9984 13824 11520 11648 13824 11648 16128 21504 21504 4096 21504 112 128 112 112 120 112 128 120 112 96 128 104 104 96 96 128 128 96 128 128 112 112 64 128 128 64 64 64 60 56 56 64 64 64 64 48 62 60 62 56 64 64 58 56 64 64 64 64 64 64 64 1.380 1.380 1.384 1.388 1.388 1.389 1.393 1.394 1.396 1.396 1.396 1.397 1.397 1.397 1.398 1.398 1.399 1.401 1.402 1.402 1.403 1.403 1.411 1.533 1.430 - 1 0.98 1 1.02 1.01 1.04 1.11 1.12 1.02 1.04 1.03 1.03 1.03 1.04 1.05 1.08 1.07 1.05 1.01 1.12 1.09 1.08 1.07 1.31 1. 0.74 Table 1 Model configurations with their corresponding LM validation loss after lightweight KD (sorted in increasing order), and relative inference throughput. Highlighted row shows the best (lowest) loss. All models have 4𝐵 parameters, except entries marked with *, which have more. To identify the optimal compression strategy for hybrid models, we conduct several ablation studies evaluating the impact of pruning different components on accuracy and inference speed. Our experiments reveal key insights and highlight differences from Transformer-only compression [5], as detailed in the following paragraphs. Depth-only vs Width-only Pruning. As shown in Table 1, width-only pruning (#1) significantly outperforms depth-only pruning (#24) at 50% compression ratio (8B to 4B). Notably, depth-pruned model with 36 layers (#25), despite having 1.4 more parameters performs worse than the least accurate width-only pruned 4B candidate (#23, with 64 Mamba heads), demonstrating the critical role of depth in maintaining accuracy as also observed with Transformer-only models. Impact on Inference Speed. Table 1 shows that depth-only pruning (#24) provides the highest speedups. Figure 6 presents the correlation between pruning various network components and performance metrics such as throughput, latency, and LM-loss for fixed 4B parameter count. We notice from the Figure that pruning Mamba components results in faster models compared to pruning FFN and embedding dimensions. Furthermore, we also compare the effects Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning Figure 4 Layer importance measured as the KLD between logits of the full model and model with that layer removed, averaged over small training subset. Vertical dotted lines indicate layer types: selfattention (green), FFN (blue), and Mamba2 (red). Figure 5 Accuracy drop relative to the 8B model across progressively depth-only pruned variants (48, 44, 40, 36, and 26 layers). Each model is directly pruned from the 8B and distilled using 126B tokens. of pruning Mamba heads to pruning head channels in Figure 7; we observe that the former yields better speed improvements than the latter within given Mamba layer. Impact on Accuracy. Table 1 shows that model depth (#24) is most sensitive to accuracy, followed by Mamba heads (#23), while FFN and embedding dimensions have less impact. Further ablations isolating the pruning of Mamba heads and head channels show that pruning head channels leads to greater accuracy loss (Figure 7). Given depth prunings effect on inference speed, we explore combined pruning strategy, starting with depth-only pruning followed by distillation to assess its limits. As shown in Figure 5, we observe significant accuracy drops on math and coding benchmarks below 44 layers. We then apply width pruning to both the 44and 48-layer variants to produce corresponding 4B-sized models. However, we notice that the best depth-width pruned candidate (#7, 44 layers) still under-performs the width-only model (#1). In Equation 19, we Mamba Scoring Ablations. chose the activations obtained from 𝑊𝑥 matrix for scoring the Mamba heads and head channels. We can alternatively get the Mamba scores by considering the activations obtained from 𝑊𝑧 and 𝑊𝑂 matrices, from Equations 5 and 14. Table 2 shows the effect of selecting the Mamba activations from different parts of the Mamba layer. For different configurations, we notice that scoring the activations from 𝑊𝑥 output often results in the best LM loss. FFN Embedding Dim Mamba LM-Loss Heads Head Channels 𝑊𝑥 𝑊𝑧 𝑊𝑂 12,288 13,056 13,056 14,592 12,288 13,824 3,072 3,072 3,072 3,072 3,072 3, 112 112 96 96 128 128 64 56 64 56 56 48 3.56 3.59 4.49 4.68 5.98 5.99 4.11 6.61 5.39 7.09 5.43 6.01 3.79 5.30 4.49 10.01 4.99 9.47 Table 2 Mamba scoring ablation. The zero-shot LM-loss for top 6 pruned models based on Mamba scores calculated from activations of 𝑊𝑥, 𝑊𝑧, and 𝑊𝑂. The 𝑊𝑥 activations result in the best zero-shot LM-loss in most of the cases. fixed size of 4 billion (4B) parameters. Within this constraint, we varied the sizes of the feed-forward network (FFN), embedding dimensions, 𝑚ℎ (Mamba heads), and 𝑚𝑑 (Mamba head channels). As result, we obtained 125 checkpoints, all with 4B parameters. For each checkpoint, we evaluated the lm-loss, time to first token, and throughput. To analyze the relationships between model parameters and performance metrics, we computed correlations and visualized them in Figure 6. Additionally, since all 125 models have the same total parameter count (4B), the model parameters exhibit negative correlations with one another. Figure 6 shows that in 4B models derived from Nemotron-H 8B, Mamba components positively correlate with latency and negatively with throughput and LM lossindicating that pruning them improves inference speed and slightly degrade accuracy. In contrast, pruning embedding and FFN dimensions improves accuracy (lower LM loss) but leads to slower models with increased latency and reduced throughput. Effect of Parameter Choice on Performance In our neural architecture search, we imMetrics. posed constraint to generate valid checkpoints with Closer Look at Mamba Pruning. We analyze the sensitivity of two axes in the Mamba layerMamba heads (𝑚ℎ) and Mamba head chan8 Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning nels (𝑚𝑑)to various metrics, including accuracy, latency, and throughput. In this study, each axis was pruned in isolation while keeping the rest of the network unchanged, preserving the architecture of the Nemotron-H 8B model. The objective was to determine which axis is more favorable for optimization. As shown in Figure 7, pruning Mamba heads (𝑚ℎ) consistently outperforms pruning Mamba head channels (𝑚𝑑) across all metrics. Specifically, reducing 𝑚ℎ consistently yields lower LM loss, reduced latency, and higher throughput, making Mamba heads particularly impactful and practical target for pruning. These findings emphasize the importance of selecting the appropriate axis for pruning when optimizing Mamba layers to balance computational efficiency and model performance. FLAP. Table 3 shows that FLAP-based importance estimation yields mixed results before lightweight KD across pruning strategies. After KD, it performs on par with the L2-based approach when applied to candidate #1; it doesnt seem to offer any clear advantage, however. Pruning Type Configuration L2 LM Loss FLAP LM Loss Baseline No pruning FFN FFN = 16384 FFN = 11568 FFN = 8192 Attention ATT Heads = 16 Mamba (SSM) Mixed Mamba Heads = 112 Mamba Heads = 96 Mamba Heads = 64 #1 #1 + Lightweight KD 1.168 1.364 1.803 2.281 1.282 1.305 2.150 9. 3.690 1.380 1.168 1.32 1.64 1.95 1.40 1.73 4.59 11.21 5.854 1. Table 3 LM loss comparison when pruning different model components using L2 and FLAP metrics. Baseline: 128 Mamba heads, 21,504 FFN size, 32 attention heads. Summary of Ablations. These findings highlight the importance of choosing the right pruning axes in hybrid models to balance accuracy and efficiency. Unlike Transformer-only modelswhere pruning attention heads is less common [5]hybrid architectures like those with Mamba layers can tolerate some head pruning, as seen with candidates #1 and #2 in Table 1. This tolerance may stem from Mamba layers having significantly more heads (128) than selfattention layers (32). choice is motivated by Nemotron-H 8Bs already compact architecture, consisting of 52 layers that include Mamba, FFN, and Attention blocksfewer than the 64 alternating Attention and FFN layers found in comparable models like Phi-4-4B. Based on the lightweight KD results in Table 1, we select the candidate with the lowest LM validation loss. Although both candidates #1 and #2 have identical losses, candidate #1 is chosen for extended KD with 380B tokens due to its higher inference throughput, enabled by the reduction in Mamba heads."
        },
        {
            "title": "Data and Training Hyperparameters",
            "content": "We use random sample from the Phase 3 data mixture employed for training Nemotron-H models [10] for both importance estimation and KD. For importance estimation, we use 1024 samples with sequence length of 8192. For KD, the batch size is 768, with sequence length of 8192, cosine decay learning rate schedule (starting at 1.6e-4 and decaying to 8e-4), with 60-step linear warmup. Alignment and Long Context Extension We perform Supervised Fine-tuning with Knowledge Distillation (SFT-KD)2 using the Nemotron-H 8B aligned model as the teacher, along with Rewardaware Preference Optimization (RPO) [24] and NeMoAligner [25]. The Nemotron-H 4B base model is finetuned using supervision from the top-k (100) logits of the teacher over two rounds of SFT-KD: the first round uses math and coding data, while the second round focuses on instruction-following and general chat data. The instruction-tuned model is then further aligned with two rounds of RPO. To extend the context length of the aligned NemotronH 4B model, we perform SFT using data designed for long-context understanding. The training data is derived by manipulating the general domain chat dataset from the second SFT-KD round during alignment. We concatenate conversation turns and introduce long-range dependencies by placing related turns far apart within the extended context. The context length is varied randomly between 128k and 512k tokens, ensuring the model learns to maintain coherence and understanding across longer sequences, enhancing its ability to process information beyond shorter context windows. We plan to explore KD for context extension as future work. Obtaining the Best Compressed Hybrid Model For our final model, we focus on width-only pruning to prioritize accuracy, avoiding depth reduction. This 2https://developer.nvidia.com/blog/data-efficientknowledge-distillation-for-supervised-fine-tuning-with-nvidianemo-aligner 9 Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning Figure 6 Left: Correlation matrix showing relationships between performance metrics and model componentsFFN, embedding dimension (𝑑𝑒), and Mamba parameters (varying both heads 𝑚ℎ and head dimension 𝑚𝑑)across 125 4B variants with fixed depth (52 layers). Right: Model parameter correlations for fixed 4B parameter budgethighlighting trade-offs where increasing one component reduces others. Figure 7 Impact of pruning Mamba heads (𝑚ℎ) versus Mamba head channels (𝑚𝑑) in isolation, with the rest of the network unchanged. Pruning 𝑚ℎ consistently outperforms 𝑚𝑑 pruning across LM loss, latency, and throughputestablishing it as the preferred target for optimization. Benchmarks (shots) Llama-3.2 Falcon-3 3B-Base 3B-Base Zamba-2 Qwen-2.5 Nemotron-H Nemotron-H 2.7B-Base 8B-Base 4B-Base 3B-Base ARC Challenge (0) ARC Easy (0) CommonsenseQA (0) GSM8K (8) HellaSwag (0) HumanEval (0, pass@1) HumanEval+ (0, pass@1) MBPP (3, pass@1) MBPP+ (0, pass@1) MMLU (5) OpenbookQA (0) PIQA (0) RACE v.3 (0) Social IQA (0) TruthfulQA MC2 (0) Winogrande (0) Average Tokens 46.5 72.0 66.5 27.1 74.1 26.8 24.4 42.0 40.7 56.3 41.4 78.0 66.7 46.8 39.3 69.5 51.1 9T 47.4 72.4 64.4 66.5 65.3 39.6 32.3 52.1 40.7 56.7 39.4 75.5 69.7 45.1 45.6 65.0 54.7 0.1T 51.5 79.5 76.2 55.0 76.6 25.0 21.3 36.2 32.8 56.8 46.4 80.4 73.7 51.8 45.8 74.3 55.2 3T 47.3 72.7 77.1 75.2 73.6 37.8 33.5 59.9 50.0 65.6 42.2 78.8 84.5 49.8 49.0 68.4 60.3 18T 54.4 81.6 70.2 69.6 77.0 59.8 55.5 65.0 61.1 68.1 44.2 79.4 80.9 45.1 49.4 71.3 64.5 0.38T 60.1 83.6 72.7 77.9 81.2 57.3 53.7 66.9 58.7 72.7 47.2 82.2 84.0 45.8 49.8 76.3 66.7 15T Table 4 Accuracy comparison of our compressed Nemotron-H 4B with other similarly sized base community models. Benchmarks (shots) MMLU (0, generative) GSM8K (0) MATH-500 (0) HumanEval (0, pass@1) HumanEval+ (0, pass@1) MBPP (0, pass@1) MBPP+ (0, pass@1) IFEval Strict (0) MT-Bench (0) BFCL v2 Live (0) Phi-4-Mini Llama-3.2 4B-Instruct-128k 3B-Instruct-32k 3B-Instruct-128k 3B-Instruct-32k 2.7B-Instruct-4k Qwen-2.5 Zamba-2 Falcon-3 Nemotron-H 4B-Instruct-128k Nemotron-H 8B-Instruct-128k 61.88 87.71 70.8 73.17 64.63 67.46 60.31 74.78 7.86 61.64 63.25 83.32 65.6 75.0 70.12 67.72 58.47 64.06 7.68 59.08 57.36 78.47 48.2 55.49 51.83 65.61 55.29 74.51 7.09 49.58 54.27 77.86 48.80 46.34 43.29 61.37 55.03 68.49 7.10 52.80 55.32 66.26 29.40 37.20 32.93 46.30 38.62 46.99 7.02 39.70 66.96 88.93 76.4 76.2 70.85 78.6 68.25 76.24 7.90 65. 68.7 90.4 77.6 79.3 74.4 81 67.7 78.6 7.90 62.6 Table 5 Accuracy comparison for instruction-tuned models. For IFEval, we report the average of prompt strict and instruction strict categories. For BFCL v2, we report live overall accuracy. For MT-Bench, we use GPT-4-Turbo as the judge. Evaluation Summary community models, and the parent 8B hybrid model. Tables 4 to 6 present accuracy comparisons between our compressed 4B hybrid model, other similar-sized Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning Context Length 16,384 32,768 65,536 131,072 Phi-4-Mini Qwen-2.5 4B-Instruct-128k 3B-Instruct-32k 3B-Instruct-128k Llama-3. Nemotron-H 4B-Instruct-128k Nemotron-H 8B-Instruct-128k 34.39 32.90 35.01 20.07 83.64 79.21 63.80 23.61 77.92 72.71 66.42 59.26 86.28 82.27 75.95 63. 91.5 89.8 87.6 81.7 Table 6 Average RULER benchmark scores up to 128k context length for aligned Nemotron-H 4B and other instruction-tuned models in similar size range. Our 4B model retains over 96% of the original 8B models accuracy while improving throughput by 1.4x. Compared to other similarly sized community models, it delivers state-of-the-art accuracy across knowledge, math, coding, commonsense reasoning, and reading comprehension tasks, despite being trained on up to 40x fewer tokens. It also achieves 2.2x higher throughput and 1.8x lower latency than the second-best Phi-4-4B model (Figures 1 and 8). The aligned version further leads in math, coding, instruction following, and tool-use tasks. To assess long-context capabilities, we use the RULER benchmark [26]. As shown in Table 6, our model demonstrates strong performance and achieves the highest scores at context lengths up to 128k tokens Figure 8 compares latency and throughput across four models: Phi-4-Mini-4B, Qwen-2.5-3B, NemotronH 8B, and Nemotron-H 4B (ours). Our model achieves the best performance on both axesdelivering time-to-first-token and highest throughputeffectively advancing the latency-throughput Pareto frontier. fastest the In summary, our compression approach successfully produces model with state-of-the-art accuracy while significantly improving inference speed and reducing training costs."
        },
        {
            "title": "Conclusions",
            "content": "In this paper, we present Nemotron-H 4B, compressed hybrid language model that combines Attention and State Space Models (SSMs) to achieve state-of-the-art accuracy and efficiency. By leveraging novel group-aware pruning strategy for Mamba layers combined with structured pruning of FFN neurons and embedding dimensions, and knowledge distillation, we reduce the model size by 50% while retaining over 96% of the original 8B models accuracy, with up to 40 fewer training tokens. Nemotron-H 4B advances the accuracy-efficiency Pareto frontier, achieving 2 faster inference and 2.6% higher accuracy across diverse set of tasks. The instruction-tuned variant further excels in longcontext reasoning (up to 128K tokens) and tool-use apFigure 8 Throughput and latency comparisons across four models: Phi-4-Mini-4B, Qwen-2.5-3B, NemotronH 8B, and Nemotron-H 4B (ours). Relative throughput and latency represents are measured for an input and output context length of 65536 and 1024, respectively. plications, making it compelling choice for resourceconstrained deployments. By open-sourcing our compression recipe, we provide practical blueprint for efficient hybrid model development."
        },
        {
            "title": "Acknowledgments",
            "content": "This work would not have been possible without contributions from many people at NVIDIA. To mention few: Akhiad Bercovich, Brandon Norick, Boris Ginsburg, Chengyu Dong, Dan Su, Deepak Narayanan, Dima Rekesh, Duncan Riach, Eileen Long, Elad Segal, Eric Harper, Izik Golan, Jared Casper, John Kamalu, Joseph Jennings, Jupinder Parmar, Kezhi Kong, Markus Klieg, Ran El-Yaniv, Roger Waleffe, Sanjeev Satheesh, Shrimai Prabhumoye, Syeda Nahida Akter, Tomer Ronen, Ying Lin."
        },
        {
            "title": "References",
            "content": "[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you 11 Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning need. Advances in neural information processing systems, 30, 2017. [2] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [3] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. [4] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in Neural Network. arXiv preprint arXiv:1503.02531, 2015. [5] Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Compact language models via pruning and knowledge distillation. arXiv preprint arXiv:2407.14679, 2024. [6] Akhiad Bercovich, Tomer Ronen, Talor Abramovich, Nir Ailon, Nave Assaf, Mohammad Dabbah, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Netanel Haber, Ehud Karpas, Roi Koren, Itay Levy, Pavlo Molchanov, Shahar Mor, Zach Moshe, Najeeb Nabwani, Omri Puny, Ran Rubin, Itamar Schen, Ido Shahaf, Oren Tropp, Omer Ullman Argov, Ran Zilberstein, and Ran El-Yaniv. Puzzle: DistillationBased NAS for Inference-Optimized LLMs, 2024. [7] Shengkun Tang, Oliver Sieberling, Eldar Kurtic, Zhiqiang Shen, and Dan Alistarh. Darwinlm: Evolutionary structured pruning of large language models. arXiv preprint arXiv:2502.07780, 2025. [8] Pablo Muñoz, Jinjie Yuan, and Nilesh Jain. Mambashedder: Post-transformer compression for efficient selective structured state space models. arXiv preprint arXiv:2501.17088, 2025. [9] Tamer Ghattas, Michael Hassid, and Roy Schwartz. arXiv preprint On pruning state-space llms. arXiv:2502.18886, 2025. [10] Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, et al. Nemotron-h: family of accurate and efficient hybrid mamba-transformer models. arXiv preprint arXiv:2504.03624, 2025. [11] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. [12] Paolo Glorioso, Quentin Anthony, and Yury Tokpanov. Zamba: compact 7b ssm hybrid model. arxiv preprint arXiv:2405.16712, 2024. [13] Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye Zhang, Binbin Lin, Deng Cai, and Xiaofei He. Model compression and efficient inference for large language models: survey. arXiv preprint arXiv:2402.09748, 2024. [14] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks. arXiv preprint arXiv:2102.00554, 2021. [15] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: filter level pruning method for deep neural network compression. In Proceedings of the IEEE international conference on computer vision, pages 5058 5066, 2017. [16] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating deep convolutional neural networks. arXiv preprint arXiv:1808.06866, 2018. [17] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language model pre-training via structured pruning. In The Twelfth International Conference on Learning Representations, 2023. [18] Saleh Ashkboos, Maximilian Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. Slicegpt: Compress large language models by deleting rows and columns. In The Twelfth International Conference on Learning Representations, 2023. [19] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. ShortGPT: Layers in Large Language Models are More Redundant Than You Expect, 2024. [20] Yifei Yang, Zouying Cao, and Hai Zhao. Laco: Large language model pruning via layer collapse. arXiv preprint arXiv:2402.11187, 2024. [21] Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, and Hyoung-Kyu Song. Shortened LLaMA: simple depth pruning for large language models. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024. [22] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang. Fluctuation-based adaptive structured pruning for large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1086510873, 2024. [23] Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Ameya Sunil Mahabaleshwarkar, Gerald Shen, Jiaqi Zeng, Zijia Chen, Yoshi Suhara, Shizhe Diao, Chenhan Yu, Wei-Chun Chen, Hayley Ross, Oluwatobi Olabiyi, Ashwath Aithal, Oleksii Kuchaiev, Daniel Korzekwa, Pavlo Molchanov, Mostofa Patwary, Mohammad Shoeybi, 12 Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning Jan Kautz, and Bryan Catanzaro. LLM Pruning and Distillation in Practice: The Minitron Approach, 2024. [24] Bo Adler, Niket Agarwal, Ashwath Aithal, Dong Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, et al. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704, 2024. [25] Gerald Shen, Zhilin Wang, Olivier Delalleau, Jiaqi Zeng, Yi Dong, Daniel Egert, Shengyang Sun, Jimmy Zhang, Sahil Jain, Ali Taghibakhshi, Markel Sanz Ausin, Ashwath Aithal, and Oleksii Kuchaiev. Nemoaligner: Scalable toolkit for efficient model alignment, 2024. [26] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models?, 2024."
        }
    ],
    "affiliations": [
        "NVIDIA"
    ]
}