{
    "paper_title": "Don't Overthink It: A Survey of Efficient R1-style Large Reasoning Models",
    "authors": [
        "Linan Yue",
        "Yichao Du",
        "Yizhi Wang",
        "Weibo Gao",
        "Fangzhou Yao",
        "Li Wang",
        "Ye Liu",
        "Ziyu Xu",
        "Qi Liu",
        "Shimin Di",
        "Min-Ling Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, Large Reasoning Models (LRMs) have gradually become a research hotspot due to their outstanding performance in handling complex tasks. Among them, DeepSeek R1 has garnered significant attention for its exceptional performance and open-source nature, driving advancements in the research of R1-style LRMs. Unlike traditional Large Language Models (LLMs), these models enhance logical deduction and decision-making capabilities during reasoning by incorporating mechanisms such as long chain-of-thought and self-reflection through reinforcement learning. However, with the widespread application of these models, the problem of overthinking has gradually emerged. Specifically, when generating answers, these models often construct excessively long reasoning chains with redundant or repetitive steps, which leads to reduced reasoning efficiency and may affect the accuracy of the final answer. To this end, various efficient reasoning methods have been proposed, aiming to reduce the length of reasoning paths without compromising model performance and reasoning capability. By reviewing the current research advancements in the field of efficient reasoning methods systematically, we categorize existing works into two main directions based on the lens of single-model optimization versus model collaboration: (1) Efficient Reasoning with Single Model, which focuses on improving the reasoning efficiency of individual models; and (2) Efficient Reasoning with Model Collaboration, which explores optimizing reasoning paths through collaboration among multiple models. Besides, we maintain a public GitHub repository that tracks the latest progress in efficient reasoning methods."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 0 2 1 2 0 . 8 0 5 2 : r Dont Overthink It: Survey of Efficient R1-style Large Reasoning Models Linan Yue1,2, Yichao Du4, Yizhi Wang1,2, Weibo Gao3, Fangzhou Yao3, Li Wang4, Ye Liu3, Ziyu Xu1,2, Qi Liu3, Shimin Di1,2, Min-Ling Zhang1,2 1: School of Computer Science and Engineering, Southeast University 2:Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education 3: University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence 4: Alibaba Group {lnyue,wang_yz,zyxu,shimin.di,zhangml}@seu.edu.cn; {ycdu666,yeliu.liuyeah}@gmail.com; {weibogao,fangzhouyao,wl063}@mail.ustc.edu.cn, qiliuql@ustc.edu.cn Github:https://github.com/yuelinan/Awesome-Efficient-R1-style-LRMs Figure 1: Taxonomy, Representative Methods and Future Applications of Efficient R1-style LRMs."
        },
        {
            "title": "Abstract",
            "content": "Recently, Large Reasoning Models (LRMs) have gradually become research hotspot due to their outstanding performance in handling complex tasks. Among them, DeepSeek R1 has garnered significant attention for its exceptional performance and open-source nature, driving advancements in the research of R1-style LRMs. Unlike traditional Large Language Models (LLMs), these models enhance logical deduction and decision-making capabilities during reasoning by incorporating mechanisms such as long chain-of-thought and self-reflection through reinforcement learning. However, with the widespread application of these models, the problem of overthinking has gradually emerged. Specifically, when generating answers, these models often construct excessively long reasoning chains with redundant or repetitive steps, which leads to reduced reasoning efficiency Corresponding Author Preprint. Under review. and may affect the accuracy of the final answer. To this end, various efficient reasoning methods have been proposed, aiming to reduce the length of reasoning paths without compromising model performance and reasoning capability. By reviewing the current research advancements in the field of efficient reasoning methods systematically, we categorize existing works into two main directions based on the lens of single-model optimization versus model collaboration: (1) Efficient Reasoning with Single Model, which focuses on improving the reasoning efficiency of individual models; and (2) Efficient Reasoning with Model Collaboration, which explores optimizing reasoning paths through collaboration among multiple models. Besides, we maintain public GitHub repository that tracks the latest progress in efficient reasoning methods. We hope this survey not only consolidates recent advances but also introduces novel organizational framework for understanding efficient reasoning, framing it through the lens of single-model optimization versus model collaboration."
        },
        {
            "title": "Introduction",
            "content": "In recent years, Large Language Models (LLMs) have made groundbreaking progress in natural language processing tasks. However, when dealing with complex tasks like mathematical reasoning, multi-hop question answering, and program verification, LLMs still fall short in their reasoning abilities. As result, Large Reasoning Models (LRMs) have attracted increasing attention (Xu et al., 2025b; Li et al., 2025e; Chen et al., 2025c). These models enhance structured reasoning and advanced cognitive abilities by introduceing Long Chain-of-Thought (Long CoT) and self-reflection methods, enabling them to tackle complex problems more effectively. Representative works include OpenAI o1 (Jaech et al., 2024), DeepSeek R1 (Guo et al., 2025), Kimi 1.5 (Team et al., 2025), and QwQ (Team, 2024). In particular, DeepSeek R1 has become benchmark for R1-style LRMs due to its outstanding reasoning accuracy and open-source accessibility, where the reasoning paths are commonly marked by the <think> and </think> tags. With the widespread deployment of R1-style LRMs in practical applications, the issue of overthinking has gradually emerged (Chen et al., 2024b; Team et al., 2025). Specifically, when generating answers, the model often constructs lengthy CoT, sometimes introducing redundant or ineffective intermediate reasoning steps. This not only significantly reduces reasoning efficiency and increases computational costs, but the extra thinking may also lead to increased uncertainty and variance in the output, thereby affecting the accuracy of the final result (Suvra Ghosal et al., 2025). For example, when handling math problem that could be solved in three steps, the model might generate redundant reasoning process with more than twenty steps, ultimately degrading overall performance. Furthermore, overthinking may introduce security risks, increasing the likelihood of the model being vulnerable to malicious attacks (Kuo et al., 2025; Fang et al., 2025b). As result, enabling models to think less but more accurately has become critical challenge in current reasoning model research. To this end, recent studies have explored methods to improve reasoning efficiency from multiple dimensions, leading to several preliminary survey studies. As shown in Table 1, these studies (Liu et al., 2025e; Qu et al., 2025b; Feng et al., 2025; Sui et al., 2025; Wang et al., 2025h; Xu et al., 2025b) mostly focus on training process, explicit and implicit CoT for effective reasoning. However, in contrast to previous works, in this survey, we present new categorization perspective based on the lens of single-model optimization versus model collaboration, systematically reviewing cutting-edge research from 2025 onward. As shown in Figure 1 and Figure 2, we categorize existing efficient reasoning methods into two main directions: (1) Efficient Reasoning with Single Model, which focuses on optimizing the reasoning path within single model to improve computational efficiency. Specific strategies include Early Exit, CoT Compression, Adaptive Reasoning, and Representation Engineering (RepE) based Efficient Reasoning. (2) Efficient Reasoning with Model Collaboration, which focuses on enhancing the reasoning efficiency through collaborative methods among multiple models. Related methods include LongShort Model Collaboration, LLM Routing, Model Consolidation, and Speculative Decoding. The framework of this survey is summarized as follows: 2 Table 1: Comparison of Existing Survey Papers Survey Paper Liu et al. (2025e) Qu et al. (2025b) Feng et al. (2025) Sui et al. (2025) Wang et al. (2025h) Xu et al. (2025b) Ours Focus on Overthinking Frontier RepE Methods Frontier Model Collaboration Taxonomy Explicit/Implicit CoT Training Process Short CoT/Small Model/Fast Decoding Training Process Post-training/Test-time Reinforced Reasoning Single-model Optimization Vs. Model Collaboration Section 2 introduces LRMs and the overthinking problem they face during reasoning, as well as the goals of efficient reasoning. Section 3 introduces efficient reasoning with single model, exploring how to optimize single models reasoning process to enhance efficiency. Section 4 discusses efficient reasoning through model collaboration, focusing on how collaborative mechanisms among multiple models can improve reasoning efficiency. Section 5 looks ahead to future development applications, covering frontier fields such as multimodal efficient reasoning, tool-integrated reasoning, multi-agent systems and truthful reasoning."
        },
        {
            "title": "2 Preliminaries",
            "content": "2.1 Large Reasoning Models The OpenAI proposed o1 model (Jaech et al., 2024) has sparked widespread interest in LRMs. For example, Qi et al. (2024) introduce rStar, self-play based mutual reasoning mechanism that significantly enhances the reasoning capabilities of small language models (SLMs) without relying on model fine-tuning or guidance from more powerful models. Zhang et al. (2024b) propose tree search reinforced self-training method guided by process rewards, which automatically generates high-quality reasoning paths through tree search, effectively improving the models coherence and reasoning performance. Marco-o1 (Zhao et al., 2024) employs self-play and Monte Carlo Tree Search (MCTS) to generate long CoT data with reflection and error correction abilities. During inference, MCTS and process rewards jointly guide the model to explore an improved reasoning space, yielding higher-quality answers. These methods typically emphasize modeling process reward mechanisms and using MCTS in test-time scaling. With the release of DeepSeek-R1 (Guo et al., 2025), researchers have increasingly focused on constructing R1-style LRMs. Such models rely solely on rule-based reward functions, such as accuracy and format rewards, for reinforcement learning (RL) training, which effectively unlocks long CoT reasoning capabilities and exhibits certain reflective behaviors. In this section, we provide the following definition for R1-style LRMs: Given any input question x, the output of an R1-style LRM consists of two parts: (1) reasoning process = {c1, c2, . . . , ct} composed of multiple reasoning units; and (2) final answer y. Here, ci denotes the i-th reasoning step or segment. Some reasoning units contain specific key tokens (e.g., wait and alternatively), which often signal the models Aha moment, reflecting the reflective transitions in the reasoning. In practice, to obtain ci, the delimiter nn is commonly used to separate reasoning units. 2.2 Overthinking Problem Overthinking refers to the tendency of LRMs to generate unnecessarily long, redundant, or overly complex reasoning paths during task execution, which can lead to response latency, increased computational cost, and even degraded answer accuracy (Chen et al., 2024b; Team et al., 2025; Cuadron et al., 2025). In R1-style LRMs, overthinking typically manifests in the following ways: 1. Overthinking Simple Problems: In real-world applications, R1-style LRMs often generate detailed and complete CoT for all inputs, even for simple queries such as What is 2 + 3?. 3 Early Exit (3.1) e.g. DEER (Yang et al., 2025c); CONCISE (Qiao et al., 2025); Elastic (Xu et al., 2025a); Flashthink (Jiang et al., 2025c); NOWAIT (Wang et al., 2025a); TIP (Wang et al., 2025d); S-GRPO (Dai et al., 2025); CoT Compression (3.2) e.g. CTS (Yuan et al., 2025b); TokenSkip (Xia et al., 2025a); RPC (Song et al., 2025c); Adaptive GoGI-Skip (Zhuang et al., 2025); PIR (Xiao et al., 2025); SPIRIT (Cui et al., 2025); DTO (An et al., 2025); Prune-on-Logic (Zhao et al., 2025); A-Thought (Xu et al., 2025c); LS-Mixture SFT (Yu et al., 2025); AutoL2S (Luo et al., 2025a); LC-R1 (Cheng et al., 2025a); Efficient Reasoning with Single Model (3) Adaptive Reasoning (3.3) e.g. LHRMs (Jiang et al., 2025d); Ada-R1 (Luo et al., 2025b); DAST (Shen et al., 2025b); Guided by Gu (Ghasemabadi et al., 2025); Thinker (Chung et al., 2025); CAR (Lu et al., 2025); AdaCtrl (Huang et al., 2025d); Thinkless (Fang et al., 2025a); AdaCoT (Lou et al., 2025); ACPO (Cheng et al., 2025b); ARM (Wu et al., 2025b); LASER (Liu et al., 2025i); HAPO (Huang et al., 2025e); SOL (Yi and Wang, 2025); ALP (Xiang et al., 2025); SelfBudgeter (Li et al., 2025a); L t - 1 t c fi Efficient Reasoning with Model Collaboration (4) Representation Engineering (3.4) e.g. SEAL (Chen et al., 2025e); Pre-allocated Direction Vectors (Sheng et al., 2025); Thinking Progress Vector (Eisenstadt et al., 2025); Manifold Steering (Huang et al., 2025a); LongShort Model Collaboration (4.1) e.g. SplitReason (Akhauri et al., 2025); ThoughtMani (Liu et al., 2025h); CoThink (Fan et al., 2025); PLAN-AND-BUDGET (Lin et al., 2025c); VeriThinker (Chen et al., 2025b); FoReaL-Decoding (Li et al., 2025d); COPE (Lee et al., 2025b); ThinkSwitcher (Liang et al., 2025); LLM Routing (4.2) e.g. RouteLLM (Ong et al., 2024); GraphRouter (Feng et al., 2024); IRT-Router (Song et al., 2025a); RouterBench (Hu et al., 2024); R2R (Fu et al., 2025); Router-R1 (Zhang et al., 2025b); TagRouter (Chen et al., 2025d); RTR (Pan et al., 2025b); Model Consolidation (4.3) e.g. TwT (Xu et al., 2025e); DAR (Wu et al., 2025a); DRP (Jiang et al., 2025b); ReCUT (Jin et al., 2025a); Speculative Decoding (4.4) e.g. RSD (Liao et al., 2025b); SpecRouter (Wu et al., 2025d); SpecReason (Pan et al., 2025a); Speculative Thinking (Yang et al., 2025d); SCoT (Wang et al., 2025b); Future Applications (5) (1) Efficient Multimodal Reasoning (5.1); (2) Efficient Tool-Integrated Reasoning (5.2); (3) Efficient Multi-Agent Systems (5.3); (4) Truthful and Efficient Reasoning (5.4) Figure 2: Taxonomy of efficient R1-style LRMs and future applications. 2. Unconfident Reasoning Behavior: During reasoning, LRMs often engage in self-verification and reflection. However, when deciding whether to reflect, the model may exhibit low confidence in its intermediate outputs, leading to unnecessary repeated reflection and self-doubt style reasoning loops, thereby exacerbating the overthinking issue (Chen et al., 2025b). To mitigate such issues, recent studies have focused on efficient reasoning, which aims to reduce the length and latency of reasoning paths while preserving answer accuracy and reflective behavior."
        },
        {
            "title": "3 Efficient Reasoning with Single Model",
            "content": "Efficient reasoning with single model aims to achieve efficient reasoning by optimizing the reasoning process of single model. This approach focuses on minimizing computational resources and reasoning time while maintaining reasoning accuracy, ensuring that the model can quickly and accurately generate answers. Specific methods include Early Exit (section 3.1), CoT Compression (section 3.2), Adaptive Reasoning (section 3.3), and Representation Engineering-based Efficient Reasoning (section 3.4). 3.1 Early Exit Early Exit in reasoning refers to the mechanism by which LLM dynamically determines whether it has acquired sufficient information during the reasoning process, and then terminates generation before completing the full CoT, making the final prediction based solely on the current reasoning content. Interestingly, studies have shown that even when reasoning is terminated prematurely, the models prediction performance can often match that of full CoT reasoning (Liao et al., 2025a). As 4 such, Early Exit has emerged as key research direction for enhancing the efficiency of R1-style reasoning models. The core challenge of Early Exit lies in determining when model should stop thinking. Existing approaches primarily address this question from three perspectives: 1. Monitoring-based Early Exit: These methods aim to dynamically monitor the models internal reasoning state to decide whether reasoning should be terminated. 2. Generation Control-based Early Exit: These methods manipulate the models generation behavior directly, e.g., by detecting and modifying the logits of specific trigger tokens, thereby preventing the model from producing redundant content. 3. Adaptive Early Exit: These approaches allow the model to autonomously decide when to stop reasoning, without relying on pre-defined monitors or trigger tokens. 3.1.1 Monitoring-based Early Exit These methods continuously monitor the models internal states or generated content to dynamically assess whether the current reasoning process is sufficient, thereby determining whether to terminate the generation of the reasoning chain early (Zhu et al., 2025). Based on the type of monitoring signal utilized, these approaches can be further categorized into four subtypes: (1) Confidence-based termination. This method relies on the models confidence in its current reasoning state to decide whether to stop. When the model exhibits high confidence in an intermediate result, it can stop the reasoning process early and directly output the current answer. Specifically, Yang et al. (2025c) propose training-free dynamic early-exit method called DEER. This method identifies pivotal tokens (e.g., wait) within long CoT sequences and replaces them with guiding tokens such as final answer to prompt the LLM to produce tentative answer based on the current reasoning. The confidence of this answer is then evaluated. If it exceeds predefined threshold, it is directly output, otherwise, the model rolls back to the turning point and continues reasoning. Similarly, Qiao et al. (2025) identify two typical redundancy patterns in reasoning: Confidence Deficit, where the model underestimates the validity of its correct intermediate steps and engages in unnecessary reflection, and Termination Delay, where the model continues reasoning even after generating correct answer. To address these issues, Qiao et al. (2025) propose the CONCISE framework. It first introduces confidence injection technique that inserts high-confidence phrases into the reasoning path to enhance trust in intermediate steps. Then, an early stopping module with confidence detector monitors the models confidence level and halts generation once it exceeds defined threshold. (2) Entropy-based dynamic control method. Unlike approaches that rely on explicit confidence signals, entropy-based control methods adopt an information-theoretic perspective (Shannon, 1948), focusing on the trend of information gain throughout the reasoning process to determine whether reasoning should be terminated. Specifically, Yong et al. (2025) first introduce two metrics: InfoBias and InfoGain. Then, they empirically find that longer reasoning paths tend to exhibit higher information bias and diminishing information gain, especially when generating incorrect answers. Based on these findings, the authors propose an entropy-based reasoning mechanism, where the reasoning process is automatically terminated if the InfoGain falls below preset threshold for consecutive steps. Additionally, an entropy regularization term is incorporated during training to encourage the model to terminate reasoning early when InfoGain becomes minimal. (3) Budget-constrained early termination method . This method explicitly imposes token usage budget on the reasoning process (Xu et al., 2025a; Li et al., 2025a; Liu and Wang, 2025), forcing termination when the consumption approaches or reaches the upper limit, thereby controlling computational cost. For example, Xu et al. (2025a) propose the elastic reasoning method, which divides the token budget into two parts: one for the thinking stage and one for the answering stage. When the thinking-stage budget is exhausted, reasoning is forcibly terminated to ensure sufficient budget remains for answer generation. Liu and Wang (2025) further propose supervised learning method that leverages internal model activation (Kapoor et al., 2025; Liu et al., 2024) sequences. An LSTM-based reasoning progress estimator is trained to dynamically predict the optimal stopping point based on model activation patterns, allowing for timely and effective early termination. 5 (4) Probe-based early termination method . This method does not rely on explicit confidence scores or entropy signals. Instead, it utilizes external probe models or verification mechanisms to predict the correctness of intermediate reasoning results and decide whether to terminate generation early (Zhu et al., 2025; Zhang et al., 2025a; Jiang et al., 2025c). Specifically, Zhang et al. (2025a) segment the full reasoning process into multiple chunks, and at the end of each chunk, the model generates an intermediate answer, which is labeled as either correct or incorrect (using binary supervision signal y). The final hidden state of each chunk is extracted as the input feature x, forming training set of (x, y) pairs. Based on this, multilayer perceptron (MLP) probe is trained to predict the probability that the current answer is correct. During inference, if the predicted probability exceeds certain threshold, reasoning is terminated early and the current answer is output. Similarly, Jiang et al. (2025c) also segment the reasoning content into multiple fragments and employs pre-trained verification model to assess whether the current fragment contains sufficient information to arrive at the correct answer. If so, the reasoning process is terminated; otherwise, it continues. 3.1.2 Generation Control-based Early Exit This category of methods bypasses internal state monitoring and instead intervenes directly in the decoding process to compress reasoning paths and improve efficiency (Wang et al., 2025a,d; Liu et al., 2025d). For example, Wang et al. (2025a) propose the NOWAIT method, which employs logit processor during decoding to explicitly prohibit the generation of specific tokens that trigger unnecessary reflection. For any predefined token, the corresponding logit value is assigned large negative value, effectively suppressing the sampling of such tokens and enabling more efficient reasoning. Similarly, Wang et al. (2025d) employ the Thought Switching Penalty (TIP), which adjusts the predicted logits of tokens associated with reasoning branch transitions, further reducing unnecessary digressions in the reasoning trajectory. Common reflection or switching-related tokens include: wait, alternatively, hmm, but, however, alternative, another, check, doublecheck, oh, maybe, verify, other, again, now, ah, any. Additionally, Liu and Wang (2025) propose another generation control strategy. Unlike the aforementioned suppression approaches, their goal is to enhance the generation probability of the endof-thinking token (i.e., the </think> token) to encourage early stopping. Specifically, Liu and Wang (2025) introduce an adaptive probability enhancement method for the </think> token. During decoding, the authors apply linear logit boosting strategy to increase the relative competitiveness of the </think> token, making it more likely to be sampled when the models output distribution is concentrated, thereby achieving early termination. 3.1.3 Adaptive Early Exit This category of methods does not rely on explicit monitoring signals or specific tokens and decoding control. Instead, it introduces learned policies that enable models to autonomously determine when they have thought enough, thereby achieving adaptive early stopping of the reasoning path. To this end, Dai et al. (2025) propose reinforcement learning approach named S-GRPO (Serial-Group Decaying-Reward Policy Optimization). This method inserts early exit instructions at different positions within single reasoning chain to construct multiple serial reasoning path groups. It then applies decaying reward strategy based on the exit position: the earlier the model terminates reasoning while still producing correct answer, the higher the reward it receives. This guides the model to stop reasoning as early as possible without sacrificing accuracy. Compared with the parallel-path-based GRPO method (Shao et al., 2024), S-GRPO models reasoning sufficiency in more fine-grained manner, improving both reasoning efficiency and answer accuracy. 3.2 CoT Compression Chain-of-Thought Compression (CoT Compression) methods aim to shorten the reasoning chains of LLMs while preserving their original reasoning effectiveness, thereby improving inference efficiency and deployment feasibility. straightforward approach is to leverage prompt learning to guide models to autonomously generate more concise reasoning paths (Renze and Guven, 2024; Nayab et al., 2024). For instance, Han et al. (2024) propose the prompt Use at most tokens to explicitly constrain 6 reasoning length. Aytes et al. (2025) further introduce the Sketch-of-Thought (SoT) framework, which employs structured prompts to elicit clear and concise reasoning steps. This method incorporates three reasoning paradigms (i.e., conceptual chaining, chunked symbolism, and expert lexicons) to adapt to different reasoning tasks, and integrates lightweight routing model for dynamic paradigm selection. While effective, these approaches often rely on manually crafted prompts and lack adaptability, limiting their applicability across diverse tasks. To improve the generality and efficiency of CoT compression, existing research mainly follows three perspectives: 1. Granularity-based CoT Compression: These methods compress CoTs at different granularities, including token-level, step/chunk-level, and chain-level. 2. Parallel thinking-based Compression: By sampling multiple reasoning paths from the model, these methods compare and aggregate the paths to construct compressed version. 3. Reward-based Compression: Instead of directly pruning reasoning paths, these methods design compression reward functions that encourage the model to learn adaptive compression strategies during training. 3.2.1 Granularity-based CoT Compression These methods build upon existing CoT reasoning paths to generate compressed data tuples (x, ˆc, y), where denotes the model input, is the final answer, and ˆc represents the compressed reasoning path. Based on such compressed datasets, LLMs are further fine-tuned via Supervised Fine-tuning (SFT) to achieve effective reasoning chain compression. Depending on the granularity focus of CoT compression, these methods can be further categorized into the following three types: (1) Token-level compression based on importance estimation. These methods focus on estimating the importance of individual tokens within the reasoning chain and removing less important tokens for compression (Yuan et al., 2025b; Xia et al., 2025a; Song et al., 2025c; Zhuang et al., 2025; Lee et al., 2025a). Specifically, Yuan et al. (2025b) propose Conditional Token Selection (CTS), which trains reference model to assess token-level importance during reasoning, and dynamically removes redundant tokens using metrics such as perplexity to construct compressed dataset. Xia et al. (2025a) further introduce TokenSkip, which estimates token importance and applies compression threshold to retain only high-weight tokens, yielding concise version of the reasoning chain. Reasoning Path Compression (RPC) (Song et al., 2025c) improves inference efficiency by periodically compressing the key-value (KV) cache in LRMs. The method uses attention mechanisms to score recently generated tokens by their importance, retaining only high-impact entries to reduce redundant computation. Besides, to ensure coherence in the compressed reasoning process, Zhuang et al. (2025) propose the Adaptive GoGI-Skip method. This approach first quantifies the contribution of each token to the final prediction by computing the loss. Then, it introduces dynamic pruning strategy based on uncertainty: when the models prediction entropy is high, indicating greater task difficulty, pruning is reduced; conversely, when entropy is low, more aggressive pruning is allowed. In addition, an Adaptive N-Constraint mechanism is used to limit the number of consecutively pruned tokens based on the moving average of entropy, preserving the continuity of reasoning. Based on these strategies, compressed dataset is constructed for retraining the model. (2) Step-level compression based on importance estimation. Unlike token-level compression methods, this category of approaches partitions the reasoning chain into higher-level semantic units, such as steps, chunks, or segments, and performs selection at that granularity (Xiao et al., 2025; Cui et al., 2025; Wang et al., 2025e; An et al., 2025; Zhao et al., 2025; Xu et al., 2025c; Lin et al., 2025b). Compared with token-level methods, these approaches place greater emphasis on semantic coherence and logical completeness. Specifically, Xiao et al. (2025) propose the Perplexity-based Importance Refinement (PIR) framework, which systematically categorizes reasoning steps into progressive and functional types. By leveraging perplexity-based scoring, PIR selectively removes low-importance functional steps and constructs refined dataset for model fine-tuning to improve inference efficiency. Similarly, Cui et al. (2025) introduce the SPIRIT algorithm, which addresses both few-shot CoT prompting and fine-tuning scenarios. SPIRIT iteratively removes or merges reasoning steps based on perplexity, while designing demonstration refinement or training data optimization strategies to ensure that the resulting reasoning chains remain both concise and semantically coherent. Wang et al. (2025e) divide the model-generated solution into well-structured semantic chunks and generate 7 multiple simplified candidates for each chunk. greedy search is then conducted across chunks to select the candidate that best balances conciseness and fidelity, measured by low language model loss. An et al. (2025) propose the Dynamic Thought Optimization (DTO) framework, which partitions the reasoning chain into segments representing different cognitive modes. DTO evaluates these segments to selectively reinforce beneficial ones and prune detrimental ones, constructing preference pairs to perform preference learning. In addition, some studies move beyond the traditional linear CoT structure by converting the reasoning process into more structured representations such as graphs. For instance, Zhao et al. (2025) introduce the Prune-on-Logic framework, which transforms CoT into logical graph and prunes redundant or ineffective nodes to achieve structurally consistent compression with stronger logical validity. Xu et al. (2025c) propose A-Thought, which models the reasoning process as search tree. This method employs bidirectional importance estimation (via bidirectional language modeling) and leverages search to optimize reasoning paths, effectively compressing long chains and accelerating LLMs inference. (3) Chain-level compression via rewriting. This line of research focuses on rewriting the entire CoT to reduce its overall length and complexity (Yu et al., 2025; Luo et al., 2025a). Unlike tokenor step-level pruning strategies, chain-level methods offer global perspective, aiming to simplify the reasoning process holistically in terms of semantics and structure. Specifically, Yu et al. (2025) propose the LS-Mixture SFT approach, which rewrites long CoT sequences into more concise versions while preserving their reasoning structure. These rewritten short chains are then mixed with the original long-chain data for supervised fine-tuning, effectively reducing redundant reasoning behavior in the model. Similarly, Luo et al. (2025a) introduce the Auto Long-Short Reasoning (AutoL2S) method. They construct training data that includes both long and short CoT paths, where the short CoTs are rewrote with special <EASY> token at the beginning to indicate the corresponding problem is simple. The model is then fine-tuned on this mixed dataset. After training, if the model generates the <EASY> token during inference, it follows simplified reasoning path, enabling dynamic compression of the reasoning process. 3.2.2 Parallel thinkingbased Compression Unlike previous approaches that optimize single sampled reasoning path, this category is inspired by Best-of-N (BoN) sampling strategies (Beirami et al., 2024; Amini et al., 2024; Agarwal et al., 2025), which parallelize the generation of multiple candidate reasoning paths and select the superior ones to guide compression (Munkhbat et al., 2025; Suvra Ghosal et al., 2025). Specifically, Munkhbat et al. (2025) leverage self-generated reasoning paths and combine naive BoN sampling, few-shot prompting (FS), and few-shot guided BoN (FS-BoN) strategies to identify the shortest correct reasoning path. This path is then used to construct compressed dataset for SFT, enabling efficient reasoning compression. Similarly, Suvra Ghosal et al. (2025) also propose BoN-style sampling strategy for efficient reasoning. Rather than explicitly shortening single reasoning path, they evenly allocate the total token budget across parallel paths and use parallel decoding to simultaneously generate multiple candidate chains. The best-performing path is selected as the final output. Beyond path selection, some studies explore parallel execution mechanisms to reduce reasoning time. Specifically, Biju et al. (2025) propose the SPRINT framework, which consists of planner and multiple executors. During reasoning, the planner generates multiple subplans from the reasoning context, which are then executed in parallel by independent agents to accelerate inference. Hassid et al. (2025) further suggest strategy where reasoning paths are generated in parallel, and once the shortest of them (k m) are completed, the generation of the remaining paths is terminated. The answers from the finished paths are then aggregated via majority voting to select the final reasoning outcome. 3.2.3 Reward-based Compression This category of methods (Cheng et al., 2025a; Zeng et al., 2025) does not directly prune or rewrite reasoning paths. Instead, it introduces compression reward mechanisms to guide models in autonomously learning compression strategies through reinforcement learning, thereby enabling dynamic optimization of reasoning content. Specifically, Cheng et al. (2025a) first propose two key principles: brevity and sufficiency. Guided by these principles, they design the LC-R1 post-training method based on GRPO (Shao et al., 2024). This approach incorporates compression reward focused on the </think> token, encouraging the model to terminate reasoning promptly after generating the correct answer and premature termination before completing effective reasoning is penalized to prevent excessive compression from harming prediction accuracy. Through this mechanism, the model adaptively balances compression rate and accuracy. Additionally, Zeng et al. (2025) combine chain rewriting with this approach by reconstructing the original long-form CoT paths into structured multi-turn interactive processes. Specifically, Zeng et al. (2025) first convert raw CoT into multi-turn dialogue format to build training data, which is then initialized by SFT. Subsequently, reinforcement learning using GRPO (Shao et al., 2024) is applied, with the reward design including interaction rounds as an optimization target, encouraging the model to complete accurate reasoning in fewer turns, thereby compressing the overall reasoning process. Notably, the above methods are all single-model compression approaches. For multi-model collaborative compression mechanisms, please refer to Section 4.3.1. 3.3 Adaptive Reasoning Adaptive Reasoning aims to enable LLMs to dynamically adjust the depth and length of their reasoning processes based on task requirements and input complexity. Unlike conventional methods that rely on static reasoning paths, adaptive reasoning empowers models with the ability to decide whether to reason, how long to reason, and how to reason autonomously. To achieve this goal, adaptive reasoning methods typically integrate RL frameworks, where carefully designed reward mechanisms guide the model to learn optimal reasoning strategies under varying conditions. Existing research in this area can be broadly categorized into three main perspectives: 1. RL-based Adaptive Reasoning: Inspired by DeepSeek R1 (Guo et al., 2025), these methods focus on reward design, by encouraging the model to learn when and how to reason effectively. 2. Reasoning-mode Switching: These methods emphasize the decision of whether to reason or which reasoning mode to choose. The core idea is to assess the complexity of given input and dynamically select an appropriate reasoning strategy, such as direct answering, short reasoning, or in-depth reasoning. 3. Adaptive Reasoning with Length Reward: As an extension of RL-based methods, these methods explicitly target reasoning path length. Models are guided to learn what constitutes an optimal reasoning length by setting length reward objectives. 3.3.1 RL-based Adaptive Reasoning This class of methods incorporates RL frameworks and carefully designed reward functions to guide LLMs in dynamically adjusting their reasoning process based on input complexity. As core approach to adaptive reasoning, RL-based methods are also widely employed in the subsequent sections on reasoning-mode switching and adaptive reasoning with length reward. Here, we focus on representative works that model adaptive reasoning primarily through reinforcement learning. Based on whether Warm-up phase is introduced prior to RL training, existing methods can be further categorized into the following two types: (1) RL methods with Warm-up phase. These methods typically begin with SFT phase using mixed reasoning-path data (i.e., both short and long chains), which enables the model to acquire the ability to perform diverse reasoning strategies (Jiang et al., 2025d; Wang et al., 2025i). This warm-up stage is followed by an RL phase to further optimize the models adaptive decision-making ability. For instance, Jiang et al. (2025d) propose Large Hybrid Reasoning Models (LHRMs). They first fine-tune the model with combination of long-chain and short-chain reasoning samples, equipping it with both reasoning styles. Then, they introduce Hybrid Group Policy Optimization to train the model to adaptively choose between reasoning modes. An evaluation metric called Hybrid Accuracy is also proposed to measure the models effectiveness in selecting the appropriate reasoning strategy. Similarly, Wang et al. (2025i) also perform SFT using mix of short and long CoT samples, followed by RL. Their reward design incorporates intra-group accuracy to guide reasoning mode selection and first-token logits loss to optimize initial decoding behavior. Distinct from the above, Luo et al. (2025b) propose Ada-R1, two-stage adaptive reasoning framework. In the first stage, they merge the parameters of long-chain reasoning model and standard LLM to form unified model capable of generating both long and short reasoning paths. The second stage introduces dual-level optimization mechanism: group-level preference optimization guides the model to select short or long reasoning modes based on input characteristics, while instancelevel preference encourages the model to generate more concise reasoning under the constraint of maintaining accuracy, thereby improving overall reasoning efficiency. (2) RL methods without Warm-up phase. Unlike the previous approaches, this class of methods (Shen et al., 2025b; Ghasemabadi et al., 2025; Chung et al., 2025; Yang et al., 2025b; Qi et al., 2025) directly trains LLMs using RL without supervised warm-up stage. Specifically, DAST Shen et al. (2025b) builds an explicit mapping between problem difficulty and response length, introducing metric called Token Length Budget (TLB). For each input query, multiple reasoning paths are sampled and their corresponding TLB values are calculated. Then, preference pairs are constructed based on reasoning quality and efficiency. These pairs are used to fine-tune the model via SimPO (Meng et al., 2024), enabling it to learn adaptive reasoning strategies. Guided by Gut (GG) (Ghasemabadi et al., 2025) leverages intrinsic signals from the LLMs own generation process, such as token-level confidence, to guide the reasoning search, without relying on external verification models. Through RL, the model is trained to optimize its internal confidence estimation, and is coupled with self-guided tree-search strategy. This framework significantly reduces computational costs while preserving reasoning quality. In addition, Chung et al. (2025) propose Thinker, four-stage reasoning framework guided by RL. The model learns to dynamically decide among four steps: Fast Thinking Verification Slow Thinking Summary. Initially, the model performs fast thinking to produce draft answer. If verification fails, it proceeds to slow-thinking phase for in-depth correction. Finally, it summarizes the full reasoning path. Each stage is paired with custom-designed reward function to enable adaptive reasoning across different reasoning demands. 3.3.2 Reasoning-mode Switching This category of methods dynamically determines whether reasoning is necessary and which reasoning mode to adopt by assessing the complexity of the current input. Typical strategies involve switching between multiple modes such as fast/slow thinking or thinking/no-thinking (Zhang et al., 2025e). Based on how the switching mechanism is implemented, these methods can be further divided into two subcategories: (1) Token-based reasoning mode switching. These approaches explicitly inject control tokens (e.g., <fast_think> and <slow_think>) to indicate different reasoning modes (Cheng et al., 2025b; Huang et al., 2025d; Fang et al., 2025a; Tu et al., 2025). For example, Cheng et al. (2025b) propose the Adaptive Cognition Policy Optimization (ACPO) framework, which introduces <fast_think> and <slow_think> tokens to enable dynamic switching between fast and slow thinking in LRMs. Concretely, they construct reasoning paths on high-quality math dataset by prompting diverse outputs of varying lengths, and use GPT-4 to conduct fine-grained comparisons. Important reasoning steps are labeled as slow-thinking, while redundant or simple steps are tagged as fast-thinking. These mixed-mode paths are used to perform SFT, followed by RL using an online TLB reward (Shen et al., 2025b) to guide adaptive depth control based on input difficulty. Similarly, Huang et al. (2025d) introduce the AdaCtrl framework, which uses cold-start SFT stage on dataset labeled with special tokens like <Easy> and <Hard> to establish initial mode-switching ability. In the subsequent RL phase, difficulty-aware response length reward and difficulty calibration mechanism are introduced to enhance adaptive reasoning across tasks. Thinkless (Fang et al., 2025a) utilizes the first token (<think> or <short>) in the output sequence to control reasoning behavior. Decoupled Groupwise Relative Policy Optimization (DeGRPO) algorithm is then used to jointly optimize both mode selection and final answer accuracy. Beyond explicit control tokens, other works have explored implicit switching signals. For instance, Tu et al. (2025) use ellipsis-style prompts (...) to invoke optional reasoning behavior in R1-style models. Zhang et al. (2025c) guide models to switch between Thinking and NoThinking modes depending on problem complexity, interpreting an initial </think> token as no-thinking decision. Similarly, Lou et al. (2025) propose the AdaCoT framework, which constructs two types of samples: one with <think>reasoning_steps</think>answer for tasks requiring reasoning, and another with <think></think>answer for straightforward queries, training the model to control whether and when to reason. Finally, Lu et al. (2025) introduce the Certainty-based Adaptive Reasoning (CAR) framework. Trained on mixed reasoning paths, the model initially generates concise answers and 10 uses perplexity as proxy for uncertainty. If confidence is low, longer CoT response is triggered, enabling dynamic trade-off between efficiency and performance. (2) Multi-mode reasoning switching. In contrast to binary reasoning mode switching, some approaches (Wu et al., 2025b; Xie et al., 2025) further extend the diversity of reasoning paradigms by enabling the model to adaptively select among three or more reasoning strategies. Specifically, Wu et al. (2025b) propose the Adaptive Reasoning Model (ARM), which supports four distinct reasoning formats. The model is trained in two stages: in the first stage, SFT is used to equip the model with multiple reasoning paradigms; in the second stage, an improved group-wise relative policy optimization algorithm (Ada-GRPO) is introduced to guide the model in dynamically selecting the optimal reasoning mode based on task requirements. In different vein, Xie et al. (2025) introduce the Interleaved Reasoning framework. Unlike the traditional think-then-answer linear paradigm, this method adopts an interleaved generation structure of thinkingansweringthinking, where intermediate informative answers are generated during the reasoning process. These answers serve as both guidance for subsequent steps and as verifiable reward signals, enabling the model to iteratively refine its reasoning and converge toward the correct final answer. 3.3.3 Adaptive Reasoning with Length Reward This category of methods focuses on controlling the length of the generated reasoning paths, typically by introducing explicit reward shaping or penalty mechanisms to guide the model toward eliminating redundant content while preserving prediction accuracy (Gao et al., 2025; Li et al., 2025c; Luo et al., 2025c; Hou et al., 2025; Su and Cardie, 2025; Aggarwal and Welleck, 2025; Yuan et al., 2025a; Song and Zheng, 2025; Liu et al., 2025a; Ling et al., 2025). Among them, Liu et al. (2025i) propose LASER (Length-Aware Shaping via Reinforcement learning), RL approach that designs stepwise reward function based on target length. It also introduces difficulty-aware dynamic reward scheme, balancing reasoning efficiency with task performance. Huang et al. (2025e) introduce History-Aware Policy Optimization (HAPO), which maintains history of the shortest correct answer length. Responses shorter than this value are rewarded, while those exceeding it are penalized, even if correct. Yi and Wang (2025) operate under the assumption that each question has Sample Optimal Length (SOL). They obtain this SOL by sampling multiple candidates per input, identify the shortest correct one, and use it to guide reward assignment via GRPO. Adaptive Length Penalty (ALP) (Xiang et al., 2025) performs multiple rollouts per input to estimate solve rate (i.e., success ratio), and dynamically adjusts the length penalty: inputs with high solve rates are penalized more heavily to discourage overlong reasoning; those with low solve rates receive weaker penalties, allowing longer reasoning chains to ensure correctness. Finally, from the perspective of token budget, Li et al. (2025a) propose the SelfBudgeter framework. In the training phase, the model first undergoes cold-start fine-tuning to learn how to predict the required token budget before answering. Then, GRPO is used to further optimize this prediction process, encouraging the model to minimize token usage while strictly adhering to the predicted length budget without compromising accuracy. 3.4 Representation Engineering based Efficient Reasoning Representation Engineering (RepE) (Zou et al., 2023) treats the internal representations of neural networks as fundamental units of operation, aiming to precisely control model behavior by analyzing and transferring these representations. In recent years, RepE has demonstrated broad applicability in domains such as hallucination mitigation (Li et al., 2023), safety enhancement (Arditi et al., 2024), and reasoning capability improvement (Zhang and Viteri, 2024; Tang et al., 2025). These methods typically follow two-stage pipeline of representation extraction and representation control. In the first stage, hidden representations from models under different states are collected, and directional vectors are computed by taking the difference between representations. These vectors capture key behavioral shifts. In the second stage, these vectors are injected into the hidden states of the target model to steer its behavior. For example, in reasoning capability improvement scenarios, given set of problems = {x1, x2, ..., xn} and two models Mshort with short-chain reasoning and Mlong with long-chain reasoning, we can achieve the following steps: In the representation extraction phase, the difference vectors can be computed as: δi = Mlong(xi) Mshort(xi). (1) 11 Aggregating these can yield reasoning-mode steering vector: pL = 1 X (cid:88) i= δi = 1 X (cid:88) i=1 (Mlong(xi) Mshort(xi)) . (2) Then, in the representation control stage, given target model Mtarget and its inputs hidden state Mlong(xi), controled hidden state is constructed as: Mlong(xi) = Mlong(xi) + λp pL, (3) where λp is scaling hyperparameter. This intervention nudges the target model toward the reasoning style of Mlong, effectively enhancing its reasoning depth. In this section, we focus on the application of RepE for mitigating overthinking (Chen et al., 2025e; Sheng et al., 2025; Eisenstadt et al., 2025; Huang et al., 2025a; Ma et al., 2025; Liu et al., 2025b; Azizi et al., 2025; Lin et al., 2025a). Specifically, (Chen et al., 2025e) propose SEAL (Steerable rEAsoning caLibration), framework that categorizes reasoning units into execution, reflection, and transition, and constructs steering vectors to represent efficient reasoning directions. These vectors are injected into the hidden space during decoding to dynamically suppress redundant reflections and non-essential transitions, while preserving core execution logic. Sheng et al. (2025) show that the number of reasoning tokens can be predicted from input activations via linear probe, indicating the models implicit control over reasoning length. They construct Pre-allocated Direction Vectors, whose subtraction reduces reasoning depth and accuracy, while addition extends reasoning and improves performance. Similarly, Eisenstadt et al. (2025) find that LLMs implicitly track their reasoning progress via internal signals. Based on this, they propose the Thinking Progress Vector to enable fine-grained control over reasoning length, thus preventing overthinking. Differently, Huang et al. (2025a) conduct mechanistic interpretability analyses and find that overthinking behaviors lie on specific low-dimensional manifold in the models activation space. They introduce Manifold Steering, which projects interventions onto this manifold to avoid high-dimensional noise, thereby reducing computational overhead and performance degradation caused by overthinking."
        },
        {
            "title": "4 Efficient Reasoning with Model Collaboration",
            "content": "Efficient reasoning with model collaboration aims to enhance reasoning efficiency and accuracy in LLMs by enabling cooperation between multiple LLMs, each leveraging distinct reasoning strengths. Unlike single model efficient reasoning method described in section 3, collaborative frameworks strategically combine long-chain reasoning models (long CoT) that excel at handling complex tasks and short-chain reasoning models (short CoT) that are lightweight and efficient for general tasks. This synergy allows for more fine-grained and cost-effective control of the reasoning process. Specific methods include LongShort Model Collaboration (section 4.1), LLM Routing (section 4.2), Model Consolidation (section 4.3), and Speculative Decoding (section 4.4). 4.1 LongShort Model Collaboration LongShort Model Collaboration refers to approaches that integrate the complementary advantages of Long CoT and Short CoT models through dynamic interactions. This section focuses on twomodel setups, where one long CoT and one short CoT model are jointly involved in the reasoning process. Depending on which model plays the dominant role in the interaction, these methods can be categorized into three types: 1. Short-to-Long Collaborative Reasoning: These methods are short-modelcentric, with the short CoT model handling most queries and selectively invoking the long CoT model for complex or uncertain reasoning tasks. 2. Long-to-Short Collaborative Reasoning: In contrast, these methods are long-modelcentric, where the long CoT model leads the reasoning and the short CoT model provides auxiliary support. 3. LongShort Interactive Reasoning: These methods allow both models to interleave and alternate during the reasoning process, enabling multi-round interaction and dynamic control of reasoning depth and complexity. 12 4.1.1 Short-to-Long Collaborative Reasoning These methods typically designate the short CoT model as the primary reasoning agent or utilize it to plan the reasoning process, which then guides the long CoT model (Akhauri et al., 2025; Liu et al., 2025h; Fan et al., 2025; Lin et al., 2025c; Chen et al., 2025b; Kim et al., 2025). Specifically, (Akhauri et al., 2025) propose SplitReason framework where the short CoT model performs most reasoning steps while dynamically offloading complex substeps to the long CoT model. The approach enables collaborative reasoning between models by allowing the short model to delegate tasks it cannot handle. Training proceeds in two stages: first, SFT phase teaches the short CoT model to insert offloading boundaries marked by special tokens <bigmodel>...</bigmodel>; second, RL phase based on GRPO optimizes the offloading behavior using reward function that jointly considers accuracy, formatting consistency, and offloading ratio to balance performance and efficiency. ThoughtMani (Liu et al., 2025h) employs short CoT model to generate CoT, which is injected as prompt between <think> and </think> tokens of the long CoT model. This design allows the long model to directly read and leverage the short models reasoning trajectory, resulting in more efficient and targeted reasoning. Similarly, CoThink (Fan et al., 2025) adopts two-stage framework in which lightweight instruction model first generates high-level solution plan, which is then used to guide the long CoT model through detailed reasoning. PLAN-AND-BUDGET (Lin et al., 2025c) proposes budget-aware planning framework that dynamically allocates reasoning budgets based on task structure and uncertainty. The short CoT model first decomposes the original question into subproblems and estimates the complexity of each sub-problem using confidence scores. normalized token budget is then assigned to each sub-task. During inference, the long CoT model generates reasoning for each sub-problem within its token budget, and an aggregation module compiles the final answer. VeriThinker (Chen et al., 2025b) introduces Supervised Verification Fine-Tuning (SVFT) approach, enabling the short CoT model to self-verify the correctness of its output. If the answer is deemed reliable, it is returned directly. Otherwise, the model triggers long CoT reasoning to produce more robust response. 4.1.2 Long-to-Short Collaborative Reasoning This category of methods typically places the long CoT model as the primary reasoning agent, or leverages it to guide the short CoT model in completing subsequent reasoning steps (Li et al., 2025d; She et al., 2025). For example, Li et al. (2025d) propose FoReaL-Decoding, framework in which strong leading model (long CoT) first generates the initial tokens of sentence to establish the reasoning direction and style. Then, lightweight draft model (short CoT) continues the generation to complete the response. To prevent the leading model from oversteering or dominating the reasoning process, FoReaL-Decoding incorporates stochastic gating mechanism that dynamically controls the frequency of intervention by the leading model, ensuring balanced division of labor and effective collaboration between the two models. 4.1.3 LongShort Interactive Reasoning These methods (Ning et al., 2025; Lee et al., 2025b; Liang et al., 2025) explore interleaved or collaborative reasoning between long and short CoT models to improve inference efficiency. Specifically, Ning et al. (2025) first fine-tune LLMs using synthetic instruction data to separately acquire long and short style reasoning capabilities. Based on this, they design multi-turn dialoguebased RL method, where rewards are defined over final answer correctness, format, and reasoning length. The long CoT model is encouraged to focus on generating key reasoning steps, while the short CoT model completes the rest with concise reasoning, thereby improving both performance and efficiency. COPE (Lee et al., 2025b) introduces multi-stage plan-and-reasoning framework. In Stage 1, the short CoT model handles both planning and reasoning. In Stage 2, the long CoT model takes over planning, while the short CoT model continues reasoning. In Stage 3, the long CoT model fully dominates both planning and reasoning. After each stage, candidate answers are collected through sampling and voting. If no consensus is reached, the system proceeds to the next stage for deeper reasoning. ThinkSwitcher (Liang et al., 2025) proposes lightweight mode-switching module that dynamically selects between long and short CoT models without retraining the base reasoning models. Given an input question, the switcher takes its representation as input and predicts the expected performance of long and short chain reasoning paths. During training, ThinkSwitcher adopts multi-sample evaluation strategy to generate multiple responses per reasoning mode, and constructs continuous 13 supervision signals based on empirical solve rates, thereby avoiding instability from binary labels. At inference time, the model selects the optimal reasoning path based on the switchers prediction. 4.2 LLM Routing LLM routing aims to dynamically select the most suitable model(s) from model pool for each input query, thereby significantly reducing computational cost while maintaining reasoning performance. The model pool typically consists of multiple pretrained models with varying scales. For instance, simple questions such as What is 2 + 3? can be routed to lightweight models (e.g., GPT-2) instead of invoking LLMs (e.g., DeepSeek-R1) to avoid the overthinking problem, thereby improving overall inference efficiency. Existing studies have proposed variety of routing mechanisms, which can be broadly categorized into the following two types: 1. Single-Step Routing: These methods perform one-time evaluation of the input query before inference, routing it to single most appropriate model to complete the task. It is characterized by simplicity and fast response time. 2. Multi-Step Routing: These methods enable dynamic routing to multiple models during the inference process, allowing for collaborative reasoning. It typically decides in real time whether to involve more powerful model based on the current reasoning state. 4.2.1 Single-Step Routing This line of work typically selects single model for inference per query, offering simplicity in implementation and low latency in response (Lu et al., 2023; Chen et al., 2024a; Ding et al., 2024; Zhuang et al., 2024; Zhang et al., 2025f; Chen et al., 2025a). For instance, RouteLLM (Ong et al., 2024) introduces four representative routing strategies: Similarity-weighted Ranking, Matrix Factorization, BERT Classifier, and Causal LLM Classifier, to enable dynamic selection and switching between small and large models. To improve the precision of routing decisions, growing body of research focuses on aligning model capabilities with query characteristics. Specifically, GraphRouter (Feng et al., 2024) employs graph neural networks (GNNs) to model the complex interactions among queries, models, and tasks, thereby optimizing model selection. IRT-Router (Song et al., 2025a) incorporates Item Response Theory (IRT) (Woodruff and Hanson, 1996; Gao et al., 2021) to capture latent relationships between LLM capabilities and query attributes, enabling more fine-grained adaptation. Some methods further introduce similarity-based routing mechanisms. For example, RouterBench (Hu et al., 2024) and Shnitzer et al. (2023) propose K-nearest neighbor (KNN) routing strategies, selecting candidate models by measuring similarity between the current input and historical queries. TagRouter (Chen et al., 2025d) presents training-free model routing approach. It consists of three key modules: TagGenerator that produces semantically relevant tags for each query, TagScorer that learns mappings from tags to model performance using existing data, and TagDecider that determines the final routing path based on these mappings. In addition, to better leverage prior samples and model capability information, He et al. (2025) construct labeled dataset to distinguish whether query requires reasoning, based on problem difficulty. They train reasoning-mode selector accordingly. During inference, lightweight prereasoning stage is introduced to extract capability-aware embeddings from intermediate model representations. These embeddings are used to estimate whether the current model can directly generate high-quality answer. If the query is deemed complex, the reasoning mode is activated to generate complete CoT. Otherwise, generic mode is used to produce concise response, thus effectively avoiding over-reasoning on simple tasks. 4.2.2 Multi-Step Routing This class of methods (Shao et al., 2025; Zhang et al., 2025b; Fu et al., 2025; Pan et al., 2025b) allows routing to different models multiple times during the inference process, typically making dynamic decisions based on the current reasoning state to determine whether additional models should be involved. This enables flexible trade-off between performance and computational cost. Specifically, R2-Reasoner (Shao et al., 2025) proposes RL based framework for collaborative multi-model reasoning. It consists of two key components: Task Decomposer, which splits complex 14 tasks into well-structured and logically ordered subtasks, and Subtask Allocator, which dispatches each subtask to the most appropriate model in heterogeneous model pool based on its difficulty and characteristics. The training process involves SFT on constructed dataset for both modules, followed by staged RL to alternately optimize their parameters, thereby enabling efficient and adaptive reasoning routes. Router-R1 (Zhang et al., 2025b) formulates the routing process as sequential decision-making problem and designs the router itself as reasoning-capable LM. This setup allows dynamic alternation between thinking and routing during task execution, enabling the system to coordinate multiple models to collaboratively complete complex reasoning tasks. The reward function integrates reasoning format consistency, answer correctness, and computational cost, guiding the model toward an effective balance between performance and resource consumption. R2R (Roads to Rome) (Fu et al., 2025) further introduces fine-grained, token-level routing strategy. The system initially lets small LLM take the lead in reasoning, but selectively invokes LLM at critical junctures where ambiguity or reasoning divergence is likely to occur. By combining automatic annotation with lightweight router, R2R significantly reduces overall computation while maintaining reasoning accuracy. Distinct from the above methods, which mainly focus on model routing, Route-To-Reason (RTR) (Pan et al., 2025b) expands the routing target by jointly routing both models and reasoning strategies. During inference, RTR not only dynamically selects which LLM to invoke, but also routes the query to the most suitable reasoning strategy module (e.g., PAL (Gao et al., 2023) or CoD (Xu et al., 2025d)), thus enabling structured and strategy-driven reasoning process. 4.3 Model Consolidation This class of methods aims to combine the strengths of LLM and SLM models to construct new model with efficient reasoning capabilities, thereby significantly reducing computational cost while maintaining strong reasoning performance. Existing approaches can be broadly categorized into two types: 1. Model Distillation: These methods typically adopt large model as the teacher and transfer its reasoning ability to smaller student model. By incorporating techniques such as long-CoT compression during the distillation process, the student model is equipped for efficient reasoning. 2. Model Merging: These methods merge the parameters of long-CoT and short-CoT models to integrate their complementary reasoning styles and capabilities, resulting in new model that supports efficient and effective reasoning. 4.3.1 Model Distillation This class of methods typically leverages LLMs to generate high-quality CoT, constructs new training datasets, and performs SFT on SLM to enable efficient transfer of reasoning capabilities (Xu et al., 2025e; Wu et al., 2025a; Jiang et al., 2025b; Wen et al., 2025). For example, TwT (Xu et al., 2025e) proposes reasoning path synthesis framework in which multiple teacher models collaboratively generate diverse candidate CoT paths. These are filtered based on quality and diversity metrics to construct high-quality reasoning dataset. Building upon this, TwT introduces Habitual Reasoning Distillation, three-stage process. Specifically, the student model first learns from the complete reasoning paths generated by the teachers. The teacher then compresses and optimizes the reasoning paths based on the students performance, creating simplified data for continued training. Finally, the student model is trained solely on final answers, thereby acquiring the ability to complete tasks without relying on explicit reasoning chains. Similarly, Wu et al. (2025a) propose an efficient distillation approach based on Difficulty-Aware Prompting (DAR). In this method, LLMs (e.g., DeepSeek-R1 (Guo et al., 2025)) rewrites CoT paths by adapting them to the difficulty of the input problem, automatically generating more concise and adaptive reasoning paths. These are used to build the LiteCoT dataset, enabling the student model to learn compressed yet effective reasoning patterns. In contrast to the above approaches that rely on teacher-generated CoT paths, DRP (Jiang et al., 2025b) begins with initial reasoning paths generated by the student model and applies pruning via teacher model. Specifically, the teacher identifies and removes redundant or irrelevant steps in the paths, merges semantically repetitive content, and outputs more compact and logically coherent reasoning units, which are then used to supervise the student. These methods can be viewed as multi-model compression approaches, in contrast to single-model compression methods (see Section 3.2). 4.3.2 Model Merging This class of methods builds new models with adaptive reasoning capabilities by merging the parameters of long-CoT and short-CoT models, thereby balancing reasoning effectiveness and inference efficiency (Wu et al., 2025c; Jin et al., 2025a; Luo et al., 2025b; Team et al., 2025). Specifically, Wu et al. (2025c) systematically investigate various model merging strategies, including Average Merging (Wortsman et al., 2022), Task Arithmetic (Ilharco et al., 2022), TIES-Merging (Yadav et al., 2023), DARE (Yu et al., 2024), AIM (Nobari et al., 2025), LoRE-Merging (Liu et al., 2025g), Twin-Merging (Lu et al., 2024), and Sens-Merging (Liu et al., 2025f). Experimental results demonstrate that model merging can significantly reduce inference length, by up to 55% in average response length, while preserving output quality, validating its effectiveness in enhancing reasoning efficiency. Similarly, Hu et al. (2025) propose three-stage framework for constructing reasoning LMs. In the first stage, multiple expert models are trained using modular RL, each specializing in distinct reasoning paradigm such as deduction, induction, or abduction. Each expert is optimized via reward function that combines format and answer correctness. In the second stage, the expert models are merged into unified model using weighted parameter fusion. In the third stage, the merged model undergoes further fine-tuning on domain-specific tasks such as mathematics and programming, resulting in notable improvements in overall reasoning ability. This framework offers viable paradigm for building efficient reasoning models. In addition, Jin et al. (2025a) introduce ReCUT, method that generates multiple reasoning paths via diverse sampling and constructs preference pairs based on both reasoning accuracy and path length. Two sub-models are then trained using Direct Preference Optimization (DPO) (Rafailov et al., 2023), each targeting different optimization goal. The final model is obtained by merging the two sub-models, achieving favorable balance between quality and efficiency. Ada-R1 (Luo et al., 2025b) similarly merges long CoT model with general LLM to build model that can handle reasoning tasks of varying depths. On top of this, RL is further employed to enhance inference efficiency and stability, with detailed training strategies described in Section 3.3.1. 4.4 Speculative Decoding Speculative Decoding is recently proposed technique for accelerating LLM inference (Li et al., 2024; Zhang et al., 2025d; Liu et al., 2025c; Huang et al., 2025b; Liao et al., 2025b; Wang et al., 2025g; Xia et al., 2024; Wu et al., 2025d). The core idea is to let SLM quickly draft segment of candidate tokens, which are then verified in parallel by the LLM. Only if the LLM deems these tokens to be consistent with its own likely generation are they accepted. Otherwise, it re-generates the corresponding content. This draft-then-verify strategy significantly reduces the number of sequential decoding steps required by the LLM, thereby improving efficiency while preserving generation quality. For instance, Reward-Guided Speculative Decoding (RSD) (Liao et al., 2025b) allows lightweight model to propose candidate reasoning steps, which are then evaluated using reward function. Only when necessary does it invoke the LLM for correction, achieving more flexible trade-off between accuracy and computational cost. SpecRouter (Wu et al., 2025d) introduces multi-stage speculative decoding framework that replaces traditional static draft-target model pairs. It dynamically selects the most appropriate draft model and intermediate verification path based on task complexity and system load, reducing rejection rates and optimizing decoding throughput. These methods can be viewed as special case of Short-to-Long Collaborative Reasoning in section 4.1.1, where the SLM proposes and the LLM verifies, enabling faster yet reliable generation through inter-model collaboration. Since (Xia et al., 2024) have provided comprehensive survey of speculative decoding methods, this section focuses on recent advancements that specifically target LRMs (Pan et al., 2025a; Yang et al., 2025d; Wang et al., 2025b). For example, SpecReason (Pan et al., 2025a) proposes speculative reasoning framework that performs fine-grained, adaptive delegation: semantically simple and noncritical reasoning steps are handled by lightweight model, while the stronger model (Long CoT) verifies the semantic validity of these steps. If verified, the reasoning proceeds, otherwise, the stronger model takes over to revise or continue the reasoning process. Similarly, Speculative Thinking (Yang et al., 2025d) dynamically identifies reflective, uncertain, or self-negating tokens in the draft generated 16 by the SLM. The LLM selectively intervenes at these critical reasoning junctures, enhancing the quality of reasoning for complex tasks while preserving overall efficiency. SCoT (Speculative Chainof-Thought) (Wang et al., 2025b) introduces training-free speculative reasoning framework that generates multiple CoT drafts using SLM and lets the LLM either select the most promising one or perform re-reasoning when needed. Unlike token-level speculative decoding, SCoT operates at the CoT-segment level, leveraging the SLMs generation efficiency while reducing latency. It also employs LoRA-based (Hu et al., 2022) alignment between the draft and selector models to mitigate variance and redundancy in the generated drafts. This method exemplifies lightweight collaborative paradigm for accelerating reasoning in LRMs."
        },
        {
            "title": "5 Future Applications",
            "content": "5.1 Efficient Multimodal Reasoning Multimodal reasoning models aim to tackle complex tasks that involve the integration of heterogeneous data sources such as text, images, and audio, and have attracted increasing attention in recent years (Li et al., 2025b). Among them, R1-style multimodal reasoning models have achieved notable performance improvements on complex reasoning tasks by introducing reinforcement learning mechanisms, particularly through the widespread adoption of the GRPO algorithm (Meng et al., 2025; Yang et al., 2025a; Huang et al., 2025c; Shen et al., 2025a). Despite their success, these methods also reveal more severe overthinking problem, characterized by redundant reasoning paths and repetitive reflective processes, which lead to substantial computational overhead. To address this challenge, natural direction is to transfer efficient reasoning methods developed in the textual domain to multimodal scenarios (Lu et al., 2025). However, there remains lack of systematic evaluation and empirical analysis to assess the applicability and effectiveness of the various efficient reasoning methods summarized in this survey under multimodal settings. There is an urgent need to construct dedicated benchmark tasks for efficient multimodal reasoning, to evaluate the transferability, generalization, and practical benefits of these methods. Furthermore, compared to textual reasoning, multimodal reasoning involves more intricate perceptionunderstandingreasoning pipeline, encompassing subtasks such as vision-language alignment and region grounding. When these components are entangled within single reasoning path, they tend to introduce unnecessary computation and information noise. more efficient strategy is to structurally decompose the multimodal reasoning process by clearly delineating the roles and boundaries of each stage. For example, Rex-Thinker (Jiang et al., 2025a) divides the reasoning process into three stages: planning, action and summarization. Similarity, Visionary-R1 (Xia et al., 2025b) adopts captionreasonanswer framework that first generates detailed image descriptions, followed by reasoning and answer generation. Building on such structured decomposition, future research may further explore stage-wise modeling and dynamic control of reasoning length, by applying stage-specific length rewards based on task complexity, thereby improving overall efficiency and stability without sacrificing reasoning quality. 5.2 Efficient Tool-Integrated Reasoning Tool learning aims to overcome the inherent limitations of LLMs in computation, memory, and access to external knowledge (Qu et al., 2025a). In recent years, it has been widely adopted to integrate external tools, such as code interpreters, calculators, and search engines, thereby enhancing the models adaptability and problem-solving capabilities. Existing Tool-Integrated Reasoning (TIR) methods primarily rely on SFT using reasoning paths extracted from stronger LLMs. While this approach can improve tool usage to some extent, it often restricts the models ability to explore tool invocation strategies autonomously, leading to rigid patterns and limited discovery of optimal solutions. To enhance adaptability in tool use, recent research has introduced RL-based approaches that enable models to dynamically decide whether and which tools to invoke based on task requirements (Jin et al., 2025b; Song et al., 2025b; Qian et al., 2025; Peng et al., 2025). For example, Song et al. (2025b) annotate reasoning paths in R1-style frameworks using <think> tags for internal thoughts, and introduce structured tags such as <begin_of_query>...<end_of_query> and <begin_of_documents>...<end_of_documents> in search scenarios to explicitly distinguish between query intents and retrieved results, thereby facilitating clearer modeling of the reasoning process. However, even with such explicit annotation schemes, these TIR methods still suffer from the overthinking problem: models may excessively invoke external tools, resulting in unnecessary computational overhead and latency. In Retrieval-Augmented Generation (RAG) settings, the situation is further exacerbated when retrieved documents are noisy, prompting models to repeatedly reason over redundant or irrelevant content. To mitigate these issues, future research could explore the following two directions: (1) incorporating reward mechanisms that penalize excessive tool calling, encouraging models to minimize redundant calls while maintaining answer accuracy; and (2) performing document refinement and filtering prior to the reasoning stage, removing uninformative or low-density content to reduce unnecessary inference costs at the source (Shi et al., 2025). These approaches hold promise for achieving better efficiencyperformance trade-off in TIR. 5.3 Efficient Multi-Agent Systems In multi-agent systems, multiple agents are typically required to collaborate on complex tasks, process that heavily relies on efficient information exchange and strategic coordination (Zhang et al., 2024a; Li et al., 2025f; Wang et al., 2025f). However, when individual agents suffer from overthinking, it can significantly slow down the overall system response and lead to substantial resource waste, ultimately degrading task execution efficiency at the system level. To alleviate this issue, LLM Routing (Yue et al., 2025) has emerged as promising solution. In this paradigm, the router serves as central component of the agent architecture, dynamically assigning tasks to appropriate models to optimize resource allocation. Specifically, the router leverages task complexity, contextual cues, or historical interaction data to route simpler tasks to lightweight models and delegate more complex ones to powerful LRMs. This approach not only reduces the average computational cost but also improves system-wide responsiveness while maintaining the quality of task completion. Furthermore, future research could explore agent-level reasoning budget scheduling, incorporating techniques such as confidence estimation and adaptive task analysis to enable more fine-grained and intelligent coordination across agents. These directions hold promise for building more efficient multi-agent reasoning systems. 5.4 Truthful and Efficient Reasoning Although R1-style LRMs demonstrate strong reasoning performance, their trustworthiness remains significant challenge due to issues such as low safety (Kuo et al., 2025; Wang et al., 2025c) and the generation of hallucinated information (Research, 2025; Sun et al., 2025). Existing efficient reasoning methods often overlook these trustworthiness risks during optimization. For example, CoT compression methods improve reasoning efficiency by shortening original long CoT sequences. However, they may inadvertently inherit and even amplify security vulnerabilities or hallucination problems present in LRMs. Therefore, ensuring model trustworthiness while enhancing reasoning efficiency is an important and urgent research direction for future work on efficient reasoning. Furthermore, beyond conventional efficient reasoning evaluation metrics focused on accuracy, computational cost, and token usage, it is essential to develop methods to evaluate the trustworthiness of the reasoning process and results. Investigating the trade-offs between trustworthiness and accuracy also represents promising direction for future research."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper presents comprehensive survey of efficient reasoning, targeting the overthinking phenomenon commonly observed in R1-style Large Reasoning Models (LRMs). We propose novel taxonomy that categorizes existing approaches into two major paradigms: single-model and multimodel reasoning. Furthermore, we outline several promising applications that stand to benefit from efficient reasoning, shedding light on potential extensions and new frontiers for future research. We hope this survey provides valuable insights and stimulates further work toward developing reasoning models that are not only capable, but also resource-efficient."
        },
        {
            "title": "References",
            "content": "Aradhye Agarwal, Ayan Sengupta, and Tanmoy Chakraborty. 2025. First Finish Search: Efficient Test-Time Scaling in Large Language Models. arXiv preprint arXiv:2505.18149 (2025). 18 Pranjal Aggarwal and Sean Welleck. 2025. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697 (2025). Yash Akhauri, Anthony Fei, Chi-Chih Chang, Ahmed AbouElhamayed, Yueying Li, and MoarXiv preprint hamed Abdelfattah. 2025. Splitreason: Learning to offload reasoning. arXiv:2504.16379 (2025). Afra Amini, Tim Vieira, Elliott Ash, and Ryan Cotterell. 2024. Variational best-of-n alignment. arXiv preprint arXiv:2407.06057 (2024). Sohyun An, Ruochen Wang, Tianyi Zhou, and Cho-Jui Hsieh. 2025. Dont Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models. arXiv preprint arXiv:2505.21765 (2025). Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, and Neel Nanda. 2024. Refusal in language models is mediated by single direction. arXiv preprint arXiv:2406.11717 (2024). Simon Aytes, Jinheon Baek, and Sung Ju Hwang. 2025. Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching. arXiv preprint arXiv:2503.05179 (2025). Seyedarmin Azizi, Erfan Baghaei Potraghloo, and Massoud Pedram. 2025. Activation Steering for Chain-of-Thought Compression. arXiv preprint arXiv:2507.04742 (2025). Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander DAmour, Jacob Eisenstein, Chirag Nagpal, and Ananda Theertha Suresh. 2024. Theoretical guarantees on the best-of-n alignment policy. arXiv preprint arXiv:2401.01879 (2024). Emil Biju, Shayan Talaei, Zhemin Huang, Mohammadreza Pourreza, Azalia Mirhoseini, and Amin Saberi. 2025. SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models. arXiv preprint arXiv:2506.05745 (2025). Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. 2025c. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567 (2025). Runjin Chen, Zhenyu Zhang, Junyuan Hong, Souvik Kundu, and Zhangyang Wang. 2025e. Seal: Steerable reasoning calibration of large language models for free. arXiv preprint arXiv:2504.07986 (2025). Shuhao Chen, Weisen Jiang, Baijiong Lin, James Kwok, and Yu Zhang. 2024a. Routerdc: Querybased router by dual contrastive learning for assembling large language models. Advances in Neural Information Processing Systems 37 (2024), 6630566328. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. 2024b. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187 (2024). Zhijun Chen, Jingzheng Li, Pengpeng Chen, Zhuoran Li, Kai Sun, Yuankai Luo, Qianren Mao, Dingqi Yang, Hailong Sun, and Philip Yu. 2025a. Harnessing multiple large language models: survey on llm ensemble. arXiv preprint arXiv:2502.18036 (2025). Zigeng Chen, Xinyin Ma, Gongfan Fang, Ruonan Yu, and Xinchao Wang. 2025b. VeriThinker: Learning to Verify Makes Reasoning Model Efficient. arXiv preprint arXiv:2505.17941 (2025). Zhou Chen, Zhiqiang Wei, Yuqi Bai, Xue Xiong, and Jianmin Wu. 2025d. TagRouter: Learning Route to LLMs through Tags for Open-Domain Text Generation Tasks. (2025). arXiv:2506.12473 [cs.CL] Xiaoxue Cheng, Junyi Li, Zhenduo Zhang, Xinyu Tang, Wayne Xin Zhao, Xinyu Kong, and Zhiqiang Zhang. 2025b. Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning. arXiv preprint arXiv:2505.16315 (2025). Zhengxiang Cheng, Dongping Chen, Mingyang Fu, and Tianyi Zhou. 2025a. Optimizing Length Compression in Large Reasoning Models. arXiv preprint arXiv:2506.14755 (2025). 19 Stephen Chung, Wenyu Du, and Jie Fu. 2025. Thinker: Learning to Think Fast and Slow. arXiv preprint arXiv:2505.21097 (2025). Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, et al. 2025. The danger of overthinking: Examining the reasoning-action dilemma in agentic tasks. arXiv preprint arXiv:2502.08235 (2025). Yingqian Cui, Pengfei He, Jingying Zeng, Hui Liu, Xianfeng Tang, Zhenwei Dai, Yan Han, Chen Luo, Jing Huang, Zhen Li, et al. 2025. Stepwise perplexity-guided refinement for efficient chain-of-thought reasoning in large language models. arXiv preprint arXiv:2502.13260 (2025). Muzhi Dai, Chenxu Yang, and Qingyi Si. 2025. S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models. arXiv preprint arXiv:2505.07686 (2025). Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Ruhle, Laks VS Lakshmanan, and Ahmed Hassan Awadallah. 2024. Hybrid llm: Cost-efficient and quality-aware query routing. arXiv preprint arXiv:2404.14618 (2024). Roy Eisenstadt, Itamar Zimerman, and Lior Wolf. 2025. Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs. arXiv preprint arXiv:2506.07240 (2025). Siqi Fan, Peng Han, Shuo Shang, Yequan Wang, and Aixin Sun. 2025. CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models. arXiv preprint arXiv:2505.22017 (2025). Gongfan Fang, Xinyin Ma, and Xinchao Wang. 2025a. Thinkless: Llm learns when to think. arXiv preprint arXiv:2505.13379 (2025). Junfeng Fang, Yukai Wang, Ruipeng Wang, Zijun Yao, Kun Wang, An Zhang, Xiang Wang, and Tat-Seng Chua. 2025b. Safemlrm: Demystifying safety in multi-modal large reasoning models. arXiv preprint arXiv:2504.08813 (2025). Sicheng Feng, Gongfan Fang, Xinyin Ma, and Xinchao Wang. 2025. Efficient reasoning models: survey. arXiv preprint arXiv:2504.10903 (2025). Tao Feng, Yanzhen Shen, and Jiaxuan You. 2024. Graphrouter: graph-based router for llm selections. arXiv preprint arXiv:2410.03834 (2024). Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai, Shengen Yan, Huazhong Yang, and Yu Wang. 2025. R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing. arXiv preprint arXiv:2505.21600 (2025). Jiaxuan Gao, Shu Yan, Qixin Tan, Lu Yang, Shusheng Xu, Wei Fu, Zhiyu Mei, Kaifeng Lyu, and Yi Wu. 2025. How Far Are We from Optimal Reasoning Efficiency? arXiv preprint arXiv:2506.07104 (2025). Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language models. In International Conference on Machine Learning. PMLR, 1076410799. Weibo Gao, Qi Liu, Zhenya Huang, Yu Yin, Haoyang Bi, Mu-Chun Wang, Jianhui Ma, Shijin Wang, and Yu Su. 2021. RCD: Relation map driven cognitive diagnosis for intelligent education systems. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval. 501510. Amirhosein Ghasemabadi, Keith Mills, Baochun Li, and Di Niu. 2025. Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence. arXiv preprint arXiv:2505.20325 (2025). Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025). Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. 2024. Token-budget-aware llm reasoning. arXiv preprint arXiv:2412.18547 (2024). Michael Hassid, Gabriel Synnaeve, Yossi Adi, and Roy Schwartz. 2025. Dont Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning. arXiv preprint arXiv:2505.17813 (2025). Yang He, Xiao Ding, Bibo Cai, Yufei Zhang, Kai Xiong, Zhouhao Sun, Bing Qin, and Ting Liu. 2025. Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning. arXiv preprint arXiv:2505.20664 (2025). Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. 2025. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning. arXiv preprint arXiv:2504.01296 (2025). Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In The Tenth International Conference on Learning Representations. https://openreview.net/forum? id=nZeVKeeFYf9 Qitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang, Benjamin Keigwin, Gaurav Ranganath, Kurt Keutzer, and Shriyash Kaustubh Upadhyay. 2024. ROUTERBENCH: Benchmark for Multi-LLM Routing System. arXiv preprint arXiv: 2403.12031 (2024). Zhiyuan Hu, Yibo Wang, Hanze Dong, Yuhui Xu, Amrita Saha, Caiming Xiong, Bryan Hooi, and Junnan Li. 2025. BeyondAha!: Toward Systematic Meta-Abilities Alignment in Large Reasoning Models. arXiv preprint arXiv:2505.10554 (2025). Chengyu Huang, Zhengxin Zhang, and Claire Cardie. 2025e. HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization. arXiv preprint arXiv:2505.11225 (2025). Langlin Huang, Chengsong Huang, Jixuan Leng, Di Huang, and Jiaxin Huang. 2025b. POSS: Position Specialist Generates Better Draft for Speculative Decoding. arXiv preprint arXiv:2506.03566 (2025). Shijue Huang, Hongru Wang, Wanjun Zhong, Zhaochen Su, Jiazhan Feng, Bowen Cao, and Yi Fung. 2025d. AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting. arXiv preprint arXiv:2505.18822 (2025). Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. 2025c. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749 (2025). Yao Huang, Huanran Chen, Shouwei Ruan, Yichi Zhang, Xingxing Wei, and Yinpeng Dong. 2025a. Mitigating Overthinking in Large Reasoning Models via Manifold Steering. arXiv preprint arXiv:2505.22411 (2025). Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2022. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089 (2022). Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720 (2024). Guochao Jiang, Guofeng Quan, Zepeng Ding, Ziqin Luo, Dixuan Wang, and Zheng Hu. 2025c. Flashthink: An early exit method for efficient reasoning. arXiv preprint arXiv:2505.13949 (2025). Lingjie Jiang, Xun Wu, Shaohan Huang, Qingxiu Dong, Zewen Chi, Li Dong, Xingxing Zhang, Tengchao Lv, Lei Cui, and Furu Wei. 2025d. Think only when you need with large hybrid-reasoning models. arXiv preprint arXiv:2505.14631 (2025). Qing Jiang, Xingyu Chen, Zhaoyang Zeng, Junzhi Yu, and Lei Zhang. 2025a. Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning. arXiv preprint arXiv:2506.04034 (2025). Yuxuan Jiang, Dawei Li, and Frank Ferraro. 2025b. DRP: Distilled Reasoning Pruning with Skillaware Step Decomposition for Efficient Large Reasoning Models. arXiv preprint arXiv:2505.13975 (2025). Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025b. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516 (2025). Zhensheng Jin, Xinze Li, Yifan Ji, Chunyi Peng, Zhenghao Liu, Qi Shi, Yukun Yan, Shuo Wang, Furong Peng, and Ge Yu. 2025a. ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails and Preference Optimization. arXiv preprint arXiv:2506.10822 (2025). Sanyam Kapoor, Nate Gruver, Manley Roberts, Katherine Collins, Arka Pal, Umang Bhatt, Adrian Weller, Samuel Dooley, Micah Goldblum, and Andrew Gordon Wilson. 2025. Large language models must be taught to know what they dont know. In Proceedings of the 38th International Conference on Neural Information Processing Systems (Vancouver, BC, Canada) (NIPS 24). Article 2729, 41 pages. Yujin Kim, Euiin Yi, Minu Kim, Se-Young Yun, and Taehyeon Kim. 2025. Guiding Reasoning in Small Language Models with LLM Assistance. arXiv preprint arXiv:2504.09923 (2025). Martin Kuo, Jianyi Zhang, Aolin Ding, Qinsi Wang, Louis DiValentin, Yujia Bao, Wei Wei, Hai Li, and Yiran Chen. 2025. H-cot: Hijacking the chain-of-thought safety reasoning mechanism to jailbreak large reasoning models, including openai o1/o3, deepseek-r1, and gemini 2.0 flash thinking. arXiv preprint arXiv:2502.12893 (2025). Ayeong Lee, Ethan Che, and Tianyi Peng. 2025a. How well do llms compress their own chain-ofthought? token complexity approach. arXiv preprint arXiv:2503.01141 (2025). Byeongchan Lee, Jonghoon Lee, Dongyoung Kim, Jaehyung Kim, and Jinwoo Shin. 2025b. Collaborative LLM Inference via Planning for Efficient Reasoning. arXiv preprint arXiv:2506.11578 (2025). Boyi Li, Zhonghan Zhao, Der-Horng Lee, and Gaoang Wang. 2025f. Adaptive Graph Pruning for Multi-Agent Communication. arXiv preprint arXiv:2506.02951 (2025). Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023. Inferencetime intervention: Eliciting truthful answers from language model. Advances in Neural Information Processing Systems 36 (2023), 4145141530. Ming Li, Zhengyuan Yang, Xiyao Wang, Dianqi Li, Kevin Lin, Tianyi Zhou, and Lijuan Wang. 2025d. What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding. arXiv preprint arXiv:2506.06998 (2025). Ruosen Li, Ziming Luo, Quan Zhang, Ruochen Li, Ben Zhou, Ali Payani, and Xinya Du. 2025c. AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control. arXiv preprint arXiv:2506.20160 (2025). Yunxin Li, Zhenyu Liu, Zitao Li, Xuanyu Zhang, Zhenran Xu, Xinyu Chen, Haoyuan Shi, Shenyuan Jiang, Xintong Wang, Jifang Wang, et al. 2025b. Perception, reason, think, and plan: survey on large multimodal reasoning models. arXiv preprint arXiv:2505.04921 (2025). Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2024. EAGLE: speculative sampling requires rethinking feature uncertainty. In Proceedings of the 41st International Conference on Machine Learning (Vienna, Austria). Article 1162, 14 pages. Zheng Li, Qingxiu Dong, Jingyuan Ma, Di Zhang, and Zhifang Sui. 2025a. SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning. arXiv preprint arXiv:2505.11274 (2025). Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. 2025e. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419 (2025). Guosheng Liang, Longguang Zhong, Ziyi Yang, and Xiaojun Quan. 2025. Thinkswitcher: When to think hard, when to think fast. arXiv preprint arXiv:2505.14183 (2025). Baohao Liao, Hanze Dong, Yuhui Xu, Doyen Sahoo, Christof Monz, Junnan Li, and Caiming Xiong. 2025a. Fractured Chain-of-Thought Reasoning. arXiv preprint arXiv:2505.12992 (2025). Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, and Caiming Xiong. 2025b. Reward-Guided Speculative Decoding for Efficient LLM Reasoning. arXiv preprint arXiv:2501.19324 (2025). Junhong Lin, Xinyue Zeng, Jie Zhu, Song Wang, Julian Shun, Jun Wu, and Dawei Zhou. 2025c. Plan and Budget: Effective and Efficient Test-Time Scaling on Large Language Model Reasoning. arXiv preprint arXiv:2505.16122 (2025). Weizhe Lin, Xing Li, Zhiyuan Yang, Xiaojin Fu, Hui-Ling Zhen, Yaoyuan Wang, Xianzhi Yu, Wulong Liu, Xiaosong Li, and Mingxuan Yuan. 2025b. TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling. arXiv preprint arXiv:2505.17155 (2025). Zhengkai Lin, Zhihang Fu, Ze Chen, Chao Chen, Liang Xie, Wenxiao Wang, Deng Cai, Zheng Wang, and Jieping Ye. 2025a. Controlling Thinking Speed in Reasoning Models. arXiv preprint arXiv:2507.03704 (2025). Zehui Ling, Deshu Chen, Hongwei Zhang, Yifeng Jiao, Xin Guo, and Yuan Cheng. 2025. Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty. arXiv preprint arXiv:2506.10446 (2025). Hanbing Liu, Lang Cao, Yuanyi Ren, Mengyu Zhou, Haoyu Dong, Xiaojun Ma, Shi Han, and Dongmei Zhang. 2025a. Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significancebased Reinforcement Learning. arXiv preprint arXiv:2506.08125 (2025). Kaiyuan Liu, Chen Shen, Zhanwei Zhang, Junjie Liu, Xiaosong Yuan, and Jieping ye. 2025d. Efficient Reasoning Through Suppression of Self-Affirmation Reflections in Large Reasoning Models. (2025). arXiv:2506.12353 [cs.CL] Sheng Liu, Tianlang Chen, Pan Lu, Haotian Ye, Yizheng Chen, Lei Xing, and James Zou. 2025b. Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute. arXiv preprint arXiv:2506.15882 (2025). Shuqi Liu, Han Wu, Bowei He, Xiongwei Han, Mingxuan Yuan, and Linqi Song. 2025f. SensMerging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models. arXiv preprint arXiv:2502.12420 (2025). Tianyu Liu, Yun Li, Qitan Lv, Kai Liu, Jianchen Zhu, Winston Hu, and Xiao Sun. 2025c. Pearl: Parallel speculative decoding with adaptive draft length. In The Thirteenth International Conference on Learning Representations. Wei Liu, Ruochen Zhou, Yiyun Deng, Yuzhen Huang, Junteng Liu, Yuntian Deng, Yizhe Zhang, and Junxian He. 2025i. Learn to Reason Efficiently with Adaptive Length-based Reward Shaping. arXiv preprint arXiv:2505.15612 (2025). Xin Liu, Farima Fatahi Bayat, and Lu Wang. 2024. Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 1043610448. Xin Liu and Lu Wang. 2025. Answer Convergence as Signal for Early Stopping in Reasoning. arXiv preprint arXiv:2506.02536 (2025). Yue Liu, Jiaying Wu, Yufei He, Hongcheng Gao, Hongyu Chen, Baolong Bi, Ruihan Gong, Jiaheng Zhang, Zhiqi Huang, and Bryan Hooi. 2025e. Efficient inference for large reasoning models: survey. arXiv preprint arXiv:2503.23077 (2025). Yule Liu, Jingyi Zheng, Zhen Sun, Zifan Peng, Wenhan Dong, Zeyang Sha, Shiwen Cui, Weiqiang Wang, and Xinlei He. 2025h. Thought manipulation: External thought can be efficient for large reasoning models. arXiv preprint arXiv:2504.13626 (2025). Zehua Liu, Han Wu, Yuxuan Yao, Ruifeng She, Xiongwei Han, Tao Zhong, and Mingxuan Yuan. 2025g. LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging. arXiv preprint arXiv:2502.10749 (2025). Chenwei Lou, Zewei Sun, Xinnian Liang, Meng Qu, Wei Shen, Wenqi Wang, Yuntao Li, Qingping Yang, and Shuangzhi Wu. 2025. AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning. arXiv preprint arXiv:2505.11896 (2025). Jinghui Lu, Haiyang Yu, Siliang Xu, Shiwei Ran, Guozhi Tang, Siqi Wang, Bin Shan, Teng Fu, Hao Feng, Jingqun Tang, et al. 2025. Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning. arXiv preprint arXiv:2505.15154 (2025). Keming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2023. Routing to the expert: Efficient reward-guided ensemble of large language models. arXiv preprint arXiv:2311.08692 (2023). Zhenyi Lu, Chenghao Fan, Wei Wei, Xiaoye Qu, Dangyang Chen, and Yu Cheng. 2024. Twinmerging: Dynamic integration of modular expertise in model merging. Advances in Neural Information Processing Systems 37 (2024), 7890578935. Feng Luo, Yu-Neng Chuang, Guanchu Wang, Hoang Anh Duy Le, Shaochen Zhong, Hongyi Liu, Jiayi Yuan, Yang Sui, Vladimir Braverman, Vipin Chaudhary, et al. 2025a. AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models. arXiv preprint arXiv:2505.22662 (2025). Haotian Luo, Haiying He, Yibo Wang, Jinluan Yang, Rui Liu, Naiqiang Tan, Xiaochun Cao, Dacheng Tao, and Li Shen. 2025b. Adar1: From long-cot to hybrid-cot via bi-level adaptive reasoning optimization. arXiv preprint arXiv:2504.21659 (2025). Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. 2025c. O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning. arXiv preprint arXiv:2501.12570 (2025). Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. 2025. CoT-Valve: Length-Compressible Chain-of-Thought Tuning. arXiv preprint arXiv:2502.09601 (2025). Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. 2025. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. CoRR (2025). Yu Meng, Mengzhou Xia, and Danqi Chen. 2024. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems 37 (2024), 124198 124235. Tergel Munkhbat, Namgyu Ho, Seo Hyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun. 2025. Self-training elicits concise reasoning in large language models. arXiv preprint arXiv:2502.20122 (2025). Sania Nayab, Giulio Rossolini, Marco Simoni, Andrea Saracino, Giorgio Buttazzo, Nicolamaria Manes, and Fabrizio Giacomelli. 2024. Concise thoughts: Impact of output length on llm reasoning and cost. arXiv preprint arXiv:2407.19825 (2024). Yansong Ning, Wei Li, Jun Fang, Naiqiang Tan, and Hao Liu. 2025. Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning. arXiv preprint arXiv:2505.11827 (2025). Amin Heyrani Nobari, Kaveh Alimohammadi, Ali ArjomandBigdeli, Akash Srivastava, Faez Ahmed, and Navid Azizan. 2025. Activation-Informed Merging of Large Language Models. arXiv preprint arXiv:2502.02421 (2025). Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. 2024. RouteLLM: Learning to Route LLMs from Preference Data. In The Thirteenth International Conference on Learning Representations. Rui Pan, Yinwei Dai, Zhihao Zhang, Gabriele Oliaro, Zhihao Jia, and Ravi Netravali. 2025a. Specreason: Fast and accurate inference-time compute via speculative reasoning. arXiv preprint arXiv:2504.07891 (2025). Zhihong Pan, Kai Zhang, Yuze Zhao, and Yupeng Han. 2025b. Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection. arXiv preprint arXiv:2505.19435 (2025). Chunyi Peng, Zhipeng Xu, Zhenghao Liu, Yishan Li, Yukun Yan, Shuo Wang, Zhiyuan Liu, Yu Gu, Minghe Yu, Ge Yu, et al. 2025. Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning. arXiv preprint arXiv:2505.22095 (2025). Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025. Optimizing Anytime Reasoning via Budget Relative Policy Optimization. arXiv preprint arXiv:2505.13438 (2025). Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. 2024. Mutual reasoning makes smaller llms stronger problem-solvers. arXiv preprint arXiv:2408.06195 (2024). Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. 2025. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958 (2025). Ziqing Qiao, Yongheng Deng, Jiali Zeng, Dong Wang, Lai Wei, Fandong Meng, Jie Zhou, Ju Ren, and Yaoxue Zhang. 2025. ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning. arXiv preprint arXiv:2505.04881 (2025). Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. 2025a. Tool learning with large language models: survey. Frontiers of Computer Science 19, 8 (2025), 198343. Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, et al. 2025b. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond. arXiv preprint arXiv:2503.21614 (2025). Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems 36 (2023), 5372853741. Matthew Renze and Erhan Guven. 2024. The benefits of concise chain of thought on problemsolving in large language models. In 2024 2nd International Conference on Foundation and Large Language Models (FLLM). IEEE, 476483. Vectara Research. 2025. Why does Deepseek-R1 hallucinate so much? (2025). https://www. vectara.com/blog/why-does-deepseek-r1-hallucinate-so-much Claude Shannon. 1948. mathematical theory of communication. The Bell system technical journal 27, 3 (1948), 379423. Chenyang Shao, Xinyang Liu, Yutang Lin, Fengli Xu, and Yong Li. 2025. Route-and-Reason: Scaling Large Language Model Reasoning with Reinforced Model Router. arXiv preprint arXiv:2506.05901 (2025). Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 (2024). Jianshu She, Zhuohao Li, Zhemin Huang, Qi Li, Peiran Xu, Haonan Li, and Qirong Ho. 2025. Hawkeye: Efficient reasoning with model collaboration. arXiv preprint arXiv:2504.00424 (2025). Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. 2025a. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615 (2025). Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, Zhaoxiang Liu, and Shiguo Lian. 2025b. Dast: Difficulty-adaptive slow-thinking for large reasoning models. arXiv preprint arXiv:2503.04472 (2025). 25 Leheng Sheng, An Zhang, Zijian Wu, Weixiang Zhao, Changshuo Shen, Yi Zhang, Xiang Wang, and Tat-Seng Chua. 2025. On Reasoning Strength Planning in Large Reasoning Models. arXiv preprint arXiv:2506.08390 (2025). Yaorui Shi, Sihang Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, and Xiang Wang. 2025. Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs. arXiv preprint arXiv:2505.11277 (2025). Tal Shnitzer, Anthony Ou, Mírian Silva, Kate Soule, Yuekai Sun, Justin Solomon, Neil Thompson, and Mikhail Yurochkin. 2023. Large language model routing with benchmark datasets. arXiv preprint arXiv:2309.15789 (2023). Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. 2025b. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592 (2025). Jiwon Song, Dongwon Jo, Yulhwa Kim, and Jae-Joon Kim. 2025c. Reasoning Path CompresarXiv preprint sion: Compressing Generation Trajectories for Efficient LLM Reasoning. arXiv:2505.13866 (2025). Mingyang Song and Mao Zheng. 2025. Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning. arXiv preprint arXiv:2505.21178 (2025). Wei Song, Zhenya Huang, Cheng Cheng, Weibo Gao, Bihan Xu, GuanHao Zhao, Fei Wang, and Runze Wu. 2025a. IRT-Router: Effective and Interpretable Multi-LLM Routing via Item Response Theory. arXiv preprint arXiv:2506.01048 (2025). Jinyan Su and Claire Cardie. 2025. Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards. arXiv preprint arXiv:2505.18298 (2025). Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, et al. 2025. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419 (2025). Zhongxiang Sun, Qipeng Wang, Haoyu Wang, Xiao Zhang, and Jun Xu. 2025. Detection and Mitigation of Hallucination in Large Reasoning Models: Mechanistic Perspective. arXiv preprint arXiv:2505.12886 (2025). Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Yifu Lu, Mengdi Wang, Dinesh Manocha, Furong Huang, Mohammad Ghavamzadeh, and Amrit Singh Bedi. 2025. Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models. arXiv e-prints (2025), arXiv2506. Xinyu Tang, Xiaolei Wang, Zhihao Lv, Yingqian Min, Wayne Xin Zhao, Binbin Hu, Ziqi Liu, and Zhiqiang Zhang. 2025. Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large Language Models via Representation Engineering. arXiv preprint arXiv:2503.11314 (2025). Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599 (2025). Qwen Team. 2024. Qwq: Reflect deeply on the boundaries of the unknown. (2024). https: //qwenlm.github.io/blog/qwq-32b-preview/ Songjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, and Dongbin Zhao. 2025. Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL. arXiv preprint arXiv:2505.10832 (2025). Chenlong Wang, Yuanning Feng, Dongping Chen, Zhaoyang Chu, Ranjay Krishna, and Tianyi Zhou. 2025a. Wait, We Dont Need to\" Wait\"! Removing Thinking Tokens Improves Reasoning Efficiency. arXiv preprint arXiv:2506.08343 (2025). Cheng Wang, Yue Liu, Baolong Bi, Duzhen Zhang, Zhong-Zhi Li, Yingwei Ma, Yufei He, Shengju Yu, Xinfeng Li, Junfeng Fang, et al. 2025c. Safety in large reasoning models: survey. arXiv preprint arXiv:2504.17704 (2025). 26 Jikai Wang, Juntao Li, Jianye Hou, Bowen Yan, Lijun Wu, and Min Zhang. 2025b. Efficient reasoning for llms through speculative chain-of-thought. arXiv preprint arXiv:2504.19095 (2025). Rui Wang, Hongru Wang, Boyang Xue, Jianhui Pang, Shudong Liu, Yi Chen, Jiahao Qiu, Derek Fai Wong, Heng Ji, and Kam-Fai Wong. 2025h. Harnessing the reasoning economy: survey of efficient reasoning for large language models. arXiv preprint arXiv:2503.24377 (2025). Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, et al. 2025d. Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs. arXiv preprint arXiv:2501.18585 (2025). Yibo Wang, Li Shen, Huanjin Yao, Tiansheng Huang, Rui Liu, Naiqiang Tan, Jiaxing Huang, Kai Zhang, and Dacheng Tao. 2025e. R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search. arXiv preprint arXiv:2505.16838 (2025). Yunhao Wang, Yuhao Zhang, Tinghao Yu, Can Xu, Feng Zhang, and Fengzong Lian. 2025i. Adaptive Deep Reasoning: Triggering Deep Thinking When Needed. arXiv preprint arXiv:2505.20101 (2025). Zhihai Wang, Jie Wang, Jilai Pan, Xilin Xia, Huiling Zhen, Mingxuan Yuan, Jianye Hao, and Feng Wu. 2025g. Accelerating Large Language Model Reasoning via Speculative Search. arXiv preprint arXiv:2505.02865 (2025). Zhexuan Wang, Yutong Wang, Xuebo Liu, Liang Ding, Miao Zhang, Jie Liu, and Min Zhang. 2025f. Agentdropout: Dynamic agent elimination for token-efficient and high-performance llm-based multi-agent collaboration. arXiv preprint arXiv:2503.18891 (2025). Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. 2025. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460 (2025). David Woodruff and Bradley Hanson. 1996. Estimation of Item Response Models Using the EM Algorithm for Finite Mixtures. (1996). Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. 2022. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International conference on machine learning. PMLR, 2396523998. Han Wu, Yuxuan Yao, Shuqi Liu, Zehua Liu, Xiaojin Fu, Xiongwei Han, Xing Li, Hui-Ling Zhen, Tao Zhong, and Mingxuan Yuan. 2025c. Unlocking efficient long-to-short llm reasoning with model merging. arXiv preprint arXiv:2503.20641 (2025). Hang Wu, Jianian Zhu, Yinghui Li, Haojie Wang, Biao Hou, and Jidong Zhai. 2025d. SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in Large Language Models. arXiv preprint arXiv:2505.07680 (2025). Siye Wu, Jian Xie, Yikai Zhang, Aili Chen, Kai Zhang, Yu Su, and Yanghua Xiao. 2025b. ARM: Adaptive Reasoning Model. arXiv preprint arXiv:2505.20258 (2025). Yifan Wu, Jingze Shi, Bingheng Wu, Jiayi Zhang, Xiaotian Lin, Nan Tang, and Yuyu Luo. 2025a. Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with Difficulty-Aware Prompting. arXiv preprint arXiv:2505.19716 (2025). Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. 2025a. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067 (2025). Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. 2024. Unlocking Efficiency in Large Language Model Inference: Comprehensive Survey of Speculative Decoding. In Findings of the Association for Computational Linguistics: ACL 2024. 76557671. Jiaer Xia, Yuhang Zang, Peng Gao, Yixuan Li, and Kaiyang Zhou. 2025b. Visionary-r1: Mitigating shortcuts in visual reasoning with reinforcement learning. arXiv preprint arXiv:2505.14677 (2025). 27 Violet Xiang, Chase Blagden, Rafael Rafailov, Nathan Lile, Sang Truong, Chelsea Finn, and Nick Haber. 2025. Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning. arXiv preprint arXiv:2506.05256 (2025). Yang Xiao, Jiashuo Wang, Ruifeng Yuan, Chunpu Xu, Kaishuai Xu, Wenjie Li, and Pengfei Liu. 2025. LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling. arXiv preprint arXiv:2505.19187 (2025). Roy Xie, David Qiu, Deepak Gopinath, Dong Lin, Yanchao Sun, Chong Wang, Saloni Potdar, and Bhuwan Dhingra. 2025. Interleaved Reasoning for Large Language Models via Reinforcement Learning. arXiv preprint arXiv:2505.19640 (2025). Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. 2025b. Towards Large Reasoning Models: Survey of Reinforced Reasoning with Large Language Models. arXiv preprint arXiv:2501.09686 (2025). Jingxian Xu, Mengyu Zhou, Weichang Liu, Hanbing Liu, Shi Han, and Dongmei Zhang. 2025e. TwT: Thinking without Tokens by Habitual Reasoning Distillation with Multi-Teachers Guidance. arXiv preprint arXiv:2503.24198 (2025). Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. 2025d. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600 (2025). Xiaoang Xu, Shuo Wang, Xu Han, Zhenghao Liu, Huijia Wu, Peipei Li, Zhiyuan Liu, Maosong Sun, and Zhaofeng He. 2025c. A*-Thought: Efficient Reasoning via Bidirectional Compression for Low-Resource Settings. arXiv preprint arXiv:2505.24550 (2025). Yuhui Xu, Hanze Dong, Lei Wang, Doyen Sahoo, Junnan Li, and Caiming Xiong. 2025a. Scalable chain of thoughts via elastic reasoning. arXiv preprint arXiv:2505.05315 (2025). Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. 2023. Resolving interference when merging models. arXiv preprint arXiv:2306.01708 1, 3 (2023). Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Qiaowei Li, Zheng Lin, Li Cao, and Weiping Wang. 2025c. Dynamic Early Exit in Reasoning Models. arXiv preprint arXiv:2504.15895 (2025). Junjie Yang, Ke Lin, and Xing Yu. 2025b. Think when you need: Self-adaptive chain-of-thought learning. arXiv preprint arXiv:2504.03234 (2025). Wang Yang, Xiang Yue, Vipin Chaudhary, and Xiaotian Han. 2025d. Speculative thinking: Enhancing small-model reasoning with large model guidance at inference time. arXiv preprint arXiv:2504.12329 (2025). Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. 2025a. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615 (2025). Jingyang Yi and Jiazheng Wang. 2025. Shorterbetter: Guiding reasoning models to find optimal inference length for efficient reasoning. arXiv preprint arXiv:2504.21370 (2025). Xixian Yong, Xiao Zhou, Yingying Zhang, Jinlin Li, Yefeng Zheng, and Xian Wu. 2025. Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens. arXiv preprint arXiv:2505.18237 (2025). Bin Yu, Hang Yuan, Yuliang Wei, Bailing Wang, Weizhen Qi, and Kai Chen. 2025. Long-short chain-of-thought mixture supervised fine-tuning eliciting efficient reasoning in large language models. arXiv preprint arXiv:2505.03469 (2025). Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. 2024. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Forty-first International Conference on Machine Learning. 28 Danlong Yuan, Tian Xie, Shaohan Huang, Zhuocheng Gong, Huishuai Zhang, Chong Luo, Furu Wei, and Dongyan Zhao. 2025a. Efficient RL Training for Reasoning Models via Length-Aware Optimization. arXiv preprint arXiv:2505.12284 (2025). Hang Yuan, Bin Yu, Haotian Li, Shijun Yang, Christina Dan Wang, Zhou Yu, Xueyin Xu, Weizhen Qi, and Kai Chen. 2025b. Not All Tokens Are What You Need In Thinking. arXiv preprint arXiv:2505.17827 (2025). Yanwei Yue, Guibin Zhang, Boyang Liu, Guancheng Wan, Kun Wang, Dawei Cheng, and Yiyan Qi. 2025. Masrouter: Learning to route llms for multi-agent systems. arXiv preprint arXiv:2502.11133 (2025). Zihao Zeng, Xuyao Huang, Boxiu Li, Hao Zhang, and Zhijie Deng. 2025. Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured Multi-Turn Decomposition. arXiv preprint arXiv:2505.19788 (2025). Anqi Zhang, Yulin Chen, Jane Pan, Chen Zhao, Aurojit Panda, Jinyang Li, and He He. 2025a. Reasoning Models Know When Theyre Right: Probing Hidden States for Self-Verification. arXiv preprint arXiv:2504.05419 (2025). Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024b. Restmcts*: Llm self-training via process reward guided tree search. Advances in Neural Information Processing Systems 37 (2024), 6473564772. Guibin Zhang, Yanwei Yue, Zhixun Li, Sukwon Yun, Guancheng Wan, Kun Wang, Dawei Cheng, Jeffrey Xu Yu, and Tianlong Chen. 2024a. Cut the crap: An economical communication pipeline for llm-based multi-agent systems. arXiv preprint arXiv:2410.02506 (2024). Haozhen Zhang, Tao Feng, and Jiaxuan You. 2025b. Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning. arXiv preprint arXiv:2506.09033 (2025). Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li. 2025c. Adaptthink: Reasoning models can learn when to think. arXiv preprint arXiv:2505.13417 (2025). Jason Zhang and Scott Viteri. 2024. Uncovering Latent Chain of Thought Vectors in Language Models. arXiv preprint arXiv:2409.14026 (2024). Lefan Zhang, Xiaodan Wang, Yanhua Huang, and Ruiwen Xu. 2025d. Learning Harmonized Representations for Speculative Sampling. In The Thirteenth International Conference on Learning Representations. Ruiqi Zhang, Changyi Xiao, and Yixin Cao. 2025f. Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models. arXiv preprint arXiv:2506.04182 (2025). Shengjia Zhang, Junjie Wu, Jiawei Chen, Changwang Zhang, Xingyu Lou, Wangchunshu Zhou, Sheng Zhou, Can Wang, and Jun Wang. 2025e. OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation. arXiv preprint arXiv:2506.02397 (2025). Shangziqi Zhao, Jiahao Yuan, Guisong Yang, and Usman Naseem. 2025. Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning. arXiv preprint arXiv:2505.14582 (2025). Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. 2024. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv preprint arXiv:2411.14405 (2024). Zihao Zhu, Hongbao Zhang, Ruotong Wang, Ke Xu, Siwei Lyu, and Baoyuan Wu. 2025. To Think or Not to Think: Exploring the Unthinking Vulnerability in Large Reasoning Models. arXiv preprint arXiv:2502.12202 (2025). Ren Zhuang, Ben Wang, and Shuifa Sun. 2025. Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping. arXiv preprint arXiv:2505.08392 (2025). 29 Richard Zhuang, Tianhao Wu, Zhaojin Wen, Andrew Li, Jiantao Jiao, and Kannan Ramchandran. 2024. EmbedLLM: Learning Compact Representations of Large Language Models. arXiv preprint arXiv:2410.02223 (2024). Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al. 2023. Representation engineering: top-down approach to ai transparency. arXiv preprint arXiv:2310.01405 (2023)."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education",
        "School of Computer Science and Engineering, Southeast University",
        "University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence"
    ]
}