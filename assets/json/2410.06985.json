{
    "paper_title": "Jointly Generating Multi-view Consistent PBR Textures using Collaborative Control",
    "authors": [
        "Shimon Vainer",
        "Konstantin Kutsy",
        "Dante De Nigris",
        "Ciara Rowles",
        "Slava Elizarov",
        "Simon Donné"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-view consistency remains a challenge for image diffusion models. Even within the Text-to-Texture problem, where perfect geometric correspondences are known a priori, many methods fail to yield aligned predictions across views, necessitating non-trivial fusion methods to incorporate the results onto the original mesh. We explore this issue for a Collaborative Control workflow specifically in PBR Text-to-Texture. Collaborative Control directly models PBR image probability distributions, including normal bump maps; to our knowledge, the only diffusion model to directly output full PBR stacks. We discuss the design decisions involved in making this model multi-view consistent, and demonstrate the effectiveness of our approach in ablation studies, as well as practical applications."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 ] . [ 1 5 8 9 6 0 . 0 1 4 2 : r Jointly Generating Multi-view Consistent PBR Textures using Collaborative Control Shimon Vainer, Konstantin Kutsy, Dante De Nigris, Ciara Rowles, Slava Elizarov, Simon Donné Unity Technologies Corresponding author: donnesimon@gmail.com e o n M l c m p Natural Golden Sandstone Steampunk bear covered in brass cogs and gears Fig. 1: We propose an end-to-end pipeline for generating graphics-ready PBR textures based only on mesh and text prompt. Conditioned on single-view PBR stack, our proposed approach directly and jointly diffuses multi-view PBR images in view space. These are multi-view consistent enough that we can naively fuse them into the mesh texture. This includes linear albedo, roughness and metallic maps, as well as normal bump maps. Abstract. Multi-view consistency remains challenge for image diffusion models. Even within the Text-to-Texture problem, where perfect geometric correspondences are known priori, many methods fail to yield aligned predictions across views, necessitating non-trivial fusion methods to incorporate the results onto the original mesh. We explore this issue for Collaborative Control workflow specifically in PBR Text-to-Texture. Collaborative Control directly models PBR image probability distributions, including normal bump maps; to our knowledge, the only diffusion model to directly output full PBR stacks. We discuss the design decisions involved in making this model multi-view consistent, and demonstrate the effectiveness of our approach in ablation studies, as well as practical applications. Keywords: PBR material generation Texture Generation Multi-view Diffusion Models Collaborative Control Text-to-Texture 2 S. Vainer et al. Fig. 2: Existing RGB-based pipelines (left) bake in lighting artifacts into the output albedo maps (taken from Fantasia3Ds Figure 4 [9]). Collaborative Control (middle, from their appendix), as artist-created albedo maps (right [24]), do not exhibit this."
        },
        {
            "title": "Introduction",
            "content": "Although Diffusion Models [18, 54, 55] have only recently garnered attention and scrutiny in the Computer Vision and Computer Graphics fields, the rate with which novel applications of these foundation models are being developed is staggering. Diffusion models learn the distribution of dataset of images in order to sample novel examples from it. In practice, they are more robust and stable to train than previous alternatives [17, 2730]. They are also much easier to control: Classifier Free Guidance [20] allows straightforward conditioning of the generated sample on input signals, be it text, images, or something else still. Text-to-Texture is one such application: given an input shape (typically mesh), the goal is to generate texture for this shape that aligns to the input text prompt. By now, many concurrent approaches attempt to extract such textures from an RGB-space diffusion model, by generating image samples aligned with given views of the object (be it through fine-tuning, SDS, or other methods). But object appearance is both environment-dependent and view-dependent: the same object can appear significantly different under different lighting or from different angle. Modern rendering pipelines therefore represent textures as physically-based-rendering (PBR) materials that encode exactly how the surfaces react to light independent of both environment and viewpoint. Inverse rendering, i.e. estimating PBR materials from RGB measurements (typically through differentiable lighting pipelines), is ambiguous and finicky and often results in lighting artifacts and sub-optimal textures as shown in Fig. 2. Instead, the Collaborative Control paradigm leverages the RGB models internal state to model the PBR material distribution directly, intuiting that the model internally already disentangles the material information from the environment and view information. As result, it is able to directly model and sample the PBR material distribution, bypassing inverse rendering woes entirely. However, the Collaborative Control paradigm has so far been restricted to single-view. In this work, we illustrate how to extend the paradigm to multi-view context We discuss our design decisions and validate them in the experimental section, showing that our method can generate high-quality multi-view textures that are consistent enough to be fused into texture using naive fusion techniques. Multi-view Consistent PBR Textures using Collaborative Control"
        },
        {
            "title": "2 Related Work",
            "content": "Image Generation Recent advances in image generation techniques have been driven by diffusion models [18, 54, 55], almost always operating in lowerresolution latent domain that can be decoded to pixel values by VAE [49]. Initial works focused around convolutional modeling with global self-attention and cross-attention to text prompts [45, 49], whereas more recent work is focused on more flexible transformer-based models, which consider images as sequence of tokens that attend to each other and the prompt doing away with convolutions entirely [16]. The cost of data acquisition and from-scratch training motivates us to use pre-trained text-to-image model as starting point. Text-to-Texture considers the problem of generating appearances for an object based on text prompt [5, 8, 32, 33, 73, 74, 76]. By conditioning on the known geometry (in the form of depth [5,8,32,32,33,74,76], normals [2,41] and/or world positions [2]), such methods generate realistic appearance by reusing or fine-tuning existing text-to-image models. However, they are typically restricted to either RGB appearance, or only albedo: metallic and roughness maps are considered too far out-of-domain for fine-tuning [2, 39]. PBR properties for use in graphics pipeline are obtained through inverse rendering techniques [9, 10, 39, 67, 68, 72] or through various post-hoc techniques [2, 39, 74], none of which model the underlying PBR distribution properly. To the best of our knowledge, Collaborative Control [60] is the only method to directly model the PBR image distribution for arbitrary shapes, while also predicting normal bump maps (which currently only exists for homogeneous texture patch generation [50, 61, 68]), modality even more ambiguous to extract using inverse rendering. However, it only operates in single view, which severely limits its applications: we extend the Collaborative Control paradigm to strongly multi-view consistent PBR generation. Multi-view Generation Several works [8, 48, 57, 74] lift single-view texturing model to multi-view through sequential in-painting: iteratively selecting new viewpoint, rendering the current partial texture, and filling in missing areas as visible. Unfortunately, these are relatively slow and not properly 3D aware: they are prone to seams and the Janus effect. similar issue occurs in techniques that generate multiple views independently based on reference image and view-position encoding [11, 25]. 3D Fusion of the independent views after every time-step can resolve these issues [5,37,38,64,65,75], but this curtails the diffusion process performance as it is essentially iterative Euclidean projection. For more holistic generation, diffusing grid of multiple views within single image inherently promotes multi-view consistency [2, 14, 35, 51, 52, 66]. It is equivalent (in terms of multi-view interaction) to techniques that extend the self-attention to global multi-view attention [21,40]. However, just as approaches based on video models [43, 62], these approaches still require significant effort in fusing the resulting images e.g. through SDS [14], large-scale reconstruction model [2], or recursive fitting and re-noising of Gaussian splats [43]. 4 S. Vainer et al. Fig. 3: Example albedo, roughness, metallic, bump map, and rendered images from an expert artist [59]. Note how PBR channels are view-independent, but renders are not. To achieve better multi-view consistency we require techniques that directly model cross-view correspondences. Text-to-3D models often restrict the cross-view attention to the known epipolar lines [23, 26, 34, 63, 71] or even tentative point correspondences [15, 22, 70]. We leverage the knowledge of the true 3D shape by attending with each pixel only to its correspondences, as in MVDiffusion [58]. Reference view Multi-view diffusion is much slower to iterate, so we assume reference view to condition the generation. This reference can be either fixed view in the multi-view diffusion [31, 43], or compressed into e.g. CLIP or DINOv2 [44, 47] spaces for cross-attention [25, 78]. We leverage both: the former is paramount for accurately transferring the reference views appearance to the final output, while the latter is crucial for ensuring that occluded areas are still efficiently textured in coherent manner."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminaries Diffusion models generate samples from an image distribution p(z) by learning mapping to pure noise (typically white Gaussian noise). They iterative sample from the conditional distribution p(ztzt+1) to remove small amount of noise at time, with the time-step varying from (pure noise) to 0 (the target image distribution). In practice, we wish to condition to generation to guide it with text prompt and the object geometry G, so we learn p(ztzt+1, , G) instead. PBR materials are compact representation of the bidirectional reflectance distribution function (BRDF), which describes the way light is reflected from the surface of an object. We use the Disney basecolor-metallic model [4], parametrization of the Cook-Torrance model that promotes physical correctness [12]; it comprises RGB albedo, metallic, and roughness components. We introduce additional local structural details through bump normal map, as shown in Fig. 3. We use the same geometry-aligned tangent space as Collaborative Control [60] for this bump map, which is independent of the arbitrary UV space. Multi-view Consistent PBR Textures using Collaborative Control 5 Latent Diffusion Models process latent images: VAE-compressed versions of the RGBor PBR-space images [49]. In multi-view setting it is paramount that the underlying data is view-independent, so that the latent values of one view are easily transferrable to other views. PBR data is therefore better suited for multi-view diffusion than the view-dependent RGB appearance (see Fig. 3). Collaborative Control models the distribution of PBR images using diffusion model conditioned on object shape in the form of normal maps. As no large datasets exist to train such model from scratch, it leverages pretrained but frozen RGB diffusion model, tightly linked to the PBR model. This RGBPBR model pair models the joint distribution of PBR images and their fixedenvironment renders (compared to approaches like SDS [46] which are inherently mode-seeking): the PBR model both controls the RGB generation process towards render-like images and extracts relevant information to generate its own PBR output. The authors show that this paradigm is able to extract much knowledge from the frozen model with only small amount of training data, while still retaining the generalizability, quality and diversity of the base RGB model. 3.2 Architecture We now describe how to lift the Collaborative Control paradigm to multi-view setting, conditioned on both object shape and reference views appearance: the goal is to generate PBR images that are pixel-wise consistent (up to variance in the VAE decoder) and can be naively merged into texture representation. We freeze the reference image and will use it to guide the multi-view generation process [31, 43]. An overview of our architecture is shown in Fig. 4, contrasted against both the vanilla RGB architecture [49] and Collaborative Controls singleview architecture [60]. We discuss how to condition on the reference view, how to leverage the known correspondences from the fixed shape, and how to fuse the multi-view PBR images into single texture map. Notations We take as input set of noisy latent RGB and PBR images pairs {zRGB,i,t} and {zPBR,i,t} (i = 0 is the reference view) with known camera poses {ci} and intrinsics {Ki}. The denoising is conditioned on text prompt and per-view encoded normal maps {zn,i}. Where relevant, we precondition the new layer inputs by adding Plücker coordinate embedding of the camera rays [53]. Cross attention to the reference images hidden states Similar to [31, 43], we process the reference view with its own diffusion process (unperturbed by noise, with fixed timestep 0) and cross-attend from every view to the reference internal states at the same layer. Reference DINOv2 feature attention We also cross attend to the DINOv2 features [44] (after linear projection inspired by [3]) of the reference RGB image. This promotes global semantic reasoning and provides further semantic guidance to the new views [25, 78]. 6 S. Vainer et al. RGB Diffusion zRGB,t+1 PBR Diffusion zPBR,t+1 Self-attention Self-attention + + Multi-view communication Multi-view communication hidden state hidden state Cross-domain communication + + hidden state hidden state Cross-attention Cross-attention + zRGB,t + zPBR,t Plücker ray embedding Reference view hidden state hidden state + Correspondence point-wise attention + hidden state + Cross attention + hidden state + Cross attention Reference view DINOv2 features + hidden state (a) (b) Fig. 4: (a) Structure of the existing attention blocks in vanilla Stable Diffusion 2.1 and Collaborative Control, as well as the multi-view communication we introduce. (b) The components of the multi-view communication block. Occlusion-aware point correspondence attention Finally, we also introduce point-wise attention mechanism [58], leveraging the fact that the geometry of target object is known, and therefore correspondences between any two views (with respective occlusion maps) are readily available. Every (latent) pixel bilinearly interpolates its correspondences in all other views (including the reference), and uses small attention layer to attend to all of these, as seen in Fig. 5. We mask the attention with the known occlusion map to ensure that only true correspondences contribute to this attention; each point also attends to its own features to stabilize training and avoid fully masked-out attention. For the lower resolution occlusion maps, we mean-pool the highest-resolution map and binarize this with threshold of 0.2, to smooth and dilate it at the same time. 3.3 Classifier-Free Guidance and Multiple Guidance Classifier-free guidance [19] finds directions in prediction space that align better to the conditions. To do so, it considers the differential between conditioned and unconditioned predictions, and exaggerates the final prediction towards this direction (effectively amplifying the effect of the conditioning signal). Multiple guidance signals allow more fine-grained control over the applied differentials at the cost of more model evaluations [3, 26, 36], by using the differentials w.r.t. unconditional [36] or partially-conditioned predictions [3, 26]. The former is most useful when the conditions are relatively independent. Our approach, with strong correlation between the conditions, benefits more from the latter approach. Multi-view Consistent PBR Textures using Collaborative Control 7 Self Neighbour 1 Neighbour 2 Neighbour 3 Neighbour 4 Neighbour 5 Features Self o i 4 4 4 6 4 Masked Attention V Fig. 5: The point-wise attention mechanism. We identify the text prompt , geometry G, reference view R, and neighboring views as our conditions. Geometry (as surface normal map) is always used, because its absence does not lead to meaningful prediction differential. We use D(zt+1ϕ, G, ϕ, ϕ) + ωΩ(D(zt+1T , G, ϕ, V) D(zt+1ϕ, G, ϕ, ϕ)) + ωV (D(zt+1T , G, R, V) D(zt+1T , G, ϕ, V)), (1) where denotes the diffusion models prediction, and ϕ dropped condition. The text prompt is dropped out as usual for the base model (replacing it with an empty string in the case of SD2), while we ignore neighboring views by emptying the occlusion masks in the masked attention of Fig. 5. Positional encodings and DINOv2 features are simplied zeroed, while the reference view process is replaced with gaussian noise and set to time step , as in IM-3D [43]. While image generation generally benefits from high CFG weight [19], the guidance by noise-free reference view is so strong that it dominates the output and introduces artifacts when amplified. In inference, we therefore use ωΩ = 7.5 and ωV = 3.5 to prevent the reference view from dominating the output. 3.4 Training Details We first train single-view Collaborative Control with Stable Diffusion 2.1 as base model [60] on 768 768 for total of 200,000 update steps with batch size of 192 and learning rate of 3 105, with frozen base model. Subsequently, we train our multi-view model by adding the multi-view layers as in Fig. 4 and training for 100,000 update steps with batch size of 24 and learning rate of 3 105 (each batch element now contains one reference view and 4 neighboring views). We have found it necessary to train both the single-view and multi-view PBR layers together to achieve best results the base model remains frozen throughout. The multiple guidance signals are trained by randomly reverting to the less-conditioned terms in Eq. (1) with probability of 5% each. Furthermore, to safe-guard the quality of the single-view model, we also drop out all multi-view related conditioning with another 5% probability. Despite training only on fixed number of views, inference is quite robust to the number of views. 8 S. Vainer et al. Algorithm 1 Fusion algorithm, using Kaolin Wisps models [56] and Pytorch. TriplanarGrid(feature dim=32, log base resolution=6, num lods=4) PositionalEmbedder(num freq=8, input dim=3, max freq log2=8) BasicDecoder(output dim=8, activation=nn.SiLU, bias=True) optimizer Adam(lr=1 102) appearances of shape 8 768 768 world positions of shape 3 768 768 bilinear interpolate(A, 1/32) bilinear interpolate(W, 1/32) for iteration = 1, . . . , 300 do modeled appearances D(T(W), P(W)) loss MSE( A, A) loss.backward() optimizer.step() end for for iteration = 1, . . . , 1000 do {Ai} , {Wi} 64 32 32 patches from A, Ai D(T(Wi), P(Wi)) MSE loss (cid:80) channel triplet {cj} draw3from8 LPIPS loss (cid:80) MSE( Ai, Ai) LPIPS( Ai,cj , Ai,cj ) loss MSE loss + 0.1 LPIPS loss loss.backward() optimizer.step() i,j end for 3.5 Fusion into texture map The generated appearances need to be fused into single texture map to be useful in graphics pipelines. After the significant efforts to make the outputs multi-view consistent, we have found that simple fusion approach is sufficient, as there are (nearly) no inconsistencies to resolve. The main drawback we still face is that the fusion is not able to fill in areas that are occluded in all views (contrary to, say, an LRM [2]). For now, we simply distribute the generated views over the top hemisphere, and leave more complex fusion for future work. We represent the object appearance in 3D space using tri-planes [7] and small neural decoder that also consumes positional embedding. For faster optimization, we first fit this to low-resolution versions of the appearances using an MSE loss before fitting to the full-resolution appearances using both perceptual LPIPS loss [77] (on randomly-drawn channel triplets [6]) as well as an MSE loss please refer to Algorithm 1 for the algorithm fusing the appearances into this global representation. Finally, we extract this global representation into texture map by querying the global representation with the respective coordinates of the texture map (assuming bijective mapping between the object and its texture). Multi-view Consistent PBR Textures using Collaborative Control 9 Fig. 6: Example of the rendered views of an object [69]. The training images are rendered on black background; the environment is shown here for illustration only."
        },
        {
            "title": "4 Experiments and Results",
            "content": "We now demonstrate the effectiveness of our proposed method, both in lifting the single-view generation to multi-view and in producing graphics-ready textures. Given that the model is trained with dropped-out conditions of various types, we can re-use it to ablate the importance of those respective components. 4.1 Dataset For training this model, both its single-view variant and its multi-view variant, we use Objaverse [13]. We remove objects without PBR textures or with nonspatially-varying textures, leaving roughly 300,000 objects [60]. The objects are rendered with fixed (camera-corotated) environment map [42] from 16 viewpoints with varying elevation and equally spaced azimuths as shown in Fig. 6. For evaluation, we re-use the held-out test set meshes as used in the Collaborative Control paper [60], with the same ChatGPT-generated prompts per mesh [1], from obvious to extremely unlikely. 4.2 Ablating the new components We compare the full model to restricted versions of itself, where we ablate respectively the pixel-wise correspondence, its occlusion detection, the reference view hidden state attention and the DINOv2 cross attention. For this section, to restrict the effect of the ablations, we only observe single guidance signal D(zt+1ϕ, G, ϕ, ϕ) + ω(D(zt+1T , G, R, V) D(zt+1ϕ, G, ϕ, ϕ)) (2) instead of Eq. (1). We have found that this formulation performs best with relatively low ω = 1.5. For visual clarity, we cherry-pick one test mesh and prompt to visualize for each of the following ablations; the reference views were generated with the single-view model and the same prompt. For compactness, we limit the visualizations to few of the PBR maps as well as the final Blender-rendering of the textured mesh from the other side; the full set of outputs, including generated images, fused textures and rendered videos, is available in the supplementary material."
        },
        {
            "title": "A blooming floral statue of a snake on a bed of roses as a",
            "content": "b e e pedestal pedestal pedestal pedestal"
        },
        {
            "title": "Roughness",
            "content": "c e e"
        },
        {
            "title": "Albedos",
            "content": "c o"
        },
        {
            "title": "Metallic",
            "content": "d n"
        },
        {
            "title": "A blooming floral statue of a snake on a bed of roses as a",
            "content": "pedestal pedestal b e e f l l r pedestal pedestal pedestal pedestal pedestal pedestal c n f e o d l A e f R o e A c n r f R o e l e r A e R"
        },
        {
            "title": "A rustic wooden vase intricately carved with floral patterns",
            "content": "e e c e"
        },
        {
            "title": "Rendered",
            "content": "A blooming floral statue of snake on bed of roses as pedestal"
        },
        {
            "title": "A rustic wooden vase intricately carved with floral patterns\nA rustic wooden vase intricately carved with floral patterns",
            "content": "A rustic wooden vase intricately carved with floral patterns rustic wooden vase intricately carved with floral patterns S. Vainer et al. d c b r f e e 10 n r f R Roughnesses Roughnesses Roughnesses Roughnesses Albedo Albedo A rustic wooden vase intricately carved with floral patterns rustic wooden vase intricately carved with floral patterns m o d e s i e - o f A e o R Roughnesses Roughnesses Roughnesses Roughnesses Fig. 7: Effect of removing the attention to pixel-wise correspondences from the multiview layers in Fig. 4b. This is crucial for best results: without it, the generated views arent pixel-consistent, resulting in ghosting after fusion into mesh texture."
        },
        {
            "title": "Albedo\nAlbedo",
            "content": "Roughness Rendered Metallic Albedos F e o l u e r R b A"
        },
        {
            "title": "Roughnesses\nRoughnesses",
            "content": "A rustic wooden vase intricately carved with floral patterns"
        },
        {
            "title": "Rendered",
            "content": "e n e e f R o e b A e n e o d l b l d o e e R c n r e e R d e l A l l F o e e n e e l d e e s R i w l e e c d n o - - t n u F o o l e d s o i s e w - - n o n l e r a w a o N m d l l u p s e r o c c e d o p e r o c o i u l c o N n i s l c o o s s n e a w a"
        },
        {
            "title": "Albedo",
            "content": "r e e"
        },
        {
            "title": "Metallic",
            "content": "e b b e e r l f f o m l u l c n o m d l l u p s e r o c c e d o p e r o c i w - - n o p N e i w - - n o p N e n e e f R e n o e b A o A e l d m l u F e e n o m d l l u p s o l e d e m c l n F e e d o n m s o i s l u u F n c s r e a o w o a n r o s i s e n c e c a w c o N e n e e f R e n e s i w d - - t l n i o d o A e s w - - n i p o l e o m l u l e o m l s e u F e a w s s n e a w n o s u c c o n o i u l c o N Pixel-wise correspondences are, unsurprisingly, crucial for multi-view consistency. As Fig. 7 shows, omitting pixel-wise correspondence attention noticeably worsens fusion fitting loss, and consequently results in blurry, inconsistent textures in the final result. Although the remaining components of the model manage to keep the style consistent and e.g. prevent Janus artifacts, the lack of pixel-precise guidance has significant impact on the result. Occlusion detection is important to prevent features leaking through the object: as shown in Fig. 8 that is most visible through reference view features appearing in the back view. s n r w Roughnesses Albedo Metallic Rendered o u n u o s r Fig. 8: Ignoring occlusions in the masked attention from Fig. 5 results in features leaking towards back views, most noticeably from the reference view. o s l c o n i u c o N l l u F r w l d m steam-punk bronze robot dog with globe on its back steam-punk bronze robot dog with globe on its back n f l steam-punk bronze robot dog with globe on its back steam-punk bronze robot dog with globe on its back and an ornate Victorian head and an ornate Victorian head and an ornate Victorian head and an ornate Victorian head b r and an ornate Victorian head and an ornate Victorian head steam-punk bronze robot dog with globe on its back steam-punk bronze robot dog with globe on its back"
        },
        {
            "title": "Albedos",
            "content": "A steam-punk bronze robot dog with globe on its back steam-punk bronze robot dog with globe on its back"
        },
        {
            "title": "Roughness",
            "content": "n A steam-punk bronze robot dog with globe on its back steam-punk bronze robot dog with globe on its back"
        },
        {
            "title": "Rendered",
            "content": "and an ornate Victorian head and an ornate Victorian head"
        },
        {
            "title": "Albedos",
            "content": "c e e e f l o o"
        },
        {
            "title": "Roughness",
            "content": "and an ornate Victorian head and an ornate Victorian head d b e e d"
        },
        {
            "title": "Albedos",
            "content": "A steam-punk bronze robot dog with globe on its back steam-punk bronze robot dog with globe on its back"
        },
        {
            "title": "Roughness",
            "content": "e n e e e d e A e n e o d b A m and an ornate Victorian head and an ornate Victorian head"
        },
        {
            "title": "A fully blossomed floral Viking shield\nA fully blossomed floral Viking shield",
            "content": "Multi-view Consistent PBR Textures using Collaborative Control 11 b Albedos e e R"
        },
        {
            "title": "A fully blossomed floral Viking shield\nA fully blossomed floral Viking shield",
            "content": "l u Albedos Albedos e o Roughness Roughness fully blossomed floral Viking shield fully blossomed floral Viking shield l i v r h n Roughness Roughness w i v"
        },
        {
            "title": "Albedos\nAlbedos\no\no\nd\nd\ne\ne\nb\nb\nl\nl\nA\nA",
            "content": "o e Albedos Albedos l e n r f R c e e e e n r f r e e n f d A h l m v F e o N"
        },
        {
            "title": "Albedos\nAlbedos",
            "content": "A steam-punk bronze robot dog with globe on its back and an ornate Victorian head"
        },
        {
            "title": "Rendered\nRendered\nRoughness\nRoughness",
            "content": "Roughness Metallic Rendered"
        },
        {
            "title": "Rendered",
            "content": "Albedos Albedos Fig. 9: Not attending to the reference view states in Fig. 4b shows clearly that the back views draw the majority of their content from this source."
        },
        {
            "title": "Roughness\nRoughness",
            "content": "A fully blossomed floral Viking shield"
        },
        {
            "title": "Rendered\nRendered",
            "content": "Albedos"
        },
        {
            "title": "Albedos\nAlbedos",
            "content": "Roughness Metallic Rendered"
        },
        {
            "title": "Albedos",
            "content": "e n e e f R e n e e f d e l A d e l A e c n o e b A o e b A e e f R e n e e f R l d o l l F l d o w e l l e u d F o o h c l n a w e l e v r o m o o h t l a l u e t t c n r e e r e N n e e f 2 v o N N D o 2 2 O I D N a - - s o o i c n e t a - - s r c e b A o e b A e e f R e n e e f R e o e l A e R c l f e l e o m l u l e o n o i l l u e t t - - s n o r c t e t a - - s o c 2 O N D o 2 2 O I D N l d o l l F l d o w e l i v F o h n a e i r o c n e c e r f e o e c n r e e r N o e n r f r N o c a l l d m d F m o 2 o e i I t u - o N e t - - n o i t n t a - - s r I 2 2 O D 2 2 v N o I Fig. 10: The DINOv2 features of the reference view carry stylistic information. Although their impact is subtle, they help transporting some additional information to the back views. In the views that overlap significantly with the reference view, the absence of these stylistic features is not noticeable. Attention to the reference view or its DINOv2 features Omitting the reference image as batch element results in output that is multi-view consistent and matches the input image stylistically, but is not multi-view consistent with, as shown in Fig. 9. The cross-attention to the reference views DINOv2 features [44], promotes high-level coherency of the style across the views [25, 78], as seen in Fig. 10 this is most important for the back views. c n r e e R d e l A N n - - o o h t W n - - m o z c n r e e R d e l A N n - - m o z t i n i - - o o o N i - - o o h t W n - - o o 3D render of van gogh painting 3D render of van gogh painting 3D render of van gogh painting 3D render of van gogh painting"
        },
        {
            "title": "Generated albedo",
            "content": "3D render of van gogh painting 3D render of van gogh painting c e r f e o d b l 12 S. Vainer et al."
        },
        {
            "title": "Generated albedo\nGenerated albedo",
            "content": "3D render of van gogh painting Generated albedo e e e o - o i - z Visualization zoom Visualization zoom Jointly generated Visualization zoom Fig. 11: As the trained model shows to be robust to both the number of neighbors and the distance to the object, we can add zoom-ins to certain areas in order to boost the local texture quality, despite the restricted inference resolution of the model. Visualization zoom Visualization zoom Visualization zoom Reference Reference Reference Generated Generated Generated Fig. 12: Here, we run the proposed method to generate only one novel view: zoom-in on the reference image. Our model exhibits super-resolution capabilities, generating significantly higher-resolution output PBR images than the reference image. For these results, we have omitted the input text caption to the diffusion model. 4.3 Number of views and varying scale Considering the training details, which fix the number of neighbors to 4 and the distance to the object to be constant, we find in practice that the approach is robust to either. Figure 11 illustrates the effect of adding zoomed-in view of part of the object. This way, we generate high-resolution textures despite the base RGB image models resolution of 768 768. Misusing this extremely powerful feature, Fig. 12 indicates that this can even serve as texture method, although we do not specifically evaluate this capability in this work, and we show only albedo images (all channels are available in the supplementary). Multi-view Consistent PBR Textures using Collaborative Control ] 4 1 [ h F O ] 4 1 [ h F O MetaTextureGen [2] bunny collection Our reproduction from similar prompts a t G [ 2 ] s Fig. 13: Qualitative comparison of renders base on our outputs and those from FlashTex [14] and MetaTextureGen [2] (images taken from their respective papers and supplementaries). Compared to FlashTex, which relies on SDS for PBR maps, we show cleaner appearances and an absence of Janus issues e.g. on the back view of the peony. Our general output quality is high, but doesnt quite reach the same level of realism as MetaTextureGen. We attribute this in part to relatively high CFG scale, and in part to the source dataset: while Objaverse has known drawbacks in terms of realism and quality, not much is known about the dataset used in MetaTextureGen. 4.4 Qualitative comparisons to concurrent methods Finally, we offer qualitative comparison to concurrent state-of-the-art Textto-Texture methods. Figure 13 shows the results of our method compared to FlashTex [14] and MetaTextureGen [2], trying to reproduce similar meshes and prompts as the ones originally reported by the respective authors. MetaTextureGen is to our knowledge the only concurrent method to also directly output full PBR stacks. However, the diffusion model only outputs albedo and specular images; an LRM [2] produces the final PBR images based on these. Finally, we are not aware of any method that generates bump maps jointly with the other PBR images High-resolution geometry is time-consuming to create and resource-intensive during rendering. typical workflow involves an artist sculpting detailed geometry before baking the details into normal map and simplifying the underlying geometry. Given the large number of objects in the Objaverse dataset [13] that underwent this process, our model has managed to learn how to reverse this process to some extent: it can generate high-resolution normal maps for the low-resolution meshes, as shown in Fig. 14. Rendered (no bump map) Rendered (no bump map) Rendered (with bump map) Rendered (with bump map)"
        },
        {
            "title": "Bump map",
            "content": "e n e e f R o e b A e n e e f R o b l e e"
        },
        {
            "title": "Bump map",
            "content": "Rendered (no bump map) Rendered (no bump map) Rendered (no bump map) Rendered (no bump map) Rendered (with bump map) Rendered (with bump map) Rendered (with bump map) Rendered (with bump map) Rendered (no bump map) Rendered (no bump map) Rendered (with bump map) Rendered (with bump map) Rendered (no bump map) Rendered (no bump map) Rendered (with bump map) Rendered (with bump map)"
        },
        {
            "title": "White marble chest with a sapphire lock",
            "content": "An enormous snake statue, sculpted from An enormous snake statue, sculpted from fiery chunks of magma, set upon volfiery chunks of magma, set upon volcanic rock pedestal canic rock pedestal Rendered (no bump map) Rendered (no bump map) Rendered (no bump map) Rendered (no bump map) Rendered (with bump map) Rendered (with bump map)"
        },
        {
            "title": "Metallic\nMetallic",
            "content": "S. Vainer et al. 14 Rendered (no bump map) Rendered (no bump map)"
        },
        {
            "title": "Bump map\nBump map",
            "content": "Rendered (no bump map) Rendered (no bump map) Rendered (no bump map) Rendered (with bump map) Rendered (with bump map) Rendered (with bump map)"
        },
        {
            "title": "White marble chest with a sapphire lock\nWhite marble chest with a sapphire lock",
            "content": "An enormous snake statue, sculpted from An enormous snake statue, sculpted from fiery chunks of magma, set upon volfiery chunks of magma, set upon vol-"
        },
        {
            "title": "Metallic\nMetallic",
            "content": "Roughness e e Albedo b Metallic White marble chest with sapphire lock"
        },
        {
            "title": "Bump map\nBump map",
            "content": "Bump map"
        },
        {
            "title": "Metallic\nMetallic",
            "content": "e r R b A"
        },
        {
            "title": "Bump map\nBump map",
            "content": "An enormous snake statue, sculpted from fiery chunks of magma, set upon volcanic rock pedestal Albedo Roughness Metallic Bump map Rendered (no bump map) Rendered (with bump map) Fig. 14: Our model introduces additional detail through the bump map whenever the geometry is too flat for the intended texture; in other cases, the detailed geometry is left intact. This allows artists to skip the time-consuming sculpting of detailed geometry and its subsequent baking out into the bump map. c e r f e o d b l e c e r f e o d e c n l A r f e o d b l e c e r f e o d e c l n e r f e o d b l e e"
        },
        {
            "title": "Albedo",
            "content": "n o d e"
        },
        {
            "title": "Roughness",
            "content": "r e e"
        },
        {
            "title": "Albedo",
            "content": "n o e l A e e b"
        },
        {
            "title": "White marble chest with a sapphire lock",
            "content": "An enormous snake statue, sculpted from An enormous snake statue, sculpted from"
        },
        {
            "title": "Roughness",
            "content": "e fiery chunks of magma, set upon volfiery chunks of magma, set upon volr R canic rock pedestal canic rock pedestal canic rock pedestal canic rock pedestal A f e e c e r f e o d b l e c e r f e o d b l e c e r f e o d A An enormous snake statue, sculpted from An enormous snake statue, sculpted from fiery chunks of magma, set upon volfiery chunks of magma, set upon voll canic rock pedestal canic rock pedestal canic rock pedestal canic rock pedestal"
        },
        {
            "title": "Metallic",
            "content": "Rendered (with bump map) Rendered (with bump map) Rendered (with bump map) Rendered (with bump map) Rendered (with bump map) Rendered (with bump map) Rendered (with bump map) Rendered (with bump map) Rendered (no bump map) Rendered (no bump map) Rendered (no bump map) Rendered (no bump map) Rendered (with bump map) Rendered (with bump map) Rendered (no bump map) Rendered (no bump map)"
        },
        {
            "title": "5 Conclusion",
            "content": "An enormous snake statue, sculpted from An enormous snake statue, sculpted from fiery chunks of magma, set upon volfiery chunks of magma, set upon volAn enormous snake statue, sculpted from An enormous snake statue, sculpted from fiery chunks of magma, set upon volfiery chunks of magma, set upon volcanic rock pedestal canic rock pedestal We have shown how to extend the single-view PBR results to multi-view consistent predictions that can easily be fused into meshs texture. As result, we now have complete pipeline from un-textured mesh to graphics-compatible texture, which inherits the quality and generalizability of the underlying RGB image model thanks to the Collaborative Control scheme [60]."
        },
        {
            "title": "Bump map\nBump map",
            "content": "The main drawback of our approach is that often, some areas of the texture remain unobserved in the generated views: we see possible solution either in the use of an LRM, or by directly learning diffusion model on the tri-plane space. Additionally, practical use in commercial or otherwise public applications requires significant attention to ethical concerns around the sourcing of training data: both for the base RGB diffusion model and for the PBR model."
        },
        {
            "title": "Bump map\nBump map",
            "content": "This work was supported fully by Unity Technologies, without external funding. Multi-view Consistent PBR Textures using Collaborative Control"
        },
        {
            "title": "References",
            "content": "1. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023) 2. Bensadoun, R., Kleiman, Y., Azuri, I., Harosh, O., Vedaldi, A., Neverova, N., Gafni, O.: Meta 3d texturegen: Fast and consistent texture generation for 3d objects (2024) 3. Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image editing instructions. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1839218402 (2023) 4. Burley, B.: Physically based shading at disney. In: ACM Transactions on Graphics (SIGGRAPH) (2012) 5. Cao, T., Kreis, K., Fidler, S., Sharp, N., Yin, K.: Texfusion: Synthesizing 3d textures with text-guided image diffusion models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 41694181 (2023) 6. Chambon, T., Heitz, E., Belcour, L.: Passing multi-channel material textures to 3-channel loss (May 2021). https://doi.org/10.1145/3450623.3464685, http://arxiv.org/abs/2105.13012v 7. Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., Mello, S.D., Gallo, O., Guibas, L., Tremblay, J., Khamis, S., Karras, T., Wetzstein, G.: Efficient geometryaware 3D generative adversarial networks. In: CVPR (2022) 8. Chen, D.Z., Siddiqui, Y., Lee, H.Y., Tulyakov, S., Nießner, M.: Text2tex: Textdriven texture synthesis via diffusion models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1855818568 (2023) 9. Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 2224622256 (2023) 10. Chen, X., Peng, S., Yang, D., Liu, Y., Pan, B., Lv, C., Zhou, X.: Intrinsicanything: Learning diffusion priors for inverse rendering under unknown illumination. arXiv preprint arXiv:2404.11593 (2024) 11. Chen, Y., Fang, J., Huang, Y., Yi, T., Zhang, X., Xie, L., Wang, X., Dai, W., Xiong, H., Tian, Q.: Cascade-zero123: One image to highly consistent 3d with self-prompted nearby views (Dec 2023), http://arxiv.org/abs/2312.04424v1 12. Cook, R.L., Torrance, K.E.: reflectance model for computer graphics. ACM Transactions on Graphics (ToG) (1982) 13. Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: universe of annotated 3d objects (Dec 2022), http://arxiv.org/abs/2212.08051v 14. Deng, K., Omernick, T., Weiss, A., Ramanan, D., Zhu, J.Y., Zhou, T., Agrawala, M.: Flashtex: Fast relightable mesh texturing with lightcontrolnet (2024), https: //arxiv.org/abs/2402.13251 15. Ding, L., Dong, S., Huang, Z., Wang, Z., Zhang, Y., Gong, K., Xu, D., Xue, T.: Text-to-3d generation with bidirectional diffusion using both 2d and 3d priors. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 51155124 (2024) 16. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Müller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., Podell, D., Dockhorn, T., English, Z., Lacey, K., Goodwin, A., Marek, Y., Rombach, R.: Scaling rectified flow transformers for high-resolution image synthesis (2024), https://arxiv.org/abs/2403.03206 16 S. Vainer et al. 17. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial networks. Communications of the ACM 63(11), 139144 (2020) 18. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in neural information processing systems 33, 68406851 (2020) 19. Ho, J., Salimans, T.: Classifier-free diffusion guidance. In: NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications (2021) 20. Ho, J., Salimans, T.: Classifier-free diffusion guidance (Jul 2022), http://arxiv. org/abs/2207.12598v 21. Höllein, L., Müller, N., Novotny, D., Tseng, H.Y., Richardt, C., Zollhöfer, M., Nießner, M., et al.: Viewdiff: 3d-consistent image generation with text-to-image models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 50435052 (2024) 22. Hu, H., Zhou, Z., Jampani, V., Tulsiani, S.: Mvd-fusion: Single-view 3d via depthconsistent multi-view generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 96989707 (2024) 23. Huang, Z., Wen, H., Dong, J., Wang, Y., Li, Y., Chen, X., Cao, Y.P., Liang, D., Qiao, Y., Dai, B., Sheng, L.: Epidiff: Enhancing multi-view synthesis via localized epipolar-constrained diffusion (Dec 2023), http://arxiv.org/abs/2312.06725v1 24. IrisProcess: Spherical ship sketchfab model, https://sketchfab.com/3d-models/ spherical-ship-a1bed2ab95544638ba24589679a0e898 [Accessed: July 23rd, 2024] 25. Jeong, Y., Lee, J., Kim, C., Cho, M., Lee, D.: Nvs-adapter: Plug-and-play novel view synthesis from single image. arXiv preprint arXiv:2312.07315 (2023) 26. Kant, Y., Wu, Z., Vasilkovsky, M., Qian, G., Ren, J., Guler, R.A., Ghanem, B., Tulyakov, S., Gilitschenski, I., Siarohin, A.: Spad : Spatially aware multiview diffusers (2024), https://arxiv.org/abs/2402.05235 27. Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196 (2017) 28. Karras, T., Aittala, M., Laine, S., Härkönen, E., Hellsten, J., Lehtinen, J., Aila, T.: Alias-free generative adversarial networks. Advances in Neural Information Processing Systems 34, 852863 (2021) 29. Karras, T., Laine, S., Aila, T.: style-based generator architecture for generative adversarial networks. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 44014410 (2019) 30. Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing and improving the image quality of stylegan. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 81108119 (2020) 31. Kim, S., Shi, Y., Li, K., Cho, M., Wang, P.: Multi-view image prompted multi-view diffusion for improved 3d generation (2024), https://arxiv.org/abs/2404.17419 32. Knodt, J., Gao, X.: Consistent mesh diffusion. arXiv preprint arXiv:2312.00971 (2023) 33. Le, C., Hetang, C., Cao, A., He, Y.: Euclidreamer: Fast and high-quality texturing for 3d models with stable diffusion depth. arXiv preprint arXiv:2311.15573 (2023) 34. Li, P., Liu, Y., Long, X., Zhang, F., Lin, C., Li, M., Qi, X., Zhang, S., Luo, W., Tan, P., et al.: Era3d: High-resolution multiview diffusion using efficient row-wise attention. arXiv preprint arXiv:2405.11616 (2024) 35. Li, S., Li, C., Zhu, W., Yu, B., Zhao, Y., Wan, C., You, H., Shi, H., Lin, Y.: Instant-3d: Instant neural radiance field training towards on-device ar/vr 3d reconstruction. In: Proceedings of the 50th Annual International Symposium on Computer Architecture. pp. 113 (2023) Multi-view Consistent PBR Textures using Collaborative Control 17 36. Liu, N., Li, S., Du, Y., Torralba, A., Tenenbaum, J.B.: Compositional visual generation with composable diffusion models. In: European Conference on Computer Vision. pp. 423439. Springer (2022) 37. Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T., Wang, W.: Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453 (2023) 38. Liu, Y., Xie, M., Liu, H., Wong, T.T.: Text-guided texturing by synchronized multi-view diffusion (2023), https://arxiv.org/abs/2311.12891 39. Liu, Z., Li, Y., Lin, Y., Yu, X., Peng, S., Cao, Y.P., Qi, X., Huang, X., Liang, D., Ouyang, W.: Unidream: Unifying diffusion priors for relightable text-to-3d generation. arXiv preprint arXiv:2312.08754 (2023) 40. Long, X., Guo, Y.C., Lin, C., Liu, Y., Dou, Z., Liu, L., Ma, Y., Zhang, S.H., Habermann, M., Theobalt, C., et al.: Wonder3d: Single image to 3d using crossdomain diffusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 99709980 (2024) 41. Lu, Y., Zhang, J., Li, S., Fang, T., McKinnon, D., Tsin, Y., Quan, L., Cao, X., Yao, Y.: Direct2.5: Diverse text-to-3d generation via multi-view 2.5d diffusion (Nov 2023), http://arxiv.org/abs/2311.15980v1 42. Majboroda, S.: Studio small 08 hdri environment map, https://polyhaven.com/ a/studio_small_08 [Accessed: July 21st, 2024] 43. Melas-Kyriazi, L., Laina, I., Rupprecht, C., Neverova, N., Vedaldi, A., Gafni, O., Kokkinos, F.: Im-3d: Iterative multiview diffusion and reconstruction for high-quality 3d generation. arXiv preprint arXiv:2402.08682 (2024) 44. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al.: Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 (2023) 45. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Müller, J., Penna, J., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image synthesis (2023), https://arxiv.org/abs/2307.01952 46. Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using 2d diffusion. In: The Eleventh International Conference on Learning Representations (2023) 47. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision. In: International Conference on Machine Learning (2021), https://api.semanticscholar.org/CorpusID:231591445 48. Richardson, E., Metzer, G., Alaluf, Y., Giryes, R., Cohen-Or, D.: Texture: Textguided texturing of 3d shapes (2023), https://arxiv.org/abs/2302.01721 49. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1068410695 (2022) 50. Sartor, S., Peers, P.: Matfusion: generative diffusion model for svbrdf capture. In: SIGGRAPH Asia 2023 Conference Papers. SA 23, ACM (Dec 2023). https://doi. org/10.1145/3610548.3618194, http://dx.doi.org/10.1145/3610548.3618194 51. Shi, R., Chen, H., Zhang, Z., Liu, M., Xu, C., Wei, X., Chen, L., Zeng, C., Su, H.: Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110 (2023) 52. Shi, Y., Wang, P., Ye, J., Mai, L., Li, K., Yang, X.: Mvdream: Multi-view diffusion for 3d generation. In: The Twelfth International Conference on Learning Representations (2024) S. Vainer et al. 53. Sitzmann, V., Rezchikov, S., Freeman, W.T., Tenenbaum, J.B., Durand, F.: Light field networks: Neural scene representations with single-evaluation rendering (2022), https://arxiv.org/abs/2106.02634 54. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsupervised learning using nonequilibrium thermodynamics. In: International conference on machine learning. pp. 22562265. PMLR (2015) 55. Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. In: International Conference on Learning Representations (2020) 56. Takikawa, T., Perel, O., Tsang, C.F., Loop, C., Litalien, J., Tremblay, J., Fidler, S., Shugrina, M.: Kaolin wisp: pytorch library and engine for neural fields research. https://github.com/NVIDIAGameWorks/kaolin-wisp (2022) 57. Tang, J., Lu, R., Chen, X., Wen, X., Zeng, G., Liu, Z.: Intex: Interactive text-to-texture synthesis via unified depth-aware inpainting. arXiv preprint arXiv:2403.11878 (2024) 58. Tang, S., Zhang, F., Chen, J., Wang, P., Furukawa, Y.: Mvdiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion (Jul 2023), http://arxiv.org/abs/2307.01097v7 59. Teczan, K.: Free game sketchfab model, https : / / sketchfab . com / 3d - models / free - game - character - ancient - 19e18c8e8d5941deb3f9dd0a2a349b56 [Accessed: July 23rd, 2024] - ancient character 60. Vainer, S., Boss, M., Parger, M., Kutsy, K., De Nigris, D., Rowles, C., Perony, N., Donné, S.: Collaborative control for geometry-conditioned pbr image generation (2024) 61. Vecchio, G., Sortino, R., Palazzo, S., Spampinato, C.: Matfuse: Controllable material generation with diffusion models (2024), https://arxiv.org/abs/2308.11408 62. Voleti, V., Yao, C.H., Boss, M., Letts, A., Pankratz, D., Tochilkin, D., Laforte, C., Rombach, R., Jampani, V.: Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion (2024), https://arxiv.org/abs/ 2403.12008 63. Wang, Z., Xu, Q., Tan, F., Chai, M., Liu, S., Pandey, R., Fanello, S., Kadambi, A., Zhang, Y.: Mvdd: Multi-view depth diffusion models. arXiv preprint arXiv:2312.04875 (2023) 64. Wen, H., Huang, Z., Wang, Y., Chen, X., Qiao, Y., Sheng, L.: Ouroboros3d: Imageto-3d generation via 3d-aware recursive diffusion. arXiv preprint arXiv:2406.03184 (2024) 65. Woo, S., Park, B., Go, H., Kim, J.Y., Kim, C.: Harmonyview: Harmonizing consistency and diversity in one-image-to-3d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1057410584 (2024) 66. Wu, K., Liu, F., Cai, Z., Yan, R., Wang, H., Hu, Y., Duan, Y., Ma, K.: Unique3d: High-quality and efficient 3d mesh generation from single image (2024), https: //arxiv.org/abs/2405.20343 67. Wu, T., Li, Z., Yang, S., Zhang, P., Pan, X., Wang, J., Lin, D., Liu, Z.: Hyperdreamer: Hyper-realistic 3d content generation and editing from single image. In: SIGGRAPH Asia 2023 Conference Papers. pp. 110 (2023) 68. Xu, X., Lyu, Z., Pan, X., Dai, B.: Matlaber: Material-aware text-to-3d via latent brdf auto-encoder. arXiv preprint arXiv:2308.09278 (2023) 69. Yakpower: Honda s800 sketchfab model, https://sketchfab.com/3dmodels/ honda-s800-fa6c6113f1e34d9baaff80b5b586a25d [Accessed: July 23rd, 2024] 70. Yang, J., Cheng, Z., Duan, Y., Ji, P., Li, H.: Consistnet: Enforcing 3d consistency for multi-view images diffusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 70797088 (2024) Multi-view Consistent PBR Textures using Collaborative Control 19 71. Ye, J., Wang, P., Li, K., Shi, Y., Wang, H.: Consistent-1-to-3: Consistent image to 3d view synthesis via geometry-aware diffusion models (2024), https://arxiv. org/abs/2310. 72. Yeh, Y.Y., Huang, J.B., Kim, C., Xiao, L., Nguyen-Phuoc, T., Khan, N., Zhang, C., Chandraker, M., Marshall, C.S., Dong, Z., et al.: Texturedreamer: Imageguided texture synthesis through geometry-aware diffusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 43044314 (2024) 73. Youwang, K., Oh, T.H., Pons-Moll, G.: Paint-it: Text-to-texture synthesis via deep convolutional texture map optimization and physically-based rendering. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 43474356 (2024) 74. Zeng, X., Chen, X., Qi, Z., Liu, W., Zhao, Z., Wang, Z., Fu, B., Liu, Y., Yu, G.: Paint3d: Paint anything 3d with lighting-less texture diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 42524262 (2024) 75. Zhang, H., Pan, Z., Zhang, C., Zhu, L., Gao, X.: Texpainter: Generative mesh texturing with multi-view consistency (2024), https://arxiv.org/abs/2406.18539 76. Zhang, J., Tang, Z., Pang, Y., Cheng, X., Jin, P., Wei, Y., Yu, W., Ning, M., Yuan, L.: Repaint123: Fast and high-quality one image to 3d generation with progressive controllable 2d repainting. arXiv preprint arXiv:2312.13271 (2023) 77. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as perceptual metric. In: CVPR (2018) 78. Zheng, C., Vedaldi, A.: Free3d: Consistent novel view synthesis without 3d representation (Dec 2023), http://arxiv.org/abs/2312.04551v"
        }
    ],
    "affiliations": [
        "Unity Technologies"
    ]
}