{
    "paper_title": "LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias",
    "authors": [
        "Haian Jin",
        "Hanwen Jiang",
        "Hao Tan",
        "Kai Zhang",
        "Sai Bi",
        "Tianyuan Zhang",
        "Fujun Luan",
        "Noah Snavely",
        "Zexiang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose the Large View Synthesis Model (LVSM), a novel transformer-based approach for scalable and generalizable novel view synthesis from sparse-view inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which encodes input image tokens into a fixed number of 1D latent tokens, functioning as a fully learned scene representation, and decodes novel-view images from them; and (2) a decoder-only LVSM, which directly maps input images to novel-view outputs, completely eliminating intermediate scene representations. Both models bypass the 3D inductive biases used in previous methods -- from 3D representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar projections, plane sweeps) -- addressing novel view synthesis with a fully data-driven approach. While the encoder-decoder model offers faster inference due to its independent latent representation, the decoder-only LVSM achieves superior quality, scalability, and zero-shot generalization, outperforming previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive evaluations across multiple datasets demonstrate that both LVSM variants achieve state-of-the-art novel view synthesis quality. Notably, our models surpass all previous methods even with reduced computational resources (1-2 GPUs). Please see our website for more details: https://haian-jin.github.io/projects/LVSM/ ."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 2 ] . [ 1 2 4 2 7 1 . 0 1 4 2 : r LVSM: LARGE VIEW SYNTHESIS MODEL WITH MINIMAL 3D INDUCTIVE BIAS Haian Jin1 Hanwen Jiang 2 Hao Tan3 Kai Zhang3 Sai Bi3 Tianyuan Zhang4 Fujun Luan3 Noah Snavely1 Zexiang Xu3 1Cornell University 2The University of Texas at Austin 3Adobe Research 4Massachusetts Institute of Technology"
        },
        {
            "title": "ABSTRACT",
            "content": "We propose the Large View Synthesis Model (LVSM), novel transformer-based approach for scalable and generalizable novel view synthesis from sparse-view inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which encodes input image tokens into fixed number of 1D latent tokens, functioning as fully learned scene representation, and decodes novel-view images from them; and (2) decoder-only LVSM, which directly maps input images to novelview outputs, completely eliminating intermediate scene representations. Both models bypass the 3D inductive biases used in previous methodsfrom 3D representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar projections, plane sweeps)addressing novel view synthesis with fully data-driven approach. While the encoder-decoder model offers faster inference due to its independent latent representation, the decoder-only LVSM achieves superior quality, scalability, and zero-shot generalization, outperforming previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive evaluations across multiple datasets demonstrate that both LVSM variants achieve state-of-the-art novel view synthesis quality. Notably, our models surpass all previous methods even with reduced computational resources (1-2 GPUs). Please see our website for more details: https://haian-jin.github.io/projects/LVSM/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Novel view synthesis is long-standing challenge in vision and graphics. For decades, the community has generally relied on various 3D inductive biases, incorporating 3D priors and handcrafted structures to simplify the task and improve synthesis quality. Recently, NeRF, 3D Gaussian Splatting (3DGS), and their variants (Mildenhall et al., 2020; Barron et al., 2021; Müller et al., 2022; Chen et al., 2022; Xu et al., 2022; Kerbl et al., 2023; Yu et al., 2024) have significantly advanced the field by introducing new inductive biases through carefully designed 3D representations (e.g., continuous volumetric fields and Gaussian primitives) and rendering equations (e.g., ray marching and splatting with alpha blending), reframing view synthesis as the optimization of the representations using rendering losses on per-scene basis. Other methods have also built generalizable networks to estimate these representations or directly generate novel-view images in feed-forward manner, often incorporating additional 3D inductive biases, such as projective epipolar lines or plane-sweep volumes, in their architecture designs (Wang et al., 2021a; Yu et al., 2021; Chen et al., 2021; Suhail et al., 2022b; Charatan et al., 2024; Chen et al., 2024). While effective, these 3D inductive biases inherently limit model flexibility, constraining their adaptability to more diverse and complex scenarios that do not align with predefined priors or handcrafted structures. Recent large reconstruction models (LRMs) (Hong et al., 2024; Li et al., 2023; Wei et al., 2024; Zhang et al., 2024) have made notable progress in removing architecture-level biases by leveraging large transformers without relying on epipolar projections or plane-sweep volumes, achieving state-of-the-art novel view synthesis quality. However, despite these advances, LRMs still rely on representation-level biasessuch as NeRFs, meshes, or 3DGS, along with their respective rendering equationsthat limit their potential generalization and scalability. This work was done when Haian Jin, Hanwen Jiang, and Tianyuan Zhang were interns at Adobe Research. 1 Figure 1: LVSM supports feed-forward novel view synthesis from sparse posed image inputs (even from single view) on both objects and scenes. LVSM achieves significant quality improvements compared with the previous SOTA method, i.e., GS-LRM (Zhang et al., 2024). (Please zoom in for more details.) In this work, we aim to minimize 3D inductive biases and push the boundaries of novel view synthesis with fully data-driven approach. We propose the Large View Synthesis Model (LVSM), novel transformer-based framework that synthesizes novel-view images from posed sparse-view inputs without predefined rendering equations or 3D structures, enabling accurate, efficient, and scalable novel view synthesis with photo-realistic quality (see Fig. 1 for visual examples). To this end, we first introduce an encoder-decoder LVSM, removing handcrafted 3D representations and their rendering equations. We use an encoder transformer to map the input (patchified) multi-view image tokens into fixed number of 1D latent tokens, independent of the number of input views. These latent tokens are then processed by decoder transformer, which uses target-view Plücker rays as positional embeddings to generate the target views image tokens, ultimately regressing the output pixel colors from final linear layer. The encoder-decoder LVSM jointly learns reconstructor (encoder), scene representation (latent tokens), and renderer (decoder) directly from data. By removing the need for predefined inductive biases in rendering and representation, LVSM offers improved generalization and achieves higher quality compared to NeRFand GS-based approaches. However, the encoder-decoder LVSM still retains key bias: the need for an intermediate, albeit fully learned, scene representation. To further push the boundaries, we propose decoder-only LVSM, which adopts single-stream transformer to directly convert the input multi-view tokens into target view tokens, bypassing any intermediate representations. The decoder-only LVSM integrates the novel view synthesis process into holistic data-driven framework, achieving scene reconstruction and rendering simultaneously in fully implicit manner with minimal 3D inductive bias. We present comprehensive evaluation of variants of both LVSM architectures. Notably, our models, trained on 2-4 input views, demonstrate strong zero-shot generalization to an unseen number of views, ranging from single input to more than 10. Thanks to minimal inductive biases, our decoder-only model consistently outperforms the encoder-decoder variant in terms of quality, scalability, and zero-shot capability with varying numbers of input views. On the other hand, the encoder-decoder model achieves much faster inference speed due to its use of fixed-length latent scene representation. Both models, benefiting from reduced 3D inductive biases, outperform previous methods, achieving state-of-the-art novel view synthesis quality across multiple object-level and scene-level benchmark datasets. Specifically, our decoder-only LVSM surpasses previous state-of-the-art methods, such as GS-LRM, by substantial margin of 1.5 to 3.5 dB PSNR. Our final models were trained on 64 A100 GPUs for 3-7 days, depending on the data type and model architecture, but we found that even with just 12 A100 GPUs for training, our model (with decreased model and batch size) still outperforms all previous methods trained with equal or even more compute resources."
        },
        {
            "title": "2 RELATED WORK",
            "content": "View Synthesis. Novel view synthesis (NVS) has been studied for decades. Image-based rendering (IBR) methods perform view synthesis by weighted blending of input reference images using proxy geometry (Debevec et al., 1996; Heigl et al., 1999; Sinha et al., 2009). Light field methods build slice of the 4D plenoptic function from dense view inputs (Gortler et al., 1996; Levoy & Hanrahan, 1996; Davis et al., 2012). Recent learning-based IBR methods incorporate convolutional networks to predict blending weights (Hedman et al., 2018; Zhou et al., 2016; 2018) or using predicted depth maps (Choi et al., 2019). However, the renderable region is usually constrained to be near the input viewpoints. Other work leverages multiview-stereo reconstructions to enable rendering under larger viewpoint changes (Jancosek & Pajdla, 2011; Chaurasia et al., 2013; Penner & Zhang, 2017). In contrast, we use more scalable network designs to learn generalizable priors from larger, real-world data. Moreover, we perform rendering at the image patch level, achieving better model efficiency, and rendering quality. Optimizing 3D Representations. NeRF (Mildenhall et al., 2020) introduced neural volumetric 3D representation with differentiable volume rendering, enabling neural scene reconstruction by minimizing rendering losses and setting new standard in novel view synthesis. Later work improved NeRF with better rendering quality (Barron et al., 2021; Verbin et al., 2022; Barron et al., 2023), faster optimization or rendering speed (Reiser et al., 2021; Hedman et al., 2021; Reiser et al., 2023), and looser requirements on the input views (Niemeyer et al., 2022; Martin-Brualla et al., 2021; Wang et al., 2021b). Other work has explored hybrid representations that combine implicit NeRF content with explicit 3D information, e.g., in the form of voxels, as in DVGO (Sun et al., 2022). Spatial complexity can be further decreased by using sparse voxels (Liu et al., 2020; Fridovich-Keil et al., 2022), volume decomposition (Chan et al., 2022; Chen et al., 2022; 2023), and hashing techniques (Müller et al., 2022). Another line of works investigates explicit point-based representations (Xu et al., 2022; Zhang et al., 2022; Feng et al., 2022). Gaussian Splatting (Kerbl et al., 2023) extends these 3D points to 3D Gaussians, improving both rendering quality and speed. In contrast, we perform novel view synthesis using large transformer models (optionally with learned latent scene representation) without the need of any inductive bias of using prior 3D representations or any per-scene optimization process. Generalizable Feed-forward Methods. Generalizable methods enable fast NVS inference by using neural networks, trained across scenes, to predict the novel views or an underlying 3D representation in feed-forward manner. For example, PixelNeRF (Yu et al., 2021), MVSNeRF (Chen et al., 2021) and IBRNet (Wang et al., 2021a) predict volumetric 3D representations from input views, utilizing 3Dspecific priors like epipolar lines or plane sweep cost volumes. Later methods improve performance under (unposed) sparse views (Liu et al., 2022; Johari et al., 2022; Jiang et al., 2024; 2023), while other work extends to 3DGS representations Charatan et al. (2024); Chen et al. (2024); Tang et al. (2024). On the other hand, approaches that attempt to directly learn rendering function (Suhail et al., 2022a; Sajjadi et al., 2022; Sitzmann et al., 2021; Rombach et al., 2021) have proven not to be scalable and lack model capacity, preventing them from capturing high-frequency details. Specifically, SRT (Sajjadi et al., 2022) intends to remove the use of handcrafted 3D representations and learn latent representation instead, similar to our encoder-decoder model. However, it utilizes CNN feature extraction and adopts cross-attention transformer with non-scalable model and rendering designs. In contrast, our models are fully transformer-based with self-attention, and we introduce more scalable decoder-only architecture that can effectively learn the novel view synthesis function with minimal 3D inductive bias, without an intermediate latent representation. Recently, 3D large reconstruction models (LRMs) have emerged (Hong et al., 2024; Li et al., 2023; Wang et al., 2023; Xu et al., 2023; Wei et al., 2024; Zhang et al., 2024; Xie et al., 2024), utilizing scalable transformer architectures (Vaswani et al., 2023) trained on large datasets to learn generic 3D priors. While these methods avoid using epipolar projection or cost volumes in their architectures, they still rely on existing 3D representations like tri-plane NeRFs, meshes, or 3DGS, along with their corresponding rendering equations, limiting their potential. In contrast, our approach eliminates these 3D inductive biases, aiming to learn rendering function (and optionally scene representation) directly from data. This leads to more scalable models and significantly improved rendering quality."
        },
        {
            "title": "3 METHOD",
            "content": "We first provide an overview of our method in Sec. 3.1, then describe two different transformer-based model variants in Sec. 3.2. 3 Figure 2: LVSM model architecture. LVSM first patchifies the posed input images into tokens. The target view to be synthesized is represented by its Plücker ray embeddings and is also tokenized. The input view and target tokens are sent to full transformer-based model to predict the tokens that are used to regress the target view pixels. We study two LVSM transformer architectures, as Decoder-only architecture (left) and Encoder-Decoder architecture (right). 3.1 OVERVIEW Given sparse input images with known camera poses and intrinsics, denoted as {(Ii, Ei, Ki)i = 1, . . . , }, LVSM synthesizes target image It with novel target camera extrinsics Et and intrinsics Kt. Each input image has shape RHW 3, where and are the image height and width (and there are 3 color channels). Framework. As shown in Fig. 2, our LVSM method uses an end-to-end transformer model to directly render the target image. LVSM starts by tokenizing the input images. We first compute pixel-wise Plücker ray embedding (Plucker, 1865) for each input view using the camera poses and intrinsics. We denote these Plücker ray embeddings as {Pi RHW 6i = 1, . . . , }. We patchify the RGB images and Plücker ray embeddings into non-overlapping patches, following the image tokenization layer of ViT (Dosovitskiy, 2020). We denote the image and Plücker ray patches of input image Ii as {Iij Rpp3j = 1, . . . , HW/p2} and {Pij Rpp6j = 1, . . . , HW/p2}, respectively, where is the patch size. For each patch, we concatenate its image patch and Plücker ray embedding patch, reshape them into 1D vector, and use linear layer to map it into an input patch token xij: xij = Linearinput ([Iij, Pij]) Rd, (1) where is the latent size, and [, ] means concatenation. Similarly, LVSM represents the target pose to be synthesized as its Plücker ray embeddings Pt RHW 6, computed from the given target extrinsics Et and intrinsics Kt. We use the same patchify method and another linear layer to map it to the Plücker ray tokens of the target view, denoted as: qj = Lineartarget (Pt j) Rd, (2) where Pt is the Plücker ray embedding of the jth patch in the target view. We flatten the input tokens into 1D token sequence, denoted as x1, . . . , xlx , where lx = HW/p2 is the sequence length of the input image tokens. We also flatten the target query tokens as q1, . . . , qlq from the ray embeddings, with lq = HW/p2 as the sequence length. LVSM then synthesizes novel view by conditioning on the input view tokens using full transformer model : y1, . . . , ylq = (q1, . . . , qlq x1, . . . , xlx ). Specifically, the output token yj is an updated version of qj, containing the information to predict the pixel RGB values of the jth patch of the target view. The implementation details of model are described in Sec. 3.2 next. (3) 4 We recover the spatial structure of output tokens using the inverse operation of the flatten operation. To regress RGB values of the target patch, we employ linear layer followed by Sigmoid function: = Sigmoid(Linearout(yj)) R3p2 ˆIt (4) We reshape the predicted RGB values back to the 2D patch in Rpp3, and then we get the synthesized novel view ˆIt by performing the same operation on all target patches independently. . Loss Function. Following prior works (Zhang et al., 2024; Hong et al., 2024), we train LVSM with photometric novel view rendering losses: = MSE(ˆIt, It) + λ Perceptual(ˆIt, It), (5) where λ is the weight for balancing the perceptual loss (Johnson et al., 2016)."
        },
        {
            "title": "3.2 TRANSFORMER-BASED MODEL ARCHITECTURE",
            "content": "In this subsection, we present the two LVSM architecturesencoder-decoder and decoder-only both designed to minimize 3D inductive biases. Following their name, encoder-decoder first converts input images to latent representation before decoding the final image colors, whereas decoder-only directly outputs the synthesized target view without an intermediate representation, further minimizing inductive biases in its design. To better illustrate the two models, we draw an analogy between them and their counterparts in the large language model (LLM) domain. In detail, the encoder-decoder model is inspired by the encoder-decoder framework in the original Transformer model (Vaswani et al., 2023) and PerceiverIO (Jaegle et al., 2021) though with different model details, e.g. more decoder layers and removal of cross-attention layers. And the decoder-only model resembles GPT-like decoder-only architecture but without adopting causal attention mask (Radford et al., 2019). Importantly, we clarify that the naming of encoder and decoder are based on their output characteristicsi.e., the encoder outputs the latent while the decoder outputs the targetrather than being strictly tied to the transformer architecture they utilize. For instance, in the encoderdecoder model, the decoder consists of multiple transformer layers with self-attention (referred to as Transformer Encoder layers in the original transformer paper). However, we designate it as decoder because its primary function is to output results. These terminologies align with conventions used in LLMs (Vaswani et al., 2023; Radford et al., 2019; Devlin, 2018). Notably, we apply self-attention to all tokens in every transformer block of both models, without introducing special attention masks or other architectural biases, in line with our philosophy of minimizing inductive bias. Encoder-Decoder Architecture. The encoder-decoder LVSM comes with learned latent scene representation for view synthesis, avoiding the use of NeRF, 3DGS, and other representations. The encoder first maps the input tokens to an intermediate 1D array of latent tokens (functioning as latent scene representation). Then the decoder predicts the outputs, conditioning on the latent tokens and target pose. Similar to the triplane tokens in LRMs (Hong et al., 2024; Wei et al., 2024), we use learnable latent tokens {ek Rdk = 1, ..., l} to aggragate information from input tokens {xi}. The encoder, denoted as TransformerEnc, uses multiple transformer layers with self-attention. We concatenate {xi} and {ek} as the input of TransformerEnc, which performs information aggregation between them to update {ek}. The output tokens that correspond to the latent tokens, denoted as {zk}, are used as the intermediate latent scene representation. The other tokens (updated from {xi}, denoted as {x i}) are unused and discarded. The decoder uses multiple transformer layers with self-attention. In detail, the inputs are the concatenation of the latent tokens {zk} and the target view query tokens {qj}. By applying selfattention transformer layers over the input tokens, we get output tokens with the same sequence length as the input. The output tokens that corresponds to the target tokens q1, . . . , qlq are treated as final outputs y1, . . . , ylq , and the other tokens (updated from {zi}, denoted as {z i}) are unused. This architecture can be formulated as: 1, . . . , , z1, . . . , zl = TransformerEnc(x1, . . . , xlx , e1, . . . , el ) lx 1, . . . , z l, y1, . . . , ylq = TransformerDec(z1, . . . , zl, q1, . . . , qlq ). (7) (6) 5 Decoder-Only Architecture. Our alternate, decoder-only model further eliminates the need for an intermediate scene representation. Its architecture is similar to the decoder in encoder-decoder architecture but differs in inputs and model size. We concatenate the two sequences of input tokens {xi} and target tokens {qj}. The final output {yj} is the decoders corresponding output for the target tokens {qj}. The other tokens (updated from {xi}, denoted as {x i}) are unused and discarded. This architecture can be formulated as: 1, . . . , x lx , y1, . . . , ylq = TransformerDec-only (x1, . . . , xlx , q1, . . . , qlq ) (8) Here the TransformerDec-only has multiple full self-attention transformer layers."
        },
        {
            "title": "4.1 DATASETS",
            "content": "We train (and evaluate) LVSM on object-level and scene-level datasets separately. Object-level Datasets. We use the Objaverse dataset (Deitke et al., 2023) to train LVSM. We follow the rendering settings in GS-LRM (Zhang et al., 2024) and render 32 random views for 730K objects. We test on two object-level datasets, i.e., Google Scanned Objects (Downs et al., 2022) (GSO) and Amazon Berkeley Objects (Collins et al., 2022b) (ABO). In detail, GSO and ABO contain 1099 and 1000 objects, respectively. Following Instant3D (Li et al., 2023) and GS-LRM (Zhang et al., 2024), we have 4 sparse views as testing inputs and another 10 views as target images. Scene-level Datasets. We use the RealEstate10K dataset (Zhou et al., 2018), which contains 80K video clips curated from 10K Youtube videos of both indoor and outdoor scenes. We follow the train/test data split used in pixelSplat (Charatan et al., 2024). 4.2 TRAINING DETAILS Improving Training Stability. We observe that the training of LVSM crashes with plain transformer layers (Vaswani et al., 2023) due to exploding gradients. We empirically find that using QKNorm (Henry et al., 2020) in the transformer layers stabilizes training. This observation is consistent with Bruce et al. (2024) and Esser et al. (2024). We also skip optimization steps with gradient norm > 5.0 besides the standard 1.0 gradient clipping. Efficient Training Techniques. We use FlashAttention-v2 (Dao, 2023) in the xFormers (Lefaudeux et al., 2022), gradient checkpointing (Chen et al., 2016), and mixed-precision training with Bfloat16 data type to accelerate training. Other Details. We train LVSM with 64 A100 GPUs using batch size of 8 per GPU. We use cosine learning rate schedule with peak learning rate of 4e-4 and warmup of 2500 iterations. We train LVSM for 80k iterations on the object and 100k on scene data. LVSM uses image patch size of = 8 and token dimension = 768. The details of transformer layers follow GS-LRM (Zhang et al., 2024) with an additional QK-Norm. Unless noted, all models have 24 transformer layers, the same as GS-LRM. The encoder-decoder LVSM has 12 encoder layers and 12 decoder layers. Note that our model size is smaller than GS-LRM, as GS-LRM uses token dimension of 1024. For object-level experiments, we use 4 input views and 8 target views for each training example by default. We first train with resolution of 256, which takes 4 days for the encoder-decoder model and 7 days for the decoder-only model. Then we finetune the model with resolution of 512 for 10k iterations with smaller learning rate of 4e-5 and smaller total batch size of 128, which takes 2.5 days. For scene-level experiments We use 2 input views and 6 target views for each training example. We first train with resolution of 256, which takes about 3 days for both encoder-decoder and decoder-only models. Then we finetune the model with resolution of 512 for 20k iterations with smaller learning rate of 1e-4 and total batch size of 128 for 3 days. For both object and scene-level experiments, the view selection details and camera pose normalization methods follow GS-LRM. We use perceptual loss weight λ as 0.5 and 1.0 on scene-level and object-level experiments, respectively. 4.3 EVALUATION AGAINST BASELINES In this section, we describe our experimental setup and datasets (Sec. 4.1), introduce our model training details (Sec. 4.2), report evaluation results (Sec. 4.3) and perform an ablation study (Sec. 4.4). 6 Table 1: Quantitative comparisons on object-level (left) and scene-level (right) view synthesis. For the object-level comparison, we matched the baseline settings with GS-LRM (Zhang et al., 2024) in both input and rendering under both resolution of 256 (Res-256) and resolution of 512 (Res-512). For the scene-level comparison, we use the same validation dataset used by pixelSplat (Charatan et al., 2024), which has 256 resolution. ABO (Collins et al., 2022a) GSO (Downs et al., 2022) RealEstate10k (Zhou et al., 2018) PSNR LPIPS SSIM PSNR SSIM LPIPS PSNR SSIM LPIPS Triplane-LRM (Li et al., 2023) (Res-512) GS-LRM (Zhang et al., 2024) (Res-512) Ours Encoder-Decoder (Res-512) Ours Decoder-Only (Res-512) LGM (Tang et al., 2024) (Res-256) GS-LRM (Zhang et al., 2024) (Res-256) Ours Encoder-Decoder (Res-256) Ours Decoder-Only (Res-256) 27.50 29.09 29.81 32.10 20.79 28.98 30.35 32.47 0.896 0.925 0.913 0.938 0.813 0.926 0.923 0.944 0.093 0.085 0.065 0. 0.158 0.074 0.052 0.037 26.54 30.52 29.32 32.36 21.44 29.59 29.19 31.71 0.893 0.952 0.933 0.962 0.832 0.944 0.932 0.957 0.064 0.050 0.052 0. 0.122 0.051 0.046 0.027 pixelNeRF (Yu et al., 2021) GPNR (Suhail et al., 2022a) Du et. al (Du et al., 2023) pixelSplat (Charatan et al., 2024) MVSplat (Chen et al., 2024) GS-LRM (Zhang et al., 2024) Ours Encoder-Decoder Ours Decoder-Only 20.43 24.11 24.78 26.09 26.39 28.10 28.58 29.67 0.589 0.793 0.820 0.863 0.869 0.892 0.893 0.906 0.550 0.255 0.213 0.136 0.128 0.114 0.114 0.098 Figure 3: Object-level visual comparison at 512 resolution. Given 4 sparse input posed images (leftmost column), we compare our high-res object-level novel-view rendering results with two baselines: Instant3Ds Triplane-LRM (Li et al., 2023) and GS-LRM (Res-512) (Zhang et al., 2024) . Both our Encoder-Decoder and Decoder-Only models exhibit fewer floaters (first example) and fewer blurry artifacts (second example), compared to the baselines. Our Decoder-Only model effectively handles complex geometry, including small holes (third example) and thin structures (fourth example). Additionally, it preserves the details of high-frequency texture (last example). Object-Level Results. We compare with Instant3Ds Triplane-LRM (Li et al., 2023) and GSLRM (Zhang et al., 2024) at resolution of 512. As shown on the left side of Table 1, our LVSM achieves the best performance against all prior works. In particular, at 512 resolution, our decoderonly LVSM achieves 3 dB and 2.8 dB PSNR gain against the best prior method GS-LRM on ABO and GSO, respectively; our encoder-decoder LVSM achieves performance comparable to GS-LRM. We also compare with LGM (Tang et al., 2024) at the resolution of 256, as the official LGM model is trained with an input resolution of 256. We also report the performance of models trained on resolution of 256. Compared with the best prior work GS-LRM, our decoder-only LVSM demonstrates 3.5 dB and 2.2 dB PSNR gain on ABO and GSO, respectively; our encoder-decoder LVSM shows slightly better performance than GS-LRM. 7 Figure 4: Scene-level visual comparison. We evaluate our encoder-decoder and decoder-only models on scene-level view synthesis, comparing them against the prior leading baseline methods, namely pixelSplat (Charatan et al., 2024), MVSplat (Chen et al., 2024), and GS-LRM (Zhang et al., 2024). Our methods exhibit fewer texture and geometric artifacts, generate more accurate and realistic specular reflections, and are closer to the ground truth images. These significant performance gains validate the effectiveness of our design target of removing 3D inductive bias. More interestingly, the larger performance gain on ABO shows that our method can handle challenging materials, which are difficult for current handcrafted 3D representations. The qualitative results in Fig. 3 and Fig. 7 also validate the high degree of realism of LVSM, especially for examples with specular materials, detailed textures, and thin, complex geometry. Scene-Level Results. We compare with prior works pixelNeRF (Yu et al., 2021), GPNR (Suhail et al., 2022a), Du et al. (2023), pixelSplat (Charatan et al., 2024), MVSplat (Chen et al., 2024) and GS-LRM (Zhang et al., 2024). As shown on the right side of Table 1, our decoder-only LVSM shows 1.6 dB PSNR gain compared with the best prior work GS-LRM. Our encoder-decoder LVSM also demonstrates comparable results to GS-LRM. These improvements can be observed qualitatively in Fig. 4, where LVSM has fewer floaters and better performance on thin structures and specular materials, consistent with the object-level results. These outcomes again validate the efficacy of our design of using minimal 3D inductive bias. LVSM Trained with Only 1 GPU. Limited computing is key bottleneck for academic research. To show the potential of LVSM using academic-level resources, we train LVSM on the scenelevel dataset (Zhou et al., 2018) following the setting of pixelSplat (Charatan et al., 2024) and MVSplat (Chen et al., 2024), with only single A100 80G GPU for 7 days. In this experiment, we use smaller decoder-only model (denoted LVSM-small) with 6 transformer layers and smaller batch size of 64 (in contrast to the default one with 24 layers and batch size 512). Our decoderonly LVSM-small shows performance of 27.66 dB PSNR, 0.870 SSIM, and 0.129 LPIPS. This performance surpasses the prior best 1-GPU-trained model MVSplat, with 1.3 dB PSNR gain. We also train the decoder-only LVSM (12 transformer layers, batch size 64) with 2 GPUs for 7 days, exhibiting performance of 28.56 dB PSNR, 0.889 SSIM, and 0.112 LPIPS. This performance is Table 2: Ablations studies on model sizes. The following experiments are run with 8 GPUs. RealEstate10k (Zhou et al., 2018) PSNR SSIM LPIPS Ours Encoder-Decoder (6 + 18) Ours Encoder-Decoder (12 + 12) Ours Encoder-Decoder (18 + 6) Ours Decoder-Only (24 layers) Ours Decoder-Only (18 layers) Ours Decoder-Only (12 layers) Ours Decoder-Only (6 layers) 28.32 27.39 26. 28.89 28.77 28.61 27.62 0.888 0.869 0.855 0.894 0.892 0.890 0.869 0.117 0.137 0.152 0.108 0.109 0.111 0.129 Table 3: Ablations studies on model attention architecture. The GSO experiments below are run with 32 GPUs and small batch size. The others are run with 64 GPUs. GSO (Downs et al., 2022) PSNR SSIM LPIPS Ours Encoder-Decoder Ours w/o latents self-updating 28.07 26.61 0.920 0. 0.053 0.061 RealEstate10k (Zhou et al., 2018) PSNR SSIM LPIPS Ours Decoder-Only Ours w/ per-patch prediction 29.67 28.98 0.906 0. 0.098 0.103 even better than GS-LRM with 24 transformer layers trained on 64 GPUs. These results show the promising potential of LVSM for academic research."
        },
        {
            "title": "4.4 ABLATION STUDIES",
            "content": "Model Size. As shown in Table 2, we ablate the model size designs of both LVSM variants. For the encoder-decoder LVSM, we maintain the total number of transformer layers while allocating different number of layers to the encoder and decoder. We observe that using more decoder layers helps the performance while using more encoder layers harms the performance. We conjecture the reason is that the encoder uses the latent representation as the compression of input images and this compression process is hard to learn from data using more encoder layers, leading to more compression errors. This is an interesting observation showing that using the inductive bias of the encoder and intermediate latent representation may not be optimal. For the decoder-only LVSM, we experiment with using different numbers of transformer layers in the decoder. The experiment verifies that decoder-only LVSM demonstrates an increasing performance when using more transformer layers. This phenomenon validates the scalability of the decoder-only LVSM. We also provide the decoder-only model scaling results on object-level reconstruction in Table 4 of Appendix, where the larger models perform relatively better. Model Architecture. We evaluate the effectiveness of our design objective, which emphasizes minimal inductive bias at the model architecture level. To do this, we replace the self-attention transformer layers with variants that incorporate human-designed priors. We first ablate the importance of updating latent representation in the decoder of the encoder-decoder LVSM. We design variant, where the input for each transformer layer of the decoder are the target tokens and raw latent tokens predicted by the encoder, rather than the updated latent tokens outputted from the previous transformer layer. Thus, we use fixed latent tokens in the decoder without updating. We perform experiments at the object level. As shown on the top part of Table 3, this variant shows degraded performance with 2 dB less PSNR. This result can be explained by that the transformer themselves is part of the latent representation, as they are used to update the latent representation in the decoder. We then ablate the importance of joint prediction of target image patches in the decoder-only LVSM. We design variant where the colors of each target pose patch are predicted independently, without applying self-attention across other target pose patches. We achieve this by letting each transformers layers key and value matrices only consist of the updated input image tokens, while both the updated input image tokens and target pose tokens form the query matrices. As shown on the bottom part of Table 3, this variant shows worse performance with 0.7 dB PSNR degradation. This result demonstrates the importance of using both input and target tokens as the context of each other for information propagation using the simplest full self-attention transformer, in line with our philosophy of reducing inductive bias. 4.5 DISCUSSIONS Zero-shot Generalization to More Input Views. We compare our LVSM with GS-LRM by taking different numbers of input views to the training. We report the results on the object level. Note that these models are trained only with 4 input views and test on other input view numbers in zero-shot manner. As shown in Fig. 5, our decoder-only LVSM shows increasingly better performance when 9 Figure 5: Zero-shot generalization to different number of input images on the GSO dataset (Downs et al., 2022). We note that all models are trained with just 4 input views. Figure 6: Rendering FPS with different number of input images. We refer to rendering as the decoding process, which synthesizes novel views from latent tokens or input images. using more input views, verifying the scalability of our model design at test time. Our encoderdecoder LVSM shows similar performance pattern with GS-LRM, i.e., exhibiting performance drop when using more than 8 input views. We conjecture the reason is the inductive bias of the encoder-decoder design, i.e. using intermediate representation as compression of input information, limits the performance. In addition, our single-input result (input view number = 1) is competitive and even beats some of our baseline which takes 4 images as input. These performance patterns validate our design target of using minimal 3D inductive bias for learning fully data-driven rendering model and cohere with our discussion in Sec. 3.2. Encoder-Decoder versus Decoder-Only. As we mentioned in Sec. 3, the decoder-only and encoder-decoder architectures exhibit different trade-offs in speed, quality, and potential. The encoder-decoder model transforms 2D image inputs into fixed-length set of 1D latent tokens, which serve as compressed representation of the 3D scene. This approach simplifies the decoder, reducing its model size. Furthermore, during the rendering/decoding process, the decoder always receives fixed number of tokens, regardless of the number of input images, ensuring consistent rendering speed. As result, this model offers improved rendering efficiency, as shown in Fig. 6. Additionally, the use of 1D latent tokens as the latent representation for the 3D scene opens up the possibility of integrating this model with generative approaches for 3D content generation on its 1D latent space. Nonetheless, the compression process can result in information loss, as the fixed latent tokens length is usually smaller than the original image tokens length, which imposes an upper bound on performance. This characteristic of the encoder-decoder LVSM mirrors prior encoder-decoder LRMs, whereas our LVSM does not have an explicit 3D structure. In contrast, the decoder-only model learns direct mapping from the input image to the target novel view, showcasing better scalability. For example, as the number of input images increases, the model can leverage all available information, resulting in improved novel view synthesis quality. However, this property also leads to linear increase in input image tokens, causing the computational cost to grow quadratically and limiting the rendering speed. Single Input Image. As shown in our project page, Fig. 1 and Fig. 5, we observe that our LVSM also works with single input view for many cases, even though the model is trained with multi-view images during training. This observation shows the capability of LVSM to understand the 3D world, e.g. understanding depth, rather than performing purely pixel-level view interpolation."
        },
        {
            "title": "5 CONCLUSIONS",
            "content": "In this work, we presented the Large View Synthesis Model (LVSM), transformer-based approach designed to minimize 3D inductive biases for scalable and generalizable novel view synthesis. Our two architecturesencoder-decoder and decoder-onlybypass physical-rendering-based 3D representations like NeRF and 3D Gaussian Splatting, allowing the model to learn priors directly from data, leading to more flexible and scalable novel view synthesis. The decoder-only LVSM, with its minimal inductive biases, excels in scalability, zero-shot generalization, and rendering quality, while the encoder-decoder LVSM achieves faster inference due to its fully learned latent scene representation. Both models demonstrate superior performance across diverse benchmarks and mark an important step towards general and scalable novel view synthesis in complex, real-world scenarios. 10 Acknowledgements. We thank Kalyan Sunkavalli for helpful discussions and support. This work was done when Haian Jin, Hanwen Jiang, and Tianyuan Zhang were research interns at Adobe Research. This work was also partly funded by the National Science Foundation (IIS-2211259, IIS-2212084)."
        },
        {
            "title": "REFERENCES",
            "content": "Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. ICCV, 2021. Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. ICCV, 2023. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1612316133, 2022. David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian In Proceedings of the splats from image pairs for scalable generalizable 3d reconstruction. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1945719467, 2024. Gaurav Chaurasia, Sylvain Duchene, Olga Sorkine-Hornung, and George Drettakis. Depth synthesis and local warps for plausible image-based navigation. ACM transactions on graphics (TOG), 32 (3):112, 2013. Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo, 2021. URL https://arxiv.org/abs/2103.15595. Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European conference on computer vision, pp. 333350. Springer, 2022. Anpei Chen, Zexiang Xu, Xinyue Wei, Siyu Tang, Hao Su, and Andreas Geiger. Factor fields: unified framework for neural fields and beyond. arXiv preprint arXiv:2302.01226, 2023. Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images, 2024. URL https://arxiv.org/abs/2403.14627. Inchang Choi, Orazio Gallo, Alejandro Troccoli, Min Kim, and Jan Kautz. Extreme view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 77817790, 2019. Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas F. Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, and Jitendra Malik. Abo: Dataset and benchmarks for real-world 3d object understanding, 2022a. URL https://arxiv.org/abs/2110.06199. Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2112621136, 2022b. 11 Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Abe Davis, Marc Levoy, and Fredo Durand. Unstructured light fields. In Computer Graphics Forum, pp. 305314, 2012. Paul E. Debevec, Camillo Jose Taylor, and Jitendra Malik. Modeling and rendering architecture from photographs: hybrid geometryand image-based approach. Seminal Graphics Papers: Pushing the Boundaries, Volume 2, 1996. URL https://api.semanticscholar.org/ CorpusID:2609415. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1314213153, 2023. Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B. McHugh, and Vincent Vanhoucke. Google scanned objects: high-quality dataset of 3d scanned household items, 2022. URL https://arxiv.org/abs/2204.11918. Yilun Du, Cameron Smith, Ayush Tewari, and Vincent Sitzmann. Learning to render novel views from wide-baseline stereo pairs, 2023. URL https://arxiv.org/abs/2304.08463. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Wanquan Feng, Jin Li, Hongrui Cai, Xiaonan Luo, and Juyong Zhang. Neural points: Point cloud In Proceedings of the IEEE/CVF representation with neural fields for arbitrary upsampling. Conference on Computer Vision and Pattern Recognition, pp. 1863318642, 2022. Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 55015510, 2022. Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F. Cohen. The lumigraph. Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, 1996. URL https://api.semanticscholar.org/CorpusID:2036193. Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. ACM Transactions on Graphics (ToG), 37(6):115, 2018. Peter Hedman, Pratul Srinivasan, Ben Mildenhall, Jonathan Barron, and Paul Debevec. Baking neural radiance fields for real-time view synthesis. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 58755884, 2021. Benno Heigl, Reinhard Koch, Marc Pollefeys, Joachim Denzler, and Luc Van Gool. Plenoptic modeling and rendering from image sequences taken by hand-held camera. In Mustererkennung 1999: 21. DAGM-Symposium Bonn, 15.17. September 1999, pp. 94101. Springer, 1999. Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization for transformers. arXiv preprint arXiv:2010.04245, 2020. Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d, 2024. URL https://arxiv.org/abs/2311.04400. 12 Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: general architecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021. Michal Jancosek and Tomas Pajdla. Multi-view reconstruction preserving weakly-supported surfaces. In CVPR 2011, pp. 31213128. IEEE, 2011. Hanwen Jiang, Zhenyu Jiang, Yue Zhao, and Qixing Huang. Leap: Liberate sparse-view 3d modeling from camera poses. arXiv preprint arXiv:2310.01410, 2023. Hanwen Jiang, Zhenyu Jiang, Kristen Grauman, and Yuke Zhu. Few-view object reconstruction with unknown categories and camera poses. In 2024 International Conference on 3D Vision (3DV), pp. 3141. IEEE, 2024. Mohammad Mahdi Johari, Yann Lepoittevin, and François Fleuret. Geonerf: Generalizing nerf with geometry priors, 2022. URL https://arxiv.org/abs/2111.13539. Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pp. 694711. Springer, 2016. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023. URL https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/. Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, et al. xformers: modular and hackable transformer modelling library, 2022. Marc Levoy and Pat Hanrahan. Light field rendering. Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, 1996. URL https://api. semanticscholar.org/CorpusID:1363510. Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model, 2023. URL https://arxiv.org/abs/2311.06214. Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. Advances in Neural Information Processing Systems, 33:1565115663, 2020. Yuan Liu, Sida Peng, Lingjie Liu, Qianqian Wang, Peng Wang, Christian Theobalt, Xiaowei Zhou, and Wenping Wang. Neural rays for occlusion-aware image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 78247833, 2022. Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7210 7219, 2021. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis, 2020. URL https://arxiv.org/abs/2003.08934. Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Transactions on Graphics, 41(4):115, July 2022. ISSN 1557-7368. doi: 10.1145/3528223.3530127. URL http://dx.doi.org/10. 1145/3528223.3530127. Michael Niemeyer, Jonathan Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 54805490, 2022. 13 Eric Penner and Li Zhang. Soft 3d reconstruction for view synthesis. ACM Transactions on Graphics (TOG), 36(6):111, 2017. Julius Plucker. Xvii. on new geometry of space. Philosophical Transactions of the Royal Society of London, pp. 725791, 1865. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural In Proceedings of the IEEE/CVF international radiance fields with thousands of tiny mlps. conference on computer vision, pp. 1433514345, 2021. Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srinivasan, Ben Mildenhall, Andreas Geiger, Jon Barron, and Peter Hedman. Merf: Memory-efficient radiance fields for real-time view synthesis in unbounded scenes. ACM Transactions on Graphics (TOG), 42(4):112, 2023. Robin Rombach, Patrick Esser, and Björn Ommer. Geometry-free view synthesis: Transformers and no 3d priors. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1435614366, 2021. Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Luˇcic, Daniel Duckworth, Alexey Dosovitskiy, et al. Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 62296238, 2022. Sudipta Sinha, Drew Steedly, and Rick Szeliski. Piecewise planar stereo for image-based rendering. In 2009 International Conference on Computer Vision, pp. 18811888, 2009. Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. Advances in Neural Information Processing Systems, 34:1931319325, 2021. Mohammed Suhail, Carlos Esteves, Leonid Sigal, and Ameesh Makadia. Generalizable patch-based neural rendering. In European Conference on Computer Vision, pp. 156174. Springer, 2022a. Mohammed Suhail, Carlos Esteves, Leonid Sigal, and Ameesh Makadia. Light field neural rendering, 2022b. URL https://arxiv.org/abs/2112.09687. Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 54595469, 2022. Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: arXiv preprint Large multi-view gaussian model for high-resolution 3d content creation. arXiv:2402.05054, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/ abs/1706.03762. Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, and Pratul P. Srinivasan. Ref-NeRF: Structured view-dependent appearance for neural radiance fields. CVPR, 2022. Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, and Kai Zhang. Pf-lrm: Pose-free large reconstruction model for joint pose and shape prediction. arXiv preprint arXiv:2311.12024, 2023. Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering, 2021a. URL https://arxiv.org/abs/2102.13090. Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. Nerf: Neural radiance fields without known camera parameters. arXiv preprint arXiv:2102.07064, 2021b. 14 Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. Meshlrm: Large reconstruction model for high-quality mesh. arXiv preprint arXiv:2404.12385, 2024. Desai Xie, Sai Bi, Zhixin Shu, Kai Zhang, Zexiang Xu, Yi Zhou, Sören Pirk, Arie Kaufman, Xin Sun, and Hao Tan. Lrm-zero: Training large reconstruction models with synthesized data. arXiv preprint arXiv:2406.09371, 2024. Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 54385448, 2022. Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, and Kai Zhang. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model, 2023. Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images, 2021. URL https://arxiv.org/abs/2012.02190. Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1944719456, 2024. Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting, 2024. URL https://arxiv. org/abs/2404.19702. Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, and Felix Heide. Differentiable point-based radiance fields for efficient view synthesis. In SIGGRAPH Asia 2022 Conference Papers, pp. 112, 2022. Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei Efros. View synthesis by appearance flow. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pp. 286301. Springer, 2016. Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. In SIGGRAPH, 2018."
        },
        {
            "title": "A APPENDIX",
            "content": "We include additional results, ablations, and model details. A.1 ADDITIONAL IMPLEMENTATION DETAILS We do not use bias terms in our model, for both Linear and LayerNorm layers. We initialize the model weights with normal distribution of zero-mean and standard deviation of 0.02/(2 (idx + 1)) 0.5, where idx means transform layer index. We train our model with AdamW optimizer (Kingma, 2014). The β1, and β2 are set to 0.9 and 0.95 respectively, following GS-LRM. We use weight decay of 0.05 on all parameters except the weights of LayerNorm layers. A.2 DISCUSSION ON ZERO-SHOT GENERALIZATION We notice that LVSM has the problem of generalizing (zero-shot) to input images with aspect ratios that are different from training data, e.g., training with 512 512 images and inferring on 512 960 input images. We observe high-quality novel view synthesis quality at the center of synthesized novel views but also observe blurred regions at the horizontal boundaries that exceed the training image aspect ratio. We conjecture the reason is that our model is trained with center-cropped images in our implementation. In detail, as the Plücker ray will have smaller density at the boundary of the long side of images, and our model is not trained with these data, it can not generalize well to them. A.3 ADDITIONAL VISUAL RESULTS We show the visualization of LVSM at the object level with 256 resolution in Fig. 7. Consistent with the findings of the experiment with 512 resolution  (Fig. 3)  , LVSM performs better than the baselines on texture details, specular material, and concave geometry. Figure 7: Object-level visual comparison at 256 resolution. Comparing with the two baselines: LGM(Tang et al., 2024) and GS-LRM (Res-256) (Zhang et al., 2024), both our Encoder-Decoder and Decoder-Only models have fewer floater artifacts (last example), and can generate more accurate view-dependent effects (third example). Our Decoder-Only model can better preserve the texture details (first two examples). 16 Table 4: Model size ablation studies on the object-level reconstruction. The following experiments are run with 8 GPUs. GSO (Downs et al., 2022) PSNR SSIM LPIPS Ours Decoder-Only (24 layers) Ours Decoder-Only (18 layers) Ours Decoder-Only (12 layers) Ours Decoder-Only (6 layers) 27.04 26.81 26.11 24.15 0.910 0.907 0.896 0.865 0.055 0.057 0.065 0.092 A.4 ADDITIONAL ABLATION RESULTS Additionally, we ablate our decoder-only LVSM on the object-level dataset. As shown in Table 4, decoder-only LVSM demonstrates better performance with more transformer layers, similar to our observation in Table 2. Interestingly, we find that decoder-only LVSM scales better, i.e., exhibits more performance gain with more transformer layers, compared with scene-level results in Table 2. We conjecture the reason is that the input views of the object-level data have larger-baseline cameras, in contrast with smaller-baseline cameras in the scene-level dataset RealEstate10K (Zhou et al., 2018). Thus, the novel view synthesis task on the scene-level RealEstate10K dataset is easier and the performance saturates beyond the certain number of transformer layers."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Cornell University",
        "Massachusetts Institute of Technology",
        "The University of Texas at Austin"
    ]
}