{
    "paper_title": "COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning",
    "authors": [
        "Xindi Wu",
        "Hee Seung Hwang",
        "Polina Kirichenko",
        "Olga Russakovsky"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) excel at simple vision-language tasks but struggle when faced with complex tasks that require multiple capabilities, such as simultaneously recognizing objects, counting them, and understanding their spatial relationships. This might be partially the result of the fact that Visual Instruction Tuning (VIT), a critical training step for MLLMs, has traditionally focused on scaling data volume, but not the compositional complexity of training examples. We propose COMPACT (COMPositional Atomic-to-complex visual Capability Tuning), which generates a training dataset explicitly controlling for the compositional complexity of the training examples. The data from COMPACT allows MLLMs to train on combinations of atomic capabilities to learn complex capabilities more efficiently. Across all benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT while using less than 10% of its data budget, and even outperforms it on several, especially those involving complex multi-capability tasks. For example, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0% improvement on MM-Vet compared to the full-scale VIT on particularly complex questions that require four or more atomic capabilities. COMPACT offers a scalable, data-efficient, visual compositional tuning recipe to improve on complex visual-language tasks."
        },
        {
            "title": "Start",
            "content": "COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning Xindi Wu1 Hee Seung Hwang1 Polina Kirichenko2 Olga Russakovsky1 1Princeton University 2Meta AI https://princetonvisualai.github.io/compact/ 5 2 0 2 0 3 ] . [ 1 0 5 8 1 2 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) excel at simple vision-language tasks but struggle when faced with complex tasks that require multiple capabilities, such as simultaneously recognizing objects, counting them, and understanding their spatial relationships. This might be partially the result of the fact that Visual Instruction Tuning (VIT), critical training step for MLLMs, has traditionally focused on scaling data volume, but not the compositional complexity of training examples. We propose COMPACT (COMPositional Atomic-to-complex Visual Capability Tuning), which generates training dataset explicitly controlling for the compositional complexity of the training examples. The data from COMPACT allows MLLMs to train on combinations of atomic capabilities to learn complex capabilities more efficiently. Across all benchmarks, COMPACT achieves comparable performance to the LLaVA-665K VIT while using less than 10% of its data budget, and even outperforms it on several, especially those involving complex multi-capability tasks. For example, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0% improvement on MM-Vet compared to the full-scale VIT on particularly complex questions that require four or more atomic capabilities. COMPACT offers scalable, data-efficient, visual compositional tuning recipe to improve on complex visual-language tasks. 1. Introduction Multimodal Large Language Models (MLLMs) have shown impressive progress in handling wide range of visionfrom early diagnoslanguage tasks [1, 2, 19]. Yet, tic works [30, 32] to recent state-of-the-art models like LLaVA [22, 23], Cambrian [37], and Eagle [21, 34], the compositionality challenge remains persistent. Consider the following question: Are there more blue squares or red circles on the image? model that is capable of recognizing Equal contribution Figure 1. Compositional Complexity Comparison. Comparison between visual instruction tuning data (LLaVA-665K [24] VIT) and our compositional tuning data (COMPACT). The VIT data is dominated by simple queries (ùëò = 1), while our COMPACT data is balanced across compositional complexity levels (ùëò = 1, 2, 3). shapes, colors and counting objects should be able to answer it correctly. Despite years of progress, state-of-the-art models still fail on such compositional questions, even though they can answer simpler ones correctly (e.g. What color is the square?). This has been long-standing issue and such failures suggest that current models do not systematically generalize to tasks with higher compositional complexity. To address this, recent efforts have primarily scaled up the amount of training data used for Visual Instruction Tuning (VIT) [21, 24, 25, 34, 37], an essential but dataand compute-heavy step for MLLM training. However, such datasets (e.g. LLaVA-665K [24]) are dominated by simple queries that require only one capability, lacking sufficient compositional complexity  (Fig. 1)  . Even with large-scale instruction tuning, recent studies show that models still struggle with integrating capabilities and generalizing to complex visual tasks due to limitations in the compositional complexity of their training data [29, 40]. Thus, we ask: Can we bridge the performance gap between different complexity regimes without simply increasing the data volume? Instead of treating compositionality as byproduct of scale, we encourage compositional capabilities in MLLMs with hierarchically structured compositional training data. In this work, we introduce COMPACT (COMPositional 1 Atomic-to-complex Visual Capability Tuning), data recipe that scales capabilities of MLLMs from atomic (ùëò = 1) to composite (ùëò > 1) complexity levels. We define set of 10 atomic capabilities and then combine them to generate compositional training dataset that can promote models internalization of the compositional structures of complex tasks in compute-efficient manner. We summarize our key contributions: We introduce COMPACT, visual compositional tuning data recipe that builds complex capabilities from simple atomic capabilities. By systematically combining 10 atomic capabilities to control the complexity of training samples, COMPACT addresses key limitation of conventional VIT methods that rely on incidental capability composition through data scaling. We develop structured data recipe that enforces balanced distribution across different levels of compositional complexity (ùëò = 1, 2, 3) to cover wider range of task regimes. This approach flattens the complexity cliff in conventional VIT datasets [24], where 90.1% of the questions require two or fewer capabilities. We demonstrate the impressive effectiveness of COMPACT. With only 10% of the size of the VIT dataset (5% of the LLaVA-665K [24] VIT data augmented with 32K samples of our compositional tuning data), training with COMPACT matches the performance of full-scale VIT (100.18% relative score). Further, it demonstrates exceptional generalization to higher-complexity tasks, improving the score from 35.3 to 64.7 on MMStar [6] and from 32.5 to 57.5 on MMVet [43] for ùëò = 4 tasks. 2. Related Work Instruction following is an Visual Instruction Tuning. essential capability in language models [38, 46]. Misalignment between models response and the format requested by question can hinder the precise evaluation of its performance and capabilities [3, 11, 12, 31]. In order to adapt MLLMs to respond appropriately to diverse question formats (e.g., multiple-choice, shortand long-response questions), Visual instruction tuning [22, 24] has been proposed. VIT involves training model on fixed set of instruction patterns that can be repeated during inference. Although VIT has shown performance improvements in general multimodal capabilities [14], recent work [10] has shown that optimizing for response formatting potentially limits the quality of language model responses. While VIT [24] focuses on learning simple capabilities via instruction following, our data recipe explicitly models compositional capabilities in the training data. Our approach directly addresses the lack of exposure to compositional questions during training, enabling models to improve on tasks that are more complex in capability space. COMPACT leverages the learning potential of both compositional tuning and instruction tuning data to create more optimal data recipe than conventional VIT. Compositionality in LLMs and MLLMs. Semantically, compositionality is the claim that the meaning of complex statement is result of the combination of its constituents. [8]. In the context of visual capabilities of MLLMs, compositional capability refers to models ability to perform complex tasks by combining multiple capabilities [13], where each capability is related to understanding basic visual concepts such as objects, attributes, relationships. Recent work has shown that compositional capability can be trained in LLMs [42, 45], but generalizations to the realms of MLLMs have been largely incomplete. Some studies highlight that while MLLMs do show signs of compositional capability [28], they struggle when constituting components and their combined patterns are not strongly learned or missing during training [4]. Furthermore, previous works have focused on limited domains such as geometry [5], visual recognition, and language [7], or employed relaxed definition of compositionality as sequential array of tasks [20] rather than integrating them. Studies show that general visual capability requires strong In order to train MLLMs to compositional ability [44]. learn complex capabilities, it is necessary to explicitly model compositionality in the training data. Our approach takes advantage of these findings to create data recipe for training complex capabilities across visual domains. Data Efficiency in MLLMs. VIT is data and computeheavy step in training [41]. Studies have found that the performance of MLLMs can be reproduced with less data and better techniques, suggesting that the amount of data needed for VIT can be reduced. For example, recent works have developed effective VIT data recipes by leveraging data selection methods and curating higher-quality training datasets [16, 26]. ICONS [39] shows that models can achieve near-perfect performance across suite of MLLM benchmarks with fraction of the original VIT dataset. On the other hand, some studies proposed an alternative approach of scaling up to improve visual capabilities even further [18]. However, these approaches treat compositionality as byproduct of scale rather than as learnable capability. Our COMPACT formalizes atomic capabilities and systematically incorporates their combinations into the training dataset to efficiently address the limitations in generalization to complex compositional tasks. By redistributing the compositional complexity of the training data, we scale the models exposure to complex tasks without scaling the data. 2 Figure 2. COMPACTs Data Generation Pipeline. (Left): We sample ùëò {1, 2, 3} atomic capabilities such as color, object recognition, and spatial relationship. (Right): We verify the quality of generated conversations and combine them with instruction tuning data to maintain instruction following capability. This structured data recipe explicitly models atomic-to-complex learning procedure, in contrast to standard LLaVA-665K [24] VIT that promotes learning from simple queries. (Center): We generate questions that integrate all ùëò sampled capabilities. 3. Method We propose COMPACT  (Fig. 2)  , data recipe that scales capabilities of MLLMs from atomic (ùëò = 1) to composite (ùëò > 1) complexity levels. We first introduce the concept of atomic visual capabilities (3.1), which serve as the foundational building blocks of complex visual tasks. We then detail COMPACTs four-step data recipe (3.2) for generating high-quality compositional training data that allows models to integrate multiple visual capabilities. 3.1. Atomic Visual Capabilities Atomic capabilities are foundational skills that can be combined to solve complex tasks. For example, model needs to acquire object recognition, color attribution, and spatial relationship understanding capabilities to identify how objects of different colors are spatially oriented. For each task ùëá, we identify set of atomic visual capabilities {ùëê1, . . . ùëêùëò } required to solve this task. We define the number of atomic capabilities required to solve the task ùëá as its compositional complexity ùëò. Our goal in COMPACT is to increase the average compositional complexity of the dataset and balance its distribution among the samples so that each question explicitly requires the model to combine multiple atomic capabilities during training. We build taxonomy of atomic capabilities from the existing literature on MLLMs and their general visual reasoning skills [15, 40]. Extremely low-frequency and nonperceptual capabilities (e.g. cultural knowledge, historical context, and math) are removed, resulting in 10 fine-grained atomic capabilities (Tab. 1) that focus on visual understanding. We categorize these atomic capabilities into three major categories: 1) Attribution: identifying visual properties of objects (e.g., color and shape). 2) Recognition: detecting and interpreting visual entities, including objects, actions, text, spatial recognition, and counts. 3) Relation: capturing how entities interact or relate to one another from an abstract or physical perspective, either through spatial relationship, object interaction, or scene understanding. 3.2. Visual Compositional Tuning Data Recipe In our proposed approach COMPACT, we generate multicapability questions Dcomp by prompting vision-language models to create questions that require natural1 integration of exactly ùëò atomic visual capabilities. This process involves four key steps: (1) randomly selecting ùëò capabilities from our predefined set of atomic visual capabilities (Capability Sampling), (2) prompting Gemini-2.0-Flash [36] to generate questions that naturally integrate all ùëò selected capabilities (Conversation Generation), (3) validating the capability requirement and the quality of each question through an independent verification step (Quality Verification), and (4) combining our generated compositional tuning data with small portion of the LLaVA-665K [24] VIT data to assemble the final dataset (Dataset Assembly). Step 1: Capability Sampling. We start by taking random sample of images from LLaVA-665K [24]. For each image, we repeatedly sample ùëò {1, 2, 3} capabilities from our predefined pool of 10 atomic visual capabilities. In order to include diverse combinations of atomic capabilities in 1We use the term natural to denote combination of visual capabilities that correspond to their co-occurrence patterns in real-world settings, wherein multiple capabilities are integrated in way that is contextually and semantically meaningful. 3 Table 1. Taxonomy of Atomic Capabilities. We identify 10 atomic capabilities and categorize them into three groups: Attribution, Recognition, and Relation. Atomic capabilities serve as building blocks for compositional instruction tuning. For each capability, we provide the definition and question example that requires the capability to answer. Group Capability Definition Attribution Color Shape Identifying or comparing colors of objects in the image Recognizing and describing the shapes of objects in the image Example Question What color is the car? What shape is the dining table? Recognition Object Recognition Action Recognition Text Recognition Spatial Recognition Counting Identifying and naming objects present in the image Identifying what action is being performed Reading and interpreting text visible in the image Understanding the overall spatial layout and arrangement of the entire scene How is the furniture arranged in this room? Determining the number of instances of something in the image What object is on the table? What is the person doing in this image? What word is written on the sign? How many people are in the room? Relation Spatial Relationship Object Interaction Scene Understanding Identifying how specific objects are positioned relative to each other Analyzing how multiple objects interact with each other Identifying the type of environment/setting What is next to the red car? How is the woman interacting with the laptop? Where is this scene taking place? our training dataset, we do the following in each round of capability sampling: (a) prioritize the capabilities that have not been selected for that image, and (b) drop duplicate combinations of capabilities for the same image. These efforts ensure that our training examples efficiently capture the visual information in the images. For each capabilStep 2: Conversation Generation. ity combination that is sampled, we prompt Gemini-2.0Flash [36] to generate conversational question-answer pair that integrates all capabilities in the combination, as well as score between 0 and 100 that represents its confidence in the quality of the conversation. Our carefully designed prompt (see Appendix B) enforces several key constraints: (a) questions must require the use of visual information from the image and cannot be answered from its text alone, (b) answers must be concise, (c) questions must integrate exactly the specified capabilities naturally (without using conjunctions to simply conjoin single-capability questions), and (d) questions must reference objects and features actually present in the image. The purpose of these constraints is to produce vision-centric conversations that are unambiguous and natural. The format of the generated output adheres to JSON template that explicitly tags the required capabilities for each question. Step 3: Quality Verification. We include verification process with Gemini-2.0-Flash [36] to ensure the quality and diversity of the training dataset. We filter out questions with uninformative answers (e.g., unknown, not visible) or those with confidence scores below 70%. We discard questions that share more than 60% of the words with those previously accepted. Additionally, we reject visually ungrounded questions whose answers can be inferred from the question alone. Then, we perform capability verification by prompting Gemini-2.0-Flash [36] to analyze whether each question indeed requires exactly the ùëò specified capabilities. Questions that require unspecified capabilities or do not utilize all ùëò capabilities are rejected. The generation and verification processes in steps 2 and 3 repeat iteratively until we collect 2-3 high-quality conversations per ùëò for each image or reach maximum of 10 verification attempts. Only images with at least two verified questions are included in the final dataset. Step 4: Dataset Assembly. The final training dataset combines two components: (1) random 5% subset of the LLaVA-665K [24] VIT dataset, and (2) our COMPACTgenerated compositional tuning data. The compositional tuning data consists of compositional multiturn conversations and their corresponding images randomly sampled from the VIT dataset. This mixture of instruction tuning and compositional tuning data serves dual purpose. First, the VIT subset maintains the models ability to handle diverse response formats and instructions required by modern MLLM benchmarks (e.g., multiple-choice questions [9], open-ended answers [24]). Second, our compositional data trains the models capability to reason about multiple visual aspects within single complex question. In this way, we delegate the instruction following capability training to the original VIT data and allow our compositional tuning data to focus on developing the models compositional capabilities. COMPACT preserves the contents of the images sampled from LLaVA-665K [24] when generating new multi-turn conversations. This enables us to fairly compare COMPACT and existing methods in their ability to extract rich and structured information from the controlled set of images. We further adjust the ratio of the VIT subset to our compositional tuning data and study how it affects performance (4.4). Our findings show the optimal balance between the preservation of instruction following capability and the training of compositional capabilities. 4. Experiments In this section, we evaluate the baseline approaches and COMPACT on existing multimodal benchmarks. First, we discuss our evaluation setup and the benchmarks (4.1). Second, we compare the performance of COMPACT with relevant baselines, including LLaVA-665K [24] VIT (4.2). Recipe LLaVA-665K [24] Random ICONS [39] COMPACT (ours) # Data 665K 65K 65K 65K InfoVQA [27] SeedBench2Plus [17] MME [9] TextVQA [35] MM-Vet [43] CV-Bench [37] MMStar [6] LLaVA-W [24] Rel. (%) 20. 20.05 21.0 23.68 41.72 41.85 42.03 43.13 1478.48 1327.70 1402.75 1379.94 46. 42.88 43.12 44.37 29.22 30.46 31.23 31.74 60.92 54.71 55.96 55.28 35. 34.13 35.96 36.13 68.50 64.30 61.8 64.50 100.00 95.38 97.47 100.18 Table 2. Baseline Comparisons. Performance comparison of COMPACT with baselines. With only 5% of the LLaVA-665K [24] VIT data and 32K of our compositional tuning data (65K total), COMPACT outperforms the random subset of the VIT data (Random), gradient-based approach selected subset of the VIT data (ICONS [39]), and even the full VIT data on diverse multimodal benchmarks. The best and second best results for each benchmark are shown in bold and underlined, respectively. COMPACT integrates atomic capabilities into tasks of higher compositional complexity, enabling models to generalize and handle complex tasks without explicit decomposition. Third, we analyze COMPACT trained models generalization to various levels of compositional complexity and investigate how the distribution of ùëò in the training dataset impacts performance (4.3). Finally, we conduct ablation studies to understand different aspects of COMPACT design, such as the distribution of compositional complexities, coverage of atomic capabilities, the range of compositional complexities, and the ratio between instruction and compositional tuning data (4.4). 4.1. Evaluation Testbed Model. We train LLaVA-v1.5-7B-LoRA [24] models previsual-instruction-tuning checkpoint2 on our COMPACT training dataset. This checkpoint has not been exposed to any visual instruction tuning data prior to COMPACT training. The training dataset includes 32K-sample compositional tuning data unless otherwise stated. Additionally, we mix 5% of LLaVA-665K [24] to preserve instruction following capability. We train the model for one epoch with its official LLaVA-v1.5 LoRA fine-tuning settings. Baselines. We compare the effectiveness of our COMPACT data recipe with several baseline datasets by training models with the same architecture under identical training configurations. LLaVA-665K: The full LLaVA-665K [24] VIT dataset (665K samples) used in LLaVA-v1.5. This serves as our primary performance baseline. Random: 65K-sample random subset of LLaVA-665K [24] that matches our training data size. This baseline controls for data volume. ICONS [39]: 65K-sample subset of LLaVA-665K [24] selected using the ICONS method, which is gradient-driven influence-consensus based data selection method that selects the most informative samples for data-efficient visual instruction tuning. Benchmarks. We evaluate models trained with different data recipes on established multimodal benchmarks that assess complex visual capabilities. 1) MM-Vet [43] includes 16 types of complex multimodal tasks integrated from 6 core 2LLaVA-v1.5-mlp2x-336px-pretrain-vicuna-7b-v1.5, which has no prior exposure to visual instruction tuning data. capabilities (recognition, OCR, knowledge, language generation, spatial awareness, and math). 2) MME [9] contains 10 perception (e.g., color, count, OCR) and 4 cognition (e.g., commonsense reasoning, text translation, code understanding) related visual subtasks. 3) LLaVA-in-the-Wild [24] is an open-ended visual question answering benchmark that asks complex questions on real-world images. 4) SeedBench2Plus [17] evaluates visual comprehension skills of MLLMs with focus on charts, maps, and webs. 5) MMStar [6] contains 1,500 visual questions that span 6 core capabilities (fine-grained perception, coarse perception, mathematics, science & technology, logical reasoning and instance reasoning), carefully curated to evaluate multimodal understanding. 6) CV-Bench [37] is MLLM benchmark specialized for 2D and 3D visual understanding that includes spatial relationship, object count, relative distance, and depth order. 7) TextVQA [35] evaluates visual understanding of texts in the image. 8) InfoVQA [27] measures visual understanding of infographic images. These benchmarks cover broad range of vision-centric capabilities. We also note that some of these benchmarks include non-visual questions involving skills as knowledge and math, which are not our primary focus. We provide more detailed discussion of model performance in these knowledge-intensive and math-intensive tasks in Appendix A. 4.2. Main Results Overall Performance. As shown in Tab. 2, COMPACT performs on par with the LLaVA-665K [24] baseline. COMPACTs training dataset, which amounts to just 10% of the full VIT data, contains mixture of 32K generated compositional tuning data and 5% of LLaVA-665K [24] VIT data (33K). The compositional tuning data trains the model on compositional capabilities, and the VIT subset maintains the models instruction-following capability. COMPACT outperforms both the random [24] and ICONS [39] baselines on most benchmarks, demonstrating superior performance on multi-capability tasks. COMPACT shows consistent improvements on different benchmarks, achieving strong gains on tasks like MM-Vet [43] (+8.6% over LLaVA-665K [24]), MMFigure 3. Performance Across Compositional Tuning Data Scales. We fix the VIT subset (5% of LLaVA-665K [24]) and scale the compositional tuning data in COMPACT from 2K to 32K. For comparison, we remove the compositional tuning data and add more VIT data (2K-32K) instead to prepare VIT only recipes with equal data budgets. COMPACT (solid lines) consistently outperforms LLaVA-665K [24] VIT (dashed lines) with fewer data. The performance gap is pronounced for complex reasoning benchmarks such as MM-Vet and MMStar, where the 8K COMPACT model often exceeds the LLaVA-665K [24] VIT baseline at 32K. This demonstrates the data efficiency of COMPACT, requiring substantially less data than LLaVA-665K [24] VIT to achieve comparable or better results. Star [6] (+2.9%), InfoVQA [27] (+13.8%), and SeedBench2Plus [17] (+3.4%) while maintaining competitive performance on TextVQA [35] and LLaVA-in-theWild [24]. Across all benchmarks, our COMPACT achieves an average relative performance of 100.18%, outperforming even the full LLaVA-665K [24]. In comparison, the random baseline achieves 95.38%, and the ICONS [39] baseline 97.47%, highlighting the effectiveness of our compositional data generation strategy. Additionally, we provide qualitative results in Appendix C. Visual Compositional Tuning is Data-Efficient. We study the data efficiency of COMPACT by analyzing how its performance changes as we scale the amount of compositional tuning data. We fix the VIT subset (5% of LLaVA665K [24]) and scale the compositional tuning data in COMPACT from 2K to 32K. As comparison, we remove the compositional tuning data and add more VIT data (2K-32K) instead to match the dataset size. Fig. 3 shows that as the number of compositional tuning samples increases, COMPACT performance trends upward across all benchmarks while the random baseline shows mixed behavior as the size of the dataset increases. Furthermore, across all dataset sizes, COMPACT performs consistently better than the random baseline, and the gap increases as the size of the dataset grows. Models trained on smaller compositional tuning data (2K-8K samples) often match or exceed the performance of random baseline models trained on much larger data. For instance, COMPACTs 2K model achieves 30.73 on MMVet [43], outperforming the random baselines 32K model at 30.46 which has eight times more data. This demonstrates that COMPACT makes more effective use of training data compared to the baselines. We hypothesize that this improvement in data efficiency comes from two factors: (1) Balanced Compositional Complexity: COMPACT continuously provides learning signals of higher compositional complexity by balancing the distribution of ùëò in the training dataset. In contrast, the LLaVA665K [24] VIT paradigm trains almost exclusively on ùëò = 1 tasks (e.g., single-capability queries). Models trained on the VIT data receive signals of higher compositional complexity less frequently, leaving them unprepared for compositional generalization, the ability to integrate combinations of capabilities not explicitly seen during training (more analysis in 4.3). (2) Diverse Capability Compositions: COMPACT sustains the learning potential during training by explicitly introducing diverse (ùëò >= 1) integrations of atomic capabilities. Meanwhile, the LLaVA-665K [24] VIT relies heavily on simpler tasks (ùëò = 1) that can be easily memorized and templatized for rapid saturation of learning potential. 4.3. Analysis Performance Gains on Complex Compositional Questions. COMPACTs notable performance improvements on complex compositional questions demonstrate its potential for strong compositional generalization. As shown in Fig. 4, COMPACT achieves competitive performance on the MMVet [43] and MMStar [6] benchmarks across various levels of compositional complexity (ùëò). Despite not being explicitly trained on ùëò > 3 data, our model effectively generalizes to even higher ùëò regimes. For MM-Vet [43], the scores are 57.5 (COMPACT) vs 32.5 (LLaVA-665K [24]) when ùëò = 4, and 20.0 (COMPACT) vs 0.0 (LLaVA-665K [24]) when ùëò = 5. For MMStar [6], the scores are 64.7 (COMPACT) vs 35.3 (LLaVA-665K [24]) when ùëò = 4. This shows that COMPACT performs robustly in scenarios with higher compositional complexity. Distribution of Visual Capabilities. We use Gemini2.0-Flash [36] to analyze each question and identify the atomic capabilities required to give an answer (see the details of the system prompt in Appendix B). Fig. 10 shows the approximate distribution of the number of capabilities required per question in the LLaVA-665K [24] VIT dataset. 6 Figure 4. Compositional Generalization to Higher-Complexities. Performance comparison across compositional complexities (ùëò). COMPACT shows competitive performance against LLaVA-665K [24] VIT training. It exceeds the LLaVA-665K [24] baseline at higher compositional complexity tasks (ùëò = 4 and ùëò = 5) while using significantly less training data. The ùëò-distribution rows show the distribution of compositional complexities in each benchmark. Example Questions with Different Compositional Complexities MM-Vet (ùëò = 3): Q: What is the color of the hat worn by the person in the front left? Required capabilities: color attribution, object recognition, spatial relationship MMStar (ùëò = 4): Q: What is the position of the red rug in the living room? Required capabilities: color attribution, object recognition, spatial relationship, scene understanding MMStar (ùëò = 5): Q: Is the number of metal cars that are left of the tiny matte school bus greater than the number of tiny cyan double bus? Required capabilities: spatial relationship, object recognition, counting, color attribution, shape attribution We sampled 5,668 questions that belong to 1,000 random data points in the VIT dataset, and analyzed their compositional complexity using Gemini-2.0-Flash [36]. The mean compositional complexity of the questions is approximately ùëò = 1.5, and the mode is ùëò = 1. 59.2% of the questions utilize only one capability, and an additional 30.9% use 2 capabilities. Together, about 90% of the questions require 2 or less visual atomic capabilities. This complexity cliff in the LLaVA-665K [24] VIT dataset characterized by the scarcity of higher ùëò questions leads to steep declines in its downstream performance on higher ùëò tasks.  (Fig. 4)  . The performance gap between COMPACT and the VIT for different ùëò values shows that the VITs complexity-agnostic training leaves models unprepared for tasks that require compositional generalization. Interestingly, small fraction of the questions (0.2%) require as many as 10 capabilities (e.g., Question: Describe this photo in detail.). We also observe that 1.1% of the questions require zero capabilities, as illustrated in Fig. 10. We further provide ùëò = 0 examples in Appendix C. Fig. 5 7 Figure 5. Comparison of Capability Distribution. The heatmaps show the frequency of each atomic capability in LLaVA (left) and COMPACT (right) samples. The capabilities are sorted by frequency based on the LLaVA capability distribution, with more common capabilities appearing closer to the top. In LLaVA, the distribution is notably imbalanced: object recognition and scene understanding are some of the most frequent, while shape and spatial recognition are less prevalent. In contrast, our COMPACT exhibits more balanced distribution across all capability categories. shows the relative frequencies of atomic capabilities in the question samples. Object recognition (38.97%) and scene understanding (28.58%) are the most common. Other notable capabilities include spatial relationship (25.14%), text recognition (24.68%), and color attribution (14.40%). Less frequent capabilities include object interaction (6.55%), action recognition (6.05%), counting (2.95%), shape attribution (1.13%), and spatial recognition (1.06%). 4.4. Ablation Studies We conduct series of ablation studies to investigate key design considerations (compositional complexity distribution, atomic capability coverage, compositional complexity range, and instruction tuning ratio) in COMPACT. Unless otherwise specified, all experiments use 5% of LLaVA665K [24] VIT data and 16K ùëò = 1, 2, 3 compositional tuning data. Effect of Matching LLaVA-665K Distribution."
        },
        {
            "title": "In order",
            "content": "Recipe LLaVA-665K [24] Random Unbalanced COMPACT COMPACT #Data 665K 49K 49K 49K InfoVQA [27] SeedBench2Plus [17] MME [9] TextVQA [35] MMVet [43] CV-Bench [37] MMStar [6] LLaVA-W [24] 20. 20.33 22.28 22.68 41.72 42.38 41.17 42.82 1478.48 1290.45 1339.24 1362.68 46. 42.22 43.08 43.73 29.22 30.18 29.22 30.78 60.92 54.75 55.84 54.69 35. 34.3 34.8 35.59 68.50 70.5 64.5 66.6 Rel. (%) 100.00 96.28 96.62 98. Table 3. Matching LLaVA-665K Distribution. Performance comparison of unbalanced COMPACT and multiple baselines. The distribution of compositional complexity in unbalanced COMPACT follows LLaVA-665K [24]. Training model on unbalanced COMPACT leads to performance on par with training on the random baseline which is subset of LLaVA-665K [24] equal in size, suggesting that balanced distribution of ùëò in compositional tuning data is critical in compositional generalization. Figure 6. Leave-One-Out Analysis on Atomic Capabilities. We measure the average performance degradation across benchmarks by excluding an atomic capability from training. Higher drop indicates higher importance of the atomic capability. Excluding scene understanding and spatial relationships have the largest impact, while that of excluding shape and action recognition are modest. to show that the performance improvement of COMPACT mainly comes from the balanced distribution of compositional complexity in the compositional tuning data, we analyze the impact on performance when its compositional complexity is unbalanced. We further generate 16K-sample compositional tuning data whose distribution of ùëò resembles that of LLaVA-665K [24], which is heavily skewed as in Fig. 10. This gives us 58,168 ùëò = 1, 30,364 ùëò = 2, and 7,468 ùëò = 3 conversations. Similar to the original COMPACT data recipe, we mix the unbalanced 16K compositional tuning data with the random 5% subset of the VIT data. We compare this unbalanced COMPACT training dataset with the following baselines: 1) same size random subset of the VIT data, 2) the original COMPACT with 16K balanced compositional tuning data, and 3) the full VIT dataset. As shown in Tab. 3, the performance of unbalanced COMPACT stands at 96.62% (in relation to the full baseline), close to the random baseline at 96.28%. However, the performance of original COMPACT jumps to 98.83%, suggesting that most of the performance gain in COMPACT comes from the fair representation of higher ùëò samples in the compositional tuning data. Impact of Atomic Capability Coverage. To validate our choice of atomic capabilities and understand their relative 8 Figure 7. Compositional Complexity Analysis: Performance comparison of models trained with different compositional complexities. ùëò = 1 refers to only one atomic capability per question, ùëò = 1, 2 to both single and dual capabilities, and ùëò = 1, 2, 3 to single, dual, and triple capabilities. Results show consistent improvements as the range of compositional complexities increases. importance, we conduct leave-one-out analysis by systematically excluding questions that require specific capability while keeping the total number of training examples fixed. As shown in Fig. 6, scene understanding and spatial relationship emerge as the most critical capabilities, with each of their exclusion leading to significant performance drop (5.22% and 4.93% respectively). Text recognition and object recognition are also essential (4.65% and 4.03% drops). The exclusion of capabilities like shape attribution and action recognition have smaller impact (0.74% and 2. 08%). This analysis validates our selection of atomic capabilities by demonstrating that each capability contributes meaningfully to overall performance without being redundant. Effect of Compositional Complexity Range. To isolate the effect of the range of compositional complexities while controlling for data quality, we generate three sets of 16Ksample compositional tuning data, each with ùëò = 1, ùëò = 1, 2 or ùëò = 1, 2, 3, using identical Gemini-2.0-Flash [36] configurations. For fair comparison, we maintain consistent sample counts and use an identical set of images in all three settings. The model trained on only ùëò = 1 (single capability per question) underperforms the model trained on ùëò = 1, 2, 3 compositional tuning data on multi-capability benchmarks: MM-Vet [43] (28.82 vs. 29.22), LLaVA-W [24] (66.1 vs. 68.5) and MMStar [6] (34.53 vs. 35.11). This shows that although the model trained on ùëò = 1 data can solve tasks with lower compositional complexity, it struggles to perform in higher compositional complexities. As shown in Fig. 7, increasing the range of compositional complexities leads to consistent improvements on all three benchmarks. Training on ùëò = 1, 2, 3 compositional tuning data achieves the highest performance on MM-Vet [43] (32.61) and MMStar [6] (0.3577), demonstrating that exposure to more complex compositional patterns during training enhances the models ability to handle complex multicapability tasks. Surprisingly, the model achieves 112% performance on MM-Vet [43] with only 16k compositional tuning data compared to the LLaVA-665K [24] baseline, suggesting that balanced mixture of different compositional complexities improves data efficiency. Impact of Instruction Tuning Data Ratio. We vary the amount of instruction tuning data sampled from the LLaVA665K [24] VIT data to understand the impact of the mixing ratio on model performance. In order to isolate the effect on visual instruction following, we exclude ùëò = 0 conversations (approximately 1.1% of the questions), which have minimal relevance to visual capabilities. As we scale the VIT subset from 0% (pure compositional tuning) to 7% of LLaVA-665K [24], we observe an upward trend in performance, indicating that the role of the instruction tuning data is crucial. Fig. 8 shows that without instruction tuning data (0%), COMPACT achieves only 74.69% of the performance relative to LLaVA-665K [24] VIT. Increasing the instruction tuning data to just 1% of the VIT data significantly improves the relative performance to 96.56%. However, further scaling gives diminishing returns, with 3% reaching 98.77% and 5% achieving nearly identical relative performance (99.99%). Interestingly, taking 7% from the VIT data causes slight decrease to 98.07%, indicating that 5% represents an optimal balance between instruction tuning and compositional tuning data in terms of data efficiency and performance. These results suggest that instruction following capability is potentially orthogonal to the capabilities of the base model and the atomic visual capabilities, and can be acquired with minimal instruction tuning data. 5. Discussion Conclusion. In this work, we introduce COMPACT, data recipe that systematically combines atomic visual capabilities (e.g., object recognition, spatial reasoning, shape attribution) into composite capabilities to solve complex multimodal tasks. Our experimental results show that explicit training on compositions of atomic capabilities matches the full LLaVA-665K [24] VIT in performance across benchmarks with less than 10% of its data budget. Our work presents the potential of structured compositional learning as scalable, data-efficient pathway toward multimodal models that can solve complex, multi-capability tasks via compositional generalization. Figure 8. Impact of Instruction Tuning Data Ratio on Performance. Relative performance of models trained on COMPACT mixed with different ratios instruction tuning data from LLaVA665K [24]. The x-axis is the percentage of LLaVA-665K [24] used as instruction tuning data, and the y-axis is the average relative score across benchmarks. The performance improves significantly with small percentage of instruction tuning data and stabilizes around 5%. Limitations. Our approach faces two key limitations. First, we rely on data generated from closed-source models (i.e., Gemini), which potentially introduce their compositional limitations and biases to our dataset. Additionally, this data generation process is costly, which could pose challenges for reproducibility. To support future research, we will publicly release the data generated in this project. Second, our approach focuses on the compositionality of vision-centric capabilities. Therefore, our approach may not be optimal for addressing knowledge-intensive tasks that lie outside the scope of visual reasoning. See Appendix for detailed discussion on knowledge-intensive task results. Future Work. We aim to extend COMPACT to accommodate higher-order compositional complexity (k > 3). Currently, our data recipe only generates data up to = 3 due to the decreasing reliability of closed-source models at higher compositional complexities. Specifically, as the number of atomic capabilities increases, their integration tends to be more inconsistent, ambiguous, or erroneous. Future work could explore hierarchical composition approaches or hybrid data generation pipelines that combine multiple sources and verification steps to improve performance on higher compositional complexities. Additionally, experimenting with explicit reasoning approaches (e.g., step-by-step decomposition [33]) could further improve the models ability to solve complex tasks while retaining data efficiency. Acknowledgments. This material is based upon work supported by the National Science Foundation under Grant 2107048 and 2112562. Any opinions, findings, and conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. All experiments, data collection, and processing activities were conducted at Princeton University. Meta was involved solely in an advisory role and no experiments, data collection or processing activities were conducted on Meta infrastructure. We thank Allison Chen for helpful discussions and feedback."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 1 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization. Text Reading, and Beyond, 2, 2023. 1 [3] Nishant Balepur, Rachel Rudinger, and Jordan Lee BoydGraber. Which of these best describes multiple choice evaluation with llms? a) forced b) flawed c) fixable d) all of the above. arXiv preprint arXiv:2502.14127, 2025. 2 [4] Declan Campbell, Sunayana Rane, Tyler Giallanza, Camillo Nicol`o De Sabbata, Kia Ghods, Amogh Joshi, Alexander Ku, Steven Frankland, Tom Griffiths, Jonathan Cohen, et al. Understanding the limits of vision language models through the lens of the binding problem. Advances in Neural Information Processing Systems, 37:113436113460, 2025. 2 [5] Hyunsik Chae, Seungwoo Yoon, Chloe Yewon Chun, Gyehun Go, Yongin Cho, Gyeongmin Lee, and Ernest Ryu. Decomposing complex visual comprehension into atomic visual skills for vision language models. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24. 2 [6] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. 2, 5, 6, 8, 9, [7] Mingyang Chen, Haoze Sun, Tianpeng Li, Fan Yang, Hao Liang, Keer Lu, Bin Cui, Wentao Zhang, Zenan Zhou, and Weipeng Chen. Facilitating multi-turn function calling for llms via compositional instruction tuning. arXiv preprint arXiv:2410.12952, 2024. 2 [8] Jerry A. Fodor and Ernest LePore, editors. The Compositionality Papers. Oxford University Press, 2002. 2 [9] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. corr abs/2306.13394 (2023), 2023. 4, 5, 8, 1 [10] Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Deepali Aneja, Zeyu Jin, Ramani Duraiswami, Dinesh Manocha, et al. closer look at the limitations of instruction tuning. arXiv preprint arXiv:2402.05119, 2024. 2 [11] Jia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin Wang, and Sadid Hasan. Does prompt formatting have any impact on llm performance? arXiv preprint arXiv:2411.10541, 2024. [12] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable 10 benchmarks for vision-language compositionality. Advances in neural information processing systems, 36:3109631116, 2023. 2 [13] Hang Hua, Yunlong Tang, Ziyun Zeng, Liangliang Cao, Zhengyuan Yang, Hangfeng He, Chenliang Xu, and Jiebo Luo. Mmcomposition: Revisiting the compositionality of pre-trained vision-language models. arXiv preprint arXiv:2410.09733, 2024. 2 [14] Jiaxing Huang, Jingyi Zhang, Kai Jiang, Han Qiu, and Shijian Lu. Visual instruction tuning towards general-purpose multimodal model: survey. arXiv preprint arXiv:2312.16602, 2023. 2 [15] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for openworld compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. [16] Jaewoo Lee, Boyang Li, and Sung Ju Hwang. Concept-skill transferability-based data selection for large vision-language models. arXiv preprint arXiv:2406.10995, 2024. 2 [17] Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024. 5, 6, 8, 1 [18] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2 [19] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. 1 [20] Ming Li, Pei Chen, Chenguang Wang, Hongyu Zhao, Yijun Liang, Yupeng Hou, Fuxiao Liu, and Tianyi Zhou. Mosaic-it: Free compositional data augmentation improves instruction tuning. arXiv preprint arXiv:2405.13326, 2024. 2 [21] Zhiqi Li, Guo Chen, Shilong Liu, Shihao Wang, Vibashan VS, Yishen Ji, Shiyi Lan, Hao Zhang, Yilin Zhao, Subhashree Radhakrishnan, et al. Eagle 2: Building post-training data strategies from scratch for frontier vision-language models. arXiv preprint arXiv:2501.14818, 2025. [22] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 1, 2 [23] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 1 [24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 1, 2, 3, 4, 5, 6, 7, 8, 9 [25] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 1 [38] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. 2 [39] Xindi Wu, Mengzhou Xia, Rulin Shao, Zhiwei Deng, Influence Pang Wei Koh, and Olga Russakovsky. consensus for vision-language data selection. arXiv preprint arXiv:2501.00654, 2024. 2, 5, 6 Icons: [40] Xindi Wu, Dingli Yu, Yangsibo Huang, Olga Russakovsky, and Sanjeev Arora. Conceptmix: compositional image generation benchmark with controllable difficulty. arXiv preprint arXiv:2408.14339, 2024. 1, 3 [41] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. Parameter-efficient fine-tuning methods for pretrained language models: critical review and assessment. arXiv preprint arXiv:2312.12148, 2023. 2 [42] Zhuoyan Xu, Zhenmei Shi, and Yingyu Liang. Do large an invesarXiv preprint language models have compositional ability? tigation into limitations and scalability. arXiv:2407.15720, 2024. 2 [43] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 2, 5, 6, 8, 9, 1 [44] Aimen Zerroug, Mohit Vaishnav, Julien Colin, Sebastian Musslick, and Thomas Serre. benchmark for compositional visual reasoning. Advances in neural information processing systems, 35:2977629788, 2022. [45] Haoyu Zhao, Simran Kaur, Dingli Yu, Anirudh Goyal, and Sanjeev Arora. Can models learn skill composition from examples? Advances in Neural Information Processing Systems, 37:102393102427, 2024. 2 [46] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. 2 [26] Zikang Liu, Kun Zhou, Wayne Xin Zhao, Dawei Gao, Yaliang Li, and Ji-Rong Wen. Less is more: High-value data selection for visual instruction tuning. arXiv preprint arXiv:2403.09559, 2024. 2 [27] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. 5, 6, 8, 1 [28] Timothy Ossowski, Ming Jiang, and Junjie Hu. Prompting large vision-language models for compositional reasoning. arXiv preprint arXiv:2401.11337, 2024. 2 [29] Simon Park, Abhishek Panigrahi, Yun Cheng, Dingli Yu, Anirudh Goyal, and Sanjeev Arora. Generalizing from simple to hard visual reasoning: Can we mitigate modality imbalance in vlms? arXiv preprint arXiv:2501.02669, 2025. [30] Arijit Ray, Karan Sikka, Ajay Divakaran, Stefan Lee, and Giedrius Burachas. Sunny and dark outside?! improving answer consistency in vqa through entailed question generation. arXiv preprint arXiv:1909.04696, 2019. 1 [31] Eva Sanchez Salido, Julio Gonzalo, and Guillermo Marco. None of the others: general technique to distinguish reasoning from memorization in multiple-choice llm evaluation benchmarks. arXiv preprint arXiv:2502.12896, 2025. 2 [32] Ramprasaath Selvaraju, Purva Tendulkar, Devi Parikh, Eric Horvitz, Marco Tulio Ribeiro, Besmira Nushi, and Ece Kamar. Squinting at vqa models: Introspecting vqa models with sub-questions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1000310011, 2020. 1 [33] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:86128642, 2025. 9 [34] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. 1 [35] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 5, 6, 8, 1 [36] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 3, 4, 6, 7, 8, [37] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2025. 1, 5,"
        },
        {
            "title": "Appendix",
            "content": "In this supplementary material, we provide additional analysis (A). We also detail the implementation process of our compositional question generation and verification step (B). Finally, we include visualizations (C) that demonstrate the effectiveness of compositional tuning through comparative case studies. A. Additional Analysis Analysis of Conversation Length Distribution in LLaVA665K. Fig. 9 shows the distribution of the number of conversations per image in LLaVA-665K [24]. 93.6% of the samples fall below the 10-pair threshold. The distributions mean of 5.18 conversations per image (ùúé = 5.62) shows that the data is heavily skewed towards lower values. We fix the target number of conversations per image in the compositional tuning dataset based on these findings. We ensure fair comparison by aligning the distribution of our data with the baseline distribution. Figure 9. Distribution of conversations per image in LLaVA665K. The overwhelming majority of images (97.69%) have 20 conversation pairs. The average of number of conversations per image is 5.18 (ùúé = 5.62). small subset (2.31%) exceeds 20 conversations, which includes sample with the maximum length of 275. Total conversations: 3,444,246. Analysis of Limited Performance Gains on KnowledgeIntensive Tasks. While our compositional tuning approach shows general improvements on various benchmarks, we observe more modest gains in knowledge-intensive tasks. Table 4 compares the performance of different approaches on OK-VQA, MMMU, and MMMU-Pro benchmarks. COMPACT with 32k compositional tuning data shows relatively small improvements over the random baseline: OK-VQA (50.02% vs 49.30%), MMMU (33.89% vs 32.89%), and MMMU-Pro (20.23% vs 18.15% on standard tasks, 11.91% vs 11.44% on vision tasks). Notably, training on the full LLaVA-665K [24] VIT dataset leads to limited performance improvements on MMMU (33.89%). Although knowledge-related tasks are not our main focus, this inspires future work on designing compositional tuning approaches that cover broader capabilities outside of the vision space. Analysis of Compositional Complexities of Multi-Capability Benchmarks The average compositional complexity ùëò varies among benchmarks. We use Gemini-2.0-Flash [36] to analyze each question and identify the atomic capabilities required to give an answer (see the details of the system prompt in B). We avFigure 10. Distribution of Compositional Complexities in LLaVA-665K samples. Majority of questions (59.2%) use one atomic capability, followed by 30.9% using two. Table 4. Limited Performance Improvements on KnowledgeIntensive Benchmarks. Comparison shows modest improvements over random baseline on tasks that require substantial world knowledge or domain expertise. Numbers reported in accuracy (%) and relative performance to full model (%). Model OK-VQA MMMU MMMU-Pro Rel. Standard Vision (Avg.) Random COMPACT LLaVA-665K [24] 49.30 50.02 57.96 32.89 33.89 33.89 18.15 20.23 20. 11.44 11.91 11.97 92.0% 96.6% 100% erage these numbers for each benchmark to compute benchmarkspecific ùëò values. This reveals varying levels of compositional complexities across benchmarks: InfoVQA [27] ( ùëò = 0.34), SeedBench2Plus [17] ( ùëò = 1.11), MME [9] ( ùëò = 1.16), TextVQA [35] ( ùëò = 1.19), MMVet [43] ( ùëò = 1.24), CV-Bench [37] ( ùëò = 1.33), MMStar [6] ( ùëò = 1.40), and LLaVA-W [24] ( ùëò = 3.05). B. Additional Experiment Details We provide the system prompt for our capability analysis where we identify all the required capabilities for given question (A). We also provide the system prompts for compositional question generation (B) and verification (C). The generation prompt includes structured guidelines to ensure that the generated multi-capability questions naturally blend different capabilities and can only be answered by checking the corresponding images. The verification prompt checks if the questions meet these guidelines and do not contain subjective interpretations or compositional flaws. (A) System Prompt for Capability Analysis Zero-Capability Samples Zero-Capability Questions: How is the weather? Should move to London? Can you provide some information about the Emirates airline? Give me long list of what duties are considered rental activity Have the cat declare her new name as ruler rewrite it from the perspective of an expensive therapist Can you tell me how to prepare Colombian dish how to do coding Can you explain Map Reduce to me? 35 year old patient presented to the emergency department with shortness of breath. Before this, he was at crowded event. He does not have history of diabetes or high blood pressure. He had positive PCR test at an outside hospital. What should be the next steps for the physician? please convert those snomed codes to FHIR Im running used car dealership, what are some emerging opportunities for me brought by large language models like GPT-3? answer it again in Chinese you are legislator. You are asked to come up with framework for new legislation that adances the science of reading for grades K-3. Write that model legislation. Im looking to create podcast, can you help me? COMPACT Data Visualization. We provide visualization of the COMPACT dataset to provide insights into its compositional structure. Figs. 12, 13, and ?? show selected examples from COMPACT dataset. Each question is generated from combination of ùëò atomic capabilities. These cases demonstrate our models enhanced ability to integrate multiple visual capabilities simultaneously, while the baseline model often struggles with such compositionally complex queries. Prompt: You are an AI assistant that analyzes questions to identify the core capabilities required to answer them. Given question, identify ALL the capabilities it requires from this list: - spatial relationship (understanding relative positions) - object interaction (how objects/people interact) - object relationship (relationships between objects) - text recognition (reading text in images) - spatial recognition (understanding 3D space) - action recognition (identifying actions/activities) - object recognition (identifying objects) - counting (counting objects/people) - color (identifying colors) - shape (identifying shapes) Return ONLY JSON array of the required capabilities, like: [capability1, capability2] C. Visualizations Qualitative Comparison. We provide qualitative visualizations that compare the outputs from our compositionally-tuned COMPACT model and the LLaVA-665K VIT model. Examples in Fig. 11 highlight the importance of compositional tuning for handling complex multi-capability tasks (ùëò 3). These cases demonstrate COMPACT models enhanced ability to integrate multiple visual capabilities, while showing the baseline models difficulty with such compositionally complex queries. (C) System Prompt for Question Verification Prompt: You are an AI assistant that verifies if questions about images properly utilize specified capabilities. Given question and its answer, analyze whether it NATURALLY requires using EXACTLY specified capabilities - no more, no less. IMPORTANT: The question should require ALL specified capabilities to be answered The question should not require additional major capabilities beyond those specified The capabilities must be naturally integrated, not artificially forced Zero-Capability Samples in LLaVA-665K. We identify subset of samples in the LLaVA-665K dataset that require no visual capabilities, which we refer to as zero-capability samples. These include general knowledge queries, subjective prompts, or requests that can be answered without inspecting the image at all. While such data may still be useful for instruction following, it does not contribute to the development of vision-centric skills. In our analysis, we find that approximately 1.1% of the questions in LLaVA665K fall into this zero-capability category. (B) System Prompt for Question Generation Prompt: You are an AI assistant that generates challenging but well-defined questions and answers about images. First, will provide you with specific capabilities. Generate 1 question that naturally integrates EXACTLY these capabilities. IMPORTANT: If the question can be answered without looking at the image (e.g., the answer can be inferred from the question itself or previous questions), its BAD question Questions should be reasonably challenging but must have clear, unambiguous answers All answers must be extremely concise - use only single word or short phrase Each question must be single, integrated question that naturally combines all given capabilities DO NOT use and or commas to combine separate questions Questions should require careful observation and reasoning Only generate questions when you can determine the answer with high confidence Avoid subjective or ambiguous questions ONLY ask about objects and capabilities that are ACTUALLY PRESENT in the image NEVER create questions about objects or features that dont exist in the image Generate diverse questions that differ in topic and required reasoning CAPABILITY DEFINITIONS: spatial relationship: Identifying how specific objects are positioned relative to each other (above, below, next to, inside, etc.) - focuses on the direct relationship between two or more particular objects spatial recognition: Understanding the overall spatial layout and arrangement of the entire scene - focuses on the general organization, depth, perspective, or environmental context, rather than relationships between specific objects text recognition: Reading and interpreting text visible in the image action recognition: Identifying what action is being performed (can involve single person/object) object interaction: Analyzing how multiple objects interact with each other (requires at least two objects) - MUST involve at least one moving/active object, not just static objects positioned together - can include humans interacting with objects and humans interacting with humans object recognition: Identifying and naming objects present in the image counting: Determining the number of instances of something in the image color: Identifying or comparing colors of objects in the image shape: Recognizing and describing the shapes of objects in the image scene understanding: Identifying where the image is taken or the type of environment/setting (indoor/outdoor, beach, mountain, kitchen, office, etc.) - focuses on identifying the overall scene, background, or context of the image Examples: BAD: What color is the car, and where is it located? (two separate questions) BAD: What might the person be thinking? (subjective/ambiguous) BAD: Is this nice room? (subjective) BAD: What breed of dog is in the corner? (when no dog exists in the image) BAD: How are the fridge and desk interacting? (static objects dont qualify as interaction) BAD: What is the color of the red car? (answer can be inferred from the question itself without seeing the image) GOOD: What color car is parked next to the red brick building? (specific, clear answer) GOOD: How many yellow tennis balls are visible on the wooden court? (requires counting + color) GOOD: What is the person in blue using to interact with the television? (proper object interaction) GOOD: Where is this image taken? (scene understanding) GOOD: Where is this scene happening? (scene understanding) Figure 11. Qualitative comparison of model outputs. Examples showing responses from our compositionally-tuned COMPACT model and LLaVA-665K [24] VIT model on complex queries that require multiple capabilities (ùëò 3). Our model demonstrates better integration of visual capabilities which leads to more accurate responses. 4 Figure 12. Visualization of COMPACT Compositional Tuning Samples. 5 Figure 13. Visualization of COMPACT Compositional Tuning Samples."
        }
    ],
    "affiliations": [
        "Meta AI",
        "Princeton University"
    ]
}