{
    "paper_title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation",
    "authors": [
        "Yang Fei",
        "George Stoica",
        "Jingyuan Liu",
        "Qifeng Chen",
        "Ranjay Krishna",
        "Xiaojuan Wang",
        "Benlin Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\\% on VBench, 21-22\\% lower FVD, and 71.4\\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\\%, surpassing REPA (92.91\\%) by 2.60\\%, and reduce FVD to 360.57, a 21.20\\% and 22.46\\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ ."
        },
        {
            "title": "Start",
            "content": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation Yang Fei1,2 George Stoica2,3 Jingyuan Liu4 Qifeng Chen1 Ranjay Krishna2 Xiaojuan Wang2 Benlin Liu2 1HKUST 2University of Washington 3Georgia Tech 4Adobe 5 2 0 2 2 1 ] . [ 1 2 9 7 1 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Reality is dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structurepreserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) bidirectional feature fusion module that extracts global structure-preserving motion priors from recurrent model like SAM2; (2) Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60% on VBench, 2122% lower FVD, and 71.4% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51%, surpassing REPA (92.91%) by 2.60%, and reduce FVD to 360.57, 21.20% and 22.46% improvement over REPAand LoRAfinetuning, respectively. The project website can be found at https://sam2videox.github.io/. 1. Introduction From Heraclitus to Bergson, philosophy has cast cognition as the apprehension of becoming rather than being [1, 9]. While image generation models have excelled at generating what is there in high-fidelity images [2, 3, 16], our best *equal advising. corresponding authors. This work was done in part while Yang Fei was an exchange student at the University of Washington. video generation still struggles to express the dynamics of change. The central challenge is structure-preserving motion: dynamics that maintain part topology and local neighborhoods while allowing constrained deformations. Without these constraints, generated motions drift; limbs shear, textures tear, and object identity is lost. Only by achieving this can models move beyond static appearance toward faithful world simulators. Motion remains major challenge. This is particularly true for articulated and highly deformable objects, such as humans and animals, where models often suffer from inconsistent or physically implausible transitions in object states. Inference-time interventions, such as ControlNet-style [37] conditioning on explicit motion representations during inference, require knowing the ideal motion apriori. Unlike rigid objects whose motion can be well represented by simple dragged trajectories [7], articulated and deformable entities lack unified motion representation common assumption is that low motion quality stems from insufficient training data, especially for high-quality complex articulated motions. However, scaling up or augmenting training data only helps marginally; training with motion proxies (optical flow [4], skeletons [13]) for objects [28, 35] still results in physically implausible transitions. Our experiments show that generated videos still produce lions walking without alternating legs and cyclists with static knees. Scaling up such priors results in noisy training data; motion priors are usually collected using imperfect models (e.g. RAFT can generate optical flow priors [29]). To improve articulated motion, we propose simple but powerful idea: deriving structure from tracking. Our model, SAM2VideoX, distills structure-preserving motion priors from video tracking model into video diffusion generator. Previous work has shown that distilling image representations improves image generation fidelity [38]. To generalize this insight from static to dynamic generation, we distill video representations to improve video generation. Specifically, we leverage SAM2 [23], state-of-the-art video tracking model trained on large-scale, diverse video data. SAM2 is capable of maintaining object identity across 1 d o V y X i 2 S Figure 1. We present training algorithm to distill structure-preserving motion priors from SAM2 into video diffusion model to improve motion accuracy and smoothness in generated videos. Compared to advanced image-to-video models CogVideoX [34] and HunyuanVid [16], our SAM2VideoX produces videos with superior or highly competitive fidelity, despite HunyuanVid having more than twice number of parameters (13B vs. our 5B). long sequences and through complex occlusions. To track, SAM2s internal representations have captured how parts move together, how limbs stay connected, and how occlusions resolve over time. Instead of conditioning generation on explicit control signals like optical flow [4] or skeletons [13], SAM2VideoX extracts implicit structural cues directly from SAM2s internal representations. By transferring these motion priors, the generator acquires an internal sense of structure and continuity. In short, we leverage the structural understanding of tracker to guide the generation of motion. However, transferring useful information from SAM2 into video generation model is technically challenging. We find that directly supervising diffusion models to predict SAM2 output masks yields only limited benefit, as the masks are discrete and boundary-focused; they fail to supervise useful fine-grained motion. Also, direct alignment between feature spaces is hindered by an architectural asymmetry: state-of-the-art video generation models use DiT [20] architecture with bidirectional attention to access global context, while SAM2 is inherently recurrent and causal. To bridge this gap, we extract supervision signal by fusing forward and backward SAM2 features, where backward features are extracted by reversing the order of frames in training video. Together, the forward and backward features represent better global video context. We align video diffusion features with this supervisory signal using Local Gram Flow loss. Although ℓ2 loss has worked well for image generation [38], we find that local Gram loss captures better motion priors by emphasizing local relational structure. Across qualitative and quantitative SAM2VideoX yields more realistic, evaluations, structurally coherent motion. On VBench [12] (matched dynamic degree) we achieve 95.51%, surpassing REPA [36] (92.91%) by 2.60 points. Our FVD is 360.57, reduction of 21.20% and 22.46% versus REPAand LoRA-finetuning [11]. Since existing benchmarks are limited in assessing preservation of structure in articulated motion, we further conduct human evaluation, where 71.4% of ratings prefer our results. Qualitatively, SAM2VideoX produces videos with the correct number of legs when animals walk, ensuring plausible human limb trajectories during complex activities, and producing accurate human-object interactions. These gains indicate that SAM2-guided distillation substantially strengthens structure-preserving motion in video diffusion models without sacrificing visual quality. 2. Related Work Video diffusion models. Video diffusion models have progressed rapidly, spanning both UNet-based [24] architectures such as Stable Video Diffusion [2], and DiTbased [20] models including Sora [3], CogVideoX [34], HunyuanVid [16], OpenSora [21], Open-Sora-Plan [17], Wan-Video [32], Cosmos [19]. While these models can generate visually impressive videos, producing realistic and coherent structure-preserving motion remains challenging, especially for articulated and deformable entities. Given the difficulty of designing reliable inference-time control signals [37] for such objects, our goal is to enhance the base models intrinsic ability to generate structure-preserving motion without relying on auxiliary handcrafted motion controls during inference. Motion understanding. Understanding motion has long been central to video analysis [10, 22]. Point trajectories 2 and optical flow [14, 29] describe local, adjacent-frame changes and carry limited semantics; they often degrade under fast or long-range motion and struggle to maintain object identity through occlusions. Mask-based tracking offers instance-level signals that are more stable in cluttered scenes. SAM2 [23] tracks user-prompted regions across long sequences and is known to generalize well across domains while preserving object identity through occlusions. However, raw masks are discrete and boundary-focused, which discards much of the appearance and motion structure that video diffusion models need. We therefore leverage SAM2s internal features as motion priors: they are dense, continuous and temporally consistent, and they provide long-range correspondences that are useful for structure preservation. Representation alignment. Representation alignment was introduced for image generation by REPA [36] and inspired several follow-ups. In the video domain, two main approaches exist: aligning diffusion features to structured motion signals such as trajectories or flow [4, 13]; or to generic video encoders like VideoMAEv2 [30], as in VideoREPA [38]. Both suffer from key limitations. First, Trajectories and flow provide local supervision and are sensitive to long-range dynamics, which weakens their usefulness for structure preservation. Second, popular video encoders like VideoMAEv2 are optimized for high-level semantic tasks rather than low-level motion understanding. In contrast, our approach aligns diffusion features to SAM2s internal features. This prior is unified and structure-centric, allowing transfer across humans and animals while yielding objectconsistent motion representations without requiring specific controller at inference. 3. Preliminaries In this work, we employ diffusion transformer (DiT) [20] as our base video generation model, aiming to distill SAM2s motion understanding into the DiT to enhance its ability to learn structure-preserving motion from in-the-wild videos. We first introduce preliminary background on latent video diffusion models and the SAM2 architecture. Latent Video Diffusion Model. Diffusion models generate samples by inverting forward noising process [8, 26, 27]. In the latent setting, pre-trained autoencoder, with encoder E() and decoder D(), maps video = {I0, . . . , IN 1} RN HW to latent representation = E(x), where RN W . At timestep t, noisy latent zt is sampled as zt = αt + σt ϵ, ϵ (0, I). DiT fθ is trained to predict the velocity target = αtϵ σtz under the standard v-prediction objective. At inference, iterative denoising maps zT z0, after which the decoder D(z0) produces the final video. Segment-Anything Model 2 (SAM2). The SegmentAnything Model 2 (SAM2) extends the image segmentation capability of SAM 1 [15] to the video domain, i.e., tracking an object in video conditioned on given prompts such as points or boxes. Given video = {I0, I1, ..., IN 1}, SAM2 processes the video recurrently and produces segmentation mask for each frame. Specifically, an image encoder extracts frame embedding from the current frame, which is then enhanced by memory attention module aggregating historical context from memory bank to produce Fmem. mask decoder Dmask subsequently takes Fmem and user prompts to generate the segmentation mask. By applying this recurrent procedure across all frames, SAM2 produces both sequence of masks: = {M0, M1, ..., MN 1} and sequence of memory features from M: Fmem = {Fmem,0, Fmem,1, ..., Fmem,N 1}. 4. Method We distill SAM2s motion structure prior into the video DiT model by aligning their internal feature representations. This is achieved through learnable feature alignment module (Sec. 4.1) that projects intermediate DiT features into latent space where they can be matched to those of SAM2. To align their relational motion structures, we introduce novel Local Gram Flow (LGF) feature matching operator (Sec. 4.2). Furthermore, due to the autoregressive nature of SAM2feature, in contrast to the bidirectional features in DiT, we propose method that combines SAM2s forward and backward video features into single bidirectional representation, providing more suitable teacher signal to be distilled from (Sec. 4.3). Finally, Local Gram Flow motion distillation loss (Sec. 4.4) is applied to enforce this alignment in the latent space. Fig. 2 provides an overview of our full method. 4.1. Feature Alignment Network Formally, given training video x, we encode it to latent representation = E(x) using the video VAE, then add noise at timestep to produce zt. The noised latent zt and timestep are then fed into the denoising network fθ, from which we extract intermediate activations as video diffusion features Fdiff RN W , where is the number of latent frames. Here Fdiff denotes the activations from selected intermediate layer. For the same video, we extract SAM2s internal features FSAM2 as distillation teacher. Compared with the output segmentation masks, the internal 3 Figure 2. Method overview. The framework consists of two parallel branches: (Top) The Motion Prior Extraction branch extracts forward and backward memory features (Ffwd mem) from SAM2 given clean video, and fuses them into bidirectional teacher representation. (Bottom) The Video Generation Backbone takes noisy latents as input, and the intermediate DiT features Fdiff are projected into the SAM2 space as ˆFdiff . Then the proposed Local Gram Flow loss (Lfeat) is used to align the spatio-temporal structure of the projected student features with the teacher priors. mem, Fbwd feature representations provide richer spatio-temporal information. They capture object motion and part-level dynamics and can teach diffusion model internalize motion priors beyond simple boundary cues. To align Fdiff and FSAM2, We project Fdiff into SAM2s feature space. Specifically, we add projection module on top of Fdiff , which consists of an interpolation layer with skip connections for temporal dimension matching, and then followed by three-layer MLP, yielding: ˆFdiff = P(Fdiff ). We then compute motion distillation loss between ˆFdiff and FSAM2. Our final objective combines the alignment loss with the standard diffusion loss: Ldiff + λ Lfeat(ˆFdiff , FSAM2), min fθ,P where Ldiff is the v-prediction loss and λ = 0.5 balances the terms. 4.2. Local Gram Flow Feature Matching To capture cross-frame spatio-temporal motion structure, rather than performing direct one-to-one feature matching between ˆFdiff and FSAM2, we instead match their respective Gram matrices, which encode pairwise dot products of token feature embeddings, i.e., pairwise token similarities. However, computing the full Gram matrices is computationally prohibitive for video diffusion models due to the large Figure 3. Illustration of the Local Gram Flow (LGF) Loss. The LGF operator captures motion structure by computing similarities between token at frame and its 7x7 spatial neighborhood in the subsequent frame + 1. Instead of matching absolute values (e.g., ℓ2), we convert the resulting similarity vectors (Pt,i, Qt,i) into probability distributions and align them using the KL Divergence. This forces the model to learn relative motion patterns, not just feature values. number of tokens. We therefore propose Local Gram Flow, which computes the dot products only between each token and the tokens within its 7 7 spatial neighborhood in the subsequent frame (see Fig. 3). This yields local similarity vectors at each position that model likely motion trajectories to the next frame. We denote this operator as LGF(). 4.3. Bidirectional Fusion of Causal SAM2 Features key challenge in aligning the feature representation of DiT and SAM2 is the architectural asymmetry between them. Video DiTs typically employ bidirectional attention that allows each token to attend to all frames, whereas 4 SAM2s recurrent mechanism constrains its memory feature Fmem at each timestep to encode only current and past frames. To bridge this gap and create teacher feature FSAM2 that is aware of the full video context (similar to DiT), we construct it from both forward and backward pass of SAM2. The backward pass is obtained by feeding the temporally reversed video into SAM2 to extract its backward features. After obtaining the backward features, we remap them to the original temporal order (i.e., (cid:55) 1t) to align with the forward features, producing:"
        },
        {
            "title": "Ffwd",
            "content": "mem = {F fwd mem,0, fwd mem,1, . . . , fwd mem,N 1},"
        },
        {
            "title": "Fbwd",
            "content": "mem = {F bwd mem,0, bwd mem,1, . . . , bwd mem,N 1}. Empirically, using separate projectors to align separately to Ffwd mem and Fbwd mem provides marginal improvement, as gradient conflicts destabilize training. We therefore fuse them into unified bidirectional teacher feature. mem +(1k)Fbwd However, fusing these two features is non-trivial. As our ablation study demonstrate  (Table 2)  , naively adding them (kFfwd mem) leads to severe performance degradation. Because the final fused feature will be aligned with the projected DiT feature through their Local Gram Flows, we instead directly fuse their Local Gram Flows via convex combination, which stabilizes training while preserving complementary information: LGF(FSAM2) = LGF(Ffwd mem) + (1 k) LGF(Fbwd mem) 4.4. Motion Distillation Loss Finally, the motion distillation loss, Lfeat, is designed to match the LGF distributions of the student ˆFdiff and the fused teacher FSAM2, rather than enforcing one-to-one correspondence as in standard ℓ2 loss. More specifically, we align the distribution of spatio-temporal motion similarities between the video DiT feature and the SAM2 feature. We therefore apply softmax to each tokens similarity vector (turning it into probability distribution) and measure the distance using the KL divergence. This approach focuses on the relative ranking of similarities, which we argue better captures the underlying motion structure. Specifically, we compute probability distributions: (cid:16) = (cid:16) = LGF(FSAM2) (cid:17) LGF(ˆFdiff ) . (cid:17) , The motion distillation loss averages the KL divergence over all spatial tokens Ω and all 1 latent frames for which LGF is computed: (cid:16)ˆFdiff , FSAM2 (cid:17) = Lfeat 1 (N 1)Ω 2 (cid:88) (cid:88) t=0 iΩ KL(cid:0)Pt,i (cid:13) (cid:13) Qt,i (cid:1) 5 Our ablation studies  (Table 2)  empirically validate that this LGF-KL combination is critical, yielding significant gains over simpler alternatives (e.g., LGF with ℓ2 loss, or direct feature matching). 5. Experiments Dataset & training configuration. We curate motionfocused dataset of 9,837 single-subject video clips from open-source video generation datasets(Panda70M [5], MMTrailer [6], MotionVid [33]), capturing diverse motion patterns across animals and humans. All videos are at 8 FPS, capped at 100 frames. Our approach builds upon CogVideoX-5B-I2V [34], extracting intermediate DiT features from the 25th block output as Fdiff . We first obtain object bounding boxes using GroundingDINO [18] to prompt SAM2 mask generation. To align with DiT features, we propagate subject masks from both temporal directions: using the first-frame mask to compute forward features Ffwd mem and the last-frame mask for backward features Fbwd mem. To avoid SAM2 inference overhead during training, we precompute features for clips starting at every 20 frames. We implement LoRA fine-tuning with rank 256 and scaling factor α = 128. Training uses AdamW optimization with learning rate 1 104 and momentum parameter (β1, β2) = (0.9, 0.95). We train for 3,000 steps on 8H200 GPUs, with global batch size 32 through gradient accumulation over four steps per GPU. Baselines. We compare our complete approach, which uses bidirectional feature fusion and Local Gram Flow loss, with four baselines: (1) CogVideoX-5B-I2V as the base model; (2) + LoRA fine-tuning, which simply fine-tunes the base model with LoRA on our curated motion dataset; (3) + Mask supervision, which adds linear projection layer on top of our projection head to directly predict the subject mask, trained with mask loss supervision instead of feature alignment; (4) +REPA , which utilizes REPA Loss [36] to align DiT features with external DINOv3 [25] features. Evaluation protocol. We evaluate across three complementary axes: objective motion metrics, perceptual quality, and human preference. For objective evaluation, we filter 85 images and compute four metrics from the VBench-I2V suite [12]: Motion Smoothness, Subject Consistency, and Background Consistency, and Dynamic Degree. We define consistency metrics as measures of temporal structure stability, while Dynamic Degree quantifies the magnitude of motion. Since consistency scores often correlate negatively with motion magnitude(i.e., static videos trivially achieve perfect consistency), to ensure fair comparison of motion quality, we exclude baselines with lower Dynamic Degree than the base model. The overall Motion Score is obtained by averaging the smoothness and consistency metrics (min-max normalized). For the Extended Motion Score, we o a i - F + . k + d 2 l m B i - F + . k + d 2 Figure 4. Qualitative comparison on articulated motion. The blue box indicates the input image. Red boxes highlight common failure modes in baselines, including structural distortion (cyclists legs) and physical implausibility (three-legged lion, inconsistent leg motion). In contrast, our method (SAM2VideoX) consistently maintains structural integrity. + Fine-tuning is the LoRA fine-tuning baseline; + Mask sup. is the mask supervision baseline. We recommend viewing the supplementary videos. incorporate I2V-Subject and I2V-Background Consistency with 0.5 weight, following the official VBench protocol. For perceptual quality, we compute Frechet Video Distance (FVD) [31] on separate set of 200 videos randomly sampled from the training dataset. For human preference, we conduct double-blind user study using 40 randomly sampled prompts. Participants are presented with two side-byside videos (Ours vs. Baseline) in random order and asked to select the preferred one based on motion smoothness and subject consistency. All methods generate 49-frame videos 6 Table 1. Quantitative comparison on established video generation benchmarks. Our method outperforms all fine-tuning baselines and achieves performance comparable to the strong open-source model HunyuanVid, while significantly surpassing it in perceptual quality (FVD). BC: Background Consistency; SC: Subject Consistency; MS: Motion Smoothness. Extended Motion Score incorporates I2V Subject and Background Consistency (weight 0.5). Higher Motion/Extended Motion Scores indicate better structure preservation; lower FVD indicates superior perceptual quality. Baselines with Dynamic Degree lower than the base model are excluded from VBench comparisons to ensure fairness."
        },
        {
            "title": "Method",
            "content": "BC SC MS Motion Score Ext Motion Score FVD CogVideoX (base model) + LoRA Fine-tuning + Mask Supervision + REPA + SAM2VideoX (Ours)"
        },
        {
            "title": "HunyuanVid",
            "content": "97.30 97.44 - 97.41 97.88 94.43 93.47 - 91.99 94.76 98.17 97.76 - 97.31 98.45 96.85 95.32 98. 94.80 94.02 - 92.91 95.51 95.62 95.50 94.74 - 93.77 96.03 96.24 660.29 465.00 397.73 457.59 360.57 583. mask supervision delineates subject boundaries, it lacks the fine-grained internal correspondence information necessary for articulating complex motion. Consequently, the mask supervision baseline suffers from severe structural artifacts (e.g., cyclist and lion legs in Fig. 4) and achieves poorer FVD score of 397.73 compared to our 360.57. Note that we exclude the mask supervision baseline from VBench consistency metrics in Table 1 because it collapses towards static generation (Dynamic Degree 44.59 vs. Base model 45.95), which would yield artificially inflated consistency scores. Video-aware priors are essential for temporal consistency. To validate the necessity of using video foundation model (SAM2) as the teacher, we compare against REPA [36], which aligns DiT features with the image-based DINOv3 encoder. As DINO is trained on static images, it lacks inherent knowledge of temporal continuity. This limitation is reflected in Table 1, where REPA yields significantly lower Motion Score (92.91) compared to our method (95.51). This result confirms that aligning with SAM2s memory-based features transfers crucial temporal coherence signals that image-only encoders cannot provide. Human evaluators consistently prefer our method. As illustrated in Figure 5, our method achieves the highest win rates in double-blind user study, outperforming the base model and fine-tuning baselines by wide margin. Qualitative results in Figure 4 corroborate this preference: while baselines often struggle with limb consistency (e.g., the disappearing/reappearing legs of the cyclist), our method maintains subject identity and structural integrity throughout the sequence. This demonstrates that our bidirectional alignment strategy successfully translates the robust segmentation priors of SAM2 into high-fidelity video generation. Figure 5. Human preference win rates. Participants were shown pairwise comparisons and asked to select the video with superior limb consistency and fewer motion artifacts. Our method was strongly preferred (> 64%) over all baselines, confirming its superior ability to generate plausible, artifact-free motion. at 8 fps and 720480 resolution, using 50 denoising steps with guidance scale 6.0. 5.1. Results SAM2VideoX achieves superior structure preservation and perceptual quality compared to all baselines. As shown in Table 1, our method outperforms the base CogVideoX model and other fine-tuning strategies across all reported metrics. Notably, we achieve an FVD of 360.57, substantial reduction compared to the strongest baseline (LoRA Fine-tuning at 465.00) and the REPA baseline (457.59). This indicates that our model generates videos with significantly higher perceptual fidelity. Furthermore, our method attains the highest scores in Motion Score (95.51) and Extended Motion Score (96.03), confirming that distilling internal priors from SAM2 effectively suppresses the temporal flickering and identity degradation often observed in standard video diffusion models. Feature distillation outperforms coarse mask supervision. We compare our feature-level alignment against baseline trained with explicit mask supervision (+ Mask Supervision). We build this baseline by adding linear projection layer on top of to predict the mask. While 7 Configurations + LoRA only w/o LGF w/o KL w/ Forward-Only Teacher w/ Separate Projectors Feature-Space Fusion SAM2VideoX (LGF Fusion) Motion Score Ext Motion Score 94.02 94.58 94.51 95.07 94.68 94. 95.51 94.74 95.24 95.16 95.58 95.24 94.83 96.03 Table 2. Ablation study on core components (VBench-I2V, ). Using simple ℓ2 loss (w/o LGF) and an ℓ2 loss within LGF space (w/o KL) both yield marginal improvements only, confirming our full LGF-KLs superiority in capturing motion structure. Removing bidirectional processing (w/ Forward-Only Teacher) or fusion mechanisms (w/ Separate Projectors) degrades performance, while feature-level adding (Feature-Space Fusion) underperforms ours (LGF Fusion), validating each design choice. o L / o L / Figure 6. Qualitative ablation of our LGF-KL Loss. (Top) Using standard ℓ2 loss (w/o LGF Loss) on raw features results in visible temporal jitter (note the flickering in the arm). (Bottom) Our full method (w/ LGF Loss) produces visibly smoother and more stable motion, validating the design of aligning relational distributions (LGF-KL) over absolute values (ℓ2). 5.2. Ablations We conduct systematic ablation studies on the VBench-I2V 85-image subset to isolate the contribution of our two core components: the LGF fused bidirectional SAM2 teacher and the LGF-KL distillation loss. LGF fusion is essential to resolve bidirectional conflicts. We validate our teacher design in Table 2. While using only the forward stream (Ffwd mem) yields strong baseline (Motion Score 95.07), naively incorporating the backward stream via separate projectors causes gradient conflicts, destabilizing training and degrading performance. More critically, fusing forward and backward streams directly in the feature space leads to catastrophic collapse (94.16), barely outperforming the LoRA-only baseline. This suggests that raw features from opposite temporal directions interfere destructively. By contrast, our proposed LGF Fusion acts as harmonic integration, resolving these conflicts to achieve 8 O - / a - F i F Figure 7. Qualitative ablation of our bidirectional LGF fusion.(Top) The forward-only teacher (w/ Fwd-Only) produces artifacts, like the ballerinas arm folding incorrectly (red box). (Middle) Feat-Space Fusion causes structural tearing. (Bottom) Our LGF Fusion correctly preserves the limbs topological structure, highlighting the necessity of both bidirectional information and robust fusion strategy. the highest score (95.51). Qualitative results in Figure 7 confirm that this relational fusion is key to leveraging bidirectional priors without introducing artifacts. KL divergence outperforms ℓ2 for structural alignment. We further ablate the loss function design given our LGF teacher. As shown in Table 2, applying standard ℓ2 loss directly on raw features performs poorly (94.58), proving that strict element-wise alignment is too rigid for transferring high-level motion priors. More revealingly, applying an ℓ2 loss within the LGF space performs even worse (94.51). This demonstrates that the LGF operator alone is insufficient; naive value-based alignment of its relational features fails to capture the correct motion priors. It is the combination of LGF (to capture relational structure) and the KL divergence (to align probabilistic distributions) that is essential. Our full LGF-KL loss achieves the best performance (95.51), confirming that aligning the relative spatiotemporal distributions via KL divergence is superior to forcing exact value matches. Figure 6 visually demonstrates the smoother temporal transitions achieved by this design. 6. Conclusion In this paper, we propose novel framework that effectively distills the rich, structure-preserving motion priors from SAM2 into video diffusion models. Departing from methods relying on external control signals or limited datasets, we demonstrate that aligning generative features with dense correspondence representations offers more intrinsic solution to articulated motion generation. Our core contributionsbidirectional feature fusion and the Local Gram Flow lossenable the seamless transfer of fine-grained motion knowledge without requiring architectural modifications. Extensive experiments validate that our approach not only achieves superior performance on standard benchmarks but also paves the way for leveraging discriminative vision foundation models to enhance generative video dynamics."
        },
        {
            "title": "References",
            "content": "[1] Henri Bergson. Creative Evolution. Henry Holt and Company, New York, 1911. 1 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1, 2 [3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 1, 2 [4] Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly Sheynin. Videojam: Joint appearance-motion representations for enhanced motion generation in video models. arXiv preprint arXiv:2502.02492, 2025. 1, 2, 3 [5] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. Panda-70m: Captioning 70m videos In Proceedings of with multiple cross-modality teachers. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 5 [6] Xiaowei Chi, Yatian Wang, Aosong Cheng, Pengjun Fang, Zeyue Tian, Yingqing He, Zhaoyang Liu, Xingqun Qi, Jiahao Pan, Rongyu Zhang, Mengfei Li, Ruibin Yuan, Yanbing Jiang, Wei Xue, Wenhan Luo, Qifeng Chen, Shanghang Zhang, Qifeng Liu, and Yike Guo. Mmtrail: multimodal trailer video dataset with language and music descriptions, 2024. [7] Yufan Deng, Ruida Wang, Yuhao Zhang, Yu-Wing Tai, and Chi-Keung Tang. Dragvideo: Interactive drag-style video editing. In European Conference on Computer Vision, pages 183199. Springer, 2024. 1 [8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 3 [9] Heraclitus. Fragments. 500BC. Translated in The Presocratic Philosophers, eds. G. S. Kirk and J. E. Raven, Cambridge University Press, 1957. 1 [10] Berthold KP Horn and Brian Schunck. Determining optical flow. Artificial intelligence, 17(1-3):185203, 1981. 2 [11] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 2, [12] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2, 5, 11 [13] Hyeonho Jeong, Chun-Hao Paul Huang, Jong Chul Ye, Niloy Mitra, and Duygu Ceylan. Track4gen: Teaching video diffusion models to track points improves video generation. arXiv preprint arXiv:2412.06016, 2024. 1, 2, 3, 11 [14] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudoarXiv preprint arXiv:2410.11831, labelling real videos. 2024. 3 [15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 3 [16] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. 1, 2, 14 [17] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [18] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. 5 [19] NVIDIA, Arslan Ali, Junjie Bai, Maciej Bala, Yogesh Balaji, Aaron Blakeman, Tiffany Cai, Jiaxin Cao, Tianshi Cao, Elizabeth Cha, Yu-Wei Chao, Prithvijit Chattopadhyay, Mike Chen, Yongxin Chen, Yu Chen, Shuai Cheng, Yin Cui, Jenna Diamond, Yifan Ding, Jiaojiao Fan, Linxi Fan, Liang Feng, Francesco Ferroni, Sanja Fidler, Xiao Fu, Ruiyuan Gao, Yunhao Ge, Jinwei Gu, Aryaman Gupta, Siddharth Gururani, Imad El Hanafi, Ali Hassani, Zekun Hao, Jacob Huffman, Joel Jang, Pooya Jannaty, Jan Kautz, Grace Lam, Xuan Li, Zhaoshuo Li, Maosheng Liao, Chen-Hsuan Lin, Tsung-Yi Lin, Yen-Chen Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Yifan Lu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Seungjun Nah, Yashraj Narang, Abhijeet Panaskar, Lindsey Pavao, Trung Pham, Morteza Ramezanali, Fitsum Reda, Scott Reed, Xuanchi Ren, Haonan Shao, Yue Shen, Stella Shi, Shuran Song, Bartosz Stefaniak, Shangkun Sun, Shitao Tang, Sameena Tasmeen, Lyne Tchapmi, WeiCheng Tseng, Jibin Varghese, Andrew Z. Wang, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Jiashu Xu, Dinghao Yang, Xiaodong Yang, Haotian 9 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 1, 3, 11 [30] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:1007810093, 2022. 3 [31] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 6 [32] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [33] Boyuan Wang, Xiaofeng Wang, Chaojun Ni, Guosheng Zhao, Zhiqin Yang, Zheng Zhu, Muyang Zhang, Yukun Zhou, Xinze Chen, Guan Huang, et al. Humandreamer: Generating controllable human-motion videos via decoupled generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1239112401, 2025. 5 [34] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 5, 11 [35] Zhangsihao Yang, Mingyuan Zhou, Mengyi Shan, Bingbing Wen, Ziwei Xuan, Mitch Hill, Junjie Bai, Guo-Jun Qi, and Yalin Wang. Omnimotiongpt: animal motion generation In Proceedings of the IEEE/CVF Conwith limited data. ference on Computer Vision and Pattern Recognition, pages 12491259, 2024. 1 [36] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. 2, 3, 5, 7 [37] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 1, 2 [38] Xiangdong Zhang, Jiaqi Liao, Shaofeng Zhang, Fanqing Meng, Xiangpeng Wan, Junchi Yan, and Yu Cheng. Videorepa: Learning physics for video generation through relational alignment with foundation models. arXiv preprint arXiv:2505.23656, 2025. 1, 2, 3 Ye, Seonghyeon Ye, Xiaohui Zeng, Jing Zhang, Qinsheng Zhang, Kaiwen Zheng, Andrew Zhu, and Yuke Zhu. World simulation with video foundation models for physical ai, 2025. [20] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2, 3 [21] Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, Yuhui Wang, Anbang Ye, Gang Ren, Qianran Ma, Wanying Liang, Xiang Lian, Xiwen Wu, Yuting Zhong, Zhuangyan Li, Chaoyu Gong, Guojun Lei, Leijun Cheng, Limin Zhang, Minghao Li, Ruijie Zhang, Silan Hu, Shijie Huang, Xiaokang Wang, Yuanheng Zhao, Yuqi Wang, Ziang Wei, and Yang You. Open-sora 2.0: Training commercial-level video generation model in $200k. arXiv preprint arXiv:2503.09642, 2025. 2 [22] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 724732, 2016. 2 [23] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 1, 3, 11 [24] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 2 [25] Oriane Simeoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothee Darcet, Theo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herve Jegou, Patrick Labatut, and Piotr Bojanowski. DINOv3, 2025. [26] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3 and Stefano Ermon. arXiv preprint [27] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 3 [28] Keqiang Sun, Dor Litvak, Yunzhi Zhang, Hongsheng Li, Jiajun Wu, and Shangzhe Wu. Ponymation: Learning articulated 3d animal motions from unlabeled online videos. In European Conference on Computer Vision, pages 100119. Springer, 2024. 1 [29] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 10 Method Track4Gen SAM2VideoX (Ours) BC SC MS Motion Score Ext Motion Score 97.32 97.88 94.67 94. 98.35 98.45 95.11 95.51 95.69 96.03 Table 3. Quantitative Analysis of Additional Baseline. Comparison between point-based control and our feature-based approach. Track4Gen denotes the modified version adapted to the CogVideoX architecture. Abbreviations: BC (Background Consistency), SC (Subject Consistency), MS (Motion Smoothness). A. Implementation Details Architecture. Within the projection head of the Video Feature Alignment module, we employ SiLU activation and Group Normalization across all convolutional and MLP layers. For the interpolation layer, we utilize temporal interpolation factor of 4, utilized alongside skip connection with kernel size of (3, 1, 1) and 768 channels. Subsequently, the widths of the following MLP layers adhere to the sequence 768 512 256 256. Training Details. We apply LoRA [11] exclusively to the attention modules, keeping all other backbone parameters frozen. For the LoRA parameters, we employ linear warmup of 200 steps, gradually increasing the learning rate to peak of 104. For the projector, we utilize cosine decay scheduler with 150-step warmup; the learning rate is initialized at 5 104 and decays to minimum of 1 105. Additionally, we implement gradient clipping threshold of 1.0. A.1. Evaluation Protocol and Metrics Evaluation Subset Selection. To ensure our evaluation aligns with the distribution of our training dataset (which focuses on articulated motion), we curated subset of 85 prompts from the VBench-I2V [12] benchmark. The selected prompts predominantly feature human or animal subjects. We excluded generic scenery or abstract textures lacking distinct structural subjects, as these samples do not adequately challenge the models structure-preserving capabilities. Metric Selection. Since our primary objective is structure-preserving video generation rather than artistic creation, we exclude general visual quality metrics such as Aesthetic Quality and Imaging Quality, as they are less relevant to evaluating structural fidelity. To provide unified quantitative evaluation, we formulate two composite scores: Motion Score and Extended Motion Score. Following the standard VBench-I2V protocol, we first apply min-max normalization to all individual sub-metrics to map them into unified range. Let ˆsbg, ˆssmooth, and ˆssubj denote the normalized scores for Background Consistency, Motion Smoothness, and Subject Consistency, respectively. The Motion Score is defined as the arithmetic mean of these three core structural metrics: Smotion = ˆsbg + ˆssmooth + ˆssubj (1) To further account for fidelity to the conditioning image, the Extended Motion Score incorporates I2V consistency metrics. We assign weight of 0.5 to both I2V Subject Consistency (ˆsi2v-s) and I2V Background Consistency (ˆsi2v-b), reflecting balance between temporal coherence and input fidelity. The formulation is given by: Sext = ˆsbg + ˆssmooth + ˆssubj + 0.5 ˆsi2v-s + 0.5 ˆsi2v-b 4 (2) B. Additional Experiments Dense Features Outperform Sparse Trajectories. To ensure fair comparison within the DiT-based CogVideoX [34] architecture, we align the experimental implementation by adapting the Track4Gen [13] projector to mirror our design: specifically, employing an interpolation layer followed by three MLP layers. Furthermore, we exclude the refiner module to strictly isolate the efficacy of the distillation objective. As shown in Table 3, our method outperforms Track4Gen. This demonstrates the superiority of dense SAM2 [23] features over sparse point trajectories, as the latter are prone to performance degradation caused by error accumulation in optical flow estimation (e.g., RAFT [29]). Explicit Backward Constraints are Redundant. We investigate the impact of extending the Local Gram Feature (LGF) loss to incorporate backward temporal constraint (i.e., computing similarity between frame and 1). As shown in Table 4, this formulation yields inferior results compared to our forward-only design. We attribute this to the inherent bidirectionality"
        },
        {
            "title": "Configurations",
            "content": "Bidirectional LGF LGF loss (Ours) 23rd Transformer Block 24th Transformer Block 25th Transformer Block (Ours) 26th Transformer Block 27th Transformer Block Motion Score Ext. Motion Score 94.87 95.51 94.93 94.44 95.51 94.44 94.68 95.52 96. 95.54 95.14 96.03 95.12 95.36 Table 4. Ablations on Temporal Directionality and Injection Depth. We analyze two key design choices: (a) the directionality of the Local Gram Feature loss, and (b) the optimal depth for feature injection within the DiT architecture. The highlighted row indicates our default configuration. of the distilled SAM 2 features; explicitly enforcing backward consistency creates computational redundancy and introduces over-constraints that hamper the generation dynamics. Therefore, we retain the forward-only formulation. C. Additional Qualitative Results We provide additional visual comparisons to further substantiate the effectiveness of SAM2VideoX. Specifically, we contrast our method with competing baselines and state-of-the-art models, highlighting superior structural fidelity and temporal coherence across diverse motion scenarios. D. Theoretical Analysis of Feature Fusion Definition of S(). As referenced in Sec. 4.4, S() denotes temperature-scaled softmax operation applied to each similarity vector, normalizing the 7 7 similarity scores into probability distribution. Given similarities {zi}K i=1, we compute and in all experiments we set the temperature to = 0.1. pi = exp (zi/T ) j=1 exp (zj/T ) (cid:80)K , Interference in Feature-Space Fusion. Let = fwd mem,t+1 represent directional SAM2 memory features across two consecutive frames. The ideal local Gram similarities for the forward and backward teachers are and d, respectively. If we first fuse forward and backward features in the feature space, mem,t+1, and = bwd mem,t, = bwd mem,t, = fwd ft = ka + (1 k)b, ft+1 = kc + (1 k)d, and then compute the local Gram, we obtain gfeat = ft ft+1 = k2(a c) + (1 k)2(b d) + k(1 k)(cid:0)a + c(cid:1). The final term introduces cross-correlations (a and c) that couple forward features at frame with backward features at frame t+1. These cross terms lack counterpart in the valid teacher similarity matrix (neither purely forward nor backward), thereby mixing incompatible temporal contexts. Consequently, the student model is supervised to learn spurious correlations absent in the teachers distribution, leading to temporal inconsistencies. In contrast, our LGF fusion computes local Gram similarities for each direction separately and fuses them only in the LGF space: glgf = k(a c) + (1 k)(b d). This formulation eliminates cross terms, yielding convex combination of two consistent teacher signals and circumventing the interference inherent in feature-space fusion. 12 o a i - F + . s M + d 2 l m B n - F + . k + d 2 Figure 8. Extended Visual Comparisons with Baselines. The blue box indicates the conditioning input image, while red boxes highlight structural failure modes observed in baseline methods. E. Limitations and Future Work Limitations. We acknowledge that our models performance is effectively bounded by the inherent capabilities of the underlying backbone, CogVideoX. specifically, in scenarios involving high-dynamic or complex motion, such as fast-paced dancing or competitive sports, we observe that generation artifacts may remain pronounced. This suggests that while our method significantly improves structural alignment, the ultimate video quality in extreme motion cases relies on the generative capacity of the base model. 13 d o V y X i 2 X i C n n X i 2 Figure 9. Visual Comparison with State-of-the-Art Open-Source Models. Notably, despite HunyuanVid [16] possessing 13B parameters, our SAM2VideoX (5B) achieves comparable generation quality. Future Work. Our current pipeline relies on SAM 2 for video object segmentation and tracking. While the model demonstrates robust performance for single objects, we observe performance degradation in multi-object scenarios. Specifically, when prompted with visual cues (e.g., bounding boxes or points) for multiple distinct entities, the tracker struggles to maintain consistent identities across temporal sequences. Consequently, exploring effective feature representations for multi-object video generation remains an open and promising direction for future research. 14 Figure 10. Additional Qualitative Results. Our SAM2VideoX maintains robust structural consistency across diverse range of motion dynamics."
        }
    ],
    "affiliations": [
        "Adobe",
        "Georgia Tech",
        "HKUST",
        "University of Washington"
    ]
}