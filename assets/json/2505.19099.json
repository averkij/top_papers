{
    "paper_title": "SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics Reasoning",
    "authors": [
        "Kun Xiang",
        "Heng Li",
        "Terry Jingchen Zhang",
        "Yinya Huang",
        "Zirong Liu",
        "Peixin Qu",
        "Jixi He",
        "Jiaqi Chen",
        "Yu-Jie Yuan",
        "Jianhua Han",
        "Hang Xu",
        "Hanhui Li",
        "Mrinmaya Sachan",
        "Xiaodan Liang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present SeePhys, a large-scale multimodal benchmark for LLM reasoning grounded in physics questions ranging from middle school to PhD qualifying exams. The benchmark covers 7 fundamental domains spanning the physics discipline, incorporating 21 categories of highly heterogeneous diagrams. In contrast to prior works where visual elements mainly serve auxiliary purposes, our benchmark features a substantial proportion of vision-essential problems (75\\%) that mandate visual information extraction for correct solutions. Through extensive evaluation, we observe that even the most advanced visual reasoning models (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60\\% accuracy on our benchmark. These results reveal fundamental challenges in current large language models' visual understanding capabilities, particularly in: (i) establishing rigorous coupling between diagram interpretation and physics reasoning, and (ii) overcoming their persistent reliance on textual cues as cognitive shortcuts."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 9 9 0 9 1 . 5 0 5 2 : r SEEPHYS: Does Seeing Help Thinking? Benchmarking Vision-Based Physics Reasoning Kun Xiang 1, Heng Li 1, Terry Jingchen Zhang 2, Yinya Huang 2, Zirong Liu1, Peixin Qu1, Jixi He1, Jiaqi Chen4, Yu-Jie Yuan3, Jianhua Han3, Hang Xu3, Hanhui Li1, Mrinmaya Sachan2, Xiaodan Liang1 1Sun Yat-sen University 2ETH Zurich 3Huawei Noahs Ark Lab 4The University of Hong Kong"
        },
        {
            "title": "Abstract",
            "content": "We present SEEPHYS, large-scale multimodal benchmark for LLM reasoning grounded in physics questions ranging from middle school to PhD qualifying exams. The benchmark covers 7 fundamental domains spanning the physics discipline, incorporating 21 categories of highly heterogeneous diagrams. In contrast to prior works where visual elements mainly serve auxiliary purposes, our benchmark features substantial proportion of vision-essential problems (75%) that mandate visual information extraction for correct solutions. Through extensive evaluation, we observe that even the most advanced visual reasoning models (e.g., Gemini-2.5pro and o4-mini) achieve sub-60% accuracy on our benchmark. These results reveal fundamental challenges in current large language models visual understanding capabilities, particularly in: (i) establishing rigorous coupling between diagram interpretation and physics reasoning, and (ii) overcoming their persistent reliance on textual cues as cognitive shortcuts. Project Page: github.com/SeePhys/seephys-project Hugging Face: huggingface.co/datasets/SeePhys/SeePhys"
        },
        {
            "title": "Introduction",
            "content": "While mathematical reasoning has been cornerstone for evaluating the reasoning capability of large language models (LLMs) and multimodal large language models (MLLMs) [9, 15, 12, 19, 22, 6, 24, 5, 49, 41, 44], the realm of natural science, especially physics, remains understudied as much larger, even more diverse testbed for complex scientific reasoning. Physics reasoning inherently binds text explanations to real-world visual contexts, exposing critical gaps in their ability to emulate human-like world modeling [32, 45, 14, 11]. Frontier models have begun to demonstrate abstract perception of physical laws and logical reasoning capabilities [13, 28, 29, 10]. Due to the highly diverse structures of physics diagrams and their abstract representations of real-world scenarios, developing comprehensive benchmark for evaluating physics reasoning abilities and cross-modal understanding is crucial for enhancing LLMs. Early research primarily focused on assessing physical commonsense [4] and basic scientific knowledge [23], which was later gradually extended to competition-level and university-level physics problems [33, 11]. Due to the broad knowledge scope and high annotation difficulty inherent, some These authors contributed equally to this work. Corresponding author. Email: xdliang328@gmail.com Preprint. Figure 1: Overview of SEEPHYS. It encompasses 7 core physics domains and 21 diagram types, spanning the full knowledge spectrum from middle school to PhD qualifying exams levels. studies only considered test samples from limited knowledge levels [50, 14, 47]. Furthermore, other works targeted the evaluation of language models and did not incorporate visual information [32, 11, 45]. Notably, compared to the mathematics domain, there has been limited exploration of models perceptual differences in processing physics diagrams, despite their richer topological structures. With these challenges in mind, we introduce SEEPHYS, to measure LLMs capability to visually understand the law of physics. It is fully multimodal benchmark for evaluating physics reasoning across middle school to PhD qualifying exam levels. SEEPHYS comprises 2,000 rigorously validated questions collected from open-source textbooks, exercises, examinations, and competitions. These questions span 7 major fields of both classical and modern physics. To assess the extent to which different models rely on visual information for reasoning, we curate two subsets with different degrees of visual information enrichment and additionally compile supplementary copies of 2,000 purely visual instances where all problem statements in texts are presented in picture form. Through meticulous selection of 22 diagram types by domain experts, each problem challenges frontier MLLMs to integrate domain knowledge with visual understanding of physics diagrams (e.g., Feynman diagrams for particle interactions and Circuit diagrams for Electromagnetism). With SEEPHYS, we conduct extensive experiments to evaluate 28 leading LLMs and MLLMs such as o4-mini [29] and Gemini-2.5-Pro [10]. The results reveal that even with extensive chain-of-thought, none of the current models could surpass 55% accuracy. Our analysis reveals that even non-essential diagrams can enhance physics reasoning performance when presented to MLLMs. We have also raised challenge3 in the 2nd AI for Math Workshop at ICML 20254. Our main contributions are summarized as follows: We propose purely multimodal benchmark spanning multiple knowledge levels and domains. The meticulously curated benchmark comprises 2,000 rigorously annotated questions paired with 2,245 images. By prioritizing physics unique blend of observation and theoretical deduction, we assess how well models emulate the way human scientists observe, deduce, and understand complex natural phenomena. Our findings reveal significant gap in models capabilities to leverage multimodal information. We conduct comprehensive evaluation of current LLMs and MLLMs, followed by an in-depth analysis of their failure modes. Results demonstrate that even frontier models struggle with physics problems across different knowledge levels in various visual contexts. 3https://www.codabench.org/competitions/7925/ 4https://sites.google.com/view/ai4mathworkshopicml2025/ 2 Figure 2: Examples of Vision-Optional/Vision-Essential questions. In Vision-Optional samples, texts provide sufficient visual descriptions (e.g., graphical attributes and spatial relationships) to help respondents with illustration. In Essential samples, images contain indispensable problem-solving information, such as numerical values for key variables and unspecified topological structures."
        },
        {
            "title": "2 Related Work",
            "content": "Math reasoning benchmarks. Mathematical reasoning has emerged as central testbed for evaluating LLMs. Early benchmarks like GSM8K [9] established the foundation for multi-step textual reasoning through elementary problems, while MATH [15] advanced the field with competition-level tasks (e.g., AMC/AIME), exposing critical limitations of early models. As these benchmarks achieved saturation, the community shifted toward higher complexity, e.g., introducing Olympiad-level challenges requiring formal theorem proving and combinatorial reasoning [12, 19]. Concurrently, the rise of multimodal reasoning spurred benchmarks such as MathVista [22] and MATH-V [41] to integrate visual comprehension (e.g., diagrams, plots) with compositional reasoning. However, MathVerse [49] has found that MLLMs tend to rely on the reasoning capabilities of language models when performing mathematical tasks. In contrast, scientific diagrams, which are abstractions derived from real-world scenarios, with their complex visual features, may provide more effective testbed for benchmarking models multimodal capabilities. Physics benchmarks. Contemporary physics benchmarks target broad scope of domains, with frontier datasets now encompassing: (1) PhD-candidate-level theoretical problems, and (2) Olympiadstyle multi-step reasoning challenges requiring specialized physics intuition. Text-based physics benchmarks like PHYBench [32], TPBench [8], and UGPhysics [45] provide challenging problems that test advanced reasoning skills, yet fundamentally lack the visual components necessary to assess diagram interpretation abilities. Multimodal physics benchmarks such as PhysReason [50], OlympiadBench [14], and PHYSICS [11] emphasize visual reasoning challenges without analysis regarding the extent of visual components impact. Moreover, constrained by the high annotation costs stemming from the extensive domain knowledge required and the scarcity of qualified visual materials, these datasets lack comprehensive coverage across knowledge hierarchies and detailed annotations of diagram types. To address these limitations, this paper contributes dataset with multimodal and full-spectrum physics problems."
        },
        {
            "title": "3 SEEPHYS",
            "content": "3.1 Data Collection Principles The SEEPHYS benchmark aims to challenge current MLLMs from multimodal perspective, especially visual understanding and reasoning capabilities of physics diagrams. The data collection adheres to the following principles: Visual information as must. We observe that the diagrams in existing data sources can be paired with the questions in two ways: (1) diagrams that substantially dominate the reasoning and problemsolving process of the problem (Vision-Essential, VE); (2) diagrams that are act as supplement 3 Table 1: Comparison of SEEPHYS and related datasets in physics. Mid: Middle school; High: High school; Olympiad: Beginner/advanced Olympiad; UG: Undergraduate/senior undergraduate; MA: Masters; PhD: PhD qualifying exams."
        },
        {
            "title": "Size Mid High Olympiad UG MA PhD",
            "content": "UGPhysics [45] PHYSICS1[11] PHYBench [32] PhysReason1[50] TPBench [8] ScienceQA [23] OlympiadBench [14] SciBench [43] SciEval [36] MMMU [47] MMMU-Pro [48] GPQA [33] ARB [34] HLE [31] Physics Benchmarks 11,040 1,297 500 1,200 57 0 298 0 972 0 Physics Subset in Cross-domain Benchmarks 3,215 2,334 291 1,657 443 60 227 129 230 2,797 1,958 177 0 444 65 0 31 28 SEEPHYS (Ours) 2,245 1 Closed-source benchmarks. 2,000 to help with illustration, but visual information does not play the major role in the thinking process (Vision-Optional, VO). To analyze the performance variations of MLLMs across different types of visual information, the SEEPHYS benchmark implements rigorous categorization to distinguish them. We resort to experts in physics to annotate problem as Vision-Essential with all essential information contained in graphical AND textual data, and to annotate problem as Vision-Optional with all key information for problem-solving fully covered in text, as examples shown in Figure 2. Wide knowledge spectrum. To provide comprehensive evaluation of the models physics knowledge comprehension and modeling capabilities, questions are systematically selected across the following eight progressive knowledge levels: middle school, high school, beginner Olympiad problems, advanced Olympiad problems, undergraduate, senior undergraduate, masters, and PhD qualifying exams. Domain experts in physics are invited to selectively curate the most representative problems at each knowledge tier. Open-ended format without ambiguity. The data format in SEEPHYS is set to open-ended questions, each with one single definitive answer, which reduces random guessing raised in the multiple-choice question setting and thereby obtaining more accurate scores. Therefore, during data collection, those problems with ambiguous expressions and multiple explainable solutions were filtered out. Appendix shows the detailed instructions for the annotators. 3.2 Annotation Collection and pre-processing. We first collect more than 7000 PDF documents from publicly available textbooks, exercise problems, examinations, and competitions, as well as the International Physics Olympiad and Cambridge AS and A-Level Physics. Data sources are from international educational systems, including Eastern Asia, Europe, North America, and many others. The resulting questions are diverse and multilingual (Chinese, English). We then utilize Mathpix5 to perform OCR parsing on the collected documents and convert them into markdown text. Afterwards, we construct the (question, reasoning, answer) triples for each 5https://mathpix.com/ 4 question. GPT-4.1 [26] is employed to process redundant line breaks, string omissions, and LaTeX syntax errors. Finally, the trained annotators use LaTeX parser to conduct manual verification on the triples. Standardization. Some of the source questions contain multiple independent sub-questions, which do not align with most of the questions in our dataset. Therefore, we manually segment the sub-questions into distinct components and recombine them with shared question stems. For those multiple-choice source questions, all are converted into the SEEPHYS standard open-ended response formats. For some numerical computation problems with decimal points in answers (e.g., 10.1), we provide the corresponding significant figures (e.g., =3). This mitigates potential approximation-induced errors. To prevent conceptual errors and ambiguity, each question instance is cross-validated by two independent annotators. Fine-grained categorization. First, questions are coarsely classified into 7 subjects based on their domain, as directly provided by the data source. To further analyze LLMs sensitivity to different visual features, we introduce fine-grained classification of 21 diagram types. Through consultation with national curriculum standards and internal discussions with physics experts, we stratify all questions into 8 levels based on knowledge spectrum. Table in Appendix A.1 lists the subjects, diagram types, difficulty levels, and detailed statistics. Notably, Olympiad competition problems exhibit significant variance in difficultywe subdivide them into beginner/advanced Olympiad based on problem-solving durations. For undergraduate-level questions, we distinguish between standard undergraduate and senior undergraduate tiers according to whether they depend on mathematical physics methods. We then categorize the collected problems into Vision-Essential (75%) and VisionOptional (25%) according to their levels of visual information enrichment. Graphical components in VE problems contain analytically indispensable information, e.g., circuit structure, kinematic diagrams with labeled vectors, or scale-dependent data visualizations. In contrast, diagrams in VO cases provide supplementary but non-critical information, e.g., sketch of physics scenario. Data leakage prevention. To minimize the risk of data leakage, we eliminate samples with inconsistent responses by toggling the search function of GPT-4o [27] on and off. Inspired by Rein et al. [33], we subsequently conduct manual search for the remaining questions with correct responses. Multimodal enhancement. To further eliminate the influence of textual modality information, we first use o4-mini [29] to add caption field to each example, containing detailed description of geometric features and numerical information. We then augment the original 2,000 questions to obtain purely visual version. Specifically, we render each question along with its corresponding diagrams into single image up to 4,0964,096 pixels. During rendering, we introduce variations in font types and sizes while ensuring readability based on the diagrams dimensions. The introduction of purely visual QA not only enhances the authenticity of evaluation but also advances the models cross-modal understanding capabilitiesmirroring human cognition by extracting key features from pixels, associating abstract concepts, and ultimately achieving problem-solving accuracy comparable to that under text-image QA conditions. Appendix demonstrates the cases. 3.3 Data Analysis SEEPHYS comprises 2,000 distinct questions paired with 2,985 diagrams (averaging 1.49 images per question). These questions comprehensively span 8 knowledge levels, 21 types of diagram categories, and 7 key physics fields. The detailed statistics are shown in Appendix A."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Evaluation Protocol To guide the models in generating reasoning-augmented responses, we design zero-shot Chain-ofThought prompts among English and Chinese (Appendix D). To evaluate model performance across varying levels of visual information availability, we deploy four experimental settings: Text+Vision, TV: question with the paired diagrams, as our baseline setting. The results reflect the models ability to simultaneously comprehend visual elements and process textual information. 5 Text+Caption, TC: question with diagram caption. The results reflect the models capability to process textual information and reconstruct graphical information from text. Text Only, TO: Only question text is given. The results represent the models pure text processing capability without any visual input. Vision Only, VO: composite image rendered from question text and the paired diagrams. The results reflect the models ability to interpret diagram elements and extract visual form text. Among them, VO setting directly uses 2,000 purely visual instances, and the remaining three are based on 2,000 multimodal instances. We conduct experiments on both Vision-Essential/Vision-Optional subsets across all four settings. Composite judgment strategy. Recent advancements in the reasoning capabilities of LLMs enable them to discern key information within complex responses, thereby mitigating the inaccuracies often introduced by applying template matching to open-ended questions. Therefore, we further develop composite judgment strategy based on combination of LLM and template matching. As first step, the model generates response to the given input question and significant figures. Subsequently, the final answer is extracted through template matching and LLM-based processing. During the scoring process, SymPy6 is first utilized to perform an initial screening for straightforward answers. Responses that do not pass the screening are then compared with the ground-truth using LLM. We apply accuracy as the metric for this deterministic evaluation. In the experiments in this paper, we use DeepSeek-V3 [17] as the extraction and judge model. 4.2 Evaluation Models We conduct experiments with 8 NVIDIA A800 GPUs. To comprehensively evaluate the difficulty of SEEPHYS and the visual comprehension and reasoning capabilities of current AI systems, we employ series of mainstream closedand open-source models as baselines, including: 9 Large language models: DeepSeek-R1 [13], DeepSeek-V3 [17], Qwen3-235B-A22B [39], Qwen2.5-72B-Instruct [46], QwQ-32B [40], R1-Distilled-Llama-70B [13], Llama-4-Scout-17B [25], Gemma3-27B [37], Llama-3.1-8B [21]. We evaluate them with the TC setting. 19 Multimodal large language models: OpenAI o4-mini [29], OpenAI o3-mini [30], OpenAI o1 [28], Gemini-2.5-Pro [10], Claude 3.7 Sonnet [2], Doubao-1.5-pro [35], GPT-4.1 [26], GPT4o [27], QvQ-72B-preview [38], Qwen-VL series [3, 42], Llama-3.2-Vision series [20], LLaVANeXT-7B [18], Phi-4-multimodal [1], InternVL2.5-8B [7], LLaVA-OneVision-7B [16]. All these MLLMs are benchmarked with the TV/TC/TO/VO settings. Detailed introduction and implementations of each model are in Appendix D. 4.3 Performance across Differential Knowledge Levels Multimodal physics reasoning is challenging. Results in Table 2 demonstrate that SEEPHYS poses significant challenges to current mainstream models. Even state-of-the-art reasoning MLLMs (Gemini-2.5-Pro and o4-mini) achieve only under 55% accuracy, while other models, such as Doubao1.5-pro and Claude-3.7-Sonnet, attain merely 43.9% and 34.6% respectively. These results clearly indicate substantial room for improvement in the physics reasoning capabilities of popular models. surprising finding is that current LLMs demonstrate competitive performance, e.g., DeepSeek-R1 achieves an accuracy of 42.2%, which is comparable to the performance of o3-mini with multimodal inputs (40.3%). It suggests that the multimodal alignment capability of current MLLMs still has significant room for improvement. Diminishing returns of knowledge injection. Table 2 further illustrates performance disparities across models at varying knowledge levels. Contrary to expectations, the difficulty progression for models does not strictly follow the knowledge level (e.g., senior undergraduate and advanced Olympiad questions exhibit lower accuracy than PhD qualifying exams). This discrepancy suggests that current models primarily rely on knowledge memorization rather than truly learning the derivation patterns of scientific laws. Furthermore, by examining results from middle school to PhD, we observe that weaker models demonstrate substantially steeper performance reduction ratio (e.g., LLaVAOneVision-7B, -74.5%) compared to stronger ones (e.g., o4-mini, -19.9%). Not only indicates that 6https://www.sympy.org 6 Table 2: Accuracy (%) of different LLMs/MLLMs by knowledge level. Mid: Middle school; High: High school; BO: Beginner Olympiad; AO: Advanced Olympiad; UG: Undergraduate; SUG: Senior undergraduate; MA: Masters; PhD: PhD qualifying exams. The highest and second-highest scores in each section are bolded and underscored, respectively. Models Mid High BO AO UG SUG MA PhD Total Large Language Models DeepSeek-R1 [13] DeepSeek-V3 [17] R1-Distilled-Llama-70B [13] Qwen3-235B-A22B [39] Qwen2.5-72B-Inst [46] QwQ-32B [40] Llama-4-Scout-17B [25] Gemma3-27B [37] Llama-3.1-8B [21] 54.9 53.9 48.0 47.1 41.2 47.1 48.0 21.6 26.5 46.9 42.6 41.4 33.7 40.2 42.2 36.5 36.5 15.7 47.7 36.4 34.6 31.8 25.2 44.9 31.8 30.8 17. 31.9 22.8 14.2 20.4 8.2 15.5 11.3 5.1 3.9 49.9 45.4 31.5 41.2 26.8 40.0 28.5 23.1 7.6 o4-mini [29] o3-mini [30] o1 [28] Gemini-2.5-Pro [10] Claude-3.7-Sonnet [2] Doubao-1.5-pro [35] GPT-4.1 [26] GPT-4o [27] QVQ-72b-preview [38] Qwen2.5-VL-72B-Inst [3] Qwen2.5-VL-7B-Inst [3] Qwen2.5-VL-3B-Inst [3] Qwen2-VL-7B-Inst [42] Llama-3.2-90B-Vision [20] Llama3.2-11B-Vision [20] LLaVA-NeXT-7B [18] Phi-4-multimodal [1] InternVL2.5-8B [7] LLaVA-OneVision-7B [16] Multimodal Large Language Models 66.7 47.1 60.8 69.6 52.9 70.6 51.0 37.3 38.2 61.8 39.2 30.4 24.5 21.6 23.5 14.5 20.6 17.6 20.6 61.8 46.2 56.6 66.7 51.8 58.2 52.6 39.0 36.5 42.2 25.3 21.3 17.3 25.7 18.5 12.7 12.4 12.4 10. 56.1 39.3 50.5 64.5 43.0 49.5 41.1 34.6 30.8 29.0 21.5 13.1 14.0 22.4 14.0 11.2 12.1 9.3 12.1 41.8 28.3 32.5 46.7 16.7 29.2 17.0 7.5 11.3 10.4 4.2 2.9 4.4 3.9 4.2 5.5 4.4 2.9 2.7 53.8 47.0 54.4 64.2 41.4 56.6 39.7 23.4 25.9 29.9 8.7 10.4 8.5 9.3 5.4 13.2 7.0 5.6 5.4 34.2 29.7 16.0 25.1 12.8 20.1 14.2 9.1 3.7 45.7 36.1 40.6 50.2 26.5 34.7 31.1 15.5 14.2 14.6 5.9 7.3 4.6 10.0 3.7 8.2 5.0 3.2 2.3 49.0 35.9 28.9 31.7 18.6 32.4 28.3 15.2 10. 51.0 48.3 52.4 53.8 33.8 40.7 42.1 24.1 26.2 18.6 10.3 6.2 10.3 12.4 4.8 11.0 8.3 4.1 6.2 41.2 37.5 25.9 30.7 17.8 24.0 26.1 11.9 8.4 53.4 42.3 40.4 44.2 32.4 37.5 35.6 21.8 20.2 19.4 7.3 6.2 7.0 8.9 7.5 9.4 4.9 5.1 5.4 42.2 36.0 26.9 31.1 21.1 29.7 24.8 16.9 9.2 51.9 40.3 45.6 54.9 34.6 43.9 35.3 21.9 22.5 24.2 11.6 9.8 9.2 11.7 8.3 8.7 7.6 6.2 6.1 cutting-edge models fail to grasp fundamental principles underlying even simple physics concepts, but it also shows that knowledge injection has reached diminishing marginal returns. 4.4 Performance across Differential Visual Dependency Problems Does seeing help thinking? Impact of visual information on MLLM reasoning. Our experiment in Table 3 employs four distinct settings to evaluate model performance under varying vision availability. Firstly, all the models in the vision-essential subset demonstrate dependence on visual information, as evidenced by the high values of 1 and 2. Interestingly, even in the vision-optional subset where the images do not contain necessary information for solving problems, most models still exhibit performance improvements (2=29.5% in o3-mini and 56.1% in Claude-3.7-Sonnet). This may be because physics diagrams, even when their intrinsic structures can be inferred from given text, can assist models in understanding abstract concepts and modeling real-world scenarios. This fundamentally distinguishes SEEPHYS data from structurally simple mathematical geometric figures. To what extent do models utilize visual perception? Since the four settings assess the models ability to leverage vision and text (TV), pure text reasoning (TC and TO), and vision-text recognition (VO), we further analyze the degree of visual dependency across different models in the vision-essential subset. With the Vision Only setting, o4-mini demonstrates high accuracy, while QvQ-72B-preview shows relatively less reduction after removing textual information (3 = 7.3%), indicating that both graphical understanding and visual-text recognition contribute significantly to their reasoning. Furthermore, both o3-mini (78.4%) and Gemini-2.5-Pro (57.1%) exhibit very high 7 Table 3: Accuracy (%) of different MLLMs under varying levels of visual information enrichment (with relative performance gaps). TV: Text+Vision. TC: Text+Caption. TO: Text Only. VO: Vision Only. 1: (TV-TC)/TV. 2: (TV-TO)/TV. 3: (TV-VO)/TV. The highest and second-highest scores in each section are bolded and underscored, respectively. The highest and lowest are highlighted in green and red, respectively. Models TV TC TO VO Avg 1 2 3 Vision-Essential subset (75%) o4-mini [29] o3-mini [30] o1 [28] Gemini-2.5-Pro [10] Claude-3.7-Sonnet [2] Doubao-1.5-pro [35] GPT-4.1 [26] GPT-4o [27] QvQ-72B-preview [38] Qwen2.5-VL-72B [3] Qwen2.5-VL-7B [3] Qwen2.5-VL-3B [3] 46.5 32.9 38.5 49.0 27.9 39.0 29.2 17.1 16.5 18.0 9.6 6.8 40.5 15.3 32.0 40.3 22.8 30.1 26.5 15.9 15.3 16.4 7.7 6.8 29.9 16.3 23.7 32.0 12.3 24.1 18.5 10.1 11.6 12.1 5.7 6.9 35.7 7.1 23.9 21.0 20.2 23.9 20.4 12.7 15.3 9.3 5.2 3.3 38.2 17.9 29.5 35.6 20.8 29.3 23.7 14.0 14.7 13.0 7.1 7. Vision-Optional subset (25%) o4-mini [29] o3-mini [30] o1 [28] Gemini-2.5-Pro [10] Claude-3.7-Sonnet [2] Doubao-1.5-pro [35] GPT-4.1 [26] GPT-4o [27] QvQ-72B-preview [38] Qwen2.5-VL-72B [3] Qwen2.5-VL-7B [3] Qwen2.5-VL-3B [3] 68.4 62.4 67.0 72.4 53.8 68.8 53.6 36.4 40.6 42.8 17.4 18.8 68.0 33.6 64.2 64.4 47.6 64.8 54.0 40.0 38.2 40.0 16.2 15.2 66.7 44.0 60.8 68.6 23.6 63.2 54.2 35.4 37.1 38.6 16.2 14.2 58.4 12.8 49.8 39.6 41.6 41.4 28.2 24.0 38.2 22.0 9.8 8. 65.4 38.2 60.5 61.3 41.7 59.6 47.5 34.0 38.5 31.3 14.9 14.2 12.9 53.5 16.9 17.8 18.3 22.8 9.2 7.0 7.3 8.9 19.8 0.0 0.6 46.2 4.2 11.1 11.5 5.8 -0.7 -9.9 5.9 6.5 6.9 19.1 35.7 50.4 38.4 34.7 55.9 38.2 36.6 40.9 29.7 32.8 40.6 -1.5 2.5 29.5 9.3 5.2 56.1 8.1 -1.1 2.7 8.6 9.8 6.9 24.5 23.2 78.4 37.9 57.1 27.6 38.7 30.1 25.7 7.3 48.3 45.8 51. 14.6 79.5 25.7 45.3 22.7 39.8 47.4 34.1 5.9 48.6 43.7 55.3 3, meaning that when utilizing visual information, they heavily rely on textual information recognition (poor OCR ability). On the other hand, we observe that some models exhibit more pronounced performance improvements when captions replace images (GPT-4o/4.1), suggesting they process textual information more efficientlythey rely more heavily on language model capabilities. In summary, different models exhibit distinct dependencies on graphical topology, visual-text recognition, and textual information understanding, which likely stems from differences in their multimodal training emphases. Impact of diagram types on model performance. In Figure 3, we compare the performance of powerful reasoning model, o4-mini, with weaker open-source MLLM (Qwen2.5-VL-3B-Instruct) across different types of physics diagrams. With Text+Vision as baseline setting, even after excluding the maximum and minimum values, the accuracy range of o4-mini across various images remains widely dispersed (31.1%), indicating that the model may have specific effects on certain visual features. The significant gaps compared to the TO setting on Wave Motion, Circuit Diagram, and Coordinate System demonstrate that these diagram types specifically challenge models multimodal reasoning capabilities. Furthermore, different models exhibit distinct strengths in processing specific diagrams, e.g., Qwen performs better in Circuit Diagram than Quantum Mechanics, while o4mini shows the opposite preference pattern. Unlike o4-mini, the accuracy for Linear Motion and Photoelectric Effect shows significant improvement after removing visual inputs. This suggests that models with weaker multimodal perception capabilities may misinterpret visual information, leading to poorer reasoning outcomes than random guessing based on text alone. 8 Figure 3: The sensitivity of models to different diagram types under TV/TC/TO/VO settings. Figure 4: Examples of primary error patterns. Quantitative analyses are presented in Appendix E. 4.5 Failure Mode Analysis Through systematic analysis of o4-minis reasoning processes across 10% stratified samples, we identify four major error types: 1) Visual Misinterpretation: Persistent errors in extracting numerical values from coordinate plots, missing critical variables/symbols/units in graphical data, and flawed interpretation of geometric relationships. 2) Modeling Flaws: Fundamental misunderstandings in translating problem statements to physical models, including incorrect circuit schema, angular relationships in optics, and boundary conditions for dynamic systems. 3) Oversimplification: Neglect explicit constraints in logical deduction and omit critical computational steps. 4) False Assumptions: Introduction of extraneous conditions or mathematical constraints absent in original specifications, arbitrarily altering problem scope, which led to major divergence from problem statement. Notably, Visual Misinterpretation and Modeling Flaws reflects current MLLMs perceptual and utilization capabilities of multimodal information, respectively. In future work, greater emphasis should be placed on enhancing the models capacity for fine-grained parsing of complex images and rule-based modeling."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce SEEPHYS, pure multimodal benchmark for physics reasoning with 2,000 questions and 2,245 images across 8 knowledge levels, 7 core subjects, and 21 diagram types. We found frontier models fail to achieve accuracy beyond 55%, showing vast gap in current MLLMs physics reasoning and visual perception capability. Our limitation lies in the lack of automated evaluation as detailed in Appendix B. We have open-sourced our benchmark and code to advance the communitys progress in visual physics reasoning."
        },
        {
            "title": "References",
            "content": "[1] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. [2] Anthropic. Claude 3.7 sonnet system card. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Daniel Bear, Elias Wang, Damian Mrowca, Felix Jedidja Binder, Hsiao-Yu Tung, RT Pramod, Cameron Holdaway, Sirui Tao, Kevin A. Smith, Fan-Yun Sun, Li Fei-Fei, Nancy Kanwisher, Joshua B. Tenenbaum, Daniel LK Yamins, and Judith Fan. Physion: Evaluating physical prediction from vision in humans and machines. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. [5] Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 33133323. Association for Computational Linguistics, 2022. [6] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric P. Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pages 513523. Association for Computational Linguistics, 2021. [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [8] Daniel J. H. Chung, Zhiqi Gao, Yurii Kvasiuk, Tianyi Li, Moritz Münchmeyer, Maja Rudolph, Frederic Sala, and Sai Chaitanya Tadepalli. Theoretical physics benchmark (tpbench) dataset and study of ai reasoning capabilities in theoretical physics. arXiv preprint arXiv:2502.15815, 2025. [9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. [10] Deepmind. Gemini pro system card. [11] Kaiyue Feng, Yilun Zhao, Yixin Liu, Tianyu Yang, Chen Zhao, John Sous, and Arman Cohan. Physics: Benchmarking foundation models on university-level physics problem solving. arXiv preprint arXiv:2503.21821, 2025. [12] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, and et al. Omni-math: universal olympiad level mathematic benchmark for large language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. [13] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 10 [14] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [15] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. [16] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [17] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [18] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [19] Junqi Liu, Xiaohan Lin, Jonas Bayer, Yael Dillies, Weijie Jiang, Xiaodan Liang, Roman Soletskyi, Haiming Wang, Yunzhou Xie, Beibei Xiong, Zhengfeng Yang, Jujian Zhang, Lihong Zhi, Jia Li, and Zhengying Liu. Combibench: Benchmarking llm capability for combinatorial mathematics, 2025. [20] Meta llama Team. Introducing llama 3.1: Our most capable models to date. [21] Meta llama Team. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. [22] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. [23] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. [24] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. IconQA: new benchmark for abstract diagram understanding and visual language reasoning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. [25] meta-llama Team. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. [26] OpenAI. Gpt-4.1 system card. [27] OpenAI. Gpt-4o system card. [28] OpenAI. Openai o1 system card. [29] OpenAI. Openai o3 and o4-mini system card. [30] OpenAI. Openai o3-mini system card. 11 [31] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Daron Anderson, Tung Nguyen, Mobeen Mahmood, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Jessica P. Wang, Pawan Kumar, Oleksandr Pokutnyi, Robert Gerbicz, Serguei Popov, John-Clark Levin, Mstyslav Kazakov, Johannes Schmitt, Geoff Galgon, Alvaro Sanchez, Yongki Lee, Will Yeadon, Scott Sauers, Marc Roth, Chidozie Agu, Søren Riis, Fabian Giska, Saiteja Utpala, Zachary Giboney, Gashaw M. Goshu, Joan of Arc Xavier, Sarah-Jane Crowson, Mohinder Maheshbhai Naiya, Noah Burns, Lennart Finke, Zerui Cheng, Hyunwoo Park, Francesco Fournier-Facio, John Wydallis, Mark Nandor, Ankit Singh, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Darling Duclosel, Jungbae Nam, Jennifer Zampese, Ryan G. Hoerr, Aras Bacho, Gautier Abou Loume, Abdallah Galal, Hangrui Cao, Alexis C. Garretson, Damien Sileo, Qiuyu Ren, Doru Cojoc, Pavel Arkhipov, Usman Qazi, Lianghui Li, Sumeet Motwani, Christian Schröder de Witt, Edwin Taylor, Johannes Veith, Eric Singer, Taylor D. Hartman, Paolo Rissone, Jaehyeok Jin, Jack Wei Lun Shi, Chris G. Willcocks, Joshua Robinson, Aleksandar Mikov, Ameya Prabhu, Longke Tang, Xavier Alapont, Justine Leon Uro, Kevin Zhou, Emily de Oliveira Santos, Andrey Pupasov Maksimov, Edward Vendrow, Kengo Zenitani, Julien Guillod, Yuqi Li, Joshua Vendrow, Vladyslav Kuchkin, and Ng Ze-An. Humanitys last exam. CoRR, abs/2501.14249, 2025. [32] Shi Qiu, Shaoyang Guo, Zhuo-Yang Song, Yunbo Sun, Zeyu Cai, Jiashen Wei, Tianyu Luo, Yixuan Yin, Haoxu Zhang, Yi Hu, Chenyang Wang, Chencheng Tang, Haoling Chang, Qi Liu, Ziheng Zhou, Tianyu Zhang, Jingtian Zhang, Zhangyi Liu, Minghao Li, Yuku Zhang, Boxuan Jing, Xianqi Yin, Yutong Ren, Zizhuo Fu, Weike Wang, Xudong Tian, Anqi Lv, Laifu Man, Jianxiang Li, Feiyu Tao, Qihua Sun, Zhou Liang, Yushu Mu, Zhongxuan Li, Jing-Jun Zhang, Shutao Zhang, Xiaotian Li, Xingqi Xia, Jiawei Lin, Zheyu Shen, Jiahang Chen, Qiuhao Xiong, Binran Wang, Fengyuan Wang, Ziyang Ni, Bohan Zhang, Fan Cui, Changkun Shao, Qing-Hong Cao, Ming xing Luo, Muhan Zhang, and Hua Xing Zhu. Phybench: Holistic evaluation of physical perception and reasoning in large language models. arXiv preprint arXiv:2504.16074, 2025. [33] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [34] Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John Nay, Kshitij Gupta, and Aran Komatsuzaki. ARB: Advanced reasoning benchmark In The 3rd Workshop on Mathematical Reasoning and AI at for large language models. NeurIPS23, 2023. [35] ByteDance Seed. Doubao-1.5-pro system card. [36] Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. Scieval: multi-level large language model evaluation benchmark for scientific research. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors, Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 1905319061. AAAI Press, 2024. [37] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [38] Qwen Team. QvQ: To see the world with wisdom, December 2024. [39] Qwen Team. Qwen3, April 2025. [40] Qwen Team. QwQ-32B: Embracing the power of reinforcement learning, March 2025. [41] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. In 12 Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. [42] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [43] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. [44] Kun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Kaixin Cai, Yiyang Yin, Runhui Huang, Haoxiang Fan, Hanhui Li, Weiran Huang, et al. Can atomic step decomposition enhance the self-structured reasoning of multimodal large models? arXiv preprint arXiv:2503.06252, 2025. [45] Xin Xu, Qiyun Xu, Tong Xiao, Tianhao Chen, Yuchen Yan, Jiaxin Zhang, Shizhe Diao, Can Yang, and Yang Wang. Ugphysics: comprehensive benchmark for undergraduate physics reasoning with large language models. arXiv preprint arXiv:2502.00334, 2025. [46] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [47] Xiang Yue, Yuansheng Ni, Tianyu Zheng, Kai Zhang, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 95569567. IEEE, 2024. [48] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. CoRR, abs/2409.02813, 2024. [49] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, Peng Gao, and Hongsheng Li. MATHVERSE: does your multi-modal LLM truly see the diagrams in visual math problems? In Ales Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol, editors, Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part VIII, volume 15066 of Lecture Notes in Computer Science, pages 169186. Springer, 2024. [50] Xinyu Zhang, Yuxuan Dong, Yanrui Wu, Jiaxing Huang, Chengyou Jia, Basura Fernando, Mike Zheng Shou, Lingling Zhang, and Jun Liu. Physreason: comprehensive benchmark towards physics-based reasoning. arXiv preprint arXiv:2502.12054, 2025."
        },
        {
            "title": "Appendix",
            "content": "A Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.1 Attributes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A.2 Data Source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 A.3 Annotators Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 B.1 Process Reward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Low-Resource Evaluation Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.3 Connection between Theory and Real-World Scenarios . . . . . . . . . . . . . . . . . . . . . . . 19 Data Collection Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 C.1 Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 C.2 Standardization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 C.3 Categorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Multimodal Enhancement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 Experimental Settings. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 D.1 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 D.2 Environment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 D.3 Inference Template. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Failure Reasoning Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 E.1 Statistical Analysis of Failure Reasonings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 E.2 Case Studies of Failure Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Statistics",
            "content": "This section provides comprehensive quantitative and qualitative analyses of SEEPHYSs composition. All data reflects the final curated version after expert validation and de-duplication. Table 5 presents the statistical summary of the dataset. Our SEEPHYS comprises of 2,000 rigorously validated questions paired with 2,245 diagrams (averaging 1.12 images per question). The questions span 7 core physics fields and are stratified across 8 knowledge levels from middle school to PhD qualifying exams. Notably, 18.6% of problems target PhDlevel reasoning, while 22.6% represent advanced Olympiad challenges. The benchmark emphasizes multimodal reasoning: 75% of questions are VisionEssential, which necessarily requires diagram interpretation for solving (e.g., analyzing Feynman diagrams), while 25% are Vision-Optional, where visuals supplement text. Questions are language-balanced (1,039 English vs. 961 Chinese) and 88% have multistep reasoning annotations, validated via expert annotation. Visual diversity is ensured through 21 diagram types (e.g., circuit schematics, free-body diagrams), curated by domain experts. The datasets composition supports granular evaluation of MLLMs physics understanding across textual, visual, and reasoning dimensions. Figure 5: Statistics of our benchmark. Statistics"
        },
        {
            "title": "Total Questions\nTotal Images\nVisual Enhanced Samples",
            "content": "Subjects Diagram Types EN: CN Reasoning"
        },
        {
            "title": "Vision Enrichment Levels",
            "content": "Vision-essential Vision-optional Knowledge Levels Middle School High School Beginner Olympiad Advanced Olympiad Undergraduate Senior Undergraduate Master PhD Number 2,000 2,245 2,000 7 21 1039: 961 88% 75% 25% 5.1% 12.5% 5.4% 22.6% 17.8% 11.0% 7.3% 18.6% A.1 Attributes The following are the basic contents of the 7 covered subjects: Classical Mechanics (CM): The study of motion and forces on macroscopic objects, from linear motion, circular motion, projectile to planetary orbits. Electromagnetism (EM): Examines electric/magnetic fields and their interactions with matter, covering RC circuits to Maxwells equations. Astrophysics, Cosmology & Gravitation (ACG): Investigates celestial phenomena, universe evolution, and gravitational interactions at all scales. Optics (OPT): Focuses on light behavior (reflection/refraction) and its applications in lenses, lasers, and optical technologies, this section also covers wave-related physics of acoustics. Atomic, Molecular, Nuclear & Particle Physics (AMONP): Studies fundamental particles and their interactions, spanning quarks to complex nuclei. It also contains emergent properties of solids/liquids and novel material design. Quantum Mechanics, Information & Technology (QMIT): Explores quantum systems for computing and communication applications. Thermodynamics & Statistical Mechanics (TSM): Analyzes energy transfer and microscopic behavior of particle ensembles. The following are the basic contents of the 21 covered diagram types: Charge Distribution: Visualizes spatial arrangements of electric charges and their field effects. Feynman Diagram: Represents particle interactions through standardized symbolic notation in quantum field theory. Relativity and Gravity: Depicts spacetime curvature and relativistic effects near massive objects. Atomic Physics: Illustrates atomic energy levels, transitions, and spectral phenomena. 15 Figure 6: Cases of our SEEPHYS. 16 Figure 7: The Distribution of knowledge levels, subjects, diagram types, and vision enrichment levels. Static Force Analysis: Demonstrates equilibrium conditions through free-body diagrams and force vectors. Photoelectric Effect: Shows electron emission processes under photon irradiation with energy thresholds. Linear Motion: Characterizes one-dimensional kinematics with position-time/velocity-time graphs. Coordinate System: Provides reference frames for analyzing physical quantities in 2D/3D space. Astrophysics: Models celestial phenomena like stellar evolution or orbital mechanics. Spring Force: Displays Hookes law applications and oscillatory systems with restoring forces. Optical Path: Traces light propagation through media with reflection/refraction principles. Simple Harmonic Motion: Visualizes periodic motion through phase-space plots or pendulum dynamics. Quantum Mechanics: Represents wavefunctions, potential wells, and quantum superposition states. Circular Motion: Analyzes centripetal forces and angular kinematics in rotational systems. Thermodynamics: Charts thermodynamic cycles, heat engines, and entropy changes. Acoustics: Demonstrates sound wave propagation, interference, and standing wave patterns. Circuit Diagram: Standardized schematics for electrical networks with component symbols. Projectile Motion: Parabolic trajectories under uniform gravity with drag effects. Wave Diagram: Graphical representations of wavelength, frequency, and wave interference. Electromagnetic Field: Maps field lines and flux distributions in electric/magnetic systems. Capacitance Resistance: Characterizes RC circuits with charge/discharge time constants. Figure 6 shows samples of our SEEPHYS. Moreover, we present the distributions of knowledge levels, subjects, diagram types, and vision enrichment levels in Figure 7. A.2 Data Source We comprehensively collect visual Physics problems from existing public question repositories: Masters and PhD. We select questions with Masters and PhD qualifying exams level from Major American Universities Ph.D. Qualifying Questions and Solutions. This collection comprises problems from graduate-school entrance and qualifying examinations at seven major U.S. universities, spanning seven volumes: Mechanics, Electromagnetism, Optics, Atomic, Nuclear and Particle Physics, Thermodynamics and Statistical Physics, Quantum Mechanics, and Solid State Physics. The series is distinguished by its comprehensive coverage, with problems that span wide spectrum of topics within each area and frequently overlap multiple areas. These problems are notable for their 17 versatility in applying physical laws and principles to up-to-date, realistic situations, while often requiring minimal complex mathematical manipulation. They effectively blend the objectives of enhancing the understanding of physical principles with the ability for practical application. Undergraduate. This collection includes College Physics, General Physics, Theoretical Mechanics and Wave Optics. College Physics designs for students in science and engineering disciplines at general higher education institutions, offering broad coverage and combination of varying difficulty levels. General Physics covering wide spectrum of foundational university physics, this collection focuses on the analysis of problem-solving strategies and the application of fundamental methods, often presenting multiple solution approaches. Theoretical Mechanics covers all the teaching contents of theoretical mechanics and includes highly specific exercises, emphasizing the techniques for solving practical problems using general theorems and methods. Wave Optics presents problems related to wide scope of wave phenomena in optics, studied within the framework of the university course of general physics. Largely associated with visual-spatial perception. Olympiad competitions. The International Physics Olympiad (IPhO) is premier global competition featuring problems of exceptional difficulty and innovative conceptual design, spanning mechanics, electromagnetism, thermodynamics, optics, and modern physics. Its challenges emphasize multidimensional problem-solving, requiring participants to synthesize physical principles in non-traditional contexts, such as astrophysical systems or nanotechnology. Problems often test advanced mathematical techniques, including tensor analysis in continuum mechanics, and demand critical modeling skills, such as dimensional analysis or symmetry-based simplifications. The Chinese Physics Olympiad (CPhO), renowned for its theoretical rigor and computational intensity, integrates calculus deeply into physics problem-solving, employing methods like variational principles in constrained dynamics. Its multi-stage problems frequently involve layered complexities, for instance, incorporating relativistic corrections in electromagnetic boundary-value problems, under strict time constraints. The CPhOs quality aligns with the IPhO, with some problems exceeding its difficulty, making it one of Asias most demanding competitions. Middle and high school. Past examination papers from the Cambridge Assessment International Education for IGCSE Physics and AS & A-Level Physics constitute this source. The data quality is high, reflecting well-established and internationally recognized curriculum. These problems are characterized by their structured approach to assessing physics knowledge and understanding, ranging from fundamental concepts at the IGCSE level to more advanced topics in the AS & A-Level, with some questions incorporating elements of undergraduate-level content. The questions emphasize conceptual clarity, data interpretation, and the application of physics principles to varied contexts. A.3 Annotators Information For data annotation and evaluation, we recruit 7 annotators from engineering and physics programs, consisting of 4 undergraduates, 3 PhD candidates. All annotators demonstrated strong competencies in both secondary and tertiary-level physics through rigorous qualification assessments. One undergraduate student and one PhD candidate, both highly familiar with all knowledge levels covered by this benchmark, conduct professional secondary review of the annotation results. Since all annotators are coauthors of this study, they are sufficiently motivated to participate in the annotation task and do not require additional compensation or benefits. Furthermore, this research is dedicated to academic AI evaluation and does not involve recruiting human subjects, making it exempt from Institutional Review Board (IRB) regulations. As part of the institutional research activities, all data used in this work were obtained from publicly available and legally permissible sources, with no collection of private or protected sensitive demographic information."
        },
        {
            "title": "B Limitations",
            "content": "B.1 Process Reward Many current models are capable of generating responses that include intermediate explanatory steps, which may reflect their internal logical reasoning patterns. This is valuable, refined evaluation of LLM physics reasoning. However, due to the high cost of process annotation and the inherent uncertainty in evaluation (intermediate results can be expressed in multiple ways, and some problems may have multiple valid solutions), this study so far provides outcome-based reward signals. Future 18 work should focus on improving the reliability of process evaluation and integrating it with outcome accuracy to design comprehensive metric for assessing reasoning capabilities. B.2 Low-Resource Evaluation Method Although SymPy is partially employed for quick result matching, the evaluation pipeline in this work still primarily relies on LLMs to provide reward signals. It is because SEEPHYS encompasses diverse open-ended question types (e.g., computation, derivation, case-based analysis) with inherent uncertainty in model output formats. Only small fraction of responses could be directly verified using automated tools, resulting in resource-intensive evaluation process that hinders broader adoption in the research community. Future work should focus on designing more efficient and accurate rules or tools for assessing open-ended question answers. B.3 Connection between Theory and Real-World Scenarios The questions used in this benchmark are sourced exclusively from existing theoretical physics databases, covering primarily high-level concepts and principles in the physics discipline, with minimal inclusion of engineering-related problems (e.g., architecture, mechanical engineering, and biomechanics) or cross-modal sensory problems that better approximate real-world applications. Future research should further examine the relationship between models theoretical reasoning and its ability to model real-world phenomenareferred to as world modeling capability."
        },
        {
            "title": "C Data Collection Pipeline",
            "content": "C.1 Collection Our SEEPHYS benchmark aggregates educational materials (textbooks, exercises, exams, and contest problems) from globally distributed education systems, covering East Asian, European, North American, and other regional curricula. To preserve authentic multilingual evaluation, we retain all source languages without translation, maintaining 961:1039 Chinese-English text ratio. The corpus comprises 7,000+ PDF pages processed through Mathpixs OCR system to generate structured Markdown representations. Each acquired question must satisfy the following criteria: Vision Information Enrichment: For Vision-Essential subset, selected images should contain essential information for problem-solving. Diagrams or illustrations should be non-decorative and directly support the questions resolution. For Vision-Optional subset, images should not contain essential problem-solving information (e.g., numerical values) and should serve only as supplementary visual cues. Knowledge Spectrum: The content should cover topics ranging from middle school to PhD qualifying exam levels. Without Ambiguity: Only questions with definitive answers are included, while open-ended questions permitting multiple interpretations are excluded. Questions requiring explanatory answers longer than three sentences are discarded. Since the collected questions may contain grammatical or formatting errors after OCR processing, we employ the prompt to guide GPT-4.1 in performing preliminary linguistic correction (Figure 9). C.2 Standardization Many source materials (particularly textbook exercises and competition problems) contain compound questions comprising multiple independent sub-questions (e.g., \"Prove and then calculate Y\"). So we systematic decomposition of compound questions into atomic units and then reconstruct them with shared contextual elements when logically dependent. It ensures each question in our dataset represents single, self-contained cognitive task while preserving original problem relationships through metadata tagging. To modify multi-choice questions to open-ended questions, we develop stem rewriting to remove choice-specific references (e.g., changing \"Which of the following\" to 19 Figure 8: Overview of the data collection pipeline. \"Determine\"). For computational problems, we address significant figures annotation based on problem constraints. This approach reduces false negatives in automated scoring while accommodating legitimate solution variants. To prevent data leakage, we implemented dual-phase verification protocol: 1) Systematically use and disabling GPT-4os web search functionality via API parameters to eliminate questions exhibiting accuracy fluctuations. 2) Manual Google verification of all correctly answered items. Our two-phase validation protocol ensures conceptual integrity: Primary annotation by domain experts. Cross-validation by secondary annotators. When the two annotators disagree in their judgment regarding physics concepts, third arbitrator holding PhD in physics is engaged to conduct the final review. Continuous validation sampling (10% of processed questions) throughout dataset development C.3 Categorization As all source materials originate from discipline-specific examinations, we initially classify questions into 7 broad thematic categories based on their subject matter. To analyze LLMs sensitivity to visual elements, we implement fine-grained classification system comprising 21 distinct diagram types. Notably, coordinate systems are treated as composite categories, as they may incorporate multiple graphical components across different subject domains. Through comparative analysis of international curricula standards and expert deliberation, we establish an 8-tier knowledge hierarchy. Olympiad competition problems are split into beginner and advanced tiers based on average accuracy rates, while undergraduate-level questions are divided into undergraduate (non-mathematical physics) and senior undergraduate (mathematical physics) categories. C.4 Multimodal Enhancement We also provide pure multimodal subset containing 2,000 composite image examples. Each example consists of single image integrating both textual and graphical elements. We first generate detailed captions for each sample using o4-mini, which include comprehensive descriptions of geometric features and numerical data through the prompt template shown in Figure 10. Subsequently, we render each question with its corresponding diagram into composite image under 40964096 pixels resolution. The rendering process incorporates varied font types and sizes, while dynamically adjusting text-to-diagram spacing based on each charts maximum dimensions to ensure optimal layout compactness. Cases are shown in Figure 11."
        },
        {
            "title": "D Experimental Settings",
            "content": "D.1 Models In our experiments, we evaluate the performance of several state-of-the-art and representative LLMs/MLLMs. For LLMs, we provide text-based prompts in the form of \"question + caption\" to guide the models in generating answers. For MLLMs, general-purpose models capable of processing interleaved image-text sequences are tested on the full benchmark. For most open-source models, we use the hyperparameter torch.dtype=torch.float16. We set temperature=0, with maximum token limit of 8192 and maximum image resolution of 40964096 pixels. Additionally, for other 20 Figure 9: Instruction for OCR Text Enhancement with GPT-4.1. parameter configurations, we generally follow the settings provided in the original papers, their code repositories, or Hugging Faces example configurations. The language models used in this study are briefly described as follows: DeepSeek-R1: It is based on four-stage training process incorporating Supervised Fine-Tuning and Reinforcement Learning. Despite utilizing only minimal annotated data, it significantly enhances the models reasoning capabilities. In tasks such as mathematics, coding, and natural language reasoning, the model, with 670 billion parameters, achieves performance comparable to OpenAIs o1 official version. DeepSeek-V3: It is powerful Mixture of Experts (MoE) language model, activating approximately 37 billion parameters per token. DeepSeek-V3 pioneers an auxiliary-loss-free load balancing strategy and incorporates multi-token prediction training objective to achieve enhanced performance. Qwen3-235B-A22B: The model has 235 billion parameters, activates 22 billion parameters per inference, and consists of 128 experts, with 8 activated during each forward pass. This design significantly enhances computational efficiency and scalability while maintaining high performance. Qwen2.5-72B-Instruct: This is dense, decoder-only language model pre-trained on dataset of up to 18 trillion tokens. Qwen2.5-72B-Instruct supports context lengths of up to 128K tokens and can generate content with maximum length of 8K tokens. 21 Figure 10: Instruction for Diagram Caption with o4-mini. Figure 11: Cases of pure multimodal subset. QwQ-32B: QwQ-32B employs reinforcement learning techniques, supporting the visualization of the models reasoning process and context length of 131,072 tokens. It is capable of solving advanced mathematical problems, including algebra, geometry, calculus, and more. R1-Distilled-Llama-70B: Built upon the Llama-3.3-70B-Instruct model, it has been meticulously fine-tuned using DeepSeek R1s outputs, enabling outstanding performance across multiple benchmarks. While maintaining low costs, its capabilities rival those of larger, cuttingedge models. Llama-4-Scout-17B: This is the latest general-purpose multimodal model in the Llama series, featuring 16 expert modules, 17 billion active parameters, and total of 109 billion parameters. Gemma3-27B: It is developed by the Google DeepMind team and incorporates several enhancements based on Gemma 2, including the addition of visual comprehension capabilities, support for more languages, and the ability to process contexts of up to 128K tokens. Llama-3.1-8B: The Llama 3.1 model features 128K context length and is optimized for scenarios with limited computational resources. The multimodal language models used in this study are briefly described as follows: 22 OpenAI o4-mini: OpenAI o4-mini is compact model optimized for fast, cost-efficient inference. Despite its reduced size and lower cost, it delivers exceptional performance in math, coding, and vision tasks, while maintaining high throughput. OpenAI o3-mini: The o3-mini demonstrates exceptional performance in STEM reasoning tasks. It achieves comparable results to the o1 model in mathematics, programming, and scientific tasks with significantly faster response times. OpenAI o1: o1 is cutting-edge model released by OpenAI specifically designed for complex reasoning tasks, trained using reinforcement learning. The model is capable of engaging in prolonged deliberation before providing answers, and its performance empirically validates the existence of test-time scaling laws. Gemini-2.5-Pro: It is hybrid reasoning model proposed by Google DeepMind, supporting native multimodal capabilities and 1 million token context window, achieving significant advancements in coding, reasoning, and multimodal tasks. In this paper, we use Gemini-2.5Pro-Exp-03-25. Claude 3.7 Sonnet: It is Anthropics most advanced large language model and the first to combine multiple reasoning approaches. Claude 3.7 Sonnet can both provide quick answers and engage in deeper, step-by-step thinkingwith the entire reasoning process visible to users. Doubao-1.5-pro: It adapts sparse MoE architecture, maintaining training-inference codesign approach from the pre-training phase. With only small fraction of activated parameters, it outperforms massive dense pre-trained models like Llama3.1-405B. GPT-4.1: The model comprehensively surpasses GPT-4o and GPT-4o mini in coding, instruction following, and long-context understanding, while being more cost-effective, faster, and capable of processing contexts up to 1 million tokens. GPT-4o: This model is trained on text, visual, and audio data. Its unified approach ensures that all inputswhether text, images, or soundcan be processed simultaneously by single neural network. QvQ-72B-preview: It is an open-source multimodal reasoning model developed by Qwen team, with special focus on enhancing visual reasoning capabilities. It supports extracting precise information (e.g., object height, quantity) from images and can interpret the deeper meaning behind pictures. Qwen-VL series: The Qwen2-VL series models employ three-stage fine-tuning approach to sequentially train different modules. It utilizes naive dynamic resolution mechanism and multimodal rotary position embedding to effectively fuse information from text, images, and videos of varying scales. Qwen2.5-VL series implements window attention, which boosts both training and inference speeds while significantly enhancing general image recognition capabilities. Llama-3.2-Vision series: Llama 3.2-Vision series is collection of pre-trained and finetuning vision-language models that support text + image inputs with text-only outputs, featuring 128K context length. LLaVA-NeXT-7B: This model is designed to improve image-text interaction capabilities, particularly in OCR (Optical Character Recognition) and commonsense reasoning. It employs Vicuna-7B as its language model and significantly boosts visual reasoning performance through dynamic high-resolution input processing and an optimized visual instruction-tuning dataset. Phi-4-multimodal: Phi-4-Multimodal is 5.6B-parameter multimodal model that integrates text, visual, and speech/audio input modalities. It employs an modality expansion approach, utilizing LoRA adapters and modality-specific routers to enable interference-free combination of diverse modalities during inference. InternVL2.5-8B: InternVL2.5-8B integrates the pre-trained InternViT-300M vision backbone with large language models (InternLM 2.5) through randomly initialized 2-layer MLP projector. The model additionally introduces native support for high-resolution multi-image inputs. LLaVA-OneVision-7B: It adopts Qwen-2 as its LLM backbone and SigLIP as the visual encoder, with the two modules connected via parameterized 2-layer MLP. This architecture achieves state-of-the-art performance for open-source multimodal large models across single-image, multi-image, and video tasks. D.2 Environment We deploy advanced reasoning models with computational infrastructure. we use Linux-based environment equipped with CUDA-enabled GPUs (8 * 80G NVIDIA A800) to accelerate tensor operations. The software stack includes PyTorch 2.5.1 with CUDA 12.4 support, alongside Python 3.10 for compatibility with modern machine learning libraries. For all the models, half-precision (FP16) quantization is enabled to optimize runtime. D.3 Inference Template During the experiments, we design efficient Chain-of-Thought (CoT) templates to enhance the models reasoning capabilities. Given that physics problems often involve approximate calculations, we incorporate significant figure hints in the input. As shown in the Figure 12, we provide customized prompts in both English and Chinese to accommodate different linguistic contexts. Figure 12: English/Chinese template for inference. D.4 Evaluation During the evaluation phase, we integrate automated verification with LLM-as-judge method to generate comprehensive reward signals. The assessment pipeline first leverages SymPy for rapid mathematical matching between model responses and ground truth. Samples failing this validation are then subjected to secondary scoring by LLM, ensuring robust evaluation coverage across all response types. Given the inherent complexity of physics reasoning tasks, the LLMs judging process is implemented as two-stage pipeline consisting of answer extraction followed by scoring. The first stage involves guiding the model to extract clean answers by removing extraneous characters, identifying numerical values and units, and handling cases with multiple valid answers. The second stage requires model to perform precise unit conversions and recognize various mathematically equivalent expressions when applying the scoring criteria. We calibrate these pipeline using carefully designed few-shot prompts as illustrated in Figure 13 and Figure 14. Through manual verification of 200 samples, the DeepSeek-V3 model demonstrates reliable judging capability with an error rate below 5%, validating the robustness of this evaluation methodology for complex physics reasoning tasks. 24 Figure 13: Prompt for answer extracting. Table 4: Error patterns comparison of o4-mini, Gemini-2.5-Pro and Qwen2.5-VL-3B. We identify the following error patterns in the models outputs: VM: Visual Misinterpretation; TM: Text Misinterpretation; MF: Modeling Flaws; FA: False Assumption; NM: Numerical Miscalculations; OS: Oversimplification; SM: Summarization Mistakes; OT: Overthinking; RO: Repetitive Output"
        },
        {
            "title": "SM OT RO",
            "content": "o4-mini [29] Gemini-2.5-Pro [10] Qwen2.5-VL-3B [3] 15 17 11 1 2 0 61 49 48 8 13 8 3 3 6 0 4 3 4 3 3 12 5 0"
        },
        {
            "title": "E Failure Reasoning Analysis",
            "content": "E.1 Statistical Analysis of Failure Reasonings To provide the community with quantitative error analysis results, we conduct manual inspection of 100 error samples common to o4-mini, Gemini-2.5-Pro, and Qwen2.5-VL-3B. We shows 9 different error patterns in Table 4. First, all three models exhibit significant Modeling Flaws (e.g., incorrect theorem applications and formula misuse), while demonstrating relatively fewer Text Misinterpretation and Numerical Miscalculation errors. This suggests that even weaker models have acquired fundamental text recognition and numerical computation capabilities, yet still show substantial gap in applying principles of physics. Second, Visual Misinterpretation emerged as the second most frequent error pattern, indicating persistent weaknesses in multimodal comprehension. Error frequencies for Overthinking and Oversimplification show notable variation across models. Particularly noteworthy is Qwen2.5-VL-3Bs high rate of Repetitive Output (21%), which is absent in the cutting-edge proprietary models. We attribute this to the models limited 3B parameter scale, which likely constrains its instruction-following capacity. E.2 Case Studies of Failure Patterns We also present concrete case studies illustrating common error patterns observed in o4-mini outputs: 25 Figure 14: Prompt for answer scoring. 26 Figure 15: Failure output on Visual Misinterpretation. 27 Figure 16: Failure output on Visual Misinterpretation. 28 Figure 17: Failure output on Visual Misinterpretation. 29 Figure 18: Failure output on Visual Misinterpretation. 30 Figure 19: Failure output on Visual Misinterpretation. 31 Figure 20: Failure output on Modeling Flaws. 32 Figure 21: Failure output on Modeling Flaws. 33 Figure 22: Failure output on Modeling Flaws. 34 Figure 23: Failure output on Modeling Flaws. 35 Figure 24: Failure output on Modeling Flaws. 36 Figure 25: Failure output on Modeling Flaws. 37 Figure 26: Failure output on Oversimplification. 38 Figure 27: Failure output on Oversimplification. 39 Figure 28: Failure output on Oversimplification. 40 Figure 29: Failure output on Oversimplification. 41 Figure 30: Failure output on Oversimplification. 42 Figure 31: Failure output on Oversimplification. 43 Figure 32: Failure output on False Assumption. 44 Figure 33: Failure output on False Assumption. 45 Figure 34: Failure output on False Assumption."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "Huawei Noahs Ark Lab",
        "Sun Yat-sen University",
        "The University of Hong Kong"
    ]
}