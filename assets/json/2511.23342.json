{
    "paper_title": "Flow Straighter and Faster: Efficient One-Step Generative Modeling via MeanFlow on Rectified Trajectories",
    "authors": [
        "Xinxi Zhang",
        "Shiwei Tan",
        "Quang Nguyen",
        "Quan Dao",
        "Ligong Han",
        "Xiaoxiao He",
        "Tunyu Zhang",
        "Alen Mrdovic",
        "Dimitris Metaxas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Flow-based generative models have recently demonstrated strong performance, yet sampling typically relies on expensive numerical integration of ordinary differential equations (ODEs). Rectified Flow enables one-step sampling by learning nearly straight probability paths, but achieving such straightness requires multiple computationally intensive reflow iterations. MeanFlow achieves one-step generation by directly modeling the average velocity over time; however, when trained on highly curved flows, it suffers from slow convergence and noisy supervision. To address these limitations, we propose Rectified MeanFlow, a framework that models the mean velocity field along the rectified trajectory using only a single reflow step. This eliminates the need for perfectly straightened trajectories while enabling efficient training. Furthermore, we introduce a simple yet effective truncation heuristic that aims to reduce residual curvature and further improve performance. Extensive experiments on ImageNet at 64, 256, and 512 resolutions show that Re-MeanFlow consistently outperforms prior one-step flow distillation and Rectified Flow methods in both sample quality and training efficiency. Code is available at https://github.com/Xinxi-Zhang/Re-MeanFlow."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 2 4 3 3 2 . 1 1 5 2 : r Flow Straighter and Faster: Efficient One-Step Generative Modeling via MeanFlow on Rectified Trajectories Xinxi Zhang1 Shiwei Tan1 Quang Nguyen1 Quan Dao1 Ligong Han2 Xiaoxiao He1 Tunyu Zhang1 Alen Mrdovic1 Dimitris Metaxas1 1 Rutgers University 2 Red Hat AI Innovation"
        },
        {
            "title": "Abstract",
            "content": "Flow-based generative models have recently demonstrated strong performance, yet sampling typically relies on expensive numerical integration of ordinary differential equations (ODEs). Rectified Flow enables one-step sampling by learning nearly straight probability paths, but achieving such straightness requires multiple computationally intensive reflow iterations. MeanFlow achieves one-step generation by directly modeling the average velocity over time; however, when trained on highly curved flows, it suffers from slow convergence and noisy supervision. To address these limitations, we propose Rectified MeanFlow, framework that models the mean velocity field along the rectified trajectory using only single reflow step. This eliminates the need for perfectly straightened trajectories while enabling efficient training. Furthermore, we introduce simple yet effective truncation heuristic that aims to reduce residual curvature and further improve performance. Extensive experiments on ImageNet at 642, 2562, and 5122 resolutions show that Re-MeanFlow consistently outperforms prior one-step flow distillation and Rectified Flow methods in both sample quality and training efficiency. Code is available at Code is available at https://github.com/Xinxi-Zhang/Re-MeanFlow."
        },
        {
            "title": "Introduction",
            "content": "Flow models [27, 28] and diffusion models [43, 40] have become central paradigm in generative modeling, enabling wide range of applications across various data domains [18, 36, 55, 13, 14, 48]. Compared with earlier paradigms such as GANs [12, 20] and Normalizing Flows [35, 52], these models offer stable training and superior fidelity, but at the cost of expensive sampling. The root cause of this inefficiency is the curvature of the generative trajectories induced by the prior and data distributions. In practice, the velocity fields governing these flows bend sharply, making them difficult to approximate few discretization steps. Rectified Flow [28, 24] addresses the multi-step sampling cost of flow models by progressively straightening trajectories through an iterative reflow process. In principle, if the trajectory Equal contribution Figure 1: Re-Meanflow (Ours) leverages the synergy of trajectory rectification and meanvelocity modeling, achieving the best computequality trade-offreaching strong FID. This synergy yields efficiency and quality that neither rectification nor mean-velocity modeling can achieve alone. All methods are initialized with pretrained EDM2-S [23]. becomes perfectly straight, one-step sampling is achievable. However, Liu et al. [28] theoretically show that achieving fully straight trajectory requires infinitely many reflow iterations. More recently, Lee et al. [24] argue that single reflow step is sufficient in practice, under the assumption that the intermediate flow mapping is approximately L-Lipschitz and that edge cases leading to curved trajectories are exceedingly rare. Although this assumption may hold for most data-noise couplings, we find that in practice noticeable edge cases still exist; for these cases, performing Euler one-step sampling on such curved paths will generate invalid samples. In contrast, MeanFlow [11] completely bypasses ODE integration by directly modeling the mean velocity field over time, eliminating the need for perfectly straight paths and achieving strong singlestep performance. However, for highly curved trajectories, MeanFlow suffers from unstable and noisy supervision signals, making the training slow and computationally expensive. To address these limitations, we propose Re-Meanflow, short for Rectified MeanFlow, conceptually simple yet training-effective framework that integrates the MeanFlow objective into the Rectified-flow framework. Specifically, we learn the mean velocity field along the rectified trajectory obtained after single reflow step. This bypasses the need for perfectly straight trajectories while making MeanFlow training significantly more efficient and stable, as the rectified trajectory exhibits substantially lower variance than the original highly curved flow. Additionally, we introduce simple and surprisingly effective truncation heuristic that reduces the influence of extreme curvature cases during training. Instead of attempting to explicitly detect curvature, we discard the top 10% couplings ranked by ℓ2 trajectory distance. While this heuristic does not guarantee that all high-curvature trajectories are removed, we hypothesize that large distances often correlate with more curved paths, and applying this filter consistently improves both training stability and sample quality in practice. Extensive experiments on ImageNet at 642, 2562, and 5122, demonstrating that Re-Meanflow consistently outperforms both state-of-the-art distillation methods and strong train-from-scratch baselines in terms of both generation quality and training efficiency. In particular, compared to its closely related counterpart, 2-rectified flow++ [24], Re-Meanflow reduces FID by 33.4% while requiring only 10% of the total compute. Beyond empirical gains, Re-Meanflow introduces practical shift in how few-step generative models can be trained. Existing distillation pipelines depend heavily on high-end training GPUs, making hyperparameter tuning and repeated runs prohibitively expensive. In contrast, Re-Meanflow offloads the majority of computation to the inference-driven reflow stage, which can be executed on widely available consumeror inference-grade accelerators, and requires only lightweight MeanFlow training phase. The training stage of Re-Meanflow accounts for merely 17% of the total GPU hours used by AYF[37]."
        },
        {
            "title": "2 Related Works",
            "content": "Diffusion and Flow Matching. Diffusion/flow-based generative models [40, 43, 17, 44, 27, 1, 28] learn to reverse gradual noising process, where the reverse-time dynamics can be formulated as deterministic probability-flow ODE [44, 21]. Although these approaches achieve strong performance, they typically require multi-step numerical integration for sampling [41, 21, 30, 53] due to the high curvature of generative paths. Few-step Diffusion/Flow Models. Rectified Flow methods [28, 46, 24] alleviate this by explicitly learning straighter trajectories that enable one-step or few-step sampling, but such straight-path construction itself incurs high computational cost due to heavy training and repeated reflow procedures. Consistency Models [45, 42, 10, 29, 49] train networks to produce invariant outputs across different timesteps, enabling direct few-step or even one-step sampling. Flow Map Models [5, 9, 11] bypass ODE integration by learning the displacement or velocity map over time. Despite their strong empirical results, these few-step paradigms often face stability challenges or high training cost because they must learn mappings along inherently curved trajectories, where supervision is noisy, and optimization is difficult. Recent work has sought to mitigate this issue by simplifying the consistency objective [29] or introducing improved loss functions and normalization strategies[6]. Concurrent work CMT [19] aims to improve the efficiency of few-step models by supervising on learned ODE trajectories. Building on these insights, this work aims to combine trajectory simplification with one-step modeling. Efficient Training for Diffusion/Flow Models. Many recent works [51, 26, 50, 47] boost the training efficiency of diffusion models by leveraging external representation learners like DINO 2 ) straightens the paths and reduces NFEs, but one-step sampling ( Figure 2: Why trajectory rectification and mean-velocity modeling reinforce each other. (a) ) still follows highly curved trajectories, requiring many ODE steps. Applying 1-rectified flow ( two rounds of rectification ( ) remains unreliable unless trajectories are nearly straight. (b) MeanFlow estimates the average velocity u(zt, r, t) over all intervals (r, t), which in principle avoids the need for straight paths. However, when the underlying velocity field is highly curved, the induced averages become complex and hard to learn. (c) Training MeanFlow on trajectories from 2-rectified flow yields significantly smoother average-velocity field, making estimation easier and enabling faster convergence and high-quality one-step generation. [33]. Orthogonal works [56, 38] adopt masked-transformer designs that exploit spatial redundancy by training on only subset of tokens. ECT [10] enables efficient consistency models through progressive training strategy that transitions from diffusion to consistency training. Re-Meanflow contributes in this direction by training MeanFlow model on significantly simplified flow path."
        },
        {
            "title": "3 Preliminaries",
            "content": "Rectified Flow[28]. Given prior distribution pz (taken as (0, I) in this work) and data distribution px, Rectified Flow trains flow model vθ that transports pz to px via an ordinary differential equation (ODE) over time [0, 1]. For data-noise coupling (x, z) pxz, where pxz denotes the joint distribution of and z, the flow path is defined as linear interpolation zt = (1 t)x + tz. The flow model vθ is trained by regressing the conditional velocity of observed couplings: LRE(θ) = Ex,zpxz, tpt (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) dt zt vθ(zt, t) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:35) with dt zt = Once vθ is learned, new sample can be generated by solving the ODE for pz: xθ(z) = z0 = (cid:90) 1 0 vθ(zτ , τ )dτ. (1) (2) Rectified Flow begins training with independently sampled and z, i.e., p0 xz(x, z) = px(x)pz(z). Training on these independent couplings produces highly curved ODE trajectories, requiring large number of function evaluations (NFEs) to accurately solve Eq. 2. To mitigate this, Rectified Flow adopts recursive reflow process to iteratively straighten the ODE trajectories. Given learned velocity field vk xz, which is sampled by the previous velocity field vk θ . For clarity, we refer to the trained vector field vk+1 θ MeanFlow [11]. Compared to Rectified Flow [28], which models the instantaneous velocity and requires solving the numerical ODE in Eq. 2 during sampling, MeanFlow enables one-step sampling by modeling the average velocity between two time points and r: xz , straighter velocity field vk+1 can be obtained by training on pk as the (k+1)-rectified flow. θ trained on pk1 θ u(zt, r, t) 1 (cid:90) v(zτ , τ )dτ. 3 (3) Figure 3: Class-imbalanced 2D Toy Example. We consider controlled 2D setup where flow model transports balanced two-component Gaussian mixture on the left to an imbalanced mixture on the right (weights 0.4 for the upper mode and 0.6 for the lower mode). Panels (a-c): (a) Linear interpolation of independently sampled couplings px pz, which serve as the training signal for the first velocity model. (b) The resulting 1-rectified flow learned from these independent couplings; the learned velocity field remains noticeably curved. (c) Using the velocity field from (b), we generate new set of couplings and train second velocity model on their linear interpolations, yielding the 2-rectified flow. Panel (d): Due to imperfect straightening, one-step Euler sampling on the 2-rectified flow still yields noticeable outliers. Panel (e): MeanFlow trained directly on independent couplings fails to converge within the training budget because high-variance conditional velocities destabilize learning. Panel (f): Re-Meanflow combines trajectory rectification with MeanFlow, eliminating most outliers and achieving more accurate one-step generation. To train neural network uθ(xt, r, t) to approximate this average velocity, MeanFlow derives an implicit training target linking the average velocity, the instantaneous velocity v(xt, t), and the time derivative of uθ: LM (θ) = Ex,z,r,t uθ(zt, r, t) sg(utgt)2 2 with utgt = v(zt, t) (t r) dt uθ(zt, r, t). (4) (5) Here, sg() denotes the stop-gradient operator, and the derivative dt u(zt, r, t) can be computed using the Jacobian-vector product (JVP) interface in frameworks such as PyTorch or JAX. once uθ is trained, the entire flow path can be reconstructed from single evaluation uθ(z, 0, 1) without numerical integration."
        },
        {
            "title": "4 Methods",
            "content": "To motivate Re-Meanflow, we first examine the limitations of existing rectified-flow and MeanFlow approaches and illustrate why combining them yields substantial synergy with toy example. 2D Toy Example Setting. Figure 3 visualizes simple transport task from balanced two-mode Gaussian mixture to an imbalanced two-mode mixture. We compare one-step generation performance across three approaches under fixed training budget of 20k iterations. We evaluate: 1. 2-rectified flow, where each reflow iteration is trained for 10k steps; 2. MeanFlow, trained for 20k steps directly on independently sampled couplings; 3. Re-Meanflow (ours), which first trains velocity model for 10k steps to obtain 1-rectified flow, followed by 10k steps of MeanFlow training on the resulting couplings. More Detail of the toy example setting are provided in Appendix B.5. 4 4.1 Limitations of Rectified flow and MeanFlow Rectified Flow Leaves Residual Curvature Degrading One-Step Sampling. Rectified Flow seeks to enable one-step sampling by progressively straightening ODE trajectories. However, Liu et al. [28] show that achieving perfectly straight trajectory theoretically requires applying an infinite number of reflow iterations. Later, Lee et al. [24] argue that single reflow step already yields trajectories that are straight enough for one-step Euler sampling, since couplings in p1 xz that would induce curved paths are exceedingly rare under the assumption that the 1-rectified flow is L-Lipschitz: x2 Lz z2 where (x, z), (x, z) p1 xz. (6) While the L-Lipschitz condition guarantees that nearby noise samples cannot map to arbitrarily distant data points, it does not prevent from being large in practice. When is large, even modest differences in noise space can translate into substantial separation in data space, producing early-time bending in the transport paths and degrading the accuracy of one-step Euler sampling. Toy Evidence (Panel d). Due to the effect of class imbalance, samples associated with the lighter target mode must traverse disproportionately long distances toward the denser component. Consequently, even when their noise samples are close, the corresponding data points can lie far apart, yielding large effective in Eq. 6 and causing trajectories to bend near 0. The severe one-step deviations in Fig. 3(d) reflect precisely this early-time curvature. Similar geometric imbalances arise naturally in real datasets, underscoring the need for methods that remain stable and accurate even in the presence of residual curvature after rectification. MeanFlow Struggles on Highly Curved Independent Couplings. Training MeanFlow directly on curved flows suffers from noisy and unstable supervision signals, which is well-known cause of slow convergence in machine learning. Concretely, in the training objective of Eq. 4, the marginal velocity field is not directly available and must be approximated from conditional velocities. Because the couplings are sampled independently, these conditional velocities exhibit high variance. Moreover, the derivative term (t r) dt u(zt, r, t) relies on Jacobian-vector products of the model output without explicit supervision, introducing additional noise that compounds the variance. Finally, when trajectories are highly curved, the corresponding average velocity field becomes irregular and difficult to approximate, as illustrated in Fig. 2b. Toy Evidence (Panel e). MeanFlow trained directly on independent couplings struggles to learn coherent transport map under the same training budget, resulting in poor overall performance. 4.2 Re-Meanflow: Efficient One-Step Generative Modeling via MeanFlow on Rectified Trajectories Motivated by these observations, we propose Re-Meanflow, which models the mean velocity field along rectified trajectories, avoiding relying on perfectly straight ODE paths. Concretely, given pretrained flow model vθ trained on the independent couplings pxz = pxpz, we obtain straighter coupling distribution p1 θ . We then train MeanFlow model uθ on these rectified couplings under the MeanFlow objective (Eq. 5). xz by transporting samples using the 1-rectified flow v1 This combination substantially accelerates MeanFlow training compared to using independent couplings. Rectified trajectories contain far fewer path intersections, allowing conditional velocities to better approximate the marginal velocity field and also keeping instantaneous velocities closer to their temporal averages. As result, the discrepancy between u(zt, r, t) and v(zt, t) in Eq. 5 is significantly reduced. Toy Evidence (Panel f). Re-Meanflow (ours) achieves more accurate generation and yields many fewer invalid one-step samplings, demonstrating the complementary roles of the two components. Takeaway. Trajectory rectification sufficiently reduces curvature to enable efficient MeanFlow training, while MeanFlow removes the need for fully straight trajectories. This synergistic advantage of Re-Meanflow is further evident on real ImageNet data in Fig. 1, where it outperforms using MeanFlow or Rectified Flow alone. visual illustration of this synergy is provided in Fig.2. 5 4.3 Distance Truncation As discussed in Sec. 4.1, small but consequential subset of couplings exhibits large data-noise distances, often arising from low-density regions where samples must travel substantially farther than typical cases. Such pairs tend to produce higher curvature even after rectification, contributing disproportionately to instability during training. This observation motivates simple filtering heuristic: after generating candidate couplings, we discard the top k% (e.g., 10%) with the largest data-noise ℓ2 distance. This Distance Truncation selectively removes the pairs most associated with excessive transport length, and in practice leads to noticeably straighter flows and more stable learning. Figure 4: Distance-Error Correlation. Histogram of data-noise ℓ2 distances on ImageNet 5122, colored by angular error (computed as arccos (cid:0) zx, uθ(z,0,1) (cid:1), the anzx2 uθ(z,0,1)2 gle between the predicted and ground-truth velocities). Errors are computed using MeanFlow model trained on untruncated 1-rectified couplings (config (d) in Tab. 3). clear high-distance high-error tail (90th percentile marked) motivates our Distance Truncation. We also examine this behavior on real datasets. As shown in Fig. 4, couplings with large data-noise distances consistently populate the high-loss tail of the flow regression objective, suggesting that they coincide with high-curvature, high-variance regions. As shown in our empirical results in Sec. 5.2, filtering these pairs yields significant performance gain in practice."
        },
        {
            "title": "5 Experiments",
            "content": "Datasets. We conduct experiments on ImageNet [7] at 642 resolution in pixel space and at 2562 and 5122 resolutions in latent space [36]. Settings. We initialize Re-Meanflow from pretrained flow or diffusion model. For ImageNet642 and ImageNet-5122, we initialize Re-Meanflow from the pretrained EDM2-S model [23]. For ImageNet-2562, we initialize from the pretrained SiT-XL model [31]. During the reflow process, we generate 5M data-noise couplings using the default procedures described in the corresponding papers. During sampling for better synthetic image quality, we apply classifier-free guidance (CFG)[16] for ImageNet-2562 and Autoguidance[22] for ImageNet-642 and 5122. More details on the implementation and the hyperparameter settings are provided in Appendix B. Baselines. We compare Re-Meanflow against recent state-of-the-art one-step flow-based methods, selecting baselines with comparable architecture or computational cost to ensure fair comparisons. 5.1 One Step Generation Quality For all experiments, we evaluate image quality using Fréchet Inception Distance (FID) [15]. When comparing with prior methods, we select those with comparable architectures and parameter counts. Across all evaluated resolutions, Re-Meanflow consistently outperforms state-of-the-art one-step flow-based generation methods. On the EDM2-S backbone [23], Re-Meanflow achieves superior performance at both 642 and 5122 resolutions. On ImageNet-642, Re-Meanflow outperforms the closely related 2-rectified flow++ [24] by 33.4% in FID, and slightly improves over recent state-of-the-art one-step baselines[29, 25, 37]. On ImageNet-5122, Re-Meanflow also delivers strong one-step image quality, achieving 9% FID gain over AYF[37] and outperforming strong consistency distillation methods[37, 19]. On the SiT backbone [31] for ImageNet-2562, Re-Meanflow slightly surpasses MeanFlow [11] despite being trained without real-image supervision and relying solely on synthesized reflow samples. This is noteworthy because training exclusively on self-generated data is known to degrade performance due to self-conditioning effects [2]. We attribute this improvement to the more favorable optimization landscape created by rectified mean-velocity learning: although the supervision is limited to synthetic couplings, rectification produces smoother and lower-variance trajectory family, effectively narrowing the search space. As result, the model converges more reliably toward high-quality solution. Table 1: Class-conditional generation on ImageNet. All results use classifier-free guidance (CFG) when supported; 2 indicates that CFG doubles the effective NFE. denotes methods initialized from, or directly comparable to, the diffusion backbones also marked with . (iCT[42] result at 2562 is reported by IMM[57], and ECT/ECD[10] results at 5122 are reported by from CMT[19]). (a) ImageNet 642 (b) ImageNet 2562 (c) ImageNet 5122 Method NFE FID Method NFE FID Method NFE FID Diffusion models EDM2-S [23] + Autoguidance [22] EDM2-XL [23] 63 2 63 2 63 2 Few-step models 2-rectified flow++ [24] iCT [42] ECD-S [10] sCD-S [29] TCM [25] AYF [37] Re-Meanflow (ours) 1 1 1 1 1 1 Diffusion models ADM [8] DiT-XL [34] SiT-XL [31] SiT-XL + REPA [51] Few-step models iCT [42] SM [9] iSM [32] iMM [57] MeanFlow [11] Re-Meanflow (ours) 1.58 1.01 1.33 4.31 4.02 3.30 2.97 2.88 2.98 2.87 250 2 250 2 250 2 250 2 10.94 2.27 2.06 1. 1 1 1 1 2 1 1 34.6 10.6 5.27 7.77 3.43 3.41 Diffusion models EDM2-S [23] + Autoguidance [22] EDM2-XXL [23] Few-step models ECT [10] ECD [10] CMT [19] sCT-S [29] sCD-S [29] AYF [37] Re-Meanflow (ours) 63 2 63 2 63 2.23 1.34 1.81 1 1 1 1 1 1 1 9.98 8.47 3.38 10.13 3.07 3.32 3.03 Figure 5: Qualitative results for Re-Meanflow (NFE=1) on ImageNet 642 (Left), 2562 (Middle), and 5122 (Right). 5.2 Training Efficiency As noted by Lee et al. [24], distillation methods involving reflow can remain computationally competitive, even though they require generating additional data-noise couplings. Synergistic Benefits of Re-Meanflow. We compare Re-Meanflow against MeanFlow [11] and 2-rectified flow++ [24] on ImageNet-5122 without using CFG. All methods are initialized from EDM2-S [23] and follow the recommended training configurations. Re-Meanflow and 2rectified flow++ are trained on the same set of 5M couplings generated during the reflow process, whereas MeanFlow is trained on independent couplings. For fairness, we follow the training settings reported in the original papers with one exception: we do not include the LPIPS [54] loss for 2rectified flow++. LPIPS operates in pixel space and cannot be used directly in the latent space setting of our experiments; moreover, LPIPS may leak perceptual information correlated with FID, potentially exaggerating improvements. Instead, we adopt the Huber loss, the strongest configuration reported for 2-rectified flow++ without LPIPS. Figure 6: Comparison of convergence on ImageNet-2562. Re-Meanflow achieves the best FID under the same training budget. To ensure fair comparison, we define the total training budget as the GPU hours required to (1) generate all 5M couplings and (2) train Re-Meanflow for 100k iterations. We allocate the same Figure 7: Comparison of total computational cost on ImageNet-642 in EFLOPs (left) and GPU hours (right). Each bar is decomposed into the training cost (solid red) and the reflow sampling cost (blue hatched). Re-Meanflow (ours) is used as the computational baseline. Numbers above the bars indicate multiplicative factor relative to Re-Meanflow (1.0). total GPU-hour budget to MeanFlow and to 2-rectified flow++, and compare the resulting FID as function of GPU hours. As shown in Fig. 1, Re-Meanflow achieves the best FID 8.6, reaching the final MeanFlow FID 2 faster, while 2-rectified flow++ converges extremely slowly, achieving only 87.8 FID within the same budget. We repeat this experiment on ImageNet-2562 and observe consistent results, as shown in Fig.6. This demonstrates that combining MeanFlow with rectified flow is not merely an additive combination of two components: rectification reduces trajectory variance and stabilizes the MeanFlow objective, while mean-velocity learning enables efficient one-step sampling. The two components reinforce each other, yielding synergistic effect that neither method achieves independently. Overall Training Cost Comparison. We further evaluate Re-Meanflow by estimating the total training cost in FLOPs and GPU hours on ImageNet-642. The protocol for computing these metrics follows Lee et al. [24] and is detailed in the Appendix C.1. We compare against AYF [37], the strongest existing distillation baseline in both quality and efficiency, and against the closely related 2-rectified flow++ [24]. As shown in Fig. 7, Re-Meanflow achieves the lowest overall compute cost among recent distillation approaches. In terms of GPU hours, Re-Meanflow is 26.6 faster than 2-rectified flow++, reinforcing the results observed in our controlled efficiency experiments. Even compared to AYF, which does not require coupling generation, Re-Meanflow remains 2.9 faster. We also compare estimated FLOPs. Although Re-Meanflow shows smaller advantage under this metric, FLOPs alone do not fully reflect practical runtime. substantial portion of our compute lies in the inference-only reflow stage, which can be executed on widely accessible inference-grade GPUs and runs significantly faster than training workloads with the same FLOP count. Training steps incur considerable overhead, such as data loading and batching, and rely on computationally expensive backward and JVP operations that typically require high-end training GPUs. Takeaway. This highlights key practical benefit of Re-Meanflow: by shifting most of the computational burden away from scarce high-end training GPUs and onto widely available inferencegrade accelerators, Re-Meanflow substantially improves the accessibility and real-world feasibility of one-step flow distillation. 5.3 Ablation Study Distance Truncation. We evaluate different truncation thresholds for Re-Meanflow, ranging from 5% to 15%, and report the resulting FID scores in Tab. 2. All truncation levels outperform the no-truncation baseline. The best result is achieved with 10% threshold, which reduces FID from 3.5 to 3.03. Table 2: Ablation on truncation strength on ImageNet-5122. Percentage improvement is computed relative to the no-truncation baseline. Truncation Strength FID Improvement (%) No Truncation [8] Top 5% Top 10% Top 15% 3.50 3.10 3.03 3.19 11.4% 13.4% 8.9% As the threshold increases beyond 10%, performance begins to degrade (e.g., 3.19 at 15%). This pattern reflects the trade-off inherent in truncation: while discarding extremely long-distance pairs helps suppress the high-curvature tail, removing too many couplings reduces data coverage and weakens the supervision signal. Overall, the empirical trend supports our analysis in Sec. 4.3: moderate truncation effectively filters the most problematic cases while preserving sufficient coupling diversity for training. Important Implementation Details. To identify the components most critical for stable and high-quality training for Re-Meanflow, we conduct ablations on ImageNet-5122 and showcase their impact on FID in Table 3. Table 3: Ablation study on ImageNet-5122 for key implementation details of Re-Meanflow. Training configurations (a) Hyperparameter adjustments. Because rectified trajectories are significantly straighter, we reduce the normalization strength in the MeanFlow adaptive loss from 1.0 to 0.5 (similar to Pseudo-Huber loss). We also find that sampling the classifier-free guidance scale from uniform distribution improves generation quality. Base (Best Setting reported in [11]) (a) + Hyperparameter adjustments (b) + Time embedding change (c) + U-shaped distribution (d) + Avoid high-variance (t, r) region (e) + Distance truncation FID 7.81 7.22 4.60 3.71 3.50 3.03 (b) Following Sabour et al. [37], Lu and Song [29], we replace the EDM2 time embedding emb(log σt) with emb(t) to obtain more stable Jacobianvector product computations. We perform brief alignment stage that trains the new embedding to match the original outputs at the corresponding noise levels. Details of this alignment are provided in Appendix B.1. (c) Since rectified trajectories are smoother and require less mid-range emphasis, we adopt the U-shaped distribution from Lee et al. [24] instead of using the standard lognormal schedule. (d) Re-Meanflow exhibits unusually high variance when > 0.95 and < 0.4, phenomenon we analyze in more detail in the Appendix. Inspired by the truncation strategy used in TCM [25], we mitigate this issue by avoding this high-variance time region. This modification not only improves FID, but also accelerates convergence. More detail is presented in Appendix B.3. (e) Applying our trajectory-distance truncation heuristic (Sec. 4.3) further improves efficiency and generation quality. Additional analysis of distanceerror correlations across resolutions, which motivates our truncation threshold, is provided in AppendixC.2."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented Re-Meanflow, simple yet efficient framework that combines the complementary strengths of Rectified Flow [28] and MeanFlow [11]. By leveraging the synergy between trajectory straightening and mean-velocity learning, Re-Meanflow outperforms both components when used independently. Empirically, on ImageNet 642, 2562, and 5122, Re-Meanflow delivers substantial improvements in both efficiency and generation quality, surpassing current state-of-the-art one-step flow-based models. Limitation. Since our method does not access real data during training and relies entirely on synthetic samples generated by pretrained diffusion or flow models, its performance naturally depends on the quality of these generated couplings. promising direction is to incorporate real data into the training process, as explored in Lee et al. [24], Seong et al. [39]. Another complementary line of work investigates how to improve generative models using only their own outputs [2, 3, 4], which suggests that self-generated data can still be leveraged effectively with appropriate regularization. Alternatively, one could improve the synthetic supervision directly by using stronger backbone models to generate the couplings. Future Work. Looking ahead, the core idea behind Re-Meanflow is architecture-agnostic. We believe that leveraging straighter trajectories is broadly applicable principle that may benefit other few-step generative paradigms. Re-Meanflow also offers practical advantages by shifting most of the training burden from scarce high-end training GPUs to widely available inference-grade accelerators capable of performing the reflow process. As proof of concept, we view Re-Meanflow as promising direction to scaling one-step paradigms to larger domains, such as text-to-image generation."
        },
        {
            "title": "References",
            "content": "[1] Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. [2] Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, and Richard Baraniuk. Self-consuming generative models go mad. In The Twelfth International Conference on Learning Representations, 2023. [3] Sina Alemohammad, Ahmed Imtiaz Humayun, Shruti Agarwal, John Collomosse, and Richard Baraniuk. Self-improving diffusion models with synthetic data. arXiv preprint arXiv:2408.16333, 2024. [4] Sina Alemohammad, Zhangyang Wang, and Richard Baraniuk. Neon: Negative extrapolation from self-training improves image generation. arXiv preprint arXiv:2510.03597, 2025. [5] Nicholas Boffi, Michael Albergo, and Eric Vanden-Eijnden. Flow map matching. arXiv preprint arXiv:2406.07507, 2, 2024. [6] Quan Dao, Khanh Doan, Di Liu, Trung Le, and Dimitris Metaxas. Improved training technique for latent consistency models. arXiv preprint arXiv:2502.01441, 2025. [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34, 2021. [9] Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel. One step diffusion via shortcut models. arXiv preprint arXiv:2410.12557, 2024. [10] Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and Zico Kolter. Consistency models made easy. arXiv preprint arXiv:2406.14548, 2024. [11] Zhengyang Geng, Mingyang Deng, Xingjian Bai, Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025. [12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. [13] Xiaoxiao He, Chaowei Tan, Ligong Han, Bo Liu, Leon Axel, Kang Li, and Dimitris Metaxas. Dmcvr: Morphology-guided diffusion model for 3d cardiac volume reconstruction. In International conference on medical image computing and computer-assisted intervention, pages 132142. Springer Nature Switzerland Cham, 2023. [14] Xiaoxiao He, Quan Dao, Ligong Han, Song Wen, Minhao Bai, Di Liu, Han Zhang, Martin Renqiang Min, Felix Juefei-Xu, Chaowei Tan, et al. Dice: Discrete inversion enabling controllable editing for multinomial diffusion and masked generative models. arXiv preprint arXiv:2410.08207, 2024. [15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [16] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [18] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in neural information processing systems, 35: 86338646, 2022. [19] Zheyuan Hu, Chieh-Hsin Lai, Yuki Mitsufuji, and Stefano Ermon. Cmt: Mid-training for efficient learning of consistency, mean flow, and flow map models. arXiv preprint arXiv:2509.24526, 2025. [20] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44014410, 2019. [21] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35: 2656526577, 2022. [22] Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. Advances in Neural Information Processing Systems, 37:5299653021, 2024. [23] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2417424184, 2024. [24] Sangyun Lee, Zinan Lin, and Giulia Fanti. Improving the training of rectified flows. Advances in neural information processing systems, 37:6308263109, 2024. [25] Sangyun Lee, Yilun Xu, Tomas Geffner, Giulia Fanti, Karsten Kreis, Arash Vahdat, and Weili Nie. Truncated consistency models. arXiv preprint arXiv:2410.14895, 2024. [26] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. arXiv preprint arXiv:2504.10483, 2025. [27] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [28] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [29] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. [30] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in neural information processing systems, 35:57755787, 2022. [31] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. [32] Anh Nguyen, Viet Nguyen, Duc Vu, Trung Dao, Chi Tran, Toan Tran, and Anh Tran. Improved training technique for shortcut models. arXiv preprint arXiv:2510.21250, 2025. [33] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [34] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [35] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International conference on machine learning, pages 15301538. PMLR, 2015. [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 11 [37] Amirmojtaba Sabour, Sanja Fidler, and Karsten Kreis. Align your flow: Scaling continuous-time flow map distillation. arXiv preprint arXiv:2506.14603, 2025. [38] Vikash Sehwag, Xianghao Kong, Jingtao Li, Michael Spranger, and Lingjuan Lyu. Stretching each dollar: Diffusion training from scratch on micro-budget. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2859628608, 2025. [39] Kim Shin Seong, Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Balanced conic rectified flow. arXiv preprint arXiv:2510.25229, 2025. [40] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. pmlr, 2015. [41] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [42] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. arXiv preprint arXiv:2310.14189, 2023. [43] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. [44] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [45] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023. [46] Alexander Tong, Kilian Fatras, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid RectorBrooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint arXiv:2302.00482, 2023. [47] Ge Wu, Shen Zhang, Ruijing Shi, Shanghua Gao, Zhenyuan Chen, Lei Wang, Zhaowei Chen, Hongcheng Gao, Yao Tang, Jian Yang, et al. Representation entanglement for generation: Training diffusion transformers is much easier than you think. arXiv preprint arXiv:2507.01467, 2025. [48] Qipan Xu, Zhenting Wang, Xiaoxiao He, Ligong Han, and Ruixiang Tang. Can large vision-language models detect images copyright infringement from genai? arXiv preprint arXiv:2502.16618, 2025. [49] Ling Yang, Zixiang Zhang, Zhilong Zhang, Xingchao Liu, Minkai Xu, Wentao Zhang, Chenlin Meng, Stefano Ermon, and Bin Cui. Consistency flow matching: Defining straight flows with velocity consistency. arXiv preprint arXiv:2407.02398, 2024. [50] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1570315712, 2025. [51] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. [52] Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, and Josh Susskind. Normalizing flows are capable generative models. arXiv preprint arXiv:2412.06329, 2024. [53] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022. [54] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 12 [55] Xinxi Zhang, Song Wen, Ligong Han, Felix Juefei-Xu, Akash Srivastava, Junzhou Huang, Vladimir Pavlovic, Hao Wang, Molei Tao, and Dimitris Metaxas. Soda: Spectral orthogonal decomposition adaptation for diffusion models. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 46654682. IEEE, 2025. [56] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023. [57] Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive moment matching. arXiv preprint arXiv:2503.07565, 2025. 13 1 2 2 2 3 3 4 5 5 5 6 8"
        },
        {
            "title": "Appendix",
            "content": "A More Discussion A.1 Broader Impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Additional Details for Sec. 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Comparison with CMT [19] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Implementation Details B.1 Pretrained Model Conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Time Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Avoiding High-Variance Time Regions . . . . . . . . . . . . . . . . . . . . . . . . B.4 Training with Guidance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.5 2D Toy Example Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . More Experiment Details and Results C.1 Computation Estimation of Each Method . . . . . . . . . . . . . . . . . . . . . . C.2 Distance-Error Correlation at Other Resolutions . . . . . . . . . . . . . . . . . . . Algorithm for Re-Meanflow More Qualitative Results E.1 ImageNet-642 . . E.2 ImageNet-2562 . E.3 ImageNet-5122 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A More Discussion",
            "content": "In this section, we provide additional analysis and context for our method. We begin by discussing the broader impact of our approach. We then revisit the theoretical motivation for using single reflow iteration and explain how our perspective complements existing analyses. Finally, we compare our method with the concurrent CMT method [19]. A.1 Broader Impact This work introduces simple yet effective framework for efficient one-step generative modeling by training MeanFlow model on rectified trajectories. Through extensive evaluations across multiple ImageNet settings, varying in resolution, latent representation, and model architecture, we demonstrate that Re-Meanflow achieves strong performance and high efficiency. Beyond empirical gains, our method highlights practical and socially relevant shift in how large generative models may be trained. Traditional few-step or one-step distillation pipelines often require expensive, high-end training hardware (e.g., A100-class GPUs), limiting accessibility to well-resourced institutions. In contrast, Re-Meanflow moves many of the computational burden to the inference reflow stage, followed by lightweight MeanFlow training phase. Because inference workloads can be executed efficiently on widely available consumeror inference-grade accelerators, our framework reduces reliance on scarce training GPUs and lowers the barrier to experimentation. 1 A.2 Additional Details for Sec. 4. Lee et al. [24] analyze when multiple Reflow iterations are truly necessary for achieving straight trajectories in rectified flows. Their argument centers on how trajectory intersections affect the learned velocity field. Consider two 1-rectified couplings (x, z) and (x, z). trajectory intersection occurs if there exists [0, 1] such that (1 t)x + tz = (1 t)x + tz. If such an intersection happens, both trajectories pass through the same intermediate point. Because rectified-flow training regresses the conditional expectation E[xxt], the model must assign single velocity to this shared point. As result, the learned velocity field cannot simultaneously point toward both and x, and it instead averages their directions. This averaging effect bends the local velocity field, producing curvature and consequently degrading the accuracy of one-step Euler sampling, which assumes the path to be straight. To understand how often this phenomenon can happen, Lee et al. [24] show that an intersection implies = + 1 (x x). Under typical training, nearly all noise samples used to form 1-rectified couplings lie in the highdensity region of the Gaussian prior. The required above usually lies far outside that region unless x2 is extremely small or is very close to 1. Therefore, intersections are statistically rare. Further assuming that the 1-rectified flow is approximately L-Lipschitz, x2 z2, nearby noise samples cannot map to widely separated data points. Combining the rarity of intersections with this Lipschitz condition, the authors conclude that the optimal 2-rectified flow is nearly straight. Hence, in their view, one additional Reflow step is sufficient, and any remaining performance gap should be attributed primarily to training inefficiency rather than insufficient straightening. Relation to Our Work. Our focus is complementary to the above perspective. While their analysis shows that intersections are rare for most couplings, our experiments reveal that in realistic settings, small but influential subset of couplings can still exhibit noticeable curvature, particularly when the effective Lipschitz constant becomes large due to geometric imbalance in the data distribution. Our method is designed to improve robustness in these challenging cases, providing stable one-step generation even when residual curvature persists after single reflow step. A.3 Comparison with CMT [19] CMT [19] is an important concurrent effort that also leverages synthetic trajectories generated by pretrained sampler to stabilize few-step flow-map training. Conceptually, CMT introduces dedicated mid-training stage that learns full trajectory-to-endpoint mapping from solver-generated paths, which then serves as trajectory-aligned initialization for subsequent post-training flow-map stage. In contrast, our method adopts fundamentally different design: rather than supervising on entire solver trajectories, we distill only the end-point couplings of rectified flows and learn the corresponding mean velocity in single training stage. This distinction yields practical advantageour pipeline avoids the compounded complexity of CMTs two-stage optimization, which is sensitive to hyperparameters at both stages and substantially more expensive to tune."
        },
        {
            "title": "B Implementation Details",
            "content": "In this section, we first describe the conditioning strategy of how Re-Meanflow utilizes previous pretrained models. We then outline the key design choices during training Re-Meanflow. We provide the training hyperparameters across all ImageNet resolutions in Tab. 4. 2 Table 4: Training settings of Re-Meanflow on ImageNet. Training details Model backbone Global batch size Learning rate Adam β1 Adam β2 Model capacity (M params) EMA rate MeanFlow settings 642 EDM2-S[23] 128 1e4 0.9 0.95 280.2 0.9999 Resolution 2562 SiT-XL[31] 128 1e4 0.9 0.95 676.7 0.9999 5122 EDM2-S[23] 128 1e4 0.9 0.95 280.5 0. Ratio of = for adaptive weight CFG effective scale Avoiding high-variance (t, r) 0.25 0.5 Uniform(1.0, 3.0) > 0.95 < 0.4 0.25 0.5 Uniform(1.0, 3.0) > 0.95 < 0.4 0.25 0.5 Uniform(1.0, 2.5) > 0.95 < 0.4 Sampling details (rectified couplings from 1-rectified flow) Pretrained model Sampling number Guidance method Distance truncate strength EDM2-S[23] 5M Autoguidance[22] Top 10% SiT-XL[31] 5M CFG[16] Top 10% EDM2-S[23] 5M Autoguidance[22] Top 10% B.1 Pretrained Model Conditioning In Re-Meanflow, the velocity network is conditioned on two time variables, and r. We implement this conditioning by learning two separate embeddings, embt(t) and embr(r), and summing them before passing the result to the rest of the network. When initializing from pretrained models, the original networks only contain single time embedding for t. Replacing this embedding with our two-embedding design requires careful initialization to ensure the model initially behaves like the pretrained flow mode. Specifically, before MeanFlow fine-tuning, we want: u(xt, t, r) v(xt, t) (7) On ImageNet 642 and 5122, which Re-Meanflow is initialized Re-Meanflow from the pretrained EDM2-S model [23], following AYS [37], we perform short alignment stage in which we train the new embeddings to reproduce the original time-embedding output for the corresponding noise level. Specifically, for EDM2-S the original embedding depends on log σt, where σt = 1t . We train the new embeddings via: Et,r[embt(t) + embr(r) embori(log σt)2 2], (8) for 10k iterations with learning rate 1e-3. This process takes only few minutes. We also convert the original VE diffusion parameterization into the flow-matching setting following Lee et al. [24]. On ImageNet 2562, we initialize Re-Meanflow from SiT-XL [31], which is already flow-based model. In this case, only the additional r-embedding needs to be introduced. We simply zero-initialize embr(r) and keep the original SiT time embedding for embt(t). B.2 Time Distribution We observe similar losstime profile to that reported in Lee et al. [24]: the training loss as function of closely matches that of 2-rectified flow++. Accordingly, for sampling we adopt the same U-shaped distribution: pt(u) exp(au) + exp(au), [0, 1], = 4. Following AYF [37], after sampling we draw the interval length r from normal distribution (Pmean, Pstd) and apply sigmoid transformation. We use the same parameters as AYF, 3 Figure 8: Heatmaps of the mean loss (left) and the standard deviation of the loss (right) for trained Re-Meanflow model on ImageNet-5122 under configuration (c). (Pmean, Pstd) = (0.8, 1.0), which emphasize medium-length intervals and substantially improve stability. As shown in Table 3, this setting yields noticeable improvement in FID and accelerates convergence. We also experimented with sampling uniformly. Despite its simplicity, uniform sampling performs competitively, and in many cases better than commonly used log-normal time distributions in diffusion and flow-matching models [21, 23, 11], which prioritize mid-range timesteps to avoid high variance near 0 and 1. We attribute the strong performance of uniform sampling to the significantly lower variance of rectified trajectories: after one reflow step, early-time and high-noise regions become much more stable, allowing us to allocate more samples to these challenging regimes without the usual degradation observed when training on independent couplings. B.3 Avoiding High-Variance Time Regions As discussed in Sec. 5.2, we observe that Re-Meanflow exhibits unusually high variance when the noise level is large while the reference time is close to zero. Intuitively, this corresponds to asking the model to predict slightly denoised sample (r [0, 0.4]) from an input that remains heavily corrupted (t 1). To quantify this effect, we sample 100k pairs of (t, r) uniformly and evaluate trained Re-Meanflow model under configuration (c) in Table 3. As shown in Fig. 8, the resulting loss landscape displays clear spike in both error and variance within this region, often even higher than the loss incurred when predicting directly from noise to clean target. Inspired by the truncation strategy in TCM [25], we adopt simple yet effective rule to avoid this problematic regime: whenever sampled pair satisfies > 0.95 and < 0.4, we set = 0. Empirically, this improves both training stability and FID. Our hypothesis is that predicting clean image from pure noise (r = 0, 1) is substantially easier than predicting lightly corrupted target: the latter requires the model to determine which noise components should be preserved, introducing ambiguity and variance at high t. By redirecting training toward these easier high-t targets, the model can allocate more capacity to learning accurate one-step predictions. This modification not only improves FID relative to configuration (c), but also accelerates convergence: in the high-t regime, more updates involve = 0, allowing the model to refine its one-step outputs more quickly and reliably. B.4 Training with Guidance Classifier-free guidance (CFG) [16] is widely used to boost the performance of diffusion and flowbased generative models. To incorporate CFG into the MeanFlow stage, we train Re-Meanflow on the CFG-enhanced velocity field: vcfg(zt, c) ω v(zt, c) + (1 ω) v(zt, t). (9) MeanFlow [11] further introduced an improved CFG method that mixes conditional and unconditional mean-velocity predictions: vcfg(zt, c) = ω v(zt, c) + κucfg(zt, t, tc) + (1 ω + κ) ucfg(zt, t, t). (10) 4 which is equivalent to using an effective guidance of ω = ω formulation for all experiments. 1κ . We adopt this improved CFG Empirically, we found that directly training MeanFlow on the CFG field results in instability, consistent with observations in Hu et al. [19]. To mitigate this, we use simple two-stage strategy: first train uθ on the unconditional flow, then on the CFG-modified flow. Usually, allocating half of the total training budget to each stage provides good balance between stability and final quality. We also found that sampling random CFG scale ω from uniform distribution (rather than fixing it) gives better results. Large values of κ is also important for stable training. In practice, we sample ω from uniform distribution, then set κ = max(1.0, ω 1), and finally compute the corresponding value for ω = ω 1κ . B.5 2D Toy Example Setting We consider controlled two-dimensional transport task designed to highlight curvature effects and the behavior of different flow-learning methods. Both the source and target distributions are mixtures of two Gaussians. The source distribution is balanced mixture with equal component weights, while the target distribution is an imbalanced mixture with weights (0.6, 0.4). Component means are placed symmetrically on the left (source) and right (target) sides of the plane, and all components use identity covariance. This setup ensures that samples associated with the lighter target mode must traverse longer paths, creating the geometric imbalance that induces curved trajectories. All velocity and MeanFlow models are implemented as small multilayer perceptrons (MLPs) with identical architectures across methods. Training is performed for fixed budget of 20k steps using Adam with learning rate 103 and batch size 1024. We compare three approaches under this shared budget: 1. 2-rectified Flow: two successive reflow steps applied to independently sampled couplings, each trained for 10k iterations. 2. MeanFlow: trained for the full 20k iterations directly on independent couplings. 3. Re-Meanflow (ours): velocity model is first trained for 10k iterations to obtain 1-rectified flow; MeanFlow model is then trained for another 10k iterations on the resulting rectified couplings. Figure 3 visualizes the learned trajectories and one-step Euler sampling results for all three settings. As discussed in the main text, 2-rectified flow retains noticeable outliers due to residual curvature, and MeanFlow trained on independent couplings fails to converge within the budget. In contrast, Re-Meanflow combines moderate rectification with stable MeanFlow training to achieve accurate one-step generation."
        },
        {
            "title": "C More Experiment Details and Results",
            "content": "In this section, we provide additional experimental details, computational analyses, and extended results. We describe how we estimate FLOPs and GPU-hours for all compared methods, and we provide additional distance-error correlation for ImageNet 642 and 2562. C.1 Computation Estimation of Each Method FLOPS Estimation. In Fig 7, we report the efficiency in terms of estimated exaFLOPs (Eflops). To ensure comparability, we estimate total training and sampling compute for each method based on their reported FLOPs per forward pass. Specifically, we use the following assumptions: The FLOPs of forward pass are reported by prior works (e.g., EDM[21]: 100 GFLOPs, EDM2-S[23]: 102 GFLOPs, SiT-XL[31]: 118.64 GFLOPs). The FLOPs of backward pass are measured empirically and are approximately 2 the cost of forward pass. (One JVP operation is also counted as backward pass), say for our example on the first stage where we will perform one forward of the model and one JVP 5 operation and one back propagation with JVP counted as one back propagation we have total flop amount of 1 + (2 2) = 1 + 4 forward flops. For training, the total compute is computed as: Total Train FLOPs = (#iters)(batch size)(forward+backward)(GFLOPs per fwd). For sampling in the reflow process, the total compute is computed as: Total Sample FLOPs = (#samples)(#steps)(forward passes per step)(GFLOPs per fwd). Example: Re-Meanflow (Ours) on ImageNet-642. For sampling, we require: 5 106 (cid:124) (cid:123)(cid:122) (cid:125) #samples 63 (cid:124)(cid:123)(cid:122)(cid:125) steps 2 (cid:124)(cid:123)(cid:122)(cid:125) fwd/step (auto-guidance) 102 (cid:124)(cid:123)(cid:122)(cid:125) GFLOPs/fwd 6.43 1010 GFLOPs 64 Eflops. For training, we have two stages, with the first stage trained on the original flow and the second stage trained on the CFG velocity field: 50,000 (cid:124) (cid:123)(cid:122) (cid:125) iters 128 (cid:124)(cid:123)(cid:122)(cid:125) batch (1 + 4) (cid:124) (cid:123)(cid:122) (cid:125) fwd+back 102 (cid:124)(cid:123)(cid:122)(cid:125) GFLOPs/fwd + 50,000 (cid:124) (cid:123)(cid:122) (cid:125) iters 128 (cid:124)(cid:123)(cid:122)(cid:125) batch (3 + 4) (cid:124) (cid:123)(cid:122) (cid:125) fwd+back 102 (cid:124)(cid:123)(cid:122)(cid:125) GFLOPs/fwd 8 Eflops. For training, the total compute is computed as: Total Train FLOPs = (#iters)(batch size)(forward+backward)(GFLOPs per fwd). For sampling in the reflow process, the total compute is computed as: Total Sample FLOPs = (#samples)(#steps)(forward passes per step)(GFLOPs per fwd). Example: Re-Meanflow (Ours) on ImageNet-642. For sampling, we require: 5 106 (cid:124) (cid:123)(cid:122) (cid:125) #samples 63 (cid:124)(cid:123)(cid:122)(cid:125) steps 2 (cid:124)(cid:123)(cid:122)(cid:125) fwd/step (auto-guidance) 102 (cid:124)(cid:123)(cid:122)(cid:125) GFLOPs/fwd 6.43 1010 GFLOPs 64 Eflops. For training, we have two stages, with the first stage trained on the original flow and the second stage trained on the CFG velocity field: 50,000 (cid:124) (cid:123)(cid:122) (cid:125) iters 128 (cid:124)(cid:123)(cid:122)(cid:125) batch (1 + 4) (cid:124) (cid:123)(cid:122) (cid:125) fwd+back 102 (cid:124)(cid:123)(cid:122)(cid:125) GFLOPs/fwd + 50,000 (cid:124) (cid:123)(cid:122) (cid:125) iters 128 (cid:124)(cid:123)(cid:122)(cid:125) batch (3 + 4) (cid:124) (cid:123)(cid:122) (cid:125) fwd+back 102 (cid:124)(cid:123)(cid:122)(cid:125) GFLOPs/fwd 8 Eflops. Example: AYF [37]. AYF does not require sampling, so only the training compute is considered: 50,000 (cid:124) (cid:123)(cid:122) (cid:125) iters 2048 (cid:124) (cid:123)(cid:122) (cid:125) batch (4 + 4) (cid:124) (cid:123)(cid:122) (cid:125) fwd+back 102 (cid:124)(cid:123)(cid:122)(cid:125) GFLOPs/fwd 8.36 1010 GFLOPs 84 Eflops. GPU Hours Estimation. In Fig.7, we also estimate the total GPU hours for AYF[37] and 2-rectified flow++ [24]. For all methods, including ours, we follow the standard convention of computing GPU hours as GPU Hours = (# of GPUs) (wall-clock training time). For example, Re-Meanflow requires 66 hours of wall-clock time on 8 A100 GPUs, yielding 8 66 = 528 GPU hours. C.2 Distance-Error Correlation at Other Resolutions In Fig. 9, we replicate the distance-error correlation analysis of Fig. 4 on ImageNet 2562 (Left) and 642 (Right), respectively. In both settings, we observe the same phenomenon: couplings with large data-noise ℓ2 distances form pronounced high-error tail of the flow regression objective. This mirrors our ImageNet 5122 results. 6 Figure 9: Distance-Error Correlation. Histogram of data-noise ℓ2 distances on ImageNet 2562 (Left) and ImageNet 642 (Right), analogous to Fig. 4. Similar high-distance, high-error tails (90th percentile marked) are observed. Algorithm for Re-Meanflow Algorithm 1 Re-Meanflow Training Require: dataset D; prior pz; time distributions pt and pr,t; number of rectified-flow iterations Tflow; number of meanflow iterations TMF; number of coupling samples Npairs; truncation ratio k%. 1: Stage 1: Train 1-rectified Flow on Independent Couplings 2: for iter = 1 to Tflow do 3: 4: 5: 6: 7: end for 8: Freeze the resulting 1-rectified velocity field v1 θ Sample D, pz, pt zt (1 t)x + tz zt Minimize zt vθ(zt, t)2 2 w.r.t. θ Sample Solve backward ODE with v1 z2 Store (x, z, d) 9: Stage 2: Build Rectified Couplings + Distance Truncation 10: for = 1 to Npairs do 11: 12: 13: 14: 15: end for 16: Let be the (100 k)th percentile of distances {di} 17: Drect {(xi, zi) : di q} θ from = 1 to = 0 to obtain 18: Stage 3: Train MeanFlow on Rectified, Truncated Couplings 19: for iter = 1 to TMF do 20: 21: 22: 23: 24: 25: end for Ensure: trained meanflow model uϕ Sample (x, z) Drect, (r, t) pr,t zt (1 t)x + tz Compute = (t r) utgt v1 Minimize uϕ(zt, r, t) sg(utgt)2 dt uϕ(zt, r, t) θ (zt, t) 2 w.r.t. ϕ JVP"
        },
        {
            "title": "E More Qualitative Results",
            "content": "In this section, we present additional selected qualitative samples generated by Re-Meanflow across all ImageNet resolutions. E.1 ImageNet-642 Figure 10: Selected qualitative results for Re-Meanflow (NFE=1) on ImageNet 642. 8 E.2 ImageNet-2562 Figure 11: Selected qualitative results for Re-Meanflow (NFE=1) on ImageNet 2562. 9 E.3 ImageNet-5122 Figure 12: Selected qualitative results for Re-Meanflow (NFE=1) on ImageNet 5122."
        }
    ],
    "affiliations": [
        "Red Hat AI Innovation",
        "Rutgers University"
    ]
}