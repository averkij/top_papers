{
    "paper_title": "SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models",
    "authors": [
        "Thinh Pham",
        "Nguyen Nguyen",
        "Pratibha Zunjare",
        "Weiyuan Chen",
        "Yu-Min Tseng",
        "Tu Vu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce SealQA, a new challenge benchmark for evaluating SEarch-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors: (1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and reasoning capabilities, with Seal-0 focusing on the most challenging questions where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3) LongSeal, which extends SealQA to test long-context, multi-document reasoning in \"needle-in-a-haystack\" settings. Our evaluation reveals critical limitations in current models: Even frontier LLMs perform poorly across all SealQA flavors. On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and o3-mini are highly vulnerable to noisy search results. Notably, increasing test-time compute does not yield reliable gains across o3-mini, o4-mini, and o3, with performance often plateauing or even declining early. Additionally, while recent models are less affected by the \"lost-in-the-middle\" issue, they still fail to reliably identify relevant documents in LongSeal when faced with numerous distractors. To facilitate future work, we release SealQA at huggingface.co/datasets/vtllms/sealqa."
        },
        {
            "title": "Start",
            "content": "SEALQA: Raising the Bar for Reasoning in Search-Augmented Language Models Thinh Pham Nguyen Nguyen Pratibha Zunjare Weiyuan Chen Yu-Min Tseng Tu Vu Virginia Tech Blacksburg, VA 24061 {thinhphp,tuvu}@vt.edu"
        },
        {
            "title": "Abstract",
            "content": "We introduce SEALQA, new challenge benchmark for evaluating SEarchAugmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SEALQA comes in three flavors: (1) SEAL-0 (main) and (2) SEAL-HARD, which assess factual accuracy and reasoning capabilitieswith SEAL-0 focusing on the most challenging questions, where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3) LONGSEAL, which extends SEALQA to test long-context, multi-document reasoning in needlein-a-haystack settings. Our evaluation reveals critical limitations in current models: Even frontier LLMS perform poorly across all SEALQA flavors. On SEAL-0, frontier agentic models equipped with tools like O3 and O4-MINI achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that advanced reasoning models such as DEEPSEEK-R1-671B and O3-MINI are highly vulnerable to noisy search results. Notably, increasing test-time compute does not yield reliable gains across O3-MINI, O4-MINI, and O3, with performance often plateauing or even declining early. Additionally, while recent models are less affected by the lost-in-the-middle issue, they still fail to reliably identify relevant documents in LONGSEAL when faced with numerous distractors. To facilitate future work, we release SEALQA at huggingface.co/datasets/vtllms/sealqa."
        },
        {
            "title": "Introduction",
            "content": "5 2 0 2 1 ] . [ 1 2 6 0 1 0 . 6 0 5 2 : r Figure 1: Test-time scaling does not lead to reliable gains on SEALQA questions, with performance often plateauing or even declining early. Preprint. Figure 2: Accuracy of LLMS across benchmarks. SEALQA poses significant challenges to frontier models. Large language models (LLMS) have entered new scaling paradigm: test-time scaling, where models dynamically allocate more compute during inference time to improve performance [OpenAI, 2024c, 2025c, Guo et al., 2025, Google, 2025, xAI, 2025, Anthropic, 2025]. This shift is embodied in reasoning models, which leverage reinforcement learning and other techniques to guide inference-time strategies such as chain-of-thought reasoning, recursive refinement, and real-time search [Muennighoff et al., 2025, Guo et al., 2025, Snell et al., 2024, Geiping et al., 2025]. These models can decompose questions into sub-queries, decide when and how to query search engine, and fuse retrieved content into structured reasoning paths [OpenAI, 2025c, Google, 2025, xAI, 2025, Anthropic, 2025, Jin et al., 2025]. As LLMS continue to advance, existing benchmarks are failing to keep pace. Many academic QA datasetsdesigned under assumptions of static knowledge and straightforward reasoningare now saturated; for example, frontier models now achieve over 90% accuracy on MMLU [Phan et al., 2025]. Furthermore, most evaluations of search-augmented LLMS focus on short and simple factual queries where top-ranked search results directly answer the question [Vu et al., 2024, Kasai et al., 2023]. These setups require only shallow comprehension and fail to reflect the messy, ambiguous nature of real-world search. To properly evaluate todays LLMS, benchmarks that go beyond simple fact lookup are needed. Real-world search often returns documents that are outdated, misleading, or superficially relevant but ultimately unhelpful. Navigating this noise requires deeper reasoning: filtering inconsistencies, reconciling contradictions, and identifying trustworthy signals. Yet benchmarks that simulate these challenges are rare, in part because they are difficult to curate and validate at scale. We introduce SEALQA, small but extremely challenging benchmark (Figure 2) for evaluating search-augmented LLMS on fact-seeking questions. Each question is carefully crafted by NLP researchers to trigger ambiguous, conflicting, or noisy search results (Figure 3). This design makes it difficult to answer questions through simple keyword matching or by relying on top-ranked documents. SEALQA spans range of question types (including time-sensitive questions) across diverse domains such as science, technology, sports, entertainment, politics, history, geography, etc. SEALQA questions probe broad spectrum of complex reasoning skills. These include distinguishing between similar entities or events, tracking changes to the same entity over time, interpreting information embedded in search-result plots, charts, or tables, counting multiple items, reasoning over non-English content, and debunking false premises or common misconceptions (Figure 4). All questions are self-contained, verifiable, and require intensive reasoning to resolve ambiguity, filter misinformation, or reconcile conflicting evidencecapabilities that are central to modern LLMS but not adequately captured by existing benchmarks. Every SEALQA question presents substantial challenge for current frontier models. To ensure both difficulty and quality, each SEALQA question undergoes rigorous multi-round vetting process: an initial phase involving two or more graduate-level reviewers, followed by approval from expert reviewers. SEALQA comes in three flavors: SEAL-0 (main; 111 questions): carefully curated core set where even frontier models like CHATGPT 4.1 with browsing consistently fail. Each question is iteratively refined until multiple models fail across several attempts (0% accuracy, hence the 0 in the name). 2 Figure 3: SEALQA requires intensive reasoning to resolve ambiguity, filter out misinformation, or reconcile conflicting evidence. SEAL-HARD (254 questions): broader set that includes SEAL-0 and additional difficult questions that did not meet our strict failure threshold but remain highly challenging. LONGSEAL (254 questions): needle-in-a-haystack variant that tests long-context, multidocument reasoning. Each question is paired with large set of retrieved documents, among which only one contains or implies the correct answerburied within irrelevant, noisy, or misleading content. We intentionally kept SEALQA small due to the high cost and complexity of question development.1 Building the full benchmark required team of six NLP researchers working over eight months through multiple development cycles. smaller benchmark also reduces API evaluation costs, allows more frequent updates, and aligns with recent emphasis on high-quality, targeted evaluations over large, noisy ones [Rein et al., 2024, Maia Polo et al., 2024].2 SEALQA is also designed for stable evaluation with low run-to-run variance.3 Our key contributions are as follows: (1) We introduce SEALQA, dynamic benchmark designed to evaluate reasoning under noisy, conflicting, and ambiguous search results. SEALQA includes three flavors: SEAL-0, SEAL-HARD, and LONGSEAL, each targeting different challenges in searchaugmented reasoning; (2) We benchmark range of LLMS and uncover significant limitations in current retrieval-augmented approaches. Even state-of-the-art models struggle across SEALQA flavors when faced with conflicting or misleading context. On SEAL-0, performance remains low even for agentic models equipped with tools. We find that advanced reasoning models can be highly vulnerable to noisy search results. Notably, increasing test-time compute does not reliably improve performance across OPENAIs O-series of modelsperformance often plateaus or declines. LONGSEAL further reveals major weaknesses in long-context reasoning: while current frontier LLMS are more robust to lost-in-the-middle effects [Liu et al., 2024], they still fail to reliably identify and prioritize relevant evidence amid distractors; and (3) We release SEALQA publicly and commit to regular updates, which provides stable and evolving benchmark for advancing search-augmented reasoning in LLMS. 1Each question required over an hour on averageroughly 45 minutes to draft, plus additional time for review and revision. Many initial ideas were discarded as they failed to meaningfully challenge frontier LLMS. 2For example, the widely used GPQA-DIAMOND [Rein et al., 2024], compact set of 198 expert-vetted questions, demonstrates how small, carefully curated dataset can effectively assess models reasoning ability. 3Our questions often lead multiple models to fail across repeated attempts. 3 Figure 4: SEALQA questions test broad range of reasoning skills that are often overlooked in existing benchmarks."
        },
        {
            "title": "2 Data collection",
            "content": "In this section, we describe SEALQA, challenging QA benchmark designed to capture the complexity of real-world information-seeking. SEALQA rigorously evaluates models reasoning ability, robustness to noisy search results, and capacity to handle dynamic, real-world knowledge. Human annotators: To build SEALQA, we recruited NLP researchers4 as human annotators. They were shown small, diverse set of exemplars to illustrate the types of questions we sought to collect (Figure 4). Question types: Our questions span several categories: (Q1) advanced reasoning, which includes multi-hop reasoning, interpreting search-result plots, charts, or tables, and performing counting or calculations; (Q2) entity/event disambiguation, focused on distinguishing between similar entities or events; (Q3) temporal tracking, which requires identifying and differentiating instances of entities 4including the authors and their colleagues 4 over time; (Q4) cross-lingual reasoning, where the question is in English but answering it requires retrieving and reasoning over non-English sources; and (Q5) false-premise questions, aimed at debunking false premises. Annotation criteria: Annotators were instructed to write questions with single, unambiguous answer (e.g., specifying on what date rather than asking when). Each question must be supported by one or more webpages that justify the reference answer, ensuring that it is verifiable. For questions that involve fresh knowledge, annotators were required to cite regularly updated sources to support future answer updates. All questions were designed to appear natural yet trigger ambiguous, conflicting, or misleading search results when entered into search engine such as GOOGLE. Annotators also provided an explanation for each answer, including any necessary clarification or subtle reasoning. Finally, each question was refined iteratively until it consistently caused multiple models to fail across repeated attempts. Quality control: We employed rigorous multi-round review process. Each question was first reviewed by two or more graduate-level annotators, followed by expert approval. We conducted multiple rounds of data cleaning, including verification of supporting URLS, answer correctness, and question clarity. We excluded questions whose answers change too frequently. For each question, we also annotated the effective year (i.e., when the answer last changed) and an expected next review date to support future maintenance. Diversity: SEALQA questions vary in length, with an average of 31 tokens and maximum of 69. SEALQA also spans diverse domains: science and technology (26.8%), sports (22.0%), entertainment (21.7%), politics (9.1%), history and geography (8.3%), and others (12.2%).5 By question category, 72.4% involve advanced reasoning (Q1), 58.3% entity/event disambiguation (Q2), 13.7% temporal tracking (Q3), 5.5% cross-lingual reasoning (Q4), and 4.3% false-premise detection (Q5). We also classify questions by freshness [Vu et al., 2024]: 31.1% are never-changing (NEVER; answers never change), 43.7% slow-changing (SLOW; answers change over several years), and 25.2% fast-changing (FAST; answers typically change within year). By effective year, 22.0% reference 2025 events, 19.3% 2024, and 58.7% prior to 2024. Finally, we manually labeled each question based on whether the search results at evaluation time contain conflicting answers, including the correct one (CONFLICT.), or are entirely unhelpful (UNHELPFUL). Datasets: To curate SEAL-0, we tested each question against GPT-4O, GPT-4.1, their MINI variants [OpenAI, 2024a,b, 2025a], and LLAMA-4-SCOUT [Meta, 2025], each with and without browsing.6 Only questions where all models failed across 10-15 attempts were retained. This follows current practices for building challenging benchmarks; for example, SIMPLEQA [Wei et al., 2024] was also adversarially collected against GPT-4 responses. SEAL-0 was then combined with other rejected-but-difficult questions to form SEAL-HARD. For LONGSEAL, each SEAL-HARD question is paired with set of retrieved documents: one helpful (gold) document from annotator-provided webpages, and up to 50 hard negatives that appear relevant but are unhelpful.7 To ensure difficulty, we used CHATGPT 4O to filter out any negatives that might allow the correct answer to be inferred. The gold document was then randomly inserted among the negatives. LONGSEAL includes over 7.6K documents and serves as testbed for long-context reasoning under noisy retrieval conditions. Evaluation protocol: Models are evaluated with the CHATGPT 4O auto-rater from Wei et al. [2024], which takes the question, predicted answer, and reference answer as input and labels responses as correct, incorrect, or not attempted. The evaluation uses relaxed protocol that judges only whether the main answer is correct. 5Topic labels were assigned post-hoc using CHATGPT 4O. 6We applied FRESHPROMPT to LLAMA-4-SCOUT. 7To collect hard negatives, we used GOOGLE to retrieve the top 10 webpages per question and extracted their main content using TRAFILATURA [Barbaresi, 2021]. To add temporal diversity and potential conflicts, we retrieved 10 more pages limited to pre-2023 content. We also used CHATGPT 4O to generate three semantically related queries per question and collected documents for each. Duplicates were removed, and documents over 10K tokens were excluded. 5 Table 1: Accuracy on SEAL-0 and SEAL-HARD. Frontier LLMS face significant challenges on SEALQA questions. indicates results using CHATGPTs built-in search; all other search-based results use FRESHPROMPT [Vu et al., 2024]. Model GPT-4O-MINI GPT-4.1-MINI GPT-4O GPT-4.1 O3-MINI-MEDIUM O4-MINI-MEDIUM O3-MEDIUM LLAMA-3.2-3B LLAMA-3.1-70B LLAMA-4-SCOUT-17B-16E (109B) QWEN3-235B-A22B DEEPSEEK-R1-DISTILL-QWEN-1.5B DEEPSEEK-R1-DISTILL-QWEN-14B DEEPSEEK-R1-671B DEEPSEEK-R1-0528-671B knowl. cutoff type SEAL-0 SEAL-HARD w/o SEARCH w/ SEARCH w/o SEARCH w/ SEARCH Closed-source models Sep 30, 2023 May 31, 2024 Sep 30, 2023 May 31, 2024 Sep 30, 2023 May 31, 2024 May 31, 2024 CHAT CHAT CHAT CHAT REASON. AGENTIC AGENTIC Open-weight models December 1, 2023 December 2023 August 2024 CHAT CHAT CHAT - - - - - REASON. REASON. REASON. REASON. REASON. 0.0 0.0 0.0 0.0 2.7 - - 0.0 0.0 0.0 0.0 0.0 0.9 5.4 5.4 0.0 0.0 0.0 0.0 2.7 5.4 17.1 0.0 0.0 0.0 5.4 2.7 3.6 1.8 4.5 9.1 13.8 11.8 15.0 14.6 - - 1.6 0.0 5.9 4.3 0.0 0.9 22.4 19.3 13.4 11.8 15.0 20.5 12.2 19.7 34.6 3.5 6.3 5.9 11.4 1.6 10.6 11.0 15.4 Auto-rater reliability: To assess the auto-raters reliability, two authors independently evaluated 100 answers. Disagreements were resolved through discussion, yielding unified set of human ratings that agreed with the auto-rater 98% of the time."
        },
        {
            "title": "3 Experiments",
            "content": "Having established SEALQA, we now set out to evaluate how well todays LLMS reason over noisy search results when navigating dynamic, real-world knowledge. Our analysis reveals limitations in their ability to reconcile conflicting parametric (internal) and retrieved (external) knowledge. 3.1 Experiment setup 3.1.1 SEAL-0 and SEAL-HARD Baselines: We benchmark wide range of open-weight and proprietary models, including chatoriented models such as GPT-4O, GPT-4.1, their MINI variants [OpenAI, 2024a,b, 2025a], LLAMA3.1-70B [Grattafiori et al., 2024], LLAMA-3.2-3B [Meta, 2024], and LLAMA-4-SCOUT-17B-16EINSTRUCT [Meta, 2025]; advanced reasoning models like O3-MINI [OpenAI, 2025b], DEEPSEEK-R1671B, DEEPSEEK-R1-0528-671B, DEEPSEEK-R1-DISTILL-QWEN-14B/1.5B [Guo et al., 2025], and QWEN3-235B-A22B [Yang et al., 2025]; and agentic tool-use models such as O3 and O4-MINI [OpenAI, 2025c].8 We simply feed each question as prompt into each model, using temperature of 0 when configurable and the default value otherwise.9 For models without browsing capabilities, we apply FRESHPROMPT [Vu et al., 2024] or SELF-ASK [Press et al., 2023] to inject GOOGLE search results into the prompt. Advanced reasoning models are evaluated under medium reasoning effort settings when configurable, unless specified otherwise. Human experts: To estimate human performance, we asked three graduate-level NLP researchers (not involved in annotation) to independently answer sample of 50 SEAL-HARD questions. They had unlimited access to GOOGLE and could use any queries they deemed useful.10 8We used the OPENAI and TOGETHER.AI APIS for OPENAI and open-weight models, respectively. 9 OPENAIs O-series models only support fixed temperature of 1.0. 10Each question had 15-minute time limit. 6 indicates results using CHATGPTs built-in search; all other search-based results use FRESHPROMPT [Vu et al., 2024]. Table 2: On SEAL-HARD, LLMS tend to underperform on cross-lingual reasoning (Q4) and falsepremise detection (Q5) compared to advanced reasoning (Q1), entity/event disambiguation (Q2), and temporal tracking (Q3). Model GPT-4.1 O3-MINI O3 LLAMA-4-SCOUT DEEPSEEK-R1 / A / GPT-4.1 O3-MINI O3 LLAMA-4-SCOUT DEEPSEEK-R1 Q1 14.1 13.0 4.9 20.7 20.1 12.0 33.7 4.3 10.3 Q2 14.2 17.6 6.8 23.0 17.6 9.5 32.4 6.8 10. Q3 25.7 17.1 5.7 22.9 25.7 20.0 48.6 8.6 14.3 Q4 0.0 0.0 0.0 7.1 21.4 0.0 7.1 0.0 0.0 Q5 0.0 0.0 0.0 0.0 9.1 9.1 27.3 0.0 18. Table 3: Questions that involve rapidly changing information, i.e., fast-changing questions, pose significant challenges for LLMS on SEAL-HARD. Model W/O SEARCH NEVER SLOW FAST W/ SEARCH SLOW NEVER GPT-4.1 O3-MINI O3 LLAMA-4-SCOUT DEEPSEEK-R1 21.5 40.5 10.1 32.9 18.0 39.6 4.5 24.3 1.6 17.2 4.1 6.2 17.7 16.5 41.8 6.3 15. 24.3 10.8 39.6 4.5 9.9 FAST 17.2 9.4 17.2 7.8 7.8 Table 4: LLMS struggle with questions that involve recent information on SEAL-HARD. Model W/O SEARCH < 2024 2024 2025 < 2024 W/ SEARCH 2024 GPT-4.1 O3-MINI O3 LLAMA-4-SCOUT DEEPSEEK-R1 23.5 45.6 8.7 35. 6.1 26.5 4.1 8.2 0.0 10.7 0.0 0.0 25.5 15.4 46.3 7.4 14.8 20.4 6.1 26.5 6.1 6.1 2025 7.1 8.9 10.7 1.8 5. Table 5: On SEAL-HARD, performance degrades more when search results are uniformly unhelpful than when they contain conflicting answers. Model GPT-4.1 O3-MINI O3 LLAMA-4-SCOUT DEEPSEEK-R1 W/O SEARCH W/ SEARCH UNHELPFUL CONFLICT. UNHELPFUL CONFLICT. 14.5 11.8 3.6 20.9 15.3 16.6 7.6 23.6 18.2 10.9 33.6 4.5 9. 22.2 13.2 35.4 6.9 12.5 3.1.2 LONGSEAL Baselines: We benchmark GPT-4O-MINI, GPT-4.1-MINI, LLAMA-4-SCOUT-17B-16E-INSTRUCT, and additionally LLAMA-3.2-11B-VISION [Meta, 2024], with context windows of 128K, 1M, 1M, and 128K tokens, respectively. 7 Figure 5: Advanced reasoning models such as DEEPSEEK-R1-671B and O3-MINI are highly vulnerable to noisy search results. We follow Liu et al. [2024] to set up multi-document QA task, where model receives question and set of documents: one gold document that suggests the correct answer, and hard negatives. The gold document is randomly placed among the negatives. To answer correctly, the model must identify and use the gold document from its input context. We evaluate three values of k: 12, 20, and 30, sampling from 50 hard negatives per question. This setup allows us to assess how performance varies with the number of negatives and the position of the gold document. 3.2 Results on SEAL-0 and SEAL-HARD SEAL-0 and SEAL-HARD present significant challenges for frontier LLMS: Table 1 shows the accuracy of various LLMS on SEAL-0 and SEAL-HARD without access to search engine (W/O SEARCH). Models perform poorly without web access, with accuracies ranging from 0.0% to 5.4% on SEAL-0 and 0.0% to 22.4% on SEAL-HARD. While proprietary models tend to outperform open-weight ones, DEEPSEEK-R1-671B stands out as notable exception, achieving the best overall performance. Interestingly, model size does not consistently correlate with performance. For example, both LLAMA-3.2-3B and LLAMA-3.1-70B score 0.0% on SEAL-0, with the smaller model slightly outperforming the larger one on SEAL-HARD (1.6% vs. 0.0%). similar pattern holds for DEEPSEEK-R1-DISTILL-QWEN, which shows negligible improvement when scaled from 1.5B to 14B (0.0% 0.9%) on both datasets. Large mixture-ofexpert (MOE) models such as LLAMA-4-SCOUT-17B-16E (109B total parameters) and QWEN3-235BA22B also fail to generalize on SEAL-0 (0.0%) and yield only modest gains on SEAL-HARD (5.9% and 4.3%, respectively). Additionally, reasoning-focused models do not consistently outperform general-purpose chat models, as seen with QWEN3-235B-A22B and LLAMA-4-SCOUT-17B-16Ewith DEEPSEEK-R1-671B as the exception. Tables 2, 3, 4, and 5 show breakdown of SEAL-HARD results by question category (see Appendix A) for full results). Overall, models perform poorly across question categories, especially on crosslingual reasoning, false-premise detection, and questions that involve recent or rapidly changing information. Performance also degrades more when search results are uniformly unhelpful than when they contain conflicting answers.12 Naive search and integration can amplify noise rather than improve accuracy: Table 1 (W/O SEARCH) and Figure 5 show the effects of web search on model performance. In general, search improves accuracy across models. O3 and O4-MINIagentic reasoning models capable of using tools within CHATGPT, including web searchperform significantly better than others. O3 achieves the highest accuracy on both datasets: 16.2% on SEAL-0 and 34.3% on SEAL-HARD. 11The average prompt lengths across all examples are 27.6K, 54.5K, and 70.1K tokens, with 100%, 99.2%, and 96.7% of prompts fitting within the 128K context window of GPT-4O-MINI and LLAMA-3.2-11B, for = 12, 20, and 30, respectively. 12Additionally, we find that open-weight models like LLAMA-4-SCOUT and DEEPSEEK-R1 choose to not attempt questions more often than proprietary models such as GPT-4.1, O4-MINI, and O3 (see Appendix B). 8 Table 6: SEALQA also poses challenges for humans. Model accuracy GPT-4O GPT-4.1 O3-MINI-HIGH O4-MINI-HIGH O3-HIGH human (avg.) human (best) 6.0 6.0 8.0 12.0 28.0 23.3 30.0 Our results suggest that training models to understand and execute search queries, as done in CHATGPTs built-in search, is more effective than retrieval-based prompting methods like FRESHPROMPT. While CHATGPT-4.1 gains performance boost from built-in search (+5.5%), FRESHPROMPT slightly reduces its accuracy (15.0% 14.6%). Built-in search generally improves performance on SEAL-HARD for both GPT-4.0 and GPT-4.1. With that said, FRESHPROMPT remains useful for most open-weight models without tool-use training. For example, QWEN3-235B-A22B and DEEPSEEKR1-DISTILL-QWEN-14B achieve gains of +7.1% and +9.7%, respectively, on SEAL-HARD when using FRESHPROMPT. However, search can sometimes be detrimental. GPT-4.1-MINI, when equipped with built-in search, drops in accuracy from 13.8% to 11.8%. Since SEALQA questions are designed to elicit conflicting or noisy search results, naive retrieval and integration can harm model accuracy. Advanced reasoning models can be highly vulnerable to noisy search results: As shown in Table 1 (W/O SEARCH) and Figure 5, DEEPSEEK-R1-671B and O3-MINI are dramatically more sensitive to input noise than other models. For example, DEEPSEEK-R1-671Bs performance drops from 22.4% to 11.0% when using FRESHPROMPT. Our ablation (Table 3 and Table 4) reveals that FRESHPROMPT improves DEEPSEEK-R1-671Bs performance on fast-changing (+1.6%) and 2025-specific (+5.4%) questions, but leads to large drops on static or older questions (-17.7% on never-changing, and -20.8% on pre-2024). GPT-4.1-MINI shows similar trend with CHATGPTs built-in search, though the decline is less pronounced. In contrast, open-weight models with weaker reasoning capabilities (e.g., QWEN3-235B-A22B and DEEPSEEK-R1-DISTILL-QWEN-14B) consistently benefit from FRESHPROMPT. Among retrieval-based prompting methods, SELF-ASK, which decomposes questions into subquestions, is generally more effective than FRESHPROMPT, which issues direct searches and thus triggers more noise for SEALQAs adversarial questions. However, both methods harm the accuracy of DEEPSEEK-R1-671B and O3-MINI. Test-time scaling does not lead to reliable gains on SEALQA: Models like O3-MINI, O3, and O4-MINI have shown strong reasoning capabilities, with consistent improvements from increased test-time compute. However, we find that this approach does not yield reliable gains on SEALQA questions. Figure 1 illustrates test-time scaling effects on SEAL-0 and SEAL-HARD questions across different reasoning effort settingslow, medium, and highwhere higher levels correspond to more reasoning tokens. On SEAL-0, O3-MINIs accuracy plateaus despite scaling, with scores of 1.8, 2.7, and 1.8 at low, medium, and high effort levels, respectively. O4-MINIs accuracy peaks at low effort (6.3%), but drops with more compute at medium (5.4%) and high (4.5%) settings. While O3 achieves the highest overall accuracy, scaling also fails to provide reliable gains on SEAL-0, with accuracies of 11.7%, 17.1%, and 14.4% across the three effort levels. Similar trends are observed on SEAL-HARD. We conjecture that increased reasoning over noisy search results may impair performance. As test-time compute grows, longer chains of thought can amplify spurious or irrelevant information, entangling the model in misleading evidence and ultimately reducing accuracy. The effect of repeated sampling: We also explore the effect of repeated sampling [Brown et al., 2024]. Each model is sampled five times, and we count an answer as correct if any of the five attempts is correct. Due to O3s high API cost, this experiment is limited to O3-MINI and O4-MINI, evaluated on SEAL-0 at medium reasoning effort. Under this setting, O3-MINI and O4-MINI reach 9% and 16.2% 9 Figure 6: Frontier LLMS fail to reliably identify relevant documents in LONGSEAL when numerous distractors are present, despite being less prone to lost-in-the-middle failures [Liu et al., 2024]. accuracy, respectively. These results again demonstrate that SEAL-0 is extremely challenging, even for agentic reasoning models with full tool access like O4-MINI. SEALQA also poses challenges for humans: Table 6 reports performance on sample of 50 SEAL-HARD questions. Frontier LLMS generally lag behind human performance: the best model, O3-HIGH, reaches 28.0% accuracy, slightly below the top human score of 30.0%, and is the only model to exceed the average human accuracy of 23.3%. On average, human participants skipped only 11.3% of questions, and in 64.7% of cases reported finding an answer within five minutes. Despite this, overall accuracy remained low. 3.3 Results on LONGSEAL We now switch gears to discuss our evaluation results on LONGSEAL (Figure 6). Frontier LLMS struggle on LONGSEAL with increased distractors: All models exhibit clear drop in accuracy as the number of hard negatives increases. For example, when the gold document appears immediately after the question (i.e., at the 1st position), GPT-4.1-MINIs accuracy decreases from 32.7% at = 12 (i.e., 12 hard negatives) to 29.9% at = 20 and 29.5% at = 30. The degradation is more pronounced in smaller or less capable models: GPT-4O-MINI falls from 24.0% to 6.3% and then 3.9%, while LLAMA-3.2-11B drops from 10.2% to 2.0% and 2.4%. These results suggest that simply increasing context size does not guarantee effective context use. When many hard negatives are present, models often struggle to identify and prioritize the gold document. The primary failure mode appears to be the inability to reliably filter relevant from 10 irrelevant content at scale. High distractor density impairs relevance estimation, even when all input documents fit within the context window. This suggests need for architectural advances or training strategies that enhance implicit retrieval and salience detection to improve performance in large-context, multi-document QA settings. Absence of classic positional bias in Liu et al. [2024]: Unlike earlier work that reports strong lost in the middle effect, our results show no clear U-shaped positional trend. GPT-4.1-MINI maintains stable accuracy across positions, with only minor fluctuations from start to end; even at = 30, its performance varies little between early, middle, and late placements. LLAMA-4-SCOUT shows slight improvement toward later positions, but no consistent dip in the middle. This absence of positional bias suggests that newer models may have mitigated some of the structural weaknesses previously associated with position encoding. However, the broader challenge remains: regardless of position, models often fail to recognize the gold document when distractors are numerous. The issue has shifted from sensitivity to position to more general difficulty in modeling relevance within large, noisy contexts. 3.4 Qualitative analysis Two authors independently evaluated 100 responses from six models: GPT-4.1 (without search, with FRESHPROMPT, and with built-in search); O3-MINI, O3 (both under medium reasoning effort); and DEEPSEEK-R1-671B. Our analysis reveals clear differences across models in their reasoning and use of external knowledge. Among the GPT-4.1 variants, the base model without search occasionally includes relevant URLS but often produces inaccurate answers due to outdated knowledge. The FRESHPROMPT version is better at detecting false-premise questions and tends to be more concise, though its accuracy depends heavily on retrieval quality. The built-in search variant produces more logically coherent answers and higher-quality citations, which supports factual verification, though it still exhibits occasional errors. We find that O3 is capable of producing more informed and concise responses; however, it sometimes overthinks and mistakenly rejects valid answers. O3-MINIs outputs are easy to follow, yet the model occasionally misses relevant reasoning paths. Notably, both models generally acknowledge their knowledge cutoffs for time-sensitive queries, seek clarification, and suggest alternative strategies to support user decision-making. Finally, DEEPSEEK-R1-671B tends to overthink and frequently repeats phrases like wait, let me think, and alternatively without arriving at clear conclusion. Its lack of structured formatting also makes its responses harder to follow compared to GPT-4.1 and O3 models."
        },
        {
            "title": "4 Related work",
            "content": "Reasoning under knowledge conflict: Prior work shows that LLMS can be vulnerable to misinformation [Pan et al., 2023], irrelevant context [Shi et al., 2023], and conflicting sources [Kazemi et al., 2023]. Retrieval quality strongly influences model output; however, contradictions between sources often have only minimal effect on model confidence [Chen et al., 2022]. Wan et al. [2024] find that models prioritize surface-level relevance over credibility indicators such as scientific references or neutral tone. While LLMS can detect conflict [Jiayang et al., 2024], they struggle to resolve it [Wang et al., 2024, Xu et al., 2024a]. Models also exhibit confirmation bias by favoring evidence that aligns with their parametric memory [Chen et al., 2022], often resolving contradictions in favor of internal knowledge [Jin et al., 2024, Jiayang et al., 2024]. Still, Xie et al. [2024b] show that models remain highly receptive to contradictory external evidence when it is coherent and convincing. Additional biases include favoring frequent evidence and relying on memory for common knowledge but external sources for long-tail knowledge [Jin et al., 2024]. See Xu et al. [2024b] for comprehensive survey. Building on these insights, recent work has introduced benchmarks targeting specific types of retrieval conflicts. Some focus on specific challenges, such as entity ambiguity [AMBIGDOCS; Lee et al., 2024], credible yet conflicting sources [WIKICONTRADICT; Hou et al., 2024], debatable questions [DEBATEQA; Xu et al., 2024a], and Shaier et al. [2024] for citation-aware QA under ambiguity. Other assess model behavior under noisy contexts, such as faithfulness under unanswerable, inconsistent, and counterfactual contexts [FAITHEVAL; Ming et al., 2025], or reasoning over conflicting contexts [QACC; Liu et al., 2025], as well as analyzing what shapes predictions, such as textual features [CONFLICTINGQA; Wan et al., 2024] and conflict sources [CONFLICTBANK; Su et al., 2024]. Most recently, Wang et al. [2025] augment AMBIGDOCS examples with simulated ambiguity, misinfor11 mation, and noise to create RAMDOCS. Our work complements this growing body by introducing unified benchmark that brings together real-world challengesambiguity, misinformation, temporal drift, and noisy retrievalthrough expert-curated, naturally occurring questions, without relying on synthetic augmentation. Measuring factuality and reasoning in LLMS: SEALQA aligns with growing body of work on time-sensitive QA benchmarks [Chen et al., 2021, Zhang and Choi, 2021, Liska et al., 2022, Kasai et al., 2023, Vu et al., 2024, inter alia]. SEALQA also fits among recent challenging benchmarks that evaluate LLMS across factuality, reasoning, and retrieval. Benchmarks like MMLU [Hendrycks et al., 2021a], MATH [Hendrycks et al., 2021b], GPQA [Rein et al., 2024], and HUMANITYS LAST EXAM [Phan et al., 2025] focus on academic or expert-level reasoning. Others evaluate open-domain retrieval [FRESHSTACK; Thakur et al., 2025], multi-hop, multi-document reasoning [FRAMES; Krishna et al., 2025], and real-world software engineering tasks [SWE-BENCH; Xie et al., 2024a]. Targeted evaluations such as SIMPLEQA [Wei et al., 2024] and BROWSECOMP [Wei et al., 2025] measure factual recall and web browsing competence. These datasets push different axes of model performance, and SEALQA complements them by providing unified benchmark spanning all three dimensionsfactuality, reasoning, and retrievalthrough naturally occurring, adversarially curated questions that reflect real-world QA complexity."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce SEALQA, benchmark for evaluating Search-Augmented Language Models on challenging factual questions where web search results may be conflicting, noisy, or irrelevant. SEALQA includes three flavors: SEAL-0, which includes questions that challenge todays most capable models; SEAL-HARD, wider collection of difficult queries; and LONGSEAL, which is designed to test long-context reasoning in needle-in-a-haystack settings. Our evaluations show that frontier LLMS, including agentic models with search tools, perform poorly and are vulnerable to noisy search results, with increased test-time compute often not leading to reliable performance gains. LONGSEAL in particular highlights the difficulty models face in identifying relevant information amid distractors, though they exhibit reduced susceptibility to the lost-in-the-middle issue. We hope that SEALQA will spur more fundamental research into tackling real-world challenges in retrieval-augmented reasoning."
        },
        {
            "title": "6 Limitations",
            "content": "SEALQA has several limitations that suggest directions for future work. First, most SEALQA questions are constructed to have single short answer, which does not fully capture the complexity of realworld information-seeking tasks. In practice, users may pose questions with no definitive answer, multiple plausible answers, or ones that require long-form responses. Addressing such cases would require LLMS not only to generate nuanced answers but also to cite credible sources and be evaluated based on source quality. Second, while some questions involve retrieving and reasoning over nonEnglish sources, it is not designed to systematically assess multilingual capabilities or cultural and linguistic generalization, an important area for future research. Finally, although SEALQA is updated periodically to maintain relevance, some answers may still become stale between releases due to the rapidly evolving nature of web content. Users should therefore be aware that the benchmark might not always reflect the most recent developments in real time."
        },
        {
            "title": "7 Ethical considerations and risks",
            "content": "Despite rigorous multi-round vetting process, some concerns remain in SEALQA. First, manual curation may inadvertently introduce cultural, regional, or domain-specific biases from annotators. While we aim for neutrality and diversity, such biases are difficult to fully eliminate. Second, as SEALQA includes questions based on dynamic or evolving knowledge, some reference answers may become outdated. To address this, we provide verifiable sources for each question and commit to regularly updating answers to ensure accuracy. Finally, the paired real-world search data in LONGSEAL may include copyrighted, sensitive, or restricted content. We urge researchers to follow ethical guidelines and respect intellectual property rights when publishing or sharing results."
        },
        {
            "title": "8 Acknowledgments",
            "content": "We thank Quyet Do, Amartya Dutta, and Rishab Balasubramanian for their contributions in creating and reviewing SEALQA questions. We are also grateful to Yeana Bond, Pin-Jie Lin, Rishab Balasubramanian, and Rituraj Sharma for their involvement in our human performance evaluation. We thank the VT LLMS group for valuable discussions and feedback. Finally, we thank Arie Cattan, Mohit Iyyer, Marzena Karpinska for their helpful suggestions."
        },
        {
            "title": "References",
            "content": "Anthropic. Introducing Claude 4. 2025. URL https://www.anthropic.com/news/claude-4. Adrien Barbaresi. Trafilatura: web scraping library and command-line tool for text discovery and extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 122131, 2021. URL https://aclanthology.org/2021.acl-demo. 15/. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. URL https://arxiv.org/abs/2407.21787. Hung-Ting Chen, Michael Zhang, and Eunsol Choi. Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 22922307, 2022. URL https://aclanthology.org/2022.emnlp-main.146/. for answering time-sensitive questions. Wenhu Chen, Xinyi Wang, William Yang Wang, Indataset formation Processing Systems Track on Datasets and Benchmarks, volume 1, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/ 2021/file/1f0e3dad99908345f7439f8ffabdffc4-Paper-round2.pdf. and William Yang Wang. In Proceedings of the Neural Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: recurrent depth approach. arXiv preprint arXiv:2502.05171, 2025. URL https://arxiv.org/abs/2502.05171. Google. Gemini 2.5: Our most intelligent models are getting even better. 2025. URL https: //blog.google/technology/google-deepmind/google-gemini-updates-io-2025/. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. URL https://arxiv.org/abs/2407.21783. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. URL https://arxiv.org/ abs/2501.12948. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021a. URL https://openreview.net/forum?id=d7KBjmI3GmQ. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021b. URL https://datasets-benchmarks-proceedings.neurips.cc/paper_files/ paper/2021/file/be83ab3ecd0db773eb2dc1b0a17836a1-Paper-round2.pdf. Yufang Hou, Alessandra Pascale, Javier Carnerero-Cano, Tigran Tchrakian, Radu Marinescu, Elizabeth Daly, Inkit Padhi, and Prasanna Sattigeri. WikiContradict: benchIn Admark for evaluating llms on real-world knowledge conflicts from wikipedia. vances in Neural Information Processing Systems, volume 37, pages 109701109747, 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/ c63819755591ea972f8570beffca6b1b-Paper-Datasets_and_Benchmarks_Track.pdf. Cheng Jiayang, Chunkit Chan, Qianqian Zhuang, Lin Qiu, Tianhang Zhang, Tengxiao Liu, Yangqiu Song, Yue Zhang, Pengfei Liu, and Zheng Zhang. ECON: On the detection and resolution of evidence conflicts. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 78167844, 2024. URL https://aclanthology.org/2024.emnlp-main. 447/. 14 Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-R1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. URL https://arxiv.org/abs/2503.09516. Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin Xu, Li Qiuxia, and Jun Zhao. Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrievalIn Proceedings of the 2024 Joint International Conference on augmented language models. Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1686716878, 2024. URL https://aclanthology.org/2024.lrec-main.1466/. Jungo Kasai, Keisuke Sakaguchi, yoichi takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah Smith, Yejin Choi, and Kentaro Inui. RealTime QA: What's the answer right In Advances in Neural Information Processing Systems, volume 36, pages 49025 now? 49043, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 9941624ef7f867a502732b5154d30cb7-Paper-Datasets_and_Benchmarks.pdf. Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, Xin Xu, Vaiva Imbrasaite, and Deepak Ramachandran. BoardgameQA: dataset for natural language reasoning with contradictory information. In Advances in Neural Information Processing Systems, volume 36, pages 39052 39074, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 7adce80e86aa841490e6307109094de5-Paper-Datasets_and_Benchmarks.pdf. Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. Fact, fetch, and reason: unified evaluation of retrievalIn Proceedings of the 2025 Conference of the Nations of the Ameriaugmented generation. cas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 47454759, 2025. URL https://aclanthology.org/2025. naacl-long.243/. Yoonsang Lee, Xi Ye, and Eunsol Choi. Ambigdocs: Reasoning across documents on different entities under the same name. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=mkYCfO822n. Adam Liska, Tomas Kocisky, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal, Cyprien De Masson DAutume, Tim Scholtes, Manzil Zaheer, Susannah Young, Ellen GilsenanMcmahon, Sophia Austin, Phil Blunsom, and Angeliki Lazaridou. StreamingQA: benchmark for adaptation to new knowledge over time in question answering models. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 1360413622. PMLR, 2022. URL https://proceedings.mlr.press/v162/ liska22a.html. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. URL https://aclanthology. org/2024.tacl-1.9/. Siyi Liu, Qiang Ning, Kishaloy Halder, Zheng Qi, Wei Xiao, Phu Mon Htut, Yi Zhang, Neha Anna John, Bonan Min, Yassine Benajiba, and Dan Roth. Open domain question answering with conflicting contexts. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 18381854, 2025. URL https://aclanthology.org/2025.findings-naacl.99/. Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and Mikhail Yurochkin. tinyBenchmarks: evaluating LLMs with fewer examples. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 3430334326, 2024. URL https://proceedings.mlr.press/v235/maia-polo24a. html. Meta. Llama 3.2: Revolutionizing edge ai customizable llama-3-2-connect-2024-vision-edge-mobile-devices/. models. 2024. URL and open, vision with https://ai.meta.com/blog/ Meta. The Llama 4 herd: The beginning of new era of natively multimodal ai innovation. 2025. URL https://ai.meta.com/blog/llama-4-multimodal-intelligence/. 15 Yifei Ming, Senthil Purushwalkam, Shrey Pandit, Zixuan Ke, Xuan-Phi Nguyen, Caiming Xiong, and Shafiq Joty. FaithEval: Can your language model stay faithful to context, even if the moon is made of marshmallows. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=UeVx6L59fg. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. URL https://arxiv.org/abs/2501.19393. OpenAI. GPT-4o system card. 2024a. URL https://openai.com/index/ gpt-4o-system-card/. OpenAI. GPT-4o mini: advancing cost-efficient intelligence. 2024b. URL https://openai.com/ index/gpt-4o-mini-advancing-cost-efficient-intelligence/. OpenAI. OpenAI o1 system card. 2024c. URL https://openai.com/index/ openai-o1-system-card/. OpenAI. Introducing GPT-4.1 in the API. 2025a. URL https://openai.com/index/gpt-4-1/. OpenAI. OpenAI o3-mini system card. 2025b. URL https://openai.com/index/ o3-mini-system-card/. OpenAI. OpenAI o3 and o4-mini system card. 2025c. URL https://openai.com/index/ o3-o4-mini-system-card/. Liangming Pan, Wenhu Chen, Min-Yen Kan, and William Yang Wang. Attacking open-domain question answering by injecting misinformation. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 525539, 2023. URL https://aclanthology.org/2023.ijcnlp-main.35/. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. URL https://arxiv.org/abs/2501.14249. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. MeasurIn Findings of the Associing and narrowing the compositionality gap in language models. ation for Computational Linguistics: EMNLP 2023, pages 56875711, 2023. URL https: //aclanthology.org/2023.findings-emnlp.378/. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof Q&A benchmark. In First Conference on Language Modeling, 2024. URL https://openreview. net/forum?id=Ti67584b98. Sagi Shaier, Ari Kobren, and Philip V. Ogren. Adaptive question answering: Enhancing language model proficiency for addressing knowledge conflicts with source citations. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1722617239, 2024. URL https://aclanthology.org/2024.emnlp-main.956/. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael Schärli, and Denny Zhou. Large language models can be easily distracted by irrelevant In Proceedings of the 40th International Conference on Machine Learning, volcontext. ume 202 of Proceedings of Machine Learning Research, pages 3121031227, 2023. URL https://proceedings.mlr.press/v202/shi23a.html. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. URL https://arxiv.org/abs/2408.03314. 16 Zhaochen Su, Jun Zhang, Xiaoye Qu, Tong Zhu, Yanshu Li, Jiashuo Sun, Juntao Li, Min Zhang, and Yu Cheng. ConflictBank: benchmark for evaluating the influence of knowledge conflicts in llms. In Advances in Neural Information Processing Systems, volume 37, pages 103242103268, 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/ baf4b960d118f838ad0b2c08247a9ebe-Paper-Datasets_and_Benchmarks_Track.pdf. Nandan Thakur, Jimmy Lin, Sam Havens, Michael Carbin, Omar Khattab, and Andrew Drozdov. FreshStack: Building realistic benchmarks for evaluating retrieval on technical documents. arXiv preprint arXiv:2504.13128, 2025. URL https://arxiv.org/abs/2504.13128. Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. FreshLLMs: Refreshing large language models with search engine augmentation. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1369713720, 2024. URL https://aclanthology.org/2024.findings-acl. 813/. Alexander Wan, Eric Wallace, and Dan Klein. What evidence do language models find convincing? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 74687484, 2024. URL https://aclanthology.org/2024. acl-long.403/. Han Wang, Archiki Prasad, Elias Stengel-Eskin, and Mohit Bansal. Retrieval-augmented generation with conflicting evidence. arXiv preprint arXiv:2504.13079, 2025. URL https://arxiv.org/ abs/2504.13079. Yike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. Resolving knowledge conflicts in large language models. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=ptvV5HGTNN. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. URL https://arxiv.org/abs/2411.04368. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. BrowseComp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. URL https://arxiv.org/abs/2504.12516. xAI. Grok 3 beta The age of reasoning agents. 2025. URL https://x.ai/news/grok-3. Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=VTF8yNQM66. Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview. net/forum?id=auKAUJZMO6. Rongwu Xu, Xuan Qi, Zehan Qi, Wei Xu, and Zhijiang Guo. DebateQA: Evaluating question answering on debatable knowledge. arXiv preprint arXiv:2408.01419, 2024a. URL https: //arxiv.org/abs/2408.01419. Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei In Proceedings of the 2024 Conference Xu. Knowledge conflicts for LLMs: survey. on Empirical Methods in Natural Language Processing, pages 85418565, 2024b. URL https://aclanthology.org/2024.emnlp-main.486/. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. URL https://arxiv.org/abs/2505.09388. Michael Zhang and Eunsol Choi. SituatedQA: Incorporating extra-linguistic contexts into QA. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 73717387, 2021. URL https://aclanthology.org/2021.emnlp-main.586/. SEAL-HARD results by question category Tables 7, 8, 9, 10, and 11 show breakdown of SEAL-HARD results by question category. Overall, models perform poorly across question categories, especially on cross-lingual reasoning, falsepremise detection, and questions that involve recent or rapidly changing information. Performance also degrades more when search results are uniformly unhelpful than when they contain conflicting answers. 18 indicates results using CHATGPTs built-in search; all other search-based results use FRESHPROMPT [Vu et al., 2024]. Table 7: On SEAL-HARD, LLMS tend to underperform on cross-lingual reasoning (Q4) and falsepremise detection (Q5) compared to advanced reasoning (Q1), entity/event disambiguation (Q2), and temporal tracking (Q3). Model GPT-4O-MINI GPT-4.1-MINI GPT-4O GPT-4.1 O3-MINI-MEDIUM O4-MINI-MEDIUM O3-MEDIUM LLAMA-3.2-3B LLAMA-3.1-70B LLAMA-4-SCOUT-17B-16E (109B) QWEN3-235B-A22B DEEPSEEK-R1-DISTILL-QWEN-1.5B DEEPSEEK-R1-DISTILL-QWEN-14B DEEPSEEK-R1-671B DEEPSEEK-R1-0528-671B W/O SEARCH Q3 Q2 Q4 Q1 Closed-source models 6.5 10.9 9.8 14.1 13.0 Open-weight models 7.4 15.5 13.5 14.2 17.6 1.4 4.7 6.8 4.1 2.0 8.1 23.0 19. 22.9 22.9 11.4 25.7 17.1 0.0 5.7 5.7 5.7 0.0 17.1 22.9 20.0 7.1 14.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 7.1 7.1 0.0 9.1 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 9. 0.0 3.3 4.9 2.2 1.1 6.5 20.7 18.5 Table 8: On SEAL-HARD, LLMS tend to underperform on cross-lingual reasoning (Q4) and falsepremise detection (Q5) compared to advanced reasoning (Q1), entity/event disambiguation (Q2), and temporal tracking (Q3). Model GPT-4O-MINI GPT-4.1-MINI GPT-4O GPT-4.1 O3-MINI-MEDIUM O4-MINI-MEDIUM O3-MEDIUM LLAMA-3.2-3B LLAMA-3.1-70B LLAMA-4-SCOUT-17B-16E (109B) QWEN3-235B-A22B DEEPSEEK-R1-DISTILL-QWEN-1.5B DEEPSEEK-R1-DISTILL-QWEN-14B DEEPSEEK-R1-671B DEEPSEEK-R1-0528-671B Q2 Closed-source models Q1 W/ SEARCH Q3 Q4 Q5 17.1 14.3 17.1 25.7 20.0 14.3 48.6 8.6 14.3 8.6 14.3 0.0 25.7 14.3 17.1 14.3 0.0 7.1 21.4 0.0 0.0 7.1 0.0 7.1 0.0 0.0 0.0 0.0 0.0 7.1 9.1 0.0 0.0 9.1 9.1 9.1 27.3 0.0 9.1 0.0 18.2 0.0 18.2 18.2 18.2 11.4 8.2 11.4 20.1 12.0 17.9 33.7 10.8 11.5 15.5 17.6 9.5 20.9 32.4 Open-weight models 2.7 4.3 4.3 9.2 1.1 8.2 10.3 15.2 2.7 4.7 6.8 10.8 2.7 9.5 10.8 12.8 19 indicates results using CHATGPTs built-in search; all other search-based results use FRESHPROMPT [Vu et al., 2024]. Table 9: Questions that involve rapidly changing information, i.e., fast-changing questions, pose significant challenges for LLMS on SEAL-HARD. Model GPT-4O-MINI GPT-4.1-MINI GPT-4O GPT-4. O3-MINI-MEDIUM O4-MINI-MEDIUM O3-MEDIUM LLAMA-3.2-3B LLAMA-3.1-70B LLAMA-4-SCOUT-17B-16E (109B) QWEN3-235B-A22B DEEPSEEK-R1-DISTILL-QWEN-1.5B DEEPSEEK-R1-DISTILL-QWEN-14B DEEPSEEK-R1-671B DEEPSEEK-R1-0528-671B W/O SEARCH W/ SEARCH NEVER SLOW FAST NEVER SLOW FAST Closed-source models 15.2 20.3 16.5 21.5 40.5 9.0 15.3 12.6 18.0 39.6 Open-weight models 1.3 7.6 10.1 7.6 0.0 7.6 32.9 31.6 0.9 2.7 4.5 3.6 1.8 9.0 24.3 18. 1.6 3.1 4.7 1.6 17.2 0.0 3.1 4.1 1.6 1.6 4.7 6.2 6.3 16.5 12.7 15.2 17.7 16.5 29.1 41.8 3.8 6.3 6.3 12.7 1.3 10.1 15.2 19.0 10.8 10.8 15.3 24.3 10.8 18.0 39.6 4.5 8.1 4.5 8.1 2.7 9.0 9.9 14. 14.1 12.5 14.1 17.2 9.4 10.9 17.2 1.6 3.1 7.8 15.6 0.0 14.1 7.8 12.5 Table 10: LLMS struggle with questions that involve recent information on SEAL-HARD. Model GPT-4O-MINI GPT-4.1-MINI GPT-4O GPT-4.1 O3-MINI-MEDIUM O4-MINI-MEDIUM O3-MEDIUM LLAMA-3.2-3B LLAMA-3.1-70B LLAMA-4-SCOUT-17B-16E (109B) QWEN3-235B-A22B DEEPSEEK-R1-DISTILL-QWEN-1.5B DEEPSEEK-R1-DISTILL-QWEN-14B DEEPSEEK-R1-671B DEEPSEEK-R1-0528-671B W/O SEARCH W/ SEARCH BEFORE 2024 Closed-source models 2024 BEFORE 2024 2024 2025 0.0 1.8 1.8 0.0 10.7 0.0 1.8 0.0 0.0 1.8 0.0 0.0 5.4 13.4 20.1 16.8 23.5 45.6 6.1 8.2 8.2 6.1 26.5 Open-weight models 0.0 2.0 4.1 2.0 2.0 6.1 8.2 10.2 1.3 6.0 8.7 6.7 0.7 10.7 35.6 27.5 20 16.1 10.7 15.4 25.5 15.4 27.5 46.3 5.4 8.7 7.4 12.8 2.7 11.4 14.8 19.5 16.3 20.4 18.4 20.4 6.1 14.3 26.5 2.0 6.1 6.1 16.3 0.0 14.3 6.1 14.3 3.6 7.1 10.7 7.1 8.9 3.6 10.7 0.0 0.0 1.8 3.6 0.0 5.4 5.4 5.4 indicates results using CHATGPTs built-in search; all other search-based results use FRESHPROMPT [Vu et al., 2024]. Table 11: On SEAL-HARD, performance degrades more when search results are uniformly unhelpful than when they contain conflicting answers. Model GPT-4O-MINI GPT-4.1-MINI GPT-4O GPT-4. O3-MINI-MEDIUM O4-MINI-MEDIUM O3-MEDIUM LLAMA-3.2-3B LLAMA-3.1-70B LLAMA-4-SCOUT-17B-16E (109B) QWEN3-235B-A22B DEEPSEEK-R1-DISTILL-QWEN-1.5B DEEPSEEK-R1-DISTILL-QWEN-14B DEEPSEEK-R1-671B DEEPSEEK-R1-0528-671B W/O SEARCH W/ SEARCH UNHELPFUL CONFLICTING UNHELPFUL CONFLICTING Closed-source models 7.2 10.0 9.0 14.5 11.8 10.4 16.6 13.8 15.3 16.6 Open-weight models 0.0 1.8 3.6 3.6 0.0 2.7 20.9 18.2 1.3 6.2 7.6 4.8 2.0 11.1 23.6 20.1 10.9 10.0 11.8 18.2 10.9 16.3 33.6 2.7 4.5 4.5 8.2 2.7 7.3 9.1 11.8 15.3 13.2 17.4 22.2 13.2 22.2 35.4 4.2 7.6 6.9 13.9 0.7 13.2 12.5 18.1 21 SEAL-HARD results by answer type Figure 7 shows SEAL-HARD results broken down by answer type: correct, incorrect, and not attempted. We find that open-weight models like LLAMA-4-SCOUT and DEEPSEEK-R1 choose to not attempt questions more often than proprietary models such as GPT-4.1, O4-MINI, and O3. Figure 7: On SEAL-HARD, open-weight models like LLAMA-4-SCOUT and DEEPSEEK-R1 choose to not attempt questions more often than proprietary models such as GPT-4.1, O4-MINI, and O3."
        }
    ],
    "affiliations": [
        "Virginia Tech, Blacksburg, VA 24061"
    ]
}