{
    "paper_title": "Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation",
    "authors": [
        "Konstantin Egorov",
        "Stepan Botman",
        "Pavel Blinov",
        "Galina Zubkova",
        "Anton Ivaschenko",
        "Alexander Kolsanov",
        "Andrey Savchenko"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Progress in remote PhotoPlethysmoGraphy (rPPG) is limited by the critical issues of existing publicly available datasets: small size, privacy concerns with facial videos, and lack of diversity in conditions. The paper introduces a novel comprehensive large-scale multi-view video dataset for rPPG and health biomarkers estimation. Our dataset comprises 3600 synchronized video recordings from 600 subjects, captured under varied conditions (resting and post-exercise) using multiple consumer-grade cameras at different angles. To enable multimodal analysis of physiological states, each recording is paired with a 100 Hz PPG signal and extended health metrics, such as electrocardiogram, arterial blood pressure, biomarkers, temperature, oxygen saturation, respiratory rate, and stress level. Using this data, we train an efficient rPPG model and compare its quality with existing approaches in cross-dataset scenarios. The public release of our dataset and model should significantly speed up the progress in the development of AI medical assistants."
        },
        {
            "title": "Start",
            "content": "Gaze into the Heart: Multi-View Video Dataset for rPPG and Health Biomarkers Estimation Stepan Botman Sber AI Lab Moscow, Russia SABotman@sber.ru Konstantin Egorov Sber AI Lab Moscow, Russia egorov.k.ser@sber.ru Pavel Blinov Sber AI Lab Moscow, Russia Blinov.P.D@sber.ru 5 2 0 2 5 2 ] . [ 1 4 2 9 7 1 . 8 0 5 2 : r Galina Zubkova Sber AI Lab Moscow, Russia GVZubkova@sber.ru Anton Ivaschenko Samara State Medical University Samara, Russia an.v.ivaschenko@samsmu.ru Alexander Kolsanov Samara State Medical University Samara, Russia a.v.kolsanov@samsmu.ru Andrey Savchenko Sber AI Lab ISP RAS Research Center for Trusted Artificial Intelligence Moscow, Russia avsavchenko@hse.ru Figure 1: Our multimodal data capture setup (left) and images from three video sources with superimposed facial mesh."
        },
        {
            "title": "ABSTRACT",
            "content": "Progress in remote PhotoPlethysmoGraphy (rPPG) is limited by the critical issues of existing publicly available datasets: small size, privacy concerns with facial videos, and lack of diversity in conditions. The paper introduces novel comprehensive large-scale multi-view video dataset for rPPG and health biomarkers estimation. Our dataset comprises 3600 synchronized video recordings from 600 subjects, captured under varied conditions (resting and post-exercise) using multiple consumer-grade cameras at different angles. To enable multimodal analysis of physiological states, each recording is paired with 100 Hz PPG signal and extended health metrics, such as electrocardiogram, arterial blood pressure, biomarkers, temperature, oxygen saturation, respiratory rate, and stress level. Using this data, we train an efficient rPPG model and compare its quality with existing approaches in cross-dataset scenarios. The public release of our dataset and model should significantly speed up the progress in the development of AI medical assistants. CCS CONCEPTS Computing methodologies Biometrics; Social and professional topics Remote medicine; Gender; Age."
        },
        {
            "title": "KEYWORDS",
            "content": "Telemedicine, Video, rPPG, biosignals"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "Pulse wave analysis is non-invasive method widely used to evaluate cardiovascular health. Recent advancements in computer vision have enabled the estimation of pulse waves using standard video cameras and ambient light. Remote PhotoPlethysmoGraphy (rPPG) has garnered significant attention for its potential to allow background health monitoring without special medical procedures. This innovative approach is now transitioning from research to practical applications [2], with some technologies already integrated into everyday devices like health monitoring mirrors. These advancements facilitate the early detection of cardiovascular issues in seemingly healthy individuals, highlighting conditions such as elevated blood pressure or cardiac stiffness, which could indicate higher risk of severe cardiovascular diseases [18]. However, factors such as shooting conditions, lighting, and video duration can significantly impact the accuracy of health parameter estimation via rPPG. Understanding how these variables affect pulse wave extraction is crucial for algorithm training and testing. Existing datasets [17, 21, 24] addressing this issue are often limited in size and lack comprehensive health data, such as temperature, pulse, photoplethysmogram, electrocardiogram (ECG), and other human biomarkers. Existing datasets usually contain medium number of subjects (10-140) recorded in laboratory or office setting, see Table 1. Only some contain other health biomarkers, like blood pressure [10], or stress levels [21]. Moreover, most known datasets are subject to restricted access and require submitting request without guarantee of approval, often due to privacy concerns, proprietary rights, or institutional policies. This limitation impedes collaborative research and slows down the pace of innovation. Our work aims to advance remote health monitoring by providing novel publicly available dataset to streamline data handling and model training for further research. Dataset is available on Huggingface platform: https://huggingface.co/datasets/kyegorov/mcd_rppg Additionally, we provide scripts for all experiments in GitHub repository: https://github.com/ksyegorov/mcd_rppg In particular, our main contributions can be summarized as follows: (1) Comprehensive Dataset: We introduce the Multi-Camera Dataset for rPPG (MCD-rPPG), large, diverse, open dataset designed to advance deep learning methods for pulse wave detection and human biomarker estimation. Featuring 600 subjects of varying genders and ages, the dataset includes three-minute multi-view videos recorded in resting states and after physical activity, synchronized with PPG signals, and enriched with 13 additional health biomarkers such as arterial blood pressure, ECG, and heart rate. (2) Fast Baseline Model: We introduce an efficient multi-task neural network that estimates pulse waves and other health biomarkers from facial video. The model operates in realtime on CPU, achieving speed improvement of up to 13% over leading models, while maintaining competitive accuracy compared to state-of-the-art approaches, even on mobile devices. (3) Benchmarking and Comparison: We thoroughly compare our rPPG model with state-of-the-art approaches, test its generalization capabilities in cross-dataset evaluation, and provide baseline and benchmark for novel tasks, such as estimating various health biomarkers from facial videos. Table 2: Distribution of anthropometric parameters and biomarkers in the dataset Egorov et al. Weight, kg Height, cm BMI, kg/m2 Age, years Systolic pressure, mm Hg Diastolic pressure, mm Hg Saturation, % Temperature, Hemoglobin, g/dL Glycated hemoglobin, % Cholesterol, mmol/L Respiratory rate, rpm Heart rate, bpm Arterial stiffness Stress (PSM-25) mean std min max 65.92 169.75 22.73 23.08 122.45 73.79 98.01 36.56 13.59 5.52 4.16 18.05 91.93 8.99 3.04 15.79 8.87 4.34 10.90 17.43 9.25 1.29 0.13 1.66 0.69 0.83 1.71 18.37 3.04 1. 43.00 147.00 15.39 18.00 80.00 50.00 86.00 36.00 8.10 3.40 0.90 15.00 49.00 1.75 1.00 168.00 201.00 47.03 83.00 202.00 108.00 99.00 37.50 17.30 13.02 8.00 24.00 153.00 34.02 7."
        },
        {
            "title": "2 MCD-RPPG DATASET",
            "content": "Our dataset was created with 600 subjects by simultaneously measuring PPG and ECG signals using medical-grade devices (Eldar and AXMA HemoCard-BT) directly from the subject and recording videos from three different angles using various recording devices (mobile phone, video camera, and webcam). For each subject, two recordings were made: one in the resting state and another after light physical activity (15 squats in 30 seconds). This approach enhances the resulting dataset, which can be used to train more stable models concerning the subjects physical state. Differences between states can be illustrated by differences in pulse, blood pressure, and respiratory rate distributions, as shown in Fig. 2. similar logic was applied when deciding whether to include different viewing angles. Each recording session lasted approximately 3 minutes, during which the video was recorded at standard VGA resolution (640480) with frame rate of 24 or 30, depending on the device. The PPG signal was sampled at rate of 100 Hz. Additional anthropometric and biometric data were gathered before the session, as shown in Table 2, using appropriate medical equipment when necessary (scales REKAM BS 630FT, thermometer Schwabe F01, tonometer AND UA-911BT-C, pulse oximeter Beurer PO 60, blood analyzers EasyTouch and EasyTouch 2, volumetric sphygmography system BPLab Angio). The stress assessment was conducted using the PSM25 psychological stress scale, measured through questionnaire. Table 1: Existing open rPPG datasets"
        },
        {
            "title": "2.1 Data collection procedure",
            "content": "name PURE BP4D+ COHFACE LGI-PPGI UBFC-rPPG UBFC-Phys SCAMPS MMPD VitalVideos iBVP MCD-rPPG year 2014 2016 2017 2018 2019 2021 2022 2023 2023 2024 2024 subjects open link 10 140 40 25 42 56 2800 (synthetic) 33 900 33 600 No No No Yes No No Yes No No No Yes [23] [29] [9] [20] [4] [21] [17] [24] [25] [10] Ours The data collection process was organized as follows. At first, the subjects filled out the consent form to participate in the study with medical intervention and the consent form to process data. Then, they were asked to fill out Psychological Stress Scale PSM-25 (Lemyr-Tessier-Fillion) by indicating the assessment of statements on an 8-point scale. Next, the subject proceeded to the medical parameter collection stand  (Fig. 1)  , where the main physiological parameters were assessed (body weight, height, temperature, blood pressure, heart rate (HR), oxygen saturation, ECG, blood glucose, glycated hemoglobin, total cholesterol, respiratory rate, and arterial wall stiffness). Body weight was measured using the REKAM BS 630FT floor scale. Blood Gaze into the Heart: Multi-View Video Dataset for rPPG and Health Biomarkers Estimation before after 100 150 (a) Diastolic BP, mm Hg 300 200 100 o 50 300 200 100 o 50 100 150 200 (b) Systolic BP, mm Hg 150 100 50 o 0 60 100 80 (c) Pulse, bpm 120 140 160 o 300 200 100 0 14 16 22 (d) Respiratory rate, rpm 20 24 Figure 2: Value distribution of diastolic blood pressure (a), systolic blood pressure (b), pulse (c) and respiratory rate (d), before and after physical activity. pressure and HR were measured using an automatic tonometer AND UA-911BT-C. ECG was recorded by registering at least three channels of 12-channel ECG using the ACSMA GemoCard-BT device. Oxygen saturation was determined by indirect oximetry of the index finger using the Beurer PO 60 pulse oximeter. All these devices can remotely transmit data to the database via Bluetooth. The blood glucose, total cholesterol, and glycated hemoglobin levels were determined by puncturing the index finger pad with disposable sterile scarifier, then using test strips and blood analyzers EasyTouch (glycated hemoglobin, total cholesterol) and EasyTouch 2 (glucose). Body temperature was measured using non-contact thermometer. Arterial wall stiffness was assessed using the Eldar photoplethysmograph or the system for volumetric sphygmography - BPLab Angio. physician measured the respiratory rate by counting the number of respiratory movements per minute. Immediately after that, the subject proceeded to the video and photoplethysmography data collection stand, which was laptop with three video cameras (mobile phone Samsung A3s, video camera Sony FDR-AX43A, and webcam Defender G-lens 2599) and photoplethysmograph connected. The subject sat down in front of the cameras, put the photoplethysmograph probe on their finger, and waited motionlessly for three minutes. An electronic clock with second value was placed behind the subject to synchronize the video. The operators task was to accompany the subject at this stand and monitor the lighting and validity of the collected data. After three minutes of recording, the subject performed physical exercise of 15 squats in 30 seconds, and then all parameter recordings were repeated. Specialized software was developed to record video and anthropometric and biometric parameters. The software operates as follows. In the user interface, the operator enters the subjects data and the recording stage (initial or after physical exercises). After that, input data is automatically validated, and the initialization process for video and PPG capturing begins. This process is performed until the internal timer reaches predefined number of seconds. Then, the operator displays the recorded PPG signal on the screen for visual control. Finally, the program reinitializes for the next recording."
        },
        {
            "title": "3 SYNCHRONIZATION CHALLENGES",
            "content": "Precise time synchronization of data is essential, as it can significantly impact the achievable metrics for training AI models."
        },
        {
            "title": "3.1 Video synchronization",
            "content": "Although time synchronization of video streams from different recording devices is assisted to some extent by software developed for this purpose, timestamps are assigned on the computer side, which does not consider the latency introduced by the recording device and the data transfer to the computer. Thus, we have enriched the data with an additional synchronization channel. As seen in Fig. 1, there is tablet with digital clock in the background, visible from all the cameras. There are two major things to take into account: Firstly, the tablet clock is not perfectly synchronized with the computer, which does not prevent us from calculating time shifts between video sources pairwise; Secondly, tablet clock lacks sub-second time resolution, which timestamps have. The latter issue is solved by employing the following algorithm: Tablet clock data is extracted from each frame using optical character recognition (EasyOCR) and cleansed; For each adjacent pair of frames with different times displayed on the tablet clock, the time shift is calculated as the difference between the time displayed in the last frame and half of the sum of the frame timestamps; The record time shift is obtained by taking the average of all the shifts calculated in the previous step; The quality of video stream synchronization is estimated for each pair of video recording devices by calculating the direct difference between their corresponding time shifts. The results for record time shifts (excluding 226 records, or approximately 6.3% of the total number of records for which OCR failed) are shown in Fig. 3. Here, the distributions for different cameras match quite closely, suggesting that the absolute value is determined by the drift of the tablets internal clock. Additionally, it can be shown that the record time shifts for different video recording devices are linearly dependent. o 300 200 100 cam1 cam2 cam3 1.5 1 0.5 s y i o 4 2 0 2 Time shift, Figure 3: Distribution of record time shifts (between frame timestamps and physical clock) estimated using KDE (cam1 is IriunWebcam, cam2 is FullHDwebcam and cam3 is USBVideo). Fig. 4 shows that the time difference is generally within 0.2 seconds, constituting only fraction of the average heartbeat cycle length. Moreover, the proposed method can further refine the temporal data."
        },
        {
            "title": "3.2 PPG synchronization",
            "content": "Considering possible rPPG applications, another important point to consider is the synchronization of video and PPG data. To estimate it, we compared ground truth PPG signal with PPG signals reconstructed from video using the POS algorithm as follows: all signals are filtered using 4-order Chebyshev Type II band-pass filter with cutoff frequencies of 0.4 Hz and 8 Hz; Discrete shift was determined in terms of the optimization problem for maximization of the Pearson correlation coefficient between reconstructed signals and ground truth. Egorov et al. cam2 cam1 cam3 cam2 cam3 cam1 0.2 0. 0 0.1 0.2 0.3 Time difference, Figure 4: Distribution of time shift between different video sources (cam1 is IriunWebcam, cam2 is FullHDwebcam and cam3 is USBVideo). 300 200 100 o 0 20 cam1 cam2 cam 20 10 0 Optimal shift, frames 10 Figure 5: Distribution of time shift between ground truth PPG and reconstructed PPG (cam1 is IriunWebcam, cam2 is FullHDwebcam and cam3 is USBVideo). algorithm failed, were excluded). The second set of peaks on the left side is most probably caused by time shift approaching half period of the PPG signal, in which case, the determination of the nearest maximum becomes ambiguous. Considering that the frontal facing camera (cam2) shows significantly better synchronization, the subpar results for other cameras are at least partially attributed to the shortcomings of the POS algorithms."
        },
        {
            "title": "4 BASELINE MODEL",
            "content": "The obtained results are shown in Fig. 5 (326 records, or approximately 9.1% of the total number of records for which the POS Modern rPPG models can be either unsupervised or supervised. Unsupervised methods can be divided into two subgroups: methods Gaze into the Heart: Multi-View Video Dataset for rPPG and Health Biomarkers Estimation Figure 6: Overview of our baseline model. based on reading micro-movements (Ballistocardiographic) [1] and methods based on micro changes in the color of facial pixels [7, 26]. Supervised methods, such as [6, 27, 28], are based on deep learning and let the neural network determine where and what to look at. Such methods are more accurate and better predict the PPG shape, but require datasets for training and often are subject to overfitting. This can be seen in the results of [15], where unsupervised POS [26] and OMI [5] models showed results that were more robust across datasets compared to supervised approaches, as well as in [11], where all supervised models showed significant drop in quality during cross-dataset evaluation. In this regard, we decided to take hybrid approach relying on domain-specific pre-processing, followed by processing with specialized neural network adapter, similar to [14]. Because the network has fewer parameters, it is much less susceptible to overfitting while maintaining high quality and the ability for fine-tuning and domain adaptation, if necessary. Additionally, adding new targets like arterial pressure or respiratory rate to neural networks is possible without significantly altering their architecture. For domain-specific pre-processing tool, we used ideas from [7, 26]. It is essential to consider the anatomical properties of the blood supply to the face [22] and detect those areas of the face with the greatest influence of the pulse wave, significantly reducing the noise level in the signal. We select the face region-of-interest (ROI) using technique from [12], allowing the neural network to choose the signal of different ROIs in the required proportions. The final model architecture is shown in Fig. 6. First, we detect face and highlight the ROI using the FaceMesh model from the mediapipe [16]. We then select set of multiple regions and compute the mean pixel value in each ROI for each frame. The received signals are fed into neural network, producing PPG signal and medical parameter predictions. The model is fully convolutional 1-dimensional feature pyramid network [13]. It allows the neural networks to operate on different lengths of the input signal without slicing it in sliding window manner. The resulting pipeline is very fast and surpasses even unsupervised models in inference time on GPU and CPU. The proposed model is good foundation for analyzing an extended set of biomarkers. While the ROI-based approach is indeed common in rPPG, our main contribution is developing blazing-fast model capable of working on small devices like phones and wearables while being on par with high-capacity models in terms of accuracy."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We trained our model by splitting videos and PPG signals into 20second windows and feeding them to the neural network. The main target of the training was to predict the PPG signal. Still, the dataset had additional targets: systolic and diastolic arterial pressure, glycated hemoglobin, cholesterol, respiratory rate, arterial stiffness, age, sex, BMI, stress level, and saturation. All targets were normalized with standard scaler on the training subset, and the loss function was the sum of mean squared error (MSE) losses for each target. For optimization, we utilized the Adam optimizer. We used the mean average error (MAE) of HR for each 10-second segment as our primary metric. To determine HR, we use the algorithm [19]. Table 3 presents the PPG and HR estimation results. Two metrics are shown: the mean average error (MAE) of the predicted PPG signal and the MAE of HR estimation. HR is predicted by selecting the most powerful frequency of 0.5-3 Hz of the predicted PPG signal. The training procedure and all hyperparameters necessary for repeating the results can be obtained from the repository. The result of the biomarkers estimation is presented in Table 5. Our model performs better than the straightforward baseline, the optimal constant value fitted on training subset. One of the goals of gathering this dataset was to research the impact of different circumstances and camera parameters on the quality of remote medical scanning. For this purpose, we provide multi-view videos from three sources and our experimental results. First, as seen in Fig. 1, three camera views cover the face of the patient differently, so we tested the drop in quality. The results are presented in Table 4. We can see the pattern where unsupervised methods, while strong in cross-dataset generalization, cannot adapt to different view angles, and neural networks generally perform better in these scenarios. Table 4 also shows our models inference time and size advantage. Table speed advantage on CPU is 13% better than the previous best model. Time was measured by running 200 20-second video segments sequentially. However, the results in terms of model MAE are not straightforward and show multidirectional trends. This is consistent with independent benchmarks like rPPG-Toolbox [15] and Remote Biosensing Benchmark [11], where different models Table 3: Comparison of performance of different models (MAE). With bold font, we highlight the best-performing model on each test dataset. With underline font, we highlight the best-performing model, which is not trained on the test dataset in question. The last line describes the performance of our model trained on the MCD-rPPG dataset. Egorov et al. Model PBV[8] OMIT[5] POS[26] PhysFormer[28] iBVPNet[10] RhythmFormer[30] Ours  (Fig. 6)  Train dataset MCD-rPPG PPG (Ours) MCD-rPPG HR (Ours) MMPD PPG MMPD HR - - - MMPD SCAMPS UBFC-rPPG MCD-rPPG MMPD SCAMPS UBFC-rPPG MCD-rPPG MMPD SCAMPS UBFC-rPPG MCD-rPPG MMPD SCAMPS UBFC-rPPG MCD-rPPG 0.85 0.80 0.87 0.890.02 1.100.00 0.810.01 0.460.01 0.990.02 1.070.01 0.960.03 0.680.01 0.940.02 1.050.01 0.870.01 0.430.00 1.320.02 1.200.01 1.170.05 0.680. 15.37 4.78 3.80 13.571.40 46.380.00 43.653.71 4.080.12 36.286.10 59.481.93 35.317.24 4.830.44 17.544.30 58.588.14 19.141.93 2.820.13 7.581.3 41.174.30 23.096.67 4.860.36 0.80 0.77 1.08 0.820.02 0.970.00 0.760.01 0.980.03 0.870.01 1.010.00 0.860.01 1.010.01 0.770.00 1.000.01 0.820.01 0.980.01 0.940.01 1.060.03 1.010.02 1.200.02 37.11 15.33 15.36 22.671.21 30.010.00 42.097.21 22.610.51 21.020.20 24.190.67 23.533.92 17.120.51 17.282.21 26.080.44 24.810.69 16.630.96 15.260.49 34.443.81 27.933.41 17.530.68 SCAMPS PPG 0.88 0.86 1.41 0.900.02 0.130.00 0.840.02 1.080.03 0.960.01 0.540.00 0.900.02 1.050.00 0.910.01 0.080.00 0.990.01 1.060.02 1.160.02 0.500.04 0.920.08 1.270.05 SCAMPS HR UBFC-rPPG PPG UBFC-rPPG HR 35.93 16.27 16.02 28.450.75 0.400.00 49.774.70 23.331.29 34.602.04 1.630.08 31.362.17 25.740.23 30.341.00 0.200.10 28.530.43 21.451.65 27.771.75 10.600.77 37.460.62 20.201.39 0.86 0.84 1.52 0.790.04 0.670.00 0.770.01 1.300.06 0.930.01 0.840.03 0.850.01 1.210.01 0.850.01 0.750.03 0.860.02 1.460.02 0.960.12 0.870.03 0.770.02 1.420.08 30.27 1.95 1.17 8.795.96 1.170.00 37.400.98 1.271.51 21.635.16 7.912.62 7.472.21 4.690.71 11.522.80 10.252.68 21.533.29 2.390.35 2.390.46 13.623.57 6.845.27 4.130.92 Table 4: Performance for different camera views"
        },
        {
            "title": "6 CONCLUSION",
            "content": "Speed of Inference (ms) Metrics (MAE) Model PBV OMIT POS PhysFormer RhythmFormer iBVPNet Ours CPU seconds GPU Size seconds Mb Frontal PPG 0.18 0.17 0.26 0.93 0.97 0.93 0.15 0.18 0.17 0.26 0.31 0.33 0.28 0.16 0 0 0 28.4 12.9 5.5 3.9 0.85 0.80 0.87 0.46 0.43 0.68 0. Side PPG 0.86 1.11 1.25 0.97 0.91 0.99 1.10 Frontal HR 15.37 4.78 3.80 4.08 2.82 4.83 4.86 Side HR 40.44 22.35 16.40 10.68 7.33 11.42 14. Table 5: Metric for predicting biomarkers compared to naive baseline (all metrics are MAE, except for Sex) Target Baseline Model Systolic pressure, mm Hg Diastolic pressure, mm Hg Glycated hemoglobin, % Cholesterol, mmol/L Respiratory rate, rpm Arterial stiffness Age, years BMI Stress (PSM-25) Saturation, % Sex (Accuracy) 13.78 7.50 0.43 0.66 1.36 2.19 5.71 3.18 1.20 0.98 0.61 12.82 8.39 0.41 0.60 1.20 2.04 3.91 3.37 1.07 0.98 0. excel in different training and testing setups. We believe it is due to the limited sizes of all available datasets. This comparison confirms our success in developing small and fast models that achieved performance metrics comparable to competitive models. In this paper, we introduce unique large MCD-rPPG dataset for rPPG and health biomarkers estimation. The dataset contains 3minute video recordings from 600 subjects of different genders and ages filmed in two states (quiet and after exercise), aligned with 13 biomarkers. photoplethysmograph was synchronized with multiple webcams to ensure proper parameter alignment, enabling pulse wave detection from facial video recordings. Additionally, we developed fast, lightweight multitask rPPG model  (Fig. 6)  trained on our dataset, which relies on domain-specific pre-processing. We believe the public release of our dataset and model will accelerate progress in AI-driven medical assistants [3] and multimedia applications (e.g., emotion-aware telemedicine, stress monitoring via video calls, or fitness tracking via smartphone cameras), which rely on robust estimation of pulse waves and health biomarkers from everyday recording devices like mobile phones and webcams."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "The work of A. Savchenko was supported by grant, provided by the Ministry of Economic Development of the Russian Federation in accordance with the subsidy agreement (agreement identifier 000000C313925P4G0002) and the agreement with the Ivannikov Institute for System Programming of the Russian Academy of Sciences dated June 20, 2025 No. 139-15-2025-011. REFERENCES [1] Guha Balakrishnan, Fredo Durand, and John Guttag. 2013. Detecting Pulse from Head Motions in Video. In 2013 IEEE Conference on Computer Vision and Pattern Recognition. 34303437. doi:10.1109/CVPR.2013.440 [2] Rohan Banerjee, Anirban Dutta Choudhury, Aniruddha Sinha, and Aishwarya Visvanathan. 2014. HeartSense: smart phones to estimate blood pressure from photoplethysmography. In Proceedings of the 12th ACM Conference on Embedded Network Sensor Systems. 322323. [3] Pavel Blinov, Konstantin Egorov, Ivan Sviridov, Nikolay Ivanov, Stepan Botman, Evgeniy Tagin, Stepan Kudin, Galina Zubkova, and Andrey Savchenko. Gaze into the Heart: Multi-View Video Dataset for rPPG and Health Biomarkers Estimation [29] Zheng et al. Zhang. 2016. Multimodal Spontaneous Emotion Corpus for Human Behavior Analysis. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 34383446. doi:10.1109/CVPR.2016. [30] Bochao Zou, Zizheng Guo, Jiansheng Chen, Junbao Zhuo, Weiran Huang, and Huimin Ma. 2024. RhythmFormer: Extracting Patterned rPPG Signals based on Periodic Sparse Attention. arXiv preprint arXiv:2402.12788 (2024). 2024. GigaPevt: multimodal medical assistant. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence. 86148618. [4] Serge Bobbia and et al. 2019. Unsupervised skin tissue segmentation for remote photoplethysmography. Pattern Recognition Letters 124 (2019), 8290. doi:10. 1016/j.patrec.2017.10.017 [5] Constantino Alvarez Casado and Miguel Bordallo López. 2023. Face2PPG: An unsupervised pipeline for blood volume pulse extraction from faces. IEEE Journal of Biomedical and Health Informatics 27, 11 (2023), 55305541. [6] Weixuan Chen and Daniel McDuff. 2018. DeepPhys: Video-Based Physiological Measurement Using Convolutional Attention Networks. In Computer Vision ECCV 2018, Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (Eds.). Springer International Publishing, Cham, 356373. [7] Gerard de Haan and Vincent Jeanne. 2013. Robust Pulse Rate From ChrominanceBased rPPG. IEEE Transactions on Biomedical Engineering 60 (2013), 28782886. [8] de Haan and van Leest. 2014. Improved motion robustness of remote-PPG by using the blood volume pulse signature. Physiological Measurement 35, 9 (aug 2014), 1913. doi:10.1088/0967-3334/35/9/ [10] [9] Guillaume Heusch, André Anjos, and Sébastien Marcel. 2017. reproducible study on remote heart rate measurement. arXiv preprint arXiv:1709.00962 (2017). Jitesh Joshi and Youngjun Cho. 2024. iBVP Dataset: RGB-Thermal rPPG Dataset with High Resolution Signal Quality Labels. Electronics 13, 7 (2024). doi:10.3390/ electronics13071334 [11] Dae Yeol Kim and et al. 2023. Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG. arXiv:2307.12644 [eess.IV] [12] Dae-Yeol Kim, Kwangkee Lee, and Chae-Bong Sohn. 2021. Assessment of ROI Selection for Facial Video-Based rPPG. Sensors 21, 23 (2021). doi:10.3390/s21237923 [13] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. 2017. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition. 21172125. [14] Xin Liu and et al. 2021. Efficientphys: Enabling simple, fast and accurate camerabased vitals measurement. arXiv preprint arXiv:2110.04447 (2021). [15] Xin Liu and et al. 2022. rPPG-Toolbox: Deep Remote PPG Toolbox. arXiv preprint arXiv:2210.00716 (2022). [16] Camillo Lugaresi and et al. 2019. MediaPipe: Framework for Perceiving and Processing Reality. In Third Workshop on Computer Vision for AR/VR at IEEE Computer Vision and Pattern Recognition (CVPR) 2019. [17] Daniel McDuff, Miah Wander, Xin Liu, Brian Hill, Javier Hernandez, Jonathan Lester, and Tadas Baltrusaitis. 2022. Scamps: Synthetics for camera measurement of physiological signals. Advances in Neural Information Processing Systems 35 (2022), 37443757. [18] Riccardo Miotto and et al. 2018. Reflecting health: smart mirrors for personalized medicine. NPJ digital medicine 1, 1 (2018), 62. [19] N. H. Mohd Sani and et al. 2015. Determination of heart rate from photoplethysmogram using Fast Fourier Transform. In 2015 International Conference on BioSignal Analysis, Processing and Systems (ICBAPS). 168170. doi:10.1109/ ICBAPS.2015.7292239 [20] Christian S. Pilz, Sebastian Zaunseder, Jarek Krajewski, and Vladimir Blazek. 2018. Local Group Invariance for Heart Rate Estimation from Face Videos in the Wild. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). 133513358. doi:10.1109/CVPRW.2018.00172 [21] Rita Meziati Sabour, Yannick Benezeth, Pierre De Oliveira, Julien Chappé, and Fan Yang. 2023. UBFC-Phys: Multimodal Database For Psychophysiological Studies of Social Stress. IEEE Transactions on Affective Computing 14, 1 (2023), 622636. doi:10.1109/TAFFC.2021.3056960 [22] V. A. Sinopalnikov and V. M. Zemskov. 2018. Telemonitoring of Capillary Blood Flow in the Human Skin: New Opportunities and Prospects. [23] Ronny Stricker, Steffen Müller, and Horst-Michael Gross. 2014. Non-contact video-based pulse rate measurement on mobile service robot. In The 23rd IEEE International Symposium on Robot and Human Interactive Communication. 10561062. doi:10.1109/ROMAN.2014.6926392 Jiankai Tang, Kequan Chen, Yuntao Wang, Yuanchun Shi, Shwetak Patel, Daniel McDuff, and Xin Liu. 2023. Mmpd: multi-domain mobile video physiology dataset. In 2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC). IEEE, 15. [24] [25] Pieter-Jan Toye. 2023. Vital Videos: dataset of videos with PPG and blood pressure ground truths. arXiv preprint arXiv:2306.11891 (2023). [26] Wenjin Wang, Albertus C. den Brinker, Sander Stuijk, and Gerard de Haan. 2017. Algorithmic Principles of Remote PPG. IEEE Transactions on Biomedical Engineering 64, 7 (2017), 14791491. doi:10.1109/TBME.2016.2609282 [27] Zitong Yu, Xiaobai Li, and Guoying Zhao. 2019. Remote Photoplethysmograph Signal Measurement from Facial Videos Using Spatio-Temporal Networks. In British Machine Vision Conference. [28] Z. Yu, Y. Shen, J. Shi, H. Zhao, P. Torr, and G. Zhao. 2022. PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer Society, Los Alamitos, CA, USA, 41764186. doi:10.1109/CVPR52688.2022."
        }
    ],
    "affiliations": [
        "ISP RAS Research Center for Trusted Artificial Intelligence, Moscow, Russia",
        "Samara State Medical University, Samara, Russia",
        "Sber AI Lab, Moscow, Russia"
    ]
}