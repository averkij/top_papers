{
    "paper_title": "Multi-Turn Code Generation Through Single-Step Rewards",
    "authors": [
        "Arnav Kumar Jain",
        "Gonzalo Gonzalez-Pumariega",
        "Wayne Chen",
        "Alexander M Rush",
        "Wenting Zhao",
        "Sanjiban Choudhury"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We address the problem of code generation from multi-turn execution feedback. Existing methods either generate code without feedback or use complex, hierarchical reinforcement learning to optimize multi-turn rewards. We propose a simple yet scalable approach, $\\mu$Code, that solves multi-turn code generation using only single-step rewards. Our key insight is that code generation is a one-step recoverable MDP, where the correct code can be recovered from any intermediate code state in a single turn. $\\mu$Code iteratively trains both a generator to provide code solutions conditioned on multi-turn execution feedback and a verifier to score the newly generated code. Experimental evaluations show that our approach achieves significant improvements over the state-of-the-art baselines. We provide analysis of the design choices of the reward models and policy, and show the efficacy of $\\mu$Code at utilizing the execution feedback. Our code is available at https://github.com/portal-cornell/muCode."
        },
        {
            "title": "Start",
            "content": "Multi-Turn Code Generation Through Single-Step Rewards Arnav Kumar Jain * 1 2 Gonzalo Gonzalez-Pumariega * 3 Wayne Chen 3 Alexander Rush 3 Wenting Zhao 3 Sanjiban Choudhury 3 5 2 0 2 7 2 ] . [ 1 0 8 3 0 2 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We address the problem of code generation from multi-turn execution feedback. Existing methods either generate code without feedback or use complex, hierarchical reinforcement learning to optimize multi-turn rewards. We propose simple yet scalable approach, µCODE, that solves multi-turn code generation using only single-step rewards. Our key insight is that code generation is one-step recoverable MDP, where the correct code can be recovered from any intermediate code state in single turn. µCODE iteratively trains both generator to provide code solutions conditioned on multi-turn execution feedback and verifier to score the newly generated code. Experimental evaluations show that our approach achieves significant improvements over the stateof-the-art baselines. We provide analysis of the design choices of the reward models and policy, and show the efficacy of µCODE at utilizing the execution feedback. Our code is available here. 1. Introduction Software engineers often iteratively refine their code based on execution errors. common strategy for machine code generation is thus to repair code using execution feedback at test time (Chen et al., 2024; Wang et al., 2024b; Zhao et al., 2024). However, prompting alone is insufficient as it cannot teach how to recover from all possible errors within limited context. We need to train models that can learn from execution feedback during training. Existing approaches fall into either single-turn or multi-turn settings. In the single-turn setting, methods either train without execution feedback (Zelikman et al., 2022) or perform one-step corrections (Welleck et al., 2023; Ni et al., 2024). However, these struggle to iteratively *Equal contributionEqual advising 1MilaQuebec AI Institute 2Universite de Montreal 3Cornell University. Correspondence to: Arnav <arnav-kumar.jain@mila.quebec>, Gonzalo <gg387@cornell.edu>. Preprint. 1 correct errors over multiple turns. Multi-turn approaches, on the other hand, rely on complex reinforcement learning (RL) (Gehring et al., 2024a; Kumar et al., 2024b; Zhou et al., 2024) to optimize long-term rewards. While effective in principle, these methods suffer from sparse learning signals which makes learning inefficient. Our key insight is that code generation is one-step recoverable Markov Decision Process (MDP), implying that the correct code can be recovered from any intermediate state in single step. This allows us to greedily maximize onestep reward instead of relying on complex multi-step reward optimization. As result, this reduces the problem from reinforcement learning, which requires exploration and credit assignment, to imitation learning, where the model simply learns to mimic correct code, leading to more stable and efficient training process. We propose µCODE, simple and scalable approach for multi-turn code generation from execution feedback. During training, µCODE follows an expert iteration (Anthony et al., 2017) framework with local search expert, enabling iterative improvement of both the generator and the expert. The process begins by rolling out the current code generator to collect interaction data with execution feedback. single-step verifier is then trained on this data and utilized to guide local search expert in refining the code and generating training labels. Finally, the generator is fine-tuned using these labels. Given recent trends of test-time scaling in generating high quality solutions (Brown et al., 2024; Snell et al., 2024; Wu et al., 2024), µCODE also uses the learned verifier for inference-time scaling. Here, µCODE samples trajectories; at each step, µCODE picks the best code solution ranked by the learned verifier. The key contributions of this work are as follows: 1. novel framework, µCODE, for training code generators and verifiers through multi-turn execution feedback. We add theoretical analysis of performance bounds using the property of one-step recoverability for this task. 2. We propose multi-turn Best-of-N (BoN) approach for inference-time scaling and present benefits of learned verifier to select the code solution at each turn. 3. Our approach µCODE outperforms leading multi-turn approaches on MBPP (Austin et al., 2021) and HuMulti-Turn Code Generation Through Single-Step Rewards Figure 1. (a) We define the task of multi-turn code generation where for an initial problem x, the generator πθ provides solution y1. This solution is evaluated with the public test to get execution feedback o1. At turn t, the generator is conditioned on the history to generate solution yt πθ(.x, y<t, o<t). The rollout ends when the turn limit is reached or the public tests pass upon which the solution is executed on private tests. Since, the agents can generate the optimal solution at any turn, this is 1-step recoverable process. (b) Training loop of our method µCODE which comprises of generator and learned verifier. During each iteration, rollouts are collected using πθ and we train verifier Rϕ to rank candidate solutions for prompt. The verifier Rϕ is then used to construct local expert and relabel the collected rollouts. Lastly, the generator is fine-tuned with this expert dataset. manEval (Chen et al., 2021) benchmarks. Our ablations demonstrate that learned verifiers aid in learning better generators and show promising scaling law trends with higher inference budgets. {x, y1, o1, . . . , yt1, ot1} where s1 = {x}, and the action is the code at = yt. The oracle reward is defined as R(st, at) = R(x, at) = C(x, yt) if yt passes all public and private tests (terminating the episode), or 0 otherwise. 2. Background Multi-turn Code Generation as MDP. In multi-turn code generation, an agent iteratively refines program to maximize its correctness on private test cases. Given an initial problem prompt x, at each turn t, the agent generates complete code snippet yt and executes it on set of public tests. The outcomes ot from these tests serve as observations that guide subsequent refinements. This process continues until the agent generates code snippet yt that passes all public tests, at which point the episode terminates, or until the maximum number of turns is reached without success. The first successful code, yt, is then evaluated on private tests to compute the correctness score C(x, yt) {0, 1}. We model this as Markov Decision Process (MDP), the interaction history st = where the state is During training, given dataset of problem prompts D, the goal is to find generator πθ(ytx, y1, o1, . . . , yt1, ot1), that maximizes the cumulative discounted reward R(x, yt): max πθ ExD,ytπθ(st) (cid:34) (cid:88) (cid:35) γtR(x, yt) . t=1 (1) 3. µCODE: Multi-turn Code Generation We propose µCODE, simple and scalable algorithm for multi-turn code generation using execution feedback. µCODE follows an expert iteration (Anthony et al., 2017) framework with local search expert. µCODE iteratively trains two components learned verifier Rϕ to score code snippets (Section 3.2), and generator πθ to imitate local search with the verifier (Section 3.3). This iterative process allows the generator and expert to bootstrap off each other, 2 Multi-Turn Code Generation Through Single-Step Rewards Algorithm 1 µCODE: Training input Initial generator π0, multi-turn code environment E, and max iterations 1: for iteration = 1 . . . do 2: 3: 4: 5: Rollout generator πθ in multi-turn environment to collect datapoints Di {(x, st, yt, ot))} Aggregate data Di Train verifier Ri ϕ(x, y) on Construct local search expert using verifier (x) = arg maxyD(x) Ri πi Relabel data with πi Train πi ϕ(x, y) (x) to get Di θ with fine-tuning (FT) on Di 6: 7: 8: end for output Best generator πθ and verifier Rϕ leading to continuous improvement. At inference time, both the generator and verifier are used as BoN search to select and execute code (Section 3.4). Finally, we analyze the performance of µCODE in Section 3.5. 3.1. The µCODE Algorithm Algorithm 1 presents the iterative training procedure. At an iteration i, the current generator πθ is rolled out in the multi-turn code environment to generate interaction data Di {(x, st, yt, rt)}. Every turn in Di includes the prompt x, interaction history st, code generated yt and the correctness score from the oracle verifier rt = R(x, yt). This data is then aggregated Di. The learned verifier Ri ϕ is trained on the aggregated data D. An expert is created using Ri ϕ to perform local search to find the optimal action πi ϕ(x, y), where D(x) are all the code completions for given prompt x. The expert πi (x) relabels the data with the optimal action. The generator πi θ is then trained via fine-tuning (FT) on D. This process iterates times, and the best generator and verifier pair on the validation dataset are returned. (x) = arg maxyD(x) Ri 3.2. Training Verifier The learned verifier provides dense scores to code solutions for given problem. At train time, this is used by the expert to perform local search to obtain optimal code. At inference time, the verifier is used for multi-turn BoN (3.4) for efficient search. The learned verifier has two distinct advantages over process reward functions typically used in multi-turn RL: (1) It is conditioned only on the initial prompt and the current solution, and is not dependent on previous states (2) It is trained via supervised learning on oracle reward labels. We explore two different losses: Binary Cross-Entropy loss (BCE): The nominal way to train the verifier is to directly predict the oracle reward 3 (Cobbe et al., 2021): LBCE(ϕ) = E(x,y,r)D[r log Rϕ(x, y) (1 r) log Rϕ(x, y)] (2) Bradley Terry Model (BT): Since the goal of the verifier is to relatively rank code solutions rather than predict absolute reward, we create preference dataset and then train with Bradley Terry loss (Ouyang et al., 2022). For every prompt x, we create pairs of correct y+ (where = 1) and incorrect (where = 0) code and define the following loss: LBT (ϕ) = (x,y+,y)D[log σ(Rϕ(x, y+) Rϕ(x, y))]. (3) where σ(.) is the sigmoid function. We hypothesize that BT is strictly easier to optimize as the verifier has to only focus on relative performance. This is also consistent with observations made for training process reward models, where the advantage function is easier to optimize than the absolute function (Setlur et al., 2024). 3.3. Training Generator µCODE comprises generator πθ trained to produce code solutions conditioned on the initial problem and execution observations from previous turns. Given dataset D, µCODE iteratively trains the generator to find the optimal code solution labeled using the local expert over the learned verifier. For this step, µCODE extracts all code solutions from for every problem x. An expert is then created by picking the best solution, y, which achieves the highest score using with the learned verifier Rϕ(x, y) and is given by = π(x) = arg max yD(x) Rϕ(x, y). (4) Using this expert dataset, we relabel the dataset with the optimal solutions for each prompt: = {(x, st, y) (x, st) D}, (5) where represents the expert dataset. The generator πθ is then trained via fine-tuning (FT) on this expert dataset D. 3.4. Inference: Multi-turn Best-of-N At inference time, the goal is to generate code solution with fixed inference budget denoting the number of times generators can provide one complete solution. In this work, we propose to leverage the learned verifier to improve search and code generations over successive turns with multi-turn Best-of-N (BoN). To achieve this, µCODE uses natural extension of BoN to the multi-turn setting. At each turn, the generator produces one-step rollouts {yn n=1 πθ(.st) and the learned verifier picks the most promising code solution among these candidates using }N = arg max Rϕ(x, yn ). (6) Multi-Turn Code Generation Through Single-Step Rewards Algorithm 2 µCODE: Inference loop input Generator πθ, learned verifier Rϕ, turn limit T, number of rollouts N, public tests, and private tests n=1 πθ(.st) to get execution feedback ot }N = arg maxn Rϕ(x, yn ) Generate rollouts {yn Choose best solution Execute if 1: Set s1 = {x}, = 1 2: while true do 3: 4: 5: 6: 7: 8: 9: 10: end while output Return to execute on public and private tests passes public tests or = then break; end if Update state st+1 = {st, , ot} and increment The selected code is executed in the environment over public tests to obtain the execution feedback ot. This solution and the feedback is provided as context to the generator at the next turn to repeat this procedure. The search ends once passes all public tests or when the turn limit is reached. Consequently, even if Rϕ() grants high score to code solution, inference continues until the solution has successfully cleared all public tests, thus mitigating potential errors by Rϕ(). The final response is then passed through the oracle verifier to check its correctness. Algorithm 2 describes description of multi-turn BoN. We found it beneficial to use the reward model trained with samples of the latest generator πθ (see Table 1). 3.5. Analysis µCODE effectively treats multi-turn code generation as an interactive imitation learning problem by collecting rollouts from learned policy and re-labeling them with an expert. It circumvents the exploration burden of generic reinforcement learning which has exponentially higher sample complexity (Sun et al., 2017). We briefly analyze why this problem is amenable to imitation learning and prove performance bounds for µCODE. Definition 3.1 (One-Step Recoverable MDP). MDP = (S, A, P, R, γ) with horizon is one-step recoverable if the advantage function of the optimal policy π, defined as A(s, a) = Q(s, a)V (s), is uniformly bounded for all (s, a), i.e. A(s, a) 1. Code generation is one-step recoverable MDP. Multiturn code generation satisfies one-step recoverability because the optimal policy π(ytst) depends only on the problem prompt and not the interaction history st = (x, y1, o1, . . . , yt1, ot1). Since the correctness of code snippet yt is fully determined by x, the optimal Q-function satisfies Q(st, yt) = R(x, yt), where R(x, yt) {0, 1}. The optimal value function is (st) = maxyt R(x, yt), 4 so the advantage function simplifies to A(st, yt) = t) 1. R(x, yt) maxy R(x, t Code generation enables efficient imitation learning. There are two challenges to applying interactive imitation learning (Ross et al., 2011; Ross & Bagnell, 2014) (1) Existence of expert policies or value functions, and (2) Recoverability of expert from arbitrary states. First, for code generation, the expert is simply the one-step reward maximizer arg maxy R(x, y). We can efficiently estimate Rϕ(x, y) to compute the expert, without needing to compute value function backups. Second, even if the learner fails to imitate the expert at any given state, the expert can perfectly recover from the next state. This results in the best possible performance bounds for imitation learning, which we formalize below. Theorem 3.2 (Performance bound for µCODE). For onestep recoverable MDP with horizon , running iterations of µCODE yields at least one policy π such that J(π) J(π) O(T (ϵ + γ(N ))). (7) where π is the expert policy, ϵ is the realizability error, and γ(N ) is the average regret. Proof is in Appendix A.1. The bound O(ϵT ) is much better than the worst-case scenario of O(ϵT 2) for unrecoverable MDPs (Swamy et al., 2021). Thus, µCODE exploits the structure of multi-turn code generation to enable imitation learning, bypassing the need for hierarchical credit assignment. More generally, this analysis suggests that for any task where the optimal action is history-independent and recoverable in one step, reinforcement learning can be reduced to efficient imitation learning without loss of performance. 4. Experiments Through our experiments, we aim to analyze (1) How does µCODE compare to other state-of-the-art methods? (2) Does the learned verifier help during training and inference-time? (3) Which loss function works better for learning verifier? 4.1. Setup Models. The generator model in µCODE is initialized with Llama-3.2-1B-Instruct or Llama-3.1-8B-Instruct (Dubey et al., 2024). The learned verifiers are initialized with the same models as generators and have randomly initialized linear layer to predict scalar score (Stiennon et al., 2020). Datasets. We conduct experiments on MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021) where the agent needs to generate code solutions in Python given natural language descriptions. We train the methods on the MBPP training set which comprises 374 problems and Multi-Turn Code Generation Through Single-Step Rewards evaluate on the MBPP test set and HumanEval (HE) dataset which have 500 and 164 problems. We further describe the prompts and the split of public and private tests in Appendix A.3 and A.4. Approach Llama-3.2-1B Llama-3.1-8B HE HE MBPP MBPP Single-Turn Baselines. We compare µCODE with single and multi-turn baselines. For the single and multi-turn settings, we report metrics by generating solutions from Llama models which we denote as Instruct. We also compare with STaR (Zelikman et al., 2022) where the correct solutions of the Instruct model are used for fine-tuning (FT). We also compare to multi-turn version of STaR, called Multi-STaR. Here, we collect multi-turn rollouts using the Instruct model and use trajectories terminating in correct code solution for FT. For multi-turn BoN search, we collect the solutions that pass public tests, and then we select the best one judged by learned verifier. Note that this verifier is specifically trained for each generator. Metrics. We measure the performance with the BoN accuracy, which quantifies the accuracy of the solution chosen by verifier from candidate solutions. The generator is allowed = 3 turns and the final turn is used for evaluation over private tests. At each turn, the verifier ranks = 5 solutions (unless stated otherwise) provided by the generator. For the BoN performance, we sample with temperature of 0.7. We also report the accuracy of generating correct solutions via greedy decoding. Instruct STaR 1 1 36.5 35.7 28.0 34. 52.1 53.7 59.8 54.9 Instruct +BoN 1 5 Multi-STaR 1 5 1 5 +BoN µCODE +BoN Multi-Turn 38.9 48.5 36.7 47.9 37.9 50.7 29.3 34.3 33.5 39.6 35.4 41.7 58.9 68.1 57.7 68.6 62.3 68.8 60.4 61.2 59.8 63.2 57.9 62.2 Table 1. Comparison of our method µCODE with baselines across MBPP and HumanEval datasets. = 1 denotes generating solutions with 0 temperature. The Best-of-N (BoN) accuracy is computed with = 5 candidate solutions at each where the public tests and learned verifier is used for selection. We observe that µCODE outperforms competing methods based on Llama-3.2-1BInstruct and Llama-3.1-8B-Instruct models. The best performance for each dataset and model-sized is highlighted in bold and similar performances (within 1%) are underlined. with BoN search on MBPP and HumanEval datasets by 2.8% and 2.1% with 1B sized-model and performs comparably on 8B-sized model. 4.2. Results 4.3. Analysis In Table 1, we compare the proposed algorithm µCODE with the baselines. Here, we first evaluate the generators using code generated via greedy sampling for each problem (N = 1). This measures the accuracy of generating correct solution with turn limit of = 3. We observe that multi-turn methods (both Instruct and Multi-STaR) perform better than their single-turn variants showing the importance of incorporating execution feedback. Our approach µCODE outperforms Multi-STaR across both benchmarks with 1Bsized model demonstrating the efficacy of training generators with data obtained with learned verifier. To highlight, our method µCODE with 1B parameter model achieves 1.9% performance gains compared to Multi-STaR on the HumanEval dataset. With an 8B-sized model, µCODE outperforms baselines on MBPP whereas there is drop when compared on HumanEval. We further evaluate the effect of having verifier for BoN search during inference, where learned verifier selects the most promising candidate solution at each turn. In Table 1, we observe that all algorithms can benefit with BoN search. Remarkably, µCODE attains performance gain of up to 12.8% with the multi-turn BoN approach compared to greedy. Moreover, µCODE outperforms leading baselines To understand the improvements, we conduct componentwise ablation study where we 1) check the effect of oracle and learned verifiers for relabeling to train the generator (4.3.1), 2) evaluate different generators trained with and without learned verifiers (4.3.2), 3) check which verifier performs better multi-turn BoN search at test-time (4.3.3), 4) assess scaling behaviors at inference time with number of candidate generations (N ) at each turn (4.3.4), and 5) study the benefits of learned verifiers during evaluation (4.3.5). 4.3.1. VERIFIER FOR RELABELING We compare different verifiers for relabeling data for training the generator. In contrast to µCODE where the learned verifier is used to relabel (LV), we compare with relabeling using correct solution (attains an oracle reward = 1) for the corresponding prompt (OV). We also compare with variant where the generator is fine-tuned over combinations of data relabeled with both the oracle verifier and the learned verifier (OV+LV). Here, we concatenate the FT dataset obtained using both LV and OV. In Figure 2, we present results with the 1B-sized models across benchmarks and observe that having corrections with the oracle verifier outcome does not perform well. However, relabeling with 5 Multi-Turn Code Generation Through Single-Step Rewards both verifiers OV+LV outperforms the OV variant, further demonstrating that gains in the generator learned by µCODE are coming from relabeling with learned verifier. Lastly, the LV variant performed best on MBPP and comparably on HumanEval dataset when compared with LV+OV. Figure 2. Comparison of relabeling with learned verifier (LV) and oracle verifier (OV) with the 1B model. The variant OV+LV aggregates dataset from both verifiers for fine-tuning the generator. Note that OV+LV performs better than OV. However, relabeling with LV outperforms on MBPP and performs comparably on HumanEval, thereby demonstrating the benefits of leveraging the learned verifier for training the generator. Figure 3. Comparison of µCODE and baselines with 1B models on the ability of the learned generator to incorporate execution feedback at each turn. We observe that µCODE consistently improves the BoN accuracy across turns on both datasets, whereas the baselines show marginal improvements with turns. 4.3.2. VARYING THE GENERATOR In this section, we compare the multi-turn agents where the generator is trained with an oracle verifier (Multi-STaR) or learned verifier (µCODE). We evaluate the ability of the trained generator to utilize execution feedback and improve the code response across turns. We report the BoN accuracy till turn t, which denotes the BoN accuracy of obtaining correct solution till turn t. In Figure 3, we present the results with 1B-sized models. We observe that BoN accuracy improves with turns for µCODE whereas the baseline agents show marginal improvements with successive turns. We believe that using learned verifier for relabeling improves the generators ability to generate solutions with high reward values, and indeed recover better at every turn by utilizing the execution feedback. 6 Approach Llama-3.2-1B Llama-3.1-8B HE MBPP HE MBPP Base Random LV PT PT+LV Multi-STaR Random LV PT PT+LV µCODE Random LV PT PT+LV 34.4 40.3 48.6 48.5 35.6 39.8 46.7 47.9 37.9 45.1 49.8 50. 23.0 27.0 31.9 34.3 30.5 31.9 37.6 39.6 31.5 35.4 39.0 41.7 59.3 61.1 67.2 68.1 59.2 61.2 67.6 68.6 60.5 64.3 68.7 68. 57.9 61.2 60.4 61.2 57.7 62.8 60.0 63.2 59.1 60.4 59.1 62.2 Table 2. Comparing BoN with different ways of picking solutions at each turn for multi-turn BoN search using the 1B sized model. The hierarchical approach of using public test and learned verifier (PT+LV) outperforms only using either public tests (PT) or the learned verifier (LV). The best performance for each dataset and model-size is highlighted in bold and similar performances (within 1%) are underlined. 4.3.3. VERIFIER AT TEST-TIME In our experiments with multi-turn BoN  (Table 2)  , we pick the best solution based on the outcome of public tests and the scores of the learned verifier. In this experiment, we study different verifiers for ranking the candidate solutions at each turn. We test with Random strategy where the policy randomly picks from the solutions at each step. We compare to the public tests (PT) outcome that picks any solution that passes the public test. Note that this involves evaluating all generated solutions at every turn with public tests. We also compare to selecting solution based on scores obtained via the learned verifier only (LV). This is crucial as in certain applications such privileged information like public tests are not available and the agents can benefit from learned verifiers during inference. Lastly, we compare with the combination of public tests and leveraging the scores of the learned verifier to break ties at each turn (PT+LV). In Table 2, we compare Base, Multi-STaR and µCODE with different verifiers at test-time. We observe that LV outperforms Random strategy which shows that learned verifier indeed aids in selecting better solutions among generations. Given the benefits of learned verifiers for multi-turn BoN search, they can be good alternative when public tests are not available. Furthermore, using the outcome of public tests (PT) performed better than using learned verifiers (LV) Multi-Turn Code Generation Through Single-Step Rewards except on the HumanEval datset for 8B-sized model. We believe that this gap can be further reduced by learning more powerful verifiers and leave it for future research. Interestingly, the hierarchical approach (PT+LV) that uses the learned verifier to break ties on the outcomes of public tests performed best across methods and datasets. We hypothesize that using learned verifiers is beneficial in two scenarios. Firstly, if multiple solutions pass the public tests, then the learned verifier can filter out incorrect solutions which may not pass private tests. Secondly, if all candidate solutions are incorrect, then the learned verifier should choose the most promising solution at each turn. This is crucial as picking better solution with the learned verifier can lead to more relevant feedback for recovering the true solution. 4.3.4. TEST-TIME SCALING also present the scaling behaviors of different learned verifiers. We observe that using verifier trained with on-policy samples obtained via the generator of µCODE performs better and showed significant improvements of up to 4.3% for different values of candidate solutions . Figure 6 presents qualitative example of multi-turn Bestof-N search with µCODE. Through this example, we demonstrate the advantages of dense scores from the learned verifier at facilitating efficient search across turns. We generate = 5 code solutions at each turn and show the top 3 ranked solutions using the dense scores. At the first turn, we observe that the last solution y3 1 is less accurate than the other 2 solutions y1 1. The top ranked solution is used to collect the environment feedback, upon which the generator comes up with new candidate solutions. Upon the top 3 solutions, the last two snippets are similar to the candidates from the previous turn. However, the top ranked solution is novel solution and is more accurate as the generated code learns to extract single digit and multiply it. With the execution feedback, µCODE generates 2 correct responses y1 3 and learned verifier chooses one of them compared to the incorrect response y3 3. 3 and y2 1 and y2 Figure 4. Test-time scaling with different values of candidate solutions at each turn and different ways of learning verifiers. We compare with verifiers learned on samples from µCODE and base policy. The candidate solutions are obtained from the 1B generator of µCODE at each turn. We observe that the BoN performance improves with larger values of on both datasets. The verifier learned with on-policy samples perform better. In the multi-turn setting, the number of candidate solutions can rise exponentially with the number of turns. To avoid this, µCODE uses the learned verifier during inference to select the most promising candidate among candidates at each turn, leading to linearly increasing number of calls to the generator. We study the inference-time scaling behaviors of µCODE where we scale the number of candidate generations at each turn. Figure 4 plots the BoN with different values of (1 11). The performance gains diminishes for larger on both datasets. On the MBPP dataset, the performance gains diminish with 5, whereas on HumanEval dataset dip is observed for > 7. In this section, we further study the importance of training the verifier with on-policy rollouts from the generator. We present comparison of verifier trained with samples from the Llama-3.2-1B-Instruct model (Base Verifier) and verifier learned with samples from the generator of µCODE. Note that we use the generator of µCODE to obtain candidate solutions at each turn during evaluation. In Figure 4, we Figure 5. Comparison between BCE and BT loss function for training the verifier. We train the verifiers on samples generated by the base model (Llama-3.2-1B-Instruct). The learned verifier then ranks the candidate solutions from base model and the BoN performance of selected solution is reported. The verifier trained with BT loss performs better increasing value of N. 4.3.5. LOSS FUNCTION FOR VERIFIER As described in 3.2, we compare against different loss functions for training the verifier. For this experiment, we first generate multiple single step rollouts and label them via oracle verifier. Given oracle labels, we train verifiers with two loss functions BCE and BT. During inference, the learned verifier picks the best ranked solution among the solutions provided by the generator. Similar to (Cobbe et al., 2021), we report the BoN plot with different values of obtained by first sampling candidate solutions, choosing the top-ranked solution using the learned verifier, and then evaluating the solution against public and private tests. We calculate this metric over multiple samples for each value of 7 Multi-Turn Code Generation Through Single-Step Rewards Figure 6. qualitative example of multi-turn BoN search using dense rewards obtained via the learned verifier in µCODE. Here, we show the top 3 ranked solutions at each turn where Rϕ(x, yi ) for < j. We observe that the learned verifier selects the better solution (in orange) at each turn. The selected solution is passed to public tests to retrieve execution feedback for the generator to improve the next code solution. The selected solution at each turn is better than the last (less errors highlighted in yellow), with the final solution passing all tests. Note that there are 2 correct solutions at the final turn. t) Rϕ(x, yj . In Figure 5, we observe that the verifier trained with BT loss consistently outperforms the verifier trained on BCE loss on both MBPP and HumanEval. 5. Related Work Prompting To Solve Multi Step Tasks common framework for tackling multi-step tasks with LLMs is promptingbased agentic systems. Self-Debugging (Chen et al., 2023b) asks the LLM to iteratively improve code by providing execution feedback while CodeT (Chen et al., 2022) asks the LLM to generate test cases. AlphaCodium (Ridnik et al., 2024) first reflects on input instructions, generates and filters from multiple code generations, and finally iterates on public and self-generated test cases. MapCoder (Islam et al., 2024) incorporates four agents to generate example problems, plans and code, and then perform debugging. However, prompting-based agents yield limited improvements. Training LLMs for Multi Step Tasks Some work has explored explicitly training critics or reward models for multi-step reasoning tasks. In the coding domain, CodeRL (Le et al., 2022) trains token-level critic to aid in code generation and to perform inference-time search. CodeRLs mechanics are similar to our method, but their generator is not trained for multi-step: CodeRL trains code repairer which conditions on one erroneous code completion while our generator incorporates multiple. ARCHER (Zhou et al., 2024), which frames multi-step tasks via two-level hierarchical MDP, where the higher level MDP considers completions as actions and the lower level MDP considers tokens as actions. Another line of work utilizes Monte Carlo Tree Search (MCTS) methods for training: rStar-Math (Guan et al., 2025) trains policy preference model to boost small LMs math abilities to match or exceed large reasoningbased LMs and ReST-MCTS (Zhang et al., 2024) trains process reward model (PRM) similarly to Math-Shepherd (Wang et al., 2024a). Although µCODEs BoN search resembles tree search, our key insight that multi-step code generation resembles one-step recoverable MDP allows us to collect training trajectories much more efficiently. Finally, some work has explored using verifiers only during inference time. In Lets Verify Step by Step (Lightman et al., 2023), the authors demonstrate that PRMs trained on erroneous math solutions annotated by humans outperform outcome reward models for filtering multiple inference time generations. Meanwhile, AlphaCode (Li et al., 2022) trains test generator to evaluate multiple code solutions. Other works omit learning critic or reward model altogether. In the coding domain, RLEF (Gehring et al., 2024b) derives rewards only on the executors result on test cases and syntax checkers, and PPOCoder (Shojaee et al., 2023) additionally considers semantic and syntactic alignment, generated via data flow graphs and abstract syntax trees respectively, with reference solution. The oracle rewards in these methods may not be informative for training, and in the case of PPOCoder, require complex constructs. We empirically show that having reward model is beneficial by comparing µCODE against the Multi-STaR baseline. Meanwhile, SCoRe (Kumar et al., 2024a) splits training into generator and correction phase, thus restricting the total number of turns to 2. RISE (Qu et al., 2024) generates recovery steps via more powerful LLM or by selecting sampled completion via the oracle rewards. Both methods are less efficient than µCODE, which doesnt require generating corrections beyond generating training trajectories. Finally, FireAct (Chen et al., 2023a) and LEAP (Choudhury & Sodhi, 2024) FT ReAct style agents while RL4VLM 8 Multi-Turn Code Generation Through Single-Step Rewards (Zhai et al., 2024) and GLAM (Carta et al., 2024) studies training LLMs with interactive environment feedback. 6. Conclusion We present µCODE, simple and scalable method for multiturn code generation through single-step rewards. µCODE models code generation as one-step recoverable MDP and learns to iteratively improve code with learned verifier to guide the search. Experimental results demonstrate that µCODE outperforms methods using oracle verifiers by large margin. We acknowledge the following limitations of this paper. Due to limited budget, we were only able to train models with up to eight-billion parameters. It is possible that the conclusions made in this paper do not generalize to models of larger scales. Additionally, we train models on MBPP, whose training set has only 374 examples. However, we hypothesize that more training examples will lead to better performance. Finally, our datasets are only in Python, and our findings might not generalize to other programming languages."
        },
        {
            "title": "Impact Statement",
            "content": "The proposed method for training code agents has the potential to streamline software development processes by automating routine coding tasks, thereby reducing human labor and accelerating production timelines. However, these advances will also introduce bugs, which can propagate at scale if no proper quality control is in place."
        },
        {
            "title": "Acknowledgements",
            "content": "AJ is supported by Fonds de Recherche du Quebec (FRQ), Calcul Quebec, Canada CIFAR AI Chair program, and Canada Excellence Research Chairs (CERC) program. The authors are also grateful to Mila (mila.quebec) IDT and Digital Research Alliance of Canada for computing resources. AMR is supported in part by NSF CAREER #2037519 and NSF #2242302. SC is supported in part by Google Faculty Research Award, OpenAI SuperAlignment Grant, ONR Young Investigator Award, NSF RI #2312956, and NSF FRR#2327973."
        },
        {
            "title": "References",
            "content": "Anthony, T., Tian, Z., and Barber, D. Thinking fast and slow with deep learning and tree search, 2017. URL https://arxiv.org/abs/1705.08439. Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Re, C., and Mirhoseini, A. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Carta, T., Romac, C., Wolf, T., Lamprier, S., Sigaud, O., and Oudeyer, P.-Y. Grounding large language models in interactive environments with online reinforcement learning, 2024. URL https://arxiv.org/abs/ 2302.02662. Chen, B., Zhang, F., Nguyen, A., Zan, D., Lin, Z., Lou, J.-G., and Chen, W. Codet: Code generation with generated tests, 2022. URL https://arxiv.org/abs/ 2207.10397. Chen, B., Shu, C., Shareghi, E., Collier, N., Narasimhan, K., and Yao, S. Fireact: Toward language agent finetuning, 2023a. URL https://arxiv.org/abs/ 2310.05915. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code, 2021. Chen, X., Lin, M., Scharli, N., and Zhou, D. Teaching large language models to self-debug, 2023b. URL https: //arxiv.org/abs/2304.05128. Chen, X., Lin, M., Scharli, N., and Zhou, D. Teaching In The Twelfth large language models to self-debug. International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=KuPixIqPiq. Choudhury, S. and Sodhi, P. Better than your teacher: Llm agents that learn from privileged ai feedback, 2024. URL https://arxiv.org/abs/2410.05434. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Multi-Turn Code Generation Through Single-Step Rewards Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Gehring, J., Zheng, K., Copet, J., Mella, V., Cohen, T., and Synnaeve, G. Rlef: Grounding code llms in execution feedback with reinforcement learning. arXiv preprint arXiv:2410.02089, 2024a. Gehring, J., Zheng, K., Copet, J., Mella, V., Cohen, T., and Synnaeve, G. Rlef: Grounding code llms in execution feedback with reinforcement learning, 2024b. URL https://arxiv.org/abs/2410.02089. Guan, X., Zhang, L. L., Liu, Y., Shang, N., Sun, Y., Zhu, Y., Yang, F., and Yang, M. rstar-math: Small llms can master math reasoning with self-evolved deep thinking, 2025. URL https://arxiv.org/abs/2501.04519. Islam, M. A., Ali, M. E., and Parvez, M. R. Mapcoder: Multi-agent code generation for competitive problem solving, 2024. URL https://arxiv.org/abs/ 2405.11403. Kakade, S. and Langford, J. Approximately optimal approximate reinforcement learning. In Proceedings of the Nineteenth International Conference on Machine Learning, pp. 267274, 2002. Kumar, A., Zhuang, V., Agarwal, R., Su, Y., Co-Reyes, J. D., Singh, A., Baumli, K., Iqbal, S., Bishop, C., Roelofs, R., Zhang, L. M., McKinney, K., Shrivastava, D., Paduraru, C., Tucker, G., Precup, D., Behbahani, F., and Faust, A. Training language models to self-correct via reinforcement learning, 2024a. URL https://arxiv.org/abs/2409.12917. Kumar, A., Zhuang, V., Agarwal, R., Su, Y., Co-Reyes, J. D., Singh, A., Baumli, K., Iqbal, S., Bishop, C., Roelofs, R., et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024b. Le, H., Wang, Y., Gotmare, A. D., Savarese, S., and Hoi, S. C. H. Coderl: Mastering code generation through pretrained models and deep reinforcement learning, 2022. URL https://arxiv.org/abs/2207.01780. Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., Hubert, T., Choy, P., de Masson dAutume, C., Babuschkin, I., Chen, X., Huang, P.-S., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J., Mankowitz, D. J., Sutherland Robson, E., Kohli, P., de Freitas, N., Kavukcuoglu, K., and Vinyals, O. Competitionlevel code generation with alphacode. Science, 378 (6624):10921097, December 2022. ISSN 1095-9203. doi: 10.1126/science.abq1158. URL http://dx.doi. org/10.1126/science.abq1158. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step, 2023. URL https: //arxiv.org/abs/2305.20050. Muennighoff, N., Liu, Q., Zebaze, A., Zheng, Q., Hui, B., Zhuo, T. Y., Singh, S., Tang, X., von Werra, L., and Longpre, S. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124, 2023. Ni, A., Allamanis, M., Cohan, A., Deng, Y., Shi, K., Sutton, C., and Yin, P. NExt: Teaching large language models to reason about code execution. In Forty-first International Conference on Machine Learning, 2024. URL https: //openreview.net/forum?id=B1W712hMBi. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022. URL https: //arxiv.org/abs/2203.02155. Qu, Y., Zhang, T., Garg, N., and Kumar, A. Recursive introspection: Teaching language model agents how to selfimprove, 2024. URL https://arxiv.org/abs/ 2407.18219. Ridnik, T., Kredo, D., and Friedman, I. Code generation with alphacodium: From prompt engineering to flow engineering, 2024. URL https://arxiv.org/abs/ 2401.08500. Ross, S. and Bagnell, J. A. Reinforcement and imitation learning via interactive no-regret learning. arXiv preprint arXiv:1406.5979, 2014. Ross, S., Gordon, G., and Bagnell, D. reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627635. JMLR Workshop and Conference Proceedings, 2011. Setlur, A., Nagpal, C., Fisch, A., Geng, X., Eisenstein, J., Agarwal, R., Agarwal, A., Berant, J., and Kumar, A. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. Shojaee, P., Jain, A., Tipirneni, S., and Reddy, C. K. Execution-based code generation using deep reinforcement learning, 2023. URL https://arxiv.org/ abs/2301.13816. 10 Multi-Turn Code Generation Through Single-Step Rewards Zhai, Y., Bai, H., Lin, Z., Pan, J., Tong, S., Zhou, Y., Suhr, A., Xie, S., LeCun, Y., Ma, Y., and Levine, S. Finetuning large vision-language models as decision-making agents via reinforcement learning, 2024. URL https: //arxiv.org/abs/2405.10292. Zhang, D., Zhoubian, S., Hu, Z., Yue, Y., Dong, Y., and Tang, J. Rest-mcts*: Llm self-training via process reward guided tree search, 2024. URL https://arxiv. org/abs/2406.03816. Zhao, W., Jiang, N., Lee, C., Chiu, J. T., Cardie, C., Galle, M., and Rush, A. M. Commit0: Library generation from scratch. arXiv preprint arXiv:2412.01769, 2024. Zheng, L., Yin, L., Xie, Z., Sun, C., Huang, J., Yu, C. H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., Barrett, C., and Sheng, Y. Sglang: Efficient execution of structured language model programs, 2024. URL https://arxiv.org/abs/2312.07104. Zhou, Y., Zanette, A., Pan, J., Levine, S., and Kumar, A. Archer: Training language model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and ChrisLearning to summarize with human tiano, P. F. feedback. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 30083021. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/ 1f89885d556929e98d3ef9b86448f951-Paper. pdf. Sun, W., Venkatraman, A., Gordon, G. J., Boots, B., and Bagnell, J. A. Deeply aggrevated: Differentiable imitation learning for sequential prediction. In International conference on machine learning, pp. 33093318. PMLR, 2017. Swamy, G., Choudhury, S., Bagnell, J. A., and Wu, S. Of moments and matching: game-theoretic framework for closing the imitation gap. In International Conference on Machine Learning, pp. 1002210032. PMLR, 2021. Wang, P., Li, L., Shao, Z., Xu, R. X., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024a. URL https://arxiv.org/abs/ 2312.08935. Wang, X., Chen, Y., Yuan, L., Zhang, Y., Li, Y., Peng, H., and Ji, H. Executable code actions elicit better LLM agents. In Forty-first International Conference on Machine Learning, 2024b. URL https://openreview. net/forum?id=jJ9BoXAfFa. Welleck, S., Lu, X., West, P., Brahman, F., Shen, T., Khashabi, D., and Choi, Y. Generating sequences In The Eleventh Inby learning to self-correct. ternational Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=hH36JeQZDaO. Wu, Y., Sun, Z., Li, S., Welleck, S., and Yang, Y. Inference scaling laws: An empirical analysis of computeoptimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2024. Zelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. 11 Multi-Turn Code Generation Through Single-Step Rewards A. Appendix A.1. Proof of Theorem 3. The proof relies on two important results. The first is the Performance Difference Lemma (PDL) (Kakade & Langford, 2002) which states that the performance difference between any two policies can be expressed as the sum of advantages. J(π) J(π) = (cid:88) t=1 Estdπ (cid:34) (cid:88) at Aπ (cid:35) (st, at)π(atst) (8) where st dπ We apply the PDL between the expert π and the learner π is the induced state distribution by π, and Aπ (st, at) = Qπ (st, at) π (st) is the advantage w.r.t. π. J(π) J(π) = (cid:88) t=1 Estdπ (cid:34) (cid:88) at A(st, at) (π(atst) π(atst)) (9) (cid:35) where the result follows from (cid:0)(cid:80) According to the one-step recoverable MDP definition, A(s, a) 1 for all (s, a). Hence we can bound the performance difference as A(st, at)π(atst) = 0(cid:1) at J(π) J(π) = (cid:88) t= Estdπ (cid:34) (cid:88) (cid:35) A(st, at) (π(ast) π(ast)) A(., .) at (cid:88) t=1 Estdπ π(.ht) π(.st) (Holders Inequality) (cid:88) t=1 Estdπ π(.st) π(.st)1 (One step recoverability) The second result we use us from interactive imitation learning DAGGER (Ross et al., 2011) that reduces imitation learning to no-regret online learning. DAGGER shows that with π as the expert teacher guarantees that after iterations, it will find at least one policy Esdπ π(.s) π(.s)1 Esdπ πclass(.s) π(.s)1 + γ(N ) where γ(N ) is the average regret, and dπ is the time average distribution of states induced by policy π, πclass is the best policy in policy class. (10) Plugging this in we have J(π) J(π) (cid:88) t=1 (cid:88) t= Estdπ π(.st) π(.st)1 Estdπ πclass(.st) π(.st)1 + γ(N ) From (10) (ϵ + γ(N )) 12 A.2. Hyperparameters Multi-Turn Code Generation Through Single-Step Rewards Model Generator Verifier Training Epochs Learning Rate Batch Size Max seq length 2 5 107 32 8192 2 1 106 64 2048 A.2.1. TRAINING PARAMETERS Table 3. Hyperparameters for SFT and RM training. Table 3 contains hyperparameters for training the generator and reward model on both models (Llama-3.1-8B-Instruct and Llama-3.2-1B-Instruct) and datasets (MBPP and HumanEval). We perform 2 iterations of training with µCODE, starting from the base model each iteration. All training runs were on machines with either 4 RTX 6000 Ada Generation GPUs for 1B models with 48 GB of memory per GPU or 4 H100 GPUs for 8B models with 80 GB of memory per GPU. A.2.2. INFERENCE PARAMETERS We use SGLang (Zheng et al., 2024) to serve our models for inference. Greedy experiments use temperature 0 with flags disable-radix-cache max-running-request 1 to ensure deterministic results while BoN search experiments use temperature of 0.7. All experiments are capped to 1000 tokens per completion per turn. A.3. Prompts A.3.1. SINGLE STEP PROMPT Immediately below is the prompt template to generate 1 code completion in single-step method or to generate the 1st step in multi-step method. Below the prompt templates are examples of the code prompt and public tests for HumanEval and MBPP. Single Step Prompt Write Python function implementation for the following prompt: {prompt} Your code should satisfy these tests: {test} Return only the implementation code, no tests or explanations. Be sure to include the relevant import statements: python code HumanEval Prompt from typing import List def has_close_elements(numbers: List[float], threshold: float) -> bool: \"\"\" Check if in given list of numbers, are any two numbers closer to each other than given threshold. >>> has_close_elements([1.0, 2.0, 3.0], 0.5) False >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True \"\"\" 13 Multi-Turn Code Generation Through Single-Step Rewards HumanEval Test def check(has_close_elements): assert has_close_elements([1.0, 2.0, 3.0], 0.5) == False assert has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) == True check(has_close_elements) MBPP Prompt Write function to find the minimum cost path to reach (m, n) from (0, 0) for the given cost matrix cost[][] and position (m, n) in cost[][]. MBPP Test assert min_cost([[1, 2, 3], [4, 8, 2], [1, 5, 3]], 2, 2) == 8 assert min_cost([[2, 3, 4], [5, 9, 3], [2, 6, 4]], 2, 2) == 12 assert min_cost([[3, 4, 5], [6, 10, 4], [3, 7, 5]], 2, 2) == 16 A.3.2. FEEDBACK PROMPT Immediately below is the prompt template for how we provide feedback in multi-step methods. The feedback only consists of executor error traces, and we provide an example from HumanEval. Multi-Step Feedback Prompt Feedback: {feedback} HumanEval Multi-Step Feedback Prompt Traceback (most recent call last): File \"test.py\", line 18, in <module> assert has_close_elements([1.0, 2.0, 3.0], 0.5) == False ˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆˆ AssertionError A.4. Public Private Tests We choose public-private test split for HumanEval and MBPP to ensure that naively passing the public tests does not guarantee private test success. For HumanEval, we use single test from the code prompts docstring as the public test and the remaining tests along with the official test suite as private tests. For ease of parsing, we utilize processed version of HumanEval, HumanEvalPack (Muennighoff et al., 2023). For MBPP, we use single test from the official test suite as the public test, and the remaining tests and any challenge test list tests as private tests."
        }
    ],
    "affiliations": [
        "Cornell University",
        "MilaQuebec AI Institute",
        "Universite de Montreal"
    ]
}