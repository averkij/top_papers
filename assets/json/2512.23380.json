{
    "paper_title": "A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers",
    "authors": [
        "Mohammad Nasirzadeh",
        "Jafar Tahmoresnezhad",
        "Parviz Rashidi-Khazaee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Log anomaly detection is crucial for preserving the security of operating systems. Depending on the source of log data collection, various information is recorded in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, multimodal methods fail to handle the interactions between these modalities. Applying multimodal sentiment analysis to log anomaly detection, we propose CoLog, a framework that collaboratively encodes logs utilizing various modalities. CoLog utilizes collaborative transformers and multi-head impressed attention to learn interactions among several modalities, ensuring comprehensive anomaly detection. To handle the heterogeneity caused by these interactions, CoLog incorporates a modality adaptation layer, which adapts the representations from different log modalities. This methodology enables CoLog to learn nuanced patterns and dependencies within the data, enhancing its anomaly detection capabilities. Extensive experiments demonstrate CoLog's superiority over existing state-of-the-art methods. Furthermore, in detecting both point and collective anomalies, CoLog achieves a mean precision of 99.63%, a mean recall of 99.59%, and a mean F1 score of 99.61% across seven benchmark datasets for log-based anomaly detection. The comprehensive detection capabilities of CoLog make it highly suitable for cybersecurity, system monitoring, and operational efficiency. CoLog represents a significant advancement in log anomaly detection, providing a sophisticated and effective solution to point and collective anomaly detection through a unified framework and a solution to the complex challenges automatic log data analysis poses. We also provide the implementation of CoLog at https://github.com/NasirzadehMoh/CoLog."
        },
        {
            "title": "Start",
            "content": "Accepted in scientiﬁc reports on 5 November 2025 DOI: 10.1038/s41598-025-27693-4 Please cite the journal version of this article. uniﬁed framework for detecting point and collective anomalies in operating system logs via collaborative transformers Mohammad Nasirzadeh1, Jafar Tahmoresnezhad1*, Parviz Rashidi-Khazaee1 1*Faculty of Information Technology and Computer Engineering, Urmia University of Technology, Band, Urmia, 57166-17165, West Azerbaijan, Iran. *Corresponding author(s). E-mail(s): j.tahmores@it.uut.ac.ir; Contributing authors: nasirzadeh.moh@it.uut.ac.ir; p.rashidi@uut.ac.ir;"
        },
        {
            "title": "Abstract",
            "content": "Log anomaly detection is crucial for preserving the security of operating systems. Depending on the source of log data collection, various information is recorded in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the diﬀerent modalities of log data. Meanwhile, multimodal methods fail to handle the interactions between these modalities. Applying multimodal sentiment analysis to log anomaly detection, we propose CoLog, framework that collaboratively encodes logs utilizing various modalities. CoLog utilizes collaborative transformers and multi-head impressed attention to learn interactions among several modalities, ensuring comprehensive anomaly detection. To handle the heterogeneity caused by these interactions, CoLog incorporates modality adaptation layer, which adapts the representations from diﬀerent log modalities. This methodology enables CoLog to learn nuanced patterns and dependencies within the data, enhancing its anomaly detection capabilities. Extensive experiments demonstrate CoLogs superiority over existing state-of-the-art methods. Furthermore, in detecting both point and collective anomalies, CoLog achieves mean precision of 99.63%, mean recall of 99.59%, and mean F1 score of 99.61% across seven benchmark datasets for log-based anomaly detection. The comprehensive detection capabilities of CoLog make it highly suitable for cybersecurity, system monitoring, and operational eﬃciency. CoLog represents signiﬁcant advancement in log anomaly detection, providing sophisticated and eﬀective solution to point and collective anomaly detection through uniﬁed framework and solution to the complex challenges 5 2 0 2 9 2 ] . [ 1 0 8 3 3 2 . 2 1 5 2 : r Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 automatic log data analysis poses. We also provide the implementation of CoLog at https://github.com/NasirzadehMoh/CoLog. Keywords: Log anomaly detection, Multimodal sentiment analysis, Point and collective anomaly detection, Class imbalance, Deep learning"
        },
        {
            "title": "1 Introduction",
            "content": "Anything that happens in system during its interactions is recorded in system logs, including time-stamped events (e.g., transactions, errors, and intrusions). Anomalies, also known as outliers, are patterns or events in log data that deviate from the systems usual behavior. Log-based anomaly detection identiﬁes and locates log outliers [1]. Nevertheless, manually analyzing logs or applying traditional methods [219] becomes more challenging due to log datas large-scale, complicated, and dynamic nature, as these methods primarily rely on manual processing and rule-setting [20 23]. Furthermore, log data originating from various systems may employ distinct terminology. Hence, deep learning-based automated log anomaly detection is crucial to address threats or challenges promptly, deﬁne reaction thresholds, and make data-driven decisions [24]. In recent years, deep learning has shown considerable promise in automating many tasks [2534]. One illustration of these tasks is the process of automated log anomaly detection. Due to deep learning techniques outstanding results in automated log anomaly detection, utilizing these techniques for anomaly detection in log ﬁles can provide real-time alerts for crucial scenarios, including unidentiﬁed threats, identifying underlying causes, and mitigating cyberattacks, fraudulent activities, or system malfunctions [3539]. Consequently, deep neural networks have become the dominant modeling method in automated log-based anomaly detection in recent years. This research direction starts with DeepLogs revolutionary results in detecting anomalies from log ﬁles [36]. Through the years, diﬀerent deep learning-based approaches have evolved to become increasingly inﬂuential in this ﬁeld [40]. Numerous research directions exist within the automated log anomaly detection domain, such as multi-layer perceptrons (MLPs) [4145] that are used as part of other deep learning architectures, convolutional neural networks (CNNs) [4651], recurrent neural networks (RNNs) [36, 38, 5267], autoencoders [42, 6872], generative adversarial networks (GANs) [42, 52, 7378], transformers [21, 7985], attention mechanisms [37, 41, 48, 8689] that are usually employed to enhance performance in other deep neural architectures, graph neural networks (GNNs) [90], evolving granular neural networks (EGNNs) [91], and large language models (LLMs) [35, 92, 93]. As demonstrated in Figure 1, depending on the type of log data collecting source, wide range of information can be stored in logs. Practically, this data is expressed through diﬀerent aspects, which we refer these aspects to as modalities. Each log ﬁle follows two primary modalities: (1) the sequence of records, which we refer to as sequence modality, and (2) the content of each record, which we refer to as semantic modality. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 - 1117839085 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.51.25.044165 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected = - 1117839085 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.51.25.220957 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected = - 1117839085 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.51.25.399362 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected = - 1117839085 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.51.25.556090 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected = - 1117839085 2005.06.03 R20-M1-N9-C:J17-U01 2005-06-03-15.51.25.759846 R20-M1-N9-C:J17-U01 RAS KERNEL INFO generating core. 784 = - 1117839085 2005.06.03 R24-M1-N6-C:J11-U11 2005-06-03-15.51.25.890083 R24-M1-N6-C:J11-U11 RAS KERNEL INFO generating core. 253 = - 1117839085 2005.06.03 R24-M1-ND-C:J16-U01 2005-06-03-15.51.25.911004 R24-M1-ND-C:J16-U01 RAS KERNEL INFO generating core. 320 = - 1117839085 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.51.25.916470 R02-M1-N0-C:J12-U11 RAS KERNELcINFO instructionccachecparity error corrected = = = = = + + 4 and 8 with combined background 4 and 8 without combined background 1 and 2 are event sequences that appear in the log. Unified semantic space Fig. 1 Illustration of how the sequence modality (B1, B2) adds more information to semantic modality (e4, e8) and how the same samples with diﬀerent backgrounds are separated in uniﬁed semantic space. For the e4 and e8 event vectors, their backgrounds, speciﬁcally B1 and B2, might be utilized to depict e4 and e8 in more meaningful manner (it should be noted that e4 is equivalent to e8). In this sense, we designate the B1 and B2 as sequence modalities and the e4 and e8 as semantic modalities. On one side, the sequence of events feature can contribute information to each log event from semantic perspective. This means that various types of information in log ﬁle can be used to create more accurate model. Figure 1 illustrates how sequence and semantic modalities are relevant and how respective sequence vectors can be utilized to distinguish identical events vectors. On the other side, these modalities allow the model to make decisions from various perspectives, thereby enhancing the precision of outcomes when performing the assigned tasks. On the one hand, most existing literature approaches concentrate on unimodal anomaly detection from logs [21, 35, 37, 38, 41, 43, 46, 47, 49, 50, 5254, 56 58, 60, 63, 64, 68, 7082, 87, 89, 90, 92, 93]. These studies only utilize speciﬁc facet of the information recorded in the log. However, disregarding other modalities of log leads to the forfeiture of valuable data. In fact, focusing the analysis of log ﬁles based on singular aspect of their information can lead to the models inability to identify more complex anomalies. On the other hand, the multimodal approaches proposed in the literature utilize early [51, 59, 61, 62, 83, 85, 86, 88, 91], intermediate [42, 45, 48, 55, 6567], and late [44] fusion mechanisms or separate models [36, 69, 84] for diﬀerent modalities that they extract, leading to various challenges, which are presented in Table 1. Concatenating unprocessed data from several modalities can lead to very high-dimensional input, which can be challenging to handle due to highdimensionality challenges. Noise in the raw data may aﬀect the model because noise in one modality may inﬂuence the information as whole, indicating noise sensitivity challenges. Concatenating raw data leads to complexity, particularly with diverse data types and formats, leading to heightened data complexity challenges. Features derived Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 from several modalities may not consistently possess direct compatibility or eﬀortless combinability, highlighting compatibility challenges. Deploying distinct models for each modality can augment the systems overall complexity, resulting in heightened network complexity challenges. The contributions of several modalities can be challenging, mainly when one modality provides more information than the others, causing balancing contribution challenges. Merging the outcomes at the decision level or utilizing distinct models for each modality might ignore signiﬁcant interactions between modalities, resulting in lack of shared information challenges. Performing processes separately for each modality can result in redundant computations, mainly if overlapping features exist, causing redundancy challenges. The potential for enhancing performance may be restricted because the modalities have not been merged until the ﬁnal decision step, leading to limited improvement challenges. Table 1 Challenges arising from the multimodal approaches covered in the log anomaly detection literature. Early fusion all except MLog 1 all all all all Methods Intermediate fusion Mdfulog2, Swisslog3, WDLog4, and FSMFLog5 Mdfulog, Swisslog, WDLog, and FSMFLog all Mdfulog, Swisslog, WDLog, and FSMFLog Late fusion Separate Models all all all all all all all all all Challenges High dimensionality Noise sensitivity Heightened data complexity Compatibility of features Heightened network complexity Balancing contributions Lack of shared information Redundancy Limited improvement 1Fu et al. proposed MLog 2Li et al. proposed Mdfulog 3Li et al. proposed Swisslog 4Niu et al. proposed WDLog 5Niu et al. proposed FSMFLog Considering the above motivations, we propose novel automated log-based anomaly detection approach - called CoLog - based on multimodal sentiment analysis (MSA) [94103] to overcome the above mentioned challenges. CoLog interprets anomalies in the log as negative sentiments. In the same way, normal samples are classiﬁed as positive sentiments. Since the two most important modalities of log ﬁle are the semantic features of an event and the features obtained from the sequence of events. The background and context of an event can be formed as sequence modality. CoLog Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 constructs semantic and sequence modalities to learn more about anomalies through the interaction between these modalities. During this process, we employ form of the guided-attention (GA) mechanism [104] to encode each modality in an collaborative manner with other modalities. Finally, to accomplish the objective of prediction through integrating modalities, we apply transformer blocks and then transform the outcome feature vectors of each modality into higher-dimensional space known as the latent space to perform meaningful fusion. In addition, logs are often the place where the point and collective anomalies are recorded. Detecting point and collective anomalies in log records is crucial, as each anomaly type exposes distinct facets of system behavior. Point anomalies, representing isolated instances of abnormal behavior, often signal immediate issues like sudden malfunction or attack. Collective anomalies are patterns of anomalous activity over time that may signify deeper, more systemic issues, such as steady performance degradation or persistent security threats. Detecting diﬀerent kinds of abnormalities provides thorough awareness of short-term and long-term issues, facilitating more eﬀective troubleshooting and preventative care. However, all existing works solely concentrate on identifying only one type of log anomaly. Besides that, to the best of our knowledge, two techniques exist outside the log-based anomaly detection area that aim to identify point and collective anomalies utilizing uniﬁed framework. Li et al. processed time series using temporal convolutional network (TCN) and calculated the anomaly score to assess whether the point or collective data was anomalous based on the ﬁxed-length sequences. However, due to its reliance on predetermined sequence length, it may fail to identify certain instances of collective anomalies. Moreover, Liu et al. used an improved local outlier factor (ILOF) to detect variable-length collective anomalies where uses identiﬁed point anomalies as boundary points to partition the residual sequence into many subsequences of varied lengths. Each subsequences anomaly score is computed using the ILOF method. This method relies on calculating anomaly scores, which can lead to false alarms, particularly in long sequences. Furthermore, neither of these approaches explicitly learns the relationships between the various modalities. Therefore, leveraging the transformer [107] architecture, we propose CoLog to handle the problem of detecting point and collective log anomalies by utilizing uniﬁed framework. To achieve this objective, we propose utilizing collaborative transformer (CT) to process log modalities in an end-to-end manner, enabling them to interact and learn about their relationships. Since CoLog is supervised approach, it does not face the challenge of accumulating errors. The summarization of our contribution is as follows. 1. We propose CoLog, the ﬁrst framework to formulate log anomaly detection as multimodal sentiment analysis problem, enabling the detection of both point and collective anomalies in uniﬁed manner. 2. We design collaborative transformer architecture with impressed attention and modality adaptation layer to capture nuanced interactions between semantic and sequence modalities. 3. We introduce balancing mechanism to adaptively weight contributions of modalities, which helps to ensure that the modalities with inherent diﬀerences are represented in the same representation space. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 4. We assess CoLog using seven benchmark datasets, achieving state-of-the-art performance, and provide the CoLog implementation to facilitate reproducibility in the GitHub repository. 5. Our research ﬁndings and specialized services are available on the Alarmif website, facilitating direct link between research and practical application. The paper is organized as follows. Section 2 discusses previous research in the domain. Deﬁne the task, the importance of transformers for this work, the threat model, and the assumptions outlined in Section 3. Section 4 will discuss CTs design and training considerations for anomaly detection from logs based on MSA. The evaluation is presented in Section 5, where we conclude our work and then oﬀer suggestions for future research in Section 6."
        },
        {
            "title": "2 Related Work",
            "content": "Numerous research endeavors have examined log-based anomaly detection [1, 20, 22 24, 40, 108]. These approaches have evolved from regular expressions and rule-based methods to applying deep learning-based techniques. Historically, traditional techniques employed regular expressions [19], rule-based methods [5, 6, 8, 9, 1518], or machine learning (ML) [4, 12, 109111] to ﬁnd and extract abnormal events. Regular expressions-based approaches often include formulating domain-speciﬁc regular expressions. These regular expressions are used to identify recently discovered vulnerabilities by comparing patterns. primary drawback of these techniques is that they can overlook the latest anomalous events. Also, when each log entry appears normal, but the entire sequence is abnormal, no abnormal activity can be detected using regular expressions-based techniques. Rule-based methods often use predeﬁned rules or signatures to detect abnormal patterns in log data. These principles are typically based on normal and abnormal system behavior observations. Data that conforms to these guidelines is classiﬁed as abnormal. The rule-based approaches can only recognize the predeﬁned abnormalities. The ML-based log analysis process for anomaly detection comprises three primary steps: log parsing, feature extraction, and anomaly identiﬁcation. Methods built on ML can not track the temporal information because they do not have mechanism to remember past events. Besides, these approaches heavily impact the model output because they need human feature extraction from raw logs. In fact, all of the approaches mentioned above need extensive domain expertise. CoLog is ﬂexible approach that does not rely on domain expertise or predeﬁned rules. CoLog exhibits notable aptitude for extracting temporal information. Outlier detection should be in real-time. Since deep neural networks are successful in real-time text analysis, numerous deep learning-based methods describe log data as natural language sequence and eﬀectively outlier ﬁnding [108]. This started trend of proposing advanced deep learning methods to identify log anomalies. These methods address the problems of approaches mentioned earlier. Subsequent subsections outline the literature on deep learning for automated logbased anomaly detection. We then review the literature on deep MSA and compare our log anomaly detection approach to deep learning methods. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-"
        },
        {
            "title": "2.1 Deep Learning for Anomaly Detection from Logs",
            "content": "The inception of deep learning-based methodologies can be traced back to the advent of DeepLog [36], which demonstrated commendable eﬃciency in detecting log data anomalies. Due to their rapid evolution over the past few years, these techniques now serve as the standard for modeling in this domain. Hence, the present subsection is most centered on models based on deep learning. In the following, we discuss unimodal and multimodal approaches."
        },
        {
            "title": "2.1.1 Unimodal Approaches",
            "content": "This subsection is dedicated to unimodal models for log anomaly detection despite the existence of various developed deep learning techniques for this purpose. These techniques primary concept is derived from natural language processing (NLP). Furthermore, they frequently prioritize the sequential nature of log data. Deep learning diﬀerent techniques, including MLPs [41, 43], CNNs [46, 47, 49, 50], RNNs [38, 52 54, 5658, 60, 63, 64], autoencoders [68, 7072], GANs [52, 7378], transformers [21, 7982], attention mechanisms [37, 41, 87, 89], GNNs [90], and LLMs [35, 92, 93], are frequently employed in these approaches. Except [68], the unimodal approaches mentioned earlier can be divided into three main categories: sequence-based, semanticbased, and LLM-based. Otomo et al. proposed log event anomaly detection method for large-scale networks that embeds diverse log data into hidden states using latent variables without requiring any preprocessing or feature extraction. The key idea is to ﬁrst translate raw log messages into log time series for each log type, then map the log time series into latent variables per day and per log type using conditional variational autoencoder. Finally, clustering method is applied to the latent variables to detect deviations from the detected clusters, interpreted as anomalies. Sequence-based unimodal approaches. Sequence-based modeling [21, 37, 41, 43, 46, 47, 49, 50, 54, 56, 57, 60, 63, 64, 7075, 7779, 81, 87, 89, 90] is widely recognized as the dominant network architecture in log-based anomaly detection. Typically, these methods interpret the log data as sequence of natural language. For example, Lu et al. extracted log keys from raw logs and assigned them unique numerical values. Shorter vectors are padded with zeros, while longer vectors are truncated to ensure that each session of log keys has the same length. The logs in vectorized form are inputted into an embedding layer, followed by three 1D convolutional layers with varying ﬁlter sizes. After that, max-pooling, dropout, and fully connected softmax layer are applied to classify anomalies. Zhang et al. demonstrated that log data is unstable due to the evolution of logging statements and noise in log data processing. They proposed LogRobust to tackle this issue by extracting semantic feature vectors from log event sequences. Then, an attention-based bidirectional long short-term Memory (BiLSTM) model is proposed to automatically identify abnormal instances and learn about the value of various log events. LogSpy [41] is method for detecting anomalies in distributed systems. It uses technique called frequent template tree (FT-Tree) to extract templates and the skip-gram model to construct feature vectors. It employs CNN architecture and an attention mechanism to analyze the relationships between log templates and improve the eﬃciency of real-time log anomaly detection. LogBERT [79] Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 uses the transformer encoder to model log sequences for self-supervised log anomaly detection. The training process involves two distinct tasks: (1) masked log key prediction to accurately identify randomly masked log keys in normal sequences and (2) volume of hypersphere minimization to bring normal log sequences close in the embedding space. LogBERT encodes normal log sequence patterns and detects anomalies after training. LogGAN [74] uses log-level GAN with permutation event modeling. log parser converts unstructured system logs into structured events. Permutation event modeling minimizes long short-term memory (LSTM)s sequential dependency concerns, allowing it to handle out-of-order log arrivals. The adversarial learning system uses generator to generate synthetic logs and discriminator to verify them. DeepCASE [87] can acquire knowledge regarding the correlation among log events in sequences. Subsequently, an interpreter arranges similar events into similar clusters to detect abnormalities. The system administrator receives reports of abnormality samples, and the administrators decisions are learned by DeepCASE and applied to future occurrences of identical sequences. Catillo et al. proposed AutoLog as semisupervised deep autoencoder. Its functionality is not contingent upon the concept of log line sequencing. The approach employed involves the extraction of numeric score vectors to handle heterogeneous logs. During the process, AutoLog does not incorporate any application-speciﬁc knowledge and refrains from making any presumptions regarding the format and order of the underlying lines within the logs. Qi et al. proposed log-based anomaly detection approach known as Adanomaly. This method employs the bidirectional GAN model for extracting features and technique for classiﬁcation based on ensemble learning to detect anomalies. Adanomaly calculates the reconstruction loss and discriminative loss as features with the help of bidirectional GAN. LogEncoder [43] is framework that uses labeled and unlabeled data to detect anomalies in system logs. The system comprises three primary elements: Log2Emb, Emb2Rep, and Anomaly Detection. The Log2Emb procedure transforms discrete log events into semantic vectors. The Emb2Rep procedure employs an attention-based LSTM model to diﬀerentiate between normal and abnormal sequences. The framework is adaptable for both oﬄine and online detection. Zhang et al. proposed LayerLog, an innovative system for detecting anomalies in log sequences. LayerLog utilizes hierarchical structure called Word-Log-Log Sequence to do this. The system comprises three layers: Word, Log, and LogSeq. It extracts semantic information from raw logs without any loss. Empirical assessments demonstrate the eﬃcacy of LayerLog in detecting abnormalities. LogFormer [37] is pipeline consisting of two stages: pre-training and tuning. It is designed to detect log anomalies and enhance generalization across diﬀerent domains. It extracts semantic information of log sequences and trains transformer-based model on speciﬁc domain. By employing adapter-based tuning, the training costs are minimized by keeping most parameters ﬁxed and only updating small number of them. FastLogAD [77] is an unsupervised technique for detecting anomalies in log data. It utilizes mask-guided anomaly generator (MGAG) and discriminative abnormality separation (DAS) model. The MGAG algorithm produces simulated anomalies based on normal log data, while the DAS model distinguishes between normal log sequences and simulated ones, allowing for eﬀective real-time detection. MLAD [81] is log anomaly detection approach that utilizes the Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 transformer and gaussian mixture model (GMM) to detect anomalies across many systems. MLAD employs SBERT [112] to transform log sequences into semantic vectors. It utilizes sparse self-attention mechanism to capture word-level dependencies and GMM to emphasize the uncertainty associated with infrequent words in the identical shortcut problem. Zhang et al. proposed E-Log, which provides ﬁne-grained elastic anomaly detection and diagnosis for databases. Qiu et al. introduced LogAnomEX, an unsupervised log anomaly detection method based on the Electra model and gated bilinear neural networks. Qiu et al. proposed FedAware, distributed IoT intrusion detection method that leverages fractal shrinking autoencoders. These methods highlight both supervised and unsupervised perspectives, reinforcing the importance of comparative baselines. Semantic-based unimodal approaches. Semantic-based unimodal log anomaly detection is limitedly used [38, 52, 53, 58, 76, 80, 82]. Several semantic-based approaches utilize log lines to extract semantic information and build models using unsupervised or semi-supervised approaches [76, 80, 82]. Other techniques incorporate sentiment analysis (SA) to detect log anomalies [38, 52, 53, 58]. These methods utilize the polarity of log messages as basis for learning and predicting normal and abnormal log events. SA-based log anomaly detection methods classify normal occurrences as positive sentiments and anomalies as negative. For example, Farzad and Gulliver proposed an approach for detecting and classifying anomalies in log messages by employing three diﬀerent deep learning architectures: Auto-LSTM, AutoBiLSTM, and Auto-gated recurrent unit (GRU). The methodology consists of two primary phases: feature extraction with autoencoders separately for positive and negative classes and concatenating outputs to provide single-label dataset for training an LSTM, BiLSTM, and GRU model. The classiﬁcation model is trained using the softmax activation function, categorical cross-entropy loss, and the Adam optimizer [116]. Farzad also proposed log message anomaly detection method using combination of GAN, autoencoder, and GRU networks. It generates synthetic negative log messages using proposed SeqGAN to oversample negative log messages and balance the dataset. The autoencoder extracts features from positive and oversampled negative log messages, with separate networks for each class. GRU network is used for anomaly detection and classiﬁcation, with improved accuracy compared to models without over-sampling. Logsy [80] is classiﬁcation-based method for detecting anomalies in unstructured log data. It uses self-attentive encoder and hyperspherical loss function to learn compact log vector representations. It uses auxiliary log datasets from other systems to improve normal log representation and prevent overﬁtting. The learned representations help distinguish normal and abnormal logs using the anomaly score as distance from the center of hypersphere. Studiawan et al. proposed pylogsemtiment, SA-based method for identifying anomalies in production OS log ﬁles. This approach uses positive and negative sentiment analysis, with negative sentiment detection similar to detecting anomalies. They used deep learning to predict unseen data and implemented GRU network to determine sentiments. They considered class imbalance due to the lower frequency of negative messages in real-world logs. They proposed using the Tomek link technique to balance the two sentiment types. SentiLog is an other approach proposed by Zhang et al. to analyze parallel ﬁle system Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 logs and detect anomalies. SentiLog uses collection of parallel ﬁle systems loggingrelated source code to train generic sentimental natural language model. In this manner, SentiLog learns the implicit semantic information based on LSTM developers have embedded within the parallel ﬁle system. Like pylogsentiment, SentiLog shows that SA is promising approach for modeling unstructured and diﬃcult-to-decode system logs. A2Log [82] is two-step, unsupervised anomaly identiﬁcation method for log data that includes anomaly scoring and decision-making. Initially, self-attention neural network is employed to assign score to each log message. Furthermore, it sets the decision boundary by augmenting the existing normal training data without using any anomalous cases. LogELECTRA [76] is self-supervised anomaly detection model designed for unstructured system logs. It examines individual log messages without the need for log parser. The system acquires an understanding of the context of normal log messages by training discriminator model to identify replaced tokens in changed sequences. During the inference process, LogELECTRA computes an anomaly score for every log message, identifying anomalies as individual anomalies in real-time without needing time-series analysis. LLM-based unimodal approaches. After demonstrating LLMs unique and emerging abilities in performing various tasks [117], recent studies have proposed using LLMs to detect anomalies from log data. For example, LogFiT [35] is an innovative approach for detecting anomalies in system logs. However, it does not rely on predetermined templates or labeled data. Instead, it utilizes language model based on bidirectional encoder representations from transformers (BERT) to analyze the logs. The system acquires knowledge of language patterns by making predictions on masked sentences using normal log data. LogFiT evaluates the accuracy of predicting the top-k tokens during the inference process to determine threshold for detecting abnormalities. LogFiT is speciﬁcally engineered for easy integration into preexisting NLP frameworks. LogPrompt [93] leverages LLMs to automate log analysis in online scenarios. The system tackles the challenges associated with the restricted ability to understand log anomaly detection results and the ability to handle unseen logs. LogPrompt employs three advanced prompt strategies: self-prompt, chain-ofthought prompt, and in-context prompt. LogPrompt allows for the eﬃcient analysis of log ﬁles and identifying abnormal patterns without requiring large amount of training data. Human assessments demonstrate that LogPrompts interpretations are valuable and easy to understand, assisting professionals in understanding log data. Pan et al. proposed RAGLog to detect anomalies in log data. RAGLog leverages the retrieval augmented generative (RAG) model with vector database and an LLM. It avoids the necessity of interpreting logs by directly consuming unprocessed log data. The LLM conducts semantic analysis to compare normal log entries with the query entry, enabling RAGLog to function as zero-shot classiﬁer that does not necessitate abnormal log samples for training."
        },
        {
            "title": "2.1.2 Multimodal Approaches",
            "content": "This section speciﬁcally examines models that utilize multimodal analysis to ﬁnd anomalies in log data. These approaches are relatively infrequent in comparison to unimodal methods in log analysis. Deep learning employs various techniques, such Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 as MLPs [42, 44, 45], CNNs [48, 51], RNNs [36, 55, 59, 61, 62, 6567], autoencoders [42, 69], GANs [42], transformers [8385], attention mechanisms [48, 86, 88], and EGNNs [91], which are commonly used in these approaches. The three main categories of multimodal techniques mentioned above are early [51, 59, 61, 62, 83, 85, 86, 88, 91], intermediate [42, 45, 48, 55, 6567], and late [44] fusion-based methods. The exceptions to this rule are [36, 69, 84]. DeepLog [36] is an early example of using deep learning for log anomaly detection. It works by analyzing the sequential relationships between log entries, treating them as sequence of natural language. Next, learn about the normal log patterns, consisting of log key and parameter value vector representing each log entry. DeepLog utilizes two LSTM models to learn about log key sequences and parameter value vectors. In addition, LSTMs are used to forecast future log events. These predictions are then compared to the ground truth to identify any anomalies. Qian et al. proposed VeLog, method for detecting anomalies in distributed systems using variational autoencoders (VAEs). The process consists of two distinct stages: oﬄine training and online detection. During the oﬄine phase, VeLog gathers log data, analyzes it, and produces the log order matrices and the log count vector matrices representing log executions. The VAE model acquires knowledge about normal sequence patterns based on these matrices. VeLog analyzes and extracts features from new logs during the online phase, generating comparable matrices. Anomalies are identiﬁed by comparing the resulting matrices with previously learned patterns. This approach is highly eﬀective when applied to large-scale distributed systems. He et al. proposed UMFLog, novel unsupervised model for detecting anomalies in log data. UMFLog employs two sub-models to capture the logs semantic and statistical features, eﬀectively. The ﬁrst model uses BERT to extract semantic features from log content. In contrast, the second model employs VAE to learn statistical features, speciﬁcally focusing on word frequency in logs. This dual-feature approach improves the models capacity to identify anomalies without needing labeled data, making it well-suited for practical applications. Early fusion-based multimodal approaches. Typically, these approaches include extracting meaningful feature vectors from various aspects of log data and constructing supervector. Finally, they utilize this supervector as the source of knowledge to train their model. For example, Yu et al. add an attention layer following the BiLSTM to assess the value of prior events in predicting subsequent events. They merge log datas sequential and quantitative features with semantic features to achieve eﬀective results. After the sequence, quantitative, and semantic feature vectors have been extracted, they are combined and fed into the model to understand the relationships between the log events. This framework can extract the underlying dependencies from event logs. LogST [51] is log anomaly detection method that combines semantic and topic features extracted from logs. The process involves utilizing BERT and singular value decomposition (SVD) to extract semantic features. Furthermore, latent dirichlet allocation (LDA) extracts topic features. The combined features are subsequently fed into the improved TCN model, which utilizes weighted residual connections for anomaly detection. Han et al. proposed Log-MatchNet, method for detecting anomalies in log data designed to handle unstructured and imbalanced log data. It employs log parsing to transform log content information into vectors, utilizing Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 the BERT model for universal feature extraction. The approach involves matching network to learn similarity scores between input and prototype vectors. The prototype vectors represent small number of labeled abnormal logs. These vectors are utilized to make generalizations about unknown log samples in few-shot scenario. Yu et al. proposed LogMS, log anomaly detection technique that leverages many sources of information and employs probability label estimation. The system utilizes an multisource information fusion-based-LSTM network to process semantic, sequential, and quantitative information and probability label estimation-based gate recurrent unit network to handle pseudolabeled data. The second stage turns on when the abnormalities are not identiﬁed in the ﬁrst stage to optimize eﬃciency. LogMS eﬃciently identiﬁes anomalies, even when the log data changes over time. Zhang et al. proposed MultiLog, multivariate log-based anomaly detection approach for distributed databases. MultiLog initially processes sequential, quantitative, and semantic information from the logs of each database node. This is done using an LSTM model paired with self-attention in the standalone estimation module. The system utilizes an autoencoder with meta-classiﬁer to normalize node probabilities and detect abnormalities throughout the entire cluster. Intermediate fusion-based multimodal approaches. These approaches extract features and process each modality independently until certain point, at which point the feature maps are concatenated before categorization or decision-making. For example, Li et al. found these challenges unmet by prior methods: (1) actively developing and maintaining software systems change log formats, (2) the trivial monitoring tools may miss the underlying reasons for performance issues. Thus, SwissLog, robust deep learning-based model for log analysis, is proposed. SwissLog utilizes BERT to embed the semantic information onto the embedding vector and puts the temporal information into another embedding vector. SwissLog feeds to an attention-based BiLSTM model, an intermediate fusion of semantic and time embedding vectors to learn the distinctions between normal and abnormal log sequences. Zhang et al. proposed LogAttn, method using an autoencoder for unsupervised log anomaly detection. LogAttn begins by parsing log data into structured representations. It generates event count and semantic vector sequences. TCN and deep neural network encoders learn temporal and statistical correlations in log data separately. The decoder reconstructs the log sequence using latent representation formed by an attention mechanism that weighs feature importance. Comparing the reconstruction error against predetermined threshold using model trained entirely on normal log data provides anomaly detection. Niu et al. proposed FSMFLog, an approach for detecting log anomalies by employing multi-feature fusion. FSMFLog overcomes the constraints of previous log parsing techniques by utilizing preﬁx tree structure to extract semantic data in word lists rather than relying on typical log templates that frequently fail to capture critical semantics. It commences with preprocessing by removing variables from logs and subsequently grouping log sentences utilizing heuristic strategies. FSMFLog utilizes bidirectional GRU model that is improved using an attention mechanism. This model utilizes semantic, time, and type features. Yang et al. proposed log anomaly detection method integrating self-attention mechanism with bidirectional GRU model. It utilizes two bidirectional GRU models: one is responsible for processing the log template Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 sequence, while the other one is dedicated to handling the log template frequency vector. The results from both models are concatenated and fed into self-attention layer, which fuses the features to predict the probability distribution of the subsequent log template. This method detects anomalies when the expected template is missing from the candidate set. Niu et al. proposed robust log-based anomaly detection framework named WDLog, which integrates wide and deep learning to address challenges in detecting anomalies in evolving log data. The method begins with an optimized log template extraction process that enhances the traditional Drain algorithm by incorporating semantic embedding and clustering, eﬀectively reducing sensitivity to change in log wording. Following this, WDLog extracts three types of features from the generated log templates: temporal features, invariant features, and statistical features. Anomaly detection is then performed using combination of GRU model with attention mechanisms for temporal features and gradient boosting decision tree model for invariant and statistical features. Late fusion-based multimodal approaches. Under these methods, the fusion occurs after each modality has been processed and classiﬁed, independently. The results from each modality are then combined to make the ﬁnal decision. For example, Zhao et al. proposed an approach for detecting web scanning behaviors in online web logs, which are crucial for identifying potential cyber-attacks. It utilizes three lightweight classiﬁers that analyze distinct features extracted from web traﬃc logs such as HTTP textual content, status codes, and request frequency. This approach categorizes logs on the source and destination IPs and evaluates the characteristics of these logs to diﬀerentiate between scanning and normal behaviors. Each classiﬁer processes speciﬁc features. Therefore, textual features are analyzed using an MLP, while HTTP status codes and request frequencies are assessed through support vector machines (SVMs). The classiﬁers generate probabilities indicating scanning behavior, which are combined using decision strategy to determine the ﬁnal classiﬁcation outcome."
        },
        {
            "title": "2.2 Deep Learning for Multimodal Sentiment Analysis",
            "content": "Traditional sentiment analysis procedures often focus on extracting user sentiment from single modality. MSA incorporates visual and audio modalities into text-based SA. Audio and visual characteristics are employed as they provide superior ability to better explain or depict sentiment compared to lengthy list of words. Many studies [94103] have been undertaken in this ﬁeld, most depending on deep learning concepts. The increasing prevalence of deep learning can be attributed to its ability to boost complex processing, thereby improving the automation process. From multimodal learning standpoint, comprehensive surveys [118, 119] have outlined foundational principles, challenges, and open questions in multimodal machine learning. These works emphasize systematic integration, balancing, and interaction of modalities - all central issues addressed in CoLog. Moreover, Qiu et al. proposed chained interactive attention mechanism for multimodal sentiment analysis, further validating that multimodal fusion strategies developed in the sentiment domain can inspire advances in anomaly detection. Delbrouck et al. proposed transformer encoder architecture that fuses any Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 information about modalities. The model at hand utilizes joint-encoding to concurrently encode each modality, with modular co-attention controlling how modality attends to itself. Hu et al. proposed framework called UniMSE. The UniMSE framework is novel method for uniﬁed multimodal sentiment analysis and emotion recognition in conversations. It reformulates tasks into generative format, integrates acoustic, visual, and textual features, and employs inter-modal contrastive learning to minimize intra-class variance and maximize inter-class variance. Li et al. proposed multimodal sentiment analysis model using transformer architecture and soft mapping. The transformer layer utilizes the attention layer to map modalities, while the soft mapping layer uses stacking modules for multimodal information fusion. This model addresses data interaction issues in multimodal sentiment analysis by considering the relationships between multiple modalities. The TETFN [97] is method for multimodal sentiment analysis that enhances video sentiment recognition by integrating textual, visual, and audio modalities. It uses preprocessing, feature extraction, LSTM networks, and TCNs to encode contextual information and generate unimodal labels. The core of TETFN is text-enhanced transformer module that leverages textoriented multi-head attention (MHA) mechanism to incorporate textual information into the visual and audio modalities, facilitating eﬀective pairwise cross-modal mappings. Ahuja et al. proposed novel multimodal sentiment analysis method for images containing textual information. Initially, it employed recognition system to extract text from images using the Google Cloud Vision API, which is noted for its high accuracy in optical character recognition (OCR). The extracted text is preprocessed and analyzed with RoBERTa-based model for textual sentiment analysis. Concurrently, visual sentiment analysis uses transfer learning approach to capture visual features. The method integrates the outputs from both analyses through weighted fusion strategy, enhancing the overall sentiment classiﬁcation accuracy. Liu et al. proposed robust multimodal sentiment analysis model that uses modality translation-based approach to handle uncertain missing modalities. It involves translating visual and audio data to text modality, encoding text, and fused into missing joint features (MJFs). The transformer encoder module then encodes these MJFs to learning longterm dependencies between modalities. Therefore, the sentiment classiﬁcation is based on the transformer encoder modules outputs."
        },
        {
            "title": "2.3 Beyond Log-Speciﬁc Approaches",
            "content": "Beyond log-speciﬁc approaches, multimodal anomaly detection has been explored in other domains such as video understanding. For instance, Su et al. proposed semantic-driven dual consistency learning for video anomaly detection that leverages semantic-driven representations to align appearance and motion features, enhancing anomaly localization under weak supervision. Similarly, Su et al. proposed federated weakly-supervised video anomaly detection with mixture of local-to-global experts that employs mixtures of experts within federated learning framework to address distributed and heterogeneous data scenarios. While these methods target video modalities, they share conceptual parallels with CoLog in terms of semantic-guided fusion and cross-modal consistency learning. Inspired by such principles, CoLogs collaborative transformer integrates semantic and Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 sequential log modalities through impressed attention and the Modality Adaptation Layer (MAL), enabling uniﬁed detection of both point and collective anomalies."
        },
        {
            "title": "2.4 Our Method: A Cut Above the Rest",
            "content": "According to the literature cited in Section 2.1, diverse deep learning techniques are progressively being employed to detect anomalies in log records, which we classiﬁed into two general categories and six subcategories. Figure 2 demonstrates concise overview of log-based anomaly detection. CoLog falls under the umbrella of multimodal-based approaches. Log-based anomaly detection"
        },
        {
            "title": "Multimodal",
            "content": "Sequence-based Semantic-based LLM-based Early fusion-based Intermediate fusion-based Late fusion-based"
        },
        {
            "title": "CoLog",
            "content": "Fig. 2 comprehensive overview of the existing literature dealing with the approaches for identifying anomalies in log data. However, our method diﬀers as we aim to use the redundancy of modalities within system logs based on MSA and CT. CT can be utilized to supplement diverse modalities. When data are deﬁcient from particular modality, data from another modality is employed. Therefore, these transformers enable the extraction of various information from modalities and interactively encode modalities. The strong performance and generalization of sentiment-based log analysis models, as claimed by [38, 52, 53, 58], makes well-trained models more robust to the evolution of log statements. We oﬀer the CoLog framework that developed based on the SA of logs. But, unlike [38, 52, 53, 58], our model employs more complex architecture for MSA-based anomaly detection. By evaluating the sentiment of log events in dependence on their background, CoLog can analyze both point and collective anomalies. Moreover, CoLog can distinguish similar events by utilizing auxiliary information from the background of their occurrence, as depicted in Figure 1. In the following sections, we will demonstrate that the context of event logs can also be utilized. In contrast to reviewed methods, CoLog relies on modality interaction within the logs. It does not concat diﬀerent modalities into super vector in any way. The challenge at hand is tackled through impressed attention based on CT. This form of attention is beneﬁcial to discovering the correlation between two modalities. Since deep learning models are commonly denoted as black-box models due to their complex nature, which surpasses human comprehension, comprehending the reasoning behind each decision is task that exceeds human cognitive capacity. The utilization of the impressed attention mechanism has the potential to augment the interpretability of our log anomaly detection model. This is because the attention scores, which are computed Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693based on the impressed attention, can oﬀer valuable insights into the models crossmodal decision-making process. CoLog projects the results of modalities into latent space above the transformer blocks for intermediate fusion and executes the encoding process, concurrently. Therefore, forming an optimal latent space is crucial due to the inherent incompatibility of various modalities. Furthermore, the CoLog architecture enables the identiﬁcation of point and collective log abnormalities using uniﬁed framework. According to our current knowledge, CoLog is the ﬁrst approach that performs this task."
        },
        {
            "title": "3 Preliminaries",
            "content": "This section discusses the tasks deﬁnition and the rationale for employing the transformer model to accomplish it. It also covers the assumptions and threat model used to evaluate potential threats."
        },
        {
            "title": "3.1 Task Deﬁnition",
            "content": "This task aims to conduct log analysis to detect anomalies during the systems execution. The scope of the analysis will include analyzing logs information to detect abnormalities. machine learning algorithm will be used to analyze these data. The performance of the anomaly detection method will be evaluated by measuring some metrics, such as accuracy, precision, recall, and F1-score, to ensure that the method does not generate too many false alerts. Event1. - 1117955319 2005.06.05 R24-M0-N5-C:J04-U01 2005-06-05-00.08.39.792163 R24-M0-N5-C:J04-U01 RAS KERNEL INFO generating core.3427 [0.7921, , 0.5237] Event2. - 1117955319 2005.06.05 R24-M0-N5-C:J06-U01 2005-06-05-00.08.39.813691 R24-M0-N5-C:J06-U01 RAS KERNEL INFO generating core.3554 Event3. - 1117955319 2005.06.05 R24-M0-N5-C:J04-U11 2005-06-05-00.08.39.834967 R24-M0-N5-C:J04-U11 RAS KERNEL INFO generating core.3435 Event4. - 1117955319 2005.06.05 R24-M0-N5-C:J02-U01 2005-06-05-00.08.39.855802 R24-M0-N5-C:J02-U01 RAS KERNEL INFO generating core. Event5. - 1117955319 2005.06.05 R24-M0-N5-C:J02-U11 2005-06-05-00.08.39.876530 R24-M0-N5-C:J02-U11 RAS KERNEL INFO generating core.3563 [0.0312, , 0.1635] [0.4297, , 0.9494] [0.0036, , 0.5555] [0.1694, , 0.2351] Event6. - 1117955319 2005.06.05 R24-M0-N7-C:J09-U11 2005-06-05-00.08.39.902495 R24-M0-N7-C:J09-U11 RAS KERNEL INFO generating core. [0.3016, , 0.0002] Event7. KERNDTLB 1117955319 2005.06.05 R24-M0-N7-C:J15-U11 2005-06-05-00.08.39.924561 R24-M0-N7-C:J15-U11 RAS KERNEL FATAL data TLB error interrupt [0.6597, , 0.3123] Event8. KERNDTLB 1117955319 2005.06.05 R24-M0-N7-C:J11-U11 2005-06-05-00.08.39.946196 R24-M0-N7-C:J11-U11 RAS KERNEL FATAL data TLB error interrupt [0.1194, ..., 0.2468] [[ [ [ ], ], ]] ], ], ], ], ]] [[ [ [ [ [ 5 [[ [ [ [ ], ], ], ]] 4 ], ], ], ], ], ]] [[ [ [ [ [ [ 6 (a) Several lines are taken from the BLueGene/L log dataset ( ) (b) Semantic vectors (c) Sequence Vectors ( ) ( ) Green: Positive sentiments Red: Negative sentiments (anomalous events) Log messages Fig. 3 (a) Several lines extracted from the BLueGene/L log dataset.; (b) Semantic modality: The semantic modality is constructed using the extracted semantic vectors from log events messages.; (c) Sequence modality: The construction of the sequence modality involves appending semantic vectors into sequence vectors based on window sizes of 3, 4, 5, and 6. When conducting multimodal sentiment analysis, researchers look at the dialogue from various perspectives (modalities) to determine the participants sentimental states. The three most commonly utilized modalities are textual, audio, and visual. In the same vein, logs possess nature that, akin to human conversations, conveys information from diﬀerent aspects, such as statistical, temporal, semantic, and information based on the events sequence. However, the semantic and sequence modalities in logs are the most crucial ones to consider. Besides, to detect anomalies from logs, we Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 can implement sentimental pipelines to classify sentiments wherein recognizing negative sentiments alerts anomalous situations. In Figure 3(a) , log messages negative and positive sentimental states are presented in red and green, respectively. Based on these observations, we propose to apply multimodal sentiment analysis to the log anomaly detection task. Figure 3(b) and Figure 3(c) respectively depict the semantic and sequence modalities. These ﬁgures show that semantic vectors of log events are derived based on log messages. These vectors generate sequence vectors based on the window size. In this case, the window size determines the length of the sequence vectors we create. These vectors would serve as input for transformer-based multimodal framework that learns cross-modal dependencies and predicts point and collective log anomalies."
        },
        {
            "title": "3.2 Transformers: why?",
            "content": "Transformers exhibit notable advantage in their processing speed when dealing with sequence. They focus more on the crucial components, resulting in enhanced speed compared to alternative models. Also, transformers exhibit multiple beneﬁts in the context of multimodal sentiment analysis. Transformers can enhance models robustness in multimodal sentiment analysis, thereby constituting an additional beneﬁt. Another advantage is their ability to incorporate impressed attention into their architecture to extract cross-modal interactions within non-aligned multimodal data. The signiﬁcance of this matter lies in the complexity and multimodality of most real-world information. Considering transformers beneﬁts, we decided to base our multimodal model around them."
        },
        {
            "title": "3.3 Assumptions",
            "content": "Anomaly detection from the log strongly depends on the quality of the log. Multiple categories of assaults exist that may not be recorded in system logs. Suppose certain auditable occurrences, such as login attempts, are not duly recorded. In that case, it is plausible that CoLog may not locate such forms of abnormalities. Another type of attack that may not be recorded in logs is log forging or log injection attack. The aforementioned is man-in-the-middle assault in which an unauthenticated user interposes between the application and the server. Therefore, it is assumed that any attempt by an adversary to modify the system logging behavior is not feasible. In manner analogous to that of pylogsentiment, we consider log entries like [c010ce54>] mtrr wrmsr+0xf/0x2e in kernel log as having positive sentiment during training if they do not provide human-readable log message. It is essential to clarify that our analogy between anomalies and negative sentiments does not imply direct mapping to human emotions. Instead, it serves as an analytical abstraction: anomalies are deviations from expected behavior (negative polarity), while normal and benign events align with expected states (positive polarity). Unlike human sentiment, which emerges from subjective perception, log polarity is determined contextually by semantic embeddings and sequence patterns. For instance, restart event can be benign in maintenance scenarios but anomalous when it occurs unexpectedly. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-"
        },
        {
            "title": "3.4 Threat Model",
            "content": "An anomaly is data instance that diverges from the normal pattern. Various abnormalities exist, including point, collective, and contextual anomalies. point anomaly refers to singular data point that exhibits signiﬁcant deviation from the overall pattern of the dataset. group of data instances can form an anomalous pattern known as collective anomaly. contextual anomaly is when given data instance exhibits abnormal behavior only in particular context while classifying normal in others. point or collective anomaly can be classiﬁed as contextual when examined in speciﬁc context. CoLog identiﬁes point and collective anomalies through uniﬁed framework based on CT. This involves evaluating each log entry for its sentiment based on the background or context in which it occurred and ﬂagging any entry that exhibits negative sentiment as an anomaly."
        },
        {
            "title": "4 Method",
            "content": "This section explains architecture and data ﬂow of CoLog, the speciﬁc procedures CoLog uses to gather and examine log data to look for anomalies. It includes the conditions under which data were collected, labeled and preprocessed, guidelines that specify how anomaly detection task should be carried out, and DL methodologies and techniques utilized for automatic log-based anomaly detection based on MSA."
        },
        {
            "title": "4.1 Overall Architecture",
            "content": "Figure 4 depicts CoLogs training framework and gives an overview of its architecture. Like most log anomaly detection techniques, CoLog begins by preprocessing raw logs. First, it extracts semantic and sequence modalities from the raw-valued logs. Before feeding the embedded vectors of modalities into the model, the Tomek link approach addresses the class imbalance issue. Two transformer blocks called collaborative transformers with modiﬁed GA mechanism called impressed attention are applied to these modalities. CoLog comprises stack of identical CT blocks, but each one has diﬀerent set of training parameters. Each modality encoder comprises multi-head impressed attention (MHIA), MLP, modality adaptation layer (MAL), and Layer Normalizations (LNs). Modalities fed into the matching modality encoder in CT. The MLP layer, which employs nonlinear activation function, receives the results of computing the attention scores from the attention layer. Each encoder has residual connection and LN after the MHIA, MLP, and MAL layers. The results of the encoding blocks are fed into the balancing layer (BL) to determine the sentiment of log message, where soft attention and fusion are performed. Remember that the negative sentiment is connected to any potential abnormal behaviors. Due to the information sharing between diﬀerent modalities in CT, the suggested architecture thus conveniently learns the shared representation between modalities. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693Preprocessing"
        },
        {
            "title": "Collaborative Transformer",
            "content": "Excel D LN LN Log Parsing Unstructured Logs 1 2 3 4 5 6 Structured Logs Keys Log Messages Log Sequences Embeddings Labels Tomek link MAL MAL LN LN MLP MLP LN LN Balancing Layer Input Vector High-Dimensional Space Balancer's weight Soft-Attention Balanced Output Vector High-Dimensional Latent Space before Fusion Train Set Valid Set Test Set MHIA MHIA Dataloader Collaboration Tunnel LN Linear LSTM MLP Fig. 4 The overview of CoLog. Light green and gold colors demonstrate modality encoders. Each encoder in the collaborative transformer consists of MHIA, MLP, MAL, and LNs. MHIA and MAL are multi-head impressed attention and modality adaptation layer modules, respectively. The preprocess layer transforms unstructured logs into easily understandable data for the model. The purpose of the balancing layer is to regulate the inﬂuences of diﬀerent modalities when calculating the ﬁnal results."
        },
        {
            "title": "4.2 Preprocessing",
            "content": "In the preprocessing phase, the log parsing procedure extracts speciﬁc element, denoted as log message, from log ﬁle. Then, every log message is segmented into tokens and transformed into lowercase. The process involves constructing lexicon based on the training dataset, resulting in dictionary comprising Vsize distinct terms. After that, each word is embedded in 300-dimensional vector. In cases where term from the validation or testing dataset is not present in our extracted vocabulary, we substitute it with the K token. In addition, irregularities exist in the length of individual log messages. Consequently, if the log message length falls below the speciﬁed threshold, it is padded with zeroes. Conversely, in cases where the length exceeds the established threshold, we truncate to abbreviate it to the predeﬁned message length."
        },
        {
            "title": "4.2.1 Log Parsing",
            "content": "Each log entry contains various separate parts. Log parsing is the act of transforming unstructured log data into structured format to enable machine interpretation. Every log ﬁle is parsed to convert it from an unstructured form to structured format with entities like timestamp, service name, level information, and log message. We must separate the log message from the log entry because it is the only entity with the sentiment. In addition, each log message is transformed into 384-dimensional vectors Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 using SBERT and stored separately from the log messages. We employ the nerlogparser [123] and Drain [124] log parsers in our implementation. Indeed, each dataset is processed and parsed using its corresponding parser."
        },
        {
            "title": "4.2.2 Log Parser: Drain",
            "content": "One of the representative algorithms for log parsing is the Drain. Drain employs ﬁxed-depth parse tree, which encodes speciﬁcally created parsing rules, to expedite the parsing procedure. Drain preprocesses logs ﬁrst using regular expressions and userdeﬁned domain knowledge. Second, Drain begins with the preprocessed log message at the root node of the parse tree. The parse trees ﬁrst layer nodes represent log groups whose log messages are of various lengths. Drain uses concept of distance called token similarity to determine how similar the log messages are to one another. The tokens in the ﬁrst positions of the log message are used to choose the subsequent internal node. Drain then determines whether to add the log message to an existing log group by calculating the similarity between each log groups log message and log event. Drain is quite eﬀective and accurate. It can also parse log data in real-time, which makes it well-suited for log anomaly detection. Additionally, Drain is scalable, making it suitable for large-scale environments."
        },
        {
            "title": "4.2.3 Log Parser: nerlogparser",
            "content": "The nerlogparser utilizes named entity recognition (NER), technique employed to extract named entities from text. Nerlogparser identiﬁes named entities in log ﬁles as words or phrases that include common ﬁelds seen in log entry, such as timestamp, hostname, or service name. Named entity extraction is the process of recognizing each element in log entry. To perform NER, nerlogparser employs BiLSTM. The primary advantage of the nerlogparser is its ability to automatically parse log data using pretrained model. Thus, there is no necessity to establish any rules or regular expressions. The nerlogparser can parse wide range of log ﬁles due to its training on multiple log types."
        },
        {
            "title": "4.3 Task Formulation",
            "content": "{ , { sem, seq We employ the notation Lm sem, seq to represent the unimodal raw-valued } Liw, Li(w1), ..., Li1, Li , where log modalities extracted from the log messages is the window size and the notation speciﬁes the two diﬀerent types of modalities: semantic and sequence. The parsed raw log messages, which include . . . , LL} sentimental words, are deﬁned in mathematical terms as = , where the raw log message Li extracted from the log entry i, denotes the total number of log messages. The architecture of model blocks, model parameters, and label space are uniﬁed through task formulation, wherein two transformer blocks receive semantic and sequence modalities. These transformers learn representations of log data. Finally, our model attempts to predict the integer value yi needed to classify log entry i. L1, L2, { 0, { { } } } Task formulation involves two subsections. formulating input features describes the process of transforming raw log modalities into input feature vectors for the Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 model. However, label formulation applies MSA for log anomaly detection tasks by transforming log message labels into space representing sentimental states."
        },
        {
            "title": "4.3.1 Formulating Input Features",
            "content": "The comprehension of events sentiments mainly relies on the semantic information in log messages. Based on this observation, constructing semantic modality involves preprocessing and segmenting each raw log message into tokens from t1 to tn as raw words. Subsequently, each token is added to list that retains preprocessed log messages represented as segmented text: where,"
        },
        {
            "title": "Lsem",
            "content": "i = [t1, t2, . . . , tn]. = number of tokens (1) (2) create embedding vectors of tokens in Lsem After this, word embedding vectors are acquired by utilizing word2vec [125] to . Furthermore, the raw sequence features of every log message are transformed into numerical sequential vectors. Log sequence vectors can be produced in two procedures: background and context. As illustrated in Figure 5, generating background and subsequent event lists for every log entry is possible. background sequence vector can be generated by concatenating background event vectors. The resulting vector is (W k)-dimensional, where represents the window size. Based on the above mentioned concepts, Lseq can be deﬁned as follows: where,"
        },
        {
            "title": "Lseq",
            "content": "i = [[Lj]]. { W, (W 1), . . . , 1 } (3) (4) Similarly, the context sequence vector, constructed by concatenating the background and subsequent event vectors, has dimension size of 2W k. where,"
        },
        {
            "title": "Lseq",
            "content": "i = [[Lj]]. { W, (W 1), . . . , 1, + 1, . . . , + (W 1), + } (5) (6) The decision-making process regarding whether to use Equation 3 or Equation 5 is based on the type of sequence modality that goes through background or context extraction operations. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 = [ . = . = [ . = [ . = [ . = [ . , , , , , , , , , , , , . . . . . . ] ] ] ] ] Background Events Log Event Subsequent Events Fig. 5 Illustration of diﬀerences between background and subsequent event vectors. Background and context sequence vectors are constructed based on background and subsequent event vectors. In mathematical terms, Bi = [V sem i+2 , sem i1 ] and Ci = [V sem i+3 ], where sem is the semantic vector of the log message extracted by SBERT, Bi is the background sequence vector of log message i, and Ci is the context sequence vector of log message i. i2 , sem i1 , sem i2 , sem i+1 , sem i3 , sem i3 , sem i"
        },
        {
            "title": "4.3.2 Label Formulation",
            "content": "The objective of CoLog is to utilize MSA to detect log anomalies and predict the sentiment reﬂected by log message i. In SA, the numerical value 1 is commonly used to denote positive sentiments, while 0 is typically employed to indicate negative sentiments. The task of identifying negative sentiments can be considered equivalent to the detection of log anomalies. Consequently, every instance within the dataset is assigned label of either 1 or 0, corresponding to its classiﬁcation as normal or abnormal sample. In this manner the universal label yi = comprises the polarity of log message i, where yp yp } 0, 1 { . Unlabeled datasets are labeled using words that reﬂect negative sentiment in the same dataset. The list of negative sentimental words for the unlabeled datasets is based on the conditions proposed in pylogsentiment [38]. { }"
        },
        {
            "title": "4.3.3 Leveling the Playing Field: Class Imbalance",
            "content": "The issue of class imbalance is prevalent concern in machine learning, characterized by an unequal distribution of classes within the training data. The potential outcome of this scenario is the development of models that exhibit bias toward the majority class, resulting in suboptimal performance on the minority class. Various techniques have been devised to address the issue of class imbalance, including data-level approaches, e.g., over-sampling methods [126, 127] and under-sampling techniques [128, 129], and algorithm-level approaches, e.g., ensemble methods [130, 131]. The techniques mentioned above are designed to enhance the eﬃciency of machine learning models when dealing with imbalanced datasets. This is achieved by either ﬁxing the data imbalance or adapting the learning algorithm to consider the distribution of classes. For class balance, we employ the under-sampling technique called the Tomek link [129]. The Tomek link is utilized for under-sampling to generate novel distribution of the majority class. The log data frequently contains repetitive majority class, which can be regarded as noise. Consequently, we opted for this approach, which is commonly employed to eliminate noisy and borderline majority class instances. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 Tomek link is established between two data items belonging to diﬀerent classes that are the nearest neighbors to each other. Assuming the entire dataset is denoted as = , the balanced dataset Lbalanced can be deﬁned mathematically { as follows: L1, L2, . . . , LL} Lbalanced = tomeklink(L). (7) where tomeklink ( ) is function aimed at removing Tomek links (Li, Lj) from dataset L. (Li, Lj) is Tomek link provided that there is no Lk whose Euclidean distance from any member of (Li, Lj) is not less than the Euclidean distance of Li and Lj. Eliminating Tomek links continues until every pair of nearest neighbor vectors belongs to the same class. It is crucial to note that Li and Lj belong to distinct classes."
        },
        {
            "title": "4.4 Collaborative Transformer",
            "content": "The attention layer is CoLogs most crucial component. To determine the mapping relationship among modalities of log data, CT is created by modifying the attention layers of two identical transformer blocks that concurrently learn data representations in an end-to-end manner. As result, when learning information from one modality, we use the information from the other modality as guidelines enabled by the MHIA mechanism. The attention mechanism of an unimodal transformer encoder is deﬁned as follows: attention(Q, K, C) = sof tmax( QK )C. (8) where Q, K, and are the respective queries, keys, and contexts. We implement the self-attention mechanism with = = C. Query, key, and context vectors enable the model to understand the relative importance of all words within the context of the full sequence. The operation QKT produces squared attention matrix that contains the correlation between row of input matrix m, if this input has size of k. The expression is factor used to adjust the scale where = k. sem, seq { } The concept of stacking several self-attentions attending data from various representation sub-spaces in diﬀerent positions is known as MHA as follows: HA(Q, K, C) = concat(h1, h2, . . . , hh)Wo. (9) where, hi = attention(QW 1, 2, . . . , i , KW 1, , CW ) { } (10) where Q, K, Rdd. We stack layers of self-attention to perform MHA to learn complex patterns between events. The projection into the balancing layer for classiﬁcation follows encoding through the transformer blocks, with the output of each transformer block being used. As illustrated in Figure 4, integrating MHIA in CT involves substituting MHA with MHIA in each modality encoder, which is demonstrated with light green and gold Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 colors. The architecture of MHIA is illustrated in Figure 6. MHIA requires concurrent encoding across utilized modalities. In addition, the MHIA implementation includes computing the attention scores of the at-hand modality by utilizing the vector from the at-hand modality and the and vectors from the secondary modality using the MHA mechanism. Each modality encoder transformer in the CT architecture contains corresponding MHIA block. Preprocessing Log Parsing Unstructured Logs Excel C 1 2 3 4 5 6 Structured Logs Keys Log Messages Log Sequences Embeddings Labels Tomek link Train Set Valid Set Test Set Dataloader Multi-Head Impressed Attention Multi-Head Impressed Attention MatMul MatMul softmax Mask Scale MatMul MHA"
        },
        {
            "title": "Collaboration \nTunnel",
            "content": "softmax Mask Scale MatMul MHA Fig. 6 The architecture of the multi-head impressed attention layer. The MHIA process involves calculating the attention scores of the current modality through the MHA mechanism, using the vector from the current modality and the and vectors from the secondary modality. According to MHIA architecture, various modalities are encoded concurrently."
        },
        {
            "title": "4.5 Concurrent-Encoding",
            "content": "The concurrent-encoding process means simultaneous encoding of each modality. As mentioned above, this concept implies that the encoding block used for particular modality is not unrolled before moving to another modality. The projection into latent space for classiﬁcation follows encoding through the transformer blocks by the balancing layer."
        },
        {
            "title": "4.6 Modality Adaptation Layer",
            "content": "Since diﬀerent modalities are collaboratively encoded in the CT architecture, it is essential to ensure that the extracted representations are free of unnecessary information. Because logs have several modalities, encoding each modality using knowledge from other modalities is crucial in obtaining an improved and more informative representation of that modality. However, due to the intrinsic variations between diﬀerent modalities, encoding them collaboratively can result in inconsistencies and impurities Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 in the representation of each modality. MAL is proposed as solution to address these challenges. MAL comprises soft attention layers, which their corresponding outputs are stacked. Each modality m, sem, seq is initially transformed into new high-dimensional representation space high to start MAL. This transformation is calculated using Equation 11 as follows: { } high = mW high. (11) RN is the input matrix on which CoLog performs MAL and where high Rk2k is transformation matrix that embeds into higher dimension shared among all MALs. In the second phase, weights for each node in the sequence must be generated from the acquired high-dimensional space to implement soft attention and extract pure and global representation of every node in the sequence. To do this, we need the wnode , wnode } is unique to each node in input sequence. The weights of each node are derived using Equation 12, based on the following notions. 2k, while the set of vectors [1, ]s. each wnode must have size of 1 { where, wm = sof tmax(V high(W low)T )."
        },
        {
            "title": "W m",
            "content": "low = concat(wnode ). (12) (13) where space for each node. low is transformation matrix that extracts weights from high-dimension Equation 14 computes the soft attention of the input matrix based on the weights acquired from the previous phase as follows: = sof tattention(V m) = N j= wm[i][j] m[j]. (14)"
        },
        {
            "title": "V m\nwhere\ni\ne\nthe sequence and",
            "content": "represents the computation output obtained from an individual node within demonstrates the Hadamard product."
        },
        {
            "title": "The output of the MAL for an input matrix V m with a sequence length of N can",
            "content": "be deﬁned as stacking all where, vectors according to following relation: out = layernorm( + m). = stacking( 1 , 2 , . . . , m ). (15) (16)"
        },
        {
            "title": "The transformation matrix W m",
            "content": "high, shared among all nodes, maps each node in the input sequence to high-dimensional space, as illustrated in Figure 7. The softmax allocate the weight wm[i] for each node i, which is distinct function and vector wnode for every node. The ﬁnal output is obtained by computing the weighted sum of all nodes, which is then fed into the balancing and classiﬁcation layers to make the ﬁnal Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 decision. We perform this to enable each modality to recognize the corresponding cross-node representations and remove impurities of each modality. By completing this task, every node vector can elicit important information from other nodes respective representations, contributing to each nodes global sentiment polarity."
        },
        {
            "title": "Modality Adaptation Layer",
            "content": "= ( + , ) + , , Input Vector High-Dimensional Space Adapter's weight Soft-Attention Adapted Output Vector Fig. 7 The architecture of modality adaptation layer. MAL achieves an overall representation for each modality by assigning weights to each node in the input sequence. It can also remove impurities of the modalities that are encoded in collaborative manner."
        },
        {
            "title": "4.7 Balancing Layer",
            "content": "At the end of the modality transformer blocks, we add balancing layer that projects the modality into new representation space. Due to the inherent diﬀerences between various log modalities, CoLog summarizes feature vectors of each modality in the same semantic space. Therefore, as seen in Figure 4, the learned outcomes from each modality encoder are projected into latent space for meaningful fusion where CoLog interprets the feature vectors of each modality in space with high dimensions. Furthermore, since diﬀerent modalities may contribute diﬀerent importance to determining the normality or abnormality of the input, we extract the balancers weight for each modality from their respective high-dimensional representations. Based on the balancers weights, we can perform soft attention to balance the contribution of modalities before fusion. The balancing layer is the same as MAL of size 1, with the distinction that the resulting vectors are projected into latent space to ensure meaningful fusion. Integrating latent space in BL is due to the diﬀerent nature of the modalities."
        },
        {
            "title": "The output of the balancing layer for semantic and sequence modalities is\nand Oseq",
            "content": "demonstrated using the symbols Osem , respectively. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-"
        },
        {
            "title": "4.8 Classiﬁcation",
            "content": "Finally, after being projected to latent space, balanced feature vectors of each modality are summed, normalized, and transformed into 2-dimensional vectors to apply the activation function. According to Figure 4, the vectors corresponding to each modality undergo element-wise summation following the successful computation of all transformer blocks. Subsequently, these vectors are used to predict each input sample by MLP and layer normalization, as demonstrated by: yi = Wclassif ication(layernorm(Osem + Oseq )). (17) where Wclassif ication denotes the learning matrix of MLP."
        },
        {
            "title": "4.9 Anomaly Detection",
            "content": "If various CoLog inputs are provided to the model within single variable, the CoLog input can be referred to as = . CoLog receives and tries to predict the set of labels y. The anomaly detection phase is similar to the training process. It is important to note that CoLog can detect anomalies in two distinct manners. This involves prioritizing the detection of point anomalies only or the detection of point and collective anomalies. keys, sem, seq, embeddings, labels } {"
        },
        {
            "title": "4.9.1 Point Anomaly Detection",
            "content": "Based on the methodology provided in Section 4, CoLog takes input and attempts to make predictions about them during the point anomaly detection procedure. This procedure aims to receive an individual log entry by CoLog, analyze it in collaboration with its background or context, and determine its polarity."
        },
        {
            "title": "4.9.2 Point and Collective Anomaly Detection",
            "content": "Point and collective anomaly detection through uniﬁed framework entail receiving input by CoLog and attempting to ascertain the polarity of an individual log event and its background or context polarity. From this concept, it can be inferred that in this scenario, the task of CoLog is 4-class classiﬁcation. These classes encompass the abnormality of an event, the abnormality of the event and its background or context, the abnormality of just the background or context of the event, and the normality of both the event and its background or context. Hence, it is imperative to label the background or context of the events in this particular scenario. { log event sequence = L1, L2, . . . , LL} is abnormal if one of its log messages reﬂects negative sentiment. Based on this, the background or context of each log event can also be labeled. Finally, based on the methodology provided in Section 4, CoLog takes input and generates predictions during the point and collective anomaly detection procedure. This procedure aims to receive an individual log entry by CoLog, analyze it in collaboration with its background or context, and determine its polarity as well as its background or context polarity. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-"
        },
        {
            "title": "5 Experiments",
            "content": "This section will begin by describing the OS log datasets. Subsequently, we will discuss the experimental setup of CoLog and the evaluation results of the proposed model."
        },
        {
            "title": "5.1 Log Datasets",
            "content": "Experiments are conducted on benchmark OS log datasets, including the publicly available BlueGene/L, Hadoop, and Zookeeper datasets. Table 2 presents cominformation of all datasets utilized for evaluating CoLogs prehensive statistical performance. Table 2 summary of publicly available OS log datasets. Dataset Spark1 Honey52 Windows2 Casper Jhuisi Nssal Honey7 Zookeeper Hadoop BlueGene/L # Lines # Positive Records # Negative Records Data Size 33, 236, 604 124, 386 25, 000 11, 086 11, 737 107, 093 8, 712 74, 380 394, 308 4, 747, 31, 513, 147 67, 798 18, 599 9, 874 9, 063 91, 349 8, 162 25, 873 382, 870 4, 399, 486 1, 723, 457 56, 588 6, 401 1, 212 2, 674 15, 732 550 48, 507 11, 438 348, 477 2.75 GB 12.6 3.48 930 KB 0.98 8.53 734 KB 9.95 48.61 708.76 1The provided datasets are intended to evaluate the robustness of CoLog. 2The provided datasets are intended to evaluate the generalization of CoLog. Spark [132], Honey5 [133], and Windows [132] datasets are not used during CoLog training. These datasets are used to test the generalizability and robustness of CoLog. Spark is management solution for handling large amounts of data. Normal and abnormal Spark system actions are both represented in the logs obtained from 32 hosts. Honey5 originates from the Forensic Challenge 5 in 2010, which the Honeynet Project conducted. The dataset provided is an instance of Linux OS that has been compromised. As the last dataset, we use the Windows dataset, obtained by consolidating many logs from the CUHK laboratory computer operating on Windows 7. The Windows OS implemented component-based servicing (CBS) mechanism to facilitate programs secure and controlled installation processes. The ﬁrst dataset of the other seven datasets of this collection used in the model training process was taken from disk image provided by Digital Corpora and given the name nps-2009-casper-rw [134]. bootable USB was used to create dump of the ext3 ﬁle system. The OS logs from machine running the Linux OS are included in this dataset. The digital forensic research workshop (DFRWS), an annual security conference, provides the second and third system log datasets we use to train CoLog. They presented an OS log forensics challenge in 2009. The DFRWS forensic challenge 2009 studied log ﬁles to ﬁnd an intruder who illegally communicated classiﬁed material. This case involves jhuisi [135] and nssal [135] hosts. Two Linux-based Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 Sony PlayStation 3 hosts. Additional OS logs were obtained via the honeynet forensic challenge 7 (Honey7) [136]. Honey7 includes disk image of compromised Linux server. The cloned disk images contained the directory /var/log/ for all datasets. Then, authentication, kernel, and system logs are obtained. Zookeeper manages distributed systems. The CUHK laboratory collected Zookeeper logs [132] from 32 hosts for 26 days. Big data tool Hadoop distributes jobs across machines. Hadoop cluster with 46 cores on ﬁve machines created the Hadoop log dataset [137]. In this dataset, machine downtime and network disconnections are anomalies. The 131,072 processor, 32,768 GB memory BlueGene/L supercomputer at lawrence livermore national laboratories (LLNL) provides an open dataset [138]. The records comprise alert and non-alert, where alerts signify abnormal behaviors."
        },
        {
            "title": "5.2 Experimental Setup",
            "content": "This section describes the hardware and software conﬁgurations of the system we used to run experiments. We then discuss CoLogs hyperparameters and the metrics utilized to evaluate CoLog."
        },
        {
            "title": "5.2.1 System Conﬁgs",
            "content": "We use Colab pro machine with 12 cores on the central processing unit (CPU), 83.5 gigabytes of random-access memory (RAM), and an NVIDIA A100 40GB graphics processing unit (GPU) to execute the experiments for both the CoLog and the other methods. Python 3.10.12 is used to develop CoLog, and PyTorch [139] 2.0.1+cu118 is used as the backend. We use the imbalanced-learn [140] library to implement the Tomek link and evaluate it against other class-balancing techniques. The Ray library [141] in Python was utilized to determine the models optimal parameters. The sentencetransformers library [112] was also employed to extract sentence embedding vectors from the log messages."
        },
        {
            "title": "5.2.2 Hyperparameters",
            "content": "The Ray Python library chooses the most optimal hyperparameters for CoLog. Using the above mentioned library, 108 distinct conﬁgurations of CoLog were analyzed. In addition, an examination of window size, class imbalance, and the training ratio has been conducted independently to determine the optimal values for each hyperparameter. The number of CT layers is 2, and the number of heads is 4. The batch size is 32. The learning rate for training the CoLog is set to 5e-5, whereas the learning rate decay is set to 0.5, and the learning rate will decrease three times at this rate. The Adam optimizer method is employed. Adam is considered suitable for this case due to its minimal RAM requirements. The training process has maximum epoch of 20 and an early stopping criterion set to 5. The training process will be terminated if no further improvement is shown after 5 epochs. The model warm-up process consists of 5 epochs. The dropout rate of 0.1 is used. The lengths of the semantic and sequence modalities are 60, whereas the embedding size of the word vectors is 300. The hidden size of 256 is used for the CoLog model. The projection vector size is also 2048. The Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 sequence modality type is set to context, and window size 1 is used. Dimensions of sequence modality are set to 384. We employ holdout validation protocol. The datasets were divided into three pieces based on the following proportions: 60% for training, 20% for validating, and 20% for testing. In the context of the BGL dataset, the ratio mentioned above is 10%, 45%, and 45%, which can be attributed to constraints imposed by limited resources. Also, the Tomek link technique was employed to address the data imbalance. Finally, CoLog is trained on each log dataset and tested on the same dataset for generating results."
        },
        {
            "title": "5.2.3 Metrics",
            "content": "The performance of CoLog is assessed using multiple metrics, including precision, recall, F1-score, and accuracy. These metrics are computed based on the values of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). The scikit-learn library [142] is used to implement metrics with the macro average option. Evaluation metrics are computed for each dataset on individual ﬁles, and then the average is derived across all ﬁles. Precision. Precision quantiﬁes the proportion of correctly identiﬁed positive instances across all positive predictions generated by the CoLog. The precision formula is deﬁned as follows: recision = P + . (18) where is correctly predicted positive instances and is incorrectly predicted positive instances. Recall or Sensitivity. Recall quantiﬁes the proportion of actual positive cases correctly detected by the CoLog. The mathematical expression representing the concept of recall is as follows: Recall = P + . (19) where is incorrectly predicted negative instances. F1-score. F1-score pertains to evaluating the balance between precision and recall. The mathematical expression representing the F1-score is as follows: 1score = 2 recision Recall recision + Recall = 2 2 P + + . (20) Accuracy. Accuracy quantiﬁes the proportion of correct predictions made by the CoLog. The following equation gives the mathematical expression representing accuracy: Accuracy = + P + + + . (21) where is correctly predicted negative instances. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-"
        },
        {
            "title": "5.3 Results",
            "content": "In this section, we discuss experiments conducted on the CoLog, whereby several diagrams, including the confusion matrix, normalized confusion matrix, receiver operating characteristic (ROC) curve, and precision-recall (PR) curve, were utilized. The comprehensive CoLog outcomes on publicly available log datasets are presented in Table 3. Additionally, the graphical depictions of these results are illustrated in Figure 8 to Figure 14. Also, subsections are organized as follows. In the subsequent analysis, we compare CoLog and other log anomaly detection approaches by applying them to the above mentioned datasets. Next, we will discuss the outcomes of the hyperparameter tuning process, the impact of the training ratio, the inﬂuence of various approaches for addressing the class imbalance, the eﬀect of window size, and the presentation of the vectors derived from the CoLog. Next, let us examine the generalization or robustness of the CoLog in detail. Subsequently, we discuss the signiﬁcant design components of the CoLog as the ablation study. Next, we undertake groundbreaking research. Subsequently, we will compare our contributions with other log anomaly detection techniques. The CoLog has exhibited exceptional versatility when applied to diverse datasets, consistently attaining high level of performance. The methods eﬃcacy is apparent through its high accuracy in classifying all tested datasets, where CoLog achieves near-perfect or perfect classiﬁcation scores. CoLog demonstrates ﬂawless classiﬁcation performance on the Casper, Jhuisi, Honey7, and Zookeeper datasets, exhibiting complete absence of false positives or negatives. Similarly, CoLog exhibited outstanding performance on the Nssal and Hadoop datasets, with few occurrences of false positives and negatives, where all anomalies in the Hadoop dataset are detected. Even when applied to the large BGL dataset, CoLog maintains exceptional performance, accurately identifying 1,964,265 true positives and 156,775 true negatives while exhibiting few mistakes with only 1 false negative. The area under the curve (AUC) values for the ROC and PR curves are consistently near 100 or 100 across all datasets, providing additional evidence of the CoLogs strong performance. The exceptional outcomes of CoLog can be attributed to several signiﬁcant contributions. Initially, in CoLog, semantic and sequence log modalities collaborate. Utilizing sequential features besides semantic features augments the available knowledge and boosts the models capacity to identify anomalies. Furthermore, distinct neural networks are used for each modality, which enables CoLog to control the interactions between modalities, yielding more reliable outcomes. Finally, latent space is used to fuse the modalities. The signiﬁcance of this matter lies in the fact that various modalities exhibit dissimilar characteristics, necessitating sophisticated fusion mechanism for their meaningful integration. These contributions demonstrate the adaptability of CoLog and establish its reliability as tool for detecting log anomalies in various scenarios. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 Table 3 Comprehensive CoLog outcomes on publicly available log datasets. Dataset Average Class # Samples Precision Recall F1-Score Accuracy Casper 0: Anomaly 243 1: Normal 1976 Macro Jhuisi 0: Anomaly 536 1: Normal 2350 Macro Nssal 0: Anomaly 3147 1: Normal 18271 21418 Macro Honey7 0: Anomaly Macro Zookeeper 1: Normal 1633 1743 0: Anomaly 9702 1: Normal 5176 Macro 14878 Hadoop 0: Anomaly 2289 1: Normal 33893 36182 Macro BlueGene/L 0: Anomaly 156807 1: Normal 1964266 Macro 2121073 100 100 100 100 100 99.936 99.972 99.955 100 100 100 100 100 100 99. 99.997 99.999 99.998 99.999 100 100 100 100 100 100 100 100 100 100 100 99.841 99.889 99.967 99.989 99.981 99. 99.935 100 100 100 100 100 100 100 100 100 100 100 100 99.913 99.956 99. 100 99.997 99.956 99.977 99.980 99. 99.998 100 99.999 99.990 99."
        },
        {
            "title": "5.3.1 CoLog Versus the Field: A Comparative Study",
            "content": "We compared CoLog and some log anomaly detection algorithms available in the loglizer [111] and deeploglizer [24] toolboxes. The loglizer is utilized for machine learning algorithms, whereas the deeploglizer is employed for deep learning-based methods. These toolboxes consist of ﬁve supervised detection approaches, including the logistic regression (LR) [143], support vector machines (SVM) [109], and Decision Tree (DT) [109] from the loglizer toolbox, as well as the Attentional BiLSTM [62] and CNN [45] from the deeploglizer toolbox. Furthermore, four unsupervised techniques, including the isolation forest (IF) [144] and principal component analysis (PCA) [12] algorithms from loglizer, the LSTM [36] and Transformer [80] implemented in the deeploglizer. In addition, we evaluate the performance of CoLog in comparison to pylogsentiment. The pylogsentiment is tool for sentiment analysis in OS logs. It uses GRU to identify abnormalities and classify log entries according to their sentiment. This aids in detecting potential issues and evaluating systems overall state. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 (a) (c) (b) (d) Fig. 8 The collection of output visualizations generated by the CoLog when applied to the Casper log dataset. (a) confusion matrix, (b) normalized confusion matrix, (c) receiver operating characteristics curve, and (d) precision-recall curve Prior to feeding the log datasets into the loglizer or deeploglizer models, we employes the Drain and nerlogparser tools to separate all the log records and extract the log message. The hyperparameters of the models are set to the same conﬁguration as recommended in the loglizer and deeploglizer toolboxes. Furthermore, sliding partitioning is implemented to perform log partitioning. Then, algorithms in loglizer and deeploglizer are employed to process these sequences of log events. Table 4 to Table 10 present the performance evaluation of CoLog and other anomaly detection methods. The F1-scores of all techniques are presented at the mean level in Table 11. Of all the methods, pylogsentiment demonstrates superior performance due to its balancing between the two sentiment classes. This results in more eﬀective deep learning model for reliably identifying anomalous activities within the minority class. The pylogsentiment achieves an average F1-score of 99.135. The performance of other deep learning-based models, including Transformer, CNN, attentional BiLSTM, and LSTM, on all datasets are similar. For instance, they yield similar mean F1-scores of 97.845, 97.812, 97.661, and 96.765, respectively. These methods exhibit enhanced performance due to their utilization of deep learning for anomaly detection. Deep learning is adept Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 (a) (c) (b) (d) Fig. 9 The collection of output visualizations generated by the CoLog when applied to the Jhuisi log dataset. (a) confusion matrix, (b) normalized confusion matrix, (c) receiver operating characteristics curve, and (d) precision-recall curve at anomaly detection in logs due to its capability to manage unstructured data and autonomously extract features. It eﬀectively handles large data volumes and, when employing log semantics, adapts to evolving abnormalities over time. Transformer produces better results because it uses word-level representations to represent log entries. In Table 11, the IF attained mean F1-score of 49.372, whereas PCA algorithm recorded mean F1-score of 47.273. Generally, machine learning methods often struggle with log anomaly detection due to the complexity and unstructured nature of log data, distribution shifts introducing unseen anomalies, and the need for sophisticated feature engineering. Additionally, these models can be sensitive to hyperparameter settings, requiring extensive tuning. Table 4 to Table 10 indicate that CoLog yields superior performance compared to other techniques across all datasets. Moreover, Table 11 clearly illustrates that CoLog signiﬁcantly improves anomaly identiﬁcation. Compared to the second-ranked pylogsentiment, it demonstrates greater mean F1-score across all datasets. Given that the pylogsentiment has mean F1-score of 99.135, further improvement of 0.865 is required to attain perfect anomaly detection on the employed benchmark datasets. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 (a) (c) (b) (d) Fig. 10 The collection of output visualizations generated by the CoLog when applied to the Nssal log dataset. (a) confusion matrix, (b) normalized confusion matrix, (c) receiver operating characteristics curve, and (d) precision-recall curve CoLog decreases this value to 0.013. This signiﬁes an estimated 98.50% improvement in the available range to enhance the results. In many cases, unsupervised approaches frequently exhibit inferior performance compared to supervised techniques in log anomaly detection. This is attributable to the absence of labeled data, which limits understanding of complex patterns and reduces accuracy. They also tend to have more false positives. Supervised approaches enhance their accuracy through training on labeled data over time. In this work, we frame log anomaly detection as multimodal sentiment analysis task, transforming the problem from traditional unsupervised anomaly detection to supervised classiﬁcation setting. Conventional anomaly detection methods often treat anomalies as rare or outlier events without explicit labels, which limits the ability of models to learn discriminative features eﬀectively due to the highly imbalanced and variable nature of log data. By interpreting anomalies as negative sentiments within log messages, our approach leverages sentiment analysis techniques to annotate log entries with sentiment labels, thereby enabling supervised learning. This shift allows the model to utilize labeled Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 (a) (c) (b) (d) Fig. 11 The collection of output visualizations generated by the CoLog when applied to the Honey7 log dataset. (a) confusion matrix, (b) normalized confusion matrix, (c) receiver operating characteristics curve, and (d) precision-recall curve data for training, improving detection accuracy by capturing subtle semantic patterns and contextual nuances that indicate anomalous system behavior. Consequently, our multimodal sentiment analysis framework beneﬁts from both the rich semantic representation of logs and the robust classiﬁcation capabilities of supervised learning models, leading to enhanced detection performance compared to purely unsupervised approaches. Additionally, CoLogs exceptional eﬃcacy in anomaly detection is due to its novel methodology for learning log data via both sequence and semantic modalities. In contrast to conventional approaches that may focus on single facet of log data, CoLog incorporates multiple modalities to produce more comprehensive representation of log data. This dual-modality learning enables CoLog to gain more thorough understanding of log data that might otherwise be overlooked. Another signiﬁcant component enhancing CoLogs eﬃcacy is its collaborative approach to acquiring log representations. Unlike previous approaches that may analyze logs in isolation or with minimal interaction across various data modalities, CoLog guarantees the simultaneous learning of sequence and semantic information. This collaborative method yields Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 (a) (c) (b) (d) Fig. 12 The output collection of visualizations generated by the CoLog when applied to the Zookeeper dataset. (a) confusion matrix, (b) normalized confusion matrix, (c) receiver operating characteristics curve, and (d) precision-recall curve more comprehensive and reﬁned representations of log data, thereby enhancing the models ability to distinguish between normal and abnormal activity. Jointly examining logs facilitates enhanced adaptability to various log contexts and scenarios. This versatility is essential in practical applications where logs may diﬀer markedly in form and content. In addition, in many traditional methods, the integration of various data modalities can lead to noise and inconsistencies, ultimately degrading performance. One of the standout features of CoLog is its modality adaptation layer, which plays crucial role in reﬁning its performance by removing impurities that may arise from combining diﬀerent modalities. This layer acts as ﬁlter, ensuring that only relevant and complementary features are fused. By eliminating conﬂicting or redundant information, the MAL prevents the degradation of performance that could occur if modalities were combined without such careful consideration. CoLogs ability to ﬁlter out these impurities ensures that the resulting log representations are cleaner and more reliable, enhancing the overall accuracy of the analysis. As result, CoLog is better equipped to handle the variability and complexity inherent in log data, making it more eﬀective tool for administrators in the ﬁeld. CoLog also utilizes latent Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 (a) (c) (b) (d) Fig. 13 The collection of output visualizations generated by the CoLog when applied to the Hadoop log dataset. (a) confusion matrix, (b) normalized confusion matrix, (c) receiver operating characteristics curve, and (d) precision-recall curve space for meaningfully fusing diﬀerent modalities of log data. Instead of simply concatenating features from each modality or processing them separately, CoLog explores shared latent space where these features can be integrated eﬀectively. By mapping log data into common latent space, CoLog can capture complex relationships and dependencies that are essential for accurate anomaly detection. Table 12 evaluates the eﬃciency of diﬀerent models by recording the time spent on training and testing processes on the Hadoop dataset. The LSTM, Attentional BiLSTM, CNN, and Transformer methodologies necessitate considerably less time for training and inference compared to pylogsentiment and CoLog. This results from utilizing log keys rather than the semantic features of logs, which require far fewer computer resources. CoLog requires less training time than pylogsentiment, although it is inferior during the inference phase. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 (a) (c) (b) (d) Fig. 14 The collection of output visualizations generated by the CoLog when applied to the BlueGene/L log dataset. (a) confusion matrix, (b) normalized confusion matrix, (c) receiver operating characteristics curve, and (d) precision-recall curve"
        },
        {
            "title": "5.3.2 Enhancing Success: Study on Hyperparameters Tuning",
            "content": "Hyperparameters tuning is essential in deep learning as it directly inﬂuences model performance and generalization [145]. This process can substantially improve model accuracy and eﬃciency [146]. Appropriate tuning addresses challenges such as overﬁtting or underﬁtting, which is essential in noisy and mutation-prone log data. Furthermore, it facilitates the examination of various model conﬁgurations, resulting in more resilient and dependable results. Considering the importance of tuning, specifically in log anomaly detection, we employ the Ray package in Python to tune CoLog. In the tuning procedure, we utilize three datasets, Casper, Jhuisi, and Honey7, and apply 324 distinct conﬁgurations of the CoLog on them. Based on the tuning results of the Casper, Jhuisi, and Honey7 datasets, the hyperparameters selection criterion for each dataset is to achieve optimal accuracy in the shortest time. The overall optimal conﬁguration is the resultant of all datasets optimum conﬁgurations. Due to page constraints, this paper only covers the 27 most eﬃcient runs out of 108 runs for each Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 Table 4 CoLog in comparison to other log anomaly detection methods on the Casper dataset. Bold numbers indicate the outstanding results. Anomaly Detection Technique Precision Recall F1-Score Accuracy Supervised Methods Logistic Regression [143] Support Vector Machines [109] Decision Tree [110] Attentional BiLSTM [63] Convolutional Neural Network [46] pylogsentiment [38] 66.959 58.614 83.466 99.766 99.766 99.487 60.863 60.757 77.037 99.872 99.872 99.413 59.700 59.482 79.488 99.819 99.819 99. Unsupervised Methods Isolation Forest [144] Principal Component Analysis [12] LSTM [36] Transformer [80] 52.407 51.205 97.973 98.409 50.650 50.282 98.843 99.100 49.926 49.362 98.380 98.738 90.993 90.967 94.998 99.834 99.834 99. 88.149 87.480 98.505 98.837 CoLog1 100 100 100 1CoLog is supervised method. Table 5 CoLog in comparison to other log anomaly detection methods on the Jhuisi dataset. Bold numbers indicate the outstanding results. Anomaly Detection Technique Precision Recall F1-Score Accuracy Supervised Methods Logistic Regression [143] Support Vector Machines [109] Decision Tree [110] Attentional BiLSTM [63] Convolutional Neural Network [46] pylogsentiment [38] 68.373 63.643 91.534 95.123 97.139 98.867 66.127 66.506 89.769 97.270 94.885 98.761 64.886 64.051 90.550 96.169 95.982 98.813 Unsupervised Methods Isolation Forest [144] Principal Component Analysis [12] LSTM [36] Transformer [80] 57.519 44.828 96.879 96.879 52.774 49.299 92.385 92.385 51.700 45.014 94.508 94.508 79.182 80.914 93.313 99.341 99.341 98.850 74.707 75.448 99.121 99.121 CoLog 100 100 100 100 1CoLog is supervised method. dataset (Complete tuning results are available on CoLogs GitHub repository.). The chosen hyperparameters are detailed in Section 5.2.2. Figure 15 illustrates the results of the top 27 executions of CoLog on the Casper, Jhuisi, and Honey7 datasets, arranged from left to right. The light green bars signify the models accuracy in that conﬁguration of CoLog, while the rose bars represent the total time required to attain that accuracy. Accordingly, conﬁgurations 3384, 6587, and 2868 have achieved optimal outcomes in the shortest time for Casper, Jhuisi, Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 Table 6 CoLog in comparison to other log anomaly detection methods on the Nssal dataset. Bold numbers indicate the outstanding results. Anomaly Detection Technique Precision Recall F1-Score Accuracy Supervised Methods Logistic Regression [143] Support Vector Machines [109] Decision Tree [110] Attentional BiLSTM [63] Convolutional Neural Network [46] pylogsentiment [38] 85.133 80.206 94.791 96.750 96.703 97.170 Unsupervised Methods Isolation Forest [144] Principal Component Analysis [12] LSTM [36] Transformer [80] 65.504 52.642 96.148 96.304 74.728 74.935 87.700 98.805 98.243 96.050 57.352 53.827 97.669 99.354 76.476 76.474 89.470 97.754 97.460 96.602 56.101 49.505 96.896 97.778 97.604 97.655 98.063 99.813 99.789 99. 80.967 80.614 99.742 99.813 CoLog1 99.955 99.915 99.935 99. 1CoLog is supervised method. Table 7 CoLog in comparison to other log anomaly detection methods on the Honey7 dataset. Bold numbers indicate the outstanding results. Anomaly Detection Technique Precision Recall F1-Score Accuracy Supervised Methods Logistic Regression [143] Support Vector Machines [109] Decision Tree [110] Attentional BiLSTM [63] Convolutional Neural Network [46] pylogsentiment [38] 68.143 68.143 93.260 100 100 99.970 70.000 70.000 83.307 100 100 99.107 69.042 69.042 86.359 100 100 99.535 Unsupervised Methods Isolation Forest [144] Principal Component Analysis [12] LSTM [36] Transformer [80] 47.509 60.871 96.212 96.923 50.948 58.898 98.810 99.048 48.064 55.943 97.429 97.932 96.286 96.286 97.471 100 100 99.943 85.966 88.279 98.155 98.524 CoLog 100 100 100 100 1CoLog is supervised method. and Honey7 datasets, respectively. Figure 15 also illustrates the tag for the optimum conﬁgurations of each dataset. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 Table 8 CoLog in comparison to other log anomaly detection methods on the Zookeeper dataset. Bold numbers indicate the outstanding results. Anomaly Detection Technique Precision Recall F1-Score Accuracy Supervised Methods Logistic Regression [143] Support Vector Machines [109] Decision Tree [110] Attentional BiLSTM [63] Convolutional Neural Network [46] pylogsentiment [38] 98.369 97.987 98.599 95.783 95.121 99.722 98.464 97.769 98.880 92.928 92.662 99.898 98.416 97.877 98.737 94.300 93.850 99.810 Unsupervised Methods Isolation Forest [144] Principal Component Analysis [12] LSTM [36] Transformer [80] 32.607 79.011 95.825 99.890 50.000 51.209 91.887 99.416 39.473 42.121 93.750 99.652 98.562 98.078 98.851 98.387 98.252 99.973 65.215 66.021 98.252 99. CoLog1 100 100 100 100 1CoLog is supervised method. Table 9 CoLog in comparison to other log anomaly detection methods on the Hadoop dataset. Bold numbers indicate the outstanding results. Anomaly Detection Technique Precision Recall F1-Score Accuracy Supervised Methods Logistic Regression [143] Support Vector Machines [109] Decision Tree [110] Attentional BiLSTM [63] Convolutional Neural Network [46] pylogsentiment [38] 48.523 48.523 48.523 97.640 99.719 99.886 Unsupervised Methods Isolation Forest [144] Principal Component Analysis [12] LSTM [36] Transformer [80] 47.702 49.995 99.850 97.280 50.000 50.000 50.000 97.955 99.847 99.732 50.000 49.996 97.397 99. 49.250 49.250 49.250 97.792 99.783 99.809 48.824 49.996 98.589 98.518 97.046 97.046 97.046 97.902 99.955 99.905 54.034 58.214 99.715 99.685 CoLog1 99. 99.956 99.977 99.994 1CoLog is supervised method."
        },
        {
            "title": "5.3.3 Consequences of Ratio: Examination of Train Ratio Impact",
            "content": "Inadequate and imbalanced training data, such as log data, can result in overﬁtting, wherein the model is talented in training data but fails on novel data [147]. Furthermore, models trained with suitable train-test ratio generally exhibit superior performance [148]. Consequently, determining an optimal train-test data ratio is essential. The percentage of training data inﬂuences the models robustness to noise and Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 Table 10 CoLog in comparison to other log anomaly detection methods on the BlueGene/L dataset. Bold numbers indicate the outstanding results. Anomaly Detection Technique Precision Recall F1-Score Accuracy Supervised Methods Logistic Regression [143] Support Vector Machines [109] Decision Tree [110] Attentional BiLSTM [63] Convolutional Neural Network [46] pylogsentiment [38] 54.028 46.314 60.576 97.640 97.640 99.892 Unsupervised Methods Isolation Forest [144] Principal Component Analysis [12] LSTM [36] Transformer [80] 53.081 51.168 97.414 97.640 51.852 50.000 50.998 97.955 97.955 99.963 50.047 54.260 98.296 97.955 52.092 48.087 50.303 97.792 97.792 99.928 51.519 38.970 97.806 97.792 90.368 92.628 92.348 97.902 97.902 99. 47.389 48.487 97.902 97.902 CoLog1 99.999 99.990 99.994 99. 1CoLog is supervised method. Table 11 Ranking CoLog and other log anomaly detection methods. Bold numbers indicate the outstanding results. Anomaly Detection Technique Principal Component Analysis [12] Isolation Forest [144] Support Vector Machines [109] Logistic Regression [143] Decision Tree [110] LSTM [36] Attentional BiLSTM [63] Convolutional Neural Network [46] Transformer [80] pylogsentiment [38] CoLog Mean F1-score 47.273 49.372 66.323 67.123 77.737 96.765 97.661 97.812 97.845 99.135 99.987 data mutation, which are abundantly seen in the log data. An optimally selected ratio enables the model to learn underlying patterns in the data while maintaining robustness against outliers and noise. This is particularly vital in practical applications, for example, log-based anomaly detection, where data may be noisy and unreliable. We assess the inﬂuence of various train ratios by applying CoLog on the Casper, Jhuisi, and Honey7 datasets, which have been preprocessed with train ratios of 0.4, 0.5, 0.6, 0.7, 0.8, and 0.9. Figure 16 illustrates the outcomes of analyzing various training data rates. The optimum result occurs when CoLog is applied to the Jhuisi and Honey7 datasets with training ratio of 0.6. The optimal ratio for the Casper dataset is 0.8. Based on the above mentioned ratios, the F1-score is 100 for the Casper dataset, Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 Table 12 Eﬃciency of deep learning models on Hadoop Dataset. Training Time (Seconds) Inference Time (Seconds) Anomaly Detection Technique Overall Per-sample Overall Per-sample LSTM [36] Attentional BiLSTM [63] Convolutional Neural Network [46] Transformer [80] pylogsentiment [38] CoLog 7.72 25.71 14.85 9.05 2615. 2479.02 0.00112 0.00372 0.00215 0.00131 0. 0.01079 0.14 0.21 0.07 0.09 94. 124.00 0.00002 0.00003 0.00001 0.00001 0. 0.00343 99.879 for the Jhuisi dataset, and 100 for the Honey7 dataset. Finding proper training ratio is crucial in log anomaly detection. The ideal proportion of training data substantially aﬀects the convergence time of deep learning models, which is vital in this context. balanced ratio guarantees the model obtains diverse and representative samples during training, accelerating the learning process. An unbalanced ratio may result in slow convergence or cause it to become bound in local minima."
        },
        {
            "title": "Solving Methods",
            "content": "Data imbalance plays crucial role in deep learning, profoundly aﬀecting model performance and generalization. Imbalanced training data causes models to exhibit bias towards the majority class, resulting in suboptimal performance on the minority class [149, 150]. This poses signiﬁcant challenges in automated log anomaly detection, where the minority class frequently signiﬁes critical instances [38]. Mitigating data imbalance enables deep learning models to generate precise predictions across all imbalanced classes, augmenting their practical applicability in real-world scenarios. Numerous approaches have been devised to alleviate the impact of data imbalance, focusing on either equalizing class distribution or modifying the learning process to prioritize the minority class [126131]. CoLog employs under-sampling techniques due to the similarity among numerous log entries in the majority class. We implement class imbalance solving across all datasets with the Tomek link approach. Figure 17 illustrates the eﬃcacy of the Tomek link relative to various class balancing techniques, including the condensed nearest neighbor rule [151], NearMiss [128], neighborhood cleaning rule [152], one-sided selection [153], and random under-sampling across the Casper, Jhuisi, and Honey7 datasets. Figure 17 illustrates that the Tomek link exhibited the highest mean values across all datasets for precision, recall, F1-score, and accuracy, achieving 99.908%, 99.972%, 99.940%, and 99.957%, respectively. In the Honey7 dataset, the outcomes of the Tomek link method surpass slightly those of alternative techniques. However, the condensed nearest neighbor rule and NearMiss algorithms exhibit poor performance on Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 (a) (b) (c) Fig. 15 Collection of graphical illustrations of hyperparameters tuning practices performed by the CoLog on the (a) Casper, (b) Jhuisi, and (c) Honey7 datasets. The criterion for selecting the optimal conﬁguration is to achieve the highest accuracy in the shortest time. Based on this criterion, the results are sorted from left to right. The optimal conﬁguration is also depicted in the respective diagram for each dataset. this dataset. Except for NearMiss, all approaches yield successful results on the Jhuisi dataset. The Tomek link and neighborhood cleaning rule techniques also produce the same results in the Jhuisi dataset. Note that the Tomek link approach attains ﬂawless results of 100% in the Casper dataset."
        },
        {
            "title": "5.3.5 Marvels of Window Sizes: Study on the Eﬀect of Window Size",
            "content": "We utilize window size 1 with context sequence type across all datasets. Table 13 to Table 15 illustrate the performance of the context-based window size 1 in contrast to various window sizes and sequence types on the Casper, Jhuisi, and Honey7 datasets. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 (a) (b) Fig. 16 Visual illustrations of various train-test conﬁgurations performed by the CoLog on the (a) Casper, (b) Jhuisi, and (c) Honey7 datasets. (c) As shown in Tables 11 to 13, among all the datasets evaluated with varying window sizes, the context-based window size 1 yields the most optimal results at the lowest cost. In the Casper (i.e. Table 13) dataset, all window sizes yield acceptable yet same results, except the background-based window size of 6 and the context-based window sizes of 1, 2, and 12. The above mentioned exceptions attain 100% result in the evaluation metrics. In the Jhuisi dataset (i.e. Table 14), all results are independently signiﬁcant, with optimal outcomes achieved with context-based window size of 1. For Honey7 dataset (i.e. Table 15), background-based window sizes of 2, 3, 6, and 9 yield 100% among all metrics, while context-based window sizes of 1 and 3 also obtain 100%. The rest conﬁgurations attain the same results of 99.97%, 99.55%, 99.76%, and 99.94% for precision, recall, F1-score, and accuracy, respectively."
        },
        {
            "title": "5.3.6 Log Landscapes: Visualization of CoLog’s Output Vectors",
            "content": "In this experiment, we collect the learned vector representations of log messages from the pre-trained CoLog. Figure 18 illustrates the log vector representations on the Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 (a) (b) Fig. 17 Comparison of Tomek link with other class imbalance solving methods performed by the CoLog on the (a) Casper, (b) Jhuisi, and (c) Honey7 datasets. (c) Casper, Jhuisi, and Honey7 datasets, demonstrating their lower-dimensional representation of the test splits through the PCA dimensionality reduction technique. We prove that the log vector representations are classiﬁed accurately. The normal samples are concentrated closely together. Most anomalies are distributed further than normal samples and clustered together, where there is less error. Consequently, optimal performance could be achieved by applying an argmax prediction function."
        },
        {
            "title": "5.3.7 Resilience: Study on Generalization and Robustness",
            "content": "We look at the robustness and ability to be generalized of CoLog in identifying unknown abnormalities in previously untrained datasets. The datasets comprise Spark, Honey5, and Windows logs. Nevertheless, CoLog continues to demonstrate excellent results in identifying unknown abnormalities in addition to known anomalies, as demonstrated in Table 16. CoLog attained F1-scores of 98.83% and 99.12% on the Honey5 and Windows datasets, respectively. CoLog has experienced training on several datasets and is capable of addressing unidentiﬁed abnormalities. There are three Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 Table 13 Comparative analysis of CoLog performance across various window sizes and sequence types within the Casper dataset. Bold numbers indicate the outstanding results. Sequence Type Background Context Window Size 1 2 3 Precision Recall F1-Score Accuracy 99.80 99.97 99.88 99.95 99.80 99.97 99.88 99.95 99.80 99.97 99.88 99.95 6 100 100 100 100 99.80 99.97 99.88 99.95 1 100 100 100 100 2 100 100 100 100 6 9 99.80 99.97 99.88 99.95 99.80 99.97 99.88 99.95 99.80 99.97 99.88 99.95 Table 14 Comparative analysis of CoLog performance across various window sizes and sequence types within the Jhuisi dataset. Bold numbers indicate the outstanding results. Sequence Type Background Context Window Size 1 3 6 9 Precision Recall F1-Score Accuracy 99.97 99.91 99.94 99.96 99.76 99.76 99.76 99. 99.94 99.81 99.88 99.91 99.88 99.88 99.88 99.91 99.78 99.25 99.51 99.66 1 100 100 100 100 3 6 9 99.92 99.72 99.82 99.87 99.52 99.52 99.52 99.66 99.85 99.79 99.82 99. 99.75 99.16 99.45 99.62 Table 15 . Comparative analysis of CoLog performance across various window sizes and sequence types within the Honey7 dataset. Bold numbers indicate the outstanding results. Window Size 1 Precision Recall F1-Score Accuracy 99.97 99.55 99.76 99. Background 2 100 100 100 100 3 100 100 100 100 100 100 100 100 Sequence Type Context 9 100 100 100 100 100 100 100 100 2 99.97 99.55 99.76 99.94 3 100 100 100 100 99.97 99.55 99.76 99.94 9 99.97 99.55 99.76 99.94 primary interpretations. Initially, we employ word2vec word embeddings to represent each log messages words. word2vec eﬀectively captures the meanings of similar words, such as warning and stopping. Secondly, we utilize sentence-transformers to derive analogous sentence embeddings for similar log messages. Third, we employ supervised MSA model in collaborative manner that attains superior performance on both known and novel datasets. Furthermore, we perform robustness experiments on the Spark Dataset. We train and evaluate CoLog using the original Spark dataset, speciﬁcally the training set comprising 10% and the validation set including 45% of the total data. Subsequently, we test the trained model on the remaining 45% of the dataset. This synthetic dataset incorporates new log events injected at ratios of 0%, 5%, 10%, 15%, and 20% from Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 (a) (b) Fig. 18 Visual illustrations of the CoLogs output vectors on the (a) Casper, (b) Jhuisi, and (c) Honey7 datasets utilizing PCA. (c) alternative datasets. The results of the experiment are demonstrated in Figure 19. CoLog shows commendable recall metrics. Despite the rising injection ratio of unstable log events, CoLog consistently achieves high recall rate even at elevated injection levels. For instance, CoLog can maintain recall of 98.51% at an injection ratio of 20%. It validates that our methodology is adequately adaptable to unstable log events. CoLog interprets all log events as semantic vectors, enabling it to detect unstable log events with similar semantic meanings. As result, CoLog can eﬃciently analyze new log events and could keep on operating on the dataset containing unseen or unstable log events."
        },
        {
            "title": "5.3.8 Unraveling the Impact: Ablation Study",
            "content": "An ablation study was performed on the Casper, Jhuisi, and Honey7 datasets, with outcomes summarized in Table 17. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 Table 16 The evaluation of CoLog on previously unobserved datasets. Dataset Average Class # Samples Precision Recall F1-Score Accuracy Honey5 0: Anomaly 56588 1: Normal 67798 Macro 124386 Windows 0: Anomaly 6404 1: Normal 18567 24971 Macro 98.375 99.773 99.074 99.855 98.876 99. 99.731 99.049 99.129 98.625 99.196 99. 99.122 96.705 98.255 99.119 99.952 99. 98.328 98.833 Fig. 19 The visual illustration of CoLogs outcomes for various injection ratios of unstable log events on the Spark dataset. To conduct an ablation study, we eliminate sequence modality from multimodal information to assess the impact of background or context of events on model performance. The elimination of sequence modality results in reduced performance, which signiﬁes that sequential signals are essential for addressing log-based anomaly detection and illustrate the complementarity between log events and the background or context of events. Additionally, we eliminate module MHIA from CoLog and update it with MHA. Furthermore, we eliminate MAL and the balancing layer, which was intended to enable the fusion of various modalities, resulting in decrease in all performance metrics. These results demonstrate the eﬃcacy of MHIA, MAL, and the balancing layer in representation learning for detecting anomalies in logs. Finally, evaluate the distinct interconnections among diverse modules. The results demonstrate that CoLogs enhancements are not merely additive; they originate from the synergistic combination of its diﬀerent modules."
        },
        {
            "title": "5.3.9 Uncharted Territory: Groundbreaking Study",
            "content": "A uniﬁed framework for detecting point and collective anomalies in logs optimizing the process and guaranteeing consistency in the analysis. This comprehensive method Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 Table 17 Ablation study of CoLog. Seq and Sem denote sequence and semantic, respectively. Also, CT, MHIA, MAL, and BL denote collaborative transformer, multi-head impressed attention, modality adaptation layer, and balancing layer, respectively. Modality Module Metric Dataset Seq Sem MHIA MAL BL Precision Recall F1-score Accuracy Casper Jhuisi Honey X X X X X X X X X X X X X X X X X X X X X X X X 98.80 98.41 99. 99.33 99.33 99.97 99.36 99.56 99.88 96.88 99.61 97.98 99.22 99. 99.77 99.63 100 99.97 96.92 99. 99.94 99.97 100 99.97 99.51 99.85 99.10 99.92 99.51 99.51 99. 99.72 99.74 100 99.88 92.39 99. 98.29 99.09 99.49 99.51 99.04 99.55 99.05 99.55 99.09 99.55 99.55 99.51 100 99.31 98.74 99. 99.42 99.42 99.88 99.54 99.65 99.88 94.51 99.58 98.13 99.15 99. 99.64 99.33 100 99.76 97.93 99. 99.51 99.76 100 99.76 99.51 99.73 98.84 99.86 99.77 99.77 99. 99.82 99.86 100 99.92 99.12 99. 98.68 99.40 99.62 99.74 99.53 99.94 98.52 99.94 99.89 99.94 99.94 99.89 100 reduces blind spots that may arise from examining each anomaly type in isolation and improves the accuracy and reliability of the detection system. Utilizing uniﬁed framework enables to deployment of resources more eﬃciently and addresses anomalies, hence enhancing the overall resilience. The uniﬁed framework enables the application of diﬀerent modalities for improved and informative point and collective anomaly detection. As the discussion in Section 4.8, CoLog assigns labels to log messages and Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693the background or context of log events. Subsequently, upon receiving input I, it aims to learn and predict the label associated with each input. This method will involve four classes. Where 0 indicates both modalities are negative, 1 signiﬁes that only the log event is anomalous, 2 denotes that only the background or context of the log event is anomalous, and 3 represents normal instance of input. The experimental results of groundbreaking study on the Casper, Jhuisi, Nssal, Honey7, Zookeeper, Hadoop, and BlueGene/L datasets are shown in Table 18. It can be seen that CoLog performs excellent performance on this task. CoLog achieves excellent stable outcomes across all metrics on all datasets. For example, CoLog achieved recall of 99.94% on the zookeeper dataset. The eﬃcacy of CoLog in detecting point and collective anomalies in log ﬁles is primarily related to its ability to learn representations from various modalities of log ﬁles via distinct transformer blocks along with its ability to capture interactions across these modalities. This approach enables the CoLog to utilize the informative characteristics of log modalities, including semantic and sequential features of logs. Through collaboration tunnel, MHIA, MAL, and the balancing layer, CoLog can more eﬃciently detect both point and collective anomalies. This collaboration guarantees that no essential insights are ignored. Note that the development of CoLog for more modalities is straightforward. Table 18 The experimental results of groundbreaking study on all training datasets. Dataset Precision Recall F1-score Accuracy Casper Jhuisi Nssal Honey7 Zookeeper Hadoop BlueGene/L 99.43 99.82 99.32 99.77 99.98 99.17 99.94 99.40 99.48 99.70 98.90 99.94 99.84 99.89 99.41 99.65 99.51 99.33 99.96 99.50 99.91 99.64 99.70 99.91 99.77 99.99 99."
        },
        {
            "title": "Contributions",
            "content": "The comprehensive evaluation of contributions in existing literature compared to our studys advancements underscores signiﬁcant distinctions and synergies. FastLogAD [77] presents eﬃcient utilization of normal data coupled with an innovative anomaly generation method. In contrast, our approach, CoLog, augments this by encoding log records across multiple modalities. The pylogsentiment [38] focuses on SA for log anomaly detection and addressing class imbalance. CoLog surpasses this by applying MSA to log anomaly detection that outperforms state-of-the-art methods. RAGLog [92], retrieval-augmented generation model, primarily emphasizes storing normal log entries in vector database. Our method, however, extends beyond mere storage to learn about signatures of abnormalities. UMFLog [84] employs dual-model Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 architecture integrating BERT for semantic feature extraction and VAE for statistical feature analysis, along with handling long log data sequences. CoLog similarly learns long sequences of log records eﬀectively but transcends UMFLog by incorporating collaborative approach to capture interactions between modalities and utilizing MAL to address heterogeneity. LogMS [59] implements two-step model. multi-source information fusion-based LSTM for anomaly detection followed by GRU network for probability label estimation. Conversely, CoLogs uniﬁed framework for detecting abnormalities oﬀers streamlined and more comprehensive solution. MDFULog [45] addresses noise in log data and adopts an informer-based anomaly detection approach, whereas CoLogs modules, i.e., collaborative transformer and MAL, inherently mitigate noise. Finally, multi-feature fusion (MFF) [44] leverages late fusion for MFF-based anomaly detection by evaluating HTTP textual content, status code, and frequency features. Yet, CoLogs intermediate fusion mechanism signiﬁcantly enhances performance metrics. Collectively, the contributions of CoLog not only embody advancements found in existing methods but also amalgamate their strengths within cohesive, uniﬁed framework. Thus, CoLogs approach is not only multifaceted, addressing wide spectrum of challenges presented by log anomaly detection, but also showcases superior performance metrics compared to individual contributions of FastLogAD, pylogsentiment, RAGLog, UMFLog, LogMS, MDFULog, and MFF. The comparative analysis in Table 19 distinctly illustrates how CoLog stands at the forefront of innovation. CoLogs ability to eﬀectively encode log records utilizing various modalities, addressing heterogeneity through MAL, and demonstrating exceptional performance in detecting both point and collective abnormalities within uniﬁed architecture validates CoLog as an innovative approach in the ﬁeld of log anomaly detection."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce CoLog, novel framework for collaborative encoding and anomaly detection that utilizes various log modalities. CoLog employs supervised learning methodology, with its outcomes in log anomaly detection highlighting its scholarly signiﬁcance in establishing an upper performance limit for this domain. With adequate labeled data, CoLog develops standard of theoretical accuracy for evaluating unsupervised or semi-supervised approaches. Consequently, CoLog serves as reference baseline for forthcoming research trajectories, oﬀering comparative value and direction for the development of more eﬃcient models. Additionally, the proposed approach addresses the inherent heterogeneity of log data by implementing MAL, ensuring robust performance across multiple benchmarks. CoLogs ability to detect both point and collective anomalies within uniﬁed framework distinguishes it from conventional methods that focus solely on one type of anomaly. This comprehensive detection capability underpins the versatility and adaptability of CoLog, making it signiﬁcant advancement in the ﬁeld of log anomaly detection. The potential applications of CoLog are vast, spanning from enhancing security measures through more accurate anomaly detection in cybersecurity logs to improving operational eﬃciency by identifying system irregularities in real-time. Additionally, its robust performance Table 19 The comprehensive overview of contributions in existing literature compared to our study. Paper FastLogAD [77] Year pylogsentiment [38] 2020 Methodology Sequence-based unimodal unsupervised learning Semantic-based unimodal supervised learning Datasets Used 1) HDFS1 [12] 2) BlueGene/L 3) Thunderbird [132] 1) Spark 2) Honey5 3) Windows 4) Casper 5) Jhuisi 6) Nssal 7) Honey7 8) Zookeeper 9) Hadoop 10) BlueGene/L RAGLog [92] UMFLog [84] 2023 LLM-based unimodal unsupervised learning 1) BlueGene/L 2) Thunderbird Independent network-based multimodal unsupervised learning 1) HDFS 2) BlueGene/L 3) Thunderbird LogMS [59] 2024 Early fusion-based multimodal unsupervised & semi-supervised learning 1) HDFS 2) BlueGene/L MDFULog [45] MFF [44] 2023 Intermediate fusion-based multimodal supervised learning 1) HDFS 2) OpenStack [132] Late fusion-based multimodal supervised learning 1) LLSD2 [44] CoLog 2025 Intermediate fusion-based multimodal supervised learning 1) Spark 2) Honey5 3) Windows 4) Casper 5) Jhuisi 6) Nssal 7) Honey7 8) Zookeeper 9) Hadoop 10) BlueGene/L 1Hadoop Distributed File System Mean F1-score Key Contributions 94.18 99.14 89.00 99. 99.10 97.00 93.10 99.99 1) Eﬃcient utilization of normal data. 2) Innovative anomaly generation method. 1) Implements sentiment analysis for log anomaly detection. 2) Addressing class imbalance. 1) retrieval-augmented generation model that employs vector database to store normal log entries. 1) Employs dual-model architecture with BERT for semantic feature extraction and VAE for statistical feature analysis. 2) Handles long sequences of log data. 1) Employs two-step model. The ﬁrst step uses multi-source information fusion-based LSTM to detect anomalies by utilizing semantic, sequential, and quantitative data. Following that, probability label estimation-based GRU network is used. 1) Addresses noise in log data. 2) Informer-based anomaly detection. 1) Detects web scanning behavior by considering HTTP textual content, status code, and frequency features. 2) Employ late fusion MFF-based network to detect anomalies. 1) Encodes log records collaboratively according to various log modalities. 2) Employs MAL to address heterogeneity among modalities. 3) Outperforms state-of-the-art methods. 4) Detects point and collective abnormalities within uniﬁed framework. c e s n ﬁ p s I : 1 0 . 1 0 3 8 / 4 1 5 9 8 - 0 2 5 - 2 7 6 9 3 - 4 Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 under varying noise conditions demonstrates its reliability and eﬀectiveness in diverse environments."
        },
        {
            "title": "6.1 Limitations",
            "content": "Real-time application. The present study assesses CoLog in batch-processing mode. Real-time anomaly detection frequently entails supplementary constraints, like latency and limitations on computational resources. Examining and enhancing CoLogs performance in real-time settings is essential for eﬀective implementation. Log entries that are not understandable by humans. Certain log entries, especially those from operating systems, may be diﬃcult for humans to decipher, presenting diﬃculty for ﬁrst-time interpretation and labeling of such data. Adapting to evolving log structures. The logs develop over time with system updates, necessitating continual adaptation and retraining of the model to preserve validity. The Capacity for Noise Resilience. While CoLog exhibits robustness to varying noise levels, certain extreme scenarios or certain noise types may still substantially impair the performance."
        },
        {
            "title": "6.2 Future Works",
            "content": "Future work could explore the integration of CoLog with real-time monitoring systems to enable proactive anomaly management and automated responses to detected anomalies on industrial systems. Moreover, extending CoLogs application to other domains, such as ﬁnancial fraud detection or health monitoring, could further validate its eﬃcacy and adaptability. Since CoLog assumes moderately imbalanced dataset and stable log templates, which generally hold in benchmark datasets but may vary in dynamic production systems. In extreme imbalance scenarios (e.g., anomaly ratio 1%) or in environments with rapidly evolving templates, performance may degrade due to representation drift. To mitigate this, CoLogs balancing layer can be combined with adaptive resampling, online calibration, or few-shot ﬁne-tuning. We also note that continual learning extensions could further improve adaptability. Future work will explore these directions to ensure robustness under diverse operational conditions. Additionally, alternative modalities of log data, including quantitative information, may also be employed in the anomaly detection process. The development of CoLog for additional modalities is straightforward. Also, investigating adaptive mechanisms for better managing various types of noise could be beneﬁcial avenue. The promising performance and broad applicability of CoLog suggest that it holds considerable potential for driving advancements in multiple industries where accurate and timely anomaly detection is critical. By addressing current limitations in log analysis, CoLog paves the way for more sophisticated and comprehensive solutions to complex data challenges. Acknowledgements. We express our profound appreciation to Mahdi Asgharzadeh for his signiﬁcant contributions to the development of the Alarmif website. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-"
        },
        {
            "title": "Declarations",
            "content": "Funding. Not applicable. Conﬂict of interest/Competing interests. The authors declare that they have no conﬂicts of interest or competing interests that could have inﬂuenced the content, analysis, or conclusions of this work. Consent for publication. All authors have provided their consent for the publication of this work, aﬃrming their agreement with its content and its dissemination in the designated journal. Data availability. The datasets generated and/or analysed during the current study are available in the loghub repository at https://github.com/logpai/loghub and in the pylogsentiment repository at https://github.com/studiawan/pylogsentiment. Materials availability. The datasets, computing resources, algorithms, and libraries employed in this AI development can be obtained upon request, contingent upon relevant access policies and license agreements. Code availability. The papers implementation code is openly accessible on GitHub at https://github.com/NasirzadehMoh/CoLog, guaranteeing transparency and reproducibility for ongoing development and collaboration. Author contribution. Methodology, theoretical analysis, algorithm, implementation, experiments, and writing - Mohammad Nasirzadeh; Supervision, architecture design, and revision - Jafar Tahmoresnezhad; Theoretical analysis and architecture design - Parviz Rashidi-Khazaee."
        },
        {
            "title": "References",
            "content": "[1] Samariya, D., Thakkar, A.: comprehensive survey of anomaly detection algorithms. Annals of Data Science 10(3), 829850 (2023) https://doi.org/10.1007/ s40745-021-00362-9 [2] Zhao, X., Rodrigues, K., Luo, Y., Yuan, D., Stumm, M.: Non-Intrusive performance proﬁling for entire software stacks based on the ﬂow reconstruction principle. In: 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pp. 603618. USENIX Association, Savannah, GA (2016). https://www.usenix.org/conference/osdi16/technical-sessions/ presentation/zhao [3] Yu, X., Joshi, P., Xu, J., Jin, G., Zhang, H., Jiang, G.: Cloudseer: Workﬂow monitoring of cloud infrastructures via interleaved logs. SIGARCH Comput. Archit. News 44(2), 489502 (2016) https://doi.org/10.1145/2980024.2872407 [4] Lin, Q., Zhang, H., Lou, J.-G., Zhang, Y., Chen, X.: Log clustering based problem identiﬁcation for online service systems. In: Proceedings of the 38th International Conference on Software Engineering Companion. ICSE 16, pp. 102111. Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693Association for Computing Machinery, New York, NY, USA (2016). https://doi. org/10.1145/2889160.2889232 . https://doi.org/10.1145/2889160.2889232 [5] Roy, S., Konig, A.C., Dvorkin, I., Kumar, M.: Perfaugur: Robust diagnostics for performance anomalies in cloud services. In: 2015 IEEE 31st International Conference on Data Engineering, pp. 11671178 (2015). https://doi.org/10.1109/ ICDE.2015.7113365 [6] Oprea, A., Li, Z., Yen, T.-F., Chin, S.H., Alrwais, S.: Detection of early-stage enterprise infection by mining large-scale log data. In: 2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, pp. 4556 (2015). https://doi.org/10.1109/DSN.2015.14 [7] Beschastnikh, I., Brun, Y., Ernst, M.D., Krishnamurthy, A.: Inferring models of concurrent systems from logs of their behavior with csight. In: Proceedings of the 36th International Conference on Software Engineering. ICSE 2014, pp. 468479. Association for Computing Machinery, New York, NY, USA (2014). https://doi. org/10.1145/2568225.2568246 . https://doi.org/10.1145/2568225.2568246 [8] Yen, T.-F., Oprea, A., Onarlioglu, K., Leetham, T., Robertson, W., Juels, A., Kirda, E.: Beehive: large-scale log analysis for detecting suspicious activity in enterprise networks. In: Proceedings of the 29th Annual Computer Security Applications Conference. ACSAC 13, pp. 199208. Association for Computing Machinery, New York, NY, USA (2013). https://doi.org/10.1145/2523649. 2523670 . https://doi.org/10.1145/2523649.2523670 [9] Cinque, M., Cotroneo, D., Pecchia, A.: Event logs for the analysis of software failures: rule-based approach. IEEE Transactions on Software Engineering 39(6), 806821 (2013) https://doi.org/10.1109/TSE.2012. [10] Lou, J.-G., Fu, Q., Yang, S., Xu, Y., Li, J.: Mining invariants from console logs for system problem detection. In: 2010 USENIX Annual Technical Conference (USENIX ATC 10) (2010) [11] Lou, J.-G., Fu, Q., Yang, S., Li, J., Wu, B.: Mining program workﬂow from interleaved traces. In: Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD 10, pp. 613622. Association for Computing Machinery, New York, NY, USA (2010). https://doi. org/10.1145/1835804.1835883 . https://doi.org/10.1145/1835804.1835883 [12] Xu, W., Huang, L., Fox, A., Patterson, D., Jordan, M.I.: Detecting large-scale system problems by mining console logs. In: Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles. SOSP 09, pp. 117132. Association for Computing Machinery, New York, NY, USA (2009). https://doi. org/10.1145/1629575.1629587 . https://doi.org/10.1145/1629575.1629587 [13] Xu, W., Huang, L., Fox, A., Patterson, D., Jordan, M.: Online system problem Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693detection by mining patterns of console logs. In: 2009 Ninth IEEE International Conference on Data Mining, pp. 588597 (2009). https://doi.org/10.1109/ ICDM.2009.19 [14] Fu, Q., Lou, J.-G., Wang, Y., Li, J.: Execution anomaly detection in distributed systems through unstructured log analysis. In: 2009 Ninth IEEE International Conference on Data Mining, pp. 149158 (2009). https://doi.org/10.1109/ ICDM.2009.60 [15] Yamanishi, K., Maruyama, Y.: Dynamic syslog mining for network failure monitoring. In: Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining. KDD 05, pp. 499508. Association for Computing Machinery, New York, NY, USA (2005). https://doi.org/10.1145/ 1081870.1081927 . https://doi.org/10.1145/1081870.1081927 [16] Rouillard, J.P.: Real-time log ﬁle analysis using the simple event correlator (sec). In: LISA, vol. 4, pp. 133150 (2004) [17] Prewett, J.E.: Analyzing cluster log ﬁles using logsurfer. In: Proceedings of the 4th Annual Conference on Linux Clusters, pp. 112 (2003). Citeseer State College, PA, USA [18] Hansen, S.E., Atkins, E.T.: Automated system monitoring and notiﬁcation with swatch. In: LISA, vol. 93, pp. 145152 (1993). Monterey, CA [19] Memon, A.U., Cordy, J.R., Dean, T.: Log File Categorization and Anomaly"
        },
        {
            "title": "Analysis Using Grammar Inference",
            "content": "[20] Cui, T., Ma, S., Chen, Z., Xiao, T., Tao, S., Liu, Y., Zhang, S., Lin, D., Liu, C., Cai, Y., Meng, W., Sun, Y., Pei, D.: LogEval: Comprehensive Benchmark Suite for Large Language Models In Log Analysis (2024). https://arxiv.org/abs/ 2407.01896 [21] Du, Q., Zhao, L., Xu, J., Han, Y., Zhang, S.: Log-based anomaly detection with multi-head scaled dot-product attention mechanism. In: Strauss, C., Kotsis, G., Tjoa, A.M., Khalil, I. (eds.) Database and Expert Systems Applications, pp. 335347. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-864729 31 [22] Le, V.-H., Zhang, H.: Log-based anomaly detection with deep learning: how far are we? In: Proceedings of the 44th International Conference on Software Engineering. ICSE 22, pp. 13561367. Association for Computing Machinery, New York, NY, USA (2022). https://doi.org/10.1145/3510003.3510155 . https:// doi.org/10.1145/3510003.3510155 [23] Pithode, K., Patheja, P.S.: study on log anomaly detection using deep learning techniques. In: 2022 International Conference on Applied Artiﬁcial Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693Intelligence and Computing (ICAAIC), pp. 290298 (2022). https://doi.org/10. 1109/ICAAIC53929.2022.9793238 [24] Chen, Z., Liu, J., Gu, W., Su, Y., Lyu, M.R.: Experience Report: Deep Learningbased System Log Analysis for Anomaly Detection (2022). https://doi.org/10. 48550/arXiv.2107.05908 . https://arxiv.org/abs/2107.05908 [25] Cheng, X., Luo, D., Chen, X., Liu, L., Zhao, D., Yan, R.: Lift yourself up: Retrieval-augmented text generation with self-memory. In: Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., Levine, S. (eds.) Advances in Neural Information Processing Systems, vol. 36, pp. 4378043799. Curran Associates, Inc., ??? (2023) [26] Huang, S.-C., Pareek, A., Jensen, M., Lungren, M.P., Yeung, S., Chaudhari, A.S.: Self-supervised learning for medical image classiﬁcation: systematic review and implementation guidelines. NPJ Digital Medicine 6(1), 74 (2023) https://doi. org/10.1038/s41746-023-00811-0 [27] Jeong, J., Tian, K., Li, A., Hartung, S., Adithan, S., Behzadi, F., Calle, J., Osayande, D., Pohlen, M., Rajpurkar, P.: Multimodal image-text matching improves retrieval-based chest x-ray report generation. In: Oguz, I., Noble, J., Li, X., Styner, M., Baumgartner, C., Rusu, M., Heinmann, T., Kontos, D., Landman, B., Dawant, B. (eds.) Medical Imaging with Deep Learning. Proceedings of Machine Learning Research, vol. 227, pp. 978990. PMLR, ??? (2024). https:// proceedings.mlr.press/v227/jeong24a.html [28] Le, M., Vyas, A., Shi, B., Karrer, B., Sari, L., Moritz, R., Williamson, M., Manohar, V., Adi, Y., Mahadeokar, J., Hsu, W.-N.: Voicebox: Text-guided multilingual universal speech generation at scale. In: Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., Levine, S. (eds.) Advances in Neural Information Processing Systems, vol. 36, pp. 1400514034. Curran Associates, Inc., ??? (2023) [29] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1001210022 (2021). https://doi.org/10.48550/arXiv.2103.14030 [30] Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., Hu, H.: Video swin transformer. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 32023211 (2022). https://doi.org/10. 48550/arXiv.2106.13230 [31] Saleki, S., Tahmoresnezhad, J.: Agry: comprehensive framework for plant diseases classiﬁcation via pretrained eﬃcientnet and convolutional neural networks for precision agriculture. Multimedia Tools and Applications 83(24), 6481364851 (2024) Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 [32] Tahmoresnezhad, J., Hashemi, S.: Visual domain adaptation via transfer feature learning. Knowledge and information systems 50(2), 585605 (2017) [33] Khorshidpour, Z., Tahmoresnezhad, J., Hashemi, S., Hamzeh, A.: Domain invariant feature extraction against evasion attack. International Journal of Machine Learning and Cybernetics 9(12), 20932104 (2018) [34] Khodadadi, M., Tahmoresnezhad, J.: HyMo: Vulnerability Detection in Smart Contracts using Novel Multi-Modal Hybrid Model (2023). https://arxiv.org/ abs/2304.13103 [35] Almodovar, C., Sabrina, F., Karimi, S., Azad, S.: Logﬁt: Log anomaly detection using ﬁne-tuned language models. IEEE Transactions on Network and Service Management 21(2), 17151723 (2024) https://doi.org/10.1109/TNSM. 2024.3358730 [36] Du, M., Li, F., Zheng, G., Srikumar, V.: Deeplog: Anomaly detection and diagnosis from system logs through deep learning. In: Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. CCS 17, pp. 12851298. Association for Computing Machinery, New York, NY, USA (2017). https://doi.org/10.1145/3133956.3134015 . https://doi.org/ 10.1145/3133956.3134015 [37] Guo, H., Yang, J., Liu, J., Bai, J., Wang, B., Li, Z., Zheng, T., Zhang, B., Peng, J., Tian, Q.: Logformer: pre-train and tuning pipeline for log anomaly detection. Proceedings of the AAAI Conference on Artiﬁcial Intelligence 38(1), 135143 (2024) https://doi.org/10.1609/aaai.v38i1. [38] Studiawan, H., Sohel, F., Payne, C.: Anomaly detection in operating system logs with deep learning-based sentiment analysis. IEEE Transactions on Dependable and Secure Computing 18(5), 21362148 (2021) https://doi.org/10.1109/TDSC. 2020.3037903 [39] Wang, S., Jiang, R., Wang, Z., Zhou, Y.: Deep Learning-based Anomaly Detection and Log Analysis for Computer Networks (2024). https://doi.org/10.48550/ arXiv.2407.05639 . https://arxiv.org/abs/2407.05639 [40] Landauer, M., Onder, S., Skopik, F., Wurzenberger, M.: Deep learning for anomaly detection in log data: survey. Machine Learning with Applications 12, 100470 (2023) https://doi.org/10.1016/j.mlwa.2023.100470 [41] Li, H., Li, Y.: Logspy: System log anomaly detection for distributed systems. In: 2020 International Conference on Artiﬁcial Intelligence and Computer Engineering (ICAICE), pp. 347352 (2020). https://doi.org/10.1109/ICAICE51518. 2020.00073 [42] Wang, Q., Zhang, X., Wang, X., Cao, Z.: Log sequence anomaly detection Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 method based on contrastive adversarial training and dual feature extraction. Entropy 24(1) (2022) https://doi.org/10.3390/e24010069 [43] Qi, J., Luan, Z., Huang, S., Fung, C., Yang, H., Li, H., Zhu, D., Qian, D.: Logencoder: Log-based contrastive representation learning for anomaly detection. IEEE Transactions on Network and Service Management 20(2), 13781391 (2023) https://doi.org/10.1109/TNSM.2023.3239522 [44] Zhao, Q., Liu, B., Wu, H., Zhang, Z., Hu, R., Hua, H., Zhang, C.: multifeature fusion method for web scanning behavior detection in online web logs. In: 2023 4th International Conference on Computers and Artiﬁcial Intelligence Technology (CAIT), pp. 246252 (2023). https://doi.org/10.1109/CAIT59945. 2023.10469208 [45] Li, M., Sun, M., Li, G., Han, D., Zhou, M.: Mdfulog: Multi-feature deep fusion of unstable log anomaly detection model. Applied Sciences 13(4) (2023) https:// doi.org/10.3390/app13042237 [46] Lu, S., Wei, X., Li, Y., Wang, L.: Detecting anomaly in big data In: 2018 IEEE 16th system logs using convolutional neural network. Intl Conf 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech), pp. 151158 (2018). https://doi.org/10.1109/DASC/PiCom/DataCom/CyberSciTec.2018. on Dependable, Autonomic and Secure Computing, [47] Wang, Z., Tian, J., Fang, H., Chen, L., Qin, J.: Lightlog: lightweight temporal convolutional network for log anomaly detection on the edge. Computer Networks 203, 108616 (2022) https://doi.org/10.1016/j.comnet.2021.108616 [48] Zhang, L., Li, W., Zhang, Z., Lu, Q., Hou, C., Hu, P., Gui, T., Lu, S.: Logattn: Unsupervised log anomaly detection with an autoencoder based attention mechanism. In: Qiu, H., Zhang, C., Fei, Z., Qiu, M., Kung, S.-Y. (eds.) Knowledge Science, Engineering and Management, pp. 222235. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-82153-1 19 [49] Hashemi, S., Mantyla, M.: Onelog: towards end-to-end software log anomaly detection. Automated Software Engineering 31(2) (2024) https://doi.org/10. 1007/s10515-024-00428-x [50] Xiao, C., Huang, J., Wu, W.: Detecting anomalies in cluster system using hybrid deep learning model. In: Shen, H., Sang, Y. (eds.) Parallel Architectures, Algorithms and Programming, pp. 393404. Springer, Singapore (2020). https://doi. org/10.1007/978-981-15-2767-8 [51] Wang, P., Zhang, X., Cao, Z.: Log anomaly detection based on semantic features Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 and topic features. In: Tari, Z., Li, K., Wu, H. (eds.) Algorithms and Architectures for Parallel Processing, pp. 407427. Springer, Singapore (2024). https:// doi.org/10.1007/978-981-97-0808-6 24 [52] Farzad, A.: Log message anomaly detection with oversampling. International Journal of Artiﬁcial Intelligence and Applications (IJAIA) 11(4) (2020) [53] Farzad, A., Gulliver, T.A.: Log Message Anomaly Detection and Classiﬁcation Using Auto-B/LSTM and Auto-GRU (2021). https://doi.org/10.48550/arXiv. 1911.08744 . https://arxiv.org/abs/1911.08744 [54] Gu, S., Chu, Y., Zhang, W., Liu, P., Yin, Q., Li, Q.: Research on system log anomaly detection combining two-way slice gru and ga-attention mechanism. In: 2021 4th International Conference on Artiﬁcial Intelligence and Big Data (ICAIBD), pp. 577583 (2021). https://doi.org/10.1109/ICAIBD51990. 2021.9459087 [55] Li, X., Chen, P., Jing, L., He, Z., Yu, G.: Swisslog: Robust and uniﬁed deep learning based log anomaly detection for diverse faults. In: 2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE), pp. 92 103 (2020). https://doi.org/10.1109/ISSRE5003.2020.00018 [56] Wang, Z., Chen, Z., Ni, J., Liu, H., Chen, H., Tang, J.: Multi-scale one-class recurrent neural networks for discrete event sequence anomaly detection. In: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. KDD 21, pp. 37263734. Association for Computing Machinery, New York, NY, USA (2021). https://doi.org/10.1145/3447548.3467125 . https:// doi.org/10.1145/3447548.3467125 [57] Yang, L., Chen, J., Wang, Z., Wang, W., Jiang, J., Dong, X., Zhang, W.: Semi-supervised log-based anomaly detection via probabilistic label estimation. In: 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pp. 14481460 (2021). https://doi.org/10.1109/ICSE43902.2021.00130 [58] Zhang, D., Dai, D., Han, R., Zheng, M.: Sentilog: Anomaly detecting on parallel ﬁle systems via log-based sentiment analysis. In: Proceedings of the 13th ACM Workshop on Hot Topics in Storage and File Systems. HotStorage 21, pp. 86 93. Association for Computing Machinery, New York, NY, USA (2021). https:// doi.org/10.1145/3465332.3470873 . https://doi.org/10.1145/3465332. [59] Yu, Z., Yang, S., Li, Z., Li, L., Luo, H., Yang, F.: Logms: multi-stage log anomaly detection method based on multi-source information fusion and probability label estimation. Frontiers in Physics 12 (2024) https://doi.org/10.3389/ fphy.2024.1401857 [60] Zhang, C., Wang, X., Zhang, H., Zhang, J., Zhang, H., Liu, C., Han, P.: Layerlog: Log sequence anomaly detection based on hierarchical semantics. Applied Soft Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 Computing 132, 109860 (2023) https://doi.org/10.1016/j.asoc.2022.109860 [61] Fu, Y., Liang, K., Xu, J.: Mlog: Mogriﬁer lstm-based log anomaly detection approach using semantic representation. IEEE Transactions on Services Computing 16(5), 35373549 (2023) https://doi.org/10.1109/TSC.2023. [62] Yu, D., Hou, X., Li, C., Lv, Q., Wang, Y., Li, N.: Anomaly detection in unstructured logs using attention-based bi-lstm network. In: 2021 7th IEEE International Conference on Network Intelligence and Digital Content (IC-NIDC), pp. 403407 (2021). https://doi.org/10.1109/IC-NIDC54101.2021.9660476 [63] Zhang, X., Xu, Y., Lin, Q., Qiao, B., Zhang, H., Dang, Y., Xie, C., Yang, X., Cheng, Q., Li, Z., Chen, J., He, X., Yao, R., Lou, J.-G., Chintalapati, M., Shen, F., Zhang, D.: Robust log-based anomaly detection on unstable log data. In: Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE 2019, pp. 807817. Association for Computing Machinery, New York, NY, USA (2019). https://doi.org/10.1145/3338906.3338931 . https://doi.org/ 10.1145/3338906.3338931 [64] Yang, R., Qu, D., Gao, Y., Qian, Y., Tang, Y.: nlsalog: An anomaly detection framework for log sequence in security management. IEEE Access 7, 181152 181164 (2019) https://doi.org/10.1109/ACCESS.2019.2953981 [65] Niu, W., Liao, X., Huang, S., Li, Y., Zhang, X., Li, B.: robust wide & deep learning framework for log-based anomaly detection. Applied Soft Computing 153, 111314 (2024) https://doi.org/10.1016/j.asoc.2024.111314 [66] Yang, H., Lin, F., Chai, Y., Qie, K., Lin, W., Wang, Y., Zhang, C., Guo, M.: An anomaly detection algorithm for logs based on self-attention mechanism and bigru model. In: Jia, Y., Zhang, W., Fu, Y., Wang, J. (eds.) Proceedings of 2023 Chinese Intelligent Systems Conference, pp. 877888. Springer, Singapore (2023). https://doi.org/10.1007/978-981-99-6847-3 76 [67] Niu, W., Li, Z., He, Z., Wang, A., Li, B., Zhang, X.: Fsmﬂog: Discovering anomalous logs combining full semantic information and multifeature fusion. IEEE Internet of Things Journal 11(3), 44424453 (2024) https://doi.org/10.1109/ JIOT.2023. [68] Otomo, K., Kobayashi, S., Fukuda, K., Esaki, H.: Latent variable based anomaly detection in network system logs. IEICE TRANSACTIONS on Information and Systems 102(9), 16441652 (2019) https://doi.org/10.1587/transinf. 2018OFP0007 [69] Qian, Y., Ying, S., Wang, B.: Anomaly detection in distributed systems via variational autoencoders. In: 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC), pp. 28222829 (2020). https://doi.org/10.1109/ Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 SMC42975.2020.9283078 [70] Wadekar, A., Gupta, T., Vijan, R., Kazi, F.: Hybrid cae-vae for unsupervised anomaly detection in log ﬁle systems. In: 2019 10th International Conference on Computing, Communication and Networking Technologies (ICCCNT), pp. 17 (2019). https://doi.org/10.1109/ICCCNT45670.2019. [71] Wang, X., Cao, Q., Wang, Q., Cao, Z., Zhang, X., Wang, P.: Robust log anomaly detection based on contrastive learning and multi-scale mass. The Journal of Supercomputing 78(16), 1749117512 (2022) https://doi.org/10.1007/s11227022-04508-1 [72] Catillo, M., Pecchia, A., Villano, U.: Autolog: Anomaly detection by deep autoencoding of system logs. Expert Systems with Applications 191, 116263 (2022) https://doi.org/10.1016/j.eswa.2021.116263 [73] Han, X., Yuan, S.: Unsupervised cross-system log anomaly detection via domain adaptation. In: Proceedings of the 30th ACM International Conference on Information & Knowledge Management. CIKM 21, pp. 30683072. Association for Computing Machinery, New York, NY, USA (2021). https://doi.org/10.1145/ 3459637.3482209 . https://doi.org/10.1145/3459637.3482209 [74] Xia, B., Bai, Y., Yin, J., Li, Y., Xu, J.: Loggan: log-level generative adversarial network for anomaly detection using permutation event modeling. Information Systems Frontiers 23, 285298 (2021) https://doi.org/10.1007/s10796-02010026-3 [75] Zhao, Z., Niu, W., Zhang, X., Zhang, R., Yu, Z., Huang, C.: Trine: Syslog anomaly detection with three transformer encoders in one generative adversarial network. Applied Intelligence 52(8), 88108819 (2022) https://doi.org/10.1007/ s10489-021-02863-9 [76] Yamanaka, Y., Takahashi, T., Minami, T., Nakajima, Y.: LogELECTRA: Selfsupervised Anomaly Detection for Unstructured Logs (2024). https://doi.org/ 10.48550/arXiv.2402.10397 . https://arxiv.org/abs/2402. [77] Lin, Y., Deng, H., Li, X.: FastLogAD: Log Anomaly Detection with MaskGuided Pseudo Anomaly Generation and Discrimination (2024). https://doi. org/10.48550/arXiv.2404.08750 . https://arxiv.org/abs/2404.08750 [78] Qi, J., Luan, Z., Huang, S., Wang, Y., Fung, C., Yang, H., Qian, D.: Adanomaly: Adaptive anomaly detection for system logs with adversarial learning. In: NOMS 2022-2022 IEEE/IFIP Network Operations and Management Symposium, pp. 15 (2022). https://doi.org/10.1109/NOMS54207.2022.9789917 [79] Guo, H., Yuan, S., Wu, X.: Logbert: Log anomaly detection via bert. In: 2021 International Joint Conference on Neural Networks (IJCNN), pp. 18 (2021). Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 https://doi.org/10.1109/IJCNN52387.2021. [80] Nedelkoski, S., Bogatinovski, J., Acker, A., Cardoso, J., Kao, O.: Self-attentive classiﬁcation-based anomaly detection in unstructured logs. In: 2020 IEEE International Conference on Data Mining (ICDM), pp. 11961201 (2020). https:// doi.org/10.1109/ICDM50108.2020.00148 [81] Zang, R., Guo, H., Yang, J., Liu, J., Li, Z., Zheng, T., Shi, X., Zheng, L., Zhang, B.: MLAD: Uniﬁed Model for Multi-system Log Anomaly Detection (2024). https://doi.org/10.48550/arXiv.2401.07655 . https://arxiv.org/abs/2401.07655 [82] Wittkopp, T., Acker, A., Nedelkoski, S., Bogatinovski, J., Scheinert, D., Fan, W., Kao, O.: A2Log: Attentive Augmented Log Anomaly Detection (2021). https:// doi.org/10.48550/arXiv.2109.09537 . https://arxiv.org/abs/2109.09537 [83] Tan, X., Han, N., Lu, S., Chen, W., Wang, D.: Semlog: semantics-based approach for anomaly detection in big data system logs. In: 2023 IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS), pp. 11991206 (2023). https://doi.org/10.1109/ICPADS60453.2023.00174 [84] He, S., Deng, T., Chen, B., Sherratt, R.S., Wang, J.: Unsupervised log anomaly detection method based on multi-feature. Computers, Materials & Continua 76(1) (2023) https://doi.org/10.32604/cmc.2023.037392 [85] Han, C., Guan, B., Li, T., Kang, D., Qin, J., Wu, Y.: Few-shot log anomaly detection based on matching networks. IEEE Transactions on Network and Service Management 21(3), 29092925 (2024) https://doi.org/10.1109/TNSM. 2024. [86] Zhang, L., Jia, T., Jia, M., Li, Y., Yang, Y., Wu, Z.: Multivariate logbased anomaly detection for distributed database. In: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. KDD 24, pp. 42564267. Association for Computing Machinery, New York, NY, USA (2024). https://doi.org/10.1145/3637528.3671725 . https://doi.org/ 10.1145/3637528.3671725 [87] Ede, T.v., Aghakhani, H., Spahn, N., Bortolameotti, R., Cova, M., Continella, A., Steen, M.v., Peter, A., Kruegel, C., Vigna, G.: Deepcase: Semi-supervised contextual analysis of security events. In: 2022 IEEE Symposium on Security and Privacy (SP), pp. 522539 (2022). https://doi.org/10.1109/SP46214.2022. 9833671 [88] Li, X., Niu, W., Zhang, X., Zhang, R., Yu, Z., Li, Z.: Improving performance of log anomaly detection with semantic and time features based on bilstm-attention. In: 2021 2nd International Conference on Electronics, Communications and Information Technology (CECIT), pp. 661666 (2021). https:// doi.org/10.1109/CECIT53797.2021.00121 Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 [89] Guo, Y., Wen, Y., Jiang, C., Lian, Y., Wan, Y.: Detecting Log Anomalies with Multi-Head Attention (LAMA) (2021). https://doi.org/10.48550/arXiv. 2101.02392 . https://arxiv.org/abs/2101. [90] Wan, Y., Liu, Y., Wang, D., Wen, Y.: Glad-paw: Graph-based log anomaly detection by position aware weighted graph attention network. In: Karlapalem, K., Cheng, H., Ramakrishnan, N., Agrawal, R.K., Reddy, P.K., Srivastava, J., Chakraborty, T. (eds.) Advances in Knowledge Discovery and Data Mining, pp. 6677. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-75762-5 6 [91] Decker, L., Leite, D., Viola, F., Bonacorsi, D.: Comparison of evolving granular classiﬁers applied to anomaly detection for predictive maintenance in computing centers. In: 2020 IEEE Conference on Evolving and Adaptive Intelligent Systems (EAIS), pp. 18 (2020). https://doi.org/10.1109/EAIS48028.2020.9122779 [92] Pan, J., Liang, W.S., Yidi, Y.: Raglog: Log anomaly detection using retrieval augmented generation. In: 2024 IEEE World Forum on Public Safety Technology (WFPST), pp. 169174 (2024). https://doi.org/10.1109/WFPST58552. 2024.00034 [93] Liu, Y., Tao, S., Meng, W., Wang, J., Ma, W., Chen, Y., Zhao, Y., Yang, H., Jiang, Y.: Interpretable online log analysis using large language models with prompt strategies. In: Proceedings of the 32nd IEEE/ACM International Conference on Program Comprehension. ICPC 24, pp. 3546. Association for Computing Machinery, New York, NY, USA (2024). https://doi.org/10.1145/ 3643916.3644408 . https://doi.org/10.1145/3643916.3644408 [94] Liu, Z., Zhou, B., Chu, D., Sun, Y., Meng, L.: Modality translation-based multimodal sentiment analysis under uncertain missing modalities. Information Fusion 101, 101973 (2024) https://doi.org/10.1016/j.inﬀus.2023.101973 [95] Deng, Y., Li, Y., Xian, S., Li, L., Qiu, H.: Mual: Enhancing multimodal sentiment analysis with cross-modal attention and diﬀerence loss. International Journal of Multimedia Information Retrieval 13(3), 31 (2024) https://doi.org/10.1007/ s13735-024-00340-w [96] Ahuja, G., Alaei, A., Pal, U.: new multimodal sentiment analysis for images containing textual information. Multimedia Tools and Applications, 130 (2024) https://doi.org/10.1007/s11042-024-19999-8 [97] Wang, D., Guo, X., Tian, Y., Liu, J., He, L., Luo, X.: Tetfn: text enhanced transformer fusion network for multimodal sentiment analysis. Pattern Recognition 136, 109259 (2023) https://doi.org/10.1016/j.patcog.2022.109259 [98] Li, Z., Guo, Q., Feng, C., Deng, L., Zhang, Q., Zhang, J., Wang, F., Sun, Q.: Multimodal sentiment analysis based on interactive transformer and soft mapping. Wireless Communications and Mobile Computing 2022(1), 6243347 (2022) Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 https://doi.org/10.1155/2022/ [99] Hu, G., Lin, T.-E., Zhao, Y., Lu, G., Wu, Y., Li, Y.: UniMSE: Towards Uniﬁed Multimodal Sentiment Analysis and Emotion Recognition (2022). https://doi. org/10.48550/arXiv.2211.11256 . https://arxiv.org/abs/2211.11256 [100] Wang, Z., Wan, Z., Wan, X.: Transmodality: An end2end fusion method with transformer for multimodal sentiment analysis. In: Proceedings of The Web Conference 2020. WWW 20, pp. 25142520. Association for Computing Machinery, New York, NY, USA (2020). https://doi.org/10.1145/3366423.3380000 . https:// doi.org/10.1145/3366423.3380000 [101] Delbrouck, J.-B., Tits, N., Brousmiche, M., Dupont, S.: transformer-based joint-encoding for emotion recognition and sentiment analysis. In: Second GrandChallenge and Workshop on Multimodal Language (Challenge-HML). Association for Computational Linguistics, ??? (2020). https://doi.org/10.18653/v1/ 2020.challengehml-1.1 . http://dx.doi.org/10.18653/v1/2020.challengehml-1.1 [102] Zadeh, A., Chen, M., Poria, S., Cambria, E., Morency, L.-P.: Tensor Fusion Network for Multimodal Sentiment Analysis (2017). https://doi.org/10.48550/ arXiv.1707.07250 . https://arxiv.org/abs/1707.07250 [103] Chen, M., Wang, S., Liang, P.P., Baltruˇsaitis, T., Zadeh, A., Morency, L.-P.: Multimodal sentiment analysis with word-level fusion and reinforcement learning. In: Proceedings of the 19th ACM International Conference on Multimodal Interaction. ICMI 17, pp. 163171. Association for Computing Machinery, New York, NY, USA (2017). https://doi.org/10.1145/3136755.3136801 . https://doi. org/10.1145/3136755.3136801 [104] Yu, Z., Yu, J., Cui, Y., Tao, D., Tian, Q.: Deep modular co-attention networks for visual question answering. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019) [105] Li, Z., Xiang, Z., Gong, W., Wang, H.: Uniﬁed model for collective and point anomaly detection using stacked temporal convolution networks. Applied Intelligence 52(3), 31183131 (2022) https://doi.org/10.1007/s10489-021-02559-0 [106] Liu, H., Zhang, H.-s., Tang, Y., Yao, Y.: uniﬁed detection approach for point and subsequence anomaly data from train axle temperature sensors. IEEE Sensors Journal 23(20), 2477224786 (2023) https://doi.org/10.1109/JSEN.2023. 3307623 [107] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., L., Polosukhin, I.: Attention is all you need. Advances in neural Kaiser, information processing systems 30 (2017) Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693- [108] Chalapathy, R., Chawla, S.: Deep Learning for Anomaly Detection: Survey (2019). https://doi.org/10.48550/arXiv.1901.03407 . https://arxiv.org/abs/ 1901.03407 [109] Liang, Y., Zhang, Y., Xiong, H., Sahoo, R.: Failure prediction in ibm bluegene/l event logs. In: Seventh IEEE International Conference on Data Mining (ICDM 2007), pp. 583588 (2007). https://doi.org/10.1109/ICDM.2007.46 [110] Chen, M., Zheng, A.X., Lloyd, J., Jordan, M.I., Brewer, E.: Failure diagnosis using decision trees. In: International Conference on Autonomic Computing, 2004. Proceedings., pp. 3643 (2004). https://doi.org/10.1109/ICAC.2004. 1301345 [111] He, S., Zhu, J., He, P., Lyu, M.R.: Experience report: System log analysis for anomaly detection. In: 2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE), pp. 207218 (2016). https://doi.org/10.1109/ ISSRE.2016.21 [112] Reimers, N., Gurevych, I.: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (2019). https://doi.org/10.48550/arXiv.1908.10084 . https:// arxiv.org/abs/1908.10084 [113] Zhang, L., Jia, T., Tan, X., Huang, X., Jia, M., Liu, H., Wu, Z., Li, Y.: E-log: Fine-grained elastic log-based anomaly detection and diagnosis for databases. IEEE Transactions on Services Computing (2025) [114] Qiu, K., Zhang, Y., Feng, Y., Chen, F.: Loganomex: An unsupervised log anomaly detection method based on electra-dp and gated bilinear neural networks. Journal of Network and Systems Management 33(2), 129 (2025) [115] Qiu, K., Yan, M., Luo, T., Chen, F.: Fedaware: distributed iot intrusion detection method based on fractal shrinking autoencoder. Journal of King Saud University Computer and Information Sciences 37(7), 121 (2025) [116] Kingma, D.P., Ba, J.: Adam: Method for Stochastic Optimization (2017). https://doi.org/10.48550/arXiv.1412.6980 . https://arxiv.org/abs/1412.6980 [117] Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X., Gao, J.: Large Language Models: Survey (2024). https://doi.org/10.48550/ arXiv.2402.06196 . https://arxiv.org/abs/2402.06196 [118] Al-Zoghby, A.M., Al-Awadly, E.M.K., Ebada, A.I., Awad, W.A.: Overview of multimodal machine learning. ACM Transactions on Asian and Low-Resource Language Information Processing 24(1), 120 (2025) [119] Liang, P.P., Zadeh, A., Morency, L.-P.: Foundations & trends in multimodal machine learning: Principles, challenges, and open questions. ACM Computing Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 Surveys 56(10), 142 (2024) [120] Qiu, K., Zhang, Y., Zhao, J., Zhang, S., Wang, Q., Chen, F.: multimodal sentiment analysis approach based on joint chained interactive attention mechanism. Electronics 13(10), 1922 (2024) [121] Su, Y., Tan, Y., An, S., Xing, M., Feng, Z.: Semantic-driven dual consistency learning for weakly supervised video anomaly detection. Pattern Recognition 157, 110898 (2025) https://doi.org/10.1016/j.patcog.2024. [122] Su, Y., Li, J., An, S., Xing, M., Feng, Z.: Federated weakly-supervised video anomaly detection with mixture of local-to-global experts. Information Fusion 123, 103256 (2025) https://doi.org/10.1016/j.inﬀus.2025.103256 [123] Studiawan, H., Sohel, F., Payne, C.: Automatic log parser to support forensic analysis. In: 16th Australian Digital Forensics Conference (2018) [124] He, P., Zhu, J., Zheng, Z., Lyu, M.R.: Drain: An online log parsing approach with ﬁxed depth tree. In: 2017 IEEE International Conference on Web Services (ICWS), pp. 3340 (2017). https://doi.org/10.1109/ICWS.2017.13 [125] Mikolov, T., Chen, K., Corrado, G., Dean, J.: Eﬃcient Estimation of Word Representations in Vector Space (2013). https://doi.org/10.48550/arXiv.1301. 3781 . https://arxiv.org/abs/1301.3781 [126] Nguyen, H.M., Cooper, E.W., Kamei, K.: Borderline over-sampling for imbalanced data classiﬁcation. International Journal of Knowledge Engineering and Soft Data Paradigms 3(1), 421 (2011) [127] He, H., Bai, Y., Garcia, E.A., Li, S.: Adasyn: Adaptive synthetic sampling approach for imbalanced learning. In: 2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence), pp. 13221328 (2008). https://doi.org/10.1109/IJCNN.2008.4633969 [128] Mani, I., Zhang, I.: knn approach to unbalanced data distributions: case study involving information extraction. In: Proceedings of Workshop on Learning from Imbalanced Datasets, vol. 126, pp. 17 (2003). ICML United States [129] Ivan, T.: An experiment with the edited nearest-nieghbor rule. I.E.E.E. TRANS. SYST. MAN CYBERN.; U.S.A.; DA. 1976; VOL. 6; NO 6; PP. 448-452; BIBL. 3 REF. (1976) [130] Seiﬀert, C., Khoshgoftaar, T.M., Van Hulse, J., Napolitano, A.: Rusboost: hybrid approach to alleviating class imbalance. IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40(1), 185197 (2010) https://doi.org/10.1109/TSMCA.2009.2029559 Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693- [131] Hido, S., Kashima, H., Takahashi, Y.: Roughly balanced bagging for imbalanced data. Statistical Analysis and Data Mining: The ASA Data Science Journal 2(5-6), 412426 (2009) https://doi.org/10.1002/sam.10061 [132] Zhu, J., He, S., He, P., Liu, J., Lyu, M.R.: Loghub: large collection of system log datasets for ai-driven log analytics. In: 2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE), pp. 355366 (2023). https://doi.org/10.1109/ISSRE59848.2023.00071 [133] Marty, R., Chuvakin, A., Tricaud, S.: Challenge 5 of the Honeynet project forensic challenge 2010-Log mysteries (2010). https://www.honeynet.org/challenges/ forensic-challenge-5-log-mysteries/ [134] Garﬁnkel, S.: nps-2009-casper-rw: An ext3 ﬁle system from bootable usb. Retrieved August 21, 2017 (2009) [135] Casey, E., Richard III, G.G.: Dfrws forensic challenge 2009. Retrieved August 21, 2017 (2009) [136] Arcas, G., Gonzales, H., Cheng, J.: Challenge 7 of the honeynet project forensic challenge 2011-forensic analysis of compromised server. Retrieved August 21, 2017 (2011) [137] Lin, Q., Zhang, H., Lou, J.-G., Zhang, Y., Chen, X.: Log clustering based problem identiﬁcation for online service systems. In: Proceedings of the 38th International Conference on Software Engineering Companion. ICSE 16, pp. 102111. Association for Computing Machinery, New York, NY, USA (2016). https://doi. org/10.1145/2889160.2889232 . https://doi.org/10.1145/2889160.2889232 [138] Oliner, A., Stearley, J.: What supercomputers say: study of ﬁve system logs. In: 37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN07), pp. 575584 (2007). https://doi.org/10.1109/DSN.2007. 103 [139] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.: Pytorch: An imperative style, high-performance deep learning library. In: Wallach, H., Larochelle, H., Beygelzimer, A., Alche-Buc, F., Fox, E., Garnett, R. (eds.) Advances in Neural Information Processing Systems, vol. 32. Curran Associates, Inc., ??? (2019) [140] Lemaˆıtre, G., Nogueira, F., Aridas, C.K.: Imbalanced-learn: python toolbox to tackle the curse of imbalanced datasets in machine learning. Journal of Machine Learning Research 18(17), 15 (2017) [141] Liaw, R., Liang, E., Nishihara, R., Moritz, P., Gonzalez, J.E., Stoica, I.: Tune: Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 Research Platform for Distributed Model Selection and Training (2018). https:// doi.org/10.48550/arXiv.1807.05118 . https://arxiv.org/abs/1807.05118 [142] Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et al.: Scikit-learn: Machine learning in python. the Journal of machine Learning research 12, 2825 2830 (2011) [143] Bodik, P., Goldszmidt, M., Fox, A., Woodard, D.B., Andersen, H.: Fingerprinting the datacenter: automated classiﬁcation of performance crises. In: Proceedings of the 5th European Conference on Computer Systems. EuroSys 10, pp. 111124. Association for Computing Machinery, New York, NY, USA (2010). https://doi.org/10.1145/1755913.1755926 . https://doi.org/10. 1145/1755913. [144] Liu, F.T., Ting, K.M., Zhou, Z.-H.: Isolation forest. In: 2008 Eighth IEEE International Conference on Data Mining, pp. 413422 (2008). https://doi.org/10. 1109/ICDM.2008.17 [145] Arnold, C., Biedebach, L., Kupfer, A., Neunhoeﬀer, M.: The role of hyperparameters in machine learning models and how to tune them. Political Science Research and Methods 12(4), 841848 (2024) https://doi.org/10.1017/psrm. 2023.61 [146] Yang, L., Shami, A.: On hyperparameter optimization of machine learning algorithms: Theory and practice. Neurocomputing 415, 295316 (2020) https://doi. org/10.1016/j.neucom.2020.07.061 [147] Li, Z., Kamnitsas, K., Glocker, B.: Analyzing overﬁtting under class imbalance in neural networks for image segmentation. IEEE Transactions on Medical Imaging 40(3), 10651077 (2021) https://doi.org/10.1109/TMI.2020.3046692 [148] Bichri, H., Chergui, A., Hain, M.: Investigating the impact of train/test split ratio on the performance of pre-trained models with custom datasets. International Journal of Advanced Computer Science & Applications 15(2) (2024) [149] Chen, W., Yang, K., Yu, Z., Shi, Y., Chen, C.P.: survey on imbalanced learning: latest research, applications and future directions. Artiﬁcial Intelligence Review 57(6), 137 (2024) https://doi.org/10.1007/s10462-024-10759- [150] Johnson, J.M., Khoshgoftaar, T.M.: Survey on deep learning with class imbalance. Journal of big data 6(1), 154 (2019) https://doi.org/10.1186/s40537-0190192-5 [151] Hart, P.: The condensed nearest neighbor rule (corresp.). IEEE transactions on information theory 14(3), 515516 (1968) Accepted in scientiﬁc reports DOI: 10.1038/s41598-025-27693-4 [152] Laurikkala, J.: Improving identiﬁcation of diﬃcult small classes by balancing class distribution. In: Quaglini, S., Barahona, P., Andreassen, S. (eds.) Artiﬁcial Intelligence in Medicine, pp. 6366. Springer, Berlin, Heidelberg (2001). https:// doi.org/10.1007/3-540-48229-6 [153] Kubat, M., Matwin, S., et al.: Addressing the curse of imbalanced training sets: one-sided selection. In: Icml, vol. 97, p. 179 (1997). Citeseer"
        }
    ],
    "affiliations": [
        "Urmia University of Technology"
    ]
}