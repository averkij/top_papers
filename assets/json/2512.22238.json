{
    "paper_title": "Masking Teacher and Reinforcing Student for Distilling Vision-Language Models",
    "authors": [
        "Byung-Kwan Lee",
        "Yu-Chiang Frank Wang",
        "Ryo Hachiuma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large-scale vision-language models (VLMs) have recently achieved remarkable multimodal understanding, but their massive size makes them impractical for deployment on mobile or edge devices. This raises the need for compact yet capable VLMs that can efficiently learn from powerful large teachers. However, distilling knowledge from a large teacher to a small student remains challenging due to their large size gap: the student often fails to reproduce the teacher's complex, high-dimensional representations, leading to unstable learning and degraded performance. To address this, we propose Masters (Masking Teacher and Reinforcing Student), a mask-progressive reinforcement learning (RL) distillation framework. Masters first masks non-dominant weights of the teacher to reduce unnecessary complexity, then progressively restores the teacher by gradually increasing its capacity during training. This strategy allows the student to learn richer representations from the teacher in a smooth and stable manner. To further refine knowledge transfer, Masters integrates an offline RL stage with two complementary rewards: an accuracy reward that measures the correctness of the generated responses, and a distillation reward that quantifies the ease of transferring responses from teacher to student. Unlike online think-answer RL paradigms that are computationally expensive and generate lengthy responses, our offline RL leverages pre-generated responses from masked teachers. These provide rich yet efficient guidance, enabling students to achieve strong performance without requiring the think-answer process."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 8 3 2 2 2 . 2 1 5 2 : r 2025-12Masking Teacher and Reinforcing Student for Distilling Vision-Language Models Byung-Kwan Lee, Yu-Chiang Frank Wang, Ryo Hachiuma Abstract Large-scale visionlanguage models (VLMs) have recently achieved remarkable multimodal understanding, but their massive size makes them impractical for deployment on mobile or edge devices. This raises the need for compact yet capable VLMs that can efficiently learn from powerful large teacher. However, distilling knowledge from large teacher to small student remains challenging due to their large size gap: the student often fails to reproduce the teachers complex, high-dimensional representations, leading to unstable learning and degraded performance. To address this, we propose Masters (Masking teacher and reinforcing student), mask-progressive reinforcement learning (RL) distillation framework. Masters first masks and non-dominant weights of the teacher to reduce unnecessary complexity, then progressively restores the teacher from mask to gradually increase the teacher capacity during training. This strategy allows the student to learn richer representations of teacher in smooth and stable manner. To further refine knowledge transfer, Masters integrates an offline RL stage with two complementary rewards: an accuracy reward that measures the correctness of the generated responses, and distillation reward that quantifies the ease of their responses transferability from teacher to student. Unlike online thinkanswer RL paradigms that are computationally expensive and generate lengthy responses, our offline RL leverages pre-generated responses from masked teachers. These provide rich yet efficient guidance, enabling the students to achieve strong performance without requiring the thinkanswer process. Extensive experiments across diverse VLM benchmarks demonstrate that Masters outperforms existing compact VLMs and partially surpasses large ones, while being far more efficient. Moreover, gradually increasing the teacher sizes during distillation (e.g., from 14B to 38B) yields smoother convergence and stronger generalization than one-shot distillation (e.g., 38B), revealing scalable path toward efficient and deployable VLMs. Figure 1 Comparing Masters-applied VLMs with diverse openand closed-source VLMs across broad model sizes for averaged performance (%) of numerous evaluation benchmarks: AI2D [1], ChartQA [2], MathVista [3], MMB [4], MM-Vet [5], MMMU [6], MMMU-Pro [7], MMStar [8], BLINK [9], SEED-Bench [10], SEED-Bench-2-Plus [11], and RealWorldQA. 1. Introduction years, In recent visionlanguage models (VLMs) [13, 14] have demonstrated remarkable capabilities across wide range of multimodal tasks [15], including visual captioning, reasoning, and open-ended question answering. By jointly understanding visual and textual information, large-scale VLMs have achieved impressive generalization and reasoning abilities Correspondence to <byungkwanl@nvidia.com> 2025 NVIDIA. All rights reserved. Masking Teacher and Reinforcing Student for Distilling Vision-Language Models Figure 2 Illustrating training dynamics of Masters, where we represent how (a) mask ratio is controlled during distillation, and (b) its averaged performance log of student (InternVL3.5-8B [12]) for evaluation benchmarks in Tab. 1. In addition, we show (c) the effect of RL under naive and mask-progressive distillation. Note that asterisk (*) represents the combined distillation of mid-size and large teacher. that in some domains approach human-level understanding [16]. However, these achievements come at the cost of massive model sizes [17, 18, 19, 12] and heavy computational requirements, making them impractical for deployment on mobile or edge devices [20]. As the demand for on-device intelligence continues to grow, there is an urgent need for compact yet powerful VLMs [21, 22, 23, 24] that can deliver competitive performance while maintaining high efficiency and deployability [25, 26]. widely adopted approach for building such lightweight yet capable models is knowledge distillation [27, 28], where large teacher transfers its knowledge to smaller student. Despite its promise, distillation remains challenging due to the substantial size gap between teacher and student [29, 30, 31]. The student often struggles to reproduce the teachers rich and high-dimensional representations, leading to unstable learning and significant performance degradation [32, 33, 34]. Recent distillation has explored modified training objectives [35, 36, 37, 38], dynamic intermediatelayer distillation [39, 40], vision-attention distillation [41, 42], multi-step distillation [43, 44], crosstoken general distillation [45, 46], and reinforcement learning (RL)based approaches [47, 48, 49]. However, few works have directly addressed the fundamental issue of the large size gap itself. To tackle this challenge, we first mask nondominant weights of the teacher based on their magnitudes [50], thereby reducing unnecessary complexity. This is because the large number of weights in the teacher is core factor that makes distillation difficult. After masking the weights in the teacher, we perform the knowledge distillation progressively where the teacher is gradually restored from the mask throughout the entire training to increase the teachers representation complexity. This mask-progressive strategy enables the student to first capture coarse-grained patterns and subsequently refine them into more detailed and higher-level representations, thereby learning richer and more complex representations of the teacher in smooth and stable manner. Furthermore, we identify critical limitations in existing supervised fine-tuning (SFT) datasets [51, 52, 53], where answer labels are typically generated by large closed-source models with an extremely large number of parameters, such as GPT-4o [14], Gemini [54], or Claude [55], and often filtered by humans [56]. Small student, however, has limited capacity to learn these rich answer labels due to its smaller vocabulary and lower hidden dimension of representation [57, 58]. To address this, we utilize the generated responses from masked teachers instead of directly using standard SFT samples. This enables the 2 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models Figure 3 Overview of mask-progressive distillation where teacher is masked with decreasing masking ratio (0.20, 0.15, 0.10, 0.05, 0), gradually restoring its full capacity. At each masking stage for teacher, student is updated using two rewards: accuracy reward ğ‘…acc and distillation reward ğ‘…distill. This progressive distillation enables smooth and stable knowledge transfer to the student. student to learn the responses that better match the students current capacity. Moreover, we incorporate the students own generated responses into training to maintain alignment between the teachers guidance and the students evolving representational capacity. This design ensures stable yet continual improvement toward the teachers behavior, going beyond the single-answer and over-rich label constraints of conventional SFT datasets. However, some generated responses may contain factual errors or exhibit linguistic complexity that hinders effective knowledge transfer, ultimately degrading the distillation performance. Hence, we aim to evaluate both the accuracy of the responses and their ease of knowledge transfer, and further refine the student to avoid such undesirable responses. To achieve this, we integrate RL into the distillation process. Specifically, we compute two types of rewards: an accuracy reward evaluated by LLM-as-a-Judge [59] to account for flexible and diverse answers, and distillation reward that measures how easily knowledge can be transferred from the teacher to the student. In practice, conventional online RL is too slow and inefficient training, since it requires the model to repeatedly generate multiple responses at every training step under the recent thinkanswer paradigm [60, 61]. As result, only very limited amount of data samples [62, 63, 64, 65, 66, 67, 68, 49] are utilized, constraining the scale and diversity of training samples. To address these limitations, we adopt an offline RL approach in which both the teacher and the student pre-generate their multiple responses for all questions, without explicit thinkanswer. These pre-generated responses are then used for RL training, significantly reducing training time and computational costs. Bringing these components together, we propose mask-progressive RL distillation framework, referred to as Masking teacher and reinforcing students (Masters). It integrates teacher weight masking, progressive distillation, multi-response learning, and offline RL into unified training paradigm. Through this design, it enables the student to effectively absorb knowledge from large teachers while maintaining high efficiency and stability. Through extensive experiments across diverse evaluation benchmarks, we demonstrate that Masters consistently outperforms existing compact VLMs and exceeds large ones in Fig. 1. Beyond empirical gains, we find that gradually increasing the teacher sizes during distillation (e.g., from 14B to 38B) leads to smoother convergence and superior generalization compared to one-shot distillation from single large teacher (e.g., 38B), revealing an effective pathway for scalable model compression. We believe this framework marks an important step toward efficient, deployable, and continually improvable VLMs for on-device intelligence. Our main contributions can be summarized in threefold as follows: Progressive capacity-aligned dis3 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models Figure 4 Depicting multiple responses generated by both the masked teacher and the student, where an accuracy reward (ğ‘…acc) evaluates the binary correctness of each response, and distillation reward (ğ‘…distill) measures the ease of knowledge transfer based on divergence objective between teacher and student logits. Note that the rank labels (1st, 2nd, 3rd, etc.) in ğ‘…distill indicate the relative magnitude of the divergence values, where the smallest divergence (1st) receives the highest reward (1.0) and the largest divergence the lowest reward (0.0). tillation: We propose progressive teacherstudent alignment strategy that adaptively matches the students learning capacity through restoring the teacher from the mask. Offline RL with dual rewards: By pregenerating diverse responses from both the teacher and the student, our offline RL substantially reduces training time while efficiently achieving strong performance with two complementary objectives an accuracy reward for correctness and distillation reward for transferability. Unified and scalable framework: We integrate progressive capacity-aligned distillation and offline RL into unified training framework with scaled-up data, enabling compact VLMs to attain large model-level performance for practical efficiency and deployability. 2. Related Work Previous approaches to building efficient VLMs (see Appendix A) have focused on inserting additional modules, modifying internal architectures, or altering propagation strategies within the models themselves. Recent emerging line of research on efficient VLMs has begun to leverage knowledge distillation [27, 28], where the knowledge of large, high-capacity teacher is transferred into smaller, more efficient student. This is because training small model alone inherently limits its representational capacity, prompting shift toward utilizing rich logit or intermediate representations from the teacher. Beyond the scope of VLM research, early studies on knowledge distillation primarily focused on aligning the logits between the teacher and student [69, 70]. Subsequent works extended this paradigm to intermediate feature representations [71, 72, 73, 74, 75], and later introduced probabilistic [76], relational [77], and contrastive formulations [78] to better capture internal dependencies [79, 80, 81, 82]. Several studies have explored multi-teacher frameworks [83, 84, 85, 86, 87, 88] to integrate diverse knowledge sources, while others have leveraged step-by-step reasoning traces from large models to enhance the students reasoning and compositional capabilities [89, 90]. More recent research has investigated alternative training objectives [35, 36, 37, 38], dynamic intermediatelayer distillation [39, 40], vision-attention distillation [41, 42], multi-step distillation [43, 44], cross-token general distillation [45, 46], and reinforcement learning (RL)based approaches [47, 48, 49]. Despite these advances, model distillation re4 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models Table 1 Comparing the performance between naive, mask-progressive, and RL-applied distillation on AI2D [1], ChartQA [2], MathVista [3], MMB/MMBCN [4], MM-Vet [5], MMMU [6], MMMU-Pro [7], MMStar [8], BLINK [9], SEED [10], SEED2+ [11], RealWorldQA (RWQA). Note that +Large Teacher refers to the largest model within the same model family in Sec. 4.1. VLMs AI2D ChartQA MathVista MMB MMBCN MM-Vet MMMU MMMU-Pro MMStar BLINK SEED SEED2+ RWQA Avg 83.9 Qwen2.5-VL-7B +Large Teacher 84.3 +Mask-Progressive 85.1 +Reward Feedback 86.3 85.7 Qwen3-VL-8B +Large Teacher 86.4 +Mask-Progressive 87.3 +Reward Feedback 88.5 85.2 InternVL3-8B +Large Teacher 85.8 +Mask-Progressive 86.5 +Reward Feedback 88.1 84.0 InternVL3.5-8B +Large Teacher 84.8 +Mask-Progressive 85.3 +Reward Feedback 86.1 87.3 87.9 89.2 92.8 88.4 90.3 92.7 95. 86.6 87.4 88.5 91.9 86.7 87.3 87.8 88.6 67.8 68.4 70.3 73.4 77.2 78.4 79.8 81.8 71.6 72.2 73.0 76.3 78.4 78.9 79.4 80. 83.5 83.9 84.8 86.7 86.8 87.5 88.4 89.5 83.4 84.0 84.8 87.3 86.5 86.9 87.4 88.2 83.4 83.8 84.6 86.5 86.5 87.4 88.5 89. 82.2 82.8 83.6 86.1 85.9 86.3 86.8 87.6 71.8 72.3 74.1 77.0 74.5 75.8 77.3 79.4 78.5 79.1 79.8 82.2 83.1 83.5 84.0 84. 55.0 55.6 57.0 59.8 69.6 70.4 71.5 72.9 62.7 63.6 64.7 67.1 73.4 73.8 74.3 75.1 38.3 38.9 40.2 42.9 55.9 57.3 59.1 61. 41.3 42.1 43.0 46.5 57.2 57.6 58.1 58.9 63.9 64.5 66.2 69.0 70.9 73.1 76.0 79.7 68.2 68.9 70.1 74.1 69.3 69.8 70.3 71. 56.4 56.9 58.5 61.3 69.1 69.9 70.9 72.3 55.5 56.2 57.4 61.0 59.5 60.0 60.5 61.3 77.0 77.4 78.5 80.2 78.5 79.3 80.3 81. 77.1 77.7 78.5 80.5 77.4 77.8 78.3 79.1 70.4 70.9 71.9 73.8 70.8 71.9 73.3 75.1 69.7 70.4 71.2 73.4 70.8 71.2 71.7 72. 69.8 68.5 70.3 68.9 70.3 71.6 72.4 74.0 75.7 69.9 76.9 71.8 74.3 78.4 77.5 80.4 71.8 70.8 72.4 71.5 72.8 73.4 75.3 76.1 75.4 67.5 75.8 67.9 68.4 76.3 69.2 77.1 mains challenging due to the substantial parameter gap between teacher and student [29, 30, 31]. The student often struggles to reproduce the teachers rich, high-dimensional representations, leading to unstable training and noticeable performance degradation [32, 33, 34]. However, only limited efforts have been made to directly narrow the parameter gap between teacher and student. Unlike existing approaches that primarily focus on modifying distillation objectives, aligning intermediate features, or combining multiple strategies, we aim to directly address the large parameter gap between teacher and student. To this end, we first mask non-dominant weights in the teacher based on their magnitudes [50], effectively reducing the number of active parameters and simplifying the teachers representational space. This masking step not only narrows the capacity gap between teacher and student but also filters out noisy or over-parameterized components that hinder stable knowledge transfer. As training proceeds, we progressively restore the teacher from mask, allowing the student to gradually learn complex representations of teacher in capacity-aligned manner. This leads to smoother optimization and mitigates the instability in direct distillation from large teacher. 3. Masters This section introduces the two pivotal components that form the core of the Masters: how to mask the teacher (Secs. 3.1 and 3.2) and reinforce the student (Sec. 3.3). 3.1. Magnitude-based Teacher Masking key challenge in distilling knowledge from large teacher to small student lies in the significant parameter gap between them. To reduce this gap, we adopt magnitude-based masking strategy inspired by classical network pruning [50], where weights with smaller magnitudes are masked to zero. Given teacher ğ’¯ with ğ‘›=1 (ğ‘¤ğ‘› R), we construct weight Wğ’¯ = {ğ‘¤ğ‘›}ğ‘ binary mask Mğ‘Ÿ = {ğ‘šğ‘›}ğ‘ ğ‘›=1 (ğ‘šğ‘› 0, 1) as follows: ğ‘šğ‘› = {1, 0, if ğ‘¤ğ‘› ğœ†ğ‘Ÿ, otherwise, (1) where ğ‘ denotes the total number of parameters (e.g., ğ‘ = 38B when using InternVL3.5-38B [12]), and ğœ†ğ‘Ÿ is magnitude threshold determined by the desired masking ratio ğ‘Ÿ [0, 1]. For example, when ğ‘Ÿ = 0, it is the original teacher. Conversely, when ğ‘Ÿ = 1, all weights are masked to zero. With ğ‘Ÿ = 0.2, the magnitude threshold ğœ†0.2 is determined by sorting the magnitude of Wğ’¯ in ascending order, such that approximately 5 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models Table 2 Comparison of the performance among naive (using large and mid teacher), mask-progressive, and RL-applied distillation. Note that +Mid Teacher denotes all intermediate models (e.g., 4B, 8B, and 14B) between student (e.g., 2B) and Large Teacher (e.g., 38B). VLMs AI2D ChartQA MathVista MMB MMBCN MM-Vet MMMU MMMU-Pro MMStar BLINK SEED SEED2+ RWQA Avg 84.0 InternVL3.5-8B 84.8 +Large Teacher +Mid Teacher 85.4 +Mask-Progressive 86.0 +Reward Feedback 87.2 86.1 Mid Teacher 82.6 InternVL3.5-4B 82.8 +Large Teacher +Mid Teacher 82.9 +Mask-Progressive 83.1 +Reward Feedback 83.3 82.9 Mid Teacher 78.8 InternVL3.5-2B 79.1 +Large Teacher 79.7 +Mid Teacher +Mask-Progressive 80.9 +Reward Feedback 83.0 80.1 Mid Teacher 86.7 87.3 87.9 88.8 95.1 88. 86.0 86.4 86.6 86.9 94.9 87.0 80.7 81.8 83.8 87.7 94.8 84.3 78.4 78.9 79.4 80.1 85.0 80.2 77.1 77.3 77.5 77.9 78.8 77.9 71.8 72.3 73.0 74.6 77.5 73.4 86.5 86.9 87.3 87.9 88.2 88. 86.9 86.9 87.0 87.2 86.1 86.8 82.3 82.6 83.1 84.1 85.9 83.5 85.9 86.3 86.8 87.4 88.3 87.6 86.1 86.2 86.3 86.5 86.0 86.3 81.8 82.2 82.8 84.0 86.3 83.2 83.1 83.5 84.0 84.6 85.6 84. 76.6 76.8 77.0 77.4 79.5 77.9 71.7 72.5 73.8 76.5 81.4 74.2 73.4 73.8 74.3 74.9 72.7 75.1 66.6 66.9 67.1 67.5 63.2 66.9 59.0 59.4 60.2 61.8 64.6 60.6 57.2 57.6 58.0 58.6 58.1 58. 51.4 51.7 51.9 52.2 52.7 52.2 46.3 46.8 47.6 49.3 52.4 47.9 69.3 69.8 70.3 71.0 80.8 71.1 65.0 65.4 65.6 66.0 70.5 65.8 62.7 63.3 64.4 66.6 70.7 64.8 59.5 60.0 60.5 61.2 67.8 61. 58.1 58.4 58.6 59.0 62.5 59.0 51.3 52.1 53.4 56.1 61.1 53.7 77.4 77.8 78.3 79.0 81.4 79.1 75.2 75.5 75.7 76.1 79.1 75.9 75.2 75.5 76.0 77.1 79.0 76.3 70.8 71.2 71.7 72.4 75.5 72. 69.4 69.7 69.9 70.3 71.4 70.1 68.0 68.3 68.8 69.8 71.6 69.0 75.4 67.5 75.8 67.9 76.3 68.4 69.1 77.0 74.9 80.0 77.1 69.2 72.9 66.3 73.1 66.6 73.3 66.8 73.6 67.2 68.1 75.1 73.5 66.9 68.6 62.0 69.1 62.5 70.0 63.3 65.0 71.8 68.1 75.1 70.4 63.6 ğ‘ ğ‘›=1 ğ‘šğ‘› ğ‘ (1 0.2). The resulting masked teacher parameters are defined as: Wğ’¯ğ‘Ÿ = Mğ‘Ÿ Wğ’¯ , (2) where denotes element-wise multiplication, and ğ’¯ğ‘Ÿ represents the masked teacher under masking ratio ğ‘Ÿ. This masking process effectively removes low-magnitude weights that contribute marginally to prediction logits [50], yielding simplified yet representative teacher. By reducing parameter of the teacher, the student ğ’® learns more capacity-aligned representation, mitigating optimization instability. Notably, unlike conventional pruning approaches [91, 92, 93, 94, 95, 96, 97, 98] designed for model compression, our masking is temporary and restored later. In practice, we found that using global threshold ğœ†ğ‘Ÿ across all layers often excessively prunes certain layers, which can make the model non-functional at inference time. To prevent this imbalance, we compute ğœ†ğ‘Ÿ per layer and apply masking separately to each layer so that the overall masking remains balanced and consistent across the teacher network. 3.2. Mask-Progressive Distillation Scheduling Masking Ratio. While the masked teacher provides simplified representation, learning solely with fixed masking ratio Figure 5 Comparing the performances by the number of generated responses from teacher and student. limits the students exposure to the teachers rich representation. To solve it, we adopt maskprogressive strategy that gradually restores the teachers full capacity over the entire training. At each training iteration ğ‘– {1, . . . , ğ¼}, masking ratio ğ‘Ÿ[ğ‘–] is formulated as written: ğ‘Ÿ[ğ‘–] = ğ‘Ÿmax ğ‘  ğ‘– , ğ‘€ ğ¼ (3) where ğ‘  denotes the decrement applied to the masking ratio at each masking stage, and ğ‘€ represents the total number of masked teachers with different masking ratios, computed as ğ‘€ = Masking Teacher and Reinforcing Student for Distilling Vision-Language Models Table 3 Comparing Masters-applied VLMs with standard or smaller model size open-source VLMs. AI2D ChartQA MathVista MMB MMBCN MM-Vet MMMU MMMU-Pro MMStar BLINK SEED SEED2+ RWQA VLMs LLaVA-OneVision-7B [99] 81.4 LLaVA-OneVision-1.5-8B [53] 84.2 82.1 MiniCPM-V2.6-8B [100] 86.1 MiniCPM-o2.6-8B [100] 86.6 Ovis2-8B [101] 87.9 GLM-4.1V-9B [102] 83.5 MiMo-VL-8B [103] 86.7 Keye-VL-8B [104] 89.5 Keye-VL-1.5-8B [105] 83.9 Qwen2.5-VL-7B [17] 85.7 Qwen3-VL-8B [18] 85.2 InternVL3-8B [19] 84.0 InternVL3.5-8B [12] Qwen2.5-VL-7B-Masters Qwen3-VL-8B-Masters InternVL3-8B-Masters InternVL3.5-8B-Masters 88.6 88.5 88.9 87. 77.8 Phi-3.5-Vision-4B [106] 83.0 Phi-4-Multimodal-5.6B [107] LLaVA-OneVision-1.5-4B [53] 83.6 85.7 Ovis2-4B [101] 81.6 Qwen2.5-VL-3B [17] 84.1 Qwen3-VL-4B [18] 82.6 InternVL3.5-4B [12] Qwen2.5-VL-3B-Masters Qwen3-VL-4B-Masters InternVL3.5-4B-Masters Aquila-VL-2B [108] Ovis2-2B [101] Qwen3-VL-2B [18] InternVL3-2B [19] InternVL3.5-2B [12] Qwen3-VL-2B-Masters InternVL3-2B-Masters InternVL3.5-2B-Masters 85.4 88.0 83.3 75.0 82.7 76.9 78.7 78. 82.1 82.1 83.0 80.0 86.5 - 86.9 - 70.0 91.7 72.5 86.3 87.3 88.4 86.6 86.7 95.6 95.9 94.8 95.1 81.8 81.4 87.1 - 84.0 87.4 86.0 95.6 94.7 94.9 76.5 - 81.2 80.2 80. 94.1 93.5 94.8 63.2 69.6 60.6 73.3 71.8 80.7 81.5 80.7 81.2 67.8 77.2 71.6 78.4 78.8 81.8 82.3 85.0 43.9 65.8 67.9 69.6 61.2 73.7 77.1 75.6 79.6 78.8 59.0 64.1 61.3 57.0 71. 70.4 66.5 77.5 80.8 84.1 - - - - 84.4 92.0 92.0 83.5 86.8 83.4 86.5 89.1 89.5 90.1 88.2 76.0 86.7 84.2 - 79.1 84.6 86.9 85.9 88.7 86.1 - - 81.9 81.1 82. 84.2 85.9 85.9 - 81.0 - - - - 82.0 - - 83.4 86.5 82.2 85.9 89.5 89.9 91.0 88.3 66.1 - 76.9 - 78.1 84.3 86.1 86.3 89.3 86.0 - - 81.4 78.4 81. 84.1 85.2 86.3 57.5 - 60.0 67.2 65.1 66.4 77.5 68.6 71.2 71.8 74.5 78.5 83.1 81.7 79.4 83.8 85.6 43.2 51.9 - 65.5 61.8 71.0 76.6 78.3 79.4 79.5 43.8 58.3 61.4 62.2 71. 71.3 77.0 81.4 48.8 55.4 49.8 50.9 57.4 68.0 66.7 71.4 71.4 55.0 69.6 62.7 73.4 71.3 72.9 74.0 72.7 43.0 56.0 52.7 49.0 51.2 67.4 66.6 61.2 70.3 63.2 47.4 45.6 53.4 48.6 59. 55.8 59.6 64.6 24.1 37.4 27.2 - - - 46.2 - - 38.3 55.9 41.3 57.2 60.6 61.4 58.8 58.1 19.7 38.5 35.3 - 31.6 53.2 51.4 51.0 57.0 52.7 - - 36.5 24.9 46. 39.5 40.1 52.4 61.9 67.7 57.5 63.3 64.6 72.9 70.8 75.5 80.5 63.9 70.9 68.2 69.3 74.9 79.7 82.0 80.8 47.5 58.9 64.9 61.9 55.9 69.8 65.0 68.9 75.3 70.5 54.9 56.7 58.3 60.7 62. 66.1 66.7 70.7 53.0 - 55.2 53.9 54.3 65.9 62.4 52.0 54.9 56.4 69.1 55.5 59.5 67.2 72.3 68.0 67.8 58.3 42.1 - 53.0 47.6 65.8 58.1 53.0 69.1 62.5 34.1 47.9 53.8 47.0 51. 62.3 53.1 61.1 76.7 77.3 74.0 75.5 77.2 - - - - 77.0 78.5 77.1 77.4 81.8 81.7 82.6 81.4 69.7 73.2 76.6 76.2 74.0 78.4 75.2 78.8 81.5 79.1 73.9 74.4 76.0 75.0 75. 78.3 78.5 79.0 65.4 69.2 65.7 67.9 70.4 71.8 72.4 67.8 - 70.4 70.8 69.7 70.8 75.9 75.1 75.0 75.5 62.2 68.5 68.9 69.3 67.6 70.2 69.4 72.1 72.9 71.4 63.0 67.4 66.4 64.6 68. 69.4 67.8 71.6 69.9 68.1 65.0 68.0 72.5 70.6 - 67.7 73.5 68.5 69.9 70.8 67.5 77.3 77.5 74.8 74.9 53.6 64.1 67.8 71.1 65.4 70.6 66.3 70.7 77.8 68.1 65.0 66.0 63.9 64.3 62. 69.0 68.4 68.1 ğ‘Ÿmax/ğ‘  + 1. For example, setting ğ‘Ÿmax = 0.2 and ğ‘  = 0.05 results in ğ‘€ = 5, meaning that the masking ratio progressively decreases as 0.20, 0.15, 0.10, 0.05, and finally 0. Fig. 2(a) illustrates how the masking ratio ğ‘Ÿ[ğ‘–] is scheduled during training. As the masking ratio changes, the distillation objective also adapt to reflect the teachers gradually increasing capacity. Formally, the distillation objective at iteration ğ‘– is formulated as: [ )] ( ğ’Ÿ ğ‘ƒğ’¯ğ‘Ÿ[ğ‘–](ğ‘¦ğ‘¥) ğ‘ƒğ’®(ğ‘¦ğ‘¥) , E(ğ‘¥,ğ‘¦)Data[ğ‘–] min Wğ’® (4) where Wğ’® indicates the weight set of the student ğ’®, and ğ‘ƒ (ğ‘¦ğ‘¥) denotes the logit-softmax output for answer label ğ‘¦ given question ğ‘¥ under the data subset Data[ğ‘–] at the iteration ğ‘–. Note that ğ’Ÿ refers to Jensen-Shannon Divergence (JSD) [36, 38], which has been empirically shown to outperform the standard KL divergence for distillation tasks. This mask-progressive distillation allows the student to first learn coarse-grained knowledge and progressively refine its representations as the teachers representational capacity is restored throughout training. Multiple Responses. While mask-progressive distillation enables the student to learn from capacity-aligned teacher, the training data samples used for distillation still exhibit two inherent limitations. First, conventional SFT datasets are typically generated by large closed-source models [14, 54] or filtered by humans [56]. Injecting such responses into small student often leads to performance degradation due to representation mismatchsmall vocabulary and low hidden dimension of representation. Second, standard SFT datasets generally provide only single answer per question, which severely limits response diversity and generalization. This single-answer guidance forces the student to overfit to narrow linguistic or reasoning style, reducing its versatility across broader visionlanguage contexts. To address these limitations, Masters gener7 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models Table 4 Comparing Masters-applied Small VLMs with Closed-source and Large Open-source VLMs. VLMs AI2D ChartQA MathVista MMB MM-Vet MMMU MMMU-Pro MMStar BLINK SEED SEED2+ RWQA Claude-3.5-Sonnet [55] Claude-3.7-Sonnet [55] Claude-4-Sonnet [55] Gemini-1.5-Pro [109] Gemini-2.0-Flash [109] Gemini-2.5-Pro [54] GLM-4.5V [102] GPT-4o [110] GPT-4.1 [110] GPT-5-Mini [110] GPT-5 [110] NVLM-72B [111] LLaVA-OneVision-72B [99] Molmo-72B [112] Qwen2.5-VL-72B [17] Qwen3-VL-32B [18] InternVL3-78B [19] InternVL3.5-38B [12] Qwen2.5-VL-7B-Masters Qwen3-VL-8B-Masters InternVL3-8B-Masters InternVL3.5-8B-Masters 81.2 82.5 83.0 79.1 83.1 89.5 88.1 84.6 85.9 88.2 89.5 85.2 85.6 83.4 88.7 89.5 89.7 87. 88.6 88.5 88.9 87.2 90.8 92.2 - 87.2 - - 86.6 85.7 - - - 86.0 83.7 87.3 89.5 89.8 89.7 88.8 95.6 95.9 94.8 95.1 67.7 66.8 74.6 63.9 70.4 80.9 84.6 63.8 70.4 79.1 81.9 66.6 67.5 58.6 74.2 83.8 79.0 81. 78.8 81.8 82.3 85.0 82.6 84.8 - 73.9 90.0 - - 83.4 - - - - 85.8 - 88.6 90.6 89.0 90.3 89.1 89.5 90.1 88.2 70.1 70.0 - 64.0 73.6 83.3 75.2 69.1 78.8 - 77.6 58.9 60.6 61.1 76.9 79.4 81.3 82. 81.7 79.4 83.8 85.6 68.3 71.0 74.4 62.2 69.9 74.7 75.4 69.1 74.0 79.0 84.2 59.7 56.8 54.1 68.2 76.0 72.2 76.9 71.3 72.9 74.0 72.7 51.5 56.5 61.6 46.9 54.4 - 65.2 51.9 - 67.3 - - 31.0 - 61.2 65.3 62.3 66. 60.6 61.4 58.8 58.1 65.1 65.1 69.4 59.1 69.4 73.6 75.3 64.7 69.8 74.1 75.7 63.7 65.8 63.3 70.8 77.7 72.5 75.3 74.9 79.7 82.0 80.8 60.1 56.6 60.4 59.1 64.0 - 65.3 68.0 68.5 - - 48.0 55.4 49.7 64.4 67.3 66.3 60. 67.2 72.3 68.0 67.8 61.7 74.3 - 76.0 - - - 77.1 78.0 - - 75.5 77.5 74.6 79.5 79.9 78.7 79.1 81.8 81.7 82.6 81.4 71.7 67.6 - 70.8 - - 74.0 72.0 73.1 - - 68.4 - 67.6 73.0 72.8 71.9 71. 75.9 75.1 75.0 75.5 65.8 55.4 69.8 67.5 72.3 - 75.4 78.7 79.0 - 69.9 71.9 73.7 75.7 79.0 78.0 75.9 77.3 77.5 74.8 74.9 ates multiple responses from the masked teacher, where the teachers representational capacity increases as the masking ratio decreases. These responses provide the student with adaptive guidance signals that evolve in step with their learning capacity. Furthermore, Masters incorporates the students own responses into training to maintain alignment between teacher guidance and the students evolving representational capacity. The formulation regarding multiple responses is written as follows: E(ğ‘¥,^ğ‘¦)Gen-Data[ğ‘–] ğ‘ƒğ’¯ğ‘Ÿ[ğ‘–](^ğ‘¦ğ‘¥) ğ‘ƒğ’®(^ğ‘¦ğ‘¥) [ ğ’Ÿ )] ( , min Wğ’® (5) where Gen-Data[ğ‘–] denotes the pre-generated multi-response dataset obtained from both the masked teacher and the student at training iteration ğ‘–. This enables stable yet continual improvement toward teacher-level behavioreffectively overcoming the single-answer and over-rich label constraints of conventional SFT datasets. curacy and the transferability of the responses and refine the student based on their feedback to avoid generating such undesirable responses. Offline RL Setup. Conventional online RL paradigms, such as the think-answer process [60, 61], require generating multiple and long responses at each training iteration, incurring substantial computational overhead. To overcome this limitation, Masters adopts an offline RL setup where both the teacher and the student pre-generate multiple responses for all questions in the training dataset. This design dramatically reduces computational cost while enabling scaled-up training with diverse responses. Accuracy Reward. To assess the correctness of generated responses, we adopt an LLM-asa-Judge [59], which measures semantic fidelity between the generated responses ^ğ‘¦ and the answer labels ğ‘¦. The accuracy reward is defined as: 3.3. Reinforcing Student with Dual Reâ„›acc = LLM-as-a-Judge(ğ‘¥, ^ğ‘¦, ğ‘¦) [0, 1]. (6) wards Although generating multiple responses helps mitigate the constraints of conventional SFT datasets, some generated responses may include factual inaccuracies or overly complex language that hampers effective knowledge transfer, thereby reducing distillation performance. To address this, we evaluate both the factual acThis reward guides the student to generate semantically accurate and faithful outputs, regardless of linguistic or stylistic variations. Unlike prior approaches such as DeepSeek-R1 [60] and its follow-up variants [64, 67, 66, 68], which depend on traditional parsers for limited domain evaluation metrics (e.g., math reasoning or spatial grounding), LLM-as-a-Judge method elimi8 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models nates such constraints. For example, traditional parsers fail when handling open-ended visual questions like Describe the persons emotion in this image. Even, when comparing about five minutes (predicted) with 5 (answer label), traditional parsing says that the predicted response is wrong answer. On the other hands, LLM-asa-Judge [59] delivers consistent and robust judgments across diverse visual question answering (VQA) scenarios including recognition, OCR, reasoning, chart and document understanding providing more generalizable and semantically grounded measure of correctness. Distillation Reward. While the accuracy reward ensures response correctness, it does not capture how effectively the student learns responses. To address this, we introduce distillation reward that measures logit-level similarity between the teacher and student. Specifically, we compute this reward using ğ’Ÿ defined in Eq. (4). Practically, we have observed that the values of ğ’Ÿ exhibit small variance across the generated responses due to its low scaling. To stabilize optimization and enhance sensitivity [113], we apply reverse min-max normalization [114] as follows: â„›distill = ğ’Ÿmax ğ’Ÿ ğ’Ÿmax ğ’Ÿmin , (7) where ğ’Ÿmin and ğ’Ÿmax denote the minimum and maximum divergence among the generated responses ^ğ‘¦ for given question ğ‘¥. This normalization ensures that smaller divergence (indicating better alignment with the teacher) yields higher reward value. The total reward is then computed as the sum of the accuracy and distillation rewards. Final Objective. We adopt RL objective of GRPO [61] (see Appendix B) and extend it with Eq. (5), as follows: 4. Experiments 4.1. Implementation Details Model Selection. We select recently released, high-performing student and teacher VLMs based on Leaderboard [115]. For student, we employ several strong series, including Qwen2.5-VL-3B and -7B [17], Qwen3-VL2B, -4B, and -8B, InternVL3-2B, and -8B [19], as well as InternVL3.5-2B, -4B, and -8B [12]. For teacher, we select corresponding family of Qwen2.5-VL-32B and -72B [17], Qwen3-VL32B [18], InternVL3-14B, -38B, and -78B [19], and InternVL3.5-14B and -38B [12]. Training Setup. We train and evaluate Masters primarily on NVIDIA A100 80GB GPUs. We first set the decrement ğ‘  to 0.05 and save multiple masked teacher checkpoints with different masking ratios. For instance, when ğ‘Ÿmax is set to 0.2, we store five masked teacher checkpoints corresponding to masking ratios of 0.20, 0.15, 0.10, 0.05, and 0. We then make all five masked teachers generate responses for the 1.5M dataset (see Appendix C), by using vLLM [116] for fast inference. Specifically, we generate 8 responses per question by setting the temperature to 1.0, top-p to 0.9, top-k to 50, and the repetition penalty to 1.05. We simultaneously evaluate the accuracy reward of the responses via LLM-as-a-Judge [59]. The model used for judging is the same as the one selected for generation, and we further refine the accuracy reward through additional parsing prompts (see Appendix D). For RL, we utilize the DeepSpeed engine with ZeRO-3 [117] to efficiently handle large teacher and student. The student is optimized using AdamW [118] with fixed learning rate of 1 106. (ğ‘¥,^ğ‘¦)Gen-Data[ğ‘–] min Wğ’® [â„’GRPO + ğ’Ÿ (ğ‘ƒğ’¯ğ‘Ÿ[ğ‘–] (^ğ‘¦ğ‘¥) ğ‘ƒğ’® (^ğ‘¦ğ‘¥))] . (8) This unified objective integrates reinforcement and distillation, allowing the student to refine its responses in stable manner. Fig. 3 and Fig. 4 represents overview of Masters. 4.2. Step-wise Dissection of Masters We conduct series of step-wise experiments to construct Masters and identify the source of its effectiveness. The results of each distillation strategy are summarized in Tab. 1. We begin with naive baseline that performs distillation from large teacher using the objective of JSD [36, 38], without incorporating mask9 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models Table 5 Ablation studies on various configurations influencing the performance of Masters. Note that (a) reports the average performance across the evaluation benchmarks in Tab. 1, while (c), (d), and (e) are conducted using the InternVL3.5 series [12], with 8B student. (a) Maximum of Masking Ratio (c) Source of Training Samples (b) Combinations of Teacher ğ‘Ÿmax Q2.5-VL-72B Q3-VL-32B IVL3-78B IVL3.5-38B Teacher Size AI2D MathVista MMB MM-Vet MMMU Source AI2D MathVsita MMB MM-Vet MMMU 0 0.1 0.2 0.3 0.4 0. 72.7 75.9 79.4 70.4 64.5 48.2 77.7 78.3 79.0 79.5 80.4 75.4 73.9 75.1 80.5 71.1 68.9 50.0 76.8 78.1 80.0 70.4 66.2 49.2 86.8 38B 86.9 78B 87.2 14B+38B 87.5 14B+78B 88.1 38B+78B 14B+38B+78B 88.9 73.4 75.7 76.8 78.3 80.8 82. 85.3 86.8 87.1 88.9 90.1 90.1 82.1 80.2 82.5 82.9 83.0 83.8 65.1 67.8 69.0 70.6 72.9 74.0 87.0 ğ’¯ (#8) 87.0 ğ’®(#2) + ğ’¯ (#6) ğ’®(#3) + ğ’¯ (#5) 87.1 ğ’®(#4) + ğ’¯ (#4) 87.2 87.0 ğ’®(#5) + ğ’¯ (#3) 86.8 ğ’®(#6) + ğ’¯ (#2) 86.4 ğ’®(#8) 84.8 84.6 84.8 85.0 84.6 84.3 83.8 86.3 86.8 87.2 88.2 87.0 86.5 86. 85.3 85.0 85.2 85.6 85.1 84.6 85.0 72.6 72.4 72.2 72.7 72.0 71.5 71.0 (d) Decomposing Components (e) Choice of Reward Design (f) Other Distillation Methods Mid Teacher Multi-Response Reward Feedback Avg Reward AI2D MathVista MMB MM-Vet MMStar VLMs Masters MathVista MMB MM-Vet MMMU Naive Naive Naive Naive Mask-Progressive Mask-Progressive Mask-Progressive Mask-Progressive 75.8 76.2 76.8 77.3 76.0 76.3 77.1 80. 82.3 â„›acc() 86.5 â„›acc â„›distill 86.3 â„›acc + â„›distill() 86.6 â„›acc + â„›distill 87.2 InternVL3.5-38B 87.8 75.6 82.3 80.3 82.5 85.0 81.9 80.7 88.1 88.0 88.1 88.2 90.3 77.2 85.0 84.8 85.2 85.6 82.2 65.3 75.8 72.0 76.3 80.8 75.3 DistiLLM-7B [37] LLaVA-KD-7B [44] VLsI-7B [39] RIL-8B [49] 61.5 69.3 61.8 70.2 74.7 75.1 77.8 82.3 84.8 86. 85.0 86.2 85.8 86.9 88.1 90.1 65.3 73.3 65.9 73.8 75.2 75. 80.1 83.8 57.9 60.8 58.2 61.7 69.3 69.8 68.6 74.0 progressive distillation or RL feedback. Next, we apply teacher masking and perform maskprogressive distillation based on Eq. (5), which yields clear improvementsconfirming that capacity alignment between the teacher and the student stabilizes optimization. Finally, adding RL allows the student to further refine the responses through feedback (correctness and transferability). Overall, combining mask-progressive distillation with RL feedback results in cumulative performance boost, demonstrating that each component contributes synergistically to the overall effectiveness of Masters. We further analyze the impact of gradually increasing the teacher sizes during distillation (e.g., from 14B to 38B), as shown in Tab. 2. We first distill knowledge from an intermediate-sized (mid) teacher as warm-up stage, and then perform distillation from larger teacher. This gradual teacher-size scaling (Blue Color) consistently yields better performance than one-shot distillation from large teacher (Green Color), indicating that progressive capacity alignment leads to smoother convergence and richer representations. Fig. 2(a) illustrates the masking ratio schedule and performance trends across different teacher settings. Notably, after the warm-up stage with the mid teacher shown in Fig. 2(b), performance accelerates significantly when distilling from the large teacher, highlighting the advantage of this teacher scaling strategy. In summary, when combined with mask-progressive distillation and RL feedback, Masters elevates the Figure 6 Comparing hours of inference time at one GPU across various models described in Tab. 3 student to highly competitive levelmatching or even surpassing many recent openand closedsource VLMs, as reported in Tab. 3 and Tab. 4 across various model scales  (Fig. 1)  . 4.3. Ablation Studies with Configurations We conduct comprehensive ablation studies to examine various configuration settings that influence the detailed performance of Masters. First, we investigate the number of pre-generated responses in Fig. 5. As this number increases, performance steadily improves and converges around eight responses. Therefore, we set the number of generated responses to eight to achieve great balance between training efficiency and performance. Masking Teacher and Reinforcing Student for Distilling Vision-Language Models Next, Tab. 5(a) shows grid search to determine the optimal maximum masking ratio (ğ‘Ÿmax) for each teacher, ranging from 0 to 0.5. For most teachers, the optimal value is ğ‘Ÿmax = 0.2, whereas Qwen3-VL-32B [18] achieves its best results with ğ‘Ÿmax = 0.4. Beyond this table, we similarly observe that InternVL3.5-8B and -14B [12] also perform best at ğ‘Ÿmax = 0.4. We then analyze the effect of teacher composition when distilling InternVL3 series [19] from large teachers to an 8B student. Consistent with the results in Tab. 2, the best performance is achieved with gradual teacher-size scaling of 14B, 38B, and 78B in Tab. 5(b). This confirms that progressive teacher scaling enables more stable and capacityaligned knowledge transfer. Furthermore, we examine how the source of generated responses affects performance. As shown in Tab. 5(c), using only teacher-generated responses limits the students adaptability to its own representational capacity, while using only student-generated responses restricts exposure to the teachers richer semantics. Consequently, we adopt balanced 1:1 ratio of teacherand studentgenerated responses to ensure both adaptability and semantic richness. Tab. 5(d) compares various configurations, including naive versus maskprogressive distillation, with and without midteacher scaling, multi-response generation, and reward feedback (Fig. 2(c)). The results demonstrate that each component contributes synergistically to performance, and removing any of them leads to degradationhighlighting their essential roles in performance acceleration. Next, we analyze the impact of reward design by selectively removing the accuracy or distillation reward. The triangle symbol () for the accuracy reward indicates the rule-based evaluation, which does not rely on LLM-as-a-Judge [59], while the triangle for the distillation reward denotes exclusion of reverse minmax normalization. Removing the accuracy reward consistently results in performance below that of the large teacher, underscoring its importance in enabling the student to surpass the teacher. Without normalization, the distillation reward becomes less discriminative, yielding performance comparable to using only the accuracy reward. As shown in Tab. 5(e), both accuracy and distillation rewards are essential for effectively identifying high-quality responses. Finally, we apply Masters to other recent distillation frameworks: DistiLLM [37], LLaVAKD [44], VLsI [39], and RIL [49]. Masters consistently mitigates degradation caused by large parameter gaps. Notably, while most existing methods rely on multi-step pipelines with additional SFT phases to maintain stability, Masters achieves superior results in single-step training processdemonstrating its simplicity, scalability, and training efficiency. 5. Discussion and Conclusion Although Masters shows strong performance and scalability, it is trained in an offline manner, which limits adaptability to real-time feedback. In principle, training could continuously improve the model via online data sampling, but it remains computationally infeasiblefor instance, even single model requires over 30 days on 256 A100 GPUs for 1.5M samples, whereas Masters completes training in just two days. Future advances in efficient multi-node training and inference from vLLM-like libraries is strongly needed for online distillation within days. Beyond this limitation, recent methods often adopt think-answer paradigm, which improves reasoning but greatly increases latency  (Fig. 6)  . Such approaches remain impractical for real-world or on-device use due to their heavy computational cost. In contrast, Masters attains strong performance without sacrificing inference speed, suggesting promising direction for balancing intelligence and scalability toward efficient VLMs. We present Masters, progressively restoring the teacher from mask and reinforcing the student with dual rewards in an offline setting. It achieves stable and scalable knowledge transfer from large to compact models. Future work may explore data-driven masking and RL-based replay buffers to better handle hard examples. We hope Masters will inspire further research toward deployable and efficient VLMs. 11 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models"
        },
        {
            "title": "References",
            "content": "[1] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. [2] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [3] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [4] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. [5] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. [6] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. [7] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. [8] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [9] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. large language models Blink: Multimodal can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. [10] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seedbench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. [11] Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024. [12] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. [13] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [14] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [15] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [16] Yifan Li, Zhixin Lai, Wentao Bao, Zhen Tan, Anh Dao, Kewei Sui, Jiayi Shen, Dong Liu, Huan Liu, and Yu Kong. Visual large language models for generalized and specialized applications. arXiv preprint arXiv:2501.02765, 2025. [17] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [18] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, 12 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [27] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [19] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and testtime recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [20] Guanqiao Qu, Qiyuan Chen, Wei Wei, Zheng Lin, Xianhao Chen, and Kaibin Huang. Mobile edge intelligence for large language models: contemporary survey. IEEE Communications Surveys & Tutorials, 2025. [21] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, et al. Mobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint arXiv:2402.03766, 2024. [22] Wei Chen, Zhiyuan Li, and Shuo Xin. Omnivlm: token-compressed, sub-billion-parameter vision-language model for efficient on-device inference. arXiv preprint arXiv:2412.11475, 2024. [23] Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokula Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, et al. Fastvlm: Efficient vision encoding for vision language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 19769 19780, 2025. [24] AndrÃ©s Marafioti, Orr Zohar, Miquel FarrÃ©, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, et al. Smolvlm: Redefining small and efficient multimodal models. arXiv preprint arXiv:2504.05299, 2025. [25] Ahmed Sharshar, Latif Khan, Waseem Ullah, and Mohsen Guizani. Vision-language models for edge networks: comprehensive survey. IEEE Internet of Things Journal, 2025. [26] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Chi Chen, Haoyu Li, Weilin Zhao, et al. Efficient gpt-4v level multimodal large language model for deployment on edge devices. Nature Communications, 16(1):5509, 2025. [28] Jianping Gou, Baosheng Yu, Stephen Maybank, and Dacheng Tao. Knowledge distillation: survey. International journal of computer vision, 129(6):17891819, 2021. [29] Chen Zhang, Yang Yang, Jiahao Liu, Jingang Wang, Yunsen Xian, Benyou Wang, and Dawei Song. Lifting the curse of capacity gap in distilling language models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 45354553, Toronto, Canada, July 2023. Association for Computational Linguistics. [30] Jia Guo, Minghao Chen, Yao Hu, Chen Zhu, Xiaofei He, and Deng Cai. Reducing the teacherstudent gap via spherical knowledge disitllation. arXiv preprint arXiv:2010.07485, 2020. [31] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 51915198, 2020. [32] Wangchunshu Zhou, Canwen Xu, and Julian McAuley. Bert learns to teach: Knowledge distillation with meta learning. arXiv preprint arXiv:2106.04570, 2021. [33] Yi Yang, Chen Zhang, and Dawei Song. Sparse teachers can be dense with knowledge. arXiv preprint arXiv:2210.03923, 2022. [34] Chen Zhang, Qiuchi Li, Dawei Song, Zheyu Ye, Yan Gao, and Yan Hu. Towards the law of capacity gap in distilling language models. arXiv preprint arXiv:2311.07052, 2023. [35] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations, 2024. [36] Shilin Xu, Xiangtai Li, Haobo Yuan, Lu Qi, Yunhai Tong, and Ming-Hsuan Yang. Llavadi: large lanWhat matters guage models distillation. arXiv preprint arXiv:2407.19409, 2024. for multimodal [37] Jongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-Young Yun. Distillm: Towards streamlined Masking Teacher and Reinforcing Student for Distilling Vision-Language Models distillation for large language models. arXiv preprint arXiv:2402.03898, 2024. [38] Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and Olivier Bachem. On-policy distillation of language models: Learning from self-generated mistakes. In The Twelfth International Conference on Learning Representations, 2024. [39] Byung-Kwan Lee, Ryo Hachiuma, YuChiang Frank Wang, Yong Man Ro, and Yueh-Hua Wu. Vlsi: Verbalized layers-tointeractions from large to small vision language models. arXiv preprint arXiv:2412.01822, 2024. [40] Jiwan Kim, Kibum Kim, Sangwoo Seo, and Chanyoung Park. Compodistill: Attention distillation for compositional reasoning in multimodal llms. arXiv preprint arXiv:2510.12184, 2025. [41] Qianhan Feng, Wenshuo Li, Tong Lin, and Xinghao Chen. Align-kd: Distilling cross-modal alignment knowledge for mobile vision-language model. arXiv preprint arXiv:2412.01282, 2024. [42] Jiajun Cao, Yuan Zhang, Tao Huang, Ming Lu, Qizhe Zhang, Ruichuan An, Ningning Ma, and Shanghang Zhang. Move-kd: Knowledge distillation for vlms with mixture of visual encoders. arXiv preprint arXiv:2501.01709, 2025. [43] Cheng Han, Qifan Wang, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Yi Fang, Qiang Guan, Lifu Huang, and Dongfang Liu. Amd: Automatic multi-step distillation of large-scale vision models. In European Conference on Computer Vision, pages 431450. Springer, 2024. [44] Yuxuan Cai, Jiangning Zhang, Haoyang He, Xinwei He, Ao Tong, Zhenye Gan, Chengjie Wang, and Xiang Bai. Llava-kd: framework of distilling multimodal large language models. arXiv preprint arXiv:2410.16236, 2024. [45] Nicolas Boizard, Kevin El Haddad, CÃ©line Hudelot, and Pierre Colombo. Towards cross-tokenizer distillation: the universal logit distillation loss for llms. arXiv preprint arXiv:2402.12030, 2024. [46] Byung-Kwan Lee, Ryo Hachiuma, Yong Man Ro, Yu-Chiang Frank Wang, and Yueh-Hua Wu. Genrecal: Generation after recalibration from large to small vision-language models. arXiv preprint arXiv:2506.15681, 2025. [47] Hongling Xu, Qi Zhu, Heyuan Deng, Jinpeng Li, Lu Hou, Yasheng Wang, Lifeng Shang, Ruifeng Xu, and Fei Mi. Kdrl: Post-training reasoning llms via unified knowledge distillation and reinforcement learning. arXiv preprint arXiv:2506.02208, 2025. [48] Chuanguang Yang, Xinqiang Yu, Han Yang, Zhulin An, Chengqing Yu, Libo Huang, and Yongjun Xu. Multi-teacher knowledge distillation with reinforcement learning for visual recognition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 91489156, 2025. [49] Byung-Kwan Lee, Ryo Hachiuma, Yong Man Ro, Yu-Chiang Frank Wang, and Yueh-Hua Wu. Unified reinforcement and imitation learning for vision-language models, 2025. [50] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. [51] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. [52] Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. 2024. [53] Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Chunsheng Wu, et al. Llava-onevision-1.5: Fully open framework for democratized multimodal training. arXiv preprint arXiv:2509.23661, 2025. [54] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [55] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www.anthropic.com, 2024. [56] Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying Shen, Di Jin, Yu Cheng, Qifan 14 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models Wang, and Lifu Huang. Vision-flan: Scaling human-labeled tasks in visual instruction tuning. arXiv preprint arXiv:2402.11690, 2024. Michael Qizhe Shieh. Noisyrollout: Reinforcing visual reasoning with data augmentation. arXiv preprint arXiv:2504.13055, 2025. [57] Sanqiang Zhao, Raghav Gupta, Yang Song, and Denny Zhou. Extremely small bert models from mixed-vocabulary training. arXiv preprint arXiv:1909.11687, 2019. [58] Songming Zhang, Xue Zhang, Zengkui Sun, Yufeng Chen, and Jinan Xu. Dual-space knowledge distillation for large language models. arXiv preprint arXiv:2406.17328, 2024. [59] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mtbench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595 46623, 2023. [60] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [61] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [62] Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. [63] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmmr1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. [64] En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, et al. Perception-r1: Pioneering perception policy with reinforcement learning. arXiv preprint arXiv:2504.07954, 2025. [65] Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, and [66] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [67] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [68] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [69] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. [70] Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962, 2019. [71] Adriana Nicolas Romero, Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014. [72] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model compression. arXiv preprint arXiv:1908.09355, 2019. [73] Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Distilling knowledge via knowledge review. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 50085017, 2021. [74] Emanuel Ben-Baruch, Matan Karklinsky, Yossi Biton, Avi Ben-Cohen, Hussam Lawen, and Nadav Zamir. Its all in the head: Representation knowledge distillation through classifier sharing. arXiv preprint arXiv:2201.06945, 2022. 15 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models [75] Jiabao Wang, Yuming Chen, Zhaohui Zheng, Xiang Li, Ming-Ming Cheng, and Qibin Hou. Crosskd: Cross-head knowledge distillation In Proceedings of the for object detection. IEEE/CVF conference on computer vision and pattern recognition, pages 1652016530, 2024. [76] Nikolaos Passalis, Maria Tzelepi, and Anastasios Tefas. Probabilistic knowledge transfer for lightweight deep representation learning. IEEE Transactions on Neural Networks and learning systems, 32(5):20302039, 2020. [77] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 39673976, 2019. [78] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. arXiv preprint arXiv:1910.10699, 2019. [79] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016. [80] Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 41334141, 2017. [81] Frederick Tung and Greg Mori. Similaritypreserving knowledge distillation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 13651374, 2019. [82] Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, and Jin Young Choi. comprehensive overhaul of feature distillation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 19211930, 2019. Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1256112571, 2023. [85] Fanqi Wan, Longguang Zhong, Ziyi Yang, Ruijun Chen, and Xiaojun Quan. Fusechat: Knowledge fusion of chat models. arXiv preprint arXiv:2408.07990, 2024. [86] Kunran Xu, Lai Rui, Yishi Li, and Lin Gu. Feature normalized knowledge distillation for image classification. In European conference on computer vision, pages 664680. Springer, 2020. [87] Wonchul Son, Jaemin Na, Junyong Choi, and Wonjun Hwang. Densely guided knowledge distillation using multiple teacher assistants. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9395 9404, 2021. [88] Guo-Hua Wang, Yifan Ge, and Jianxin Wu. Distilling knowledge by mimicking features. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):81838195, 2021. [89] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. arXiv preprint arXiv:2305.02301, 2023. [90] Yijun Tian, Yikun Han, Xiusi Chen, Wei Wang, and Nitesh Chawla. Beyond answers: Transferring reasoning capabilities to smaller llms using multi-teacher knowledge distillation. arXiv preprint arXiv:2402.04616, 2024. [91] Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information processing systems, 2, 1989. [92] Babak Hassibi, David Stork, and Gregory Wolff. Optimal brain surgeon and general network pruning. In IEEE international conference on neural networks, pages 293299. IEEE, 1993. [83] Inar Timiryasov and Jean-Loup Tastet. Baby llama: knowledge distillation from an ensemble of teachers trained on small dataset with no performance penalty. arXiv preprint arXiv:2308.02019, 2023. [93] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In Proceedings of the IEEE international conference on computer vision, pages 13891397, 2017. [84] Young-Suk Lee, Md Sultan, Yousef El-Kurdi, Tahira Naseem, Asim Munawar, Radu Florian, Salim Roukos, and RamÃ³n Fernandez Astudillo. Ensemble-instruct: Instruction tuning data generation with heterogeneous mixture of lms. In [94] Chaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong Zhang. Eigendamage: Structured pruning in the kronecker-factored eigenbasis. In International conference on machine learning, pages 65666575. PMLR, 2019. 16 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models [95] Sidak Pal Singh and Dan Alistarh. Woodfisher: Efficient second-order approximation for neural network compression. Advances in Neural Information Processing Systems, 33:1809818109, 2020. [96] Byung-Kwan Lee, Junho Kim, and Yong Man Ro. Masking adversarial damage: Finding adversarial saliency for robust and sparse network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1512615136, 2022. [97] Yang He and Lingao Xiao. Structured pruning for deep convolutional neural networks: survey. IEEE transactions on pattern analysis and machine intelligence, 46(5):29002919, 2023. [98] Hongrong Cheng, Miao Zhang, and Javen Qinfeng Shi. survey on deep neural network pruning: Taxonomy, comparison, analysis, and recommendations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [99] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [100] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [101] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and HanJia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv:2405.20797, 2024. [102] Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pages arXiv2507, 2025. [103] Core Team, Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zihan Jiang, Zhixian Zheng, Zhichao Song, Zhenbo Luo, Yue Yu, Yudong Wang, Yuanyuan Tian, Yu Tu, Yihan Yan, Yi Huang, Xu Wang, Xinzhe Xu, Xingchen Song, Xing Zhang, Xing Yong, Xin Zhang, Xiangwei Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, Weiji Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shihua Yu, Shaohui Liu, Shande Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kangyang Zhou, Kang Zhou, Kai Fang, Jun Shi, Jinhao Dong, Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongshen Xu, Heng Qu, Haochen Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong Ma, Chang Liu, Can Cai, and Bingquan Xia. Mimo-vl technical report, 2025. [104] Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, et al. Kwai keye-vl technical report. arXiv preprint arXiv:2507.01949, 2025. [105] Biao Yang, Bin Wen, Boyang Ding, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, et al. Kwai keye-vl 1.5 technical report. arXiv preprint arXiv:2509.01563, 2025. [106] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [107] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi4-mini technical report: Compact yet powerful multimodal language models via mixture-ofloras. arXiv preprint arXiv:2503.01743, 2025. [108] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [109] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 17 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models [110] OpenAI. Gpt-4v(ision) system card, https://openai.com/research/ on accessed"
        },
        {
            "title": "Last",
            "content": "2023. gpt-4v-system-card, 2024-02-13. [111] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: Open frontier-class multimodal llms. arXiv preprint, 2024. [112] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for stateof-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [113] Tom Schaul, Georg Ostrovski, Iurii Kemaev, and Diana Borsa. Return-based scaling: Yet another normalisation trick for deep rl. arXiv preprint arXiv:2105.05347, 2021. [114] Mingqi Yuan, Roger Creus Castanyer, Bo Li, Xin Jin, Wenjun Zeng, and Glen Berseth. Rlexplore: Accelerating research in intrinsicallymotivated reinforcement learning. arXiv preprint arXiv:2405.19548, 2024. [115] OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https://github.com/open-compass/ opencompass, 2023. Conference on Neural Information Processing Systems, 2023. [120] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [121] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instructionfinetuned language models. arXiv preprint arXiv:2210.11416, 2022. [122] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. [123] Young-Jun Lee, Byung-Kwan Lee, Jianshu Zhang, Yechan Hwang, Byungsoo Ko, Han-Gyu Kim, Dongyu Yao, Xuankun Rong, Eojin Joo, Seung-Ho Han, et al. Multiverse: multi-turn conversation benchmark for evaluating large vision and language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 708719, 2025. [116] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626, 2023. [124] Young-Jun Lee, Seungone Kim, ByungKwan Lee, Minkyeong Moon, Yechan Hwang, Jong Myoung Kim, Graham Neubig, Sean Welleck, and Ho-Jin Choi. Refinebench: Evaluating refinement capability of language models via checklists. arXiv preprint arXiv:2511.22173, 2025. [117] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. [118] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. [119] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirty-seventh [125] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1935819369, 2023. [126] Maxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 18 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models [127] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4015 4026, 2023. [128] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. [129] Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12901299, 2022. [130] Matthias Minderer, Alexey A. Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [131] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou, Wayne Zhang, and Ziwei Liu. Panoptic scene graph generation. In European Conference on Computer Vision, pages 178196. Springer, 2022. [132] Yuning Du, Chenxia Li, Ruoyu Guo, Cheng Cui, Weiwei Liu, Jun Zhou, Bin Lu, Yehua Yang, Qiwen Liu, Xiaoguang Hu, et al. Pp-ocrv2: Bag of tricks for ultra lightweight ocr system. arXiv preprint arXiv:2109.03144, 2021. [133] Byung-Kwan Lee, Chae Won Kim, Beomchan Park, and Yong Man Ro. Meteor: Mamba-based traversal of rationale for large language and vision models. arXiv preprint arXiv:2405.15574, 2024. [134] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. [135] Derong Xu, Xinhang Li, Ziheng Zhang, Zhenxi Lin, Zhihong Zhu, Zhi Zheng, Xian Wu, Xiangyu Zhao, Tong Xu, and Enhong Chen. Harnessing large language models for knowledge graph question answering via adaptive multiaspect retrieval-augmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2557025578, 2025. [136] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseekvl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. [137] Elizaveta Goncharova, Anton Razzhigaev, Matvey Mikhalchuk, Maxim Kurkin, Irina Abdullaeva, Matvey Skripkin, Ivan Oseledets, Denis Dimitrov, and Andrey Kuznetsov. Omnifusion technical report. arXiv preprint arXiv:2404.06212, 2024. [138] Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng Li, and Yu Liu. Mova: Adapting mixture of vision experts to multimodal context. arXiv preprint arXiv:2404.13046, 2024. [139] Byung-Kwan Lee, Beomchan Park, Chae Won Kim, and Yong Man Ro. Collavo: Crayon large language and vision model. arXiv preprint arXiv:2402.11248, 2024. [140] Byung-Kwan Lee, Beomchan Park, Chae Won Kim, and Yong Man Ro. Moai: Mixture of all intelligence for large language and vision models. arXiv preprint arXiv:2403.07508, 2024. [141] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. [142] Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M. Anwer, Michael Felsberg, Tim Baldwin, Eric P. Xing, and Fahad Shahbaz Khan. Mobillama: Towards accurate and lightweight fully transparent gpt, 2024. [143] Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, et al. Openelm: An efficient language model family with open-source training and inference framework. arXiv preprint arXiv:2404.14619, 2024. [144] Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang 19 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models Shi, Raghuraman Krishnamoorthi, et al. Mobilellm: Optimizing sub-billion parameter language models for on-device use cases. arXiv preprint arXiv:2402.14905, 2024. [145] Byung-Kwan Lee, Sangyun Chung, Chae Won Kim, Beomchan Park, and Yong Man Ro. Trol: Traversal of layers for large language and vision models. arXiv preprint arXiv:2406.12246, 2024. [146] Byung-Kwan Lee, Sangyun Chung, Chae Won Kim, Beomchan Park, and Yong Man Ro. Phantom of latent for large language and vision models. arXiv preprint arXiv:2409.14713, 2024. [147] Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei Huang. Tinyllava: framework of smallscale large multimodal models. arXiv preprint arXiv:2402.14289, 2024. [148] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. Mobilevlm: fast, reproducible and strong vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886, 2023. [149] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan. Moe-llava: Mixture of experts for large vision-language models. arXiv preprint arXiv:2401.15947, 2024. [150] Yanyuan Qiao, Zheng Yu, Longteng Guo, Sihan Chen, Zijia Zhao, Mingzhen Sun, Qi Wu, and Jing Liu. Vl-mamba: Exploring state space models for multimodal learning. arXiv preprint arXiv:2403.13600, 2024. [151] Han Zhao, Min Zhang, Wei Zhao, Pengxiang Ding, Siteng Huang, and Donglin Wang. Cobra: Extending mamba to multi-modal large language model for efficient inference. arXiv preprint arXiv:2403.14520, 2024. [152] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [153] Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. [154] David Eigen, MarcAurelio Ranzato, and Ilya Sutskever. Learning factored representations in deep mixture of experts. arXiv preprint arXiv:1312.4314, 2013. [155] Yoshua Bengio, Nicholas LÃ©onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. [156] Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-ofexperts layer. In International Conference on Learning Representations, 2017. [157] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, AndrÃ© Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:85838595, 2021. [158] Shweta Singh, Aayan Yadav, Jitesh Jain, Humphrey Shi, Justin Johnson, and Karan Desai. Benchmarking object detectors with coco: new path forward. In European Conference on Computer Vision, pages 279295. Springer, 2024. [159] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 87698778, 2018. [160] Yash Goyal, Tejas Khot, Douglas SummersStay, Dhruv Batra, and Devi Parikh. Making the in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. [161] Zhuowan Li, Xingrui Wang, Elias StengelEskin, Adam Kortylewski, Wufei Ma, Benjamin Van Durme, and Alan Yuille. Super-clevr: virtual benchmark to diagnose domain robustness in visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1496314973, 2023. [162] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv e-prints, pages arXiv2407, 2024. [163] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun 20 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. In The 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021. [164] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. [165] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107, 2023. [166] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635 651, 2023. [167] Manoj Acharya, Kushal Kafle, and Christopher Kanan. Tallyqa: Answering complex counting questions. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 80768084, 2019. [168] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semistructured mathematical reasoning. arXiv preprint arXiv:2209.14610, 2022. [169] Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe. Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment. IEEE Transactions on Image Processing, 29:40414056, 2020. [170] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024. [171] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, et al. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024. [172] Adam Dahlgren LindstrÃ¶m and Savitha Sam Abraham. Clevr-math: dataset for compositional language, visual and mathematical reasoning. arXiv preprint arXiv:2208.05358, 2022. [173] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 15161520. IEEE, 2019. [174] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [175] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ãkos KÃ¡dÃ¡r, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300, 2017. [176] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. [177] Minesh Mathew, Viraj Bagal, RubÃ¨n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. [178] Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. M3cot: novel benchmark for multi-domain multi-step multimodal chain-of-thought. In Proc. of ACL, 2024. [179] Shuaichen Chang, David Palzer, Jialin Li, Eric Fosler-Lussier, and Ningchuan Xiao. Mapqa: dataset for question answering on choropleth maps. arXiv preprint arXiv:2211.08545, 2022. [180] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. [181] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi 21 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. [182] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision: Evaluating visionlanguage models in the wild with human preferences. arXiv preprint arXiv:2406.11069, 2024. [183] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 56485656, 2018. [184] Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In Proceedings of the 29th international conference on computational linguistics, pages 15111520, 2022. [185] Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. Solving geometry problems: Combining text and diagram interpretation. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 14661476, 2015. [186] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021. [187] Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. arXiv preprint arXiv:2212.02746, 2022. [188] Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi Chen, and Radu Soricut. Geomverse: systematic evaluation of large models for geometric reasoning. arXiv preprint arXiv:2312.12241, 2023. [189] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. Gllava: Solving geometric problem with multimodal large language model. arXiv preprint arXiv:2312.11370, 2023. [190] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. [191] Xinyu Huang, Yi-Jie Huang, Youcai Zhang, Weiwei Tian, Rui Feng, Yuejie Zhang, Yanchun Xie, Yaqian Li, and Lei Zhang. Open-set image tagging with multi-grained text supervision. arXiv preprint arXiv:2310.15200, 2023. [192] Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, Zhaohu Xing, Liangdong Wang, Zhou Cao, Jintao Jia, Zhuoyi Zhang, Yixuan Wang, et al. Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction data. arXiv preprint arXiv:2410.18558, 2024. 22 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models A. Related Work of Efficient VLMs With the advent of visual instruction tuning [15, 119] and the scaling of large language models (LLMs) [120, 121, 122], both large-scale open-source [17, 18, 19, 12] and closed-source [14, 54, 55] visionlanguage models (VLMs) have emerged. However, these large-scale VLMs impose substantial computational demands in real-world scenarios [123, 124], such as on-device or edge processing. Consequently, there is growing demand for lightweight VLMs that can be efficiently deployed on resource-constrained devices while maintaining fast inference, driving active research in efficient VLM design. Early efforts have mainly focused on integrating additional visual encoders [125, 126, 127, 128], multiple computer vision backbones [129, 130, 131, 132], or rational embeddings [133, 134, 135] into LLMs [136, 137, 138, 139, 140, 141]. In addition, growing body of research [142, 143, 144, 145, 146] has explored architectural strategiessuch as shared or repetitive feed-forward network (FFN) structures and expanded hidden dimensionsto enhance efficiency without significant performance degradation. Furthermore, several studies [147, 148, 21, 149, 150, 151] propose visiontext aligned training strategies, adopt Mamba architecture [152], or incorporate the mixture-of-experts paradigm [153, 154, 155, 156, 157] to achieve scalable model capacity. B. The Objective of GRPO For question ğ‘¥ and its multiple generated responses {^ğ‘¦ğ‘—}ğº (Generalized Reinforcement Policy Optimization) is defined as: ğ‘—=1, the RL objective of GRPO [61] â„’GRPO = Eğ‘— [Eğ‘¡ [min (ğ‘Ÿğ‘—,ğ‘¡ğ´ğ‘—, clip(ğ‘Ÿğ‘—,ğ‘¡, 1 ğœ€, 1 + ğœ€)ğ´ğ‘— ) ğ›½ğ·KL(ğœ‹ğœƒğœ‹ref )]] , where ğ‘Ÿğ‘—,ğ‘¡ = ğœ‹ğœƒ(^ğ‘¦ğ‘—,ğ‘¡ ğ‘¥, ^ğ‘¦ğ‘—,<ğ‘¡) ğœ‹ğœƒold(^ğ‘¦ğ‘—,ğ‘¡ ğ‘¥, ^ğ‘¦ğ‘—,<ğ‘¡) and ğ´ğ‘— = â„›ğ‘— mean ( {â„›ğ‘—}ğº ğ‘—=1 ) ) . ( std {â„›ğ‘—}ğº ğ‘—= (9) (10) Here, ğ‘Ÿğ‘—,ğ‘¡ denotes the policy ratio for new policy ğœ‹ğœƒ and old policy ğœ‹ğœƒold for each token ğ‘¡, and ğ´ğ‘— indicates the advantage computed by normalized rewards â„›. This objective encourages the new policy ğœ‹ğœƒ to improve upon the old policy ğœ‹ğœƒold according the advantage ğ´. The clipped surrogate objective limits the policy update ratio ğ‘Ÿğ‘—,ğ‘¡ to the range [1 ğœ€, 1 + ğœ€], preventing excessively large updates. In addition, KL divergence term ğ·KL(ğœ‹ğœƒğœ‹ref ) penalizes deviation from reference policy ğœ‹ref , ensuring regularization for stable training. In our Masters training setup, the total reward is computed as the sum â„› = â„›acc + â„›distill, from which the advantage is directly derived. Since the updating model is the student, the policy ğœ‹ğœƒ corresponds to the students logit-softmax output ğ‘ƒğ’®, and the parameter ğœƒ represents the students weight set Wğ’®. In our setup, the policy ratio ğ‘Ÿğ‘—,ğ‘¡ is always one because the student is updated only once per training iteration ğ‘–; hence, the old policy ğœ‹ğœƒold and the new policy ğœ‹ğœƒ are identical. Therefore, the clipped surrogate term becomes redundant, and the objective of GRPO simplifies to â„’GRPO = Eğ‘— [ğ‘Ÿğ‘—,ğ‘¡ğ´ğ‘— ğ›½ğ·KL(ğœ‹ğœƒğœ‹ref )] , (11) where ğ‘Ÿğ‘—,ğ‘¡ = 1. Technically, we still keep the ratio term in the expression to ensure the gradient properly flows to the student parameters during training. Additionally, we set ğ›½ = 0.1 to prevent the student from being updated excessively, providing stable regularization. 1 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models C. Visual Instruction Tuning Data We assemble 1.5M-sample visual instruction tuning dataset that encompasses both real-world and synthetic sources: COCO-ReM [158], iNaturalist2018 [159], VQA-v2 [160], Super-CLEVR [161], MAVIS [162], Geometry3K [163], SQA [164], AI2D [1], SA-1B [127], LLaVAR [165], VSR [166], TallyQA [167], TabMWP [168], KonIQ [169], InternVL [170]-filtered synthetic knowledge dataset covering politics, math, physics, chemistry, RLAI-F [171], CLEVR-Math [172], SROIE [173], ChartQA [2], DocVQA [174], FigureQA [175], GQA [176], InfoVQA [177], M3CoT [178], MapQA [179], OKVQA [180], TextVQA [181], WildVision [182], DVQA [183], GeoQA+ [184], GeOS [185], IconQA [186], UniGEO [187], GeomVerse [188], Geo170K [189], MathV360K [190], and RAM++ [191]-filtered synthetic data of Infinity-MM [192] covering coarse and fine-grained perception, relation, attribute, and logic reasoning. D. Additional Parsing Prompts for Accuracy Reward"
        },
        {
            "title": "Prediction Evaluation Prompt",
            "content": "System: You are an evaluation assistant that gives accuracy scores compared with Ground Truth and Generated Text from AI. Question is in <question> </question> tag. Ground Truth is in <ground truth> </ground truth> tag. Generated Text in <generated text> </generated text> tag. After reading the Question, compare the Generated Text against the Ground Truth summary: - If the Generated Text fully and correctly captures the core point 1 - If it is incorrect or irrelevant 0 - If it has repetitive response 0 - If it has empty response 0 Output the numerical evaluation score (0 or 1) after giving brief explanation. - The evaluation score should be wrapped in <answer> </answer> tag. User: <question> {} </question> <ground truth> {} </ground truth> <generated text> {} </generated text> Provide the numerical evaluation score after giving brief explanation. The evaluation score should be wrapped in <answer> </answer> tag. 2 Masking Teacher and Reinforcing Student for Distilling Vision-Language Models"
        },
        {
            "title": "Accuracy Reward Parsing Prompt",
            "content": "System: You are an evaluation assistant that gives binary accuracy scores (0 or 1) based on the provided overall summary. The summary will be wrapped inside <overall_summary> and </overall_summary> tag. After reading the summary, briefly output the integer score (0 or 1) without any text. Your final output must include only the integer value. User: <overall_summary> {} </overall_summary> Please output your integer accuracy score (0 or 1) based on the summary above without any text."
        }
    ],
    "affiliations": [
        "NVIDIA"
    ]
}