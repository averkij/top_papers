{
    "paper_title": "VIBE: Visual Instruction Based Editor",
    "authors": [
        "Grigorii Alekseenko",
        "Aleksandr Gordeev",
        "Irina Tolstykh",
        "Bulat Suleimanov",
        "Vladimir Dokholyan",
        "Georgii Fedorov",
        "Sergey Yakubson",
        "Aleksandra Tsybina",
        "Mikhail Chernyshov",
        "Maksim Kuprashevich"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation."
        },
        {
            "title": "Start",
            "content": "VIBE: Visual Instruction Based Editor Grigorii Alekseenko* Aleksandr Gordeev* Irina Tolstykh* Bulat Suleimanov Vladimir Dokholyan Georgii Fedorov Sergey Yakubson Aleksandra Tsybina Mikhail Chernyshov Maksim Kuprashevich R&D Department, SALUTEDEV Abstract *Equal contribution Corresponding author Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached new level, with dozens of open-source models released alongside highly capable commercial systems. However, only limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents compact, high-throughput instruction-based image editing pipeline that uses modern 2Bparameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation. Project page: https://riko0.github.io/VIBE/ 6 2 0 2 5 ] . [ 1 2 4 2 2 0 . 1 0 6 2 : r Figure 1. Illustrative examples of image edits generated by VIBE. 1 Figure 2. Illustrative examples of image edits generated by VIBE. 2 Figure 3. Illustrative examples of image edits generated by VIBE. 1. Introduction Instruction-based image editing models allow visual content to be modified according to natural-language commands and promise to democratize content creation. Compared to traditional retouching tools, which require substantial expertise, intuitive, language-based interfaces that are accessible to nonexperts. Consequently, instruction-guided editing has become one of the most active directions in generative AI. such generative models offer Recent proprietary systems have demonstrated rapid progress, including Google Nano Banana Pro [21] (Gemini 3 Pro Image [20]), OpenAIs GPT Image 1.5 [46] [47], and Black Forest Labs FLUX.1 Kontext models [6]. In contrast, open-source research generally trails in both quality and usability. Most open models remain large (6B to 20B parameters) and expensive to train and iterate on, which slows experimentation and limits accessibility [38]. Many practical systems start from pretrained textto-image diffusion backbone and adapt it to instructionbased editing. Under this setting, diffusion-based editing is shaped by three design axes: (i) how the reference image is injected, (ii) how the instruction is interpreted, and (iii) how the training pipeline is constructed. For reference-image guidance, two common families are (a) channel-wise concatenation of reference latents or features [7] and (b) tokenizing visual content and feeding it through the model as part of the input sequence [38]. For textual guidance, key architectural choice is whether to rely mainly on the diffusion backbones native text conditioning [6], or to add an external model that rewrites, expands, or structures the edit intent before conditioning the generator [15]. Many widely used text-to-image diffusion backbones are optimized for text-conditioned generation and therefore rely on text-only conditioning modules (e.g., CLIP [54], T5 [56], or even an LLM as in Sana1.5 [72]). In such pipelines, the conditioning module cannot observe the source image, so it cannot interpret the instruction in the context of the reference content. For image editing, this joint interpretation is often essential. The model must ground the request in what is actually in the input image to resolve ambiguity and preserve source-faithful details. We therefore use an instruction-tuned VLM that ingests both the instruction and the source image and produces clearer, image-aware conditioning signal for the diffusion. Since the diffusion backbone still expects conditioning in the representation space of its native text encoder, an additional design decision is the connector that maps the VLM representations into the diffusion models conditioning space [15, 38]. This paper investigates these architectural questions lowunder strict efficiency constraints. We target cost inference by combining computationally efficient channel-wise concatenation with learnable meta-tokens mechanism [49]. We train with four-stage pipeline: Alignment: adapting VLM to interface with the latent diffusion space via text-to-image objective on highaesthetic samples. Pre-training: learning core editing capabilities by adding relatively noisy image-to-image tasks on large-scale, data. Supervised Fine-Tuning: carefully tuning on clean and diverse triplets. Direct Preference Optimization (DPO) [65]: aligning the model using high-quality preference data with realworld instructions. The proposed pipeline is flexible and can be applied to other LLM/VLM and diffusion backbones. It also supports backbones that rely on relatively lightweight text encoders, such as the CLIP text encoder [54], because the alignment stage explicitly bridges the language model and the diffusion latent space. Another focus of our approach is to adopt model for real-world challenges, rather than for technical benchmarks. We focus on real user requests and curate or synthesize instructions that better match human phrasing than templated or purely LLM-generated prompts. The data collected for this pipeline spans diverse sources and is optimized for low noise and in-the-wild distributions. We combine specialist-model pipelines, distilled signals from both open and proprietary editing systems, autonomous triplet-mining pipelines, filtered open-source image editing and computer vision datasets, manually collected tripod-captured photographs, and additional sources. We also apply extensive augmentation, in particular, the pipeline relies heavily on triplet inversion and bootstrapping, which reduces data cost in both compute and annotation. Historically, different instruction-guided image editing methods assume different tolerances for unintended modifications to the source image, including the degree to which pixel-level appearance, scene composition, subject identity, and other attributes must be preserved. In this work, we target strict source consistency: any change not explicitly requested by the instruction is treated as an issue and addressed throughout all stages of training and evaluation. This objective is particularly challenging for edit categories that intrinsically encourage global transformations, such as style transfer. To maintain dataset quality, we use multi-stage filtering framework, including learned triplet scoring via finetuned Gemini-based validator and auxiliary checks such as face-embedding constraints for identity preservation and image-quality scoring to prevent quality degradation. In summary, our primary contributions are: 1. We present an open-source, ultra-fast, compact 4 instruction-guided image editing system trained on 15 million triplets, based on Qwen3-VL-2B-Instruct [4] and the Sana1.5-1.6B diffusion model [72]. 2. We propose flexible four-stage training pipeline that can be adapted to different diffusion backbones and LLM/VLM front-ends, enabled by our architectural choices. 3. We provide results, analysis, and insights covering experimental design, data collection, augmentation, and filtering, along with ablation studies. 2. Related Works Instruction-based image editing has rapidly evolved, with progress driven by innovations in model architectures, guidance mechanisms, and training strategies. Early methods were often training-free, operating directly on pretrained diffusion models via inversion or attention control [8, 14, 23, 44, 63]. While cost-efficient, these approaches struggle to achieve high-quality results. As result, the field has shifted toward training-based paradigms that fine-tune diffusion backbones on large-scale triplets Interestingly, many widely used [7, 18, 26, 76, 79, 83]. training triplets were bootstrapped with earlier editing systems, underscoring the tight coupling between scalable data generation and model progress [7, 79, 83]. 2.1. Production-oriented open editors and efficiency constraints. Despite rapid progress, production-level editing quality remains concentrated in limited number of systems. Recent open foundation editors increasingly unify text-to-image generation and instruction-based editing within single model family, but often rely on relatively large diffusion backbones: ranging from 6B to 20B parameters in recent releases (e.g., LongCat-Image/Z-Image at 6B, FLUX.1 Kontext [dev] at 12B, and Qwen-Image-Edit built on 20B Qwen-Image backbone) [5, 41, 53, 78]. Such scale raises both training and inference cost: it slows development iteration (ablations, retraining/fine-tuning, and production updates) and increases user-facing latency and cost per edit, reducing the number of interactive refinement cycles user can afford before reaching the desired result. Motivated by these costs, recent work has begun to study more computeefficient diffusion transformers and training recipes, including Sana-style backbones [72]. In this work, we focus on the same efficiency-first setting and pair compact 2B-class VLM with 1.6B diffusion backbone to deliver low-latency, low-cost edits with strict source consistency. 2.2. Architectures for Conditioning the Source"
        },
        {
            "title": "Image",
            "content": "A core design choice in diffusion-based editing is how to condition the denoising process on the source image. widely used and computationally efficient approach is channel-wise concatenation, introduced by InstructPix2Pix [7], where the source-image latent is concatenated with the noisy latent along the channel dimension. This design keeps inference lightweight and is often favored in latencysensitive settings. Another family uses token-wise multimodal conditioning, where visual content is tokenized and injected through attention as part of the model input sequence. This enables richer interactions between the source image, the instruction, and intermediate representations throughout the network [6, 38], but often comes with higher architectural and computational overhead. Recent foundation editors further popularize single-stream diffusion transformers that process text and image tokens in unified sequence, and report strong editing behavior as part of general generation-andediting capability [41, 68, 78]. In contrast, we retain the practical efficiency of channel-wise conditioning while relying on compact VLM guidance and data/recipe choices to reach production-level behavior under tight deployment constraints. 2.3. Architectures for Interpreting Instructions Another major axis is how the textual instruction is represented and grounded in the source image. Many editors rely primarily on the diffusion backbones native text conditioning and improve instruction following through data scaling and training recipes [6, 7, 40, 76, 79]. complementary line of work introduces stronger VLM to interpret the instruction in the context of the source image and to produce clearer edit intent for the generator [15]. Recent open foundation editors increasingly integrate strong VLM components directly into the editing stack. For example, Qwen-Image-Edit extends an open image foundation model with multimodal conditioning for instructiondriven edits [53], while LongCat-Image-Edit and Z-ImageEdit report dedicated editing variants trained within similarly unified generation-and-editing frameworks [41, 78]. Our pipeline follows the same high-level direction using modern VLM to guide image editing, but is explicitly optimized for throughput and strict consistency at compact scale. 2.4. Training Pipelines, Data, and Alignment Beyond model architecture, the training pipeline itself is crucial factor. While early works focused on dataset curation [7, 18, 26, 79, 83], recent research has investigated more sophisticated schemes, including multi-stage training and auxiliary objectives [16, 40, 60, 70]. common practical issue in editing fine-tuning is catastrophic forgetting, where adapting pretrained text-to-image model to specialized editing triplets can degrade its original generative prior, harming robustness and aesthetic quality. Another recur5 ring difficulty is interface alignment: when VLM is used to interpret edits, its representations must be mapped into the conditioning space expected by the diffusion backbone, and naive end-to-end training can be unstable or sampleinefficient. Many recent open-source pipelines refine editing behavior with post-training alignment signals, for example via preference-based objectives (and, in some cases, distillation from stronger teacher editors), to improve perceptual quality and instruction adherence [65]. Separately, recent foundation editors emphasize large-scale joint pretraining (often including image-to-image objectives) followed by supervised post-training and alignment as practical route to strong editing performance [41, 68, 78]. In our four-stage setup, we first perform an alignment stage that establishes VLM-to-diffusion connection by adapting the new VLM and connector to the frozen DiT models embedding space. This stage uses text-to-image objective on high-aesthetic data, stabilizing the interface before the model learns editing-specific behaviors. We then introduce large-scale image-to-image pre-training, followed by supervised fine-tuning on curated triplets, and finally apply preference-based post-training (DPO) to improve edit quality and reliability [55, 65]. To maintain real-world behavior, we emphasize aggressive quality control throughout data construction and training, including augmentation (e.g., triplet inversion and bootstrapping) and multi-stage filtering/validation to reduce unintended modifications and enforce strict source consistency. 3. Method Our architecture integrates two primary components: (i) Large Vision-Language Model which employs learnable meta tokens (detailed in Section 3.2) to interpret the users instruction within the context of the input image; and (ii) diffusion transformer that employs generative process to synthesize the edited image. To bridge these components, we use connector module designed to align the editing intent with the diffusion model, as detailed in Section 3.3. The overall pipeline is illustrated in Figure 4. In this work, we introduce model that generates images at 2K-class resolutions with diverse aspect ratios. This substantially improves quality in terms of preserving finegrained details from the source image. 3.1. Reference Image Guidance To guide the diffusion process with reference image IR, we first encode it into latent representation LR Rchw utilizing frozen VAE block. To integrate LR into the denoising pipeline, we employ channel-wise concatenation. In contrast to sequence-wise concatenation, which concatenates the reference latents along the token dimension thereby increasing the sequence length and directly increasing the computational cost of attention mechanism the channel-wise formulation concatenates the reference latents LR with the noise latents along the channel dimension. widened input convolution then restores the original channel dimensionality and projects the result into token space. This preserves the number of tokens and therefore leaves the attention complexity unchanged, maintaining high generation throughput. 2.5. Consistency and Real-World Instruction Dis3.2. Textual Guidance Based on VLM tributions Instruction-guided editing methods differ substantially in their tolerance for unintended changes to the source image, including identity preservation, background stability, lighting consistency, and fine-grained appearance control. Maintaining strict source consistency is especially challenging for edit categories that encourage global transformations (e.g., stylization) or that require delicate, localized modifications without collateral drift. Another practical gap is the instruction distribution. In many academic datasets, instructions are annotator-written or LLM-generated and can differ from real user queries in phrasing, ambiguity, and intent. While recent datasets and human-feedback efforts improve coverage and quality [26, 81, 83], matching in-the-wild instruction style remains challenging. Our work explicitly targets real user behavior by grounding instruction text in realworld queries and filtering aggressively for consistency, enabling compact model to behave reliably under realistic prompting. Interface between VLM and Diffusion model In [49] the authors demonstrate that directly using hidden states from the final layer of an VLM is suboptimal way to guide diffusion model. To effectively bridge the modality gap, we randomly initialize special meta-tokens and add them into the VLMs vocabulary while keeping the model weights frozen. The number of tokens is treated as hyperparameter. During the forward pass, meta tokens are concatenated manually with instruction tokens and propagated through all layers of the network. It is noteworthy that we do not rewrite the user instruction into expressive instructions as seen in MGIE [15], but do add prompt prefix such as What would the image look like if {user instruction}?. ˆTM = VLM(cid:0)IR, UI, TM (cid:1). (1) Here, IR denotes the reference image, UI is the sequence of instruction tokens, and ˆTM RN are learnable meta-token embeddings. The VLM jointly processes the 6 Figure 4. Model architecture. concatenated sequence through all transformer layers and outputs contextualized meta-token hidden states ˆTM. 3.3. Connector Design The contextualized meta-token hidden states ˆTM are then mapped to the conditioning space of the diffusion backbone via lightweight trainable connector module. Concretely, the connector is implemented as stack of Transformer encoder blocks operating only on the meta-token sequence: CT = Connector (cid:16) ˆTM (cid:17) , (2) where CT denotes the resulting conditioning features used by the diffusion model. 3.4. Training Approach Connector Alignment In our configuration, both the VLM and the diffusion model are initialized from the pretrained checkpoints. Only connector module and meta tokens are randomly initialized and trained from scratch. Consequently, during the initial training phases, the signal transmitted from the VLM to the diffusion model via the connector is significantly distorted. While the weights of the connector are coming to reasonable values, the weights of the pretrained and unfrozen diffusion model undergo significant alterations. This leads to an irreversible degradation of its generative capabilities, thereby reducing the quality of the final output. To address this issue, we propose an intermediate preadaptation step for the connector. We freeze the VLM and the diffusion model and train the pipeline exclusively on text-to-image generation task. Once satisfactory performance metrics are achieved, we consider the connector to be aligned. Subsequently, we proceed to train the model on the primary image editing task. Observation Incorporating this additional alignment stage not only enhances the quality of the generated images but also improves the models ability to follow instructions. consisting Injections Data training editing models image datasets T2I practice common use to is for of exclusively specialized source image, instruction, target image triplets. However, we find that this strategy can substantially degrade the models foundational text-to-image generation capability. In practice, the model overfits to artifacts in the relatively limited editing data and consequently generalizes poorly to real-world images and user instructions. To address this issue, we propose mixed-data training strategy that frames editing as constrained image generation, rather than as plain image-to-image translation. We train the model on instruction-based editing triplets together with set of high-quality text-to-image pairs. Technically, we mix both data types within each batch. For T2I samples, we feed an empty (black pixels) conditioning image which is masked out in the attention layers. Additionally, we employ task-specific text templates: for T2I, the input is struc7 tured as generate the image by description: {prompt}, while for editing, we use what will this image be like if {prompt}. This joint training provides two benefits: (i) it regularizes learning and reduces overfitting to the limited, often artificial editing data, and (ii) it preserves the models original generative prior by keeping standard text-to-image generation active throughout training. Observation 2 Multi-task training prevents drift from the robust pre-trained initialization, which is crucial for highfidelity edits that require synthesizing new content (e.g., object addition). Text-to-image data acts as distributional anchor, keeping the final model both strong editor and capable generator for flexible, creative image manipulation. Multi-stage training To enhance training efficiency following the connector alignment phase (performed at resolution of 5122), we adopt multi-stage training strategy for the DiT model. The detailed configuration of our training pipeline, including data ratios and resolution strategies, is summarized in Table 1. During the pre-training stage, the model is trained at an average resolution of 10242 with variable aspect ratios. Subsequently, we execute the SFT phase, performed at resolutions up to 20482. In this phase, we utilize large-scale, high-quality, and strictly filtered dataset (described in Section 5.2 and Section 5.3) comprising both synthetic and real images. Throughout the pre-training and SFT phases, we jointly optimize the model for two tasks: image editing and T2I generation. During these stages, the learnable meta-tokens, the connector module, and the diffusion model are updated, while the VLM backbone remains frozen. Following these supervised stages, we employ DPO for the DiT model (see Section 3.5). Notably, regarding resolution management, we diverge from traditional progressive resizing [13]. Since we finetune pre-trained diffusion backbone rather than training from scratch, the standard low-resolution warm-up becomes redundant. Instead, we employ mixed-resolution strategy during both pre-training and SFT, training simultaneously on data spanning resolutions from 3842 to 20482 with diverse aspect ratios. This approach yields several key benefits: It ensures the model preserves its robust high-resolution generative priors while adapting to the editing task. The simultaneous processing of varied resolutions preserves the diversity of triplets, and allow us to avoid image upscaling, which can harm generation quality. To implement this efficiently, we utilize adaptive batch sizing. We dynamically adjust the batch size based on input dimensions by increasing the batch size for lower-resolution inputs to ensure full utilization of GPU resources. Observation 3 Simultaneous multi-resolution training with diverse aspect ratios significantly accelerates convergence and results in superior generation quality compared to iterative resolution increase. 3.5. Preference Alignment Preliminaries Diffusion-DPO [65] adapts the Direct Preference Optimization framework [55] to align diffusion models with human preferences. Unlike RLHF[48], which requires training separate reward model, DPO optimizes the policy directly using ranked pairs of images (xw, xl) conditioned on context c. While standard DPO relies on exact log-likelihoods log pθ(xc), these are intractable for diffusion models. To address this, Diffusion-DPO approximates the likelihood ratio using the Evidence Lower Bound (ELBO), reformulating the objective via denoising errors. The loss function is defined as: LDiff-DPO(θ) = E(xw,xl,c),t,ϵ [log σ (β (δθ(xw) δθ(xl)))] , (3) where δθ(x) represents the implicit reward derived from the difference in reconstruction errors between the reference model (ϵref) and the trained model (ϵθ): δθ(x) = ϵ ϵref(xt, t, c)2 2 ϵ ϵθ(xt, t, c)2 2 (4) Here, xt denotes the noisy latent at timestep t, ϵ is the added noise, and β is regularization hyperparameter. Intuitively, this objective encourages the model to minimize the denoising error for the preferred image xw relative to the reference model, while effectively increasing it for the disfavored image xl. Post-training During the post-training phase, we employ Diffusion-DPO to align the model with human preferences. Specifically, we utilize DPO to address two primary challenges: (i) visual artifacts that arise during real image editing, and (ii) failures in instruction adherence. The detailed process for dataset construction is provided in Section 5.6. In the context of multi-reward optimization, targeting both instruction adherence and aesthetic quality, we eschew scalarization techniques, such as weighted sums or geometric means, to define the preference direction. These rigid 8 formulations often fail to reconcile the inherent inconsistencies between conflicting objectives; for instance, optimizing aggressively for image aesthetics may inadvertently degrade the models faithfulness to the editing instructions. Consequently, relying on single aggregated score as proxy for utility risks reward over-optimization and results in unbalanced alignment. While recent approaches, such as DreamBoothDPO [2] and CaPO [36], introduce complex sampling strategies to navigate the multi-preference distribution, we explore more direct avenue to achieve Pareto optimality. We adopt strict dominance strategy for preference pair construction: during training, we select pair (xw, xl) only if the preferred sample xw strictly outperforms the rejected sample xl across both reward criteria simultaneously. Observation 4 Strict-dominance pair filtering reduced reward overoptimization and produced more balanced gains than scalarized objectives. In our experiments, it matched or outperformed more involved multipreference sampling strategies. 3.6. Implementation Details We employ the Qwen3-VL-2B* [4] model as the VLM backbone, which produces hidden states with an embedding dimension of 2048. For our text-to-image generation backbone, we use the Sana1.5-1.6B model[73]. We utilize 224 learnable meta tokens and the connector consists of 4 Transformer encoder blocks, with these hyperparameters selected through extensive empirical experimentation. 4. Assessor Accurate evaluation of image editing quality remains an open problem, as standard metrics often fail to correlate sufficiently with human perception. In our work, robust automated metric is essential, serving as the primary tool for filtering training data. Following the approach in [34], we developed specialized assessor. Initially, we fine-tuned Gemini 2.0 Flash model on set of 4350 examples. Subsequently, we expanded the dataset to 12 335 examples and trained nonproprietary Qwen-2.5-VL-7B model utilizing LoRA [25]. Validation was performed on held-out set of 2994 samples. Table 2 presents the performance of our models compared to vanilla baselines. As shown, the fine-tuned models demonstrate significantly higher correlation with human *https : / / huggingface . co / Qwen / Qwen3 - VL - 2B - Instruct https://huggingface.co/EfficientLargeModel/ SANA1.5_1.6B_1024px judgments compared to their vanilla counterparts. This confirms that task-specific fine-tuning is essential for establishing reliable filtering tool. 5. Datasets For pretraining, mixtures of publicly available largescale editing datasets, together with synthetic data from perception and recognition datasets, were initially explored, totaling up to 21 million triplets. The goal was to initialize the model for image editing with broad coverage by training on many edit types, scenes, and instruction styles. However, this early-experiment mixed corpus was too noisy and led to degradation in downstream quality. Observation 5 Despite large-scale, high-quality SFT, we observed persistent negative transfer from noisy pretraining, with artifacts and failure modes introduced early not fully overridden during SFT. Using early prototype models and recent open source models, the most diverse dataset was therefore remastered and smaller but higher-quality subset was selected, totaling 7.7 million triplets. This size was still large enough to maintain diversity, but it was close to the size of the SFT dataset, so SFT could shape the final behavior, while pretraining still added broad instruction and content diversity. For T2I, an additional 48 million aesthetically curated images from multiple T2I datasets were assembled. These images or subsets were used during both pretraining and SFT to improve the models ability to generate visually appealing content. For SFT, 6.8 million high-quality triplets from diverse sources were used, including inverted samples and compositional bootstrapping. For DPO, specially designed Generation-Augmented Retrieval-based dataset with 176 532 highest-quality triplets and real-world instructions was used. Summary can be seen in Table 3. 5.1. Pretraining UltraEdit Remake In early experiments, the strongest pretraining results were observed when using UltraEdit as basis for extension among other large-scale datasets, due to its diversity. At the same time, the original UltraEdit images were low resolution (512512) and all images were square, which was problem for our multi-resolution training. Overall noise was also extremely high due to different 9 Table 1. Training stages of the proposed architecture. The pipeline consists of an initial alignment of the connector, followed by multistage training of the diffusion backbone (DiT). The columns Edit (%) and T2I (%) denote the data sampling ratio between editing triplets and text-to-image pairs. Note that the VLM backbone remains frozen throughout the entire process. Training Stage Resolution Trainable Modules Data Ratio T2I (%) Edit (%) Data Composition I. Connector Alignment 5122 10242 II. Pre-training 20482 III. SFT IV. Preference Alignment 20482 Connector, Meta Tokens 100% 0% Text-to-Image pairs DiT, Connector, Meta Tokens DiT 68% 34% 0% 32% Editing triplets + T2I data 62% Large-scale high-quality filtered triplets + T2I 100% Preference pairs (xw, xl) Table 2. Quality metrics of the assessor models on validation data (N = 2994). Instruction, Aesthetic. Here (V) denotes vanilla base models, and (F) indicates fine-tuned models. Table 3. Principal triplet sources after filtering. LVIS, HaGRID, and EasyPortrait contribute to both stages; only subset of their samples is used during pretraining."
        },
        {
            "title": "Model",
            "content": "I MAE ρ MAE ρ Qwen-2.5-VL-7B (V) Gemini 2.5 Flash (V) Gemini 3 Flash (V) Gemini 2.0 Flash (F) Qwen-2.5-VL-7B (F) 1.030 1.040 0.863 0.687 0.672 0.437 0.452 0. 0.649 0.641 0.936 0.862 0.709 0.601 0.551 0.198 0.289 0.486 0.496 0.573 types of issues (see Figure 5, first row). Eventually, despite very good diversity, this dataset had the lowest overall quality among all large-scale datasets, as shown in [34]. Because it includes text captions of source images, the images were regenerated with proprietary and internal models. Higher resolutions were sampled randomly from the range [860, 2200] for each dimension, with the aspect ratio restricted to [1:6, 6:1]. Prompt adherence and content consistency were validated with Qwen2-VL [3]. Then, the automated self-mining pipeline initially described in [34] was applied, excluding the Gemini-based validation stage [19] to reduce cost at the pretraining scale. Conceptually, this pipeline over-generates multiple candidate edits for each source image, instruction pair using an instruction-guided image editing model, and then filters or ranks candidates with validator to retain only high-fidelity source image, instruction, edited image triplets. Given the dataset size, retry strategy was used for this dataset: candidates were generated until one passed all In total, 6 420 724 checks or 5 attempts were exhausted. triplets were obtained, including the same inversion described in [34]. See Figure 5 for examples. 5.2. Supervised Fine-Tuning datasets In this section, several of the most novel and practically important approaches used to mine triplets for the SFT stage are described. Pretraining UltraEdit Remake Aurora LVIS HaGRID EasyPortrait Total Supervised fine-tuning (SFT) Autonomous self-mining pipelines LVIS NHR-Edit Stylization Concept Sliders SEEDPS (parts 2 and 3) Automated inpainting HaGRID EasyPortrait GIER Low-level processing dataset Real tripod photos Other sources (manual in-house retouching, manual inpainting, 3D renders, and smaller curated collections) Total GAR based Dataset Total 6 420 724 160 373 1 000 000 107 619 40 000 7 728 2 913 829 1 000 000 720 088 726 560 195 525 189 572 177 739 107 619 40 000 5462 3597 4139 800 000 6 800 000 176 532 5.2.1. Real Tripod Photos substantial limitation of most automated mining methods is that either the input or the output image contains synthetic artifacts that can bias training. Because the target is pixel-accurate editing and physical plausibility (e.g., shadows, reflections, transparent materials), real triplets with strict camera immobility were additionally collected. Prior work (e.g., ObjectDrop [67] and OmniPaint [77]) 10 Instruction: Add unicorn to the scene Instruction: Turn Big Ben into clock tower covered in vines and flowers Figure 5. Examples of UltraEdit. Top row is the original set, bottom row is the remastered version. Figure 6. Example of background removal on the LVIS dataset. High-quality dataset annotations and carefully crafted engineering heuristics enable automatic instruction generation, making localization and object pointing somewhat tricky. Instruction: Preserve exclusively the left zebra. 11 Instruction: Make him 30 years younger Instruction: Go from smile to an intense surprise Instruction: Make the dingo slim, without muscles Figure 7. Examples of Visual Concept Sliders triplets. Instruction (inverted): Swap the modern refrigerator on the left for large antique wooden hutch with glass doors. Figure 8. Example of triplet obtained with an inpainting model and the LVIS dataset. 12 Instruction: Add the woman, wearing gray jacket and jeans, who is transporting black rolling crate, holds yellow and black object in her hand. The hallway, with its industrial lighting and cream-colored walls, stretches out behind her. Figure 9. Example of triplet obtained from the RORD dataset. Instruction: Transform style to real Instruction: Stylize the image in colorful origami style Instruction: Draw giraffe in cubist style Figure 10. Examples from the LVIS stylization dataset. suggests that even few thousand high-quality real pairs can substantially improve the modeling of object-induced effects such as shadows and reflections. Motivated by this, crowdsourcing platform was used with task that required capturing before and after photo under strict noshift protocol (tripod or equivalent locking method). Detailed user instructions described what can and cannot be photographed. In total, 4139 triplets (including augmentations) were collected. See Example XXX. 13 5.2.2. Real Triplets from Videos To obtain more triplets of comparable physical realism, the existing RORD dataset [57] was leveraged, which consists of frames extracted from videos recorded with static camera. Only indoor scenes were used, since outdoor videos often contain small background changes (e.g., pedestrians, cars, moving leaves, animated billboards, or traffic lights) that violate the no-shift requirement. Because the videos contain many near-duplicate frames, only about 10% were retained. Two selection strategies were evaluated: sampling I-frames from MP4 files, and selecting diverse frames using MobileNetV3 embeddings [24] via [27]; the embedding-based approach performed better. For person addition, it was required that the target image included at least the upper body and fully visible head (not partial body parts). Samples were filtered with Qwen2VL [66] to enforce this constraint. Multiple blur measures (Variance of Laplacian, FFTbased metrics, and the Tenengrad measure) were evaluated, and the Blur Effect metric [11] (implemented in [64]) worked best. Finally, Qwen2-VL [66] was used to generate editing instructions. See an example of dataset triplet in Figure 9. 5.2.3. Virtual Try-On The VITON-HD dataset [10] was processed with OOTDiffusion [74] to obtain paired examples for garment changes. To minimize artifacts, only images where the persons hands and hair do not overlap the clothing were kept. To make the resulting triplets more realistic and diverse, set of background images was collected and the person was composited onto them at several positions and scales. Person mattes were extracted with StyleMatte [9], then both the original VITON images and the OOT-Diffusion outputs were composited onto the same backgrounds. After compositing, the images were harmonized with DucoNet [62] to make the lighting more consistent. To generate instructions, the target garment image (without the person) from VITONHD was first captioned using LLaMA-3.2-Vision-Instruct11B [43], and then rewritten into editing instructions of varying lengths using LLaMA-3.1-8B-Instruct [42]. Despite these steps, some artifacts remained, e.g., mismatched skin tone, missing or distorted tattoos, neck and jewelry inconsistencies, sleeve artifacts, and matting issues such as white halos around the subject or coarse masks, so final filtering stage with an assessor was applied. 5.2.4. Stylization The stylization dataset was composed of 2 parts: Object-level stylization. The LVIS dataset [22] and its instance segmentation annotations were used to stylize only selected objects in an image. To our knowledge, there is no existing dataset for this setting, although the task is challenging and highly relevant for real-world applications. The entire image was first stylized using Stable Diffusion XL [51] with Depth ControlNet [80], and then the original image was composited with the stylized object region using the LVIS mask. Full-image stylization. Images from LAION [58] were stylized using SDXL with Depth ControlNet, and images from Open Images Dataset v7 [45] were stylized using Qwen-Image [68]. To enable an additional capability, these triplets were inverted to obtain the task change any style to realistic. Overall, the dataset covered more than 500 styles and contained 363 280 stylized triplets, along with the same number of inverted triplets. See Figure 10 for the examples. 5.2.5. Visual Concept Sliders [17] provide fine-grained attribute control in diffusion models with LoRA adaptors [25]. Using this approach, base images were generated with SDXL [51] and paired edits were produced that modify single attribute, with controllable intensity and direction when supported by the slider. To reduce ambiguous cases, prompts were crafted that discourage the attribute from being already shifted (e.g., man of medium build is standing in the center of the square... for muscularity slider), then controlled variations were generated. Using this approach, the following slider categories increases apparent age. Only the positive (aging) were mined: Surprised: controls the degree of surprise. Age: direction was used. Chubby: controls perceived chubbiness. Muscular: controls muscularity. It was applied to both people and generated animals, where it worked unexpectedly well. Tropical: controls perceived tropicalness of scene. This slider performed poorly on average, so prompts were restricted to scenes where the concept is visually supported (e.g., forested environments). To enrich and standardize instructions, MiVOLOv2 [32, 33] was used for age and gender estimation. This enabled instructions such as Create an image of this {gender} at {age} years with explicit age values. All images were required to contain exactly one person, enforced using detector model. Because slider-based edits can unintentionally alter nontarget attributes, additional constraints were applied using age and gender estimation. Gender preservation was enforced for all sliders, and for non-age sliders the age change was limited to at most 3 years. Fixed-seed generations were also used to create additional transitions between sliders, e.g., an original image, surprised variant, and smiling variant from the same seed can yield an instruction like Make surprised {gender} smile little. 14 See Figure 7. Using this method, 195 525 triplets were mined. 5.2.6. Autonomous triplet-mining pipelines 3) (189 572 triplets), GIER [59] (5462 triplets), low-levelprocessing dataset [50] (3597 triplets), and NHR-Edit [34] (720 088 triplets). the configurations of self-mining pipeline Multiple from [34] described in Section 5.1 were used. While configurations differed in the generator stack and filtering stages, they shared the same high-level structure: overgenerate candidate edits and retain only those that pass automated validation. For SFT, diverse input sources were used including Open Images Dataset v7 [45], multiple open-source collections of real photos, and images scraped from range of internet domains, with an emphasis on realistic user-like photography. As generative models, Qwen-Image [68] and proprietary models were used. Using this method, 2 913 829 triplets were mined, including additional filtering described in Section 5.4 and the same augmentation techniques. 5.2.7. Automated Inpaint Inpainting triplets were generated using inpainting-capable diffusion models with ControlNet conditioning [80]. Combined with LVIS and Alimama datasets that include segmentation annotations, this yielded 177 739 triplets. See Figure 8 for the example. 5.2.8. Perception and Recognition Datasets For pretraining, and smaller portion mixed into SFT, several computer vision and perception datasets were incorporated to strengthen base visual understanding, with an emphasis on human body and face anatomy, as well as object localization. HaGRID [29] was used to construct instruction-based triplets by inpainting gestures within annotated bounding boxes and generating prompts such as add gesture X. Using this procedure, 107 619 triplets were mined. For facial anatomy, EasyPortrait [28] was used: selected face parts were masked and then inpainted, yielding 40 000 triplets. Finally, LVIS was used to generate segmentation-centric triplets. One or more objects were sampled from LVIS annotations and instructions were produced that require localizing and segmenting these objects. Background-removal triplets were also created where the model is instructed to remove the background and all objects except one (or small set) of selected instances, resulting in 1 000 000 triplets. See Figure 6 for an example. 5.2.9. Open Source Datasets For pretraining, the Aurora [31] dataset (160 373 triplets) was also used. For SFT, the following open-source datasets were used (filtered or augmented depending on quality and whether multi-turn edits were available): SEEDPS [18] (parts 2 and 15 5.3. Generation Augmented Retrieval based"
        },
        {
            "title": "Dataset",
            "content": "Data format and motivation There are several common ways to build datasets for image editing: Fully synthetic (automatic). Captions and edit instructions are generated by an LLM; the input image is synthesized with text-to-image model; the edited image is produced by either specialized module (e.g., inpainting, sliders) or general-purpose image editor. This approach scales well, but is highly prone to domain shift. Semi-synthetic (automatic). The same pipeline, but the input image is real photograph rather than generated one. Semi-synthetic (manual). human writes the instruction and professional artist performs the edit. This typically yields much higher quality, but is expensive and hard to scale. Fully real (manual). Both images are real photos captured under controlled conditions (e.g., using tripod/locked camera), which best preserves pixel-level alignment and faithfully captures lighting, shadows, reflections, and transparency. However, it is difficult to scale and limited to constrained set of edits (e.g., it cannot cover stylization or adding fantastical objects). Real images (automatic). Triplets mined from videos. This can scale well (especially if instructions are generated), but extracting consistent pairs is challenging when the camera or the scene is dynamic. Across these settings, instructions are often treated as real if they are written by person. In practice, however, asking annotators to invent edit instructions for dataset creation does not match how image editing models are used. If the text distribution is expected to reflect real-world behavior, it requires genuine user edit queries. One option is to use Photoshop request datasets, but these are typically small. Another is to reuse prompts from model-comparison platforms (e.g., diffusion arenas), similar to what is done for text-to-image in the Open Image Preferences dataset. However, this source is still biased: the prompts are written for anonymous model ranking rather than natural user intent. Even large-scale prompt collections like DiffusionDB (from Discord) tend to overrepresent experienced prompt writers and include keywordheavy phrasing that is unnatural as everyday language Therefore, we collected real-world requests from all available open and internal sources, cleaned this corpus to remove noise and non-edit intents, removed duplicates, and made all other necessary preparations. Image Data Sources Open Images V7 [35] was used as the source image dataset and 200k samples were downloaded at 2K resolution, which served as anchor images for the editing queries. To model natural instruction language, set of in-the-wild editing instructions was used. Discovering an edit taxonomy To identify the most common user intents, the collected corpus was clustered. First, each instruction was embedded using the FRIDA embedder [61]. Next, clustering yielded 50 large clusters that correspond to stable semantic groups of requests. Finally, Qwen3-VL-32B [4] was used to interpret each cluster by generating human-readable category name and cluster description. The category list was further expanded with small set of heuristic additions to improve coverage for rare but important edit modes. The result of this stage is practical taxonomy of edits that corresponds to the actual distribution of user requests and is suitable as basis for subsequent instruction generation. Image-Conditioned Instruction Generation Using the discovered semantic clusters, an initial set of imageconditioned edit instructions was synthesized for downloaded samples from the Open Images dataset. For each image, 8 instructions were generated that span different categories in the taxonomy, using Qwen3-VL for generation. This stage yielded instructions that were semantically valid and diverse across edit types, but the phrasing could still be noticeably synthetic and might require further grounding to better match real user language. Retrieval-based grounding to real user phrasing To replace synthetic wording with natural user language, retrieval pipeline was built over FRIDA embeddings in paraphrase mode. All unique instructions were indexed in Qdrant [52] vector database. For each artificial instruction Qart, its embedding was computed and the top-K nearest instructions were retrieved (with K=20). One user intent Quser was then selected via stochastic sampling from the top-K candidates, converting similarities into probability distribution using softmax: pi = exp(si) j=1 exp(sj) (cid:80)K , (5) where si is the cosine similarity score between Qart and the i-th Quser. This choice (instead of deterministic top-1) increases lexical diversity and reduces overuse of the same ideal formulations. Mitigating bias toward popular prompts Pairs with insufficient semantic similarity were filtered out using threshold criterion, limiting the risk of semantic drift when replacing text. nearly inevitable effect of retrieval matching is concentration on small set of well-phrased instructions that match many artificial instructions. To preserve diversity, frequency cap was introduced: the same instruction may appear in the final dataset at most 3 times. Validating instruction applicability to the image Even with strong semantic similarity to the synthetic intent, mismatches with the image can occur (e.g., the instruction refers to an object absent from the scene). Therefore, VLM-based validation stage was added: Gemini 3 Flash checked whether the instruction is applicable to the image x. If the instruction was not applicable, the model attempted to apply minimal text edit (preserving the original style) to make the instruction executable for the given image; if minimal correction was not possible, the pair was discarded. This step acts as an instruction image consistency filter and gentle correction mechanism for borderline cases. Generating target images After filtering and deduplication, we obtained 10k images, each associated with 4 to 8 valid in-the-wild instructions. To produce target images y, pairs (x, t) were sent to high-capacity proprietary image editing models, and the edited results were collected to form final triplets (x, t, y). We distributed the workload across several models to balance quality and diversity. The editing results were filtered using an in-house Qwen2.5-VL assessor (a detailed model description is provided in Sec. 4). Inverted and composite instructions. After generating the target images y, the number of training triplets was further increased by reusing already generated edits for the same source image. visual illustration is provided in Figure 11. Assume that for an input image we obtain edited variants {yi}N i=1, where each mapping (x, ti) (cid:55) yi forms base triplet (x, ti, yi). i=1 with corresponding instructions {ti}N Instruction inversion. For each edited image yi, the reverse editing task is constructed: recover the original image from yi. This corresponds to building an inverse instruction describing the transformation (yi, t1 ) (cid:55) x, which yields additional triplets of the form (yi, t1 , x) and thus makes the dataset bidirectional with respect to the source scene. Composite transitions between two edits. In addition, using the shared anchor x, transitions between pairs of edited images (yi, yj) for = are constructed. Intuitively, 16 Figure 11. Composite mining process. Figure 12. Cases of artifacts on edited images. to move from yi to yj, one needs (i) to undo the edit that produced yi (i.e., apply t1 ), and then (ii) apply the edit tj. composite instruction tij was formed that is semantically equivalent to the sequence (t1 then tj) and corresponds to the transformation (yi, tij) (cid:55) yj. Therefore, for fixed source image and set of edits, the number of possible directed transitions between edited variants is (N 1). Resulting dataset integrated (i) wide coverage of scenes and object categories from Open Images V7, (ii) user-like instruction phrasing obtained by grounding synthetic intents in large-scale in-the-wild queries, (iii) explicit instructionimage consistency enforcement via VLM-based applicability filter, and (iv) standardized pipeline for generating target edits. The final dataset comprised 176 532 triplets. 5.4. Issues and Filtering task-tuned Gemini validator [19] from [34] was initially employed to clean all SFT datasets, covering forward, backward, and bootstrapped operations. filtering threshold of 3.5 was applied, resulting in the removal of approximately 15% of the data. Visual inspection of the retained samples confirmed that this metric effectively preserved high-quality instruction alignment. These issues were addressed directly. The diffusionbased editing models occasionally produced high-frequency 17 artifacts resembling checkerboard patterns or JPEG compression noise at the borders of outpainting regions and in random parts of the image, particularly on human faces and uniform regions like the sky. Empirical analysis revealed strong correlation between these visual artifacts and spatial shifts, most notably the repositioning of human faces from their positions in the input images. Since standard detection algorithms proved ineffective, geometric filtering heuristic based on facial alignment was implemented. For each input-output pair, faces were detected and the Intersection over Union (IoU) of the largest detected face was calculated. strict spatial constraint was enforced, discarding any training pairs where the face IoU fell below threshold of 0.9. While this aggressive filtering resulted in the removal of approximately 35% of the data, it proved essential for eliminating the visual artifacts and preventing the model from memorizing these degradation patterns. Additionally, both generated and real data (predominantly generated samples) were found to frequently suffer from minor global geometric inconsistencies, such as small shifts, unintended crops, or stretching. To mitigate this, the homography between the input and output images was calculated to align the pairs precisely. This correction ensured spatial consistency, allowing the model to focus on the editing task rather than compensating for trivial misalignment. 5.5. Synthetic Augmentation Pipeline To robustly adapt the model to varied user inputs, Just-inTime (JIT) synthetic augmentation strategy was employed. Instead of generating static files, images from the prepared dataset were dynamically transformed during training to create new triplets on the fly. This effectively multiplied the dataset size and enforced consistency across diverse editing scenarios. Bidirectional Photometric and Restoration Operations. Reversible transformations were grouped into pairs to facilitate bidirectional learning. The model was trained to both apply and reverse effects for blur/deblur, noise/denoise, sepia/desepia, and grayscale/colorization. Crucially, for the colorization task, the source image was not simply desaturated. Instead, an upgraded grayscale synthesis pipeline was employed that simulated analog film characteristics through randomized channel mixing, sigmoid contrast adjustments, and realistic grain injection. Additionally, scalar adjustments for brightness, contrast, and saturation were employed in both increasing and decreasing directions to cover full spectrum of global photometric changes. Instruction Adherence and Invariance. To prevent over-editing and ensure strict adherence to prompts, two specific constraints were introduced: Identity Mapping (Do Not Change): Triplets where the source and target images are identical were generated. Paired with passive instructions (e.g., do nothing), this taught the model to preserve image fidelity when no edits are requested. Mirror Augmentation: Horizontal flipping was selectively applied to inputs to increase visual diversity. Crucially, this was conditional: mirroring was disabled for prompts containing directional terms (e.g., left, text) to ensure the model correctly grounds spatial instructions while becoming invariant to global orientation elsewhere. Structural and Typographic Editing. Complex structural changes were simulated by overlaying geometric primitives (synthetic inpainting) or rendering variable text. These were paired with precise instructions to fill areas or modify specific words, training fine-grained spatial control. Real-world Quality Adaptation. To bridge the gap between pristine training data and potential low-quality user uploads, synchronized JPEG compression was applied to both source and target images. This accustomed the model to processing inputs with high-frequency loss and compression artifacts without editing degradation. 5.6. DPO Data Preparation To effectively align the model with human preferences and ensure robustness across different editing scenarios, composite preference dataset DDPO was constructed. This dataset was derived from three distinct sources, each targeting specific aspects of generation quality: 1. Self-Generated Preferences (On-Policy). dataset was constructed by generating large corpus of images using the SFT model itself. These generations were subsequently annotated by the in-house assessment model described in Section 4, which assigned scores for both aesthetic quality and instruction adherence. Based on these scores, pairs (xw, xl) were formed. This dataset served as feedback signal for selfcorrection. By exposing the model to its own failures (xl) versus its successes (xw), it effectively targeted the suppression of model-specific visual artifacts, hallucinations, and distortions that arose during the SFT phase. 2. Symmetric Preference Optimization. Similar to the InstructEngine framework [39], which employs crossvalidation alignment for T2I generation, symmetric preference optimization strategy was adopted for the image editing task. For each input pair (x, c1), where is the source image and c1 is the target editing instruction, multiple negative instructions were synthesized. These instructions aimed to perform the same type of editing operation but differed in fine-grained details. instruction c1 = For example, given an original make the chair wooden, hard negative instructions such as c2 = make the table wooden (object substitution) or c3 = make the chair wicker (material subImages (y1, y2, y3) correstitution) were generated. sponding to these prompts were generated using the SFT model and filtered using the assessor model (see Section 4) to ensure semantic consistency. Preference pairs were then constructed symmetrically: For the original instruction c1, its corresponding generation y1 was designated as the winner (xw), while generations from alternative prompts (e.g., y2, y3) served as losers (xl). Reciprocally, for any alternative instruction (e.g., c2), its specific generation y2 became the winner, while the generation from the original prompt y1 and other variants (e.g., y3) functioned as losers. This approach ensured that every generated image served as both positive and hard negative example depending on the conditioning instruction. Consequently, this strategy improved the instruction-following capabilities of the trained model by forcing it to distinguish between closely related semantic concepts. 3. Distillation from Strong Teachers. To enhance the aesthetic quality of generated images, high-quality data from advanced proprietary models was leveraged. This subset was constructed using the proprietary generations collected in Section 5.3. To form preference pairs, these proprietary samples were augmented with corresponding images generated by the SFT model. These SFT generations were evaluated using the in-house assessor model described in Section 4 to facilitate the construction of 18 training pairs. This composite strategy acted as direct distillation mechanism. By aligning the model with the superior outputs of more complex editors, it explicitly encouraged it to emulate their high visual appeal and artistic quality. 6. Results In this section, we further analyzed topics that were not fully discussed in Section 3 and benchmarked our bestperforming configuration against current state-of-the-art methods. 6.1. Ablation Studies Reference Image Guidance: sequence-wise vs. channelwise We first investigated two strategies for incorporating the reference image: sequence-wise and channel-wise concatenation. Our experiments showed that sequence-wise concatenation consistently outperformed channel-wise concatenation in all benchmarks, with the largest gains observed in the models instruction-following abilities. However, sequence-wise concatenation introduced clear computational overhead because it increased the token sequence length, thereby slowing down inference. With Sanas linearthe inference time approximately complexity attention, doubled. For DiT-based models with standard quadratic attention, the slowdown was even more pronounced, scaled superlinearly with the increased number of tokens, and often became the primary bottleneck at high resolutions. This trade-off led to the following practical observation: Observation We observed consistent gains with sequence-wise guidance in metrics, but the practical gains were often incremental relative to its latency cost. In many cases, similar outcomes could be achieved with the channel-wise variant by re-sampling few times. In contrast, channel-wise concatenation substantially reduced generation latency, yielding clear improvement in user experience. Therefore, channelwise guidance is used in our final high-throughput configuration. Textual Guidance: Meta Tokens and VLM Connector Design In this section, we analyzed how textual guidance was formed and injected into the diffusion denoising process, focusing on the connector design that bridged the VLM representation space with the diffusion conditioning space. We compared three guidance paradigms. (i) Native text encoder. As baseline, we used the diffusion models native text encoder, which conditioned gen19 eration only on the text prompt and did not explicitly incorporate the input image. This setup was attractive because it required no additional connector and avoided an extra alignment stage. However, it was fundamentally limited by the absence of vision-language reasoning: the instruction could not be interpreted in the context of the reference image, which often led to ambiguous edits, especially for compositional modifications that depend on understanding the scene. (ii) Query-based expansion. Following the approach proposed in [30], the VLM produced compact set of 8 guidance tokens, which were then expanded by Q-Former connector. The Q-Former was initialized with set of learnable queries whose size matched the maximum conditioning sequence length expected by the diffusion backbone, allowing it to map short VLM output into full-length diffusion conditioning sequence. (iii) Meta-token generation. Inspired by [49], we prompted the VLM with set of meta-tokens and let it generate the full conditioning sequence required by the diffusion model in single forward pass. To bridge the representation gap between the VLM output space and the diffusion conditioning space, we evaluated connectors of different types and depths, including (i) standard Transformer encoder and (ii) an ELLA-based connector. We first tested whether the meta-queries paradigm with standard encoder consistently outperformed the Q-Former setup. To ensure fair comparison, we used the same connector depth (four blocks [30]) in both cases. We also compared these results against baseline trained with native text-only encoder, without multimodal support. This comparison led to the following observation: Observation 7 The meta-queries configuration drastically improved the models instruction-following capabilities compared to the Q-Former and native-encoder baselines. Next, we evaluated timestep-aware conditioning with ELLA against standard encoder-based connector. For each setup, we examined how connector depth affected performance by sweeping the number of layers from 2 to 8. The depth sweep led to the following observation: Observation For both connector configurations, depth of four blocks was optimal. Compared to standard encoder, the ELLA connector yielded only minor improvements that were not consistent across settings. Table 4. Quantitative comparison on ImgEdit [75]. Overall is calculated by averaging all scores across tasks. VIBE achieves top-tier overall performance and leads several core edit categories. Model Add Adjust Extract Replace Remove Background Style Hybrid Action Overall Instruct-Pix2Pix [7] 2.45 MagicBrush [79] 2.84 AnyEdit [76] 3.18 UltraEdit [83] 3.44 OmniGen [71] 3.47 ICEdit [82] 3.58 Step1X-Edit-v1.1 [38] 3.88 BAGEL [12] 3.56 UniWorld-V1 [37] 3.82 OmniGen2 [69] 3.57 FLUX.1 Kontext [Dev] [6] 4.12 4.40 Z-Image [78] VIBE 3.89 1.83 1.58 2.95 2.81 3.04 3.39 3.14 3.31 3.64 3.06 3.80 4.14 4. 1.44 1.51 1.88 2.13 1.71 1.73 1.76 1.70 2.27 1.77 2.04 4.30 2.90 2.01 1.97 2.47 2.96 2.94 3.15 3.40 3.30 3.47 3.74 4.22 4.57 4.34 1.50 1.58 2.23 1.45 2.43 2.93 2.41 2.62 3.24 3.20 3.09 4.13 4.42 1.44 1.75 2.24 2.83 3.21 3.08 3.16 3.24 2.99 3.57 3.97 4.14 4.22 3.55 2.38 2.85 3.76 4.19 3.84 4.63 4.49 4.21 4.81 4.51 4.85 4.40 1.20 1.62 1.56 1.91 2.24 2.04 2.64 2.38 2.96 2.52 3.35 3.63 3. 1.46 1.22 2.65 2.98 3.38 3.68 2.52 4.17 2.74 4.68 4.25 4.50 2.75 1.88 1.90 2.45 2.70 2.96 3.05 3.06 3.20 3.26 3.44 3.71 4.30 3.85 Table 5. GEdit-Bench-EN [38] (Full set): Semantic Consistency (G SC), Perceptual Quality (G PQ), and Overall Score (G O). Model SC PQ AnyEdit [76] Instruct-Pix2Pix [7] MagicBrush [79] UniWorld-V1 [37] OmniGen [71] FLUX.1 Kontext [Dev] [6] OmniGen2 [69] BAGEL [12] Step1X-Edit-v1.1 [38] Z-Image [78] VIBE 3.18 3.58 4.68 4.93 5.96 6.52 7.16 7.36 7.66 8.11 7.91 5.82 5.49 5.66 7.43 5.89 7.38 6.77 6.83 7.35 7.72 6.33 3.21 3.68 4.52 4.85 5.06 6.00 6.41 6.52 6.97 7.57 6.81 6.2. Benchmarks and Metrics We evaluated the final model on GEdit-Bench [38] and ImgEdit-Bench [75], strictly following the authors official evaluation protocols. We compare against broad set of leading instruction-based editing systems that are either open-weight or otherwise publicly accessible for controlled benchmarking, including several substantially larger backbones. For GEdit-Bench, we used the VIEScore setup with GPT-4.1 [1] to report Semantic Consistency (SC, 0 10), Perceptual Quality (PQ, 010), and Overall (O). For ImgEdit-Bench, we adopted the original authors protocol: GPT-4.1 was used to score edited images across several criteria, each rated on 15 scale. 6.3. Comparison with Existing Methods VIBE achieved an overall score of 3.85 on the ImgEdit benchmark, ranking second among the compared methods 20 in Table 4, and delivering distinctly strong editor profile. In particular, VIBE leads multiple core categories that demand strict preservation of the input image, including Adjust (4.22), Remove (4.42), and Background (4.22). It also ranks among the top performers on Replace, Extract, and Hybrid edits, indicating robust instruction grounding across broad range of operations, despite using markedly smaller diffusion backbone than several of the strongest baselines in the comparison. We observe that the most challenging cases for VIBE are highly complex, non-local edits, such as Action, that require substantial geometric and compositional changes  (Table 4)  , which likely benefit from larger, more complex models. On GEdit-Bench-EN, VIBE achieved an overall score of 6.81  (Table 5)  . Notably, the model received the secondhighest score for semantic consistency (7.91), demonstrating reliable instruction-following behavior. Although our perceptual quality score (6.33) trails behind systems optimized specifically for visual fidelity, the data suggests this gap is due to fine details and minor artifacts rather than failure in semantic alignment. Together, ImgEdit and GEdit suggest that VIBE prioritizes faithful, minimally invasive edits over aggressive scene redrawing. 7. Conclusions The presented work shows that high-quality instructionbased image editing can be achieved with relatively small model, with the right design and training setup. strong but compact 2B VLM is enough to read complex user requests in the context of the input image and provide stable guidance via learnable meta-tokens and lightweight connector. This work shows that even 1.6B diffusion backbone can deliver high-quality edits. With channel-wise reference guidance, the pipeline keeps high throughput, fits into GB of GPU memory, and can generate 2K images in about 4 seconds on an NVIDIA H100 in BF16. We show that stability and strict source consistency come not only from architecture choices, but also from consistent work with training stages and data. The paper uses four-stage setup: first align the VLM-to-diffusion interface with text-to-image objective (freezing the backbones), then do large-scale pretraining and SFT with mixed editing and T2I data as an anchor, train in mixed resolution with diverse aspect ratios, and finally apply Diffusion-DPO to improve both instruction following and visual quality, including symmetric hard negatives and distillation from strong complex editors. Data quality is critical here, and real-world triplets are hard to get. Instead of only imitating user prompts, the work grounds synthetic intents to real user phrasing via retrieval over real-world requests, validates instruction applicability to the image, and scales triplets further with inversion and compositional bootstrapping. Ultimately, we show that with clean data and disciplined training recipe, practical editing system can match or surpass significantly larger models on core tasks, especially those requiring strict preservation of input content. Remaining challenges are concentrated in complex edits requiring major geometric changes, as well as fine-grained visual artifacts that continue to limit perceptual quality. 8. Limitations Despite strong benchmark results and overall high quality, the model has limited capacity due to its relatively low complexity. Very complex operations can still fail, and some hard aesthetic requests remain unstable. In practice, several categories of real-world photos are harder than generated images, since the in-the-wild domain is much more diverse. The range of capture conditions, from old mobile cameras to professional DSLR setups, makes the problem extremely challenging even for large proprietary systems. For the same reason, the pipeline tends to be more robust on generated images from modern generators, where the data distribution is closer to the training data. Despite extensive filtering, the generative signal in the input, output, or instruction can still dominate over the real-photo signal. The main purpose of this model is research. The pipeline relies on pretrained components (VLM and diffusion), and substantial part of the training data is generated automatically. As with other generative systems, we do not guarantee correct or safe behavior in all situations, and the model may produce incorrect, misleading, or otherwise undesirable outputs. Users are responsible for appropriate use in their own setting, including any required rights and consent, and for any decisions made based on the outputs. We do not commit to providing support, updates, or fixes. We did not perform systematic evaluation of bias or fairness. Since the pipeline relies on pretrained components, auxiliary models, large-scale open data, and automatically generated samples, the system may inherit biases from these sources. Strict source consistency can also be intrinsically difficult for some edit types. Even significantly larger closed systems can fail in these cases, so the presented compact model may drift as well. Finally, the VLM backbone is kept frozen across the whole pipeline to preserve its original knowledge, so the effect of full end-to-end VLM adaptation on final quality is not studied. 9. Future works clear next step is to reduce inference cost by distilling the model for fewer diffusion steps and removing CFG. Quantization is also practical direction to improve throughput and memory footprint, potentially enabling faster inference on lower-end hardware. Another important direction is to increase the share of real-world signal in training data, in both triplets and validation, to improve robustness on real photos. Stronger adaptation strategies also remain open, including partial or full VLM finetuning, to study the trade-off between preserving general knowledge and improving editing-specific behaviors."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 20 [2] Shamil Ayupov, Maksim Nakhodnov, Anastasia Yaschenko, Andrey Kuznetsov, and Aibek Alanov. Dreamboothdpo: Improving personalized generation using direct preference optimization. arXiv preprint arXiv:2505.20975, 2025. 9 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 10 [4] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report, 2025. 5, 9, 16 21 [5] black-forest-labs. FLUX.1-kontext-dev (model card). Hugging Face, 2025. Accessed 2025-12-23. [6] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. 4, 5, 20 [7] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 4, 5, 20 [8] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2256022570, 2023. 5 [9] Sergej Chicherin and Karen Efremyan. Adversarially-guided portrait matting. arXiv preprint arXiv:2305.02981, 2023. 14 [10] Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul try-on via Choo. misalignment-aware normalization. In CVPR, 2021. 14 [11] Frederique Crete, Thierry Dolmi`ere, Pierrick Ladret, and Marion Nicolas. The blur effect: Perception and estimation with new no-reference perceptual blur metric. In Proceedings of SPIE, Human Vision and Electronic Imaging XII, 2007. 14 Viton-hd: High-resolution virtual [12] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 20 [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 8 [14] Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, and Zeyu Wang. Dit4edit: DifIn Proceedings of fusion transformer for image editing. the AAAI Conference on Artificial Intelligence, pages 2969 2977, 2025. [15] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. arXiv preprint, 2023. 4, 5, 6 [16] Tsu-Jui Fu, Yusu Qian, Chen Chen, Wenze Hu, Zhe Gan, and Yinfei Yang. Univg: generalist diffusion model for unified image generation and editing. arXiv preprint arXiv:2503.12652, 2025. 5 [17] Rohit Gandikota, Joanna Materzynska, Tingrui Zhou, Antonio Torralba, and David Bau. Concept sliders: Lora adaptors for precise control in diffusion models. arXiv preprint arXiv:2311.12092, 2023. 14 22 [18] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024. 5, 15 [19] Gemini Team. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 10, [20] Google AI for Developers. Nano banana (image generation) gemini api. Documentation, 2025. Accessed 2025-12-23. 4 [21] Google DeepMind. Introducing nano banana pro. https: //blog.google/technology/ai/nano-bananapro/, 2025. Published 2025-11-20. Accessed 2025-12-24. 4 [22] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: dataset for large vocabulary instance segmentation. In CVPR, 2019. 14 [23] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 5 [24] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, and Hartwig Adam. Searching for mobilenetv3. arXiv preprint arXiv:1905.02244, 2019. 14 [25] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 9, [26] Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. 5, 6 [27] Tanuj Jain, Christopher Lennan, Zubin John, and Dat Tran. Imagededup. https://github.com/idealo/ imagededup, 2019. 14 [28] Alexander Kapitanov, Karina Kvanchiani, and Kirillova Sofia. Easyportrait - face parsing and portrait segmentation dataset. arXiv preprint arXiv:2304.13509, 2023. 15 [29] Alexander Kapitanov, Karina Kvanchiani, Alexander Nagaev, Roman Kraynov, and Andrei Makhliarchuk. Hagrid hand gesture recognition image dataset. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 45724581, 2024. 15 [30] Jing Yu Koh, Daniel Fried, and Russ Salakhutdinov. Generating images with multimodal language models. Advances in Neural Information Processing Systems, 36:2148721506, 2023. 19 [31] Benno Krojer, Dheeraj Vattikonda, Luis Lara, Varun Jampani, Eva Portelance, Christopher Pal, and Siva Reddy. Learning action and reasoning-centric image editing from videos and simulations. arXiv preprint arXiv:2407.03471, 2024. [32] Maksim Kuprashevich and Irina Tolstykh. Mivolo: MultiarXiv input transformer for age and gender estimation. preprint arXiv:2307.04616, 2023. 14 [33] Maksim Kuprashevich, Grigorii Alekseenko, and Irina Tolstykh. Beyond specialization: Assessing the capabilities arXiv preprint of mllms in age and gender estimation. arXiv:2403.02302, 2024. 14 [34] Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh, Georgii Fedorov, Bulat Suleimanov, Vladimir Dokholyan, and Aleksandr Gordeev. Nohumansrequired: Autonomous high-quality image editing triplet mining. arXiv preprint arXiv:2507.14119, 2025. 9, 10, 15, 17 [35] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. 16 [36] Kyungmin Lee, Xiahong Li, Qifei Wang, Junfeng He, Junjie Ke, Ming-Hsuan Yang, Irfan Essa, Jinwoo Shin, Feng Yang, and Yinxiao Li. Calibrated multi-preference optimization for aligning diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 18465 18475, 2025. [37] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. 20 [38] Shiyu Liu et al. Step1x-edit: practical framework for general image editing. arXiv preprint, 2025. 4, 5, 20 [39] Xingyu Lu, Yuhang Hu, YiFan Zhang, Kaiyu Jiang, Changyi Liu, Tianke Zhang, Jinpeng Wang, Chun Yuan, Bin Wen, Fan Yang, et al. Instructengine: Instruction-driven text-to-image alignment. arXiv preprint arXiv:2504.10329, 2025. 18 [40] Chaojie Mao, Jingfeng Zhang, Yulin Pan, Zeyinzi Jiang, Zhen Han, Yu Liu, and Jingren Zhou. Ace++: Instructionbased image creation and editing via context-aware content filling. arXiv preprint arXiv:2501.02487, 2025. 5 [41] Meituan LongCat Team, Hanghang Ma, Haoxian Tan, Jiale Huang, Junqiang Wu, Jun-Yan He, Lishuai Gao, Songlin Xiao, Xiaoming Wei, Xiaoqi Ma, Xunliang Cai, Yayong Guan, and Jie Hu. Longcat-image technical report. arXiv preprint arXiv:2512.07584, 2025. 5, 6 [42] Meta. Introducing llama 3.1: Our most capable models to date. Meta AI Blog, 2024. 14 [43] Meta. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. Meta AI Blog, 2024. 14 [44] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 60386047, 2023. 5 [45] Open Images Team. Open images dataset v7 and extensions. Online, 2022. V7 released Oct 2022. 14, 15 [46] OpenAI. The new chatgpt images is here. OpenAI Index, 2025. Accessed 2025-12-23. [47] OpenAI. Gpt image 1.5 model openai api. OpenAI Platform Documentation, 2025. Accessed 2025-12-23. 4 [48] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 8 [49] Xichen Pan et al. Transfer between modalities with metaqueries. arXiv preprint, 2025. 4, 6, 19 [50] Sayak Paul. instruction-tuning-sd/low-level-image-proc: Instruction-prompted low-level image processing dataset, 2023. Commit 13c02dd (May 11, 2023). Accessed 202512-29. [51] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In International Conference on Learning Representations (ICLR), 2024. 14 [52] Qdrant Team. Qdrant: High-performance vector database and vector search engine. https://github.com/ qdrant/qdrant, 2025. Version v1.16.3 (released 202512-19), accessed 2025-12-26. 16 [53] Qwen Team. Qwen-image-edit (model card). Hugging Face, 2025. Accessed 2025-12-30. 5 [54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, pages 87488763. PMLR, 2021. 4 [55] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. arXiv preprint, 2023. 6, 8 [56] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. [57] Min-Cheol Sagong, Yoon-Jae Yeo, Seung-Won Jung, and Sung-Jea Ko. Rord: real-world object removal dataset. In 33rd British Machine Vision Conference 2022, BMVC 2022, London, UK, November 21-24, 2022. BMVA Press, 2022. 14 [58] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. 14 [59] Jing Shi, Ning Xu, Trung Bui, Franck Dernoncourt, Zheng Wen, and Chenliang Xu. benchmark and baseline for language-driven image editing. In Proceedings of the Asian Conference on Computer Vision, 2020. 15 [60] Yichun Shi, Peng Wang, and Weilin Huang. Seededit: 23 Align image re-generation to image editing. arXiv preprint arXiv:2411.06686, 2024. 5 [61] Artem Snegirev, Maria Tikhonova, Maksimova Anna, Alena Fenogenova, and Aleksandr Abramov. The russian-focused embedders exploration: rumteb benchmark and russian embedding model design. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 236254, 2025. 16 [62] Linfeng Tan, Jiangtong Li, Li Niu, and Liqing Zhang. Deep In ACM MM, image harmonization in dual color spaces. 2023. 14 [63] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19211930, 2023. 5 [64] Stefan van der Walt, Johannes L. Schonberger, Juan NunezIglesias, Francois Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle Gouillart, Tony Yu, and the scikit-image contributors. scikit-image: Image processing in python. PeerJ, 2:e453, 2014. 14 [65] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. 4, 6, 8 [66] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 14 [67] Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. Objectdrop: Bootstrapping counterfactuals for photorealistic object removal and insertion. arXiv preprint arXiv:2403.18818, 2024. [68] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, Zenan Liu, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 5, 6, 14, 15 [69] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 20 [70] Bin Xia, Yuechen Zhang, Jingyao Li, Chengyao Wang, Yitong Wang, Xinglong Wu, Bei Yu, and Jiaya Jia. Dreamomni: Unified image generation and editing. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2853328543, 2025. 5 [71] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 1329413304, 2025. 20 [72] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint, 2024. 4, 5 [73] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. 9 [74] Yuhao Xu, Tao Gu, Weifeng Chen, and Chengcai Chen. Ootdiffusion: Outfitting fusion based latent diffusion for controllable virtual try-on. arXiv preprint arXiv:2403.01779, 2024. 14 [75] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. [76] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2612526135, 2025. 5, 20 [77] Yongsheng Yu, Ziyun Zeng, Haitian Zheng, and Jiebo Luo. Omnipaint: Mastering object-oriented editing via disentangled insertion-removal inpainting. arXiv preprint arXiv:2503.08677, 2025. 10 [78] Z-Image Team, Huanqia Cai, Sihan Cao, Ruoyi Du, Peng Gao, Steven Hoi, Shijie Huang, Zhaohui Hou, Dengyang Jiang, Xin Jin, Liangchen Li, Zhen Li, Zhong-Yu Li, David Liu, Dongyang Liu, Junhan Shi, Qilong Wu, Feng Yu, Chi Zhang, Shifeng Zhang, and Shilin Zhou. Z-image: An efficient image generation foundation model with single-stream arXiv preprint arXiv:2511.22699, diffusion transformer. 2025. 5, 6, 20 [79] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. 5, 20 [80] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 14, 15 [81] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al. Hive: Harnessing human feedback for instructional visual editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90269036, 2024. [82] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in24 context generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. 20 [83] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. 5, 6,"
        }
    ],
    "affiliations": [
        "R&D Department, SALUTEDEV"
    ]
}