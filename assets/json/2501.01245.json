{
    "paper_title": "SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization",
    "authors": [
        "Yongle Huang",
        "Haodong Chen",
        "Zhenbang Xu",
        "Zihan Jia",
        "Haozhou Sun",
        "Dian Shao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human action understanding is crucial for the advancement of multimodal systems. While recent developments, driven by powerful large language models (LLMs), aim to be general enough to cover a wide range of categories, they often overlook the need for more specific capabilities. In this work, we address the more challenging task of Fine-grained Action Recognition (FAR), which focuses on detailed semantic labels within shorter temporal duration (e.g., \"salto backward tucked with 1 turn\"). Given the high costs of annotating fine-grained labels and the substantial data needed for fine-tuning LLMs, we propose to adopt semi-supervised learning (SSL). Our framework, SeFAR, incorporates several innovative designs to tackle these challenges. Specifically, to capture sufficient visual details, we construct Dual-level temporal elements as more effective representations, based on which we design a new strong augmentation strategy for the Teacher-Student learning paradigm through involving moderate temporal perturbation. Furthermore, to handle the high uncertainty within the teacher model's predictions for FAR, we propose the Adaptive Regulation to stabilize the learning process. Experiments show that SeFAR achieves state-of-the-art performance on two FAR datasets, FineGym and FineDiving, across various data scopes. It also outperforms other semi-supervised methods on two classical coarse-grained datasets, UCF101 and HMDB51. Further analysis and ablation studies validate the effectiveness of our designs. Additionally, we show that the features extracted by our SeFAR could largely promote the ability of multimodal foundation models to understand fine-grained and domain-specific semantics."
        },
        {
            "title": "Start",
            "content": "SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization Yongle Huang1,2*, Haodong Chen1,2*, Zhenbang Xu1, Zihan Jia3, Haozhou Sun4, Dian Shao1 1Unmanned System Research Institute, Northwestern Polytechnical University, Xian, China 2School of Automation, Northwestern Polytechnical University, Xian, China 3School of Computer Science, Northwestern Polytechnical University, Xian, China 4School of Software, Northwestern Polytechnical University, Xian, China {yonglehuang, chd}@mail.nwpu.edu.cn, shaodian@nwpu.edu.cn 5 2 0 2 2 ] . [ 1 5 4 2 1 0 . 1 0 5 2 : r Abstract Human action understanding is crucial for the advancement of multimodal systems. While recent developments, driven by powerful large language models (LLMs), aim to be general enough to cover wide range of categories, they often overlook the need for more specific capabilities. In this work, we address the more challenging task of Fine-grained Action Recognition (FAR), which focuses on detailed semantic labels within shorter temporal duration (e.g., salto backward tucked with 1 turn). Given the high costs of annotating finegrained labels and the substantial data needed for fine-tuning LLMs, we propose to adopt semi-supervised learning (SSL). Our framework, SeFAR, incorporates several innovative designs to tackle these challenges. Specifically, to capture sufficient visual details, we construct Dual-level temporal elements as more effective representations, based on which we design new strong augmentation strategy for the TeacherStudent learning paradigm through involving moderate temporal perturbation. Furthermore, to handle the high uncertainty within the teacher models predictions for FAR, we propose the Adaptive Regulation to stabilize the learning process. Experiments show that SeFAR achieves state-of-the-art performance on two FAR datasets, FineGym and FineDiving, across various data scopes. It also outperforms other semisupervised methods on two classical coarse-grained datasets, UCF101 and HMDB51. Further analysis and ablation studies validate the effectiveness of our designs. Additionally, we show that the features extracted by our SeFAR could largely promote the ability of multimodal foundation models to understand fine-grained and domain-specific semantics. Code & Datasets: https://github.com/KyleHuang9/SeFAR. Introduction Understanding videos has attracted increasing attention as videos contain vivid visual information and rich temporal dynamics absent in text and images. In the past year, we have seen remarkable progress in multimodal large language models (MLLMs) (Chen et al. 2023; Li et al. 2024, 2023b; Lin et al. 2023), aiming at acquiring more general and comprehensive abilities. However, as pointed out by recent studies (Zhao et al. 2024; Yuan et al. 2023), chasing generality *These authors contributed equally. Corresponding Author Copyright 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: Fine-grained Action Instances. The two samples are drawn from the FineGym (Shao et al. 2020a) dataset, specifically the pike sole circle backward with 0.5 turn to handstand at the top and the ... 1 turn ... at the bottom. We further test popular MLLMs on the bottom instance for both coarse-grained and fine-grained: GPT-4V (OpenAI 2024), VideoChat2 (Li et al. 2024), VideoLLaVA (Lin et al. 2023), and InternLM-XComposer-2.5 (Zhang et al. 2024). may sacrifice some task-specific performance, which motivates us to delve into perpendicular direction: focus on more specific tasks to promote the fine-grained understanding ability of models. Specifically, we focus on Fine-grained Action Recognition (FAR), challenging human-centric video understanding task. To explain, classical action recognition (Xiong et al. 2021; Xiao et al. 2022; Dave et al. 2023; Xing et al. 2023) only demands the model to provide relatively coarsegrained category such as gymnastics, while FAR aims to provide more detailed, specific, and semantically accurate descriptions as pike sole circle backward with 0.5 turn to handstand. To demonstrate the difficulty of this task, we evaluate four powerful MLLMs (OpenAI 2024; Li et al. 2024; Lin et al. 2023; Zhang et al. 2024), as shown in Fig. 1. Unfortunately, they all fail to correctly recognize the fine-grained semantics of the given action. In such sense, FAR holds significance in further enhancing the capability of MLLM (Driess et al. 2023; Vemprala et al. 2024), especially in application scenes requiring more accurate and professional information. However, limited research on FAR not only owes to its higher demands for method design but also the dataset construction (Shao et al. 2020a; Xu et al. 2022a). For example, providing annotations such as 5237D with 3.5 twists (Xu et al. 2022a) requires adequate expert knowledge, huge annotation time, and large checking efforts to ensure the quality (Shao et al. 2020a). This leads to the scarcity of finegrained labels and makes it difficult to directly re-train or fine-tune large models with huge annotated data. Keep this in mind, we further adopt the semi-supervised learning (SSL) setting, where only small percentage of labeled data is needed (Zhu 2005). Consequently, targeting semisupervised FAR, besides those intrinsic challenges from both sides, we have to tackle intractable new challenges that emerged when combined. Specifically, FAR needs enough visual details, effective information aggregation, and comprehensive understanding of temporal dynamics (Shao et al. 2020a; Xu et al. 2022a; Li et al. 2022; Tang et al. 2023). For SSL, the core is to equip the unlabeled data with stable and reasonable supervision (e.g., pseudo-labels) (Sohn et al. 2020; Zhu 2005; Kurakin et al. 2020). However, when training semi-supervised FAR model, the generated pseudolabels may not be reliable, since FAR is rather challenging, making the whole learning process easily collapse. In this paper, we propose novel framework, SeFAR, to address the above challenges. Due to the semi-supervised setting, SeFAR is developed based on the FixMatch (Sohn et al. 2020) SSL paradigm, including the weak-to-strong consistency regularization and the Teacher-Student setup, as shown in Fig. 2. Moreover, there are also delicately designed strategies and modules incorporated in SeFAR: ❶ First, to effectively mine adequate and useful data for FAR, duallevel information modeling strategy is proposed. This process combines both fine-grained temporal elements with the temporal context to effectively capture multi-granular temporal information, enhancing the ability to discriminate subtle actions in the video. ❷ Then, to construct weak-strong contrast data pairs more tailored for FAR which differs from the traditional spatial-only augmentations (Yun et al. 2019; DeVries 2017; Kurakin et al. 2020), we highlight the significance of temporal dynamics and design new strong augmentation strategy. Specifically, we introduce moderate temporal perturbation into the fine-grained temporal elements achieved previously, while keeping the temporal order of context element. ❸ Moreover, in order to provide reliable pseudo-labels for unlabeled data even when the Teacher model suffers from unstable predictions, we design an Adaptive Regulation to stabilize the training process by calculating coefficients to adjust the losses. In addition, to directly tackle the problems outlined in Fig. 1, we adhere to the standard MLLM framework, which includes vision encoder, language encoder, and an alignment adapter. By incorporating our SeFAR model as an innovative video encoder, we observe that all MLLMs perform better on FAR, as shown in Tab. 5. To summarize, our contributions are as follows: To the best of our knowledge, this is the first work to explore the highly challenging task of Semi-supervised Fine-grained Action Recognition and an effective framework SeFAR is proposed for this purpose, which is based on the FixMatch paradigm but incorporates new augmentation strategy to form the weak-to-strong data pairs; Moreover, SeFAR incorporates several innovative designs to address specific challenges, including the dual-level temporal elements modeling, careful involvement of moderate temporal perturbation, as well as the adaptive regulation for steady learning process; SeFAR achieves state-of-the-art performance on both finegrained (FineGym, FineDiving) and coarse-grained action recognition datasets (UCF101, HMDB51), demonstrating its effectiveness. Additional analysis shows that SeFAR could also serve as powerful visual encoder to assist current MLLMs in domain-specific scenes. Related Work Fine-grained Action Recognition (FAR). FAR aims to differentiate between similar human actions at finer semantic granularity (e.g.,switch leap with 0.5 turns vs. split jump with 1 turn), while coarse-grained actions (Zhou et al. 2018; Carreira and Zisserman 2017; Xu et al. 2022b; Yang et al. 2020; Wang et al. 2018), stop at the granularity of gymnastics. To achieve this, abundant and subtle motion details are extremely desired (Shao et al. 2020a). There are several pioneer works (Li et al. 2022; Leong et al. 2022, 2021; Tang et al. 2023; Hong et al. 2021; Wang et al. 2021) to tackle the problem of FAR. However, they have predominantly focused on fully supervised or few-shot learning. Among them, LCDC (Mac et al. 2019) capture local spatio-temporal features, HAAN (Li, He, and Xu 2022) use hierarchical modeling with atomic actions and visual concepts, while M3Net (Tang et al. 2023) implement multi-view encoding, matching, and fusion. Distinct from the above works, we propose to address more challenging task and propose the first semi-supervised FAR framework, SeFAR, integrating with the dual-level temporal elements modeling, which tackles the subtle inter-class differences but also contends with limited annotations. Data Augmentation in Semi-supervised Learning (SSL). Data augmentation plays an essential role in SSL, serving as one of the two core components of the FixMatch (Sohn et al. 2020)-based paradigm, specifically consistency regularization achieved through both strong and weak data augmentation. This has been previously demonstrated. For instance, (Xie et al. 2020) emphasizes that robust model should withstand variations in input examples or hidden states. However, most existing semi-supervised video action recognition studies (Xu et al. 2022b; Xiong et al. 2021; Xiao Figure 2: Overview of SeFAR pipeline. We target Semi-supervised FAR, assuming most input samples are unlabeled. During unsupervised learning, SeFAR adopts dual-level temporal elements modeling and performs augmentation in two manners (Weak vs. Strong). Strongly augmented/distorted samples by moderate temporal perturbation are used by the student model, while the teacher model offers pseudo-labels based on weakly augmented samples. Consistency is enforced through loss minimization (Lun). The unsupervised loss is further adjusted by our proposed Adaptive Regulation. The framework is trained with weighted combination of supervised Lsup and unsupervised Lun losses. et al. 2022; Dave et al. 2023) focus primarily on spatial augmentations achieved through image-based strategies (e.g., Cutmix (Yun et al. 2019), Cutout (DeVries 2017), or their variants (Kurakin et al. 2020; Cubuk et al. 2020)). We argue that temporal augmentation is equally important inspired by (Xing et al. 2023), especially in FAR, as spatial augmentations can often disrupt critical information within actions. To address this, we design new temporal augmentation strategy, moderate temporal perturbation. Furthermore, to maintain stability in the pseudo-labeling process, another core component of the FixMatch-based paradigm, we have developed the Adaptive Regulation during training. Methodology To tackle the challenging task of semi-supervised finegrained action recognition, we propose the SeFAR framework, and the complete pipeline is shown in Fig. 2. Before delving into specific details, we first elaborate on the preliminaries about semi-supervised learning, especially the FixMatch (Sohn et al. 2020) paradigm. Preliminaries Teacher vs. Student Model. line of SSL frameworks adopts the Teacher-Student setting, where the Teacher provides pseudo-labels to supervise the Student model. Instead of directly sharing weights between teacher and student models (Sohn et al. 2020), we adopt an average of consecutive student models to obtain Mean teacher, whose effectiveness has been verified (Tarvainen and Valpola 2017). Formally, at given time step, the weights of the Teacher model, θt, is updated as an exponential moving average of the student weights θs: θt ωθs + (1 ω)θt. (1) As pointed out in (Xing et al. 2023), such EMA-Teacher is more suitable and stable for human action recognition. Weak vs. Strong Augmentation. One core component within FixMatch (Sohn et al. 2020) is the construction of contrastive data pairs to facilitate consistency regularization. This involves the incorporation of both strong and weak augmentations, wherein the term augmentation here means distortion rather than enhancement, contrary to intuition. Specifically, strong augmentation (Astrong) usually causes significant perturbation to the original data and thus serves as the input for the Student model, while the Aweak produces moderately distorted data samples for the Teacher model to derive better predictions, as demonstrated in the center part of Fig. 2. Learning by Labeled vs. Unlabeled Data. In the SSL setting, only small portion of data is annotated, denoted by {xi, yi}Bl j=1, are all unlabeled. Usually the labeling ratio α = Bl is small (e.g., 0.1). Learning based on the labeled data is straightforward by minimizing the cross-entropy loss between model predictions red(xi) and labels yi: Bl(cid:88) i=1. The left Bu samples, {xj}Bu Bl+Bu Lsup = H(yi, red(xi)). (2) 1 Bl i=1 However, for the unlabeled data xj, there is no supervision. To solve this, we generate pseudo-labels from the Teacher model predictions , and then calculate the unsupervised loss as follows: ˆyj = max(Ft(Aweak(xj)), Lun ="
        },
        {
            "title": "1\nBu",
            "content": "Bu(cid:88) j=1 1( ˆyj > τ )H( ˆyj, Fs(AStrong(xj))), (3) where τ is the predefined threshold for confidence scores and 1 denotes the indicator function. The whole pipeline is trained using both losses, weighted by hyperparameters, = γ1Lsup + γ2Lun. (4) The SeFAR Framework In this work, we focus on the task of Fine-grained Action Recognition (FAR) in the Semi-Supervised Learning (SSL) setting. This new task brings unprecedented challenges, including: ❶ How to mine abundant and detailed visual information for differentiating subtle differences between finegrained actions? ❷ How to adapt the original SSL strategies, e.g., consistency regularization, to fit the temporal-matters FAR task? ❸ How to deal with the unstable pseudo-labels since the model hesitates between appearance-similar action samples? In the following paragraphs, we will introduce specific designs to address the above challenges. Dual-level Temporal Elements. Given fine-grained action video with frames, we first trim it into segments (Wang et al. 2016), and randomly sample one frame in each segment, obtaining frame sequence {f1, ..., fK} to represent the video. Since in FAR, the high similarity is shared in large parts of visual content (e.g., scenes, objects), models are usually required to perceive subtle changes and abundant details for accurate discrimination. To achieve this, we propose to construct several small temporal elements pi, where small means the size (i.e., the number of containing frames) of pi is moderate. Intuitively, small value of could help the model focus on quick and subtle changes, since details are usually missed when going through too many frames. Given frames, the sampling step is . After sampling times, we could get set of temporal elements with the same temporal lengths, denoted by: i=1, (5) Besides these temporally fine-grained elements, we also propose to obtain context element pcontext to encode longterm information and macro temporal dynamics. pcontext is composed of more frames, usually two times more than the fine-grained temporal elements pi. Such dual-level information modeling ensures that multi-granular information is preserved. As result, we obtain an effective representation of the input video, denoted by {p1, ...pM , pcontext}. Perturbation of Fine-grained Temporal Elements. Adopting the FixMatch (Sohn et al. 2020) based semisupervised learning setting, one key problem is how to form the weak-to-strong augmentation pair for consistency regularization. For weak augmentation, we could use random horizontal flipping or random scaling, since it largely preserves both spatial and temporal original information. pi = L. {pi}M (a) For unlabeled videos, the Teacher model Figure 3: predicts each video multiple times to capture the distribution of predictions, which shows less variability on coarsegrained data and more on fine-grained data. An adaptive coefficient η is calculated from the mean and variance of the distribution to stabilize training. (b) MLLM construction pipeline with SeFARs fine-grained features. Unfortunately, as pointed out in (Xing et al. 2023), strong augmentation designed for images is insufficient for video tasks, since it fully ignores the temporal dynamics evolving in videos. For the challenging FAR task, temporal variations are even more crucial and require the extreme attention of the model. Therefore, to design more effective strong augmentation strategy Astrong for FAR, we emphasize the following core insights: ① the proposed Astrong should make perturbations to the most crucial part of the data that we want the model to attend to (Sohn et al. 2020; Xie et al. 2020; Kurakin et al. 2020); ② Employing Astrong should not affect the semantic distinctiveness of action categories. Therefore, combing with the above dual-level temporal modeling strategy, we propose new strong augmentation operation through introducing temporal perturbation ψ into the fine-grained temporal elements {pi}. We experiment with different implementations of ψ, and the final choice is simple but effective: reversing the frame order. Specifically, we have: Astrong( {pi}M i=1, pi = ψ(pi) i=1 ) = { pi }M (6) Note that for the temporal context element pcontext, the temporal order is preserved, which ensures the temporal directionality to be inherent in actions (e.g., giant circle backward vs. giant circle forward, etc.), as shown in the bottom-left of Fig. 2. Our augmentation strategy introduces moderate temporal perturbation compared with total shuffling, and it also outperforms previous strategies, e.g., temporal warping (Xing et al. 2023), as shown in Tab. 4. Stabilizing Optimization via Adaptive Regulation. As mentioned, due to the challenging intrinsic of FAR, models usually swayed precariously between categories with subtle differences. During experiments, the greater the uncertainty of the models predictions, the less reliable the models predictions are. Such unstable predictions of the teacher model will result in ambivalent and invalid pseudoTable 1: Comparison with state-of-the-art semi-supervised action recognition methods on fine-grained datasets. We employ SeFAR with sampling combination of {2-2-4}. The primary evaluation metric is top-1 accuracy. In this table, within Input denotes RGB video, while represents temporal gradients. ImgNet indicates the utilization of models pre-trained on ImageNet (Russakovsky et al. 2015), while #F signifies the number of input frames. The labeling rates of the data are indicated by 5%, 10%, and 20% in the datasets. The best results are highlighted in Bold, and the second-best Underlined. Method Backbone Input ImgNet Params #F Epoch MemDPC (ECCV20) (Han, Xie, and Zisserman 2020) 3D-ResNet-18 3D-ResNet-18 VG LTG (CVPR22) (Xiao et al. 2022) SVFormer (CVPR23) (Xing et al. 2023) SeFAR-S (Ours) SeFAR-B (Ours) ViT-B VIT-S VIT-B Gym99 Gym288 Diving 5% 10% 5% 10% 5% 10% 15.4M 16 500 10.8 24.1 14.5 21.3 54.3 62.0 180 34.3 45.8 16.2 38.7 59.8 64.3 68.3M 8 31.4 47.9 21.3 39.6 59.1 70.8 30 121.4M 8 36.7 56.3 27.8 46.9 72.2 78.4 30 31.2M 8 39.0 56.9 28.3 48.1 72.8 80.9 30 122.1M 8 (a) Results of elements across all events. Method MemDPC LTG SVFormer SeFAR-S (Ours) SeFAR-B (Ours) UB 10m FX 10% 20% 10% 20% 10% 20% 71.2 20.7 83.5 50.5 85.9 52.9 94.0 56.9 94.6 58.5 65.4 75.2 73.8 85.5 87.4 15.9 21.6 28.8 42.9 44.2 13.8 19.6 20.1 23.8 27.6 19.1 60.5 66.8 73.8 75. Method MemDPC LTG SVFormer SeFAR-S (Ours) SeFAR-B (Ours) 5253B UB-S1 FX-S1 10% 20% 10% 20% 10% 20% 89.5 17.2 76.9 21.3 90.1 28.9 97.3 36.6 97.8 37.1 20.1 19.3 22.5 25.5 26. 15.4 14.6 18.8 19.2 20.1 21.1 29.7 47.3 55.3 56.8 82.2 64.6 86.6 96.4 97.0 (b) Results of elements within an event. (c) Results of elements within set. labels for the student, making the whole learning process suffer. To solve this, we first let the Teacher model generate predictions times (U is set to 10 in experiments) for given unlabeled video, and these predictions may vary largely. Then, based on these, we calculate the mean prediction confidence and standard deviation for each category. For the ith prediction, the predicted probability across all categories constitutes probability distribution. From this distribution, we can obtain the maximum prediction confidence value µi and calculate its standard deviation σi. We select the highest confidence value µ = max(µi), along with its corresponding standard deviation σ (see Fig. 5). Based on such µ and σ, we propose to calculate the dynamic coefficients τ1 and τ2 to obtain η, which is further used for adjusting losses derived from unlabeled samples: τ1 = sigmoid(eµ τ2 = (sigmoid( e), 1 βσ + ϵ ) 0.5), (7) where β is related to the model dropout and ϵ is steady parameter. To elaborate, τ1 will increase rapidly as µ increases, which enhances high-confidence predictions, while on the other hand, τ2 suppresses the unstable predictions (i.e., with high standard deviation σ). The obtained adaptive coefficient η = τ1 τ2, is more flexible and beneficial than predefined hyperparameter. Additionally, for unlabeled data, we also adopt the mixing strategy as in SVFormer (Xing et al. 2023), where the mixture of two unlabeled samples, λx1 + (1 λ)x2, could also serve as input, and the supervision is correspondingly obtained as mixed version (Details could be found in (Xing et al. 2023)). Here for adjusting Lmix, we achieve its coefficient in similar mixed manner, denoted by η = λη1 + (1 λ)η2, where η1, η2 are individually calculated based on x1 and x2. Finally, the total loss of the whole SeFAR framework is as follows: = Lsup + ξ(ηLun + η Lmix), (8) ) is warmup coefficient calculated uswhere ξ = sin( Mn ing the current epoch number and the max epoch Mn. SeFAR Empowers MLLMs. Efforts towards foundation models have led to the development of MLLMs, with vision being the primary modality (Gao et al. 2024). Although shown impressive general capabilities, they may fail in specific and more challenging tasks such as FAR, as illustrated in Fig. 1. This may largely be due to the systematic shortcomings in the visual part as analyzed in (Tong et al. 2024). Given that our SeFAR is designed to be effective for FAR in semi-supervised scenarios, the question: Could SeFAR benefit current MLLMs through providing better visual perception? The answer is yes as supported by the results in Tab. 5. To elaborate, in line with the typical MLLM framework, frozen visual encoder is usually combined with LLM. This setup facilitates multimodal functionality by aligning visual and textual features using an adaptor, e.g., Q-Former (Li et al. 2023a). Given such setting, we could use the features extracted by SeFAR to replace those provided by the original visual encoder as shown at the bottom of Fig. 5. Similarly, by aligning the visual features with the textual domain and concatenating with text embeddings, we could feed them into the LLM to produce the answers. Results show that SeFAR features could lead to much better results compared to those used in original MLLM settings."
        },
        {
            "title": "Experiment",
            "content": "Experiment Setup Datasets and Evaluation. We perform evaluations on fine-grained datasets Gym99, Gym288 (Shao et al. 2020a), and FineDiving (Xu et al. 2022a), as well as coarse-grained datasets UCF-101 (Soomro 2012) and HMDB-51 (Kuehne et al. 2011), using Top-1 accuracy as metrics. Specifically, FineGym includes hierarchical annotations at three semantic granularity: events, sets, and elements. At the finest level (elements), there are two versions of benchmarks, i.e., gym99 Table 2: Comparison with state-of-the-art semi-supervised action recognition methods on coarse-grained datasets.V within Input signifies RGB video, indicates optical flow, while denotes temporal gradients. Method Backbone Input ImgNet #F Epoch 3D-ResNet-18 MT+SD (WACV21) (Jing et al. 2021) 3D-ResNet-50 VFG MvPL (ICCV21) (Xiong et al. 2021) 3D-ResNet-18 TCLR (CVIU22) (Dave et al. 2022) R50+R50-1/4 CMPL (CVPR22) (Xu et al. 2022b) 3D-ResNet-18 VG LTG (CVPR22) (Xiao et al. 2022) 3D-ResNet-50 TimeBalance (CVPR23) (Dave et al. 2023) VIT-S SeFAR (Ours) FixMatch (NeurlPS20) (Sohn et al. 2020) SlowFast-R50 MemDPC (ECCV20) (Han, Xie, and Zisserman 2020) 3D-ResNet-18 R(2+1)D-34 ActorCM (CVIU21) (Zou et al. 2023) 3D-ResNet-18 VideoSSL (WACV21) (Jing et al. 2021) 3D-ResNet-50 TACL (TSVT22) (Tong, Tang, and Wang 2023) 3D-ResNet-18 L2A (ECCV22) (Gowda et al. 2022) ViT-S SVFormer-S (CVPR23) (Xing et al. 2023) ViT-B SVFormer-B (CVPR23) (Xing et al. 2023) VIT-S SeFAR (Ours) VIT-B SeFAR (Ours) 16 8 16 8 8 8 8 8 16 8 16 16 8 8 8 8 8 500 600 1200 200 180 250 30 200 500 360 500 200 400 30 30 30 30 - UCF-101 HMDB-51 1% 5% 10% 40% 50% 35.1 33.9 - - 48.4 53.9 59.2 - - 39.5 36.2 40.2 46.3 58.2 64.3 62.9 65.7 31.2 40.7 22.8 41.2 80.5 66.1 - 26.9 79.1 - 25.1 44.8 62.4 - 30.1 53.3 81.1 35.2 64.1 78.3 55.1 - 16.1 44.2 - - 45.1 53.0 - 32.4 42.0 - 35.6 55.6 - - 60.1 - - 79.1 31.4 46.1 84.6 - 46.0 73.2 84.3 50.3 77.6 87.0 32.6 30.5 - - 46.5 52.6 55.9 - - 35.7 32.7 38.7 42.1 56.2 59.9 58.5 61.5 Figure 4: Ablation Studies. We compare SeFAR-B with different sampling combinations on Gym-99 5%, as illustrated on the left. We also contrast fixed threshold methods with our Adaptive Regulation strategy on FineDiving 5% in the middle. On the right side, we demonstrate the fluctuation of predictions made by the Teacher model across different datasets. and gym288, with 99 and 288 categories, respectively. Note that all the experiments on FineGym are performed at the element level, but within different scopes. FineDiving is diving dataset comprising 3000 annotated clips with timestamps, encompassing 52 action types, 29 sub-action types, and 23 difficulty levels. Baselines. We employ the ViT (Dosovitskiy 2020) extended model TimeSformer (Bertasius, Wang, and Torresani 2021) as the backbone. The choice of hyperparameters remains as original. We instantiate the SeFAR-S model based on ViT-Small, with the number of total parameters comparable to most previous Conv-based methods (Han, Xie, and Zisserman 2020; Xiong et al. 2021; Xu et al. 2022b; Xiao et al. 2022; Tong, Tang, and Wang 2023; Gowda et al. 2022; Dave et al. 2023). Moreover, we implement the SeFAR-B model based on ViT-B, with more parameters. We configure the sampling combination by default as {2 2 4} for SeFAR, as commonly used 8-frame input. Implementation Details. We employ our SeFAR using PyTorch, with input video frames resized and cropped to 224 224 pixels. We adopt the SGD optimizer and employ learning rate of 0.005, momentum of 0.9, and weight decay of 0.001. The weights in Eq.4 are set as: λ1 = λ2 = 2. Main Results The main quantitative results on the two fine-grained action recognition datasets, i.e., FineGym and FineDiving, are demonstrated in Tab. 1. We evaluate all the methods at different semantic granularities. Specifically, we first conduct experiments on Gym99 and Gym288. Then, by narrowing the semantic scope, we focus on those element-level categories belonging to specific event. For instance, in Gym99, 25 classes belong to Uneven-bars (UB), while 35 classes are from Floor-exercise (FX). Further, we delve into the finer granularity, collecting sampling within that same set in the same event. Here we get all the circles in UB-set1 (UB-S1) and all the jumps in FX-set1 (FX-S1) for evaluation. We can observe that on both the FineGym and FineDiving, SeFAR-S significantly outperforms previous opensourced semi-supervised action recognition methods across all semantic granularities with moderate parameters. Additionally, when increasing the parameters comparative with SVFormer (Xing et al. 2023), the larger model, SeFAR-B, performs even better. Both SeFAR-S and SeFAR-B display the effectiveness of our proposed SeFAR framework for addressing the challenging task. Moreover, to further inspect the effectiveness and robustness of SeFAR, we conducted experiments on two classical coarse-grained action recognition datasets, UCF-101 and Table 3: Ablations of different components with SeFAR, where means w/. To adhere to the principle of consistency regularization in SSL, we employ strong augmentation consistent with SVFormer (Xing et al. 2023), i.e., temporal warping, once our Mod-Perturb is eliminated. Dual-Ele Mod-Perturb Ada-Reg Gym99 Gym288 Diving 60.4 64.6 67.4 72.2 22.7 25.4 26.6 27.8 32.6 34.8 35.9 36.7 Table 4: Ablation of different temporal augmentations. and denote the Speedand Order-focused. Perturbation S/O Gym99 Gym288 Diving G.-New Sth.-Sth. 24.4 Spatial-only 25.2 Slow (T-Drop) 26.3 All shuffle 27.6 Local-shuffle 24.7 Warping 24.8 T-Half 27.3 All reverse 27.8 Mod-Perturb 39.4 41.2 41.9 43.3 40.8 42.1 42.7 44. 45.6 45.0 45.5 45.3 44.8 44.8 45.9 46.2 67.9 68.6 69.0 71.9 68.2 68.4 71.2 72.2 34.2 35.6 35.2 36.4 35.9 36.0 36.3 36.7 HMDB-51. As shown in Tab. 2, SeFAR-B achieves approximately 3.3% improvement on UCF101 and approximately 1.7% improvement on HMDB51, achieving new state-ofthe-art results compared with those competitive baselines. Ablation Studies To achieve an in-depth comprehension of our SeFAR framework, we perform ablation studies on the impact of each component, namely dual-level temporal elements modeling (Dual-Ele), moderate temporal perturbation (Mod-Perturb) and Adaptive Regulation (Ada-Reg), as demonstrated in Tab. 3. Each module contributes significantly as an essential part of SeFAR. Furthermore, we conduct comprehensive analysis of the designs and choices of each proposed strategy or module. Details can be found as follows. Analysis of Dual-level Temporal Elements Modeling. We first compare different combinations of sampled elements, each context element has varying temporal lengths, e.g., 4, 6, 8. To facilitate comparison, we fix the length of the temporal fine-grained elements to be 2, consistent with our default setting {2-2-4}. Results are depicted in the left part of Fig. 4. We can find that even with limited input of only 6 frames, i.e., {2-4}, our proposed SeFAR surpasses the 8-frame input baseline SVFormer (Xing et al. 2023). This observation justifies the capability of our dual-level temporal elements modeling to capture abundant information details from video data, contributing to better discerning subtle differences among fine-grained actions. Additionally, it is noteworthy that increasing the number of the fine-grained elements, i.e., {2-2-2-4}, or extending the temporal length of the context element, i.e., {2-2-6} and {2-2-8}, all leads to performance improvements. This is attributed to the fact that more frames entail richer action information. Analysis of Moderate Temporal Perturbation. To better explore the impact of our proposed moderate temporal perTable 5: Ablation of Pre-trained Visual Encoder. We employ Vicuna-7B (Chiang et al. 2023) as the base LLM, comparing SeFARs features with the pre-trained features of commonly used visual encoders in MLLMs further finetuned on 5% data (i.e., : VideoLLaMA, Visual Encoder CLIP-ViT-L/16 EVA-CLIP ViT-G/14 ViT-L/14 SeFAR (Ours) MLLM Gym-QA-99 Gym-QA-288 : VideoChat, and 41.0 44.8 46.0 56.2 37.3 43.7 44.3 49. : VideoLLaVA) : VideoChat2, : LLaVA, , , - turbation (Mod-Perturb), we first selected 40 classes of action pairs that are reversing to each other (e.g., giant circle backward vs. giant circle forward) from FineGym, forming subset called Gym-New (G.-New). As shown in Tab. 4, SeFAR also maintains superior performance even on such actions, as well as on the Something-Something V2 (Sth.- Sth.) dataset (Goyal et al. 2017). Furthermore, we compare our Mod-Perturb with other temporal perturbation strategies in both Speedand Order-focused (e.g., slow-rate (Singh et al. 2021), temporal warping (Xing et al. 2023), T-Drop and T-Half (Zou et al. 2023)), the results can be found in Tab. 4. We can observe that: 1) Our Mod-Perturb exhibits superior stability and efficacy compared to other temporal augmentations and spatial-only (temporal information wellkept). 2) Spatial-only is less effective in Gym99 but outperforms most temporally augmented in Gym-New. This suggests that preserving accurate temporal information is crucial for more complex datasets, whereas reasonable temporal perturbations can enhance model stability in larger and more diverse datasets, and Mod-Perturb benefits from both. Analysis of Adaptive Regulation. To justify the usefulness of our stabilizing coefficients for adaptive losses, we perform two analyses: ① We compare this strategy with the fixed thresholding strategy widely used in the classical SSL method, the results are displayed in Fig. 4 (b), showing our method is both stable and effective. ② In Fig. 4 (c), We demonstrate the unstable predictions provided by the teacher models for FAR. Specifically, we randomly draw 30 data samples from different datasets, UCF101, HMDB51, and FineGym, for the teacher model to offer predictions. The highly varying predictions on FineGym further justify the motivation of our stabilizing design for FAR. Analysis of SeFAR Features. To further demonstrate the capability of SeFAR in enhancing MLLMs, we first constructed the Gym-QA dataset, which is derived from FineGym and presented in multiple-choice format as illustrated in Fig. 1. We then selected three widely used MLLM visual encoders, i.e., CLIP-ViT-L/16, EVA-CLIP ViT-G/14, and ViT-L/14). For fair comparisons, we conduct semisupervised training on these backbones with 5% labeling data from FineGym. Subsequently, we froze the weights of these encoders along with the weights from our 5%- trained SeFAR, and fine-tuned the Q-former using 5% of the annotated data from Gym-QA. As shown in Tab. 5, the SeFAR-empowered LLM significantly outperformed the other MLLM visual encoders on the Gym-QA task. This also mitigates the challenge of fine-tuning MLLMs in scenarios with low labeling rates."
        },
        {
            "title": "Conclusion",
            "content": "In this work, we shed light on more challenging and specific video understanding task, Semi-supervised Finegrained Action Recognition (FAR). To tackle the unique challenges that emerged, we propose new framework, SeFAR, which adopts ideas from the FixMatch setting and possesses innovative components delicately devised for FAR. Specifically, SeFAR is distinguished due to the following designs: 1) Dual-level temporal elements modeling is used to mine visual cues more thoroughly and capture rich temporal dynamics better; 2) Augmentation via moderate temporal perturbation is to produce temporally strong-distorted samples for weak-to-strong consistency regularization; 3) Stabilizing Optimization via Adaptive Regulation is to address the issue of large uncertainty in model predictions. To highlight, SeFAR also demonstrates superior performance in empowering MLLMs fine-grained visual understanding capability. SeFAR not only outperforms all the baselines largely on two representative FAR datasets, FineGym and FineDiving, but also achieve new state-of-the-art results on classical benchmarks (i.e., UCF101 and HMDB51). Comprehensive analysis and Extensive ablation studies further justify the effectiveness of our framework design."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was founded by the National Natural Science Foundation of China (NSFC) under Grant 62306239, and was also supported by National Key Lab of Unmanned Aerial Vehicle Technology under Grant WR202413. References Bertasius, G.; Wang, H.; and Torresani, L. 2021. Is spacetime attention all you need for video understanding? In ICML, volume 2, 4. Carreira, J.; and Zisserman, A. 2017. Quo vadis, action recognition? new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 62996308. Chen, H.; Huang, H.; Dong, J.; Zheng, M.; and Shao, D. 2024a. Finecliper: Multi-modal fine-grained clip for dynamic facial expression recognition with adapters. In Proceedings of the 32nd ACM International Conference on Multimedia, 23012310. Chen, H.; Huang, Y.; Huang, H.; Ge, X.; and Shao, D. 2024b. GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting Editing with Image Prompting. arXiv preprint arXiv:2405.07472. Chen, H.; Wang, L.; Yang, H.; and Lim, S.-N. 2024c. OmniCreator: Self-Supervised Unified Generation with Universal Editing. arXiv preprint arXiv:2412.02114. Chen, J.; Zhu, D.; Shen, X.; Li, X.; Liu, Z.; Zhang, P.; Krishnamoorthi, R.; Chandra, V.; Xiong, Y.; and Elhoseiny, M. 2023. Minigpt-v2: large language model as unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478. Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3): 6. Cubuk, E. D.; Zoph, B.; Shlens, J.; and Le, Q. V. 2020. Randaugment: Practical automated data augmentation with reduced search space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, 702703. Dave, I.; Gupta, R.; Rizve, M. N.; and Shah, M. 2022. TCLR: Temporal contrastive learning for video representation. Computer Vision and Image Understanding, 219: 103406. Dave, I. R.; Rizve, M. N.; Chen, C.; and Shah, M. 2023. Timebalance: Temporally-invariant and temporallydistinctive video representations for semi-supervised action In Proceedings of the IEEE/CVF Conference recognition. on Computer Vision and Pattern Recognition, 23412352. Dave, I. R.; Rizve, M. N.; and Shah, M. 2025. Finepseudo: improving pseudo-labelling through temporal-alignablity for semi-supervised fine-grained action recognition. In European Conference on Computer Vision, 389408. Springer. DeVries, T. 2017. lutional Neural Networks with Cutout. arXiv:1708.04552. Improved Regularization of ConvoarXiv preprint Dosovitskiy, A. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Driess, D.; Xia, F.; Sajjadi, M. S.; Lynch, C.; Chowdhery, A.; Ichter, B.; Wahid, A.; Tompson, J.; Vuong, Q.; Yu, T.; et al. 2023. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378. Gao, P.; Zhang, R.; Liu, C.; Qiu, L.; Huang, S.; Lin, W.; Zhao, S.; Geng, S.; Lin, Z.; Jin, P.; et al. 2024. Sphinxx: Scaling data and parameters for family of multi-modal large language models. arXiv preprint arXiv:2402.05935. Gowda, S. N.; Rohrbach, M.; Keller, F.; and Sevilla-Lara, L. 2022. Learn2augment: learning to composite videos for data augmentation in action recognition. In European conference on computer vision, 242259. Springer. Goyal, R.; Ebrahimi Kahou, S.; Michalski, V.; Materzynska, J.; Westphal, S.; Kim, H.; Haenel, V.; Fruend, I.; Yianilos, P.; Mueller-Freitag, M.; et al. 2017. The something something video database for learning and evaluating viIn Proceedings of the IEEE internasual common sense. tional conference on computer vision, 58425850. Han, T.; Xie, W.; and Zisserman, A. 2020. Memoryaugmented dense predictive coding for video representation learning. In European conference on computer vision, 312 329. Springer. Hong, J.; Fisher, M.; Gharbi, M.; and Fatahalian, K. 2021. Video pose distillation for few-shot, fine-grained sports action recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 92549263. Ma, K.; Huang, H.; Chen, J.; Chen, H.; Ji, P.; Zang, X.; Fang, H.; Ban, C.; Sun, H.; Chen, M.; et al. 2024. Beyond uncertainty: Evidential deep learning for robust video temporal grounding. arXiv preprint arXiv:2408.16272. Huang, H.; Qiao, X.; Chen, Z.; Chen, H.; Li, B.; Sun, Z.; Chen, M.; and Li, X. 2024. Crest: Cross-modal resonance through evidential deep learning for enhanced zeroshot learning. In Proceedings of the 32nd ACM International Conference on Multimedia, 51815190. Jing, L.; Parag, T.; Wu, Z.; Tian, Y.; and Wang, H. 2021. Videossl: Semi-supervised learning for video classification. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, 11101119. Kuehne, H.; Jhuang, H.; Garrote, E.; Poggio, T.; and Serre, T. 2011. HMDB: large video database for human motion recognition. In 2011 International Conference on Computer Vision, 25562563. Kurakin, A.; Raffel, C.; Berthelot, D.; Cubuk, E. D.; Zhang, H.; Sohn, K.; and Carlini, N. 2020. Remixmatch: Semisupervised learning with distribution matching and augmentation anchoring. Leong, M. C.; Tan, H. L.; Zhang, H.; Li, L.; Lin, F.; and Lim, J. H. 2021. Joint learning on the hierarchy represenIn 2021 tation for fine-grained human action recognition. IEEE International Conference on Image Processing (ICIP), 10591063. IEEE. Leong, M. C.; Zhang, H.; Tan, H. L.; Li, L.; and Lim, J. H. 2022. Combined CNN transformer encoder for enhanced arXiv preprint fine-grained human action recognition. arXiv:2208.01897. Li, J.; Li, D.; Savarese, S.; and Hoi, S. 2023a. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, 1973019742. PMLR. Li, K.; He, Y.; Wang, Y.; Li, Y.; Wang, W.; Luo, P.; Wang, Y.; Wang, L.; and Qiao, Y. 2023b. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355. Li, K.; Wang, Y.; He, Y.; Li, Y.; Wang, Y.; Liu, Y.; Wang, Z.; Xu, J.; Chen, G.; Luo, P.; et al. 2024. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2219522206. Li, T.; Foo, L. G.; Ke, Q.; Rahmani, H.; Wang, A.; Wang, J.; and Liu, J. 2022. Dynamic spatio-temporal specialization In European learning for fine-grained action recognition. Conference on Computer Vision, 386403. Springer. Li, Z.; He, L.; and Xu, H. 2022. Weakly-supervised temporal action detection for fine-grained videos with hierarchical atomic actions. In European conference on computer vision, 567584. Springer. Lin, B.; Ye, Y.; Zhu, B.; Cui, J.; Ning, M.; Jin, P.; and Yuan, L. 2023. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122. Mac, K.-N. C.; Joshi, D.; Yeh, R. A.; Xiong, J.; Feris, R. S.; and Do, M. N. 2019. Learning motion in feature space: Locally-consistent deformable convolution networks In Proceedings of the for fine-grained action detection. IEEE/CVF International Conference on Computer Vision, 62826291. OpenAI. 2024. GPT-4 System Card. https://openai.com/ index/gpt-4v-system-card/. Accessed: 2024-08-03. Rizve, M. N.; Duarte, K.; Rawat, Y. S.; and Shah, M. 2021. In defense of pseudo-labeling: An uncertainty-aware semi-supervised pseudo-label learning. arXiv preprint arXiv:2101.06329. selection framework for Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.; Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.; Imagenet large scale visual recognition chalet al. 2015. lenge. International journal of computer vision, 115: 211 252. Shao, D.; Xiong, Y.; Zhao, Y.; Huang, Q.; Qiao, Y.; and Lin, D. 2018. Find and focus: Retrieve and localize video events with natural language queries. In Proceedings of the European Conference on Computer Vision (ECCV), 200216. Shao, D.; Zhao, Y.; Dai, B.; and Lin, D. 2020a. Finegym: hierarchical video dataset for fine-grained action underIn Proceedings of the IEEE/CVF conference on standing. computer vision and pattern recognition, 26162625. Shao, D.; Zhao, Y.; Dai, B.; and Lin, D. 2020b. Intra-and inter-action understanding via temporal action parsing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 730739. Shu, W.-J.; Dou, H.-X.; Wen, R.; Wu, X.; and Deng, L.-J. 2024. CMT: Cross Modulation Transformer with Hybrid Loss for Pansharpening. arXiv preprint arXiv:2404.01121. Singh, A.; Chakraborty, O.; Varshney, A.; Panda, R.; Feris, R.; Saenko, K.; and Das, A. 2021. Semi-supervised action recognition with temporal contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1038910399. Sohn, K.; Berthelot, D.; Carlini, N.; Zhang, Z.; Zhang, H.; Raffel, C. A.; Cubuk, E. D.; Kurakin, A.; and Li, C.-L. 2020. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33: 596608. Soomro, K. 2012. UCF101: dataset of 101 human arXiv preprint actions classes from videos in the wild. arXiv:1212.0402. Tang, H.; Liu, J.; Yan, S.; Yan, R.; Li, Z.; and Tang, J. 2023. M3net: multi-view encoding, matching, and fusion for fewshot fine-grained action recognition. In Proceedings of the 31st ACM international conference on multimedia, 1719 1728. Tarvainen, A.; and Valpola, H. 2017. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30. Tong, A.; Tang, C.; and Wang, W. 2023. Semi-Supervised Action Recognition From Temporal Augmentation Using IEEE Transactions on Circuits and Curriculum Learning. Systems for Video Technology, 33(3): 13051319. Tong, S.; Liu, Z.; Zhai, Y.; Ma, Y.; LeCun, Y.; and Xie, S. 2024. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9568 9578. Vemprala, S. H.; Bonatti, R.; Bucker, A.; and Kapoor, A. 2024. Chatgpt for robotics: Design principles and model abilities. IEEE Access. Wang, J.; Wang, Y.; Liu, S.; and Li, A. 2021. Few-shot finegrained action recognition via bidirectional attention and contrastive meta-learning. In Proceedings of the 29th ACM International Conference on Multimedia, 582591. Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.; and Van Gool, L. 2016. Temporal segment networks: Towards good practices for deep action recognition. In European conference on computer vision, 2036. Springer. Wang, L.; Xiong, Y.; Wang, Z.; Qiao, Y.; Lin, D.; Tang, X.; and Van Gool, L. 2018. Temporal segment networks for action recognition in videos. IEEE transactions on pattern analysis and machine intelligence, 41(11): 27402755. Wu, X.; Wu, X.; Luan, T.; Bai, Y.; Lai, Z.; and Yuan, J. 2024. FSC: Few-point Shape Completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2607726087. Xiao, J.; Jing, L.; Zhang, L.; He, J.; She, Q.; Zhou, Z.; Yuille, A.; and Li, Y. 2022. Learning from temporal gradient for semi-supervised action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 32523262. Xie, Q.; Dai, Z.; Hovy, E.; Luong, T.; and Le, Q. 2020. Unsupervised data augmentation for consistency training. Advances in neural information processing systems, 33: 6256 6268. Xing, Z.; Dai, Q.; Hu, H.; Chen, J.; Wu, Z.; and Jiang, Y.- G. 2023. Svformer: Semi-supervised video transformer for In Proceedings of the IEEE/CVF conaction recognition. ference on computer vision and pattern recognition, 18816 18826. Xiong, B.; Fan, H.; Grauman, K.; and Feichtenhofer, C. 2021. Multiview pseudo-labeling for semi-supervised learning from video. In Proceedings of the IEEE/CVF international conference on computer vision, 72097219. Xu, J.; Rao, Y.; Yu, X.; Chen, G.; Zhou, J.; and Lu, J. 2022a. Finediving: fine-grained dataset for procedure-aware acIn Proceedings of the IEEE/CVF tion quality assessment. conference on computer vision and pattern recognition, 29492958. Xu, Y.; Wei, F.; Sun, X.; Yang, C.; Shen, Y.; Dai, B.; Zhou, B.; and Lin, S. 2022b. Cross-model pseudo-labeling In Proceedings of for semi-supervised action recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 29592968. Yan, Y.; Wang, S.; Huo, J.; Li, H.; Li, B.; Su, J.; Gao, X.; Zhang, Y.-F.; Xu, T.; Chu, Z.; et al. 2024a. Errorradar: Benchmarking complex mathematical reasoning of multimodal large language models via error detection. arXiv preprint arXiv:2410.04509. Yan, Y.; Wen, H.; Zhong, S.; Chen, W.; Chen, H.; Wen, Q.; Zimmermann, R.; and Liang, Y. 2024b. Urbanclip: Learning text-enhanced urban region profiling with contrastive language-image pretraining from the web. In Proceedings of the ACM on Web Conference 2024, 40064017. Yang, C.; Xu, Y.; Shi, J.; Dai, B.; and Zhou, B. 2020. Temporal pyramid network for action recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 591600. Yuan, L.; Gundavarapu, N. B.; Zhao, L.; Zhou, H.; Cui, Y.; Jiang, L.; Yang, X.; Jia, M.; Weyand, T.; Friedman, L.; et al. 2023. Videoglue: Video general understanding evaluation of foundation models. arXiv preprint arXiv:2307.03166. Yun, S.; Han, D.; Oh, S. J.; Chun, S.; Choe, J.; and Yoo, Y. 2019. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, 60236032. Zhang, P.; Dong, X.; Zang, Y.; Cao, Y.; Qian, R.; Chen, L.; Guo, Q.; Duan, H.; Wang, B.; Ouyang, L.; Zhang, S.; Zhang, W.; Li, Y.; Gao, Y.; Sun, P.; Zhang, X.; Li, W.; Li, J.; Wang, W.; Yan, H.; He, C.; Zhang, X.; Chen, K.; Dai, J.; Qiao, InternLM-XComposerY.; Lin, D.; and Wang, J. 2024. 2.5: Versatile Large Vision Language Model SupportarXiv preprint ing Long-Contextual Input and Output. arXiv:2407.03320. Zhao, L.; Gundavarapu, N. B.; Yuan, L.; Zhou, H.; Yan, S.; Sun, J. J.; Friedman, L.; Qian, R.; Weyand, T.; Zhao, Y.; et al. 2024. VideoPrism: Foundational Visual Encoder for Video Understanding. arXiv preprint arXiv:2402.13217. Zheng, M.; Xu, Y.; Huang, H.; Ma, X.; Liu, Y.; Shu, W.; Pang, Y.; Tang, F.; Chen, Q.; Yang, H.; et al. VideoGen-of-Thought: Collaborative Frame2024. arXiv preprint work for Multi-Shot Video Generation. arXiv:2412.02259. Zhou, B.; Andonian, A.; Oliva, A.; and Torralba, A. 2018. Temporal relational reasoning in videos. In Proceedings of the European conference on computer vision (ECCV), 803 818. Zhu, X. J. 2005. Semi-supervised learning literature survey. Zou, Y.; Choi, J.; Wang, Q.; and Huang, J.-B. 2023. Learning representational invariances for data-efficient action recognition. Computer Vision and Image Understanding, 227: 103597."
        },
        {
            "title": "Appendix",
            "content": "Introduction The content of our Appendix is organized as follows: In Section A, we present the data processing employed in our SeFAR framework, as well as baseline analysis; In Section B, we expound upon the categorical analysis of model uncertainty; In Section C, we provide more discussions regarding our SeFAR; In Section D, we present detailed information regarding the newly built Gym-QA and Gym-New datasets. A. Data Processing and Baseline Analysis Data Processing. In order to ensure rigorous and equitable comparison, we adopt identical data processing procedures and input formats across both SeFAR and the baseline methods. It is noteworthy that the input data format utilized in our experiments may not be the same as the original versions presented in FineGym (Shao et al. 2020a) and FineDiving (Xu et al. 2022a) since we conduct experiments at the finest level within these two datasets. We release the data pre-processing scripts together with the whole project code for the convenience of future work. Baseline Analysis. Semi-supervised fine-grained action recognition is challenging task that has not been previously explored. This is evident from the experimental results (e.g., Tab. 1), which show that models designed for coarse-grained action perform poorly on fine-grained actions. It is important to clarify that the baselines compared in Tab. 1 were evaluated and tested by us on the FineGym and FineDiving datasets for the first time, rather than by previous studies. The reason for including only three baselines in this comparison is that, although many studies have explored semisupervised coarse-grained action recognition (e.g., baselines in Tab. 2), only the three works presented in Tab. 1 are opensource and reproducible. We also attempted to reproduce models from non-open-source works based on their methodology sections, but unfortunately, the results we obtained differed from those reported in their original papers. Notably, we have identified an impressive concurrent work, FinePseudo (Dave, Rizve, and Shah 2025), which is dedicated to addressing the problem of semi-supervised finegrained action recognition. We will give it further attention and exploration in our future work. B. Visualization of Model Uncertainty As highlighted by (Rizve et al. 2021), the escalation of uncertainty within model predictions inversely impacts the models reliability. parallel phenomenon is discernible in our exploration of semi-supervised fine-grained action recognition. Specifically, we conducted random sampling of 1000 data points from various datasets and employed the Teacher model to predict each set of 1000 data points, subsequently evaluating the accuracy of the Teacher model. The left panel of Fig. 5 illustrates the correlation between the confidence level associated with the Teacher models predictions and their corresponding accuracy; notably, predictions characterized by heightened confidence demonstrate Figure 5: The relationship between the Teacher models prediction accuracy and its confidence (left), as well as its standard deviation (right). augmented accuracy. Conversely, the right panel of Fig. 5 depicts the connection between the standard deviation of predictions generated by the Teacher model and their accuracy; diminished variance in predictions is concomitant with heightened accuracy. This visual representation corroborates the rationale underpinning our conceptualization of the Adaptive Regulation. C. More Discussions on SeFAR In this section, we further discuss the following research questions (RQ): RQ1: How does each module contribute to enhancing finegrained action recognition? RQ2: How does the dual-level temporal elements modeling differ from previous modeling strategies? RQ3: Will the moderate temporal perturbation alter the actions? RQ4: Can the adaptive regulation be effective under more challenging conditions? RQ5: Does the teacher models prediction frequency affect performance? RQ6: What are the limitations of the current approach, and what directions should future research take? Module Analysis of SeFAR (RQ1): ❶ Dual-level temporal elements modeling: As discussed earlier, fine-grained actions, compared to coarse-grained actions, not only rely heavily on global semantic context but also contain richer visual detail, presenting unique challenges. The dual-level temporal elements we designed divide video into two hierarchical levels (i.e., fine-grained elements pi, and context element pcontext). This provides multi-granular information for fine-grained action recognition, allowing the model to capture features at different temporal scales (e.g., varying numbers of giant swings), and offering diverse representations for actions of different durations. ❷ Moderate temporal perturbation: In semi-supervised learning, data augmentation is essential for consistency regularization, which leads to more stable and superior model performance. Traditional coarsegrained action recognition often uses spatial augmentations that may disrupt critical details needed for fine-grained actions. For example, while coarse-grained actions like running can be recognized even with masked frames, finegrained actions are characterized by complexity and coherence. Therefore, we focus on temporal augmentations in this Table 6: Ablation of different labeling rates. The first two raw demonstrate our SeFAR w/o and w/ the Adaptive Regulation (Ada-Reg) respectively. The third raw further shows the performance increase rates at different labeling rates. Method SeFAR w/o Ada-Reg SeFAR Increase (%) FineDiving 1% 3% 5% 7% 10% 73.4 67.2 61.5 66.3 78.4 72.2 7.8% 7.6% 7.4% 7.0% 6.8% 64.6 69.5 69.7 74.6 Table 7: Deeper comparison of temporal augmentations. Perturbation Speed/Order FX 10m UB-S1 5253B 92.8 Slow-rate 92.8 T-Drop 93.5 All shuffle 94.9 Local-shuffle 92.9 Warping 93.4 T-Half 95.1 All reverse 96.4 Mod-Perturb 22.4 81.2 22.4 81.2 23.5 82.8 23.0 84.1 23.4 81.9 23.3 83.0 23.6 83.7 23.8 85. Speed Speed Order Order Order Order Order Order 35.6 35.6 36.1 36.5 34.7 35.3 35.5 36.6 work. As shown in Tab. 4, excessive perturbations can disrupt sequence information, hindering the models ability to capture subtle action differences. Our experiments show that sequence reversal provides strong perturbations while preserving action continuity, making them more effective for temporal augmentation. Additionally, our moderate temporal perturbation retains global context, enabling the model to benefit from augmentation while maintaining coherent understanding of actions. ❸ Adaptive regulation: In finegrained action recognition, subtle differences between similar actions (e.g., examples in Fig. 1) can lead to significant fluctuations in the predictions made by the Teacher model, particularly in semi-supervised setting, as illustrated in Fig. 4. The adaptive regulation strategy plays crucial role in stabilizing the training process by automatically adjusting the weights of the loss functions based on the distribution of the Teacher models predictions, which is essential for effective semi-supervised fine-grained action recognition. Dual-level Temporal Elements Modeling (RQ2): Sampling at different temporal scales is actually not new approach in action recognition. However, unlike previous methods, e.g., TPN (Yang et al. 2020)), which model at the feature level and sample once at each level, following an L1 < L2... < LN hierarchy for an N-level pyramid, often leading to high frame sampling and computational demands, our dual-level temporal elements modeling represents different video speeds through multi-level sampling at the input stage. We employ multiple fine-grained elements, each with the same number of frames (i.e., 2), and single context element to capture local and global features, respectively. This design allows us to achieve better performance while minimizing the total number of sampled frames. To achieve deeper comparison with other temporal perturbations, we assessed each method on sub-tasks involving the recognition of elements with an event (FX, 10m) Figure 6: Examples of Gym-QA Figure 7: Examples of Gym-New and within set (UB-S1, 5253B) using 10% labeled data as shown in Tab. 7. Consistent with the results in the main text (Tab. 4), our proposed moderate temporal perturbation (Mod-Perturb) consistently outperformed all other strategies across all sub-tasks, demonstrating its superior efficacy. Potential Action Directionality Changes (RQ3): Human actions are inherently complex to certain extent (Shao et al. 2018, 2020b). Intuitively, the reversal of action videos introduces challenges related to the directionality of actions (e.g., sitting down vs. standing up). We have taken this into account in our dual-level temporal elements modeling design, which includes both fine-grained elements containing local details and context elements capturing global information. During temporal perturbation, we only reverse the fine-grained elements, preserving the original temporal order in the context elements. This allows us to achieve consistency regularization through temporal perturbation while maintaining the original global temporal structure, which Figure 8: Confusion matrix of baseline (left) and ours (right) on Gym-New 10%, where the horizontal coordinate represents the predicted label and the vertical coordinate represents the true label. The labels corresponding to actions are shown in Fig. 9. differs significantly from complete reversal and previous temporal augmentation methods applied at the video level. This also indicates that our dual-level temporal elements modeling is coupled with moderate temporal perturbation, rather than being simple modular combination. Furthermore, as demonstrated in Tab. 4, we validated this approach by constructing variant of the FineGym dataset composed of completely opposite action pairs, named Gym-New, for experimentation. The results further confirm that for finegrained action recognition tasks, which require temporal and spatial coherence, common temporal augmentation strategies may disrupt this coherence, whereas our moderate temporal perturbation maintains coherence while introducing significant temporal disturbance. Adaptive Regulation (RQ4): With the continuous advancement of deep learning (Chen et al. 2024a; Yan et al. 2024b; Chen et al. 2024b; Ma et al. 2024; Huang et al. 2024; Yan et al. 2024a; Shu et al. 2024; Wu et al. 2024), the data-hungry paradigm of fully supervised learning has increasingly revealed certain limitations. Unlike the extensively studied fully-supervised setting, semi-supervised learning typically operates with label rate ranging from 1% to 10%, making it particularly suitable for tasks like finegrained action recognition that require high-quality data. However, low label rates can lead to instability during training, as discussed in the main text. To address this challenge, we designed the Adaptive Regulation process. In semisupervised setting, lower label rates present greater difficulties. Therefore, to further explore the potential of our strategy, we conducted experiments under varying label rates, as shown in Tab. 6. The results demonstrate that as the label rate decreases, the performance enhancement provided Table 8: Computation analysis of teacher model predictions. Time shown in (ms). 2 1 Prediction Times Teacher time / Iter. 29.9 68.5 Total time / Iter. Portion (%) Accuracy (%) 15 160.4 260.1 361.3 982.8 991.6 1005.1 1080.7 1220.6 1417.6 3.0 - 5 75. 14.8 36.7 21.3 36.8 25.5 37.0 6.9 35.3 7.5 36.2 10 by adaptive regulation becomes more pronounced, further validating that our strategy can effectively maintain strong performance under more challenging conditions. Efficiency of Teacher Model Prediction (RQ5): During inference, only the student model is used, incurring no additional computational cost from the teacher model. In the training phase, as shown in Tab. 8, we conduct further analysis focusing on ❶ Time Cost: Teacher prediction time increases with more predicted but remains small fraction of total training time (e.g., 14.8% at 10 predictions). This efficiency is achieved as teacher predictions are parallelized and do not involve gradient computations. ❷ Accuracy Impact: Model accuracy improves with the number of predictions, tending to saturate around 10 predictions. Therefore, we set the number of teacher predictions to 10 to balance performance and computational efficiency. Limitation and Future Work (RQ6): In this work, we introduce SeFAR to address the challenging task of semisupervised fine-grained action recognition for the first time, achieving superior performance with the aid of our carefully designed modules. This advancement establishes robust baseline for future research. However, one limitation of this Figure 9: Labels corresponding to actions in Gym-New. tions, as illustrated in Fig. 8, we present the confusion matrices of our baseline, namely SVFormer (Xing et al. 2023) (Left), and our proposed SeFAR (Right) applied to FineGym-New dataset. Our method capitalizes on dual-level temporal elements modeling, which yields diverse temporal features, and moderate temporal perturbation, which enhances the models focus on temporal feature modeling. This leads to two notable improvements over the baseline: a) Our method effectively mitigates the impact of class imbalance, manifesting in significant increase in the accuracy of under-represented classes; b) Our approach minimizes confusion between actions with opposing temporal directions (e.g., forward vs. backward), while also reducing confusion among similar actions, e.g., giant circle backward with 1 turn to handstand vs. giant circle backward. study is that we focused on temporal augmentation to emphasize its importance in fine-grained action understanding, while neglecting further exploration of spatial augmentation. We plan to address this in future work. Another potential limitation is that our core modules rely solely on RGB video input, overlooking the contribution of multimodal information in visual tasks. While we acknowledge that multimodal inputs, e.g., pose and textual descriptions, can significantly enhance model performance, we think that for the specific task of fine-grained action recognitionwhere data collection and annotation are particularly challengingrelying on such inputs could limit the models generalizability. Moreover, the extraction and annotation of fine-grained action-related pose and textual descriptions pose significant challenges due to their complex nature and the domain-specific knowledge required. With the advancement of generative models (Chen et al. 2024c; Zheng et al. 2024), we will strive to overcome these limitations in future work and further explore advanced models fine-grained visual understanding and generation capabilities. D. Gym-QA and Gym-New Gym-QA. To facilitate the evaluation of MLLMs in finegrained action understanding, we adapted the FineGym dataset into multiple-choice format, creating the GymQA dataset, as illustrated in Fig. 6. Following the coarsegrained action recognition paradigm from VideoChat2 (Li et al. 2024), we posed the question: What action is the athlete performing in the video? The answer options included one correct label and three distractor labels from the FineGym dataset. Gym-New. As demonstrated in Fig. 7, the Gym-New dataset is created by selecting direction-opposite action pairs from FineGym. This aims to provide more challenging environment for fine-grained action understanding, further testing the temporal perturbation that is the focus of our work. To delve deeper into the temporal directionality of ac-"
        }
    ],
    "affiliations": [
        "School of Automation, Northwestern Polytechnical University, Xian, China",
        "School of Computer Science, Northwestern Polytechnical University, Xian, China",
        "School of Software, Northwestern Polytechnical University, Xian, China",
        "Unmanned System Research Institute, Northwestern Polytechnical University, Xian, China"
    ]
}