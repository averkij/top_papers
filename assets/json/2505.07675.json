{
    "paper_title": "Simple Semi-supervised Knowledge Distillation from Vision-Language Models via $\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead $\\mathbf{\\texttt{O}}$ptimization",
    "authors": [
        "Seongjae Kang",
        "Dong Bok Lee",
        "Hyungjoon Jang",
        "Sung Ju Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) have achieved remarkable success across diverse tasks by leveraging rich textual information with minimal labeled data. However, deploying such large models remains challenging, particularly in resource-constrained environments. Knowledge distillation (KD) offers a well-established solution to this problem; however, recent KD approaches from VLMs often involve multi-stage training or additional tuning, increasing computational overhead and optimization complexity. In this paper, we propose $\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead $\\mathbf{\\texttt{O}}$ptimization ($\\mathbf{\\texttt{DHO}}$) -- a simple yet effective KD framework that transfers knowledge from VLMs to compact, task-specific models in semi-supervised settings. Specifically, we introduce dual prediction heads that independently learn from labeled data and teacher predictions, and propose to linearly combine their outputs during inference. We observe that $\\texttt{DHO}$ mitigates gradient conflicts between supervised and distillation signals, enabling more effective feature learning than single-head KD baselines. As a result, extensive experiments show that $\\texttt{DHO}$ consistently outperforms baselines across multiple domains and fine-grained datasets. Notably, on ImageNet, it achieves state-of-the-art performance, improving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively, while using fewer parameters."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 5 7 6 7 0 . 5 0 5 2 : r Simple Semi-supervised Knowledge Distillation from Vision-Language Models via Dual-Head Optimization Seongjae Kang, Dong Bok Lee, Hyungjoon Jang Sung Ju Hwang, VUNO Inc. KAIST DeepAuto.ai {seongjae.kang,hyungjoon.jang}@vuno.co, {markhi, sjhwang}@kaist.ac.kr Equal contribution"
        },
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) have achieved remarkable success across diverse tasks by leveraging rich textual information with minimal labeled data. However, deploying such large models remains challenging, particularly in resourceconstrained environments. Knowledge distillation (KD) offers well-established solution to this problem; however, recent KD approaches from VLMs often involve multi-stage training or additional tuning, increasing computational overhead and optimization complexity. In this paper, we propose Dual-Head Optimization (DHO)a simple yet effective KD framework that transfers knowledge from VLMs to compact, task-specific models in semi-supervised settings. Specifically, we introduce dual prediction heads that independently learn from labeled data and teacher predictions, and propose to linearly combine their outputs during inference. We observe that DHO mitigates gradient conflicts between supervised and distillation signals, enabling more effective feature learning than single-head KD baselines. As result, extensive experiments show that DHO consistently outperforms baselines across multiple domains and fine-grained datasets. Notably, on ImageNet, it achieves state-of-the-art performance, improving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively, while using fewer parameters."
        },
        {
            "title": "Introduction",
            "content": "Vision-Language Models (VLMs), which learn joint vision-language representations through contrastive learning [72, 42], have shown promising results in few-shot scenarios by leveraging rich textual information from pretraining. Recent work has explored various adaptation strategies for VLMs with limited labeled data, including parameter-efficient approaches such as linear probing [72, 51, 40], lightweight adapters [28, 115, 109, 78], and prompt-based fine-tuning methods [43, 120, 119, 44, 121, 45, 117, 75, 113, 49], demonstrating the potential of VLMs for data-limited visual recognition tasks. However, the substantial computational requirements and large model sizes of foundation models often make them impractical for deployment in resource-constrained environments, such as mobile devices [77]. While compact models could address these challenges, they typically struggle with limited performance, especially with limited labeled data. Semi-supervised learning techniques [80, 2, 7, 118] leveraging unlabeled data can help Figure 1: (Top): Previous multi-stage VLMs distillation methods. (Bottom): Our single-stage distillation framework with DHO. Preprint. Under review. Figure 2: (Left): DHO consistently outperforms single-head baselines on 11 datasets under 16-shot semisupervised setting. The improvements are evaluated in comparison to the second-best one. (Right): DHO achieves new SoTA on ImageNet in both 1% and 10% labeled data setting, with fewer parameters. address these performance limitations, but they are often suboptimal for small models compared to the performance of foundation models, such as VLMs. Knowledge distillation (KD) [38] has emerged as promising approach for transferring capabilities from large VLMs to more compact models. However, existing methods primarily focus on distilling large VLMs into smaller ones through general-purpose training [24, 85, 98, 101, 90, 88, 103] or preceding with unsupervised distillation stage [91, 97], which often requires additional fine-tuning for specific target tasks (Figure 1-(Top)). This multi-stage pipeline not only incurs extra computational overhead but also limits the direct transfer of the teacher models zeroand few-shot capabilities to the student model for task-specific applications, suggesting need for more efficient distillation methods that can preserve the teachers expertise for the intended task. In contrast, conventional KD methodssuch as logits distillation [38, 13] and feature matching [101]enable efficient single-stage distillation. However, we find these approaches suboptimal in semi-supervised settings. We attribute this limitation to the discrepancy between the labeled training data and the teacher models pre-trained knowledge [116], often leading to gradient conflicts in in Figure 4). Such conflicts are well known to hinder effective feature the feature extractor ( learning [110, 54, 11]. This issue is particularly challenging in few-shot settings, where the strong distillation signal of teacher can overwhelm the limited labeled data, necessitating careful balancing between the two signals. To address the above issue, we propose simple yet effective distillation framework, DHO (Dual-Head Optimization), which jointly leverages labeled samples and the teacher models probabilistic outputs by learning two distinct heads, each optimized with separate loss: the supervised loss and the KD loss, respectively (Figure 1(Bottom)). We observe that DHO mitigates gradient conflicts arising from the two distinct training signals ( in Figure 4). As result, DHO yields improved feature representations compared to conventional KD baselines, as demonstrated in Table 3 and Figure 7. Furthermore, to control the relative influence of the teacher and supervised predictions, we propose to generate the final output by linearly combining the predictions from both heads, whose effectiveness is also empirically validated in Figure 8. To validate the empirical effectiveness of the proposed DHO, we conduct extensive experiments across 11 different datasets including various tasks, such as generic object recognition [76, 26], fine-grained classification [67, 59, 69], domain-specific recognition [6], scene understanding [99], texture analysis [17], satellite imagery [34], and human actions [81]. The experimental results validate the effectiveness of DHO over conventional KD methods across all the datasets (Figure 2-(Left)). Furthermore, in comparison to previous state-of-the-art (SoTA) semi-supervised methods on the ImageNet, DHO achieves new SoTA performance, improving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively, while using fewer parameters (Figure 2-(Right)). Our contributions and findings are summarized as follows: We identify gradient conflicts in conventional single-head KD baselines under semi-supervised settings and propose simple yet effective framework, DHO, which mitigates such conflicts using dual prediction heads that separately learn from labeled data and teacher signals. We show that DHO improves feature learning compared to baselines and supports post-training adjustment of supervised and teacher signals via linear combination of the dual head outputs. We demonstrate that DHO outperforms single-head KD baselines across 11 datasets, achieving state-of-the-art results on ImageNet in semi-supervised settings with fewer parameters."
        },
        {
            "title": "2 Related Work",
            "content": "Vision-language pre-training. The emergence of vision-language pre-training has marked significant breakthrough, enabling the use of extensive image-text pairs collected from the web [95, 9] to train powerful vision encoders transferable to various vision tasks [27, 114]. Early works such as CLIP [72] and ALIGN [42] leveraged contrastive learning techniques to align images and text into joint representation space, facilitating zero-shot transfer via language prompts. Building on these foundations, subsequent research has focused on improving vision-language models through enhanced training methodologies [19, 29, 107, 112], as well as scaling models and datasets [107, 53, 18, 83, 15, 22, 84, 30] with their zero-shot transfer capabilities [42, 111, 71, 55]. In contrast, our work focuses specifically on target tasks with compact models, aiming to distill knowledge from these large VLMs effectively. Data-limited adaptation of VLMs. To preserve pretrained semantic features of VLMs during adaptation with limited data, several approaches have been proposed. Prompt tuning [50], initially designed for language models, has been successfully extended to vision tasks. Various methods [43, 120, 119, 44, 121, 45, 60, 117, 75, 113, 49] have demonstrated the effectiveness of training learnable prompts while keeping the base model frozen. Adapters [28, 115, 109, 78] provide an alternative approach by introducing lightweight, trainable modules while maintaining the pre-trained backbone intact. LP++ [40] has shown that simple linear layers can effectively adapt CLIP representations in data-limited settings. Note that our work is orthogonal to these approaches: we aim to distill the knowledge of pretrained VLMs into compact models under data-scarce scenarios, making these adaptation methods complementary and applicable to both teacher VLMs in our framework and student models when they are also VLMs. Knowledge Distillation (KD) [38] enables transferring knowledge from large teacher models to compact student architectures, particularly in data-constrained settings. Researchers have explored synthetic data generation [57, 47, 65, 106, 10, 105, 23, 66, 70, 108, 56, 86, 96], semi-supervised [13, 33, 21, 102], and unsupervised KD using self-supervised teachers [25, 1, 64, 94, 100, 79]. In the VLM domain, recent works [24, 98, 85, 101, 90, 88, 103] distill from large-scale vision-language models into smaller architectures, often using transductive [46, 14] or multi-stage unsupervised strategies [91, 97, 63]. Meanwhile, KD remains challenging due to numerous issues, including model capacity gaps [16, 62, 122, 39, 52] and inconsistencies between soft and hard targets [116]. These challenges are further complicated by misalignment between labeled data and foundational knowledge, especially in few-shot learning scenarios where limited labeled examples may not fully capture the rich semantic understanding of foundation models. While previous KD methods adopt dual-head architecturessuch as SSKD [33], which trains separate heads for labeled and unlabeled data, and DHKD [104], which applies binary KD loss to avoid neural collapse [68]they do not target distillation from foundation models or combine predictions at inference. In contrast, DHO addresses gradient conflicts in this setting and leverages dual-head aggregation at inference to enhance performance with minimal hyperparameter tuning costs."
        },
        {
            "title": "3 Methodology",
            "content": "We now elaborate on our approach, Dual-Head Optimization (DHO), simple yet effective Knowledge Distillation (KD) framework for semi-supervised settings. We first present the preliminaries of our method, such as background on VLMs, problem formulation, single-head KD baselines, in 3.1, followed by our proposed DHO method in 3.2. 3.1 Preliminary Background on VLMs. Our work builds upon Vision-Language Models (VLMs) such as CLIP [72] and ALIGN [42]. These models consist of multimodal encoders: an image encoder fX : Rd and text encoder fT : Rd where and denote the domains of images and texts, respectively. The encoders project their respective inputs, i.e., either an image or text description T, into shared embedding space Rd. They are trained via contrastive learning [12] to align corresponding image-text pairs in the shared embedding space. For zero-shot classification of VLMs across classes, we use predefined prompt templates, e.g., photo of [CLASS], where [CLASS] is the name of class. Given set of target classes, i.e., 3 Figure 3: Conceptual illustration on KD frameworks, Single-Head Optimization (SHO) and Dual-Head Optimization (DHO), for semi-supervised settings. As demonstrated in Figure 4, we observe gradient conflict of SHO. In contrast, DHO mitigates such conflicts by leveraging dual-head architectures. [C] := {1, . . . , C}, we generate prompted text descriptions {t1, t2, . . . , tC}. The zero-shot inference for categorical probability vector over the classes is defined as follows: = σ (cid:18) 1 ζ [CosSim(fX(x), fT(t1)), . . . , CosSim(fX(x), fT(tC))] (cid:19) C1, (1) where C1 is the probability simplex of dimension 1, σ : RC C1 denotes the soft- , ζ R>0 is the temperature max function defined as σ(l) = , . . . , (cid:105) (cid:104) (cid:80)C exp(lC ) c=1 exp(lc) scaling [38], and CosSim(x, y) = is the cosine similarity function. The final classification is determined by arg maxc[C] pc. Furthermore, for few-shot transfer of VLMs, we adopt Tip-Adapter [115], which builds key-pair queue using few labeled examples. (cid:80)C exp(l1) c=1 exp(lc) xy x2y2 }M , yn)}N +M 0.01) or 10% ( Problem Formulation. In this work, we focus on transferring knowledge from VLMs to compact, task-specific models under few-shot or low-shot semi-supervised learning scenarios, where both labeled and unlabeled data are utilized. Specifically, given K-shot and C-class classification problem, we are provided with labeled dataset D(l) = {(x(l) n=1, where = is the total number of labeled examples, and yn [C] denotes the class labels. Additionally, we have access to an unlabeled dataset D(u) = {x(u) m=1 consisting of unlabeled images. Low-shot learning represents more realistic setting than traditional few-shot learning where only small fraction, e.g., 1% ( +M 0.1) of the total dataset is labeled. Our goal is then to develop an student model by leveraging D(l) and D(u), guided by the knowledge of the VLM encoders fX and fT. The student model consists of feature extractor : Rd and prediction head : Rd RC, followed by the softmax function σ : RC C1. Single-head KD baselines. Building upon traditional KD approaches [38], numerous studies have explored semi-supervised KD [13, 33, 21, 102], unsupervised KD using self-supervised teacher models [25, 1, 64, 94, 100, 79], and KD from VLMs [24, 98, 85, 101, 90, 88, 103], to name just few. Our method builds on simple logit distillation in semi-supervised settings [38, 13] that combines supervised loss LCE on the labeled dataset D(l) with KD loss LKD on both labeled and unlabeled datasets D(l) D(u), i.e., λLCE + (1 λ)LKD, where λ [0, 1] is loss balancing hyperparameter. Specifically, the supervised loss LCE and KD loss LKD are defined as follows: LCE = LKD = 1 1 (cid:88) (cid:16) ℓ σ(h(z(l) )), yn (cid:17) , (cid:88) (cid:104) DKL σ(h(z(l) p(l) )) (cid:105) + 1 (cid:88) DKL (cid:104) σ(h(z(u) p(u) )) (cid:105) , (2) (3) ) and z(u) = g(x(u) C1 are the categorical probability vectors of labeled x(l) where ℓ denotes DKL represent cross-entropy and Kullback-Leibler divergence, respectively. z(l) = g(x(l) ) Rd are feature representations obtained by the feature extractor g. p(l) and p(u) and unlabeled data x(u) , respectively, obtained by teacher VLM encoders fX and fT, as described in Eq. 1. To optimize the parameters of and h, we use mini-batch version of the above objective with stochastic gradient descent. Another well-studied single-head KD baseline is feature distillation, which leverages mean squared error (MSE) loss to directly align feature representations of student and teacher fX. We defer the details of feature distillation for VLMs to CLIP-KD [101]. 4 3.2 Dual-Head Optimization (DHO) Gradient conflicts in single-head KD. One of the single-head KD baselines, logit distillation, described in 3.1, offers simple and efficient approach for transferring knowledge from VLMs in semi-supervised settings. However, we observe that its performance gain is suboptimal, which we attribute to gradient conflicts between the supervised and KD loss signalsa phenomenon well known to hinder effective feature representations in multi-task learnof Figing literature [110, 54, 11]. As illustrated in ure 4, the parameter vector θ of feature extractor suffers from this issue: the cosine similarity between the gradients often turns negative, i.e., CosSim(θLCE, θLKD) < 0, indicating misaligned optimization directions. Figure 4: The average cosine similarity and inner product across 10 datasets. ˆpCE = ˆpKD = σ(h(z)) for SHO. Gradient analysis of single-head KD. To understand this, we analyze the gradient w.r.t feature representation z. We denote h(z) = + b, where RCd, RC, and ˆp = σ(h(z)). Then, gradients for labeled data and their cosine similarity are1: zLCE = (ˆp y), zLKD = (ˆp p), (ˆp y)W (ˆp p) (ˆp y)2 (ˆp p) CosSim(zLCE, zLKD) = . (4) (5) denotes the one-hot ground-truth label and the teacher prediction. In few-shot settings, labeled data is scarce, making supervised signals noisy, while teacher predictions reflect pretraining priors that may not align with the limited labels. This mismatch often leads to prediction-level conflict, indicated by (ˆp y)(ˆp p) < 0, which rapidly falls below zero during training ( in Figure 4). As the dominant eigenvalue of grows to enhance class separability (Figure 13-(left)), it amplifies the conflict magnitude, causing CosSim(zLCE, zLKD) to become strongly negative (Figure 13-(right)). Eventually, this drives CosSim(θLCE, θLKD) down to 0.6 ( Dual-head architecture. To mitigate this issue, we propose to decouple the supervised and KD objectives via two independent prediction heads: hCE(z) = WCEz + bCE and hKD(z) = WKDz + bKD, with WCE, WKD RCd and bCE, bKD RC. The corresponding losses are: ). LCE = LKD = 1 1 (cid:88) (cid:88) (cid:16) σ(hCE(z(l) )), yn (cid:17) , ℓ (cid:104) DKL σ(hKD(z(l) p(l) )) (cid:105) + 1 (cid:88) DKL (cid:104) σ(hKD(z(u) p(u) )) (cid:105) , (6) (7) and the final loss is λLCE + (1 λ)LKD, following single-head KD. Denoting ˆpCE = σ(hCE(z)) and ˆpKD = σ(hKD(z)), the gradients for labeled data and their cosine similarity become: zLCE = CE(ˆpCE y), zLKD = KD(ˆpKD p), CosSim(zLCE, zLKD) = (ˆpCE y)WCEW CE(ˆpCE y)2 KD(ˆpKD p) KD(ˆpKD p)2 (8) (9) . Crucially, as shown in of Figure 4, the prediction discrepancy term (ˆpCE y)(ˆpKD p) of DHO remains near zero throughout training. This is because the separated heads hCE and hKD are learned independently, without shared projections or alignment constraints. This is further supported by the high-dimensional geometry (e.g., > 100): random vectors drawn from simplex or sphere tend to be nearly orthogonal due to the concentration of measure [92], resulting in weak correlation between their directions. Although spectral amplification occurs for WCEW KD (Figure 13-(left)), the discrepancy term (ˆpCE y)(ˆpKD p) remains negligible, keeping CosSim(zLCE, zLKD) close to zero (Figure 13-(right)). DHO thus maintains consistently positive gradient alignment, i.e., CosSim(θLCE, θLKD) > 0 (as shown in of Figure 4), enabling stable and conflict-free representation learning. See Algorithm 1 for the full training procedure of DHO. 1Note that this is not an exact cosine similarity, which depends on the full gradients θLCE, θLKD involving the Jacobian z/θ. Also, (ˆp y)(ˆp p) < 0 (ˆp y)W (ˆp p) < 0. Still, our analysis with observations offers useful intuition on how the discrepancy (ˆp y)(ˆp p) can amplify gradient conflicts. Language-aware initialization and KD head alignment for VLM students. We initialize the parameters of the dual heads hCE and hKD, i.e., WCE, WKD, bCE, bKD, using Kaiming initialization [31]. In the case of VLM-to-VLM distillation, however, we can leverage the teachers text encoder fT. Following prior work on language-aware initialization [51], we initialize the weights as WCE, WKD [fT(t1), . . . , fT(tC)] RCd. Furthermore, we align the prediction logic of KD head with that of teacher as follows: hKD = 1 ζ [CosSim(g(x), w1), . . . , CosSim(g(x), wC)] RC, (10) where wc Rd denotes the c-th row of WKD. This alignment encourages the KD head to mimic the teachers cosine similarity-based prediction behavior, leading to more stable distillation and improved performance, as shown in Table 5. Dual-heads interpolation for inference. We propose simple yet effective inference strategy for dual-head models by linearly interpolating: ˆp = α σ(hCE(z)) + (1 α) σ(hKD(z)/β) C1, (11) where α [0, 1] is an interpolation hyperparameter that balances the influence of the supervised and KD heads, and β R>0 is temperature parameter that softens the KD logits. The final prediction is obtained via arg maxc[C] ˆpc. This interpolation scheme is inspired by the mixture-of-experts (MoE) paradigm [41], where multiple predictive sources are combined, and their relative importance is adjusted based on task conditions. In our setting, α allows flexible weighting of the two heads depending on which sourcesupervised labels or teacher predictionsoffers more reliable guidance for given dataset. This balance can be tuned using validation set to reflect dataset-specific supervision quality and teacher accuracy. See Algorithm 2 for the detailed inference process of DHO."
        },
        {
            "title": "4 Experiments",
            "content": "We now elaborate the empirical effectiveness of the proposed method DHO. We first detail experimental setups in 4.1, and report the following empirical findings in 4.2: [F1]: In comparison to single-head KD baselines, DHO is an effective framework for semi-supervised settings, with negligible additional computational cost. [F2]: The improvement of DHO is attributed to gradient conflict mitigation, which leads to better feature representations compared to single-head KD baselines. [F3]: The proposed dual-heads output probability interpolation for inference further improves the performance of DHO on top of its enhanced feature representations. [F4]: DHO achieves state-of-the-art performance on ImageNet under low-shot semi-supervised settings, while using fewer parameters. 4.1 Experimental Setups Datasets. We use 11 diverse image classification datasets, including generic object recognition such as ImageNet [76], Caltech101 [26] and other widely-adopted benchmarks for fine-grained classification (Cars [48], Flowers102 [67], FGVCAircraft [59], OxfordPets [69]), domain-specific recognition (Food101 [6]), scene understanding (SUN397 [99]), texture analysis (DTD [17]), satellite imagery (EuroSAT [34]), and human actions (UCF101 [81]). See Appendix for details. Baselines. We first compare DHO with conventional single-head KD baselines; CE: we train model only on labeled dataset D(l) with cross entropy loss (Eq. 2), KD (logit): on unlabeled dataset D(u) with logit distillation (Eq. 3), and KD (feature): on D(u) with feature distillation [101]. We train model on both D(l) and D(u) with CE+KD (logit) or CE+KD (feature): which combines CE with each KD variant with the balancing hyperparameter λ. We also consider existing dual-head KD approaches: SSKD [33] uses dual-heads, with each head trained on labeled and unlabeled sets respectively, assuming different data distributions between them, and DHKD [104] introduces binary KD loss working on logits before softmax to prevent neural collapse [68], but both methods predict labels using only hCE. We further compare DHO with state-of-the-art methods on ImageNet with lowshot semi-supervised settings, including self and semi-supervised learning [118, 2, 13, 13, 3, 7, 7], CLIP-based-training [51, 55], co-training [74], KD [13], and zero-shot VLMs [112, 22]. Table 1: Results on ImageNet under few-shot semi-supervision using ResNet-18 and ResNet-50. Method 0-shot Single-head KD methods KD CE CE+KD (feature) [101] CE+KD (logit) [13] Dual-head KD methods DHO (zero-shot teacher [72]) DHO-F (few-shot teacher [115]) Teacher Models CLIP [72] Tip-Adapter-F [115] 51.0 - - - - - 50.0 - ResNet-18 trained from scratch 8-shot 1-shot 2-shot 4-shot 16-shot 0-shot 1-shot 2-shot 4-shot 8-shot 16-shot Self-supervised ResNet-50 - 0.7 17.1 50.5 51.8 53.7 - 52.0 - 1.1 23.5 50. 52.4 54.2 - 52.4 - 1.8 28.0 50.6 52.6 54.8 - 53.2 - 3.4 32.2 51. 53.3 56.2 - 54.0 - 8.2 33.8 51.2 54.5 57.7 - 55.3 61.0 - - - - - 60.3 - - 11.4 23.0 60.4 61.0 62.3 - 61.0 - 17.3 32.3 60. 62.1 63.1 - 61.6 - 26.4 41.3 61.2 62.5 63.9 - 62.5 - 36.7 48.2 61. 63.7 65.5 - 63.8 - 47.0 54.3 62.3 64.7 66.8 - 65.4 Figure 5: Results on 10 datasets under few-shot semi-supervision using ResNet-18 with zero-shot teacher [72]. Architecture choices for teacher and student models. For zero-shot teachers, we use CLIP ResNet-50 [72] in few-shot settings and VIT-H/14 from DFN [22] in low-shot settings. For few-shot teachers, we adopt Tip-Adapter-F [115], learnable adapter model, and denote the corresponding variant as DHO-F. Student models are initialized from pretrained checkpoints [87]. On ImageNet, to avoid label leakage, we either train ResNet-18 from scratch or use self-supervised ResNet-50 from DINO [8]. For other datasets, we use ResNet-18 and MobileNetV2 [77] without such concerns. In low-shot settings, we use CLIP ViT-B/16 and ViT-L/14 [72]. See Table 6 for an overview. Implementation details. We consider few-shot (1, 2, 4, 8, 16-shot) and low-shot (1% or 10%) settings, treating the rest as unlabeled. Prompt templates follow Tip-Adapter [115]. As detailed in 3.2, we use linear heads for all methods; additional analysis on non-linear heads is provided in Appendix E.2. All methods use consistent hyperparameters, including ζ = 0.01, λ = 0.5, and others. For KD (feature) baselines, we scale the MSE loss to match LCE following [101]. α and β of DHO are tuned on validation sets. When validation data is unavailable (e.g., ImageNet), we fix β = 0.5 across all settings. We heuristically set α = 0.4 for zero-shot teachers and α = 0.2 for few-shot teachers, reflecting the latters higher reliability. In low-shot settings, we use α = 0.5 due to increased label availability. Results are from single run due to limited compute. See Appendix B.2 for more details. 4.2 Experimental Results FLOPs (G) Params (M) Table 2: Inference overhead using RTX 4090. Throughput Model (im/s) [F1]: Effectiveness of DHO compared to SHO. We first compare DHO with SHO on ImageNet under 1, 2, 4, 8, and 16-shot settings using ResNet-18 (trained from scratch) and self-supervised ResNet-50. As shown in Table 1, DHO consistently outperforms SHO, with further gains from DHO-F that incorporates few-shot teachers. We also compare against dual-head KD methods on ResNet-50: SSKD and DHKD achieve 55.2/58.1/60.0/62.3/64.0 and 25.6/34.8/42.7/49.2/55.2, respectively, while DHO surpasses both across all shot counts. To assess generality, we evaluate on 10 additional datasets using ResNet-18. DHO consistently outperforms SHO, with larger improvements from few-shot teachers (Figures 5 and 6). Results for MobileNetV2 are shown in Figures 11 and 12. Table 2 shows these gains incur minimal overhead in FLOPs and throughput, especially on ImageNet and negligible for smaller datasets with fewer classes (C < 1000). See inference overhead for MobileNetV2, ViT-B/16, and ViT-L/14 in Table 7. 3525.7 3518.6 (-0.20%) 1018.4 1016.4 (-0.20%) 25.56 27.61 (+8.0%) 11.69 12.20 (+4.4%) 4.14 4.15 (+0.2%) 1.83 1.83 (+0.0%) ResNet-50 + DHO ResNet-18 + DHO 7 Figure 6: Results on 10 datasets using ResNet-18 with either zeroor few-shot teacher [115]. Figure 8: Results of ablation studies on dual-heads interpolation strategy in Eq. 11 of DHO. Method Top-1 (%) Top-5 (%) Table 3: Linear evaluation results. CE+KD (feature) [101] CE+KD (logit) [13] DHO [F2]: Enhanced feature representation of DHO. To validate our claim that mitigating gradient conflicts improves feature representations, we evaluate features using the standard linear evaluation protocol [13]. We train CE+KD (feature), CE+KD (logit), and DHO under the 16-shot semisupervised setting on ImageNet, freeze the feature extractor g, and train new prediction linear head hLE on top of using fully labeled data. As shown at the top of Table 3, DHO achieves higher Top-1 and Top-5 accuracy than other methods. To further assess feature quality, we visualize embeddings using t-SNE [89] in Figure 7. Compared to the CE+KD (logit) baseline, DHO produces more compact and class-separated feature clusters. These results support our claim that DHO enhances feature representations by mitigating gradient conflicts; this improvement leads to better performance compared to SHO, as discussed in [F1] and illustrated in Table 1, Figures 5, 6, 11 and 12. Figure 7: t-SNE visualization. 85.0 88.8 89.3 62.3 66.2 67.1 [F3]: Effectiveness of dual-head interpolation. We evaluate the effectiveness of dual-head interpolation (Eq. 11) by comparing DHO with CE+KD (logit), CE+KD (feature), and ablations DHO (hCE) and DHO (hKD), which predict using only one head, i.e., α = 1 or 0. We use ResNet-50 for ImageNet and ResNet-18 for the other datasets. As shown in Figure 8, DHO outperforms DHO (hCE) by an average of 1.6% across 11 datasets, with maximum gain of +3.4% on ImageNet and no degradation on any dataset. Since α and β are inference-time hyperparameters, dual-head interpolation introduces minimal overhead while consistently improving or maintaining performance. We further conduct case studies on its effect. Figure 9 illustrates three challenging examples: CE head (hCE) Figure 9: Qualitative results on challenging cases. is correct in the first, KD head (hKD) in the second, and both fail in the third case, yet the proposed combined prediction is correctdemonstrating the ability of DHO to resolve individual head failures. See additional analysis in Appendix E.3. 8 Table 4: Results on ImageNet under low-shot settings. For CT and MCT methods, numbers in parentheses indicate the number of different architectures for co-training. Table 5: Results of ablation studies on languageaware initialization and KD-head alignment for VLM students on ImageNet with 1% labeled data. Method Architecture Params 1% 10% (M) (%) (%) Init (hCE/hKD). Align. Accuracy (%) Self and Semi-supervised Learning MSN [3] Semi-ViT [7] Semi-ViT [7] ViT-B/4 ViT-L/14 ViT-H/ CLIP-based Training ViT-B/16 CLIP [51] REACT [55] ViT-B/16 REACT (Gated-Image) [55] ViT-B/16 ViT-L/14 CLIP [51] REACT [55] ViT-L/14 REACT (Gated-Image) [55] ViT-L/14 Co-training based Methods CT [74] MCT [74] CT [74] MCT [74] Multi-arch (2) Multi-arch (2) Multi-arch (4) Multi-arch (4) Knowledge Distillation SimCLR v2 distill [13] SimCLR v2 self-distill [13] ResNet-154 (3+SK) DHO (ours) DHO (ours) ResNet-50 (2+SK) ViT-B/16 ViT-L/ / / / / 78.3 78.5 (+0.2) 78.6 (+0.3) 78.7 (+0.4) 86 75.7 80.2 307 77.3 83.3 632 80.0 84.3 86 74.3 80.4 86 76.1 80.8 129 77.4 81.8 304 80.5 84.7 304 81.6 85.1 380 81.6 85.0 608 80.1 85.1 608 80.7 85.2 1071 80.0 84.8 1071 80.5 85. 140 75.9 80.2 795 76.6 80.9 86 81.6 82.8 304 84.6 85.9 Zero-shot VLMs SigLIP [112] DFN [22] ViT-SO400M/14 ViT-H/14 400 83.1 632 84.4 - - Figure 10: Visualization of grid search results for α and β using ResNet-50 on ImageNet under the 16-shot semi-supervised setting. [F4]: DHO achieves SoTA performance on ImageNet under low-shot settings. We compare DHO to previous state-of-the-art (SoTA) methods on ImageNet under 1% and 10% labeled data. All results are taken from published papers, except for DHO. As shown in Table 4, DHO with ViT-L/14 surpasses the previous SoTA by 3% (1% data) and 0.1% (10% data), while using 218M and 767M fewer parameters, respectively. Notably, DHO achieves 81.6% accuracy with only 86M parametersthe same as REACT [55] using 304M. These results show that DHO is simple yet effective method. Effectiveness of language-aware initialization and KD head alignment. Table 5 shows ablation results on language-aware initialization (Init.) and KD-head alignment (Align.) on ImageNet (1%). We use ViT-B/16 student and ViT-L/14 teacher for computational efficiency. We observe that applying Init. on each head independently improves performance, and applying Align. further enhances it. Effect of α and β. To understand how α and β in DHO affect performance, we visualize grid search results on ImageNet with ResNet-50 under the 16-shot setting. Figure 10 shows performance peaks at balanced heads (α 0.5) and degrades at extreme values (α 0 or 1) regardless of β. With balanced heads, performance remains stable for β [0.1, 1], optimizing around β = 0.3. Furthermore, under mild assumptions, Appendix shows that DHO ϵ-approximates SHO w.r.t the ℓ1 norm by setting α = λ and β = τ , where τ is the corresponding temperature hyperparameter for β in SHO. Notably, λ and τ are training hyperparameters, while α and β are inference hyperparameters. This allows DHO to emulate the effects of tuning hyperparameters of SHO without retraining. OOD generalization. Although this paper focuses on in-distribution generalization, we also evaluate DHO on ImageNet-based out-of-distribution (OOD) benchmarks [73, 93, 37, 36]. Due to page limits, we present the details in Appendix D.2; in summary, combining DHO with VPT [43], CoOP [120], and PromptSRC [45] consistently improves their OOD performance. Training cost and inference efficiency with ToMe. Table 8 reports the training cost of DHO, and Table 9 shows ToMe [5] significantly reduces inference overhead with minimal performance drop."
        },
        {
            "title": "5 Conclusion, Limitation, and Future Work",
            "content": "We presented DHO, framework that leverages both labeled samples and teacher predictions for knowledge distillation from pretrained VLMs in semi-supervised settings. Through [F14], we empirically demonstrated that DHO is simple yet effective approach. However, our work is currently limited to image classification tasks. Future work could extend DHO to other computer vision and multimodal tasks, such as object detection, segmentation, and language modeling. With suitable architectural adaptations, we believe the dual-head design of DHO can facilitate efficient knowledge transfer from foundation models across broader range of applications."
        },
        {
            "title": "References",
            "content": "[1] Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Compress: Selfsupervised learning by compressing representations. Advances in Neural Information Processing Systems, 33:1298012992, 2020. [2] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Armand Joulin, Nicolas Ballas, and Michael Rabbat. Semi-supervised learning of visual features by non-parametrically predicting view assignments with support samples. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 84438452, 2021. [3] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Mike Rabbat, and Nicolas Ballas. Masked siamese networks for label-efficient learning. In European Conference on Computer Vision, pages 456473. Springer, 2022. [4] Jimmy Lei Ba. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [5] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. [6] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101mining discriminative components with random forests. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part VI 13, pages 446461. Springer, 2014. [7] Zhaowei Cai, Avinash Ravichandran, Paolo Favaro, Manchen Wang, Davide Modolo, Rahul Bhotika, Zhuowen Tu, and Stefano Soatto. Semi-supervised vision transformers at scale. Advances in Neural Information Processing Systems, 35:2569725710, 2022. [8] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [9] Fei-Long Chen, Du-Zhen Zhang, Ming-Lun Han, Xiu-Yi Chen, Jing Shi, Shuang Xu, and Bo Xu. Vlp: survey on vision-language pre-training. Machine Intelligence Research, 20(1): 3856, 2023. [10] Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi, Chunjing Xu, Chao Xu, and Qi Tian. Data-free learning of student networks. In Proceedings of the IEEE/CVF international conference on computer vision, pages 35143522, 2019. [11] Jie Chen and Meng Joo Er. Mitigating gradient conflicts via expert squads in multi-task learning. Neurocomputing, 614:128832, 2025. [12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PMLR, 2020. [13] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems, 33:2224322255, 2020. [14] Yifan Chen, Xiaozhen Qiao, Zhe Sun, and Xuelong Li. Comkd-clip: Comprehensive knowledge distillation for contrastive language-image pre-traning model. arXiv preprint arXiv:2408.04145, 2024. [15] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 28182829, 2023. [16] Jang Hyun Cho and Bharath Hariharan. On the efficacy of knowledge distillation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 47944802, 2019. [17] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36063613, 2014. [18] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 74807512. PMLR, 2023. [19] Xiaoyi Dong, Jianmin Bao, Yinglin Zheng, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang, Lu Yuan, Dong Chen, et al. Maskclip: Masked self-distillation advances contrastive language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1099511005, 2023. [20] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [21] Pan Du, Suyun Zhao, Zisen Sheng, Cuiping Li, and Hong Chen. Semi-supervised learning via weight-aware distillation under class distribution mismatch. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1641016420, 2023. [22] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. [23] Gongfan Fang, Jie Song, Xinchao Wang, Chengchao Shen, Xingen Wang, and Mingli Song. Contrastive model inversion for data-free knowledge distillation. arXiv preprint arXiv:2105.08584, 2021. [24] Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lijuan Wang, Yezhou Yang, and Zicheng In Proceedings of Liu. Compressing visual-linguistic model via knowledge distillation. the IEEE/CVF International Conference on Computer Vision, pages 14281438, 2021. [25] Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang, Yezhou Yang, and Zicheng Liu. Seed: Self-supervised distillation for visual representation. arXiv preprint arXiv:2101.04731, 2021. [26] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision and pattern recognition workshop, pages 178178. IEEE, 2004. [27] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Visionlanguage pre-training: Basics, recent advances, and future trends. Foundations and Trends in Computer Graphics and Vision, 14(34):163352, 2022. [28] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. International Journal of Computer Vision, 132(2):581595, 2024. [29] Yuting Gao, Jinfeng Liu, Zihan Xu, Jun Zhang, Ke Li, Rongrong Ji, and Chunhua Shen. Pyramidclip: Hierarchical feature alignment for vision-language model pretraining. Advances in neural information processing systems, 35:3595935970, 2022. [30] Qingpei Guo, Furong Xu, Hanxiao Zhang, Wang Ren, Ziping Ma, Lin Ju, Jian Wang, Jingdong Chen, and Ming Yang. M2-encoder: Advancing Bilingual Image-Text Understanding by Large-scale Efficient Pretraining. arXiv preprint arXiv:2401.15896, 2024. [31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 10261034, 2015. [32] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 11 [33] Lingxiao He, Wu Liu, Jian Liang, Kecheng Zheng, Xingyu Liao, Peng Cheng, and Tao arXiv preprint Semi-supervised domain generalizable person re-identification. Mei. arXiv:2108.05045, 2021. [34] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):22172226, 2019. [35] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. [36] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: critical analysis of out-of-distribution generalization. ICCV, 2021. [37] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1526215271, 2021. [38] Geoffrey Hinton. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [39] Tao Huang, Shan You, Fei Wang, Chen Qian, and Chang Xu. Knowledge distillation from stronger teacher. Advances in Neural Information Processing Systems, 35:3371633727, 2022. [40] Yunshi Huang, Fereshteh Shakeri, Jose Dolz, Malik Boudiaf, Houda Bahig, and Ismail Ben Ayed. Lp++: surprisingly strong linear probe for few-shot clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2377323782, 2024. [41] Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. [42] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, YunHsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 49044916. PMLR, 2021. [43] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pages 709727. Springer, 2022. [44] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1911319122, 2023. [45] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation without forgetting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1519015200, 2023. [46] Gyeongman Kim, Doohyuk Jang, and Eunho Yang. Promptkd: Distilling studentfriendly knowledge for generative language models via prompt tuning. arXiv preprint arXiv:2402.12842, 2024. [47] Akisato Kimura, Zoubin Ghahramani, Koh Takeuchi, Tomoharu Iwata, and Naonori Ueda. Few-shot learning of neural networks from scratch by pseudo example optimization. arXiv preprint arXiv:1802.03039, 2018. [48] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554561, 2013. 12 [49] Marc Lafon, Elias Ramzi, Clément Rambour, Nicolas Audebert, and Nicolas Thome. Gallop: Learning global and local prompts for vision-language models. In European Conference on Computer Vision, pages 264282. Springer, 2025. [50] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. [51] Chunyuan Li, Haotian Liu, Liunian Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, et al. Elevater: benchmark and toolkit for evaluating language-augmented visual models. Advances in Neural Information Processing Systems, 35:92879301, 2022. [52] Xin-Chun Li, Wen-Shu Fan, Bowen Tao, Le Gan, and De-Chuan Zhan. Exploring dark knowledge under various teacher capacities and addressing capacity mismatch. arXiv preprint arXiv:2405.13078, 2024. [53] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training via masking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2339023400, 2023. [54] Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-averse gradient descent for multi-task learning. Advances in Neural Information Processing Systems, 34: 1887818890, 2021. [55] Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jianfeng Gao, Yong Jae Lee, and Chunyuan Li. Learning customized visual models with retrieval-augmented knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1514815158, 2023. [56] He Liu, Yikai Wang, Huaping Liu, Fuchun Sun, and Anbang Yao. Small scale data-free knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60086016, 2024. [57] Raphael Gontijo Lopes, Stefano Fenu, and Thad Starner. Data-free knowledge distillation for deep neural networks. arXiv preprint arXiv:1710.07535, 2017. [58] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [59] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Finegrained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. [60] Cristina Menghini, Andrew Delworth, and Stephen Bach. Enhancing clip with clip: Exploring pseudolabeling for limited-label prompt tuning. Advances in Neural Information Processing Systems, 36:6098461007, 2023. [61] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. [62] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 51915198, 2020. [63] Marco Mistretta, Alberto Baldrati, Marco Bertini, and Andrew Bagdanov. Improving zeroshot generalization of learned prompts via unsupervised knowledge distillation. In European Conference on Computer Vision, pages 459477. Springer, 2025. [64] Navaneet, Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Simreg: Regression as simple yet effective tool for self-supervised knowledge distillation. In British Machine Vision Conference (BMVC), 2021. [65] Gaurav Kumar Nayak, Konda Reddy Mopuri, Vaisakh Shaj, Venkatesh Babu Radhakrishnan, and Anirban Chakraborty. Zero-shot knowledge distillation in deep networks. In International Conference on Machine Learning, pages 47434751. PMLR, 2019. 13 [66] Dang Nguyen, Sunil Gupta, Kien Do, and Svetha Venkatesh. Black-box few-shot knowledge distillation. In European Conference on Computer Vision, pages 196211. Springer, 2022. [67] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pages 722729. IEEE, 2008. [68] Vardan Papyan, XY Han, and David Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):2465224663, 2020. [69] Omkar Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pages 34983505. IEEE, 2012. [70] Gaurav Patel, Konda Reddy Mopuri, and Qiang Qiu. Learning to retain while acquiring: Combating distribution-shift in adversarial data-free knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 77867794, 2023. [71] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al. Combined scaling for zero-shot transfer learning. Neurocomputing, 555:126658, 2023. [72] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [73] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International conference on machine learning, pages 53895400. PMLR, 2019. [74] Jay Rothenberger and Dimitrios Diochnos. Meta co-training: Two views are better than one. arXiv preprint arXiv:2311.18083, 2023. [75] Shuvendu Roy and Ali Etemad. Consistency-guided prompt learning for vision-language models. arXiv preprint arXiv:2306.01195, 2023. [76] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211252, 2015. [77] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 45104520, 2018. [78] Julio Silva-Rodriguez, Sina Hajimiri, Ismail Ben Ayed, and Jose Dolz. closer look at the few-shot adaptation of large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2368123690, 2024. [79] Aditya Singh and Haohan Wang. Simple unsupervised knowledge distillation with space similarity. In European Conference on Computer Vision, pages 147164. Springer, 2025. [80] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semisupervised learning with consistency and confidence. Advances in neural information processing systems, 33:596608, 2020. [81] Soomro. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. [82] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):19291958, 2014. 14 [83] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. [84] Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong Wang. Eva-clip-18b: Scaling clip to 18 billion parameters. arXiv preprint arXiv:2402.04252, 2024. [85] Ximeng Sun, Pengchuan Zhang, Peizhao Zhang, Hardik Shah, Kate Saenko, and Xide Xia. In Proceedings of the Dime-fm: Distilling multimodal and efficient foundation models. IEEE/CVF International Conference on Computer Vision, pages 1552115533, 2023. [86] Minh-Tuan Tran, Trung Le, Xuan-May Le, Jianfei Cai, Mehrtash Harandi, and Dinh Phung. Large-scale data-free knowledge distillation for imagenet via multi-resolution data generation. arXiv preprint arXiv:2411.17046, 2024. [87] Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962, 2019. [88] Vishaal Udandarao, Nikhil Parthasarathy, Muhammad Ferjad Naeem, Talfan Evans, Samuel Albanie, Federico Tombari, Yongqin Xian, Alessio Tonioni, and Olivier Hénaff. Active data curation effectively distills large-scale multimodal models. arXiv preprint arXiv:2411.18674, 2024. [89] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. [90] Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, and Oncel Tuzel. Mobileclip: Fast image-text models through multi-modal reinforced training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1596315974, 2024. [91] Raviteja Vemulapalli, Hadi Pouransari, Fartash Faghri, Sachin Mehta, Mehrdad Farajtabar, Mohammad Rastegari, and Oncel Tuzel. Knowledge transfer from vision foundation models for efficient training of small task-specific models. In International Conference on Machine Learning (ICML), 2024. [92] Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge university press, 2018. [93] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pages 1050610518, 2019. [94] Kai Wang, Fei Yang, and Joost van de Weijer. Attention distillation: self-supervised vision transformer students need more guidance. arXiv preprint arXiv:2210.00944, 2022. [95] Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang, Yonghong Tian, and Wen Gao. Large-scale multi-modal pre-trained models: comprehensive survey. Machine Intelligence Research, 20(4):447482, 2023. [96] Yuzheng Wang, Dingkang Yang, Zhaoyu Chen, Yang Liu, Siao Liu, Wenqiang Zhang, Lihua Zhang, and Lizhe Qi. De-confounded data-free knowledge distillation for handling distribution shifts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1261512625, 2024. [97] Ge Wu, Xin Zhang, Zheng Li, Zhaowei Chen, Jiajun Liang, Jian Yang, and Xiang Li. Cascade prompt learning for vision-language model adaptation. In European Conference on Computer Vision, pages 304321. Springer, 2025. [98] Kan Wu, Houwen Peng, Zhenghong Zhou, Bin Xiao, Mengchen Liu, Lu Yuan, Hong Xuan, Michael Valenzuela, Xi Stephen Chen, Xinggang Wang, et al. Tinyclip: Clip distillation via affinity mimicking and weight inheritance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2197021980, 2023. 15 [99] Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 34853492. IEEE, 2010. [100] Haohang Xu, Jiemin Fang, Xiaopeng Zhang, Lingxi Xie, Xinggang Wang, Wenrui Dai, Hongkai Xiong, and Qi Tian. Bag of instances aggregation boosts self-supervised distillation. arXiv preprint arXiv:2107.01691, 2021. [101] Chuanguang Yang, Zhulin An, Libo Huang, Junyu Bi, Xinqiang Yu, Han Yang, Boyu Diao, and Yongjun Xu. Clip-kd: An empirical study of clip model distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1595215962, 2024. [102] Jing Yang, Xiatian Zhu, Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. Knowledge distillation meets open-set semi-supervised learning. International Journal of Computer Vision, pages 120, 2024. [103] Kaicheng Yang, Tiancheng Gu, Xiang An, Haiqiang Jiang, Xiangzi Dai, Ziyong Feng, Weidong Cai, and Jiankang Deng. Clip-cid: Efficient clip distillation via cluster-instance discrimination. arXiv preprint arXiv:2408.09441, 2024. [104] Penghui Yang, Chen-Chen Zong, Sheng-Jun Huang, Lei Feng, and Bo An. Dual-head knowledge distillation: Enhancing logits utilization with an auxiliary head. arXiv preprint arXiv:2411.08937, 2024. [105] Hongxu Yin, Pavlo Molchanov, Jose Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj Jha, and Jan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 87158724, 2020. [106] Jaemin Yoo, Minyong Cho, Taebum Kim, and Kang. Knowledge extraction with no observable data. Advances in Neural Information Processing Systems, 32, 2019. [107] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022. [108] Shikang Yu, Jiachen Chen, Hu Han, and Shuqiang Jiang. Data-free knowledge distillation via feature exchange and activation region constraint. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2426624275, 2023. [109] Tao Yu, Zhihe Lu, Xin Jin, Zhibo Chen, and Xinchao Wang. Task residual for tuning visionlanguage models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1089910909, 2023. [110] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. Advances in neural information processing systems, 33:58245836, 2020. [111] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1812318133, 2022. [112] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. [113] Ji Zhang, Shihan Wu, Lianli Gao, Heng Tao Shen, and Jingkuan Song. Dept: Decoupled prompt tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1292412933, 2024. [114] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 16 [115] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprint arXiv:2111.03930, 2021. [116] Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Jialu Liu, Michael Bendersky, Marc Najork, and Chao Zhang. Do not blindly imitate the teacher: Using perturbed loss for knowledge distillation. arXiv preprint arXiv:2305.05010, 2023. [117] Cairong Zhao, Yubin Wang, Xinyang Jiang, Yifei Shen, Kaitao Song, Dongsheng Li, and Duoqian Miao. Learning domain invariant prompt for vision-language models. IEEE Transactions on Image Processing, 2024. [118] Mingkai Zheng, Shan You, Lang Huang, Chen Luo, Fei Wang, Chen Qian, and Chang Xu. Simmatchv2: Semi-supervised learning with graph consistency. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1643216442, 2023. [119] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1681616825, 2022. [120] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):23372348, 2022. [121] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. Prompt-aligned gradient for prompt tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1565915669, 2023. [122] Yichen Zhu and Yi Wang. Student customized knowledge distillation: Bridging the gap between student and teacher. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 50575066, 2021."
        },
        {
            "title": "Appendix Overview",
            "content": "This appendix provides supplementary material to support the main paper and is organized as follows: Theoretical Analysis (Appendix A): provides mathematical foundations and theoretical guarantees for our approach. Algorithms and Implementation (Appendix B): presents detailed pseudocode (Appendix B.1), implementation specifics (Appendix B.2), and computational overhead analysis (Appendix B.3). Datasets (Appendix C): describes the datasets used in our experiments, including statistics and preprocessing details. Additional Experiments (Appendix D): presents MobileNet experiments (Appendix D.1) and out-of-distribution generalization results (Appendix D.2). Additional Analyses (Appendix E): contains gradient analysis (Appendix E.1), non-linear head design studies (Appendix E.2), and further dual-head investigations (Appendix E.3)."
        },
        {
            "title": "A Theoretical Analysis",
            "content": "In this section, we provide theoretical analysis of our Dual-Head Optimization (DHO) framework. We establish that DHO effectively addresses single-head logit distillation [38, 13] by decoupling conflicting gradients through specialized heads during training. We prove that post-training, the optimal prediction from our dual-head modelformulated as weighted combination of the heads outputsis mathematically equivalent to the optimal solution of conventional single-head distillation. This equivalence provides theoretical justification for our approach while eliminating gradient conflicts. Furthermore, DHO enables efficient adaptation to various datasets through tunable hyperparameters (α and β) without requiring model retraining. Note that in this section we slightly abuse the notation of the main paper for clarity, e.g., we denote pτ as teacher predictions with temperature scaling τ . A.1 Single-Head Optimization We begin by considering two target probability distributions: the ground truth label distribution and the teachers softened distribution pτ for input , where: represents the ground truth label distribution, typically one-hot encoded vectors where yc = 1 for the true class and 0 elsewhere pτ denotes the teachers softened distribution with temperature scaling: pτ = σ(zt/τ ), where zt represents the teachers logits and σ is the softmax function Theorem A.1 (Optimal Distribution for Single-Head Optimization). The distribution ˆp that minimizes the weighted combination of cross-entropy loss with respect to and Kullback-Leibler divergence with respect to pτ : L(ˆp) = λℓ(ˆp, y) + (1 λ)DKL(pτ ˆp) is given by the weighted arithmetic mean: where λ [0, 1] is the weighting hyperparameter. ˆp = λy + (1 λ)pτ Proof. We begin by expanding the objective function: 18 (12) (13) L(ˆp) = λℓ(ˆp, y) + (1 λ)DKL(pτ ˆp) (cid:88) (cid:88) yc log ˆpc + (1 λ) = λ c=1 (cid:88) c=1 = λ yc log ˆpc + (1 λ) c=1 (cid:88) c=1 pτ,c log pτ,c ˆpc (14) (15) pτ,c log pτ,c (1 λ) (cid:88) c=1 pτ,c log ˆpc (16) (cid:88) [λyc + (1 λ)pτ,c] log ˆpc + (1 λ) = c=1 (cid:88) c=1 pτ,c log pτ,c (17) Since the last term is constant with respect to ˆp, the optimization problem reduces to minimizing: L(ˆp) = (cid:88) [λyc + (1 λ)pτ,c] log ˆpc c=1 Subject to the probability constraints: (cid:88) c=1 ˆpc = 1, ˆpc 0 {1, 2, . . . , C} Applying the method of Lagrange multipliers with multiplier µ: L(ˆp, µ) = (cid:88) [λyc + (1 λ)pτ,c] log ˆpc + µ c= (cid:33) ˆpc 1 (cid:32) (cid:88) c=1 Taking the partial derivative with respect to ˆpc and setting it to zero: Solving for ˆpc: λyc + (1 λ)pτ,c ˆpc + µ = 0 ˆpc = λyc + (1 λ)pτ,c µ (18) (19) (20) (21) (22) Using the constraint (cid:80)C probability distributions): c=1 ˆpc = 1, and observing that (cid:80)C c=1 yc = 1 and (cid:80)C c=1 pτ,c = 1 (both being (cid:88) c=1 ˆpc = (cid:88) c=1 λyc + (1 λ)pτ,c µ = 1 1 µ 1 µ [λ (cid:88) c=1 (cid:88) c=1 1 µ [λyc + (1 λ)pτ,c] = 1 yc + (1 λ) (cid:88) c=1 pτ,c] = 1 [λ + (1 λ)] = 1 µ = 1 Therefore, the optimal solution is: ˆp = λyc + (1 λ)pτ,c (23) (24) (25) (26) (27) (28) This weighted arithmetic mean of the two target distributions is the optimal solution that minimizes our objective function. 19 A.2 Dual-Head Optimization In our proposed Dual-Head Optimization (DHO) framework, we extract shared features g(x) from input and apply two specialized classification heads: hCE(z) = WCEz + bCE: optimized exclusively to match ground truth labels using cross-entropy loss ℓ(σ(hCE(z)), y) hKD(z) = WKDz + bKD: optimized exclusively to match teacher predictions using KL divergence DKL(pτ σ(hKD(z)/β)) where = g(x) is the feature representation, and the parameter β controls the temperature during inference, while fixed temperature of 1 is used during training of the knowledge distillation head. Assumption A.2 (ε-Convergence). We assume that after sufficient training, both heads have converged to their respective target distributions with bounded error: σ(hCE(z)) y1 ε, sup sup σ(hKD(z)/β) pτ 1 ε (29) where 1 denotes the ℓ1 norm and ε > 0 is small constant. Theorem A.3 (Inference Equivalence Under ε-Convergence). Under Assumption A.2, by combining the outputs of both heads as: ˆpDHO = α σ(hCE(z)) + (1 α) σ(hKD(z)/β), where α = λ we obtain prediction that approximates the optimal single-head solution with bounded error: ˆpDHO ˆp1 ε Proof. We analyze the ℓ1 distance between the DHO prediction and the optimal solution: ˆpDHO ˆp1 = α σ(hCE(z)) + (1 α) σ(hKD(z)/β) λy (1 λ)pτ 1 = λ(σ(hCE(z)) y) + (1 λ)(σ(hKD(z)/β) pτ )1 λσ(hCE(z)) y1 + (1 λ)σ(hKD(z)/β) pτ 1 λε + (1 λ)ε = ε where we applied the triangle inequality for the ℓ1 norm and used Assumption A.2. Therefore, we have established that: ˆpDHO ε ˆp where ε denotes approximation with ℓ1 error bound ε. (30) (31) (32) (33) (34) (35) (36) Lemma A.4 (Temperature Matching via KL Divergence). Assume the knowledge distillation head is trained to minimize KL divergence with respect to the teachers predictions at temperature 1, such that: DKL(p1σ(hKD(z))) δ (37) Then, setting the temperature parameter β = τ at inference time allows the KD head to approximate the teachers prediction at temperature τ with error bound: σ(hKD(z)/β) pτ 1 2δ (38) Proof. When logits are properly scaled and under appropriate conditions of the softmax function, we can reasonably approximate: DKL(pτ σ(hKD(z)/τ )) DKL(p1σ(hKD(z))) δ (39) Applying Pinskers inequality, which establishes relationship between KL divergence and the L1 norm difference between probability distributions: pτ σ(hKD(z)/τ )1 (cid:112)2DKL(pτ σ(hKD(z)/τ )) 2δ (40) 20 To ensure ε-convergence between the KD head at temperature τ and the teachers prediction at temperature τ , it is sufficient to guarantee: 2δ ε δ ε2 2 (41) Corollary A.5 (Optimal DHO Configuration). With proper training ensuring ε-convergence of both heads, dual-head optimization with temperature parameter β = τ and mixing parameter α = λ approximates the optimal single-head objective with error bounded by ε: ˆpDHO ε ˆp = λy + (1 λ)pτ (42) This demonstrates that our DHO approach achieves the same theoretical optimality as SHO."
        },
        {
            "title": "B Algorithms and Implementation",
            "content": "B.1 Pseudocode We present the pseudocode for DHO in Algorithms 1 and 2 for training and inference, respectively. i=1, unlabeled set D(u) = {x(u) }M j=1, student feature extractor g, prediction heads hCE, hKD, teacher encoders fX, fT, prompt template photo of [CLASS], temperature scaling factors ζ, η, balancing hyperparameter λ, supervised mini-batch size B, and unsupervised mini-batch size B. , yi)}N Algorithm 1 DHO Training with zero-shot CLIP [72] teacher 1: Input: labeled set D(l) = {(x(l) 2: 3: 4: 5: 6: while not converged do 7: 8: 9: Sample mini-batch B(l) = {(x(l) // Process labeled data for each (x(l) , yb)}B , yb) B(l) do b=1 from D(l), B(u) = {x(u) }B b=1 from D(l) D(u). 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: z(l) g(x(l) ) ˆp(l) CE,b σ(hCE(z(l) )) η hKD(z(l) ˆp(l) KD,b σ( 1 )) (cid:16) 1 p(l) ζη [CosSim(fX(x(l) σ end for // Process unlabeled data for each x(u) B(u) do z(u) g(x(u) ) η hKD(z(u) ˆp(u) KD,b σ( 1 )) (cid:16) 1 ζη [CosSim(fX(x(u) p(u) σ end for // Compute losses and update b=1 ℓ(ˆp(l) LCE 1 CE,b, yb) b=1 DKL(ˆp(l) LKD 1 λLCE + (1 λ)LKD Update parameters of g, hCE, hKD using KD,bp(l) ) + 1 (cid:80)B (cid:80)B 23: 24: 25: 26: end while ), fT(t1)), . . . , CosSim(fX(x(l) ), fT(tC))](cid:17) ), fT(t1)), . . . , CosSim(fX(x(u) ), fT(tC))](cid:17) (cid:80)B b=1 DKL(ˆp(u) KD,bp(u) ) 21 Algorithm 2 Dual-Head Optimization Inference 1: Input: an image x, feature extractor g, prediction heads hCE, hKD, linear coefficient α, temperature scaling β 2: g(x) 3: ˆpCE σ(hCE(z)) 4: ˆpKD σ(hKD(z)/β) 5: ˆp α ˆpCE + (1 α) ˆpKD 6: ˆy arg maxc(ˆpc) 7: Return: ˆy B.2 Implementation Details Table 6: Implementation details for our experiments across different settings. Few-shot Semi-supervised Settings on ImageNet Model Configuration Student Training Details Student: ResNet18 [32] from scratch or ResNet50 from DINO [8] Input size: 224224 Zero-shot Teacher: ResNet50 from CLIP [72] Few-shot Teacher: ResNet50 from Tip-AdapterF [115] Teacher input size: 224224 labeled data: {1, 2, 4, 8, 16} shots ζ, η, and λ: 0.01, 2, 0.5 α and β: α = 0.4, β = 0.5 (zero-shot); α = 0.2, β = 0.5 (few-shot) Epochs: 20 Optimizer: AdamW (β1=0.9, β2=0.999) Learning rate: 1 103, weight decay: 1 102 Batch size: 512 (labeled: 256, unlabeled: 256) Scheduler: Cosine decay without warmup Augmentation: Random crops (x0.5-1.0), horizontal flips Few-shot Semi-supervised Settings on 10 Fine-Grained Datasets Model Configuration Student Training Details Student: ResNet18 [32] or MobileNet [77] pretrained on ImageNet under supervision Input size: 224224 Zero-shot Teacher: ResNet50 from CLIP [72] Few-shot Teacher: ResNet50 from Tip-AdapterF [115] Teacher input size: 224224 labeled data: {1, 2, 4, 8, 16} shots ζ, η, and λ: 0.01, 2, 0.5 α and β: determined by validation Epochs: 200 Optimizer: AdamW (β1=0.9, β2=0.999) Learning rate: 1 103, weight decay: 1 102 Batch size: 128 (labeled: 64, unlabeled: 64) Scheduler: Cosine decay without warmup Augmentation: Random crops (x0.5-1.0), horizontal flips Model Configuration Student Training Details Low-shot Semi-supervised Settings on ImageNet Student: CLIP ViT-B/16 or ViT-L/14 [72] Input size: 224224 (ViT-B/16) or 336336 (ViTL/14) Zero-shot Teacher: CLIP ViT-L/14 or ViTH/14 [22] 336336 (ViT-L/14) or Teacher input size: 378378 (ViT-H/14) Few-shot Teacher: N/A labeled data: 1% ( ( +M 0.1) of training data ζ, η, and λ: 0.01, 2, 0.5 α and β: α = 0.5, β = 0.5 +M 0.01) or 10% Epochs: 32 Optimizer: AdamW (β1=0.9, β2=0.999) Learning rate: 5 105, weight decay: 5 102 Batch size: 512 (labeled: 256, unlabeled: 256) Scheduler: Cosine warmup decay (5000 steps) Augmentation: Random crops (x0.5-1.0), horizontal flips Table 6 provides comprehensive overview of the implementation details for our three experimental settings. For ImageNet, we train ResNet50 from DINO [8] or ResNet18 [32] from scratch over 20 epochs with teacher of ResNet50 from CLIP [72], while using Tip-Adapter-F [115] as few-shot teacher. In our 10 datasets experiments, ImageNet pre-trained ResNet18 [32] or MobileNet [77] serve as students with the same teachers but trained for longer period of 200 epochs. For VLM distillation, we use smaller CLIP models (ViT-B/16, ViT-L/14) [72] as students learning from larger models (ViT-L/14 [72], ViT-H/14 [22]) over 32 epochs. All configurations use AdamW [58] optimization with appropriate learning rates, weight decay, and cosine scheduling. Table 6 also details the fewshot settings (ranging from 1-16 shots where applicable), inference parameters, and consistent data augmentation strategies across all experiments. The batch composition maintains 1:1 ratio between labeled and unlabeled samples, i.e., = = 128 in Algorithm 1, with λ = 0.5 for loss weighting. B.3 Computational Costs Table 7: Inference overhead using RTX 4090 across different architecture. Model Params (M) FLOPs (G) Throughput (im/s) MobileNetV2 + DHO 3.50 4.79 (+36.5%) 0.33 0.34 (+3.0%) 1.83 1.83 (+0.0%) 4.14 4.15 (+0.2%) 11.69 12.20 (+4.4%) 25.56 27.61 (+8.0%) 86.57 87.34 (+0.9%) 16.87 16.87 (+0.0%) 304.33 305.35 (+0.3%) 59.70 59.70 (+0.0%) 2978.4 2971.2 (-0.24%) 3525.7 3518.6 (-0.20%) 1018.4 1016.4 (-0.19%) 290.2 290.1 (-0.02%) 255.1 255.6 (+0.18%) ResNet-18 + DHO ResNet-50 + DHO ViT-B/16 + DHO ViT-L/16 + DHO Inference overhead of DHO over SHO. Table 7 presents computational overheads at inference time introduced by DHO over SHO for all the architectures in this paper, such as MobileNetV2 [77], ResNet-18 [32], ResNet-50 [32], ViT-B/16 [20], and ViT-L/16 [20]. Student Teacher Training Time Table 8: Training time and hardware. Training time and hardware requirements. Table 8 presents the training time required for our experiments. For VLM distillation experiments, which represent the most resourceintensive component of our work, we used 8 NVIDIA RTX 4090 GPUs. The ViT-H/14 to ViT-L/14 distillation required approximately 80 hours, while the ViT-H/14 to ViT-B/16 and ViT-L/14 to ViT-B/16 distillations required approximately 40 and 28 hours, respectively. For the ViT-H/14 to ViT-L/14 distillation, we implemented gradient accumulation with 4 steps and mixed precision training [61] to optimize computational efficiency. For ImageNet experiments, we used 4 NVIDIA RTX 4090 GPUs, with ResNet-18 and ResNet-50 models requiring approximately 6 and 8 hours of training time, respectively. We provide these details to facilitate reproduction of our results and to give researchers clear understanding of the computational resources needed to implement our approach at scale. ResNet-18 ResNet-50 ResNet-50 ResNet-50 ViT-B/16 ViT-B/16 ViT-L/14 4 RTX 4090 4 RTX 4090 8 RTX 4090 8 RTX 4090 8 RTX 4090 6 hours 8 hours 28 hours 40 hours 80 hours ViT-L/14 ViT-H/14 ViT-H/ Hardware Table 9: Performance and inference overhead of DHO with Token Merging (ToMe) on ImageNet under low-shot semi-supervised settings using RTX 4090. Method Labeled Accuracy (%) Params (M) FLOPs (G) Throughput (im/s) DHO DHO + ToMe DHO DHO + ToMe 1% 1% 10% 10% 81.6 81.4 (0.2) 82.8 82.5 (0.3) 17.58 13.12 (25.4%) 243.35 323.39 (+32.9%) 17.58 13.12 (25.4%) 238.11 308.49 (+29.6%) 87.22 87. 87.22 87.22 23 Inference overhead improvements with ToMe. To further improve the computational efficiency of our approach, we explored integrating Token Merging (ToMe) [5] with DHO. ToMe is technique that reduces the number of tokens in ViTs by merging similar tokens to improve the efficiency of ViTs. Table 9 shows that combining DHO with ToMe significantly reduces computational costs with minimal impact on performance."
        },
        {
            "title": "C Datasets",
            "content": "Table 10: Overview of datasets used in our experiments, organized into three categories: (top) standard classification datasets, (middle) ImageNet, and (bottom) ImageNet variants for out-ofdistribution (OOD) evaluation. For few-shot semi-supervised learning experiments, we report both the absolute number of labeled samples and their percentage relative to the full training set. Dataset # Classes # Train # Val # Test # Labeled (1-shot) # Labeled (16-shot) Fine-grained 10 Datasets Caltech101 [26] OxfordPets [69] StanfordCars [48] Flowers102 [67] Food101 [6] FGVCAircraft [59] SUN397 [99] DTD [17] EuroSAT [34] UCF101 [81] Coarse-grained Dataset ImageNet [76] ImageNet OOD Variants ImageNet-V2 [73] ImageNet-Sketch [93] ImageNet-A [37] ImageNet-R [36] 100 37 196 102 101 100 397 47 10 4,128 2,944 6,509 4,093 50,500 3,334 15,880 2,820 13,500 7,639 1,649 736 1,635 1,633 20,200 3,333 3,970 1,128 5,400 1,898 2,465 3,669 8,041 2,463 30,300 3,333 19,850 1,692 8,100 3,783 100 (2.42%) 37 (1.26%) 196 (3.01%) 102 (2.49%) 101 (0.20%) 100 (3.00%) 397 (2.50%) 47 (1.67%) 10 (0.07%) 101 (1.32%) 1,600 (38.76%) 592 (20.11%) 3,136 (48.18%) 1,632 (39.87%) 1,616 (3.20%) 1,600 (48.00%) 6,352 (40.00%) 752 (26.67%) 160 (1.19%) 1,616 (21.15%) 1, 1.28M 1,000 1,000 200 200 - - - - - - - - - 50, 1,000 (0.08%) 16,000 (1.25%) 10,000 50,889 7,500 30,000 - - - - - - - - We evaluated our approach on 11 diverse datasets, with ImageNet [76] serving as our primary benchmark. The datasets span general object recognition [76, 26], fine-grained classification tasks (vehicles [48, 59], natural entities [67, 69, 6]), and specialized domains (scenes [99], textures [17], remote sensing [34], and human actions [81]). Additionally, we conduct experiments on four outof-distribution test sets to further validate the models generalization capabilities. To assess our models robustness to distribution shifts, we evaluate it on several challenging variants of ImageNet: ImageNet-v2 [73], ImageNet-Sketch [93], ImageNet-A [37], and ImageNet-R [36]. The detailed results and analyses are presented in Appendix D. We summarize the overview of datasets used in Table 10, these datasets exhibit diversity in their characteristics, with varying numbers of classes and samples per dataset. This diversity enabled us to thoroughly validate our method across different few-shot semi-supervised learning scenarios by systematically varying the ratios between labeled and unlabeled samples."
        },
        {
            "title": "D Additional Experiments",
            "content": "D.1 Experiments on MobileNet To demonstrate the versatility of our DHO approach beyond ResNet [32] and ViT [20] architectures, we extended our experiments to the MobileNetV2 [77], which is specifically designed for real-world applications with compact models. We maintained identical experimental settings as described in Appendix B.2, using MobileNetV2 as the student model while distilling from CLIP ResNet50. As illustrated in Figure 11, our DHO consistently outperforms all single-head baseline methods, demonstrating its effectiveness on lightweight model architectures along with ResNet18. Furthermore, Figure 12 reveals patterns similar to our ResNet18 experiments regarding few-shot integration. Our 24 Figure 11: Results on 10 datasets under few-shot semi-supervision using MobileNetV2 with zeroshot teacher [72]. Figure 12: Results on 10 datasets using MobileNetV2 with either zeroor few-shot teacher [115]. method successfully incorporates few-shot teacher knowledge, although we observe that the few-shot teacher does not consistently yield improvements over the zero-shot teacher. Notably, our distilled MobileNetV2 model sometimes achieves superior performance to the few-shot teacher (ResNet-50) despite having significantly fewer parameters. D.2 Out-of-Distribution Generalization While our DHO framework is primarily designed for task-specific adaptation, we extensively evaluate its generalization capabilities across distribution-shifted scenarios. We conduct comprehensive experiments on four widely-used distribution-shifted variants of ImageNet: ImageNet-v2 [73], ImageNet-Sketch [93], ImageNet-R [36], and ImageNet-A [37] (detailed dataset characteristics are presented in Table 10). Our analysis consists of two main components. First, we directly evaluate our ImageNet semisupervised models on these distribution-shifted benchmarks to assess how DHO impacts robustness. Second, we investigate the integration of established adaptation techniques while preserving generalization capabilities. Our investigation begins with linear evaluation protocols [72] and Visual Prompt Tuning [43] using frozen image backbones. We then extend our analysis to state-of-the-art CLIP-based few-shot adaptation methods, specifically examining CoOp [120] for unimodal prompt tuning and PromptSRC [45] for multimodal prompt tuning approaches. Additionally, we compare our framework with CasPL [97], which leverages domain-specific unlabeled data with separate distillation stage to enhance adaptation performance. 25 Table 11: Implementation details for our out-of-distribution generalization experiments. Full Training Model Configuration Training Details Student: CLIP ViT-B/16 [72] Student input size: 224224 Zero-shot Teacher: CLIP ViT-L/14 [72] Teacher input size: 336336 Labeled data: 1% and 10% ImageNet ζ, η, and λ: 0.01, 2, 0.5 α and β: 0.5 and 1 Epochs: 32 Optimizer: AdamW (β1=0.9, β2=0.999) Learning rate: 5 105, weight decay: 5 102 Batch size: 512 (labeled: 256, unlabeled: 256) Scheduler: Cosine warmup decay (5000 steps) Augmentation: Random crops (x0.5-1.0), horizontal flips Adaptation Methods (Linear Evaluation & Visual Prompt Tuning) Method Configuration Training Details Linear evaluation [8] Visual prompt tuning [43] Frozen backbone: CLIP ViT-B/16 [72] Input size: 224224 Zero-shot Teacher: CLIP ViT-L/14 [72] Teacher input size: 336336 Labeled data: 1% and 10% ImageNet ζ, η, and λ: 0.01, 2, 0.5 α and β: 0.5 and 1 Epochs: 20 Optimizer: AdamW (β1=0.9, β2=0.999) Learning rate: 5 105, weight decay: 5 102 Batch size: 512 (labeled: 256, unlabeled: 256) Scheduler: Cosine warmup decay (5000 steps) Augmentation: Random crops (x0.5-1.0), horizontal flips Method Configuration Training Details Adaptation Methods (Prompt Tuning) Prompt tuning: CoOp [120], PromptSRC [45] Frozen backbone: CLIP ViT-B/16 [72] Input size: 224224 Zero-shot Teacher: CLIP ViT-L/14 [72] Teacher input size: 336336 Labeled data: 1% and 10% ImageNet ζ, η, and λ: 0.01, 2, 0.5 α and β: 0.5 and 1 Prompt tuning: Following PromptSRC [45] configurations Comparison: CasPL [97] with domain-specific unlabeled data To evaluate our approachs effectiveness under different data availability scenarios, we conducted experiments under both 1% and 10% labeled data settings, maintaining balanced loss weights of 0.5-0.5 and fixed values of α = 0.5 and β = 1.0 for inference. Throughout the training process, we ensured consistent conditions by maintaining 1:1 ratio of labeled to unlabeled samples in the batch composition. For experiments utilizing only labeled samples, we matched the number of training steps to those experiments using both labeled and unlabeled data to ensure fair comparison. Evaluation upon Full Training Model. DHO significantly outperformed zero-shot baselines on similar-distribution ImageNet variants  (Table 12)  , demonstrating particularly strong performance on ImageNet-V2 while maintaining reasonable accuracy on ImageNet-Sketch. This performance pattern remained consistent across both ViT-B/16 and ViT-L/14 architectures, with our approach showing improvements on similar-distribution test sets. However, when evaluated on out-of-distribution datasets such as ImageNet-R and ImageNet-A, the model exhibited performance degradation compared to zero-shot predictions, suggesting that full model training led to increased distribution overfitting and compromised generalization capability across domains. particularly insightful observation emerged from our analysis of different teacher-student configurations. Interestingly, ViT-B/16 models distilled from ViT-L/14 handled shifted distributions better than those taught by the larger ViT-H/14 DFN [22], despite the latters superior performance on shifted distributions such as ImageNet-R and ImageNet-A. We attribute this to the shared training background between ViT-B/16 and ViT-L/14 in the CLIP framework [72], which appears to better 26 Table 12: Accuracy(%) of DHO with full training model on the ImageNet distribution-shifted variants. Student Model Params (M) Labeled Data Teacher Model Val V2 Sketch ViT-B/16 Student ViT-B/16 [72] ViT-B/16 ViT-B/16 ViT-B/16 ViT-B/ ViT-L/14 Student ViT-L/14 [72] ViT-L/14 ViT-L/14 Zero-shot VLM ViT-H [22] 86M 86M 86M 86M 86M 304M 304M 304M zero-shot 1% 10% 1% 10% zero-shot 1% 10% - ViT-L/14 ViT-L/14 ViT-H/14 ViT-H/14 - ViT-H/14 ViT-H/14 66.7 78.7 80.8 81.6 82.8 75.3 84.6 85.9 60.8 70.1 71.3 72.6 73.6 68.3 77.0 77. 46.2 48.0 47.4 50.6 50.7 59.2 61.5 61.7 74.0 70.9 71.7 65.5 67.7 86.5 79.9 82.8 47.0 41.1 41.4 35.6 37.8 74.6 60.8 64. 632M zero-shot - 83.6 77.2 71. 92.3 77.4 Table 13: Accuracy(%) of DHO with adaptation methods on the ImageNet distribution-shifted variants. : backbone frozen during training. VPT-n indicates Visual Prompt Tuning with prompt depth of layers. For fair comparison, we used α = 0.5 and β = 1.0 for all DHO evaluation. Labeled Data Teacher Unlabeled Val Model Data V2 Sketch Method CLIP Zero-shot CLIP [72] Linear Evaluation Linear Evaluation DHO DHO Visual Prompt Tuning VPT-3 [43] VPT-3+DHO VPT-3+DHO VPT-9+DHO VPT-9+DHO - 1% 1% 1% 1.25% 1.25% 1.25% 1.25% 1.25% VLM Text-encoder Prompt Tuning CoOp [120] CoOp+CasPL [97] CoOp+DHO CoOp+DHO 1.25% (16-shots) 1.25% (16-shots) 1.25% (16-shots) 1.25% (16-shots) VLM Multimodal Prompt Tuning PromptSRC [45] PromptSRC+CasPL [97] PromptSRC+DHO PromptSRC+DHO 1.25% (16-shots) 1.25% (16-shots) 1.25% (16-shots) 1.25% (16-shots) - - - - - - - - - - - - - - - 66.7 60.8 46.2 74.0 47. 72.8 73.4 74.6 73.6 73.6 75.1 74.0 75.6 71.5 71.9 72.8 73.4 71.3 72.8 73.0 73.6 64.2 65.3 66.4 64.6 65.3 66.8 65.9 67. 64.2 64.3 65.5 66.2 64.4 65.7 65.3 66.1 47.0 48.3 49.2 47.7 48.7 50.0 48.9 50.8 48.0 48.3 49.3 49.5 49.6 49.7 49.5 49. 74.6 75.8 76.1 75.2 75.9 76.9 75.8 77.3 75.2 76.0 76.4 77.0 77.8 77.9 77.8 78.1 48.6 49.4 49.8 48.7 49.1 50.5 46.5 46. 49.7 - 49.5 50.5 50.9 - 51.3 51.0 preserve generalization capabilities during the adaptation. This points to an important insight: our method works best on out-of-distributions when the teacher and student models share similar training distributions, suggesting that successful knowledge distillation also depends on the alignment between teacher and student than just the teachers raw capabilities. Evaluation upon Adaptation Methods. As demonstrated in Table 13, we first evaluated DHO in scenarios where only the image encoder is utilized: linear evaluation [72] and visual prompt tuning [43] with frozen CLIP ViT-B/16 encoder. Notably, we discovered that linear evaluation with language initialization, as proposed in [51], performs remarkably well, even outperforming prompt tuning-based methods such as CoOp [120] and PromptSRC [45] on in-domain distribution benchmarks. With the additional pathway for teacher knowledge distillation, DHO achieves substantial performance improvements across all in-distribution and out-of-distribution evaluations, even with just 1% of labeled data. Furthermore, leveraging unlabeled data yields additional performance gains. 27 In experiments with visual prompt tuning, similar to our linear probing experiments, introducing new optimization pathway for distillation resulted in performance improvements across all evaluation benchmarks, with further enhancements when utilizing unlabeled data. Moreover, increasing the number of learnable parameters by adding prompt layers led to improved performance across all datasets, with ImageNet-A being the sole exception. We also evaluated DHO on various VLM adaptation methods, including the text-encoder prompt learning approach CoOp [120], the multi-modal prompt learning method PromptSRC [45], and the recently proposed CasPL [97], which employs teacher network for unsupervised distillation on unlabeled data as separate stage. We used the same implementation from the domain generalization setting of the original papers [120, 45], with only linear layer jointly trained on top of image encoder features as the distillation pathway. Our results demonstrate that the simple integration of DHO with existing approaches yields remarkable outcomes as shown in Table 13, achieving average improvements of 0.9% and 0.3% across four generalization benchmarks upon CoOp and PromptSRC, respectively. Notably, when compared to CasPL, which utilizes teacher network with domain unlabeled datasets, our DHO approach demonstrates superior performance across all in-distribution and out-of-distribution benchmarks, achieving new state-of-the-art results when leveraging both teacher and unlabeled domain datasets. This suggests that joint training of labeled supervision and teacher distillation signals is more effective strategy than the sequential approach used in CasPL, aligning with our core hypothesis about the benefits of dual-head optimization for resolving gradient conflicts."
        },
        {
            "title": "E Additional Analysis",
            "content": "E.1 Gradient Analysis Figure 13: (Left): The average dominant eigenvalue λ of and WCEW (Right): The average cosine similarity of gradients w.r.t features CosSim(zLCE, zLKD). KD during training. We analyze the gradient conflict occurs in one of single-head KD baselines, logit distillation in 3.2. In this section, we present the dominant eigen value of and WCEWKD during training in Figure 13- (Left). We further present the cosine similarity between gradients w.r.t feature representation in Figure 13-(Right). The trend of cosine similarity between feature gradients is similar to that of full gradients CosSim(θLCE, θLKD) in Figure 4. E.2 Additional Analysis on Non-Linear Head Design To further investigate the architectural advantages of dual head optimization, we conducted experiments with non-linear head designs, replacing the linear heads used in our main experiments. The non-linear classifier enhanced model representational capacity through sequence of layers: an initial linear projection layer, followed by layer normalization [4], GELU activation [35], dropout [82], and final linear classification layer. We compared DHO with three non-linear configurations; DHO+NLHead-CE: non-linear CE head, DHO+NL-Head-KD: non-linear KD head, and DHO+NL-HeadCE+KD: non-linear both CE and KD heads. All experiments followed the few-shot semi-supervised setting detailed in Appendix B.2. 28 Figure 14: Results of different non-linear head configurations on 11 datasets including ImageNet under few-shot semi-supervision using ResNet-18 with zero-shot teacher [72]. Table 14: Results of different non-linear head configurations on 11 datasets including ImageNet under few-shot semi-supervision using ResNet-18 with zero-shot teacher [72]. We report averaged accuracy for 10 visual recognition datasets except the ImageNet. ImageNet Average of 10 tasks Configuration 1-shot 2-shot 4-shot 8-shot 16-shot 1-shot 2-shot 4-shot 8-shot 16-shot DHO (base) DHO+NL-Head-CE DHO+NL-Head-KD DHO+NL-Head-CE+KD 61.7 61.7 62.1 62. 62.2 61.9 62.6 62.3 62.6 62.2 62.9 62.6 63.8 63.1 64.0 63.8 65.1 64.8 65.9 65.4 58.9 59.3 58.3 59.1 62.2 62.1 62.1 62. 68.4 68.0 67.8 68.6 73.1 72.7 72.4 72.4 76.5 76.2 76.5 76.5 Table 15: Results on dual-heads interpolation strategy of different non-linear head configurations on ImageNet under 16-shots semi-supervised setting. Performance Analysis. Our experiments on ImageNet revealed several key findings regarding head architecture (Table 14, Figure 14). Nonlinear architectures in the KD head consistently improved performance compared to the linear approach, suggesting that complex architectures better capture teacher predictions. However, increasing complexity in the CE head resulted in performance degradation, most likely due to this heads focus on limited labeled data, which impaired its capacity to learn generalizable shared feature representations. While dual non-linear heads outperformed their fully linear counterparts, this configuration was less effective than implementing non-linearity solely in the KD head. DHO (base) DHO+NL-Head-CE DHO+NL-Head-KD DHO+NL-Head-CE+KD CE Head KD Head Combined 65.37 64.91 65.97 65.59 61.55 61.39 61.76 61.81 60.64 60.18 60.95 61.66 Configuration When extending our analysis to the other 10 datasets (Table 14, Figure 14), we observed different pattern. Unlike on ImageNet, the optimal architectural configuration varied considerably across these diverse datasets, with no single configuration emerging as consistently superior. This contrast 29 highlights that the effectiveness of non-linear transformations exhibits strong dataset-specific dependencies and varies according to head functionality. Given these findings, we adopted the linear head architecture for all subsequent experiments, as it achieved comparable performance while providing substantial benefits in computational efficiency and architectural simplicity. Head Decomposition Analysis. Our analysis under the 16-shot semi-supervised setting revealed intricate relationships between architectural choices and head-wise performance. When employing non-linear CE branch, we observed decline in CE head performance from 60.64% to 60.18%, despite increased parameters. Conversely, implementing non-linear architecture for the KD head improved both heads performance: CE head accuracy increased from 60.64% to 60.95%, and KD head prediction improved from 61.55% to 61.76%. However, when applying non-linear architectures to both heads simultaneously, despite enhanced individual performance, the combined performance decreased from 65.97% to 65.59%, suggesting that head specialization may compromise joint feature representation quality. These findings underscore the complex interplay between architectural decisions and multi-head learning dynamics in semi-supervised frameworks. E.3 Additional Dual-Head Analysis Figure 15: Analysis of DHO on the ImageNet under 16-shot semi-supervised setting. (Left) Maximum probability distributions for predictions from CE head, KD head, and their combined output. (Middle) Prediction agreement diagram analysis, categorizing cases where both heads are correct, only one head is correct, and both heads are incorrect. (Right) Error reduction analysis comparing single-head failure cases against improvements achieved through combined predictions. In this section, we further analyze the prediction behavior of DHO. As shown in Figure 15 (left), despite sharing feature representations, the CE head (hCE), trained on labeled data, produces sharper predictions, whereas the KD head (hKD), guided by teacher distillation, generates smoother distributions. Prediction agreement analysis (Figure 15, middle) shows that the two heads agree in 76.2% of cases while complementing each other: the CE head correctly classifies 11.5% of cases where the KD head fails, and vice versa for 12.4%. Error reduction analysis (Figure 15, right) further demonstrates that our combined approach reduces failure rates from 11.5% to 2.5% for the KD head and from 12.4% to 5.2% for the CE head, confirming the effectiveness of DHO. We also present additional qualitative results of DHO, both on ImageNet (see Figures 16 and 17) and on other 10 datasets (see Figures 18 and 19). 30 Figure 16: Additional qualitative results on ImageNet under 16-shot semi-supervised setting. 31 Figure 17: Additional qualitative results on ImageNet under 16-shot semi-supervised setting. Figure 18: Additional qualitative results 1 on other 10 datasets of model trained under 16-shot semi-supervised setting 33 Figure 19: Additional qualitative results 2 on other 10 datasets of model trained under 16-shot semi-supervised setting"
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: Yes. Our main contributions are also detailed in 1. Our gradient conflict analysis is presented in 3.2 and Appendix E.1. We also provide post-hoc analysis on dual-head combination in Appendix with empirical analysis in Appendix E.3. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Yes, we discussed limitations in 5. We also reported computational cost in training in Appendix B.2, and analyzed the computational efficiency of our method in 4.2  (Table 2)  and Appendix B.3. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs 35 Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [Yes] Justification: Yes, we provided the full set of assumptions and complete (and correct) proof for gradient conflict analysis and post-hoc dual-head balancing in 3.2 and A, respectively. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We used publicly available datasets and models for our experiments. Our proposed method is also simple and easy to implement, with described details in 4. We will also release our code and instructions for reproducing the results. Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in 36 some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We will provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, once the blind review is over. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so \"No\" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We specified all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results in 4.1 and Appendix B.2. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We couldnt report error bars due to computational resource constraints, as described in Table 8 in Appendix B.2. Our experiments cover variety of few-shot settings (1, 2, 4, 8, 16 shots) across multiple datasets (11 datasets including ImageNet), making multiple runs prohibitively expensive. However, our work demonstrates consistent performance gaps compared to baselines across all experimental settings, providing strong evidence for the effectiveness of our approach. 37 Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provided type of hardware, and time of execution needed to reproduce the experiments in Table 8 inside Appendix B.2 for experiments taking the most time. Also we provided extensive information of computational efficiency of our method in 4.2 and Appendix B.3. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes, we followed the NeurIPS Code of Ethics https://neurips.cc/ public/EthicsGuidelines. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts 38 Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work uses only standard academic datasets like ImageNet for improving semi-supervised learning methods. We dont use any private or personal data, and our techniques dont present obvious negative societal impacts. Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our work focuses on semi-supervised learning methods using standard academic datasets and does not pose high risks for misuse. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We properly credit all creators of assets used in our work. 39 Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our work does not release any new assets such as datasets, code, or models. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work does not involve crowdsourcing or research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] 40 Justification: Our work does not involve crowdsourcing or research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [NA] Justification: Our work does not involve any LLMs as important, original, or non-standard components. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        }
    ],
    "affiliations": [
        "DeepAuto.ai",
        "KAIST",
        "VUNO Inc."
    ]
}