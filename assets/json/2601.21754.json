{
    "paper_title": "Language-based Trial and Error Falls Behind in the Era of Experience",
    "authors": [
        "Haoyu Wang",
        "Guozheng Ma",
        "Shugang Cui",
        "Yilun Kong",
        "Haotian Luo",
        "Li Shen",
        "Mengya Gao",
        "Yichao Wu",
        "Xiaogang Wang",
        "Dacheng Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight \"scouts\" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption."
        },
        {
            "title": "Start",
            "content": "Language-based Trial and Error Falls Behind in the Era of Experience Haoyu Wang 1 Guozheng Ma 1 Shugang Cui 2 Yilun Kong 1 Haotian Luo Li Shen 3 Mengya Gao 2 Yichao Wu 2 Xiaogang Wang 2 Dacheng Tao"
        },
        {
            "title": "Abstract",
            "content": "While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work (Chen et al., 2025) attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-anderror, which is computationally unsustainable for parameter-heavy LLMs operating in high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), novel framework that decouples exploration from exploitation. We employ lightweight \"scouts\" (e.g., small MLPs) to probe environmental dynamics at speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5Pro (0.60), while saving about 60% GPU hours consumption. The code is available at https: //github.com/Harry-mic/SCOUT. 6 2 0 2 9 2 ] . [ 1 4 5 7 1 2 . 1 0 6 2 : r 1. Introduction Large Language Models (LLMs) have demonstrated remarkable capabilities across wide range of tasks (Guo et al., 2025; Wang et al., 2025d; Zhang et al., 2025; Wang et al., 2025b; Yao et al., preprint; Shridhar et al., 2020), primarily driven by extensive pretraining on high quality text cor1Nanyang Technological University 2Sense Time 3Sun Correspondence to: Dacheng Tao Yat-sen University. <dacheng.tao@ntu.edu.sg>. Figure 1. Exploration and Distillation Stage will either directly award the language models the skills, such as the performance on Sokoban-Box2 (below) that leads to direct convergence (0.0 to 0.45), or indirectly teach relevant knowledge that will be later activated via Evolving Stage, such as the performance on Sudoku (above figure, from 0.0, to 0.29, then to 0.97). pora (Brown et al., 2020; Touvron et al., 2023; Yang et al., 2025; Zhou et al., 2023). This equips LLMs with broad world knowledge, enabling strong zero-shot generalization in language correlated scenarios such as creative writing, summarization, reasoning, and even language based agentic tasks (Li et al., 2023; Zheng et al., 2023; Shao et al., 2024; Shridhar et al., 2020; Yao et al., preprint). However, when deployed in unseen, non-linguistic tasks such as spatial tasks (Ghugare et al., 2025; Gu et al., 2023), symbolic 1 Language-based Trial and Error Falls Behind in the Era of Experience Figure 2. Overview of the SCOUT framework. The pipeline consists of three stages: (1) Exploration Stage: Lightweight scouts efficiently capture environmental dynamics to generate expert trajectories; (2) Distillation Stage: These trajectories are textualized to \"warm-up\" the LLM via supervised fine-tuning; (3) Evolving Stage: The LLM further refines its reasoning and decision making capabilities through multi-turn PPO. tasks (Brockman et al., 2016) and complex long horizon tasks (Luo et al., 2025; Wang et al., 2025c), existing pretraining is far from sufficient. These tasks demonstrate that the real world is unbounded, involving \"endless complexity\" (Sutton, 2019). Therefore it is hard to fully simplify and cover all tasks during pretraining. LLMs rich pretrained knowledge struggles in these scenarios because now they need to internalize the environmental dynamics from scratch rather than directly utilizing the pretrained world knowledge. SPA (Chen et al., 2025) attributes this performance gap to the fact that LLMs are significantly less familiar with symbolic state-based tasks (Brockman et al., 2016) compared to the language based state tasks like Webshop (Yao et al., preprint),ALFWorld (Shridhar et al., 2020). SPA separates the in-distribution and out-of-distribution tasks by the state perplexity against random guess. In this work, we focus on these out-of-distribution tasks that are often composed of various symbols or numbers, rather than natural language, and are more alien to language agents. The newly introduced tasks in this work are also OOD tasks, and we show the higher state perplexity against random guess as evidence of OOD tasks as SPA (Chen et al., 2025) does in Table 5. We mainly focus on these symbolic and spatial tasks, and call them \"unseen tasks\" in this work. Beyond the pretraining stage, the inefficiency of LLM agents in mastering these new tasks also stems from two fundamental mismatches. First, there is mismatch between the action space and the generation space. Generating token requires forward pass through billions of parameters of LLMs, resulting in low efficiency for both exploration and exploitation. Furthermore, an LLM is exploring vast vocabulary space (typically exceeding 30,000 tokens), whereas many reasoning and symbolic tasks (Brockman et al., 2016) require only discrete, low dimensional set of actions. Although settings like temperature and top-k can reduce the candidate tokens, forcing an LLM to search for optimal policies within such high-dimensional semantic space is computationally wasteful and hinders efficient exploration. Second, depending solely on language priors limits scalability. The \"Bitter Lesson\" (Sutton, 2019) teaches us that leveraging computation (searching and learning) is far more effective than relying on predefined knowledge in the long run. Although LLMs are semantically rich, they struggle to grasp the specific dynamics of the physical world (Ghugare et al., 2025; Wang et al., 2024; Yamada et al., 2023) that cannot be fully encoded in text. To bridge this gap, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), novel agent framework that harmonizes the exploitation on world knowledge of LLM agents with the exploration efficiency of \"scouts\". As shown in Figure 2, our key insight is to decouple the heavy exploration phase of LLM agents from the exploitation phase. Specifically, we employ lightweight neural networks (e.g., small MLPs or CNNs) to serve as \"scouts\". Their low parameter count and high inference speed enable them to evolve rapidly with classic Reinforcement Learning (RL) algorithms (e.g., DQN, PPO) to master the environmental dynamics and generate high quality expert trajectories. These trajectories then serve as warm-up for the LLM. This process effectively distills the specific task dynamics captured by the scouts into the LLMs, activating the LLMs internal relevant world knowledge with the specific unseen 2 Language-based Trial and Error Falls Behind in the Era of Experience task. We further conduct multi-turn RL on the LLMs to align them with new tasks. This allows the LLMs to skip the heavy and inefficient exploration phase and focus on the exploitation of newly learned knowledge. We test our methods on several symbolic and dynamic worlds such as FrozenLake, Sokoban and Sudoku. We further investigate the long horizon and spatial ability, and respectively introduce: 1) 2048, grid game which needs above 800 turns to reach the 2048 tile; 2) Rubiks Cube, game of restoring scrambled Rubiks Cube. Empirical results demonstrate SCOUT significantly outperforms baselines. SCOUT enables Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming the tested baselines and proprietary models, within which Gemini-2.5-Pro achieves the highest 0.60 score. To summarize, we propose novel framework, SCOUT. By leveraging small neural networks as scouts for rapid exploration and trajectory generation, we alleviate the exploration efficiency bottleneck inherent in pure LLM agents and activate the learned world knowledge with multi-turn RL. We demonstrate that SCOUT effectively activates the LLMs potential for unseen OOD tasks, allowing it to model the new environment efficiently and effectively. 2. Sub-Scale Collaboration On Unseen Tasks In this section, we introduce the agentic framework SCOUT (Sub-Scale Collaboration On Unseen Tasks) from four aspects. First, we explain the Preliminaries. Then we introduce the Exploration Stage where the small neural network scouts self-evolve in the agentic task environments. We further explain the Distillation Stage that teaches the LLMs unseen task dynamics with expert trajectories. Finally, we introduce the Evolving Stage, which activates the learned knowledge in LLMs with multi-turn RL on the unseen tasks. 2.1. Preliminaries We formulate the unseen tasks (e.g., symbolic tasks) as Markov Decision Process (MDP) for the LLMs, and as different MDP for the small neural network scouts. LLMs MDP For LLMs, the environment is defined as tuple MLLM = S, I, A, P, R. Here, st represents symbolic states (e.g., grid matrices) at timestep t, and it represents the language augmentations (e.g., task descriptions, transition rules). The LLM observes the full context (it, st). is the action space, P(st+1st, at) denotes the transition dynamics, and R(st, at) provides the scalar reward signal. τLLM = {i0, s0, athink , r0, ..., iT , sT } denotes the LLM interaction history. , araw 0 0 bolic state st. Unlike the LLM which could be hinted by language to infer environmental rules (e.g., \"slippery ice\"), the scout implicitly learns the underlying transition dynamics directly through extensive trial-and-error. Consequently, the symbolic state serves as sufficient statistic for the physical environment, allowing the scouts to master the dynamics without linguistic descriptors. The interaction history of the scouts is τscout = {s0, a0, r0, ..., sT }. State-Text Mapping To bridge the modality gap between Mscout and MLLM, we define trajectory transformation function that automatically converts scout experiences into multi-turn dialogue formats. Instead of requiring complex manual rule design, this function leverages the inherent interfaces of the environment to deterministically translate the symbolic trajectories τscout into their corresponding language dialogue τLLM by Textualizer Φ without manual engineering, where thought content is set to blank. More details about the transformation are provided in Appendix and Table 7. The detailed notations are listed in Table 4. 2.2. Exploration Stage In this Exploration Stage, our primary objective is to bypass the inefficient exploration capability of Large Language Models by delegating the task of learning environmental dynamics to lightweight proxy agent, denoted as the scout. As formulated in the preliminaries, the scout operates within the reduced environment Mscout, observing only the symbolic state st without language augmentations. We parameterize the scout agent with learnable parameters ψ using lightweight neural network (e.g., an MLP or small CNN), which is significantly smaller than the LLM πθ. Given that the action spaces of the targeted tasks are discrete, we employ standard Reinforcement Learning algorithms, specifically DQN (Mnih et al., 2015) and PPO (Schulman et al., 2017) to train the scout. Our general goal is to maximize the expected cumulative reward: J(ψ) = Eτ π (cid:35) γtrt (cid:34) (cid:88) t=0 (1) To achieve this, we adopt distinct optimization objectives depending on the algorithm employed. For DQN, where the policy is implicitly derived from value estimates, we approximate the optimal action-value function Qψ by minimizing the temporal difference (TD) error against target network: LDQN(ψ) = E(st,at,rt,st+1)B (cid:104)(cid:16) rt + γ max t+1 Qψ(st+1, t+1) Qψ(st, at) (2) (cid:17)2(cid:105) Scouts MDP In contrast, for the scouts, we model the task as an intrinsic symbolic MDP defined by Mscout = S, A, P, R. The scouts observation consists of the symwhere denotes the replay buffer, Qψ is the Q-network parameterized by ψ, and ψ represents the parameters of the frozen target network. 3 Language-based Trial and Error Falls Behind in the Era of Experience Conversely, when utilizing PPO, ψ directly parameterizes the stochastic policy πψ. We maximize clipped surrogate objective to ensure monotonic improvement without dangerously large policy updates: LPPO(ψ) = Et [min (ρt(ψ)At, clip(ρt(ψ), 1 ϵ, 1 + ϵ)At)] (3) πψold (atst) is the probability ratio, At is the where ρt(ψ) = πψ(atst) estimated advantage, and ϵ controls the clipping range. Due to the low dimensionality of the scouts parameter space and the absence of complex token generation overhead, the scout can interact with the environment at frequency orders of magnitude higher than that of πθ. This high throughput interaction allows the scout to rapidly balance the exploration and exploitation trade-off, effectively mapping the transition dynamics and identifying high reward regions in the state space. After convergence, we utilize the better scout policy π ψ between DQN and PPO to generate dataset of expert trajectories Dscout = {τ1, τ2, . . . , τN } on each task. 2.3. Distillation Stage The second stage focuses on bridging the modality gap between the symbolic mastery of the scout and the linguistic reasoning of the LLM. The raw trajectories in Dscout lack the language context required by the LLMs input space. Therefore, we introduce trajectory transformation function defined in Preliminaries. Formally, we define this trajectory transformation function that converts the numerical scout trajectories into multi-turn dialogue formats. For each trajectory τscout = (s0, a0, r0, s1, a1, . . . , sT ) collected by the scout, we apply the Textualizer Φ to each item to reconstruct the linguistic context. The transformed trajectory τLLM is constructed as sequence of dialogue turns: τLLM = (τscout) = {Φ(s0) , Φ(a0) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) User Asst = {i0, s0, athink , Φ(r0) (cid:124) (cid:123)(cid:122) (cid:125) User , araw 0 0 , Φ(s1) (cid:124) (cid:123)(cid:122) (cid:125) User } , . . . , Φ(sT ) (cid:124) (cid:123)(cid:122) (cid:125) User (4) , r0, ..., iT , sT } This results in an augmented dataset DLLM = {T (τ ) τ Dscout}, where symbolic state dynamics are explicitly grounded in language descriptions. Notably, we leave the athink blank: <think> </think>, as the trajectories do not include thinking content. We further analyze the emerging thinking content within the Evolving Stage in Section 4.4. We then employ Supervised Fine-Tuning (SFT) to warm-up the LLM policy πθ with DLLM. The optimization objective is to minimize the negative log-likelihood of the actions given the language-augmented context: 4 L(θ) = Eτ DLLM (cid:104) 1 (cid:88) t=0 log πθ (cid:0)athink , araw it, st, athink <t , araw <t (cid:1)(cid:105) (5) This distillation process serves critical purpose: it internalizes the \"physics\" of the unseen task into the LLM. Unlike standard pretraining where the model learns general world knowledge, this stage forces the LLM to align its internal representations with the specific, often counter intuitive dynamics of the unseen environment (e.g., the slipping mechanism in FrozenLake or the spatial permutations in Rubiks Cube). By cloning the behavior of the scout, πθ effectively skips the prohibitively expensive initial exploration phase, starting its own learning curve from distinct point of competence rather than random initialization. 2.4. Evolving Stage In the final stage, we unleash the full potential of the LLM by transitioning from imitation to self-evolution. While the Distillation Stage equips the LLM with basic environmental dynamics and rule adherence, the policy πθ is initially constrained by the upper bound of the scouts limited capacity and the supervised nature of the loss. To transcend these limitations, we conduct multi-turn Reinforcement Learning directly on the warm-up πθ within the fully interactive environment MLLM. Trajectory-Level Optimization Standard RLHF methods (Rafailov et al., 2023; Ouyang et al., 2022) typically treat alignment as single-turn optimization. The objective is to maximize the expected reward of single full response given prompt x, constrained by KL-divergence term to prevent deviation from the reference policy πref: Jresp(θ) = xD yπθ(x) (cid:21) (cid:20) R(x, y) βDKL(πθ(x) πref(x)) (6) However, this formulation overlooks the temporal dependencies in agentic tasks, where current actions at (comprising both the thought process athink ) determine future states st+1 and the ultimate success of the episode. To address this, we employ trajectory-level optimization via multi-turn PPO (Wang et al., 2025d), aiming to maximize the expected cumulative return over the entire interaction history τ : and execution araw Jtraj(θ) = Eτ πθ (cid:20) (cid:88) (cid:18) γtrt t=0 βDKL(πθ(ht) πref(ht)) (7) (cid:19)(cid:21) Language-based Trial and Error Falls Behind in the Era of Experience where ht = (it, st, τ<t) denotes the full context defined in the preliminaries. Unlike in the Distillation Stage where the reasoning process is set to blank, in this stage we encourage the model to generate meaningful think blocks that serve as planning steps to maximize long term rewards. Activation and Refinement of Capabilities This stage acts as catalyst, interacting with the distilled knowledge in two distinct ways: refinement and activation. As shown in Figure 1, in environments like FrozenLake and Rubiks Cube, the Distillation Stage alone grants the LLM substantial proficiency, indicating that the scout has successfully transferred the core task dynamics. Here, the multi-turn RL serves to refine this solid foundation, closing the gap to this almost perfect performance. Conversely, in tasks like Sudoku, the distilled policy initially appears limited, achieving success rate of only 0.29. However, this seemingly low score masks critical achievement: the LLM has internalized the valid rules but lacks strategic foresight. The subsequent RL rapidly \"activates\" this latent capability, boosting the score to 0.97. This dual effect demonstrates the versatility of SCOUT: whether the scout provides near complete solution or latent structural understanding, the subsequent RL effectively evolves this foundation into strong policy. 3. Experiment Setup Models For the trained models, we mainly use Qwen-2.5 series models (Yang et al., 2025) as our backbone, which aligns with our baselines (Wang et al., 2025d; Chen et al., 2025). We compare our method with existing baselines: RAGEN (Wang et al., 2025d), State Estimation RL (Chen et al., 2025), SPA (Chen et al., 2025), and some proprietary models like GPT-4o-mini (Hurst et al., 2024), DeepSeekV3 (Liu et al., 2024), GPT-OSS-120B (Agarwal et al., 2025), GPT-5-nano (OpenAI, 2025), Gemini-2.5-Pro (Comanici et al., 2025) with reasoning activated system prompt as described in Appendix F. Tasks We introduce 6 tasks with different settings and difficulty. As illustrated in Introduction, we mainly focus on unseen tasks: tasks with higher state perplexity than random guess. We validate this in Appendix, Table 5. We follow RAGEN (Wang et al., 2025d) to include Bandit, FrozenLake, Sokoban, Sudoku, and extend the environments to long-horizon symbolic task: 2048 and symbolic spatial task: Rubiks Cube. Notably, for FrozenLake, Sokoban, Rubiks Cube, they have different difficulties. The ground of the FrozenLake could transfer from static to slippery, meaning that the action made by the agents could transfer from deterministic action to random action that \"slips\" on the ground. Increasing the box number of Sokoban could increase the difficulty of the Sokoban. Rubiks Cube is game that recovery 2x2 Cube with 6 surface. The rotation number means the times that rotate the Cube from its intact state. Increasing rotation numbers make it harder to recovery the cube. The agents also need spatial imagination to accurately image the next spatial state. More details about each envs setting and difficulty are shown in Appendix D.1. Training Settings For the multi-turn PPO in Evolving Stage, we conduct our experiments on RAGENs codebase (Wang et al., 2025d), and follow their default setting to train the models for 200 steps. For the Supervised Fine-tuning(SFT) process in Distillation Stage, we employ LLaMA-Factorys codebase (Zheng et al., 2024). For the training of the scouts, we include cleanrl (Huang et al., 2022) as reference. Evaluation We use the default codebase in RAGEN to eval the trained LLMs and the Proprietary Models by api to ensure fair comparison. The RAGEN codebase provides both the evaluation for the local models and api models. More details are provided in Appendix D. 4. Experimental Results and Findings 4.1. Main Experimental Results We conduct detailed experiments on the 6 unseen tasks with different levels of difficulty: Whether the FrozenLake is slippery or not, the number of the boxes in Sokoban, the rotation times of the Rubiks Cube. The details of each environment are introduced in Appendix D.1. Table 1 shows the main experiments results. SCOUT significantly surpasses the baselines across various model sizes and types. With SCOUT, Qwen2.5-3B-It even beats several proprietary models like DeepSeek-V3, GPT-4o-mini, Gemini-2.5-Pro with an average score of 0.86. Increasing the model size from 0.5B to 3B consistently improves the performance from 0.81 to 0.86. On different model type: LLaMA3.1, SCOUT also performs well that achieves 0.83 score. Compared to the scouts, relying solely on the Distillation Stage is far from enough. Although the language agents learn the format and knowledge from the scout trajectories that achieve score of above 0.6, even better than Multi-turn PPO, they still lag behind the performance of the scouts. Therefore, further RL on the SFT checkpoint is crucial that yields performance gain of nearly 0.2. We could also observe the different task dynamics from this table. In tasks like Rubiks Cube, the LLMs could already perform well with only SFT that achieves nearly 90% win rate. However, on tasks like Sudoku, the learned dynamics from the expert trajectories need further RL to be activated. 4.2. Surpass the Subagent in the Unseen World Scout Comparison In Table 1, we compare two different scouts abilities. We initialize small neural networks (MLPs for Bandit, 2048, FrozenLake, Rubiks Cube, Sudoku; CNNs for Sokoban) and compare the performance of Deep Q-Network (DQN) with that of Proximal Policy OptiLanguage-based Trial and Error Falls Behind in the Era of Experience Table 1. Main results on 6 unseen tasks. The 2048 score is normalized as (Max-N)/2048 to align with the [0,1] scale of other tasks. The scores represent the success rate (pass@1). Model/Task Bandit 2048 FrozenLake Sokoban Rubiks Cube Sudoku Average Max-N Return Static Slippery Box Box2 Rotation1 Rotation2 Rotation3 Small Neural Networks as Scouts Scout-DQN Scout-PPO 0.930.02 1024 4319.40251.38 0.900.05 0.800.05 0.980.01 0.470.03 1.000.00 0.960.01 0.910.01 0.800.04 0.83 3677.6479.70 0.900.02 0.850.01 0.990.01 0.500.02 1.000.00 0.940.01 0.810.02 0.850.02 0.79 0.790.01 512 Qwen2.5-0.5B-It - Multi-turn PPO - State Estimation RL - SPA - Exploration & Distillation Stage + Evolving Stage Qwen2.5-1.5B-It - Multi-turn PPO - State Estimation RL - SPA - Exploration & Distillation Stage + Evolving Stage Qwen2.5-3B-It - Multi-turn PPO - State Estimation RL - SPA - Exploration & Distillation Stage + Evolving Stage LLaMA3.1-1B-It - Multi-turn PPO - Exploration & Distillation Stage + Evolving Stage GPT-4o-mini DeepSeek-V3 GPT-OSS-120B GPT-5-nano Gemini-2.5-Pro 0.39 0.62 0.54 0. 0.60 0.74 0.63 0.72 0.71 0.23 0. 0.95 0.77 0.87 0.63 0.23 0. 0.93 0.43 0.63 0.80 0.81 0. 0.81 0.66 0.71 0.69 128 256 128 1024 1024 256 256 512 1024 1024 256 256 256 512 1024 256 256 128 47.11 1091. 1193.56 309.60 5203.39 5452.16 248.84 1649. 2503.17 2332.41 4679.11 5585.95 556.77 1571. 1490.47 1493.60 5479.43 5577.14 741.54 2173. 39.82 1024 3697.29 256 256 256 256 1507 3864 3320 2436 SCOUT vs Baselines 0.17 0.39 0.27 0. 0.89 0.93 0.16 0.66 0.30 0. 0.57 0.95 0.24 0.87 0.31 0. 0.91 0.96 0.16 0.37 0.88 0. 0.14 0.24 0.24 0.47 0.46 0. 0.20 0.38 0.28 0.71 0.85 0. 0.33 0.74 0.25 0.41 0.86 0. 0.19 0.38 0.84 0.88 Proprietary Models 0. 0.93 0.95 0.84 1.00 0.84 0. 0.88 0.66 0.88 0.04 0.15 0. 0.37 0.58 0.98 0.11 0.25 0. 0.60 0.89 0.97 0.13 0.37 0. 0.50 0.93 0.97 0.08 0.26 0. 0.95 0.34 0.45 0.95 0.96 0. 0.00 0.06 0.06 0.07 0.52 0. 0.00 0.06 0.08 0.09 0.08 0. 0.02 0.06 0.08 0.07 0.45 0. 0.01 0.07 0.32 0.51 0.10 0. 0.71 0.56 0.59 0.14 0.45 0. 0.31 0.94 1.00 0.11 0.67 0. 0.34 0.99 1.00 0.14 0.34 0. 0.36 0.84 1.00 0.01 0.38 1. 1.00 0.02 0.06 0.19 0.00 0. 0.08 0.22 0.23 0.21 0.95 0. 0.05 0.26 0.23 0.22 0.98 0. 0.04 0.25 0.26 0.22 0.94 0. 0.00 0.02 0.96 1.00 0.00 0. 0.19 0.00 0.28 0.03 0.11 0. 0.12 0.80 0.84 0.04 0.11 0. 0.13 0.81 0.84 0.04 0.14 0. 0.11 0.89 0.90 0.00 0.09 0. 0.85 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.05 0.18 0.63 0. 0.00 0.18 0.39 0.60 0.45 0. 0.00 0.06 0.24 0.70 0.29 0. 0.00 0.03 0.44 0.92 0.71 0. 1.00 1.00 0.97 0.11 0.24 0. 0.26 0.69 +0.58 0.81 +0.70 0.14 0.34 0.33 0.40 0.71 +0.57 0.85 +0.71 0.18 0.38 0. 0.37 0.73 +0.55 0.86 +0.68 0.10 0.24 0.63 +0.53 0.83 +0.73 0.38 0.44 0.57 0. 0.60 mization (PPO). The detailed small neural network scouts architectures are shown in Appendix G.1. As evidenced by the results, Scout-DQN generally outperforms Scout-PPO across some of the evaluated metrics. The Scout-DQN achieves superior or equal best performance in 4 out of the 10 detailed tasks, and often by significant margin (e.g., achieving value of 1024 in the second column compared to PPOs 512), equal performance in 2 tasks, and fall behind in the other 4 tasks. While Scout-PPO shows competitive results in certain tasks (e.g., scoring 0.85 and 0.85 in the FrozenLake Slippery and Sudoku, respectively, against DQNs 0.80 and 0.80), it does not match the general performance of DQN. This empirical advantage of DQN is likely due to the discrete nature of the action spaces in the tested environments, where off-policy value-based methods often demonstrate higher sample efficiency than on-policy methods like PPO (Schulman et al., 2017; Mnih et al., 2015). Exploration Efficiency It is notable that, after conducting multi-turn PPO on the SCOUT Distillation checkpoint, the learned but not yet demonstrated capability is effectively activated. With SCOUT, the Qwen2.5-3B-It and Qwen2.51.5B-It even surpass the average performance of Scout-DQN and Scout-PPO. This result validates our core hypothesis: the bottleneck of language agents in unseen tasks lies less in the reasoning capacity, but more in the efficiency of initial exploration. By leveraging the Scout to handle the heavy burden of trial-and-error, the LLM effectively bypasses the computationally expensive phase of learning environmental dynamics from scratch. Instead, it directly focuses on exploitation and high level reasoning, seamlessly integrating the distilled \"physics\" of the task with its intrinsic semantic capabilities. Consequently, the agent not only masters the symbolic mechanics faster but also transcends the limits of its teacher (the Scout), demonstrating that the \"Sub-Scale Collaboration\" can unlock latent potential while preserv6 Language-based Trial and Error Falls Behind in the Era of Experience ing the versatility of the language model. This stark contrast is quantified by the difference in parameter size and memory footprint. As shown in Table 2, Scouts utilize approximately 1.0 105 billion parameters which are nearly 105 times smaller than the LLMs, allowing them to operate faster. This lightweight nature effectively decouples the exploration phase from expensive GPU resources, transforming the high-cost, low-efficiency trial-and-error process of LLMs into computationally inexpensive task. Thus, SCOUT achieves superior exploration coverage with minimal energy consumption. Table 2. Resource Efficiency Comparison. Scouts operate primarily on CPUs, demonstrating significant reductions in hardware dependency and resource consumption compared to LLMs. Metric LLMs Small Scouts Efficiency Gain Training Device Parameter Size Memory Footprint > 40 GB (VRAM) < 1 GB (RAM) Commodity CPU GPU Independent 1.0 105 105 Smaller 102 Lower High-end GPU 0.5B 3B GPU Cost Analysis To further quantify the computational efficiency, we perform detailed cost analysis on the challenging Rubiks Cube Rotation3 task using Qwen2.5-3BInstruct, as shown in Table 3. The Direct PPO baseline incurs substantial computational overhead, consuming 24.0 GPU hours (on an 8H100 node) to complete 200 training steps. This inefficiency arises because the heavy LLM is forced to perform the entire trial-and-error exploration process on expensive GPU hardware. In contrast, SCOUT strategically optimizes resource allocation. By delegating the initial exploration to the lightweight Scout, we incur nearly zero GPU cost during the most uncertain phase of learning. The GPU resources are subsequently utilized only for efficient knowledge transfer (SFT) and activation (PPO). Consequently, SCOUT achieves the same training milestone with total cost of only 9.6 GPU hours, representing dramatic 60% reduction in computational expense. The significant speedup (24.0h vs 9.6h) stems from the context efficiency. Direct PPO involves long exploration trajectories that fill the thought content athink. In contrast, SCOUTs Evolving Stage starts from high quality, concise expert paths (with blank thoughts athink), which involves far fewer token counts and accelerating the optimization process. This result confirms that SCOUT provides an economically viable path for scaling RL to complex, long horizon tasks. Table 3. Training Cost Comparison on Rubiks Cube Rotation3. We report wall-clock time and GPU hours for 200 training steps. SCOUT significantly reduces total time and cost."
        },
        {
            "title": "Device",
            "content": "Time GPU-h Time GPU-h"
        },
        {
            "title": "Direct PPO RL Training",
            "content": "8H"
        },
        {
            "title": "SCOUT",
            "content": "Exploration CPU Only 8H100 Distillation 8H100 Evolving 3.00 0.17 0.20 1.00 24.0 3.00 24. 0.0 1.6 8.0 1.37 9.6 (-60%) 7 4.3. Enabling Multi-task Language Agents via"
        },
        {
            "title": "Sequential RL with SCOUT",
            "content": "Previous sections have shown the great potential of SCOUT in those single tasks. However, it remains question whether the SCOUT paradigm could extend to multi-task setting. The critical question is whether the language agents will collapse and fall into catastrophic forgetting when conducting multi-task training, or maintain the trained task ability while extending their skills to new tasks? In this section, we conduct multi-task sequential RL on the language agents. We design sequential curriculum order: Bandit FrozenLake Sokoban Rubiks Cube Sudoku. We evaluate two distinct settings as shown in Table 6: 1) Direct Sequential RL, where we directly apply PPO on the initial model sequentially; and 2) Sequential RL with SCOUT, where we first apply multi-task SFT using trajectories collected by scouts from all environments, followed by the same sequential PPO. We average the scores under different settings for the same task. The Role of Scout Initialization Comparing the first two phases in Figure 3, we observe that the necessity of the Exploration and Distillation Stage is evident. Without the warm-up by scout trajectories, the Direct Sequential RL (left figure) struggles to explore effectively. The average score only marginally improves from 0.19 to 0.37 after five tasks of PPO. In the Bandit and Sokoban tasks, it even experienced performance fluctuations and declines. In contrast, the SCOUT paradigm (right figure) starts with strong foundation (Multi-task SFT via Exploration and Distillation Stage) and further evolves to multi-task expert with an average score of 0.91. This confirms that the lightweight Scouts effectively compress the dynamics of multiple unseen tasks into the LLM, providing robust initialization for subsequent evolution. Plasticity and Stability Tradeoff major concern in multitask learning is catastrophic forgetting, where learning new task degrades performance on previously learned tasks. As observed in the right side of Figure 3, our approach demonstrates remarkable stability. For instance, after the agent finishes the final training stage on Sudoku (Stage 5), it not only masters the new Sudoku task (from 0.38 after SFT to 0.98) but also retains high proficiency in the learned tasks. Specifically, the Bandit score remains stable at 1.0, and the FrozenLake scores (0.89) stay comparable to their initial post-SFT levels (0.89). Moreover, we observe positive transfer in complex tasks; for example, training on Sokoban and Rubiks Cube appears to aid the reasoning required for Sudoku, which improves significantly in performance. This suggests that the SCOUT framework allows the LLM to internalize generalized \"world model\" rather than overfitting to isolated tasks, effectively mitigating catastrophic forgetting while continuously expanding its capability boundaries. Language-based Trial and Error Falls Behind in the Era of Experience Figure 3. Comparison of task performance during sequential RL. While the Sequential RL (Left) exhibits some performance degradation on previously learned tasks, SCOUT (Right) successfully preserves historical task knowledge (e.g., Bandit, FrozenLake) while adapting to new environments (e.g., Sudoku), achieving near optimal multi-task agent. 4.4. From Implicit Modeling to Explicit Modeling"
        },
        {
            "title": "Examples of Implicit to Explicit Modeling",
            "content": "At the Distillation Stage in Section 2.3, we leave the thinking content blank in DLLM. However, we explicitly require the language models to first output their thinking content, then follow their final answer in the multi-turn RL process in Evolving Stage. We find the RL finetuned language models fill in the blank between these thinking tags. The language models sometimes will directly output their intended action within the thought block before repeating it in the answer section. This is very obvious in tasks whose action space is short and simple, like FrozenLake, Rubiks Cube. However, for tasks that need more language to answer, like Sudoku, the language models tend to output an analysis in the thinking part, then make their final decisions. In the following table, the RL trained language model successfully discovers the missing number in the three rows and three columns, and correctly outputs the answer. 5. Conclusion In this paper, we identify the exploration inefficiency and dimension mismatch as key barriers for LLM agents in mastering unseen, non-linguistic tasks. To address these challenges, we propose SCOUT, novel framework that harmonizes the rapid exploration of lightweight \"scout\" networks with the reasoning capabilities of LLMs. By decoupling exploration from exploitation, SCOUT efficiently distills environmental dynamics into the LLM, followed by further evolution via multi-turn RL. Empirical results across symbolic and spatial tasks, including long horizon challenges like 2048 and Rubiks Cube, demonstrate that SCOUT significantly outperforms existing baselines. Our work validates that Sub-Scale Collaboration\" is promising path to bridge the gap between linguistic priors and the physical dynamics of the real world. 8 <im_start>system You are helpful assistant. <im_end> <im_start>user You are solving Sudoku puzzle. Fill in the grid so that every row, column, and 2x2 box contains the numbers 1-4 without repetition. Initial cells are shown in [brackets] and cannot be modified. Empty cells are shown as dots (.). Place numbers one at time using the format, for example: <answer>place 1 at row 2 col 3</answer> or <answer>1,2,3</answer> The environment will provide feedback on valid/invalid moves and show conflicts. Turn 1: =============== SUDOKU PUZZLE =============== [1] . [3][2] [2][3] [1] . [4] . . . [3][2] [4] . Legend: [N]=initial cell, .=empty Steps: 0/20 You have 20 actions left. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer> with no extra text. Strictly follow this format. Max response length: 150 words (tokens). <im_end> <im_start>assistant <think>Left col 3 in row 3 should contain 2</think><answer>place 2 at row 3 col 3</answer><im_end> Language-based Trial and Error Falls Behind in the Era of Experience"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents SCOUT, framework that significantly improves the efficiency of LLM agents in unseen environments. By offloading the computationally expensive exploration phase to lightweight neural networks, our approach addresses the \"computational wastefulness\" of using large models for trial-and-error tasks. This has positive implications for Green AI, as it reduces the energy footprint required to train competent agents. Furthermore, by demonstrating that smaller models (e.g., 3B parameters) can outperform larger proprietary ones through effective collaboration, our work promotes the democratization of capable AI agents, allowing researchers with limited computational resources to achieve state-of-the-art results. We do not foresee immediate negative societal consequences, though the advancement of autonomous agents warrants standard ethical monitoring regarding their deployment in real-world automated systems."
        },
        {
            "title": "References",
            "content": "Agarwal, S., Ahmad, L., Ai, J., Altman, S., Applebaum, A., Arbus, E., Arora, R. K., Bai, Y., Baker, B., Bao, H., et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Gu, J., Xiang, F., Li, X., Ling, Z., Liu, X., Mu, T., Tang, Y., Tao, S., Wei, X., Yao, Y., Yuan, X., Xie, P., Huang, Z., Chen, R., and Su, H. Maniskill2: unified benchmark for generalizable manipulation skills. In International Conference on Learning Representations, 2023. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International conference on machine learning, pp. 18611870. Pmlr, 2018. Huang, C., Zheng, T., Huang, L., Li, J., Liu, H., and Huang, J. Relayllm: Efficient reasoning via collaborative decoding. arXiv preprint arXiv:2601.05167, 2026. Huang, S., Dossa, R. F. J., Ye, C., Braga, J., Chakraborty, D., Mehta, K., and Araújo, J. G. Cleanrl: High-quality singlefile implementations of deep reinforcement learning algorithms. Journal of Machine Learning Research, 23(274): 118, 2022. URL http://jmlr.org/papers/ v23/21-1342.html. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. arXiv preprint arXiv:1606.01540, 2016. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Jin, B., Zeng, H., Yue, Z., Yoon, J., Arik, S., Wang, D., Zamani, H., and Han, J. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Chen, S., Zhu, T., Wang, Z., Zhang, J., Wang, K., Gao, S., Xiao, T., Teh, Y. W., He, J., and Li, M. Internalizing world models via self-play finetuning for agentic rl. arXiv preprint arXiv:2510.15047, 2025. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Ghugare, R., Ji, C., Wantlin, K., Schofield, J., and Eysenbach, B. Builderbencha benchmark for generalist agents. arXiv preprint arXiv:2510.06288, 2025. Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval: An automatic evaluator of instruction-following https://github.com/tatsu-lab/ models. alpaca_eval, 5 2023. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Liu, B., Guertler, L., Yu, S., Liu, Z., Qi, P., Balcells, D., Liu, M., Tan, C., Shi, W., Lin, M., et al. Spiral: Selfplay on zero-sum games incentivizes reasoning via multiagent multi-turn reinforcement learning. arXiv preprint arXiv:2506.24119, 2025a. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Liu, W., Luo, H., Lin, X., Liu, H., Shen, T., Wang, J., Mao, R., and Cambria, E. Prompt-r1: Collaborative automatic prompting framework via end-to-end reinforcement learning. arXiv preprint arXiv:2511.01016, 2025b. 9 Language-based Trial and Error Falls Behind in the Era of Experience Luo, H., Zhang, H., Zhang, X., Wang, H., Qin, Z., Lu, W., Ma, G., He, H., Xie, Y., Zhou, Q., et al. Ultrahorizon: Benchmarking agent capabilities in ultra long-horizon scenarios. arXiv preprint arXiv:2509.21766, 2025. Wang, H., Ren, L., Zhao, T., and Jiao, L. Collm: Industrial large-small model collaboration with fuzzy decisionmaking agent and self-reflection. IEEE Transactions on Fuzzy Systems, 2025a. Mialon, G., Fourrier, C., Wolf, T., LeCun, Y., and Scialom, T. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. nature, 518(7540): 529533, 2015. OpenAI. Gpt-5 system card. Technical report, OpenAI, 2025. URL https://cdn.openai.com/ gpt-5-system-card.pdf. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36: 5372853741, 2023. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Shridhar, M., Yuan, X., Côté, M.-A., Bisk, Y., Trischler, A., and Hausknecht, M. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020. Sutton, R. S. The bitter //www.incompleteideas.net/IncIdeas/ BitterLesson.html, 2019. (blog), March 13, 2019. lesson."
        },
        {
            "title": "Incomplete Ideas",
            "content": "http: Wang, J., Ming, Y., Shi, Z., Vineet, V., Wang, X., Li, S., and Joshi, N. Is picture worth thousand words? delving into spatial reasoning for vision language models. Advances in Neural Information Processing Systems, 37: 7539275421, 2024. Wang, K., Zhang, P., Wang, Z., Gao, Y., Li, L., Wang, Q., Chen, H., Wan, C., Lu, Y., Yang, Z., et al. Vagen: Reinforcing world model reasoning for multi-turn vlm agents. arXiv preprint arXiv:2510.16907, 2025b. Wang, W., Han, D., Diaz, D. M., Xu, J., Rühle, V., and Rajmohan, S. Odysseybench: Evaluating llm agents on long-horizon complex office application workflows. arXiv preprint arXiv:2508.09124, 2025c. Wang, Z., Wang, K., Wang, Q., Zhang, P., Li, L., Yang, Z., Jin, X., Yu, K., Nguyen, M. N., Liu, L., et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025d. Xue, Z., Zheng, L., Liu, Q., Li, Y., Zheng, X., Ma, Z., and An, B. Simpletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning. arXiv preprint arXiv:2509.02479, 2025. Yamada, Y., Bao, Y., Lampinen, A. K., Kasai, J., and Yildirim, I. Evaluating spatial understanding of large language models. arXiv preprint arXiv:2310.14540, 2023. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2022. Yao, S., Shinn, N., Razavi, P., and Narasimhan, K. taubench: benchmark for tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024. Yao, S., Chen, H., Yang, J., and Narasimhan, K. Webshop: Towards scalable real-world web interaction with grounded language agents. In ArXiv, preprint. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Yu, Z., Zhang, J., Su, H., Zhao, Y., Wu, Y., Deng, M., Xiang, J., Lin, Y., Tang, L., Li, Y., et al. Recode: Unify plan and action for universal granularity control. arXiv preprint arXiv:2510.23564, 2025. 10 Language-based Trial and Error Falls Behind in the Era of Experience Zeng, W., Zhang, X., Shi, Y., Hu, C., Chen, Y., Shen, B., and Gu, X. Glimprouter: Efficient collaborative inference by glimpsing one token of thoughts. arXiv preprint arXiv:2601.05110, 2026. Zhang, J., Ma, G., Liu, S., Wang, H., Huang, J., Lin, T.- E., Huang, F., Li, Y., and Tao, D. Merf: Motivationenhanced reinforcement finetuning for large reasoning models. arXiv preprint arXiv:2506.18485, 2025. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36: 4659546623, 2023. Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z., and Ma, Y. Llamafactory: Unified efficient finetuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403. 13372. Zhou, J., Lu, T., Mishra, S., Brahma, S., Basu, S., Luan, Y., Zhou, D., and Hou, L. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. Zhou, Y., Jiang, S., Tian, Y., Weston, J., Levine, S., Sukhbaatar, S., and Li, X. Sweet-rl: Training multiturn llm agents on collaborative reasoning tasks. arXiv preprint arXiv:2503.15478, 2025a. Zhou, Z., Qu, A., Wu, Z., Kim, S., Prakash, A., Rus, D., Zhao, J., Low, B. K. H., and Liang, P. P. Mem1: Learning to synergize memory and reasoning for efficient longhorizon agents. arXiv preprint arXiv:2506.15841, 2025b. 11 Language-based Trial and Error Falls Behind in the Era of Experience Language-based Trial and Error Falls Behind in the Era of Experience"
        },
        {
            "title": "Appendix for",
            "content": "Part I: Background"
        },
        {
            "title": "C Related Works",
            "content": "Part II: Experiments Setup"
        },
        {
            "title": "D Experiments",
            "content": "D.1 Models, Datasets, Tasks . D.2 Experiment Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E Extra Experimental Results",
            "content": "Part III: Prompts and Architectures"
        },
        {
            "title": "F Used Prompts",
            "content": "F.1 System Prompts . . . . . . F.2 State Estimation Prompts ."
        },
        {
            "title": "G Other Details",
            "content": "G.1 Scout Architecture G.2 Textualizer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 13 13 13 14 15 15 17 17 18 20 22 Language-based Trial and Error Falls Behind in the Era of Experience A. Notation In this section, we list the detailed notation that we adopt in the main text. Table 4. Summary of important notations used in the SCOUT framework."
        },
        {
            "title": "Language Models",
            "content": "st State athink Think Tokens araw Raw Action rt Reward Language Augmentation it τ Trajectory Language Agent Policy πθ"
        },
        {
            "title": "State\nRaw Action\nReward\nTrajectory\nScout Policy\nToken Index\nTrajectory Return\nAdvantage Estimate\nDiscount Factor",
            "content": "st at rt τ πψ i, R(τ ) γ Ground-truth environment state at turn t. The think tokens at turn t. The raw action tokens at t, not augmented with think tokens. Scalar reward at turn t, rt = R(st, at) where is the reward function. The augmentated text at turn t. Rollout τ = (i0, s0, athink , r0, , iT , sT ). Policy parameterized by LLM with parameters θ. , araw"
        },
        {
            "title": "Scouts",
            "content": "Ground-truth environment state at turn t. The raw action tokens at t, not augmented with think tokens. Scalar reward at turn t, rt = R(st, at) where is the reward function. Rollout τ = (s0, o0, a0, r0, , sT ), not augmentated by I. Policy parameterized by neural network with parameters ψ. τi: the i-th token. τi:j: tokens ij. τ<i: prefix up to 1. τt,i: the i-th token of the t-th turn. Sum of rewards over trajectory, (cid:80) Ai: advantage for token i; Aturn γ [0, 1]; γtoken: within-turn discount; γturn: across-turn discount. : advantage for token in turn t. : advantage for turn t; Atoken R(st, at). t,i B. Limitations In this work, we validate the efficiency of SCOUT framework on different model sizes, from 0.5B to 3B and on different model types, from Qwen to LLaMA. Due to the resource limitation, we do not validate larger size models or other type models, which may perform better than existing models. We mainly conduct our experiments with the mostly used and stable multi-turn PPO in this work, while other RL algorithms like GRPO (Guo et al., 2025) may also be effective. In some tasks, we observe the performance degradation after several RL training steps, which aligns with the findings in RL community (Wang et al., 2025d; Xue et al., 2025). Further improvements on stabilizing multi-turn RL training is essential. C. Related Works LLM Agents and Environment Interaction. Recent studies (Yao et al., 2022; Liu et al., 2025a; Zhou et al., 2025a; Luo et al., 2025) have explored the capacity of Large Language Models (LLMs) to master complex environments through multiturn interaction. These benchmarks range from text-based scenarios like ALFWorld (Shridhar et al., 2020), WebShop (Yao et al., preprint), TauBench (Yao et al., 2024), and GAIA (Mialon et al., 2023) to symbolic reasoning tasks such as FrozenLake (Brockman et al., 2016). Existing research primarily aims to enhance performance by optimizing reinforcement learning (RL) algorithms (Wang et al., 2025d; Xue et al., 2025), incorporating memory-based architectures (Zhou et al., 2025b; Jin et al., 2025), filtering instruction-tuning datasets (Xue et al., 2025), or converting textual inputs into visual representations (Wang et al., 2025b). In contrast, our work focuses on decoupling the computationally expensive exploration phase from the reasoning phase by introducing lightweight \"scout\" networks. Deep RL and Exploration Efficiency. Deep Reinforcement Learning (DRL) has achieved significant success in mastering environmental dynamics (Mnih et al., 2015; Schulman et al., 2017; Haarnoja et al., 2018), ranging from Atari games to robotic control (Gu et al., 2023; Brockman et al., 2016). key advantage of classical DRL is its ability to optimize policies within compact state spaces, enabling high throughput interaction that captures the underlying dynamics of the environment. We leverage this efficiency by employing lightweight networks (e.g., MLPs or CNNs) as \"scouts.\" With significantly fewer parameters than LLMs, these scouts can rapidly balance the exploration-exploitation trade-off to generate expert trajectories, effectively addressing the \"cold start\" problem for the subsequent language agent. Large-Small Model Collaboration. Collaborative frameworks involving large and small models have also gained much 13 Language-based Trial and Error Falls Behind in the Era of Experience attention (Zeng et al., 2026; Wang et al., 2025a; Liu et al., 2025b; Huang et al., 2026; Yu et al., 2025). Typically, larger model acts as planner while smaller language model handles execution or tool use. Unlike these approaches, SCOUT employs non-linguistic neural networks to address the exploration bottleneck. Crucially, these scouts operate without pretrained linguistic knowledge, learning environmental dynamics from scratch. This design separates physical rule acquisition from semantic reasoning, allowing the LLM to learn from grounded experience via distillation rather than relying on the small model for runtime inference. D. Experiments D.1. Models, Datasets, Tasks Models We follow previous agentic methods (Wang et al., 2025b;d; Chen et al., 2025), we utilize models of varying sizes and types. We adopt the instruct version of Qwen2.5B series models (Yang et al., 2025) as the backbone of finetuning. The sizes vary from 0.5B to 1.5B, 3B. We complement LLaMA3.1-1B-It (Grattafiori et al., 2024) to validate the SCOUT on different model types. We adopt several strong properity models as our baselines. For proprietary solutions, we employ GPT-4o-mini (Hurst et al., 2024) as representative of cost-effective agents, DeepSeek-V3 (Liu et al., 2024) for its robust reasoning, Gemini2.5-Pro (Comanici et al., 2025) for its superior general ability, and the newest OpenAI model GPT-5-nano (OpenAI, 2025). We also evaluate high-performing open-source models, specifically the powerful GPT-OSS-120B (Agarwal et al., 2025). We adopt MLPs for Bandit, 2048, FrozenLake, Rubiks Cube, Sudoku and CNNs for Sokoban. These small neural networks are only about 1.0 105 that interact very fast with the environments. Datasets In this work, the included datasets are Dscout and DLLM. For Dscout, we collect 4k trajectories each task. We utilize the final checkpoint of the trained scout to collect these trajectories. The collected τscout = (s0, a0, r0, s1, a1, . . . , sT ), where the state is represented by one-shot vector. For DLLM, we utilize the predefined trajectory transformation function to convert the dataset Dscout into the multi-turn , r0, ..., iT , sT }, where we leave the dialogue dataset DLLM. The trajectories in DLLM is τLLM = {i0, s0, athink athink blank. , araw 0 0 Tasks We follow SPA (Chen et al., 2025) and focus on out of distribution tasks that often compose of various symbols or numbers, rather than natural language, and are more unseen to language agents. We mainly focus on the symbolic and spatial tasks whose state perplexity are larger than random guess as shown in table 5, and we call these tasks as \"unseen tasks\" in this work. We introduce two new tasks: 2048 and Rubiks Cube. Bandit: fundamental reinforcement learning benchmark serving as sanity check. The agent must interact with two arms, each associated with specific reward probability distribution, to identify and select the optimal arm for maximizing cumulative returns. FrozenLake: grid world navigation task where the agent must reach goal while avoiding holes. We evaluate two difficulty settings: Static and Slippery. In the Static setting, transitions are deterministic. In the Slippery setting, the ground is simulated as frictionless ice, meaning the agent may move in direction perpendicular to the chosen action with certain probability (i.e., slipping). This tests the agents robustness against stochastic environmental dynamics. Sokoban: classic planning puzzle requiring the agent to push boxes to designated target locations without getting stuck. We control the difficulty by varying the number of boxes (e.g., Box1, Box2). Increasing the box number exponentially expands the state space and increases the likelihood of irreversible deadlocks, demanding complex multistep reasoning and path planning. 14 Language-based Trial and Error Falls Behind in the Era of Experience Sudoku: logic-based combinatorial number-placement puzzle. The agent must fill 4 4 grid such that every row, column, and subgrid contains unique digits. This task serves as testbed for pure symbolic constraint satisfaction and deductive reasoning. 2048: long-horizon symbolic task where the agent slides and merges numbered tiles on grid to reach the target number 2048. Unlike other short horizon tasks, successful game typically requires more than 800 turns. This environment challenges the agents ability to plan strategically for long term sustainability and maintain grid tidiness over long horizon. Rubiks Cube: spatial intelligence and symbolic task that requires restoring scrambled 2 2 cube to its original state. We define the difficulty based on the \"rotation number\" (or scramble depth), which represents the number of random rotations applied to an intact cube to generate the initial state (e.g., Rotation1, Rotation2, Rotation3). Higher rotation numbers increase the complexity of restoration, requiring the agent to possess strong spatial imagination to mentally simulate 3D state transitions. Table 5. Quantifying Distribution Shift: Average Perplexity (PPL) of state representations evaluated with Qwen2.5-Instruct-1.5B. Comparison between our symbolic tasks and standard language-based agent tasks. The significantly higher PPL than random guess in symbolic tasks (e.g., Sokoban, Frozen Lake) indicates that these environments are essentially Out-of-Distribution (OOD) and unseen to the LLM, contradicting the concern of data contamination. However, for language-base tasks like WebShop and ALFWorld, the PPLs are smaller than the random guess, indicating that they are much more in-distribution tasks."
        },
        {
            "title": "Task Environment",
            "content": "Symbolic / Unseen Tasks Sokoban Frozen Lake Rubiks Cube 2048 Sudoku Language / In-Distribution Tasks (Reference)"
        },
        {
            "title": "WebShop\nALFWorld",
            "content": "D.2. Experiment Settings PPL (Perplexity)"
        },
        {
            "title": "Random Guess",
            "content": "163.90 187.10 24.38 15.85 15.50 11.70 6.00 7 6 6"
        },
        {
            "title": "Vocabulary Size\nVocabulary Size",
            "content": "In this work, we utilize SFT and multi-turn PPO to train the models. This lead to several hype-parameters. We conduct SFT with LLaMA-Factory (Zheng et al., 2024). The training configuration includes cutoff length of 4096, batch size of 64, 3 training epochs, cosine learning rate scheduler, and warm-up ratio of 0.1. For full finetuning, we set learning rate to 1e 5. We conduct the training on an 8 H100 device. We conduct multi-turn PPO with RAGEN (Wang et al., 2025d). The training configuration includes an 0.28 clip ratio, an 0.25 rollout filter ration. We train all the checkpoint for 200 steps, keeping in line with RAGEN (Wang et al., 2025d). We set the max model len to 16384 to avoid the unexpected dialog cutoff. We request the agent to give one action per turn, and set the max turn to 25, except 2048, where we set the max turn to 1k. Also, we include in-context sliding window in 2048 with 5 dialogue segments. This greatly reduces the pressure of the context, making it possible for the model to complete such long horizon task. We conduct the multi-turn RL training on an 8 H100 devices. E. Extra Experimental Results In this section, we give the detailed scout training curves on the 6 unseen tasks and their different settings as Figure 4 and Figure 5 show. We also include the detailed results on the sequential RL in Table 6. This sequential RL results correspond to the Figure 3 in the main context. Language-based Trial and Error Falls Behind in the Era of Experience Figure 4. Scout-DQN detailed performance on 6 unseen tasks. Figure 5. Scout-PPO detailed performance on 6 unseen tasks. Table 6. Multi-task Agent via Sequential RL Model/Method"
        },
        {
            "title": "Sokoban",
            "content": "Rubiks Cube"
        },
        {
            "title": "Sudoku Average",
            "content": "Static Slippery Box1 Box2 Rotation1 Rotation2 Rotation"
        },
        {
            "title": "Sequential RL with SCOUT",
            "content": "Qwen2.5-3B-It +Exploration & Distillation Stage +PPO on Bandit +PPO on FrozenLake +PPO on Sokoban +PPO on Rubiks Cube +PPO on Sudoku Qwen2.5-3B-It +PPO on Bandit +PPO on FrozenLake +PPO on Sokoban +PPO on Rubiks Cube +PPO on Sudoku 0.77 1.0 1.0 1.0 1. 1.0 1.0 0.77 0.86 0.84 0. 0.70 0.80 0.24 0.91 0.91 0. 0.89 0.89 0.89 0.24 0.26 0. 0.51 0.52 0.59 0.02 0.15 0. 0.15 0.59 0.59 0.59 0.02 0. 0.06 0.10 0.09 0.10 0.33 0. 0.86 0.90 0.88 0.88 0.88 0. 0.46 0.46 0.50 0.93 0.95 0."
        },
        {
            "title": "Sequential RL",
            "content": "0.33 0.25 0.30 0.39 0.50 0. 0.13 0.14 0.17 0.40 0.37 0. 16 0.14 0.04 1.0 1.0 1. 1.0 1.0 1.0 0.14 0.11 0. 0.17 0.33 0.33 1.0 1.0 1. 1.0 1.0 1.0 0.04 0.04 0. 0.10 0.18 0.22 0.04 0.88 0. 0.88 0.86 0.88 0.89 0.04 0. 0.08 0.07 0.11 0.11 0.00 0. 0.40 0.43 0.48 0.52 0.98 0. 0.00 0.00 0.00 0.02 0.34 0.19 0.74 +0.55 0.74 +0.00 0.75 +0.01 0.85 +0.10 0.86 +0.01 0.91 +0. 0.19 0.19 +0.00 0.22 +0.03 0.28 +0.06 0.31 +0.03 0.37 +0.06 Language-based Trial and Error Falls Behind in the Era of Experience F. Used Prompts In this section, we introduce the detailed System Prompts, State Estimation Prompts that we used in this paper. We follow SPA (Chen et al., 2025) on their state estimation prompts, and introduce new ones for Bandit, Rubiks Cube and 2048. F.1. System Prompts System Prompt: Bandit You are playing bandit game. Goal: Maximize your total reward by choosing which arm to pull. Game Rules: 1. There are 2 arms, named {namea} and {nameb} 2. Each arm has its own reward distribution, related to their names. 3. Analyze the symbolic meaning of each arms name to guess how their reward distribution might behave. 4. Based on the symbolic meaning of their names, which arm do you think is more likely to give higher rewards on average? Choose between {namea} and {nameb}, and output like <answer> {namea} </answer> or <answer> {nameb} </answer>. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer>with no extra text. Strictly follow this format. System Prompt: 2048 You are playing the 2048 game on 4x4 grid. Merge equal tiles by sliding Up, Right, Down, or Left. If move is invalid (no tiles move), small penalty is applied. Respond with single action. Example: <answer>Up</answer> Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer>with no extra text. Strictly follow this format. System Prompt: FrozenLake-Static You are solving the FrozenLake puzzle. The observation includes both symbol grid and zero-indexed coordinates for the start, goal, player, and any holes. Coordinates range from the top-left corner (0, 0) to the bottom-right corner (5, 5). Respond with sequence of actions such as <answer>Left Up Up</answer>. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer>with no extra text. Strictly follow this format. System Prompt: FrozenLake-Slippery You are solving the FrozenLake puzzle. The observation includes both symbol grid and zero-indexed coordinates for the start, goal, player, and any holes. Coordinates range from the top-left corner (0, 0) to the bottom-right corner (5, 5). Beware that the ice is slippery, so the agent might slide and end up in an unintended tile. Respond with sequence of actions such as <answer>Left Up Up</answer>. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer>with no extra text. Strictly follow this format. System Prompt: Sokoban You are solving the Sokoban puzzle. You are the player and you need to push all boxes to targets. You are provided with symbol grid and the zero-indexed coordinates of the player, each box, and each target. Coordinates range from the top-left corner (0, 0) to the bottom-right corner (5, 5). When you are exactly next to box, you can push it by moving in the same direction. You cannot push box through wall, and you cannot pull box. The answer should be sequence of actions, like <answer>Right Right Up</answer>. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer>with no extra text. Strictly follow this format. 17 Language-based Trial and Error Falls Behind in the Era of Experience System Prompt: Rubiks Cube You are solving 2x2 Rubiks Cube (Pocket Cube). The goal is to restore the cube so that each of the faces consists of single, unique color. Available actions use standard Singmaster notation for face rotations: U, U, D, D, L, L, R, R, F, F, B, B. - Faces: (Up), (Down), (Left), (Right), (Front), (Back). - Modifiers: letter alone means 90 clockwise (e.g., R). letter with prime () means 90 counter-clockwise (e.g., \"R\"). Respond with sequence of actions separated by \"\". Example: <answer>U</answer> Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer>with no extra text. Strictly follow this format. System Prompt: Sudoku You are solving Sudoku puzzle. Fill in the grid so that every row, column, and 2x2 box contains the numbers 1-4 without repetition. Initial cells are shown in [brackets] and cannot be modified. Empty cells are shown as dots (.). Place numbers one at time using the format, for example: <answer>place 1 at row 2 col 3</answer> or <answer>1,2,3</answer> The environment will provide feedback on valid/invalid moves and show conflicts if any occur. Always output: <think> [Your thoughts] </think> <answer> [your answer] </answer>with no extra text. Strictly follow this format. F.2. State Estimation Prompts State Estimation Prompt: Bandit ou are playing bandit game. Goal: Maximize your total reward by choosing which arm to pull. Game Rules: 1. There are 2 arms, named ${name_a}$ and ${name_b}$. 2. Each arm has its own reward distribution, related to their names. 3. You need to analyze the symbolic meaning of each arm's name to guess their reward potential. Example answer format: <think> <observation> Arm is named ${name_b}$. Symbolically, this implies analysis of name B. </observation> Based on the analysis, ${name_a}$ seems to represent risk/low value, while ${ name_b}$ implies wealth/stability. <prediction> If pull ${name_b}$, expect higher average reward because [reasoning]. </prediction> </think> <answer> ${name_b}$ </answer> sample full output is as follows: <think> <observation> Arm is named \"Rotten Apple\". Symbolically, this implies decay and zero value. Arm is named \"Golden Chalice\". Symbolically, this implies treasure and high value. </observation> Comparing the two, the Golden Chalice is clearly superior in potential value. <prediction> Pulling \"Golden Chalice\" will likely yield high positive reward, whereas \"Rotten Apple\" might give zero or negative reward. </prediction> </think> Language-based Trial and Error Falls Behind in the Era of Experience <answer> Golden Chalice </answer> State Estimation Prompt: 2048 ou are playing the 2048 game on 4x4 grid. Merge equal tiles by sliding Up, Right, Down, or Left. Use zero-indexed grid where (0,0) is top-left and (3,3) is bottom-right. If move is invalid (no tiles move), small penalty is applied. Example answer format: <think> <observation> [Current Grid Symbol Representation] Tile values and positions: 2 at (0,0), 2 at (0,1), 4 at (3,3). </observation> want to merge the two 2s in the top row. Sliding Right will merge them at (0,3). <prediction> [Predicted Grid Symbol Representation after move] Predicted changes: The 2 at (0,0) and 2 at (0,1) merge into 4 at (0,3). (3,3) remains 4. </prediction> </think> <answer>Right</answer> sample full output is as follows: <think> <observation> 2 2 . . . . . . . . . . . . . 4 Non-empty tiles: 2 at (0,0), 2 at (0,1), 4 at (3,3). </observation> Checking moves: Up: No merge. Right: (0,0) and (0,1) are equal (2), they will merge to the right. <prediction> . . . 4 . . . . . . . . . . . 4 Merged tile 4 created at (0,3). Tile at (3,3) stays. </prediction> </think> <answer>Right</answer> State Estimation Prompt: Rubiks Cube ou are solving 2x2 Rubik's Cube (Pocket Cube). The goal is to restore the cube so that each of the faces consists of single, unique color. Available actions: U, U', D, D', L, L', R, R', F, F', B, B'. State representation format: string of colors representing faces U, D, L, R, F, B. Example answer format: <think> <observation> Current State: [Color String or Grid] Face (Up): W 19 Language-based Trial and Error Falls Behind in the Era of Experience Face (Front): G Face (Right): O ... </observation> need to fix the Front face top row. Rotating (Up) clockwise will shift the top row of F, R, B, to the left. <prediction> Predicted State: [Predicted Color String or Grid] Face top row becomes the colors from Face R. Face top row becomes the colors from Face B. ... </prediction> </think> <answer>U</answer> sample full output is as follows: <think> <observation>"
        },
        {
            "title": "B B\nB B",
            "content": "Face U: All White. Face F: Green. Face R: Red. Face B: Blue. Face L: Orange. Face D: Yellow. Cube is currently solved. </observation> To disrupt the state for demonstration, will apply (Right face clockwise). This will move columns: right col -> right col -> left col (inverted) -> right col -> right col. <prediction>"
        },
        {
            "title": "B B\nB B",
            "content": "Face rotates clockwise (colors cycle). Right column of becomes Yellow (from D). </prediction> </think> <answer>R</answer> G. Other Details In this section, we show the the Scout Architecture and the Textualizer (Φ) we use in this paper. We give the Textualizer of Sudoku as an example. G.1. Scout Architecture 1 class DQN_QNetwork(nn.Module): 2 def __init__(self, obs_dim: int, act_dim: int, hidden: int): Listing 1. Scout-DQN Architecture: MLP 3 4 5 6 8 9 10 super().__init__() self.net = nn.Sequential( layer_init(nn.Linear(obs_dim, hidden)), nn.ReLU(), layer_init(nn.Linear(hidden, hidden)), nn.ReLU(), layer_init(nn.Linear(hidden, act_dim), std=0.01), ) 20 Language-based Trial and Error Falls Behind in the Era of Experience Listing 2. Scout-DQN Architecture: CNN 1 class DQN_QConv(nn.Module): 2 def __init__(self, obs_shape: Tuple[int, int, int], act_dim: int, dueling: bool = True): 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 super().__init__() c, h, = obs_shape self._act_dim = act_dim self.features = nn.Sequential( layer_init(nn.Conv2d(c, 32, 3, 1, 1)), nn.ReLU(), layer_init(nn.Conv2d(32, 64, 3, 1, 1)), nn.ReLU(), layer_init(nn.Conv2d(64, 64, 3, 1, 1)), nn.ReLU(), nn.Flatten(), ) fc_in = 64 * * self.head = nn.Sequential( layer_init(nn.Linear(fc_in, 512)), nn.ReLU(), layer_init(nn.Linear(512, act_dim), std=0.01), ) Listing 3. Scout-PPO Architecture: MLP 1 class PPOAgent(nn.Module): def __init__(self, envs): super().__init__() obs_shape = int(np.array(envs.single_observation_space.shape).prod()) hidden = 64 self.critic = nn.Sequential( layer_init(nn.Linear(obs_shape, hidden)), nn.Tanh(), layer_init(nn.Linear(hidden, hidden)), nn.Tanh(), layer_init(nn.Linear(hidden, 1), std=1.0), ) self.actor = nn.Sequential( layer_init(nn.Linear(obs_shape, hidden)), nn.Tanh(), layer_init(nn.Linear(hidden, hidden)), nn.Tanh(), layer_init(nn.Linear(hidden, envs.single_action_space.n), std=0.01), ) Listing 4. Scout-PPO Architecture: CNN 1 2 class CNN_Agent(nn.Module): 3 def __init__(self, envs): super().__init__() c, h, = envs.single_observation_space.shape hidden = 1024 self.net = nn.Sequential( layer_init(nn.Conv2d(c, 64, kernel_size=3, stride=1, padding=1)), nn.ReLU(), layer_init(nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)), nn.ReLU(), layer_init(nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)), nn.ReLU(), nn.Flatten(), layer_init(nn.Linear(128 * * w, hidden)), 21 3 5 6 7 8 9 11 12 13 14 15 17 18 19 4 5 7 8 9 10 11 13 14 15 Language-based Trial and Error Falls Behind in the Era of Experience 16 18 19 nn.ReLU(), ) self.critic = layer_init(nn.Linear(hidden, 1), std=1.0) self.actor = layer_init(nn.Linear(hidden, envs.single_action_space.n), std =0.01) G.2. Textualizer In this section, we provide concrete example of the Textualizer that transforms scout trajectories Dscout into languagebased trajectories DLLM . Since the original task environments are standard Gym-style environments, the corresponding language-based environments are constructed by expressing environment states, feedback, and actions as natural language descriptions. To ensure that the scouts interact with exactly the same underlying environments, we directly train the scouts in the original Gym-style tasks, without any language augmentation. Moreover, as the RAGEN codebase (Wang et al., 2025d) already provides canonical language descriptions for all tasks, the transformation from Dscout to DLLM is implemented by deterministically substituting the symbolic states and actions in these predefined templates with those observed in Dscout. This process performs direct serialization of existing information and does not introduce additional task structure, transition rules, or planning heuristics. Table 7. Mapping from Scout Trajectories to Language Trajectories via Textualizer (Φ). This table demonstrates the full mapping process. We take Sokoban as an example. The left column represents the trajectories the collected Dscout. The right column represents the structured language trajectories DLLM transferred from the Dscout by the Textualizer. Example: Sokoban"
        },
        {
            "title": "Language Trajectories",
            "content": "State ###### ###__# ###X_# #_#OP# #____# ###### State SYSTEM INSTRUCTION: You are solving the Sokoban puzzle. You are the player and you need to push all boxes to targets. You are provided with symbol grid and the zero-indexed coordinates of the player, each box, and each target. Coordinates range from the top-left corner (0, 0) to the bottom-right corner (5, 5). When you are exactly next to box, you can push it by moving in the same direction. You cannot push box through wall, and you cannot pull box. The answer should be sequence of actions, like <answer>Right Right Up</answer>. Action 1 Reward 0."
        },
        {
            "title": "Augmentations\nNaN",
            "content": "Current Turn: Turn 1: State: Grid Map: ###### ###__# ###X_# #_#OP# #____# ###### Action <think></think><answer>Down</answer> Reward Reward: 0.0 have Augmentations You </think><answer>[your answer] </answer> with no extra text. low this format. Max response length: words (tokens)."
        },
        {
            "title": "Always",
            "content": "output: actions left. <think>[Your thoughts] Strictly fol-"
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "Sense Time",
        "Sun Yat-sen University"
    ]
}