{
    "paper_title": "ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents",
    "authors": [
        "Feiteng Fang",
        "Ting-En Lin",
        "Yuchuan Wu",
        "Xiong Liu",
        "Xiang Huang",
        "Dingwei Chen",
        "Jing Ye",
        "Haonan Zhang",
        "Liang Zhu",
        "Hamid Alinejad-Rokny",
        "Min Yang",
        "Fei Huang",
        "Yongbin Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic and engaging human-computer interactions. However, traditional reward models often struggle with scalability and adapting to subjective conversational preferences. We propose ChARM, a Character-based Act-adaptive Reward Model, addressing these challenges through two innovations: (1) an act-adaptive margin that significantly enhances learning efficiency and generalizability, and (2) a self-evolution mechanism leveraging large-scale unlabeled data to improve training coverage. Additionally, we introduce RoleplayPref, the first large-scale preference dataset specifically for RPLAs, featuring 1,108 characters, 13 subcategories, and 16,888 bilingual dialogues, alongside RoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13% improvement over the conventional Bradley-Terry model in preference rankings. Furthermore, applying ChARM-generated rewards to preference learning techniques (e.g., direct preference optimization) achieves state-of-the-art results on CharacterEval and RoleplayEval. Code and dataset are available at https://github.com/calubkk/ChARM."
        },
        {
            "title": "Start",
            "content": "ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents Feiteng Fang1,5, Ting-En Lin5, Yuchuan Wu5, Xiong Liu4, Xiang Huang5, Dingwei Chen1, Jing Ye2, Haonan Zhang4,5, Liang Zhu1, Hamid Alinejad-Rokny3, Min Yang1*, Fei Huang5, Yongbin Li5* 1Shenzhen Key Laboratory for High Performance Data Mining, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences 2State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, CAS, Beijing, China 3University of New South Wales 4Tongji University 5Tongyi Laboratory fangx373@gmail.com, min.yang@siat.ac.cn, {ting-en.lte, shuide.lyb}@alibaba-inc.com 5 2 0 2 9 2 ] . [ 1 3 2 9 3 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic and engaging human-computer interactions. However, traditional reward models often struggle with scalability and adapting to subjective conversational preferences. We propose ChARM, Character-based Act-adaptive Reward Model, addressing these challenges through two innovations: (1) an act-adaptive margin that significantly enhances learning efficiency and generalizability, and (2) self-evolution mechanism leveraging large-scale unlabeled data to improve training coverage. Additionally, we introduce RoleplayPref, the first large-scale preference dataset specifically for RPLAs, featuring 1,108 characters, 13 subcategories, and 16,888 bilingual dialogues, alongside RoleplayEval, dedicated evaluation benchmark. Experimental results show 13% improvement over the conventional Bradley-Terry model in preference rankings. Furthermore, applying ChARM-generated rewards to preference learning techniques (e.g., direct preference optimization) achieves state-of-the-art results on CharacterEval and RoleplayEval. Code and dataset are available at https://github.com/calubkk/ ChARM."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have achieved near-human performance across growing spectrum of real-world tasks (Achiam et al., 2023; Liu et al., 2024a; Bubeck et al., 2023; Brown et al., 2020), powering applications from search to creative writing. Among these, Role-Playing Language Agents (RPLAs) are emerging as new frontier, aiming to simulate lifelike characters capable of nuanced, emotionally rich, and context-aware interactions (Chen et al., 2024; Zhou et al., 2024b). RPLAs differ from general-purpose LLMs by embodying specific personalities, motivations, and *Corresponding author. 1 Figure 1: An example illustrating the difficulty of role-playing quality annotation. Three LLMs continue conversation between Sasuke Uchiha and Orochimaru from Naruto, with differing responses, making it challenging to assess the quality of their replies. narrative arcs (e.g., Doraemon, Taylor Swift), driving more immersive and personalized human-AI experiences in domains such as entertainment and education. Recent advancements in RPLAs are largely driven by personalized alignment strategies, such as RLHF and DPO, which deliver strong performance in character emulation (Li et al., 2024; Mondal et al., 2024; Zhou et al., 2024b,a; Yu et al., 2024; Lu et al., 2024; Shao et al., 2023; Yang et al., 2024b; Samuel et al., 2024; Wang et al., 2023). However, reward modeling for RPLAs remains challenging. On one hand, effective evaluation requires capturing the nuanced traits, motivations, and contextual appropriateness of character-driven responsestasks that demand complex, subjective, and domain-specific judgment (Zhou et al., 2024b; Tu et al., 2024). This lead to heavy reliance on expert-labeled preference data, which is costly and difficult to scale. For example, Figure 1 presents three LLMs portraying Sasuke Uchiha from Naruto in conversation with Orochimaru, each generating distinct response. Selecting preference pairs from such samples is inherently. On the other hand, classical approaches such as the Bradley-Terry model often struggle from limited generalization and instability, especially in the diverse, context-dependent scenarios typical of role-playing (Liu et al., 2024b; Wang et al., 2024b; Qin et al., 2024). To address the above issues, we propose ChARM, Character-based Act-adaptive Reward Modeling for advanced RPLAs. ChARM enhances the models understanding of role-playing tasks by introducing the act-adaptive margin and the selfevolution strategy. The act-adaptive margin dynamically measures the reward models confidence in the quality of character dialogues for different preference pairs and adjusts the learning intensity based on this confidence. ChARM also uses selfevolution, which leverages large-scale unlabeled data to iteratively refine reward modeling, reducing reliance on costly human annotations. These innovations boost learning efficiency, generalization, and the overall fidelity of role-playing dialogues. The main contributions of this paper can be summarized as follows: We propose ChARM, novel reward modeling framework, designed to provide accurate rewards for enhancing role-playing abilities in RPLA, dynamically adjusting optimization strength through an act-adaptive margin and leveraging self-evolution to expand training data. We train ChARM-based reward model on Qwen2.5-7B, which outperforms the traditional Bradley-Terry model by 13% in preference ranking. When combined with DPO, it achieves stateof-the-art performance on both CharacterEval and our newly developed role-playing benchmark RoleplayEval. We create the first role-playing preference dataset RoleplayPref, with 1,108 characters across 13 subcategories and 16,888 bilingual dialogues. Additionally, we design new evaluation benchmark RoleplayEval to advance research in this area."
        },
        {
            "title": "2.1 Role-Playing Language Agents",
            "content": "RPLAs generally refer to LLMs that are endowed with specific role knowledge background. Such agents can simulate roles emotions, actions, tone, and thought processes. (Chen et al., 2024). Recently, RPLAs have gained significant attention in the practical deployment of LLMs, with several companies introducing role-playing products, such as Glow1, Character.AI2, and Tongyi Xingchen3. Unlike traditional conversational agents, RPLAs emphasize enhancing user interaction, increasing engagement, and ensuring that generated responses remain faithful to the intended character. To achieve character customization, researchers have explored various technical approaches (Zhou et al., 2024a; Sadeq et al., 2024). Li et al. (2023) leverage retrieval-augmented generation (RAG) to develop role-playing system that allows LLMs to mimic the tone and knowledge of specific film and anime characters by utilizing extensive corpora of dialogues and plot elements. Wang et al. (2023) introduce data augmentation techniques aimed at improving the efficiency of role-agent construction. However, these methods primarily leverage the generative capabilities of LLMs and adapt them to specific roles through data augmentation, rather than fundamentally enhancing the models intrinsic role-playing abilities. To address these limitations, Lu et al. (2024) explore self-alignment techniques to define cognitive boundaries within LLMs, enabling more consistent and controlled character simulation."
        },
        {
            "title": "2.2 Reward Modeling",
            "content": "Alignment techniques (e.g., RLHF, DPO) have long been key focus of research in the field of artificial intelligence as methods to enhance the capabilities of LLMs. However, designing appropriate reward signals for reinforcement learning still poses significant challenges. Many studies are dedicated to building more robust and efficient reward models (Lambert et al., 2024b). For example, Sun et al. (2024) conduct an in-depth theoretical and optimization analysis of the BradleyTerry reward model, while Yang et al. (2024c) attempt to improve the generalization ability of reward models by regularizing the models hid1http://www.glowapp.tech/ 2https://www.character.ai 3https://tongyi.aliyun.com/xingchen/ 2 den states. Additionally, some methods effectively mitigate the overfitting problem of reward models through techniques like reward model ensemble (Coste et al., 2023) or adaptive margin strategies (Qin et al., 2024). Recently, researchers have explored variety of innovative approaches for constructing reward models, such as token-wise dense (Chan et al., 2024) rewards, multi-objective rewards (Wang et al., 2024c), and pair-wise rewards (Liu et al., 2025), further advancing the development of this field."
        },
        {
            "title": "3 Preliminaries",
            "content": "In general, reward modeling is typically based on the Bradley-Terry model (Bradley and Terry, 1952). By learning relative preferences from human feedback, Bradley-Terry can effectively predict the relative quality of each behavior, thereby generating reward signals for each state-action pair. In reward modeling, given pair of responses (yw, yl) for input x, where yw is preferred over yl, the preference probability is defined as: (yw yl x) = exp(rθ(x, yw)) exp(rθ(x, yw)) + exp(rθ(x, yl)) (1) where rθ : is the reward model parameterized by θ."
        },
        {
            "title": "The model is trained via maximum likelihood",
            "content": "estimation with cross-entropy loss: LBT(θ) = E(x,yw ,yl)D [log σ (rθ(x, yw) rθ(x, yl))] , (2) where σ(z) = (1 + exp(z))1 is the sigmoid function, and denotes the preference dataset, rθ denotes reward function. Limitations of Bradley-Terry model. Although the Bradley-Terry model effectively captures preference relationships, it remains sensitive to data noise and exhibits limited generalization capability. Role-playing tasks introduce additional complexity due to diverse scenes, character backgrounds, emotional expressions, and topic variations. Without strong generalization, reward model may perform well on the training dataset but fail to adapt to unseen scenarios. Furthermore, the subjective nature of role-playing dialogue quality assessment makes it susceptible to annotation noise, further affecting stability. Equation 2 has notable limitation: it applies uniform optimization granularity to all preference pairs, failing to account for variations in quality differences (Qin et al., 2024). In roleplaying dialogue preferences, the gap between chosen and rejected responses is not constant but varies significantly. During model training, it is essential not only to distinguish between preferred and nonpreferred responses but also to capture the relative distance between them. Ignoring this factor can lead to overfitting, particularly when training on noisy data."
        },
        {
            "title": "4.1 Overview",
            "content": "To address the above challenges, we propose ChARM, character-based act-adaptive reward modeling approach designed to improve the generalization and robustness of reward models. ChARM comprises two key components: ActAdaptive Margin and Self-Evolution. Figure 2 provides an overview of ChARM."
        },
        {
            "title": "4.2 Act-Adapative Margin",
            "content": "Motivations. Adaptive margin is widely regarded as an effective method to enhance the generalization ability of the Bradley-Terry reward model (Touvron et al., 2023; Wang et al., 2024a). Traditional approaches often require additional margin annotations for each preference pair in the dataset, allowing the model to learn quality differences across samples. However, this approach presents two major challenges: (1) It significantly increases the cost of data annotation, and (2) It does not fully exploit the reward models potential as large language model, preventing it from autonomously adjusting optimization strength across different preference pairs. Based on these insights, we propose an innovative act-adaptive margin. Implementation. Consider two distributions, πw and πl, representing the likelihood distributions of the chosen and rejected responses, respectively. In conventional settings, the distribution over outputs from model θ is typically defined as: πθ(yx) = (cid:89) t=1 p(yt y<t, x) (3) where is generated autoregressively given input x. However, in our context, πw and πl are not derived from the policy model θ; instead, they serve as reference distributions, akin to supervision signals. These distributions originate from human preferences or preference-aligned models. This framing aligns with the notion that humans can be viewed as next-token generators, and thus their outputs implicitly form probability distribution. We 3 Figure 2: An overview of the proposed ChARM framework, featuring an act-adaptive margin and self-evolution mechanism. The construction process of RoleplayPref is also presented. design act-adaptive margin based on hypothesis: when πθ generated by the reward models generation head is closer to πw, it indicates that the model possesses superior role-playing capabilities and more precise understanding of how to effectively play role. Consequently, it can better assess the quality of role-playing action. Accordingly, we can leverage πθ to construct an attribute M, which can, to some extent, reflect the reward models confidence in the data quality of the preference pair (yw, yl). The Kullback-Leibler (KL) divergence serves as an effective measure of this confidence: M(θ) = DKL(πl πθ) DKL(πw πθ) (4) where DKL denotes the KL divergence. When πθ is close to πw and far from πl, is large, indicating that the reward model θ believes yw is indeed better than yl. Conversely, when πθ is far from yw and close to yl, is small, indicating that the reward model θ is uncertain about whether yw is better than yl, and whether this preference relationship is correct. We know that the KL divergence DKL(p q) can be expanded as the difference between the cross-entropy and the entropy: DKL(p q) = H(p, q) H(p), where H(p, q) = Ep[log q] is the cross-entropy, and H(p) = Ep[log p] is the entropy. Substituting this expansion into the Equation 4 gives: M(θ) = [H(πl, πθ) H(πl)] [H(πw, πθ) H(πw)] (5) Since H(πw) and H(πl) are constants independent of θ, we can simplify this to M(θ) H(πl, πθ) H(πw, πθ). Coincidentally, as cross-entropy is exactly the commonly used supervised fine-tuning loss LSFT(θ) = (cid:80)y t=1 log Pθ(yt x, y<t) in the training process of large language models. Finally, we obtain the following objective function: M(θ) = LSFT(yl, θ) LSFT(yw, θ) (6) We can see that is optimized as the difference between the supervised-finetuning loss of rejected responses and the supervised-finetuning loss of chosen responses. Therefore, if we adopt multi-task learning approach to constrain the hidden states of the reward model, ensuring that the role-playing ability of model θ is not degraded lot during the training of the reward model, then the confidence becomes highly suitable attribute for the adaptive margin. We name as Act-Adaptive Margin(AAM). Finally, the reward model loss used in ChARM is: LRM(θ) = LBAAM(θ) + αLSFT(θ) (7) where α is hyperparameter, and LBAAM is the Bradley-Terry loss with act-adaptive margin (BAAM): LBAAM(θ) = E(yw ,yl)D [log σ (rθ(yw) rθ(yl) M)] (8) By leveraging the generative distribution of reward models to assess the quality of different preference pairs and quantify its confidence, the model effectively achieves self-regulated optimization through margin control."
        },
        {
            "title": "4.3 Self-Evolution",
            "content": "inal context to replace the rejected responses. Challenge of Limited Data. Role-playing dialogue evaluation requires considering fluency, coherence, and character consistency, making annotation challengingespecially in multi-turn contexts (Wang et al., 2024d; Tu et al., 2024). The process is time-consuming and difficult to scale, limiting the availability of high-quality preference data (Zhou et al., 2024b).To address these challenges, we propose self-evolving reward modeling approach designed to expand the training dataset while enhancing the reward models ability to understand diverse roles and scenarios. Implementation. The core idea is to use seed reward model to identify and filter high-confidence samples from large pool of unlabeled data, which are then incorporated into subsequent training iterations. Specifically, we first train an initial reward model capable of providing preliminary evaluations of role-playing dialogue quality, distinguishing responses of varying quality for different characters. We then introduce threshold-based filtering strategy to extract high-confidence samples from the unlabeled preference dataset. In this filtering process, the seed reward model scores the chosen and rejected responses for each unlabeled data entry, calculates the reward score gap G, and integrates it with the act-adaptive margin to obtain quality evaluation score: = + M. Based on the computed values, we define two thresholds, Thigh and Tlow, to categorize the unlabeled dataset into three subsets: (1). Precise set (Q > Thigh), high-confidence preference pairs that are directly added to the training set; (2). Uncertain set (Tlow < Thigh), samples requiring further processing before inclusion; (3). Difficult set (Q Tlow), low-quality preference data that require refinement. To improve the data quality of the difficult set, we introduce three targeted rewriting strategies: Fine-grained Rewriting We utilize top-tier LLMs (e.g., Claude, Qwen2.5-72B, GPT-4) to modify low-quality negative samples, generating responses with reduced fluency and engagement. Character Profile Replacement We replace the character profile in the prompt (e.g., swap Snape for Hermione and continue the conversation between Hermione and Harry Potter) and generate new responses based on the origExpression and Action Removal We remove elements like actions, tone, and expressions from the characters responses to reduce the diversity and attractiveness of the replies. Examples of the three rewriting strategies can be found in Table 7. These rewriting strategies can be flexibly applied based on the needs of different role-playing dimensions. The refined difficult set is then combined with the precise set and incorporated into the training set for retraining the seed reward model. This iterative process continues by reapplying the threshold-based filtering strategy to the uncertain set until either its size is significantly reduced or the performance of the reward model converges. By iteratively expanding the training dataset and refining low-confidence samples, this self-evolving framework not only improves data quality and scalability but also enhances the reward models ability to evaluate complex role-playing scenarios with greater accuracy and robustness."
        },
        {
            "title": "5.1 Data Curation",
            "content": "In this section, we introduce RoleplayPref, roleplaying dialogue preference dataset. The dataset construction process is illustrated in Figure 2. We begin by collecting diverse set of high-quality character profiles and designing user prompts that reflect different personality traits. Following the Scene-Character-User framework, we utilize roleplaying-capable models (e.g., Claude, DoubaoCharacter) to iteratively generate and refine dialogues. In each dialogue round, user and character are randomly selected from their respective pools. Using their background information, GPT4o generates an initial dialogue scenario and an opening statement for either participant. Subsequently, two advanced LLMs assume the roles of the user and character, engaging in free-form dialogue within the defined context. Once substantial number of role-playing dialogues are collected, we employ six LLMs to generate multiple responses based on the dialogue context and user queries, including GPT-4o (Achiam et al., 2023), Claude-3.5-sonnet (Anthropic, 2024), DoubaoCharacter (Bytedance, 2025), and Qwen2.5 models (7B, 32B, and 72B) (Yang et al., 2024a). Finally, we use the Qwen2.5-7B, trained with ChARM, 5 It covers twelve metrics in ing 4,564 samples. three dimensions: Character Attractiveness, Conversational Ability, and Knowledge Consistency. RolePlayEval is new benchmark we proposed with 800 samples (400 Chinese, 400 English) spanning 160 characters from 9 domains. Each sample contains dialogue and user query for response evaluation across 6 dimensions. More detailed introduction can be found in Appendix A.1 Baselines. To validate our method, we compare the DPO-enhanced model with open-source (LLaMa3.1 8B/70B (Meta, 2024), Qwen2.5 7B/32B/72B (Yang et al., 2024a)), closed-source (GPT-4o (Achiam et al., 2023), GPT-4o-mini, Claude-3.5-sonnet (Anthropic, 2024)), and proprietary models (Doubao-PRO-Character (Bytedance, 2025), aba minimax5.5s (Minimax, 2024)). Implementation details. During reward model training, the regularization coefficient α is set to 0.01, with 2 training epochs and learning rate of 1e-5. For DPO training, Qwen2.5-7B is finetuned with full-parameter tuning (2 epochs, 1e-6 learning rate), while Qwen2.5-32B is trained with LoRA using 2 epochs and 5e-5 learning rate. All experiments are run on cluster with eight NVIDIA A100 GPUs (80GB each)."
        },
        {
            "title": "6.2 Evaluation methods",
            "content": "To evaluate the ChARM-based reward model, we design two experimental setups: (1) Evaluations for DPO Training. (Section 6.3.1) and (2) Evaluations for Reward Models. (Section 6.3.2). Evaluations for DPO Training. We first use it to provide reward signals for DPO training and assess downstream performance. Specifically, we sample 6,864 instances (3,432 Chinese, 3,432 English) from the RoleplayPref dataset to train language-specific agents on Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct. Evaluations for Reward Models. We then directly evaluate the reward model itself, focusing on its scoring accuracy and generalization ability. To this end, we construct pairwise test set with 1,000 in-domain and 1,000 out-of-domain samples. Indomain samples include seen characters, while outof-domain samples involve novel ones."
        },
        {
            "title": "6.3.1 Evaluations for DPO Training\nEvaluation on CharacterEval and RolePlayEval.\nTable 1 presents the role-playing capabilities of\nvarious open-source and closed-source models,",
            "content": "Figure 3: The character distribution in RoleplayPref consists of 3 primary categories and 13 subcategories. to evaluate and score these responses. Table 4 compares RoleplayPref with other role-playing datasets."
        },
        {
            "title": "5.2 Data Statistics",
            "content": "The RoleplayPref dataset comprises 16,888 dialogues generated by 1,108 characters and 230 virtual users. Of these, 16,388 dialogues are included in the training set, while 500 are allocated to the test set. Each dialogue consists of character, user, the conversational context, and responses generated by six different LLMs. The dataset includes characters spanning 13 categories: comics, movies, teleplays, novels, games, influencers, musicians, writers, scientists, actors, athletes, artists, and custom characters. detailed distribution of character categories is provided in Figure 3."
        },
        {
            "title": "6.1 Experiment Setting",
            "content": "Reward Model Training. We build reward models with Qwen2.5-7B-Instruct, focusing on two key aspects of role-playing. Knowledge Consistency measures alignment with the characters identity and background. Character Attractiveness captures emotional expression, style, and interactivity. We use 4,600 (attractiveness) and 4,873 (consistency) expert-annotated preference pairs to train initial seed models. Then, following the interactive evolution method in Section 4.3, we construct 20,000-sample unlabeled pool for self-evolution. After training the Chinese reward models, we translate the data to train the English versions. Benchmarks. CharacterEval (Tu et al., 2024) is Chinese role-playing benchmark with 1,785 multi-turn dialogues across 77 characters, total-"
        },
        {
            "title": "RolePlayEval",
            "content": "Attr. Conv. Know. Avg. Konwledge zh zh/en zh zh zh Fluency Behavior Diversity Empathy Consistency zh/en zh/en zh/en zh/en zh/en GPT4o GPT4o-mini Claude3.5-sonnet MiniMax-abab5.5s Doubao-Pro-Character Qwen2.5-7B Qwen2.5-32B Qwen2.5-72B LLaMA3.1-8B LLaMA3.1-70B ChARM-DPO-7b -w/o Evol -w/o AAM & Evol ChARM-DPO-32b -w/o Evol -w/o AAM & Evol 3.21 3.15 3.31 2.91 3.62 3.14 3.20 3.28 2.81 3.00 3.61 3.59 3.22 3.77 3.74 3. 3.65 3.42 3.79 3.72 3.81 3.69 3.68 3.82 3.20 3.56 3.79 3.84 3.57 4.05 4.06 3.76 3.02 2.98 3.15 2.71 3.36 2.92 3.03 3.07 2.67 2. 3.35 3.31 2.98 3.44 3.40 3.25 3.29 3.18 3.42 3.11 3.59 3.25 3.31 3.39 2.89 3.12 3.58 3.58 3.26 3.75 3.73 3. 4.07/3.99 3.90/3.95 3.93/4.08 3.52/3.13 3.85/3.84 3.59/3.66 3.73/3.67 3.89/3.99 3.64/3.73 3.63/3.97 3.54/3.70 3.54/3.74 3.32/3.44 3.84/3.93 3.89/3.90 3.78/3.85 4.48/4.45 4.62/4.54 4.61/4.61 4.32/3.68 4.60/4.29 4.47/4.42 4.42/4.48 4.48/4.42 4.31/4.43 4.37/4. 4.51/4.48 4.40/4.32 4.32/4.40 4.65/4.66 4.63/4.72 4.48/4.62 4.06/4.05 4.06/3.93 4.14/3.98 3.61/3.02 4.16/4.01 3.85/3.92 4.02/4.04 4.10/4.09 3.85/4.06 3.97/4.22 3.99/4.22 3.97/4.12 3.80/3.92 4.20/4.21 4.21/4.12 4.05/4. 3.70/3.77 3.54/3.72 3.67/3.87 3.41/2.79 3.62/3.34 3.52/3.61 3.59/3.66 3.55/3.74 3.63/3.73 3.34/3.95 3.68/3.80 3.62/3.72 3.54/3.55 3.71/3.98 3.68/3.93 3.77/3.79 4.11/4.18 4.10/4.08 4.20/4.20 3.66/2.91 4.06/3.65 4.00/3.90 4.10/4.04 4.14/4.12 3.87/3.89 3.94/4. 4.00/3.65 4.00/3.72 3.98/3.65 4.05/4.00 3.94/3.97 4.08/4.10 3.79/3.55 3.71/3.60 3.88/4.07 3.54/2.90 4.00/3.57 3.77/3.48 3.86/3.52 3.71/3.60 3.67/3.55 3.64/3.66 3.71/3.60 3.69/3.57 3.60/3.45 3.98/3.79 3.95/3.84 3.74/3. Avg. zh/en 4.04/4.00 3.99/3.97 4.07/4.14 3.68/3.07 4.05/3.78 3.87/3.83 3.95/3.90 3.98/3.99 3.83/3.90 3.82/4.07 3.91/3.91 3.87/3.86 3.76/3.73 4.07/4.10 4.05/4.08 3.98/4.04 Table 1: Experimental results of various models on CharacterEval and RolePlayEval. Attr. refers to Character Attractiveness, Conv. refers to Conversational Ability, and Know. refers to Knowledge Consistency. The Qwen2.5 series models, enhanced with ChARM, demonstrate significant improvements over both open-source and closed-source models. along with the evaluation results of ChARM on DPO. As shown in the table, Doubao-Pro-Character and Claude3.5-sonnet demonstrate strong roleplaying abilities. Compared to GPT-4o, DoubaoPro-Character exhibits performance gap of 0.3 on CharacterEval. In RolePlayEval, whether in Chinese or English, Claude3.5-sonnet consistently achieves high role-playing proficiency. However, RolePlayEval also highlights that while DoubaoPro-Character surpasses GPT-4o in Chinese roleplaying tasks, it lags behind in English role-playing performance. The Qwen2.5-7B and Qwen2.5-32B models, enhanced by ChARM, achieve significant improvements across all dimensions of roleplaying ability. ChARM-DPO-32B performs on par with Claude3.5-sonnet in RolePlayEval. Notably, ChARM-DPO-32B outperforms DoubaoPro-Character by 0.16 on CharacterEval, achieving the SOTA (State-of-the-Art) role-playing performance among all open-source, closed-source, and proprietary models. Human Evaluations. Additionally, we conduct human evaluation to compare ChARM-DPO32B with three baseline models: Claude3.5-sonnet, GPT-4o, and Doubao-Pro-Character. In each pairwise comparison, both models generate responses to the same role-playing dialogue context. Five human annotators then assess the responses, categorizing the results as win, tie, or loss for ChARMDPO-32B relative to each baseline. The average results from 200 test samples, along with annotations from the five evaluators, are presented in Figure 4. Notably, ChARM-DPO-32B significantly outperFigure 4: Results of Human Evaluations. forms all three models in role-playing capabilities, providing strong evidence of the effectiveness of our proposed methodology. Ablation Study. To assess the impact of key components in ChARM, we conduct an ablation study by removing self-evolution (Evol) and act-adaptive margin (AAM), as shown in Table 1. Both components contribute to performance, with AAM having greater effect. Removing both reduces ChARM to the Bradley-Terry model. Using Bradley-Terry models rewards to train Qwen2.5 leads to gains in knowledge consistency and character attractiveness, but hurts conversational ability (e.g., 3.69 3.57 on Qwen2.5-7B). In contrast, rewards from ChARM improve all dimensions, confirming the effectiveness of AAM in producing more reliable reward signals. Given the effectiveness of AAM, we extend our experiments to wider range of general tasks. More details are provided in Appendix B."
        },
        {
            "title": "6.3.2 Evaluations for Reward Models",
            "content": "Generalization Evaluation. We evaluate the impact of different components of ChARM on gen-"
        },
        {
            "title": "Method",
            "content": "Knowledge Consistency Avg. OOD ID ChARM -w/o Evol -w/o Evol & Sft -w/o Evol & AAM 74.4 73.8 67.4 62.2 64.4 63.4 62.6 50.8 69.4 68.6 65.0 56."
        },
        {
            "title": "Character Attractiveness\nAvg\nOOD\nID",
            "content": "ChARM -w/o Evol -w/o Evol & SFT -w/o Evol & AAM 73.8 73.6 64.8 58.2 62.6 59.2 58.2 52.0 68.2 66.4 61.5 55.1 Consistency acc(#sample) Attractiveness Avg. acc acc(#sample) 2250-sized seed data 60.2 (2,250) 61.2 (3,176) 61.6 (3,722) 61.4 (4,225) 62.8 (4,535) 57.4 (2,250) 58.6 (3,505) 60.8 (4,181) 61.6 (4,627) 61.4 (4,904) 4800-sized seed data 63.4 (4800) 64.4 (6,375) 63.6 (7,084) 63.4 (7,738) 63.8 (8,030) 59.2 (4800) 61.4 (6,918) 62.6 (7,652) 61.0 (8,149) 58.8 (8,328) 58.8 59.9 61.2 61.5 62.1 61.3 62.9 63.1 62.2 61.3 Seed Loop1 Loop2 Loop3 Loop4 Seed Loop1 Loop2 Loop3 Loop4 Table 2: Generalization evaluation conducted on the character attractiveness and knowledge consistency reward models. We report the consistency(%) between the reward model and human annotations. Table 3: Evaluation of the self-evolution mechanism. Each cell shows the accuracy (in %) followed by the number of training samples in parentheses. eralization using in-domain and out-of-domain datasets. The in-domain dataset consists of characters from the training set, while the out-of-domain dataset is derived from the subset of RoleplayPref, which contains characters not included in the training set. These datasets are manually annotated with preference labels. We then compare the consistency between the reward model scores and the human preference labels. The experimental results, as shown in Table 2, demonstrate that ChARM outperforms the Bradley-Terry reward model(denoted as w/o Evol & AAM) in both in-domain and outof-domain tests. Specifically, ChARM achieves 12.9% improvement in character attractiveness evaluation and 13.1% improvement in knowledge consistency evaluation, with an average improvement of 13%. These results strongly indicate that ChARM significantly enhances the generalization ability of reward models. Effect of the Regularization Term. We investigate the impact of the regularization term in the act-adaptive margin. As shown in Table 2, removing the SFT regularization term results in decline in model performance, with knowledge consistency dropping from 68.6 to 65.0 and character attractiveness decreasing from 66.4 to 61.5. This suggests that the SFT regularization term plays crucial role in constraining the reward model, preserving roleplaying ability and generation capacity, ultimately improving overall performance. Effect of Self-Evolution. We also analyze the effectiveness of self-evolution. In Table 3, we perform experiments using the RoleplayPref subset, recording the dataset size used in each round of self-evolution along with the corresponding test results. The experimental results show that under the two different initial seed training set sizes of 2,250 and 4,800, self-evolution can lead to certain degree of performance improvement within limited number of iterations, both in the knowledge consistency reward model and the character attractiveness reward model. Notably, the best results are observed with the 2,250-sized initial training set, where 3.3% improvement is achieved after 4 rounds of evolution. However, the results from the 4,800-sized initial training set indicate that more evolution iterations do not necessarily lead to better results. As the number of iterations increases, the generalization ability of the reward model declines instead of improving, suggesting that the model may have overfitted to certain noisy data during training."
        },
        {
            "title": "7 Conclusion",
            "content": "In this study, we propose ChARM, framework for building role-playing reward models. It introduces an act-adaptive margin to dynamically adjust optimization based on preference levels, improving generalization across characters and scenarios. self-evolution strategy further boosts its ability by using unlabeled data. Experiments show ChARMtrained models outperform the Bradley-Terry baseline. Incorporating ChARM into DPO training, Qwen2.5-32B achieves state-of-the-art results on role-playing benchmarks. To facilitate further research, we release the first large-scale role-playing preference dataset, providing valuable resource for advancing role-playing AI systems."
        },
        {
            "title": "8 Limitations",
            "content": "In this section, we analyze the limitations of our study to better optimize our approach and provide more effective guidance for researchers in training reward models in the role-playing tasks. We discuss two main shortcomings of our work. First, our reward model is only constructed based on two dimensions: knowledge consistency and character attractiveness. However, there are many other important dimensions to consider when evaluating role-playing quality, such as plot development and emotional perception. Therefore, in the future, we plan to collect more high-quality, multidimensional evaluation data for role-playing and construct more comprehensive and refined model. Second, while many studies suggest that improving critique generation ability can enhance the performance of reward models, we do not adopt multi-task learning approach to integrate critique capability, due to the difficulty in obtaining roleplaying evaluation data. In future work, we plan to develop specialized critique model to further optimize RPLAs."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Anthropic. 2024. Claude 3.5 sonnet. Ralph Allan Bradley and Milton Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324 345. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712. Bytedance. 2025. Doubao. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. 2024. Internlm2 technical report. Preprint, arXiv:2403.17297. Alex Chan, Hao Sun, Samuel Holt, and Mihaela van der Schaar. 2024. Dense reward for free in reinforcement learning from human feedback. arXiv preprint arXiv:2402.00782. Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li, Ruihan Yang, Tinghui Zhu, et al. 2024. From persona to personalization: survey on role-playing language agents. arXiv preprint arXiv:2404.18231. Nuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Yuhan Li, Ziyang Chen, Longyue Wang, and Jia Li. 2023. Large language models meet harry potter: dataset for aligning dialogue agents with characters. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 85068520. Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. 2023. Reward model ensembles arXiv preprint help mitigate overoptimization. arXiv:2310.02743. Tear Gosling, Alpin Dale, and Yinhe Zheng. 2023. Pippa: partially synthetic conversational dataset. Preprint, arXiv:2308.05884. Shengyi Costa Huang, Agustín Piqueres, Kashif Rasul, Philipp Schmid, Daniel Vila, and Lewis Tunstall. 2024. Open hermes preferences. https://huggingface.co/datasets/argilla/ OpenHermesPreferences. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. 2024a. Rewardbench: Evaluating reward models for language modeling. Preprint, arXiv:2403.13787. 9 Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. 2024b. Rewardbench: Evaluating reward arXiv preprint models for language modeling. arXiv:2403.13787. Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi Mi, Yaying Fei, Xiaoyang Feng, Song Yan, HaoSheng Wang, et al. 2023. Chatharuhi: Reviving anime character in reality via large language model. arXiv preprint arXiv:2308.09597. Xinyu Li, Ruiyang Zhou, Zachary Lipton, and Liu Leqi. 2024. Personalized language modeling from personalized human feedback. arXiv preprint arXiv:2402.05133. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024a. arXiv preprint Deepseek-v3 technical report. arXiv:2412.19437. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. 2024b. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. 2024c. Skywork-reward: Bag of tricks for reward modeling in llms. Preprint, arXiv:2410.18451. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. 2025. Pairwise rm: Perform best-of-n sampling with knockout tournament. arXiv preprint arXiv:2501.13007. Keming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. 2024. Large language models are superpositions of all characters: Attaining arbitrary role-play via selfalignment. arXiv preprint arXiv:2401.12474. Meta. 2024. Llama3.1. Minimax. 2024. Minimax-abab5.5s. Ishani Mondal, Shwetha, Anandhavelu Natarajan, Aparna Garimella, Sambaran Bandyopadhyay, and Jordan Boyd-Graber. 2024. Presentations by the humans and for the humans: Harnessing llms for genIn erating persona-aware slides from documents. Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 26642684. Bowen Qin, Duanyu Feng, and Xi Yang. 2024. Towards understanding the influence of reward margin on preference model performance. arXiv preprint arXiv:2404.04932. Nafis Sadeq, Zhouhang Xie, Byungkyu Kang, Prarit Lamba, Xiang Gao, and Julian McAuley. 2024. Mitigating hallucination in fictional character role-play. arXiv preprint arXiv:2406.17260. Vinay Samuel, Henry Peng Zou, Yue Zhou, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Ameet Deshpande, Karthik Narasimhan, and Vishvak Murahari. 2024. Personagym: Evaluating persona agents and llms. arXiv preprint arXiv:2407.18416. Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023. Character-llm: trainable agent for roleplaying. In EMNLP. Hao Sun, Yunyi Shen, and Jean-Francois Ton. 2024. Rethinking bradley-terry models in preference-based reward modeling: Foundations, theory, and alternatives. arXiv preprint arXiv:2411.04991. Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Y. Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, and Ion Stoica. 2025. Judgebench: benchmark for evaluating llm-based judges. Preprint, arXiv:2410.12784. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Quan Tu, Shilong Fan, Zihang Tian, and Rui Yan. 2024. Charactereval: chinese benchmark for role-playing conversational agent evaluation. arXiv preprint arXiv:2401.01275. Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, et al. 2024a. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint arXiv:2401.06080. Binghai Wang, Rui Zheng, Lu Chen, Zhiheng Xi, Wei Shen, Yuhao Zhou, Dong Yan, Tao Gui, Qi Zhang, and Xuan-Jing Huang. 2024b. Reward modeling requires automatic adjustment based on data quality. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 40414064. Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, and Tong Zhang. 2024c. Arithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards. arXiv preprint arXiv:2402.18571. Lei Wang, Jianxun Lian, Yi Huang, Yanqi Dai, Haoxuan Li, Xu Chen, Xing Xie, and Ji-Rong Wen. 2024d. Characterbox: Evaluating the role-playing capabilities of llms in text-based virtual worlds. arXiv preprint arXiv:2412.05631. 10 Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, et al. 2023. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. arXiv preprint arXiv:2310.00746. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024a. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Bohao Yang, Dong Liu, Chen Tang, Chenghao Xiao, Kun Zhao, Chao Li, Lin Yuan, Guang Yang, Lanxiao Huang, and Chenghua Lin. 2024b. Crafting customisable characters with llms: Introducing simschat, persona-driven role-playing agent framework. arXiv preprint arXiv:2406.17962. Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. 2024c. Regularizing hidden states enables learning generalizable reward model for llms. arXiv preprint arXiv:2406.10216. Yeyong Yu, Runsheng Yu, Haojie Wei, Zhanqiu Zhang, and Quan Qian. 2024. Beyond dialogue: profile-dialogue alignment framework towards general role-playing language model. arXiv preprint arXiv:2408.10903. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. 2024. Advancing llm reasoning generalists with preference trees. Preprint, arXiv:2404.02078. Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Pei Ke, Guanqun Bi, Libiao Peng, et al. 2024a. Characterglm: Customizing social characters with large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 14571476. Jinfeng Zhou, Yongkang Huang, Bosi Wen, Guanqun Bi, Yuxuan Chen, Pei Ke, Zhuang Chen, Xiyao Xiao, Libiao Peng, Kuntian Tang, et al. 2024b. Characterbench: Benchmarking character customizaarXiv preprint tion of large language models. arXiv:2412.11912."
        },
        {
            "title": "A Appendix",
            "content": "A.1 RoleplayEval We propose new role-playing evaluation benchmark, RoleplayEval, designed to automatically assess the performance of RPLA by utilizing GPT4o and 800 test samples. Before constructing RoleplayEval, we first generate 160 role profiles and prompts using Claude3.5-sonnet, GPT-4o, and Doubao-Pro-Character. These are then manually refined to improve the accuracy and quality of the role information. The generated roles cover 9 common categories: Custom Roles, Anime, Novels, Telepaly, Movies, Games, Scientists, Actors, and Musicians. After obtaining accurate role information, we adopt method similar to RoleplayPref-Scene-Character-User Framework, generating 1000 dialogue contexts. To ensure that RoleplayEval can comprehensively assess the RPLAs capabilities, we focus on six key dimensions. Consistency refers to the ability of RPLA to understand and remember the context of the conversation, providing coherent responses based on the prior dialogue. If RPLA frequently fails to recall previous interactions, it indicates poor contextual consistency. Knowledge evaluates whether RPLAs cognition aligns with the characters background knowledge, which is crucial for maintaining the authenticity of the character. If RPLAs knowledge diverges from the characters established traits, it will negatively impact character development. Behavior assesses whether RPLAs actions, expressions, and tone accurately reflect the characters personality traits. successful RPLA should be able to convey its unique characteristics through these details; failure to do so indicates flaw in character portrayal. Empathy is key dimension for evaluating RPLAs emotional interaction quality. model with good empathy not only increases the characters appeal but also enhances its emotional support capabilities. Diversity focuses on the richness of content presented by the character during the conversation, assessing whether RPLA can demonstrate variety of thoughts and expressions. Fluency measures the basic conversational ability of RPLA, evaluating whether it can engage in natural, fluent dialogues. Based on these 6 dimensions and 160 role characteristics, we ask human annotators to design user query for each dialogue context, matching the current role and dimension, to continue the conversation and assess RPLAs performance in that particular dimension. From the 1000 dialogue samples, we select 400 to construct the RoleplayEval benchmark. Each sample is accompanied by set of evaluation criteria, helping GPT-4o to provide more accurate scoring. During evaluation, the model replies to each sample, and GPT-4o scores RPLAs response on scale from 1 to 5 based on the context, the models reply, and the specific evaluation criteria. Finally, we compute the average score across all dimensions to obtain the overall RPLA score in RoleplayEval. After completing the annotation and quality check for the 400 Chinese samples, we translate them into English, resulting in the English version of RoleplayEval. Figure 5 presents an example of RoleplayEval sample to help readers better understand the evaluation process. Table 4 provides detailed information about RoleplayEval and compares it with other role-playing datasets."
        },
        {
            "title": "B Evaluating AAM on General Tasks",
            "content": "During the training of role-playing reward models, Act-Adaptive Margin demonstrate strong performance. To further evaluate its generalization ability on other tasks, two general-purpose reward model evaluation benchmarks are selected: RewardBench (Lambert et al., 2024a) and JudgeBench (Tan et al., 2025). RewardBench is benchmark dataset designed to evaluate reward models across challenging prompts in the domains of chat, reasoning, and safety. JudgeBench is benchmark aimed at assessing the reliability of LLM-based judges on difficult tasks across knowledge, reasoning, math, and coding. The reward models are trained using the Skywork-Reward-Preference-80K-v0.2 (Liu et al., 2024c). The experimental results are shown in Table 5 and Table 6. Specifically, the Bradley-Terry model trained on Qwen2.5-7B is denoted as Qwen2.57B-BT; the model trained with Act-Adaptive Margin is labeled Qwen2.5-7B-AAM; and Qwen2.57B-GPTM refers to the model trained using margins directly generated by GPT-4o between prefer12 Dataset Source Type Multi-turn Open-source Multilingual #Roles #Sessions #Avg.Turns HPD (Chen et al., 2023) CharacterGLM (Zhou et al., 2024a) RoleLLM (Wang et al., 2023) CharacterLLM (Shao et al., 2023) RIPPA (Gosling et al., 2023) ChatHaruhi (Li et al., 2023) WIKIROLE (Lu et al., 2024) CharacterEval (Tu et al., 2024) RoleplayEval Novel Dialogue Novel&Human&LLM Dialogue LLM LLM Human Novel&LLM LLM Novel LLM&Human QA Dialogue Dialogue Dialogue Dialogue Dialogue Dialogue Preference Preference 113 250 100 9 1254 32 7086 77 - 1108 1042 1034 23463 1600 26000 54726 7086 4564 800 3060 16888 13.8 15.78 - 13.2 40.34 1.23 5.1 9.28 8.79 - 12.8 OpenHermesPreferences (Huang et al., 2024) RoleplayPref LLM LLM Table 4: Comparison of different datasets used for role-playing tasks. The table lists key attributes, such as source, type, multilingual support, and the number of roles, sessions, and average turns for each dataset."
        },
        {
            "title": "Reward Model",
            "content": "Chat Chat-Hard"
        },
        {
            "title": "Safety Reasoning Avg",
            "content": "Skywork-Reward-Gemma-2-27B (Liu et al., 2024c) InternLM-20B-Reward (Cai et al., 2024) Eurus-RM-7b (Yuan et al., 2024) Qwen2.5-7B-BT Qwen2.5-7B-GPTM Qwen2.5-7B-AAM 96.1 98.9 98.0 88.3 89.1 88.9 90.0 76.5 65.6 83.1 84.7 86. 93.0 89.9 81.2 90.3 91.5 91.9 97.4 95.8 86.3 88.0 87.4 94.8 94.1 90.3 82.8 87.4 88.2 90. Table 5: Performance comparison of different reward models on RewardBench."
        },
        {
            "title": "Reward Model",
            "content": "knowledge reasoning math coding Avg Skywork-Reward-Gemma-2-27B (Liu et al., 2024c) InternLM-20B-Reward (Cai et al., 2024) InternLM-7B-Reward (Cai et al., 2024) Qwen2.5-7B-BT Qwen2.5-7B-GPTM Qwen2.5-7B-AAM 59.7 62. 56.5 59.7 60.4 64.3 66.3 69.4 61.2 62.2 66.3 66.3 83.9 66.1 71.4 85.7 78.6 75.0 50.0 50. 50.0 59.5 59.5 66.7 65.0 62.0 59.8 66.8 66.2 68.1 Table 6: Performance comparison of different reward models on JudgeBench."
        },
        {
            "title": "Content",
            "content": "You are Doraemon. Doraemon, have an exam coming up. Please give me the memory bread! Calm down, Nobita. Why are you always so anxious? Cant you study seriously just once, instead of always relying on me? (shakes head gently) Doraemon, youre the best. Nobita, Ill lend it to you this time, but dont eat too much of it. wont help you again next time, got it? (helplessly starts rummaging through the gadget pouch) wont lend it to you! You never study properly. How are you going to manage like this?! (stomps angrily) a. Character Profile Replacement Im not Doraemon, Nobita, Im Shizuka. dont have any memory bread. Why do you want it anyway? Oh! Dont tell me youre planning to cheat? b. Fine-grained Rewriting Im not helping you this time. c. Expression and Action Removal wont lend it to you! You never study properly. How are you going to manage like this?! Table 7: Three targeted rewriting strategies. In this table, we use dialogue between Doraemon and Nobita as an example. After obtaining the context, Doraemon responds to Nobita. The chosen response represents high-quality reply in the role of Doraemon, while the rejected response represents lower-quality one. To further enlarge the quality gap, we apply three targeted rewriting strategies to intentionally degrade the rejected responses. ence pairs. As observed from the tables, models trained with Act-Adaptive Margin outperform the standard Bradley-Terry model. Moreover, the way margins are constructed plays crucial role, GPT4o-generated margins do not lead to performance gains on JudgeBench. 13 focus on persona consistency and behavioral traits, potentially leading to less emphasis on emotional modeling."
        },
        {
            "title": "D Case study",
            "content": "To help readers intuitively understand the improvements in role-playing abilities brought by ChARM, we select some examples for case studies, as shown in Figure 6 and Figure 7. In these figures, we manually evaluate the responses from ChARM-DPO32b, GPT-4o, and Claude 3.5-Sonnet. It can be observed that ChARM-DPO-32b outperforms the other models in both knowledge consistency and diversity, as well as in maintaining context consistency across these two examples. In contrast, GPT-4o and Claude 3.5-Sonnet occasionally make minor errors in their responses."
        },
        {
            "title": "C Further Discussion",
            "content": "Low-quality reward signals or weak reward models can induce \"seesaw effect\" during DPO alignment. The seesaw effect refers to trade-off phenomenon observed in alignment: when optimizing one performance dimension, others may degrade, resulting in unbalanced improvements. For example, in Table 1, the Qwen2.5-7B model aligned with Bradley-Terry reward model demonstrates improvements in \"character attractiveness\" and \"knowledge consistency\" under the CharacterEval benchmark. However, its performance on \"general dialogue ability\" decreases noticeably. This is typical case of the seesaw effect. In contrast, when aligned using our proposed ChARM method, Qwen2.5-7B achieves consistent gains across all evaluation dimensions. This result not only showcases ChARMs superior alignment capability, but also underscores the critical role of high-quality reward signals in achieving multiaspect performance gains. Knowledge-related role abilities are harder to optimize than character attractiveness. Our analysis shows that improving role-specific knowledge is more challenging than enhancing character attractiveness. This is likely because knowledgecentric abilities are strongly correlated with pretraining corpora and model scale, both of which are difficult to compensate for during the alignment stage through surface-level preference modeling. By contrast, character attractiveness tends to depend more on stylistic mimicry and surfacelevel language patterns, which can be more readily enhanced through reward model optimization. This observation suggests that improving character knowledge requires stronger logical reasoning abilities and precise knowledge grounding, calling for more powerful reward modeling and training strategies. General-purpose LLMs often outperform role-specific models in empathy. Interestingly, we find that general-purpose language models tend to outperform role-specific models in terms of empathy. While role-specific models excel in dimensions like character consistency, they often lag behind in emotional understanding and empathetic response. This may be because general LLMs are exposed to large volume of high-quality multiturn dialogues during pretraining, equipping them with better capabilities in emotion recognition and generation. In contrast, role-playing models often 14 Figure 5: An example used to demonstrate the RoleplayEval evaluation process. Figure 6: case example of comparison between ChARM-DPO-32b and GPT4o. 16 Figure 7: case example of comparison between ChARM-DPO-32b and Claude3.5-Sonnet."
        }
    ],
    "affiliations": [
        "Shenzhen Key Laboratory for High Performance Data Mining, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
        "State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, CAS, Beijing, China",
        "Tongji University",
        "Tongyi Laboratory",
        "University of New South Wales"
    ]
}