{
    "paper_title": "ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability",
    "authors": [
        "Wenhan Liu",
        "Xinyu Ma",
        "Weiwei Sun",
        "Yutao Zhu",
        "Yuchen Li",
        "Dawei Yin",
        "Zhicheng Dou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker \\textbf{ReasonRank} outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. \\textbf{Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard\\footnote{https://brightbenchmark.github.io/}.} Our codes are available at https://github.com/8421BCD/ReasonRank."
        },
        {
            "title": "Start",
            "content": "ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability Wenhan Liu1, Xinyu Ma2, Weiwei Sun3, Yutao Zhu1, Yuchen Li2, Dawei Yin2, Zhicheng Dou1*, 1Gaoling School of Artificial Intelligence, Renmin University of China 2Baidu Inc. 3Carnegie Mellon University lwh@ruc.edu.cn, xinyuma2016@gmail.com, dou@ruc.edu.cn 5 2 0 2 9 ] I . [ 1 0 5 0 7 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose two-stage post-training approach, which includes cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design multi-view ranking reward, which is more effective than ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker ReasonRank outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard1. Our codes are available at https://github.com/8421BCD/ReasonRank. Introduction Passage ranking plays crucial role in Information Retrieval (IR) by refining initial retrieval results to enhance the quality of search results. Recently, large language models (LLMs) have demonstrated impressive capabilities in zero-shot passage ranking (Sun et al. 2023). Among these LLM-based ranking approaches, listwise ranking has emerged as particularly effective, as it evaluates and ranks list of passages simultaneously for given query, capturing global relevance patterns more comprehensively than pointwise ranking (Zhuang et al. 2023) and pairwise ranking (Qin et al. *Corresponding author Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 1https://brightbenchmark.github.io/ Figure 1: The left part shows the average NDCG@10 on BRIGHT benchmark by reranking ReasonIR-retrieved top100 passages. The right part compares the ranking latency of ReasonRank (7B) and Rank1 (7B) on Earth Science dataset. 2023). And it has demonstrated the state-of-the-art (SOTA) performance on many IR benchmarks (Sun et al. 2023). Recently, Large Reasoning Models (LRMs) such as DeepSeek R1 (DeepSeek-AI et al. 2025) have shown remarkable success across challenging NLP tasks. These LRMs demonstrate that explicit step-by-step reasoning during test time could help generate more accurate answers. Such reasoning ability is also desirable for passage reranking, where understanding query intent and reasoning across multiple passages are critical to accurate ranking. Motivated by this success, recent studies (Weller et al. 2025; Zhuang et al. 2025) have attempted to inject such reasoning ability into passage rerankers, which yield better performance than directly outputting the final ranking results. However, due to the scarcity of reasoning-intensive training data, their rerankers are primarily trained on traditional web search data MSMARCO (Nguyen et al. 2016), whose relevance often relies on simple lexical or semantic matching. While many real-world search scenarios, such as StackExchange platforms, typically involve complex reasoningintensive queries whose relevant passages are those that support user queries with key evidence or provide solutions to queries with similar methodologies or logic. Such data gap between training and inference makes it difficult for existing rerankers to generalize to many complex search scenarios. Although it is possible to acquire some training data through human annotation, it is highly costly and unfeasible. These challenges create bottlenecks in training rerankers for complex search scenarios. To alleviate this issue, we propose an automated data synthesis framework to construct high-quality reason-intensive training data without any human involvement. Specifically, we first collect diverse user queries covering four different types: complex QA, coding, math, and web search. Then we apply the strong reasoning model DeepSeek-R1 to automatically mine positive passages and hard negative passages to construct training passage lists, and generate reasoning chains as well as gold ranking lists as the training labels. Different from pure teacher distillation (Pradeep, Sharifymoghaddam, and Lin 2023b; Weller et al. 2025), we supply DeepSeek-R1 with the gold answer of query to enhance its understanding of queries and generate better ranking labels. We also design self-consistency data filtering mechanism to ensure the quality of the constructed data. Finally, we obtain 13K high-quality and diverse training data for subsequent training. With high-quality listwise training data, we further propose two-stage training framework for our reasoningintensive reranker ReasonRank. We first introduce coldstart Supervised Fine-Tuning (SFT) strategy to help the backbone LLM learn the listwise reasoning pattern as well as the gold ranking list. Then, we use Reinforcement Learning (RL) to help LLM explore better reasoning patterns and enhance the LLMs ranking ability. Previous RL-based rerankers (Liu et al. 2025; Zhang et al. 2025) solely use ranking metric (e.g., NDCG) as reward to evaluate the rollout of the ranking list. However, such reward only evaluates single-turn ranking, which ignores the multi-turn nature of sliding-windows-based listwise ranking and cannot accurately evaluate the final ranking list. In this paper, we propose multi-view ranking reward tailored for listwise ranking, which considers rewards of both single-turn and multiturn ranking. Extensive experiments on two reasoning-intensive IR benchmarks (BRIGHT (Su et al. 2025) and R2MED (Li, Zhou, and Liu 2025)) demonstrate the SOTA performance of ReasonRank. Notably, as shown in Figure 1, the 7B-scale and 32B-scale of our ReasonRank outperform the previous state-of-the-art model by 3 and 5 points on BRIGHT, respectively, and our ReasonRank performs more efficiently than the pointwise reranker Rank1 (Weller et al. 2025). The contributions of the paper are listed as follows: To address the scarcity of reasoning-intensive ranking data, we design an automated data synthesis framework that generates 13K high-quality and diverse reasoning-intensive training data for the reasoning-intensive ranking task. we propose two-stage training framework, which includes cold-start SFT strategy for reasoning pattern learning and multi-view ranking reward-based RL approach for further ranking ability enhancement. Extensive experiments on the reasoning-intensive IR benchmarks BRIGHT and R2MED demonstrate the effectiveness and efficiency advantages of our ReasonRank."
        },
        {
            "title": "Related Work",
            "content": "LLMs for Ranking The application of Large Language Models (LLMs) to ranking tasks has revolutionized traditional IR (Zhu et al. 2023, 2024). Current LLM-based ranking methods can be categorized into three paradigms: pointwise, pairwise, and listwise methods. Pointwise methods (Liang et al. 2022; Sachan et al. 2022; Liu, Zhu, and Dou 2024) evaluate each query-document pair independently. While computationally efficient, such method lacks crossdocument comparison. Pairwise methods (Qin et al. 2023; Luo et al. 2024) compare document pairs to establish relative relevance. Such method usually suffers from efficiency issues due to the large number of pairwise comparisons. Listwise methods (Sun et al. 2023; Pradeep, Sharifymoghaddam, and Lin 2023b; Yoon et al. 2024; Liu et al. 2024b; Yoon et al. 2025; Chen et al. 2024; Liu et al. 2024a) leverage LLM to rerank passage list. Through global passage comparison, listwise ranking has achieved SOTA performance on many IR benchmarks. techniques Reasoning Language Models Recent in LLMs, such as Chain-of-Thought (CoT) (Wei et al. 2022) and Tree of Thoughts (ToT) (Yao et al. 2023) have demonstrated remarkable improvements in complex reasoning capabilities. Large reasoning models like OpenAIs o1 (Jaech et al. 2024) have pushed these capabilities further, with some incorporating RL to incentivize longer reasoning chains. Recently, these reasoning capabilities have also been applied to reasoning-intensive IR scenarios (Su et al. 2025; Shao et al. 2025; Weller et al. 2025; Yang et al. 2025b; Zhuang et al. 2025; Qin et al. 2025), particularly in passage ranking. For example, Rank1 (Weller et al. 2025) and RankK (Yang et al. 2025b) propose to distill the reasoning chain of Deepseek-R1 into rerankers. Zhuang et al. (2025) proposes using the RL algorithm GRPO to optimize setwise reranker. While these works improve rerankers performance, they still suffer from the scarcity of reasoningintensive training data, which makes them perform poorly in reasoning-intensive benchmarks. In this paper, we propose to synthesize reasoning-intensive training data from diverse domains and propose two-stage training approach to enhance ranking performance."
        },
        {
            "title": "Preliminaries",
            "content": "Listwise Ranking Passage ranking aims to rerank list of retrieved passages [p1, . . . , pN ] based on their relevance to query q. The listwise ranking approach takes both the query and set of passages as input and outputs reranked sequence of passage IDs (e.g., [3] > [1] > . . . ). Through comparing multiple passages, listwise ranking demonstrates SOTA performance in IR benchmarks (Sun et al. 2023). Due to the limited context length of many LLMs, listwise ranking usually applies sliding window strategy to process subset of passages iteratively. This strategy uses window size and step size to promote relevant passage from back to the front. Following existing studies (Sun et al. 2023; Pradeep, Sharifymoghaddam, and Lin 2023a,b), we set = 100, = 20, and = 10 in this paper. Figure 2: An overview of reasoning-intensive ranking data synthesis on four domains. Reasoning-based Listwise Ranking When output reasoning during test-time, the listwise reranker takes prompt template P, the query q, and the passage list [p1, . . . , pN ] as input, and outputs structured response combining reasoning traces and the reranked list: = LLM(cid:0)P, q, [p1, . . . , pN ](cid:1), (1) where the reasoning traces are enclosed by tags <think> and </think> and the final ranked list (e.g., [3] > [1] > . . . ) is enclosed by tags <answer> and </answer>."
        },
        {
            "title": "Methodology",
            "content": "To train our reasoning-intensive reranker, namely ReasonRank, we propose an automated data synthesis framework to collect high-quality reasoning-intensive training data and two-stage training approach for enhancing the ranking ability of our reranker. Reasoning-intensive Ranking Data Synthesis To mitigate the scarcity issue of reasoning-intensive training data, we introduce an automated training data synthesis framework. Existing reasoning-intensive IR benchmarks (Su et al. 2025; Li, Zhou, and Liu 2025; Xiao, Hudson, and Moubayed 2024) involve queries from three main domains: complex QA, math, and coding. Following these benchmarks, we also use these three kinds of queries, as well as web search query (to ensure ReasonRanks performance on short search queries), to synthesize our training data. The overall process is shown in Figure 2. We further design self-consistency data filtering mechanism to remove lowquality training data. Next, we will detail each part. All the prompts used in this section are shown in Appendix. Complex QA Complex QA queries are often long and involve complex reasoning. StackExchange is platform where users pose high-quality questions requiring deep understanding and complex reasoning. Thus, we source user questions from StackExchange2 as our complex QA training queries. We select six sub-domains of StackExchange, including Biology, Earth Science, Economics, Robotics, StackOverflow, and Sustainability. The training data of listwise reranker usually comprises passage list with both relevant passages (i.e., positives) and irrelevant passages (i.e., negatives). The relevant passages for complex QA queries often contain key concepts that support or help answer the query. On StackExchange, relevant passages often exist in external documentation linked within answers of the user question. To mine the positives of each query, we first obtain the gold answer (accepted by the user and has at least one URL). Then we crawl all the documents of each URL in the gold answer and split them into passages (see the Appendix for the specific splitting method). After that, we employ strong model, Deepseek-R1 (abbreviated as R1), to perform the listwise positives selection which takes list of candidate passages, the query, and its gold answer as input and outputs the IDs of positives. The aim of inputting list of candidate passages is to allow R1 to comprehensively compare different passages, and providing the gold answer is to better assist R1 in understanding the query and judging relevance. After judging all the candidate passages, we obtain the positive set + and treat the rest as the negative set . To enhance the training passage list, we further mine some hard negatives that share similar topics with the query but can not be used to solve it. Specifically, we first use the Google Search API to retrieve the top 10 most similar docu2https://archive.org/download/stackexchange ments. Then we split these documents into passages and employ R1 to perform listwise hard negatives selection, which takes list of passages, the user question, and its gold answer as input and outputs the IDs of hard negatives. We denote the selected hard negative set as hard. Finally, we combine the collected passages of each query (P +, and hard) into passage list (capped at 20 passages, i.e., the window size of sliding windows) and use R1 to perform the listwise ranking, generating the reasoning chain and the reranked list for the query. So far, we have obtained two types of training labels: (1) pointwise labels (the binary relevance of each passage) and (2) listwise labels (reasoning chains and gold ranking lists), which will be used for our two-stage training approach. Note that to avoid revealing the gold answer in reasoning and maintain consistency with ReasonRanks training, we do not provide the gold answer when generating the reasoning chain using R1 (same for the following coding and math domains). Coding The relevant passages of coding-type queries usually share the same algorithm or similar logic as the query. In this part, we use the coding problems of the Leetcode dataset3 as our training queries. We use the corpus of the Leetcode dataset (including range of code-format passages) from the BRIGHT benchmark (Su et al. 2025) as the source for mining positives and negatives. Mining positives is challenging because they do not necessarily have high similarity with the query. To mine the positives, we first use the query to retrieve large passage set of size (K = 40) based on dense retriever E5-mistral-7binstruct4, and then perform the positives selection using R1 (same as the positives selection of Complex QA). After that, we randomly sample negatives from the unselected passages to form passage list of length 20, where each passage is assigned pointwise label (relevant or not). Finally, we apply R1 to perform listwise ranking on the passage list, obtaining the listwise label (reasoning chain and gold ranking list). Math When issuing math query, users often want to refer to math problems with similar solutions or related theorems. Following BRIGHT, we focus on two types of passage ranking tasks: (1) ranking passages (math problemsolution pairs) with similar problem-solving logic to the query (denoted as Math-Problem task) and (2) ranking passages (math theorems) used to solve the query (denoted as Math-Theorem task). We use math problems in MATH dataset (Hendrycks et al. 2021) as our training queries and sample 2K math questions per task from its training set as the training queries for the two tasks. For the Math-Problem task, we use the STEM question-solution corpus built by (Su et al. 2025) as the passage corpus. For the Math-Theorem task, we utilize theorem statements from ProofWiki5 as our passage corpus, which contains 20K math theorems. Using the same approaches for coding queries, we first retrieve the top-K passage set from the corresponding passage corpus using E5-mistral-7b-instruct and then use R1 to per3https://huggingface.co/datasets/greengerong/leetcode 4https://huggingface.co/intfloat/e5-mistral-7b-instruct 5https://proofwiki.org form positives selection and sample negatives to construct the training passage list. Finally, R1 is applied to perform listwise ranking on the passage list to obtain listwise labels. Web Search Beyond the above three domains, we also incorporate web search queries to ensure the models ranking ability in simple search scenarios. We sample 4K queries from the MSMARCO training set whose passages have been annotated with pointwise relevance labels. Following previous studies (Pradeep, Sharifymoghaddam, and Lin 2023a; Liu et al. 2024b), we use BM25 to retrieve top-20 passages and apply R1 for listwise ranking to obtain listwise labels. Self-Consistency Data Filtering Through the data synthesis above, we have obtained our training queries, passage lists, and corresponding labels, which consist of pointwise labels and listwise labels generated by R1. To ensure the quality of training labels, we propose selfconsistency data filtering mechanism. Inspired by LLM selfconsistency (Wang et al. 2023), we believe that the labels with higher self-consistency from R1 should have higher quality. Thus, we calculate the ranking metric NDCG@10 for the gold ranking lists in listwise labels using the pointwise labels and filter out training samples with NDCG@10 below threshold α. After that, we obtain quality-filtered dataset, which will be used for training. Detailed information about the training dataset is provided in the Appendix. Two-stage Training Framework In this section, based on our synthesized data, we propose two-stage training approach to empower our reasoningintensive listwise reranker with strong ranking ability. We first use the listwise labels to SFT the backbone LLM, enabling it to have an initial reasoning capability for listwise ranking. Then, we design novel multi-view ranking reward tailored for the nature of sliding windows in listwise ranking and apply RL to further enhance the ranking ability of our listwise reranker. Cold-Start SFT To help the backbone LLM learn to reason for listwise ranking, we utilize the listwise labels, which include both the reasoning chain and the reranked list, for SFT. The input to the backbone LLM consists of query and passage list. The training process optimizes the model by minimizing the standard language modeling loss, as shown in the equation below: = (cid:88) i=1 log(Pθ(yi x, y<i)), (2) where and represent the input prompt and the listwise label, respectively. We show the input prompt in the Appendix. Multi-view Ranking based RL In this section, we employ reinforcement learning to help the SFT reranker explore better reasoning chains and improve the performance of listwise ranking. The reward signal functions as the optimization target that directly affects the performance of the policy model during training. Previous studies (Liu et al. 2025; Zhang et al. 2025) merely utilize the ranking metric NDCG@10 as the Figure 3: An overview of our two-stage training framework. reward signal. However, we argue that such single-turn reward is suboptimal for sliding window-based listwise ranking. The sliding window strategy requires multi-turn sequential ranking, where optimizing NDCG for single window doesnt necessarily benefit the ranking of subsequent windows. For example, with window size 20 and step size of 10, ranking two relevant passages at positions 2 and 11 yields higher NDCG@10 (0.38) than placing them at 9 and 10. However, the latter ensures both passages remain in the top 10 and propagate to subsequent windows, which may yield better ranking performance in the end. Thus, besides NDCG@10, we propose to incorporate metric Recall@10 as part of our ranking reward. Furthermore, compared to using pointwise labels for NDCG@10 computation, we contend that the gold list in our listwise labels contains more granular ranking signals. Consequently, we use the rank-biased overlap (RBO) metric (Webber, Moffat, and Zobel 2010), which measures the ranking similarity as another supplementary ranking reward. The combination of NDCG@10, Recall@10, and RBO forms our multi-view ranking reward Rm. To ensure proper output structure, we implement format rewards considering two kinds of formats: (1) output format which ensures the presence of both <think> and <answer> tags, and (2) answer format that validates the content within <answer> tags adheres to the specified ranking list format (e.g., [4] > [2] > . . . ). The final reward is computed as follows: Rm, Both Formats are Good, 0, 1, Both Formats are Incorrect. Rm = NDCG@10 + ϕ Recall@10 + γ RBO, Only Output Format is Good, = RBO = (1 p) ylist (cid:88) d=1 pd1 1:d ylist 1:d , (3) (4) (5) where ϕ, γ, and are pre-defined parameters, and ylist represent the rollout ranking list and gold ranking list in our listwise label, respectively. Finally, we employ the RL algorithm GRPO (Shao et al. 2024) for optimization. During training, we sample group of output sequences = {y1, y2, . . . , yG} for each input x. Each sequence yi receives reward ri, which is then normalized within group to produce advantages ˆAi. The tokenlevel optimization objective is formulated as: JGRPO(θ) = 1 G (cid:88) i=1 1 yi yi (cid:88) t=1 (cid:16) min ri,t(θ) ˆAi,t, clip (ri,t(θ), 1 ϵ, 1 + ϵ) ˆAi,t (cid:17) βDKL, ri,t(θ) = πθ(yi,t x, yi,<t) πref(yi,t x, yi,<t) , DKL = DKL(πθ πref) (6) where ϵ and β are hyper-parameters."
        },
        {
            "title": "Experiment",
            "content": "Experimental Setup Evaluation Datasets We choose two reasoning-intensive IR benchmarks, BRIGHT (Su et al. 2025) and R2MED (Li, Zhou, and Liu 2025), for evaluation. BRIGHT consists of 12 datasets from diverse domains and is widely used to test the reasoning-intensive IR models. R2MED is benchmark designed for reasoning-driven medical retrieval, which contains 8 datasets. Baselines We compared two types of rerankers: nonreasoning rerankers and reasoning rerankers. As for non-reasoning rerankers, we choose RankT5 (3B) and RankZephyr (7B). As for reasoning rerankers, we compare with Rank1 (7B, 32B), Rank-R1 (7B, 14B), and Rank-K (32B). The details of the baselines is shown in the Appendix. As for the BRIGHT benchmark, we use strong reasoning dense model ReasonIR (8B) (Shao et al. 2025) as the initial retriever. Following previous studies (Shao et al. 2025;"
        },
        {
            "title": "Models",
            "content": "Econ. Earth. Rob. Bio. Psy. Stack. Sus. Leet. Pony AoPS TheoQ. TheoT. Avg. ReasonIR (8B) 32.65 43.00 20.82 43. 39.57 30.96 27.34 31.69 19.55 7. 33.93 36.68 30.59 Non-reasoning reranker rankT5 (3B) RankZephyr (7B) 11.35 19.87 Reasoning reranker Rank-R1 (7B) Rank-R1 (14B) Rank1 (7B) Rank1 (32B) Rank-K (32B) ReasonRank (7B) ReasonRank (32B) 20.19 27.39 25.34 25.43 30.09 35.05 36.64 22.11 17.36 10.94 12.35 13.62 34. 11.40 24.72 11.35 13.35 15.96 22.34 27.45 29.29 38.05 32.37 9.24 6. 27.91 38.73 38.91 37.97 39.78 47.75 48.90 18.13 23.11 16.77 17.14 26.60 31.22 33.88 36.70 44.45 39.91 42.43 50.64 56.70 58. 30.22 37.10 35.32 34.85 43.45 47.77 53.27 11.32 27.82 24.81 23.82 29.87 32.47 38.68 29.28 36.77 33.47 31.18 35.17 40.87 45. 17.13 21.27 12.71 12.20 27.23 23.17 25.78 9.35 19.23 28.14 40.98 22.80 24.95 20.93 3.23 8.80 2.58 4.79 7.61 7.68 9. 18.27 28.98 14.43 31.66 30.67 29.28 37.06 39.49 38.46 9.46 30.07 16.60 22.64 28.98 39.53 38.18 39.95 41. 41.80 46.28 20.57 29.66 27.23 28.34 32.61 35.74 38.03 Table 1: The results (NDCG@10) on the BRIGHT benchmark. All baselines rerank the ReasonIR-retrieved top-100 passages. The top two rerankers are highlighted in bold and underlined."
        },
        {
            "title": "Models",
            "content": "E5-mistral (7B)"
        },
        {
            "title": "Biology",
            "content": "18.28 Bioin. 41.47 MedS. MedE. MedD. PMCT. PMCC. IIYiC. 41.01 6.44 11. 19.81 30.97 21.37 Non-reasoning reranker RankT5 (3B) RankZephyr (7B) 13.20 22.86 Reasoning reranker Rank-R1 (7B) Rank-R1 (14B) Rank1 (7B) Rank1 (32B) Rank-K (32B) ReasonRank (7B) ReasonRank (32B) 34.04 38.78 32.64 31.84 32.94 46.80 45.56 32.84 43.07 51.63 53.81 55.57 61.65 53.94 59.70 67. 23.38 48.24 50.96 57.94 54.74 59.74 51.47 60.11 63.45 2.09 6.97 12.79 15.15 12.78 16.56 11.41 16.48 18. 4.18 10.45 21.96 25.49 19.98 26.90 22.93 24.92 30.60 0.61 26.64 34.79 40.57 34.40 41.31 34.44 39.21 41. 14.55 7.78 31.70 42.52 30.17 45.56 38.38 39.13 46.11 12.39 14.59 25.08 29.63 18.15 29.49 26.24 29.85 29. Avg. 23.84 12.91 22.58 32.87 37.99 32.30 39.13 33.97 39.53 42.85 Table 2: The results (NDCG@10) on the R2MED benchmark. All baselines rerank E5-mistral-retrieved top-100 passages. Yang et al. 2025b), we use the GPT4-rewritten queries (provided in the test set) as test queries due to better retrieval performance and use the original test query for passage reranking. As for the R2MED benchmark, we use E5-Mistral-7BInstruct as the initial retrievers and original test query for retrieval and reranking. We rerank the top 100 retrieved passages and use NDCG@10 as the evaluation metric. threshold α of Implementation Details The selfconsistency data filtering is set as 0.4. After data filtering, we split our training data equally for cold-start SFT and multi-reward ranking based RL. As for post training, we use two backbone LLMs with different sizes: Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct. The Qwen2.5-32B-Instruct is trained with LoRA adapter (Hu et al. 2021). The hyperparameters ϕ and γ used in the multiview ranking reward are set as 0.2 and 0.1, respectively. Due to space limitations, we put more details in the Appendix. Main Results We evaluate our ReasonRank (7B and 32B) on BRIGHT and R2MED, and show the results in Table 1 and Table 2, respectively. From the results, we have the following observations: (1) Our ReasonRank (7B and 32B) demonstrate superior performance compared with all baselines on Avg. of two benchmarks. Notably, our ReasonRank (32B) outperforms the best baseline Rank-K (32B) on BRIGHT by about 5 points and Rank1 (32B) on R2MED by about 4 points. Besides, our ReasonRank (7B) even outperforms the 32Bscale baselines significantly. For example, it surpasses RankK (32B) by 3 and 9 points on BRIGHT and R2MED, respectively. These results demonstrate the effectiveness of our ranking data synthesis and two-stage training framework. (2) Existing baselines struggle in reasoning-intensive reranking. In the BRIGHT benchmark, the baselines, except for Rank-K (32B), can hardly improve the initial retrieval results. In the R2MED benchmark, the two nonreasoning reranker underperforms the retriever E5-Mistral on the Avg. metric. This suggests that traditional training data and existing training methods struggle to produce an effective reasoning-intensive reranker. (3) The performance of the reranker scales with the model size. For example, on the BRIGHT benchmark, Rank-R1 (32B) exceeds Rank-R1 (7B) by 9 points (Avg.), and our ReasonRank (32B) exceeds ReasonRank (7B) by about 2.3 points (Avg.). This indicates that larger models"
        },
        {
            "title": "Models",
            "content": "ReasonRank (7B) Training Data only MSMARCO w/o Quality Normalization Training Approach w/o SFT (only RL) w/o RL (only SFT) w/o Rm non-reasoning SFT BRIGHT Avg. 35.74 - 30.08 34.20 28.69 33.15 34.20 32.96 -5.66 -1.54 -7.05 -2.59 -1.54 -2.78 Table 3: Ablation study based on ReasonRank (7B). have stronger reasoning and ranking capabilities. Ablation Study To demonstrate the effectiveness of different components in our methods, we conduct ablation experiments from two different aspects: training data and training approach. We experiment on the BRIGHT benchmark with ReasonRank (7B) and show the results in Table 3. Detailed results of each dataset are shown in the Appendix. From the results, we have the following observations: (1) Training Data. Only using the MSMARCO subset in our training data (denoted as only MSMARCO) shows significant performance drop (5.66 points), indicating the necessity of constructing training data for diverse reasoning-intensive domains. Besides, the performance drop of abandoning self-consistency data filtering (w/o SelfConsistency) also demonstrates its effectiveness. (2) Training Approach. We first validate the effectiveness of our two-stage training framework by removing the cold-start SFT (w/o SFT (only RL)) and the RL stage (w/o RL (only SFT)). Removing cold-start SFT leads to decrease of 7.05 points, indicating that cold-start SFT with reasoning chains is crucial for the backbone LLM to learn reasoning for listwise ranking. Removing RL also results in 2.59-point drop, demonstrating that further reinforcement learning can enhance the ranking performance. We also remove our multi-view ranking reward and use only NDCG@10 as the ranking reward (w/o Rm), and the decrease in performance proves its effectiveness. Finally, to confirm the effectiveness of reasoning in listwise ranking, we fine-tuned non-reasoning SFT reranker using only the gold rankings from our training data (same as RankZephyr). The 2.78-point performance gap shows that reasoning significantly enhances the ReasonRanks effectiveness, proving the importance of reasoning for ranking. Traditional IR Benchmark To evaluate the generalization of ReasonRank, we conduct experiments on the traditional IR benchmark BEIR (Thakur et al. 2021). Considering the large size of some test sets in BEIR, following previous studies (Sun et al. 2023; Liu et al. 2024b), we selected 7 datasets with smaller number of queries from BEIR. We choose BM25 as our retriever and compare with several competitive baselines: RankZephyr"
        },
        {
            "title": "Models",
            "content": "BM25 RankZephyr (7B) Rank-R1 (14B) Rank1 (32B) Rank-K (32B) ReasonRank (7B) ReasonRank (32B) BEIR Avg. 43.74 54.14 54.61 50.99 48. 54.35 55.44 - +10.40 +10.87 +7.25 +4.60 +10.61 +11.70 Table 4: The averaged NDCG@10 on 7 BEIR datasets, including Covid, DBPedia, SciFact, NFCorpus, Signal, Robust04 and News. Figure 4: Ranking latency (seconds per query) of Rank1 (7B) and ReasonRank (7B) on eights datasets. (7B), Rank-R1 (14B), Rank1 (32B), and Rank-K (32B). From the results in Table 4, we can see that ReasonRank (32B) outperforms all baselines on Avg., demonstrating its strong generalization ability on the traditional IR benchmark. Besides, the gap of our ReasonRank (7B and 32B) over the baselines is smaller than that on BRIGHT and R2MED. This might be because the queries in BEIR are much simpler and do not require complex reasoning."
        },
        {
            "title": "Efficiency Analysis",
            "content": "ReasonRank improves ranking performance through reasoning but also introduces additional latency. In this section, we test the time latency of ReasonRank (7B) and compare it with the pointwise reranker Rank1 (7B). We select 8 datasets from BRIGHT and rerank the top-100 ReasonIR-retrieved passages on 4*A800 80G GPUs with the vLLM framework. As shown in Figure 4, surprisingly, our listwise ReasonRank is 2-2.7 faster than pointwise Rank1, which is contrary to conclusions from non-reasoning rerankers (Zhuang et al. 2024). This efficiency stems from Rank1 generating reasoning chain for each passage, while ReasonRank processes multiple passages at time with only one reasoning chain, significantly decreasing the number of output tokens."
        },
        {
            "title": "Models",
            "content": "BRIGHT Avg. ReasonIR (8B) ReasonRank (32B) RaDeR + BM25 (Hybrid) (1) ReasonRank (32B) (2) ReasonRank (32B) 30.59 38.03 35.71 39.08 40.80 - +7. - +3.37 +5.09 Table 5: Further ranking enhancement on BRIGHT. Both retrievers ReasonIR (8B) and RaDeR + BM25 (Hybrid) use GPT4-rewritten queries for retrieval. Further Ranking Enhancement Although our ReasonRank has already achieved superior results compared to existing rerankers, we believe its ranking performance can be further improved by adjusting some ranking settings. Previous studies (Pradeep, Sharifymoghaddam, and Lin 2023a,b) have revealed that the quality of retrieval results has an impact on ranking performance: better retrieval results can lead to better ranking performance to some extent. In this section, we used the retrieval results provided by RaDeR (Das, Nualláin, and Rahimi 2025), which combines the retrieval results of the dense retriever RaDeR and sparse retriever BM25 and achieves higher retrieval quality than ReasonIR. We apply ReasonRank (32B) as our reranker to rerank the top-100 retrieved passages. As shown in Figure 5 (1), with better retrieval results, the ranking performance further improved to 39.08 (1 point higher than the ranking results based on ReasonIR), which aligns with the previous conclusion. Furthermore, previous work (Sun et al. 2023) pointed out that the size and stride of the sliding window also affect listwise ranking performance. In this section, different from the original settings with window size of 20 and stride of 10, we experimented with new sliding window parameters with window size of 10 and stride of 5 (denoted as ReasonRank (32B)). The results in Table 5 (2) show that this change brought about 1.7-point improvement. This may be because smaller window size can reduce the difficulty of modeling context and ranking in listwise ranking, thereby improving performance. The detailed results on all datasets are shown in Table 10 of the Appendix. Remarkably, with an average score of 40.6, ReasonRank (32B) has achieved SOTA performance on BRIGHT leaderboard. Conclusion In this paper, we propose ReasonRank, state-of-the-art reasoning-intensive passage reranker. To train ReasonRank, we first propose reasoning-intensive ranking data synthesis framework to generate high-quality training data. Then, we propose two-state training framework which includes cold-start SFT and multi-view ranking based RL. Extensive experiments demonstrate the superior performance as well as the efficiency of our ReasonRank. References Chen, Y.; Liu, Q.; Zhang, Y.; Sun, W.; Shi, D.; Mao, J.; and Yin, D. 2024. TourRank: Utilizing Large Language Models for Documents Ranking with Tournament-Inspired Strategy. CoRR, abs/2406.11678. Dao, T. 2023. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. CoRR. Das, D.; Nualláin, S. Ó.; and Rahimi, R. 2025. RaDeR: CoRR, Reasoning-aware Dense Retrieval Models. abs/2505.18405. DeepSeek-AI; Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; Zhang, X.; Yu, X.; Wu, Y.; Wu, Z. F.; Gou, Z.; Shao, Z.; Li, Z.; Gao, Z.; Liu, A.; Xue, B.; Wang, B.; Wu, B.; Feng, B.; Lu, C.; Zhao, C.; Deng, C.; Zhang, C.; Ruan, C.; Dai, D.; Chen, D.; Ji, D.; Li, E.; Lin, F.; Dai, F.; Luo, F.; Hao, G.; Chen, G.; Li, G.; Zhang, H.; Bao, H.; Xu, H.; Wang, H.; Ding, H.; Xin, H.; Gao, H.; Qu, H.; Li, H.; Guo, J.; Li, J.; Wang, J.; Chen, J.; Yuan, J.; Qiu, J.; Li, J.; Cai, J. L.; Ni, J.; Liang, J.; Chen, J.; Dong, K.; Hu, K.; Gao, K.; Guan, K.; Huang, K.; Yu, K.; Wang, L.; Zhang, L.; Zhao, L.; Wang, L.; Zhang, L.; Xu, L.; Xia, L.; Zhang, M.; Zhang, M.; Tang, M.; Li, M.; Wang, M.; Li, M.; Tian, N.; Huang, P.; Zhang, P.; Wang, Q.; Chen, Q.; Du, Q.; Ge, R.; Zhang, R.; Pan, R.; Wang, R.; Chen, R. J.; Jin, R. L.; Chen, R.; Lu, S.; Zhou, S.; Chen, S.; Ye, S.; Wang, S.; Yu, S.; Zhou, S.; Pan, S.; and Li, S. S. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. CoRR, abs/2501.12948. Dong, G.; Chen, Y.; Li, X.; Jin, J.; Qian, H.; Zhu, Y.; Mao, H.; Zhou, G.; Dou, Z.; and Wen, J. 2025a. Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning. CoRR, abs/2505.16410. Dong, G.; Mao, H.; Ma, K.; Bao, L.; Chen, Y.; Wang, Z.; Chen, Z.; Du, J.; Wang, H.; Zhang, F.; et al. 2025b. Agentic Reinforced Policy Optimization. arXiv preprint arXiv:2507.19849. Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.; Goyal, A.; Hartshorn, A.; Yang, A.; Mitra, A.; Sravankumar, A.; Korenev, A.; Hinsvark, A.; Rao, A.; Zhang, A.; Rodriguez, A.; Gregerson, A.; Spataru, A.; Rozière, B.; Biron, B.; Tang, B.; Chern, B.; Caucheteux, C.; Nayak, C.; Bi, C.; Marra, C.; McConnell, C.; Keller, C.; Touret, C.; Wu, C.; Wong, C.; Ferrer, C. C.; Nikolaidis, C.; Allonsius, D.; Song, D.; Pintz, D.; Livshits, D.; Esiobu, D.; Choudhary, D.; Mahajan, D.; Garcia-Olano, D.; Perino, D.; Hupkes, D.; Lakomkin, E.; AlBadawy, E.; Lobanova, E.; Dinan, E.; Smith, E. M.; Radenovic, F.; Zhang, F.; Synnaeve, G.; Lee, G.; Anderson, G. L.; Nail, G.; Mialon, G.; Pang, G.; Cucurell, G.; Nguyen, H.; Korevaar, H.; Xu, H.; Touvron, H.; Zarov, I.; Ibarra, I. A.; Kloumann, I. M.; Misra, I.; Evtimov, I.; Copet, J.; Lee, J.; Geffert, J.; Vranes, J.; Park, J.; Mahadeokar, J.; Shah, J.; van der Linde, J.; Billock, J.; Hong, J.; Lee, J.; Fu, J.; Chi, J.; Huang, J.; Liu, J.; Wang, J.; Yu, J.; Bitton, J.; Spisak, J.; Park, J.; Rocca, J.; Johnstun, J.; Saxe, J.; Jia, J.; Alwala, K. V.; Upasani, K.; Plawiak, K.; Li, K.; Heafield, K.; Stone, K.; and et al. 2024. The Llama 3 Herd of Models. CoRR, abs/2407.21783. Hendrycks, D.; Burns, C.; Kadavath, S.; Arora, A.; Basart, S.; Tang, E.; Song, D.; and Steinhardt, J. 2021. Measuring Mathematical Problem Solving With the MATH Dataset. In NeurIPS Datasets and Benchmarks. Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; and Chen, W. 2021. LoRA: Low-Rank Adaptation of Large Language Models. CoRR, abs/2106.09685. Jaech, A.; Kalai, A.; Lerer, A.; Richardson, A.; El-Kishky, A.; Low, A.; Helyar, A.; Madry, A.; Beutel, A.; Carney, A.; Iftimie, A.; Karpenko, A.; Passos, A. T.; Neitz, A.; Prokofiev, A.; Wei, A.; Tam, A.; Bennett, A.; Kumar, A.; Saraiva, A.; Vallone, A.; Duberstein, A.; Kondrich, A.; Mishchenko, A.; Applebaum, A.; Jiang, A.; Nair, A.; Zoph, B.; Ghorbani, B.; Rossen, B.; Sokolowsky, B.; Barak, B.; McGrew, B.; Minaiev, B.; Hao, B.; Baker, B.; Houghton, B.; McKinzie, B.; Eastman, B.; Lugaresi, C.; Bassin, C.; Hudson, C.; Li, C. M.; de Bourcy, C.; Voss, C.; Shen, C.; Zhang, C.; Koch, C.; Orsinger, C.; Hesse, C.; Fischer, C.; Chan, C.; Roberts, D.; Kappler, D.; Levy, D.; Selsam, D.; Dohan, D.; Farhi, D.; Mely, D.; Robinson, D.; Tsipras, D.; Li, D.; Oprica, D.; Freeman, E.; Zhang, E.; Wong, E.; Proehl, E.; Cheung, E.; Mitchell, E.; Wallace, E.; Ritter, E.; Mays, E.; Wang, F.; Such, F. P.; Raso, F.; Leoni, F.; Tsimpourlas, F.; Song, F.; von Lohmann, F.; Sulit, F.; Salmon, G.; Parascandolo, G.; Chabot, G.; Zhao, G.; Brockman, G.; Leclerc, G.; Salman, H.; Bao, H.; Sheng, H.; Andrin, H.; Bagherinezhad, H.; Ren, H.; Lightman, H.; Chung, H. W.; Kivlichan, I.; OConnell, I.; Osband, I.; Gilaberte, I. C.; and Akkaya, I. 2024. OpenAI o1 System Card. CoRR, abs/2412.16720. Li, L.; Zhou, X.; and Liu, Z. 2025. R2MED: BenchCoRR, mark for Reasoning-Driven Medical Retrieval. abs/2505.14558. Liang, P.; Bommasani, R.; Lee, T.; Tsipras, D.; Soylu, D.; Yasunaga, M.; Zhang, Y.; Narayanan, D.; Wu, Y.; Kumar, A.; Newman, B.; Yuan, B.; Yan, B.; Zhang, C.; Cosgrove, C.; Manning, C. D.; Ré, C.; Acosta-Navas, D.; Hudson, D. A.; Zelikman, E.; Durmus, E.; Ladhak, F.; Rong, F.; Ren, H.; Yao, H.; Wang, J.; Santhanam, K.; Orr, L. J.; Zheng, L.; Yüksekgönül, M.; Suzgun, M.; Kim, N.; Guha, N.; Chatterji, N. S.; Khattab, O.; Henderson, P.; Huang, Q.; Chi, R.; Xie, S. M.; Santurkar, S.; Ganguli, S.; Hashimoto, T.; Icard, T.; Zhang, T.; Chaudhary, V.; Wang, W.; Li, X.; Mai, Y.; Zhang, Y.; and Koreeda, Y. 2022. Holistic Evaluation of Language Models. CoRR, abs/2211.09110. Liu, Q.; Wang, B.; Wang, N.; and Mao, J. 2024a. Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models. CoRR, abs/2406.14848. Liu, W.; Ma, X.; Zhu, Y.; Su, L.; Wang, S.; Yin, D.; and Dou, Z. 2025. CoRanking: Collaborative Ranking with Small and Large Ranking Agents. CoRR, abs/2503.23427. Liu, W.; Ma, X.; Zhu, Y.; Zhao, Z.; Wang, S.; Yin, D.; and Dou, Z. 2024b. Sliding Windows Are Not the End: Exploring Full Ranking with Long-Context Large Language Models. CoRR, abs/2412.14574. Liu, W.; Zhu, Y.; and Dou, Z. 2024. DemoRank: Selecting Effective Demonstrations for Large Language Models in Ranking Task. CoRR, abs/2406.16332. Luo, J.; Chen, X.; He, B.; and Sun, L. 2024. PRP-Graph: Pairwise Ranking Prompting to LLMs with Graph Aggregation for Effective Text Re-ranking. In ACL (1), 57665776. Association for Computational Linguistics. Nguyen, T.; Rosenberg, M.; Song, X.; Gao, J.; Tiwary, S.; Majumder, R.; and Deng, L. 2016. MS MARCO: Human Generated MAchine Reading COmprehension Dataset. In Besold, T. R.; Bordes, A.; dAvila Garcez, A. S.; and Wayne, G., eds., Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org. Pradeep, R.; Sharifymoghaddam, S.; and Lin, J. 2023a. RankVicuna: Zero-Shot Listwise Document Reranking CoRR, with Open-Source Large Language Models. abs/2309.15088. Pradeep, R.; Sharifymoghaddam, S.; and Lin, J. 2023b. RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is Breeze! CoRR, abs/2312.02724. Qin, X.; Bai, J.; Li, J.; Jia, Z.; and Zheng, Z. 2025. TongSearch-QR: Reinforced Query Reasoning for Retrieval. CoRR, abs/2506.11603. Qin, Z.; Jagerman, R.; Hui, K.; Zhuang, H.; Wu, J.; Shen, J.; Liu, T.; Liu, J.; Metzler, D.; Wang, X.; and Bendersky, M. 2023. Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting. CoRR, abs/2306.17563. Rasley, J.; Rajbhandari, S.; Ruwase, O.; and He, Y. 2020. DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. KDD 20. Sachan, D. S.; Lewis, M.; Joshi, M.; Aghajanyan, A.; Yih, W.; Pineau, J.; and Zettlemoyer, L. 2022. Improving Passage In GoldRetrieval with Zero-Shot Question Generation. berg, Y.; Kozareva, Z.; and Zhang, Y., eds., Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 37813797. Association for Computational Linguistics. Shao, R.; Qiao, R.; Kishore, V.; Muennighoff, N.; Lin, X. V.; Rus, D.; Low, B. K. H.; Min, S.; Yih, W.; Koh, P. W.; and Zettlemoyer, L. 2025. ReasonIR: Training Retrievers for Reasoning Tasks. CoRR, abs/2504.20595. Shao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Zhang, M.; Li, Y. K.; Wu, Y.; and Guo, D. 2024. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. CoRR, abs/2402.03300. Song, H.; Jiang, J.; Tian, W.; Chen, Z.; Wu, Y.; Zhao, J.; Min, Y.; Zhao, W. X.; Fang, L.; and Wen, J. 2025. R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning. CoRR, abs/2505.17005. Su, H.; Yen, H.; Xia, M.; Shi, W.; Muennighoff, N.; Wang, H.; Liu, H.; Shi, Q.; Siegel, Z. S.; Tang, M.; Sun, R.; Yoon, J.; Arik, S. Ö.; Chen, D.; and Yu, T. 2025. BRIGHT: Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval. In ICLR. OpenReview.net. Yoon, S.; Choi, E.; Kim, J.; Yun, H.; Kim, Y.; and Hwang, S. 2024. ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval. In ACL (1), 22872308. Association for Computational Linguistics. Yoon, S.; Kim, G.; Cho, G.; and Hwang, S. 2025. AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking. CoRR, abs/2505.18512. Zhang, L.; Wang, B.; Qiu, X.; Reddy, S.; and Agrawal, A. 2025. REARANK: Reasoning Re-ranking Agent via Reinforcement Learning. CoRR, abs/2505.20046. Zheng, Y.; Zhang, R.; Zhang, J.; Ye, Y.; Luo, Z.; and Ma, Y. 2024. LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models. CoRR, abs/2403.13372. Zhu, Y.; Yuan, H.; Wang, S.; Liu, J.; Liu, W.; Deng, C.; Dou, Z.; and Wen, J. 2023. Large Language Models for Information Retrieval: Survey. CoRR, abs/2308.07107. Zhu, Y.; Zhang, P.; Zhang, C.; Chen, Y.; Xie, B.; Liu, Z.; Wen, J.; and Dou, Z. 2024. INTERS: Unlocking the Power of Large Language Models in Search with Instruction TunIn Ku, L.; Martins, A.; and Srikumar, V., eds., Proing. ceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, 27822809. Association for Computational Linguistics. Zhuang, H.; Qin, Z.; Jagerman, R.; Hui, K.; Ma, J.; Lu, J.; Ni, J.; Wang, X.; and Bendersky, M. 2023. RankT5: FineTuning T5 for Text Ranking with Ranking Losses. In Chen, H.; Duh, W. E.; Huang, H.; Kato, M. P.; Mothe, J.; and Poblete, B., eds., Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023, 23082313. ACM. Zhuang, S.; Ma, X.; Koopman, B.; Lin, J.; and Zuccon, G. 2025. Rank-R1: Enhancing Reasoning in LLM-based Document Rerankers via Reinforcement Learning. CoRR, abs/2503.06034. Zhuang, S.; Zhuang, H.; Koopman, B.; and Zuccon, G. 2024. Setwise Approach for Effective and Highly Efficient Zeroshot Ranking with Large Language Models. In SIGIR, 38 47. ACM. Sun, W.; Yan, L.; Ma, X.; Wang, S.; Ren, P.; Chen, Z.; Yin, D.; and Ren, Z. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents. In Bouamor, H.; Pino, J.; and Bali, K., eds., Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, 1491814937. Association for Computational Linguistics. Thakur, N.; Reimers, N.; Rücklé, A.; Srivastava, A.; and Gurevych, I. 2021. BEIR: Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models. In NeurIPS Datasets and Benchmarks. Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G. 2023. LLaMA: Open and Efficient Foundation Language Models. CoRR, abs/2302.13971. Wang, L.; Yang, N.; Huang, X.; Yang, L.; Majumder, R.; and Wei, F. 2024. Improving Text Embeddings with Large Language Models. In ACL (1), 1189711916. Association for Computational Linguistics. Wang, X.; Wei, J.; Schuurmans, D.; Le, Q. V.; Chi, E. H.; Narang, S.; Chowdhery, A.; and Zhou, D. 2023. SelfConsistency Improves Chain of Thought Reasoning in Language Models. In ICLR. OpenReview.net. Webber, W.; Moffat, A.; and Zobel, J. 2010. similarity measure for indefinite rankings. ACM Trans. Inf. Syst., 28(4): 20:120:38. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E. H.; Le, Q.; and Zhou, D. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. CoRR, abs/2201.11903. Weller, O.; Ricci, K.; Yang, E.; Yates, A.; Lawrie, D. J.; and Durme, B. V. 2025. Rank1: Test-Time Compute for Reranking in Information Retrieval. CoRR, abs/2502.18418. Xiao, C.; Hudson, G. T.; and Moubayed, N. A. 2024. CoRR, RAR-b: Reasoning as Retrieval Benchmark. abs/2404.06347. Yang, A.; Li, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Gao, C.; Huang, C.; Lv, C.; Zheng, C.; Liu, D.; Zhou, F.; Huang, F.; Hu, F.; Ge, H.; Wei, H.; Lin, H.; Tang, J.; Yang, J.; Tu, J.; Zhang, J.; Yang, J.; Yang, J.; Zhou, J.; Zhou, J.; Lin, J.; Dang, K.; Bao, K.; Yang, K.; Yu, L.; Deng, L.; Li, M.; Xue, M.; Li, M.; Zhang, P.; Wang, P.; Zhu, Q.; Men, R.; Gao, R.; Liu, S.; Luo, S.; Li, T.; Tang, T.; Yin, W.; Ren, X.; Wang, X.; Zhang, X.; Ren, X.; Fan, Y.; Su, Y.; Zhang, Y.; Zhang, Y.; Wan, Y.; Liu, Y.; Wang, Z.; Cui, Z.; Zhang, Z.; Zhou, Z.; and Qiu, Z. 2025a. Qwen3 Technical Report. CoRR, abs/2505.09388. Yang, E.; Yates, A.; Ricci, K.; Weller, O.; Chari, V.; Durme, B. V.; and Lawrie, D. J. 2025b. Rank-K: Test-Time Reasoning for Listwise Reranking. CoRR, abs/2505.14432. Yao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.; Cao, Y.; and Narasimhan, K. 2023. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. CoRR, abs/2305.10601."
        },
        {
            "title": "Data Num",
            "content": "Benchmarks In our experiments, we utilize three IR benchmarks for evaluation: BRIGHT (Su et al. 2025), R2MED (Li, Zhou, and Liu 2025), and BEIR (Thakur et al. 2021). Each of these benchmarks is instrumental in assessing different aspects of our retrieval models, ensuring comprehensive evaluation across varied scenarios. The detailed introduction of each benchmark is as follows: BRIGHT: The BRIGHT benchmark is designed to challenge information retrieval systems with complex queries that necessitate deep reasoning capabilities. Unlike traditional benchmarks that focus on simple keyword or semantic matches, BRIGHT comprises 1,384 queries drawn from diverse domains such as economics, psychology, mathematics, and coding. These queries are sourced from both naturally occurring data and curated human inputs, making the benchmark robust test for advanced retrieval models. The requirement for thorough reasoning to identify relevant documents makes BRIGHT particularly demanding, as it pushes models to go beyond surface-level matching and engage in deeper cognitive processing. R2MED: R2MED is specialized benchmark for medical information retrieval, emphasizing the importance of reasoning in retrieving relevant medical information. It features 876 queries that cover three main tasks: Q&A reference retrieval, clinical evidence retrieval, and clinical case retrieval. These tasks span wide array of medical scenarios and body systems, highlighting the necessity for retrieval systems to understand and process medical information accurately. R2MED underscores the critical role reasoning plays in medical decision-making, challenging retrieval systems to move beyond simple lexical or semantic matching to support complex clinical judgments and evidence-based practice. BEIR: BEIR is comprehensive benchmark designed to evaluate the performance of information retrieval systems across 18 diverse datasets. These datasets encompass variety of tasks, including fact-checking, question answering, and more, allowing for thorough assessment of models generalization capabilities. BEIR is particularly focused on the ability of retrieval systems to adapt across different domains and tasks, emphasizing the need for balance between performance and computational efficiency. By encouraging the development of robust and adaptable retrieval systems, BEIR serves as pivotal benchmark in the advancement of information retrieval technologies. Baselines The baselines we used for comparison are as follows, each selected for its unique strengths and capabilities in specific retrieval or reranking tasks: ReasonIR (Shao et al. 2025): ReasonIR (8B) is reasoning-intensive retriever training on reasoning-intensive synthetic data based on LLAMA3.1-8B (Touvron et al. Complex QA Coding Math Web Search Biology Earth Science Economics Robotics Sustainable Living Stackoverflow Leetcode Math-QA Math-Theorem MS MARCO 1700 566 787 451 147 1741 1633 1726 1673 3093 Table 6: The statistics of our synthesized reasoning-intensive training data (totally 13.5k) on four domains, including Complex QA, Coding, Math and Web search. 2023). This baseline is particularly adept at handling complex queries that require in-depth reasoning, making it an ideal choice as our initial retriever on the BRIGHT benchmark. E5-mistral-7b-instruct (Wang et al. 2024): E5-mistral7b-instruct is fine-tuned on mixture of multilingual datasets and has some multilingual capability. We use it as our initial retriever on R2MED benchmark. RankT5 (Zhuang et al. 2023): RankT5 is reranker that leverages the T5 architecture in pointwise manner, optimized through ranking loss function. RankZephyr (Pradeep, Sharifymoghaddam, and Lin 2023b): This listwise re-ranker is distilled from the powerful models GPT-3.5 and GPT-4, utilizing sophisticated dualphase training framework. During inference, it employs sliding window method for listwise reranking. Rank1 (Weller et al. 2025): Rank1 functions as pointwise reasoning reranker, distilled from the R1 model. It computes relevance scores by evaluating the probability of the final token being true or false, providing nuanced approach to ranking that is particularly effective in scenarios requiring detailed reasoning. Rank-R1 (Zhuang et al. 2025): Rank-R1 is setwise reasoning reranker developed through GRPO reinforcement learning. In the inference phase, it identifies the most pertinent passage from set and employs heap sort to reorder all passages. Rank-K (Yang et al. 2025b): Rank-K represents listwise reasoning reranker distilled from R1, trained on the MSMARCO dataset with QwQ 32B as its backbone. It utilizes sliding window strategy during inference to rearrange all passages, which allows it to achieve trade-off between ranking effectiveness and efficiency. Implementation Details To help researchers better understand our approach, we have provided comprehensive and detailed explanation of the implementation details of the Reasoning-intensive Ranking Data Synthesis and the Two-Stage Training Framework here. Details about Reasoning-intensive Ranking Data Synthesis Prompt of Hard Negatives Selection for Complex Queries using R1 In this part, we will introduce the construction of our training data. After constructing our data from multiple domains, we apply self-consistency data filtering mechanism to filter low-quality training data. The statistics of final training data are shown in Table 6. Next, we will elaborate on the details of constructing data for each domain. Complex QA When splitting long documents into passages, we use simple heuristics with separators such as double new lines, without making extra assumptions about the file structure. This approach allows for flexibility and adaptability to various document formats, ensuring that the passages remain coherent and contextually meaningful. When using R1 to perform listwise positives selection, we input list of candidate passages, the query, and its gold answer to R1 and output the IDs of positives. The prompt is shown as follows: Prompt of listwise Positives Selection for Complex Queries using R1 You are an expert in evaluating the relevance of passages to stackexchange query. passage is considered relevant to the query if it helps reason through the query with critical concepts or theories and contains useful information to help users derive the answer. Crucially, avoid selecting passages that: - Merely mention keywords or entities from the query/answer but fail to provide explanatory details. - Are generic descriptions, background facts, or tangential information that do not contribute to reasoning about the querys specific questions. will provide you with query, the answer of the query and list of passages to evaluate. Each passage is indicated by number identifier []. Please output the identifiers of the relevant passages. The output format should be [] [], e.g., [2] [4]. If there are no relevant passages, output \"None\". Only output the relevant passage identifiers, do not output irrelevant passage identifiers and do not explain. Here are the Query, the Answer to the Query and list of passages: Query: {query}. Answer: {answer_content} Passage [1]: {passage_1} Passage [2]: {passage_2} (more passages)... You are an expert in finding hard negative passages for given query. Given query, the answer to the query and list of passages, you need to find the hard negative passages from these passages. The hard negative passage contains some relevant information with superficial lexical overlapping, but it should be not helpful to address the query with critical concepts or theories and does not contain useful information to help users derive the answer. will provide you with query, the answer of the query, and list of passages to evaluate. Each passage is indicated by number identifier []. Please output the identifiers of the hard negative passages. The output format should be [] [], e.g., [2] [4]. If there are no hard negative passages, output \"None\". Only output the hard negative passage identifiers, do not output relevant passage identifiers and do not explain. Here are the Query, the Answer to the Query, and list of passages: Query: {query} Answer: {answer_content} Passage [1]: {passage_1} Passage [2]: {passage_2} (more passages)... After obtaining the training candidate passages, we use R1 to perform listwise ranking on these passages using the following prompt (the same for the subsequent data synthesis for coding, math, and web search): Prompt for R1 Listwise Passage Ranking User: You are an intelligent assistant that can rank passages based on their relevance to the query. will provide you with {num} passages, each indicated by numerical identifier []. Rank the passages based on their relevance to the query: {query}. Assistant: Okay, please provide the passages. User: [1] {passage 1} Assistant: Received passage [1] User: [2] {passage 2} Assistant: Received passage [2] (more passages)... User: Search Query: {query}. Rank the {num} passages above based on their relevance to the search query. The passages should be listed in descending order using identifiers. The most relevant passages should be listed first. The output format should be [] > [], e.g., [4] > [2]. Only response the ranking results, do not say any word or explain. As for the hard negatives selection by R1, we input list of candidate passages, the query, and its gold answer to the model and output the IDs of hard negatives. The prompt we used is shown as follows: Coding As for coding queries, we apply the following prompt for mining positives: Prompt of positives selection for coding query using R1 Prompt of positives selection for Math-Theorem using R1 You are an expert in evaluating the relevance of passages to query (coding problem). The relevance between the query and relevant passage is defined by whether the query either requires the corresponding syntax documentation in the passage or involves the same algorithm and/or data structure. will provide you with query, the solution code of the query, and list of passages to evaluate. Each passage is indicated by number identifier []. Please output the identifiers of the relevant passages. The output format should be [] [], e.g., [2] [4]. If there are no relevant passages, output \"None\". Only output the relevant passage identifiers, do not output irrelevant passage identifiers and do not provide any explanation. Here are the Query, the Solution to the Query, and list of passages: Query: {{query}} Solution: {{solution_content}} Passage [1]: {{passage_1}} Passage [2]: {{passage_2}} (more passages)... Math We have two different ranking tasks for math queries, including Math-QA and Math-Theorem. When using R1 to select positives for Math-QA task, we use the following prompt: Prompt of positives selection for Math-QA using R1 You are an expert in evaluating the relevance of passages to query (a math problem). query is relevant to passage if the passage references exactly the same theorem (such as Gausss lemma) used in the query. will provide you with query, the solution of the query, and list of passages to evaluate. Each passage is indicated by number identifier []. Please output the identifiers of the relevant passages. The output format should be [] [], e.g., [2] [4]. If there are no relevant passages, output \"None\". Only output the relevant passage identifiers, do not output irrelevant passage identifiers and do not provide any explanation. Here are the Query, the Solution of the Query, and list of passages: Query: {{query}} Solution: {{solution_content}} Passage [1]: {{passage_1}} Passage [2]: {{passage_2}} (more passages)... When using R1 to select positives for Math-Theorem task, we use the following prompt: You are an expert in evaluating the relevance of passages (a math theorem) to query (a math problem). passage is relevant to query if the querys solution either applies the same theorem as presented in the passage, or follows reasoning steps substantially similar to those used in the passages theorem. will provide you with query, the solution of the query, and list of passages to evaluate. Each passage is indicated by number identifier []. Please output the identifiers of the relevant passages. The output format should be [] [], e.g., [2] [4]. If there are no relevant passages, output \"None\". Only output the relevant passage identifiers, do not output irrelevant passage identifiers and do not provide any explanation. Here are the Query, the Solution of the Query, and list of passages: Query: {{query}} Solution: {{answer_content}} Passage [1]: {{passage_1}} Passage [2]: {{passage_2}} (more passages)... Web Search For web search, we sample 4k training queries from the MS MARCO (Nguyen et al. 2016) training set and apply the same listwise ranking prompt used for complex QA. Details about Two-Stage Training Framework Cold-Start SFT During the cold-start SFT stage, we use LlamaFactory (Zheng et al. 2024) to train two different backbone LLMs: Qwen2.5-7B-Instruct6 and Qwen2.5-32BInstruct7. The input prompt is shown as follows: As for Qwen2.5-7B-Instruct, we set the learning rate as 5e-6 and batch size per GPU as 1 with gradient accumulation steps as 8. We use DeepSpeed ZeRO-1 (Rasley et al. 2020) and FlashAttention2 (Dao 2023) for our training. We apply mixed precision BF16 for training, set the maximum reasoning length as 3072, and train the model for 5 epochs. As for Qwen2.5-32B-Instruct, we use LoRA (Hu et al. 2021) for efficient SFT. The lora parameters rank and alpha are both set to 32. We set learning rate to 1e-4 and batch size per GPU to 1 with gradient accumulation steps of 8. We use DeepSpeed ZeRO-3 (Rasley et al. 2020) with mixed precision BF16 and train the model for 4 epochs. Multi-view Ranking based RL RL has been shown in many works (Dong et al. 2025a; Song et al. 2025; Dong et al. 2025b) to further enhance the performance of SFT models. After our cold-start SFT, we further use the GRPO reinforcement learning algorithm based on VERL framework 8 to train our models (i.e., ReasonRank (7B) and ReasonRank 6https://huggingface.co/Qwen/Qwen2.5-7B-Instruct 7https://huggingface.co/Qwen/Qwen2.5-32B-Instruct 8https://github.com/volcengine/verl"
        },
        {
            "title": "Models",
            "content": "Econ. Earth. Rob. Bio. Psy. Stack. Sus. Leet. Pony AoPS TheoQ. TheoT. Avg. ReasonRank (7B) 35.05 47.75 31.22 56.70 47.77 32. 40.87 23.17 24.95 7.68 39.49 41. 35.74 Training Data only MSMARCO w/o Quality Normalization Training Approach w/o SFT (only RL) w/o RL (only SFT) w/o Rm non-reasoning SFT 29.08 33.96 36.97 50.35 24.34 27. 43.60 55.48 37.95 46.63 26.05 31.96 31.42 40.17 26.96 22.50 24.51 18. 7.08 7.32 33.22 35.16 39.75 40.67 30.08 34.20 27.44 30.47 30.86 26.70 35.23 44.83 47.14 45. 23.73 27.42 29.75 27.89 50.36 50.53 59.6 51.48 34.48 43.10 44.40 43.65 25.76 30.38 35.04 29.72 29.62 38.79 36.87 41.44 18.59 25.11 20.47 25. 23.51 22.46 20.80 17.64 6.78 7.06 8.23 5.39 34.46 37.53 37.52 38.31 34.28 40.12 39.66 42.4 28.69 33.15 34.20 32.96 Table 7: The performance (NDCG@10) of ablated models on each BRIGHT dataset. (32B)). For both models, the training batch size is set as 16, and the mini-batch size is set as 8. The maximum output length is set to 3072, and rollout per sample is 8. Both models are trained for 200 steps on 8 NVIDIA A800 GPUs. As for ReasonRank (7B), we use the 7B-scale SFT model as our policy model and set the learning rate as 1e-6. As for ReasonRank (32B), we use the 32B-scale SFT model as our policy model and set the learning rate as 1e-5. The lora parameters rank and alpha are both set as 32. During cold-start SFT and multi-view ranking based RL, we use the following input prompt for training ReasonRank:"
        },
        {
            "title": "Input Prompt during ReasonRank Training",
            "content": "You are RankLLM, an intelligent assistant that can rank passages based on their relevance to the query. Given query and passage list, you first thinks about the reasoning process in the mind and then provides the answer (i.e., the reranked passage list). The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. will provide you with {num} passages, each indicated by numerical identifier []. Rank the passages based on their relevance to the search query: query. [1]: {{passage_1}} [2]: {{passage_2}} (more passages)... Search Query: {{query}}. Rank the {num} passages above based on their relevance to the search query. All the passages should be included and listed using identifiers, in descending order of relevance. The format of the answer should be [] > [], e.g., [2] > [1]."
        },
        {
            "title": "Detailed Results of Ablation Study",
            "content": "We show the results of our ablation study based on ReasonRank (7B) on each BRIGHT dataset in Table 7. The ablated models show performance decline on almost all datasets, proving the effectiveness of the corresponding component. Detailed Results on BEIR Benchmark In this part, we present the comparison results on 7 datasets of the traditional IR benchmark BEIR (Thakur et al. 2021). We use BM25 as our retriever and rerank the top-100 retrieved passages. We also add rankT5 (3B), Rank-R1 (7B), and Rank1 (7B) baselines for more comprehensive comparison. The results are shown in Table 8. Analysis of Reasoning Length In this section, we explore the average length of reasoning chains using ReasonRank across various BRIGHT datasets. We select ReasonRank (7B) and rerank the top-20 passages retrieved by ReasonIR. The average reasoning length for queries in each dataset is shown in Figure 5. From the results, we observe that the reasoning length of coding tasks (i.e., Leetcode and Pony) and theorem tasks (e.g., AoPS, TheoremQA-Q and TheoremQA-T) tend to be longer than stackexchange tasks (e.g., Robotics and Stack Overflow). This may be because coding queries and theorem queries are more complex and challenging compared to StackExchange queries, thus requiring longer reasoning to understand queries and rerank the passages. Comparison with Teacher DeepSeek-R1 During our reasoning-intensive training data synthesis, we use DeepSeek-R1 to mine positive and negative passages based on the gold answer to obtain pointwise training labels. Theoretically, this should generate better labels compared to pure teacher distillation (which does not use gold answers and is based only on the teacher), potentially allowing the trained model to outperform DeepSeek-R1. To verify this, we compare the ranking performance of our trained ReasonRank (7B, 32B) with DeepSeek-R1 on BRIGHT. Considering the high time and API cost of reranking with DeepSeek-R1, we choose to rank the top-20 (instead of top100) passages retrieved by ReasonIR. The ranking prompt used by R1 is the same as the prompt used when generating listwise labels based on R1. The results are shown in Table 9. From the results, we can see that ReasonRank (32B) achieves comparable performance to DeepSeek-R1 (34.18 vs 34.13) and surpasses DeepSeek-R1 on 7 datasets (Economics, Robotics, Biology, Sustainable Living, Pony, AoPS, and TheoremQA-Q), proving the superiority of training with our synthesized data compared to pure teacher distillation."
        },
        {
            "title": "Models",
            "content": "BM25 Non-reasoning reranker RankT5 (3B) RankZephyr (7B) Reasoning reranker Rank-R1 (7B) Rank-R1 (14B) Rank1 (7B) Rank1 (32B) Rank-K (32B) ReasonRank (7B) ReasonRank (32B)"
        },
        {
            "title": "Covid",
            "content": "59.47 80.19 82.92 83.71 84.63 79.04 80.59 74.76 82.01 83."
        },
        {
            "title": "Signal",
            "content": "Robust04 31.80 67.89 33.75 33.04 40. 44.85 44.42 42.27 44.05 35.79 34.79 38.29 46.03 45.65 74.60 75.42 72.16 75.96 73.32 74.78 60.83 75.55 77. 37.40 38.26 38.94 38.58 37.52 37.29 36.48 39.60 40.04 31.73 31.41 33.08 32.95 25.41 25.58 32.64 31.36 31. 51.45 53.73 54.46 56.91 57.11 58.30 49.79 55.40 58."
        },
        {
            "title": "News",
            "content": "39.52 49.11 52.80 50.60 49.20 47.67 45.57 45.59 50.50 52.24 Avg. 43. 52.76 54.14 53.60 54.61 50.84 50.99 48.34 54.35 55.44 Table 8: The results (NDCG@10) on 7 BEIR datasets. All models rerank BM25-retrieved top-100 passages. The best reranker is highlighted in bold. Figure 5: The reasoning length of ReasonRank (7B) on BRIGHT."
        },
        {
            "title": "Models",
            "content": "Econ. Earth. Rob. Bio. Psy. Stack. Sus. Leet. Pony AoPS TheoQ. TheoT. Avg. ReasonIR (8B) 32.65 43.00 20.82 43.49 39. 30.96 27.34 31.69 19.55 ReasonRank (7B) ReasonRank (32B) DeepSeek-R1 (671B) 31.32 33.98 33. 43.18 42.30 45.64 27.07 28.66 26.91 50.06 51.97 50.34 45.02 47.45 49.47 33.46 36.31 37.22 37.25 38.51 36. 24.14 23.15 25.74 22.93 22.48 21.08 7.37 8.23 8.59 6.81 33.93 35.39 34.51 33. 36.68 30.59 40.69 42.27 42.42 33.23 34.18 34.13 Table 9: The comparison between ReasonRank and DeepSeek-R1. All models rerank ReasonIR-retrieved top-20 passages. We use GPT-4 rewritten queries during retrieval and use original query for reranking. The best reranker is marked in bold. Case Studies of ReasonRank (7B) To better understand the reasoning process of our ReasonRank, we present the reasoning chains of our ReasonRank (7B) on several BRIGHT datasets, including Earth Science  (Table 11)  , Biology  (Table 12)  , Robotics  (Table 13)  , LeetCode  (Table 14)  , AoPS  (Table 15)  , and TheoremQATheorems  (Table 16)  . We use the ReasonIR-retrieved top-20 passages as the candidate passages to rerank."
        },
        {
            "title": "Models",
            "content": "Econ. Earth. Rob. Bio. Psy. Stack. Sus. Leet. Pony AoPS TheoQ. TheoT. Avg. ReasonIR (8B) ReasonRank (32B) RaDeR + BM25 (Hybrid) ReasonRank (32B) ReasonRank (32B) 32.65 36.64 30.85 37.67 36.70 43.00 48.90 56.08 50.61 55.53 20.82 43.49 39.57 33.88 58.17 53. 24.13 52.66 46.69 35.44 59.04 53.06 35.69 62.72 54.64 30.96 38.68 34.93 39.04 38.03 27.34 31.69 19.55 45.97 25.78 20.93 32.22 31.91 25.87 45.75 22.64 25.30 44.81 29.46 25.56 7.37 9. 11.98 11.41 14.38 33.93 38.46 37.73 41.27 41.99 36.68 46.28 43.43 47.73 50.06 30.59 38. 35.71 39.08 40.80 Table 10: Further ranking enhancement on BRIGHT. Both retrievers ReasonIR (8B) and RaDeR + BM25 (Hybrid) use GPT4-rewritten queries for retrieval. Limitations and Future Directions Despite the superior performance achieved by reasonrank, we believe that our work still has some limitations. Firstly, ReasonRank does not include non-reasoning type data during training, which results in its inability to seamlessly switch between reasoning and non-reasoning modes when faced with search scenarios of varying difficulty. In the future, we plan to introduce non-reasoning type data into the training set to enhance reasonranks flexibility in handling different scenarios (like Qwen3 (Yang et al. 2025a)). Secondly, in our work, we only use Qwen2.5-series LLMs as the backbones for ReasonRank and did not use other models (such as Llama 3.1 (Dubey et al. 2024) and reasoningbased models Qwen3 (Yang et al. 2025a)). In the future, we will try using other models as the foundation to validate the effectiveness of our ReasonRank. Thirdly, ReasonRank still relies on the sliding window strategy for passage reranking. Existing study (Liu et al. 2024b) have demonstrated that LLMs have strong full-list ranking capabilities (i.e., directly ranking 100+ passages in one forward pass), which exhibit both superior efficiency and effectiveness compared to sliding window approaches. In future work, we plan to explore reasoning-intensive listwise reranking based on full ranking to further improve the scalability and performance of our framework. Table 11: Case on Earth Science. The model first analyzes the search query and considers the criteria that make passage relevant. Then, it examines some relevant passages and compares them to the criteria. Finally, it quickly goes through the remaining passages and determines the final ranking."
        },
        {
            "title": "Query",
            "content": "The line on the ground at the Greenwich Observatory is itself on tectonic plate that is moving, so is the prime meridian still 0 degrees, 0 minutes, 0 seconds? If not - what are the implications (if any)? For example, if my GPS says am 100 km west of Greenwich Observatory, am 100 km west of 0 0 0 or 100 km west of the line on the ground? Who (and what) decides this kind of thing? Curious punter, not scientist...."
        },
        {
            "title": "ReasonRank Output",
            "content": "<think> Okay, let me try to figure out how to rank these passages for the users query. The main question is whether the prime meridian at Greenwich is still exactly 0 0 0\" because the tectonic plate is moving, and what that means for GPS coordinates. The user is also curious about who decides this. First, need to identify which passages address the movement of the prime meridian due to tectonic shifts and how that affects GPS. The query mentions tectonic plates moving, so passages explaining that the physical line has moved are key. Also, implications like GPS readings and reference frames are important. Passage [1] seems very relevant. It talks about the Airy Transit Circle at Greenwich and how its longitude is now 00005.3 West in ITRF and WGS84. It mentions the offset of 102m and discusses astronomical vs geodetic coordinates. This directly answers if the prime meridian has moved and explains GPS references. Passage [3] also discusses the offset and explains it through deflection of the vertical and geodetic coordinates. It addresses GPS receivers showing east of zero, which relates to the users example. So this is probably high relevance. Passage [8] talks about Earths drift due to mantle movements, ice melting, etc., and mentions the pole drift. While not directly about Greenwich, it explains why tectonic/mass movements cause shifts, which ties into the prime meridians position. Passage [9] discusses polar motion, which is about Earths rotational axis, but the users query is about longitude. However, it might touch on reference frames, so could be somewhat relevant. Passage [10] explains why Greenwich was chosen in 1884 and mentions tectonic maps using British ones. Its historical but might not address the current movement issue. Passage [11] is titled \"Why the Greenwich meridian moved\" that sounds directly relevant. It likely explains the reasons for the shift, so high priority. Passage [12] mentions the prime meridian through Greenwich and longitude measurement. Its more about definition than movement, so maybe less relevant. Passage [14] discusses geographical coordinates and WGS, which relates to GPS references. Explains how modern systems differ from simple models. Passage [7] and [18] seem to be about the 1884 conference and references to the meridian movement, so they might explain the decision-making body (BIH, IERS), which answers who decides. Passage [2] mentions BIH Terrestrial System and geodetic references, which ties into reference frames and who decides. Passage [4] and [5] talk about Earths equator and axis changes, but not directly about the prime meridians movement. Passage [16] is about geomagnetic poles, which is different type of movement, so less relevant. Passage [19] and [20] explain longitude but not the tectonic aspect. So the top ones would be [1], [3], [11], [8], [7], [18], [2], [14], [10], then others. Need to check each again. [1] is definitely top as it directly answers the movement and GPS. [3] explains the offset and geodetic systems. [11] is about why it moved, so very relevant. [8] explains Earth drift causes. [7] and [18] mention BIH and IERS, which decide references. [2] talks about reference frames. [14] about WGS. [10] historical context. Others less so. So the order might be [1] > [3] > [11] > [8] > [7] > [18] > [2] > [14] > [10] > [9] > [4] > [5] > [16] > [12] > [17] > [20] > [13] > [6] > [19] > [15] </think> <answer> [1] > [3] > [11] > [8] > [7] > [18] > [2] > [14] > [10] > [9] > [4] > [5] > [16] > [12] > [17] > [20] > [13] > [6] > [19] > [15] </answer> Table 12: Case on Biology. The model first analyzes the search query and considers the criteria that make passage relevant. Then, it examines some relevant passages and compares them to the criteria. Finally, it quickly goes through the remaining passages and determines the final ranking."
        },
        {
            "title": "Query",
            "content": "Why does hair turn grey or white, and why does it happen later for some? The question is pretty simple: what is happening molecularly when hair turns grey or white? would imagine that it is due to the lack of particular compound. Im also interested in why some peoples onset of grey hair happens much later than others."
        },
        {
            "title": "ReasonRank Output",
            "content": "<think> Okay, let me try to figure out how to rank these passages. The user wants to know why hair turns gray or white molecularly and why some people gray later. So need to focus on passages that explain the biological mechanisms, like lack of compound, melanocyte stem cells, oxidative stress, telomeres, etc. Passage [1] talks about melanocyte stem cells and Bcl2, which affects survival. Thats relevant. Passage [2] discusses mouse models and factors like oxidative damage and telomerase, so thats good. Passage [4] mentions melanocyte stem cells and homeostasis, which ties into the molecular process. Passage [5] is an abstract about melanocyte stem cells and aging, so key. Passage [6] details melanocyte compartments and stem cells, explaining the process. Passage [8] covers melanocyte stem cell properties and bulge niche, which is crucial. Passage [15] includes DNA repair and telomere theories, which relate to aging and molecular causes. Passage [16] mentions melanocyte stem cell maintenance in the niche, so thats relevant. Passages [3], [7], [10], [11], [12], [18], [20] have some info but less directly on the molecular mechanisms. Others like [9], [13], [14], [17] are more about hair care or demographics, so less relevant. So the top ones are those explaining stem cell depletion, oxidative stress, telomeres, and niche environment. Then others that support those points. The care-related ones are less important here. </think> <answer> [4] > [1] > [5] > [6] > [8] > [2] > [15] > [16] > [10] > [3] > [7] > [12] > [20] > [18] > [9] > [11] > [13] > [14] > [17] > [19] </answer> Table 13: Case study on Robotics. The model successfully rank the passage with relatively short reasoning length."
        },
        {
            "title": "Query",
            "content": "ROS2: Adding parameters to YAML launch file How do use YAML launch file to pass parameters to node? The tutorial covers passing parameters using python launch file but not YAML."
        },
        {
            "title": "ReasonRank Output",
            "content": "<think> Okay, let me try to figure out how to rank these passages for the users query about using YAML launch files to pass parameters in ROS2. The user mentioned that the tutorial covers Python launch files but not YAML, so theyre specifically looking for YAML examples. First, Ill go through each passage to see which ones directly address YAML launch files and parameters. Passage [2] has YAML config example in the launch file, mentioning turtlesim_custom_params.yaml, which seems very relevant. Passage [3] talks about the <rosparam> tag used in launch files with YAML, which is key here. Passage [8] also explains the <rosparam> tag in the context of YAML files. Passages [13][18] all mention rosparam and using YAML with roslaunch, which is related. Passage [19] discusses debugging with YAML files. Passage [6] gives an example of YAML parameters for Nav2, which is good example. Passage [5] mentions YAML config files for parameters. Passage [1] also talks about YAML config files for parameters. Passages [10] and [11] are about parameter defaults and loading from YAML. The others, like [4], [7], [12], and [20], are either about Python or C++ or not directly answering the YAML question. So the most relevant are [2], [3], [8], then [13]-[18], [6], [5], [19], [1], followed by others. Need to make sure the order prioritizes direct YAML examples and explanations of the <rosparam> tag. </think> <answer> [2] > [3] > [8] > [13] > [14] > [15] > [16] > [17] > [18] > [6] > [5] > [19] > [1] > [10] > [11] > [9] > [4] > [7] > [12] > [20] </answer> Table 14: Case on LeetCode. The model rethinks its initial ranking and repeatedly verifies and compares the top-ranked passages.."
        },
        {
            "title": "Query",
            "content": "Given an n integers matrix, return the length of the longest increasing path in matrix. From each cell, you can either move in four directions: left, right, up, or down. You **may not** move **diagonally** or move **outside the boundary** (i.e., wrap-around is not allowed)."
        },
        {
            "title": "ReasonRank Output",
            "content": "<think> Okay, let me try to figure out how to rank these passages based on the query about the longest increasing path in matrix. The user is looking for the length of the longest path. First, need to go through each passage and see which ones are relevant. The query is specifically about dynamic programming approaches for this problem, possibly using memoization since its grid traversal with constraints. Passage [3] talks about the longest increasing subsequence using dynamic programming. Thats related but not exactly the same as the matrix problem. Its about sequences, not paths in grid. So maybe not the top. Passage [6] is the length of the longest increasing subsequence again. Similar to [3], so not directly relevant. Passage [13] discusses maximal square area, which uses DP but not for paths. Not relevant. Passage [16] is about the longest palindromic subsequence. Again, subsequence, not grid paths. Not relevant. Passage [19] deals with unique paths with obstacles using DP. Its about grid paths but not increasing paths. Might be somewhat related but not directly. Passage [11] is shortest clear path in binary matrix using BFS. Different problem but grid paths. Maybe somewhat related but not the same. Passage [15] involves collecting gold in grid with DFS. Its about grid traversal but not increasing paths. Not directly relevant. Passage [20] is about longest increasing subsequence with difference constraints. Subsequence again, not grid. Not relevant. Passage [4] counts the number of longest increasing subsequences. Still subsequence, not grid paths. Passage [9] is longest common subsequence. Not relevant. Passage [10] is LCS again. Not applicable. Passage [14] involves min operations for robots on 2xN grid. Not related to increasing paths. Passage [5] is about Excel cells, not relevant. Passage [18] is about work hours and palindromic intervals. Not related. Passage [2] is about strictly increasing paths in grid, returning the count. Passage [1] discusses max points in queries starting from top-left, using DFS. It involves grid traversal and paths but not the specific increasing path problem. Might be somewhat related but not directly. Passage [17] is about LCS length again. Not relevant. So the most relevant is passage [2], which directly addresses strictly increasing paths in grid. Then passage [19] and [8] deal with grid paths but not increasing. Passage [3] and [6] are about subsequences, not paths. Passage [15] is about grid traversal but not increasing. Wait, passage [2] has code that seems to count strictly increasing paths, which is exactly the problem. The example in [2] matches the problems examples. So [2] is top. Then maybe [19] and [8] for grid DP, but theyre not directly solving the same problem. Passage [20] is about subsequences with difference constraints, not paths. So the ranking would have [2] first, then perhaps [19], [8], [7], etc., but the key is [2] is the most relevant. Then maybe [3] and [6] since they handle increasing sequences, but not grid paths. So the order should start with [2], then others that involve grid paths or DP for paths. But the query is specifically about the longest increasing path in matrix. So [2] is the most relevant. Then maybe [19] and [8] for grid DP, but theyre not the same. So the top passages are [2], followed by [19], [8], [7], [15], [1], etc. But need to check each again. Looking at passage [20], its about subsequences, not paths. Passage [13] is about squares, not paths. Passage [16] is LCS. So the most relevant is [2], then maybe [19] and [8] for grid DP. But the problem in the query is graph problem. Passage [2] might use similar approach. The code in [2] seems to count paths, which could be part of the solution. So the ranking would be: [2] > [19] > [8] > [7] > [15] > [1] > [3] > [6] > [20] > [4] > [9] > [10] > [13] > [16] > [17] > [11] > [14] > [5] > [18] > [12] But need to verify. The problem in the query is classic DP with memoization on the grid. Passage [2] might be about counting all paths, which is different problem but uses similar techniques. However, the code in [2] might not directly solve the longest path. Alternatively, passage [19] is about unique paths with obstacles, which uses DP but not for increasing paths. Passage [8] is min path sum, which uses DP but again not for increasing. So the most relevant is [2], then maybe [19], [8], [7], [15], [1], etc. But Im not sure. The querys problem is graph traversal with DP, so passages that deal with grid paths and DP would be more relevant. Passage [2] is about strictly increasing paths, which is exactly the problem. So [2] is definitely the top. Then perhaps [19] and [8] for grid DP. So the final ranking would start with [2], then others that involve grid paths or DP for paths. The rest are less relevant. </think> <answer> [2] > [19] > [8] > [7] > [15] > [1] > [3] > [6] > [20] > [4] > [9] > [10] > [13] > [16] > [17] > [11] > [14] > [5] > [18] > [12] </answer> Table 15: Case on AoPS. We can see that the model first analyzes the search query. Then, it looks at each passage and judges their relevance. Finally, it takes closer look at the most relevant ones and makes detailed comparison."
        },
        {
            "title": "Query",
            "content": "Triangle ABC is inscribed in circle ω. Points and are on side AB with AP < AQ. Rays CP and CQ meet ω again at and (other than C), respectively. If AP = 4, = 3, QB = 6, BT = 5, and AS = 7, then ST = , where and are relatively prime positive integers. Find + n."
        },
        {
            "title": "ReasonRank Output",
            "content": "<think> Okay, let me try to figure out how to rank these passages for the given problem. The query is about triangle inscribed in circle with some points and intersections. The problem involves calculating ST given various lengths. First, need to understand what the problem is asking. Its geometry problem involving circle, triangle ABC, points and on AB, rays CP and CQ intersecting the circle again at and T, and given specific segment lengths like AP=4, PQ=3, QB=6, BT=5, AS=7. The goal is to find ST as fraction m/n and then m+n. Now, need to rank the 20 passages based on relevance. The most relevant ones would be those that deal with similar geometry problems, especially involving circles, triangles, intersecting lines, power of point, chords, or Cevas theorem, Menelaus, etc. Looking at the passages: [1] Talks about circle, chord lengths, power of point. Might be relevant since the problem involves chords and circle properties. [2] Involves triangle inscribed in circle, tangents, and intersections. Could be relevant as its about triangle geometry with circle. [3] Circles intersecting, common tangents, trapezoid area. Less relevant, seems more about trapezoids and tangents. [4] Circumcenter and distance calculation. Maybe somewhat related but not directly. [5] Incircle trisecting median. Not directly related. [6] Tangent from origin to circle. Maybe not. [7] Parabola and circle, tangent length. Not relevant. [8] Parallelogram and circle tangent. Not relevant. [9] Circle tangent to other circle. Not relevant. [10] Perimeter with Law of Cosines. Maybe trigonometry, but not directly. [11] Circumference calculation. Not relevant. [12] Circumcircle and tangents. Could be relevant as it involves circumcircle and intersections. [13] Point inside triangle with congruent angles. Not directly related. [14] Concentric circles and tangents. Not relevant. [15] Circle tangent to square. Not relevant. [16] Hexagon inscribed in circle. Not relevant. [17] Tangent and angle bisector. Maybe involves some circle theorems. [18] Similar to [14], concentric circles. Not relevant. [19] Right triangle legs. Basic Pythagorean, not relevant. [20] Circle tangent to parabola. Not relevant. So the most relevant ones are probably [1], [2], [12], [17], maybe [4], [20]. Passage [1] discusses chord lengths and power of point, which might be useful here since the problem involves ST as chord. Passage [2] is about triangle inscribed in circle with tangents and intersections, which is very similar to the querys setup. Passage [12] involves circumcircle and tangent points, which could relate to the problems intersections at and T. Passage [17] deals with tangents and angle bisectors, which might involve some theorems used in the problem. Passage [4] uses circumradius and power of point, which could be applicable. Passage [20] involves circle tangents and equations, but maybe less so. So the ranking would start with [2], then [1], [12], [17], [4], followed by others. Need to check which directly address circle geometry with intersecting chords, power of point, or Cevas/Menelaus. </think> <answer> [2] > [1] > [12] > [17] > [4] > [20] > [8] > [14] > [18] > [5] > [3] > [6] > [7] > [10] > [13] > [9] > [15] > [16] > [19] > [11] </answer> Table 16: Case study on TheoremQA-Theorems. The model first attempts to solve the problem, then checks the candidate passages."
        },
        {
            "title": "Query",
            "content": "You are working with four possible machines, each of which is represented by matrix that operates on an input vector v: (a) (ΦT Φ)1. (b) Φ(ΦT Φ)1. (c) Φ(ΦT Φ)1ΦT . (d) ΦT (ΦT Φ)1ΦT . Which of them takes any vector and projects it onto the space spanned by the columns of Φ?"
        },
        {
            "title": "ReasonRank Output",
            "content": "<think> Okay, let me try to figure out how to rank these passages for the query. The user is asking which of the four matrices projects any vector onto the space spanned by the columns of Φ. The options are: (a) (ΦT Φ)1 (b) Φ(ΦT Φ)1 (c) Φ(ΦT Φ)1ΦT (d) ΦT (ΦT Φ)1ΦT First, need to recall what an orthogonal projection matrix looks like. From linear algebra, the projection matrix onto the column space of Φ is usually given by Φ(ΦT Φ)1ΦT . So options (c) and (d) might be candidates. But let me check each one. Option (a) is just the inverse of ΦT Φ, which isnt projection matrix. Projection matrices must be idempotent (P 2 = ) and symmetric. (a) is likely not idempotent. Option (b) is Φ multiplied by that inverse. Multiplying Φ by that matrix might not yield projection. Let me think: Φ(ΦT Φ)1 is the Moore-Penrose pseudoinverse of Φ, but the projection is Φ times that times ΦT . So (c) is ΦΦΦT , which is the projection. Option (d) is ΦT pseudoinverse times ΦT . Wait, nothats not right. The correct form is Φ pseudoinverse times ΦT . So (c) is correct. Option (d), written as ΦT (ΦT Φ)1ΦT , is not the same. Therefore, the correct answer is (c). Now, looking at the passages, which ones discuss orthogonal projections onto the column space? Passage [2] talks about orthogonal projection onto the closed linear span using an orthonormal basis, which is exactly whats needed here. Passage [3] mentions orthogonal projections as linear transformations, which is relevant. Passage [8] discusses properties of projections, like composed with is P. Passage [17] defines projections in Hilbert spaces, which includes the kernel condition. Passage [20] talks about bounded projections, which might be related but less directly. Passage [6] explains linear transformations as matrix products, which is relevant for understanding how these matrices act. Passage [9] involves inner products and matrix factors, which could relate to the projection formula. Passage [14] is about matrix products as linear transformations, which is key here. So the most relevant passages would be those that explain orthogonal projections, especially using the formula Φ(ΦT Φ)1ΦT . Passage [2] gives the formula for orthogonal projection onto subspace. Passage [3] confirms its linear transformation. Passage [8] discusses idempotence, property of projections. Passage [17] defines projections. Passage [6] connects matrices to linear transformations. Others, like [9] and [14], might offer supporting information but are less directly relevant. So the ranking would start with [2], then [3], [8], [17], [6], followed by others like [20], [9], [14], etc. Need to check which passages explicitly mention the projection formula or idempotence. </think> <answer> [2] > [3] > [8] > [17] > [6] > [20] > [9] > [14] > [12] > [1] > [7] > [10] > [18] > [13] > [15] > [4] > [5] > [11] > [16] > [19] </answer>"
        }
    ],
    "affiliations": [
        "Baidu Inc.",
        "Carnegie Mellon University",
        "Gaoling School of Artificial Intelligence, Renmin University of China"
    ]
}