{
    "paper_title": "FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing",
    "authors": [
        "Junyi Wu",
        "Zhiteng Li",
        "Haotong Qin",
        "Xiaohong Liu",
        "Linghe Kong",
        "Yulun Zhang",
        "Xiaokang Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-guided image editing with diffusion models has achieved remarkable quality but suffers from prohibitive latency, hindering real-world applications. We introduce FlashEdit, a novel framework designed to enable high-fidelity, real-time image editing. Its efficiency stems from three key innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses costly iterative processes; (2) a Background Shield (BG-Shield) technique that guarantees background preservation by selectively modifying features only within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA) mechanism that ensures precise, localized edits by suppressing semantic leakage to the background. Extensive experiments demonstrate that FlashEdit maintains superior background consistency and structural integrity, while performing edits in under 0.2 seconds, which is an over 150$\\times$ speedup compared to prior multi-step methods. Our code will be made publicly available at https://github.com/JunyiWuCode/FlashEdit."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 4 4 2 2 2 . 9 0 5 2 : r FLASHEDIT: DECOUPLING SPEED, STRUCTURE, AND SEMANTICS FOR PRECISE IMAGE EDITING Junyi Wu1, Zhiteng Li1, Haotong Qin2, Xiaohong Liu1, Linghe Kong1, Yulun Zhang1, Xiaokang Yang1 1Shanghai Jiao Tong University, 2ETH Zurich Figure 1: FlashEdit produces superior visual results for text-guided image editing, addressing background instability and semantic entanglement with an over 150 speedup against DDIM (Song et al. (2020b)) + P2P (Hertz et al. (2022))."
        },
        {
            "title": "ABSTRACT",
            "content": "Text-guided image editing with diffusion models has achieved remarkable quality but suffers from prohibitive latency, hindering real-world applications. We introduce FlashEdit, novel framework designed to enable high-fidelity, real-time image editing. Its efficiency stems from three key innovations: (1) One-Step Inversion-and-Editing (OSIE) pipeline that bypasses costly iterative processes; (2) Background Shield (BG-Shield) technique that guarantees background preservation by selectively modifying features only within the edit region; and (3) Sparsified Spatial Cross-Attention (SSCA) mechanism that ensures precise, localized edits by suppressing semantic leakage to the background. Extensive experiments demonstrate that FlashEdit maintains superior background consistency and structural integrity, while performing edits in under 0.2 seconds, which is an over 150 speedup compared to prior multi-step methods. Our code will be made publicly available at https://github.com/JunyiWuCode/FlashEdit. Equal contribution Corresponding author: Yulun Zhang, yulun100@gmail.com"
        },
        {
            "title": "INTRODUCTION",
            "content": "Text-guided image editing with diffusion models (Brooks et al. (2023),Dong et al. (2023)) has demonstrated remarkable capabilities, allowing users to perform complex semantic modifications with high fidelity. The standard methodology is built upon two-stage inversion-denoising pipeline: an initial inversion process maps source image to its corresponding noise latent, which is then progressively denoised to generate the edited output according to target prompt (Ju et al. (2023),Cao et al. (2023)). The objective is to achieve high fidelity in both content preservation and target prompt alignment, which often necessitates computationally intensive, multi-step process. Recent research has pursued several distinct strategies to improve accuracy and speed. To tackle the latency of the multi-step denoising process, methods based on model distillation have been proposed to enable editing in faster way (Deutch et al. (2024)). These approaches must carefully address challenges such as mismatched noise statistics and insufficient editing strength that arise when adapting multi-step frameworks to fast samplers (Mokady et al. (2023b),Miyake et al. (2025)). To improve edit precision and prevent semantic leakage into the background, another category of work modifies the models internal mechanisms, primarily by re-weighting or replacing attention maps to ensure the edit is spatially constrained (Fang et al. (2024); Xu et al. (2024)). Recognizing that the final edit quality is highly dependent on the starting point, other approaches focus on refining the inversion technique itself (Ju et al. (2023)). These methods aim to find more accurate initial latent vector, with recent insights revealing that separating the objectives of content preservation and edit fidelity can yield significant performance gains and speedups (Wang et al. (2025b)). However, these existing methods approach speed and quality as trade-off rather than as interconnected components of singular, complex control problem. They offer partial solutions like accelerating the sampler at the cost of inversion fidelity, or preserving the background without addressing the precision of the foreground edit. This results in fragmented landscape of techniques that fail to deliver solution that is simultaneously fast, robust, and precise. truly practical editing framework requires more holistic methodology that addresses control at every level of editing. To address this multifaceted challenge, we introduce novel editing methodology that establishes control at three progressively finer levels of granularity. At the foundational level, we tackle the macro-problem of temporal control. We propose One-Step Inversion-and-Editing (OSIE) pipeline, built upon an Anchor-and-Refine training strategy, which conquers the prohibitive latency of prior work and makes real-time interaction possible. With this temporal control established, we address the meso-level problem of spatial control. Our Background Shield (BG-Shield) mechanism provides structural integrity by performing surgical intervention in the self-attention layers. It uses background memory and foreground-core querying to create hard separation between edited and unedited regions, guaranteeing background stability. Finally, with speed and structure secured, we target the micro-level problem of semantic control. We develop Sparsified Spatial Cross-Attention (SSCA), refinement of the cross-attention mechanism that prunes irrelevant text tokens pre-softmax, ensuring the edit is guided by clean, unambiguous semantic signal. Each component logically builds upon the last, forming cohesive solution (Figure 1). Our main contributions can be summarized as follows: We propose novel, multi-level methodology for image editing that cohesively integrates control over three distinct levels: the temporal latency of the pipeline, the spatial structure of the image, and the semantic content of the edit with an over 150 speedup compared to prior multi-step methods. At the temporal level, we introduce the One-Step Inversion-and-Editing (OSIE) pipeline and its Anchor-and-Refine training strategy, which for the first time enables high-fidelity inversion for one-step diffusion models. At the spatial level, we propose Background Shield (BG-Shield), structural intervention in self-attention that uses memory caching and selective core querying to enforce pixelperfect background preservation, ensuring the structural integrity of the edit. At the semantic level, we develop Sparsified Spatial Cross-Attention (SSCA), crossattention mechanism that performs pre-softmax token pruning. This provides the final layer of fine-grained control, eliminating attribute bleeding and enabling precise edits with complex text prompts."
        },
        {
            "title": "2.1 DIFFUSION MODELS",
            "content": "Recent advances in image synthesis have been largely driven by diffusion models (Peebles & Xie (2023),Kulikov et al. (2024)), which have become leading paradigm for generating high-fidelity images from text. The core mechanism involves an iterative denoising process that progressively refines random noise vector into coherent image conditioned on text prompt. landmark contribution in this area is Stable Diffusion (Rombach et al. (2021)), Latent Diffusion Model (LDM) (Rombach et al. (2022)) that performs the computationally intensive denoising process in lower-dimensional latent space, making the technology widely accessible. Parallel to this, alternative frameworks have emerged, such as Flow Matching models like Flux (Labs (2024)). Instead of an iterative refinement process, these models learn to map noise to an image via more direct, straightline trajectory, representing different theoretical foundation for high-quality generative modeling. To mitigate the high computational cost of these iterative models, various acceleration techniques have been proposed. Model quantization (Li et al. (2024a;b; 2025c); Yan et al. (2025b)), cache mechanism (Xu et al. (2025); Pan et al. (2025)), sparse attention (Li et al. (2025a)), pruning (Wang et al. (2025a),Yan et al. (2025a)), and distillation (Hinton et al. (2015)) are general acceleration techniques for deep learning model. In diffusion models, specifically, one primary category is model quantization (Li et al. (2025b)), which reduces memory footprint and computational load by converting full-precision models into lower-bit representations. Another category involves cache mechanisms (Liu et al. (2025); Xu et al. (2018)), which enhance inference efficiency by exploiting temporal redundancy. These methods reuse intermediate features computed at earlier denoising steps to avoid redundant calculations in later steps. While effective in isolation, recent work like QuantCache (Wu et al. (2025)) demonstrates unified framework can yield greater gains. 2.2 EDITING MODELS The task of editing real images with pre-trained generative models introduces the fundamental challenge of inversion: finding latent representation that can faithfully reconstruct given source image. This problem was first extensively studied in the context of Generative Adversarial Networks (GAN) Inversion (Wang et al. (2022),Zhu et al. (2020),Zhu et al. (2016)). In comparison, DDIM Inversion (Song et al. (2020b)) provides deterministic method to find corresponding noise latent for source image. Once this latent is obtained, various editing mechanisms are employed during the denoising process to apply the desired changes. prominent family of methods focuses on attention control, where the cross-attention maps between text and image are manipulated. For example, to change photo of red car to blue car, Prompt-to-Prompt (Hertz et al. (2022)) identifies the attention weights corresponding to the word red and replaces them with those for blue, preserving the attention for car and the background. Another powerful technique is feature injection, exemplified by Plug-and-Play (PnP) (Zhang et al. (2021)). To preserve the identity of subject, PnP injects the self-attention featureswhich encode structure and appearancefrom the source images generation process into the edited one. third approach is mask-based editing, where methods like DiffEdit (Couairon et al. (2022)) generate mask indicating the region to be altered and then apply the denoising process only within that area. Despite these advances, core challenge persists in perfectly disentangling the edited foreground from the unedited background."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 ONE-STEP INVERSION-AND-EDITING Challenge: Dual-Constraint Optimization Problem. The task of learning an effective inversion mapping is fundamentally dual-constraint optimization problem. The predicted noise latent, εinv, must simultaneously satisfy two competing objectives. The first is fidelity constraint, requiring εinv to encode sufficient information to perfectly reconstruct the source image. The second is distributional constraint, requiring εinv to adhere to the generators prior distribution, (0, I), to ensure editability. While both constraints can be explicitly supervised when using synthetic data, the distributional constraint becomes non-trivial and unsupervised for real-world images where the ground-truth noise is unknown. Naively optimizing for fidelity alone causes severe violation of the distributional constraint, leading to uneditable latents. 3 Figure 2: Overview of our One-Step Inversion-and-Editing framework, which introduces direct image conditioning branch, trained via two-stage Anchor-and-Refine strategy that uses direct supervision for synthetic data (Stage 1) and teacher-student objective for real images (Stage 2). Motivation. Our motivation is to design training strategy that explicitly decouples and progressively solves these two constraints. We posit that the network must first learn to jointly satisfy both objectives in fully-supervised setting before it can be adapted to handle the unsupervised nature of real-image inversion. This leads to our Anchor-and-Refine approach. The Anchor stage uses synthetic data to ground the network in parameter space that respects both constraints. The Refine stage then adapts this mapping to real images, where we introduce powerful generative prior from teacher model to act as proxy for the now-unsupervised distributional constraint. This ensures that even for real images, fidelity is pursued without sacrificing editability. Proposed Method. Shown in Figure 2, our primary architectural modification is designed to resolve fundamental tension in the inversion process. The inverted noise vector is typically burdened with two conflicting tasks: perfectly preserving the source images identity and remaining generic enough for subsequent editing. To decouple these roles, we introduce dedicated visual adapter which provides the decoder with direct visual information from the source image. This way, the decoders outputthe reconstructed latent zbecomes function of three distinct inputs: the inverted noise n, the text condition ct, and the explicit image features ci. By directly supplying the visual identity via ci, we liberate the noise vector from its strict reconstruction duty. It can now remain closer to pure Gaussian distribution, drastically improving its malleability for downstream editing tasks. Stage 1: Anchoring the Solution via Supervised Training. The first stage aims to find robust initialization, or anchor, for the inversion network Iθ. We use synthetic dataset of (εgt, z0) tuples from the base generator G, which allows for direct and strong supervision. The training objective is twofold: LStage1 = λrecz0 ˆz02 2 + λregεgt εinv2 2. (1) The regression term Lreg is critical in this stage. It constrains the network to region of the loss landscape where its outputs naturally conform to the target distribution (0, I). During this stage, we train both the inversion network Iθ and the newly introduced image adapter. This teaches the adapter how to effectively provide visual priors that aid in reconstruction. This anchoring step prevents the network from converging to trivial solutions in the next stage. Stage 2: Refining with Teacher-Student Objective. With the network anchored, the second stage refines its mapping for the complexities of real-world images where the ground-truth noise εgt is unknown. To prevent the distribution of εinv from drifting, we introduce regularization scheme 4 Figure 3: Illustration of our Background Shield (BG-Shield) mechanism. The top of the figure illustrates the problem of background inconsistency in standard editing, while the bottom details the pipeline of our method designed to solve it. framed as teacher-student distillation process. We leverage pre-trained teacher model, ϕ, to provide dynamic, supervisory signal for our student inversion network, Iθ. For each real image latent z0, we first create noisy version zt = αtz0 +σtεinv at random timestep t. The teacher model ϕ then predicts the noise from this input, yielding pseudo-ground-truth target, εteacher: εteacher = ϕ(αtz0 + σtεinv, t, c). (2) We then define refinement loss, LRefine, that minimizes the L2 distance between our networks output εinv and the teachers prediction. Crucially, we treat the teachers output as fixed target by applying stop-gradient operator. LRefine = Et (cid:104) w(t) εinv sg (εteacher)2 2 (cid:105) , (3) where sg() denotes the stop-gradient operation. This formulation turns the problem into simple regression task where the student (Iθ) is trained to produce noise latent that the teacher (ϕ) would have predicted. This distillation-style loss effectively regularizes the training, ensuring that for any given real image, the predicted noise εinv is solution that is not only perceptually accurate (as enforced by parallel perceptual loss) but also highly plausible under the teachers learned world model. 3.2 BACKGROUND SHIELD Challenge: Background Inconsistency. critical challenge in localized image editing is maintaining strict background consistency. We observe that even with precise masks, many methods fail at this task. For instance, in Figure 3 when performing seemingly simple edit such as changing an orange cat to black cat, the background suffers from unintended alterations, leading to shifts in color, lighting, or style. We identify the root cause of this instability as the inherent nature of the self-attention mechanism. As global operator that computes all-to-all relationships between image tokens, it allows the strong semantic signal from the foreground edit to propagate and contaminate the background features, undermining the goal of truly localized edit. Motivation. Having identified the global nature of self-attention as the cause of this background inconsistency, our motivation is to move beyond merely scaling influences and propose direct structural intervention. To achieve background stability, hard constraint that structurally isolates the background from the editing process is required. We introduce Background Shield (BG-Shield), method designed to enforce this consistency by replacing the backgrounds feature computation with direct recall from background memory. Proposed Method. Shown in Figure 3, BG-Shield operates as two-pass mechanism within selfattention layers. Let RSD be the input feature sequence, and let binary mask {0, 1}S define the foreground indices and background indices B. Background Memory Caching. During forward pass with the source prompt csrc, we compute the Key and Value matrices, Ksrc, Vsrc. We then extract and cache the background-specific keyvalue pairs: = Ksrc[B, :], = Vsrc[B, :]. (4) This cached memory, (K B, ), serves as high-fidelity record of the original background state. Figure 4: Illustration of our Sparsified Spatial Cross-Attention (SSCA) method resolving semantic entanglement. The top row demonstrates how standard attention fails on precise edits, resulting in edit attenuation and attribute leakage. The bottom row details our SSCA mechanism, which prevents this by computing attention only over subset of relevant text tokens to ensure clean edit. Mask-Driven Recomposition and Selective Querying. During the editing pass with the target prompt ctgt, we compute new queries, keys, and values (Qtgt, Ktgt, Vtgt RSdk ). We then construct spatially-aware, full key-value set, Kf ull, Vf ull, by combining the background memory with the current foreground features: (cid:26)K (cid:26)V B[rankB(j), :] if if , Vf ull[j, :] = [rankB(j), :] Vtgt[j, :] if if , (5) Kf ull[j, :] = Ktgt[j, :] where rankB(j) ensures correct positional alignment. To mitigate boundary artifacts, we introduce foreground core by applying morphological erosion to the mask . This is implemented using 2D max-pooling operation (with kernel size k, stride s, and padding p) on the inverted mask. The resulting core mask Mcore is binarized with threshold τ to yield the core index set Fc F: Mcore = 1 MaxPool2d(1 M, kernel size, stride, padding), (6) Fc = {i (Mcore)i > τ }. (7) The attention computation is then performed only for queries within this core region. Let Qtgt,c = Qtgt[Fc, :] be the subset of queries corresponding to the core indices. The attention output for this region, Hc RFcdk , is computed as: Hc = softmax (cid:33) (cid:32) Qtgt,cK dk ull Vf ull. (8) The full output matrix RSdk is then constructed by scattering the computed values Hc back to their original positions, while all other positions corresponding to the background and boundary are set to zero. Residual Fusion. The sparse output matrix is projected and added back to the input features: = Proj(H) + X. Since Hi = 0 for all / Fc, this step functions as an identity map for the background and boundary regions, ensuring they are perfectly preserved. 3.3 SPARSIFIED SPATIAL CROSS-ATTENTION Challenge: Semantic Entanglement in Image Editing. key challenge in precise editing is semantic entanglement, where textual attributes are not cleanly bound to their intended objects. This is clearly demonstrated in Figure 4, where the task is to change cat with yellow eyes to cat with green eyes. Standard models often fail, resulting in either edit attenuation, where the eyes are incompletely colored, or significant attribute leakage, causing an unnatural green tint to bleed onto the cats face. This failure stems from the competitive nature of the softmax function in cross-attention. It forces all text tokens to compete for influence over each pixel, allowing the powerful green signal to suppress the essential structural tokens like cat, which leads to the incorrect generalization. Motivation. Based on this diagnosis, we contend that semantic concepts must be disentangled before the attention softmax allows them to interfere. Our motivation is to implement pre-emptive disentanglement strategy. Instead of allowing all text tokens to participate in the attention calculation for the foreground, we introduce Sparsified Spatial Cross-Attention (SSCA), method that 6 Figure 5: Qualitative comparison of editing results. Each row corresponds to unique editing task, with the source image displayed in the first column and the source/target prompts listed below. forces the softmax to operate only on clean, disentangled subset, thus preventing attribute leakage at its source. Proposed Method. Our Sparsified Spatial Cross-Attention (SSCA) mechanism fundamentally redefines the text attention computation by breaking it down into three sequential steps: identifying key semantic tokens, computing focused sparse attention signal, and integrating this signal into the final feature map, shown in Figure 4. Identifying Key Semantic Tokens. Before computing attention, we first identify the most relevant tokens from the text prompt for the given edit region . We compute the similarity between the set of image queries within the mask, Ql,M , and all text keys Ky. The top-k text key-value pairs that exhibit the highest aggregate similarity are selected. This pre-selection step acts as filter, creating task-relevant subset of textual information, denoted as (K , ). Computing Sparse Attention Signals. With the pruned set of text tokens, we then compute sparse attention result, Asparse, only for the image queries within the edit region, Ql,M . This ensures that the computationally expensive attention operation is focused where it is needed most. (cid:32) Asparse = softmax Ql,M (K )T (cid:33) . (9) The resulting matrix Asparse RF contains highly precise and disentangled guidance signal, where is the number of foreground pixels. Constructing and Integrating the Full Attention Matrix. The sparse signal Asparse must be placed into full-size matrix to be used in the model. We construct the final text attention matrix, ASSCA RSd, by scattering the values from Asparse into zero matrix according to the mask indices F. This structurally enforces that the text prompt has zero influence on the background. ASSCA[i, :] = (cid:26)Asparse[rankF (i), :] 0 if if / , (10) 7 Table 1: Comprehensive comparison of editing quality, evaluating background preservation and CLIP similarity across various methods. Method Background Preservation CLIP Similarity Inverse Editing PSNR LPIPS 103 MSE 104 SSIM P2P DDIM P2P NT-Inv MasaCtrl DDIM Direct Inversion MasaCtrl DDIM P2P-Zero Direct Inversion P2P-Zero DDIM PnP Direct Inversion PnP ReNoise(SDXL) TurboEdit FlashEdit FlashEdit(w/ GT masks) 17.87 27.03 22.17 22.64 20.44 21.53 22.28 22.46 20.85 22.51 25.29 25. 208.80 60.67 106.62 87.94 172.22 138.98 113.46 106.06 176.84 107.27 62.55 62.78 219.88 35.86 86.97 81.09 144.12 127.32 83.64 80.45 51.78 9.32 4.36 4. 102 Whole Edited 25.01 24.75 23.96 24.38 22.80 23.31 25.41 25.41 22.44 21.86 21.16 21.35 20.54 21.05 22.55 22.62 71.14 84.11 79.67 81.33 74.67 77.05 79.05 79.68 72.44 80.09 83.21 83.08 24.41 25. 25.43 25.53 21.88 21.82 22.13 22.25 where rankF (i) maps the global index to its local index within the foreground. Finally, this purified text guidance is integrated with the source image condition, Aimg, to compute the updated hidden state hl: hl = sy ASSCA + sedit Aimg + snonedit (1 ) Aimg. This multi-step process provides maximally disentangled and precise guidance signal for the edit. (11)"
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4.1 EXPERIMENTAL SETUP Implementation Details. Our inversion network, Iθ, is initialized from SwiftBrush (Nguyen & Tran (2024),Dao et al. (2024)). Inspired by (Song et al. (2024),Ye et al. (2023),Zhang et al. (2023)), the image conditioning branch is based on an adapter, utilizing pre-trained CLIP image encoder. We train the model using the Adam optimizer (Kingma & Ba (2014)) with learning rate of 2e-5, weight decay of 2e-4, and an Exponential Moving Average (EMA). Anchoring the Solution via Supervised Training runs for 150k iterationson synthetic data from SwiftBrush. Refining with Teacher-Student Objective continues for 200k iterations using real images from CommonCanvas (Gokaslan et al. (2024)). All experiments were conducted on single NVIDIA A6000 GPU. Metrics. We evaluate our method on the PieBench benchmark (Zhang et al. (2021)), which features 700 samples across 10 editing types. We report metrics along two primary axes. As for Background Preservation, We compute PSNR (Huynh-Thu & Ghanbari (2008)), LPIPS (Zhang et al. (2018)), MSE and SSIM (Wang et al. (2004)) on the unedited regions to measure fidelity to the source image. As for Semantic Alignment, We report CLIP-Whole (Radford et al. (2021)) for prompt-image alignment and CLIP-Edited (Radford et al. (2021)) for alignment within the masked edit region. Baselines. We compare our method against state-of-the-art multi-step and few-step baselines. For multi-step methods, we evaluate Prompt-to-Prompt (P2P) (Hertz et al. (2022)), MasaCtrl (Cao et al. (2023)), Pix2Pix-Zero (Parmar et al. (2023)), and Plug-and-Play (PnP) (Zhang et al. (2021)), paired with powerful inversion techniques like DDIM (Song et al. (2020a)), Null-text Inversion (NT-Inv) (Mokady et al. (2023a)), and Direct Inversion (Ju et al. (2023)). For few-step methods, we compare against Renoise (Garibi et al. (2024)) and TurboEdit (Deutch et al. (2024)). 4.2 QUANTITATIVE ANALYSIS As shown in Table 1, our method establishes new state-of-the-art for accelerated editing. FlashEdit significantly outperforms recent few-step methods like ReNoise (Garibi et al. (2024)) and Tur8 Table 2: Ablation Study on Core Model Components. We evaluate the contribution of each module by measuring the impact on background preservation and semantic similarity (CLIP Score). The final row represents our full method. Components Background Preservation CLIP Similarity OSIE BG-Shield SSCA PSNR LPIPS103 MSE104 SSIM102 Whole Edited - - - 23.33 24.63 25. 92.37 75.36 62.55 6.60 5.01 4.36 79.97 81.65 83.21 24.14 24.77 25.43 21.23 21.22 22.13 boEdit (Deutch et al. (2024)) across all reported metrics. Crucially, it also achieves quality on par with, and in several metrics superior to, top-performing but prohibitively slow multi-step methods. This high fidelity is delivered with an extraordinary efficiency gain of over 150  (Table 3)  . Furthermore, an experiment using ground-truth (GT) masks reveals negligible performance difference, confirming the high accuracy of our self-guided masking mechanism. 4.3 QUALITATIVE ANALYSIS Visual comparisons in Figure 5 reinforce our quantitative findings. The outputs from FlashEdit consistently exhibit high semantic fidelity to the target prompt while maintaining pristine background integrity, avoiding the bleeding artifacts common in other methods. In contrast, other baselines often display noticeable quality degradation or fail to preserve is background details. unique in providing both state-of-theart visual quality and the real-time performance that multi-step methods lack. FlashEdit 4.4 ABLATION STUDIES Table 3: Efficiency comparison of individual editing methods, with the denoising steps and speedup factor for each specific combination. Method Denoising Steps Speedup Inverse Editing DDIM P2P NT-Inv P2P MasaCtrl DDIM Direct Inversion MasaCtrl P2P-Zero DDIM P2P-Zero Direct Inversion PnP DDIM PnP Direct Inversion ReNoise(SDXL) TurboEdit FlashEdit(Ours) 1.00 0.19 1.12 0.88 0.73 0.73 2.06 2.03 5.08 19.68 Multi-steps Few-steps One-step 150.84 To validate the contribution of each component in our framework, we conduct comprehensive ablation study, with the results presented in Table 2. Our baseline, consisting of the OSIE pipeline alone, establishes strong performance foundation. Integrating BG-Shield brings marked improvement across background preservation metrics, confirming its effectiveness in isolating background features. The final addition of SSCA further boosts metrics. It substantially enhances semantic alignment, evidenced by large increase in the CLIP-Edited score, which validates our pre-softmax token pruning strategy. SSCA also improves reconstruction quality, suggesting synergistic effect where cleaner textual guidance benefits the entire process. This demonstrates that all three components are critical and work in concert to achieve the final state-of-the-art performance of FlashEdit."
        },
        {
            "title": "5 CONCLUSION",
            "content": "This paper introduces FlashEdit, new paradigm for text-guided image editing that redefines the performance standard for real-time generative applications. We demonstrate that the long-standing trade-off between speed and quality is not fundamental but can be overcome with holistic, multilevel control strategy. Our approach begins by establishing temporal control with foundational OSIE pipeline for one-step inversion and editing. It then enforces spatial control with BG-Shield and fine-grained semantic control with SSCA. Together, these components transform diffusionbased editing from slow, offline process into an interactive and expressive creative tool."
        },
        {
            "title": "REFERENCES",
            "content": "Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1839218402, 2023. Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2256022570, October 2023. Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusionbased semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022. Trung Dao, Thuan Hoang Nguyen, Thanh Le, Duc Vu, Khoi Nguyen, Cuong Pham, and Anh Tran. Swiftbrush v2: Make your one-step diffusion model better than its teacher. In European Conference on Computer Vision, pp. 176192. Springer, 2024. Gilad Deutch, Rinon Gal, Daniel Garibi, Or Patashnik, and Daniel Cohen-Or. Turboedit: Text-based image editing using few-step diffusion models. In SIGGRAPH Asia 2024 Conference Papers, pp. 112, 2024. Wenkai Dong, Song Xue, Xiaoyue Duan, and Shumin Han. Prompt tuning inversion for text-driven image editing using diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 74307440, 2023. Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Shi Jie, Xiang Wang, Xiangnan He, and Tat-Seng Chua. Alphaedit: Null-space constrained knowledge editing for language models. arXiv preprint arXiv:2410.02355, 2024. Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, and Daniel Cohen-Or. Renoise: Real image inversion through iterative noising. In European Conference on Computer Vision, pp. 395413. Springer, 2024. Aaron Gokaslan, Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel, Jonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov. Commoncanvas: Open diffusion models trained on creative-commons images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82508260, 2024. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. Q. Huynh-Thu and M. Ghanbari. ment. Electronics Letters, 44:800801, 2008. //digital-library.theiet.org/doi/abs/10.1049/el%3A20080522. Scope of validity of psnr in image/video quality assessdoi: 10.1049/el:20080522. URL https: Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Direct inversion: Boosting diffusion-based editing with 3 lines of code. arXiv preprint arXiv:2310.01506, 2023. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Vladimir Kulikov, Matan Kleiner, Inbar Huberman-Spiegelglas, and Tomer Michaeli. Flowedit: Inversion-free text-based editing using pre-trained flow models. arXiv preprint arXiv:2412.08629, 2024. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Jinhao Li, Jiaming Xu, Shan Huang, Yonghua Chen, Wen Li, Jun Liu, Yaoxiu Lian, Jiayi Pan, Li Ding, Hao Zhou, et al. Large language model inference acceleration: comprehensive hardware perspective. arXiv preprint arXiv:2410.04466, 2024a. Jinhao Li, Jiaming Xu, Shiyao Li, Shan Huang, Jun Liu, Yaoxiu Lian, and Guohao Dai. Fast and efficient 2-bit llm inference on gpu: 2/4/16-bit in weight matrix with asynchronous dequanIn Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided tization. Design, pp. 19, 2024b. Xingyang Li, Muyang Li, Tianle Cai, Haocheng Xi, Shuo Yang, Yujun Lin, Lvmin Zhang, Songlin Yang, Jinbo Hu, Kelly Peng, et al. Radial attention: (nlog n) sparse attention with energy decay for long video generation. arXiv preprint arXiv:2506.19852, 2025a. Zhiteng Li, Hanxuan Li, Junyi Wu, Kai Liu, Linghe Kong, Guihai Chen, Yulun Zhang, and Xiaokang Yang. Dvd-quant: Data-free video diffusion transformers quantization. arXiv preprint arXiv:2505.18663, 2025b. Zhiteng Li, Xianglong Yan, Tianao Zhang, Haotong Qin, Dong Xie, Jiang Tian, zhongchao shi, Linghe Kong, Yulun Zhang, and Xiaokang Yang. Arb-llm: Alternating refined binarizations for large language models. In ICLR, 2025c. Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Junjie Chen, and Linfeng Zhang. From reusing to forecasting: Accelerating diffusion models with taylorseers. arXiv preprint arXiv:2503.06923, 2025. Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 20632072. IEEE, 2025. Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 60386047, 2023a. Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 60386047, 2023b. Thuan Hoang Nguyen and Anh Tran. Swiftbrush: One-step text-to-image diffusion model with variational score distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 78077816, 2024. Jiayi Pan, Jiaming Xu, Yongkang Zhou, and Guohao Dai. Specdiff: Accelerating diffusion model inference with self-speculation. arxiv preprint 2509.13848, 2025. Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. In ACM SIGGRAPH 2023 conference proceedings, pp. Zero-shot image-to-image translation. 111, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. arXiv preprint arXiv:2112.10752, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv:2010.02502, October 2020a. URL https://arxiv.org/abs/2010.02502. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020b. Kunpeng Song, Yizhe Zhu, Bingchen Liu, Qing Yan, Ahmed Elgammal, and Xiao Yang. Moma: Multimodal llm adapter for fast personalized image generation. In ECCV, 2024. Hanzhen Wang, Jiaming Xu, Jiayi Pan, Yongkang Zhou, and Guohao Dai. Specprune-vla: Accelerating vision-language-action models via action-aware self-speculative pruning. arxiv preprint 2509.05614, 2025a. Jia Wang, Jie Hu, Xiaoqi Ma, Hanghang Ma, Xiaoming Wei, and Enhua Wu. Image editing with diffusion models: survey. arXiv preprint arXiv:2504.13226, 2025b. Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, and Qifeng Chen. High-fidelity gan inversion for image attribute editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600612, 2004. doi: 10.1109/TIP.2003.819861. Junyi Wu, Zhiteng Li, Zheng Hui, Yulun Zhang, Linghe Kong, and Xiaokang Yang. Quantcache: Adaptive importance-guided quantization with hierarchical latent and layer caching for video generation. In ICCV, 2025. Jiaming Xu, Jiayi Pan, Yongkang Zhou, Siming Chen, Jinhao Li, Yaoxiu Lian, Junyi Wu, and Guohao Dai. Specee: Accelerating large language model inference with speculative early exiting. In Proceedings of the 52nd Annual International Symposium on Computer Architecture, pp. 467 481, 2025. Mengwei Xu, Mengze Zhu, Yunxin Liu, Felix Xiaozhu Lin, and Xuanzhe Liu. Deepcache: Principled cache for mobile deep vision. In Proceedings of the 24th annual international conference on mobile computing and networking, pp. 129144, 2018. Yu Xu, Fan Tang, Juan Cao, Yuxin Zhang, Xiaoyu Kong, Jintao Li, Oliver Deussen, and Tong-Yee Lee. Headrouter: training-free image editing framework for mm-dits by adaptively routing attention heads. arXiv preprint arXiv:2411.15034, 2024. Xianglong Yan, Zhiteng Li, Tianao Zhang, Linghe Kong, Yulun Zhang, and Xiaokang Yang. Recalkv: Low-rank kv cache compression via head reordering and offline calibration. arXiv preprint arXiv:2505.24357, 2025a. Xianglong Yan, Tianao Zhang, Zhiteng Li, and Yulun Zhang. Progressive binarization with semistructured pruning for llms. arXiv preprint arXiv:2502.01705, 2025b. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arxiv:2308.06721, 2023. Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc Van Gool, and Radu Timofte. Plug-and-play image restoration with deep denoiser prior. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):63606376, 2021. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing. In European conference on computer vision, pp. 592608. Springer, 2020. Jun-Yan Zhu, Philipp Krahenbuhl, Eli Shechtman, and Alexei Efros. Generative visual manipulation on the natural image manifold. In European conference on computer vision, pp. 597613. Springer, 2016."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "Shanghai Jiao Tong University"
    ]
}