{
    "paper_title": "DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation",
    "authors": [
        "Peiqi Liu",
        "Zhanqiu Guo",
        "Mohit Warke",
        "Soumith Chintala",
        "Chris Paxton",
        "Nur Muhammad Mahi Shafiullah",
        "Lerrel Pinto"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Significant progress has been made in open-vocabulary mobile manipulation, where the goal is for a robot to perform tasks in any environment given a natural language description. However, most current systems assume a static environment, which limits the system's applicability in real-world scenarios where environments frequently change due to human intervention or the robot's own actions. In this work, we present DynaMem, a new approach to open-world mobile manipulation that uses a dynamic spatio-semantic memory to represent a robot's environment. DynaMem constructs a 3D data structure to maintain a dynamic memory of point clouds, and answers open-vocabulary object localization queries using multimodal LLMs or open-vocabulary features generated by state-of-the-art vision-language models. Powered by DynaMem, our robots can explore novel environments, search for objects not found in memory, and continuously update the memory as objects move, appear, or disappear in the scene. We run extensive experiments on the Stretch SE3 robots in three real and nine offline scenes, and achieve an average pick-and-drop success rate of 70% on non-stationary objects, which is more than a 2x improvement over state-of-the-art static systems. Our code as well as our experiment and deployment videos are open sourced and can be found on our project website: https://dynamem.github.io/"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 9 9 9 4 0 . 1 1 4 2 : r DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation Peiqi Liu1, Zhanqiu Guo1, Mohit Warke1, Soumith Chintala1, 3, Chris Paxton2, Nur Muhammad Mahi Shafiullah*1 and Lerrel Pinto*1 1New York University, 2Hello Robot Inc., 3Meta Inc. Project page: dynamem.github.io Significant progress has been made in open-vocabulary mobile manipulation, where the goal is for robot to perform tasks in any environment given natural language description. However, most current systems assume static environment, which limits the systems applicability in real-world scenarios where environments frequently change due to human intervention or the robots own actions. In this work, we present DynaMem, new approach to open-world mobile manipulation that uses dynamic spatio-semantic memory to represent robots environment. DynaMem constructs 3D data structure to maintain dynamic memory of point clouds, and answers open-vocabulary object localization queries using multimodal LLMs or open-vocabulary features generated by state-of-the-art vision-language models. Powered by DynaMem, our robots can explore novel environments, search for objects not found in memory, and continuously update the memory as objects move, appear, or disappear in the scene. We run extensive experiments on the Stretch SE3 robots in three real and nine offline scenes, and achieve an average pick-and-drop success rate of 70% on non-stationary objects, which is more than 2 improvement over state-of-the-art static systems. Figure 1: An illustration of how our online dynamic spatio-semantic memory DynaMem responds to open vocabulary queries in dynamic environment. During operation and exploration, DynaMem keeps updating its semantic map in memory. DynaMem maintains voxelized pointcloud representation of the environment, and updates with dynamic changes in the environment by adding and removing points. 1. Introduction Recent advances in robotics have made it possible to deploy robots in real world settings to tackle the open vocabulary mobile manipulation (OVMM) problem [1]. Here, the robots are tasked with navigating in unknown environments and interacting with objects following open vocabulary language instructions, such as Pick up from and put it in Z, where X, Y, and could be any object name or location. The two most common approaches to tackling OVMM are using policies trained in simulation and deploying them in the real world [24], or training modular systems that combine open vocabulary navigation (OVN) [58] with different robot manipulation skills [913]. Modular Corresponding author: pl2285@nyu.edu, mahi@cs.nyu.edu. (*) denotes equal advising. DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation systems enjoy greater efficiency and success in real-world deployment [14] as they can directly leverage advances in vision and language models [9, 12], and are able to handle more diverse and out-of-domain environments with no additional training. However, as recent analysis has shown, the primary challenge in deploying modular OVMM is that limitations of module propagate to the entire system [9]. One key module in any OVMM system is the open vocabulary navigation (OVN) module responsible for navigating to goals in the environment. While many such OVN systems have been proposed in the literature [1, 513], they share common limitation: they assume static, unchanging environments. Contrast this with the real world, where environments change and objects are moved by either robots or humans. Making such restrictive assumption thus limits these systems applicability in real-world settings. The primary reason behind this assumption is the lack of an effective dynamic spatio-semantic memory that can adapt to both addition and removal of objects and obstacles in the environment online. In this work, we propose novel spatio-semantic memory architecture, Dynamic 3D Voxel Memory (DynaMem), that can adapt online to changes in the environment. DynaMem maintains voxelized pointcloud representation of an environment and adds or removes points as it observes the environment change. Additionally, it supports two different ways to query the memory with natural language: vision-language model (VLM) featurized pointcloud, and multimodal-LLM (mLLM) QA system. Finally, DynaMem enables efficient exploration in changing environments by offering dynamic obstacle map and value-based exploration map that can guide the robot to explore unseen, outdated, or query-relevant parts of the world. We evaluate DynaMem as part of full open-vocabulary mobile manipulation stack in three real world environments with multiple rounds of changes and manipulating multiple non-stationary objects, improving the static baseline by more than 2 (70% vs. 30%). Additionally, we identify an obstacle in efficiently developing dynamic spatio-semantic memory, namely the lack of dynamic benchmarks, since many OVN systems use static simulated environments [15, 16] or static datasets [17, 18]. We address this by developing new dynamic benchmark, DynaBench. It consists of 9 different environments, each changing over time. We ablate our design choices in this benchmark. To the best of our knowledge, DynaMem is the first spatio-semantic memory structure supporting both adding and removing objects. 2. Related Works 2.1. Open Vocabulary Mobile Manipulation (OVMM) Navigating to arbitrary goals in open ended environments and manipulating them has become key challenge in robotic manipulation [19, 20]. This line of query follows Open-Vocabulary Navigation systems [5, 21], which builds upon prior object and point goal navigation literature [12, 14, 22 28] which attempted navigation to points, or fixed set of objects and object categories. OVMM is naturally harder challenge as it requires an ability to handle arbitrary queries, and navigation to manipulation transfer which means unlike pure navigation, the robot needs to get close to the environment objective and obstacles. In the OVMM challenge [19], modular solutions such as [1, 13, 29] outperformed the competition. More recently, OK-Robot [9] performed extensive real-world evaluations of the challenges in OVMM and demonstrated system that achieves 58.5% success rate in static home environments. We extend this work by enabling manipulation in changing environments. 2 DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation 2.2. Spatio-semantic Memory Early works in spatio-semantic memory [3034] created semantic maps for limited categories based on mostly ad-hoc deep neural networks. Later work builds upon representations derived from pretrained vision language models, such as [6, 7, 3540]. These works use voxel map or neural feature field as their map representation. Some recent models [41, 42] have used Gaussian splats [43] to represent semantic memory for manipulation. Most of these models show object localization in pre-mapped scenes, while CLIP-Fields [5], VLMaps [21], and NLMap-SayCan [38] show integration with real robots for indoor navigation tasks. Some recent works [10, 44, 45] extend this task to include an affordance model or manipulation primitives. Our work builds upon the voxel map based spatio-semantic memory literature and extends them to dynamic environments where both objects and obstacles can change over time. Concurrent to our work, DovSG [46] looks at dynamic semantic scene graphs. As scene graphs deal with an object level abstraction, DovSG needs to handle object merging, association, and deduplication explicitly, which are all handled implicitly in DynaMem. 2.3. Mapping and Navigating Dynamic Environments For robot navigation, Simultaneous Localization and Mapping (SLAM) [47] methods are crucial. However, practical SLAM instances based on voxels [48, 49], objects [50, 51], landmark [31, 52], NeRF [53, 54], and Gaussian splats [55, 56] tend to make the simplifying assumption that the world is static. Some sparse SLAM methods improve on dynamic environments by estimating underlying state [5765] or explicitly modeling moving objects [6668]. Some methods also forego map and rely on reactive policies to navigate dynamic environments [6973], although they generally tackle local movement and not global navigation. Our work relies on SLAM systems that are stable under environment dynamics, and focuses on building dynamic semantic memory based off of online exploration and observations. 3. Method In this section, we define our problem setup, and then describe our online, dynamic spatio-semantic memory for open world, open vocabulary mobile manipulation. 3.1. Problem Statement We create our algorithm, DynaMem, to solve open vocabulary mobile manipulation (OVMM) problems in an open, constantly changing world. The goal in OVMM is for mobile robot to execute series of manipulation commands given arbitrary language goals. We assume the following requirements for the memory module for dynamic, online operation: Observations: The mobile robot is equipped with an on-board RGB-D camera, and unlike prior work [9], doesnt start with map of the environment. Rather, the robot explores the world and use the online observed sequence of posed RGB-D images to build its map. Environment dynamism: The environment can change without the knowledge of the robot. Localization queries: Given natural language query (i.e. \"teddy bear\"), the memory module has to return the 3D location of the object or determine that the object doesnt exist in the scene observed thus far. Obstacle queries: The memory module must determine whether point in space is occupied by an obstacle. Both the location of the objects and obstacles can move, previous observations often contradict each other and must be resolved by the memory. 3 DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation Figure 2: (Left) DynaMem keeps its memory stored in sparse voxel grid with associated information at each voxel. (Right) Updating DynaMem by adding new points to it, alongside the rules used to update the stored information. Note the significant upgrade in challenge in multiple facets compared to prior work [1, 513]: almost no prior work dealing with open-vocabulary queries support dynamic environments with both addition and deletion, some assumes access to prior map data, and many dont handle negative results, i.e. objects not found in memory, and instead return the best match. 3.2. Dynamic 3D Voxel Map Our answer to the challenge posed in the Section 3.1 is DynaMem. DynaMem is an evolving sparse voxel map with associated information stored at each voxel, as shown in Figure 2. In each non-empty voxel, alongside its 3D location (ğ‘¥, ğ‘¦, ğ‘§), we also store the observation count ğ¶ (how many times that voxel was observed), source image ID ğ¼ (which image the voxel was backprojected from), high-dimensional semantic feature vector ğ‘“ coming from VLM like CLIP [74] or SigLIP [75], and the latest observation time, ğ‘¡, in seconds. To make this data structure dynamic, we describe the process with which we add and update with new observations and remove outdated objects and associated voxels. Adding Points: When the robot receives new set of observations, i.e. RGB-D images with global poses, we convert them to 3D coordinates in global reference frame, and generate semantic feature vector for each point. The global coordinates are calculated from the global camera pose and the backprojected depth image using the known camera transformation matrix. We calculate the pointwise image feature by first converting the images to object patches by using segmentation model such as SAM-v2 [76], and then aggregating each patch feature over the output of vision-language models like CLIP [74] or SigLIP [75]. For more details about image-to-feature vector mapping, we refer to earlier works [5, 8, 9]. Once we have calculated the points and associated features, we cluster the new points and assign them to the nearest voxel grids. In Figure 2, we show how each voxels metadata is updated. The count keeps track of the total number of assigned points to each voxel grid, and the feature vector keeps track of the weighted average of all feature vectors assigned to that voxel. Finally, the observation time and image ID are updated to keep track of the latest observation contributing to particular voxel. If voxel was empty before assignment, we assume its count ğ¶ = 0 and feature vector ğ‘“ = 0 . Removing Points: When an object is moved or removed, its associated voxels in DynaMem may get removed. We use ray-casting to find the outdated voxels. The operation follows simple principle: if voxel falls within the frustum between the camera plane and the associated view point cloud, that 4 DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation Figure 3: high-level, 2D depiction of how adding and removing voxels from the voxel map works. New voxels are included which are in the RGB-D cameras view frustum, and old voxels that should block the view frustum but does not are removed from the map. voxel must be unoccupied. To reduce the impact of the depth noise at long range, we dont consider any pixel whose associated depth value is over 2m. We illustrate simplified 2D representation of this algorithm in Figure 3. In practice, to speed up the intersection between the sparse voxelmap and the view frustum, we project each existing voxel to the camera plane and calculate the camera distance. If the image height and width are (ğ», ğ‘Š ), the depth image is D, and certain voxel is projected to points (â„, ğ‘¤) in the camera plane with depth ğ‘‘, it gets removed if both Eq. 1 and 2 hold. (â„, ğ‘¤) [0, ğ»] [0, ğ‘Š ] ğ‘‘ (0, min(2, D[â„, ğ‘¤] + ğœ–)) (1) (2) Where Eq. 1 ensures that the point falls within the camera view, and Eq 2 ensures that (a) the depth ğ‘‘ > 0, or the object is in front of camera, (b) ğ‘‘ < 2m, or the voxel isnt too far away from the camera, and (c) ğ‘‘ < D[â„, ğ‘¤] denoting the voxel is between the camera and the currently visible object. 3.3. Querying DynaMem for Object Localization As described in Section 3.1, we define the object localization or 3D visual grounding problem as function mapping text query and posed RGBD images to either the 3D coordinate of the query object, or if the object is not in the scene. Unlike previous work, we abstain from returning location when an object is not found. To enable this, we factor this grounding problem into two sub-problems. The first is finding the latest image where the queried object could have appeared. The second is identifying whether the object is actually present in that image. For the first sub-problem, 5 DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation Figure 4: Querying DynaMem with natural language query. First, we find the voxel with the highest alighnment to the query. Next, we find the latest image of that voxel, and query with an open-vocabulary object detector to confirm the object location or abstain. we introduce two alternate approaches of visual grounding: one using the intrinsic semantic features of DynaMem, and another using state-of-the-art multimodal LLMs such as GPT-4o [77] and Gemini 1.5 Pro [78]. Embedded Vision Language Features: Vision Language Models (VLMs) such as CLIP [74] and SigLIP [75] possess an ability to embed both images and languages into the same latent space, where the similarity between an image and text object can be calculated by simply taking the dot product between the two latent representation vectors. We use this property of the embedding vectors to query our voxel map with open-vocabulary text queries. As described in Section 3.2, we convert the incoming images to point-wise image features, and embed them into our voxels. When we have new language query, we calculate its latent embedding using the VLM text encoder, and find the voxel whose feature has the highest dot product with the text embedding. Once we find the right voxel, we simply retrieve its associated latest image from our data structure as shown in Figure 4. As bonus feature, our VoxelMap can also return ğ‘˜ > 1 possible objects and associated images for single query. We do this by using DBSCAN clustering of voxels similar to [79], and returning the images associated with the most aligned voxel in top-ğ‘˜ clusters. Multimodal Large Language Models (mLLMs): For this approach, we note that the problem of finding the latest image where an object may appear is similar to the problem of visual question-answer (VQA) [80]. Since we fully rely on pretrained models to build our map, we pose this multi-image VQA problem as an mLLM QA problem similar to OpenEQA [81]. We show in Figure 5 how we query the mLLMs to solve the visual grounding query. We give the model sequence of our latest environment observations images and ask the model for the index of the last image where the queried object was observed. We additionally instruct the model to respond None if the object was not observed in any image. Note that, unlike OpenEQA [81], we only pass the RGB images to the mLLM, and not the depth or camera pose. Similarly, we only ask for an image 6 DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation Figure 5: The prompting system for querying multimodal LLMs such as GPT-4o or Gemini-1.5 for the image index for an object query. index, and not full textual answer. One important hyperparameter for this mLLM query method is the maximum number of images included in the prompt. Longer context needs longer processing time and potentially includes outdated information, while short context might not include all information and thus will miss objects. We optimize the context by excluding completely outdated images: all images ğ¼ with no voxel pointing to them are deleted. This filtering increases mLLM context utilization. We set Gemini as our base model and 60 as our query image limit since Gemini context can fit 60 images, which is twice as large as GPT-4os context size. Combining the Approaches: From the discussion above, and from our real-world experiments as shown in Section 4, we see that the downsides of these two methods in practice are somewhat complementary. The VoxelMap can easily process large number of observation images over time, but it struggles to disambiguate between multiple similar but not quite same objects. On the other hand, mLLM based methods can distinguish between fine differences in query objects, but can only handle few images at time. As result, we come up with hybrid approach taking advantage of the best of both methods. For this approach, we build and process the VoxelMap as usual. For the hybrid querying, parametrized by an integer ğ‘˜, we retrieve top ğ‘˜ candidate images from the VoxelMap for the given query. Then, we pass those ğ‘˜ images into the mLLM, and query them as usual for the mLLM approach. The mLLM answers with the latest image where the query object may appear, which we then use for the downstream processing. Note that, this approach generalizes both of our individual approaches: when ğ‘˜ = 1, it converges to the VLM-feature-only approach, and when ğ‘˜ it converges to the mLLM-only approach. Handling Absence of Queried Object: Several previous methods [5, 8, 9] assume that the queried 7 DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation object is always present in the scene, and always responds with the object that is the best match to the query. However, this often results in high false-positive failure cases. For example, in scene with no red cups and blue cup, the method may respond with the location of the blue cup in response to the query red cup. For this reason, we locate objects in two stages. First, we find the best candidate image where the object may have been seen (Section 3.3). Then, we use an open-vocabulary object detector model such as OWL-v2 [82] to search that image for the queried object (Figure 4). If we dont find the queried object, we assume that the object has either moved, or the response from the voxelmap or mLLM was inaccurate, and respond with object not found. If the open-vocabulary object detector returns an object bounding box, we find the median pixel from the object mask and return its 3D location. 3.4. Robot Navigation and Exploration To navigate in real-world environment, robots use an obstacle map in conjunction with navigation algorithm like A* in [9, 21]. We use simple voxel-projection strategy to build an obstacle map. Due to the depth observation noise, we simply set threshold for the ground (0.2m for our experiments), and project all the voxels above that ğ‘§-threshold as the obstacles in our map. The voxels below the threshold are projected into the 2D obstacle map as navigable points. Finally, the points in the map that are not marked as either obstacle or navigable are marked as explorable points. Exploration Primitives: Since our robot does not start with an environment map, it explores the environment with frontier based methods to build the map. We can further accelerate this process by providing exploration guidance. Based on the current status of the map, DynaMem provides an exploration value function to accelerate the exploration process both for building and updating the map. We provide two value-based exploration maps: one time-based, and one semantic-similaritybased [83]. The time-based value map prioritizes the least-recently seen points. If the current time is ğ‘‡ , and the last-seen time of voxel (ğ‘¥, ğ‘¦, ğ‘§) is ğ‘¡ğ‘¥,ğ‘¦,ğ‘§, the temporal value map Vğ‘‡ is expressed as: T*[ğ‘¥, ğ‘¦] = max Vğ‘‡ [ğ‘¥, ğ‘¦] = ğœ (ğ›½ğ‘‡ ğ‘§ (ğ‘‡ ğ‘¡ğ‘¥,ğ‘¦,ğ‘§) (T*[ğ‘¥, ğ‘¦] ğœ‡ğ‘‡ )) where ğ›½ğ‘‡ , ğœ‡ğ‘‡ are hyper-parameters and ğœ is the sigmoid function. Similarly, if the VLM feature at voxel (ğ‘¥, ğ‘¦, ğ‘§) is ğ‘“ğ‘¥,ğ‘¦,ğ‘§, and the VLM feature for the language query is ğ‘“ğ‘, then the similarity-based value map Vğ‘† is be expressed as: S*[ğ‘¥, ğ‘¦] = max Vğ‘†[ğ‘¥, ğ‘¦] = ğœ (ğ›½ğ‘† ğ‘§ (ğ‘“ğ‘ ğ‘“ğ‘¥,ğ‘¦,ğ‘§) (S*[ğ‘¥, ğ‘¦] ğœ‡ğ‘† )) where once again ğ›½ğ‘†, ğœ‡ğ‘† are hyperparameters. We may also linearly combine Vğ‘‡ , Vğ‘† to balance our exploration between last seen time and semantic similarity. Finally, since the environment may be dynamic, we convert our navigation algorithm from openloop to closed-loop. The robot, instead of executing the entire navigation plan generated by A*, stops after the first seven waypoints (approx. 0.7 to 1 meters). Then, the robot scans the environment, updates the map, and moves according to new plan. The robot repeats these steps until its distance to the target is lower than predefined threshold. 8 DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation 4. Experiments We evaluate our method, DynaMem, on Hello Robot: Stretch SE3 in real world environments, and perform series of ablation experiments in an offline benchmark. Figure 6: Real robot experiments in three different environments: kitchen, game room, and meeting room. In each environment, we modify the environment thrice and run 10 pick-and-drop queries. 4.1. Real-world Experiments We evaluate DynaMem and its impact on open-vocabulary mobile manipulation in three real-world dynamic environments (Figure 6). In each environment, we set up multiple objects as potential manipulation targets, change the environment in three rounds, and execute 10 pick-and-drop queries over the rounds We use the Hello Stretch SE3 as our mobile robot platform, and use its head-mounted 9 DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation Intel RealSense D435 RGB-D camera to collect the input data. To build complete pick-and-drop system around DynaMem, we follow the system architecture in OK-Robot [9]. In particular, we use the AnyGrasp [84] based open-vocabulary grasp system and use the heuristic based dropping system. However, we use DynaMems exploration primitives let the robot build the map of the environment and allow the robot to explore when an object is not found in the memory. As baseline, we compare with OK-Robot [9], state-of-the-art method for OVMM. OK-Robot uses static voxelmap as its memory representation, and thus it highlights the importance of dynamic memory for OVMM in changing environment. For DynaMem, we run two variations of the algorithm in the real world: one with VLM-feature based queries and one with mLLM-QA based queries. Figure 7: Statistics of failure, broken down by failure modes, in our real robot experiments in the lab and in home environments. Statistics are collected over three environments and 30 open-vocabulary pick-and-drop queries for the lab experiments, and two environments and 17 pick-and-drop queries for the home environments, on objects whose locations change over time. Results: Our experiments in three dynamic environments and with 30 queries is summarized in Figure 7(a & b). We find that DynaMem with both VLM-feature based and mLLM-QA based queries have total success rate of 70%. This is significant improvement over the OK-Robot system, which has total success rate of 30%. Notably, DynaMem is particularly adept at handling dynamic objects in the environment: only 6.7% of the trials failed due to our system not being able to navigate to such dynamic objects in the scene. This is in contrast to the OK-Robot system, where 53.3% of the trials failed because it could not find an object that moved in the environment. In contrast, navigating to static goals fails in only 10% of the cases for DynaMem with VLM-feature, 13.3% for OK-Robot and 20% for DynaMem with mLLM-QA. DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation We observe that common localization failure for VLM-feature based queries is that those features often act like bag-of-words model; for example they may find blue bowl when queried for red bowl. On the other hand, mLLM-QA based queries often fail because the objects are not present in the short context window, or the images in the context window are outdated. Since their failure modes are complementary, we hope to combine the two methods in future work to improve the overall performance. 4.2. Deployment in Home Environments To understand the applicability of our system beyond lab environments that emulate real living space, we deployed DynaMem to two apartments in New York City. We ran experiments with DynaMem on one one-room and one two-room environment. We ran total of 17 long-horizon trials to understand the possible failure modes when deployed on real scene. Since we observe complementary cases of failure from the two querying methods (Section 3.3) in our lab experiments in Section 4.1, we run these experiments with the hybrid querying method, setting the number of images returned by VoxelMap to be processed by mLLMs to be ğ‘˜ = 3. We found total of 9 successes in our 17 real home trials, but it is significantly more interesting for us to understand how these systems may fail in real environments, as shown in Figure 7(c). We first observe that out of the 8 failures, 4 happened due to poor navigation, 3 happened due to our manipulation skills failing, and 1 was an error in placing the object at the target. Out of the navigation failures, half happened because of the target object was not found or was confused with another object, and the other half happened due to navigating to suboptimal location for manipulation. 4.3. Ablations on an Offline Benchmark Running real robot OVMM experiments can be expensive and time-consuming. So, we developed an offline benchmark called DynaBench to easily evaluate dynamic 3D visual grounding algorithms on dynamic environments and perform algorithmic ablations. The benchmark isolates the query-response part of the dynamic semantic memory without robot navigation, exploration, and manipulation. Data Collection: In the real world, the robot collects its own map-building data by exploring the environments. Following this, we collect the robots runtime sensor data from three environments. To further enrich our benchmark, we simulate this process by taking posed RGB-D images on an iPhone Pro in six more environments. In all cases, we emulate environment dynamics by moving objects and obstacle locations in three successive rounds. Data Labeling and Evaluation: We manually annotate queries and responses in the dataset. Each query has an associated natural language label ğ‘, object location ğ‘‹ = (ğ‘¥, ğ‘¦, ğ‘§), and an object radius ğœ–. Since the environment is dynamic, each query also has an associated time ğ‘¡. For evaluation, at time ğ‘¡ (i.e. after the memory algorithm has observed all the input data points with timestamp < ğ‘¡), we query the model with ğ‘. If it predicts an object location ğ‘‹ = (ğ‘¥, ğ‘¦, ğ‘§), its success if ğ‘‹ ğ‘‹ 2 ğœ– and failure otherwise. Since the robot may also encounter queries for objects it has not observed yet, we emulate negative queries by adding queries for objects (a) that have not been observed yet, or (b) that have been observed but were subsequently removed. For both of these query types, the model must respond with not found; otherwise its counted as failure. Evaluation Results: Using our offline benchmark, we ablate design decisions of DynaMem as discussed in Section 3. Among these design decisions, the primary are: using feature embedding-based vs. mLLM-QA based language grounding, ablating components such as point removal or abstentiation from the algorithm, and trying different mLLMs. Due to API costs, we only evaluate Gemini models 11 DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation Table 1: Ablating the design choices for our query methods for DynaMem on the offline DynaBench benchmark. We also present results from five human participants to ground the performances. Query type Variant Success rate Human (average over five participants) VLM-feature default (adding and removing points) only adding points no OWL-v2 cross-check no similarity thresholding mLLM-QA default (Gemini Pro 1.5) Gemini Pro 1.5, no voxelmap filtering Gemini Flash 1.5 Hybrid VLM-feature mLLM (ğ‘˜ = 3) 81.9% 70.6% 67.8% 59.2% 66.8% 67.3% 66.8% 63.5% 74.5% on the benchmark. We present our results in Table 1. We see that performance of VLM-features and mLLM-QA follows the same order in the real world in the benchmark, corroborating the benchmark design. The best design choices are to both add and remove points, and to cross check with OWL-v2 on top of similarity thresholding for VLMfeature based grounding. For mLLM-QA based grounding, Gemini Pro outperforms Gemini Flash, and voxelmap based image filtering benefits the method. Moreover, we see that the hybrid method that uses VoxelMap feature filtering and then sends the top 3 images to the mLLM performs better than either method individually. 5. Conclusions and Limitations In this work, we introduced DynaMem, spatio-semantic memory for open-vocabulary mobile manipulation that can handle changes to the environment during operation. We showed in three real world environments that DynaMem can navigate to, pick, and drop objects even while object and obstacle locations are changing. In the future, we could improve DynaMem performance by merging the VLM-feature queries and mLLM-QA queries, as they show complementary failure cases. Similarly, reasoning over both object and voxel level abstraction could speed up environment update when objects move. Our current system experiences large number of manipulation failures: using mLLMs to detect and recover from failures [85] may increase the performance of the overall system. Finally, integrating with more skills, such as searching in cabinets or drawers, would improve the applicability of such OVMM systems in the real world."
        },
        {
            "title": "References",
            "content": "[1] S. Yenamandra, A. Ramachandran, K. Yadav, A. Wang, M. Khanna, T. Gervet, T.-Y. Yang, V. Jain, A. W. Clegg, J. Turner, Z. Kira, M. Savva, A. Chang, D. S. Chaplot, D. Batra, R. Mottaghi, Y. Bisk, and C. Paxton, Homerobot: Open-vocabulary mobile manipulation, 2024. [Online]. Available: https://arxiv.org/abs/2306.11565 [2] K. Ehsani, T. Gupta, R. Hendrix, J. Salvador, L. Weihs, K.-H. Zeng, K. P. Singh, Y. Kim, W. Han, A. Herrasti, et al., Imitating shortest paths in simulation enables effective navigation and manipulation in the real world, arXiv preprint arXiv:2312.02976, 2023. 12 DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation [3] R. Ramrakhya, D. Batra, E. Wijmans, and A. Das, Pirlnav: Pretraining with imitation and rl finetuning for objectnav, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2023, pp. 17 89617 906. [4] K.-H. Zeng, Z. Zhang, K. Ehsani, R. Hendrix, J. Salvador, A. Herrasti, R. Girshick, A. Kembhavi, and L. Weihs, Poliformer: Scaling on-policy rl with transformers results in masterful navigators, 2024. [Online]. Available: https://arxiv.org/abs/2406. [5] N. M. M. Shafiullah, C. Paxton, L. Pinto, S. Chintala, and A. Szlam, Clip-fields: robotic memory, 2023. [Online]. Available: Weakly supervised semantic fields for https://arxiv.org/abs/2210.05663 [6] Q. Gu, A. Kuwajerwala, S. Morin, K. M. Jatavallabhula, B. Sen, A. Agarwal, C. Rivera, W. Paul, K. Ellis, R. Chellappa, C. Gan, C. M. de Melo, J. B. Tenenbaum, A. Torralba, F. Shkurti, and L. Paull, Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning, 2023. [Online]. Available: https://arxiv.org/abs/2309.16650 [7] D. Maggio, Y. Chang, N. Hughes, M. Trang, D. Griffith, C. Dougherty, E. Cristofalo, L. Schmid, and L. Carlone, Clio: Real-time task-driven open-set 3d scene graphs, 2024. [Online]. Available: https://arxiv.org/abs/2404.13696 [8] J. Kerr, C. M. Kim, K. Goldberg, A. Kanazawa, and M. Tancik, Lerf: Language embedded radiance fields, in International Conference on Computer Vision (ICCV), 2023. [9] P. Liu, Y. Orru, J. Vakil, C. Paxton, N. M. M. Shafiullah, and L. Pinto, Ok-robot: What really matters in integrating open-knowledge models for robotics, 2024. [Online]. Available: https://arxiv.org/abs/2401.12202 [10] R.-Z. Qiu, Y. Hu, G. Yang, Y. Song, Y. Fu, J. Ye, J. Mu, R. Yang, N. Atanasov, S. Scherer, and X. Wang, Learning generalizable feature fields for mobile manipulation, 2024. [Online]. Available: https://arxiv.org/abs/2403.07563 [11] B. Bolte, A. Wang, J. Yang, M. Mukadam, M. Kalakrishnan, and C. Paxton, Usa-net: Unified semantic and affordance representations for robot memory, 2023. [Online]. Available: https://arxiv.org/abs/2304.12164 [12] M. Chang, T. Gervet, M. Khanna, S. Yenamandra, D. Shah, S. Y. Min, K. Shah, C. Paxton, S. Gupta, D. Batra, R. Mottaghi, J. Malik, and D. S. Chaplot, Goat: Go to any thing, 2023. [Online]. Available: https://arxiv.org/abs/2311.06430 [13] A. Werby, C. Huang, M. BÃ¼chner, A. Valada, and W. Burgard, Hierarchical open-vocabulary 3d scene graphs for language-grounded robot navigation, 2024. [Online]. Available: https://arxiv.org/abs/2403.17846 [14] T. Gervet, S. Chintala, D. Batra, J. Malik, and D. S. Chaplot, Navigating to objects in the real world, Science Robotics, vol. 8, no. 79, p. eadf6991, 2023. [Online]. Available: https://www.science.org/doi/abs/10.1126/scirobotics.adf [15] D. Z. Chen, A. X. Chang, and M. NieÃŸner, Scanrefer: 3d object localization in rgb-d scans using natural language, 2020. [Online]. Available: https://arxiv.org/abs/1912.08830 [16] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. NieÃŸner, Scannet: Richly-annotated 3d reconstructions of indoor scenes, 2017. [Online]. Available: https: //arxiv.org/abs/1702.04405 13 DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation [17] K. Yadav, R. Ramrakhya, S. K. Ramakrishnan, T. Gervet, J. Turner, A. Gokaslan, N. Maestre, A. X. Chang, D. Batra, M. Savva, et al., Habitat-matterport 3d semantics dataset, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 49274936. [18] G. Baruch, Z. Chen, A. Dehghan, T. Dimry, Y. Feigin, P. Fu, T. Gebauer, B. Joffe, D. Kurz, A. Schwartz, et al., Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data, arXiv preprint arXiv:2111.08897, 2021. [19] S. Yenamandra, A. Ramachandran, M. Khanna, K. Yadav, D. S. Chaplot, G. Chhablani, A. Clegg, T. Gervet, V. Jain, R. Partsey, R. Ramrakhya, A. Szot, T.-Y. Yang, A. Edsinger, C. Kemp, B. Shah, Z. Kira, D. Batra, R. Mottaghi, Y. Bisk, and C. Paxton, The homerobot open vocab mobile manipulation challenge, in Thirty-seventh Conference on Information Processing Systems: Competition Track, 2023. [Online]. Available: Neural https://aihabitat.org/challenge/2023_homerobot_ovmm/ [20] N. Yokoyama, A. Clegg, J. Truong, E. Undersander, T.-Y. Yang, S. Arnaud, S. Ha, D. Batra, and A. Rai, ASC: Adaptive skill coordination for robotic mobile manipulation, arXiv preprint arXiv:2304.00410, 2023. [21] C. Huang, O. Mees, A. Zeng, and W. Burgard, Visual language maps for robot navigation, 2023. [Online]. Available: https://arxiv.org/abs/2210.05714 [22] A. Majumdar, A. Shrivastava, S. Lee, P. Anderson, D. Parikh, and D. Batra, Improving visionand-language navigation with image-text pairs from the web, in ECCV. Springer, 2020, pp. 259274. [23] J. Krantz, S. Lee, J. Malik, D. Batra, and D. S. Chaplot, Instance-specific image goal navigation: Training embodied agents to find object instances, arXiv preprint arXiv:2211.15876, 2022. [24] M. Hahn, D. S. Chaplot, S. Tulsiani, M. Mukadam, J. M. Rehg, and A. Gupta, No rl, no simulation: Learning to navigate without navigating, Advances in Neural Information Processing Systems, vol. 34, pp. 26 66126 673, 2021. [25] D. S. Chaplot, D. P. Gandhi, A. Gupta, and R. R. Salakhutdinov, Object goal navigation using goal-oriented semantic exploration, in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 42474258. [Online]. Available: https://proceedings.neurips.cc/paper_files/ paper/2020/file/2c75cf2681788adaca63aa95ae028b22-Paper.pdf [26] N. Yokoyama, S. Ha, and D. Batra, Success weighted by completion time: dynamics-aware evaluation criteria for embodied navigation, in 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021, pp. 15621569. [27] X. Zhao, H. Agrawal, D. Batra, and A. G. Schwing, The surprising effectiveness of visual odometry techniques for embodied pointgoal navigation, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2021, pp. 16 12716 136. [28] D. Batra, A. Gokaslan, A. Kembhavi, O. Maksymets, R. Mottaghi, M. Savva, A. Toshev, and E. Wijmans, Objectnav revisited: On evaluation of embodied agents navigating to objects, CoRR, vol. abs/2006.13171, 2020. [Online]. Available: https://arxiv.org/abs/2006.13171 [29] A. Melnik, M. BÃ¼ttner, L. Harz, L. Brown, G. C. Nandi, A. PS, G. K. Yadav, R. Kala, and R. Haschke, Uniteam: Open vocabulary mobile manipulation challenge, 2023. [Online]. Available: https://arxiv.org/abs/2312.08611 14 DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation [30] P. Henry, M. Krainin, E. Herbst, X. Ren, and D. Fox, Rgb-d mapping: Using kinect-style depth cameras for dense 3d modeling of indoor environments, International Journal of Robotic Research - IJRR, vol. 31, pp. 647663, 04 2012. [31] S. L. Bowman, N. Atanasov, K. Daniilidis, and G. J. Pappas, Probabilistic data association for semantic slam, in 2017 IEEE international conference on robotics and automation (ICRA). IEEE, 2017, pp. 17221729. [32] L. Zhang, L. Wei, P. Shen, W. Wei, G. Zhu, and J. Song, Semantic slam based on object detection and improved octomap, IEEE Access, vol. 6, pp. 75 54575 559, 2018. [33] L. Ma, J. StÃ¼ckler, C. Kerl, and D. Cremers, Multi-view deep learning for consistent semantic mapping with rgb-d cameras, in 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2017, pp. 598605. [34] D. S. Chaplot, D. P. Gandhi, A. Gupta, and R. R. Salakhutdinov, Object goal navigation using goal-oriented semantic exploration, Advances in Neural Information Processing Systems, vol. 33, pp. 42474258, 2020. [35] H. Ha and S. Song, Semantic abstraction: Open-world 3d scene understanding from 2d visionlanguage models, 2022. [36] N. M. M. Shafiullah, C. Paxton, L. Pinto, S. Chintala, and A. Szlam, Clip-fields: Weakly supervised semantic fields for robotic memory, 2023. [37] C. Huang, O. Mees, A. Zeng, and W. Burgard, Visual language maps for robot navigation, IEEE, 2023, pp. in 2023 IEEE International Conference on Robotics and Automation (ICRA). 10 60810 615. [38] B. Chen, F. Xia, B. Ichter, K. Rao, K. Gopalakrishnan, M. S. Ryoo, A. Stone, and D. Kappler, Open-vocabulary queryable scene representations for real world planning, in arXiv preprint arXiv:2209.09874, 2022. [39] K. M. Jatavallabhula, A. Kuwajerwala, Q. Gu, M. Omama, T. Chen, S. Li, G. Iyer, S. Saryazdi, N. Keetha, A. Tewari, et al., Conceptfusion: Open-set multimodal 3d mapping, arXiv preprint arXiv:2302.07241, 2023. [40] J. Kerr, C. M. Kim, K. Goldberg, A. Kanazawa, and M. Tancik, Lerf: Language embedded radiance fields, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 19 72919 739. [41] M. Ji, R.-Z. Qiu, X. Zou, and X. Wang, Graspsplats: Efficient manipulation with 3d feature splatting, arXiv preprint arXiv:2409.02084, 2024. [42] O. Shorinwa, J. Tucker, A. Smith, A. Swann, T. Chen, R. Firoozi, M. D. Kennedy, and M. Schwager, Splat-mover: Multi-stage, open-vocabulary robotic manipulation via editable gaussian splatting, in 8th Annual Conference on Robot Learning, 2024. [43] B. Kerbl, G. Kopanas, T. LeimkÃ¼hler, and G. Drettakis, 3d gaussian splatting for real-time radiance field rendering, ACM Transactions on Graphics, vol. 42, no. 4, July 2023. [Online]. Available: https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/ [44] B. Bolte, A. Wang, J. Yang, M. Mukadam, M. Kalakrishnan, and C. Paxton, Usa-net: Unified semantic and affordance representations for robot memory, 2023. 15 DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation [45] Y. Wang, Z. Li, M. Zhang, K. Driggs-Campbell, J. Wu, L. Fei-Fei, and Y. Li, D3 fields: Dynamic 3d descriptor fields for zero-shot generalizable robotic manipulation, arXiv preprint arXiv:2309.16118, 2023. [46] Z. Yan, S. Li, Z. Wang, L. Wu, H. Wang, J. Zhu, L. Chen, and J. Liu, Dynamic open-vocabulary 3d scene graphs for long-term language-guided mobile manipulation, 2024. [Online]. Available: https://arxiv.org/abs/2410. [47] H. Durrant-Whyte and T. Bailey, Simultaneous localization and mapping: part i, IEEE robotics & automation magazine, vol. 13, no. 2, pp. 99110, 2006. [48] Z. Song, G. Zhang, J. Xie, L. Liu, C. Jia, S. Xu, and Z. Wang, Voxelnextfusion: simple, unified and effective voxel fusion framework for multi-modal 3d object detection, arXiv preprint arXiv:2401.02702, 2024. [49] W. Shi, J. Xu, D. Zhu, G. Zhang, X. Wang, J. Li, and X. Zhang, Rgb-d semantic segmentation and label-oriented voxelgrid fusion for accurate 3d semantic mapping, IEEE transactions on circuits and systems for video technology, vol. 32, no. 1, pp. 183197, 2021. [50] J. McCormac, R. Clark, M. Bloesch, A. Davison, and S. Leutenegger, Fusion++: Volumetric IEEE, 2018, pp. 3241. object-level slam, in 2018 international conference on 3D vision (3DV). [51] G. S. Krishna, K. Supriya, and S. Baidya, 3ds-slam: 3d object detection based semantic slam towards dynamic indoor environments, arXiv preprint arXiv:2310.06385, 2023. [52] E. Michael, T. Summers, T. A. Wood, C. Manzie, and I. Shames, Probabilistic data association for semantic slam at scale, in 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2022, pp. 43594364. [53] D. Maggio, M. Abate, J. Shi, C. Mario, and L. Carlone, Loc-nerf: Monte carlo localization using neural radiance fields, in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 40184025. [54] A. Rosinol, J. J. Leonard, and L. Carlone, Nerf-slam: Real-time dense monocular slam with neural radiance fields, in 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2023, pp. 34373444. [55] H. Matsuki, R. Murai, P. H. Kelly, and A. J. Davison, Gaussian splatting slam, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 18 03918 048. [56] C. Yan, D. Qu, D. Xu, B. Zhao, Z. Wang, D. Wang, and X. Li, Gs-slam: Dense visual slam with 3d gaussian splatting, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 19 59519 604. [57] Y. Qiu, C. Wang, W. Wang, M. Henein, and S. Scherer, Airdos: Dynamic slam benefits from articulated objects, in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 80478053. [58] L. Cui and C. Ma, Sof-slam: semantic visual slam for dynamic environments, IEEE access, vol. 7, pp. 166 528166 539, 2019. [59] N. Brasch, A. Bozic, J. Lallemand, and F. Tombari, Semantic monocular slam for highly dynamic environments, in 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018, pp. 393400. 16 DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation [60] C. Yu, Z. Liu, X.-J. Liu, F. Xie, Y. Yang, Q. Wei, and Q. Fei, Ds-slam: semantic visual slam towards dynamic environments, in 2018 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2018, pp. 11681174. [61] S. Song, H. Lim, A. J. Lee, and H. Myung, Dynavins: visual-inertial slam for dynamic environments, IEEE Robotics and Automation Letters, vol. 7, no. 4, pp. 11 52311 530, 2022. [62] P. Yu, C. Guo, y. Liu, and H. Zhang, Fusing semantic segmentation and object detection for visual slam in dynamic scenes, in Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology, 2021, pp. 17. [63] B. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, Dynaslam: Tracking, mapping, and inpainting in dynamic scenes, IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 40764083, 2018. [64] L. Schmid, M. Abate, Y. Chang, and L. Carlone, Khronos: unified approach for spatio-temporal metric-semantic slam in dynamic environments, in Proc. of Robotics: Science and Systems, 2024. [65] J. C. Virgolino Soares, V. S. Medeiros, G. F. Abati, M. Becker, G. Caurin, M. Gattass, and M. A. Meggiolaro, Visual localization and mapping in dynamic and changing environments, Journal of Intelligent & Robotic Systems, vol. 109, no. 4, p. 95, 2023. [66] B. Bescos, C. Campos, J. D. TardÃ³s, and J. Neira, Dynaslam ii: Tightly-coupled multi-object tracking and slam, IEEE robotics and automation letters, vol. 6, no. 3, pp. 51915198, 2021. [67] M. Henein, J. Zhang, R. Mahony, and V. Ila, Dynamic slam: The need for speed, in 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2020, pp. 21232129. [68] D. F. Henning, T. Laidlow, and S. Leutenegger, Bodyslam: Joint camera localisation, mapping, and human motion tracking, in European Conference on Computer Vision. Springer, 2022, pp. 656673. [69] J. Haviland, N. SÃ¼nderhauf, and P. Corke, holistic approach to reactive mobile manipulation, IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 31223129, 2022. [70] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, et al., Do as can, not as say: Grounding language in robotic affordances, arXiv preprint arXiv:2204.01691, 2022. [71] Y. Du, D. Ho, A. Alemi, E. Jang, and M. Khansari, Bayesian imitation learning for end-to-end PMLR, 2022, pp. mobile manipulation, in International Conference on Machine Learning. 55315546. [72] J. Wong, A. Tung, A. Kurenkov, A. Mandlekar, L. Fei-Fei, S. Savarese, and R. MartÃ­n-MartÃ­n, Error-aware imitation learning from teleoperation data for mobile manipulation, in Conference on Robot Learning. PMLR, 2022, pp. 13671378. [73] S. Uppal, A. Agarwal, H. Xiong, K. Shaw, and D. Pathak, Spin: Simultaneous perception, interaction and navigation, 2024. [Online]. Available: https://arxiv.org/abs/2405. [74] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning transferable visual models from natural language supervision, 2021. [Online]. Available: https://arxiv.org/abs/2103.00020 [75] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer, Sigmoid loss for language image pre-training, 2023. [Online]. Available: https://arxiv.org/abs/2303.15343 17 DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation [76] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. RÃ¤dle, C. Rolland, L. Gustafson, E. Mintun, J. Pan, K. V. Alwala, N. Carion, C.-Y. Wu, R. Girshick, P. DollÃ¡r, and C. Feichtenhofer, Sam 2: Segment anything in images and videos, arXiv preprint arXiv:2408.00714, 2024. [Online]. Available: https://arxiv.org/abs/2408. [77] O. Team, Gpt-4 technical report, 2024. [Online]. Available: https://arxiv.org/abs/2303.08774 [78] G. T. Google, Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. [Online]. Available: https://arxiv.org/abs/2403.05530 [79] J. Yang, X. Chen, S. Qian, N. Madaan, M. Iyengar, D. F. Fouhey, and J. Chai, Llm-grounder: Open-vocabulary 3d visual grounding with large language model as an agent, 2023. [Online]. Available: https://arxiv.org/abs/2309.12311 [80] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh, Vqa: Visual question answering, in Proceedings of the IEEE international conference on computer vision, 2015, pp. 24252433. [81] A. Majumdar, A. Ajay, X. Zhang, P. Putta, S. Yenamandra, M. Henaff, S. Silwal, P. Mcvay, O. Maksymets, S. Arnaud, et al., Openeqa: Embodied question answering in the era of foundation models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 16 48816 498. [82] M. Minderer, A. Gritsenko, and N. Houlsby, Scaling open-vocabulary object detection, 2024. [Online]. Available: https://arxiv.org/abs/2306.09683 [83] N. Yokoyama, S. Ha, D. Batra, J. Wang, and B. Bucher, Vlfm: Vision-language frontier maps for zero-shot semantic navigation, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 4248. [84] H.-S. Fang, C. Wang, H. Fang, M. Gou, J. Liu, H. Yan, W. Liu, Y. Xie, and C. Lu, Anygrasp: Robust and efficient grasp perception in spatial and temporal domains, IEEE Transactions on Robotics, 2023. [85] H. Etukuru, N. Naka, Z. Hu, S. Lee, J. Mehu, A. Edsinger, C. Paxton, S. Chintala, L. Pinto, and N. M. M. Shafiullah, Robot utility models: General policies for zero-shot deployment in new environments, arXiv preprint arXiv:2409.05865, 2024."
        }
    ],
    "affiliations": [
        "New York University",
        "Hello Robot Inc.",
        "Meta Inc."
    ]
}