{
    "paper_title": "EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization",
    "authors": [
        "Wei-Po Hsin",
        "Ren-Hao Deng",
        "Yao-Ting Hsieh",
        "En-Ming Huang",
        "Shih-Hao Hung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL."
        },
        {
            "title": "Start",
            "content": "EVOLVE: EVOLUTIONARY SEARCH FOR LLM-BASED VERILOG GENERATION AND OPTIMIZATION 6 2 0 2 6 2 ] . [ 1 7 6 0 8 1 . 1 0 6 2 : r Wei-Po Hsin* Department of Electrical Engineering National Taiwan University b10901053@ntu.edu.tw Ren-Hao Deng* Department of Computer Science and Information Engineering National Taiwan University r13922212@csie.ntu.edu.tw Yao-Ting, Hsieh* Institute of Information Science Academia Sinica, Taiwan elton731888@gmail.com En-Ming Huang Department of Computer Science and Information Engineering National Taiwan University r13922078@csie.ntu.edu.tw Shih-Hao Hung Department of Computer Science and Information Engineering National Taiwan University hungsh@csie.ntu.edu.tw"
        },
        {
            "title": "ABSTRACT",
            "content": "Verilogs design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL. 1 Keywords Evolution Framework Verilog Code Generation PPA Optimization LLM-Aided Design"
        },
        {
            "title": "Introduction",
            "content": "As digital hardware grows increasingly complex, the industry faces critical need for faster development cycles without sacrificing circuit performance. While Hardware Description Languages (HDLs) like Verilog are the standard for providing fine-grained control over synchronous circuits, relying solely on manual RTL coding has become significant bottleneck as the process is inherently labor-intensive and prone to delays. To overcome these limitations, promising new paradigm has emerged: leveraging Large Language Models (LLMs) to automate and accelerate hardware design and verification. Fundamentally, LLMs are powerful probabilistic models trained on vast text and code corpora. Their fundamental nature is to learn complex statistical patterns to predict the next token in sequence, capability that enables logical, step-by-step reasoning. Recent advancements (Novikov et al., 2025; Yuan et al., 2025) have further enhanced this Equal contribution Corresponding author 1The EvolVE codebase, Mod-VerilogEval v2, and evaluation scripts will be released shortly. EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization potential through advanced reasoning techniques and evolutionary frameworks, enabling agents to iteratively refine their outputs and solve intricate logical problems. However, this proficiency in sequential reasoning lacks alignment with the requirements of hardware design. HDLs like Verilog describe highly concurrent systems, where thousands of operations execute in parallel, synchronized by clock. This strict and concurrent paradigm poses significant hurdle for probabilistic models trained on sequential logic. Consequently, direct application of LLMs to HDL generation often results in functionally incorrect or non-synthesizable Verilog. This performance gap underscores the insufficiency of the base model alone, with the true promise for IC design residing in novel frameworks that enable LLMs to reason about concurrency, formally explore the design space, and iteratively optimize results. Building on this realization, recent works have emerged to address the limitations of standalone LLMs in hardware design. These efforts can be categorized into two main streams. The first focuses on RTL specialization, creating expert models such as RTLCoder (Liu et al., 2024a), ScaleRTL (Deng et al., 2025), and CodeV-R1 (Zhu et al., 2025) through supervised fine-tuning or reinforcement learning on Verilog-specific datasets, including Pyranet (Nadimi et al., 2025) and MGVerilog (Zhang et al., 2024). The second, more recent stream, employs multi-agent systems to decompose the complex design process. These frameworks orchestrate LLMs for various roles, such as leveraging intermediate representations (Wang et al., 2025c; Yao et al., 2024), exploring diverse design candidates (Zhao et al., 2025), or managing hierarchical pipelines of specialized agents (Tasnia et al., 2025; Wang et al., 2025a). Some even optimize the workflow itself (Wei et al., 2025). Despite these advancements, accurate assessment remains bottleneck. To evaluate methodology efficacy, the community established standardized benchmarks, such as RTLLM v2 (Liu et al., 2024b), for natural language specification-toRTL generation. Concurrently, the VerilogEval v2 effort (Pinckney et al., 2024) extended the original benchmark from code-completion to specification-to-RTL tasks. However, our analysis reveals inconsistencies in the golden models and problem specifications of VerilogEval v2 that degrade evaluation precision. Moreover, we find that RTLLM v2 lacks the complexity of industrial designs, failing to challenge models on complex optimization techniques. To overcome these validation gaps, we introduce Mod-VerilogEval v2, rectified standard benchmark detailed in Appendix B. Additionally, we present IC-RTL, high-complexity suite derived from the Taiwan National IC Design Contest (Ministry of Education, Taiwan, 2025) and expert-crafted custom designs. To address the challenges of automated Verilog generation, given the scarcity of domain-specific training data and the inherent reasoning limitations of fixed-capacity models, we propose flexible framework without fine-tuning that leverages evolutionary algorithms to unify design exploration and debugging, effectively mirroring the iterative refinement process natural to hardware design. To mitigate the computational latency typically associated with evolutionary frameworks, we introduce Structured Testbench Generation (STG). By automatically categorizing essential signals to generate rigorous test vectors, STG provides high-coverage test cases and fine-grained feedback that goes beyond simple binary results. This mechanism effectively transfers the heavy computational cost of iterative debugging from expensive LLM reasoning to efficient, free EDA simulation tools, thereby simultaneously accelerating the evolutionary process and significantly increasing the frameworks capabilities. The main contributions of this paper are summarized as follows: EvolVE Framework: We introduce an evolutionary framework that utilizes two distinct search strategies: Idea-Guided Refinement (IGR) and Monte Carlo Tree Search (MCTS). By integrating these search mechanisms, our framework enables LLMs to reason about concurrency and iteratively explore the design space without relying on massive domain-specific datasets or requiring prohibitively large model scales. This approach facilitates targeted self-correction, enabling DeepSeek-R1-FP4 to achieve 92% on RTLLM v2 and 98.1% on VerilogEval v2, while boosting the performance of smaller models like Siliconmind-7B to 92.3% on the latter. IC-RTL and Verilog Optimization: To address the scarcity of rigorous evaluation of PPA, we further validate our framework on our novel IC-RTL benchmark suite. In this high-complexity environment, our framework outperforms reference implementations authored by contest participants, reducing the PPA product by up to 66% in Q5 and 17% in the geometric mean across all problems. Structured Testbench Generation (STG): We develop STG, detachable and automated high-coverage verification engine designed to accelerate the evolutionary cycle. By automatically categorizing signals and providing fine-grained feedback, STG transfers the cost of iterative debugging to free EDA tools. 2 EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization"
        },
        {
            "title": "2 Background and Motivation",
            "content": "2.1 Hardware Description Languages HDLs bridge the gap between behavioral intent and physical implementation. Currently, the industry heavily relies on Verilog for chip design, as it provides the granularity needed for fine-grained optimization, ensuring that synthesized circuits meet strict Performance, Power, and Area (PPA) targets. Furthermore, the ubiquity of Verilog has created vast corpus of existing designs that far exceeds that of newer languages such as Chisel and SystemC for High-Level Synthesis. This comparative abundance of data makes Verilog the optimal candidate for training customized LLMs for HDL generation and for leveraging LLMs to conduct automated PPA optimizations. 2.2 LLMs for Code Generation Leveraging LLMs as automated coding agents has evolved into pivotal branch of research. Initial tools like Microsoft Copilot and Aider (Gauthier, 2025) demonstrated strong proficiency in sequential languages such as Python and C++. However, while these single-turn agents perform well on standard benchmarks, they often struggle with the complexity of project-level engineering and long-horizon problem solving. To address these limitations, research has shifted toward sophisticated multi-agent frameworks. Systems such as MetaGPT (Hong et al., 2024) and AgileCoder (Nguyen et al., 2024) mimic human agile methodologies, assigning diverse roles (e.g., architect, engineer, QA) to distinct agents to enable collaborative software development. Recently, the emergence of evolutionary coding agents from the domain of automated scientific discovery has been observed. Frameworks such as AlphaEvolve (Novikov et al., 2025) and Dolphin (Yuan et al., 2025) move beyond static prompting by leveraging closed-loop optimization. In these systems, agents iteratively conduct reasoning, generate implementation code, and utilize execution feedback to refine their solutions. For instance, AlphaEvolve employs an evolutionary pipeline to discover novel algorithms, while Dolphin automates the research loop of idea generation, practice, and feedback. This shift from generation to iterative evolution provides the foundational methodology for solving the strict constraints of hardware design. 2.3 LLMs for Verilog Generation While LLMs excel at step-by-step reasoning, they struggle to model the massive parallelism and cycle-accurate synchronization required by Verilog. To bridge this gap, recent efforts utilize two primary strategies: domain adaptation via fine-tuning and structural guidance via agentic frameworks. Domain-Specific Fine-Tuning. This approach embeds hardware domain knowledge directly into the model weights. RTLCoder (Liu et al., 2024a) established baseline by generating Verilog data pairs with GPT-assisted feedback quality scheme, achieving performance comparable to GPT-4. ScaleRTL (Deng et al., 2025) advanced this by leveraging Chain-of-Thought (Wei et al., 2022) reasoning traces to fine-tune models for complex logic. More recent efforts incorporate Reinforcement Learning (RL). VeriReason (Wang et al., 2025b) utilizes Group Relative Policy Optimization (Shao et al., 2024) to enhance reasoning, while CodeV-R1 (Zhu et al., 2025) employs testbench generator for round-trip data synthesis. However, despite these specialized architectures, fine-tuned models often fail to consistently outperform top-tier proprietary models due to limitations in model scale and the persistent scarcity of high-quality, open-source Verilog training data compared to the vast corpora available for software languages. Agentic and Search-Based Frameworks. by structuring the problem-solving workflow. This can be categorized into three methodologies: In parallel with fine-tuning, second stream guides general-purpose LLMs Optimization and Debugging: Early frameworks focused on local code rectification. RTLRewriter (Yao et al., 2024) leverages Cost-aware Monte Carlo Tree Search (C-MCTS) to rewrite Verilog, while SymRTLO (Wang et al., 2025c) employs neuro-symbolic framework using Abstract Syntax Trees (ASTs) to enforce PPA constraints. Collaborative Multi-Agent Systems: Inspired by human design teams, these frameworks decompose complex specifications into manageable sub-tasks. MAGE (Zhao et al., 2025) utilizes specialized agents with checkpoint mechanism for state tracking. RTLSquad (Wang et al., 2025a) divides the workflow into exploration, implementation, and evaluation squads to generate interpretable decision paths, while VeriOpt (Tasnia et al., 2025) integrates PPA constraints via role-based prompting. VerilogCoder (Ho et al., 2025) advances this approach by utilizing Task and Circuit Relation Graph (TCRG) to decompose module descriptions into sub-tasks, coupled with an AST-based waveform tracing tool that enables agents to autonomously debug and fix functional errors. 3 EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization Workflow and Evolutionary Search: Recent advancements treat generation as global search problem. VFlow (Wei et al., 2025) optimizes the agentic workflow itself, using cooperative evolution (CEPE-MCTS) to discover the optimal sequence of LLM calls. Similarly, REvolution (Min et al., 2025) applies dual-population algorithm and evolutionary prompt selection to conduct both generation and optimization of the Verilog design. 2.4 The Complexity Gap of Benchmarking PPA Currently, optimization efforts primarily target the RTLLM v2 benchmark, such as those seen in REvolution (Min et al., 2025). However, these are often restricted to small-scale modules that can be trivially optimized by standard synthesis tools without deep domain knowledge. To make automated design applicable for both academia and industry, the need for complex, large-scale benchmarks is urgent. Such benchmarks must support both commercial EDA tools for precise assessment and open-source tools for broad accessibility, while providing initial reference designs to rigorously evaluate the true hardware optimization capabilities of different frameworks. 2.5 Current Limitations and Our Approach While fine-tuned models demonstrate impressive capabilities, they remain constrained by limited model capacity and the scarcity of high-quality, open-source Verilog data. Reinforcing these concerns, recent survey (Yang et al., 2025) highlights specific hurdles: existing benchmarks are small-scale (<100 samples) and focus on basic modules, neglecting system-level complexities. Furthermore, current metrics often rely solely on functional pass rates (functional-pass@k), ignoring critical dimensions like PPA. Finally, reliance on single-turn generation restricts architectural exploration and limits the capacity to recover from logic failures. To overcome the dual bottlenecks of data scarcity and limited model scale, we shift from learning-centric to search-based paradigm. Since high-quality Verilog data is proprietary and finite, and scaling model parameters yields diminishing returns without corresponding data growth, we instead leverage evolutionary search to utilize test-time compute. This treats HDL generation as state-space exploration rather than as single-pass process. It allows the LLM to iteratively self-correct against strict logical constraints, effectively mirroring human debugging dynamics. Furthermore, to address the evaluation gaps identified in Section 2.4, validation must move beyond simple functional pass rates to system-level benchmarks capable of assessing optimization metrics. Finally, the effectiveness of an iterative evolutionary search depends on high-quality feedback. This feedback facilitates more accurate debugging, which in turn drives the evolutionary process toward faster convergence."
        },
        {
            "title": "3 Methodology",
            "content": "Recognizing that the intricate nature of Verilog generation demands both extensive exploration and precise debugging, we architect two distinct algorithmic strategies: Idea-Guided Refinement (IGR) and Monte Carlo Tree Search (MCTS). The IGR strategy separates idea generation from implementation by incorporating internal knowledge from LLM or the referenced papers. MCTS utilizes tree-based search to navigate the design space. We select these independent strategies to target complementary optimization goals: IGR is employed to ensure global exploration of the architectural space to escape local optima for PPA optimization, while MCTS is chosen for precise exploitation to rigorously resolve local functional and timing constraints during Verilog code generation. This section formally defines the Verilog generation and optimization task in our framework, outlines the core algorithms and optimization objective, and presents the strategies that yield significant performance improvements. 3.1 Problem Definition We formulate the Verilog development process as search optimization problem. The generation process explores set of nodes = {N1, N2, . . . , Nn}. The state of the i-th node is defined as tuple Ni = (Vi, Si, Fi), where Vi is the candidate Verilog code, Si is the score, and Fi is the diagnostic feedback. The code Vi is validated against testbench , which is defined as set of distinct test cases = {t1, t2, . . . , tT }. An evaluator function executes these tests to derive the score and feedback: Our framework utilizes an LLM to iteratively generate and refine these nodes to find an optimal design : (Si, Fi) = E(Vi, ) = arg max NiN Si 4 (1) (2) EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization Figure 1: Overview of the EvolVE search framework. The definition of the scoring metric Si depends on the specific task objectives: Generative Synthesis or PPA Optimization. For Generative Synthesis, the objective is strictly functional correctness. Let pi denote the number of test cases in successfully passed by Vi. The score is defined as the pass rate, with large negative constant Cpenalty applied for compilation or simulation failures: Sgen(Vi) = (cid:40) pi Cpenalty if simulation success, otherwise (3) For PPA Optimization, the objective is to minimize Power, Performance, and Area metrics while maintaining functional integrity. We formulate this as constrained maximization problem where valid designs are scored based on the product of Area A(Vi) and Latency L(Vi), normalized by scaling factor η: Sopt(Vi) = (cid:40) A(Vi)L(Vi) η Cpenalty if pi = , otherwise Finally, the diagnostic feedback Fi adapts dynamically to guide the LLMs next iteration: Fi = if Si = Cpenalty, ErrorMsg else if Task is Gen, Design Summary Opt Guidance Design Summary else if Task is Opt (4) (5) Equation 4 formulates the Area-Latency product as rapid proxy for the evaluation function within our Verilog optimization framework. This approach offers two primary advantages. First, utilizing open-source tools such as Yosys and Iverilog yields speedup of over 30 compared to commercial workflows like Synopsys VCS and Design Compiler. Second, because area reduction is strongly correlated with power optimization, focusing on area allows for faster simulation while still achieving positive PPA outcomes. This eliminates the need to generate FSDB or VCD files for power evaluation, further accelerating the process. These improvements were validated using PrimeTime and the aforementioned commercial suite, as shown in Figure 6. 3.2 Frameworks Our two evolutionary strategies share the unified framework illustrated in Figure 1 and detailed in Appendix A. Starting with problem description, the framework prompts the LLM to generate the initial code. An evaluator assesses this code against testbench to yield quantitative score and textual feedback. As defined in Section 3.1, node comprises the code, score, and feedback. In the evolutionary phase, the parent node selection strategy retrieves parent node from the archive by choosing one of two methods: IGR and MCTS. Using the parent nodes code and feedback, the LLM produces refined version. The cycle continues until design passes all test cases (Sgen = 1) in generative tasks, or until the maximum node limit is reached. 5 EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization Figure 2: Comparative procedural flow for IGR and MCTS. Idea-Guided Refinement (IGR). Figure 2(a) illustrates the IGR frameworks multi-start strategy. We formalize this process into two distinct phases: 1. Idea Generation. The LLM first generates set of high-level architectural concepts = {I1, I2, . . . , Ik}. This process leverages the LLMs intrinsic knowledge, optionally augmented by external insights drawn from referenced papers via Retrieval-Augmented Generation (RAG). Each idea Ik is conditioned on the problem description and the history of preceding ideas {I1, . . . , Ik1} to enforce diversity in the solution space. 2. Sequential Refinement. Each idea Ik instantiates an independent refinement chain Ck. The chain is initialized with root node Nk,1 derived from implementing Ik. We then apply sequence of 1 refinement steps. For each step {2, . . . , m}, an Aider-style coder (Gauthier, 2025) generates the child code Vk,j by applying differential edits to the parent code Vk,j1 based on the feedback Fk,j1. This approach applies targeted, diff-based edits instead of regenerating the full file, thereby reducing LLM token consumption. Moreover, this single-chain approach aids sequential debugging through preserved, consistent code states across the context window. In contrast, applying such partial edits in MCTS is problematic, as managing multiple divergent children would saturate the context window. This yields sequence of nodes Ck = (Nk,1, . . . , Nk,m). After exploring total of Ntotal = nodes across parallel chains, the framework selects the optimal node that maximizes the score S. Monte Carlo Tree Search (MCTS). Our framework employs Monte Carlo Tree Search (MCTS) (Coulom, 2007) to organize candidate nodes within search tree , where the root node Nroot is initialized using LLM-generated Verilog code. Each node maintains visit count C(N ) and cumulative quality value Q(N ), which aggregates scores propagated from its child nodes via backpropagation. The search proceeds iteratively through the following steps: 1. Selection. Starting from Nroot, the algorithm recursively selects child nodes to traverse the tree until leaf node Nleaf is reached. At each step, the child node Nchild with the highest score based on variant of the Upper Confidence Bound for Trees (UCT) (Kocsis and Szepesvári, 2006) is selected. In practice, unexplored children are prioritized maximally to encourage exploration. The selection score CT (Nparent, Nchild) is defined as follows: CT (Nparent, Nchild) = (cid:40)+, Q(Nchild) C(Nchild) + if C(Nchild) = 0, max(1,C(Nparent)) 1+C(Nchild) , otherwise (6) with controlling the relative strength of the exploration term. 2. Code Generation and Evaluation. We guide the code evolution using design summary from the selected leaf node Nleaf. Incorporating this summary into the LLM prompt, alongside the parents code Vleaf, enables improvement-focused generation of new program Vnew. Immediately following generation, Vnew is evaluated against the testbench to obtain quality score Snew. 3. Expansion and Backpropagation. Using the generated code and its evaluation score, new node Nnew is created and inserted into the search tree as child of Nleaf. The node is assigned depth value 6 EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization depth(Nnew) = depth(Nleaf) + 1. Finally, the score Snew is backpropagated up the tree to the root. For every ancestor node Na along the path, we update the statistics as follows: C(Na) C(Na) + 1, Q(Na) Q(Na) + Snew (7) 3.3 Evaluation Mechanism: Structured Testbench Generation Existing benchmarks, such as VerilogEval, are limited by sparse input stimulus coverage and binary output feedback. This lack of evaluation fidelity obscures the distinction between near-correct solutions and fundamental failures. To address this, we introduce the Structured Testbench Generation (STG) mechanism, deterministic verification engine designed to provide high-fidelity, dense rewards through three distinct phases. Phase 1: Automated Signal Classification. STG first parses the Design Under Test (DUT) port interface using rigorous set of Regular Expressions to categorize signals into semantic groups: Clock/Reset, Control, and Datapath. By targeting standard industry naming conventions (e.g., clk, rst, valid, ready), STG ensures deterministic, fast testbench instantiation without relying on LLM-based inference. Phase 2: Scalable Hybrid Stimulus Generation. To balance exhaustive coverage with simulation feasibility, STG employs width-constrained stimulus strategy. For control signals with width 8, STG generates exhaustive 2w state-space coverage to verify all potential mode transitions. For wider control buses (w > 8), the system automatically switches to constrained-random sampling to prevent runtime explosion. Simultaneously, for datapath buses, STG utilizes optimized random sampling seeded with corner cases (e.g., zero, max-value, alternating bits). The resulting testbenches adhere to Standard IEEE 2005 Verilog, making the STG simulator-agnostic and fully compatible with open-source tools like Icarus Verilog as well as commercial simulators like VCS. Phase 3: Fine-Grained Functional Gradient. STG enforces strict temporal alignment between the DUT and the golden reference model. Checks are triggered at critical state transitions, such as post-reset and after brief delay following clock edges, to isolate sequential errors. Crucially, unlike prior works that rely on sparse bandit feedback, STG calculates continuous fine-grained correctness score Pstg [0, 1], which is defined as the pass rate across the generated test vectors. This acts as functional gradient, allowing the search engine to prioritize candidates that are partially correct, thereby accelerating algorithmic convergence in complex design spaces."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup We evaluate our framework using three self-hosted models, Siliconmind-7B (detailed in Appendix C), GPT-OSS120B (Agarwal et al., 2025), and DeepSeek-R1-FP4 (Guo et al., 2025), alongside the proprietary Gemini-2.0-Flash. These models are assessed across three generative benchmarks focusing on functional correctness: VerilogEval v2 (Pinckney et al., 2024), Mod-VerilogEval v2, and RTLLM v2 (Liu et al., 2024b). Additionally, we evaluate PPA optimization capabilities using our proposed IC-RTL benchmark. Our evaluation toolchain integrates both open-source and commercial ecosystems. We employ Icarus Verilog (Iverilog) (Williams and Baxter, 2002) and Yosys (Wolf et al., 2013) for rapid simulation and synthesis, alongside the Synopsys suite (VCS, Design Compiler, and PrimeTime) for rigorous validation. Underlying this framework, our self-hosted models operate on cluster of 8 NVIDIA H100 GPUs. Specifically, DeepSeek-R1-FP4 leverages tensor and expert parallelism within its Mixture-of-Experts (MoE) architecture, while Siliconmind-7B utilizes pure data parallelism. Regarding hyperparameters, we standardize the evaluation budget to 300 nodes across all experiments. We configure the LLM temperature to 0.6 and the penalty constant Cpenalty to 105. For specific search strategies, IGR is set to generate = 60 initial ideas refined over = 5 steps, while MCTS employs an expansion rate of 3 child nodes and an exploration constant of = 1.4. 4.2 Verilog Code Generation Performance Comparison. In this section, we quantitatively analyze the efficacy of the EvolVE framework across varying LLM architectures. By benchmarking against established baselines, we show that our inference-time search strategy consistently augments the base capabilities of the underlying models. Table 1 presents comprehensive quantitative evaluation of the original VerilogEval v2 and RTLLM v2 benchmarks. Our results demonstrate that inference-time search significantly augments the design generation capabilities of LLMs, 7 EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization Table 1: Performance comparison on the original VerilogEval v2 and RTLLM v2. The Pass columns show the Pass rate (%) progression at computational node budgets of 15 / 50 / 100 / 200 / 300. Method Gemini-2.0-Flash Siliconmind-7B GPT-OSS-120B DeepSeek-R1-FP VerilogEval v2 Pass (%) RTLLM v2 Pass (%) Baselines (Pass@15 / 50 / 100 / 200 / 300) 72.4 / 74.4 / 77.6 / 78.8 / 80.1 82.1 / 87.8 / 89.7 / 91.7 / 91.7 92.9 / 93.6 / 94.2 / 94.2 / 94.2 91.0 / 94.2 / 94.9 / 94.9 / 95. 62.0 / 72.0 / 76.0 / 78.0 / 78.0 76.0 / 82.0 / 84.0 / 84.0 / 84.0 76.0 / 76.0 / 76.0 / 76.0 / 76.0 82.0 / 84.0 / 84.0 / 86.0 / 86.0 REvolution (Deepseek-V3 w/ 200 nodes) REvolution (Llama-3.3-70B w/ 200 nodes) VerilogCoder (Llama-3-70B w/ 100 nodes) VerilogCoder (GPT-4-Turbo w/ 100 nodes)"
        },
        {
            "title": "Previous Work",
            "content": "95.5 88.5 67.3 94.2 88.0 84.0 - - Our Work w/o STG (Nodes: 15 / 50 / 100 / 200 / 300) Gemini-2.0-Flash-IGR Gemini-2.0-Flash-MCTS Siliconmind-7B-MCTS GPT-OSS-120B-IGR GPT-OSS-120B-MCTS DeepSeek-R1-FP4-MCTS Denotes models accessed via API calling. 76.3 / 85.3 / 86.5 / 86.5 / 86.5 77.6 / 81.4 / 84.6 / 86.5 / 87.8 84.0 / 85.3 / 86.5 / 90.4 / 92.3 89.1 / 95.5 / 96.2 / 96.8 / 96.8 89.7 / 93.6 / 94.9 / 95.5 / 96.2 94.2 / 96.8 / 96.8 / 97.4 / 98. 66.0 / 70.0 / 74.0 / 80.0 / 80.0 68.0 / 78.0 / 80.0 / 82.0 / 82.0 76.0 / 80.0 / 82.0 / 84.0 / 88.0 80.0 / 84.0 / 88.0 / 90.0 / 90.0 78.0 / 84.0 / 86.0 / 88.0 / 88.0 82.0 / 84.0 / 86.0 / 90.0 / 92.0 consistently outperforming both standard baselines and prior state-of-the-art methodologies. The most notable performance is observed with the DeepSeek-R1-FP4-MCTS configuration. Even under highly constrained budget of 15 computational nodes, this model achieves pass rate of 94.2% on VerilogEval v2, effectively matching the best reported performance of VerilogCoder (GPT-4-Turbo w/ 100 nodes). As the computational budget scales to 300 nodes, DeepSeek-R1-FP4-MCTS achieves peak pass rates of 98.1% on VerilogEval v2 and 92.0% on RTLLM v2. This represents substantial improvement over the REvolution framework, which achieved 95.5% using Deepseek-V3, validating the superiority of our MCTS implementation in navigating complex design spaces. We further observe performance gains in the commercial model API. For Gemini-2.0-Flash, the standard baseline notably saturates at 80.1% on VerilogEval v2. However, by applying our evolutionary strategies, we successfully unlock latent reasoning capabilities, boosting performance to 86.5% with IGR and 87.8% with MCTS. While both approaches drive significant gains, MCTS yields higher stability and superior peak performance compared to IGR across most architectures. Scalability and Computational Budget. Figure 3 illustrates the performance scaling trajectories of our MCTS approach across varying computational budgets (from 1 to 300 nodes). Unlike standard baselines, which often plateau after few attempts, our framework continues to extract performance gains as the node count increases. We observe distinct scaling behaviors contingent on the base models reasoning capacity. For highly capable models like DeepSeekR1-FP4, the search is exceptionally sample-efficient. As detailed in our collected data, this configuration resolves 84.6% of the benchmark problems at the very first node and requires an average of only 2.49 nodes to reach solution. Conversely, for models with lower baseline pass rates, the MCTS framework effectively utilizes the expanded budget address the subset of high-difficulty problems. This is best exemplified by the Siliconmind-7B-MCTS experiments. While the model solves the majority of easier tasks early, it continues to harvest solutions deep into the search tree. Specifically, Siliconmind-7B-MCTS demonstrates steady climb, rising from 85.3% (15 nodes) to 92.9% (300 nodes). This monotonic increase confirms that the framework does not merely perform rejection sampling, but actively utilizes additional compute to correct subtle logic errors in hard-to-solve benchmarks. Efficacy of Structured Testbench Generation. To quantify the specific contribution of the STG module, we conducted ablation studies comparing the frameworks behavior with and without syntax guidance. As illustrated in Figure 3, the STG-enhanced configuration maintains performance parity with the unguided variant at the upper bound of the computational budget (300 nodes). However, the decisive advantage of STG is revealed in Figure 4, which EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization Figure 3: Performance scaling of MCTS across benchmarks and models. Figure 4: Convergence efficiency gains from STG on Mod-VerilogEval v2 benchmark. highlights dramatic improvement in convergence efficiency. By strictly enforcing syntactic validity during the search, STG significantly lowers the computational overhead required to reach these solutions. For the DeepSeek-R1-FP4 architecture, the average node count required to resolve the Mod-VerilogEval v2 benchmark drops from 4.78 to 2.49. Furthermore, the total output token consumption decreases by over 60% (from 38,818 to 15,440). These metrics confirm that while STG achieves comparable asymptotic accuracy, it optimizes the search trajectory, allowing the framework to attain target performance levels with substantially reduced latency and inference cost. 4.3 PPA Optimization Benchmark: IC-RTL. To rigorously evaluate the frameworks capability in handling complex, industry-relevant hardware logic, we introduce IC-RTL, suite comprising hand-crafted design problems and advanced tasks derived from the Taiwan National IC Design Contest (Ministry of Education, Taiwan, 2025). Unlike existing benchmarks that prioritize simple syntax at limited scales, IC-RTL demands the implementation of specific algorithmic structures and memory optimization techniques to achieve high-performance targets. Crucially, these designs are compatible with both commercial and open-source platforms and offer significant design space for PPA optimization. The constituent tasks are detailed below: Local Binary Patterns (LBP): This module computes texture descriptors for 128 128 grayscale image using sliding 3 3 window. The core challenge lies in minimizing memory access latency. An optimized design typically employs line buffering to store previous rows locally, enabling the concurrent processing of pixel neighborhoods without redundant fetches from main memory. Systolic Array (General Matrix Multiplication): This task involves designing highly parallel matrix multiplication engine using systolic architecture. The key optimization objective is data reuse and synchronization. The design must orchestrate precise data flow where operand tiles are broadcast across an array of Processing Elements (PEs), maximizing throughput while minimizing I/O bandwidth. Specifically, the reported latency captures only the dataflow and execution time of the systolic engine. 9 EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization Figure 5: PPA improvements relative to human baselines on the IC-RTL benchmark. Image Convolutional Circuit (CONV): This task implements hardware accelerator for Convolutional Neural Network (CNN) layer, including zero-padding, 3 3 convolution, bias addition, ReLU activation, and 2 2 max-pooling. significant constraint is the handling of fixed-point arithmetic (Q4.16) and the management of intermediate data dependencies between the convolution and pooling layers. Job Assignment Machine (JAM): This module solves combinatorial optimization problem to minimize the cost of assigning 8 workers to 8 jobs. Since the problem requires exploring all 8! (40, 320) permutations, the hardware must implement an efficient lexicographic permutation generation algorithm alongside pipelined cost-accumulation path to meet timing constraints. Huffman Coding (HC): This task requires the hardware generation of variable-length Huffman codes based on symbol frequency statistics. The complexity lies in implementing sorting and tree-construction algorithms in hardware. An optimized implementation typically utilizes parallel sorting networks or priority queue structures to perform the combine-and-split phases within limited clock cycles. Distance Transform (DT): The engine computes the chessboard distance for binary image using two-pass algorithm (Forward and Backward passes). The optimization challenge involves managing read-after-write dependencies where the backward pass depends on the results of the forward pass, requiring efficient memory arbitration and pipeline stalling mechanisms to prevent data hazards. Optimization Metrics. For the PPA optimization task (Equation 4), the normalization factor is set to η = 105. This scaling ensures that the scores for functionally valid designs remain strictly greater than the penalty threshold, facilitating effective gradient discrimination during search. PPA Performance Analysis. Figure 5 highlights the capability of the EvolVE framework on the IC-RTL benchmark through the Idea-Guided Refinement (IGR) strategy utilizing the DeepSeek-R1-FP4 model. To validate the results in realistic setting, all generated RTL designs are synthesized using Synopsys Design Compiler under the TSMC 180nm technology node. The chart reports the improvement ratio normalized against baseline solutions, where value greater than 1.0 indicates superior efficiency. Crucially, this evaluation sweeps discrete target clock periods (Tclk {3, . . . , 7} ns) and defines latency as the product of simulation cycles and the clock period. We prioritize this methodology over measuring timing slack for two reasons. First, commercial synthesis tools are constraint-driven: they fundamentally alter the circuit topology (e.g., selecting different standard cell drive strengths or arithmetic architectures) based on the specific clock target. Consequently, simply measuring slack on design synthesized with relaxed clock is meaningless, as it fails to capture the structural optimizations the tool would perform under tighter constraint. Second, the global optimum for PPA efficiency rarely aligns with the maximum achievable frequency. By optimizing under discrete clock constraints, we identify the specific operating point that maximizes the Area-Latency (AT) product, rather than forcing the tool to close timing at theoretical limits, which incurs disproportionate area and power penalties. 10 EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization Figure 6: Area and Latency evaluation on the IC-RTL benchmark. Figure 5 compares the optimized and initial implementations under identical operation frequency, with full details provided in Appendix D. The results demonstrate consistent optimization gains across diverse architectural patterns: Baseline Convergence (Q1_LBP): The framework converged to design that matches the manual baselines performance profile. The optimized latency and area metrics remain within 1% of the initial implementation, indicating that the human-designed Local Binary Pattern kernel effectively occupies local optimum within the explored design space. Systolic Array Efficiency (Q2_GEMM): The Matrix Multiplication task demonstrates the frameworks strength in optimizing regular dataflows. The design achieved 12% reduction in Latency while maintaining neutral Area and lower Power. This gain stems from the optimizer effectively rescheduling the systolic arrays control logic to maximize data reuse and reduce pipeline bubbles, directly improving the Area-Latency (AT) product without incurring hardware overhead. Area-Power Trade-off (Q3_CONV): For the Convolution kernel, the framework executed targeted trade-off: it reduced Area by 6% to improve spatial efficiency while maintaining fixed Latency. However, this compaction required higher switching activity, leading to Power regression (0.45 mW to 0.50 mW) that neutralized the overall PPA gain. Area-Power Co-Optimization (Q4_JAM): The Job Assignment Machine task highlights strategy prioritizing Area efficiency. The framework achieved significant reductions in both Area (31%) and Power (26%) without sacrificing latency, driving 36% composite PPA improvement. This confirms the models ability to identify complex optimization paths that aggressively minimize the number of logic gates while simultaneously maximizing energy efficiency. It further demonstrates that the draft of the AT product is effectively in the final PPA optimization. Sorting Network Optimization (Q5_HC): The impressive gain refers to the complex design space for the Huffman Coding task, where the framework optimized the complex decision trees and sorting networks. It delivered 66% peak PPA improvement, driven by simultaneous reductions in Area (30%) and Power (25%). This confirms the models ability to efficiently restructure irregular control-flow logic. Datapath Scaling (Q6_DT): The Distance Transform task exhibits improvements across Area and Power metrics, resulting in robust 13% PPA improvement, indicating our frameworks ability to handle different kinds of complex problems. Oriented Design Space Exploration Beyond generalized scalar improvements, Figure 6 demonstrates the frameworks capability to steer optimization trajectories toward specific constraints. Using the Huffman Coding (Q5_HC) and Distance Transform (Q6_DT) tasks, we directed the agent to generate distinct architectural variants by injecting targeted prompts: one prioritizing spatial efficiency (Opt Area) and another prioritizing temporal speed (Opt Cycle). EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization Table 2: Detailed performance comparison for GEMM candidate programs. Candidate (Best Area*Time) Clock Cycle Timing (ns) AT (Area * Timing) Area (µm2) (ns) Initial Program 269657 Dolphins Program 257906 4 4 362 320 1448 1280 390463336 330119680 Sweeping across clock periods (Tclk {3, . . . , 7} ns), the results in Figure 6 reveal divergent optimization behaviors that effectively populate the Pareto frontier: Area-Centric Optimization (Opt Area): As shown by the square markers in Figure 6 (left), this strategy strictly enforces the baseline latency constraints while aggressively minimizing logic utilization. For Q5_HC, this resulted in 31% reduction in Area (from 2.39 104 to 1.82 104 µm2 at 3ns) without degrading performance. The vertical alignment between the baseline (circles) and area-optimized (squares) points confirms that the framework successfully isolated area savings without impacting timing closure. Latency-Centric Optimization (Opt Cycle): Represented by the triangle markers, this variant successfully decoupled latency from the standard clock scaling. For Q5_HC, the framework achieved 26% reduction in execution time (lowering latency from 429ns to 339ns at 3ns clock). Crucially, this speedup incurred necessary area penalty (increasing from 2.4 104 to 2.9 104 µm2), demonstrating the frameworks ability to navigate the classic Area-Delay trade-off to deliver high-performance solutions when requested. This divergence confirms that EvolVE does not merely converge to single local optimum but allows designers to negotiate the PPA trade-off space dynamically. Whether the goal is minimizing the number of logic gates for IoT applications or maximizing throughput for high-performance computing, the framework enables specialized, constraint-aware implementation. 4.4 Ablation Study Case Study: Architectural Evolution in GEMM. To illustrate the depth of optimization, we analyze the results for Q2_GEMM, where the framework simultaneously improved both area and cycle counts. The baseline human design implements an output-stationary systolic array that handles variable matrix dimensions by decomposing them into 4 4 matrix multiplication operations. This architecture includes dedicated input and weight reshape buffers to align data for the systolic flow, necessitating flattened port interface due to limitations in Icarus Verilog. Micro-Architectural Refinement: We found that the framework successfully optimized the data buffering scheme. For matrix multiplication, the baseline design utilized (2n 1)-row buffer to handle the parallelogram data shaping required for the systolic array. The optimized design compressed this to rows by employing intelligent multiplexing to select data. Additionally, the framework eliminated redundant buffer registers among PE arrays and the input buffer and retimed the critical path. Under 4ns clock constraint, this reduced the total latency from 1448ns to 1280ns, while simultaneously reducing area from 339,266 to 315,770 µm2. Novel Architectural Discovery: Through extended evolutionary search, the framework autonomously evolved the design from pure output-stationary model to weight-output stationary hybrid, reducing latency from 1280ns to 776ns. This transformation eliminated separate weight buffers in favor of direct weight insertion and broadcast inputs for the operand matrix. This result is significant as it validates the frameworks ability to derive architectural-level optimization, transcending local RTL fixes to discover superior high-level topologies."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present EvolVE, unified, model-agnostic evolutionary framework that enables LLMs to generate and optimize high-quality, functionally correct Verilog code directly from natural language specifications. By instantiating two distinct search strategies, Idea-Guided Refinement (IGR) and Monte Carlo Tree Search (MCTS), we provide the first study establishing MCTS as superior for functional generation and IGR as optimal for PPA optimization. Another innovation, the Structured Testbench Generation (STG) engine, replaces coarse-grained feedback with fine-grained scores, significantly accelerating convergence and enabling the framework to solve complex logic problems where baseline models saturate. Furthermore, we introduce Mod-VerilogEval v2 to address inconsistencies in existing 12 EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization standards, and IC-RTL, benchmark suite that bridges the gap between academic datasets and industry-level complexity, providing rigorous platform for evaluating design optimization capabilities. Extensive experiments across VerilogEval v2, Mod-VerilogEval v2, RTLLM v2, and IC-RTL demonstrate state-of-theart performance, with the framework reaching 98.1% pass rate using limited evaluation budgets. The approach proves effective across all model scales, notably boosting the accuracy of the Siliconmind-7B model from 82.1% to over 92%. In terms of optimization, our framework outperforms human-designed references, reducing the PPA product by up to 66% on Q5 and 17% in the geometric mean across the IC-RTL suite. Crucially, we observe that EvolVE can actively steer optimization trajectories and autonomously discover architectural transformations. These results suggest that when augmented with evolutionary guidance, LLMs can transcend the limitations of model scale and data scarcity, evolving into active agents capable of specification to RTL generation, targeted optimization, and microarchitectural design space exploration. Despite these advances, several limitations remain. Currently, STG relies on the availability of executable reference models (golden designs in or Verilog) to verify signal correctness, which may not always be available for novel IP. Additionally, benchmarks specifically targeted at PPA optimization remain scarce, and integrating fully automated synthesis flows to provide real-time PPA feedback requires further development. Future work will focus on two key directions. First, inspired by methodologies like ACE (Zhang et al., 2025), we aim to integrate formal knowledge base (playbook) of microarchitectural optimization patterns to guide PPA-driven exploration. Second, we plan to leverage the exploratory nature of MCTS to better optimize for power, performance, and area, rather than functional correctness alone. We believe these directions will help transform RTL design from manual, expert-limited process into an automated, systematic discipline, driving fundamental shift in the IC industry."
        },
        {
            "title": "References",
            "content": "Agarwal, S., Ahmad, L., Ai, J., Altman, S., Applebaum, A., Arbus, E., Arora, R. K., Bai, Y., Baker, B., Bao, H., et al. (2025). gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925. Coulom, R. (2007). Efficient selectivity and backup operators in monte-carlo tree search. In Computers and Games, pages 7283. Deng, C., Tsai, Y.-D., Liu, G.-T., Yu, Z., and Ren, H. (2025). Scalertl: Scaling llms with reasoning data and test-time compute for accurate rtl code generation. arXiv preprint arXiv:2506.05566. Gauthier, P. (2025). Aider: Ai pair programming in your terminal. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. (2025). Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Ho, C.-T., Ren, H., and Khailany, B. (2025). Verilogcoder: Autonomous verilog coding agents with graph-based planning and abstract syntax tree (ast)-based waveform tracing tool. Hong, S., Zhuge, M., Chen, J., Zheng, X., Cheng, Y., Zhang, C., Wang, J., Wang, Z., Yau, S. K. S., Lin, Z., Zhou, L., Ran, C., Xiao, L., Wu, C., and Schmidhuber, J. (2024). Metagpt: Meta programming for multi-agent collaborative framework. Kocsis, L. and Szepesvári, C. (2006). Bandit based monte-carlo planning. In Machine Learning: ECML 2006, pages 282293. Liu, S., Fang, W., Lu, Y., Wang, J., Zhang, Q., Zhang, H., and Xie, Z. (2024a). Rtlcoder: Fully open-source and efficient llm-assisted rtl code generation technique. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. Liu, S., Lu, Y., Fang, W., Li, M., and Xie, Z. (2024b). Openllm-rtl: Open dataset and benchmark for llm-aided design rtl generation. In Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design, pages 19. Min, K., Cho, K., Jang, J., and Kang, S. (2025). Revolution: An evolutionary framework for rtl generation driven by large language models. arXiv preprint arXiv:2510.21407. Ministry of Education, Taiwan (2025). National university integrated circuit design contest. https://proj.moe.edu. tw/moeisoc/cl.aspx?n=6147. Accessed: 2026-01-19. Nadimi, B., Boutaib, G. O., and Zheng, H. (2025). Pyranet: multi-layered hierarchical dataset for verilog. In 2025 62nd ACM/IEEE Design Automation Conference (DAC), pages 17. IEEE. Nguyen, M. H., Chau, T. P., Nguyen, P. X., and Bui, N. D. Q. (2024). Agilecoder: Dynamic collaborative agents for software development based on agile methodology. 13 EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization Novikov, A., Vu, N., Eisenberger, M., Dupont, E., Huang, P.-S., Wagner, A. Z., Shirobokov, S., Kozlovskii, B., Ruiz, F. J., Mehrabian, A., et al. (2025). Alphaevolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131. Pinckney, N., Batten, C., Liu, M., Ren, H., and Khailany, B. (2024). Revisiting verilogeval: Newer llms, in-context learning, and specification-to-rtl tasks. arXiv e-prints, pages arXiv2408. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. (2024). Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Tasnia, K., Garcia, A., Farheen, T., and Rahman, S. (2025). Veriopt: Ppa-aware high-quality verilog generation via multi-role llms. arXiv preprint arXiv:2507.14776. Wang, B., Xiong, Q., Xiang, Z., Wang, L., and Chen, R. (2025a). Rtlsquad: Multi-agent based interpretable rtl design. arXiv preprint arXiv:2501.05470. Wang, Y., Sun, G., Ye, W., Qu, G., and Li, A. (2025b). Verireason: Reinforcement learning with testbench feedback for reasoning-enhanced verilog generation. arXiv preprint arXiv:2505.11849. Wang, Y., Ye, W., Guo, P., He, Y., Wang, Z., Tian, B., He, S., Sun, G., Shen, Z., Chen, S., et al. (2025c). Symrtlo: Enhancing rtl code optimization with llms and neuron-inspired symbolic reasoning. arXiv preprint arXiv:2504.10369. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. (2022). Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Wei, Y., Huang, Z., Li, H., Xing, W. W., Lin, T.-J., and He, L. (2025). Vflow: Discovering optimal agentic workflows for verilog generation. arXiv preprint arXiv:2504.03723. Williams, S. and Baxter, M. (2002). Icarus verilog: open-source verilog more than year later. Linux Journal, 2002(99):3. Wolf, C., Glaser, J., and Kepler, J. (2013). Yosys-a free verilog synthesis suite. In Proceedings of the 21st Austrian Workshop on Microelectronics (Austrochip), volume 97. Yang, G., Zheng, W., Chen, X., Liang, D., Hu, P., Yang, Y., Peng, S., Li, Z., Feng, J., Wei, X., Sun, K., Ma, D., Cheng, H., Shen, Y., Hu, X., Zhuo, T. Y., and Lo, D. (2025). Large language model for verilog code generation: Literature review and the road ahead. Yao, X., Wang, Y., Li, X., Lian, Y., Chen, R., Chen, L., Yuan, M., Xu, H., and Yu, B. (2024). Rtlrewriter: Methodologies for large models aided rtl code optimization. In Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design, pages 17. Yuan, J., Yan, X., Zhang, B., Chen, T., Shi, B., Ouyang, W., Qiao, Y., Bai, L., and Zhou, B. (2025). Dolphin: moving towards closed-loop auto-research through thinking, practice, and feedback. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2176821789. Zhang, Q., Hu, C., Upasani, S., Ma, B., Hong, F., Kamanuru, V., Rainton, J., Wu, C., Ji, M., Li, H., Thakker, U., Zou, J., and Olukotun, K. (2025). Agentic context engineering: Evolving contexts for self-improving language models. Zhang, Y., Yu, Z., Fu, Y., Wan, C., and Lin, Y. C. (2024). Mg-verilog: Multi-grained dataset towards enhanced llm-assisted verilog generation. Zhao, Y., Zhang, H., Huang, H., Yu, Z., and Zhao, J. (2025). Mage: multi-agent engine for automated rtl code generation. in 2025 62nd acm/ieee design automation conference (dac). Zhu, Y., Huang, D., Lyu, H., Zhang, X., Li, C., Shi, W., Wu, Y., Mu, J., Wang, J., Zhao, Y., et al. (2025). Codev-r1: Reasoning-enhanced verilog generation. arXiv preprint arXiv:2505.24183. 14 EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization"
        },
        {
            "title": "A Detailed Evolutionary Framework",
            "content": "The evolutionary process, formalized in Algorithm 1, unifies the search strategies (IGR and MCTS) under single optimization loop. As defined in the methodology, the search space consists of set of nodes , where each node = (V, S, ) contains the Verilog code, the quantitative score derived from the STG, and the diagnostic feedback. Algorithm 1 General Evolutionary Framework for EvolVE 1: Input: Problem description D, Testbench 2: Output: Optimized node 3: Parameters: Evaluation function E(V, ), LLM , Max Nodes Lmax 4: 5: Initialize node set with seed code generated from D. 6: Initialize archive of best solutions . 7: ncount 8: 9: while (Smax < 1.0 or Task is Opt) and (ncount < Lmax) do {Step 1: Parent Selection via Strategy (IGR or MCTS)} 10: Select parent node Nparent = (Vp, Sp, Fp) from . 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: end while 26: return The node from with the highest score. {Step 3: Evaluation via STG} Compute score and feedback: (Schild, Fchild) E(Vchild, ). Create new node Nchild = (Vchild, Schild, Fchild). {Step 2: Child Generation} Construct prompt using D, Vp, score Sp and feedback Fp. Generate child code Vchild (prompt). {Step 4: Update State and Archive} Update {Nchild}. Update with Nchild if Schild improves. ncount ncount +"
        },
        {
            "title": "B Benchmark Improvements",
            "content": "Mod-VerilogEval v2. VerilogEval v2 is widely adopted benchmark for evaluating Verilog generation models, comprising 156 problems designed to assess hardware design capabilities. For our evaluation, we utilize the spec-to-rtl subset. However, our preliminary analysis revealed critical flaws in the original dataset, including inaccurate problem descriptions, unsynthesizable syntax, and logical inconsistencies in the reference models. These issues frequently led to correct designs being penalized, hindering effective evolutionary optimization. To address this, we collaborated with experienced IC engineers to create Mod-VerilogEval v2. This revision ensures every problem is clearly defined, syntactically compliant with standard Verilog, and logically solvable, thereby providing stable ground truth for the evolutionary search. Our specific improvements include: Initialization and Reset Handling: We removed non-synthesizable initial blocks from all reference designs. To ensure proper register initialization, we introduced an explicit reset signal and updated the corresponding problem prompts (e.g., Problems 34, 53, 66, 104). Synthesizability and Logic Corrections: In several problems (e.g., 116, 124, 151, 156), the reference designs produced unsynthesizable high-impedance or undefined outputs. We rewrote these modules to adhere to standard synthesizable SystemVerilog conventions. Specification Alignment and Disambiguation: We resolved inconsistencies between prompts and implementations. For Problem 62, the module logic was corrected to match the selection requirement (selecting or when sel is 1). For Problem 63, we explicitly defined the shift registers direction in the prompt to eliminate ambiguity. EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization FSM and Interface Refinement: For problems involving Finite State Machines (FSMs) and complex interfaces (e.g., Problems 89, 93, 123, 134), we refined the descriptions to clarify state transition conditions and port declarations, removing extraneous information. Naming Standardization: We corrected naming inconsistencies to ensure uniform testing. This includes fixing port mappings in Problem 99 (correcting Y2/Y4 to Y3/Y1), resolving top-module naming issues in Problem 130. Siliconmind 7b Model Siliconmind-7B is the lightweight, open-source 7-billion parameter student model forming the core of an end-to-end LLM system designed to generate functionally correct and PPA optimized Verilog modules from natural language. The system is developed to overcome challenges in the semiconductor industry, such as the scarcity of high-quality Verilog data and the need to train and serve on limited, on-site computational resources. Its training begins with data augmentation: larger teacher model, DeepSeek-R1, filters and expands public datasets into high-quality samples that include < problem, reasoning, code, testbench > components. Siliconmind-7B is then enhanced through Supervised Fine-Tuning (SFT) and GRPO using this augmented data to learn advanced Verilog design capabilities. IC-RTL Detailed Results This section presents the detailed performance metrics for the IC-RTL benchmark. The tables below are arranged in parallel to maximize space efficiency. Cyc (ns) 3 4 5 6 7 3 4 5 6 7 Cyc (ns) 3 4 5 6 3 4 5 6 7 Table 3: Q1_LBP Results Area (µm2) Pwr (mW) Time (ns) PPA AT Cyc (ns) Table 4: Q2_GEMM Results Area (µm2) Pwr (mW) Time (ns) PPA AT Initial Implementation Initial Implementation 6.49E+03 1.93E+05 0.29 3.68E+08 4.19E+08 6.06E+03 2.58E+05 0.29 4.67E+08 3.91E+08 6.05E+03 3.22E+05 0.29 5.75E+08 3.90E+08 6.05E+03 3.87E+05 0.29 6.90E+08 3.90E+08 6.05E+03 4.51E+05 0.29 8.05E+08 3.90E+08 Optimized Version 6.56E+03 1.93E+05 0.29 3.75E+08 4.23E+08 6.06E+03 2.58E+05 0.29 4.59E+08 3.91E+08 6.05E+03 3.22E+05 0.29 5.65E+08 3.90E+08 6.05E+03 3.87E+05 0.29 6.79E+08 3.90E+08 6.05E+03 4.51E+05 0.29 7.92E+08 3.90E+08 3 4 5 6 3 4 5 6 7 3.12E+05 1.08E+03 0.66 2.25E+08 1.13E+08 3.39E+05 1.45E+03 0.68 3.36E+08 1.23E+08 2.90E+05 1.81E+03 0.55 2.91E+08 1.05E+08 2.47E+05 2.17E+03 0.50 2.73E+08 8.98E+07 2.34E+05 2.54E+03 0.50 3.02E+08 8.52E+07 Optimized Version 3.14E+05 9.63E+02 0.64 1.96E+08 1.01E+08 3.15E+05 1.28E+03 0.67 2.73E+08 1.01E+08 2.96E+05 1.60E+03 0.54 2.57E+08 9.50E+07 2.49E+05 1.92E+03 0.50 2.44E+08 8.01E+07 2.35E+05 2.24E+03 0.50 2.69E+08 7.56E+07 Table 5: Q3_CONV Results Area (µm2) Pwr (mW) Time (ns) PPA AT Cyc (ns) Table 6: Q4_JAM Results Area (µm2) Pwr (mW) Time (ns) PPA AT Initial Implementation Initial Implementation 3.64E+04 1.78E+05 0.59 3.83E+08 6.49E+09 3.63E+04 2.37E+05 0.57 4.93E+08 8.51E+09 3.00E+04 2.97E+05 0.51 4.58E+08 8.93E+09 2.35E+04 3.56E+05 0.47 3.93E+08 8.37E+09 2.36E+04 4.16E+05 0.46 4.52E+08 9.84E+ Optimized Version 3.58E+04 1.78E+05 0.69 3.92E+08 6.37E+09 3.53E+04 2.38E+05 0.62 5.15E+08 8.38E+09 3.09E+04 2.97E+05 0.55 5.02E+08 9.18E+09 2.33E+04 3.56E+05 0.50 4.18E+08 8.30E+09 2.23E+04 4.16E+05 0.50 4.67E+08 9.29E+09 3 4 5 6 7 3 4 5 6 7 1.31E+04 1.21E+06 0.82 1.30E+10 5.27E+09 1.09E+04 1.61E+06 0.77 1.35E+10 4.38E+09 9.14E+03 2.02E+06 0.53 9.83E+09 3.69E+09 8.98E+03 2.42E+06 0.44 9.59E+09 3.62E+09 8.97E+03 2.82E+06 0.37 9.34E+09 3.62E+09 Optimized Version 1.30E+04 1.21E+06 1.05 1.66E+10 5.26E+09 1.01E+04 1.61E+06 0.61 9.91E+09 4.07E+09 8.77E+03 2.02E+06 0.41 7.32E+09 3.54E+09 8.74E+03 2.42E+06 0.34 7.22E+09 3.53E+09 8.73E+03 2.82E+06 0.29 7.18E+09 3.52E+09 16 EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization Table 7: Q5_HC Results Cyc (ns) Area (µm2) Time (ns) Pwr (mW) PPA AT Table 8: Q6_DT Results Cyc (ns) Area (µm2) Time (ns) Pwr (mW) PPA AT Initial Initial Implementation 3 4 5 6 7 3 4 5 6 7 3 4 5 6 7 2.39E+04 4.29E+02 2.49 2.56E+07 3.43E+06 1.98E+04 5.72E+02 1.41 1.60E+07 2.84E+06 1.84E+04 7.15E+02 1.05 1.38E+07 2.63E+06 1.78E+04 8.58E+02 0.85 1.30E+07 2.55E+06 1.80E+04 1.00E+03 0.73 1.32E+07 2.58E+06 Area Opt. 1.82E+04 4.29E+02 1.97 1.54E+07 2.61E+06 1.54E+04 5.72E+02 1.17 1.03E+07 2.20E+06 1.48E+04 7.15E+02 0.90 9.56E+06 2.11E+06 1.45E+04 8.58E+02 0.71 8.94E+06 2.07E+06 1.44E+04 1.00E+03 0.61 8.92E+06 2.06E+06 Cycle Opt. 2.92E+04 3.39E+02 2.25 2.23E+07 3.30E+06 2.80E+04 4.52E+02 2.42 3.07E+07 3.17E+06 2.74E+04 2.79E+04 6.78E+02 1.19 2.26E+07 3.15E+06 2.64E+04 7.91E+02 1.61 3.37E+07 2.99E+06 3 4 5 6 7 3 4 5 6 7 3 4 5 6 7 1.10E+04 3.59E+05 0.53 2.14E+09 1.32E+09 9.12E+03 4.78E+05 0.37 1.63E+09 1.09E+09 8.01E+03 5.98E+05 0.34 1.64E+09 9.59E+09 8.19E+03 7.18E+05 0.36 2.13E+09 9.80E+08 7.89E+03 8.38E+05 0.34 2.26E+09 9.45E+08 Area Optimized 1.01E+04 3.59E+05 0.52 1.89E+09 1.21E+09 8.82E+03 4.78E+05 0.40 1.70E+09 1.05E+09 7.60E+03 5.98E+05 0.34 1.57E+09 9.11E+08 7.81E+03 7.18E+05 0.36 2.06E+09 9.36E+08 7.47E+03 8.38E+05 0.34 2.18E+09 8.95E+ Cycle Optimized 1.07E+04 3.48E+05 0.52 1.94E+09 1.24E+09 9.36E+03 4.65E+05 0.38 1.67E+09 1.08E+09 8.12E+03 5.81E+05 0.35 1.66E+09 9.44E+08 8.28E+03 6.97E+05 0.36 2.09E+09 9.63E+08 7.95E+03 8.13E+05 0.34 2.22E+09 9.25E+"
        }
    ],
    "affiliations": [
        "Department of Computer Science and Information Engineering, National Taiwan University",
        "Department of Electrical Engineering, National Taiwan University",
        "Institute of Information Science, Academia Sinica, Taiwan"
    ]
}