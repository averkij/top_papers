{
    "paper_title": "Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training",
    "authors": [
        "Jiachen Lei",
        "Keli Liu",
        "Julius Berner",
        "Haiming Yu",
        "Hongkai Zheng",
        "Jiahong Wu",
        "Xiangxiang Chu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving a persistent performance and efficiency gap. In this paper, we introduce a novel two-stage training framework that closes this gap for pixel-space diffusion and consistency models. In the first stage, we pre-train encoders to capture meaningful semantics from clean images while aligning them with points along the same deterministic sampling trajectory, which evolves points from the prior to the data distribution. In the second stage, we integrate the encoder with a randomly initialized decoder and fine-tune the complete model end-to-end for both diffusion and consistency models. Our training framework demonstrates strong empirical performance on ImageNet dataset. Specifically, our diffusion model reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75 number of function evaluations (NFE), surpassing prior pixel-space methods by a large margin in both generation quality and efficiency while rivaling leading VAE-based models at comparable training cost. Furthermore, on ImageNet-256, our consistency model achieves an impressive FID of 8.82 in a single sampling step, significantly surpassing its latent-space counterpart. To the best of our knowledge, this marks the first successful training of a consistency model directly on high-resolution images without relying on pre-trained VAEs or diffusion models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 6 8 5 2 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "ADVANCING END-TO-END PIXEL-SPACE GENERATIVE MODELING VIA SELF-SUPERVISED PRE-TRAINING Jiachen Lei1 Keli Liu1 Jiahong Wu1 Xiangxiang Chu1 1 AMAP, Alibaba Group, 2 NVIDIA, 3 Caltech Julius Berner2 Haiming Yu1 Hongkai Zheng"
        },
        {
            "title": "ABSTRACT",
            "content": "Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving persistent performance and efficiency gap. In this paper, we introduce novel two-stage training framework that closes this gap for pixel-space diffusion and consistency models. In the first stage, we pre-train encoders to capture meaningful semantics from clean images while aligning them with points along the same deterministic sampling trajectory, which evolves points from the prior to the data distribution. In the second stage, we integrate the encoder with randomly initialized decoder and fine-tune the complete model end-to-end for both diffusion and consistency models. Our training framework demonstrates strong empirical performance on ImageNet dataset. Specifically, our diffusion model reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75 number of function evaluations (NFE), surpassing prior pixel-space methods by large margin in both generation quality and efficiency while rivaling leading VAE-based models at comparable training cost. Furthermore, on ImageNet-256, our consistency model achieves an impressive FID of 8.82 in single sampling step, significantly surpassing its latent-space counterpart. To the best of our knowledge, this marks the first successful training of consistency model directly on high-resolution images without relying on pre-trained VAEs or diffusion models. Our code is available at https://github.com/AMAP-ML/EPG. Figure 1: (Left) Our pixel-space diffusion model achieves state-of-the-art generation quality with less inference cost. The x-axis indicates the log-scaled GFLOPs for generating an image. The bubble size denotes number of model parameters. (Right) Images generated by our diffusion model. Project Lead"
        },
        {
            "title": "Preprint",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion-based generative models have become the cornerstone of modern image synthesis. This family includes both traditional, iterative diffusion models (Ho et al., 2020; Song et al., 2021; Karras et al., 2022) and more recent one/few-shot generators like consistency models (Song et al., 2023; Geng et al., 2025; Lu & Song, 2025) and flow maps (Boffi et al., 2025). Much of their success on high-resolution tasks relies on training in compressed latent space (Rombach et al., 2022) of pre-trained VAEs (Kingma & Welling, 2022). However, this reliance on VAEs introduces its own set of significant challenges. Firstly, training the VAE itself is difficult due to the need to balance compression with high-fidelity reconstruction. Besides, even when trained properly, the VAE often produces imperfect reconstructions for latents far from the training set. While pre-training the VAE on massive datasets can mitigate this, it will incur heavy computational costs prior to training generative models. Moreover, it induces additional overhead of maintaining two separate models. To bypass the training difficulty and generalization problem rooted in VAEs and simplify the training pipeline, numerous works (Jabri et al., 2023; Hoogeboom et al., 2023; Ho et al., 2021; Dhariwal & Nichol, 2021) have explored training diffusion models directly on raw pixels, developing specialized model architectures (Jabri et al., 2023) or training techniques (Hoogeboom et al., 2023). Despite these efforts, none have achieved training performance and inference efficiency comparable to VAEbased methods primarily due to the high computational cost originating from the model backbone or the slow convergence rate, representing the two major challenges of operating in pixel space. In our work, we aim at addressing these challenges to bridge the performance and efficiency gap between pixel-space training and its latent-space counterparts. We take inspiration from selfsupervised learning (SSL) approaches: the encoders serve as general visual semantics learners and decoders as task-specific heads (He et al., 2021; Chen et al., 2021; Chen & He, 2020). This decomposition significantly improves training efficiency and model performance in downstream tasks. Motivated by this, we argue that encoders in diffusion-based generative models primarily learn visual semantics from noisy inputs, and the decoders act as pixel generators conditioned on encoder outputs. However, the mean-squared pixel prediction loss typically used to train diffusion models is inefficient in capturing meaningful semantics. Yu et al. (2025) address this problem in the latent space by aligning the diffusion models intermediate features with representations from off-theshelf SSL models, e.g., DINO (Caron et al., 2021). Nevertheless, we empirically observe that this hard-coded alignment is suboptimal in pixel-space training. We attribute this failure to the strong alignment regularization, which limits the pixel-space model from efficiently capturing low-level details during training (Wang et al., 2025). In contrast, we propose to decompose the training paradigm into two separate stages as in SSL. During pre-training, the encoder captures meaningful visual semantics from images at varying noise levels. Note that this is different from the Gaussian noise data augmentation utilized in SSL, as the noise level follows pre-defined diffusion schedules and is much higher than the one used in augmentation. Subsequently, after pre-training, the decoder is fine-tuned end-to-end with the encoder under task-specific configurations and predicts pixels given representations from the encoder. This framework endows the model with solid discriminative capability at the start of training, while offering more flexible way for it to adapt learned representations to contain detailed visual semantics, which is preferable in generative tasks. Moreover, designing such pre-training method is non-trivial, as typical visual learning methods struggle in learning meaningful semantics from images with larger noise levels. In particular, directly utilizing SSL as pre-training method is ineffective, due to its representation collapse on images of strong noise. To address this, by extending the idea of rRCM (Lei et al., 2025), we pre-train encoders to capture visual semantics from clean images while aligning them with corresponding points on the same deterministic sampling trajectory, which evolves points from pure Gaussian to the data distribution. In practice, this is realized by matching images with shared noise but different noise levels as in consistency tuning (Song et al., 2023). This pre-training approach reformulates representation learning on noisy images as generative alignment task, connecting features of noisy samples to their progressively cleaner versions. Subsequently, given the pre-trained encoders, we fine-tune it alongside randomly initialized decoder in an end-to-end manner under task-specific configurations."
        },
        {
            "title": "Preprint",
            "content": "To verify the effectiveness of our method, we conduct thorough experiments on ImageNet dataset without relying on any external models. We focus on two distinct downstream generative tasks: training diffusion and consistency models. Built upon pre-trained weights, our diffusion model achieves an impressive FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75 NFEs, surpassing prior pixel-space methods by large margin in both generation quality and efficiency. Under the same training budget for diffusion model, we close the gap with the dominant latentspace paradigm and achieve performance that is competitive with leading latent diffusion models, establishing new state-of-the-art (SOTA) for VAE-free generative modeling (See Figure 1). Crucially, our total compute cost remains substantially lower than the combined expense of training VAEs and latent-space models, indicating our method provides promising direction for training in pixel-space. Moreover, our consistency model achieves remarkable FID score of 8.82 in single generation step, demonstrating superior performance and training efficiency compared to the latentspace counterparts. This result, to the best of our knowledge, marks the first time consistency model has been successfully trained directly on ImageNet-256 without utilizing pre-trained VAEs or diffusion models. While we rely on an SSL pre-training, this is significantly more efficient and shows promising way of stabilizing consistency model training. Our contributions can be summarized as follows: i. We propose novel training framework that enables efficient and performant pixel-space generative modeling in high resolution. ii. Our End-to-end Pixel-space Generative model (EPG) demonstrates state-of-the-art results for pixel-space generation on ImageNet dataset. The diffusion variant of EPG significantly surpasses prior pixel-based methods with an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet512. For the first time, it closes the performance and efficiency gap with leading VAE-based models, all while using similar training budget. iii. EPGs consistency variant achieves remarkable 8.82 FID via one-step generation. To the best of our knowledge, we are the first to achieve such strong performance on ImageNet-256 with consistency models directly trained in pixel space and without relying on pre-trained VAEs or diffusion models."
        },
        {
            "title": "2 BACKGROUND",
            "content": "In this section, we introduce necessary backgrounds on diffusion models and consistency models. Let pdata(x) denote the probability density of target data x, pt(x) the probability density of x(t), x(0) pdata(x). Diffusion models (DM) approximate pdata by learning to reverse pre-defined forward stochastic differential equation (SDE) that gradually transports points x(0) from pdata(x) to prior distribution. The corresponding reverse SDE generate samples by evolving points from the prior distribution back to the data distribution, guided by the score function log pt(x). To estimate the score, diffusion models train neural network sθ(x(t), t) using the objective (Karras et al., 2022) (cid:20) λ(t)sθ(x(t), t) x(0)2 (cid:21) . arg min θ (1) Here, λ(t) is scalar weighting function. In our work, following EDM (Karras et al., 2022), we define the forward process as dx = 2tdw, [0, ], = 80. is the standard Wiener process. 2td with The corresponding reverse SDE can be depicted by: dx = 2tx log pt(x) + being standard Wiener process that flows backwards in time. key property of the reverse SDE is the existence of the probability flow (PF) ordinary differential equation (ODE) that shares the same marginal distribution pt(x) Let (x(t), t) denote the solution of equation 2 from to 0 dx = tx log pt(x). (x(t), t) = x(t) + (cid:90) 0 sx log ps(x)ds. (2) (3) Diffusion models generate samples by solving equation 3 numerically over discrete time intervals. As result, it requires multiple function evaluations, incurring heavy computational cost."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of our method. (Left) Our pre-training framework. is learnable token [CLS], t0, tn, tn1 are time conditions, (y1, y2) are augmented views of clean image xt0 , (xtn , xtn1 ) are temporally adjacent points from the same PF ODE trajectory with xt0 as the initial point. θ is the exponential moving average of θ, and sg is the stop gradient operation. (Right) Fine-tuning stage. After pre-training, we discard the projector and train Eθ along side randomly initialized decoder Dθ in downstream generation tasks. Consistency models (CM) (Song et al., 2023) address this sampling inefficiency through directly approximating the trajectory end point (x(t), t) with neural network fθ(x(t), t), which is trained by enforcing the self-consistency condition: fθ(x(t), t) = fθ(x(s), s), t, [0, ]. Note that x(t) and x(s) lie on the same PF ODE trajectory and fθ satisfies the boundary condition: fθ(x(0), 0) = x(0). To ensure this, prior works parameterize fθ as fθ(x(t), t) = cskip(t)x(t) + cout(t)Fθ(x(t), t), (4) where cskip(t) and cout(t) are scalar functions and satisfy cskip(0) = 1, cout(0) = 0. CMs discretize the time horizon into 1 non-overlapping time intervals {tn}N 1 n=0 , tn [σmin*, ], and optimize the following consistency training objective, which minimizes metric between adjacent points on the PF ODE sampling trajectory arg min θ E(cid:2)λ(t)d(fθ(xtn , tn), fθ(xtn1, tn1))(cid:3), (5) where is distance metric, θ = stopgrad(θ), xtn , xtn1 are adjacent points on the same PF ODE trajectory. During training, xtn is sampled from the forward SDE given clean image xt0. xtn1 is sampled by following equation 2, yet the score at xtn is necessary but unknown during training. To address this, one can either utilize pre-trained diffusion model, or leverage the unbiased estimator E[ xtx xt] (Song et al., 2023) to approximate the score value, meaning that log pt(x) (xt x)/t2. As result, according to equation 2, xtn1 can be approximated by t2 xtn1 = xt0 + tn1ϵ, (6) where the noise ϵ is the same perturbation used when creating xtn . In our work, we sample xtn1 by following equation 6 without relying on any trained diffusion models. To further improve model performance, iCM (Song & Dhariwal, 2023) proposes annealing the discretization steps following pre-defined discretization schedule, which increases step-wisely according to training steps. With an initially small , this approach reduces the gradient variance at higher noise levels and accelerate the model convergence speed at small diffusion time steps, providing solid supervising signals for aligning predictions on images of strong noise magnitude. ECT (Geng et al., 2025) extends the above framework by additionally taking advantage of trained diffusion model weights to initialize fθ, and trains fθ on continuous time intervals. To produce paired noisy samples (x(t), x(r)), r, [σmin, ], they sample from predefined distribution p(t) and derive with deterministic mapping function p(rt). As training proceeds, the ratio r/t gradually increases and approximates 1.0. This reduces early-stage variance of equation 5, while reducing the bias as r/t becomes larger, ultimately improving model generation quality. *The σmin is chosen such that pσmin (x) pdata(x) and xt0 can be approximately seen as samples from the data distribution pdata(x)."
        },
        {
            "title": "3.1 OVERVIEW",
            "content": "In our following discussions, we adopt the diffusion settings as discussed in Section 2, definitions and notation for SDEs, variables naming conventions, and time discretization scheme. We first formalize the core principles of the pre-training method and then introduce our improvements to it, addressing the original hyperparameter brittleness while enabling seamless adaptation to generation tasks."
        },
        {
            "title": "3.2 REPRESENTATION CONSISTENCY LEARNING",
            "content": "Specifically, the pre-training objective is composed of two components: contrastive loss that facilitates semantic learning, and representation consistency loss that enforces alignment of semantics across points on the same ODE trajectory. Both objectives are in the form of the equation 5, while adopting the infoNCE (van den Oord et al., 2019) as the distance metric. Formally, the infoNCE metric for both terms can be written as dnce(q, q+) = log exp (q q+/τ ) exp (q q+/τ ) + (cid:80) exp (q q/τ ) , (7) where is an encoded sample, with q+ and representing its positive and negative counterparts respectively. For the contrastive loss, the positive pairs are constructed through data augmentation, while other samples in the batch are treated as negatives. The representation consistency loss uses temporally adjacent points along the ODE trajectory as positive pairs, e.g., (xtn , xtn1 ), while adjacent points from different trajectories serve as negatives, e.g., (xtn , ). The temporal data pairs is crafted following equation 6 as discussed in Section 2. As result, the overall pre-training objective can be formally depicted as tn1 (cid:20) dnce(Lθ(Eθ(y1, t0)), Lθ(Eθ (y2, t0))) (cid:125) (cid:123)(cid:122) (cid:124) Contrastive Loss (cid:21) + dnce(Eθ(xtn , tn), Esg(θ)(xtn1, tn1)) , (cid:125) (cid:123)(cid:122) Representation Consistency Loss (cid:124) (8) where θ denotes model parameter and is updated to minimize the above objective, (y1, y2) are augmented views of x, Eθ and Esg(θ) are target models used when computing corresponding loss term. θ denotes exponential moving average of θ, respectively, sg means stop gradient operation. Lθ is projector layer, 3-layer MLP, U(0, ), and λ(t) = 1. Figure 2 illustrates the pre-training framework, which features three branches: semantic learning branch (Eθ and Lθ ), an online branch (Eθ and Lθ), and representation consistency learning branch (Esg(θ)). The input of each encoder includes learnable token [CLS], time condition token, and image tokens. Notably, the online model Eθ encodes both augmented samples and noisy samples xtn that contain more noises during each step. When computing losses, both contrastive and representation consistency loss utilize model outputs at the class token position, while contrasting the projector outputs and encoder outputs respectively. core design of the original pre-training framework involves annealing EMA coefficient of θ following manually designed schedule. This mechanism aims to regulate the learning rate of representations on clean images, mitigating the challenges of aligning them with noisy samples at large diffusion time steps, critical for representation quality at high noise levels. This philosophy parallels consistency model training, where consistency at small time steps governs behavior at larger steps (Song & Dhariwal, 2023). However, this approach introduces intricately coupled hyperparameters and brittle training process, where even minor deviations from the prescribed configuration risk training collapse. As result, such fragility poses significant barrier to adaptation in image generation tasks, where achieving better generation quality demands greater hyperparameter flexibility. In contrast, the temperature value τ in representation consistency loss intuitively plays pivotal role in determining the learned representation quality on noisy images. Specifically, small τ enforces strong separability between samples on distinct PF ODE trajectories while tightly aligning points within the same trajectory to their clean-image endpoints. To begin with, we set the temperature"
        },
        {
            "title": "Preprint",
            "content": "value to 0.1, achieving downstream performance comparable to our final results However, its early training exhibits transient instability. We speculate this is because the gradient signals from noisy samples introduces bias as the model lacks meaningful features to reconcile alignment. To mitigate this, we implement linear interpolated temperature schedule: τ (t) = τ1 (1 t) + τ2 t, with τ1 τ2(t), [0, 1], leading to loose alignment for points at large time steps. As training progresses, τ2 converges to τ1 via cosine schedule. Crucially, this schedule operates independently of other hyper-parameters. While this design resolves potential early-stage instability, we emphasize that fixed small temperature value (e.g., 0.1) remains viable solution and acts as strong baseline. We ablate this design in Appendix C. We also study the learned semantics of pre-trained model on noisy images (See Appendix D)."
        },
        {
            "title": "3.3 FINE-TUNING",
            "content": "After pre-training, the projector is discarded and we combine Eθ with randomly initialized decoder Dθ and then fine-tune the complete model fθ end-to-end under either denoising or consistency training configurations (see Figure 2 Right). In specific, given pre-trained weights, we train diffusion models with equation 1. We choose diffusion model, instead of flow matching (Lipman et al., 2023), as it works seamlessly with our pre-training method, which builds on the theoretical foundations of consistency models (Song et al., 2023). We argue that our pre-training model also supports flow matching in downstream task as the time condition and noise schedules are properly aligned, akin to the recent effort (Lu & Song, 2025). In addition, we train consistency models with equation 5 while following the continuous time schedule in ECT (Geng et al., 2025). In early experiments, we observe standard consistency model suffers from slow convergence and suboptimal generation quality due to the supervising signals only come from the clean data. To address this, we empirically introduce an auxiliary contrastive loss between the model outputs fθ (xtn , tn) and their corresponding clean images x0 used to generate the noisy inputs xtn . With an θ of Eθ, utilized to initialize fθ, the auxiliary loss can be formally depicted additional frozen copy by (cid:20) dnce(Eθ (fθ(xtn, tn), tn), Eθ (xt0, t0)) (cid:21) . arg min θ (9) This approach provides complementary supervision during fine-tuning while incurring negligible cost, and has similar spirit to the concurrent works (Stoica et al., 2025; Wang & He, 2025). Note that we do not use any off-the-shelf external models. Instead, we take full advantage of our pretrained weights. We ablate the impact of the above auxiliary loss in Appendix C, and defer diffusion training objective, weighting function, etc.) to Apand consistency model training settings (e.g. pendix B.2."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "Training. We conduct experiments on ImageNet-1K (Deng et al., 2009) dataset. We use vision transformer (ViT) (Vaswani et al., 2017) as our backbone, and utilize 1616 patch size on ImageNet256 and 3232 on ImageNet-512. The model input tokens include learnable token [CLS], time token, and image tokens. During fine-tuning, we use same number of blocks in encoder and decoder. To further improve training efficiency in pixel space, we add residual connections between encoder and decoder as in (Hoogeboom et al., 2023; Bao et al., 2023), and additionally incorporate time condition into decoder using adaLN-Zero (Peebles & Xie, 2023). We consider our pre-training as combination of representation learning and consistency training (Song & Dhariwal, 2023). Therefore, hyper-parameters for representation learning are grounded in SSL best practices: we use MoCo v3s augmentation strategies (Chen et al., 2021) for contrastive loss computation, set the EMA coefficient for the momentum encoder Eθ to 0.99, and adopt linear learning rate schedule scaled to batch size (6e-4 for 1024 batch size). We use AdamW optimizer with default beta values. Built on top of these, we determine hyper-parameters of the representation consistency loss in equation 8. In specific, we adopt the time discretization schedule from iCM (Song & Dhariwal, 2023), and implement temperature schedule linearly interpolated between τ1 = 0.1 and τ2 = 0.2 (see Appendix C). We use similar hyper-parameter settings across all pre-training experiments, except for weight decay, which is adjusted based on the model size. In main results (Table 1 2 4), we use 1024 batch size in both pre-training and fine-tuning. We pre-train our model for 600K steps (480 epochs). We"
        },
        {
            "title": "Preprint",
            "content": "Table 1: System-level comparison on ImageNet-256 with CFG. For latent-space models, we display model parameters and sampling GFLOPs of both the VAE and the generative model. We report GFLOPs of our EPG following DiT. : both model parameters and GFLOPs are composed of two generative models and one classifier. Text in gray: method that requires external models in addition to VAE. Model FID IS Precision Recall NFE Epochs #Params GFLOPs Models in Latent Space LDM (Rombach et al., 2022) USP DiT-XL/2 (Chu et al., 2025) U-ViT-H/2 (Bao et al., 2023) MaskDiT (Zheng et al., 2024) DiT-XL/2 (Peebles & Xie, 2023) SiT-XL/2 (Ma et al., 2024) REPA (Yu et al., 2025) 3.60 247.7 2.33 267.0 2.29 263.9 2.28 276.6 2.27 278.2 2.06 277.5 1.42 305.7 Models in Pixel Space 4.88 158.7 CDM (Ho et al., 2021) ADM (Dhariwal & Nichol, 2021) 3.94 215.8 3.42 182.0 RIN (Jabri et al., 2023) 2.44 256.3 SiD (Hoogeboom et al., 2023) 2.12 278.1 VDM++ (Kingma & Gao, 2023) 2.04 283.2 EPG/16 0.87 - 0.82 0.80 0.83 0.83 0.80 - 0.83 - - - 0. 0.48 2502 167 55M + 400M 336 + 104 2502 240 84M + 675M 312 + 119 - 502 400 84M + 501M 312 + 133 0.57 792 1600 84M + 675M 312 + 119 0.61 0.57 2502 1400 84M + 675M 312 + 119 0.59 2502 1400 84M + 675M 312 + 119 800 84M + 675M 312 + 119 0.65 422 - 0.53 - - - 0.61 2160 4100 963 500 1000 480 2502 800 2502 75 - 800 - 673M 410M 2.46B 2.46B 583M - 761 334 - - 128 ablate the impact of pre-training batch size and model parameters in Section 4.2. During fine-tuning, we follow EDM (Karras et al., 2022) to train diffusion models for 1M steps (800 epochs), and follow (Song & Dhariwal, 2023; Geng et al., 2025) to train consistency models for 700K steps (560 epochs). We conduct experiments in FP16 mixed precision training mode. More details, including hyper-parameters and computational cost, are deferred to Appendix B.2. We also display more qualitative results in Appendix E. Evaluation. For fair comparison with existing works, we use evaluation suite provided by ADM (Dhariwal & Nichol, 2021). Evaluation metrics include Frechet Image Distance (FID) (Heusel et al., 2017), sFID (Nash et al., 2021), Inception Score (IS) (Salimans et al., 2016), Precision and Recall (Kynkaanniemi et al., 2019). We report generation results with 50K images sampled by utilizing the Heun ODE sampler (Karras et al., 2022) with 32 sampling steps in Table 3 and additionally with classifier-free guidance (CFG) (Ho & Salimans, 2022) in Table 1 and 2, where we use intervalcfg (Kynkaanniemi et al., 2024) with 2.5 guidance scale and the recommended guidance interval (0.19, 1.61]. In ablation studies, we sample diffusion models using Euler-Maruyama ODE sampler with 50 sampling steps, and report FID of diffusion models with 10K images. For consistency models, we report FID with 50K images produced via one-step sampling across all experiments. Pre-training cost comparison with VAE. On ImageNet-256, utilizing 8xH200, we compare our pre-training cost with the sd-vae-mse, which is widely adopted in VAE-based methods. It is originally trained for 840k steps in total with batch size of 192 on large-scale open-sourced datasets. To achieve our performance reported in Table 1, 2, and 4, the pre-training takes 57 hours, 111 hours, and 100 hours respectively. In contrast, it takes 160 hours to train sd-vae-mse. Since our pre-training cost is smaller than the VAE model, we only report our fine-tuning cost (in epochs) when comparing with latent-space methods. 4.1 MAIN RESULTS Diffusion Model. In Table 1, without relying on any external models, EPG achieves 2.04 FID with merely 75 NFEs. In comparison with diffusion models trained in the latent space, EPG surpasses one of the leading models SiT-XL/2, which is grounded on improved theoretical framework, with approximately 50% of its training cost and more than 6 faster sampling speed. While slightly https://huggingface.co/stabilityai/sd-vae-ft-mse"
        },
        {
            "title": "Preprint",
            "content": "Table 2: System-level comparison on ImageNet-512 with CFG. Evaluation settings are the same as Table 1. Model FID IS Precision Recall NFE Epochs #Params GFLOPs Models in Latent Space RIN ADM SiD VDM++ Models in Pixel Space U-ViT-H/4 DiT-XL/2 SiT-XL/2 MaskDiT EPG/32 EPG/32 3.95 216.0 3.85 221.7 3.02 248.7 2.65 278.1 4.05 263.8 3.04 240.8 2.62 252.2 2.50 256.7 2.43 289.7 2.35 295. - 0.84 - - 0.84 0.84 0.84 0.83 0.82 0.82 - 0.53 - - 1000 500 800 1081 2502 800 2502 320M 731M 2.46B 2.46B 415 2813 - - 502 0.48 400 84M + 501M 1260 + 133.0 0.54 2502 600 84M + 675M 1260 + 524.6 0.57 2502 600 84M + 675M 1260 + 524.6 800 84M + 675M 1260 + 524.6 0.56 792 0.57 0. 75 75 600 800 540M 540M 113 113 Table 3: Class-conditional generation FID comparisons with DiT and SiT on ImageNet-256. Table 4: Benchmarking few-step class-conditional image generation on ImageNet-256. Model FID NFE Epochs #Params Model FID sFID IS Epochs #Params Models in Latent Space - - DiT-B/2 43.5 SiT-B/2 33.0 6.46 43.71 EPG 52.0 8.25 24.34 EPG 31.9 6.80 42.70 EPG 25.1 6.27 54.25 DiT-XL/2 19.5 SiT-XL/2 17.2 - - - - EPG 15.9 6.28 72. 80 80 80 160 240 80 80 80 84M+130M 84M+130M 229M 229M 229M 84M+675M 84M+675M 583M iCT-XL/2 (Song et al., 2023) 34.24 20.30 Shortcut-XL/2 (Frans et al., 2025) 10.60 7.80 8. IMM (Zhou et al., 2025) Models in Pixel Space EPG 8.82 1 2 1 4 1 - 84M + 675M - 84M + 675M 250 84M + 675M 84M + 675M 250 6395 84M + 675M 560 540M DiT-XL/2 9.6 SiT-XL/2 8.3 6.6 EPG 6.85 121.50 1400 84M+675M 1400 84M+675M 6.32 131.7 5.16 141.09 583M lagging behind REPA, method that utilizes off-the-shelf SSL models to speed up training, we argue that EPG is in parallel to these training acceleration methods and can also benefit from incorporating external supervision, which we leave to our future work. When comparing with pixel-space diffusion models, EPG is significantly efficient than prior works in both training and inference, while setting new SOTA that bridges the performance and efficiency gap with VAE-based methods. Additionally, we compare class-conditional image generation performance with SiT and DiT of different sizes in Table 3. On ImageNet-512  (Table 2)  , utilizing 3232 patch size, EPG retains its superior generation quality while maintaining computational costs comparable to its ImageNet-256 variant. The training (pretraining + fine-tuning) and inference costs remain stable across resolutions, demonstrating that our framework scales efficiently to high-resolution pixel-space modeling without external models (e.g., VAEs). This positions EPG as practical solution for raw-pixel generation at scale. Consistency Model. To the best of our knowledge, we are the first to successfully train consistency model without relying on pre-trained VAEs or diffusion models. As shown in Table 4, EPG reaches 8.82 FID with one step generation, substantially outperforming iCT-XL/2, our latent-space counterpart. Besides, EPG outperforms Shortcut-XL/2 in one-step generation, while leaving small performance gap to its 4-step generation result. Noticeably, IMM marginally surpasses EPG by 0.77 FID but requires 11 more training compute, highlighting practical advantages of our method."
        },
        {
            "title": "4.2 ABLATION STUDY",
            "content": "Table 5: Comparing FID scores with baseline methods in downstream tasks. Comparing with pixel-space baselines. On ImageNet-224, we compare with several pixel-space baselines in training diffusion and consistency model. (1) train with REPA (Yu et al., 2025), (2) load pre-trained MoCo v3 (Chen et al., 2021) weights, (3) train from scratch, and (4) pre-train with original rRCM framework (Lei et al., 2025). We display FID results in Table 5. The model equipped with REPA yields the worst generation quality. Besides, The model loaded with pre-trained MoCo v3 weights demonstrate better results and training stability than the one trained from scratch. In comparison with the model pre-trained by rRCM, our EPG demonstrates consistently better generation performance in both diffusion and consistency training tasks, indicating the remarkable effectiveness of our improvements. Notably, our framework is much concise and does not introduce coupled hyper-parameters. We defer implementation details of the baseline methods to Appendix B.1. REPA MoCo v3 Scratch rRCM EPG 72.71 56.26 59.69 46.51 41.36 - 36.77 NaN 37.55 33.12 Experiment DM CM Scalability. We study the scalability of our pre-training method on batch size and model parameters. We first compare generation quality of the diffusion and consistency models, where the encoders are pre-trained with 256 and 1024 batch size respectively (Figure 3 first two columns). In addition, we scale up encoder parameters from 64M to 107M and 225M. We use the same number of blocks and dimensions in encoders and decoders during fine-tuning. The complete model comprises of 116M, 229M, and 540M parameters, respectively. We display results of scaling up pre-training parameters in the last two columns of Figure 3. The generation quality of our models improves monotonically with increased training budgets. This scalability positions our framework as promising solution for high-resolution and multi-modal image generation tasks, where training and sampling efficiency are critical. Figure 3: Performance of downstream generative models scales with pre-training compute budgets."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Training diffusion model in pixel-space Training diffusion models directly in pixel space is notoriously challenging due to high computational costs and slow convergence on high-resolution data. Prior work addresses this through optimizing model architectures (Jabri et al., 2023), improving diffusion formulations (Hoogeboom et al., 2023; Kingma & Gao, 2023), and decomposed training stages with masked pre-training (Lei et al., 2023) or cascaded diffusion models (Ho et al., 2021). While these efforts enhance generation quality of pixel-space diffusion, they still lag behind latentspace methods in both performance and efficiency. Our work bridges this gap, establishing new state-of-the-art for pixel-space approaches utilizing similar training cost as latent-space baselines yet with fewer sampling steps. Accelerate diffusion model training. On one hand, current works in diffusion models are focusing on accelerating training speed by incorporating additional representational supervision. However, as far as we know, all these efforts remain confined in the latent-space training formulation. Yu et al. (2025) aligns intermediate features of diffusion models with those from off-the-shelf SSL models. As follow up works, several variants (Leng et al., 2025; Yao et al., 2025) have been proposed. Leng et al. (2025) train both VAE and diffusion models in an end-to-end manner, achieving superior training efficiency yet it still relies on trained VAEs. In contrast, another line of works (Chu et al.,"
        },
        {
            "title": "Preprint",
            "content": "2025; Stoica et al., 2025; Wang & He, 2025) speed up model training by incorporating representation learning loss, without relying on external SSL models. The most related work to ours is USP (Chu et al., 2025), which contains strong representation learning stage to accelerate the training of diffusion models. In addition, Stoica et al. (2025) and Wang & He (2025) incorporate contrastive loss in either the final denoising objective or intermediate features. In contrast, our study focuses on the image space instead of latent space. Few-step generative models. Consistency models (CMs) (Song & Dhariwal, 2023; Song et al., 2023; Lu & Song, 2025) enable single-step sampling by enforcing consistency across deterministic diffusion trajectories. While theoretically appealing, CMs face amplified training challenges due to sparse supervision at narrow noise intervals. Solutions like weight initialization from pretrained diffusion models (Geng et al., 2025) mitigate this but remain infeasible without access to trained diffusion models. Meanwhile, Shortcut model (Frans et al., 2025) and IMM (Zhou et al., 2025) provide insightful solutions for achieving few-step generation with solid theoretical innovations. In contrast, from an empirical perspective, we offer an effective solution for training one-shot generators on high-resolution images directly in pixel space."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We present two-stage training framework for mitigating the training difficulty of diffusion and consistency models in pixel space. It bridges the performance and efficiency gap between pixel-space trainings and the latent-space approaches, surpassing the performance of current leading latent-space diffusion models under the same training budget while with less inference cost. It also outperforms latent-space few-step generation models in one-step generation quality. By decoupling and enhancing the implicit representation learning stage in generative modeling, we provide practical pathway for efficiently training the generative models with solid theoretical foundation. We hope it will motivate future efforts to improve generation quality with well-established insights from the traditional visual domain."
        },
        {
            "title": "REFERENCES",
            "content": "Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models, 2023. URL https://arxiv.org/abs/2209. 12152. Nicholas M. Boffi, Michael S. Albergo, and Eric Vanden-Eijnden. How to build consistency model: Learning flow maps via self-distillation, 2025. URL https://arxiv.org/abs/ 2505.18825. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers, 2021. URL https: //arxiv.org/abs/2104.14294. Xinlei Chen and Kaiming He. Exploring simple siamese representation learning, 2020. URL https://arxiv.org/abs/2011.10566. Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96409649, 2021. Xiangxiang Chu, Renda Li, and Yong Wang. Usp: Unified self-supervised pretraining for image generation and understanding. arXiv preprint arXiv:2503.06132, 2025. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021. URL https://arxiv.org/abs/2105.05233."
        },
        {
            "title": "Preprint",
            "content": "Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel. One step diffusion via shortcut models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=OlzB6LnXcS. Zhengyang Geng, Ashwini Pokle, Weijian Luo, Justin Lin, and Zico Kolter. Consistency models made easy. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=xQVxo9dSID. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners, 2021. URL https://arxiv.org/abs/2111. 06377. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/ paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. URL https://arxiv. org/abs/2207.12598. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. URL https://arxiv.org/abs/2006.11239. Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation, 2021. URL https: //arxiv.org/abs/2106.15282. Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: End-to-end diffusion for high resolution images, 2023. URL https://arxiv.org/abs/2301.11093. Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive computation for iterative generation, 2023. URL https://arxiv.org/abs/2212.11972. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in neural information processing systems, 35:2656526577, 2022. Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36:6548465516, 2023. Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. URL https: //arxiv.org/abs/1312.6114. Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. Advances in Neural Information Processing Systems, 37:122458122483, 2024. Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models, 2019. URL https://arxiv. org/abs/1904.06991. Jiachen Lei, Peng Cheng, Zhongjie Ba, and Kui Ren. Masked diffusion models are fast learners. arXiv preprint arXiv:2306.11363, 2023. Jiachen Lei, Julius Berner, Jiongxiao Wang, Zhongzhu Chen, Chaowei Xiao, Zhongjie Ba, Kui Ren, Jun Zhu, and Anima Anandkumar. Robust representation consistency model via contrastive denoising. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=armbJRJdrH. Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers, 2025. URL https://arxiv.org/abs/2504.10483."
        },
        {
            "title": "Preprint",
            "content": "Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023. URL https://arxiv.org/abs/2210.02747. Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models, 2025. URL https://arxiv.org/abs/2410.11081. Nanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers, 2024. URL https://arxiv.org/abs/2401.08740. Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W. Battaglia. Generating images with sparse representations, 2021. URL https://arxiv.org/abs/2103.03841. William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. URL https: //arxiv.org/abs/2212.09748. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models, 2022. URL https://arxiv.org/ abs/2112.10752. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans, 2016. URL https://arxiv.org/abs/1606.03498. Yang Song and Prafulla Dhariwal. preprint arXiv:2310.14189, 2023. Improved techniques for training consistency models. arXiv Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations, 2021. URL https://arxiv.org/abs/2011.13456. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models, 2023. URL https://arxiv.org/abs/2303.01469. George Stoica, Vivek Ramanujan, Xiang Fan, Ali Farhadi, Ranjay Krishna, and Judy Hoffman. Contrastive flow matching. arXiv preprint arXiv:2506.05350, 2025. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding, 2019. URL https://arxiv.org/abs/1807.03748. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Runqian Wang and Kaiming He. Diffuse and disperse: Image generation with representation regularization. arXiv preprint arXiv:2506.09027, 2025. Ziqiao Wang, Wangbo Zhao, Yuhao Zhou, Zekai Li, Zhiyuan Liang, Mingjia Shi, Xuanlei Zhao, Pengfei Zhou, Kaipeng Zhang, Zhangyang Wang, Kai Wang, and Yang You. Repa works until it doesnt: Early-stopped, holistic alignment supercharges diffusion training, 2025. URL https: //arxiv.org/abs/2505.16792. Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models, 2025. URL https://arxiv.org/abs/2501.01423. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think, 2025. URL https://arxiv.org/abs/2410.06940. Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. In Transactions on Machine Learning Research (TMLR), 2024. Linqi Zhou, Stefano Ermon, and Jiaming Song. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/ forum?id=pwNSUo7yUb. Inductive moment matching."
        },
        {
            "title": "Preprint",
            "content": "A THE USE OF LARGE LANGUAGE MODELS (LLMS) To enhance the clarity and readability of this work, we utilized DeepSeek for language refinement in several sections (e.g., abstract and introduction) of the manuscript. The tools suggestions were reviewed and edited to ensure technical accuracy and alignment with the papers scientific content."
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "B.1 BASELINE METHODS We introduce the implementation details for baseline methods listed in Table 5. The experiments are conducted under the same fine-tuning settings. MoCo v3. Utilizing pre-trained MoCo v3 ViT-B model (pre-trained for 300 epochs), we attach an additional decoder comprising 12 ViT layers and fine-tune the complete model end-to-end. Note that we also add residual connections between the encoder and decoder, and introduce adaLN-Zero to the decoder. Similar to our Base/16 model, the complete model contains 229M parameters. To enable class-conditioned generation and diffusion denoising, we prepend an extra zero embedding to the encoders original positional embedding. Besides, we concatenate the class and timestep embeddings with the noisy image tokens and feed them as input tokens to the model. REPA in pixel space. We use the official implementation and adopt SiT-L/16 as backbone. We use the default 0.5 coefficient for representation alignment loss. rRCM. We use the official checkpoint of pre-trained rRCM encoder. It has the same structure to our Base model and is pre-trained with 4096 batch size. B.2 TRAINING DETAILS We list pre-training and fine-tuning hyper-parameters in Table 7 and 8. Experiment configurations regarding diffusion settings (e.g., training objective, weighting function, noise sampling, etc.) are specified in Table 6. We pre-process ImageNet dataset by following ADM (Dhariwal & Nichol, 2021). In specific, each image is center cropped into 256x256 and saved into lossless PNG format. We use horizontal flip when training EPGs diffusion and consistency variants. Following (Lei et al., 2025), we name the pre-trained model as Representation Consistency Model (RCM) in our discussions below. B.3 NETWORK CONFIGURATION. We use four different network configurations in paper, as listed in Table 9. We use the Small and Base model in our ablation studies, the Large model in Table 4, the XL model in Table 1, and the Base and XL model in Table 3. In early experiments, we observe that scaling decoder width is more efficient than scaling up model depth. notable benefit is that its not urgent to scale up pre-training in order to achieve better generation performance, making the pre-training lightweight Warmup stage. B.4 COMPUTATIONAL COST Utilizing 8xH200 GPUs, the encoder pre-training requires 57 hours for XL/16 and 100 hours for Large/16 on ImageNet-256, and 111 hours for Large/32 on ImageNet-512. During fine-tuning, the denoising training requires 139 hours for XL/16 on ImageNet-256 and 100 hours for Large/32 on ImageNet-512. In terms of consistency model on ImageNet-256, the fine-tuning requires 156 hours for Large/16."
        },
        {
            "title": "Preprint",
            "content": "Table 6: Our pre-training and fine-tuning diffusion settings. : In denoising training, we first sample from continuous lognormal distribution and then map it to the nearest discrete value tn. : the denoiser predicts clean image at different noise levels. Pre-training Denoising Training Consistency Training Time Discretization or r/t schedule Weighting λ(t) from 20 to 1280 follow iCM (Song & Dhariwal, 2023) 1. Noise Sampling Training objective Model parameterization Shifting of noise level (Hoogeboom et al., 2023) U(0, ) equation 8 - 256/64 1280 Constant 1 tntn1 tn lognormal(1.2, 1.6) equation 1 x-prediction (Continuous) follow ECT (Geng et al., 2025) 1 tr lognormal(0.4, 1.6), p(rt) equation 5 + equation 9 fθ(xt, t) = cskip(t)x(t) + coutFθ(xt, t) 256/ 256/64 Table 7: Hyper-parameters used in Pre-training. Small & Base & Large XL Bs Training steps Lr Optimizer Wd τ for Rep consistency loss τ for contrastive loss EMA of momentum encoder Eθ Diffusion settings Data augmentation 1024 600k 6e-4, Cosine Decay AdamW(0.9, 0.999) 0.03 0.05 linear interpolation with annealing, τ1 = 0.1, τ2 = 0.2 0.2 0.99 see Table 6 Follow MoCo v3 (Chen et al., 2021) Table 8: Hyper-parameters in Fine-tuning. We use step-wisely decayed learning rate when finein the starting 400k steps, lr=1e-4, from 400k to 500k, lr=3e-5, and tuning consistency model: lr=8e-6 thereafter. : the dropout is employed after each Attention block in decoder. Params Diffusion variant  (Table 1)  Diffusion variant  (Table 2)  Consistency variant  (Table 4)  Dataset Bs Training steps Lr Grad clip Optimizer Wd Dropout Patch size Diffusion settings EMA value Data augmentation ImageNet-256 1024 1000k 1e-4 0.5 AdamW(0.9, 0.999) 0.01 - 1616 See Table 6 0.9999 Horizontal Flip ImageNet-512 1024 1000k 1e-4 0.5 AdamW(0.9, 0.999) 0.01 - 3232 See Table 6 0.9999 Horizontal Flip ImageNet-256 1024 600k step-wisely 0 AdamW(0.9, 0.99) 0.01 0.5 1616 See Table 6 0.9999 Horizontal Flip Table 9: Network configurations. Model Blocks Dim Heads Params 6,6 12,12 12,12 Small/16 Base/16 12,12 Large/16 16,16 1024,1024 16,16 XL/16 116M 229M 540M 768, 1584 12, 22 583M 768,768 768, 12,"
        },
        {
            "title": "Preprint",
            "content": "Table 10: Ablation study on auxiliary loss. Table 11: Ablation study on temperature value of representation consistency loss. Model Scratch w/o aux w/o RCM EPG CM NaN 129.16 65.05 36. Model DM CM No consistency τ1 = τ2 = 0.1 58.18 56.41 52.79 46.44 τ1 = 0.1, τ2 = 0.5 56.22 45.69 τ1 = 0.1, τ2 = 0.2 52.67 46.99 Figure 4: Avg per-channel std of RCMs outputs at varying time steps."
        },
        {
            "title": "C ADDITIONAL ABLATION STUDY",
            "content": "Impact of temperature value. We compare different temperature settings for representation consistency loss (1) No representation consistency loss (2) τ1 = τ2 = 0.1: fixed temperature 0.1, (3) τ2 = 0.5: linear interpolation with τ2 = 0.5, and (4) Ours: linear interpolation with τ2 = 0.2. We display results in 11. We pre-train 300K steps, then fine-tune diffusion model for 100K steps with 1024 batch size, and consistency model for 200K steps with 256 batch size. We recommend using fixed temperature value τ1 = τ2 = 0.1 whenever training is stable. Otherwise, we suggest choosing the linear interpolated temperature schedule with annealing. Impact of auxiliary loss in training consistency loss. In Table 10, we compare consistency models trained without auxiliary loss ( equation 9) or without loaded RCM weights."
        },
        {
            "title": "D SEMANTICS LEARNED BY RCM ON NOISY SAMPLES",
            "content": "To demonstrate that the RCM encodes noisy samples into meaningful representations, we compute the per-channel standard deviation of the ℓ2-normalized encoder outputs averaged across 1000 samples at various time steps/ For RCM, the channel-wise std approximates 1 , where is the feature dimension. In contrast, the encoder trained solely with contrastive loss exhibits near-zero std across channels at varying noise levels, indicating representation collapse, as noted in prior work (Chen & He, 2020)."
        },
        {
            "title": "E QUALITATIVE RESULTS",
            "content": "We display more qualitative results in figures 5 6 7 8 9 10 11 12 13 14. Images are downsampled and compressed to optimize storage efficiency. https://www.deepseek.com"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Images generated by EPG-L via one-step sampling. Figure 6: Uncurated samples generated by EPG-XL. 32-step Heun ODE sampler, classifier-free guidance 4.5, class 1."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Uncurated samples generated by EPG-XL. 32-step Heun ODE sampler, classifier-free guidance 4.5, class 22. Figure 8: Uncurated samples generated by EPG-XL. 32-step Heun ODE sampler, classifier-free guidance 2.5, class 89. Figure 9: Uncurated samples generated by EPG-XL. 32-step Heun ODE sampler, classifier-free guidance 4.5, class 207."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Uncurated samples generated by EPG-XL. 32-step Heun ODE sampler, classifier-free guidance 4.5, class 250. Figure 11: Uncurated samples generated by EPG-XL. 32-step Heun ODE sampler, classifier-free guidance 4.5, class 429. Figure 12: Uncurated samples generated by EPG-XL. 32-step Heun ODE sampler, classifier-free guidance 4.5, class 620."
        },
        {
            "title": "Preprint",
            "content": "Figure 13: Uncurated samples generated by EPG-XL. 32-step Heun ODE sampler, classifier-free guidance 4.5, class 812. Figure 14: Uncurated samples generated by EPG-XL. 32-step Heun ODE sampler, classifier-free guidance 3.5, class 933."
        }
    ],
    "affiliations": [
        "AMAP, Alibaba Group",
        "Caltech",
        "NVIDIA"
    ]
}