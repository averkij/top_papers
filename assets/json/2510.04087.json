{
    "paper_title": "A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling",
    "authors": [
        "Hyung Gyu Rho"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pairwise comparison data. While effective at learning relative preferences, this paradigm fails to capture a signal of response acceptability, leaving systems vulnerable to selecting the least bad of many unacceptable options. This is particularly problematic for hard prompts, where the risk of such false acceptances increases with the number of samples. In this paper, we address this critical reliability gap by introducing a new data collection and modeling framework. By augmenting preference data with an outside option, inspired by discrete choice models, we train a reward model that can distinguish not just what is \\textit{better}, but what is \\textit{good enough}. We leverage this capability to create an adaptive inference strategy, best of mini-N in-loop, which partitions the generation budget into sequential loops with a calibrated, early-exit condition. Our experiments show that when tuned as an alignment guardrail, it reduces reliability failures by 70\\%, and when tuned as an inference accelerator, it improves average inference speed by over 22\\% in IMDB-sentiment setting. We thus provide a principled and flexible framework for practitioners to explicitly manage the trade-off between reliability and computational efficiency."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . s [ 1 7 8 0 4 0 . 0 1 5 2 : r CONTEXTUAL QUALITY REWARD MODEL FOR RELIABLE AND EFFICIENT BEST-OF-N SAMPLING"
        },
        {
            "title": "A PREPRINT",
            "content": "Hyung Gyu Rho sirano1004@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pairwise comparison data. While effective at learning relative preferences, this paradigm fails to capture signal of response acceptability, leaving systems vulnerable to selecting the least bad of many unacceptable options. This is particularly problematic for hard prompts, where the risk of such false acceptances increases with the number of samples. In this paper, we address this critical reliability gap by introducing new data collection and modeling framework. By augmenting preference data with an outside option, inspired by discrete choice models, we train reward model that can distinguish not just what is better, but what is good enough. We leverage this capability to create an adaptive inference strategy, best of mini-N in-loop, which partitions the generation budget into sequential loops with calibrated, early-exit condition. Our experiments show that when tuned as an alignment guardrail, it reduces reliability failures by 70%, and when tuned as an inference accelerator, it improves average inference speed by over 22% in IMDB-sentiment setting. We thus provide principled and flexible framework for practitioners to explicitly manage the trade-off between reliability and computational efficiency. Keywords Reward Model Inference Time Alignment Discrete Choice"
        },
        {
            "title": "Introduction",
            "content": "The capabilities of Large Language Models (LLMs) have been significantly enhanced through alignment with human feedbackmaking them more helpful and harmless [Bai et al., 2022], instruction-following [Ouyang et al., 2022], and capable of sophisticated tasks [Ziegler et al., 2019]. Such advances are driven by preference-based alignment methods, ranging from inference-time techniques like Best-of-N (BoN) sampling [Nakano et al., 2021, Ouyang et al., 2022] to post-training optimization approaches like Direct Preference Optimization (DPO) [Rafailov et al., 2023]. Crucially, the effectiveness of these approaches depends entirely on the data used to train their underlying preference models. Current methods predominantly rely on pairwise comparison datasets, where humans select the better of two responses. This paradigm, rooted in models like BradleyTerry [Bradley and Terry, 1952], has an inherent limitation: it only captures relative preference, not response acceptability. While the field often implicitly treats the chosen response as desirable [Jung et al., 2024, Ethayarajh et al., 2024], the data merely indicates that it is better than the alternative. As result, reward model trained this way can rank responses but cannot determine whether response is acceptable or unacceptable [Wang et al., 2023]. This limitation critically undermines BoN sampling. There is no guarantee that the best of options meets minimum quality threshold. It is possible that BoN simply selects the least bad of many poor options. In this paper, we address this gap by introducing new data collection protocol that extends standard pairwise comparisons with an outside option. By allowing annotators to reject all candidate responses, our method captures direct signal of contextual acceptability. We show that reward model trained on this richer data can distinguish between responses that are merely better and those that are genuinely acceptable within the context. This seemingly simple modification unlocks several critical capabilities that we demonstrate in this paper: Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling PREPRINT First, we provide rigorous analysis and empirical evidence showing that standard Best-of-N (BoN) sampling is unreliable for challenging prompts. We demonstrate that as more candidates are generated, the risk of false acceptanceselecting response that is merely the least bad and does not meet minimum quality thresholdincreases significantly. Our experiments show that the number of such reliability failures for the baseline method more than doubles when scaling from = 1 to = 32. Building on this, we introduce novel adaptive inference strategy, Best of mini-N in-loop, which leverages the signal of contextual acceptability captured by our reward model. This single, flexible framework partitions large generation budget into smaller, sequential loops and checks if an acceptable response has been found, enabling an early exit. This strategy is powerful because it can be tuned for two distinct goals: serving as robust guardrail for reliability-critical tasks or as an efficiency optimizer for speed-critical ones. For reliability critical applications, the framework can be configured as what we term an Alignment Guardrail. By setting calibrated quality threshold, the system refrains from outputting response unless it is demonstrably acceptable. This is essential in contexts like customer-facing chatbots. For instance, rather than providing policy-compliant but unhelpful response (the least bad option it could generate), the system can recognize that no candidate will satisfy the user and instead escalate the query to human agent. This prevents user frustration and ensures higher standard of interaction. Our experiments confirm this is highly effective, reducing the number of false acceptances by 70% compared to the standard BoN baseline. This ability to abstain or invoke fallback strategies is crucial capability for safe and reliable systems [Lightman et al., 2023, Snell et al., 2024, Kang et al., 2025, Bai et al., 2022]. Conversely, for speed critical applications where slight misalignment is tolerable, such as document summarization, the same framework can be configured as fast Inference Accelerator. Here, the goal shifts from finding the best response to finding the first acceptable one. By terminating the generation process as soon as good-enough candidate is identified, our method achieves the fastest possible inference time. Our experiments show this approach outperforms the baseline by over 22% on average. This establishes clear and tunable trade-off between reliability and computational efficiency that is absent in current methods. The remainder of this paper is structured as follows. Section 2 reviews related work in preference alignment and inference efficiency. Section 3 details the choice-theoretic formulation of our reward model. Section 4 presents the framework for estimating the models parameters and evaluating its performance. Section 5 introduces our primary methodological contribution, the best of mini-N in-loop strategy, and develops the mechanics for its two configurations: the alignment guardrail and the inference accelerator. Section 6 provides experimental results, first confirming the failure modes of standard BoN and then demonstrating the practical effectiveness of our two configurations. Finally, Section 7 concludes and discusses future work."
        },
        {
            "title": "2 Related Work",
            "content": "Most modern LLM alignment techniques, from RLHF to DPO, are trained on datasets of pairwise preferences [Ouyang et al., 2022, Rafailov et al., 2023]. While effective for learning relative ranking, this paradigm fails to capture signal of contextual acceptability. The concept of using an outside option to enrich this data has been explored by Wang et al. [2024]. However, in that work, the outside option was used as data cleaning heuristic; any sample where the annotator chose to reject all responses was simply discarded from the dataset. Similarly, Rho [2025] investigated using an outside option for post-training alignment methods like DPO and RLHF. Their analysis suggests that while the outside option is not directly applicable to the DPO setup and does not alter RLHF outcomes, removing samples where the outside option was chosen can improve DPO performancefurther treating the signal as data cleaning tool. Our work departs from prior approaches by treating the outside option not as data cleaning heuristic, but as core learning signal. We use this signal within discrete choice framework to explicitly teach the reward model the concept of contextual acceptability threshold. This allows the model to distinguish not just what is better, but what is good enougha critical capability for reliable inference-time control. BoN sampling is powerful but computationally expensive method for improving response quality at inference time. Several recent works have focused on mitigating its high cost. For example, speculative rejection sampling aims to halt the generation of unpromising candidates midway through [Sun et al., 2024], while Adaptive BoN seeks to dynamically select smaller for easier prompts [Raman et al., 2025]. Our best of mini-N in-loop method offers complementary strategy. Rather than pruning unpromising candidates, our approach leverages its signal of contextual acceptability to actively identify the first certifiably acceptable candidate and terminate the process early. This provides statistically grounded stopping criterion that other efficiency-oriented methods lack. While we do not explore this 2 Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling PREPRINT here, our early-exit strategy could likely be composed with methods like speculative rejection sampling or Adaptive BoN for further efficiency gains, representing promising direction for future work."
        },
        {
            "title": "3 A Choice-Based Reward Model",
            "content": "In this section, we lay out the theoretical formulation of our choice-based reward model. We begin by adapting the McFadden discrete choice model [McFadden, 2001], standard framework from economics, to include an outside option representing labellers choice to reject all candidate responses. We then specify the utility model and derive the multinomial logit choice probabilities. From this, we define normalized reward function that is anchored to the outside option. We then analyze the properties of this function, showing that it preserves relative preference rankings while providing an interpretable signal of contextual acceptability, as the underlying normalized reward can be uniquely identified from choice data. 3.1 Model Specification Consider scenario where human labeller is presented with choice set = {y0, y1, ..., yJ }. The options yi for all = 0 are responses generated by policy π(x) for given prompt x. Option y0 is symbolic representation of the outside option, signifying the choice to reject all yi for = 0. We model the utility labeller derives from each option yi as the sum of deterministic reward component, R(x, yi), and stochastic error term, ϵi: (x, yi) = R(x, yi) + ϵi. Following standard discrete choice literature, we assume the error terms ϵi are independent and identically distributed according to Gumbel distribution with zero location and unit scale. rational labeller selects the option with the highest utility. That is, option yi is chosen if: R(x, yi) + ϵi max jJ Under this assumption, the probability of the labeller choosing option yi, denoted by the indicator di = 1, takes the familiar multinomial logit form: (1) . (cid:16) R(x, yj) + ϵj (cid:17) Pr(di = 1x, Y) = = = exp( R(x, yi)) j=0 exp( R(x, yj)) exp( R(x, yi) R(x, y0)) j=0 exp( R(x, yj) R(x, y0)) (cid:80)J (cid:80)J exp(R(x, yi)) j=1 exp(R(x, yj)) 1 + (cid:80)J (2) where we define the normalized reward as R(x, yi) R(x, yi) R(x, y0). Consequently, the normalized reward for the outside option is R(x, y0) = 0. Because the errors are i.i.d. Gumbel, the induced model satisfies the independence of irrelevant alternatives; in our fixed-prompt setting with an explicit outside option this is tolerable approximation, but tasks with correlated alternatives would call for richer error structure. The utility of the outside option, R(x, y0), is not tied to specific generated response. Instead, we model it as promptdependent rejection threshold, C(x). This is critical assumption: labellers tolerance for imperfect responsesand thus their threshold for what is good enoughnaturally varies with the prompts context. For example, fact-based question demanding precision will have much higher rejection threshold than creative prompt where minor errors are tolerable. Our normalized reward, R(x, yi), therefore represents the quality of response measured against this prompt-specific, contextual standard of acceptability. We do not estimate C(x) separately; we learn R(x, y) directly and use R(x, y) > 0 as the acceptability criterion. 3.2 Properties and Advantages 3.2.1 Invariance of Relative Preference The normalization of the reward function does not alter its primary function for preference ranking. The subtraction of the outside options utility, R(x, y0) = C(x), is an affine transformation applied to all potential responses for Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling PREPRINT given prompt x. This ensures that the relative preference ordering between any two generated responses, yi and yj, is preserved. That is, R(x, yi) > R(x, yj) if and only if R(x, yi) > R(x, yj). Therefore, the models capacity to function as preference-based reward signal for standard alignment techniques remains intact. 3.2.2 Identification of Contextual Acceptability significant advantage of incorporating an outside option is that the normalized reward model R(x, yi) is fully identified. Unlike standard pairwise comparison models such as the Bradley-Terry model, whose reward functions are only identified up to an arbitrary constant (e.g., R(x, yi) + c) [Wang et al., 2023], our model is anchored by the outside option. This can be seen by rearranging Eq. (2) to express the normalized reward directly in terms of choice probabilities: R(x, yi) = log(Pr(di = 1x, Y)) log(Pr(d0 = 1x, Y)) The reward R(x, yi) is the log-odds of choosing response yi over the outside option, quantity that can be directly estimated from choice data. This complete identification makes crucial difference in interpretation. Bradley-Terry model can only reveal which response is better (relative preference), whereas our model reveals whether response is good in contextual sense. The criterion for this is the sign of the normalized reward: If R(x, yi) > 0, the utility of response yi exceeds the rejection threshold C(x) and can be considered acceptable. If R(x, yi) < 0, the utility is below the threshold and the response can be considered unacceptable. This provides clear, interpretable measure of response quality that goes beyond simple pairwise ranking."
        },
        {
            "title": "4 Estimation & Evaluation",
            "content": "This section outlines the empirical framework for estimating the parameters θ of our reward model and evaluating its performance. We begin by defining the log-likelihood function used for training, which is derived directly from the choice probabilities established in Section 3. Next, to assess the models core capability of identifying context acceptability, we introduce novel evaluation protocol. We detail how the preference data can be transformed for binary classification task and discuss key metricssuch as Precision and False Positive Rate (FPR) for its role as robust alignment guardrail, and Recall for its effectiveness as fast inference Accelerator. Finally, we address the important issue of class imbalance, arguing that the observed choice proportions are vital diagnostic signal of the generators quality and should not be artificially altered through re-sampling. 4.1 Log-Likelihood Function We estimate the parameters θ of the reward function Rθ(x, y) using Maximum Likelihood Estimation (MLE). The objective is to find the parameters that maximize the probability of observing the choices made by human labellers in our dataset, D. The dataset consists of observations, where each observation is tuple (xk, Yk, dk). Here, xk is the prompt, Yk = {yk,0, yk,1, . . . , yk,Jk } is the choice set presented to the labeller, and dk {0, 1, . . . , Jk} is the index of the chosen option. The size of the choice set, Jk, can vary for each observation, with Jk 1. The total log-likelihood of the dataset is the sum of the log-probabilities for each observed choice: L(θ; D) = (cid:88) k=1 log Pr(dkxk, Yk, θ) (3) By substituting the choice probability from Eq. (2), the log-likelihood for single observation where the labeller chose option dk is given by: log Pr(dkxk, Yk, θ) = Rθ(xk, yk,dk ) log 1 + exp(Rθ(xk, yk,j)) Jk(cid:88) j= Note that the 1 in the denominator of the log term corresponds to exp(Rθ(xk, yk,0)), as the normalized reward of the outside option is zero by definition. The model is trained by maximizing the total log-likelihood in Eq. (3) using standard gradient-based optimizer. 4 Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling PREPRINT 4.2 Evaluation Metrics To evaluate the models ability to identify response acceptability within the context, we reframe the evaluation as binary classification task. This allows us to use standard classification metrics to assess how well the learned reward function, ˆRθ(x, y), distinguishes between acceptable and unacceptable responses. 4.2.1 Data Transformation for Binary Evaluation First, we transform our original evaluation dataset, De, into binary classification dataset, Dbin. Each observation in the original dataset is processed as follows: 1. If response was chosen (dk > 0): The labellers choice reveals the preference (xk, yk,dk ) (xk, yk,0). This corresponds to single binary event where the chosen response is preferred over the outside option. We therefore map this observation to single positive instance in the new dataset: (xk, yk,dk , 1), where 1 is the ground truth label for acceptable. 2. If the outside option was chosen (dk = 0): This choice reveals that the utility of the outside option was greater than that of every generated response: (xk, yk,0) (xk, yk,j) for all {1, . . . , Jk}. This single observation unpacks into Jk distinct pairwise preferences. We therefore map this observation to Jk negative instances: {(xk, yk,j, 0)}Jk j=1, where 0 is the ground truth label for unacceptable. It is important to note that this data transformation uses the observed human choice as proxy for the true, latent quality of response. Ideally, the ground truth label for response would be determined by the sign of its true reward, i.e., R(x, y) > 0, as this represents the average preference across population of labellers, not just the stochastic utility of single individual. However, due to the stochastic nature of the choice model specification (1), we only observe noisy realization of this preference. The metrics are therefore conditioned on the observed choice, dk, rather than the unobserved reward sign. For given threshold τ (which is 0 in our framework), we are essentially using the labellers choice to approximate the ideal but unobservable target. Instead of directly measuring quantity like Pr( ˆRθ(x, y) > τ R(x, y) > 0), our metrics are based on the observable proxy Pr( ˆRθ(x, y) > τ response was chosen). This is the best available approximation of the desired target. 4.2.2 Probabilistic Model and Metrics Given this binary dataset and the choice model specification (1), we can assess the models performance. The probability of response being acceptable is modeled as (1x, y) = σ( ˆRθ(x, y)), where σ() is the sigmoid function. The probability of it being unacceptable is (0x, y) = 1 σ( ˆRθ(x, y)) = σ( ˆRθ(x, y)). From this, we can compute several key metrics to understand different aspects of the models performance: Precision: Measures the proportion of responses identified as acceptable (Rθ > 0) that are truly acceptable (label 1). High precision is the key statistic for the alignment guardrail, ensuring the model is trustworthy when it approves response. This minimizes the risk of falsely endorsing candidate that falls short of the labellers standard of acceptability. Recall: Measures the models capacity to identify all truly acceptable responses. This metric is the primary driver of the inference accelerators performance. model with high recall excels at finding an acceptable candidate swiftly, maximizing the probability of an early exit and thereby slashing unnecessary computational overhead. False Positive Rate (FPR): Measures the proportion of unacceptable responses (ground truth label 0) that are incorrectly classified as acceptable ( ˆRθ > 0). low FPR is the most critical measure of the alignment guardrails strictness. It directly quantifies the escape rate-the frequency at which misaligned responses are shown to the usermaking it fundamental to ensuring reliability and trust. These metrics serve as key examples, and the framework allows researchers to employ any standard binary classification metric to evaluate the models performance beyond simple preference ranking. 4.3 Note on Class Imbalance and Choice-Based Sampling This subsection addresses the potential issue of class imbalance when using an outside option and argues that researchers should not attempt to correct for it through re-sampling. 5 Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling PREPRINT In standard preference models without an outside option, such as the Bradley-Terry model, class imbalance is not primary concern. The responses {y1, . . . , yJ } for prompt are typically drawn i.i.d. from generator policy π(x). As the index of response carries no intrinsic meaning, the population proportion of each response being chosen, Q(i), should be equal for all {1, . . . , J}. Ideally, Q(i) = 1/J, meaning no inherent imbalance exists to be corrected. However, the introduction of an outside option fundamentally changes this dynamic. The proportion of the outside option being chosen, Q(0), can be substantially different from the choice proportions of the generated responses, Q(i) for > 0. While it may be tempting to rebalance the dataset to equalize these proportions, this should be avoided. Attempting to correct for this imbalance via re-sampling techniques introduces form of sampling bias. As demonstrated by Manski and Lerman [1977], estimating choice model on dataset where the choice proportions have been artificially altered leads to inconsistent estimates of the underlying reward function. Specifically, if one uses an altered sampling distribution H(i) instead of the true population distribution Q(i) to estimate the model with the maximum likelihood objective in Eq. (3), the resulting reward function, R, becomes biased estimator of the true reward R: R(x, yi) = R(x, yi) + (cid:20) log (cid:19) (cid:18) H(i) Q(i) log (cid:18) H(0) Q(0) (cid:19)(cid:21) . The bracketed term represents bias that is function of the artificial sampling strategy. This bias corrupts the rewards scale and offset, invalidating its interpretation as measure of contextual acceptability. Therefore, manual rebalancing of the dataset must be avoided. 4.3.1 Interpreting Observed Imbalances Instead of being problem to be corrected, significant class imbalance between the outside option and the generated responses should be treated as valuable diagnostic signal about the quality of the generator policy π(x): If Q(0) is very high: This indicates that the base policy is frequently incapable of producing responses that surpass the labellers acceptance threshold. The correct remedy is not to down-sample the outside option, but to improve the generator policy itself, for instance, through another round of Supervised Fine-Tuning (SFT) on high-quality data. If Q(0) is very low: This suggests that the base policy is already strong and almost always produces acceptable responses. In such cases, the signal of contextual acceptability provided by the outside option is less informative, as nearly everything is good. simpler, standard Bradley-Terry preference model may be sufficient for learning the relative ranking between these high-quality responses. The primary takeaway is that the observed choice proportions are crucial source of information about the generator policys quality and should not be obscured through artificial rebalancing."
        },
        {
            "title": "5 Best of mini-N in-loop",
            "content": "This section introduces our primary contribution: an adaptive inference strategy we term best of mini-N in-loop. We show how this single framework can be configured to serve two distinct purposes: as robust alignment guardrail to maximize reliability, or as fast inference accelerator to maximize speed. While both configurations are critical, this sections primary focus is on developing the theory and mechanics behind the Alignment Guardrail. We begin by providing mathematical analysis of critical failure mode in standard BoN, demonstrating how the probability of false acceptance inflates for difficult prompts as more candidates are sampled. We then present our in-loop method and its core mechanism for the guardrail: sequence of calibrated, adaptive thresholds designed to control this risk. Finally, we discuss how this approach allows practitioners to navigate the explicit trade-off between the reliability offered by the guardrail and the latency improvements of the accelerator. 5.1 The Failure Mode of Standard BoN The purpose of this subsection is to demonstrate that for difficult prompts, the standard BoN sampling process is increasingly prone to false acceptances when is not sufficiently large enough. We define prompt as hard if the generator policy π(x) is unlikely to produce an acceptable response, meaning the probability of any single response being good, pg = Pryπ(x)(R(x, y) > 0), is very small. For our analysis, we consider single, representative hard prompt. 6 Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling PREPRINT The BoN process draws i.i.d. samples {y1, . . . , yN } from π(x) and selects the one with the highest estimated reward, which we denote as y: = arg max i{1,...,N } ˆRθ(x, yi). We are interested in the probability of false acceptance, PF A(N ), which is the joint probability that the winning response is incorrectly deemed acceptable while being truly unacceptable: PF A(N ) = Pr( ˆRθ(x, y) > 0 and R(x, y) < 0N ). To analyze how this probability changes with , we can decompose it using the law of total probability by conditioning on the number of good responses, K, in the set of samples. Let = k, where {0, . . . , }. The probability of this event, qN (k) = Pr(K = kN ), follows binomial distribution: qN (k) = (cid:0)N (1 pg)N k. The total probability of false acceptance is then: (cid:1)pk PF A(N ) = (cid:88) k= Pr(false acceptanceN, = k) qN (k). Lets denote the conditional probability as ak,N Pr(false acceptanceN, = k). This term represents the probability of false acceptance given pool of good and bad candidates. The central insight is that for fixed number of good candidates k, increasing the total number of samples can only increase this probability. Adding another bad candidate to the pool can either leave previous bad winner in place or replace it with new bad winner that scored even higher due to estimation noise. It cannot, however, cause previously false-positive outcome to resolve correctly. Thus, ak,N is monotonically non-decreasing in for any fixed k: To formally analyze the behavior of PF A(N ), we examine the difference PF A(N + 1) PF A(N ). Using the binomial recurrence relation qN +1(k) = (1 pg)qN (k) + pgqN (k 1), where pb 1 pg, we can write: ak,N +1 ak,N for N. (4) PF A(N + 1) = = +1 (cid:88) k=0 +1 (cid:88) k=0 ak,N +1qN +1(k) ak,N +1[pbqN (k) + pgqN (k 1)] (with qN (1) = qN (N + 1) = 0) = aN +1,N +1pgqN (N ) + (cid:88) k=0 ak,N +1[pbqN (k) + pgqN (k 1)] = (cid:88) k=0 ak,N +1[pbqN (k) + pgqN (k 1)]. Note that aN +1,N +1pgqN (N ) is zero by definition. false acceptance requires selecting an unacceptable response, which is impossible when the pool of candidates contains only acceptable options. The difference is then found by applying the binomial recurrence and rearranging the terms: PF A(N + 1) PF A(N ) = (cid:88) k=0 qN (k)[ak,N +1 ak,N ] pg (cid:88) k=0 ak,N +1[qN (k) qN (k 1)]. We analyze the terms in this expression to determine the sign of the difference. The first term, (cid:80) qN (k)[ak,N +1 ak,N ], is also non-negative due to the monotonicity of ak,N as established in Eq. (4). The sign of the final term depends on the difference qN (k) qN (k 1), which is positive when < (N + 1)pg and negative when > (N + 1)pg. For hard prompt, pg is very small, causing the mode of the binomial distribution, (N + 1)pg, to be close to zero. This means that for the vast majority of the sum (i.e., for 1), the term qN (k) qN (k 1) is negative, making (cid:80) ak,N +1[qN (k) qN (k 1)], predominantly positive. Even in the case where < (N + 1)pg the third term, pg and the term is negative, its magnitude is scaled by pg, which is close to zero. Consequently, its effect is dominated by the first two non-negative terms. Since the significant terms in the expression are non-negative and the negative contributions are negligible, the difference PF A(N + 1) PF A(N ) is positive for small pg. This confirms our claim that for hard prompts, the probability of false acceptance increases with . We will validate this finding empirically in Section 6.2.1. 7 Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling PREPRINT 5.2 The Best of mini-N in-loop Method Having established the risks of standard BoN sampling, we now introduce our solution: best of mini-N in-loop. The fundamental idea is to partition large generation budget, , into sequential loops, each generating smaller batch of candidates where = L. After each loop, the single best response found so far is checked against threshold to determine if the process can terminate early. The power of this framework lies in the choice of this threshold, which allows the method to be configured for two distinct applications. To create robust alignment guardrail, we use pre-calculated sequence of progressively adjusted thresholds, {τNl }L l=1. This adaptive threshold is designed to maintain consistent level of reliability as the total sample size grows, directly controlling the risk of false acceptance. The precise methodology for calibrating these thresholds is detailed in Section 5.3. Conversely, to configure the method as fast inference accelerator, we use simple fixed threshold of τ = 0. This setting is designed to maximize the chance of an early exit as soon as any response is deemed acceptable, thereby minimizing average inference time for speed-critical applications. If all loops complete without candidate meeting the specified threshold, the system can either return the best response found or abstain entirelya crucial feature for the guardrail configuration. The precise mechanics of this strategy are detailed in Algorithm 1. 5.3 Calibrating the Thresholds The effectiveness of the alignment guardrail hinges on its pre-calculated sequence of thresholds, = {τnl}L l=1. This section details our practical, data-driven procedure for setting these values, which is designed to be robust on hard prompts where the risk of false acceptance is highest. The approach involves two key steps. First, we use hypothesis testing to formally identify set of hard prompts from larger pool. Second, we construct an empirical reward distribution from the responses generated for these prompts and use it to derive statistically consistent acceptance threshold for any given sample size, . 5.3.1 Identifying Hard Prompts via Hypothesis Testing hard prompt is defined to be one for which the generator policys probability of producing an acceptable response, pg, is very low. To identify such prompts from large pool of candidates, we frame the problem as statistical hypothesis test. We first set minimum acceptable goodness probability, pmin, that we believe any reasonable policy should achieve (e.g., pmin = 0.05). For any given prompt, we want to test if its true pg is below this threshold. The procedure is as follows: 1. Set Hyperparameters: Choose minimum acceptable response probability, pmin, and statistical significance level, α (e.g., α = 0.01). 2. Collect Samples: For each candidate prompt x, generate large number of i.i.d. responses, B, from the policy π(x). 3. Count Successes: Using our trained reward model, count the number of acceptable responses, (x) = (cid:80)B i=1 I( ˆRθ(x, yi) > 0). 4. Perform Binomial Test: We test the null hypothesis that the prompts true goodness probability is higher than our minimum acceptable level (H0 : pg(x) > pmin). We calculate the p-value of observing count as low as (x): p-value = Pr(Bin(B, pmin) (x)) (5) If this p-value is less than our significance level α, we reject the null hypothesis and classify the prompt as hard, adding it to our set Dh. This process provides collection of rewards generated specifically from prompts where the policy is known to struggle. 5.3.2 Calculating the Threshold from the Empirical CDF With the rewards from the hard prompts collected, we construct an empirical Cumulative Distribution Function (CDF), denoted (r), which represents the proportion of collected reward scores less than or equal to value r. This empirical distribution is the foundation for our threshold calibration. The key to our approach is the value (0). In our framework, 8 Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling PREPRINT score of zero is the dividing line between an acceptable and unacceptable response. Therefore, (0) represents the observed probability that single response drawn from hard prompt is deemed unacceptable. We set this as our baseline reliability level. Our goal is to find an adjusted threshold, τN , that maintains consistent, pre-defined level of reliability as the sample size grows. To ensure the resulting guardrail is robust, our calibration strategy is intentionally conservative. We derive the thresholds from an empirical reward distribution constructed exclusively from hard promptsthose for which the generator is least likely to produce an acceptable response (pg 0). This focus on the worst-case distribution is direct response to our earlier finding that the reliability of standard BoN degrades most significantly on such prompts. From order statistics, we know that the CDF of the maximum reward from samples is given by [ (r)]N . We want to find the threshold τN where the probability of the BoN winner being unacceptable is equal to our baseline reliability level, (0). We achieve this by solving the equivalence: [ (τN )]N = (0) (6) Solving for τN yields our final formula for the adjusted threshold: (cid:110) 1 (cid:16) [ (0)]1/N (cid:17) (cid:111) τN = max (7) Here, 1(q) is the empirical quantile function, which finds the reward value at the q-th percentile of our collected data. The addition of the max{, 0} operation ensures the calculated threshold τN is always non-negative. This is critical safeguard. Since score of zero is the dividing line for acceptability, negative threshold would defeat the purpose of the alignment guardrail by permitting the acceptance of responses that the model itself scores as unacceptable. This data-driven procedure therefore provides robustly calibrated and conceptually sound guardrail thresholds for use in Algorithm 1. Crucially, because the calibration is performed on the empirical distribution of the normalized reward R, all thresholds τN operate on this same scale and are not tied to any separate estimate of C(x). , 5.4 Two Configurations: The Alignment Guardrail and the Inference Accelerator The best of mini-N in-loop method offers clear advantage over standard BoN by allowing practitioners to configure its behavior for their applications needs. The choice of threshold creates direct trade-off, tuning the framework to act as either robust alignment guardrail or fast inference accelerator. It is important to note that both configurations may result in slightly lower maximum reward compared to full Best-of-N, as the early-exit condition prioritizes finding an acceptable, rather than the absolute best, response. The Alignment Guardrail (τN ): This configuration uses the pre-calculated, adaptive threshold, τN , detailed in Section 5.3. Primary Goal: To maximize reliability by minimizing the risk of false acceptances. Benefit: Provides robust guardrail with low and stable False Positive Rate, ensuring that approved responses are highly trustworthy. Cost: May be slightly slower on average than the accelerator configuration, as the higher threshold requires better response to trigger an early exit. Use Case: Ideal for applications where correctness and reliability are critical. For example, in customer-facing AI, medical information bots, or systems handling sensitive tasks, it is more important to avoid providing misaligned or harmful answer than to respond as quickly as possible. The Inference Accelerator (τ = 0): This configuration uses fixed threshold of zero. Primary Goal: To maximize speed by finding any acceptable answer as quickly as possible. Benefit: Achieves the lowest average inference time due to the high probability of an early exit. Cost: Sacrifices the reliability guarantees of the guardrail. The risk of false acceptances is significantly higher, making it unsuitable for applications requiring high trustworthiness. Use Case: Suitable for applications where speed is paramount and the cost of suboptimal or slightly misaligned response is low. For example, generating creative text, summarizing non-critical documents, or formatting outputs where the factual content is generally correct but the users stylistic preference might not be perfectly met. This flexibility allows practitioners to tune the algorithms behavior, establishing clear trade-off between the robust guarantees of the alignment guardrail and the raw speed of the inference accelerator. 9 Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling PREPRINT"
        },
        {
            "title": "6 Experiment",
            "content": "This section presents the experimental validation for our framework. We begin by detailing the setup, including the models, datasets, and the multi-stage procedure for training our reward model. We then present our results, which are structured to first empirically validate the central problem motivating this work, and then to demonstrate the effectiveness of our solution. First, we provide evidence confirming that the reliability of standard BoN sampling degrades for hard prompts as increases. Having established this failure mode, we then evaluate the two configurations of our best of mini-N in-loop method: we test the alignment guardrail ability to control false acceptances, and the inference accelerator capacity to reduce inference latency. 6.1 Experimental Setup Our experimental design is inspired by the methodology of Chowdhury et al. [2024]. All models were trained using Lora [Hu et al., 2022] on single NVIDIA T4 GPU. 6.1.1 Models and Datasets Base Language Model: We use google/gemma-3-270M [Team, 2025] as the base model for all fine-tuning tasks. Ground Truth Reward Model: To simulate human preferences and generate synthetic training dataset, we use cardiffnlp/twitter-roberta-base-sentiment-latest, powerful sentiment analysis model [Camacho-Collados et al., 2022, Loureiro et al., 2022]. Dataset: All experiments use the stanfordnlp/imdb dataset [Maas et al., 2011], which contains movie reviews. 6.1.2 Training Procedure The creation of our reward model involves three-stage process: 1. Supervised Fine-Tuning (SFT): We first fine-tune the base Gemma model to generate positive-sentiment text. We use the final 12,000 positive reviews from the IMDB training set, splitting them into 10,000 for training and 2,000 for evaluation. For each review, the first 21 tokens (including the bos token) are used as the prompt, and the model is trained to generate the remainder with maximum length of 196 tokens. 2. Synthetic Preference Data Generation: We use the first 12,000 negative reviews from the IMDB dataset as prompts for our SFT model. For each prompt, we generate two distinct responses (y1, y2), with maximum length of 196 tokens. We then score these responses using the ground-truth RoBERTa model. We extract the probability of the positive label and rescale it to range of [1, 1] by applying the transformation (p) = 2(p 0.5). positive score indicates an acceptable response, while negative score indicates an unacceptable one. Finally, we simulate human labellers choice by adding i.i.d. Gumbel noise to the ground-truth scores of y1, y2, and the outside option (with score of 0). The option with the highest noisy score is recorded as the choice, {0, 1, 2} as illustrated by Eq. (1). This process yields dataset of tuples: (x, y1, y2, d). 3. Reward Model Training: The final reward model is trained on 10,000 samples from this synthetic preference dataset, with 2,000 samples held out for evaluation. The model is trained to maximize the log-likelihood as defined in Eq. (3). 6.1.3 Guardrail Threshold Calibration To calculate the adjusted thresholds, τN , for our guardrail experiments, we followed the practical approximation method detailed in Section 5.3. We used the first 500 prompts from the IMDB test set as our candidate pool for identifying hard prompts. For each candidate prompt, we performed binomial hypothesis test by generating = 150 responses from our fine-tuned policy. We set minimum acceptable goodness probability of pmin = 0.05 at significance level of α = 0.01. Prompts that were statistically likely to have true goodness probability lower than pmin were classified as hard. The reward scores from all responses generated for these identified hard prompts were collected to construct an empirical CDF. This distribution was then used to calculate the τN values required for our experiments. The resulting thresholds are shown in Table 1. 10 Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling PREPRINT Table 1: Calculated guardrail thresholds (τN ) for different sample sizes (N ). These values are used in the Best of mini-N in-loop experiments. Sample Size (N) Threshold (τN ) 4 0. 8 0.380 12 0.517 16 0.599 20 0.659 24 0.698 28 0. 32 0.748 6.2 Results Our experimental results validate the two primary claims of this paper. First, we demonstrate that standard BoN sampling suffers from critical reliability issue, with the rate of false acceptances increasing alongside the sample size. Second, we show that our best of mini-N in-loop strategy provides robust solution by demonstrating the effectiveness of its two configurations: the alignment guardrail and the inference accelerator. All experiments were conducted on held-out slice of 1,000 prompts from the IMDB test set. 6.2.1 Standard Best-of-N is Unreliable for Hard Prompts Our first experiment confirms the central problem motivating our work: standard BoN is prone to an increasing number of false acceptances as the sample size, , grows. Figure 1 illustrates this trade-off. While the mean ground-truth reward improves with larger (the green line), this perceived increase in quality comes at steep cost. The absolute count of false positivesunacceptable responses incorrectly rated as goodinflates significantly, rising from 104 at N=1 to 210 at N=32 (the red line). This demonstrates that for hard prompts, the standard BoN approach becomes progressively less reliable, amplifying the risk of showing low-quality or inappropriate response to the user. Figure 1: In standard BoN, the False Positive Count increases with the number of samples (N ), even as the mean reward improves. This highlights critical reliability vulnerability. 6.3 Best of mini-N in-loop: Validating the Two Configurations Our second set of experiments demonstrates that our best of mini-N in-loop strategy offers superior alternative to standard BoN sampling. By analyzing the performance of its two distinct configurationsthe alignment guardrail and the inference acceleratorwe show that our framework allows practitioners to successfully optimize for either reliability or speed, outperforming the monolithic baseline in both cases. 11 Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling PREPRINT Figure 2: Performance of the alignment guardrail configuration compared to the BoN-32 baseline. The Mini-16 in 2 loops setting dramatically reduces the False Positive Count with only marginal decrease in mean reward. The Alignment Guardrail: 70% Reduction in Reliability Failures The primary advantage of our method is its ability to serve as robust guardrail against unacceptable responses. As shown in Figure 2, the alignment guardrail configuration is highly effective. The Mini-16 (2 loops) setting reduces the number of false positives by remarkable 70%, from 210 in the baseline to just 63. Critically, this substantial reliability gain is achieved with negligible trade-off in response quality, with the mean ground truth reward dropping only marginally from 0.486 to 0.466. The detailed metrics in Table 2 reveal how this is achieved. The guardrail configuration significantly boosts Precision to 94.3% (from 88.0%) and slashes the False Positive Rate (FPR) to just 15.8% (from 54.1%). This confirms the guardrails effectiveness at correctly identifying and approving high-quality responses while rejecting those that fail to meet the acceptability threshold. Figure 3: Performance of the inference accelerator configuration. The Mini-16 in 2 loops setting provides the fastest mean execution time, outperforming the BoN-32 baseline by over 22%. The Inference Accelerator: Over 22% Faster Inference In scenarios where speed is the priority, the same framework can be configured as powerful inference accelerator. Figure 3 shows that our method again outperforms the standard BoN baseline. The Mini-16 (2 loops) setting is the fastest of all configurations, reducing the average execution time by over 22%from 7.98 seconds for the baseline to 6.16 seconds. As shown in Table 2, this speed advantage is achieved by maintaining high Recall of 95.4%, equivalent to the baseline. This indicates that the accelerator is just as effective at finding an acceptable response, but its early-exit mechanism allows it to do so much more efficiently. This establishes our method as clear and tunable framework for achieving significant latency improvements with only minimal trade-off in the final response quality. Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling PREPRINT Table 2: Detailed performance metrics for key methods at total budget of = 32. The table highlights the trade-offs between our two configurations: the alignment guardrail enhances reliability by maximizing Precision and minimizing the FPR, while the inference accelerator achieves its speed by maintaining high Recall. Method Baseline Standard BoN-32 Alignment Guardrail Best of mini-16 (2 loops) Inference Accelerator Best of mini-16 (2 loops) TP FP TN FN Precision Recall FPR 1539 210 177 88.0% 95.4% 54.1% 1053 63 336 548 94.3% 65.7% 15.8% 1510 225 192 73 87.0% 95.4% 54.0%"
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we addressed fundamental limitation of preference alignment techniques that rely on pairwise comparisons: their inability to distinguish between what is merely better and what is genuinely good enough. We demonstrated that this gap leads to critical reliability failures in Best-of-N (BoN) sampling, where systems may select the least bad of many unacceptable options. Our primary contribution is new reward modeling framework that, by incorporating an outside option into choice-theoretic model, shifts the paradigm from learning relative rankings to capturing direct signal of contextual quality. This new reward model enables our main practical contribution: Best of mini-N in-loop, single, adaptive inference framework that can be configured for two distinct applications. Our experiments validated both configurations, showing that when tuned as an alignment guardrail, it reduces reliability failures by remarkable 70%. When tuned as an inference accelerator, it improves average inference speed by over 22% compared to the standard BoN baseline. Our framework provides practitioners with principled and tunable method for navigating the critical trade-off between reliability and computational efficiency. Future work could extend this approach to more complex, multiturn conversational settings or reasoning tasks. Furthermore, exploring the synergies between our method and other inference-time techniques, such as speculative decoding, represents promising avenue for developing even more reliable and efficient large language models. 13 Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling PREPRINT"
        },
        {
            "title": "References",
            "content": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Seungjae Jung, Gunsoo Han, Daniel Wontae Nam, and Kyoung-Woon On. Binary classifier optimization for large language model alignment. arXiv preprint arXiv:2404.04656, 2024. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Model alignment as prospect theoretic optimization. In Forty-first International Conference on Machine Learning, 2024. Zihao Wang, Chirag Nagpal, Alexander DAmour, Victor Veitch, and Sanmi Koyejo. Reward model aggregation. In Instruction Workshop @ NeurIPS 2023, 2023. URL https://openreview.net/forum?id=KGBNAJCuPf. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Zhewei Kang, Xuandong Zhao, and Dawn Song. Scalable best-of-n selection for large language models via self-certainty. arXiv preprint arXiv:2502.18581, 2025. Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, and Yi Dong. Helpsteer2-preference: Complementing ratings with preferences. arXiv preprint arXiv:2410.01257, 2024. Hyung Gyu Rho. Investigating the Outside Option in RLHF reward models, August 2025. URL https://github. com/sirano1004/Investigating-the-Outside-Option-in-RLHF-reward-models. Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. Fast best-of-n decoding via speculative rejection. Advances in Neural Information Processing Systems, 37:3263032652, 2024. Vinod Raman, Hilal Asi, and Satyen Kale. Abon: Adaptive best-of-n alignment. arXiv preprint arXiv:2505.12050, 2025. Daniel McFadden. Economic choices. American economic review, 91(3):351378, 2001. Charles Manski and Steven Lerman. The estimation of choice probabilities from choice based samples. Econometrica: Journal of the Econometric Society, pages 19771988, 1977. Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. Provably robust dpo: Aligning language models with noisy feedback. arXiv preprint arXiv:2403.00409, 2024. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Gemma Team. Gemma 3. 2025. URL https://arxiv.org/abs/2503.19786. Jose Camacho-Collados, Kiamehr Rezaee, Talayeh Riahi, Asahi Ushio, Daniel Loureiro, Dimosthenis Antypas, Joanne Boisson, Luis Espinosa Anke, Fangyu Liu, and Eugenio Martínez Cámara. TweetNLP: Cutting-edge natural language processing for social media. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3849, Abu Dhabi, UAE, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-demos.5. Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling PREPRINT Daniel Loureiro, Francesco Barbieri, Leonardo Neves, Luis Espinosa Anke, and Jose Camacho-collados. TimeLMs: In Proceedings of the 60th Annual Meeting of the Association for Diachronic language models from Twitter. Computational Linguistics: System Demonstrations, pages 251260, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi:10.18653/v1/2022.acl-demo.25. URL https://aclanthology.org/2022. acl-demo.25. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P11-1015. 15 Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling PREPRINT"
        },
        {
            "title": "A Acknowledgements",
            "content": "We used OpenAI ChatGPT, Google Gemini, and xAI Grok for literature search support, wording/clarity edits, mathematical cross-checks, and code review/debugging. The authors verified all outputs and are solely responsible for the papers content. The experiments presented in this paper were made possible by the computational resources provided by Google Colab and the Kaggle platform."
        },
        {
            "title": "B Best of Mini N in Loop Algorithm",
            "content": "Algorithm 1 Best of mini-N in-loop 1: Input: Prompt x, total budget , mini-batch size n, number of loops = N/n, tuning mode {Alignment Guardrail, Inference Accelerator}. l=1. l=1 using the method in Section 5.3. Set constant threshold: = {0}L Pre-calculate thresholds: = {τnl}L 2: Initialize: Set of all responses Yall . 3: Initialize: Maximum observed reward ˆRmax . 4: Initialize: Best response found None. 5: Initialize: Loop success flag found_acceptable False. 6: if = Alignment Guardrail then 7: 8: else 9: 10: end if 11: for = 1 to do 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: end for 24: if found_acceptable then Return: 25: 26: else 27: 28: end if found_acceptable True. break Let current total sample size be Nl = l. Retrieve the pre-calculated threshold for this stage, τNl , from . Generate new mini-batch of responses: Ynew = {y1, . . . , yn} π(x). Update the set of all responses: Yall Yall Ynew. Find the best response so far: Update the maximum reward: ˆRmax = ˆRθ(x, Update the best response: y if ˆRmax > τNl then current = arg maxyYall current). Return: or None ˆRθ(x, y). current. end if Early exit: an acceptable candidate has been found. Return the best acceptable response. Optionally, refuse to respond if no candidate met the final threshold."
        }
    ],
    "affiliations": []
}