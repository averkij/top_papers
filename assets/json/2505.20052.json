{
    "paper_title": "Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion Enhances Protein Representations",
    "authors": [
        "Hazem Alsamkary",
        "Mohamed Elshaffei",
        "Mohamed Elkerdawy",
        "Ahmed Elnaggar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Protein language models (PLMs) have emerged as powerful tools to detect complex patterns of protein sequences. However, the capability of PLMs to fully capture information on protein sequences might be limited by focusing on single pre-training tasks. Although adding data modalities or supervised objectives can improve the performance of PLMs, pre-training often remains focused on denoising corrupted sequences. To push the boundaries of PLMs, our research investigated a multi-task pre-training strategy. We developed Ankh3, a model jointly optimized on two objectives: masked language modeling with multiple masking probabilities and protein sequence completion relying only on protein sequences as input. This multi-task pre-training demonstrated that PLMs can learn richer and more generalizable representations solely from protein sequences. The results demonstrated improved performance in downstream tasks, such as secondary structure prediction, fluorescence, GB1 fitness, and contact prediction. The integration of multiple tasks gave the model a more comprehensive understanding of protein properties, leading to more robust and accurate predictions."
        },
        {
            "title": "Start",
            "content": "Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion Enhances Protein Representations Hazem Alsamkary , Mohamed Elshaffei , Mohamed Elkerdawy, and Ahmed Elnaggar 5 2 0 2 6 ] . [ 1 2 5 0 0 2 . 5 0 5 2 : r Proteinea Inc."
        },
        {
            "title": "Abstract",
            "content": "Protein language models (PLMs) have emerged as powerful tools to detect complex patterns of protein sequences. However, the capability of PLMs to fully capture information on protein sequences might be limited by focusing on single pre-training tasks. Although adding data modalities or supervised objectives can improve the performance of PLMs, pre-training often remains focused on denoising corrupted sequences. To push the boundaries of PLMs, our research investigated multi-task pre-training strategy. We developed Ankh3, model jointly optimized on two objectives: masked language modeling with multiple masking probabilities and protein sequence completion relying only on protein sequences as input. This multi-task pre-training demonstrated that PLMs can learn richer and more generalizable representations solely from protein sequences. The results demonstrated improved performance in downstream tasks, such as secondary structure prediction, fluorescence, GB1 fitness, and contact prediction. The integration of multiple tasks gave the model more comprehensive understanding of protein properties, leading to more robust and accurate predictions."
        },
        {
            "title": "1 Introduction",
            "content": "Protein language models (PLMs) have led to paradigm shift in the field of synthetic biology, enabling effective modeling of diverse tasks and the generation of novel proteins. Current language models typically undergo training on single task, masked language modeling (MLM) utilizing bi-directional encoder models (Rives et al., 2019), encoder-decoder models (), or decoder-only models (Madani et al., 2020). In the present research, we introduce Ankh3, PLM pre-trained on only two tasks out of three from the UL2 objective (Tay et al., 2022). The UL2 objective contains three tasks: R-denoiser [NLU] that focuses on high corruption of short spans, S-denoiser [S2S] that is sequential denoiser, and Xdenoiser [NLG] that focuses on extreme denoising of mix of short and long spans with high and low denoising. X-denoiser was not used due to previous experiences of long span masking in former Ankh models that resulted in poor performance. Hence, this research only focuses on short-span sequence variable denoising/demasking [NLU] and sequence completion/sequential denoising [S2S]. Sequence denoising aligns with the regular T5 denoising objective in which sentinel tokens are incorporated and sequence completion is modeled as completing the remaining half of given protein sequence presented to the model. Two sizes of Ankh3 are provided  (Table 1)  , Ankh3-Large and Ankh3-XL having the following configurations: Correspondence: publications@proteinea.com 1 Table 1: Overview of the main model parameters and design choices Parameters Embedding dim Layers Encoder layers Decoder layers Tie word embeddings Feedforward dim Non-linearity Num heads KV dim Relative attention max distance Relative attention num buckets Num sentinel tokens Vocab size Large 1536 72 48 24 no XL 2560 72 48 24 no"
        },
        {
            "title": "SwiGLU SwiGLU",
            "content": "16 64 128 64 225 256 32 64 128 64"
        },
        {
            "title": "2.1 Modeling",
            "content": "In contrast to using the conventional masked language modeling method, as implemented in Ankh (Elnaggar et al., 2023), ESM2 (Rives et al., 2019) and ESM3 (Hayes et al., 2024) models or autoregressive models as implemented in ProGen2 (Nijkamp et al., 2022), we added another pre-training task where the model is tasked to complete the remaining 50% of an input sequence. The first 50% of the sequence was given to the model as input to the encoder, and the remaining 50% of the sequence was generated by the decoder conditioned on the output representation of the first half. This approach aims to enhance the understanding of the models for protein sequences in terms of generation and performance on downstream tasks. Although variable completion percentages may enhance the performance of the model, it was not experimented in this work due to the limited computation power. However, in inference, the model can generate the whole protein sequence with variable sequence length input. Finally, because one more task was added during the pre-training, the increase in the capacity of Ankh3-XL model did not lead to performance degradation as in ProtT5 (Elnaggar et al., 2021) when scaled from ProtT5-XL to ProtT5-XXL while all hyperparameters were fixed, including the dataset."
        },
        {
            "title": "2.2 Architecture",
            "content": "As in preceding Ankh models, Ankh3 is based on T5 architecture (Raffel et al., 2020) with the model configuration mentioned in table 1, an encoder-decoder transformer model with T5 relative positional bias. Table 2 presents the total number of parameters for each model. Table 2: Number of parameters for each Ankh3 model Model Ankh3-Large Ankh3-XL"
        },
        {
            "title": "Encoder parameters Decoder Parameters Total Parameters",
            "content": "1,151,879,680 3,484,799,488 727,169,536 2,246,107,648 1,879,049,216 5,730,907,"
        },
        {
            "title": "2.3 Pre-training Data",
            "content": "Inspired by its predecessors Ankh (Elnaggar et al., 2023) and ProtTrans (Elnaggar et al., 2021), the protein language model Ankh3 underwent pre-training with substantial dataset comprising 59,130,945 distinct sequences sourced from UniRef50. The UniProt Reference Clusters (UniRef) are collection of databases that offer comprehensively clustered sets of protein sequences from the UniProt Knowledgebase (UniProtKB), which includes isoforms, along with chosen records from UniParc as detailed in (Suzek et al., 2015). 2 This clustering methodology employs varying sequence similarity thresholds to ensure that the resulting clusters are non-redundant and maintain homogeneity among the sequences within each cluster. Specifically, in the UniRef100 database, every entry or cluster consists of identical sequences, including sub-fragments, irrespective of the organism from which they originate. Subsequently, the UniRef50 database is created by further clustering the representative sequences from UniRef90 (which is built by clustering UniRef100 sequences at 90% identity threshold) using 50% sequence identity cutoff (Suzek et al., 2015). This hierarchical approach ensures reduced redundancy dataset ideal for training large-scale protein models. (Suzek et al., 2015)."
        },
        {
            "title": "2.4 Compute Infrastructure",
            "content": "Both Ankh3 models were trained utilizing TPUv4 with 64 chips. The T5x library (Roberts et al., 2022), developed in JAX (Bradbury et al., 2018), served as the implementation platform. Ankh3-Large was trained without replication or sharding, while Ankh3-XL employed both 2-way data sharding and 2-way model sharding. Optimizer states were not sharded in either model."
        },
        {
            "title": "2.5 Training Setup",
            "content": "While conventional masked language modeling traditionally utilizes single masking probability, the training here employed three distinct masking probabilities: 15%, 20%, and 30%. These probabilities governed the proportion of masked tokens and were sampled uniformly to ensure that each masking level had an equal chance of selection in every training step. This uniform distribution facilitated the exposure of the model to varied range of masking conditions. In the previous Ankh model (Elnaggar et al., 2023), different denoising probabilities were explored before reaching the final masking probability, and it was concluded that some tasks performed better when Ankh was trained on higher denoising probabilities, while other tasks performed better with lower denoising probabilities. Therefore, MLM was utilized here with multiple masking probabilities during pre-training so that good performance is achieved in most of the tasks. For the multi-task training setup, each sequence from the dataset was randomly assigned to either the MLM or sequence completion task each time it was selected for training batch. This random assignment strategy ensured that when sequence was sampled multiple times over the course of training, the model could be exposed to that same sequence under different task objectivesfor instance, processing it for MLM in one training step and for sequence completion in another. This allowed the model to learn diverse aspects from the identical input data by experiencing it through varied task contexts. In sequence completion, the sequence was divided into two segments: the first half was given to the encoder as input, and the second segment was predicted by the decoder, conditioned on the representation of the first segment that was output by the encoder model. The training hyperparameters for each model are shown in Table 3. Table 3: Training hyperparameters of the Ankh3 models"
        },
        {
            "title": "Hyperparameters\nSequence length\nMasking probability\nWarmup scheduler\nWarmup steps\nLearning rate\nOptimizer\nNum steps\nBatch size\nWeight decay",
            "content": "Ankh3 Large 512 Ankh3-XL 512 15% 20% 30% 15% 20% 30% rsqrt decay 10K 1e-2 Adafactor 4M 1024 0.0 rsqrt decay 10K 1e-2 Adafactor 5M 1024 0."
        },
        {
            "title": "3 Downstream Tasks Evaluation",
            "content": "The performance of the two Ankh3 models was assessed based on four benchmarking tasks: secondary structure prediction, fluorescence prediction, GB1, and contact prediction. The benchmark settings were freezing the backbone model, extracting the representation of each sequence, pooling the representation using average pooling in sequence prediction tasks (e.g., fluorescence prediction and GB1), and 3 finally, these representations are passed to ConvBERT model (Jiang et al., 2020). ConvBERT hyperparameters were fixed across all tasks. The hyperparameters of the ConvBERT model are summarized in Table 4. Table 4: ConvBERT hyperparameters during benchmarking Hyperparameters Num layers Feedforward dim Num heads Dropout Kernel size Pooling (if needed) Learning rate Warmup steps Num epochs Gradient accumulation steps Batch Size Weight Decay ConvBERT 1 Embedding dim / 2 4 0.1 7 Global average pooling 1e-2 1000 20 16 1 0. Secondary structure prediction: Secondary structures are the conformational arrangements of the polypeptide backbone in either α-helix, β-strand, or coils (Ma et al., 2018). This task gives insights into the quality of the protein sequence representation of the model being tested. It has two levels of granularity: the first level involves predicting one of three states of the secondary structure (SSP-3) for each amino acid, and the second level involves predicting one of eight states (SSP-8) of the secondary structure for each amino acid, which is more challenging. Accuracy was used as the primary metric to measure the proportion of correctly predicted states. To test the models performance, CASP12 (Abriata et al., 2018) and CASP14 (Kryshtafovych et al., 2021) were used, and DSSP was used to compute the labels for each sequence. Fluorescence prediction: Fluorescence is phenomenon where certain proteins, like Green Fluorescent Protein (GFP), emit light after absorbing it. The light intensity can be highly sensitive to the amino acid sequence due to the change in structure and the efficiency of the light-emitting machinery (Sample et al., 2009). Each PLM was trained on fluorescence dataset (Rao et al., 2019), where the model is tasked to map protein sequence to its corresponding fluorescence value. The fluorescence value is single continuous value; hence, all sequence information was aggregated into single vector using global average pooling. Spearman correlation was used as the primary metric to measure how well the model is able to order the sequences based on their predicted fluorescence. GB1 fitness: GB1 is the binding part of Protein that binds to immunoglobulins, and it is important for purifying antibodies (Sommer et al., 2012). The study of mutations in GB1 is considered the benchmark for understanding how mutations interact in non-additive ways, phenomenon known as epistasis (Wu et al., 2016). This task uses regression to assess how well variants of the GB1 protein bind after mutations were introduced at four specific locations. The GB1 dataset for this task was sourced using the FLIP benchmark (Dallago et al., 2022). The model aims to predict continuous fitness score for given protein sequence. To achieve this, information from the protein sequence as understood by the model was condensed into single vector by global average pooling. The performance of the model was assessed using Spearman correlation to indicate how accurately the model can rank the protein sequences according to their predicted GB1 binding fitness. Contact prediction: Contact prediction assesses the ability of the model to infer the spatial proximity between pairs of amino acids from their sequence. Contacts were defined based on the Euclidean distance between the Cα atoms of residue pairs. contact is assumed if this Cα-Cα distance is less than 8.0 Angstroms, resulting in an binary contact map for each protein of length . For this task, ProteinNet (AlQuraishi, 2019) and CASP14 (Kryshtafovych et al., 2021) were used. For evaluation, the predicted contacts between residue pairs (i, j) were specifically considered with sequence separation of j 6. Performance for these contacts was assessed 4 using precision. The primary metrics were Precision@L (P@L) and Precision@L/5 (P@L/5), where is the sequence length. P@L measures the precision among the highest-scoring predicted contacts satisfying this separation criterion, while P@L/5 evaluates this precision for the top L/5 such contacts."
        },
        {
            "title": "3.1 Evaluation Details",
            "content": "The performance of the PLMs was tested in two different settings: the first setting is concatenating the [NLU] token at the beginning of each sequence, and the second setting is concatenating the [S2S] token at the beginning of each sequence. Table 5 illustrates the average performance of each model with both [NLU] and [S2S] tokens. Three different seeds (7, 0, and 42) were run for each task, and the average performance was reported. The results of the previous Ankh and ESM2 models were concluded from the original paper (Elnaggar et al., 2023). Table 5: Average performance of models is reported in percentage (%). For Ankh3 models, [NLU] and [S2S] tokens are concatenated at the beginning of the sequence. The exceptions to percentage-based reporting are Fluorescence (FL) and GB1, for which the reported metric is Spearman correlation. Other metrics include accuracy for SSP and Precision@K for CP. SSP: Secondary structure prediction; FL: Fluorescence; CP: Contact prediction. Models under the protein sequence only input category were pre-trained using only protein sequences, while multimodal models utilized protein sequences and other modalities during pre-training (e.g. secondary structure and structure tokens). Protein Sequence Only Input Multi-task Single-task Multimodal Multi-task Task Dataset Ankh3-L Ankh3-XL Ankh Base* Ankh Large* ESM2-650M* ESM2-15B* ESM3-open CASP-12 CASP-12 CASP-14 CASP-14 TAPE FLIP NLU S2S NLU S2S 78.03 0. 75.49 0.21 84.40 0.05 83.76 0.10 65.29 0.05 62.74 0.22 72.53 0. 72.25 0.16 79.28 0.15 77.96 0.09 82.19 0.13 82.30 0.35 65.50 0. 65.88 0.10 69.85 0.09 69.51 0.30 64.56 0.23 64.89 0.36 64.23 0. 65.43 0.30 90.30 0.39 89.44 0.77 89.62 1.55 90.44 0.89 ProteinNet (L/1) 46.35 0.49 43.39 0.44 60.95 0.35 60.76 0.21 ProteinNet (L/5) 69.42 0. 66.60 0.87 83.89 0.47 83.31 1.21 CASP-14 (L/1) 16.94 0.56 18.59 0. 29.23 0.36 28.68 0.31 CASP-14 (L/5) 26.65 0.49 29.96 2.09 46.17 1. 47.30 2.09 SSP-3 SSP-8 SSP-3 SSP-8 FL GB1 CP CP CP CP 80. 68.85 76.67 62.33 62.0 85.0 43. 66.63 13.50 28.65 83.59 71.69 77. 63.17 62.0 84.0 48.93 73.49 16. 29.91 82.43 70.50 76.97 62.10 48. 82.0 29.36 50.74 13.71 22.25 83. 71.17 76.56 61.81 56.0 57.0 31. 52.97 14.44 26.61 83.43 0.02 73.50 0.37 83.20 0. 71.70 0.11 58.78 0.57 82.91 2.53 56.46 8.78 77.40 5.80 28.65 1. 47.35 4.60 * Sourced from the Ankh paper; standard deviation is not provided here as the original publication reported error bars for these specific entries instead."
        },
        {
            "title": "4 Discussion",
            "content": "While NLU and S2S tasks are different, neither of them consistently performed better than the other across all the tasks and model sizes. The preferred objective seems to be task-dependent. One interesting observation that requires deeper investigation is that Ankh3-XL performed better with S2S in sequence classification tasks, such as GB1 and fluorescence  (Table 5)  . However, this pattern should be tested with more tasks to confirm its consistency. In the context of scaling Ankh3-Large to Ankh3-XL, Ankh3-XL performed significantly better in both NLU and S2S tasks, indicating that the capacity of Ankh3-Large was insufficient to handle both tasks. As previously discussed in Section 2.1 and demonstrated by experiments in ProtT5 (Elnaggar et al., 2021), no improvement in performance is achieved by solely increasing the model size and using protein sequences as the only input while keeping all other factors constant (including the dataset), for example, CASP12 reached 84.4% in Ankh3-XL compared to 79.2% in ProtT5-XXL (4.8B encoder parameters) and 81.4% in ProtT5-XL (1.2B encoder parameters), this indicates that the addition of multiple masking probabilities and sequence completion were the main contributors to the performance boost while using protein sequences as the only input. Ankh3 performed better in all tasks compared to protein sequence-only models like ESM2 and Ankh. When compared to ESM3, Ankh3 has competitive performance on tasks such as SSP and CP. However, in tasks that neither Ankh3 nor ESM3 encountered during pre-training, Ankh3-XL performed significantly better, which indicates that Ankh3-XL may have better generalization, especially in tasks like fluorescence and GB1 fitness prediction. 5 ESM3 was primarily trained on masked language modeling, which processes multiple discrete inputs and outputs; these inputs included secondary structure tokens with eight states, structure tokens, and other different modalities. Since ESM3 was already trained with such secondary structure inputs, it was expected to outperform Ankh3 in tasks evaluating this feature, as reported in Table 5. It was also hypothesized that ESM3 would outperform Ankh3 in contact prediction, as its training included structure tokens that can enhance contact prediction accuracy. This hypothesis was confirmed with seeds 7 and 0, where ESM3 performed noticeably better than Ankh3. However, with seed 42, ESM3s performance was significantly lower and also exhibited large standard deviation  (Table 5)  . For example, when run with seed 42 on ProteinNet (L/1), ESM3 showed standard deviation of 8.78%. This high variability observed with seed 42 likely contributed to ESM3s overall poorer performance metrics when compared to Ankh3, which demonstrated stability across all three seeds in all tasks."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we presented Ankh3, the latest version of the Ankh family. Ankh3 performance was demonstrated in the setting of multi-task pre-training. Sequence completion was explored as an additional task, and masked language modeling with multiple masking probabilities was shown to enhance the performance of the model. These results highlight the continued potential for advancing sequence-only protein language models through innovative multi-task learning, yielding more robust and versatile protein representations without immediate reliance on additional data modalities. To foster reproducibility and further research, we shared all of our work and details, including pre-training hyperparameters, along with the dataset on Huggingface. Finally, the weights of both Ankh3 models are available open source on Huggingface. Our future research directions include scaling Ankh3 to incorporate multiple modalities and further exploring sequence completion with variable completion percentages."
        },
        {
            "title": "6 Data Availability",
            "content": "The dataset used for pre-training is available at https://huggingface.co/datasets/agemagician/uniref50. Model weights of both Ankh3-Large and Ankh3-XL models are available at https://huggingface.co/ ElnaggarLab/ankh3-large and https://huggingface.co/ElnaggarLab/ankh3-xl."
        },
        {
            "title": "Acknowledgment",
            "content": "The authors gratefully acknowledge the significant contributions of Proteineas team, especially Nehal Adel Abdelsalam, for the invaluable guidance and for ensuring that the quality of this research paper meets the standards. We also gratefully acknowledge Proteineas deep learning and bioinformatics teams, who provided essential assistance with hardware, software, and numerous other project facets. We are indebted to Google for their comprehensive support, including Jonathan Caton, Shira Genauer, Astitva Chopra, and the Google Cloud, Google Innovator, JAX, and TRC Teams, for their help in configuring the project on Google Cloud and troubleshooting technical challenges. This research was also financially supported by Google through the Google Research Innovator and Google TPU Cloud Research Credits Programs. We also extend our thanks to the HuggingFace team, particularly Patrick von Platen, Julien Chaumond, and Clement Delangue, whose support was crucial for making the trained models publicly accessible. Lastly, we express our gratitude to the global research community for making the datasets utilized in this study freely available."
        },
        {
            "title": "References",
            "content": "Abriata, L. A., Tam`o, G. E., Monastyrskyy, B., Kryshtafovych, A., & Dal Peraro, M. (2018). Assessment of hard target modeling in casp12 reveals an emerging role of alignment-based contact prediction methods. Proteins: Structure, Function, and Bioinformatics, 86(S1), 97112. https: //doi.org/https://doi.org/10.1002/prot.25423 6 AlQuraishi, M. (2019). ProteinNet: standardized data set for machine learning of protein structure. BMC bioinformatics, 20(1), 110. Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., & Zhang, Q. (2018). JAX: composable transformations of Python+NumPy programs (Version 0.3.13). http://github.com/jax-ml/jax Dallago, C., Mou, J., Johnston, K. E., Wittmann, B. J., Bhattacharya, N., Goldman, S., Madani, A., & Yang, K. K. (2022). Flip: Benchmark tasks in fitness landscape inference for proteins. bioRxiv. https://doi.org/10.1101/2021.11.09. Elnaggar, A., Essam, H., Salah-Eldin, W., Moustafa, W., Elkerdawy, M., Rochereau, C., & Rost, B. (2023). Ankh: Optimized protein language model unlocks general-purpose modelling. arXiv preprint arXiv:2301.06568. Elnaggar, A., Heinzinger, M., Dallago, C., Rehawi, G., Wang, Y., Jones, L., Gibbs, T., Feher, T., Angerer, C., Steinegger, M., et al. (2021). Prottrans: Towards cracking the language of lifes code through self-supervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44, 71127127. Hayes, T., Rao, R., Akin, H., Sofroniew, N. J., Oktay, D., Lin, Z., Verkuil, R., Tran, V. Q., Deaton, J., Wiggert, M., Badkundri, R., Shafkat, I., Gong, J., Derry, A., Molina, R. S., Thomas, N., Khan, Y. A., Mishra, C., Kim, C., . . . Rives, A. (2024). Simulating 500 million years of evolution with language model. bioRxiv. https://doi.org/10.1101/2024.07.01.600583 Jiang, Z.-H., Yu, W., Zhou, D., Chen, Y., Feng, J., & Yan, S. (2020). Convbert: Improving bert with span-based dynamic convolution. Advances in Neural Information Processing Systems, 33, 1283712848. Kryshtafovych, A., Schwede, T., Topf, M., Fidelis, K., & Moult, J. (2021). Critical assessment of methods of protein structure prediction (casp)round xiv. Proteins: Structure, Function, and Bioinformatics, 89(12), 16071617. https://doi.org/https://doi.org/10.1002/prot.26237 Ma, Y., Liu, Y., & Cheng, J. (2018). Protein secondary structure prediction based on data partition and semi-random subspace method. Scientific reports, 8(1), 9856. Madani, A., McCann, B., Naik, N., Keskar, N. S., Anand, N., Eguchi, R. R., Huang, P.-S., & Socher, R. (2020). Progen: Language modeling for protein generation. arXiv preprint arXiv:2004.03497. Nijkamp, E., Ruffolo, J., Weinstein, E. N., Naik, N., & Madani, A. (2022). Progen2: Exploring the boundaries of protein language models. https://arxiv.org/abs/2206.13517 Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2020). Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140), 167. http://jmlr.org/papers/v21/20-074.html Rao, R., Bhattacharya, N., Thomas, N., Duan, Y., Chen, X., Canny, J., Abbeel, P., & Song, Y. S. (2019). Evaluating protein transfer learning with tape. https://arxiv.org/abs/1906.08230 Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C. L., Ma, J., & Fergus, R. (2019). Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. PNAS. https://doi.org/10.1101/622803 Roberts, A., Chung, H. W., Levskaya, A., Mishra, G., Bradbury, J., Andor, D., Narang, S., Lester, B., Gaffney, C., Mohiuddin, A., Hawthorne, C., Lewkowycz, A., Salcianu, A., van Zee, M., Austin, J., Goodman, S., Soares, L. B., Hu, H., Tsvyashchenko, S., . . . Gesmundo, A. (2022). Scaling up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189. https : //arxiv.org/abs/2203. Sample, V., Newman, R. H., & Zhang, J. (2009). The structure and function of fluorescent proteins. Chemical Society Reviews, 38(10), 28522864. Sommer, L. A., Meier, M. A., & Dames, S. A. (2012). fast and simple method for probing the interaction of peptides and proteins with lipids and membrane-mimetics using gb1 fusion proteins and nmr spectroscopy. Protein Science, 21(10), 15661570. Suzek, B. E., Wang, Y., Huang, H., McGarvey, P. B., Wu, C. H., & Consortium, U. (2015). Uniref clusters: comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics, 31(6), 926932. Tay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Wei, J., Wang, X., Chung, H. W., Shakeri, S., Bahri, D., Schuster, T., et al. (2022). Ul2: Unifying language learning paradigms. arXiv preprint arXiv:2205.05131. Wu, N. C., Dai, L., Olson, C. A., Lloyd-Smith, J. O., & Sun, R. (2016). Adaptation in protein fitness landscapes is facilitated by indirect paths (R. A. Neher, Ed.). eLife, 5, e16965. https://doi. org/10.7554/eLife."
        }
    ],
    "affiliations": [
        "Proteinea Inc."
    ]
}