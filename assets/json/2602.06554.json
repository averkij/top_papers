{
    "paper_title": "SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees",
    "authors": [
        "Tianyi Hu",
        "Qingxu Fu",
        "Yanxi Chen",
        "Zhaoyang Liu",
        "Bolin Ding"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies. In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios. To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction. Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability."
        },
        {
            "title": "Start",
            "content": "2026-02-09 SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees Tianyi Hu, Qingxu Fu, Yanxi Chen, Zhaoyang Liu, Bolin Ding Tongyi Lab , Alibaba Group {wenju.hty, fuqingxu.fqx, chenyanxi.cyx, jingmu.lzy, bolin.ding}@alibaba-inc.com Abstract Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies. In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPOs original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios. To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-byturn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction. Experiments on AppWorld and BFCL v4 demonstrate SeeUPOs substantial improvements over existing backbone algorithms: relative gains of 43.3%54.6% on Qwen3-14B and 24.1%41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability. 6 2 0 F 6 ] . [ 1 4 5 5 6 0 . 2 0 6 2 : r Figure 1: Performance comparison of training Qwen3-14B model on the AppWorld and BFCL-v4 benchmarks. (a)-(b) show training curves, (c)-(f) show test curves. SeeUPO algorithm demonstrates significantly stronger training stability and optimal performance compared to other backbone RL algorithms. Corresponding authors."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of large language models (LLMs) (Liu et al., 2024; Yang et al., 2025a) has catalyzed the emergence of autonomous AI agents capable of executing complex tasks through tool use and multi-turn interactions with diverse environments. These AI systems have demonstrated remarkable capabilities across wide spectrum of real-world applications, including web navigation (Liu et al., 2023), software development (Trivedi et al., 2024), and interactive tool-augmented environments (Patil et al., 2025). The significant potential of agentic AI has motivated extensive research efforts, making the development of robust, scalable training methods key research focus. Within this landscape, reinforcement learning (RL) has emerged as the predominant paradigm for training LLM-based agents through interaction and feedback. growing body of work has explored RL-driven agent training across various dimensions: (interaction scope) from single-turn task reasoning to multiturn interactive planning (Chai et al., 2025; Wang et al., 2025; Xi et al., 2025; Jiang et al., 2025); (capability integration) from static capability modules to unified policy optimization, where RL transforms planning, tool use, and memory into interdependent, trainable policies (Qian et al., 2025; Yan et al., 2025); (multimodal integration) from single-modal text generation to embodied and multimodal perception (Qi et al., 2025; Feng et al., 2025; Li et al., 2025); and (evolutionary mechanism) from heuristic-based self-correction to internalized self-improvement, allowing agents to iteratively refine knowledge and strategies through RL-driven self-evolution (Wang et al., 2025; Guan et al., 2025; Zhai et al., 2025). Notably, while these methods span diverse research directions, they converge on common foundation: small set of backbone RL algorithms and their variants (Srivastava & Aggarwal, 2025). For instance, Proximal Policy Optimization (PPO) (Schulman et al., 2017) serves as the foundational algorithm for actor-critic RL (Ouyang et al., 2022), where the canonical PPO implementation estimates advantage functions through Generalized Advantage Estimation (GAE) (Schulman et al., 2016) and critic networks, and updates policy networks via proximal policy optimization. RLOO (Ahmadian et al., 2024) achieves critic-free advantage estimation approach by sampling multiple within-group responses for the same query to compute group relative advantages, and performs sequence-level policy updates via REINFORCE. Group Relative Policy Optimization (GRPO) (Shao et al., 2024) combines the Group Relative Advantage Estimation (GRAE) insight of RLOO with PPO-style policy updates, becoming representative algorithm that combines PPO with critic-free approaches. Group Sequence Policy Optimization (GSPO) (Zheng et al., 2025b) further refines GRPO with sequence-level mechanisms, becoming representative of sequence-level RL algorithms. However, the theoretical soundness of these backbone algorithms in agentic RL scenarios, particularly in multi-turn settings, remains an open question (Zhang et al., 2026). To address this question, we categorize mainstream backbone RL algorithms from the perspectives of advantage estimation and policy update mechanisms, and analyze the convergence properties of different combinations in both single-turn and multi-turn scenarios. Specifically, we examine two types of advantage estimation methods: Generalized Advantage Estimation (GAE) (critic-dependent), and Group Relative Advantage Estimation (GRAE) (critic-free). We also examine two types of policy update mechanisms: REINFORCE (fully on-policy, based on Vanilla Policy Gradient), and Proximal Policy Update (PPU)* (partially on-policy, based on Importance Sampling correction). We prove that the combination of GRAE and REINFORCE converges to the globally optimal solution only under undiscounted settings. We discover that the combination of GRAE and PPU breaks PPOs original monotonic improvement property in most cases, with convergence guarantees only in contextual bandit scenarios. Through comprehensive analysis, we find that mainstream backbone RL algorithms cannot simultaneously enjoy both critic-free operation and good monotonic improvement / convergence guarantees in multi-turn scenarios. To address these limitations, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), novel algorithm that provides convergence guarantees for multi-turn agentic RL while maintaining the critic-free approach. SeeUPO models multi-turn interaction problems as sequentially executed multi-agent bandit problems, where each turn is abstracted as virtual agent. The core mechanism of SeeUPO is turn-by-turn sequential policy updates in reverse execution order (T T1 1). This reverse-order sequential update mechanism enables backward induction, where each turn optimizes against the already-updated optimal policies of subsequent turns, thereby achieving global optimality. We prove that SeeUPO inherits monotonic improvement guarantees from the HAML framework (Zhong et al., 2024), and further establish that the reverse update order guarantees convergence to the globally optimal policy in multi-turn contextual bandit settings. Through implicit turn-level credit assignment via advantage function decomposition, SeeUPO correctly attributes each turns contribution to the global return, achieving stable and theoretically sound training in multi-turn scenarios. *To avoid confusion, in this paper we use PPO to refer to the complete algorithm in the LLM-RL-Training domain, and PPU to specifically refer to the proximal policy update method characterized by importance sampling and clipping mechanisms. 2 We evaluate SeeUPO on two multi-turn agentic benchmarks, AppWorld (Trivedi et al., 2024) and BFCL v4 (Patil et al., 2025), demonstrating substantial improvements over existing backbone RL algorithms. On Qwen3-14B, SeeUPO achieves 60.80% avg@4 and 72.85% pass@4, with relative improvements ranging from 43.3% to 54.6% compared to baseline methods. On Qwen2.5-14B, SeeUPO achieves 53.07% avg@4 and 63.59% pass@4, with relative improvements ranging from 24.1% to 41.9% over baselines. SeeUPO maintains stable training without the catastrophic failures observed in baseline methods. Additional comparative experiments validate our theoretical insights: the reverse update order achieves the best performance, confirming our proof that backward induction enables convergence to global optimality; batch-level normalization preserves the drift functional properties required for monotonic improvement guarantees while providing numerical stability. Contributions Our main contributions are summarized as follows: Theoretical Analysis of Backbone RL Algorithms. We provide comprehensive convergence analysis of mainstream backbone RL algorithms by categorizing them along two dimensions: advantage estimation (GAE vs. GRAE) and policy update mechanisms (REINFORCE vs. PPU). We prove the convergence conditions and limitations of each combination in both single-turn and multi-turn scenarios, revealing fundamental trade-off: existing algorithms cannot simultaneously achieve critic-free operation and convergence guarantees in multi-turn settings. Novel Algorithm with Theoretical Guarantees. We propose SeeUPO, sequence-level sequential update policy optimization algorithm that resolves the above trade-off. By modeling multi-turn interactions as sequentially-executed multi-agent bandit problems, SeeUPO inherits monotonic improvement guarantees and achieves convergence to global optimality through reverse-order sequential updates via backward induction. Comprehensive Experimental Validation. We evaluate SeeUPO on two challenging multiturn agentic benchmarks (AppWorld and BFCL v4), demonstrating substantial performance improvements (43.3%54.6% on Qwen3-14B, 24.1%41.9% on Qwen2.5-14B) and superior training stability compared to existing backbone RL algorithms. Organization The remainder of this paper is organized as follows. Section 2 introduces necessary preliminaries, including token / sequence-level RL modeling, advantage estimation methods, policy update mechanisms, and the convergence theory frameworks. Section 3 provides systematic convergence analysis of existing backbone RL algorithms, analyzing the properties of different advantage estimation and policy update combinations (Subsection 3.1) and summarizing the fundamental trade-off between critic-free operation and convergence guarantees in multi-turn scenarios (Subsection 3.2). Section 4 presents SeeUPO, including its theoretical framework(Subsection 4.1) and practical implementation details (Subsection 4.2). Section 5 reports experimental results. Section 6 concludes this paper."
        },
        {
            "title": "2 Preliminaries",
            "content": "2.1 Token-Level RL and Sequence-Level RL From modeling-level perspective, mainstream RL for LLMs can be roughly categorized into two types: token-level and sequence-level. In token-level RL, each token generation step corresponds to timestep in the MDP, where the state st represents the concatenation of the query and previous tokens, and the action at is the next token to be generated. In single-turn scenarios, the state transition is deterministic: st+1 = concat(st, at). In multi-turn interactive scenarios, the state transition becomes non-deterministic, as token-level RL models interactions by incorporating turn transitions and environmental feedback into the state through concatenation. In sequence-level RL, each timestep corresponds to generating complete sequence (sentence or turn), where the state represents the environmental state and the action is the complete response sequence. Under this formulation, single-turn task degenerates into contextual bandit model, while multi-turn tasks correspond to standard MDPs with true environmental state transitions (Zheng et al., 2025a; Wei et al., 2025). Sequence-level RL provides more natural abstraction for multi-turn interactions, as it directly models the environmental dynamics and enables more straightforward credit assignment across turns. Throughout this paper, we use plain symbols (e.g., s, a) to denote token-level elements and bold symbols (e.g., s, a) to denote sequence-level elements. In discussions of conventional RL methods, we use plain symbols. 3 2.2 GAE and GRAE Advantage estimation is key component of reinforcement learning. In traditional RL, GAE (Schulman et al., 2016) estimates advantages using temporal difference (TD) errors computed via value function network (critic). Specifically, for trajectory segment, GAE computes the advantage estimate as ˆAGAE(st, at) = l=0(γλ)lδt+l, where δt = rt + γVϕ(st+1) Vϕ(st) is the TD error, Vϕ is the value network, γ is the discount factor, and λ [0, 1] controls the bias-variance trade-off. Typically, GAE trades off some unbiasedness for lower estimation variance. The computational cost of training separate critic network in LLM-targeted RL has motivated the development of critic-free advantage estimation methods, with GRAE being representative approach. Given query (initial state) s0, GRAE samples independent responses (trajectories) {τ(i)}N i=1 from the i=1 R(i), where R(i) denotes the cumulative policy πθ and computes the group mean reward = 1 reward of trajectory τ(i). For (st, at) τ(i), GRAE assigns the advantage estimate as: ˆAGRAE(st, at) = R(i) R. For token-level RL, GRAE further broadcasts the advantage to each token. We emphasize that GRAE here represents general (ideal) formulation, and different practical RL algorithms may implement it differently. For instance, algorithms such as GRPO and GSPO further divide by the group variance, which has been shown to introduce bias (Hu et al., 2025), and we will show that this practice breaks the monotonic improvement property. For ease of discussion, we omit the leave-one-out operation in GRAE formulations (Ahmadian et al., 2024). 2.3 REINFORCE and PPU Policy update mechanisms are essential for translating advantage estimates into policy improvements (Sutton et al., 1998). We discuss two mainstream update mechanisms used in backbone RL algorithms, namely REINFORCE and PPU. REINFORCE (Sutton et al., 1998) is fully on-policy algorithm that directly optimizes the expected return. The policy gradient is estimated by sampling state-action pairs (st, at) from the current policy πθ: θ JREINFORCE(θ) = (st,at)πθ (cid:2)θ log πθ(atst) ˆA(st, at)(cid:3) , (1) where ˆA(st, at) is the estimated advantage. PPU typically refers to the mechanism used in PPO (Schulman et al., 2017), which allows for multiple updates per data batch (partially on-policy) by constraining the policy shift. PPU maximizes clipped surrogate objective estimated over samples collected by an old policy πθold : (cid:2)min (cid:0)rt(θ) ˆAt, clip(rt(θ), 1 ϵ, 1 + ϵ) ˆAt (cid:1)(cid:3) , (2) θ JPPU(θ) = θ (st,at)πθold where ˆAt denotes ˆA(st, at), rt(θ) = πθ (atst) clipping parameter. πθold (atst) is the importance sampling probability ratio and ϵ is the 2.4 Mirror Learning and Heterogeneous-Agent Mirror Learning Convergence guarantees are crucial for ensuring training stability and optimal policy discovery in reinforcement learning. Mirror Learning (Grudzien et al., 2022) provides unified theoretical framework for analyzing the convergence properties of policy optimization algorithms. The framework introduces key concepts: (1) the drift function Dπ( πs), which quantifies the update cost of new policy relative to the old policy, (2) the neighborhood operator , which defines the search space for policy updates, and (3) the mirror operator, which combines the advantage-related objective with the drift function to guide policy updates. When the drift function and neighborhood operator satisfy specific conditions, Mirror Learning guarantees monotonic policy improvement and convergence to optimal policies. This framework has been successfully applied to prove convergence guarantees for algorithms such as GPI (Sutton et al., 1998) and PPO. We leverage this work to help analyze the convergence properties of backbone RL algorithms used for training LLMs. Heterogeneous-Agent Mirror Learning (HAML) (Zhong et al., 2024) extends Mirror Learning to multiagent settings. The core insight is that by decomposing the joint advantage function into conditional advantages and performing sequential policy updates, policy improvements can be coordinated across agents, avoiding conflicts that could arise from simultaneous updates. Under HAML, algorithms guarantee monotonic improvement of the joint return and convergence to Nash equilibrium. We leverage HAML in our proposed SeeUPO algorithm to achieve convergence guarantees in multi-turn scenarios. For comprehensive introduction to the Mirror Learning and HAML framework, including detailed definitions and formal properties, we refer readers to Appendix A."
        },
        {
            "title": "3 Analysis",
            "content": "In this section, we provide systematic convergence analysis of mainstream backbone RL algorithms. We first present the algorithm comparison table and key findings, then analyze two fundamental components: advantage estimation methods (GAE and GRAE) and policy update mechanisms (REINFORCE and PPU), followed by analyzing their combined effects, and finally summarize the performance of existing algorithms in single-turn and multi-turn scenarios. Table 1 provides systematic comparison of mainstream backbone reinforcement learning algorithms used for language model training, analyzing their core components, modeling levels, and convergence properties. Our systematic analysis reveals fundamental trade-off: mainstream backbone RL algorithms struggle to simultaneously achieve both critic-free operation and convergence guarantees in multi-turn scenarios. Table 1: Systematic analysis of mainstream backbone RL algorithms for language model training. The table compares algorithms based on their advantage estimation methods, policy update mechanisms, modeling level, and convergence guarantees in single-turn and multi-turn scenarios. Detailed theoretical analysis is provided in the corresponding appendix sections. Advantage Policy Update Modeling Single-Turn Multi-Turn Estimation Mechanism Level Convergence Convergence Algorithm Instance Notes GAE GRAE PPU PPU Token-level Token-level GRAE REINFORCE Sequence-Level GRAE GRAE PPU HAML Sequence-Level Sequence-Level PPO (Schulman et al., 2017) REINFORCE++ (W/ BASELINE) (Hu et al., 2025), GRPO (Shao et al., 2024) RLOO (Ahmadian et al., 2024) GSPO (Zheng et al., 2025b) SEEUPO Critic-dependent; Assumes perfect value function approximation (Appendix C, F) Structural bias breaks monotonic improvement (Appendix D, G) Requires undiscounted settings (Appendix E) Requires removal of group-variance normalization (Appendix H) Converts multi-turn to sequential multi-agent single-turn (Appendix B) 3.1 Advantage Estimation and Policy Update Analysis We analyze two mainstream advantage estimation methods and their combinations with policy update mechanisms. Detailed theoretical analysis and proofs are provided in Appendix CH. GAE provides unbiased estimates under perfect value function approximation, with bias bounded by Bias 1+γ2γλ ϵmax where ϵmax is the maximum value estimation error (Appendix C). GRAE is biased but provides unbiased gradient estimates under undiscounted (γ = 1) settings; when the conditions are violated, both the estimator and gradient become biased (Appendix D). 1γλ We analyze how different combinations affect convergence, summarized in Table 1: GRAE-REINFORCE (e.g., RLOO): Guarantees gradient unbiasedness, monotonic improvement, and convergence under undiscounted objective (γ = 1) with bounded rewards in finite-horizon MDPs. REINFORCE can be viewed as an instance of Mirror Learning (Grudzien et al., 2022) with trivial drift (D 0) and trivial neighbourhood (N = Π), which ensures the convergence guarantee (Appendix E). GAE-PPU (e.g., PPO): Guarantees monotonic improvement and convergence to globally optimal policy under perfect value function approximation (Appendix F). GRAE-PPU (e.g., GRPO, REINFORCE++ w/ Baseline (Hu et al., 2025), GSPO): Does not guarantee monotonic improvement or convergence in general, because GRAEs structural bias (st) = V(st) V(s0) cannot be eliminated in PPUs clipped objective (Appendix G). Exception: In Contextual Bandit settings (single-turn & sequence-level), GRAE becomes unbiased and GRAE-PPU converges; however, group-variance normalization (as in GSPO) breaks this guarantee (Appendix H). GAE-REINFORCE (no existing algorithm instance): This combination is Pareto-dominated by existing alternatives. It retains GAEs critic dependency while lacking PPUs trust region constraints that bound policy updates. When value function approximation is imperfect, biased GAE estimates can be amplified through REINFORCEs unbounded updates. In contrast, GAE-PPU achieves better sample efficiency with bounded updates, while GRAE-REINFORCE eliminates critic dependency entirely. 3.2 Summary: The Fundamental Trade-off Based on the analysis above, we summarize the convergence properties. In single-turn scenarios: RLOO converges under undiscounted settings; PPO converges under perfect value function approximation; GSPO (without group-variance normalization) converges in Contextual Bandit settings. In multi-turn scenarios, our analysis reveals fundamental trade-off: mainstream backbone RL algorithms struggle to simultaneously achieve both critic-free operation and convergence guarantees. Critic-dependent methods (GAE-PPU): Require accurate token-level value function estimation, which becomes challenging in multi-turn scenarios due to non-stationary state transitions, with additional computational overhead from maintaining the critic network (Zhang et al., 2026). Critic-free methods (GRAE-based): GRAE-PPU breaks monotonic improvement due to structural bias in PPUs clipped objective. GRAE-REINFORCE requires undiscounted settings that are difficult to satisfy in multi-turn scenarios. Moreover, in sequence-level settings, GRAE becomes biased in multi-turn scenarios due to credit assignment problems and amplified structural bias (st) = V(st) V(s0). This fundamental limitation motivates new algorithmic framework that achieves both critic-free operation and convergence guarantees in multi-turn scenarios, which we address in Section 4."
        },
        {
            "title": "4 Method",
            "content": "To address this limitation, we propose new algorithmic framework: Sequence-level Sequential Update Policy Optimization (SeeUPO). The core mechanism of SeeUPO is to model multi-turn interaction problems as sequentially-executed multi-agent contextual bandit problems, where each turn is abstracted as virtual agent, and the order of turns corresponds to the execution order among agents. Fundamental Principles of SeeUPO This modeling approach is built upon two fundamental principles: 1. Transforming convergence analysis at the turn-level into convergence analysis at the agent-level, which allows leveraging existing multi-agent reinforcement learning (MARL) theories to solve the problem. 2. Transforming multi-timestep MDP problems into bandit problems, which enables unbiased advantage estimation without requiring value function estimation. 4.1 Theoretical Framework: Sequence-level Sequential Update Policy Optimization Multi-Agent Modeling SeeUPO abstracts multi-turn interaction tasks into sequentially-decision multiagent single-turn bandit problems, where each turn is mapped to virtual agent {1, 2, . . . , T} (as depicted in Figure 2). All agents share common global state s0 SS (the initial task state). The sequencelevel action at AS of agent corresponds to the complete response of the t-th turn. The joint action a1:T = (a1, a2, . . . , aT) denotes the concatenation of all agents actions. Each agent ts sequence-level policy πt(ats0, a1:t1) takes as input the global state s0 and the action history from preceding agents a1:t1. The state transition function is implicitly modeled through sequentially executed policies: the evolution of the interaction is determined by agent ts action selected according to policy πt(s0, a1:t1). We adopt shared reward (Team-Reward) mechanism r(s0, a1:T), where the team reward equals the final task reward or cumulative return across all turns, ensuring all agents jointly optimize the global objective. We emphasize that this multi-agent modeling only exhibits training-phase specificity and offers no inherent advantages for execution optimization. This abstraction constitutes methodological shift in data treatment during the training phase, but does not correspond to functional mechanism for multi-agent coordination during actual execution. Policy Update After modeling is completed, the optimization of Sequence-level policies is transformed into optimizing the joint policy of multi-agent system. SeeUPO adopts the sequential update mechanism from the HAML framework (see Appendix A), with crucial design choice: the update order is set to the reverse of the execution order (T T1 1). This reverse update order not Throughout this section, we use π to denote policy to be optimized, and ˆπ to denote the joint policy. Figure 2: The core idea of SeeUPO is to abstract multi-turn interaction tasks into sequentially-decision multi-agent single-turn tasks, and adopt reverse-order sequential updates to achieve global optimality via backward induction. The figure shows an example scenario with three turns, from left to right showing the original task scenario, the multi-agent modeling of the scenario, and the reverse update mechanism based on MARL theory. only resolves agent-level update conflicts by updating policies turn by turn, but also enables backward induction to achieve global optimality (see Theorem 2 in Appendix B). At each iteration k, the algorithm updates each agents policy sequentially following the reverse order of execution (T T1 1). The policy update process consists of three key components: (1) Policy Update Rule. For turn in the reverse update order, by substituting the HAML into the update rule, the policy update can be expressed as: ˆπt k+1 = arg max πtU ( ˆπt k) ˆπk s0β ˆπk (cid:34) at+1:T ˆπt+1:T k+1 ,at πt (cid:104) At ˆπk (s0, at, at+1:T) (cid:105) Dt ˆπk ( πt s0, ˆπt+1:T k+1 ) (cid:35) , (3) where s0 is the sequence-level joint state (i.e., the global initial state), ˆπk denotes the joint policy at ( ˆπt iteration k, k) is the neighborhood operator for turn t, β ˆπk is the sampling state distribution, ˆπk (cid:104) is the expectation of the local advantage function for turn t, and Dt is the ˆπk at+1:T ˆπt+1:T At ˆπk (cid:105) k+1 ,at πt drift functional. Crucially, under the reverse update order, each turn considers the already-updated policies ˆπt+1:T k+1 of subsequent turns, ensuring coordination in the update process and enabling backward induction. (2) Local Advantage Function Computation. To compute the expectation of the local advantage function , SeeUPO leverages the global advantage function. Given the global advantage (cid:105) (cid:104) at+1:T ˆπt+1:T function ˆA ˆπk k+1 ,at πt At ˆπk (s0, a1:T), this expectation can be estimated: at+1:T ˆπt+1:T k+1 ,at πt (cid:34)(cid:32) = a1:T ˆπk (cid:105) (cid:104) (s0, at, at+1:T) (cid:33) At ˆπk πt(ats0, a1:t1) k(ats0, a1:t1) ˆπt 1 ˆπt+1:T k+1 ˆπt+1:T (at+1:Ts0, a1:t) (at+1:Ts0, a1:t) (cid:35) ˆA ˆπk (s0, a1:T) , (4) where the first term (cid:18) πt(ats0,a1:t1) k(ats0,a1:t1) ˆπt (cid:19) 1 involves the candidate policy πt (to be optimized) and the current policy ˆπt subsequent turns ˆπt+1:T . Note that the 1 term in the first factor has zero gradient with respect to πt and can be omitted in practical gradient computation. The computation (at+1:T s0,a1:t) (at+1:T s0,a1:t) and the previous policy ˆπt+1:T involves the already-updated joint policy of k, while the second ratio ˆπt+1:T k+1 ˆπt+1:T k+ 7 of the local advantage function effectively performs implicit credit assignment across turns (Zhong et al., 2024), as it decomposes the global advantage into turn-specific contributions by incorporating the importance sampling ratios from subsequently updated turns. (3) Global Advantage Function Computation. In the bandit setting, the global advantage function ˆA ˆπk (s0, a1:T) can be estimated directly from sampled rewards. Specifically, for given initial state s0 and sequence-level joint action a1:T, the global advantage function degenerates to: ˆA ˆπk (s0, a1:T) = r(s0, a1:T) a1:T ˆπk(s0)[r(s0, a1:T)], (5) a1:T ˆπk(s0)[r(s0, a1:T)] is the expected reward under the where r(s0, a1:T) is the immediate reward and current policy. This formulation provides an unbiased estimate of the advantage function in the bandit setting. Theoretical Guarantees SeeUPO inherits the monotonic improvement guarantee from the HAML framework (Theorem 1 in Appendix A). Beyond this, we establish stronger result for the multi-turn contextual bandit setting: the reverse update order guarantees convergence to the globally optimal policy (Theorem 2 in Appendix B). The key insight is that, unlike general cooperative games where random update orders are required for Nash equilibrium convergence, the fixed sequential execution order in our setting enable backward induction. Specifically, the reverse update order ensures that when updating turn t, all subsequent turns + 1, . . . , have already been updated to their optimal policies given their continuation values. This allows each turn to optimize against the true optimal continuation value V, yielding global optimality. The complete proof is provided in Appendix B. 4.2 Practical Methods Algorithm 1: SeeUPPO-GRAE Input: Initial sequence-level joint policy π0 with parameters θ0, maximum turns T, batch size B, group size G, clipping parameter ϵ, learning rate α Output: Optimized policy πK after iterations 1. Initialize π0 with parameters θ0 2. For = 0, 1, . . . , 1: (a) Data Collection: Sample dataset Dk = {(s0, a1:T, r)} by: For each of initial states s0, sample trajectories a1:T and collect rewards Organize data into sample pools: for each turn {1, . . . , T}, construct pool r(s0, a1:T) Dt = {(s0, a1:t1, at)} (b) Joint Advantage Estimation: For each (s0, a1:T) DT: ˆA ˆπk Compute joint advantage: Initialize MT+1(s0, a1:T) = ˆA ˆπk (c) Sequential Policy Update (Reverse Order): (s0, a1:T) = r(s0, a1:T) r(s0) (s0, a1:T) For = T, T1, . . . , 1: Update policy parameters to obtain πt If > 1: * Update Mt for next batch via Equation. 8 Else: * Set θk+1 = θ1 θk+ k+1 via Equation. 6: 3. Return πK In this part, we present concrete example of SeeUPO (Algorithm 1 presents the pseudocode). The practical implementation instantiates the theoretical framework in two key aspects: (1) adopting PPOstyle clipping mechanism to implement the mirror operator through gradient-based updates, and (2) using GRAE for joint advantage estimation. This approach essentially combines GRAE with HAPPO 8 (Zhong et al., 2024). We refer to this practical algorithm as SeeUPPO-GRAE, though for convenience, we still refer to it as SeeUPO in the remainder of this paper. We emphasize that SeeUPPO-GRAE is not the only instantiation of SeeUPOthe theoretical framework admits various variants by substituting different components, such as replacing the PPO-style clipping with TRPO-style trust region constraints, or replacing GRAE with other advantage estimators. The algorithm operates iteratively, with each iteration comprising data collection, advantage estimation, and sequential policy updates. SeeUPO adopts turn-oriented batch construction approach that separately organizes samples from identical turns, as illustrated in Figure 3. This approach enables sequential policy updates by maintaining turn-level sample pools, in contrast to methods that construct batches using entire trajectories or concatenated sliced turns. For tasks with fewer than the maximum turns, placeholder samples (e.g., Sample 6 in the figure) are introduced as no-op (null action) samples. Note that the figure demonstrates batch construction patterns using the React + Reasoning-Augmented Template paradigm (Zhai et al., 2025). Figure 3: The batch construction approach of SeeUPO. Unlike methods that construct batches using entire trajectories or by concatenating sliced turns, SeeUPO implements turn-oriented approach that separately organizes samples from identical turns. This figure demonstrates the divergent batch construction patterns between SeeUPO and the Vanilla approach under two tasks with maximum three-turn interactions, via React + Reasoning-Augmented Template paradigm (Zhai et al., 2025). PPO-Style Policy Update In our multi-turn RL setting, all turns share the same policy parameters θ. We to denote the policy after updating turn ts data in iteration k. For notational clarity, we denote use πθt (s0, a1:T) as joint trajectory sample, where s0 is the initial state (query) and a1:T is the sequence-level joint action as defined in Section 4.1. k+1 Specifically, for turn in the reverse update order (T T1 1) at iteration k, the policy update is performed to obtain πθt by computing the gradient of the policy parameters θ with respect to the following expectation: k+ θ (s0,a1:t1,at)Dt (cid:104) min (cid:16) rt(θ)Mt+1(s0, a1:T), clip(rt(θ), 1 ϵ)Mt+1(s0, a1:T) (cid:17)(cid:105) , (6) where Dt = {(s0, a1:t1, at)} is the turn-specific sample pool constructed during data collection (see Algorithm 1), containing samples organized by turn t. The sequence-level importance sampling ratio rt(θ) = πθ(ats0,a1:t1) (ats0,a1:t1) for turn is computed in manner similar to GSPO, where at denotes the πθk action at turn and (s0, a1:t1) is the conditioning context (initial state and previous actions). ϵ is the clipping parameter, and Mt+1(s0, a1:T) is maintained quantity that captures the sequential advantage information from subsequently updated turns. The quantity Mt(s0, a1:T) is initialized and updated sequentially to incorporate the importance sampling ratios from previously updated turns. Specifically, we initialize: 9 MT+1(s0, a1:T) = ˆA ˆπk (s0, a1:T), (7) where ˆA ˆπk updating turn t, Mt(s0, a1:T) is computed recursively: (s0, a1:T) is the global advantage estimate (computed via GRAE as described below). After Mt(s0, a1:T) = πθt k+1 πθk (ats0, a1:t1) (ats0, a1:t1) Mt+1(s0, a1:T), (8) where θt k+1 denotes the parameters after optimizing turn t, and θk denotes the parameters at the beginning of iteration (i.e., the reference policy used for sampling). Due to parameter sharing, this sequential update mechanism ensures that Mt(s0, a1:T) incorporates the importance sampling ratios from all previously updated turns, matching the expectation structure in Equation 4. GRAE-based Advantage Estimation In the bandit setting, the global advantage function ˆA ˆπk (s0, a1:T) can be estimated directly from sampled rewards, as established in Equation 5. For each initial state in the batch, SeeUPO samples different joint actions and collects the corresponding Team-Rewards. The global advantage estimate is computed as: ˆA ˆπk (s0, a1:T) = r(s0, a1:T) r(s0), (9) where r(s0) is the mean reward over trajectories sampled from the same initial state s0, serving as a1:T ˆπk(s0)[r(s0, a1:T)]. This approach provides an unbiased estiMonte Carlo estimator of ˆπk mate of the advantage function in the bandit setting, without requiring separate critic (see Appendix for detailed analysis). (s0) = In practice, we apply batch-level normalization to the advantage estimates for numerical stability. = ( ˆA µB)/σB, where µB is the Specifically, we normalize all advantage estimates in batch as: batch mean and σB is the batch standard deviation. This normalization approach maintains theoretical convergence guarantees while improving training stability: since µB and σB are constants independent of the candidate policy, the argmax of the optimization problem remains unchanged, leaving the drift functional completely unaffected (see Appendix H.3.3 for detailed analysis). This is in contrast to group normalization which applies state-dependent scaling factors that can violate these properties (see Appendix H.3). Moreover, experimental results in Section 5.3.2 demonstrate that batch-level normalization performs comparably to group normalization and no normalization, while preserving the theoretical convergence properties."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we conduct series of comprehensive experiments to systematically evaluate the performance of our proposed SeeUPO framework. We begin by detailing the experimental settings (Section 5.1), followed by presentation of the main results that demonstrate the superior performance of SeeUPO compared to existing selected backbone RL algorithms (Section 5.2). Finally, we perform additional comparative experiments to investigate the impact of update order and advantage normalization strategies on overall training performance (Section 5.3). 5.1 Experimental Settings 5.1.1 Benchmarks and Evaluations We evaluate SeeUPO on two tool-augmented, agentic benchmarks: AppWorld (Trivedi et al., 2024) and BFCL v4 (Patil et al., 2025). Both benchmarks expose multi-step API/tool interactions under sparse terminal rewards, making them ideal testbeds for evaluating multi-turn agent training algorithms. AppWorld: controllable world of apps and people for benchmarking interactive coding agents. AppWorld exposes multi-step API interactions where agents must navigate complex application workflows to accomplish user-specified goals. The environment provides programmatic evaluation through task goal completion tests. BFCL v4: The Berkeley Function Calling Leaderboard multi-turn benchmark. We use the multiturn split and follow the official evaluation protocol: at the end of each turn, an example is 10 marked correct only if it simultaneously passes state-based checks (final backend state matches ground truth on non-private attributes) and response-based checks (subset-matched execution path); force-terminated runs are counted as incorrect. For evaluation metrics, we report avg@4 (averaging task completion scores over 4 independent rollouts per instance) and pass@4 (the success rate when sampling 4 independent rollouts per instance). Trajectories are truncated at maximum of 10 steps to ensure computational efficiency while maintaining sufficient horizon for multi-turn interactions. 5.1.2 Baselines and Backbone Models Our experiments leverage two backbone models: Qwen2.5-14B-Instruct (Yang et al., 2025b) and Qwen314B (Yang et al., 2025a). These models serve as the agents policy network. To rigorously evaluate the efficacy of our method, we compare against three mainstream backbone RL algorithms for agent training: PPO: representative critic-dependent algorithm with theoretical convergence guarantees in multi-turn scenarios. GRPO: representative critic-free algorithm at the token level, which employs group-relative advantage estimation and PPO-style update mechanism. GSPO: representative critic-free algorithm at the sequence level, which extends GRPO with sequence-level mechanisms. 5.1.3 Implementation Details To ensure fair comparison, all baselines and our SeeUPO use the same training configuration. We train our agent policies using SeeUPO as described in Section 4. The general training configuration is as follows: Training hyperparameters: We use learning rate of 1 106 for the actor network and batch size of 32. The clipping parameter ϵ in the PPO-style objective is all set to 0.2 for both lower and upper bounds. The KL penalty coefficient is 0.002. We sample 8 rollouts per instance during training. We train for 75 epochs on AppWorld and 50 epochs on BFCL v4. To ensure fairness, all algorithms use the same number of samples, and samples are utilized the same number of times within each update step. All experiments on both base models adopt the React + Reasoning-Augmented Template paradigm (Zhai et al., 2025). SeeUPO-specific settings: For the sequential update mechanism, we use reverse order for both AppWorld and BFCL v4 benchmarks, as our subsequent experiments demonstrate that reverse order achieves the best performance. The advantage estimation follows group-relative approach with batch-level normalization, where we normalize advantages by subtracting the batch mean and dividing by the batch standard deviation. Infrastructure: All experiments were conducted on clusters of NVIDIA H20 (96GB) GPUs, where each cluster consists of 8 GPUs. For computational resource allocation, PPO uses 2 clusters (16 GPUs total), while GRPO, GSPO, and SeeUPO each use 1 cluster (8 GPUs total). Our implementation is built on PyTorch and leverages the veRL library for distributed training infrastructure. The agent-related infrastructure is built upon AgentEvolver (Zhai et al., 2025). 5.2 Main Results We conduct comprehensive experiments to evaluate the overall performance of the proposed SeeUPO framework and compare it against selected backbone RL algorithms. As shown in Table 2 and Figure 4, SeeUPO demonstrates substantial performance improvements over existing backbone RL algorithms across both benchmarks and models, with the training dynamics revealing several critical insights into SeeUPOs superior performance and robustness. 5.2.1 Performance and Training Dynamics Analysis Training Stability: Across all four scenarios, SeeUPO maintains stable training curves without catastrophic failures. In contrast, both GRPO and GSPO exhibit significant performance collapse in the Qwen-2.5 + AppWorld setting (subplot (c)). This catastrophic failure reflects fundamental limitation https://github.com/volcengine/verl https://github.com/modelscope/AgentEvolver 11 Table 2: Performance comparison on two benchmark environments. Columns show avg@4 and pass@4 for each benchmark, plus their averages (Avg.). All values are in percent (%). Bolded numbers highlight the best results. Improvement percentages (shown as blue superscripts) indicate SeeUPOs relative improvement over each baseline. Model & Method AppWorld BFCL v4 Avg. Qwen2.5-14B +PPO +GRPO +GSPO SeeUPO (ours) Qwen3-14B +PPO +GRPO +GSPO SeeUPO (ours) avg@ pass@4 avg@4 pass@4 avg@4 pass@4 4.825 40.79+24.7% 35.53+43.3% 24.12+111.0% 50. 20.18 35.09+81.2% 40.35+57.6% 32.89+93.4% 63.60 7.018 57.89+21.2% 49.12+42.9% 38.60+81.8% 70.18 35.09 54.39+48.4% 57.89+39.4% 52.63+53.3% 80.70 25.50 44.75+23.5% 46.25+19.5% 50.75+8.9% 55.25 40.25 43.75+32.6% 44.50+30.3% 45.75+26.8% 58.00 35.75 56.00+1.8% 55.00+3.6% 56.00+1.8% 57. 47.00 52.00+25.0% 53.00+22.6% 55.00+18.2% 65.00 15.16 42.77+24.1% 40.89+29.8% 37.44+41.9% 53.07 30.22 39.42+54.3% 42.43+43.3% 39.32+54.6% 60.80 21.39 56.95+11.7% 52.06+22.1% 47.30+34.4% 63.59 41.05 53.20+36.9% 55.45+31.4% 53.82+35.4% 72.85 Figure 4: Training success rate comparison of SeeUPO and baselines. Subplots (a)-(d) show results for Qwen-3 model on Appworld and BFCL, and Qwen-2.5 model on Appworld and BFCL, respectively. of existing critic-free methods in multi-turn scenarios: they suffer from advantage estimation bias when applied to multi-turn tasks, and lack monotonic improvement guarantees. In contrast, SeeUPO addresses these issues through its theoretically-grounded sequential update mechanism, which provides monotonic improvement guarantees inherited from the HAML framework (Theorem 1 in Appendix A) and enables implicit turn-level credit assignment through advantage function decomposition. Final Performance: The training curves consistently show SeeUPO achieving the highest final success rates across all configurations. The performance gaps are particularly pronounced in the Qwen-3 scenarios, where SeeUPO maintains clear 20-30 percentage point advantage in pass@4 over baselines throughout training. As shown in Table 2, SeeUPO achieves substantial improvements on Qwen3-14B, model with built-in reasoning capabilities, with avg@4 improvements ranging from 43.3% to 54.6% across different baselines. On the weaker Qwen2.5-14B model, SeeUPO still demonstrates significant gains, with avg@4 improvements ranging from 24.1% to 41.9%. These substantial gains validate that training stability translates directly to superior generalization, consistent with the theoretical guarantee that the reverse-order sequential update mechanism enables convergence to globally optimal policies through backward induction (Theorem 2 in Appendix B). 5.2.2 Computational Efficiency Analysis SeeUPO introduces additional computational overhead compared to baseline methods due to its turnoriented sequential update mechanism and advantage correction term computation. Therefore, it is necessary to analyze its computational efficiency. We evaluate two key efficiency metrics: Training Step Time (time per training step) and Computational Resources (number of H20 GPUs required). All 12 experiments were conducted on clusters of NVIDIA H20 (96GB) GPUs, with each cluster consisting of 8 GPUs. The efficiency results are presented in Table 3. Table 3: Computational efficiency comparison across methods. Columns show Training Step Time (seconds per training step) for each benchmark and GPUs (number of H20 GPUs required). Multiplier factors (shown as red superscripts) indicate SeeUPOs computation time relative to each baseline method. Method AppWorld (s) BFCL v4 (s) Avg. (s) GPUs Qwen2.5-14B PPO GRPO GSPO SeeUPO (ours) Qwen3-14B PPO GRPO GSPO SeeUPO (ours) 3026.741.39 2377.591.76 2263.561.85 4194.70 2602.881.38 2739.731.31 2466.141.46 3595. 2814.811.38 2558.661.52 2364.851.65 3894.89 3002.291.82 2938.371.86 3066.941.78 5462.35 2579.041.22 2667.631.18 2710.481.16 3145.06 2790.671.54 2802.991.54 2888.711.49 4303.71 16 8 8 8 16 8 8 As shown in Table 3, SeeUPO demonstrates significantly superior performance compared to other algorithms, but its turn-by-turn sequential update mechanism inevitably incurs longer training time. We find that this computational overhead is within an acceptable range: with approximately 1.5 the training time, SeeUPO achieves notably faster convergence speed and reaches final performance levels far exceeding those of other algorithms. Moreover, in terms of computational resource consumption, SeeUPO uses the same amount of resources as the two critic-free algorithms (GRPO and GSPO), requiring only 8 GPUs (1 cluster) compared to PPOs 16 GPUs (2 clusters). 5.3 Additional Comparative Experiments To investigate the impact of different design choices on overall training performance, we conduct additional comparative experiments focusing on two critical aspects of SeeUPO: the update order, which corresponds to our novel sequential update mechanism that enables backward induction for global optimality (Section 4.1), and the normalization strategy, which corresponds to our bandit-based advantage estimation mechanism (Section 4.2). All comparative experiments are conducted using Qwen3-14B on both AppWorld and BFCL v4 benchmarks. 5.3.1 Comparison on Update Order The sequential update mechanism is core component of SeeUPO (Section 4.1). As discussed in Section 4.1, the reverse update order is theoretically motivated by backward induction, which enables convergence to globally optimal policies (Theorem 2 in Appendix B). However, the specific order in which agents are updated may influence training dynamics and final performance in practice. To empirically validate this theoretical insight, we compare three update order strategies: Natural order: Agents are updated in the natural execution order (turn 1, turn 2, ..., turn T). Reverse order: Agents are updated in reverse execution order (turn T, turn 1, ..., turn 1) (our default setting). Random order: Agents are updated in randomly sampled permutation at each iteration. Figure 5: Training dynamics comparison of different update order strategies: (a) Qwen-3 evaluated on AppWorld and (b) Qwen-3 evaluated on BFCL-v4. 13 Table 4: Comparison of update order strategies. Columns show avg@4 and pass@4 for each benchmark, plus their averages (Avg.). All values are in percent (%). The backbone model is Qwen3-14B. Bolded numbers highlight the best results. Update Order AppWorld BFCL v4 Avg. avg@4 pass@4 avg@ pass@4 avg@4 pass@4 Natural order Random order Reverse order (ours) 56.14 33.33 63.60 75.44 45.61 80. 54.25 51.25 58.00 59.00 61.00 65.00 55.20 42.29 60.80 67.22 53.31 72.85 The results are presented in Table 4 and Figure 5. As expected, the reverse order strategy achieves the best performance across both benchmarks, confirming the theoretical insight that updating agents in reverse execution order enables backward induction to achieve global optimality. On AppWorld, reverse order achieves 63.60% avg@4 and 80.70% pass@4, outperforming natural order (56.14% / 75.44%) and random order (33.33% / 45.61%). On BFCL v4, reverse order achieves 58.00% avg@4 and 65.00% pass@4, outperforming natural order (54.25% / 59.00%) and random order (51.25% / 61.00%). Natural order achieves competitive performance, as it still maintains the monotonic improvement guarantee inherited from the HAML framework. However, each update in natural order only achieves local optimality at that moment, preventing direct optimization toward the global optimum. In contrast, random order performs substantially worse, particularly on AppWorld. Unlike natural and reverse orders, which maintain fixed update logic that respects the sequential execution structure, random order completely disrupts the logical chain inherent to the execution sequence. While random update orders have been proven effective in synchronous MARL settings (Zhong et al., 2024), they fail in sequential multi-agent systems where the update order must be strongly coupled with the execution order. Specifically, random ordering breaks the backward induction mechanism by preventing the guarantee that subsequent turns have been updated to their optimal policies when updating an earlier turn, thereby destroying the logical dependency structure required for global optimality. 5.3.2 Comparison on Normalization The advantage estimation in SeeUPO uses group-relative approach. While without any subsequent normalization is theoretically grounded, different normalization strategies may impact training stability and performance. We investigate three normalization variants: No normalization: Use raw advantage estimates without any normalization, preserving drift function properties required for convergence guarantees. Group-level normalization: Normalize advantages within each group of joint actions sampled from the same initial state, which violates convergence guarantees similar to GSPO. Batch-level normalization: Normalize advantages by subtracting the batch mean and dividing by the batch standard deviation (our default setting), balancing theoretical correctness and empirical performance. Table 5: Comparison of normalization strategies. Columns show avg@4 and pass@4 for each benchmark, plus their averages (Avg.). All values are in percent (%). The backbone model is Qwen3-14B. Bolded numbers highlight the best results. Normalization AppWorld BFCL v4 Avg. avg@ pass@4 avg@4 pass@4 avg@4 pass@4 No normalization Group-level normalization Batch-level normalization (ours) 39.91 62.35 63.60 57.89 79.21 80.70 54.25 58.20 58.00 59.00 61.00 65.00 47.08 60.28 60.80 58.45 70.10 72. The results are presented in Table 5. No normalization is theoretically grounded, as it preserves the drift function properties required for convergence guarantees. However, it achieves the lowest empirical performance (39.91% avg@4 and 57.89% pass@4 on AppWorld; 54.25% avg@4 and 59.00% pass@4 on BFCL v4), indicating that some form of normalization is essential for numerical stability during training. Group normalization achieves competitive test performance (62.35% avg@4 and 79.21% pass@4 on AppWorld; 58.20% avg@4 and 61.00% pass@4 on BFCL v4) but violates convergence guarantees similar to GSPO. To balance theoretical correctness and numerical stability, we use batch-level normalization, which achieves the best overall performance (63.60% avg@4 and 80.70% pass@4 on AppWorld; 58.00% 14 avg@4 and 65.00% pass@4 on BFCL v4) while maintaining convergence properties. The group-relative mean subtraction (used in our advantage estimation) provides sufficient variance reduction, and batch normalization further stabilizes training without breaking theoretical guarantees."
        },
        {
            "title": "6 Conclusions",
            "content": "In this work, we provide systematic theoretical analysis of mainstream backbone RL algorithms for LLM training and propose SeeUPO, novel critic-free algorithm with convergence guarantees in multi-turn scenarios. Specifically, we categorize existing algorithms along two dimensionsadvantage estimation (GAE vs. GRAE) and policy update mechanisms (REINFORCE vs. PPU)and analyze their convergence properties in both single-turn and multi-turn scenarios. Our analysis reveals fundamental trade-off: mainstream algorithms cannot simultaneously achieve both critic-free operation and convergence guarantees in multi-turn settings. To address this limitation, we propose SeeUPO, which models multi-turn interactions as sequentially-executed multi-agent bandit problems and employs reverse-order sequential policy updates. SeeUPO inherits monotonic improvement guarantees from the HAML framework and achieves convergence to global optimality through backward induction. Experimental results on AppWorld and BFCL v4 demonstrate SeeUPOs substantial improvements: relative gains of 43.3%54.6% on Qwen3-14B and 24.1%41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability that avoids the catastrophic failures. Limitations and Future Work For limitations, the HAML framework theoretically requires heterogeneous policies across agents, which in traditional RL corresponds to non-shared network parameters. In the LLM context, we argue that the sufficiently large parameter space provides ample representational capacity for different turns or roles to develop functionally distinct behaviors, making the policies non-homogeneous from turn-level perspective despite parameter sharing. This assumption becomes increasingly justified with larger-scale architectures such as Mixture-of-Experts (MoE) models, where different experts can specialize for different turns. Future work could explore explicit turn-specific parameterization to better satisfy the heterogeneity assumption. Whats more, our discussion of token-level and sequence-level RL modeling is grounded in the current mainstream paradigm of next-token prediction for LLMs. While SeeUPO is designed within this framework, the sequence-level RL approach is essentially meta turn-level modeling and optimization method. This approach may have potential applications in alternative paradigms such as multi-token/sentence prediction (Barrault et al., 2024) or rectified flow diffusion architectures (Zhang et al., 2025), which could be explored in future research."
        },
        {
            "title": "References",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ust un, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Lawrence Ausubel and Raymond Deneckere. generalized theorem of the maximum. Economic Theory, 3(1):99107, 1993. Loıc Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Belen Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta Costa-juss`a, David Dale, et al. Large concept models: Language modeling in sentence representation space. arXiv preprint arXiv:2412.08821, 2024. Jiajun Chai, Guojun Yin, Zekun Xu, Chuhuai Yue, Yi Jia, Siyu Xia, Xiaohan Wang, Jiwen Jiang, Xiaoguang Li, Chengqi Dong, et al. Rlfactory: plug-and-play reinforcement learning post-training framework for llm multi-turn tool-use. arXiv preprint arXiv:2509.06980, 2025. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. Jakub Grudzien, Christian Schroeder De Witt, and Jakob Foerster. Mirror learning: unifying framework of policy optimisation. In International Conference on Machine Learning, pp. 78257844. PMLR, 2022. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. 15 Jian Hu, Jason Klein Liu, Haotian Xu, and Wei Shen. Reinforce++: Stabilizing critic-free policy optimization with global advantage normalization. arXiv preprint arXiv:2501.03262, 2025. Daniel Jiang, Jalaj Bhandari, Yukai Yang, Remi Munos, and Tyler Lu. Aligning llms toward multi-turn conversational outcomes using iterative ppo. arXiv preprint arXiv:2511.21638, 2025. Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Shishir Patil, Huanzhi Mao, Fanjia Yan, Charlie Cheng-Jie Ji, Vishnu Suresh, Ion Stoica, and Joseph Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. Zhangyang Qi, Zhixiong Zhang, Yizhou Yu, Jiaqi Wang, and Hengshuang Zhao. Vln-r1: Vision-language navigation via reinforcement fine-tuning. arXiv preprint arXiv:2506.17221, 2025. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-T ur, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2016. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Saksham Sahai Srivastava and Vaneet Aggarwal. technical survey of reinforcement learning techniques for large language models. arXiv preprint arXiv:2507.04136, 2025. Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, and Niranjan Balasubramanian. Appworld: controllable world of apps and people for benchmarking interactive coding agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1602216076. Association for Computational Linguistics, 2024. doi: 10.18653/v1/2024.acl-long.850. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025. Quan Wei, Siliang Zeng, Chenliang Li, William Brown, Oana Frunza, Wei Deng, Anderson Schneider, Yuriy Nevmyvaka, Yang Katie Zhao, Alfredo Garcia, et al. Reinforcing multi-turn reasoning in llm agents via turn-level reward design. arXiv preprint, 2025. Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, et al. Agentgym-rl: Training llm agents for long-horizon decision making through multi-turn reinforcement learning. arXiv preprint arXiv:2509.08755, 2025. Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Kristian Kersting, Jeff Pan, Hinrich Sch utze, et al. Memory-r1: Enhancing large language model agents to manage and utilize memories via reinforcement learning. arXiv preprint arXiv:2508.19828, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2025b. Yunpeng Zhai, Shuchang Tao, Cheng Chen, Anni Zou, Ziqian Chen, Qingxu Fu, Shinji Mai, Li Yu, Jiaji Deng, Zouying Cao, et al. Agentevolver: Towards efficient self-evolving agent system. arXiv preprint arXiv:2511.10395, 2025. Guibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhong-Zhi Li, Xiangyuan Xue, Yijiang Li, et al. The landscape of agentic reinforcement learning for llms: survey. Transactions on Machine Learning Research, 2026. Lingzhe Zhang, Liancheng Fang, Chiming Duan, Minghua He, Leyi Pan, Pei Xiao, Shiyu Huang, Yunpeng Zhai, Xuming Hu, Philip Yu, et al. survey on parallel text generation: From parallel decoding to diffusion language models. arXiv preprint arXiv:2508.08712, 2025. Chujie Zheng, Kai Dang, Bowen Yu, Mingze Li, Huiqiang Jiang, Junrong Lin, Yuqiong Liu, Hao Lin, Chencan Wu, Feng Hu, et al. Stabilizing reinforcement learning with llms: Formulation and practices. arXiv preprint arXiv:2512.01374, 2025a. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025b. Yifan Zhong, Jakub Grudzien Kuba, Xidong Feng, Siyi Hu, Jiaming Ji, and Yaodong Yang. Heterogeneousagent reinforcement learning. Journal of Machine Learning Research, 25(1-67):1, 2024. Mirror Learning and Multi-Agent Mirror Learning This section provides comprehensive introduction to the Mirror Learning framework and its extension to multi-agent settings, Heterogeneous-Agent Mirror Learning (HAML). These frameworks serve as the theoretical foundation for analyzing the convergence properties of various reinforcement learning algorithms discussed in this paper. A.1 Mirror Learning Framework The Mirror Learning framework (Grudzien et al., 2022) provides unified theoretical foundation for It introduces three key concepts: analyzing the convergence of reinforcement learning algorithms. drift-function, neighbourhood operator, and mirror operator. Drift Function The drift-function is defined as mapping : Π {Dπ( s) : (A) R}, which captures the update cost of the new policy relative to the old policy, where Π is the policy space. Neighbourhood Operator The neighbourhood operator is defined as : Π P(Π), where P(Π) is the power set of Π, reflecting the search space for optimizing the new policy. Mirror Operator The mirror operator is mapping applied to the value function under certain policy, expressed as: (cid:2)M π DVπ (cid:3) (s) = Ea π [Aπ(s, a)] ν π π (s) βπ(s) Dπ( πs), (10) where π and π represent the old and new policies, respectively, Aπ(s, a) denotes the advantage function, which can equivalently be replaced by Qπ(s, a). The term ν π π (s) is functional that ensures continuity of expectations between the two policies under the drift context, and βπ(s) represents the sampling state distribution. Lastly, Dπ( πs) quantifies the drift-function of the new policy relative to the old policy at state s. Policy Update Rule. The mirror learning framework establishes theoretical convergence guarantees for algorithms utilizing the mirror operator. These guarantees are contingent upon both the drift-function and the neighbourhood operator satisfying certain properties. The update rule is formally defined as: πnew = arg max πN (πold) sβπ (cid:2)(cid:2)M π DVπold (cid:3) (s)(cid:3) . (11) Required Conditions for Convergence. Specifically, the conditions that the drift-function must satisfy are: 1. (Nonnegativity) For all S, π and π Π, Dπ( π s) Dπ(π s) = 0. 2. (Zero gradient) For all S, π and π Π, π(s)Dπ( π s)(cid:12) (cid:12) π=π = 0. (All its Gˆateaux derivatives are zero). The conditions that the neighbourhood operator must satisfy are: 1. (Continuity) is continuous map. 2. (Compactness) Every (π) is compact set. 3. (Closed ball) There exists metric χ : Π Π R, such that for all π Π, there exists ζ > 0, such that χ(π, π) ζ implies π (π). The mirror learning in Equation 11 represents generalized form of policy update, which is compatible with commonly used convergent algorithms such as TRPO and PPU. It is worth noting that mirror learning assumes an accurate estimation of the value function or advantage function. This assumption is readily satisfied in classical reinforcement learning (Sutton et al., 1998). However, this assumption may be violated in some backbone RL algorithms used for training LLMs as we show in the above sections. A.2 Heterogeneous-Agent Mirror Learning Building upon the mirror learning framework, (Zhong et al., 2024) propose the Heterogeneous-Agent Mirror Learning (HAML) framework to address cooperative multi-agent reinforcement learning problems. HAML extends the mirror learning paradigm to the multi-agent setting while preserving theoretical guarantees of monotonic improvement and convergence to Nash Equilibrium. In our work, we leverage this theory to address convergence issues in multi-turn policy updates. Multi-Agent Advantage Decomposition. The key challenge in the multi-agent setting lies in coordinating policy updates among multiple agents. HAML addresses this challenge by virtually decomposing the joint advantage function into conditional advantage functions for agent subsets (multi-agent advantage decomposition lemma), and sequentially updating each agents policy based on its contribution to the global advantage. Specifically, the multi-agent advantage decomposition lemma states that for any agent subset i1:m: Ai1:m π (s, ai1:m ) = ij π(s, ai1:j1, aij ), (12) j=1 ij π(s, ai1:j1 , aij ) = (s, ai1:j1 ) represents the multi-agent advantage function where that evaluates the contribution of agent ij given previous agents actions ai1:j1 . Note that this advantage decomposition is performed virtually rather than in actual execution; whether agents execute actions sequentially or simultaneously does not affect the decomposition. i1:j π (s, ai1:j ) i1:j1 π Heterogeneous-Agent Drift Functional Beyond the advantage decomposition, HAML extends other concepts from mirror learning in conditional manner. Correspondingly, HAML introduces three key components: (1) The heterogeneous-agent drift functional (HADF) for agent is defined as: Di : Π Π P(i) {Di π(s, πj1:m ) : (Ai) R}, (13) where Π denotes the joint policy space, P(i) is the power set of agents excluding i, and Ai is the action space of agent i. The drift Di π( ˆπis, πj1:m ) captures the update cost between πi and ˆπi, given that agents j1:m just updated to πj1:m . In this context, convergence refers to properties such as monotonic increase in the expected return and eventual convergence to an optimal policy. 18 (2) The neighbourhood operator : Π Πi P(Πi) for agent is defined Neighbourhood Operator such that πi Πi, π(πi) contains closed ball, i.e., there exists state-wise monotonically nondecreasing metric χ : Πi Πi such that πi Πi there exists δi > 0 such that χ(πi, πi) δi = πi π(πi). Heterogeneous-Agent Mirror Operator grates the multi-agent advantage function with the HADF: (cid:105) (cid:104) (cid:104) M( ˆπi) Di, πj1:m Aπ (s) aj1:m πj1:m ,ai ˆπi (3) The heterogeneous-agent mirror operator (HAMO) inteAi π(s, aj1:m , ai) (cid:105) Di π( ˆπi s, πj1:m ), (14) where the expectation is taken over the joint action of agents j1:m following their updated policies and agent following the candidate policy ˆπi. HAML Update Rule. The HAML update rule follows sequential structure: at each iteration k, random permutation i1:n of agents is drawn, and each agent im updates by solving: πim k+1 = arg max (πim πim im ) πk sβπk M( πim ) Dim ,π i1:m1 k+1 (cid:34)(cid:34) (cid:35) (cid:35) Aπk (s) , (15) where im πk (πim ) is the neighbourhood operator for agent im, and βπk is the sampling distribution. Required Conditions for HAML Convergence. The HADF must satisfy analogous properties to the single-agent case: 1. (Nonnegativity) For all S, joint policy π Π, agent is policies πi, πi Πi, and joint policy π j1:m of agents j1:m {i}: Di (16) 2. (Zero gradient) For all S, joint policy π Π, and joint policy π j1:m of agents j1:m {i}, π( πi s, π j1:m ) Di π(πi s, π j1:m ) = 0. all Gˆateaux derivatives vanish at the reference policy: π( πi s, π j1:m ) πi(s)Di (cid:12) (cid:12) (cid:12) πi=πi = 0. (17) The neighbourhood operator : Π Πi P(Πi) must satisfy: (i) continuity as set-valued map, (ii) π(πi) for all π, πi, and (iii) the closed ball property: there exists metric χ : Πi Πi compactness of such that for all πi Πi, there exists δi > 0 with χ(πi, πi) δi πi π(πi). Theorem 1: Monotonic Improvement under HAML (Adapted from the Fundamental Theorem of HAML (Zhong et al., 2024)) Let = {1, . . . , n} be the set of agents. For each agent , let Di be heterogeneous-agent drift functional satisfying the nonnegativity and zero gradient properties, and let be neighbourhood operator satisfying the closed ball property. Consider the policy sequence {πk} k=0 generated by the HAML update rule: at each iteration k, agents update sequentially via: πim k+1 = arg max (πim πim im ) πk sβπk (cid:34)(cid:34) M( πim ) Dim ,π i1:m1 k+1 (cid:35) (cid:35) Aπk (s) , (18) where βπ (S) is positive sampling distribution. Then, the expected return is monotonically non-decreasing: J(πk+1) J(πk), N. (19) Convergence Properties. Under additional regularity conditions (continuity and compactness of the neighbourhood operators, continuous dependence of the sampling distribution on π), the HAML framework (Zhong et al., 2024) further establishes: (i) convergence of value functions limk Vπk (s), and (ii) when the update order is randomly sampled at each iteration with every permutation having positive probability, the limit points are Nash equilibria. This random ordering requirement is crucial for the Nash equilibrium guarantee in general cooperative games. However, in our multi-turn contextual bandit setting, the problem structure is fundamentally different: the fixed sequential execution order and the absence of state transitions enable backward induction. This special structure allows us to establish resultconvergence to global optimum under fixed reverse update order, without requiring random permutations (see Section B). 19 Assumption on Advantage Estimation. HAML assumes accurate estimation of the joint advantage function Aπ(s, a). In contextual bandits, the advantage equals the centered reward: Aπ(s0, a) = r(s0, a) Vπ(s0). The group relative advantage estimator in our practical method provides unbiased estimates by using the group mean reward as baseline, eliminating the structural bias that arises in MDP settings (see Section 3.1)."
        },
        {
            "title": "B Global Optimality of SeeUPO",
            "content": "Building upon the HAML framework introduced in Section A, we now establish the global optimality guarantee for SeeUPO. While HAML provides monotonic improvement and convergence to Nash equilibria under random update orders, the multi-turn contextual bandit structure of our problemwith its fixed execution order and absence of state transitionsenables different result. This section proves that the reverse update order guarantees convergence to the global optimum by exploiting backward induction. Motivation for the optimal continuation value. To formalize backward induction in the multi-turn contextual bandit, we introduce the optimal continuation value V. It converts the remaining decision horizon into single scalar value for each history, which allows us to express the global optimal policy in per-turn form and to link the fixed-point condition in Step 2 to global optimality in Steps 34. Definition 1: Optimal Continuation Value For reward function r(s0, a1:T), define recursively: V(s0, a1:T) = r(s0, a1:T), V(s0, a1:t1) = max at V(s0, a1:t), = T, . . . , 1. (20) (21) Here denotes the unique function defined by the terminal condition and the backward maximization recursion. The globally optimal policy satisfies, for all {1, . . . , T}, π,t(s0, a1:t1) = arg maxat V(s0, a1:t). Theorem 2: Global Optimality under Reverse Update Order Consider multi-turn contextual bandit with execution order 1 2 T, where each turn has policy πt(ats0, a1:t1) over compact joint policy space ˆΠ. Let r(s0, a1:T) be bounded reward function with r(s0, a1:T) Rmax for all s0, a1:T. For each turn t, let Dt be drift functional satisfying the nonnegativity and zero gradient properties, and let be neighbourhood operator satisfying the continuity, compactness, and closed ball properties. Suppose the advantage function is accurately estimated. Consider the policy sequence { ˆπk} k=0 generated by the HAML update rule under reverse update order (T T1 1): at each iteration k, turn updates via ˆπt k+1 = arg max ( ˆπt πtU k) ˆπk s0β ˆπk (cid:20)(cid:20) M(πt) Dt, ˆπt+1:T k+1 (cid:21) (cid:21) , (s0) ˆπk where β ˆπ (S) is positive sampling distribution. Then: 1. (Value convergence) The value sequence converges: V(s0) such that limk ˆπk V(s0), s0. 2. (Global optimality) Any limit point π of the policy sequence is globally optimal: π = π, J( π) = = max ˆπ ˆΠ J( ˆπ). (22) (s0) = (23) Proof. Step 1 (Value Convergence). By Theorem 1, the policy sequence satisfies the monotonic improvement property: Since the reward function is bounded by Rmax, we have ˆπk {V ˆπk k=0 is monotonically non-decreasing and bounded above, hence converges: (s0)} J( ˆπk+1) J( ˆπk), N. (24) (s0) Rmax for all s0 and k. The sequence V(s0) such that lim ˆπk (s0) = V(s0), s0. (25) 20 Step 2 (Characterization of Limit Points). By compactness of ˆΠ and the Bolzano-Weierstrass theorem, the policy sequence { ˆπk} k=0 has at least one limit point π. We now characterize the properties of such limit points by establishing that they satisfy fixed-point condition. Consider subsequence { ˆπki order, turn updates by solving the HAML optimization problem: } i=1 that converges to π. At each iteration ki, under the reverse update ˆπt ki+1 = arg max πtU ( ˆπt ki ˆπki ) s0βki (cid:20) M(πt) Dt, ˆπt+1:T ki +1 (cid:21) , ˆπki (s0) (26) where βki is the sampling distribution over initial states s0 at iteration ki, and ˆπt+1:T ki+1 are the alreadyupdated subsequent policies (from turns + 1 to T). We now apply Berges Maximum Theorem (Ausubel & Deneckere, 1993) to establish convergence of the optimization problem. The objective function in Equation 26 is continuous in ˆπki because: 1. The advantage function ˆπki 2. The drift functional Dt is continuous by assumption. (s0) is continuous in ˆπki (Grudzien et al., 2022). 3. The neighbourhood operator is continuous and compact-valued by assumption. 4. The sampling distribution βki depends continuously on the policy. Therefore, as , by Berges Maximum Theorem, the optimization problem in Equation 26 converges to: max πtU π ( πt) s0 β (cid:104) M(πt) Dt, πt+1:T π(s0) (cid:105) , (27) where β is the limiting sampling distribution over initial states. Furthermore, since ˆπt Equation 26 for all i, there exists subsequence of { ˆπt solution to the limiting problem in Equation 27. ki+1 is solution to ki+1} that converges to some policy π, which is Claim: The limit policy πt is fixed point of the HAML operator, i.e., the solution to the limiting problem satisfies π = πt. Proof of Claim. We prove this by contradiction, following the approach of (Grudzien et al., 2022). For notational convenience, let πtπ denote the joint policy obtained by replacing the turn-t component of π with π: πtπ ( π1, . . . , πt1, π, πt+1, . . . , πT). (28) Suppose π = πt. Since π solves the limiting optimization problem in Equation 27 and the argmax is unique on compact set (otherwise we could take π = πt), the objective value achieved by π must be strictly greater than that achieved by πt: s0 β (cid:104) M(π) Dt, πt+1:T π(s0) (cid:105) > s0 β (cid:104) M( πt) Dt, πt+1:T π(s0) (cid:105) . (29) By the monotonic improvement property of HAML (Theorem 1), maximizing the mirror operator guarantees policy improvement. Specifically, since π achieves strictly higher mirror objective than πt, there exists some s0 such that: πtπ (s0) > π(s0), a1:T π[r(s0, a1:T)]. (s0) = V(s0). Furthermore, since π is the limit of the updated ki+1 (from further subsequence), and the value sequence is monotonically non-decreasing (s0) = V(s0). Hence πtπ (s0) = π(s0), which where the global value function is Vπ(s0) = However, by Step 1, π(s0) = limi ˆπki policies ˆπt and bounded, we have πtπ (s0) = limi ˆπki +1 contradicts Inequality 30. Therefore, π = πt. This establishes that at the limit point π, for each turn t, the policy πt satisfies the fixed-point condition: (cid:104) (cid:105) πt = arg max πtU π ( πt) s0 β M(πt) Dt, πt+1:T π(s0) . (30) (31) 21 This fixed-point condition reflects conditional optimality: πt maximizes the HAML mirror operator given the fixed subsequent policies πt+1:T. However, this does not immediately imply global optimality (i.e., π = π). The key insight is that reverse update order combined with the contextual bandit structure enables us to strengthen this conditional optimality to global optimality via backward induction, which we establish in the following steps. Step 3 (Turn Global Optimality via Dropping the Drift). We first prove that the terminal turn achieves global optimality: πT = π,T. By Step 2, for any history (s0, a1:T1), the policy πT satisfies the fixed-point condition: (cid:110) aT πT (cid:104) r(s0, a1:T1, aT) DT π(πT s0) (cid:105) (cid:111) . (32) πT = arg max πT We now apply the dropping-the-drift argument (Grudzien et al., 2022) to show that this fixed point is globally optimal. Suppose for contradiction that there exists some history (s such that: ) and action aT 0, a1:T1 r(s 0, a1:T1 , aT ) > aT πT r(s 0, a1:T1 , aT) (cid:104) (cid:105) . (33) aT πT [r(s aT πT(aT) r(s The expected reward written as Dirac delta distribution δaT By the zero gradient property of the drift functional DT, at the limit point we have: 0, a1:T1 (which concentrates all probability mass on aT 0, a1:T1 ), since it can be , aT). Therefore, the Gˆateaux derivative in the direction toward the , aT)] is an affine function of πT(s ) is strictly positive at πT. 0, a1:T1 πT (s 0 ,a1:T1 )DT π(πT 0) (cid:12) (cid:12) (cid:12)πT = πT = 0. (34) at state (s Therefore, the Gˆateaux derivative of the complete HAML objective (expected reward minus drift) in the is also strictly positive. By the closed ball property of the neighbourhood operator direction toward δaT π( πT) by moving small step from πT toward (Zhong et al., 2024), we can construct policy πT ), such that πT achieves strictly higher objective value than πT. This contradicts δaT the fixed-point condition that πT maximizes the HAML objective. Hence, the assumption in Equation 33 must be false. Therefore, for all histories (s0, a1:T1): πT(s0, a1:T1) = arg max r(s0, a1:T), 0, a1:T1 (35) which means πT = π,T (the globally optimal policy for turn T). aT Step 4 (Backward Induction to Earlier Turns). We now use backward induction to extend global optimality from turn to all earlier turns. Base case: Step 3 establishes πT = π,T. Inductive hypothesis: Assume πt+1:T = π,t+1:T for some {1, . . . , 1}. Inductive step: We prove πt = π,t. By the inductive hypothesis, for any history (s0, a1:t), the continuation value under πt+1:T equals the optimal continuation value: (cid:105) (cid:104) at+1:T πt+1:T r(s0, a1:T) = V(s0, a1:t). By Step 2, πt satisfies the fixed-point condition for turn given πt+1:T. Substituting the above equality, the HAML objective for turn becomes: (cid:110) (cid:111) (cid:104) (cid:105) πt = arg max atπt V(s0, a1:t) Dt π(πt s0) . πtU We now apply the dropping-the-drift argument (as in Step 3) to this optimization problem. Suppose for contradiction that there exists history (s 0, a1:t1 ) and action at (cid:104) V(s such that: 0, a1:t1 , at) 0, a1:t1 ) > , at V(s at πt (38) (cid:105) . By the same affinity argument, zero gradient property of Dt, and closed ball property of t, we can construct πt achieving higher objective value, contradicting the fixed-point condition. Therefore: πt(s0, a1:t1) = arg max V(s0, a1:t) = π,t. (39) at 22 (36) (37) By backward induction from = down to = 1, we conclude π = π. Step 5 (Conclusion). Since any limit point π of the policy sequence satisfies π = π: s0d [V π(s0)] = J( π) = s0d[V(s0)] = = max ˆπ ˆΠ J( ˆπ). (40) Therefore, all limit points are globally optimal, completing the proof. Why non-reverse update orders lack global-optimality guarantee. Step 2 yields fixed-point condition that is conditional on the subsequent policies πt+1:T. Reverse order is special because the backwardinduction hypothesis ensures πt+1:T = π,t+1:T, which makes the continuation value equal to and allows the dropping-the-drift argument to certify global optimality. Under any other fixed order (e.g., forward order), the subsequent policies conditioned on during the update are generally not optimal, so the fixed-point condition only guarantees optimality with respect to the current continuation value rather than V. Hence the proof does not extend to global optimality for non-reverse orders, even though monotonic improvement may still hold. Summary: Convergence guarantee of SeeUPO. Combining the results above with the advantage estimation analysis in subsequent sections, we summarize the convergence guarantee of SeeUPO as follows. Theorem 2 establishes global optimality under the assumption that the advantage function is accurately estimated. In the contextual bandit setting of SeeUPO, the Advantage Estimator provides unbiased advantage estimates by using the mean reward as baseline (see Section 3.1). This unbiasedness, combined with the reverse update order and the multi-turn contextual bandit structure, ensures that SeeUPO satisfies all conditions of Theorem 2, thereby guaranteeing convergence to the global optimum. For the practical instantiation SeeUPPO-GRAE (Section 4.2), we verify that both components satisfy the required conditions. First, the SeeUPPO component adopts PPU-style clipping for policy updates: as proven in (Grudzien et al., 2022; Zhong et al., 2024), the clipping mechanism corresponds to valid drift functional (satisfying nonnegativity and zero gradient properties) and the gradient-based update defines valid neighbourhood operator (satisfying continuity, compactness, and closed ball properties). HAPPO (Zhong et al., 2024) has established that such PPU-style sequential updates constitute valid HAML instantiation. Second, the GRAE component provides unbiased advantage estimation in the contextual bandit setting: since the advantage function degenerates to Aπ(s0, a) = r(s0, a) [r(s0, a)], GRAE estimates this by using the group mean reward as baseline without requiring value function approximation (see Section 3.1). Combining the HAML-compliant SeeUPPO updates with the unbiased GRAE estimation, SeeUPPO-GRAE satisfies all conditions of Theorem B, thereby guaranteeing convergence to the global optimum."
        },
        {
            "title": "C Bias of GAE",
            "content": "Theorem 3: GAE Bias Bound Let Vπ : be the true value function under policy π, and Vϕ : be the estimated value function. Define: State-action value function: Qπ(s, a) (cid:104) k=0 γkrt+k st = s, at = (cid:105) True advantage function: Aπ(s, a) Qπ(s, a) Vπ(s) Estimation error: ϵ(s) Vϕ(s) Vπ(s) Maximum error: ϵmax maxsS ϵ(s) rt + γVϕ(st+1) Vϕ(st) Vϕ t+l, where γ, λ [0, 1) Vϕ TD error: δ GAE estimator: ˆAGAE Under on-policy sampling, the bias of GAE satisfies: l=0(γλ)lδ (cid:104) (cid:12) (cid:12) (cid:12) ˆAGAE st, at (cid:105) Aπ(st, at) (cid:12) (cid:12) (cid:12) 1 + γ 2γλ 1 γλ ϵmax. (41) This appendix establishes the bias bound of Generalized Advantage Estimation (GAE). The main result (Theorem 3) shows that the bias of GAE is bounded by 1+γ2γλ ϵmax, where ϵmax is the maximum value function estimation error. Consequently, under perfect value function approximation (ϵmax = 0), 1γλ 23 Since for all 1. Therefore: (cid:34) l=0 GAE provides unbiased advantage estimates. This unbiasedness condition is crucial for the convergence guarantees of GAE-based algorithms such as PPU, as analyzed in Section F. Proof. Step 1 (GAE Expectation with True Value Function). Define the TD error using the true value function: δVπ rt + γVπ(st+1) Vπ(st). For = 0, conditioning on (st, at): st, at] = E[rt + γVπ(st+1) st, at] Vπ(st) = Qπ(st, at) Vπ(st) = Aπ(st, at), E[δVπ (42) where the second equality follows from the Bellman equation Qπ(s, a) = E[r + γVπ(s) s, a]. For 1, under on-policy sampling where at+l π(st+l), applying the law of iterated expectations: E[δVπ t+l st, at] = Est+l (cid:2)Eat+l π[Aπ(st+l, at+l) st+l] st, at (cid:3) . (43) aπ(s)[Aπ(s, a)] = Eaπ[Qπ(s, a) Vπ(s)] = Vπ(s) Vπ(s) = 0, we have E[δVπ t+l st, at] = (γλ)lδVπ t+l st, at (cid:35) = (γλ)0 Aπ(st, at) + l=1 (γλ)l 0 = Aπ(st, at). (44) Step 2 (Bias Computation). Substituting Vϕ(s) = Vπ(s) + ϵ(s) into the TD error definition: Vϕ = rt + γVϕ(st+1) Vϕ(st) δ = rt + γ[Vπ(st+1) + ϵ(st+1)] [Vπ(st) + ϵ(st)] = [rt + γVπ(st+1) Vπ(st)] +γϵ(st+1) ϵ(st). (cid:125) (cid:123)(cid:122) δVπ (cid:124) (45) Substituting into the GAE definition and taking conditional expectation: E[ ˆAGAE st, at] = = (cid:34) l=0 (cid:34) l=0 (γλ)lδ Vϕ t+l st, at (γλ)lδVπ t+l st, at (cid:35) (cid:35) + l=0 (γλ)lE[γϵ(st+l+1) ϵ(st+l) st, at]. (46) By Equation 44, the first term equals Aπ(st, at). We analyze the second term involving estimation errors: l= (γλ)lE[γϵ(st+l+1) ϵ(st+l) st, at]. (47) Expanding this sum and regrouping by coefficients of each ϵ(st+m) term: + = ϵ(st) (cid:124) (cid:123)(cid:122) (cid:125) l=0 = ϵ(st) + m=1 m=1 (cid:104) (γλ)m1γ (γλ)m(cid:105) E[ϵ(st+m) st, at] (γλ)m1γ(1 λ)E[ϵ(st+m) st, at] = ϵ(st) + (1 λ) m=1 γmλm1E[ϵ(st+m) st, at]. (48) Note that the coefficient for the m-th term is γmλm1(1 λ), which is well-defined for all λ [0, 1). For λ = 0, the series reduces to the single term γϵ(st+1), consistent with GAE(γ, 0) being the one-step TD error. Therefore, the bias is: E[ ˆAGAE st, at] Aπ(st, at) = ϵ(st) + (1 λ) m=1 γmλm1E[ϵ(st+m) st, at]. (49) Step 3 (Bound). Applying the triangle inequality and ϵ(s) ϵmax: Bias ϵ(st) + (1 λ) m=1 ϵmax + ϵmax(1 λ)γ γmλm1E[ϵ(st+m) st, at] k= (γλ)k (let = 1) = ϵmax + ϵmax (cid:18) = ϵmax 1 + γ(1 λ) 1 γλ (cid:19) γ γλ 1 γλ = 1 + γ 2γλ 1 γλ ϵmax. (50) Discussion. Equation 41 reveals several important properties of GAE bias: 1. Unbiasedness under perfect estimation. When the value function is perfectly estimated, i.e., ϵmax = 0, the bound becomes zero, implying that GAE provides unbiased advantage estimates. This is the key assumption underlying the convergence guarantees of GAE-based algorithms such as PPU. 2. Linear dependence on estimation error. The bias scales linearly with ϵmax, indicating that reducing value function approximation error directly reduces advantage estimation bias. 3. Effect of λ. Define C(γ, λ) 1+γ2γλ 1γλ . As λ 1, C(γ, λ) 1, minimizing the bias amplification. As λ 0, C(γ, λ) 1 + γ, increasing the bound. Thus, larger λ values reduce bias sensitivity to value estimation errors. 4. Effect of γ. For fixed λ, larger γ generally increases C(γ, λ), making long-horizon tasks more susceptible to bias from value estimation errors."
        },
        {
            "title": "D Bias of GRAE",
            "content": "This appendix provides detailed proofs for Theorem 4 regarding the bias and gradient (un)biasedness of GRAE in general MDPs. The key messages are: The GRAE advantage estimator is structurally biased because it uses the group mean (an s0-level baseline) for all states. The corresponding policy-gradient estimator is unbiased under the undiscounted objective (γ = 1) with total return. When the objective uses discounting (γ < 1), the GRAE gradient becomes biased. The contextual bandit case is special (GRAE becomes unbiased) and is analyzed separately in Section H. Theorem 4: GRAE Bias and Gradient (Un)biasedness in MDPs Consider finite-horizon MDP and policy πθ. For fixed initial state s0, sample i.i.d. trajectories and define the total return for trajectory τ(i) as R(i). Define the GRAE estimator ˆAGRAE(st, at) = R(i) with = 1 i=1 R(i). Then: 1. (Structural bias) For any (st, at), E[ ˆAGRAE st, at] = Q(st, at) V(s0), so the bias is V(st) V(s0), which is nonzero in general. 2. (Gradient unbiasedness under undiscounted objective) When γ = 1, the policy-gradient t=0 θ log πθ(at st) ˆAGRAE satisfies E[gGRAE] = θ J(θ) for the estimator gGRAE = undiscounted objective. 25 D.1 Baseline Invariance Lemma Lemma 1: Baseline Invariance (Adapted from the Theorem of RL (Sutton et al., 1998)) For any function b(st) that depends only on the state st and not on the current action at: atπθ (st) [θ log πθ(at st) b(st)] = 0. (51) Proof. atπθ (st) [θ log πθ(at st) b(st)] = b(st) at = b(st) at = b(st) θ at (cid:124) πθ(at st) (cid:123)(cid:122) =1 (cid:125) = 0. πθ(at st) θπθ(at st) πθ(at st) θπθ(at st) (52) (53) (54) (55) D.2 Proof of GRAE Bias and Gradient Unbiasedness Proof. Part 1: Bias of GRAE. Consider response τ(j) containing state-action pair (st, at) with reward R(j). For this trajectory, taking the conditional expectation over future trajectories given (st, at): Eτ>tπθ [ ˆAGRAE(st, at) st, at] = Eτ>tπθ [R(j) st, at] = Q(st, at) R. (56) (57) Note that is computed from other responses in the group and is independent of the future actions in the current response given st. As , by the law of large numbers: (58) τπθ (s0)[R] = V(s0), where denotes the reward of response starting from s0. Thus, in the large sample limit: Eτ>tπθ [ ˆAGRAE(st, at) st, at] Q(st, at) V(s0). The bias is computed as: Bias(st, at) = Eτ>tπθ [ ˆAGRAE st, at] Atrue(st, at) = [Q(st, at) V(s0)] [Q(st, at) V(st)] = V(st) V(s0) = (st). (59) (60) (61) (62) In general, V(st) = V(s0) since the value function evolves as the trajectory unfolds. Therefore, ˆAGRAE is biased. Part 2: Unbiasedness of Policy Gradient. For response τ(j) with reward R(j), define the gradient estimators: gtrue = gGRAE = t=0 t=0 θ log πθ(at st) (R(j) V(st)), θ log πθ(at st) (R(j) R), 26 (63) (64) where the sum is over all state-action pairs (st, at) in response τ(j), and all pairs share the same reward R(j) and group mean R. As , V(s0). Their difference becomes: gGRAE gtrue = t=0 θ log πθ(at st) (V(st) V(s0)). (65) The term V(st) V(s0) depends on the state history (s0, a0, . . . , at1) but not on the current action at. Thus, it can be treated as b(st). Taking expectations using the law of iterated expectations: Eτπθ [gGRAE gtrue] = t=0 t=0 = 0. = Estπθ (cid:104) atπθ (st) [θ log πθ(at st) (V(st) V(s0))] (cid:105) Estπθ [0] (by Lemma 1) Since Eτπθ [gtrue] = θ J(θ) by the policy gradient theorem, we conclude: Eτπθ [gGRAE] = θ J(θ). (66) (67) (68) (69) D.3 Proof of GRAE Gradient Bias When γ = 1 As mentioned in the key messages at the beginning of this section, when the objective uses discounting (γ < 1), the GRAE gradient becomes biased. This subsection provides detailed analysis of why this bias arises. When the objective uses discount factor γ (0, 1), the GRAE gradient estimator based on total return is biased in general. We now provide detailed analysis. Let the discounted return from time be Gγ = Tt1 gradient estimator for the discounted objective is γkrt+k+1, and let Vγ(s) = E[Gγ 0 s0 = s]. The true k= gtrue = t=0 θ log πθ(at st) (cid:0)Gγ Vγ(st)(cid:1) . GRAE instead uses the total return = T1 k=0 rk+1 and the group mean R: t= θ log πθ(at st) (R R). gGRAE = In the large-sample limit, E[R s0]. Their difference is gGRAE gtrue = t=0 θ log πθ(at st) (cid:0)R Gγ + Vγ(st) E[R s0](cid:1) . (70) (71) (72) By Lemma 1, the term Vγ(st) E[R s0] is baseline and vanishes in expectation. The remaining term can be expanded as Gγ = t1 k=0 rk+1 + Tt1 k=1 (1 γk)rt+k+1. (73) The first sum depends only on the past and can be treated as baseline given st. The second sum depends on future rewards (and thus on at) with positive coefficients (1 γk), so its contribution to the expected gradient is nonzero in general. Hence gGRAE is biased when γ = 1. Discussion. In multi-turn scenarios, GRAE faces additional challenges: 1. Credit assignment problems: Using group mean reward computed from initial state s0 to estimate advantages for states st at later turns creates credit assignment issues. 2. Amplified structural bias: The structural bias V(st) V(s0) grows with the number of turns. 3. Gradient bias: When γ < 1 (which is the more common setting in practical MDPs), the gradient estimator becomes biased, compounding the problems above. Therefore, in multi-turn settings, directly applying GRAE, especially in token-level RL, is not suitable. 27 Detailed Proofs for GRAE-REINFORCE Convergence This appendix provides detailed proofs for Theorem 5 regarding the convergence of GRAE-REINFORCE. The key insight is that REINFORCE can be viewed as an instance of the Mirror Learning framework (Grudzien et al., 2022), specifically as parameterized implementation of Generalized Policy Iteration (GPI). Combined with the gradient unbiasedness of GRAE under the undiscounted objective (γ = 1, Theorem 4), we establish the convergence guarantee. E.1 REINFORCE as an Instance of Mirror Learning We first establish that REINFORCE fits within the Mirror Learning framework. Recall from Section that Mirror Learning requires: (1) drift function D, (2) neighbourhood operator , and (3) sampling distribution βπ. Lemma 2: REINFORCE as Mirror Learning REINFORCE is an instance of Mirror Learning with the following components: 1. Drift function: 0 (trivial drift). 2. Neighbourhood operator: = Π (trivial neighbourhood). 3. Sampling distribution: βπ = ρπ, the state visitation distribution under the current policy. Proof. By (Grudzien et al., 2022), when 0 and = Π, the Mirror Learning update reduces to GPI: (74) S. Ea π[Qπold (s, a)], πnew(s) = arg max π(s)P (A) REINFORCE is the parameterized, stochastic gradient implementation of GPI. The policy gradient theorem (Sutton et al., 1998) shows that: θ J(θ) = Esρπθ ,aπθ [θ log πθ(as) Qπθ (s, a)] , which is exactly the gradient of the GPI objective weighted by the state visitation distribution ρπθ . Hence, REINFORCE approximately solves the GPI step via gradient ascent. (75) E.2 Convergence Theorem Theorem 5: Convergence of GRAE-REINFORCE Consider finite-horizon MDP with γ = 1 and r(s, a) Rmax for all (s, a). Let πθ be parameterized policy over compact parameter space Θ. Suppose the policy is updated by REINFORCE using the GRAE advantage estimator. Then: 1. (Gradient unbiasedness) Eτπθ [gGRAE] = θ J(θ). 2. (Monotonic improvement) J(πk+1) J(πk), 3. (Convergence) such that limk J(πk) = J. N. Proof. Step 1 (Gradient Unbiasedness). By Theorem 4, under γ = 1, the GRAE-based gradient estimator satisfies: Eτπθ [gGRAE] = θ J(θ). (76) This follows from the baseline invariance property (Lemma 1): adding or subtracting any state-dependent baseline from the return does not change the expected gradient. Step 2 (Monotonic Improvement and Convergence). By Lemma 2, REINFORCE is an instance of Mirror Learning with trivial drift 0 and trivial neighbourhood = Π. The trivial drift satisfies nonnegativity (Dπ( πs) = 0 0) and zero gradient ( πDπ( πs) π=π = 0). By the Fundamental Theorem of Mirror Learning (Grudzien et al., 2022), the policy sequence {πk} k=0 satisfies: and J(πk+1) J(πk), N, J(πk) = J. lim 28 (77) (78) Detailed Proofs for GAE-PPU Convergence This appendix provides detailed proofs for Theorem 6 regarding the convergence of GAE-PPU. The key insight is that PPU can be viewed as an instance of the Mirror Learning framework (Grudzien et al., 2022) with non-trivial drift function derived from the clipping objective. Combined with the unbiasedness of GAE under perfect value function approximation (Theorem 3), we establish the convergence guarantee. F.1 PPU as an Instance of Mirror Learning We first establish that PPU fits within the Mirror Learning framework. Recall from Section that Mirror Learning requires: (1) drift function D, (2) neighbourhood operator , and (3) sampling distribution βπ. Lemma 3: PPU as Mirror Learning PPU is an instance of Mirror Learning with the following components: 1. Drift function: DPPU r( π) = π(as) π(as) . π ( πs) = Eaπ [ReLU ([r( π) clip(r( π), 1 ϵ)] Aπ(s, a))], where 2. Neighbourhood operator: = Π (trivial neighbourhood). 3. Sampling distribution: βπ = ρπ, the normalized marginal discounted state distribution. Proof. We derive the drift function by reformulating the PPU clipping objective. Starting from the PPU update rule: πPPU new = arg max πΠ Esρπ,aπ [min (r( π)Aπ(s, a), clip(r( π), 1 ϵ)Aπ(s, a))] . (79) The complete derivation proceeds as follows: πPPU new = arg max πΠ = arg max πΠ Esρπ,aπ [min (r( π)Aπ(s, a), clip(r( π), 1 ϵ)Aπ(s, a))] Esρπ [Eaπ [min (r( π)Aπ(s, a), clip(r( π), 1 ϵ)Aπ(s, a))] Ea π [Aπ(s, a)] + Ea π [Aπ(s, a)]] = arg max πΠ Esρπ [Eaπ [min (r( π)Aπ(s, a), clip(r( π), 1 ϵ)Aπ(s, a))] Eaπ [r( π)Aπ(s, a)] + Ea π [Aπ(s, a)]] = arg max πΠ Esρπ [Ea π [Aπ(s, a)] (Eaπ [r( π)Aπ(s, a)] Eaπ [min (r( π)Aπ(s, a), clip(r( π), 1 ϵ)Aπ(s, a))])] = arg max πΠ Esρπ [Ea π [Aπ(s, a)] Eaπ (r( π)Aπ(s, a) min (r( π)Aπ(s, a), clip(r( π), 1 ϵ)Aπ(s, a)))] = arg max πΠ Esρπ [Ea π [Aπ(s, a)] Eaπ (max (0, r( π)Aπ(s, a) clip(r( π), 1 ϵ)Aπ(s, a)))] = arg max πΠ = arg max πΠ Esρπ [Ea π [Aπ(s, a)] Eaπ (max (0, [r( π) clip(r( π), 1 ϵ)] Aπ(s, a)))] Esρπ [Ea π [Aπ(s, a)] Eaπ [ReLU ([r( π) clip(r( π), 1 ϵ)] Aπ(s, a))]] . Comparing with the Mirror Learning update (Equation 11), we identify the drift function as: DPPU π ( πs) = Eaπ [ReLU ([r( π) clip(r( π), 1 ϵ)] Aπ(s, a))] . We now verify that DPPU Nonnegativity: Since ReLU(x) = max(0, x) 0 for all R: satisfies the required properties: π DPPU π ( πs) = Eaπ [ReLU()] 0. 29 (80) (81) (82) When π = π, we have r( π) = 1 and clip(1, 1 ϵ) = 1, so: DPPU π (πs) = Eaπ [ReLU(0)] = 0. Zero gradient: When π = π, the argument of ReLU is zero: [r( π) clip(r( π), 1 ϵ)] Aπ(s, a) (cid:12) (cid:12) (cid:12) π=π = [1 1]Aπ(s, a) = 0. Since ReLU(0) = 0 and the ReLU function has subgradient 0 at = 0: π(s)DPPU π ( πs)(cid:12) (cid:12) π=π = 0. (83) (84) (85) The trivial neighbourhood operator = Π satisfies continuity, compactness, and the closed ball property. Hence, PPU is valid instance of Mirror Learning. F.2 Convergence Theorem Theorem 6: Convergence of GAE-PPU Consider an MDP with r(s, a) Rmax for all (s, a). Let πθ be parameterized policy over compact parameter space Θ. Suppose the policy is updated by PPU using the GAE advantage estimator with perfect value function approximation (Vϕ = Vπ). Then: 1. (Advantage unbiasedness) E[ ˆAGAE 2. (Monotonic improvement) J(πk+1) J(πk), 3. (Convergence) limk J(πk) = J. st, at] = Aπ(st, at). N. Proof. Step 1 (Advantage Unbiasedness). By Theorem 3, the bias of GAE is bounded by: Bias(st, at; λ) 1 + γ 2γλ 1 γλ ϵmax, (86) where ϵmax = maxs Vϕ(s) Vπ(s). Under perfect value function approximation (Vϕ = Vπ), we have ϵmax = 0, thus: E[ ˆAGAE st, at] = Aπ(st, at). (87) Step 2 (Monotonic Improvement and Convergence). By Lemma 3, PPU is an instance of Mirror Learning and trivial neighbourhood = Π. The drift function satisfies nonnegativity with drift function DPPU (DPPU ( πs) π=π = 0). By the π Fundamental Theorem of Mirror Learning (Grudzien et al., 2022), the policy sequence {πk} k=0 satisfies: J(πk+1) J(πk), ( πs) 0 with equality when π = π) and zero gradient ( πDPPU N, (88) π π and J(πk) = J. lim (89) Detailed Proofs for GRAE-PPU Convergence Analysis This appendix provides detailed proofs for Theorem 7 regarding the failure of GRAE-PPU to guarantee convergence in general MDPs. The key insight is that in MDPs (as opposed to contextual bandits), GRAE introduces structural bias that breaks the drift function properties required by the Mirror Learning framework (Grudzien et al., 2022). G.1 GRAE-PPU in the Mirror Learning Framework We first analyze how GRAE-PPU fits within the Mirror Learning framework, and show that the structural bias in GRAE leads to malformed drift function. 30 Lemma 4: Structural Bias of GRAE in MDPs In general MDPs, GRAE introduces structural bias (st) = V(st) V(s0) that does not vanish as the number of samples increases: E[ ˆAGRAE(st, at) Atrue(st, at) st, at] = V(st) V(s0) = 0. (90) lim Proof. Consider response τ(j) containing state-action pair (st, at) with reward R(j). In the large sample limit , we have V(s0) by the law of large numbers. The bias decomposes as: (st, at) = ˆAGRAE(st, at) Atrue(st, at) = (R(j) V(s0)) (Q(st, at) V(st)) = (R(j) Q(st, at)) + (V(st) V(s0)) (cid:123)(cid:122) (cid:125) (cid:123)(cid:122) (cid:125) ϵt Bt (cid:124) (cid:124) . (91) (92) (93) The term ϵt satisfies E[ϵt st, at] = 0 by definition of Q, and vanishes under averaging. However, Bt = V(st) V(s0) is deterministic given st and independent of sample size. Thus: E[ st, at] = Bt = 0. lim (94) Lemma 5: GRAE-PPU Drift Function GRAE-PPU induces drift function of the form: DGRAE-PPU π ( πst) = Eatπ [ReLU ((r( π) clip(r( π), 1 ϵ))(Atrue(st, at) + (st)))] (st), (95) where (st) = V(st) V(s0) is the structural bias from Lemma 4. Proof. The GRAE-PPU objective substitutes the true advantage Atrue(st, at) with the biased estimator ˆAGRAE(st, at) = Atrue(st, at) + (st): πnew = arg max πΠ Estρπ,atπ (cid:2)min (cid:0)r( π) ˆAGRAE, clip(r( π), 1 ϵ) ˆAGRAE (cid:1)(cid:3) , (96) where r( π) = π(atst) π(atst) . To derive the drift function, we must express this objective in the Mirror Learning form: Ea π[Atrue(st, a)] D( πst). The key insight is that the Mirror Learning framework requires using the true advantage function Atrue, not the biased estimate. Following the derivation in Lemma 3, the GRAE-PPU clipped objective can be rewritten as: Eatπ = Eat π (cid:2)min (cid:0)r( π) ˆAGRAE, clip(r( π), 1 ϵ) ˆAGRAE (cid:1)(cid:3) (cid:2)ReLU (cid:0)[r( π) clip(r( π), 1 ϵ)] ˆAGRAE (cid:2) ˆAGRAE (cid:3) Eatπ (cid:1)(cid:3) . (97) Now we substitute ˆAGRAE = Atrue + (st) and expand: Eat π [Atrue + (st)] Eatπ [ReLU ([r( π) clip(r( π), 1 ϵ)] (Atrue + (st)))] = Eat π [Atrue] + (st) Eatπ [ReLU ([r( π) clip(r( π), 1 ϵ)] (Atrue + (st)))] , where the second equality uses the fact that Eat π[(st)] = (st) since (st) is constant with respect to at. To express this in Mirror Learning form Ea π[Atrue] D( πst), we identify the drift function as: (98) DGRAE-PPU π ( πst) = Eatπ [ReLU ([r( π) clip(r( π), 1 ϵ)] (Atrue + (st)))] (st). (99) G.2 Convergence Failure Theorem Theorem 7: Convergence Failure of GRAE-PPU Consider general MDP where there exist states st with V(st) = V(s0). Let GRAE-PPU update the policy using the drift function from Lemma 5. Then: 1. (Drift zero-at-origin violation) st : DGRAE-PPU 2. (Drift nonnegativity violation) st, π : DGRAE-PPU 3. (Policy degradation) πold : J(πnew) < J(πold). (πst) = 0. ( πst) < 0. π π Proof. 1. Violation of Zero at Origin: Mirror Learning requires that DGRAE-PPU Equation 95, we have r(πold) = 1 and clip(r(πold), 1 ϵ) = 1. The ReLU term becomes: (πoldst) = 0 for all states st. Substituting π = πold into πold ReLU ((1 1)(Atrue(st, at) + (st))) = ReLU(0) = 0. (100) Thus: (101) Since (st) = V(st) V(s0) = 0 generally (by Lemma 4), the drift function is non-zero at the origin, breaking the fundamental distance metric property required by Mirror Learning. (πoldst) = 0 (st) = (st). DGRAE-PPU πold 2. Violation of Non-negativity: Mirror Learning requires that DGRAE-PPU consider scenario where: πold ( πst) 0 for all states st, policies πold, and π Π. However, (st) is large and positive (e.g., when the current state value V(st) is much higher than the initial value V(s0)), The policy π is close to πold (i.e., r( π) 1), making the ReLU term small or zero. In this case, DGRAE-PPU πold ( πst) (st) < 0. 3. Failure of Monotonic Improvement: The negative drift term fundamentally breaks the Mirror Learning convergence proof. Recall that in the proof of monotonic improvement (Lemma 3.3 in (Grudzien et al., 2022)), the key step establishes: π(s) Vπold (s) γ inf [V π(s) Vπold (s)] + ν(s) β(s) Dπold ( πs). Taking infimum over and rearranging yields: [V π(s) Vπold (s)] inf 1 1 γ inf (cid:20) ν(s) β(s) Dπold ( πs) (cid:21) . (102) (103) πold When DGRAE-PPU ( πs) 0, the right-hand side is non-negative, guaranteeing π(s) Vπold (s) for all s. However, when DGRAE-PPU ( πs) < 0, the right-hand side becomes negative, and the proof can only establish that the value difference is lower-bounded by some negative quantitythis provides no guarantee that π(s) Vπold (s). Thus, the violation of drift nonnegativity causes the proof chain to break down entirely, invalidating the monotonic improvement guarantee and potentially leading to policy degradation. πold 4. Existence of Policy Degradation: We now provide concrete example demonstrating how GRAE-PPU can lead to policy degradation. Consider simple 2-state MDP where V(s0) = 0 and V(s1) = 10, yielding structural bias (s1) = V(s1) V(s0) = 10. In state s1, suppose there are two actions: agood with true advantage Atrue(s1, agood) = 2 and abad with true advantage Atrue(s1, abad) = 5. By Lemma 4, GRAE introduces state-dependent bias (st) = V(st) V(s0) that applies uniformly to all actions at the same state. Therefore, the GRAE advantage estimates become: ˆAGRAE(s1, agood) = Atrue(s1, agood) + (s1) = 2 + 10 = 12, ˆAGRAE(s1, abad) = Atrue(s1, abad) + (s1) = 5 + 10 = 5. (105) (104) 32 Why standard policy gradients are unaffected. Before analyzing PPU, we note an important subtlety: standard (vanilla) policy gradients are invariant to state-dependent baselines (Theorem D). For softmax policies, the total gradient sums over all actions, and since θπθ(as) = 0, constant shifts in advantages cancel out. Concretely, for the two-action case at s1: gtrue log πgood (2) + log πbad (5), log πgood (12) + log πbad (5). gGRAE (106) (107) Using the fact that log πgood and log πbad point in roughly opposite directions for binary actions, both gradients yield the same direction (favoring agood). Thus, GRAE does not cause degradation for vanilla policy gradients. Why PPUs clipping breaks baseline invariance. However, the PPU clipped objective is non-linear with respect to the advantage estimate: LCLIP(a) = min (cid:0)r(θ) ˆA, clip(r(θ), 1 ϵ, 1 + ϵ) ˆA(cid:1) , (108) where r(θ) = πθ(as)/πold(as). This non-linearity fundamentally breaks baseline invariance because the clipping behavior depends on the sign of ˆA. Let us analyze the PPU objective for abad under both scenarios with ϵ = 0.2: True scenario (Atrue(s1, abad) = 5): LCLIP true (abad) = min (5r, clip(r, 0.8, 1.2) (5)) = (cid:26)5r 4 if 0.8 if < 0. Maximizing this objective pushes 0.8 (the lower bound), i.e., decreasing π(abad) by up to 20%. GRAE scenario ( ˆAGRAE(s1, abad) = +5): LCLIP GRAE(abad) = min (+5r, clip(r, 0.8, 1.2) (+5)) = (cid:26)+6 +5r if > 1.2 if 1.2 (109) (110) Maximizing this objective pushes 1.2 (the upper bound), i.e., increasing π(abad) by up to 20%. The clipping directions are completely reversed. The true objective wants to decrease π(abad) and clips at 1 ϵ; the GRAE objective wants to increase π(abad) and clips at 1 + ϵ. Although GRAE preserves the relative ranking ( ˆAGRAE(agood) = 12 > 5 = ˆAGRAE(abad)), the PPU update for abad is fundamentally wrong: 1. Under true advantages: PPU allows π(abad) to decrease from πold(abad) to 0.8 πold(abad). 2. Under GRAE advantages: PPU allows π(abad) to increase from πold(abad) to 1.2 πold(abad). Consequently, after the PPU update, we have: π(abad s1) 1.2 πold(abad s1) > πold(abad s1), whereas the optimal update should yield π(abad s1) 0.8 πold(abad s1). Since abad has true negative advantage (5), increasing its probability degrades expected return: (111) J( π) < J(πold). (112) Therefore, GRAE-PPU violates the fundamental properties required for Mirror Learning convergence guaranteesnot because the gradient direction is wrong (it is correct for vanilla PG), but because PPUs non-linear clipping mechanism cannot tolerate the sign flip in advantage estimates, leading to trust region constraints being enforced in the completely wrong direction. GRAE-PPU Convergence in Contextual Bandit Settings and Variance"
        },
        {
            "title": "Normalization Issues",
            "content": "This appendix provides detailed proofs for Theorem 8 regarding GRAE unbiasedness and convergence in Contextual Bandit settings, and demonstrates why variance normalization operations (as used in GSPO) break convergence guarantees. The key insight is that in Contextual Bandits, the structural bias of GRAE vanishes because there is only one state (s0), making GRAE an unbiased estimator. 33 H.1 GRAE-PPU as an Instance of Mirror Learning in Contextual Bandits We first establish that GRAE-PPU fits within the Mirror Learning framework when applied to Contextual Bandits. Lemma 6: GRAE Unbiasedness in Contextual Bandits In the Contextual Bandit setting, GRAE provides unbiased advantage estimates: E[ ˆAGRAE(s, a) s, a] = ABandit π (s, a). (113) Proof. In the Contextual Bandit setting, GRAE estimates the advantage as: ˆAGRAE(s, a) = r(s, a) R, (114) where = 1 As , by the law of large numbers: i=1 r(s, ai) is the group mean reward for responses sampled from the same query s. aπ(s)[r(s, a)] = VBandit π (s). The true state-action value function in the Contextual Bandit setting is: (s, a) = E[r(s, a) s, a] = r(s, a), QBandit π since rewards are deterministic given the state-action pair. The true value function is: VBandit π (s) = aπ(s)[QBandit π (s, a)] = aπ(s)[r(s, a)]. The true advantage function is: ABandit π (s, a) = QBandit π (s, a) VBandit π (s) = r(s, a) VBandit π (s). Therefore: E[ ˆAGRAE(s, a) s, a] = r(s, a) VBandit π (s) = QBandit π = ABandit π (s, a) VBandit (s, a). π (s) Lemma 7: GRAE-PPU as Mirror Learning in Contextual Bandits In the Contextual Bandit setting, GRAE-PPU is an instance of Mirror Learning with: ( πs) = Eaπ 1. Drift function: DGRAE-PPU [r( π) clip(r( π), 1 ϵ)] ABandit ReLU (cid:16) (cid:104) π π (115) (116) (117) (118) (119) (120) (121) (s, a) (cid:17)(cid:105) , which is identical to the standard PPU drift function. 2. Neighbourhood operator: = Π (trivial neighbourhood). 3. Sampling distribution: βπ = ρπ. Proof. By Lemma 6, GRAE provides unbiased advantage estimates in the Contextual Bandit setting. (s, a) for ˆAGRAE(s, a) in the GRAE-PPU Therefore, we can substitute the true advantage function ABandit objective: π πnew = arg max πΠ where r( π) = π(as) π(as) . Esρπ ,aπ (cid:104) min (cid:16) r( π)ABandit π (s, a), clip(r( π), 1 ϵ)ABandit π (s, a) (cid:17)(cid:105) , (122) Following the same derivation as in Lemma 3, the drift function is: DGRAE-PPU π ( πs) = Eaπ (cid:104) ReLU (cid:16) [r( π) clip(r( π), 1 ϵ)] ABandit π (s, a) (cid:17)(cid:105) , (123) which is identical to the standard PPU drift function (Equation 81). Since this drift function satisfies nonnegativity and zero gradient properties (as established in Lemma 3), GRAE-PPU is valid instance of Mirror Learning in the Contextual Bandit setting. 34 H.2 Convergence Theorem Theorem 8: Convergence of GRAE-PPU in Contextual Bandits Consider Contextual Bandit with r(s, a) Rmax for all (s, a). Let πθ be parameterized policy over compact parameter space Θ. Suppose the policy is updated by GRAE-PPU. Then: 1. (Advantage unbiasedness) E[ ˆAGRAE(s, a) s, a] = ABandit 2. (Monotonic improvement) J(πk+1) J(πk), 3. (Convergence) limk J(πk) = J. π N. (s, a). Proof. Step 1 (Advantage Unbiasedness). By Lemma 6, GRAE provides unbiased advantage estimates in the Contextual Bandit setting: E[ ˆAGRAE(s, a) s, a] = ABandit π (s, a). (124) Step 2 (Monotonic Improvement and Convergence). By Lemma 7, GRAE-PPU is an instance of Mirror Learning with drift function identical to the standard PPU drift function. The drift function satisfies nonnegativity (DGRAE-PPU ( πs) 0 with equality when π = π) and zero gradient ( πDGRAE-PPU ( πs) π=π = 0). By the Fundamental Theorem of Mirror Learning (Grudzien et al., π 2022), the policy sequence {πk} π k=0 satisfies: and J(πk+1) J(πk), N, J(πk) = J. lim (125) (126) H.3 Variance Normalization Breaking Convergence: The Case of GSPO This subsection demonstrates why variance normalization operations, as used in GSPO (and GRPO), break convergence guarantees. We use GSPO as concrete example to illustrate the fundamental issues with variance normalization. It is worth noting that (Hu et al., 2025) has shown that the original GRPOs advantage estimation is biased when variance normalization is applied. H.3.1 GSPO Drift Function Derivation and Properties In single-turn scenarios, GSPO applies variance normalization to the advantage estimates. Specifically, GSPOs advantage estimation can be written as: AGSPO πold (s, a) = (s, a) ABandit π δ(s) , (127) where ABandit standard deviation of rewards within the group of responses sampled from the same query s. (s) is the true Bandit advantage function, and δ(s) is the (s, a) = QBandit (s, a) VBandit π π π The key observation is that δ(s) depends only on the state (the query) and not on the specific action a, since it is computed from the empirical distribution of rewards across all responses to the same query. This state-dependent normalization factor δ(s) plays crucial role in the convergence analysis, as we will demonstrate that variance normalization breaks convergence guarantees unless δ(s) 1 identically for all states. Following the same derivation procedure as in Section F, we can identify the drift function as: DGSPO πold ( πs) = Eaπold r( π)ABandit πold (s, a) min r( π) (cid:34) (cid:32) (s, a) ABandit πold δ(s) , clip(r( π), 1 ϵ) (s, a) ABandit πold δ(s) (cid:33)(cid:35) . (128) Theorem 9: GSPO Drift Function Properties The GSPO drift function DGSPO πold gradient property required for mirror learning convergence if and only if δ(s) 1. ( πs) satisfies both the nonnegativity property and the zero 35 To prove this theorem, we establish two auxiliary lemmas and then combine them. The proof proceeds in three steps: (1) establish Lemma 8 showing that δ(s) 1 is necessary and sufficient for nonnegativity, (2) establish Lemma 9 showing that δ(s) 1 implies zero gradient, and (3) combine these lemmas to prove the main theorem. Step (1): Nonnegativity Analysis. To analyze the drift function, we define the integrand: (cid:18) rA δ(s) g(A, r) = rA min , clip(r, 1 ϵ)A δ(s) (cid:19) , (129) where = r( π) = π(as) πold(as) > 0, and = ABandit πold (s, a). The drift function is DGSPO πold ( πs) = Eaπold [g(A, r)]. We now state and prove the first auxiliary lemma. Lemma 8: Necessary and Sufficient Condition for Nonnegativity δ(s) 1 if and only if DGSPO πold ( πs) 0 for all states s, policies πold, and π Π. Proof. We analyze g(A, r) by cases based on the sign of and the relationship between and [1 ϵ, 1 + ϵ]. Case 1: 0 For [1 ϵ, 1 + ϵ]: g(A, r) = rA(1 1/δ(s)), requiring δ(s) 1. For > 1 + ϵ: g(A, r) = A(r (1 + ϵ)/δ(s)). The critical constraint comes from [1 ϵ, 1 + ϵ]. For < 1 ϵ: g(A, r) = rA(1 1/δ(s)), requiring δ(s) 1. Case 2: < 0 For [1 ϵ, 1 + ϵ]: g(A, r) = rA(1 1/δ(s)), requiring δ(s) 1. For > 1 + ϵ: g(A, r) = rA(1 1/δ(s)), requiring δ(s) 1. For < 1 ϵ: g(A, r) = A(r (1 ϵ)/δ(s)). As (1 ϵ), this requires δ(s) 1. Necessity: Case 1 requires δ(s) 1; Case 2 requires δ(s) 1. Therefore, δ(s) = 1 is necessary. Sufficiency: When δ(s) = 1, the drift function reduces to the PPU form, which satisfies nonnegativity. Step (2): Zero Gradient Analysis. We now state and prove the second auxiliary lemma. Lemma 9: Zero Gradient Property If δ(s) = 1, then π(s)DGSPO πold ( πs)(cid:12) (cid:12) π=πold = 0. Proof. When π = πold, we have = 1. With δ(s) = 1, the drift function becomes: DGSPO πold (πolds) = Eaπold (cid:104) ABandit πold (s, a) ABandit πold (s, a) (cid:105) = 0, (130) and the gradient is zero. Step (3): Combining the Lemmas. We are now ready to prove the main theorem by combining the two lemmas. Proof. By Lemma 8, we have δ(s) 1 Nonnegativity. By Lemma 9, we have δ(s) 1 Zero Gradient. Forward direction: δ(s) 1 (Nonnegativity Zero Gradient) follows directly from the two lemmas. Backward direction: If both properties hold, then Nonnegativity holds in particular. By Lemma 8, this implies δ(s) 1. Therefore, δ(s) 1 (Nonnegativity Zero Gradient). 36 H.3.2 Why Variance Normalization Breaks Convergence As established in Theorem 9, variance normalization breaks convergence guarantees unless δ(s) 1 identically for all states. However, in practice, δ(s) is computed from empirical reward distributions and varies across different queries based on their reward distributions. This variation leads to violations of the fundamental properties required for monotonic improvement guarantees. The key issue is that when δ(s) = 1, the drift function can become negative for certain combinations of advantages and policy ratios, violating the nonnegativity property required for Mirror Learning convergence. Recall that in the proof of monotonic improvement (Lemma 3.3 in (Grudzien et al., 2022)), the critical inequality is: [V π(s) Vπold (s)] inf 1 1 γ inf (cid:20) ν(s) β(s) Dπold ( πs) (cid:21) . (131) πold When DGSPO ( πs) < 0, the right-hand side becomes negative, and the proof can only establish that the value difference is lower-bounded by some negative quantity. This means the proof chain breaks down entirelyit no longer guarantees that π(s) Vπold (s) for all states. This demonstrates why variance normalization operations like those used in GSPO fundamentally break the convergence guarantees provided by Mirror Learning, even in the simplified Contextual Bandit setting. H.3.3 Batch-Level Normalization Preserves Convergence In contrast to group-level variance normalization (as used in GSPO), batch-level normalization applies single normalization to all advantage estimates within batch. We now show that batch-level normalization preserves convergence guarantees because it does not change the argmax of the optimization problem, and therefore the drift functional remains completely unchanged. Consider batch = {(s computes normalized advantages as: (i) 0 , a1:T,(i), ˆA(i))}B i=1 of advantage estimates. Batch-level normalization , (132) j=1 ˆA(j) is the batch mean and σB = j=1( ˆA(j) µB)2 is the batch standard deviation. where µB = 1 During single policy update iteration k, the batch is fixed and sampled from the current policy ˆπk. Therefore, both µB and σB are deterministic constants that do not depend on the candidate policy π being optimized. The PPU policy update with batch-normalized advantages solves: π = arg max π (s,a)B (cid:2)min (cid:0)r(π) A, clip(r(π), 1 ϵ) A(cid:1)(cid:3) . (133) Substituting the definition of A, the objective function becomes 1 σB constant term µB σB multiplying by positive constants, we have: times the original objective minus . Since the argmax is invariant to both (i) adding/subtracting constants and (ii) arg max π (cid:2)min (cid:0)r(π) A, clip(r(π)) A(cid:1)(cid:3) = arg max (cid:2)min (cid:0)r(π) ˆA, clip(r(π)) ˆA(cid:1)(cid:3) . (134) π Therefore, batch-level normalization yields the same optimal policy as no normalization, and the drift functional DPPU ( πs) remains completely unchanged. π This conclusion extends straightforwardly to the multi-agent HAML framework: since each agents sequential update also takes the form of an argmax over an objective function, the same invariance properties hold, and the heterogeneous-agent drift functional remains unchanged under batch-level normalization. The key distinction from group-level normalization (as in GSPO) is that batch-level normalization applies constants (µB and σB) that are identical for all samples in the batch, whereas group-level normalization applies state-dependent factors δ(s) that vary across queries, breaking the drift functional properties as established in Theorem 9. 37 A(i) = ˆA(i) µB σB (cid:113) 1 B"
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group"
    ]
}