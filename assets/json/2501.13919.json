{
    "paper_title": "Temporal Preference Optimization for Long-Form Video Understanding",
    "authors": [
        "Rui Li",
        "Xiaohan Wang",
        "Yuhui Zhang",
        "Zeyu Wang",
        "Serena Yeung-Levy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite significant advancements in video large multimodal models (video-LMMs), achieving effective temporal grounding in long-form videos remains a challenge for existing models. To address this limitation, we propose Temporal Preference Optimization (TPO), a novel post-training framework designed to enhance the temporal grounding capabilities of video-LMMs through preference learning. TPO adopts a self-training approach that enables models to differentiate between well-grounded and less accurate temporal responses by leveraging curated preference datasets at two granularities: localized temporal grounding, which focuses on specific video segments, and comprehensive temporal grounding, which captures extended temporal dependencies across entire video sequences. By optimizing on these preference datasets, TPO significantly enhances temporal understanding while reducing reliance on manually annotated data. Extensive experiments on three long-form video understanding benchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO establishes itself as the leading 7B model on the Video-MME benchmark, underscoring the potential of TPO as a scalable and efficient solution for advancing temporal reasoning in long-form video understanding. Project page: https://ruili33.github.io/tpo_website."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 9 1 9 3 1 . 1 0 5 2 : r Temporal Preference Optimization for Long-Form Video Understanding Rui Li1,2 Xiaohan Wang1 Yuhui Zhang1 Zeyu Wang1 Serena Yeung-Levy1 1Stanford University 2University of Science and Technology of China Equal contribution {ruili0,xhanwang,syyeung}@stanford.edu"
        },
        {
            "title": "Abstract",
            "content": "Despite significant advancements in video large multimodal models (video-LMMs), achieving effective temporal grounding in long-form videos remains challenge for existing models. To address this limitation, we propose Temporal Preference Optimization (TPO), novel post-training framework designed to enhance the temporal grounding capabilities of video-LMMs through preference learning. TPO adopts self-training approach that enables models to differentiate between well-grounded and less accurate temporal responses by leveraging curated preference datasets at two granularities: localized temporal grounding, which focuses on specific video segments, and comprehensive temporal grounding, which captures extended temporal dependencies across entire video sequences. By optimizing on these preference datasets, TPO significantly enhances temporal understanding while reducing reliance on manually annotated data. Extensive experiments on three long-form video understanding benchmarksLongVideoBench, MLVU, and VideoMMEdemonstrate the effectiveness of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO establishes itself as the leading 7B model on the VideoMME benchmark, underscoring the potential of TPO as scalable and efficient solution for advancing temporal reasoning in long-form video understanding. Project page: https://ruili33.github.io/tpo_website. 1. Introduction Recent advancements in video large multimodal models (video-LMMs) [2, 45, 51] have marked significant step forward for generalizable video understanding. While image-based LMMs (image-LMMs) [5, 20, 39] primarily focus on spatial reasoning, video-LMMs face the additional complexity of modeling temporal dependenciesa critical aspect for capturing the dynamic nature of video content. Work done at Stanford University Current video-LMM approaches often follow twostage training paradigm: pre-training on large-scale multimodal datasets, followed by supervised fine-tuning (SFT) [6] on curated video-text instruction tuning datasets [7, 65]. This process typically involves dense captioning of video frames using cutting-edge large image-LMMs like GPT-4o [2]. These captions are then transformed into questionanswer pairs by leveraging large language models (LLMs). The hope is that this two-stage training enables the model to align video content with LLM capabilities via the text interaction, ultimately learning instruction-following behavior for both general and temporal understanding tasks. However, these methods for temporal modeling have significant challenges. First, they depend on large-scale, meticulously curated instruction-tuning datasets, which rely heavily on costly advanced models for dataset generation. these approaches lack explicit optimization for Second, temporal grounding capabilities, as the training signals primarily originate from weakly aligned video content and instruction-tuning pairs. As result, these methods often fail to capture nuanced temporal relationships, particularly in long-form videos, leading to suboptimal performance on tasks requiring fine-grained or long-context temporal grounding. Zhang et al. [64] takes step forward by utilizing Direct Preference Optimization (DPO) [44] to align the outputs of video-LMMs with those of advanced LLMs. However, this approach still evaluates preferences based solely on textual inputs, overlooking the complex temporal information inherent in the video modality. These limitations underscore the urgent need for more effective and efficient strategies to enhance temporal understanding, particularly for long-form video comprehension. In this work, we introduce Temporal Preference Optimization (TPO), self-training framework designed to enhance the temporal grounding capabilities of video-LMMs. Under the preference learning framework, TPO utilizes the contrast of the preferred and dis-preferred responses to encourage the model to prioritize preferred and temporally grounded responses. To achieve this, TPO injects two different granularities of temporal preference into the preferFigure 1. Temporal Preference Optimization (TPO) is self-improvement preference optimization technique designed to enhance video comprehension in video-LMMs by modeling temporal preferences at two granular levels: localized and comprehensive TPO. In localized TPO (upper-left), we generate queries focused on short segments, with contrastive responses that retain or exclude the target segment. For comprehensive TPO (lower-left), queries are designed for high-level understanding, using intact video versus sparse downsampled video for contrasting responses. After post-filtering, the contrast response pairs are serving as the preference dataset to train video-LMM, guiding the model to prioritize preferred responses for improved video understanding. ence dataset: localized temporal grounding and comprehensive temporal grounding. For localized temporal grounding, questions target specific video segments, with preferred responses derived from the corresponding segment and dispreferred responses from unrelated segments. Comprehensive temporal grounding, in contrast, involves broader video content, where preferred responses are generated using the full video, and dis-preferred responses from subsampled version that omits key information. By manipulating the video input according to the input query, we are able to automatically inject desired temporal preference into the generated preference dataset via simple transformations of the input video. The resulting preference dataset is then used to optimize the video-LMMs temporal grounding capabilities through Direct Preference Optimization (DPO) [44], method chosen for its flexibility and stability. This approach ensures that TPO effectively enhances temporal reasoning across both fine-grained and long-context scenarios, making it robust solution for advancing long-form video understanding tasks. We conducted extensive experiments on three challenging long-form video understanding benchmarks, and the results clearly demonstrate that TPO significantly improves the temporal understanding capabilities of video-LMMs, achieving the performance boost of 2.9%, 3.1%, and 2.5% on LongVideoBench [54], MLVU [68], and VideoMME [15] on top of state-of-the-art video-LMM LongVA7B [63]. Furthermore, LLaVA-Video-TPO achieves the strongest 7B model on the Video-MME benchmark. 2. Preliminaries Video large multimodal model (video-LMM) [32, 34, 51, 65] usually consists of large language model (LLM) πθ, visual encoder fV and multimodal projector fp. VideoLMM accepts the input of video and sequence of input text x. The input video is first tokenized into the visual token XV by video-LMMs visual encoder, which is then projected into the textual embedding space by the multimodal projector = fp(fV (V )). Such video-LMM models the probability of response y: (yx, ) = (cid:89) i=1 πθ(yiy<i, x, v) (1) In our experiments, we implement TPO based on two widely-used video-LMMs, LongVA-7B [63] and LLaVAVideo-7B [66], leveraging their publicly available opensource checkpoints. LongVA-7B is specifically designed for long-form video comprehension by adapting methodologies from long-text processing, providing robust foundation for our work. LLaVA-Video-7B is one of the opensource state-of-the-art video-LMMs. Preference learning [41, 49, 70] focuses on modeling human preferences to align model behavior with user expectations. In LLMs and image-LMMs, this involves training models to generate responses favored by users. This is typically achieved by collecting human feedback [41, 70] on pairs of model-generated outputs and learning function that predicts which output is preferred. Formally, given an input and two outputs y+ (preferred) and (dispreferred), the model aims to satisfy: πθ(y+x) > πθ(yx) (2) where πθ(yx) is the models probability of generating output given input with parameters θ. [44] Direct Preference Optimization (DPO) is methodology that directly integrates human preference data into the optimization of model parameters. Compared to Proximal Policy Optimization (PPO) [41, 49, 70], another popular preference learning implementation, DPO eliminates the need for explicit reward models or complex reinforcement learning algorithms. By leveraging human preference data as guiding signal during optimization, DPO enhances the models ability to generate outputs that are better aligned with human values and expectations. grounded response, and is the dis-preferred response. In this context, r+ is designed to be more aligned with the temporal context of the video-query pair (V, q) than r. What constitutes good temporal grounding for videoLMMs? Effective temporal grounding in videos can be intuitively understood through two key scenarios: 1) Localized Temporal Grounding (Sec. 3.2): Given question and long video, the answer often pertains primarily to small segment of the video. video-LMM with strong temporal grounding capabilities should be able to localize and focus on this short duration, generating an accurate response without being distracted by irrelevant parts of the video. 2) Comprehensive Temporal Grounding (Sec. 3.3): When question involves multiple keyframes across video, the video-LMM is expected to identify and ground all relevant keyframes, ensuring no crucial information is overlooked. Motivated by these two scenarios, we propose multigranularity temporal preference data generation pipeline that systematically captures these aspects of temporal grounding. This pipeline is designed to create temporal preference data that emphasizes both localized and comprehensive temporal grounding, enabling video-LMMs to excel across varying levels of temporal complexity. 3. Temporal Preference Optimization 3.2. Localized Temporal Preference Data Unlike approaches that align image-LMMs outputs with human preferences, our work focuses on aligning model outputs with intrinsic temporal preferences. Specifically, given video input, the model is expected to produce responses that are better aligned with temporally grounded video content. To address this challenge, as illustrated in Figure 1, we introduce Temporal Preference Optimization (TPO), framework designed to encourage the model to prioritize outputs that are more temporally grounded. To minimize reliance on extensive human annotations or advanced teacher models, we develop scalable pipeline for generating temporal preference data by bootstrapping the models existing capabilities (Sec. 3.1). Using this pipeline, we automatically generate paired datasets consisting of temporally preferred and dispreferred examples. These pairs are then utilized for preference optimization (Sec. 3.5), enabling the model to learn to differentiate and favor temporally grounded outputs while preserving the capabilities of the original pre-trained model. Using this self-training approach, TPO effectively enhances temporal reasoning while maintaining scalability and practicality. 3.1. Temporal Preference Data Curation Overview Formally, temporal preference dataset is constructed as collection of tuples (V, q, r+, r), where denotes video, represents query, r+ is the preferred temporally Query Generation The queries are specifically crafted to focus on subsegment of the video, ensuring they cannot be accurately answered using only general knowledge of the entire video. To generate such queries, we first sample subsegment from the full video. Next, we sparsely sample frames from the selected subsegment and use an image-based vision-language model (CogVLM2 [20]) to generate captions for each frame. These captions serve as the basis for creating targeted questions. To ensure diversity and relevance, we design multiple question types and feed structured question-generation prompt, along with the generated captions, into LLM (GPT-4o-mini). This process produces set of candidate questions specifically tailored to the sampled subsegment, facilitating the creation of localized temporal preference data. Preferred Response Generation The preferred response in the curated dataset is expected to be of high quality and closely grounded in the corresponding temporal content. Current video-LMMs exhibit strong performance in understanding short video clips, making them well-suited to generate reliable responses when the video clip is both concise and directly relevant to the given query. To generate the preferred response, we provide the model with the same video segment used to curate the query, along with the query itself. By ensuring that the video clip is both brief and highly relevant to the query, we create conditions that maximize the likelihood of the model producing high-quality, temporally grounded response. This process ensures that the preferred response aligns with the ideal characteristics for localized temporal grounding in video-LMMs. to the query, it often produces suboptimal responses based on the partial content. This approach simulates scenarios where the video-LMM fails to ground all critical video content necessary to answer comprehensive questions. Dis-Preferred Response Generation The dis-preferred response in the preference dataset reflects the type of output the model aims to discourage. Specifically, it highlights responses where the model fails to localize relevant information in the video, thereby exposing shortcomings in temporal grounding. To generate the dis-preferred response, we clip out the query-related subsegment of the video and provide the model with the remaining portion of the video along with the query. Unlike the preferred response, where the model receives only the specific segment used to curate the query, this setup simulates an extreme scenario where the model entirely misses the relevant content. Consequently, the model must rely solely on the global context, high-possible producing responses with errors in temporal reasoningprecisely the kind of outputs our framework seeks to mitigate. This process ensures clear contrast between preferred and dis-preferred responses, guiding the model to improve its temporal grounding capabilities. 3.3. Comprehensive Temporal Preference Data Query Generation Similar to the process used for localized temporal query generation, we employ pre-trained image-LMM to generate captions. However, in this case, frames are sparsely sampled from the entire video to ensure broader coverage of the video content. These captions are then input into an LLM, which generates queries designed to capture comprehensive temporal relationships across multiple frames. Preferred Response Generation To generate the preferred response for the query in comprehensive TPO, we input the same sampled frames used during query generation into the video-LMM, ensuring an unaltered and exhaustive source of visual information. is set to maximum of 32, striking balance between information richness and generation precision. This complete visual input allows the video-LMM to effectively identify salient keyframes and perform reasoning across the entire temporal sequence of the video, producing comprehensive and accurate responses. Dis-Preferred Response Generation For the comprehensive task, we divide the video into subsegments and uniformly drop specific subsegments to create modified version of the video with incomplete information. This truncated video, along with the comprehensive query, is then fed into the video-LMM to generate dis-preferred responses. Since the model lacks access to the full information relevant 3.4. Preference Data Post Filtering After generating the queries and preference data pairs, some incorrect pairs may exist, such as cases where the dispreferred response is equal to or better than the preferred response, or where the preferred response is incorrect in relation to the given query. To address this, we implement post-filtering pipeline to enhance data quality and reduce noise. Using the LLM (GPT-4o-mini), we provide the captions along with their corresponding queries and preference data pairs, instructing the model to filter out pairs that meet these conditions. This post-filtering step allows us to eliminate corner cases that could introduce noise into the preference dataset, resulting in more refined and higher-quality dataset that better supports the optimization process. 3.5. Training Objective The generated preference dataset is subsequently leveraged to optimize the video-LMMs temporal preferences using Direct Preference Optimization (DPO) [44], chosen for its flexibility and stability in handling preference-based learning tasks. Given the preference dataset (V, q, r+, r) and the video-LMM πθ, the DPO loss function is defined as: LDP O(πθ; πref ) = E(V,q,r+,r)D (cid:20) logσ(β(log πθ(r+V, q) πref (r+V, q) log πθ(rV, q) πref (rV, q) (cid:21) )) (3) where σ is the sigmoid function. This objective drives the model to assign higher probabilities to preferred outputs, aligning its behavior more closely with human judgments, while preventing the model from deviating too much from its pretrained distribution. To better align the model with the preferred responses, we incorporate supervised fine-tuning objective into the DPO training framework. This combined objective is controlled by the hyperparameter α, following [8, 10, 13]. LSF (πθ) = E(V,q,r+,r)D log πθ(r+V, q) (4) L(πθ; πref ) = LDP + αLSF (5) 4. Experiments 4.1. Experimental Settings Evaluation Benchmarks We evaluate TPO and baselines on three widely recognized benchmarks in multimodal video understanding, with particular attention to long-form video comprehension. Model LongVideoBench MLVU (M-avg) LongVA-7B [63] + SFTSelf + SFTLLM + Hound-DPO [63, 64] LongVA-TPO (ours) 51.3 52.7 53.1 52. 54.2 58.8 58.9 59.9 59.1 61.7 Video-MME Short Medium Long Average 61.1/61.6 62.6/67.7 63.7/64.9 62.2/65.8 50.4/53.6 52.4/52.7 52.6/54.3 52.4/54.8 46.2/47.6 46.8/47.4 46.3/47.9 46.1/46.3 52.6/54.3 53.9/55.9 54.2/55.7 53.6/55. 63.1/66.6 54.8/55.3 47.4/47.9 55.1/56.6 Table 1. Results of LongVA-TPO on LongVideoBench [54], MLVU [68] and Video-MME [15] benchmarks compared to 3 baseline methods mentioned in 4.2. The Video-MME results are presented in the format w/o subs / w/ subs. The results for LongVA and LongVA+Hound-DPO are based on publicly available checkpoints, while the other results are evaluated using our trained model. Model GPT-4o [2] Video-LLaVA [31] LLaVA-1.5 [33] PLLaVA [55] Qwen-VL-Max [5] ShareGPT4Video [7] InternVL-Chat-V1.5 [11] VideoChat2 [28] LongLLaVA [59] Video-CCAM [14] NVILA [37] Qwen2-VL [51] Apollo [72] LongVA-7B [63] LLaVA-Video-7B [66] LongVA-TPO (ours) LLaVA-Video-TPO (ours) Size LongVideoBench MLVU (M-avg) - 7B 7B 7B - 8B 20B 7B 7B 14B 7B 7B 7B 7B 7B 7B 7B 66.7 39.1 40.3 40.2 - 39.7 51.2 39.3 - - 57.7 55.6 58.5 51.3 58. 54.2 60.1 64.6 47.3 - - 42.2 46.4 50.4 47.9 56.3 63.1 70.1 - 70.9 58.8 70.8 61.7 71.1 Video-MME Short Medium Long Average 80.0/82.8 70.3/76. 65.3/72.1 71.9/77.2 45.3/46.1 - - 55.8/57.6 48.3/53.6 50.7/52.4 48.3/52.8 61.9/66.2 62.2/66.0 75.7/77.6 - - 38.0/40.7 - - 49.2/48.9 36.3/39.3 60.2/61.7 37.0/39.4 51.4/54.7 50.6/56.3 62.2/69.0 - - 36.2/38.1 - - 48.9/47.0 35.0/37.9 46.4/49.1 33.2/39.2 45.4/50.3 46.7/49.9 54.8/63.3 - - 39.9/41.6 - - 51.3/51.2 39.9/43.6 45.6/46.6 39.5/43.8 52.9/57.1 53.2/57.4 64.2/70.0 63.3/69.0 61.3/63. 61.1/61.6 - 50.4/53.6 - 46.2/47.6 - 52.6/54.3 63.3/69.7 63.1/66.6 76.8/78.7 54.8/55.3 64.6/69. 47.4/47.9 55.4/66.4 55.1/56.6 65.6/71.5 Table 2. Results on LongVideoBench [54], MLVU [68] and Video-MME [15] compared with state-of-the-art models. The Video-MME results are presented in the format w/o subs / w/ subs. Video-MME [15] offers comprehensive multi-modal evaluation across diverse video lengths, spanning from 11 seconds to 1 hour. 2. LLaVA-Video-TPO: optimized based on LLaVA-Video7B [66], the current state-of-the-art 7B video-LMM. Without other states, our ablation study and analysis utiLongVideoBench [54] emphasizes reasoning tasks lize LongVA-TPO by default. within extended video contexts. MLVU [68] supports multitask evaluation specifically designed for long-form video understanding. Models We test the effectiveness of TPO on two popular video-LMMs, LongVA-7B [63] and LLaVA-Video-7B [66], deriving the following models: 1. LongVA-TPO: optimized based on LongVA-7B [63], capable video-LMM with the long-context video understanding capability transferred from language. Implementation Details For the video source of preference dataset generation, we manually curated 200 keywords, which we used to retrieve 8k videos from the internet to curate diverse and comprehensive dataset. From these crawled videos, we created 10k preference data pairs for LongVA-TPO using our established pipeline. For LLaVAVideo-TPO, we employ subset of the original LLaVAVideo-178K dataset, which was used for supervised finetuning (SFT), to generate TPO data, resulting in total of 10K preference data pairs. The model is trained on 8 Nvidia A100 80GB GPUs, with batch size of 1. For the preference optimization on LongVA, we set the KL-divergence weight β = 0.3 and the SFT loss weight α = 0.5, while for LLaVA-Video, we set the KL-divergence weight β = 0.2 and the SFT loss weight α = 1. We train the model on our curated data for 1 epoch. It takes about 4 hours for TPO to perform on LongVA-7B with learning rate of 4e6 and also about 4 hours for LLaVA-Video-7B with learning rate of 3e7. During data preparation, we employ the GPT-4o-mini language model (text-only input) for question curation and post-filtering. This choice balances cost-effectiveness with efficiency, facilitating streamlined and scalable data processing workflow. 4.2. Results We conducted comprehensive experiments across three established datasets to rigorously assess the effectiveness of TPO in long-form video understanding tasks. We first compare TPO with three different training strategies: SFTSelf: Supervised fine-tuning using the self-generated data. For fair comparison, we utilize the same preferred response in our curated preference dataset to optimize LongVA. SFTLLM: Supervised fine-tuning using the LLMgenerated data. Following the commonly used data curation pipeline [7, 65]. We employ LLM (GPT-4o-mini) to generate responses given the query and the video captions, which are subsequently used to perform supervised fine-tuning on LongVA. We use the same video data as TPO for fair comparison. Hound-DPO [64] is previous method that applies Direct Preference Optimization (DPO) [44] on video-LMMs. Their approach leverages ChatGPT [2] to generate ratings for preference data, resulting in dataset of 17k samples. In contrast, TPO employs smaller preference dataset generated through self-generation pipeline, offering more streamlined alternative. results, presented in TaThe primary experimental ble 1, compare TPO against the baseline methods on LongVA. The results consistently indicate that LongVATPO achieves superior performance, with improvements of 2.9%, 3.1%, and 2.5% on LongVideoBench [54], MLVU [68], and Video-MME [15], respectively. These findings underscore TPOs capacity to enhance the long-form video understanding capabilities of pre-trained video-LMM. Compared to SFTSelf, LongVA-TPO achieves consistent performance gain of 1.2% to 2.8% by utilizing carefully designed temporal dis-preferred response to contrast with the preferred response. Furthermore, LongVA-TPO outperforms SFTLLM, demonstrating the effectiveness and stability of our self-training paradigm. When compared Figure 2. The performance of LongVA-TPO and LongVA on MLVU with different input lengths. LongVA-TPO consistently shows performance improvements with longer inputs, whereas LongVA experiences performance degradation when the input exceeds 64 frames. to Hound-DPO [64], LongVA-TPO achieves significant performance improvement by injecting temporal preference priors into the preference dataset. However, LongVA-TPO underperforms SFT methods on the Video-MME-short subset, which is expected since LongVA-TPO primarily focuses on optimizing long-form video capabilities. In addition, the comparisons between TPO and current state-of-the-art video-LMMs are presented in Table 2. With the introduction of TPO, both the LongVA-TPO and LLaVA-Video-TPO model significantly outperform their corresponding baselines 2.5% and 2.3% on the videoMME benchmark, demonstrating the efficacy of our TPO pipeline. After TPO on LLaVA-Video-7B, our LLaVAVideo-TPO model outperforms all 16 baseline models in the table, including the concurrent work, NVILA [37], as well as several 14B and 20B models, achieving state-ofthe-art results on long-form video understanding. The original LongVA model performed worse than Video-CCAM [14] and LongLLaVA [59] on the Video-MME benchmark. However, after incorporating TPO, it successfully outperformed these competitive baselines on Video-MME. Overall, LLaVA-Video-TPO achieves the strongest 7B model on the Video-MME benchmark, setting new state-of-the-art performance on video comprehension. 4.3. Ablation Study 4.3.1. Performance with Different Input Frame Count We evaluate the performance of both our LongVA-TPO model and the original LongVA model across varying input lengths, ranging from 16 to 128 frames, as shown in Figure 2. The results indicate that the LongVA model experiences performance degradation with 128 frames compared to 64 frames. In contrast, our LongVA-TPO model consisFigure 3. Comparison of our LongVA-TPO model and the original LongVA model on the needle-in-a-haystack task. tently benefits from longer input sequences, leveraging the additional information effectively. This demonstrates the LongVA-TPO models robustness in handling extended inputs and its capacity to localize relevant information within long sequences, further validating the efficacy of our TPO approach. 4.3.2. Effect of Dataset Sizes Model LongVideoBench MLVU VideoMME LongVA TPO2k TPO5k TPO10k 51.3 52.5 53.7 54. 58.8 57.8 59.5 61.7 52.6 52.8 53.6 55.1 Table 3. Results of LongVA-TPO (TPO) trained on different data scales. TPO achieves consistent performance improvements as the data scale increases. The performance on the VideoMME benchmark is evaluated without subtitles. Scalability is critical metric in the evaluation of algorithms in the era of large-scale models, reflecting an algorithms performance as data volume expands. To examine the scalability of the TPO algorithm, we conduct experiments with LongVA-TPO across incremental sizes of 2k, 5k, and 10k (the complete preference dataset). The results, presented in Table 3, highlight the impact of dataset scaling. Our findings reveal that LongVA-TPO demonstrates superior scalability, achieving consistent performance gains with increasing dataset size across all three benchmarks. This pattern highlights TPOs robustness and adaptability in larger data contexts, suggesting its potential to deliver enhanced results when scaled to larger datasets. 4.3.3. Effect of different data granularity In TPO, we design two levels of data granularity to model temporal information at different scales, termed localized TPO and comprehensive TPO. Localized TPO captures local temporal information, while comprehensive TPO models comprehensive, overarching temporal information. To evaluate the individual capabilities, limitations, and necessity of combining localized and comprehensive TPO, we conducted an ablation study. In this study, we maintained the same overall dataset size as the full TPO and trained two separate models based on the LongVA model: one using only Localized TPO data and the other using only Comprehensive TPO data. The performance comparison between these two models and the full TPO model is shown in Table 4. The performance of localized TPO and comprehensive TPO on the LongVideoBench [54] and VideoMME [15] datasets compellingly demonstrates their effectiveness. However, their limited improvements on the MLVU [68] dataset highlight inherent constraints. When combined, the two methods not only achieve further gains on the LongVideoBench and VideoMME datasets but also deliver substantial 3% improvement on the MLVU dataset, leading to notable advancements. These findings strongly support the complementary nature of localized and comprehensive TPO, emphasizing the critical importance of constructing multi-granularity preference datasets to fully exploit temporal modeling capabilities. Model LongVideoBench MLVU VideoMME LongVA TPOlocalized TPOcomprehensive TPOfull 51.3 53.5 53.4 54. 58.8 58.7 58.5 61.7 52.6 54.0 53.8 55.1 Table 4. Ablation study of different data granularities. 4.3.4. Needle-in-a-Haystack The Needle-in-a-Haystack task refers to the challenge of identifying rare or specific event within large volume of unstructured video data. Building on the work of Zhang et al. [63], we frame the task using image-based question answering (QA), where images are embedded within 3hour-long video, and the model is tasked with answering Figure 4. Qualitative comparison between LongVA-TPO model and LongVA on two videos from VideoMME benchmark. Our LongVATPO model demonstrates superior generation quality on both comprehensive and localized questions. the corresponding image QA questions. In our experiments, we adopt the same five image QAs as Zhang et al. [63], and present the results in Figure 3. While LongVA, optimized for long-context processing, significantly outperforms LLaVA-NeXT-Video [65] on the Needle-in-aHaystack task (refer to Figure 4 in [63]), our LongVA-TPO model still demonstrates superior performance, achieving even better results in long-context retrieval. 4.4. Qualitative Analysis The qualitative analysis of our LongVA-TPO model and the LongVA model on two videos from the Video-MME benchmark is provided in Figure 4. In the first example, which involves temporal localization and OCR task, our LongVATPO model demonstrates superior performance by accurately localizing the relevant information within the video and providing the correct answer to the OCR question. In the second example, video discussing the Moons formation, LongVA misinterprets the video content by relating it to the Earths formation. In contrast, our LongVA-TPO model successfully comprehends and captures the key details of the videos content. 5. Related Work Video Large Multimodal Models Recently, significant efforts have been devoted to extending the capabilities of large language models (LLMs) [2, 45] into the visual domain, developing various video large multimodal models (video-LMMs), including both proprietary [2, 45] and open-source models [1, 12, 16, 23, 26, 31, 34, 47, 51, 57]. Early approaches focused on curating high-quality videotext instruction-tuning datasets [7, 33, 42, 65, 66], to equip LLMs with video comprehension capabilities. However, these datasets often rely on synthetic data derived from video captions, which limits their effectiveness in capturing visual-temporal dynamics. Other studies have focused on extending pretrained video-LMMs to handle longer video contexts [3537, 48, 59, 63], while multimodal interleaved datasets [27, 32] and mixed training strategies [25, 72] have been explored to enhance video-LMM performance. Despite these advancements, the post-training stage for videoLMMs remains underexplored. Recent efforts like LLaVAHound [64] utilize ChatGPT to rank model outputs and create preference datasets but fall short in leveraging the temporal information inherent in video data. In contrast, our work pioneers post-training strategies that explicitly incorporate temporal priors to address these limitations. Temporal grounding is crucial for comprehending the video modality, particularly in long-form videos. Various efforts have been made to enhance temporal localization, including dense captioning [52, 56, 58], highlight detection [24, 40], and temporal video grounding [17, 60], among others. Recent advancements have introduced temporalaware designs in video large multimodal models (videoLMMs) [9, 21, 29, 46] and have explored the development of agentic systems with temporal grounding capabilities [53]. Unlike these existing approaches, our work focuses on temporal preference optimization during the posttraining stage, offering complementary enhancement to current methods. Proximal Policy Optimization (PPO) [41, 49, 70] and Direct Preference Optimization (DPO) [44] are two widely used implementations of Reinforcement Learning from Human Feedback (RLHF) [41, 70], serving as key algorithms In the imagein preference learning and post-training. LMM domain, Sun et al. [50] enhanced model visual capabilities by incorporating image captions into the reward modeling process within RLHF. Similarly, Ahn et al. [4] fine-tuned multimodal foundation models using Reinforcement Learning from AI Feedback (RLAIF). Other approaches, such as those proposed by Li et al. [30] and Gunjal et al. [18], directly distilled GPT-4Vs preferences from sampled model responses. notable strategy involves using text as an intermediate modality, leveraging captions and other descriptive information to extract and distill large language models (LLMs) preferences for both images [67] and videos [64]. Furthermore, Pi et al. [43], Zhou et al. [69], and Deng et al. [13] advanced preference learning in image-LMMs by curating preference data through image input manipulation. Self-Training in Foundation Models To address the challenge of scaling up annotated datasets, several works have explored self-improvement and self-training methods [19, 22]. Zelikman et al. [61] introduced Self-Taught Reasoners (Star), which leverage generated chain-of-thought rationales to enhance LLMs complex reasoning capabilities. In the image domain, BPO [43] STIC [13] and POVID [69] improve image-LMMs responses by incorporating viIn the video domain, Video-STaR [71] uses sual priors. existing labels as weak supervision to guide model selfimprovement while Ahn et al. [3] explores iterative selfimprovement in preference optimization. 6. Conclusion We introduced Temporal Preference Optimization (TPO), scalable post-training framework that enhances temporal grounding in video-LMMs. By leveraging preference learning at both localized and comprehensive levels, TPO effectively captures the intricate temporal dependencies required for long-form video understanding. Extensive experiments across three challenging benchmarksLongVideoBench, MLVU, and Video-MMEdemonstrated TPOs robust improvements, achieving state-of-the-art performance. By integrating multi-granularity temporal preferences, TPO addresses diverse challenges in long-form video understanding, offering robust and efficient solution for advancing temporal reasoning in multimodal tasks. One future direction is scaling the preference data to improve coverage and diversity, thereby potentially enhancing TPOs generalizability. Additionally, while this work focuses on LongVA7B and LLaVA-Video-7B as representative Video-LMMs, applying TPO to broader range and larger scale of VideoLMMs would provide insights into its adaptability and performance across different architectures."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by the National Science Foundation under Grant No. 2026498. Serena Yeung-Levy is Chan Zuckerberg Biohub San Francisco Investigator."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 8 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 5, 6, 8 [3] Daechul Ahn, Yura Choi, San Kim, Youngjae Yu, Dongyeop Kang, and Jonghyun Choi. i-srt: Aligning large multimodal models for videos by iterative self-retrospective judgment. arXiv preprint arXiv:2406.11280, 2024. 9 [4] Daechul Ahn, Yura Choi, Youngjae Yu, Dongyeop Kang, and Jonghyun Choi. Tuning large multimodal models for videos using reinforcement learning from ai feedback. arXiv preprint arXiv:2402.03746, 2024. 9 [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 1, 5 [6] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. [7] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang. Sharegpt4video: Improving video understandarXiv preprint ing and generation with better captions. arXiv:2406.04325, 2024. 1, 5, 6, 8 [8] Shuo Chen, Gang Niu, Chen Gong, Jun Li, Jian Yang, and Masashi Sugiyama. Large-margin contrastive learning with In International Conferdistance polarization regularizer. ence on Machine Learning, pages 16731683. PMLR, 2021. 4 [9] Shimin Chen, Xiaohan Lan, Yitian Yuan, Zequn Jie, and Lin Ma. Timemarker: versatile video-llm for long and short video understanding with superior temporal localization ability. arXiv preprint arXiv:2411.18211, 2024. 9 [10] Zixiang Chen, Yihe Deng, Yuanzhi Li, and Quanquan Gu. Understanding transferable representation learning and zeroshot transfer in clip. arXiv preprint arXiv:2310.00927, 2023. 4 [11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Internvl: Scaling up vision foundation models and Dai. aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 5 [12] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 8 [13] Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, Quanquan Gu, James Zou, Kai-Wei Chang, and Wei Wang. Enhancing large vision language models with self-training on image comprehension. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 4, [14] Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. Video-ccam: Enhancing video-language understanding with causal cross-attention masks for short and long videos. arXiv preprint arXiv:2408.14023, 2024. 5, 6 [15] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, 5, 6, 7 [16] Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, et al. Vita-1.5: Towards gpt-4o level arXiv preprint real-time vision and speech interaction. arXiv:2501.01957, 2025. 8 [17] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pages 52675275, 2017. 9 [18] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1813518143, 2024. 9 [19] Namgyu Ho, Laura Schmid, and Se-Young Yun. Large arXiv preprint language models are reasoning teachers. arXiv:2212.10071, 2022. [20] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language models for image and video understanding. arXiv:2408.16500, 2024. 1, 3 arXiv preprint [21] De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. Lita: Language instructed temporal-localization assistant. In ECCV, 2024. 9 [22] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022. 9 [23] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, 2024. [24] Jie Lei, Tamara Berg, and Mohit Bansal. Detecting moments and highlights in videos via natural language queries. Advances in Neural Information Processing Systems, 34: 1184611858, 2021. 9 [25] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 8 [26] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-ofexperts model. arXiv preprint arXiv:2410.05993, 2024. 8 [27] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next: Tackling multi-image, video, and 3d in large multimodal models, 2024. 8 [28] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 5 [29] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. [30] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong. Silkie: Preference distillation for large visual language models. arXiv preprint arXiv:2312.10665, 2023. 9 [31] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represenarXiv preprint tation by alignment before projection. arXiv:2311.10122, 2023. 5, 8 [32] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. 2, 8 [33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 5, 8 [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 2, [35] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. Kangaroo: powerful video-language model supporting long-context video input. arXiv preprint arXiv:2408.15542, 2024. 8 [36] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. [37] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, An-Chieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna, Daguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, and Yao Lu. Nvila: Efficient frontier visual language models, 2024. 5, 6, 8 [38] Ilya Loshchilov and Frank Hutter. Sgdr: StochasarXiv preprint tic gradient descent with warm restarts. arXiv:1608.03983, 2016. 1 [39] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world visionlanguage understanding, 2024. 1 [40] WonJun Moon, Sangeek Hyun, SangUk Park, Dongchan Park, and Jae-Pil Heo. Query-dependent video representaIn Protion for moment retrieval and highlight detection. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2302323033, 2023. 9 [41] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 3, 9 [42] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122, 2023. 8 [43] Renjie Pi, Tianyang Han, Wei Xiong, Jipeng Zhang, Runtao Liu, Rui Pan, and Tong Zhang. Strengthening multimodal large language model with bootstrapped preference optimization. arXiv preprint arXiv:2403.08730, 2024. 9 [44] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. 1, 2, 3, 4, 6, 9 [45] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 1, 8 [46] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1431314323, 2024. 9 [47] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo J. Kim, Bilge Soran, Raghuraman Krishnamoorthi, Mohamed Elhoseiny, and Vikas Chandra. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv:2410.17434, 2024. 8 [48] Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie Zhou, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. arXiv preprint arXiv:2409.14485, 2024. 8 [49] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:30083021, 2020. 3, 9 [50] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. 9 [51] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 2, 5, [52] Teng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, Ran Cheng, and Ping Luo. End-to-end dense video captioning In Proceedings of the IEEE/CVF with parallel decoding. international conference on computer vision, pages 6847 6857, 2021. 9 [53] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena YeungLevy. Videoagent: Long-form video understanding with large language model as agent. In European Conference on Computer Vision, pages 5876. Springer, 2025. 9 [54] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding, 2024. 2, 5, 6, 7 [55] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava : Parameter-free llava extension from images to videos for video dense captioning, 2024. 5 [56] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of visual language model for dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1071410726, 2023. 9 [57] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [58] Serena Yeung, Olga Russakovsky, Ning Jin, Mykhaylo Andriluka, Greg Mori, and Li Fei-Fei. Every moment counts: fang Wang, Felix Juefei-Xu, Ning Zhang, Serena YeungLevy, and Xide Xia. Apollo: An exploration of video unarXiv preprint derstanding in large multimodal models. arXiv:2412.10360, 2024. 5, 8 Dense detailed labeling of actions in complex videos. International Journal of Computer Vision, 126:375389, 2018. 9 aws- [59] Yin Song and Chen Wu and Eden Duthie. prototyping/long-llava-qwen2-7b, 2024. 5, 6, 8 [60] Yitian Yuan, Lin Ma, Jingwen Wang, Wei Liu, and Wenwu Zhu. Semantic conditioned dynamic modulation for temporal sentence grounding in videos. Advances in Neural Information Processing Systems, 32, 2019. 9 [61] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. [62] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmmseval: Reality check on the evaluation of large multimodal models, 2024. 1 [63] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 2, 5, 7, 8, 1 [64] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language model reward. arXiv preprint arXiv:2404.01258, 2024. 1, 5, 6, 8, 9 [65] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llavanext: strong zero-shot video understanding model, 2024. 1, 2, 6, 8 [66] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. 2, 5, 8, 1 [67] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. arXiv preprint arXiv:2311.16839, 2023. 9 [68] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 2, 5, 6, 7 [69] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large lanarXiv preprint guage models via preference fine-tuning. arXiv:2402.11411, 2024. [70] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 3, 9 [71] Orr Zohar, Xiaohan Wang, Yonatan Bitton, Idan Szpektor, and Serena Yeung-Levy. Video-star: Self-training enables video instruction tuning with any supervision. arXiv preprint arXiv:2407.06189, 2024. 9 [72] Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, XiaoTemporal Preference Optimization for Long-Form Video Understanding"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Reproducibility Statement To ensure reproducibility, we have released the full scripts and source code of the TPO pipeline, accompanied by the curated preference dataset, which includes videos, associated queries, and corresponding preference responses, as well as the trained model weights. This release will include detailed implementations of all steps involved in the preference dataset curation and the preference optimization process. By providing these resources, we aim to facilitate the replication of our results and support further advancements in this area of research. B. Appendix overview This document provides more details of our approach and additional experimental results, organized as follows: More Implementation Details of TPO. More Details of the Preference Dataset Curation. More Examples in the Preference Dataset. Additional Ablation Study on Data Mixture. More Qualitative Examples. and max rames num = 128 for the rest of the benchmarks. D. Preference Dataset Curation We manually curated set of 200 keywords assisted with GPT-4o-mini [2], which were utilized to retrieve 8,000 videos from the internet, forming diverse and comprehensive dataset. Using this dataset, we further developed 10,000 queries paired with their corresponding preference responses, covering broad range of tasks. The detailed prompts for preference dataset curation are provided in Figure 7 and Figure 8. For LLaVA-Video, we sampled subset of 10k QA pairs from the LLaVA-Video-178k dataset for comprehensive TPO. The distribution of video lengths in our collected dataset is presented in Figure 5. The distribution of tasks is illustrated in Figure 6, encompassing Temporal Reasoning (8.7%), Action Reasoning (12.4%), Causal Reasoning (11.1%), Information Extraction (18.0%), Descriptive Questions (12.8%), Summarization (7.5%), Object Reasoning (14.9%), and Spatial Reasoning (13.5%). C. Implementation Details E. Preference Dataset Examples We conduct Temporal Preference Optimization (TPO) on LongVA [63] and LLaVA-Video [66], two state-of-the-art video-LMMs. The two TPO models are trained using 8 Nvidia A100 80GB GPUs, with batch size of 1. For preference optimization, we set the KL-divergence weight (β) to 0.3 and the supervised fine-tuning (SFT) loss weight (α) to 0.5 for LongVA-TPO and we set the KL-divergence weight (β) to 0.2 and the supervised fine-tuning (SFT) loss weight (α) to 1 for LLaVA-Video-TPO. We employ full fine-tuning for both the multimodal projector and the language model while keeping the visual encoder frozen, using learning rate of 4 106 for LongVA-TPO and 3 107 for LLaVA-Video-TPO. The training is performed on curated dataset of 10k samples for one epoch for LongVATPO and 10k samples for one epoch for LLaVA-VideoTPO. cosine learning rate scheduler with warm-up ratio of 0.1 is utilized [38]. The entire TPO fine-tuning process takes approximately 4 hours on both two models. For evaluation, we adopt the protocol outlined by LongVA [63] and LLaVA-Video [66], leveraging the official lmms-eval repository [62] to assess our models performance on three benchmarks. For LongVA-TPO, we set the parameter max rames num = 128 across all three benchmarks. For LLaVA-Video-TPO, we set the parameter max rames num = 96 for the Video-MME benchmark We provide three additional examples of preference datasets, as illustrated in Figure 9. For instance, in Example (a), the task involves an OCR-based query aimed at retrieving the quote located beneath mural. The dispreferred response incorrectly identifies the relevant frame, failing to locate the quote below the mural and instead referencing another frame containing the phrase Forward, Warrior. In contrast, the preferred response accurately identifies the corresponding frame based on the question. This is achieved by leveraging the highly relevant sub-video segment provided to the video-LMM, enabling the correct extraction of both the quote and its attribution. For Example (b), the task involves summarizing information by identifying the four levels depicted in pyramid diagram. The dis-preferred response, based on irrelevant video clips, provides incorrect names and an incorrect order for the four levels. In contrast, the preferred response accurately identifies both the correct names and the proper order of the four levels, demonstrating better understanding of the context and alignment with the video content. For Example (c), the task involves high-level descriptive query requiring summary of the exercise routine depicted in the video. The dis-preferred response, relying only on down-sampled frames, omits significant key information and provides an incomplete summary. In contrast, Figure 5. The distribution of lengths for 8K crawled videos. Figure 6. The distribution of question types for 10K curated preference dataset for LongVA-TPO. Localized/Comprehensive TPO Ratio LongVideoBench MLVU VideoMME 10:0 8:2 5:5 (Our final TPO model) 2:8 0:10 53.5 53.8 54.2 53.4 53.4 58.7 59.9 61.7 59.1 58.5 54.0 54.0 55.1 54.2 53. Table 5. Results of TPO trained on different data mix ratios for localized TPO and comprehensive TPO data based on the LongVA model. the preferred response accurately summarizes the entire exercise routine, offering both detailed and correctly ordered information, thereby demonstrating comprehensive understanding of the video content. F. Additional Ablation Study Different Data Mix Ratio We assessed the performance of our TPO model under various data mixing ratios for localized TPO and comprehensive TPO data based on the LongVA [63] model, keeping the total data size constant at 10k. The evaluated ratios included 10:0, 8:2, 5:5, 2:8, and 0:10. The experimental results, summarized in Table 5, clearly demonstrate that the model achieves optimal performance on general video understanding tasks when an equal proportion of localized TPO and comprehensive TPO data is used. This balanced data distribution effectively integrates fine-grained local temporal information with broader high-level temporal context, leading to superior overall model performance. G. Qualitative Analysis Examples We provide three additional qualitative analysis examples from the Video-MME dataset [15], as illustrated in Figure 10. Example (a) involves an information extraction and optical character recognition (OCR) task, where the question asks for the total number of measurements involved in chip manufacturing. The original LongVA model failed to accurately locate the relevant frame containing the necessary information, resulting in an incorrect response. In contrast, our LongVA-TPO model, enhanced through temporal preference optimization, successfully identified the pertinent frame within the lengthy input and provided the correct answer to the question. Example (b) involves high-level video understanding and information extraction task, where the question asks for the main topic introduced in the video. The original LongVA model failed to capture the overarching theme, instead responding with an unrelated term, Criminal Trial, mentioned elsewhere in the video. In contrast, our LongVATPO model effectively identified the videos central theme and accurately provided the correct topic introduced in the content. Example (c) involves an object reasoning task, where the question asks what the three curved lines extending from the bottom upward symbolize. The original LongVA model failed to interpret the representation accurately, erroneously stating that the lines represent three stages of the water cycle, which was hallucination. In contrast, our LongVATPO model successfully understood the symbolic meaning of the three curved lines as representing evaporation, providing correct and detailed response. <Video Caption> Using the caption of video, create question-answer pair that focuses on <Task Prompt>. Please generate question tailored to the given caption. If its inappropriate to generate such question, please (cid:44) output None. Output format: Q: <question> A: <answer> Figure 7. Detailed prompt for the query generation given the video captions. <Video Caption> Question: <Query> Answer1: <Preferred Answer> Answer2: <Dis-Preferred Answer> Task1: You are given question, the golden answer and related captions. Is answer1 better than answer2? Please (cid:44) answer with Yes or No or Equally. Task2: Please check if this question and Answer1 contradicts to any part of the golden caption or this question might (cid:44) have another answer different from the given answer. Please respond with Yes or No. Task3: Is the Answer1 is correct given the question and golden caption? Please respond with Yes or No. Figure 8. Detailed prompt for the post-filtering process for the preference data pairs. Figure 9. Examples from the preference dataset. Figure 10. Additional qualitative examples from Video-MME [15]."
        }
    ],
    "affiliations": [
        "Stanford University",
        "University of Science and Technology of China"
    ]
}