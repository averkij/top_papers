{
    "paper_title": "Agent-as-a-Judge",
    "authors": [
        "Runyang You",
        "Hongru Cai",
        "Caiqi Zhang",
        "Qiancheng Xu",
        "Meng Liu",
        "Tiezheng Yu",
        "Yongqi Li",
        "Wenjie Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation."
        },
        {
            "title": "Start",
            "content": "A Survey on Agent-as-a-Judge 2026-1-8 Runyang You*1 Hongru Cai*1 Caiqi Zhang2 Qiancheng Xu1 Meng Liu3 Yongqi Li1 Wenjie Li1 Tiezheng Yu4 1 The Hong Kong Polytechnic University 2 University of Cambridge 3 Shandong Jianzhu University * Equal contribution 4 Huawei Technologies Corresponding author 6 2 0 2 J 8 ] . [ 1 1 1 1 5 0 . 1 0 6 2 : r runyang.y@outlook.com, {henry.hongrucai, liyongqi0}@gmail.com, cswjli@comp.polyu.edu.hk Abstract LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing clear roadmap for the next generation of agentic evaluation. Project Page: https://github.com/ModalityDance/Awesome-Agent-as-a-Judge"
        },
        {
            "title": "1 Introduction",
            "content": "The rapid advancement of Large Language Models (LLMs) has revolutionized the field of AI evaluation, giving rise to the LLM-as-a-Judge paradigm [1]. While traditional metrics fail to capture semantic nuance and human judgment remains unscalable, this new approach leverages LLMs advanced understanding and decision-making abilities to deliver near-human quality assessments across diverse domains [2]. Moreover, serving as scalable proxy for human preference, LLM judges can provide reward signals for reinforcement learning [3] and enable the automated curation of massive synthetic datasets [4]. As such, LLM judgment has established itself as cornerstone of AI evaluation and optimization pipelines, where the precision of the judge critically determines the success of downstream applications [5]. across specialized domains, the reliability of LLMas-a-Judge has become inevitably constrained [2, 6]. First, single-pass evaluators are prone to inherent parametric biasessuch as favoring verbosity or their own output patternswhich compromise their neutrality when assessing high-complexity responses that deviate from their training distribution [7]. Second, naive LLM judges are passive observers, unable to react to real-world observations; they assess answers based on linguistic patterns without verification, leading to hallucinated evaluations in specialized domains [8]. Furthermore, in evaluation tasks that require multifaceted assessment rubrics, traditional LLM judges experience cognitive overload when attempting to evaluate all dimensions comprehensively within single inference step, which results in coarse-grained scores that fail to reflect specific nuances [9]. However, as generative AI applications evolve from simple text responses to complex, multi-step tasks These limitations have catalyzed the transition from LLM-as-a-Judge to Agent-as-a-Judge. As shown 1 Survey on Agent-as-a-Judge LLM-as-a-Judge Agent-as-a-Judge Figure 1: Comparison between LLM-as-a-Judge (1a) and Agent-as-a-Judge (1b). The former performs direct single-pass evaluation, while the latter leverages planning, memory, and tool-augmented capabilities for enhanced evaluation. in Figure 1, agentic judges proactively engage in evaluation through multiple capabilities: they decompose complex objectives into subtasks, mitigate biases through multi-agent collaboration [10], ground assessments via tool-augmented evidence collection and correctness verification [8], and enable finegrained assessment by persisting intermediate states, autonomously planning the evaluation across reasoning steps [11, 12]. This paradigm shift enables more robust, verifiable, and nuanced assessments that effectively address the multifaceted nature of sophisticated AI-generated evaluands. Despite the above potentials and rapid proliferation of agentic evaluation systems, the field lacks survey to summarize and navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey for Agent-as-a-Judge through the following contributions: We identify and characterize the shift from LLMas-a-Judge to Agent-as-a-Judge and summarize the agentic judges development trend into three progressive stages (Section 2). sional domains (Section 4). We analyze frontier challenges and identify promising research directions, providing strategic roadmap for the next generation of robust and verifiable AI judgment."
        },
        {
            "title": "2 Evolution: From LLM-as-a-Judge to",
            "content": "Agent-as-a-Judge This section traces the evolution of automated evaluation from LLM-as-a-Judge to Agent-as-a-Judge paradigms. We begin by reviewing the foundational LLM-as-a-Judge and its limitations. We then examine the shift toward Agent-as-a-Judge, analyzing key dimensions that characterize the agentic approach. Finally, we summarize Agent-as-a-Judges development trend into three progressive stages with distinct levels of autonomy and adaptability."
        },
        {
            "title": "2.2 From LLM-as-a-Judge to Agent-as-a-Judge\nAs evaluands evolve from simple text responses to\ncomplex, multi-step tasks across specialized domains,\ntraditional LLM-as-a-Judge has become increasingly\ninadequate, focusing on final outputs while failing\nto verify intermediate actions or satisfy the rigorous\nstandards of professional fields [2, 6]. To bridge this\ngap, the paradigm is shifting toward Agent-as-a-Judge\nthat employs decentralized deliberation, executable\nverification, and fine-grained assessment to mitigate\nthese limitations.",
            "content": "We organize core methodologies into five key parts according to agents abilities (Section 3) and survey their applications across general and profesEvolving Robustness: From Monolithic to Decentralized. To mitigate the inherent parametric biases of monolithic LLM judgessuch as the tendency to 2 Survey on Agent-as-a-Judge u - - - g Multi-Agent Collaboration (3.1) Planning (3.2) E.g., ChatEval [10], M-MAD [56], SAGEval [44], HiMATE [9], CAFES [57], GEMA-Score [24], CourtEval [58] E.g., MATEval [52], Evaluation Agent [28], EvalAgents [53], AGENT-X [45], ARJudge [54], OnlineRubrics [55] Methodologies (3) Tool Integration (3.3) E.g., Agent-as-a-Judge [48], CodeVisionary [51], Evaluation Agent [28], ARM-Thinker [12], HERMES [37], VerifiAgent [38], Agentic Reward Modeling [8] Memory and Personalization (3.4) E.g., HERMES [37], ARM-Thinker [12], Agent-as-a-Judge [48], RLPA [11], SynthesizeMe [46], PersRM-R1 [49], FSPO [50] Optimization Paradigms (3.5) E.g., Prompting (Evaluation Agent [28], HERMES [37], Multi-Agent LLM Judge [43], SAGEval [44], AGENT-X [45]), SFT (SynthesizeMe [46]), RL (TIR-Judge [47], ARM-Thinker [12]) Application (4) General Domains (4.1) Professional Domains (4.2) Math and Code E.g., HERMES [37], VerifiAgent [38], CompassVerifier [39], xVerify [40], Agentic Reward Modeling [8], Multi-Agent Verification [41], Popper [42] Fact-Checking E.g., FACT-AUDIT [34], UrduFactCheck [35], NarrativeFactScore [36], Conversation and Interaction Multimodal and Vision Medicine Law Finance Education E.g., IntellAgent [30], ESC-Judge [31], Sentient Agent [32], PSYCHE [33] E.g., CIGEval [27], Evaluation Agent [28], LRQ-Fact [29], ARM-Thinker [12] E.g., MAJ-Eval [13], GEMA-Score [24], ChatCoach [25], AI Hospital [26] E.g., AgentsCourt [21], SAMVAD [22], AgentsBench [23] E.g., FinResearchBench [17], FinDeepResearch [18], SAEA [19], M-SAEA [20] E.g., MAJ-Eval [13], AutoSCORE [14], GradeOpt [15], Grade-Like-a-Human [16] Figure 2: taxonomy of Agent-as-a-Judge organizing Methodologies (3) and Applications (4). Background gradients illustrate the coverage of developmental stages, from Procedural to Reactive and then to Self-Evolving . favor verbosity or their own output patternsAgentas-a-Judge paradigms employ specialized, decentralized agents that collaborate through autonomous decision-making [10, 13]. Crucially, this decentralized architecture facilitates the injection of expert prior knowledge: by decomposing complex evaluation goals into sub-tasks or structuring specific interaction workflows, we can enforce domain-specific constraints that generalist model typically overlooks [16, 24]. Furthermore, multi-agent deliberation ensures collective robustness; distinct roles can isolate specific information points to neutralize bias, while debate and self-reflection allow agents to audit their own cognitive shortcuts, ensuring the final judgment transcends the heuristics of any single model [62, 7]. Evolving Verification: From Intuition to Execution. Static LLM judges are fundamentally passive observers, unable to react to real-world feedback. They assess answers based on linguistic plausibility how correct response looks without verification or evidence collection, leading to \"hallucinated correctness\" in complex tasks [8]. Agent-as-a-Judge bridges this reality gap by replacing intuition with execution. By interacting with external environments, agentic judges can query system states to verify side effects (e.g., file operations) [48, 51], use code interpreters or theorem provers to validate logical consistency [37], and employ search tools to ground factual claims in real-time documentation [38, 8]. This shifts the evaluative anchor from internal model knowledge to objective verification. Evolving Granularity: From Global to Fine grained. Many evaluation tasks inherently require multifaceted assessment rubrics, yet traditional LLM judges face cognitive overload to evaluate these dimensions comprehensively within single inference step, results in coarse-grained scores that fail to reflect specific nuances [9]. Agent-as-a-Judge addresses this by transforming evaluation from single-pass inference into autonomous, hierarchical reasoning [9]. Instead of monolithic assessment, an agentic judge can dynamically select or create task-specific rubrics, autonomously planning the evaluation to examine each component of the evaluand independently [44], utilizing memory to track historical reasoning states and synthesize fragmented evidence into coherent verdict. Consequently, these agents can pinpoint 3 Survey on Agent-as-a-Judge specific flaws that would otherwise be obscured in global score, providing fine-grained feedback on each aspect [45]."
        },
        {
            "title": "2.3 Agent-as-a-Judge\nAgent-as-a-Judge represents a rapidly expanding field\nwhere the term \"agent\" is often applied loosely, span-\nning a heterogeneous range from procedural agentic\nworkflows to autonomous self-evolvers [10, 45, 12].\nTo provide a clear roadmap through this complexity,\nwe summarize the ongoing development of agency\nas follows.",
            "content": "Procedural Agent-as-a-Judge decouples monolithic inference into agentic predefined workflows [57, 24] or engages in structured discussions among fixed sub-agents [10, 56]. These systems enable complex judgments through coordinated multi-agent interactions, yet remain constrained by predetermined decision rules that cannot adapt to novel evaluation scenarios. Reactive adaptive Agent-as-a-Judge enables decision-making by routing execution paths [28, 45] and invoking external tools [8] or sub-agents [13] based on intermediate feedback. However, such reactivity remains confined to conditional routing within fixed decision spaces, lacking autonomy to refine underlying rubrics. Self-Evolving Agent-as-a-Judge represents the cutting edge of the field, characterized by high autonomy and the ability to refine internal components during operationsynthesizing evaluation rubrics on-the-fly [53] and updating memory with lessons learned. This paradigm opens new frontiers for adaptive evaluation systems, though challenges remain in ensuring stability during self-modification [63]."
        },
        {
            "title": "3 Methodologies",
            "content": "This section categorizes Agent-as-a-Judge methodologies into five dimensions: multi-agent collaboration, planning, tool integration, memory and personalization, and optimization paradigms. As shown in Figure 2, implementation sophistication reveals the evolutionary stages: foundational methodologies (collaboration, tool integration, optimization) evolve across all stages, while others (planning, memory) emerge more prominently in advanced paradigms. The fol4 Collective Consensus Task Decomposition Figure 3: Multi-agent collaboration paradigms. lowing subsections examine how each methodology manifests across these stages."
        },
        {
            "title": "3.1 Multi-Agent Collaboration",
            "content": "Multi-agent collaboration leverages collective reasoning to mitigate single-LLM biases in Agent-as-aJudge systems. Early systems followed Procedural paradigms with fixed protocols, while recent work evolves toward Reactive approaches that adapt agent selection based on feedback. We categorize these into two topologies: Collective Consensus. Horizontal debate mechanisms leverage agents representing diverse perspectives to counteract the inherent biases of single-LLM evaluators, illustrated in Figure 3. Early approaches exemplified the Procedural stage: ChatEval [10] pioneered this with courtroom-inspired discussion mechanism where agents debate as equals following predefined protocols. This paradigm was later extended to machine translation in M-MAD [56], while subsequent research [64] introduced explicit stances and \"judge\" roles to prevent agents from blindly conforming to the majority. Recent methods have become more Self-Evolving: approaches like Multi-agent-as-judge [13] have moved beyond static ensembles by creating domain-specific experts based on intermediate feedback. Task Decomposition. Task Decomposition employs \"Divide and Conquer\" strategy, delegating distinct subtasks to specialized agents for systematic evaluation, illustrated in Figure 3. Early frameworks followed Procedural designs: sequential approaches like CAFES [57] and GEMA-Score [24, 58] structure evaluation into predefined stages (e.g., Evidence Gathering, Reasoning, Scoring), while SAGEval [44] introduces supervision via \"Judge the Judge\" metaevaluator that reviews previous agents decisions, Survey on Agent-as-a-Judge with hierarchical approaches like HiMATE [9] organizing agents into tree structures for varying error granularities. More recent work has shifted toward Reactive paradigms: AGENT-X [45] employs adaptive router agent that dynamically selects the most relevant base agents based on intermediate analysis results. spoke detection guidelines. ARJudge [54] adaptively formulates rubrics by iteratively generating contextsensitive questions, and OnlineRubrics [55] integrates planning into reinforcement learning, evolving rubrics alongside policy optimization to detect reward hacking. Takeaway Multi-agent evaluation frameworks adopt two main topologies: Collective Consensus and Task Decomposition. Recent advances have evolved toward more autonomous systems that can select or generate subagents."
        },
        {
            "title": "3.2 Planning",
            "content": "Planning serves as core capability in the Agent-as-aJudge paradigm, enabling the decomposition of highlevel evaluation objectives into executable sub-tasks and the dynamic adaptation of assessment trajectories based on intermediate analysis. This section examines planning capabilities from two perspectives: Workflow Orchestration. Workflow orchestration in Agent-as-a-Judge systems spans from static frameworks to dynamic agency, primarily characterizing Procedural and Reactive stages of agentic evaluation. Approaches like MATEval [52] rely on static decomposition, breaking tasks into fixed sequences of sub-dimensions. While this ensures systematic assessment through predefined control flows, it limits adaptability in complex scenarios. Conversely, Evaluation Agent [28] introduces dynamic multi-round planning, where agents adjust strategies based on intermediate feedback. This system further optimizes efficiency through autonomous termination, allowing the agent to self-monitor information gain and proactively halt execution once sufficient evidence is gathered. Rubric Discovery. Unlike general agents focused on task completion, Judge Agents have the distinct capability to autonomously formulate and refine rubrics, representing hallmark of the Self-Evolving stage, where agents can refine their internal evaluation components. EvalAgents [53] exemplifies this by employing Query Generator that plans web searches to discover implicit rubrics, while AGENT-X [45] uses an Adaptive Router to infer domain context and plan beTakeaway Serving as the strategic engine, planning shifts evaluation from rigid flows to adaptive exploration, enabling agents to optimize how they evaluate (workflow orchestration) and what they evaluate (rubric discovery)."
        },
        {
            "title": "3.3 Tool Integration",
            "content": "Tool integration is defining capability of Agent-asa-Judge frameworks, enabling judges to ground evaluation in external evidence and explicit checks. As shown in Table 1, existing approaches can be grouped into evidence collection and correctness verification based on the purpose of tool use. Evidence Collection. common use of tools in Agent-as-a-Judge frameworks is to collect additional evidence that supports evaluations. Such evidence includes intermediate artifacts, execution results, and perceptual signals that cannot be reliably obtained through text-based reasoning. In code-related tasks, Agent-as-a-Judge [48] and CodeVisionary [51] allow judges to inspect execution artifacts or run automated checks to expose execution feedback for evaluation. Similar methods are adopted in multimodal settings. Evaluation Agent [28] enables judges to invoke external visual models to obtain visual quality or alignment signals, while ARM-Thinker [12] gathers fine-grained visual and contextual evidence through document access and localized visual operations. Overall, these works integrate tools to surface observable and task-relevant evidence, expanding the judges access to execution-level, perceptual, and contextual information, and supporting more reliable evaluation. Correctness Verification. Another line of work employs tools to verify whether the evaluands outputs or intermediate reasoning steps satisfy explicit correctness constraints, such as logical validity, mathematical soundness, or factual consistency. In these frameworks, the judge agent identifies which claims 5 Survey on Agent-as-a-Judge Tool Purpose Method Evaluation Task Tool Type Agent-as-a-Judge [48] Code generation Graph, locate, read, search, retrieve Evidence collection CodeVisionary [51] Code generation Code execution, static linter, unit tests, screenshot, web browsing Evaluation Agent [28] Visual generation Visual generative models ARM-Thinker [12] Multimodal generation Instruction following checks, crop/zoom tools, document retrieval tools Correctness verification HERMES [37] Math reasoning Translator, theorem prover VerifiAgent [38] Factual & Math reasoning Search engine, Python interpreter, theorem prover Agentic RM [8] Factual & Math reasoning Search engine, Python interpreter Table 1: Tool integration in representative Agent-as-a-Judge methods, grouped by primary tool usage purpose. or steps require verification and invokes appropriate tools to check them. The resulting verification signals are then interpreted by the agent in context to inform the final evaluation. HERMES [37] verifies mathematical reasoning through formal theorem proving, while VerifiAgent [38] invokes programmatic and symbolic checkers to validate factual and computational claims. Agentic Reward Modeling [8] further integrates correctness verification by combining factchecking tools and programmatic validators to produce structured correctness signals that inform the final evaluation. intermediate proof states when combining reasoning with formal theorem proving, enabling consistent verification across long reasoning chains. ARMThinker [12] preserves intermediate evidence such as visual reasoning outputs and tool interaction results, which are later reused to ground evaluation. Agent-as-a-Judge [48] records execution traces and step-level feedback, enabling evaluation beyond final outputs to account for intermediate behavior. Collectively, these methods use memory to retain intermediate states that support cumulative, step-aware evaluation. Tool Takeaway integration in Agent-as-a-Judge grounds evaluation in observable and verifiable signals by allowing judges to actively gather evidence and check correctness through external tools."
        },
        {
            "title": "3.4 Memory and Personalization\nMemory enables Agent-as-a-Judge frameworks to re-\ntain information across evaluation steps, supporting\nmulti-step reasoning, consistent judgment, and reuse\nof prior results. We categorize prior work by the role\nof memory, including intermediate state tracking and\npersonalized context preservation.",
            "content": "In multi-step evaluation setIntermediate State. tings, Agent-as-a-Judge frameworks use memory to retain intermediate evaluation states generated during assessment, providing the necessary context for conditional routing and adaptive decision-making based on intermediate feedbacka fundamental mechanism for Reactive Agent-as-a-Judge. HERMES [37] retains Personalized Context. Agent-as-a-Judge frameworks often incorporate memory to retain userrelated information that conditions evaluation across interactions. Such memory captures user preferences, evaluation standards, or prior feedback, allowing judgments to remain consistent over time. PersRMR1 [49] and FSPO [50] store preference data derived from historical interactions, including preference labels or few-shot examples, which are reused to condition subsequent evaluations for the same user. More advanced approaches abstract historical preference signals into persistent user personas or long-term profiles. RLPA [11] and SynthesizeMe [46] exemplify this by constructing and maintaining user personas that are stored and reused to guide evaluation. Such long-term user profiling often serves to support SelfEvolving Agent-as-a-Judge, enabling continuous optimization based on evolving preferences. Together, these methods use memory to preserve personalized 6 Survey on Agent-as-a-Judge context that shapes evaluation behavior and ensures consistency across interactions. Takeaway Memory enables Agent-as-a-Judge to preserve intermediate states and personalized context, supporting multi-step evaluation, consistent judgment, and efficient reuse of prior information."
        },
        {
            "title": "3.5 Optimization Paradigms",
            "content": "Optimization paradigms define how Agent-as-a-Judge improves evaluation quality by updating model parameters or adapting evaluation behaviors. We organize prior work into two groups: training-time optimization and inference-time optimization. Training-Time Optimization. Training-time optimization improves Agent-as-a-Judge by updating model parameters to align judgment behavior with evaluation objectives. Supervised fine-tuning is commonly used to standardize judge behavior, training models to follow explicit criteria, and produce structured judgments across tasks. For example, SynthesizeMe [46] shapes evaluation behavior using persona-guided supervision derived from historical data. Reinforcement learning optimizes judges to perform evaluation actions more effectively, especially in settings that require tool use and multi-step verification. TIR-Judge [47] and ARM-Thinker [12] train judges to decide when and how to invoke tools, integrate external signals, and verify intermediate results. Together, training-time optimization shapes internal decision processes, enabling more reliable, structured evaluation. Inference-Time Optimization. Inference-time optimization adapts evaluation behavior without updating model parameters by controlling how judgments are produced through prompts, workflows, or agent interactions. Existing approaches can be broadly grouped into two types. 1) The first type follows predefined evaluation procedures, where reasoning steps, verification routines, or prompts are fixed in advance to ensure consistency and efficiency. Evaluation Agent [28] and HERMES [37] exemplify this setting by adopting structured, step-by-step evaluation pipelines. 2) The second type allows evaluation behavior to adapt during inference, where the evaluation process, participating agents, or applied Figure 4: An overview of Agent-as-a-Judge application domains and their fine-grained task categories. criteria change based on intermediate results. MultiAgent LLM Judge [43] iteratively refines prompts and context through multi-judge coordination, while SAGEval [44] introduces meta-judge to monitor and revise judge behavior. ChatEval [10] and AGENTX [45] further support adaptive evaluation through agent interaction and dynamic guideline selection. Overall, inference-time optimization enables flexible control over evaluation behavior, ranging from fixed procedures to adaptive, interaction-driven judgment. Takeaway Optimization improves Agent-as-a-Judge by either learning evaluation behavior through parameter updates at training-time or adjusting evaluation strategies at inference time."
        },
        {
            "title": "4 Application",
            "content": "Building on the methods above, this section describes how Agent-as-a-Judge methods are applied in different evaluation tasks. As shown in Figure 4, we organize representative applications into two groups: general domains and professional domains."
        },
        {
            "title": "4.1 General Domains",
            "content": "In math and code evaluation, Math and Code. Agent-as-a-Judge systems move beyond single-pass scoring by grounding judgment in verifiable reasoning signals. One line of work augments free-form 7 Survey on Agent-as-a-Judge reasoning with explicit correctness checks. HERMES [37] anchors LLM reasoning to intermediate formal proof steps, reducing drift in long derivations. VerifiAgent [38] decouples high-level reasoning assessment from tool-based correctness verification, enabling adaptive checking across reasoning types. CompassVerifier [39] and xVerify [40] focus on mathematical and logical outputs, addressing equivalence checking under diverse surface forms. Other approaches strengthen judgment by aggregating multiple evaluation signals. Multi-Agent Verification [41] distributes evaluation across aspect-specific judges. Agentic Reward Modeling [8] integrates preferencebased supervision with verifiable correctness signals to improve reward reliability. Popper [42] formulates judgment as controlled falsification, using statistical tests to validate free-form claims. In fact-checking, Agent-as-a-Judge Fact-Checking. reframes evaluation from static label prediction to interactive verification with evidence gathering and justification. FACT-AUDIT [34] models fact-checking as an agentic loop with multi-agent collaboration, jointly evaluating verdict accuracy and justification quality. This paradigm is particularly effective when evidence is scarce or inconsistencies are subtle. UrduFactCheck [35] improves robustness in low-resource settings through multilingual retrieval and evidence boosting. NarrativeFastScore [36] addresses longcontext factual consistency by constructing characterlevel knowledge representations, enabling detection of state and relation errors with actionable feedback. In conversation and Conversation and Interaction. interaction, Agent-as-a-Judge shifts from grading isolated replies to constructing multi-turn exchanges, enabling evaluation under evolving goals, constraints, and user reactions. For task-oriented dialogue, IntellAgent [30] uses interactive user simulations to synthesize conversational benchmarks, while Kazi et al. [65] introduces frameworks for controllable user goals and automatic measures. For affective and social interaction, ESC-Judge [31] constructs emotional-support agents via standardized counseling skills, Sentient Agent [32] tracks emotion trajectories over time to reflect higher-order social cognition, and PSYCHE [33] builds psychiatric patient profiles for ethical assessment validation. Wu et al. [66] frames evaluation as multi-perspective role play with diverse reviewer personas to cover both objective and subjective dimensions. Multimodal and Vision. In the multimodal and vision domain, Agent-as-a-Judge shifts from static scoring to interactive inspection. For visual generation, CIGEval [27] orchestrates specialized tools to probe control adherence and subject consistency, while Evaluation Agent [28] runs multi-round checks to provide user-tailored, explainable analyses. For truthfulness evaluation, LRQ-Fact [29] generates targeted factchecking questions across image and text to guide evidence retrieval, while ARM-Thinker [12] selectively invokes tools like image inspection for finalizing judgments."
        },
        {
            "title": "4.2 Professional Domains",
            "content": "In high-stakes clinical NLP, Agent-as-aMedicine. Judge appears in two forms: 1) multi-agent evaluators that decompose clinical quality into specialized roles, and 2) agentic simulators that interactively elicit clinical behaviors. For 1), MAJ-Eval [13] constructs multiple evaluator personas to debate and cross-verify responses, while GEMA-Score[24] uses agent collaboration to compute granular, toolassisted scores covering disease severity and uncertainty. For 2), Chat-Coach [25] pairs autonomous patient and coach agents to critique trainee-doctor dialogues, while AI Hospital [26] evaluates LLM doctors in multi-agent simulators, though final scoring often still requires conventional metrics. Law. In the legal domain, Agent-as-a-Judge simulates the adversarial and deliberative nature of jurisprudence through multi-agent interaction. AgentsCourt [21] introduces adversarial debate frameworks where agents role-play as prosecutors, defense attorneys, and judges, exposing the evaluating agent to conflicting arguments to improve verdict robustness. SAMVAD [22] and AgentsBench [23] model judicial consensus by simulating bench deliberation processes, capturing interactions between concurring and dissenting opinions to enhance legal judgment prediction. Finance. In finance, Agent-as-a-Judge addresses two limitations of static benchmarks: 1) capturing the internal research logic of long-form analyst reports, and 8 Survey on Agent-as-a-Judge 2) detecting deployment risks like hallucinations and temporal staleness. For 1), FinResearchBench [17] extracts logic trees from reports as intermediate structures for comprehensive assessment, whereas FinDeepResearch [18] can synthesize hierarchical rubrics but still relies on predefined workflows. For 2), SAEA [19] proposes auditing agent trajectories to mitigate hallucinations and temporal misalignment. From Tasks to Teams [20] extends this approach with M-SAEA to trace multi-agent failures, such as crossagent divergence and error propagation. Education. In the educational domain, Agent-as-aJudge systems emulate pedagogical nuance through collaborative, role-specialized workflows. GradeLike-Human [16] and AutoSCORE [14] decompose grading into staged processes (rubric construction, evidence recognition, cross-review) to improve grounding and consistency. Beyond static scoring, MAJ-Eval [13] uses multi-persona debates to align with multi-dimensional human evaluation, while GradeOpt [15] introduces agents that diagnose discrepancies and iteratively refine grading guidelines."
        },
        {
            "title": "5 Discussion",
            "content": "This section discusses broader issues that arise when deploying Agent-as-a-Judge systems in practice. We first summarize key challenges that limit scalability, reliability, and real-world adoption, and then outline several future directions that may help address these limitations and further advance agentic evaluation."
        },
        {
            "title": "5.1 Challenges",
            "content": "Agent-as-a-Judge improves evaluation reliability through planning, tool use, memory, and multi-agent collaboration, but these capabilities also introduce new challenges beyond static LLM-as-a-Judge. Key challenges include computational cost, latency, safety, and privacy. Computational Cost. Agent-as-a-Judge introduces heavier computational burden in both training and inference. 1) Training judge agent is expensive. Supervised fine-tuning alone is often insufficient to support agentic behaviors such as tool invocation, long-horizon planning, and adaptive decision making. Reinforcement learning provides natural way to acquire these capabilities, but it significantly increases training cost, especially when the judge operates over long trajectories or complex tool-calling sequences. 2) Inference with Agent-as-a-Judge is also costly. Unlike single-pass judgment, agentic evaluation typically involves multiple reasoning steps, intermediate decisions, and coordination among multiple agents, all of which increase computation per evaluation. Latency. In addition to higher computational cost, Agent-as-a-Judge often suffers from increased inference latency. Agentic evaluation requires sequential reasoning steps, external tool calls, or multi-agent communication, each of which introduces additional delays. This latency can be particularly problematic in real-time or interactive settings, such as online model evaluation, user-facing content moderation, or reinforcement learning loops where rapid feedback is required. As result, there exists tension between evaluation reliability and practical deployment constraints, where more thorough agentic judgment may not be feasible under strict latency budgets. Safety. While Agent-as-a-Judge is designed to improve evaluation robustness, it also raises new safety concerns. Tool-augmented judges may access external systems such as search engines, code executors, or databases, which expands the attack surface for prompt injection, tool misuse, or unintended side effects. Multi-agent collaboration can further amplify risks if unsafe behaviors propagate across agents or if adversarial interactions emerge. Moreover, when judge agents are used to provide reward signals for model optimization, systematic biases or errors in agentic judgment may be reinforced and amplified during training, leading to unintended model behaviors. Privacy. Agent-as-a-Judge also introduces privacy challenges, particularly in settings that involve persistent memory or personalized evaluation. To maintain consistency or adapt judgments to specific users or contexts, judge agents may store intermediate states, user information, or historical interaction data. If not carefully designed, such memory mechanisms can increase the risk of sensitive data leakage or unauthorized inference about user attributes. This issue becomes more pronounced in professional domains such as medicine, law, or education, where evaluation 9 Survey on Agent-as-a-Judge often relies on confidential or personally identifiable information. the judge refines its criteria through multi-turn alignment, ensuring higher trust and interpretability."
        },
        {
            "title": "5.2 Future Directions",
            "content": "Personalization. Current Agent-as-a-Judge systems are constrained by static, one-size-fits-all evaluation criteria, failing to align with diverse individual preferences. To bridge this gap, future research should focus on enhancing the autonomy and adaptivity of judge agents. critical enabler is proactive memory management: rather than passively retrieving history, agents must actively manage the lifecycle of user-specific knowledgeautonomously deciding when to register new preferences, update evolving standards, or prune obsolete feedback. This agentic control transforms memory into dynamic belief system, allowing the judge to continuously refine its criteria and maintain alignment with the users specific values and usage contexts. Generalization. Current systems rely on predefined rubrics constructed offline, limiting their ability to generalize across diverse or open-ended tasks. Future judge agents should leverage planning capabilities to dynamically discover and adapt evaluation criteria. 1) Context-Aware Rubric Generation: Agents should synthesize evaluation criteria on-the-fly by analyzing the specific intent and complexity of responses, identifying relevant assessment dimensions not anticipated during design. 2) Adaptive Multi-Granularity Scoring: Rubrics should dynamically scale based on task difficultyapplying high-level holistic criteria for straightforward tasks, while decomposing into fine-grained sub-rubrics for complex workflows. Interactivity. Current systems operate as passive, one-way observers. Future agents should evolve into interactive evaluators that actively engage with both the environment and human stakeholders. 1) Interactive Environmental Feedback: Instead of static test suites, judge agents should dynamically tailor evaluation trajectoriesautonomously escalating task complexity or isolating edge cases to rigorously probe the evaluands failure boundaries. 2) Human-Agent Collaborative Calibration: To address subjective or ambiguity-rich scenarios, agents should leverage human-in-the-loop mechanisms. By proactively consulting experts to verify intent or resolve conflicts, Optimization. Current approaches predominantly rely on inference-time engineering, which is fundamentally bottlenecked by the fixed capabilities of frozen backbones. To transcend these limits, the field must pivot towards Training-based Optimization. This paradigm shift entails two key levels: 1) Individual Capability: Utilizing Reinforcement Learning (RL) to internalize complex agentic behaviorssuch as sequential planning and adaptive tool usethat are difficult to elicit via prompting alone. 2) Learned Coordination: Extending optimization to multi-agent settings. Rather than ad-hoc inference collaboration, agents should be trained with joint objectives to intrinsically learn effective communication and consensus strategies. Concluding Remarks: Towards True Autonomy. As characterized in Section 2, existing implementations exhibit varying degrees of agency. The future directions discussed abovepersonalization, generalization, interactivity, and optimizationcollectively point towards an evolutionary trajectory towards autonomy. The next generation of judge agents must transcend fixed protocols to become genuinely agentic entities capable of self-directed adaptation, active context curation, and continuous self-refinement, ultimately realizing the full potential of agents that actively perceive, reason, and evolve alongside the models they assess."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper provides the first comprehensive survey of Agent-as-a-Judge. We established novel taxonomy and demonstrated how agentic capabilities, including multi-agent collaboration, autonomous planning, tool integration, and memory, overcome the limitations of naive LLM judges to deliver more robust, verifiable and nuanced judgments across general and professional domains. While promising, this evolution presents challenges in computational cost, latency, safety, and privacy. Future progress should prioritize personalization, generalization, and optimization, ultimately realizing truly autonomous evaluators that continuously adapt to the evolving AI landscape. 10 Survey on Agent-as-a-Judge"
        },
        {
            "title": "Limitations",
            "content": "Early Stage of Paradigm Consensus. As pioneering survey exploring the evolution of Agent-asa-Judge, this study faces the challenge that the field has not yet gained complete widespread recognition in academia. Although the transition from LLM-as-aJudge to Agent-as-a-Judge has begun to take shape, there is still lack of long-term consensus regarding the definition of evaluation agents. Nevertheless, establishing this foundational framework is essential to orienting future research. We are committed to iteratively refining this taxonomy as the paradigm matures and gains broader recognition. Inclusion of Early Prompting Methods. We acknowledge potential gap between early methodologies and the increasingly rigorous definitions of agents. Many pioneering works in automated evaluation, though named as \"agent\", rely heavily on prompting engineering, such as fixed role-play, which may not align with the strict criteria for autonomy, dynamic planning, or tool-use held by the current community. Nevertheless, we deliberately include these prompt-based frameworks as they represent the initial shift from monolithic inference toward dynamic decomposition and self-evolving systems. Excluding them would obscure the transition thus compromising complete understanding of the fields evolution."
        },
        {
            "title": "Ethics Statement",
            "content": "This work does not involve the use or creation of datasets or scientific artifacts that would require specific ethical clearance, data privacy considerations, or licensing agreements. We believe this work adheres to the ethical guidelines of the conference and poses no immediate negative social impact."
        },
        {
            "title": "References",
            "content": "[1] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llmas-a-judge with mt-bench and chatbot arena. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. [2] Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. From generation to judgment: Opportunities and challenges of llm-as-a-judge. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 27572791, 2025. [3] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. Rlaif vs. rlhf: scaling reinforcement learning from human feedback with ai feedback. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. [4] Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. On LLMs-driven synthetic data generation, curation, and evaluation: survey. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 1106511082, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl. 658. URL https://aclanthology.org/2024. findings-acl.658/. [5] Hanyu Lai, Xiao Liu, Junjie Gao, Jiale Cheng, Zehan Qi, Yifan Xu, Shuntian Yao, Dan Zhang, Jinhua Du, Zhenyu Hou, Xin Lv, Minlie Huang, Yuxiao Dong, and Jie Tang. survey of post-training scaling in large language models. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 27712791, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.140. URL https:// aclanthology.org/2025.acl-long.140/. [6] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: survey. Science China Information Sciences, 68(2):121101, 2025. [7] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94409450, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 11 Survey on Agent-as-a-Judge 10.18653/v1/2024.acl-long.511. URL https:// aclanthology.org/2024.acl-long.511/. [8] Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Bin Xu, Lei Hou, and Juanzi Li. Agentic reward modeling: Integrating human preferences with verifiable correctness signals for reliable reward systems. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1593415949, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 9798-89176-251-0. doi: 10.18653/v1/2025.acl-long. 775. URL https://aclanthology.org/2025. acl-long.775/. [9] Shijie Zhang, Renhao Li, Songsheng Wang, Philipp Koehn, Min Yang, and Derek F. Wong. HiMATE: hierarchical multi-agent framework for machine translation evaluation. In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng, editors, Findings of the Association for Computational Linguistics: EMNLP 2025, pages 11121 11145, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979-8-89176335-7. doi: 10.18653/v1/2025.findings-emnlp. 593. URL https://aclanthology.org/2025. findings-emnlp.593/. [10] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better LLM-based evaluators through multi-agent debate. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=FQepisCUWu. [11] Weixiang Zhao, Xingyu Sui, Yulin Hu, Jiahe Guo, Haixiao Liu, Biye Li, Yanyan Zhao, Bing Qin, and Ting Liu. Teaching language models to evolve with users: Dynamic profile modeling for personalized alignment. In Proceedings of the Thirty-Ninth Conference on Neural Information Processing Systems (NeurIPS 2025), 2025. [12] Shengyuan Ding, Xinyu Fang, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiangyu Zhao, Haodong Duan, Xiaoyi Dong, Jianze Liang, Bin Wang, Conghui He, Dahua Lin, and Jiaqi Wang. Arm-thinker: Reinforcing multimodal generative reward models with agentic tool use and visual reasoning, 2025. [13] Jiaju Chen, Yuxuan Lu, Xiaojie Wang, Huimin Zeng, Jing Huang, Jiri Gesi, Ying Xu, and Dakuo Wang. Multi-agent-as-judge: Aligning LLM-agent-based automated evaluation with multi-dimensional human evaluation. In First Workshop on Multi-Turn Interactions in Large Language Models, 2025. URL https: 12 //openreview.net/forum?id=7AetgL7eVL. [14] Yun Wang, Zhaojun Ding, Xuansheng Wu, Siyue Sun, Ninghao Liu, and Xiaoming Zhai. Autoscore: Enhancing automated scoring with multi-agent large language models via structured component recognition. arXiv preprint arXiv:2509.21910, 2025. [15] Yucheng Chu, Hang Li, Kaiqi Yang, Harry Shomer, Hui Liu, Yasemin Copur-Gencturk, and Jiliang Tang. llm-powered automatic grading framework with human-level guidelines optimization. arXiv preprint arXiv:2410.02165, 2024. [16] Wenjing Xie, Juxin Niu, Chun Jason Xue, and Nan Guan. Grade like human: Rethinking automated assessment with large language models. arXiv preprint arXiv:2405.19694, 2024. [17] Rui Sun, Zuo Bai, Wentao Zhang, Yuxiang Zhang, Li Zhao, Shan Sun, and Zhengwen Qiu. Finresearchbench: logic tree based agent-as-a-judge evaluation framework for financial research agents. In Proceedings of the 6th ACM International Conference on AI in Finance, pages 656664, 2025. [18] Fengbin Zhu, Xiang Yao Ng, Ziyang Liu, Chang Liu, Xianwei Zeng, Chao Wang, Tianhui Tan, Xuan Yao, Pengyang Shao, Min Xu, et al. Findeepresearch: Evaluating deep research agents in rigorous financial analysis. arXiv preprint arXiv:2510.13936, 2025. [19] Zichen Chen, Jiaao Chen, Jianda Chen, and Misha Sra. Standard benchmarks fail auditing llm agents in finance must prioritize risk, 2025. URL [https://arxiv.org/abs/2502.15865] (https://arxiv.org/abs/2502.15865). [20] Zichen Chen, Jianda Chen, Jiaao Chen, and Misha Sra. From tasks to teams: risk-first evaluation framework for multi-agent llm systems in finance. In ICML 2025 Workshop on Reliable and Responsible Foundation Models, 2025. [21] Zhitao He, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen, Jiexin Xu, Huaijun Li, Kang Liu, and Jun Zhao. Agentscourt: Building judicial decisionmaking agents with court debate simulation and legal knowledge augmentation. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 93999416, 2024. [22] Prathamesh Devadiga, Omkaar Jayadev Shetty, and Pooja Agarwal. Samvad: multi-agent system for simulating judicial deliberation dynamics in india. arXiv preprint arXiv:2509.03793, 2025. [23] Cong Jiang and Xiaolei Yang. Agentsbench: multiagent llm simulation framework for legal judgment prediction. Systems, 13(8):641, 2025. [24] Zhenxuan Zhang, Kinhei Lee, Weihang Deng, Huichi Zhou, Zihao Jin, Jiahao Huang, Zhifan Gao, Dominic C. Marshall, Yingying Fang, and Guang Survey on Agent-as-a-Judge Yang. Gema-score: Granular explainable multiagent score for radiology report evaluation. CoRR, abs/2503.05347, March 2025. URL https://doi. org/10.48550/arXiv.2503.05347. [25] Hengguan Huang, Songtao Wang, Hongfu Liu, Hao Wang, and Ye Wang. Benchmarking large language models on communicative medical coaching: In Findings of the dataset and novel system. Association for Computational Linguistics ACL 2024, pages 16241637, 2024. [26] Zhihao Fan, Lai Wei, Jialong Tang, Wei Chen, Wang Siyuan, Zhongyu Wei, and Fei Huang. Ai hospital: Benchmarking large language models in multiagent medical interaction simulator. In Proceedings of the 31st International Conference on Computational Linguistics, pages 1018310213, 2025. [27] Jifang Wang, Xue Yang, Longyue Wang, Zhenran Xu, Yiyu Wang, Yaowei Wang, Weihua Luo, Kaifu Zhang, Baotian Hu, and Min Zhang. unified agentic framework for evaluating conditional image generation. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1262612646, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.620. URL https:// aclanthology.org/2025.acl-long.620/. [28] Fan Zhang, Shulin Tian, Ziqi Huang, Yu Qiao, and Ziwei Liu. Evaluation agent: Efficient and promptable evaluation framework for visual generative models. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 75617582, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.374. URL https:// aclanthology.org/2025.acl-long.374/. [29] Alimohammad Beigi, Bohan Jiang, Dawei Li, Tharindu Kumarage, Zhen Tan, Pouya Shaeri, and Huan Liu. Lrq-fact: Llm-generated relevant questions for multimodal fact-checking. arXiv preprint arXiv:2410.04616, 2024. URL https://arxiv. org/abs/2410.04616. [30] Elad Levi and Ilan Kadar. Intellagent: multi-agent framework for evaluating conversational ai systems. arXiv preprint arXiv:2501.11067, 2025. [31] Navid Madani and Rohini Srihari. ESC-judge: framework for comparing emotional support conversational agents. In Christos Christodoulopoulos, 13 Tanmoy Chakraborty, Carolyn Rose, and Violet Peng, editors, Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1605916076, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979-889176-332-6. doi: 10.18653/v1/2025.emnlp-main. 811. URL https://aclanthology.org/2025. emnlp-main.811/. [32] Bang Zhang, Ruotian Ma, Qingxuan Jiang, Peisong Wang, Jiaqi Chen, Zheng Xie, Xingyu Chen, Yue Wang, Fanghua Ye, Jian Li, et al. Sentient agent as judge: Evaluating higher-order social cognition in large language models. arXiv preprint arXiv:2505.02847, 2025. [33] Jingoo Lee, Kyungho Lim, Young-Chul Jung, and Byung-Hoon Kim. Psyche: multi-faceted patient simulation framework for evaluation of psychiatric assessment conversational agents. arXiv preprint arXiv:2501.01594, 2025. [34] Hongzhan Lin, Yang Deng, Yuxuan Gu, Wenxuan Zhang, Jing Ma, See-Kiong Ng, and Tat-Seng Chua. FACT-AUDIT: An adaptive multi-agent framework for dynamic fact-checking evaluation of large language models. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 360381, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.17. URL https:// aclanthology.org/2025.acl-long.17/. [35] Sarfraz Ahmad, Hasan Iqbal, Momina Ahsan, Numaan Naeem, Muhammad Ahsan Riaz Khan, Arham Riaz, Muhammad Arslan Manzoor, Yuxia Wang, and Preslav Nakov. UrduFactCheck: An agentic factchecking framework for Urdu with evidence boosting and benchmarking. In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng, editors, Findings of the Association for Computational Linguistics: EMNLP 2025, pages 22788 22802, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979-8-89176335-7. doi: 10.18653/v1/2025.findings-emnlp. 1240. URL https://aclanthology.org/ 2025.findings-emnlp.1240/. [36] Yeonseok Jeong, Minsoo Kim, Seung-won Hwang, Agent-as-judge for facand Byung-Hak Kim. tual summarization of long narratives. In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng, editors, Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 23602 Survey on Agent-as-a-Judge 23619, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979-889176-332-6. doi: 10.18653/v1/2025.emnlp-main. URL https://aclanthology.org/ 1204. 2025.emnlp-main.1204/. [37] Azim Ospanov, Zijin Feng, Jiacheng Sun, Haoli Bai, Xin Shen, and Farzan Farnia. Hermes: Towards efficient and verifiable mathematical reasoning in llms, 2025. URL https://arxiv.org/ abs/2511.18760. [38] Jiuzhou Han, Wray Buntine, and Ehsan Shareghi. VerifiAgent: unified verification agent in language model reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2025, 2025. [39] Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei Zhang, Derek F. Wong, Songyang Zhang, and Kai Chen. CompassVerifier: unified and robust verifier for LLMs evaluation and outcome reward. In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng, editors, Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 3345433482, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979-889176-332-6. doi: 10.18653/v1/2025.emnlp-main. 1698. URL https://aclanthology.org/ 2025.emnlp-main.1698/. [40] Ding Chen, Qingchen Yu, Pengyuan Wang, Wentao Zhang, Bo Tang, Feiyu Xiong, Xinchi Li, Minchuan Yang, and Zhiyu Li. xverify: Efficient answer verifier for reasoning model evaluations. arXiv preprint arXiv:2504.10481, 2025. [41] Shalev Lifshitz, Sheila McIlraith, and Yilun Du. Multi-agent verification: Scaling test-time compute with multiple verifiers. arXiv preprint arXiv:2502.20379, 2025. [42] Kexin Huang, Ying Jin, Ryan Li, Michael Li, Emmanuel Candes, and Jure Leskovec. Automated hypothesis validation with agentic sequential falsifications. In Proceedings of the 42nd International Conference on Machine Learning, 2025. [43] Hongliu Cao, Ilias Driouich, Robin Singh, and Eoin Thomas. Multi-agent llm judge: automatic personalized llm judge design for evaluating natural language generation applications, 2025. [44] Reshmi Ghosh, Tianyi Yao, Lizzy Chen, Sadid Hasan, Tianwei Chen, Dario Bernal, Huitian Jiao, and HM Hossain. Sageval: The frontiers of satisfactory agent based nlg evaluation for reference-free openended text. arXiv preprint arXiv:2411.16077, 2024. [45] Jiatao Li, Mao Ye, Cheng Peng, Xunjian Yin, and Xiaojun Wan. Agent-x: Adaptive guideline-based 14 expert network for threshold-free ai-generated text detection. arXiv preprint arXiv:2505.15261, 2025. [46] Michael J. Ryan, Omar Shaikh, Aditri Bhagirath, Daniel Frees, William Held, and Diyi Yang. SynthesizeMe! inducing persona-guided prompts for personalized reward models in LLMs. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2025. [47] Ran Xu, Jingjing Chen, Jiayu Ye, Yu Wu, Jun Yan, Carl Yang, and Hongkun Yu. Incentivizing agentic reasoning in llm judges via tool-integrated reinforcement learning, 2025. [48] Mingchen Zhuge, Changsheng Zhao, Dylan R. Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, Yangyang Shi, Vikas Chandra, and Jrgen Schmidhuber. Agent-as-ajudge: Evaluate agents with agents. In Forty-second International Conference on Machine Learning, 2025. [49] Mengdi Li, Guanqiao Chen, Xufeng Zhao, Haochen Wen, Shu Yang, and Di Wang. Persrm-r1: Enhance personalized reward modeling with reinforcement learning, 2025. [50] Anikait Singh, Sheryl Hsu, Kyle Hsu, Eric Mitchell, Stefano Ermon, Tatsunori Hashimoto, Archit Sharma, and Chelsea Finn. FSPO: Few-shot preference optimization of synthetic preference data elicits LLM personalization to real users. In 2nd Workshop on Models of Human Feedback for AI Alignment, 2025. [51] Xinchen Wang, Pengfei Gao, Chao Peng, Ruida Hu, and Cuiyun Gao. Codevisionary: An agent-based framework for evaluating large language models in code generation, 2025. [52] Yu Li, Shenyu Zhang, Rui Wu, Xiutian Huang, Yongrui Chen, Wenhao Xu, Guilin Qi, and Dehai Min. Mateval: multi-agent discussion framework for advancing open-ended text evaluation. In Database Systems for Advanced Applications: 29th International Conference, DASFAA 2024, Gifu, Japan, July 2-5, 2024, Proceedings, Part VII, page 415426, Berlin, Heidelberg, 2024. Springer-Verlag. ISBN 978-981-97-5574-5. doi: 10. 1007/978-981-97-5575-2_31. URL https://doi. org/10.1007/978-981-97-5575-2_31. [53] Manya Wadhwa, Zayne Rea Sprague, Chaitanya Malaviya, Philippe Laban, Junyi Jessy Li, and Greg Durrett. Evalagents: Discovering implicit evaluation criteria from the web. In Second Conference on Language Modeling, 2025. URL https: //openreview.net/forum?id=erGpkHCybv. [54] Kaishuai Xu, Tiezheng Yu, Yi Cheng, Wenjun Hou, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, and Survey on Agent-as-a-Judge tional Conference on Learning Representations, 2023. [61] Lianghui Zhu, Xinggang Wang, and Xinlong Wang. JudgeLM: Fine-tuned large language models are scalable judges. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=xsELpEPn4A. [62] Yougang Lyu, Shijie Ren, Yue Feng, Zihan Wang, Zhumin Chen, Zhaochun Ren, and Maarten de Rijke. Self-adaptive cognitive debiasing for large language models in decision-making, 2025. URL https:// arxiv.org/abs/2504.04141. [63] Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, et al. survey of self-evolving agents: On path to artificial super intelligence. arXiv preprint arXiv:2507.21046, 2025. [64] Mahnaz Koupaee, Jake W. Vincent, Saab Mansour, Igor Shalyminov, Han He, Hwanjun Song, Raphael Shu, Jianfeng He, Yi Nian, Amy Wing-mei Wong, Kyu J. Han, and Hang Su. Faithful, unfaithful or ambiguous? multi-agent debate with initial stance for summary evaluation. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1220912246, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025. naacl-long.609. URL https://aclanthology. org/2025.naacl-long.609/. [65] Taaha Kazi, Ruiliang Lyu, Sizhe Zhou, Dilek Hakkani-Tr, and Gokhan Tur. Large language models as user-agents for evaluating task-orienteddialogue systems. In 2024 IEEE Spoken Language Technology Workshop (SLT), pages 913920. IEEE, 2024. [66] Ning Wu, Ming Gong, Linjun Shou, Shining Liang, and Daxin Jiang. Large language models are diverse role-players for summarization evaluation. In CCF international conference on natural language processing and Chinese computing, pages 695707. Springer, 2023. Wenjie Li. Learning to align multi-faceted evaluation: unified and robust framework. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Findings of the Association for Computational Linguistics: ACL 2025, pages 94889502, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-889176-256-5. doi: 10.18653/v1/2025.findings-acl. 494. URL https://aclanthology.org/2025. findings-acl.494/. [55] MohammadHossein Rezaei, Robert Vacareanu, Zihao Wang, Clinton Wang, Bing Liu, Yunzhong He, and Afra Feyza Akyrek. Online rubrics elicitation from pairwise comparisons. arXiv preprint arXiv:2510.07284, 2025. [56] Zhaopeng Feng, Jiayuan Su, Jiamei Zheng, Jiahan Ren, Yan Zhang, Jian Wu, Hongwei Wang, and Zuozhu Liu. M-MAD: Multidimensional multiagent debate for advanced machine translation evaluation. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 70847107, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.351. URL https:// aclanthology.org/2025.acl-long.351/. [57] Jiamin Su, Yibo Yan, Zhuoran Gao, Han Zhang, Cafes: colXiang Liu, and Xuming Hu. laborative multi-agent framework for multigranular multimodal essay scoring. arXiv preprint arXiv:2505.13965, 2025. [58] Sandeep Kumar, Abhijit Nargund, and Vivek Sridhar. Courteval: courtroom-based multi-agent evaluation framework. In Findings of the Association for Computational Linguistics: ACL 2025, pages 25875 25887, 2025. [59] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG evaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2511 2522, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.153. URL https://aclanthology. org/2023.emnlp-main.153/. [60] Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained evaluation capability in language models. In The Twelfth Interna-"
        }
    ],
    "affiliations": [
        "Huawei Technologies",
        "Shandong Jianzhu University",
        "The Hong Kong Polytechnic University",
        "University of Cambridge"
    ]
}