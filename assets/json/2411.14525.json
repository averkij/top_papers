{
    "paper_title": "SegBook: A Simple Baseline and Cookbook for Volumetric Medical Image Segmentation",
    "authors": [
        "Jin Ye",
        "Ying Chen",
        "Yanjun Li",
        "Haoyu Wang",
        "Zhongying Deng",
        "Ziyan Huang",
        "Yanzhou Su",
        "Chenglong Ma",
        "Yuanfeng Ji",
        "Junjun He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Computed Tomography (CT) is one of the most popular modalities for medical imaging. By far, CT images have contributed to the largest publicly available datasets for volumetric medical segmentation tasks, covering full-body anatomical structures. Large amounts of full-body CT images provide the opportunity to pre-train powerful models, e.g., STU-Net pre-trained in a supervised fashion, to segment numerous anatomical structures. However, it remains unclear in which conditions these pre-trained models can be transferred to various downstream medical segmentation tasks, particularly segmenting the other modalities and diverse targets. To address this problem, a large-scale benchmark for comprehensive evaluation is crucial for finding these conditions. Thus, we collected 87 public datasets varying in modality, target, and sample size to evaluate the transfer ability of full-body CT pre-trained models. We then employed a representative model, STU-Net with multiple model scales, to conduct transfer learning across modalities and targets. Our experimental results show that (1) there may be a bottleneck effect concerning the dataset size in fine-tuning, with more improvement on both small- and large-scale datasets than medium-size ones. (2) Models pre-trained on full-body CT demonstrate effective modality transfer, adapting well to other modalities such as MRI. (3) Pre-training on the full-body CT not only supports strong performance in structure detection but also shows efficacy in lesion detection, showcasing adaptability across target tasks. We hope that this large-scale open evaluation of transfer learning can direct future research in volumetric medical image segmentation."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 2 ] . e [ 1 5 2 5 4 1 . 1 1 4 2 : r SegBook: Simple Baseline and Cookbook for Volumetric Medical Image Segmentation Jin Ye1 Ying Chen1,2 Yanjun Li1,3 Haoyu Wang1 Zhongying Deng4 Chenglong Ma Ziyan Huang1 Yuanfeng Ji5 Yanzhou Su1 Junjun He1 1Shanghai AI Laboratory 2Xiamen University 3East China Normal University 4University of Cambridge {yejin, hejunjun}@pjlab.org.cn 5Stanford University Project Page: https://uni-medical.github.io/SegBook/index.html"
        },
        {
            "title": "Abstract",
            "content": "Computed Tomography (CT) is one of the most popular modalities for medical imaging. By far, CT images have contributed to the largest publicly available datasets for volumetric medical segmentation tasks, covering full-body anatomical structures. Large amounts of full-body CT images provide the opportunity to pre-train powerful models, e.g., STU-Net pre-trained in supervised fashion, to segment numerous anatomical structures. However, it remains unclear in which conditions these pre-trained models can be transferred to various downstream medical segmentation tasks, particularly segmenting the other modalities and diverse targets. To address this problem, large-scale benchmark for comprehensive evaluation is crucial for finding these conditions. Thus, we collected 87 public datasets varying in modality, target, and sample size to evaluate the transfer ability of full-body CT pre-trained models. We then employed representative model, STU-Net with multiple model scales, to conduct transfer learning across modalities and targets. Our experimental results show that (1) there may be bottleneck effect concerning the dataset size in fine-tuning, with more improvement on both smalland large-scale datasets than medium-size ones. (2) Models pre-trained on full-body CT demonstrate effective modality transfer, adapting well to other modalities such as MRI. (3) Pre-training on the full-body CT not only supports strong performance in structure detection but also shows efficacy in lesion detection, showcasing adaptability across target tasks. We hope that this large-scale open evaluation of transfer learning can direct future research in volumetric medical image segmentation."
        },
        {
            "title": "Introduction",
            "content": "Computed Tomography (CT) has emerged as one of the most indispensable modalities in medical imaging Panayides et al. [2020], primarily due to its ability to provide detailed cross-sectional images of the human body. Notably, CT images have contributed to the largest publicly available datasets for volumetric medical segmentation tasks, like TotalSegementor Wasserthal et al. [2023a] which covers Equal contribution. Corresponding author. full-body anatomical structures. The comprehensive coverage of full-body anatomical structures as well as large amount of image numbers can possibly provide full-body prior knowledge that is invaluable for holistic understanding of human anatomy. Such prior knowledge is beneficial for transferring to variety of downstream segmentation tasks. It thus offers us the opportunity to utilize full-body CT to explore models transfer capabilities in various downstream medical segmentation tasks, particularly segmenting the other modalities and diverse targets. This problem is interesting because if these full-body CT-based models can be well transferred, full-body CT data can potentially serve as foundations for wide range of medical segmentation tasks. To investigate the conditions under which these pre-trained models can be transferred to various downstream medical segmentation tasks, we first identify key factors likely to impact transfer effectiveness, such as dataset size, modality, segmentation target, and model size. The first three factors (dataset size, modality, and segmentation target) require benchmark with large-scale dataset size, various modalities, and diverse segmentation targets for comprehensive evaluations of these factors. The last factor, model sizes, necessitates model with adjustable parameter scales. Scalable and Transferable U-Net (STU-Net) Huang et al. [2023] is thus an ideal candidate due to its scalable model sizes and impressive transfer abilities to downstream tasks. However, no large-scale evaluation has been conducted to benchmark the efficacy of transfer learning in volumetric medical image segmentation, leaving some important questions unanswered, such as the impact of dataset scales, the role of modality/target in transfer ability, and so on. Therefore, in this work, we first collected 87 public datasets varying in modality (e.g., CT, MRI, PET), target (e.g., structure, lesion), and size (small, medium, large). We then employed STU-Net of different scales to conduct large-scale evaluation of transfer learning, focusing on modality and target transfer capabilities and the influence of dataset scales and model scales. We evaluate model transferability by training from scratch and fine-tuning pre-trained models on selected public datasets. Additionally, to demonstrate the level of performance in diverse tasks, we chose the significant training-from-scratch task-specific baseline nnU-Net Isensee et al. [2021] for comparison. Looking beyond STU-Net, our work benchmarks the transferability of large-scale supervised fullybody CT pre-training across 87 downstream datasets with diverse medical scenarios. We hope that the findings can provide insights for advancing transfer learning in volumetric medical images. In summary, our key findings include: When fine-tuning the model on datasets of different sizes, interestingly, we observed the improvement in model performance does not increase linearly with the size of the dataset. Specifically, fine-tuning significantly enhanced the performance on both small and large datasets, whereas the improvement was relatively modest on medium-sized datasets. This indicates that there may be bottleneck effect in which the benefits of fine-tuning decrease at certain data scales. Models pre-trained on the CT with the full-body structure can effectively transfer to other modalities such as MRI, regardless of whether the downstream task targets appeared during the pre-training. This indicates the models strong modality transfer capabilities. Pre-training on CT structural data demonstrates promising performance in structure detection and efficacy in lesion detection, showcasing adaptability to various medical targets."
        },
        {
            "title": "2 Materials and Methods",
            "content": "2.1 Downstream Datasets We assembled comprehensive collection of 87 public datasets, which span diverse data sizes, targets, and modalities, to conduct an extensive evaluation, aimed at benchmarking the efficacy of transfer learning in volumetric medical image segmentation. These datasets were meticulously curated to support segmentation tasks targeting structures, lesions, and their combinations, encapsulating wide range of imaging modalities, including CT, MRI, PET, and Ultrasound (US), as well as integrated modalities such as CT&PET. Additionally, structures can be further categorized into seen structures and unseen structures: the former refers to those that appeared in the training set, while the latter refers to those that did not. Table 1 provides detailed summary of the distribution of datasets across various targets and modalities. Furthermore, these datasets exhibit substantial variation in data 2 scale, ranging from as few as 11 cases to as many as 1,250 cases per dataset, thereby offering broad spectrum of data characteristics to robustly assess the performance of transfer learning strategies in medical imaging. We chose 100 as the boundary between small and medium categories and selected 400 as the boundary between medium and large categories. These categories respectively account for 40.23%, 42.53%, and 17.24% of the total dataset population, as detailed in Figure 3. This categorization facilitates structured analysis of transfer learning performance across varying dataset complexities. Figure 1: Overview of 87 datasets. Figure 2: Numbers of datasets with different modalities. Figure 3: Proportions of datasets in different scales. 3 Table 1: Summary of number of datasets across targets and modalities. Modality CT MRI Target Lesion datsets 11 Seen Sturctute 17 Unseen Structure Lesion &Sturcture 9 Lesion 12 Seen Sturctute 9 Unseen Structure 9 Lesion &Sturcture CT&PET Ultrasound Lesion Lesion Total 3 87 2.2 Pretrained Models To benchmark transfer learning with full-body CT supervised pretraining, we select STU-Net Huang et al. [2023], pre-trained on large-scale full-body CT dataset, Totalsegmentator Wasserthal et al. [2023a]. STU-Net introduces flexible architecture that scales adeptly from lightweight to super-large configurations, catering to broad spectrum of computational demands. The largest variant, STUNet-H, boasts staggering 1.4 billion parameters, demonstrating the models exceptional scalability and robustness. STU-Net enhances the traditional convolutional block of nnU-Net by varying the networks depth and width, which results in four distinct configurations: small, base, large, and huge. The pre-trained weights of STU-Net we used are trained in fully supervised manner for 4000 epochs on the TotalSegmentator dataset, which includes 1204 CT images annotated across 104 structures Wasserthal et al. [2023b]. This strategic utilization equips STU-Net with robust foundational knowledge, enabling it to excel in various downstream tasks with minimal fine-tuning. In this work, we assessed the impact of pre-trained STU-Net across three different model scales: base, large, and huge. To highlight the performance variability across various tasks, we also included the well-established nnU-Net Isensee et al. [2021] as task-specific baseline, which was trained from scratch for comparison. 2.3 Transfer Learning Setup We trained and evaluated STU-Net across 87 public datasets under the setting of supervised pretraining followed by fine-tuning. During fine-tuning on the downstream datasets, we initialize the segmentation head randomly while maintaining the pre-trained weights in the remaining layers, with all weights being learnable. The segmentation head uses 10 learning rate compared to other layers. To facilitate the comprehensive comparison, we also provide the results of training from scratch as baselines. Our experiments utilize the nnU-Net framework Isensee et al. [2021], with patch-based training and sliding-window inference approach. Data pre-processing and augmentation for specific downstream tasks follow the automatic settings of nnU-Net. 2.4 Evaluation Metric We employ the Dice Similarity Coefficient (DSC) as our primary metric to evaluate segmentation performance. We formulate the definitions of the DSC as follows: DSC(G, ) = 2G + , where G, denote the ground truth and the predicted mask, respectively. The DSC measures the overlap ratio between and G, where higher DSC value signifies enhanced segmentation outcomes."
        },
        {
            "title": "3 Experiments and Analysis",
            "content": "In this section, we present the outcomes of comprehensive set of experiments designed to evaluate the effectiveness of transfer learning in volumetric medical image segmentation. We investigate the benefit of fine-tuning in diverse scenarios, exploring how model size, dataset scale, modality, and target influence transfer capabilities. 3.1 Overall Performance To investigate the overall transfer learning performance of STU-Net, we conducted experiments on 87 downstream datasets. As shown in Table 2, we reported average DSC for various models and detailed DSC for three data scales: small (S), medium (M), and large (L). The models evaluated 4 Table 2: Dice Scores were calculated across 87 downstream datasets at different data scales: small (S), medium (M), and large (L). The symbol () denotes the improvement attributed to Pre-training (PT). Method PT Params TFLOPs Dataset Scale Average nnU-Net 31M 0.54 74.83 74.47 68.73 STU-Net-base STU-Net-base (base) STU-Net-large STU-Net-large (large) STU-Net-huge STU-Net-huge (huge) 58M 58M 440M 440M 1.4B 1.4B 0.51 0.51 3.81 3.81 12.60 12. 73.96 76.17 2.21 74.14 77.05 2.91 73.55 76.87 3.32 74.84 76.59 1.75 75.71 77.23 1.52 75.2 77.14 1.94 70.05 72.81 2.76 70.48 73.84 3.36 70.55 74.21 3.66 73.62 73.66 75.77 2.11 74.18 76.57 2.39 73.73 76.53 2.80 Figure 4: Violin plot for DSC for all 87 datasets with STU-Net in different scales. include nnU-Net, STU-Net-base, STU-Net-large, and STU-Net-huge, with and without pre-training (PT). With pre-training, the performance of STU-Net at different sizes consistently increases under different dataset scale settings. Notably, the average performance of larger models (i.e., STU-Net-large and STU-Net-huge) as well as their performance across different data scales is better than smaller models (i.e., nnU-Net and STU-Net-base). In addition, we have observed an intriguing phenomenon where models of various scales exhibit significantly higher performance gains when fine-tuned on both smalland large-scale datasets, approximately around 3%. However, on medium-scale datasets, the fine-tuning gains are only about 1%. It appears that the benefits of fine-tuning do not proportionally increase with the scale of the dataset. Figure 4 presents violin plots comparing the DSC distributions across 87 datasets for different scales of the STU-Net model. The results highlight that models with pre-training (e.g., base(PT) and large(PT)) consistently achieve higher median DSC scores with less variance, demonstrating the effectiveness of fine-tuning. While larger models such as large and huge show competitive median performance, their broader distributions indicate some instability across datasets. Fine-tuning these models improves both stability and overall performance. Summary: We observed clear enhancement in the models performance across various dataset sizes through fine-tuning techniques. Interestingly, the improvements were substantial for both small and large datasets, but are relatively less pronounced for medium-sized datasets. This suggests non-linear relationship between dataset size and performance gains, implying bottleneck effect where the benefits of fine-tuning diminish beyond certain data scale. 5 3.2 Transfer across Targets Table 3: Evaluation on the transferability across imaging modalities with STU-Net. Method nnU-Net STU-Net-base STU-Net-base (base) STU-Net-large STU-Net-large (large) STU-Net-huge STU-Net-huge (huge) PT Seen Structure CT Unseen Structure Lesion Seen Structure MRI Unseen Structure US CT&PET Lesion Lesion Lesion 82. 82.79 85.00 2.21 83.60 85.87 2.27 82.73 85.90 3.17 69.59 70.06 73.85 3.79 70.77 75.81 5.04 70.67 75.15 4.48 58.86 58.88 63.14 4.26 59.27 63.83 4.56 57.35 63.61 6.26 87. 86.67 87.47 0.80 87.03 87.58 0.55 86.94 87.59 0.65 83.27 82.20 82.62 0.42 81.86 82.89 1.03 82.11 83.49 1.38 68.50 68.15 69.14 0.99 68.43 69.70 1.27 68.54 69.45 0.91 49. 53.70 54.54 0.84 50.18 52.65 2.47 49.38 52.78 3.40 58.88 62.44 66.45 4.01 63.09 68.33 5.24 62.83 68.74 5.91 common scenario arises when an evaluation dataset from the same modality (i.e., CT) is available, but the target of interest differs between the upstream and downstream tasks. In this case, we evaluate STU-Nets ability to effectively generalize across these varying targets, as shown in Table 3. The experimental results demonstrate that fine-tuning significantly enhances the performance of STU-Net models on both structure and lesion segmentation tasks. Across the different model scales (base, large, huge), the training-from-scratch performance of STU-Net remains relatively consistent, indicating that model size alone does not drastically affect baseline performance. However, when it comes to fine-tuning improvements, larger models tend to show greater enhancements and higher gains. Summary: Pre-training on CT structure data not only yields excellent results in structure detection but also shows effective lesion detection, showcasing notable degree of adaptability to the target task. 3.3 Transfer across Modalities 3.3.1 Modality Transfer with Seen Targets Table 3 presents the performance of the STU-Net model across different imaging modalities. We first focus on the transfer from CT to MRI, where the structure has been seen during CT-based pre-training. The findings demonstrate STU-Nets ability to effectively generalize across modalities when the target category remains consistent. We also observed that, across models of various scales, fine-tuning consistently leads to an improvement in DSC compared to training from scratch, demonstrating the effectiveness of modal transfer with seen targets. 3.3.2 Modality Transfer with Unseen Targets Table 3 also demonstrates the transferability of models across different modalities and targets, specifically evaluating the DSC on datasets with unseen modalities and targets. We find that, despite targets being unseen, fine-tuning still leads to an improvement in DSC compared to training from scratch across all modalities and model scales, demonstrating the strong ability of modality transfer. For single modalities, larger models also tend to show greater improvements from fine-tuning, which is more obvious to US transfer, with (huge) of 0.84, 2.47, and 3.40 in STU-Net-base, STU-Netlarge and STU-Net, respectively. It shows that larger models can be adapted to complex and diverse datasets more effectively. Summary: Models pre-trained on CT scans that include the full-body structure can effectively transfer to other modalities like MRI. This effectiveness holds regardless of whether the specific downstream task targets were present during pre-training, highlighting the models strong modality transfer capabilities. 6 Table 4: Evaluation on the transferability across different structures. PT Head and Neck Pelvic Limb Thoracic Abdominal other Bone Vessel 79. 75.85 80.68 4.83 79.75 81.51 1.76 78.99 80.37 1.38 84.2 62.9 84.11 84.99 0.88 84.52 85.15 0.63 84.29 85.99 1.70 66.12 72.74 6.62 66.31 74.16 7.85 65.88 73.97 8.09 73. 73.48 74.12 0.64 74.00 74.80 0.8 73.90 74.92 1.02 89.52 89.61 89.71 0.10 89.93 90.28 0.35 89.85 90.37 0.52 63.84 65.47 80. 65.98 68.22 2.24 66.59 68.82 2.23 65.99 69.86 3.87 67.10 71.29 4.19 67.75 72.39 4.64 67.32 72.33 5.01 80.19 81.24 1.05 80.86 82.06 1.20 80.53 82.00 1.47 Method nnU-Net STU-Net-base STU-Net-base (base) STU-Net-large STU-Net-large (large) STU-Net-huge STU-Net-huge (huge) 3.4 Transferability for Different Structures As shown in Table 4, models pre-trained on CT structural data demonstrate effective transferability across wide range of anatomical structures. The consistent improvements across different regions, especially with fine-tuning, suggest that these pre-trained models capture meaningful representations that generalize well beyond their initial training data. STU-Net-large and STU-Net-huge models, in particular, show strong adaptability, achieving high performance in complex regions like the thoracic and abdominal structures. Although in certain regions, such as the pelvic area, the models exhibit limited gains (), the overall results validate the versatility of the pre-trained models with higher scores. This highlights the potential of using CT-based pre-training as foundational step to enhance model generalization across diverse anatomical contexts, enabling broader applicability in medical imaging tasks. 3.5 Transferability for Bone and Vessel Table 4 presents the evaluation results for bone and vessel segmentation tasks. Pre-training on CT structural data proves to be highly effective for transferring knowledge to complex segmentation tasks, such as bone and vessel segmentation. The consistent performance improvements across all models, especially after fine-tuning, demonstrate the robustness and adaptability of these pretrained models. STU-Net-huges notable 5.01% DSC gain in bone segmentation highlights that even challenging anatomical structures benefit significantly from pre-trained knowledge. This suggests that CT-based pre-training equips models with generalizable features, enabling efficient adaptation to various medical imaging tasks, including those requiring high precision in fine-grained regions like bones and vessels."
        },
        {
            "title": "4 Conclusion and Future Work",
            "content": "4.1 Conclusion In this study, we examine the transfer ability of full-body CT pre-trained models. We collected largescale public datasets varying in modality, target, and size. We utilized STU-Net of different scales to conduct series of transfer learning experiments for volumetric medical image segmentation. We compare overall performance to measure the effectiveness of modality and target transfers, leveraging various datasets and model scales. We hope that this large-scale evaluation of transfer learning can direct future research on volumetric medical image segmentation. 4.2 Future Work In our research, we mainly explored full-body CT-structure pre-training transferring to other modalities and targets and observed strong effectiveness. In future work, we will consider exploring the transfer effect between more models and targets and also consider using different fine-tuning techniques for further exploration. We hope that our research can bring more guidance for pre-training and transfer learning in volumetric medical image segmentation. 7 Table 5: Detailed datsets Dataset Task001-BrainTumour Antonelli et al. [2022] Task002-Heart Antonelli et al. [2022] Task003-Liver Antonelli et al. [2022] Task004-Hippocampus Antonelli et al. [2022] Task005-Prostate Antonelli et al. [2022] Task006-Lung Antonelli et al. [2022] Task007-Pancreas Antonelli et al. [2022] Task008-HepaticVessel Antonelli et al. [2022] Task009-Spleen Antonelli et al. [2022] Task010-Colon Antonelli et al. [2022] Task011-BTCV Landman et al. [2015] Task012-BTCV-Cervix Landman et al. [2015] Task013-ACDC Bernard et al. [2018] Task019-BraTS21 Baid et al. [2021] Task020-AbdomenCT1K Ma et al. [2021] Task021-KiTS2021 Heller et al. [2023] Task023-FLARE22 Ma et al. [2023] Task029-LITS Bilic et al. [2023] Task034-Instance22 Li et al. [2023] Task036-KiPA22 He et al. [2020] Task037-CHAOS-Task-3-5-Variant1 Kavur et al. [2021] Task039-Parse22 Luo et al. [2023a] Task040-ATM22 Zhang et al. [2023] Task041-ISLES2022 Hernandez Petzsche et al. [2022] Task044-CrossMoDA23 DOR [2023] Task044-KiTS23 Heller et al. [2021] Task050-LAScarQS22-task1 Li et al. [2022] Task051-AMOS-CT Ji et al. [2022] Task051-LAScarQS22-task2 Li et al. [2022] Task052-AMOS-MR Ji et al. [2022] Task053-AMOS-Task2 Ji et al. [2022] Task083-VerSe2020 Sekuboyina et al. [2021] Task103-ADAM2020 Fang et al. [2022] Task104-Colorectal-Liver-Metastases Simpson et al. [2024] Task105-DICOM-LIDC-IDRI-Nodules Fedorov et al. [2018] Task106-AIIB2023 Nan et al. [2023] Task107-HCC-TACE-Seg Moawad et al. [2021] Task108-ISBI-MR-Prostate-2013 Bloch et al. [2015] Task109-SMILE-UHURA2023 Organizers [2023b] Task110-ISPY1-Tumor-SEG-Radiomics Chitalia et al. [2022] Task111-LUAD-CT-Survival Goldgof Dmitry et al. [2017] Task112-PROSTATEx-Seg-HiRes Schindele et al. [2020] Task113-PROSTATEx-Seg-Zones Schindele et al. [2020] Task114-Prostate-Anatomical-Edge-Cases Thompson et al. [2023] Task115-QIBA-VolCT-1B McNitt-Gray et al. [2015] Task116-ISPY1 Chitalia et al. [2022]"
        },
        {
            "title": "Modality\nMRI\nMRI\nCT\nMRI\nMRI\nCT\nCT\nCT\nCT\nCT\nCT\nCT\nMRI\nMRI\nCT\nCT\nCT\nCT\nCT\nCT",
            "content": "MRI CT CT MRI MRI CT MRI CT MRI MRI MRI CT MRI CT CT"
        },
        {
            "title": "CT\nCT\nMRI\nMRI",
            "content": "MRI CT MRI MRI CT CT MRI 8 Target Lesion Seen Structure Structure&Lesion Unseen Structure Seen Structure Lesion Structure&Lesion Structure&Lesion Seen Structure Lesion Seen Structure Seen Structure Seen Structure Lesion Seen Structure Structure&Lesion Seen Structure Structure&Lesion Unseen Structure Structure&Lesion Seen Structure Seen Structure Unseen Structure Lesion Structure&Lesion Structure&Lesion Seen Structure Seen Structure Seen Structure Seen Structure Seen Structure Seen Structure Structure&Lesion Structure&Lesion Case 484 20 130 260 31 63 280 303 40 125 30 30 200 1250 1000 300 70 130 100 70 40 100 300 250 226 489 60 300 130 60 360 350 196 Unseen Structure 1018 Unseen Structure Structure&Lesion Unseen Structure Unseen Structure Lesion Lesion Unseen Structure Unseen Structure Seen Structure Lesion Structure&Lesion 120 224 79 11 40 65 98 130 149 820 Task166-Longitudinal Multiple Sclerosis Lesion Segmentation Carass et al. [2017] Task502-WMH Kuijf et al. [2019] Task503-BraTs2015 Menze et al. [2014a] Task504-ATLAS LaBella et al. [2023] Task507-Myops2020 Luo and Zhuang [2022] Task511-ATLAS2023 Quinton et al. [2023] Task525-CMRxMotions Wang et al. [2022] Task556-FeTA2022-all Payette et al. [2021] Task559-WORD Luo et al. [2022] Task601-CTSpine1K-Full Deng et al. [2021] Task603-MMWHS Gonzalez Serrano [2019] Task605-SegThor Lambert et al. [2020] Task606-orCaScore Wolterink et al. [2016] Task611-PROMISE12 Litjens et al. [2014] Task612-CTPelvic1k Liu et al. [2021] Task613-COVID-19-20 Roth et al. [2022] Task614-LUNA16 Setio et al. [2017] Task615-Chest-CT-Scans-with-COVID-19 Task616-LNDb Pedrosa et al. [2019] Task628-StructSeg2019-subtask1 Heimann et al. [2009] Task629-StructSeg2019-subtask2 Heimann et al. [2009] Task630-StructSeg2019-subtask3 Heimann et al. [2009] Task631-StructSeg2019-subtask4 Heimann et al. [2009] Task666-MESSEG Styner et al. [2008] Task700-SEG-A-2023 Radl et al. [2022] Task701-LNQ2023 Organizers [2023a] Task701-SegRap2023 Luo et al. [2023b] Task702-CAS2023 Chen et al. [2023] Task702-SegRap2023-Task2 Luo et al. [2023b] Task703-TDSC-ABUS2023 Zhou et al. [2021] Task704-ToothFairy2023 Cipriano et al. [2022] Task710-autoPET Gatidis et al. [2022] Task711-autoPET-PET-only Gatidis et al. [2022] Task712-autoPET-CT-only Gatidis et al. [2022] Task720-HIE2023 Bao et al. [2023] Task894-BraTS2023-MET Moawad et al. [2023] Task895-BraTS2023-SSA Adewole et al. [2023] Task896-BraTS2023-PED Kazerooni et al. [2023] Task898-BraTS2023-MEN LaBella et al. [2023] Task899-BraTS2023-GLI Menze et al. [2014b] Task966-HaN-Seg Podobnik et al. [2023]"
        },
        {
            "title": "MRI\nMRI\nMRI\nMRI\nMRI\nMRI\nMRI\nCT\nCT\nCT\nCT\nCT\nMRI\nCT\nCT\nCT\nCT\nCT",
            "content": "CT CT CT CT MRI CT CT CT MRI CT Ultrasound CT CT&PET CT&PET CT&PET MRI MRI MRI MRI MRI MRI CT"
        },
        {
            "title": "Lesion",
            "content": "Unseen Structure Structure&Lesion Lesion Structure&Lesion Structure&Lesion Seen Structure Unseen Structure Seen Structure Seen Structure Seen Structure Seen Structure Unseen Structure Unseen Structure Seen Structure Lesion Unseen Structure Lesion Lesion Unseen Structure Seen Structure Lesion Lesion Lesion Seen Structure Lesion Seen Structure Unseen Structure Lesion Lesion Unseen Structure Lesion Lesion Lesion Lesion Lesion Lesion Lesion Lesion Structure&Lesion Unseen Structure 20 60 274 655 25 60 139 120 120 1005 40 40 31 50 1105 199 888 50 235 50 50 50 40 55 393 120 100 120 100 153 1014 500 1014 85 238 43 99 1000"
        },
        {
            "title": "References",
            "content": "Crossmoda 2021 challenge: Benchmark of cross-modality domain adaptation techniques for vestibular schwannoma and cochlea segmentation. Medical Image Analysis, 83:102628, 2023. ISSN 13618415. doi: https://doi.org/10.1016/j.media.2022.102628. M. Adewole, J. D. Rudie, A. Gbadamosi, O. Toyobo, C. Raymond, D. Zhang, O. Omidiji, R. Akinola, M. A. Suwaid, A. Emegoakor, N. Ojo, K. Aguh, C. Kalaiwo, G. Babatunde, A. Ogunleye, Y. Gbadamosi, K. Iorpagher, E. Calabrese, M. Aboian, M. Linguraru, J. Albrecht, B. Wiestler, F. Kofler, A. Janas, D. LaBella, A. F. Kzerooni, H. B. Li, J. E. Iglesias, K. Farahani, J. Eddy, 9 T. Bergquist, V. Chung, R. T. Shinohara, W. Wiggins, Z. Reitman, C. Wang, X. Liu, Z. Jiang, A. Familiar, K. V. Leemput, C. Bukas, M. Piraud, G.-M. Conte, E. Johansson, Z. Meier, B. H. Menze, U. Baid, S. Bakas, F. Dako, A. Fatade, and U. C. Anazodo. The brain tumor segmentation (brats) challenge 2023: Glioma segmentation in sub-saharan africa patient population (brats-africa), 2023. M. Antonelli, A. Reinke, S. Bakas, K. Farahani, A. Kopp-Schneider, B. A. Landman, G. Litjens, B. Menze, O. Ronneberger, R. M. Summers, et al. The medical segmentation decathlon. Nature communications, 13(1):4128, 2022. U. Baid, S. Ghodasara, S. Mohan, M. Bilello, E. Calabrese, E. Colak, K. Farahani, J. KalpathyCramer, F. C. Kitamura, S. Pati, et al. The rsna-asnr-miccai brats 2021 benchmark on brain tumor segmentation and radiogenomic classification. arXiv preprint arXiv:2107.02314, 2021. R. Bao, Y. Song, S. V. Bates, R. J. Weiss, A. N. Foster, C. J. Cobos, S. Sotardi, Y. Zhang, R. L. Gollub, P. E. Grant, et al. Boston neonatal brain injury dataset for hypoxic ischemic encephalopathy (bonbid-hie): Part i. mri and manual lesion annotation. bioRxiv, 2023. O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng, I. Cetin, K. Lekadir, O. Camara, M. A. G. Ballester, et al. Deep learning techniques for automatic mri cardiac multistructures segmentation and diagnosis: is the problem solved? IEEE transactions on medical imaging, 37(11):25142525, 2018. P. Bilic, P. Christ, H. B. Li, E. Vorontsov, A. Ben-Cohen, G. Kaissis, A. Szeskin, C. Jacobs, G. E. H. Mamani, G. Chartrand, et al. The liver tumor segmentation benchmark (lits). Medical Image Analysis, 84:102680, 2023. N. Bloch, A. Madabhushi, H. Huisman, J. Freymann, J. Kirby, M. Grauer, A. Enquobahrie, C. Jaffe, L. Clarke, and K. Farahani. Nci-isbi 2013 challenge: Automated segmentation of prostate structures. The Cancer Imaging Archive, 2015. URL http://doi.org/10.7937/K9/TCIA. 2015.zF0vlOPv. DOI: 10.7937/K9/TCIA.2015.zF0vlOPv. A. Carass, S. Roy, A. Jog, J. L. Cuzzocreo, E. Magrath, A. Gherman, J. Button, J. Nguyen, F. Prados, C. H. Sudre, et al. Longitudinal multiple sclerosis lesion segmentation: resource and challenge. NeuroImage, 148:77102, 2017. H. Chen, X. Zhao, H. Sun, J. Dou, C. Du, R. Yang, X. Lin, H. Jiang, S. Yu, J. Liu, Z. Han, C. Yuan, and N. Balu. Cerebral artery segmentation challenge (cas) 2023. Hosted on CodaLab, 2023. URL https://codalab.lisn.upsaclay.fr/competitions/9804#learn_the_details. R. Chitalia, S. Pati, M. Bhalerao, S. P. Thakur, N. Jahani, V. Belenky, E. S. McDonald, J. Gibbs, D. C. Newitt, N. M. Hylton, et al. Expert tumor annotations and radiomics for locally advanced breast cancer in dce-mri for acrin 6657/i-spy1. Scientific data, 9(1):440, 2022. M. Cipriano, S. Allegretti, F. Bolelli, F. Pollastri, and C. Grana. Improving segmentation of the inferior alveolar nerve through deep label propagation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2113721146. IEEE, Jun 2022. Y. Deng, C. Wang, Y. Hui, Q. Li, J. Li, S. Luo, M. Sun, Q. Quan, S. Yang, Y. Hao, et al. Ctspine1k: large-scale dataset for spinal vertebrae segmentation in computed tomography. arXiv preprint arXiv:2105.14711, 2021. H. Fang, F. Li, H. Fu, X. Sun, X. Cao, F. Lin, J. Son, S. Kim, G. Quellec, S. Matta, et al. Adam challenge: Detecting age-related macular degeneration from fundus images. IEEE transactions on medical imaging, 41(10):28282847, 2022. A. Fedorov, M. Hancock, D. Clunie, M. Brockhhausen, J. Bona, J. Kirby, J. Freymann, H. Aerts, R. Kikinis, and F. Prior. Standardized representation of the tcia lidc-idri annotations using dicom. Published online, 10, 2018. S. Gatidis, T. Hepp, M. Früh, C. La Fougère, K. Nikolaou, C. Pfannenberg, B. Schölkopf, T. Küstner, C. Cyran, and D. Rubin. whole-body fdg-pet/ct dataset with manually annotated tumor lesions. Scientific Data, 9(1):601, 2022. 10 H. Goldgof Dmitry, H. Samuel, S. Matthew, S. Olya, G. Alberto, B. Yoganand, K. Jongphil, E. Steven, B. Anders, G. Robert, et al. Long and short survival in adenocarcinoma lung cts. The Cancer Imaging Archive, 2017. G. Gonzalez Serrano. Cad-pe: dataset for the detection of pelvic organs in medical imaging. IEEE DataPort, 2019. URL https://ieee-dataport.org/open-access/cad-pe. Y. He, G. Yang, J. Yang, Y. Chen, Y. Kong, J. Wu, L. Tang, X. Zhu, J.-L. Dillenseger, P. Shao, et al. Dense biased networks with deep priori anatomy and hard region adaptation: Semi-supervised learning for fine renal artery segmentation. Medical image analysis, 63:101722, 2020. T. Heimann, B. Van Ginneken, M. A. Styner, Y. Arzhaeva, V. Aurich, C. Bauer, A. Beck, C. Becker, R. Beichel, G. Bekes, et al. Comparison and evaluation of methods for liver segmentation from ct datasets. IEEE transactions on medical imaging, 28(8):12511265, 2009. N. Heller, F. Isensee, and K. H. The state of the art in kidney and kidney tumor segmentation in contrast-enhanced ct imaging: Results of the kits19 challenge. Medical Image Analysis, 67:101821, 2021. ISSN 1361-8415. doi: 10.1016/j.media.2020.101821. URL https://www. sciencedirect.com/science/article/pii/S1361841520301857. N. Heller, F. Isensee, D. Trofimova, R. Tejpaul, Z. Zhao, H. Chen, L. Wang, A. Golts, D. Khapun, D. Shats, et al. The kits21 challenge: Automatic segmentation of kidneys, renal tumors, and renal cysts in corticomedullary-phase ct. arXiv preprint arXiv:2307.01984, 2023. M. R. Hernandez Petzsche, E. de la Rosa, U. Hanning, R. Wiest, W. Valenzuela, M. Reyes, M. Meyer, S.-L. Liew, F. Kofler, I. Ezhov, et al. Isles 2022: multi-center magnetic resonance imaging stroke lesion segmentation dataset. Scientific data, 9(1):762, 2022. Z. Huang, H. Wang, Z. Deng, J. Ye, Y. Su, H. Sun, J. He, Y. Gu, L. Gu, S. Zhang, et al. Stunet: Scalable and transferable medical image segmentation models empowered by large-scale supervised pre-training. arXiv preprint arXiv:2304.06716, 2023. F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein. nnu-net: self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2):203211, 2021. Y. Ji, H. Bai, C. Ge, J. Yang, Y. Zhu, R. Zhang, Z. Li, L. Zhanng, W. Ma, X. Wan, et al. Amos: large-scale abdominal multi-organ benchmark for versatile medical image segmentation. Advances in neural information processing systems, 35:3672236732, 2022. A. E. Kavur, N. S. Gezer, M. Barıs, S. Aslan, P.-H. Conze, V. Groza, D. D. Pham, S. Chatterjee, P. Ernst, S. Özkan, et al. Chaos challenge-combined (ct-mr) healthy abdominal organ segmentation. Medical Image Analysis, 69:101950, 2021. A. F. Kazerooni, N. Khalili, X. Liu, D. Haldar, Z. Jiang, S. M. Anwar, J. Albrecht, M. Adewole, U. Anazodo, H. Anderson, S. Bagheri, U. Baid, T. Bergquist, A. J. Borja, E. Calabrese, V. Chung, G.-M. Conte, F. Dako, J. Eddy, I. Ezhov, A. Familiar, K. Farahani, S. Haldar, J. E. Iglesias, A. Janas, E. Johansen, B. V. Jones, F. Kofler, D. LaBella, H. A. Lai, K. V. Leemput, H. B. Li, N. Maleki, A. S. McAllister, Z. Meier, B. Menze, A. W. Moawad, K. K. Nandolia, J. Pavaine, M. Piraud, T. Poussaint, S. P. Prabhu, Z. Reitman, A. Rodriguez, J. D. Rudie, I. S. Shaikh, L. M. Shah, N. Sheth, R. T. Shinohara, W. Tu, K. Viswanathan, C. Wang, J. B. Ware, B. Wiestler, W. Wiggins, A. Zapaishchykova, M. Aboian, M. Bornhorst, P. de Blank, M. Deutsch, M. Fouladi, L. Hoffman, B. Kann, M. Lazow, L. Mikael, A. Nabavizadeh, R. Packer, A. Resnick, B. Rood, A. Vossough, S. Bakas, and M. G. Linguraru. The brain tumor segmentation (brats) challenge 2023: Focus on pediatrics (cbtn-connect-dipgr-asnr-miccai brats-peds), 2023. H. J. Kuijf, J. M. Biesbroek, J. De Bresser, R. Heinen, S. Andermatt, M. Bento, M. Berseth, M. Belyaev, M. J. Cardoso, A. Casamitjana, et al. Standardized assessment of automatic segmentation of white matter hyperintensities and results of the wmh segmentation challenge. IEEE transactions on medical imaging, 38(11):25562568, 2019. 11 D. LaBella, M. Adewole, M. Alonso-Basanta, T. Altes, S. M. Anwar, U. Baid, T. Bergquist, R. Bhalerao, S. Chen, V. Chung, G.-M. Conte, F. Dako, J. Eddy, I. Ezhov, D. Godfrey, F. Hilal, A. Familiar, K. Farahani, J. E. Iglesias, Z. Jiang, E. Johanson, A. F. Kazerooni, C. Kent, J. Kirkpatrick, F. Kofler, K. V. Leemput, H. B. Li, X. Liu, A. Mahtabfar, S. McBurney-Lin, R. McLean, Z. Meier, A. W. Moawad, J. Mongan, P. Nedelec, M. Pajot, M. Piraud, A. Rashid, Z. Reitman, R. T. Shinohara, Y. Velichko, C. Wang, P. Warman, W. Wiggins, M. Aboian, J. Albrecht, U. Anazodo, S. Bakas, A. Flanders, A. Janas, G. Khanna, M. G. Linguraru, B. Menze, A. Nada, A. M. Rauschecker, J. Rudie, N. H. Tahon, J. Villanueva-Meyer, B. Wiestler, and E. Calabrese. The asnr-miccai brain tumor segmentation (brats) challenge 2023: Intracranial meningioma, 2023. Z. Lambert, C. Petitjean, B. Dubray, and S. Kuan. Segthor: Segmentation of thoracic organs at risk in ct images. In 2020 Tenth International Conference on Image Processing Theory, Tools and Applications (IPTA), pages 16. IEEE, 2020. B. Landman, Z. Xu, J. Igelsias, M. Styner, T. Langerak, and A. Klein. Miccai multi-atlas labeling beyond the cranial vaultworkshop and challenge. In Proc. MICCAI Multi-Atlas Labeling Beyond Cranial VaultWorkshop Challenge, volume 5, page 12, 2015. L. Li, V. A. Zimmer, J. A. Schnabel, and X. Zhuang. Atrialjsqnet: new framework for joint segmentation and quantification of left atrium and scars incorporating spatial and shape information. Medical image analysis, 76:102303, 2022. X. Li, G. Luo, K. Wang, H. Wang, J. Liu, X. Liang, J. Jiang, Z. Song, C. Zheng, H. Chi, et al. The state-of-the-art 3d anisotropic intracranial hemorrhage segmentation on non-contrast head ct: The instance challenge. arXiv preprint arXiv:2301.03281, 2023. G. Litjens, R. Toth, W. Van De Ven, C. Hoeks, S. Kerkstra, B. Van Ginneken, G. Vincent, G. Guillard, N. Birbeck, J. Zhang, et al. Evaluation of prostate segmentation algorithms for mri: the promise12 challenge. Medical image analysis, 18(2):359373, 2014. P. Liu, H. Han, Y. Du, H. Zhu, Y. Li, F. Gu, H. Xiao, J. Li, C. Zhao, L. Xiao, et al. Deep learning to segment pelvic bones: large-scale ct datasets and baseline models. International Journal of Computer Assisted Radiology and Surgery, 16:749756, 2021. G. Luo, K. Wang, J. Liu, S. Li, X. Liang, X. Li, S. Gan, W. Wang, S. Dong, W. Wang, et al. Efficient automatic segmentation for multi-level pulmonary arteries: The parse challenge. arXiv preprint arXiv:2304.03708, 2023a. X. Luo and X. Zhuang. X-metric: An n-dimensional information-theoretic framework for groupwise registration and deep combined computing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(7):92069224, 2022. X. Luo, W. Liao, J. Xiao, J. Chen, T. Song, X. Zhang, K. Li, D. N. Metaxas, G. Wang, and S. Zhang. Word: large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from ct image. Medical Image Analysis, 82:102642, 2022. X. Luo, J. Fu, Y. Zhong, S. Liu, B. Han, M. Astaraki, S. Bendazzoli, I. Toma-Dasu, Y. Ye, Z. Chen, et al. Segrap2023: benchmark of organs-at-risk and gross tumor volume segmentation for radiotherapy planning of nasopharyngeal carcinoma. arXiv preprint arXiv:2312.09576, 2023b. J. Ma, Y. Zhang, S. Gu, C. Zhu, C. Ge, Y. Zhang, X. An, C. Wang, Q. Wang, X. Liu, et al. Abdomenct1k: Is abdominal organ segmentation solved problem? IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):66956714, 2021. J. Ma, Y. Zhang, S. Gu, C. Ge, S. Ma, A. Young, C. Zhu, K. Meng, X. Yang, Z. Huang, et al. Unleashing the strengths of unlabeled data in pan-cancer abdominal organ quantification: the flare22 challenge. arXiv preprint arXiv:2308.05862, 2023. M. F. McNitt-Gray, G. H. Kim, B. Zhao, L. H. Schwartz, D. Clunie, K. Cohen, N. Petrick, C. Fenimore, Z. J. Lu, and A. J. Buckler. Determining the variability of lesion size measurements from ct patient data sets acquired under no change conditions. Translational oncology, 8(1):5564, 2015. 12 B. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, Y. Burren, N. Porz, J. Slotboom, R. Wiest, L. Lanczi, E. Gerstner, M.-A. Weber, T. Arbel, B. Avants, N. Ayache, P. Buendia, L. Collins, N. Cordier, J. Corso, A. Criminisi, T. Das, H. Delingette, C. Demiralp, C. Durst, M. Dojat, S. Doyle, J. Festa, F. Forbes, E. Geremia, B. Glocker, P. Golland, X. Guo, A. Hamamci, K. Iftekharuddin, R. Jena, N. John, E. Konukoglu, D. Lashkari, J. Antonio Mariz, R. Meier, S. Pereira, D. Precup, S. J. Price, T. Riklin-Raviv, S. Reza, M. Ryan, L. Schwartz, H.-C. Shin, J. Shotton, C. Silva, N. Sousa, N. Subbanna, G. Szekely, T. Taylor, O. Thomas, N. Tustison, G. Unal, F. Vasseur, M. Wintermark, D. Hye Ye, L. Zhao, B. Zhao, D. Zikic, M. Prastawa, M. Reyes, and K. Van Leemput. The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS). IEEE Transactions on Medical Imaging, page 33, 2014a. doi: 10.1109/TMI.2014.2377694. URL https://hal.inria.fr/hal-00935640. B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, Y. Burren, N. Porz, J. Slotboom, R. Wiest, et al. The multimodal brain tumor image segmentation benchmark (brats). IEEE transactions on medical imaging, 34(10):19932024, 2014b. A. Moawad, D. Fuentes, A. Morshid, A. Khalaf, M. Elmohr, A. Abusaif, J. Hazle, A. Kaseb, M. Hassan, A. Mahvash, et al. Multimodality annotated hcc cases with and without advanced imaging segmentation [data set]. The Cancer Imaging Archive, 2021. A. W. Moawad, A. Janas, U. Baid, D. Ramakrishnan, L. Jekel, K. Krantchev, H. Moy, R. Saluja, K. Osenberg, K. Wilms, M. Kaur, A. Avesta, G. C. Pedersen, N. Maleki, M. Salimi, S. Merkaj, M. von Reppert, N. Tillmans, J. Lost, K. Bousabarah, W. Holler, M. Lin, M. Westerhoff, R. Maresca, K. E. Link, N. hoda Tahon, D. Marcus, A. Sotiras, P. LaMontagne, S. Chakrabarty, O. Teytelboym, A. Youssef, A. Nada, Y. S. Velichko, N. Gennaro, C. Students, G. of Annotators, J. Cramer, D. R. Johnson, B. Y. M. Kwan, B. Petrovic, S. N. Patro, L. Wu, T. So, G. Thompson, A. Kam, G. G. Perez-Carrillo, N. Lall, G. of Approvers, J. Albrecht, U. Anazodo, M. G. Lingaru, B. H. Menze, B. Wiestler, M. Adewole, S. M. Anwar, D. Labella, H. B. Li, J. E. Iglesias, K. Farahani, J. Eddy, T. Bergquist, V. Chung, R. T. Shinohara, F. Dako, W. Wiggins, Z. Reitman, C. Wang, X. Liu, Z. Jiang, K. V. Leemput, M. Piraud, I. Ezhov, E. Johanson, Z. Meier, A. Familiar, A. F. Kazerooni, F. Kofler, E. Calabrese, S. Aneja, V. Chiang, I. Ikuta, U. Shafique, F. Memon, G. M. Conte, S. Bakas, J. Rudie, and M. Aboian. The brain tumor segmentation (brats-mets) challenge 2023: Brain metastasis segmentation on pre-treatment mri, 2023. Y. Nan, J. Del Ser, Z. Tang, P. Tang, X. Xing, Y. Fang, F. Herrera, W. Pedrycz, S. Walsh, and G. Yang. Fuzzy attention neural network to tackle discontinuity in airway segmentation. IEEE Transactions on Neural Networks and Learning Systems, 2023. M. L. N. Q. L. C. Organizers. Mediastinal lymph node quantification (lnq): Segmentation of heterogeneous ct data. LNQ 2023 Grand Challenge, 2023a. URL https://lnq2023.grand-challenge. org/lnq2023/. Accessed: 2024-11-06. S.-U. C. . Organizers. Smile-uhura challenge 2023: Small vessel segmentation at mesoscopic scale from ultra-high resolution 7t magnetic resonance angiograms, 2023b. URL https://www. synapse.org/Synapse:syn47164761/wiki/620033. A. S. Panayides, A. Amini, N. D. Filipovic, A. Sharma, S. A. Tsaftaris, A. Young, D. Foran, N. Do, S. Golemati, T. Kurc, et al. Ai in medical imaging informatics: current challenges and future directions. IEEE journal of biomedical and health informatics, 24(7):18371857, 2020. K. Payette, P. de Dumast, H. Kebiri, I. Ezhov, J. C. Paetzold, S. Shit, A. Iqbal, R. Khan, R. Kottke, P. Grehten, et al. An automatic multi-tissue human fetal brain segmentation benchmark using the fetal tissue annotation dataset. Scientific data, 8(1):167, 2021. J. Pedrosa, G. Aresta, C. Ferreira, M. Rodrigues, P. Leitão, A. S. Carvalho, J. Rebelo, E. Negrão, I. Ramos, A. Cunha, et al. Lndb: lung nodule database on computed tomography. arXiv preprint arXiv:1911.08434, 2019. G. Podobnik, P. Strojan, P. Peterlin, B. Ibragimov, and T. Vrtovec. Han-seg: The head and neck organ-at-risk ct and mr segmentation dataset. Medical physics, 50(3):19171927, 2023. 13 F. Quinton, R. Popoff, B. Presles, S. Leclerc, F. Meriaudeau, G. Nodari, O. Lopez, J. Pellegrinelli, O. Chevallier, D. Ginhac, et al. tumour and liver automatic segmentation (atlas) dataset on contrast-enhanced magnetic resonance imaging for hepatocellular carcinoma. Data, 8(5):79, 2023. L. Radl, Y. Jin, A. Pepe, J. Li, C. Gsaxner, F.-h. Zhao, and J. Egger. Avt: Multicenter aortic vessel tree cta dataset collection with ground truth segmentation masks. Data in brief, 40:107801, 2022. H. R. Roth, Z. Xu, C. Tor-Díez, R. S. Jacob, J. Zember, J. Molto, W. Li, S. Xu, B. Turkbey, E. Turkbey, et al. Rapid artificial intelligence solutions in pandemicthe covid-19-20 lung ct lesion segmentation challenge. Medical image analysis, 82:102605, 2022. D. Schindele, A. Meyer, D. von Reibnitz, V. Kiesswetter, M. Schostak, M. Rak, and C. Hansen. High resolution prostate segmentations for the prostatex-challenge [dataset]. The Cancer Imaging Archive, page 131, 2020. A. Sekuboyina, M. E. Husseini, A. Bayat, M. Löffler, H. Liebl, H. Li, G. Tetteh, J. Kukaˇcka, C. Payer, D. Štern, et al. Verse: vertebrae labelling and segmentation benchmark for multi-detector ct images. Medical image analysis, 73:102166, 2021. A. A. A. Setio, A. Traverso, T. De Bel, M. S. Berens, C. Van Den Bogaard, P. Cerello, H. Chen, Q. Dou, M. E. Fantacci, B. Geurts, et al. Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the luna16 challenge. Medical image analysis, 42:113, 2017. A. L. Simpson, J. Peoples, J. M. Creasy, G. Fichtinger, N. Gangai, K. N. Keshavamurthy, A. Lasso, J. Shia, M. I. DAngelica, and R. K. Do. Preoperative ct and survival data for patients undergoing resection of colorectal liver metastases. Scientific Data, 11(1):172, 2024. M. Styner, J. Lee, B. Chin, M. Chin, O. Commowick, H. Tran, S. Markovic-Plese, V. Jewells, and S. Warfield. 3d segmentation in the clinic: grand challenge ii: Ms lesion segmentation. MIDAS journal, 2008:16, 2008. R. F. Thompson, A. Kanwar, B. Merz, E. Cohen, H. Fisher, S. Rana, C. Claunch, and A. Hung. Stresstesting pelvic autosegmentation algorithms using anatomical edge cases (prostate anatomical edge cases) (version 1), 2023. URL https://doi.org/10.7937/QSTF-ST65. DOI: 10.7937/QSTFST65. S. Wang, C. Qin, C. Wang, K. Wang, H. Wang, C. Chen, C. Ouyang, X. Kuang, C. Dai, Y. Mo, Z. Shi, C. Dai, X. Chen, H. Wang, and W. Bai. The extreme cardiac mri analysis challenge under respiratory motion (cmrxmotion), 2022. J. Wasserthal, H.-C. Breit, M. T. Meyer, M. Pradella, D. Hinck, A. W. Sauter, T. Heye, D. T. Boll, J. Cyriac, S. Yang, M. Bach, and M. Segeroth. Totalsegmentator: Robust segmentation of 104 anatomic structures in ct images. Radiology: Artificial Intelligence, 5(5):e230024, 2023a. J. Wasserthal, H.-C. Breit, M. T. Meyer, M. Pradella, D. Hinck, A. W. Sauter, T. Heye, D. T. Boll, J. Cyriac, S. Yang, et al. Totalsegmentator: Robust segmentation of 104 anatomic structures in ct images. Radiology: Artificial Intelligence, 5(5), 2023b. J. M. Wolterink, T. Leiner, B. D. De Vos, J.-L. Coatrieux, B. M. Kelm, S. Kondo, R. A. Salgado, R. Shahzad, H. Shu, M. Snoeren, et al. An evaluation of automatic coronary artery calcium scoring methods with cardiac ct using the orcascore framework. Medical physics, 43(5):23612373, 2016. M. Zhang, Y. Wu, H. Zhang, Y. Qin, H. Zheng, W. Tang, C. Arnold, C. Pei, P. Yu, Y. Nan, et al. Multi-site, multi-domain airway tree modeling. Medical image analysis, 90:102957, 2023. Y. Zhou, H. Chen, Y. Li, Q. Liu, X. Xu, S. Wang, P.-T. Yap, and D. Shen. Multi-task learning for segmentation and classification of tumors in 3d automated breast ultrasound images. Medical Image Analysis, 70:101918, 2021."
        }
    ],
    "affiliations": [
        "East China Normal University",
        "Shanghai AI Laboratory",
        "Stanford University",
        "University of Cambridge",
        "Xiamen University"
    ]
}