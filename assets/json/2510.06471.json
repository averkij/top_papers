{
    "paper_title": "Test-Time Scaling of Reasoning Models for Machine Translation",
    "authors": [
        "Zihao Li",
        "Shaoxiong Ji",
        "Jörg Tiedemann"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Test-time scaling (TTS) has enhanced the performance of Reasoning Models (RMs) on various tasks such as math and coding, yet its efficacy in machine translation (MT) remains underexplored. This paper investigates whether increased inference-time computation improves translation quality. We evaluate 12 RMs across a diverse suite of MT benchmarks spanning multiple domains, examining three scenarios: direct translation, forced-reasoning extrapolation, and post-editing. Our findings show that for general-purpose RMs, TTS provides limited and inconsistent benefits for direct translation, with performance quickly plateauing. However, the effectiveness of TTS is unlocked by domain-specific fine-tuning, which aligns a model's reasoning process with task requirements, leading to consistent improvements up to an optimal, self-determined reasoning depth. We also find that forcing a model to reason beyond its natural stopping point consistently degrades translation quality. In contrast, TTS proves highly effective in a post-editing context, reliably turning self-correction into a beneficial process. These results indicate that the value of inference-time computation in MT lies not in enhancing single-pass translation with general models, but in targeted applications like multi-step, self-correction workflows and in conjunction with task-specialized models."
        },
        {
            "title": "Start",
            "content": "Test-Time Scaling of Reasoning Models for Machine Translation 1University of Helsinki Zihao Li,1 Shaoxiong Ji,2,3 Jörg Tiedemann1 2 University of Turku firstname.lastname@{1helsinki.fi, 2utu.fi}"
        },
        {
            "title": "3 ELLIS Institute Finland",
            "content": "5 2 0 2 7 ] . [ 1 1 7 4 6 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Test-time scaling (TTS) has enhanced the performance of Reasoning Models (RMs) on various tasks such as math and coding, yet its efficacy in machine translation (MT) remains underexplored. This paper investigates whether increased inference-time computation improves translation quality. We evaluate 12 RMs across diverse suite of MT benchmarks spanning multiple domains, examining three scenarios: direct translation, forced-reasoning extrapolation, and post-editing. Our findings show that for general-purpose RMs, TTS provides limited and inconsistent benefits for direct translation, with performance quickly plateauing. However, the effectiveness of TTS is unlocked by domain-specific fine-tuning, which aligns models reasoning process with task requirements, leading to consistent improvements up to an optimal, self-determined reasoning depth. We also find that forcing model to reason beyond its natural stopping point consistently degrades translation quality. In contrast, TTS proves highly effective in post-editing context, reliably turning self-correction into beneficial process. These results indicate that the value of inference-time computation in MT lies not in enhancing single-pass translation with general models, but in targeted applications like multi-step, self-correction workflows and in conjunction with task-specialized models."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have dramatically advanced machine translation (MT), evolving from statistical and neural paradigms to systems capable of handling diverse languages, domains, and complexities with unprecedented accuracy (Lyu et al., 2023; Kocmi et al., 2024; Zhu et al., 2024; Cui et al., 2025). Recent developments in Reasoning Models (RMs)models designed to incorporate structured reasoning processes like Chain-of-Thought (CoT)have further transformed MT by reframing 1 Figure 1: Illustration of the effectiveness of test-time scaling in reasoning models for machine translation. (1) TTS for general-purpose RMs yields only small initial performance gain, but quickly plateauing as increased inference cost. (2) Forcing RMs to reason beyond their natural stopping point degrades quality by introducing noise. (3) In contrast, TTS becomes effective when applied to RMs specifically developed for MT. (4) TTS shows improvements in post-editing workflows. All these highlight TTSs value in MT lies in task-specialized models and multi-step self-correction, rather than as robust strategy for enhancing single-pass translation with general-purpose RMs. it as cognitive task requiring contextual analysis, cultural adaptation, and self-reflection (Liu et al., 2025). For instance, RMs can resolve ambiguities in stylized texts and maintain coherence across documents, thereby outperforming traditional LLMs in semantically demanding scenarios (Ye et al., 2025). Test-time scaling (TTS) has emerged as transformative approach for enhancing model performance, which allocates additional computational resources during inference to enhance performance without requiring model retraining or parameter expansion (Snell et al., 2024). The effectiveness of TTS has been particularly pronounced for RMs such as DeepSeek-R1 (Guo et al., 2025), Gemini 2.5 (Comanici et al., 2025), and OpenAIs oSeries (Jaech et al., 2024; OpenAI, 2025), which have achieved breakthrough performance on challenging benchmarks by extending their reasoning chains. Moreover, relatively small RMs have demonstrated impressive results on mathematical and coding tasks through TTS (Muennighoff et al., 2025; Li et al., 2025), suggesting that inferencetime computation can partially compensate for limited model capacity. Nevertheless, applying TTS to RMs for MT introduces distinct challenges and untapped potential that warrant deeper exploration. Unlike math or coding tasks, where correctness can often be objectively determined, MT demands not only linguistic accuracy but also reasoning over cultural nuances, domain-specific terminology, and long-range dependencies, areas where unstructured compute scaling may yield diminishing returns. Moreover, interventions like forced extrapolation (e.g., inserting wait\" tokens to extend reasoning) could disrupt natural deliberation, potentially introducing noise. In post-editing (PE) contexts, where models refine their own drafts, TTS might unlock iterative improvements, though this demands rigorous testing across varied benchmarks."
        },
        {
            "title": "This paper investigates these open questions",
            "content": "through three research questions: RQ1: How effective is test-time scaling for MT? We examine whether increased inference computation reliably boosts translation quality across general-purpose and fine-tuned MT-specific RMs. RQ2: Does extrapolation by inserting wait forcibly help? We investigate if overriding models natural stopping points, which further scales up the inference computation, enhances or hinders performance. RQ3: Does test-time scaling work in postediting? We evaluate TTS in self-correction scenarios, assessing its role in refining initial translations. To address these questions, we assemble comprehensive array of MT benchmarks encompassing literary, biomedical, cultural, commonsense, constrained terminology, and retrieval-augmented domains. We assess 12 RMs, spanning open-source series (Qwen-3 (Yang et al., 2025), Cogito (Deep Cogito, 2025), DRT (Wang et al., 2025a)) and the proprietary Grok-3-Mini. Our key contributions and findings are as follows: We demonstrate that for general-purpose RM, TTS provides limited and inconsistent benefits for direct machine translation. After small initial improvements at very low budgets, performance plateaus across metrics and datasets, indicating that more thinking alone is not robust path to better translations. We show that the effectiveness of TTS is unlocked by domain-specific fine-tuning, which aligns the models reasoning process with task requirements. For DRT models fine-tuned on specific domain data, performance improves with budget on in-domain tasks and saturates once models naturally stop increasing their internal token usage, suggesting an emergent alignment between optimal reasoning depth and task demands. This alignment largely disappears out of domain. We find that forcing model to reason by inserting single wait beyond its natural stopping point consistently degrades translation quality, highlighting the importance of the models intrinsic deliberation process. We establish that TTS is highly effective in post-editing context, in which the inference cost is higher than the cost of direct translation. TTS turns self-correction into reliably beneficial process. These findings provide implications for deploying RMs in production MT systems and highlight the critical interplay between model capacity, taskspecific training, and inference-time computation in determining when and how test-time scaling benefits translation quality."
        },
        {
            "title": "2.1 Datasets",
            "content": "To comprehensively evaluate the reasoning capabilities and scaling properties of models at test time, we curated diverse suite of eight machine translation benchmarks. These datasets span multiple domains, granularities, and languages, targeting wide spectrum of reasoning challenges  (Table 1)  . For tasks requiring deep contextual and stylistic understanding, we use three literary benchmarks: the document-level WMT24-Literary (Wang et al., 2024b), paragraph-level LitEval-Corpus (Zhang et al., 2025), and sentence-level MetaphorTrans (Wang et al., 2025a). These datasets are rich in complex linguistic phenomena, cultural references, and figurative language (similes and metaphors), demanding sophisticated reasoning to preserve literary style and meaning. Similarly, the WMT23/24-Biomedical (Neves et al., 2023, 2024) 2 benchmark tests reasoning within specialized domain, demanding accurate translation of technical terminology from PubMed abstracts. To probe more targeted reasoning abilities, we incorporate four specialized datasets. CAMT (Yao et al., 2024) assesses cross-cultural reasoning on expressions requiring cultural adaptation. Commonsense-MT (He et al., 2020) comprises subsets targeting lexical, contextless syntactic, and contextual syntactic ambiguities, each requiring commonsense reasoning. RTT (Zhang et al., 2023) evaluates constrained reasoning by requiring models to correctly translate specific terminology under highly constrained conditions. Lastly, RAGTrans (Wang et al., 2024a) examines models capacity to reason over and integrate retrieved external evidence into its translation. Collectively, these benchmarks provide rigorous and multifaceted framework for analyzing the effects of scaling on translation reasoning."
        },
        {
            "title": "2.2 Models",
            "content": "Our evaluation encompasses 12 RMs, including 11 open-source models from three distinct families and one proprietary model for comparison. The open-source models investigated are as follows: Qwen-3: Six models from this family were selected, with parameter sizes of 0.6B, 1.7B, 4B, 8B, 14B, and 32B (Yang et al., 2025). These models are hybrid reasoning LLMs that support seamless switching between standard generation mode and deliberative reasoning mode. Cogito: Two models, sized 3B and 8B, were included (Deep Cogito, 2025). Cogito-3B and Cogito-8B are trained on top of Llama-3.23B and Llama-3.1-8B (Dubey et al., 2024), respectively, and similarly implement hybrid reasoning capabilities with controllable switching between generation and reasoning modes. DRT: Three models from this family were evaluated. These models are fine-tuned from existing LLMs using the training set of MetaphorTrans (Wang et al., 2025a). Specifically, DRT-7B, DRT-8B, and DRT-14B are built upon Qwen2.5-7B-Instruct (Qwen et al., 2024), Llama-3.1-8B-Instruct, and Qwen2.514B-Instruct respectively. In addition to the open-source models, we included the proprietary model Grok-3-Mini. This model provides tunable reasoning_effort parameter (equivalent to reasoning budget but can only be set to low or high) to control the amount of deliberation performed prior to generating response."
        },
        {
            "title": "2.3 Evaluation Metrics",
            "content": "We assess translation quality using suite of automatic metrics, encompassing both reference-based and reference-free approaches, alongside specialized LLM-based judge for literary texts. COMET Metrics. For standardized assessment, we employ two variants from the COMET framework (Rei et al., 2020): the reference-based COMET-22 (Rei et al., 2022a) and the referencefree COMETKiwi-22 (Rei et al., 2022b). LLM as Judge. For LLM-based evaluation, we employ Gemini-2.0-Flash. We first define two general-purpose metrics, Gemini ReferenceBased (GRB) and Gemini Reference-Free (GRF), which provide quality score on 0-100 scale. Furthermore, for the specific challenges of literary translation, we follow Wang et al. (2025a) and apply the Gemini Evaluation with Anchors (GEA) metric exclusively to the three literary benchmarks. This specialized metric assesses nuances like style and expressiveness, and we collect scores at two levels of granularity: GEA100 [0, 100] and GEA5 1, . . . , 5. The evaluation prompts are adapted from Kocmi and Federmann (2023) and Wang et al. (2025a), are illustrated in Appendix A."
        },
        {
            "title": "2.4 Budget Forcing",
            "content": "We regulate test-time reasoning with logits processor that enforces thinking-token budget inside <think>. . . </think> span. While in this span, the processor counts tokens, softly encourages closure near 95% of the budget by upweighting newline and </think>, then deterministically emits newline (penultimate step) and </think> (final step) at the budget limit before continuing normal answer decoding. Conversely, to probe extrapolation, we optionally insert single wait token if the model attempts to stop: specifically, after at least 5 thinking tokens, if </think> is the next token from the output of the argmax function and the budget is not yet exhausted, we override the next token to wait once and resume unconstrained decoding. We insert wait at most once."
        },
        {
            "title": "Languages",
            "content": "Language Pair(s) Sample Size"
        },
        {
            "title": "Literature",
            "content": "WMT24-Literary (Wang et al., 2024b) MetaphorTrans (Wang et al., 2025a) LitEval-Corpus (Zhang et al., 2025) Document-level ZH, DE, RU Sentence-level Paragraph-level ZH, EN ZH, EN, DE 3"
        },
        {
            "title": "Biomedical",
            "content": "WMT24-Biomedical (Neves et al., 2024) WMT23-Biomedical (Neves et al., 2023) Document-level Document-level EN, DE, ES, FR, IT, PT, RU"
        },
        {
            "title": "Culture",
            "content": "CAMT (Yao et al., 2024) Sentence-level EN, ES, FR, HI, TA, TE, ZH"
        },
        {
            "title": "Commonsense",
            "content": "Commonsense-MT (He et al., 2020) (Lexical Ambiguity) Sentence-level Sentence-level Commonsense-MT (Contextless Syntactic Ambiguity) Sentence-level Commonsense-MT (Contextual Syntactic Ambiguity) ZH, EN"
        },
        {
            "title": "Terminology",
            "content": "RTT (Zhang et al., 2023) Misc. RAGTrans (Wang et al., 2024a) Sentence-level EN, DE Sentence-level ZH, EN 1 2 1 43 2000 187 600 6948 400 450 350 100 1999 Table 1: Overview of the MT benchmarks used in our evaluation."
        },
        {
            "title": "2.5 Post-Editing",
            "content": "Post-editing (or self-correction) involves two-stage translation, i.e., stage one of direct translation and stage two that corrects or post-edits the direct translation, which enables models to review and refine their own outputs (Feng et al., 2025; Wang et al., 2024c; Li et al., 2024). We explore two prompting strategies to guide this self-correction process, with full details provided in Appendix D. The first is standard PE prompt, which we term No QS (No Quality Score). It provides the model with only the source text and its own draft translation to be refined. The second is an enhanced prompt, QS (with Quality Score), which additionally includes numerical quality score of the draft, calculated as the average of the GRB and GRF scores from the initial translation. This provides the model with an explicit signal about the quality of the translation it needs to correct, potentially guiding more targeted reasoning process."
        },
        {
            "title": "3.1 Effectiveness of Test-Time Scaling",
            "content": "General-Purpose Models Show Limited Gains from Increased Budget. Our initial investigation focused on the efficacy of test-time scaling for general-purpose RMs, including the Qwen-3 and Cogito families, as well as the proprietary Grok-3Mini model. These models were evaluated out-ofthe-box\" without any fine-tuning on our benchmark datasets. Figure 2 plots the average GRB scores across all datasets against varying thinking-token budgets for the Qwen-3 and Cogito model series. After small initial performance gain when moving from zero budget to minimal budget (e.g., 100 tokens), the models performance curves almost completely plateau in most cases. Further increases in the budget up to 2000 tokens yield no meaningful improvements, indicating that simply allocating more computational steps does not enable the models to produce more refined or accurate translations when they lack specific task-related knowledge. This conclusion is further corroborated and nuanced by our analysis of the Grok-3-Mini model. We analyzed its performance using both referencebased (GRB) and reference-free (GRF) metrics, visualized in Figure 3a and Figure 3b, respectively. Both metrics reveal highly inconsistent, dataset-dependent impact. For instance, while higher effort improves scores on CommonsenseMTLexical across both GRB (+0.450) and GRF (+0.376), it significantly degrades performance on CommonsenseMT-Contextless in both cases (- 0.780 for GRB, -0.367 for GRF). Crucially, the average effect across all datasets is negligible and even flips its sign depending on the metric: the mean GRB delta is slightly negative -0.064, while the mean GRF delta is slightly positive +0.033. Given that both scores are on 100-point scale, these near-zero average changes underscore that the potential benefits and drawbacks of increased computational effort effectively cancel each other out, leading to no reliable overall improvement. Therefore, our analyses of both open-source models with varying budgets and proprietary model with different effort levels converge on single conclusion: for general-purpose LLMs without specific in-domain training, test-time scaling is not robust strategy for enhancing machine translation performance. In-Domain Fine-Tuning Unlocks the Benefit of Test-Time Scaling. In contrast to the generalpurpose models, the DRT models reveal that the effectiveness of test-time scaling is highly contingent on domain-specific training, which appears to create an efficient alignment between reasoning effort and performance. These models were Figure 2: Average GRB scores of Qwen-3 and Cogito models across all datasets with varying thinking budgets. to around 500 tokens, both the number of generated tokens and the GEA scores steadily rise. However, beyond 500-token budget, critical pattern emerges: the models stop generating more thinking tokens, and concurrently, their performance plateaus. We hypothesize that the finetuning successfully aligns the models reasoning behavior with the effective reasoning boundary of the task. This efficient alignment vanishes on the other out-of-domain literary translation tasks. The most striking counterexample is the document-level WMT24-Literary task. Here, the actual thinking tokens continue to scale almost linearly with the budget, indicating the models are using the provided extra capacity to think\" longer. Yet, this extended reasoning does not translate into better performance; the GEA scores remain erratic and show no consistent improvement. This disconnect suggests the models are engaged in unproductive or unfocused reasoning, spinning their wheels\" without the specialized knowledge required for this different type of literary translation. This dichotomy underscores our central argument: test-time scaling is most effective when fine-tuning has equipped model with not only domain-specific knowledge but also an efficient strategy for how and when to apply it."
        },
        {
            "title": "Degrades Performance",
            "content": "Building on the observation that models possess natural reasoning length, we next address RQ2: Does performance improve if we force model to think longer? We test this by applying the wait token extrapolation method, detailed in Section 2.4, to prompt continued reasoning when the models are about to stop the thinking process. The results, averaged across all datasets for the Qwen-3 and Cogito models, are presented in Table 2. The findings are unambiguous. First, as evidenced by the Thinking Length columns, the (a) (b) Figure 3: Performance of Grok-3-mini across tasks, showing the difference between highand low-effort reasoning. Subfigure (a) reports results under the GRB metric, and (b) shows results under GRF. fine-tuned on the training set of MetaphorTrans (in-domain), while LitEval-Corpus and WMT24Literary serve as related but out-of-domain literary benchmarks. Figure 4 visualizes the translation quality (GEA score, right y-axis) and the actual number of thinking tokens generated by the models (left y-axis). On the in-domain MetaphorTrans task, we observe clear and consistent positive correlation between the thinking budget and translation performance. As the thinking budget increases from"
        },
        {
            "title": "3.3 Test-Time Scaling is Effective for",
            "content": "Post-Editing Finally, we investigate RQ3 by evaluating the effectiveness of test-time scaling in self-correction post-editing scenario. For this task, we define the baseline as the translation generated by each Qwen3 model with zero thinking budget in our prior experiments. Subsequently, we task the same model with refining its own translation, applying thinking budgets of 0, 500, and 1000 tokens. The detailed results are presented in Table 12 and Table 13, with trends visualized in Figure 5. In striking contrast to its ineffectiveness in direct translation, test-time scaling proves to be highly effective strategy for post-editing, reliably elevating translation quality above the original baseline for most models. The effect is most pronounced for mid-sized models, as shown in both the GRB (Figure 5a) and GRF (Figure 5b) plots. For models in the 1.7B to 14B parameter range, applying post-editing with zero budget often yields results that are similar to or worse than the original translation. Increasing the budget to 500 or 1000 tokens consistently pushes performance significantly above this baseline, demonstrating that thinking budget is crucial for turning self-correction into reliably beneficial process. However, this scaling trend does not hold for the extremes of the model family. The smallest model, Qwen3-0.6B, displays erratic behavior, with its performance fluctuating without clear improvement as the budget increases. Conversely, the largest model, Qwen3-32B, already surpasses the baseline with zero-budget correction, and additional thinking time provides no further gains, suggesting it performs near its peak without extended deliberation. comparison of the two prompting strategies further highlights the importance of the thinking budget. At zero-token budget, the QS prompt (with quality score) generally underperforms the No QS prompt. However, once the budget is increased to 500 or 1000 tokens, their performances converge and become nearly indistinguishable. This demonstrates that while prompting strategy matters, it is the allocation of computational budget that is the key factor for post-editing to reliably improve upon the initial translation."
        },
        {
            "title": "Machine Translation with Large Language\nModels LLMs have rapidly advanced machine",
            "content": "Figure 4: Performance (dashed lines, right axis) and actual generated thinking tokens (solid lines, left axis) of DRT models across 3 literary translation tasks. intervention was effective in its primary goal: it consistently and significantly extended the models reasoning chains, often by over 100-200 tokens. However, this artificially prolonged reasoning process did not translate into better translations. In fact, it was overwhelmingly detrimental. Across all four metrics (COMET, COMETKiwi, GRB, and GRF), the wait token intervention consistently reduces performance: under both the 1000and 2000-token budgets, 55 of the 64 metric scores across eight models dropped after forced extrapolation. While there are few isolated instances of negligible score increases in one metric (e.g., Qwen3-4B on GRB with 2000-token budget), these are exceptions that are contradicted by decreases in other metrics for the same model. This leads to clear conclusion: models decision to terminate its reasoning chain is meaningful signal. It indicates that the model has reached what it considers to be sufficient state of deliberation for the given task. Forcing the model to continue to reason beyond its own stopping point appears to introduce noise, repetition, or less relevant reasoning steps, which ultimately harms the quality of the final translation. In short, we find that forced extrapolation is counterproductive strategy for improving translation quality."
        },
        {
            "title": "After",
            "content": "Qwen3-0.6B Qwen3-1.7B Qwen3-4B Qwen3-8B Qwen3-14B Qwen3-32B Cogito-3B Cogito-8B 1000 2000 1000 2000 1000 2000 1000 2000 1000 2000 1000 2000 1000 2000 1000 2000 426 454 655 717 737 822 741 821 668 715 700 748 577 661 568 665 519 556 820 987 873 1098 878 1117 812 955 835 992 695 844 697 882 0.6959 0.6895 0.7687 0.7645 0.7914 0.7871 0.7979 0.7965 0.7992 0.7966 0.8025 0.7993 0.7071 0.7040 0.7678 0. 0.6904 0.6894 0.7475 0.7496 0.7738 0.7784 0.7865 0.7860 0.7924 0.7908 0.7828 0.7786 0.7063 0.7049 0.7658 0.7687 0.6155 0.6087 0.6851 0.6762 0.7092 0.7005 0.7180 0.7118 0.7184 0.7125 0.7233 0.7174 0.6313 0.6324 0.6824 0.6823 0.6026 0.6016 0.6543 0.6586 0.6835 0.6862 0.6954 0.6951 0.7022 0.7020 0.6957 0.6937 0.6310 0.6331 0.6807 0.6809 68.8577 69.2139 83.6481 83.6647 89.7781 89.7023 91.6706 91.8186 92.5609 92.6259 92.6026 92.5393 84.0098 84.2981 89.6573 89.5962 68.6333 69.0237 83.5185 83.4715 89.5598 90.0098 91.7601 91.6111 92.5677 92.5421 92.5537 92.6689 83.8203 83.9921 89.3849 89.4339 67.2944 67.6465 83.1170 83.1076 89.7522 89.6536 91.8085 91.8123 92.5242 92.5617 92.6328 92.6698 83.1502 82.4356 89.2655 89. 67.1203 67.3326 82.7297 82.7472 89.5203 89.7275 91.7027 91.9008 92.5148 92.5392 92.5322 92.7696 82.5440 81.8855 88.8525 89.2452 Table 2: Effect of forcibly inserting wait token to extend the reasoning process. The Before columns show standard generation, while After shows results from the intervention. translation, with growing emphasis on open-weight systems that balance translation quality, multilingual coverage, and general-purpose capabilities. Seed-X (Cheng et al., 2025) introduces 7B-parameter open-source family of translationoriented LLMs trained on large-scale monolingual and bilingual corpora across 28 languages, achieving performance competitive with closed-source systems such as GPT-4o (Hurst et al., 2024) and Gemini-2.5 (Comanici et al., 2025). Similarly, Hunyuan-MT (Zheng et al., 2025) develops 7Bparameter models: Hunyuan-MT and HunyuanMT-Chimera, the latter integrates multiple outputs under slow thinking paradigm to yield higher-quality translations, and ranks first in the WMT2025 shared task across 30 of 31 directions. Tower+ (Rei et al., 2025) addresses the tradeoff between translation specialization and generalpurpose ability by combining continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. Machine Translation with Reasoning Models Recent research has explored how RMs can be adapted to MT, particularly in linguistically and culturally challenging domains. Wang et al. (2025a) propose Deep Reasoning Translation (DRT), which leverages long chain-of-thought (CoT) reasoning within multi-agent framework to tackle similes and metaphors in EnglishChinese literary translation. The resulting models surpass standard LLMs by synthesizing long-thought training data. Building on this, DeepTrans (Wang et al., 2025b) applies reinforcement learning (RL) with carefully designed reward functions targeting both translation fidelity and reasoning quality, showing that RL without labeled pairs can significantly boost performance. ExTrans (Wang et al., 2025c) complements this direction with an exemplar-enhanced RL approach that employs stronger RM (DeepSeek-R1) as reward reference. ExTrans achieves state-ofthe-art results in EnglishChinese literary MT, and its multilingual extension (mExTrans) scales effectively to 90 directions with lightweight reward modeling. Beyond literary MT, R1-T1 (He et al., 2025) generalizes reasoning-based MT by modeling six CoT templates inspired by human translator strategies. Through RL, it enables self-evolving reasoning trajectories, improving performance across diverse domains and low-resource languages. Test-Time Scaling Recent studies have examined the potential of test-time scaling. Tan et al. (2025) propose best-of-N reranking framework where multiple translation candidates are generated and the best one is selected using quality estimation model. They show that smaller models can, through TTS, match or even surpass larger model. For example, 14B model with 8 achieves parity with 72B model at = 1 while requiring substantially less GPU memory. Beyond MT, Son et al. (2025) analyze TTS in multilingual mathematical reasoning, finding that outcome 7 (a) GRB scores for post-editing across Qwen-3 models. (b) GRF scores for post-editing across Qwen-3 models. Figure 5: Effectiveness of test-time scaling in post-editing scenario. and process reward modeling, as well as budget forcing, yield notable gains in English but limited improvements across 55 languages, highlighting cross-lingual fragility. Tran et al. (2025) study low-resource reasoning tasks and propose Englishpivoted CoT generation, where reasoning occurs in English before producing final answers in the target language, yielding substantial accuracy improvements. Yong et al. (2025) further study crosslingual reasoning with English-centric RMs, finding that scaling inference budgets with long CoTs improves multilingual mathematical reasoning and even allows smaller models to outperform larger baselines, but also highlighting language-mixing behaviors and weaker generalization to cultural commonsense domains."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we systematically explored the application of test-time scaling (TTS) to reasoning models (RMs) for machine translation (MT), addressing three core research questions through extensive experiments across diverse benchmarks, models, and evaluation metrics. Our findings reveal that TTS offers limited value for general-purpose RMs in direct translation tasks, where performance quickly plateaus after minimal initial gains, underscoring that additional inferencetime computation alone cannot compensate for lack of task-aligned reasoning strategies. In contrast, domain-specific fine-tuning emerges as pivotal enabler, allowing TTS to yield consistent improvements on in-domain tasks until models reach their natural reasoning depth, beyond which further scaling provides no benefit. This highlights an emergent efficiency in fine-tuned models, where optimal deliberation aligns with task demands, though such alignment erodes out-of-domain. Furthermore, forcibly extending reasoning via interventions like wait tokens consistently degrades quality, emphasizing the importance of respecting models intrinsic stopping points. Finally, TTS proves particularly potent in post-editing scenarios, transforming self-correction into reliable mechanism for refining initial drafts, especially for midsized models when paired with adequate budgets. The implications of this work are twofold. First, for practitioners, simply allocating more inference compute to general-purpose models is an inefficient path to better translations. Instead, resources are better invested in targeted fine-tuning, which aligns the models reasoning capabilities with specific task demands. Second, our results suggest that the most promising application of TTS in MT is not in direct, single-pass translation but in multi-stage workflows, such as rapid initial draft followed by more deliberate, computationally-intensive selfcorrection phase. Future work could explore more dynamic budget allocation strategies and extend to hybrid TTS approaches integrated with external tools like retrieval-augmented generation."
        },
        {
            "title": "References",
            "content": "While our study provides comprehensive analysis of test-time scaling in machine translation, we acknowledge several limitations that frame the scope of our conclusions and suggest avenues for future research. First, our investigation, while encompassing 12 different models, is primarily focused on opensource RM families and single, smaller proprietary model. The performance characteristics and scaling behaviors of the largest, state-of-the-art proprietary models (e.g., Gemini-2.5-Pro) may differ from our observations. Furthermore, the linguistic diversity of our benchmarks is largely centered around English or Chinese as either source or target language. Consequently, our findings on the effectiveness of TTS, particularly the interplay with fine-tuning, may not generalize directly to lowresource languages where the reasoning challenges could be substantially different. Second, our evaluation methodology relies exclusively on automatic and LLM-based metrics. Although we employed suite of reference-based, reference-free, and specialized LLM-judge metrics to ensure robustness, this approach lacks the nuance of human evaluation. human assessment would be invaluable for validating our findings, especially on the literary and cultural benchmarks where subtle aspects of style, tone, and appropriateness are critical and may not be fully captured by our current metrics. The potential biases inherent in LLM-as-a-judge frameworks also represent confounding factor. Finally, our study implements test-time scaling through specific budget-forcing mechanism and simple wait token intervention for extrapolation. Other methods for encouraging or extending deliberation, such as alternative prompting strategies or more complex reasoning frameworks, were not explored and could yield different outcomes. Additionally, our analysis is primarily quantitative; we did not perform qualitative analysis of the content within the models reasoning chains. deeper investigation into what the models are thinking could provide valuable insights into why performance plateaus for general-purpose models or why forced extrapolation leads to degradation. Shanbo Cheng, Yu Bao, Qian Cao, Luyang Huang, Liyan Kang, Zhicheng Liu, Yu Lu, Wenhao Zhu, Jingwen Chen, Zhichao Huang, and 1 others. 2025. Seedx: Building strong multilingual translation llm with 7b parameters. arXiv preprint arXiv:2507.13618. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and 1 others. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, and Bin Wang. 2025. Multilingual machine translation with open large language models at practical scale: An empirical study. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 54205443, Albuquerque, New Mexico. Association for Computational Linguistics. Deep Cogito. 2025. Cogito v1 Preview Insupertroducing IDA as path to general https://www.deepcogito.com/ intelligence. research/cogito-v1-preview. Accessed: 202509-09. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and 1 others. 2024. The llama 3 herd of models. arXiv e-prints, pages arXiv2407. Zhaopeng Feng, Yan Zhang, Hao Li, Bei Wu, Jiayu Liao, Wenqiang Liu, Jun Lang, Yang Feng, Jian Wu, and Zuozhu Liu. 2025. TEaR: Improving LLM-based machine translation with systematic self-refinement. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 39223938, Albuquerque, New Mexico. Association for Computational Linguistics. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Jie He, Tao Wang, Deyi Xiong, and Qun Liu. 2020. The box is in the pen: Evaluating commonsense reasoning in neural machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 36623672, Online. Association for Computational Linguistics. Minggui He, Yilun Liu, Shimin Tao, Yuanchang Luo, Hongyong Zeng, Chang Su, Li Zhang, Hongxia Ma, Daimeng Wei, Weibin Meng, and 1 others. 2025. R1-t1: Fully incentivizing translation capability in llms via reasoning learning. arXiv preprint arXiv:2502.19735. 9 Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, and 1 others. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondˇrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Marzena Karpinska, Philipp Koehn, Benjamin Marie, Christof Monz, Kenton Murray, Masaaki Nagata, Martin Popel, Maja Popovic, and 3 others. 2024. Findings of the WMT24 general machine translation shared task: The LLM era is here but MT is not solved yet. In Proceedings of the Ninth Conference on Machine Translation, pages 146, Miami, Florida, USA. Association for Computational Linguistics. Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 193203, Tampere, Finland. European Association for Machine Translation. Dacheng Li, Shiyi Cao, Chengkun Cao, Xiuyu Li, Shangyin Tan, Kurt Keutzer, Jiarong Xing, Joseph Gonzalez, and Ion Stoica. 2025. S*: Test time scaling for code generation. arXiv preprint arXiv:2502.14382. Xinnuo Li, Yunxiang Zhang, and Lu Wang. 2024. Improving language model self-correction capability with meta-feedback. OpenReview. Sinuo Liu, Chenyang Lyu, Minghao Wu, Longyue Wang, Weihua Luo, Kaifu Zhang, and Zifu Shang. 2025. New trends for modern machine translation with large reasoning models. arXiv preprint arXiv:2503.10351. Chenyang Lyu, Zefeng Du, Jitao Xu, Yitao Duan, Minghao Wu, Teresa Lynn, Alham Fikri Aji, Derek Wong, Siyou Liu, and Longyue Wang. 2023. paradigm shift: The future of machine translation lies with large language models. arXiv preprint arXiv:2305.01181. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393. Mariana Neves, Cristian Grozea, Philippe Thomas, Roland Roller, Rachel Bawden, Aurélie Névéol, Steffen Castle, Vanessa Bonato, Giorgio Maria Di Nunzio, Federica Vezzani, Maika Vicente Navarro, Lana Yeganova, and Antonio Jimeno Yepes. 2024. Findings of the WMT 2024 biomedical translation shared task: Test sets on abstract level. In Proceedings of the Ninth Conference on Machine Translation, pages 124138, Miami, Florida, USA. Association for Computational Linguistics. Mariana Neves, Antonio Jimeno Yepes, Aurélie Névéol, Rachel Bawden, Giorgio Maria Di Nunzio, Roland Roller, Philippe Thomas, Federica Vezzani, Maika Vicente Navarro, Lana Yeganova, Dina Wiemann, and Cristian Grozea. 2023. Findings of the WMT 2023 biomedical translation shared task: Evaluation of ChatGPT 3.5 as comparison system. In Proceedings of the Eighth Conference on Machine Translation, pages 4354, Singapore. Association for Computational Linguistics. OpenAI. 2025. Openai o3 and o4-mini system card. System card, OpenAI. Published April 16, 2025. Yang Qwen, Baosong Yang, Zhang, Hui, Zheng, Yu, Chengpeng Li, Liu, Huang, Wei, and 1 others. 2024. Qwen2. 5 technical report. arXiv preprint. Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022a. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Ricardo Rei, Nuno Guerreiro, José Pombal, João Alves, Pedro Teixeirinha, Amin Farajian, and André FT Martins. 2025. Tower+: Bridging generality and translation specialization in multilingual llms. arXiv preprint arXiv:2506.17080. Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. 2020. COMET: neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26852702, Online. Association for Computational Linguistics. Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. 2022b. CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 634645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Guijin Son, Jiwoo Hong, Hyunwoo Ko, and James Thorne. 2025. Linguistic generalizability of test-time scaling in mathematical reasoning. arXiv preprint arXiv:2502.17407. Shaomu Tan, Ryosuke Mitani, Ritvik Choudhary, and Toshiyuki Sekiya. 2025. Investigating test-time scaling with reranking for machine translation. arXiv preprint arXiv:2509.19020. Khanh-Tung Tran, Barry OSullivan, and Hoang Nguyen. 2025. Scaling test-time compute for lowresource languages: Multilingual reasoning in llms. arXiv preprint arXiv:2504.02890. Jiaan Wang, Fandong Meng, Yunlong Liang, and Jie Zhou. 2025a. DRT: Deep reasoning translation via long chain-of-thought. In Findings of the Association for Computational Linguistics: ACL 2025, pages 67706782, Vienna, Austria. Association for Computational Linguistics. Jiaan Wang, Fandong Meng, Yingxue Zhang, and Jie Zhou. 2024a. Retrieval-augmented machine translation with unstructured knowledge. arXiv preprint arXiv:2412.04342. Jiaan Wang, Fandong Meng, and Jie Zhou. 2025b. Deep reasoning translation via reinforcement learning. arXiv preprint arXiv:2504.10187. Jiaan Wang, Fandong Meng, and Jie Zhou. 2025c. Extrans: Multilingual deep reasoning translation via exemplar-enhanced reinforcement learning. arXiv preprint arXiv:2505.12996. Longyue Wang, Siyou Liu, Chenyang Lyu, Wenxiang Jiao, Xing Wang, Jiahao Xu, Zhaopeng Tu, Yan Gu, Weiyu Chen, Minghao Wu, Liting Zhou, Philipp Koehn, Andy Way, and Yulin Yuan. 2024b. Findings of the WMT 2024 shared task on discourse-level literary translation. In Proceedings of the Ninth Conference on Machine Translation, pages 699700, Miami, Florida, USA. Association for Computational Linguistics. Yutong Wang, Jiali Zeng, Xuebo Liu, Fandong Meng, Jie Zhou, and Min Zhang. 2024c. TasTe: Teaching large language models to translate through selfreflection. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 61446158, Bangkok, Thailand. Association for Computational Linguistics. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Binwei Yao, Ming Jiang, Tara Bobinac, Diyi Yang, and Junjie Hu. 2024. Benchmarking machine translation with cultural awareness. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1307813096. Yongshi Ye, Biao Fu, Chongxuan Huang, Yidong Chen, and Xiaodong Shi. 2025. How well do large reasoning models translate? comprehensive evaluation for multi-domain machine translation. arXiv preprint arXiv:2505.19987. Zheng-Xin Yong, Farid Adilazuarda, Jonibek Mansurov, Ruochen Zhang, Niklas Muennighoff, Carsten Eickhoff, Genta Indra Winata, Julia Kreutzer, Stephen Bach, and Alham Fikri Aji. 2025. Crosslingual reasoning through test-time scaling. arXiv preprint arXiv:2505.05408. Huaao Zhang, Qiang Wang, Bo Qin, Zelin Shi, Haibo Wang, and Ming Chen. 2023. Understanding and improving the robustness of terminology constraints in neural machine translation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 60296042, Toronto, Canada. Association for Computational Linguistics. Ran Zhang, Wei Zhao, and Steffen Eger. 2025. How good are LLMs for literary translation, really? literary translation evaluation with humans and LLMs. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 10961 10988, Albuquerque, New Mexico. Association for Computational Linguistics. Mao Zheng, Zheng Li, Bingxin Qu, Mingyang Song, Yang Du, Mingrui Sun, and Di Wang. 2025. Hunyuan-mt technical report. arXiv preprint arXiv:2509.05209. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2024. Multilingual machine translation with large language models: Empirical results and analysis. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 27652781, Mexico City, Mexico. Association for Computational Linguistics."
        },
        {
            "title": "A Evaluation Prompts",
            "content": "GRB Prompt: 11 Score the following translation from {src_lang} to {tgt_lang} with respect to the human reference on continuous scale from 0 to 100, where score of zero means no meaning preserved and score of one hundred means perfect preservation of meaning, with faithfulness, expressiveness, and elegance. Only output the score number. {src_lang} source: {src_text} {tgt_lang} human reference: {tgt_text} {tgt_lang} translation: {hyp_text} Score: GRF Prompt: Score the following translation from {src_lang} to {tgt_lang} on continuous scale from 0 to 100, where score of zero means no meaning preserved and score of one hundred means perfect preservation of meaning, with faithfulness, expressiveness, and elegance. Only output the score number. {src_lang} source: {src_text} {tgt_lang} translation: {hyp_text} Score: GEA100 Prompt: Please evaluate the following {tgt_lang} translation of an {src_lang} text. Rate the translation on scale of 0 to 100, where: - 10 points: Poor translation; the text is somewhat understandable but contains significant errors and awkward phrasing that greatly hinder comprehension for {tgt_lang} reader. - 30 points: Fair translation; the text conveys the basic meaning but lacks fluency and contains several awkward phrases or inaccuracies, making it challenging for {tgt_lang} reader to fully grasp the intended message. - 50 points: Good translation; the text is mostly fluent and conveys the original meaning well, but may have minor awkwardness or slight inaccuracies that could confuse {tgt_lang} reader. - 70 points: Very good translation; the text is smooth and natural, effectively conveying the intended meaning, but may still have minor issues that could slightly affect understanding for {tgt_lang} reader. - 90 points: Excellent translation; the text is fluent and natural, conveying the original meaning clearly and effectively, with no significant issues that would hinder understanding for {tgt_lang} reader. Please only output the score number. GEA5 Prompt: 12 Please evaluate the following {tgt_lang} translation of an {src_lang} text. Rate the translation on scale of 0 to 5, where: - 1 point: Poor translation; the text is somewhat understandable but contains significant errors and awkward phrasing that greatly hinder comprehension for {tgt_lang} reader. - 2 points: Fair translation; the text conveys the basic meaning but lacks fluency and contains several awkward phrases or inaccuracies, making it challenging for {tgt_lang} reader to fully grasp the intended message. - 3 points: Good translation; the text is mostly fluent and conveys the original meaning well, but may have minor awkwardness or slight inaccuracies that could confuse {tgt_lang} reader. - 4 points: Very good translation; the text is smooth and natural, effectively conveying the intended meaning, but may still have minor issues that could slightly affect understanding for {tgt_lang} reader. - 5 points: Excellent translation; the text is fluent and natural, conveying the original meaning clearly and effectively, with no significant issues that would hinder understanding for {tgt_lang} reader. Please only output the score number. Evaluation Results of General-purpose"
        },
        {
            "title": "Models",
            "content": "Tables 36 summarize the average performance of the General-purpose models across all datasets with respect to the COMET, COMETKiwi, GRB, and GRF metrics, respectively."
        },
        {
            "title": "Budget",
            "content": "0 100 200 300"
        },
        {
            "title": "0.712\nCogito-3B\nCogito-8B\n0.765\nQwen3-0.6B 0.702\nQwen3-1.7B 0.760\n0.789\nQwen3-4B\n0.801\nQwen3-8B\n0.805\nQwen3-14B\nQwen3-32B\n0.804\nGrok-3-Mini",
            "content": "0.671 0.762 0.694 0.757 0.785 0.800 0.807 0.804 0.695 0.764 0.699 0.764 0.789 0.799 0.803 0.802 0.698 0.763 0.695 0.763 0.790 0.800 0.806 0.803 0.704 0.762 0.695 0.764 0.791 0.799 0.805 0.802 0.707 0.768 0.696 0.769 0.791 0.798 0.799 0.802 0.704 0.769 0.689 0.764 0.787 0.797 0.797 0."
        },
        {
            "title": "Budget",
            "content": "0 100 200 300"
        },
        {
            "title": "0.634\nCogito-3B\nCogito-8B\n0.677\nQwen3-0.6B 0.618\nQwen3-1.7B 0.675\n0.706\nQwen3-4B\n0.719\nQwen3-8B\n0.725\nQwen3-14B\nQwen3-32B\n0.724\nGrok-3-Mini",
            "content": "0.599 0.676 0.617 0.671 0.702 0.718 0.724 0.720 0.619 0.681 0.616 0.678 0.705 0.717 0.723 0.721 0.625 0.679 0.617 0.678 0.707 0.718 0.725 0.722 0.629 0.680 0.614 0.679 0.709 0.717 0.725 0.724 0.631 0.682 0.615 0.685 0.709 0.718 0.718 0.723 0.632 0.682 0.609 0.676 0.700 0.712 0.713 0. 0.701 0.701 Table 4: Average COMETKiwi scores of generalpurpose models across all datasets with varying thinking budgets. Model Budget 100 200 300 500 1000 low high 82.546 Cogito-3B Cogito-8B 88.177 Qwen3-0.6B 58.165 Qwen3-1.7B 74.422 84.909 Qwen3-4B 89.204 Qwen3-8B 90.899 Qwen3-14B Qwen3-32B 90.949 Grok-3-Mini 78.016 88.552 57.507 74.634 84.921 89.394 90.875 91.103 79.590 88.312 57.518 75.522 85.361 89.441 90.707 91.204 80.571 88.402 57.813 74.883 85.809 89.623 91.075 91. 81.176 87.948 57.317 75.076 86.165 89.651 91.163 91.340 81.497 88.432 57.372 75.678 86.379 89.541 91.253 91.556 81.937 88.351 57.734 75.753 86.148 90.001 91.294 91.455 92.529 92.451 Table 5: Average GRB scores of general-purpose models across all datasets with varying thinking budgets."
        },
        {
            "title": "Token Statistics of DRT Models on\nLiterary Translation Tasks",
            "content": "Tables 710 present the performance of the DRT models across three literary translation benchmarks with respect to the GEA100, GEA5, GRB, and GRF metrics, respectively. Table 11 shows the token statistics under different budgets. Figure 6 visualizes DRT models translation quality (GRB&GRF score, right y-axis) and the actual number of thinking tokens (left y-axis). On the in-domain MetaphorTrans task, there is positive growth of the quality scores as the thinking budget increases to around 300 tokens, then the score generally stabilizes. While on the other two out-of-domain tasks, the performance fluctuates and shows an overall downward trend as thinking tokens increase. Post-editing Prompts and Detailed"
        },
        {
            "title": "Results",
            "content": "0.794 0.795 D.1 Prompts Table 3: Average COMET scores of general-purpose models across all datasets with varying thinking budgets. We experiment with two variants of post-editing prompts: with and without an additional quality score (QS). Examples are provided below. Post-editing with QS: 13 Model Budget 0 100 300 500 1000 2000 low high Task Model Budget 80.928 Cogito-3B Cogito-8B 89.470 Qwen3-0.6B 56.858 Qwen3-1.7B 74.954 86.238 Qwen3-4B 90.321 Qwen3-8B 92.084 Qwen3-14B Qwen3-32B 92.122 Grok-3-Mini 77.468 89.550 56.943 75.377 86.229 90.625 92.016 92.268 80.178 88.707 57.549 76.210 86.722 90.648 91.932 92. 81.680 88.942 57.137 75.829 86.965 90.816 92.359 92.608 81.577 88.082 57.232 75.723 87.603 91.024 92.503 92.702 81.843 89.067 56.890 76.770 87.806 91.123 92.492 92.835 80.632 89.284 56.990 76.591 87.577 91.258 92.547 92.782 MetaphorTrans LitEval-Corpus 93.494 93.594 WMT24-Literary 100 200 400 500 1000 2000 75.08 DRT-7B DRT-8B 71.81 DRT-14B 76.88 65.09 DRT-7B DRT-8B 58.11 DRT-14B 67. 79.45 DRT-7B DRT-8B 63.00 DRT-14B 75.34 79.63 75.36 81.02 63.66 56.92 69.90 75.48 59.66 74.38 80.52 76.57 81.62 65.07 55.00 68. 74.50 61.57 77.18 80.72 77.48 81.90 64.38 55.39 73.83 77.35 59.75 74.06 80.81 77.70 82.16 62.75 56.10 74. 78.03 67.05 75.54 80.74 77.89 82.39 68.08 57.98 76.07 77.15 61.18 81.37 80.89 78.38 82.45 64.47 58.26 73. 76.62 57.16 80.01 Table 7: GEA100 scores of DRT models on literary translation tasks with varying thinking budgets."
        },
        {
            "title": "MetaphorTrans",
            "content": "LitEval-Corpus WMT24-Literary 100 200 300 500"
        },
        {
            "title": "4.02\nDRT-7B\nDRT-8B\n2.67\nDRT-14B 3.73",
            "content": "3.92 3.75 3.97 3.26 2.95 3.45 3.85 2.93 3.73 3.95 3.81 3.98 3.27 2.85 3.37 3.65 2.90 3. 3.96 3.85 4.00 3.31 2.96 3.43 3.58 3.13 3.79 3.96 3.86 4.00 3.12 2.96 3.58 3.83 3.09 3. 3.96 3.87 4.00 3.29 3.08 3.55 3.83 3.01 3.86 3.95 3.88 4.01 3.23 3.01 3.56 3.64 2.60 3. Table 8: GEA5 scores of DRT models on literary translation tasks with varying thinking budgets. D.2 Detailed Results Tables 12 and 13 report the GRB and GRF scores respectively. Table 6: Average GRF scores of general-purpose models across all datasets with varying thinking budgets. Figure 6: Performance and actual generated thinking tokens of DRT models across 3 literary translation tasks. You are professional translator, and your task is to refine the {tgt_lang} draft translation below based on the {src_lang} source text and its quality evaluation. Please only provide me with the refined translation, without any additional explanations. Source Text: {src_text} Draft Translation: {hyp_text} Quality Score: {quality_score}/ Post-editing without QS: You are professional translator, and your task is to refine the {tgt_lang} draft translation below based on the {src_lang} source text. Please only provide me with the refined translation, without any additional explanations. Source Text: {src_text} Draft Translation: {hyp_text} 14 Task Model Budget MetaphorTrans LitEval-Corpus WMT24-Literary 100 200 400 500 1000 2000 92.51 DRT-7B DRT-8B 91.30 DRT-14B 92.36 67.81 DRT-7B DRT-8B 68.13 DRT-14B 78. 89.54 DRT-7B DRT-8B 88.83 DRT-14B 90.56 93.01 91.87 93.45 65.00 63.33 76.99 88.13 90.59 89.81 93.04 92.07 93.44 63.08 60.89 74. 89.06 87.96 87.06 92.81 92.08 93.25 63.73 65.02 73.73 89.26 87.10 88.87 92.73 92.04 93.34 63.02 61.95 74. 88.46 86.26 88.53 92.66 91.85 93.20 61.25 62.55 74.75 88.25 85.18 89.19 92.69 91.95 93.27 59.89 62.12 71. 88.03 88.76 89.63 Table 9: GRB scores of DRT models on literary translation tasks with varying thinking budgets. Task Model Budget MetaphorTrans LitEval-Corpus WMT24-Literary 100 200 300 500 1000 2000 89.91 DRT-7B DRT-8B 88.25 DRT-14B 89.67 68.17 DRT-7B DRT-8B 66.01 DRT-14B 75.18 76.06 DRT-7B DRT-8B 39.38 DRT-14B 57. 90.81 89.14 91.35 65.50 62.22 75.88 64.40 29.54 60.20 90.84 89.26 91.21 65.32 60.67 73.99 70.39 40.48 59. 90.66 89.37 91.09 65.66 61.75 72.16 60.50 40.19 64.58 90.71 89.52 91.44 64.52 61.15 74.13 69.66 41.83 74. 90.66 89.42 91.34 63.25 62.80 73.36 60.51 38.72 68.64 90.66 89.65 91.23 61.53 61.43 71.27 69.00 35.03 79. Table 10: GRF scores of DRT models on literary translation tasks with varying thinking budgets. Task Model Budget MetaphorTrans LitEval-Corpus WMT24-Literary 100 200 300 400 1000 2000 96.50 DRT-7B DRT-8B 95.50 DRT-14B 94.27 95.97 DRT-7B DRT-8B 91.12 DRT-14B 94.90 95.89 DRT-7B DRT-8B 90.71 DRT-14B 88.43 195.74 194.87 195. 193.87 189.58 194.57 195.91 196.56 180.07 295.37 294.83 293.20 293.37 290.99 294.72 290.76 268.99 258.03 387.48 390.00 382. 395.45 393.68 391.62 394.55 353.15 353.63 443.83 459.57 439.87 492.04 486.73 486.15 493.93 458.29 453.92 476.93 516.67 474. 831.38 876.08 812.47 966.17 902.59 876.82 476.94 514.38 472.84 913.88 1049.21 919.86 1870.02 1764.82 1727.59 Table 11: Actual thinking tokens of DRT models on literary translation tasks with varying thinking budgets."
        },
        {
            "title": "No QS",
            "content": "QS Budget=0 Budget=500 Budget=1000 Budget=0 Budget= Budget=1000 Qwen3-0.6B Qwen3-1.7B Qwen3-4B Qwen3-8B Qwen3-14B Qwen3-32B 69.293 82.327 88.695 91.253 92.199 92.197 69.756 (+0.463) 80.913 (-1.415) 88.623 (-0.072) 91.275 (+0.022) 92.241 (+0.042) 92.415 (+0.219) 69.417 (+0.124) 82.896 (+0.568) 89.734 (+1.039) 91.618 (+0.365) 92.482 (+0.283) 92.502 (+0.305) 69.445 (+0.152) 82.930 (+0.602) 89.888 (+1.193) 91.747 (+0.494) 92.529 (+0.330) 92.497 (+0.301) 69.112 (-0.181) 81.438 (-0.889) 88.883 (+0.188) 91.173 (-0.080) 92.294 (+0.095) 92.365 (+0.169) 69.538 (+0.246) 82.878 (+0.551) 89.725 (+1.030) 91.653 (+0.400) 92.448 (+0.249) 92.282 (+0.085) 69.410 (+0.117) 83.275 (+0.947) 89.835 (+1.140) 91.657 (+0.404) 92.472 (+0.272) 92.334 (+0.138) Table 12: Post-editing GRB score with and without QS at different budgets."
        },
        {
            "title": "No QS",
            "content": "QS Budget=0 Budget=500 Budget=1000 Budget=0 Budget= Budget=1000 Qwen3-0.6B Qwen3-1.7B Qwen3-4B Qwen3-8B Qwen3-14B Qwen3-32B 67.477 81.727 88.588 91.208 92.261 92.256 67.757 (+0.281) 79.159 (-2.568) 88.625 (+0.038) 90.440 (-0.768) 92.613 (+0.352) 92.767 (+0.511) 67.936 (+0.459) 81.988 (+0.261) 89.936 (+1.349) 92.007 (+0.799) 92.763 (+0.501) 92.923 (+0.667) 67.989 (+0.512) 82.305 (+0.578) 90.057 (+1.470) 92.018 (+0.810) 92.853 (+0.592) 92.864 (+0.608) 67.987 (+0.510) 80.260 (-1.467) 88.755 (+0.168) 89.499 (-1.709) 92.642 (+0.381) 92.816 (+0.560) 68.085 (+0.608) 82.445 (+0.718) 89.885 (+1.298) 92.051 (+0.843) 92.757 (+0.495) 92.768 (+0.512) 68.057 (+0.580) 82.730 (+1.003) 90.031 (+1.444) 91.933 (+0.725) 92.810 (+0.549) 92.899 (+0.643) Table 13: Post-editing GRF score with and without QS at different budgets."
        }
    ],
    "affiliations": [
        "University of Helsinki",
        "University of Turku"
    ]
}