{
    "paper_title": "Causal Attention with Lookahead Keys",
    "authors": [
        "Zhuoqing Song",
        "Peng Sun",
        "Huizhuo Yuan",
        "Quanquan Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In standard causal attention, each token's query, key, and value (QKV) are static and encode only preceding context. We introduce CAuSal aTtention with Lookahead kEys (CASTLE), an attention mechanism that continually updates each token's keys as the context unfolds. We term these updated keys lookahead keys because they belong to earlier positions yet integrate information from tokens that appear later relative to those positions, while strictly preserving the autoregressive property. Although the mechanism appears sequential, we derive a mathematical equivalence that avoids explicitly materializing lookahead keys at each position and enables efficient parallel training. On language modeling benchmarks, CASTLE consistently outperforms standard causal attention across model scales, reducing validation perplexity and improving performance on a range of downstream tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 1 0 3 7 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Causal Attention with Lookahead Keys",
            "content": "Zhuoqing Song1,2,, Peng Sun1, Huizhuo Yuan1, Quanquan Gu1, 1ByteDance Seed, 2Princeton University Work done during internship at ByteDance Seed, Corresponding author"
        },
        {
            "title": "Abstract",
            "content": "In standard causal attention, each tokens query, key, and value (QKV) are static and encode only preceding context. We introduce CAuSal aTtention with Lookahead kEys (CASTLE), an attention mechanism that continually updates each tokens keys as the context unfolds. We term these updated keys lookahead keys because they belong to earlier positions yet integrate information from tokens that appear later relative to those positions, while strictly preserving the autoregressive property. Although the mechanism appears sequential, we derive mathematical equivalence that avoids explicitly materializing lookahead keys at each position and enables efficient parallel training. On language modeling benchmarks, CASTLE consistently outperforms standard causal attention across model scales, reducing validation perplexity and improving performance on range of downstream tasks. Date: September 8, 2025 Correspondence: Quanquan Gu at quanquan.gu@bytedance.com"
        },
        {
            "title": "Introduction",
            "content": "Causal attention [25] is cornerstone of autoregressive sequence modeling, allowing each token to condition on its past while preserving the autoregressive structure that underpins language generation. Building on this mechanism, large language models (LLMs) have transformed natural language processing by scaling up the model size and the number of tokens trained [3, 11, 18]. While this trend has delivered impressive capabilities, it increasingly runs up against practical bottleneck, that is high-quality tokens. This reality makes it imperative to improve the attention layer itself and to develop model architectures that are more token-efficient, delivering stronger performance under fixed training-token budgets. In standard causal attention, each tokens query, key, and value (QKV) are computed from that tokens representation and remain fixed; they cannot incorporate information from subsequent tokens. As result, tokens QKV can encode only its preceding context. Recent work shows that such causal mask, which blocks each tokens access to its future information, limits models ability to capture global context, and impairs natural language understanding [8, 13, 21, 26, 30]. Here are some vivid illustrations that give intuitions for the limitations of causal masking. Garden-path sentences [17] are structurally ambiguous, often inducing an incorrect initial parse. For example, the old man the boat. Because garden-path sentences correct interpretation typically depends on information that appears later in the sentence, the causal mask restricting tokens to past context can cause models to struggle in resolving such ambiguities effectively [1]. In many tasks, the question/focus appears at the end of the input. Without access to future context, earlier tokens cannot encode the relevant information needed to anticipate the question/focus. As result, early token 1 representations may fail to capture important cues and global context dependencies [26]. Several recent works have proposed remedies to address these limitations on sentence embedding. BeLLM (Backward Dependency Enhanced LLM) [13] introduces backward dependencies which enables sentence embeddings to incorporate both past and future context, significantly improving performance on semantic textual similarity tasks and downstream applications. Echo Embeddings [21] duplicate the input and extracting representations from the second occurrence. The model effectively allows early tokens to attend to later ones and thus the model improves over classical LM embeddings without changing the architecture or requiring fine-tuning. Re-Reading (RE2) [26] enhances reasoning by prompting decoder-only LLMs to process the same question twice. The second pass provides tokens with access to global information from the first pass, capturing global dependencies and yielding stronger performance on arithmetic, commonsense, and symbolic reasoning tasks. Despite these advances, it remains unclear whether the above methods can benefit pretraining. To address the shortcomings of standard causal attention in pretraining, we propose novel attention mechanism, CAuSal aTtention with Lookahead kEys (CASTLE). In this approach, when generating the (t + 1)-th token, we update keys of all preceding tokens so that keys of token (1 t) are able to additionally encode information from token + 1 to t. These keys are called lookahead keys. This design preserves the autoregressive structure while allowing the keys to evolve as the context unfolds. In Figure 1, we give an illustration of receptive fields of the keys in CASTLE. Although the mechanism appears recurrent, we establish mathematical equivalence that avoids explicitly materializing the lookahead keys and enables efficient parallel training. We evaluate our approach on language modeling across multiple model scales. Experimental results show that CASTLE consistently outperforms standard causal attention in terms of validation perplexity and downstream task performance. The remainder of this paper is organized as follows. We discuss related work in Section 1.1. In Section 2.1, we elaborate on our motivations. In Section 2.2, we formally define CASTLE in its recurrent form. Section 2.3 proves an equivalent parallel formulation of CASTLE and develops efficient parallel training algorithms Section 2.4 introduces efficient inference algorithms together with the counterpart of the KV cache in CASTLE. Finally, Section 3 presents empirical results demonstrating the effectiveness of CASTLE across diverse tasks and model scales."
        },
        {
            "title": "1.1 Related Work",
            "content": "Several studies have observed that the causal mask, by preventing tokens from accessing future information, can hinder models ability to capture global context and thereby degrade its natural language understanding [8, 13, 21, 26, 30]. Much effort has been made to overcome this shortcoming in sentence embedding. BeLLM (Backward Dependency Enhanced LLM) [13] modifies decoder layers to enable sentence embeddings to integrate both past and future information. This yields substantial improvements on semantic textual similarity and downstream tasks. Echo Embeddings [21] duplicate the input sequence and extract embeddings from the second occurrence, letting early tokens attend to later ones without modifying the architecture. Similarly, Re-Reading (RE2) [26] prompts models to process inputs twice, so the second pass captures global information obtained in the first. These methods improve embedding quality and reasoning, but their benefits in large-scale pretraining remain unclear. Selective Attention [12] introduces parameter-free modification where tokens can mask out irrelevant or outdated tokens from future attention. By subtracting accumulated selection scores from the attention logits, selective attention reduces reliance on unneeded context. As result, it achieves substantial memory and compute savings without degrading perplexity. However, selective attention primarily emphasizes filtering unneeded past tokens to enhance efficiency. As discussed in the introduction, many scenarios, such as garden-path sentences or cases where the key information appears at the end of the input, require mechanisms that actively incorporate crucial future information. Whether selective attention can address this challenge remains uncertain. 2 Figure 1 Receptive fields of keys in standard causal attention and CASTLE. The top row shows standard causal attention when generating the 4th token (left) and the 6th token (right). The bottom row shows CASTLE under the same settings. Here, key1, key2, denote the keys corresponding to tokens 1, 2, , while T1, T2, denote the tokens. In standard causal attention, keys are static: when generating the (t + 1)-th token, each key with 1 can only access information from {T1, , Ti}, and key remains the same for all later steps. In contrast, CASTLE continuously updates keys at each prediction step, i.e., when generating the (t + 1)-th token, the receptive field of any key with 1 t, is expanded to contain information from {T1, , Tt}."
        },
        {
            "title": "2 Causal Attention with Lookahead Keys",
            "content": "Our motivations are discussed in Section 2.1. Formal mathematical definitions of CASTLE in recurrent form are provided in Section 2.2. Direct application of the recurrent form of CASTLE in Section 2.2 is impractical for large-scale pretraining. To address this, we present efficient pretraining algorithms in Section 2.3. In Section 2.4, we describe efficient inference algorithms along with the counterparts of the KV cache in CASTLE."
        },
        {
            "title": "2.1 Motivations",
            "content": "We first recall the standard causal attention. Given contextualized representations RLdmodel where is the sequence length and dmodel is the hidden dimension. The standard causal attention first computes = LW Q, = LW K, = LW RLd, where is the head dimension. Then, the standard causal attention is computed as follows CausalAttention(X L) = row_softmax (cid:16) QK + C(cid:17) RLd, (1) where RLL is the causal mask which prevents each token from attending to its future tokens, i.e., ij = 0 if and ij = otherwise. To explain our motivations, we begin with the recurrent form of standard causal attention. Consider the case Rtdmodel, where 1 2 ... when generating the (t + 1)-th token. Given contextualized representations = is the representation of token s. Unless otherwise specified, all vectors in this paper are treated as row vectors rather than column vectors. The query, key and value of token are = sW Q, = sW K, = sW . We also denote = tW and = tW . Then, the standard causal attention follows the recurrent form causal-attention(X t) = softmax (cid:33) (cid:32) tK = (cid:80)t s=1 exp (cid:0)q tk / s=1 exp (cid:0)q tk (cid:80)t / d(cid:1)v d(cid:1) R1d. Due to the autoregressive structure, each only encodes information from token 1 to s. Thus, when generating token + 1 with + 1 > s, each only contains information from token 1 to without containing information from token + 1 to t. This can impair models ability of natural language understanding, yielding high-quality text embedding and capturing global context as mentioned in the introduction. This motivates us to propose novel attention mechanism, causal attention with lookahead keys (CASTLE), i.e., when generating the (t + 1)-th token, we first update keys of preceding tokens with < + 1 to additionally incorporate information from token + 1 to t. We refer to these as lookahead keys because their representations renew with the growing context. In this way, lookahead keys may lead to more accurate attention scores while preserving the autoregressive property of the model. Before describing the details of this mechanism, we first answer the following questions. Why do we use lookahead keys instead of lookahead queries? The answer parallels the reason why keyvalue (KV) pairs are cached instead of queries (Q). Each is only used once, namely when generating token s+1. Because we are designing an autoregressive model, past queries cannot be modified after generation, making it meaningless to update of all subsequent tokens s. Keeping updated therefore can benefit all later tokens by possibly producing more accurate attention scores. . In contrast, is multiplied by the queries We also remark that renewing the values in similar way to what we do for lookahead keys could potentially be beneficial. In this work, however, we focus on updating keys because doing so admits mathematical equivalence (shown in Section 2.3) with O(L2d) computational complexity. Designing efficient parallel training algorithms that utilize lookahead values remains an interesting direction for future research. How do we maintain the model autoregressive with lookahead keys? When generating token + 1, we update keys of each preceding token < + 1 with information from token + 1 to t. Thus, all keys only contain information from token 1 to t. Queries and values are naturally only containing information from tokens up to t. No future information from tokens > is used. Thus, the model maintains autoregressive property. Further details of our design are presented in Section 2.2."
        },
        {
            "title": "2.2 Mathematical Definition in Recurrent Form\nWe give mathematical definitions of CASTLE in this section. Let L denote sequence length and dmodel, d\ndenote the hidden dimension and head dimension, respectively.",
            "content": "Throughout Section 2.2, we fix {1, 2, , L} and consider the setting where tokens have already been generated and the model is about to generate the (t + 1)-th token. Denote the input contextualized representations = 1 2 ... Rtdmodel, where is the representation of 4 token s. Utilizing lookahead keys lies at the core of CASTLE. However, the way model learns to encode information into keys of token from past tokens (k s) may differ from how it encodes information from subsequent tokens (tokens < < + 1) when generating the (t + 1)-th token. To address this, we adopt hybrid design. Specifically, we keep half of the keys the same as in standard causal which we call causal keys, while allowing the remaining half to renew as the context progresses which we call lookahead keys. For each preceding token (1 < + 1), causal keys of token is projection of s, while lookaheads keys of token contain information from representations {x s+1, , t}. The receptive fields of causal keys and lookahead keys are illustrated in Figure 2. We first project into key and value matrices , , , Rtd by"
        },
        {
            "title": "K U",
            "content": "t = tW K, = tW , and = tW K, = tW as well as query matrix , U , , Rdmodeld are learnable matrices. = tW Rtd and query vector = tW R1d. Here, , , , The matrices , and the lookahead key are multiplied by the query vector are multiplied by the attention weights to get the output. Before we elaborate on details in the definition of CASTLE, we first give formal definitions of causal keys and lookahead keys. are used to generate the lookahead key t. Then, the causal key to get the attention scores. Then, Causal Keys. The causal keys in CASTLE is defined similarly to the keys in standard causal attention. More specifically, causal keys are defined as t = = tW Rtd. 1 2 ... The s-th row of and s-th rows of satisfying = tW t are equal to each other whenever t, s. is the causal key of token s. Causal keys are static, i.e., the Lookahead Keys. We utilize structure similar to the attention mechanism to define lookahead keys. More specifically, the lookahead keys are defined as = 1 2 ... = sigmoid(cid:0) K + (cid:1)V Rtd, (2) Rtt is mask matrix with [M where ]ij = otherwise. The s-th row of is the lookahead key of token s. We remark that in is defined when tokens have already been generated and we are about to generate the (t + 1)-th token, while the subscript indicates guarantees that the lookahead key of token s, , the superscript indicates that t ]ij = 0 if < and [M is the s-th row of t. keeps renewing as the context goes, it is natural that , is exposed to information from . = t+1 The definition of {x s+1, , t}. Since 5 Figure 2 Receptive fields of causal keys and lookahead keys with respect to contextualized representations and tokens (excluding the first layer) when generating the 6th token. Tokens are denoted by Ti and their contextualized representations by i. Causal key corresponds to the causal key of token i, while lookahead key corresponds to the lookahead key of token i. When generating the (t + 1)-th token, for token (s < + 1), the causal key of token is projection of s. Due to the softmax in attention, except in the first layer, causal keys of attend over tokens {T1, , Ts}. For token < t, lookahead keys of incorporate information from {x s+1, , t} and attend over all ]t,: = ()1t, lookahead keys of token are existing tokens {T1, , Tt}. Since the last row of is defined as [M all-zeros vectors and thus have empty receptive fields when generating the (t + 1)-th token. An illustration for lookahead keys can be found in Figure 3. We have the following remarks regarding the definition of lookahead keys in (2). Why are we using sigmoid? The sigmoid function is used in (2) instead of softmax due to the consideration that when generating token + 1, for token with < + 1, synthesizing information contained in tokens + 1 to should not be compulsory. However, since the probabilities in softmax sum up to 1, which forces to incorporate information from tokens + 1 to and is not desired. Lookahead keys maintains autoregressive property. First, CASTLE is defined in recurrent form is only exposed which is naturally autoregressive. Second, when generating the (t + 1)-th token, each to information from representations of tokens + 1 to as in (2). No information from tokens which are not yet generated is exposed. Lookahead keys only occur in CASTLEs recurrent form definition and inference, but cannot be may vary, this prevents us from materializing for materialized in parallel training. Since each t. The computation cost in (2) is O(t2d). If we materialize all in parallel, the computational cost is at least (cid:80)L t=1 t2d = O(L3d) which makes training on large-scale datasets impractical. In Section 2.3, we will give an equivalent form which removes the need of materializing each and enables efficient parallel training. and t+ 6 Figure 3 Illustration for the definition of lookahead keys in (2) when generating the 4-th token. Let dmodel and denote the hidden dimension and head dimension, respectively. In this figure, we set = 3 and = 2. CASTLE in Recurrent Form. After defining causal keys and lookahead keys, we are ready to give the formula of CASTLE in recurrent form. To generate the (t + 1)-th token, we utilize both the causal keys Rtd and the lookahead keys t. More specifically, let the causal-key attention scores be Let the lookahead-key attention scores be = q R1t. = t R1t. Then, we define attention weights by combining the above attention scores as follows = softmax (cid:0)s SiLU (cid:0)s (cid:1)(cid:1) R1t, where SiLU(x ) = sigmoid(x ). Then, the output is calculated as attention (cid:0)X t(cid:1) = tV R1d. (3) (4) (5) (6) An illustration of CASTLE in its recurrent form can be found in Figure 4. (cid:1)(cid:1) We remark that SiLU is applied in (5) because our ablation study shows that = softmax (cid:0)s outperforms = softmax (cid:0)s (cid:1) in downstream evaluation. We hypothesize that this improvement arises because many past tokens become noise as the context grows. The SiLU transformation effectively acts as gate, regulating the degree to which each past token should be forgotten. SiLU (cid:0)s U Figure 4 Illustration of CASTLE in recurrent form when generating the 4th token. The causal and lookahead keys are queried by to generate their respective attention scores, which are combined and then goes through softmax to yield attention weights t. These weights are then multiplied by the value matrix to compute the output = attention (cid:0)X t(cid:1) R1d"
        },
        {
            "title": "2.3 Efficient Parallel Training",
            "content": "In this section, we introduce our efficient parallel training algorithm. As discussed in Section 2.2, straightforward materializing each in parallel incurs at least O(L3d) computational costs. Such complexity makes training on large-scale datasets infeasible. To address this, we derive an equivalent parallel formulation of CASTLE that eliminates the need to materialize lookahead keys. This formulation then serves as the basis for designing an efficient algorithm. CASTLE in Parallel Form. Let attention (cid:0)X t(cid:1) R1d denote the output when generating the (t + 1)-th token as in (6). Then, given the inputs RLdmodel, the concatenated outputs are denoted by Attention(X L) = attention(X 1) attention(X 2) ... attention(X L) RLdmodel. (7) The following theorem provides parallel formulation of CASTLE that is equivalent to the recurrent form introduced in Section 2.2, but eliminates the need to explicitly materialize lookahead keys. Theorem 1. Consider inputs RLdmodel, where is the sequence length and dmodel is the hidden dimension. Let = LW Q, = LW K, = LW K, = LW Q, = LW , = LW . Define matrix RLL as (cid:32) = CV (cid:102)M (cid:33) (cid:32) sigmoid (cid:32) U (cid:33)(cid:33) + . (8) 8 Then, the outputs Attention(X L) as in (7) satisfies that Attention(X L) = row_softmax (cid:32) CK + SiLU(cid:0)S (cid:1) (cid:33) C. (9) Here, C, (cid:102)M if and U ij = 0 if < and ij = otherwise. are the causal masks which prevent tokens from attending to their future tokens, i.e., ij = 0 ij = 0 otherwise. is the same as in (2), with ij = 1 if and (cid:102)M ij = otherwise; (cid:102)M The proof of Theorem 1 is deferred to Appendix A. Efficient Parallel Training. Theorem 1 establishes the equivalence between the recurrent and parallel formulations of CASTLE. However, computing Attention(X L) directly from Theorem 1 still requires Ω(L3) complexity, since (8) involves matrix multiplications between matrices. is masked low-rank matrix because the To reduce this cost, notice that in (8), the term (cid:0)Q CV (cid:1) (cid:102)M matrix CV is of rank which is typically much smaller than L. This structure enables more efficient computation of , which we exploit to design parallel training algorithm. K, = LW Theorem 2. Given Ls query, key and value matrices = LW , = LW , Algorithm 1 (forward pass) and Algorithm 3 (backward pass) enables efficient parallel training and can compute Attention(X L) and the gradients with computational complexity O(L2d) and space complexity O(Ld). K, = LW Q, = LW Q, = LW The proof of Theorem 2 is given in Appendix C."
        },
        {
            "title": "2.4 Efficient Inference with UQ-KV Cache",
            "content": "In this section, we introduce the inference algorithm. We first introduce the decoding algorithm. The decoding algorithm consists of the following 2 steps: updating step and combining step. Fix {1, 2, . . . , L}, and consider generating the (t + 1)-th token. Updating step. We generate lookahead keys in the updating step. First, we compute U = tW computation, we update recursively = tW , . Next, Rather than computing directly from (2), which requires O(t2d) = tW and t = (cid:32) t1 + sigmoid (cid:16) t1k (cid:17) t (cid:33) . (10) 01d The proof of (10) is given in Appendix D. Next, we discuss the caching strategy and FLOPs of the updating step. Caching in updating step. First, it is obvious that we need to cache so that we can implement the is used in any update from t1 to recursive formula (10). Second, we need to cache U with 1. As are only used in the update from t1 to and never used again in update from j1 to with = t, we do not need to cache any other variables except and for the updating step. because and FLOPs in updating step. With cached t1 and , the updating formula (10) needs only O(td) needs O(ddmodel) FLOPs, yielding total FLOPs of O(ddmodel + t1 FLOPs. And computing td). , and . Next, Combining step. In the combining step, we compute the attention outputs are then obtained by applying (3), (4), (5) and (6) with O(td) FLOPs. At this stage, since we already cached in the updating step, only need to be stored. = tW = tW = tW and and , 9 , which we collectively refer to as the UQ-KV cache. All other variables, including UQ-KV cache. From the above analysis, the counterpart of the KV cache in CASTLE consists of t, , , and , and , can be safely disposed of after use. For the prefilling stage, let the prompt length be and inputs RLdmodel. We first compute = LW = LW . Then, we apply the forward pass of the efficient parallel training algorithm (Algorithm 1) to get Attention(X L). For the UQ-KV cache, since we already have , we only need to obtain L. This can be done similarly to FlashAttention-2 [5] due to the similarity between (2) and standard causal attention. The complete prefilling procedure is given in Algorithm 4. = LW = LW = LW L = LW and , , , , , , , The analysis above leads to the following theorem. Theorem 3. Given inputs RLdmodel, prefilling has computational complexity O(Lddmodel + L2d) and space complexity O(Ld). During the decoding stage, when generating the t-th token, the computational complexity is O(td + ddmodel) and the UQ-KV cache requires O(td) memory."
        },
        {
            "title": "2.5 Multi-Head Causal Attention with Lookahead Keys\nAs in standard causal attention, we also utilize multi-head mechanism. Let n denote the number of heads. In\neach head i, when generating the t-th token, denote the output as in (6) by attentioni(X t) ∈ R1×d. Then,\nthe output of multi-head causal attention with lookahead keys (multi-head CASTLE) is calculated as",
            "content": "multi-head-attention(X t) = Concat (cid:0)attention1(X t), . . . , attentionn(X t)(cid:1) R1d, (11) where Rnddmodel is learnable matrix. The formula for forward pass in parallel training and more details of multi-head CASTLE are in Appendix B."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "Our baseline follows the improved Transformer architecture with SwiGLU [20], Rotary Positional Embeddings [22], and RMSNorm [29] following LLaMA [24]. We train models on four scales from scratch: small (0.16B), medium (0.35B), large (0.75B) and XL (1.3B). The medium, large and XL baseline models mirror the configuration of GPT-3 [3]. For the small setting, we increase the number of heads and the hidden dimension relative to the original GPT-3 configuration to better align parameter counts between standard causal attention and CASTLE. To isolate the effect of the attention mechanism, we replace standard causal attention with dynamic attention with evolving keys while keeping all other components unchanged for fair comparison. We use the AdamW optimizer [14] and follow the training recipe of [6]. All models are trained on FineWeb-Edu dataset [15] for 50B tokens. Further details of experimental setup can be found in Appendix E."
        },
        {
            "title": "3.2 Training & Validation Loss",
            "content": "We report training and validation loss and perplexity in Table 1. Training loss and validation loss curves are shown in Figure 5, Figure 7, Figure 8 and Figure 9. CASTLE consistently outperforms the baseline models in both training and validation loss across all model scales. Specifically, after training 50B tokens, CASTLE outperforms baselines across all model scales and achieves validation losses that are 0.0059, 0.0245, 0.0356, and 0.0348 lower than the baseline for the small, medium, large, and XL models, respectively. The performance gains are particularly significant in the medium, large, and XL models. We hypothesize that this improvement stems from lookahead keys in CASTLE. By incorporating lookahead keys, CASTLE facilitates better global context capture. However, smaller models may struggle to fully leverage this mechanism due to limited capacity to model complex global relationships. As result, the improvement margin for the small model is less significant compared to larger models. 10 Furthermore, as shown in Table 4, CASTLE has the same or fewer parameters than their baseline counterparts. This further underscores CASTLEs effectiveness in improving model performance. Table 1 Training and validation loss and perplexity for models with standard causal attention and CASTLE. We use S, M, L, XL to denote model scales. Each model is trained on FineWeb-Edu for 50B tokens. Significant improvements in loss or perplexity are in bold. Train Eval Baseline-S CASTLE-S Loss 2.795 2.789 Baseline-M 2.641 CASTLE-M 2. Baseline-L CASTLE-L Baseline-XL CASTLE-XL 2.513 2.476 2.430 2.401 PPL 16.364 16. 14.030 13.684 12.346 11.897 11.360 11.031 Loss 2.798 2.792 2.639 2. 2.507 2.472 2.426 2.391 PPL 16.411 16.315 14.004 13.665 12.269 11. 11.309 10.922 Figure 5 Training and validation loss curves of XL models. Training loss curve is smoothened with moving window of 2000 training steps. Validation loss is evaluated every 100 training steps on 40M tokens, and its curve is smoothened by moving window of 20 evaluation intervals. Loss curves for the small, medium and large models can be found in Figure 7, Figure 8 and Figure 9 of Appendix E.3. After 50B training tokens, CASTLE-XL achieves 0.0294 lower training loss and 0.0348 lower validation loss compared to Baseline-XL."
        },
        {
            "title": "3.3 Downstream Evaluation",
            "content": "We evaluate CASTLE on diverse set of downstream benchmarks, including ARC [27], BoolQ [4], HellaSwag [28], MMLU [10], OBQA [16], PIQA [2], Winograde [19] using lm-evaluation-harness [9]. Table 1 reports normalized accuracy for ARC-Challenge, HellaSwag, OBQA, and PIQA, and standard accuracy for the other benchmarks. Results are reported in Table 2 (0-shot) and Table 3 (5-shot). Across all model scales and evaluation settings, CASTLE consistently outperforms the baseline in average downstream accuracy. In the 0-shot setting, CASTLE demonstrates robust improvements on ARC-E, BoolQ, HellaSwag, and OBQA across all scales, highlighting enhanced reasoning and commonsense capabilities. In the 5-shot setting, CASTLE further strengthens performance, with consistent gains on ARC-C across all scales, underscoring its strong few-shot generalization on reasoning and knowledge integration abilities. These findings accompany lower loss and perplexity in Section 3.2 demonstrate that CASTLE not only lowers perplexity but also translates these gains into stronger performance on diverse downstream tasks. 11 Table 2 Evaluation results (0-shot) for downstream tasks of different model scales. Each model is pretrained on FineWeb-Edu for 50B tokens. All values denote accuracy in percentage (%). The higher accuracy values are shown in bold. Hella.=HellaSwag, Wino.=Winograde. Model Name ARC-C ARC-E BoolQ Hella. MMLU OBQA PIQA Wino. Avg. Baseline-S CASTLE-S Baseline-M CASTLE-M Baseline-L CASTLE-L Baseline-XL CASTLE-XL 26.71 26.19 28.58 30. 32.59 32.34 33.79 35.32 54.76 56.69 60.90 61.36 65.07 65.15 66.62 67. 52.51 59.85 53.61 58.01 57.49 57.65 61.04 62.81 35.78 36.28 43.01 43. 47.45 47.87 51.40 52.15 22.89 23.00 23.21 25.34 23.57 24.51 26.72 23. 30.40 31.60 33.40 34.60 32.60 35.60 36.20 37.00 63.98 64.25 67.95 67. 70.51 70.78 72.58 70.67 52.57 52.25 50.91 52.64 50.75 53.51 54.06 56. 42.45 43.76 45.20 46.67 47.50 48.43 50.30 50.72 Table 3 Evaluation results (5-shot) for downstream tasks of different model scales. The higher accuracy values are shown in bold. All values denote accuracy in percentage (%). Each model is pretrained on FineWeb-Edu for 50B tokens. Hella.=HellaSwag, Wino.=Winograde. Model Name ARC-C ARC-E BoolQ Hella. MMLU OBQA PIQA Wino. Avg. Baseline-S CASTLE-S Baseline-M CASTLE-M Baseline-L CASTLE-L Baseline-XL CASTLE-XL 25.68 26.02 31.06 32.17 33.36 37.37 35.58 39.08 54.97 54.25 62.46 64. 63.64 67.89 65.78 70.24 56.09 57.13 48.47 54.62 59.24 50.95 61.07 62. 33.81 35.24 42.83 43.47 46.16 47.71 50.84 51.63 25.54 25.22 25.22 25. 26.82 26.11 26.71 24.16 28.20 29.80 33.00 33.80 33.40 34.20 36.20 37. 63.98 64.53 68.39 69.48 69.53 70.18 71.27 71.00 52.57 50.99 51.78 52. 54.06 54.06 52.72 58.33 42.60 42.90 45.40 46.91 48.28 48.56 50.02 51."
        },
        {
            "title": "4 Conclusion",
            "content": "We introduced CAuSal aTtention with Lookahead kEys (CASTLE), novel attention mechanism that continually updates keys as the context evolves. This design allows each key representation to incorporate more recent information at every prediction step while strictly preserving the autoregressive property. Although CASTLE is defined recurrently, we derived mathematical equivalence that eliminates the need to explicitly materialize lookahead keys at each position, enabling efficient parallel training. Experimental results on language modeling demonstrate that CASTLE consistently outperforms standard causal attention, achieving lower perplexity and stronger downstream performance."
        },
        {
            "title": "References",
            "content": "[1] Samuel Joseph Amouyal, Aya Meltzer-Asscher, and Jonathan Berant. When the lm misunderstood the human chuckled: Analyzing garden path effects in humans and language models. arXiv preprint arXiv:2502.09307, 2025. [2] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [4] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. 12 [5] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations. [6] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. In International Conference on Machine Learning, pages 1004110071. PMLR, 2024. [7] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. [8] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320335, 2022. [9] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. URL https://zenodo.org/ records/12608602. [10] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [11] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [12] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Selective attention improves transformer. arXiv preprint arXiv:2410.02703, 2024. [13] Xianming Li and Jing Li. Bellm: Backward dependency enhanced large language model for sentence embeddings. arXiv preprint arXiv:2311.05296, 2023. [14] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations. [15] Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu: the finest collection of educational content, 2024. URL https://huggingface. co/datasets/HuggingFaceFW/fineweb-edu. [16] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. [17] Bradley Louis Pritchett. Garden path phenomena and the grammatical basis of language processing. Harvard University, 1987. [18] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [19] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 87328740, 2020. [20] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [21] Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan. Repetition improves language model embeddings. In The Thirteenth International Conference on Learning Representations. [22] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [23] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 1019, 2019. [24] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 13 [25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [26] Xiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long, Jian-Guang Lou, and Shuai Ma. Re-reading improves reasoning in large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1554915575, 2024. [27] Vikas Yadav, Steven Bethard, and Mihai Surdeanu. Quick and (not so) dirty: Unsupervised selection of justification sentences for multi-hop question answering. arXiv preprint arXiv:1911.07176, 2019. [28] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [29] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in neural information processing systems, 32, 2019. [30] Siyue Zhang, Yilun Zhao, Liyuan Geng, Arman Cohan, Anh Tuan Luu, and Chen Zhao. Diffusion vs. autoregressive language models: text embedding perspective. arXiv preprint arXiv:2505.15045, 2025."
        },
        {
            "title": "Appendix",
            "content": "A Proof of Theorem 1 First, recall the notations in Section 2.3. More specifically, consider inputs = RLdmodel, where 1 2 ... t is the representation of the t-th input token, is the sequence length and dmodel is the hidden dimension. For each 1 L, denote = tW Q, = tW Q, = tW K, = tW K, = tW V , = tW V . = tW Q, = = tW K, = = tW Q, t = = tW K, = 1 2 ... k 1 2 ... = tW , = tW . 1 2 ... v 1 2 ... t = = 1 2 ... q 1 2 ... C , (cid:102)M and t And attending to their future tokens, i.e., [M ]ij = 0 otherwise; [M and [ (cid:102)M For the projection matrices of the entire sequence L, we drop as in Theorem 1 for simplicity, i,e, are t-by-t mask matrices. t ]ij = 0 if and [M is the t-by-t causal mask which prevents tokens from ]ij = 1 if ]ij = otherwise; [ (cid:102)M ]ij = 0 if < and [M ]ij = otherwise. = C = = LW = LW Q, = Q, = = LW = LW K, = K, = = LW , = LW . And = submatrix of C. , = . Then, is t-by-t submatrix of = . Similarly, is also Consider when we are generating the (t + 1)-th token. As in (2) = 1 2 ... K = sigmoid(cid:0) + (cid:1)V Rtd Then, the lookahead-key attention scores as in (4) are (cid:18) q U = t = sigmoid(cid:0) K (cid:19) + (cid:1) . (12) We will need the following lemma to proceed. Lemma 1. For any vector aaa R1t, let (cid:101)aaa = (aaa, 01(Lt)) R1L, where 01(Lt) is the all-zeros vector of size (1, t). Then, (cid:32) aaa sigmoid (cid:18) K d (cid:19)(cid:33) + , 01(Lt) = (cid:101)aaa (cid:32) sigmoid (cid:18) U (cid:19)(cid:33) . + Proof of Lemma 1. The proof is straightforward by the fact that 1. The upper triangular entries of the transposed matrix (cid:18) (cid:18) sigmoid U + (cid:19)(cid:19) are all 0 by the definition of . 2. The matrix sigmoid i.e., (cid:18) K + (cid:19) equals an upper-left block of the matrix sigmoid (cid:18) U + (cid:19) , sigmoid (cid:18) K (cid:34) (cid:19) = + sigmoid (cid:18) U + (cid:19)(cid:35) , 1:t,1:t where for any matrix A, A1:t,1:t refers to its top-left t-by-t submatrix. Define the vector aaa R1L as [aaa t]i = (cid:0)q U (cid:1) if 1 and [aaa t]i = 0 otherwise. Denote = (cid:0)s (cid:101)s , 01(Lt)(cid:1). Then, by combining (12) with Lemma 1, we have (cid:101)s = (cid:16) , 01(Lt)(cid:17) = (cid:18) q (cid:18) sigmoid K d (cid:19)(cid:19) + , 01(Lt) (cid:18) aaa (cid:18) sigmoid = (cid:19)(cid:19) + . U equals the submatrix which consists of first rows of , we have [aaa t]i = (cid:16) (cid:17) for 1 t. Since Then, by stacking (cid:0)aaa (cid:1) together, we have (cid:16) CV (cid:17) = (cid:102)M , aaa 1 aaa 2 ... aaa where (cid:102)M is defined in Theorem 1 with (cid:102)M ij = 1 if and (cid:102)M ij = 0 otherwise. 16 We concatenate (cid:17) (cid:16) (cid:101)s in an L-by-L matrix. Then, 1tL 1 2 (cid:101)s (cid:101)s ... (cid:101)s = 1 aaa 1 aaa 2 ... aaa (cid:32) (cid:18) U (cid:19)(cid:33) + sigmoid (cid:32) = CV (cid:102)M = , where is given in (8). The causal-key attention scores in (3) is (cid:33) (cid:32) sigmoid (cid:18) U (cid:19)(cid:33) + (13) t = . q (cid:101)s 1tL 2 1 (cid:101)s (cid:101)s ... (cid:101)s L We also denote = (cid:0)s (cid:101)s equaling . Then, by concatenating , ()1(Lt)(cid:1), where ()1(Lt) is (L t)-dimensional vector with all entries into RLL, we have (cid:17) (cid:16) = Then, the outputs Attention(X L) satisfies = CK + C. (14) Attention (cid:16) L(cid:17) = attention (cid:0)X 1(cid:1) attention (cid:0)X 2(cid:1) ... attention L(cid:17) (cid:16) = softmax (cid:0)s softmax (cid:0)s softmax (cid:0)s 1 SiLU(s 2 SiLU(s ... SiLU(s 1 )(cid:1) 2 )(cid:1) 2 1 )(cid:1) (cid:17) (cid:17) (cid:16) (cid:16) = softmax softmax 1 SiLU((cid:101)s (cid:101)s 1 ) 2 SiLU((cid:101)s (cid:101)s 2 ) ... SiLU((cid:101)s (cid:101)s ) (cid:16) (cid:16) SiLU = row_softmax (cid:32) softmax (cid:16) (cid:17) (cid:17)(cid:17) = row_softmax + SiLU CK (cid:33)(cid:33) (cid:32) V C, where the last inequality above is from (13) and (14). This completes the proof of Theorem 1. Further Details on Multi-Head CASTLE Forward pass for multi-head CASTLE. Denote = nhead. Given contextualized representations L, for each head, we can get Attentioni(X L) as in (9). Then, the outputs of multi-head CASTLE can be obtained by Multi-head-Attention(X L) = Concat (cid:16) Attention1(X L), . . . , Attentionn(X L) (cid:17) RLd. 17 , Parameter count of multi-head CASTLE. In each head, the learnable parameters are W The matrix has nheadddmodel parameters. Thus, the multi-head CASTLE has 7nheadddmodel learnable parameters. Rdmodeld. These parameters sum up to 6nheaddmodeld. , , , , Parameter count of standard causal attention. The standard causal attention as in (1) has learnable parameters Q, K, Rdmodeld for each head and Rnddmodel. These parameters sum up to 4nheadddmodel. Efficient Parallel Training Algorithm and Proof of Theorem 2 C.1 Forward Pass Algorithm 1: Efficient parallel forward pass Require: , , , C, C, RLd 1 # Initialization 2 Initialize = 0dL, = 0Ld, ℓ = 0L1 and = ()L1 in HBM 3 # Diagonal blocks 4 for = 0, . . . , 1 (in parallel) do , 5 , Load On chip, compute Tj ,Tj On chip, compute ATj ,Tj Implement online softmax update for block (Tj, Tj) by calling Algorithm , , Tj ,: as in (19) as in (16) from HBM to on-chip SRAM 6 7 8 Tj ,: Tj ,: Tj ,: Tj ,: 9 end 10 # 1-st, , (N 1)-th off-diagonal blocks 11 for = 1, . . . , 1 (sequential) do 12 for = 0, . . . , 1 (in parallel) do 14 15 16 17 18 Tj ,: Tj+k Tj+k1,: , , , , as in (20) , Load :,Tj On chip, update :,Tj On chip, compute Write :,Tj to HBM On chip, compute ATj+k,Tj Implement online softmax update for block (Tj+k, Tj) by calling Algorithm 2 as in (16) as in (21) , Tj+k1,: Tj+k,Tj Tj+k,: Tj+k,: from HBM to on-chip SRAM end 20 end 21 Compute diag (ℓ)1 RLd, + log(ℓ) RL1 22 Save RL1, RdL for backward pass 23 Return the output RLd and Ti Algorithm 2: Online softmax update for block (Ti, Tj) Require: ATi,Tj on chip, ℓ, m, O, in HBM from HBM to on-chip SRAM = max(m Ti , row_max(ATi,Tj )) 1 Load ℓTi 2 On chip, compute new Ti 3 On chip, compute (cid:101)P Ti,Tj = exp(ATi,Tj new Ti 4 On chip, compute ℓnew Ti (cid:16) 5 Write Ti,: diag 6 Write ℓTi ℓnew Ti em Ti new , Ti new Ti ) Ti ℓTi + row_sum( (cid:101)P Ti,Tj ) Ti,: + (cid:101)P Ti,Tj to HBM = em Ti new Tj ,: (cid:17) Ti to HBM 18 Figure 6 Parallel scheme of Algorithm 1 (forward pass). We begin by computing the diagonal blocks of the attention score matrix (Iteration 0). In each subsequent iteration k, the k-th off-diagonal blocks are computed. Blocks with different colors represent different kernel instances. In each iteration, each kernel instance is responsible for computing single block of and applying online softmax (Algorithm 2) to it. Kernel instances within the same iteration are launched in parallel, while the iterations are executed sequentially. The parallel scheme of Algorithm 3 (backward pass) is the reverse of the forward passs parallel scheme. Theorem 1 has shown matrix form of Attention(X L) as in (9). However, computing Attention(X L) directly from (9) still need O(L3) computational costs because to compute , we need matrix multiplication between L-by-L matrices in (8). In this section, we give an efficient algorithm that enables efficient parallel training and reduces computational complexity to O(L2d). We first introduce the notations in this section. We divide the sequence {1, . . . , L} into blocks of size B. For simplicity, we assume that is divisible by B. Let = and Ti = {i + 1, . . . , (i + 1) B} for 0 1. Then, {1, 2, . . . , L} = 1 For any matrix , we use Ti,Tj respectively. Ti,: refers to the submatrix whose row indexes are in Ti. Analogously, :,Tj submatrix whose column indexes are in Tj. to denote the submatrix whose row and column indexes are in Ti and Tj, refers to the i=0 Ti. 19 Denote the attention score matrix by Then, = CK + SiLU (cid:16) (cid:17) . Attention(X L) = row_softmax(A)V and for any 0 i, 1, the block ATi,Tj satisfies ATi,Tj ="
        },
        {
            "title": "Q C",
            "content": "Ti,:K Tj ,: + Ti,Tj SiLU (cid:16)"
        },
        {
            "title": "S U",
            "content": "Ti,Tj (cid:17) . (15) (16) As in FlashAttention-2 [5], we compute each block of and apply online softmax (Algorithm 2) on it to obtain Attention(X L). The first term in ATi,Tj FlashAttention-2. We mainly focus on the second term SiLU complexity of O(L3 + L2d) if we apply (8) directly. Notice that as in (8), as given by (16) can be computed similarly to (cid:16) which can incur total computational Ti,:K Tj ,: Ti,Tj (cid:17) = (cid:32) CV (cid:102)M (cid:33) (cid:32) sigmoid (cid:32) U (cid:33)(cid:33) + , (cid:16) U C(cid:17) is low-rank matrix multiplied by mask matrix because CV has where the term rank at most which is normally much smaller than sequence length L. This motivates our algorithm to compute with computational complexity of O(L2d) while still enabling parallel training. (cid:102)M First, as the upper triangular entries in are all 0, we only need to focus on lower triangular entries in . We divide the computation of into the following parts diagonal blocks: Tj ,Tj with 0 1. k-th off-diagonal blocks: Tj+k,Tj with 0 1 By the definition of , we can write Tj+k,Tj in blockwise way as follows Tj+k,Tj = Tj+k,: j+k1 (cid:88) i=j Ti,: sigmoid (cid:18) Tj ,:K Ti,: (cid:19) + Tj ,Ti + Tj+k,:V Tj+k,: (cid:102)M Tj+k,Tj+k sigmoid (cid:18) Tj ,:K Tj+k,: + Tj ,Tj+k (cid:19) . (17) However, computing each efficient way to reduce training costs. Tj+k,Tj this way will lead to O(L3) computational cost in total. We need more (cid:102)M The key observation in the development of this efficient parallel training algorithm is that the matrix (cid:16) CV (cid:17) is low-rank matrix multiplied by mask because CV has rank equal to or less than the head dimension rather than the sequence length L. This enables us to compute defined in (8) with lower computational costs. More specifically, we will rely on the following auxiliary variable to reduce computational costs. 20 We first init an auxiliary variable (0) = 0dL. Then, when computing the k-th off-diagonal blocks, we will maintain the following relation that (k) :,Tj = j+k1 (cid:88) i=j"
        },
        {
            "title": "V U",
            "content": "Ti,: sigmoid (cid:18) Tj ,:K Ti,: (cid:19) . + Tj ,Ti (18) We will compute the blocks of in the following order: diagonal blocks, 1st off-diagonal blocks, 2nd off-diagonal blocks, , (N 1)-th off-diagonal blocks. Diagonal blocks of . For any 0 1,"
        },
        {
            "title": "S U",
            "content": "Tj ,Tj ="
        },
        {
            "title": "Q C",
            "content": "Tj ,:V Tj ,: (cid:102)M Tj ,Tj sigmoid (cid:18) Tj ,:K Tj ,: (cid:19) . + Tj ,Tj (19) 1st off-diagonal blocks of . For any 0 2, update (1) :,Tj as (1) :,Tj = Tj ,: sigmoid (cid:18) Tj ,:K Tj ,: (cid:19) + Tj ,Tj This satisfies (18) with = 1. Then, it follows from (17) that Tj+1,Tj = Tj+1,:D (1) :,Tj + Tj+1,:V Tj+1,: (cid:102)M Tj+1,Tj+ sigmoid (cid:18) Tj ,:K Tj+1,: + Tj ,Tj+1 (cid:19) . The k-th off-diagonal blocks of . If we have already computed the (k 1)-th off-diagonal blocks and (k1) satisfies (18) with 1, the k-th diagonal blocks can be computed in the following way. First, we update (k) as follows (k) :,Tj = (k1) :,Tj + Tj+k1,: sigmoid (cid:18) Tj ,:K Tj+k1,: + Tj ,Tj+k1 (cid:19) . (20) By induction hypothesis (18), (k) :,Tj also satisfies (18) with k. Then, it follows by (17) that Tj+k,Tj = Tj+k,:D (k) :,Tj + Tj+k,:V Tj+k,: (cid:102)M Tj+k,Tj+k sigmoid (cid:18) Tj ,:K Tj+k,: + Tj ,Tj+k (cid:19) . (21) We can compute all k-th off-diagonal blocks in the above way. When computing each to update (k) and then compute Tj+k,Tj as (21). Both (20) and (21) take O(L2d) FLOPs. Tj+k,Tj , we only need Next, we describe the design of parallel algorithm. The parallelization scheme must satisfy the following requirements: Tj+k,Tj must be computed after Tj+k1,Tj because computing Tj+k,Tj in (21) requires (k) which is derived by updating (k1) in (20). Therefore, ATj+k,Tj should be computed after ATj+k1,Tj . To ensure correctness, online softmax cannot be applied simultaneously to ATi,Tj and ATi,Tk for = k. To meet these constraints, we launch kernel instances to compute attention outputs with respect to blocks in the following order: starting with the diagonal blocks, followed by the first off-diagonal blocks, then the second, and so on up to the (N 1)-th off-diagonal blocks. This parallel execution strategy for Algorithm 1 is illustrated in Figure 6. 21 C.2 Backward Pass In this section, we introduce the backward pass algorithm for efficient parallel training. It is mainly derived by reversing the forward pass. Recall that in the forward pass, we compute the blocks in the following order: diagonal blocks, 1st off-diagonal blocks, 2nd off-diagonal blocks, , (N 1)-th off-diagonal blocks. Then, in the backward pass, we compute the derives in the inverse order as follows: (N 1)-th off-diagonal blocks, (N 2)-th off-diagonal blocks, , 1-st off-diagonal blocks, diagonal blocks. Algorithm 3: Efficient parallel backward pass Require: dO RLdmodel 1 # Initialization 2 Initialize dD = 0dL, dQ = 0Ld, dK = 0Ld, dV = 0Ld, dQ = 0Ld, dK = 0Ld, dV = 0Ld 3 Let RL1, RdL be the tensors saved in forward pass (Algorithm 1) 4 # Preprocess 5 Compute RL1 with = dO i,:O i,: 6 # (N 1)-th, , 1st off-diagonal blocks 7 for = 1, . . . , 1 (sequential) do for = 0, . . . , 1 (in parallel) do Tj+k,Tj Compute Tj+k,Tj Compute Tj+k,Tj Compute dS Update dV Update dQ Update dV Compute (k1) Tj+k,: Tj ,: Tj+k1,: :,Tj , dQ Tj+k,: , dD (k) :,Tj , dQ Tj ,: as in (30) as in (21) and ATj+k,Tj and dP Tj+k,Tj and dS as in (16) as in (22) and (23) as in (25) and (26) Tj+k,Tj , dK , dV Tj ,: Tj+k,: , dK Tj+k1,: as in (24) and (27) , dQ , Tj ,: as in (29) Tj+k,: as in (28) end 17 end 18 # Diagonal blocks 19 for = 0, . . . , 1 (in parallel) do 20 Compute Tj ,Tj Compute Tj ,Tj Compute dS Update dV Tj ,: Tj ,Tj as in (19) and ATj ,Tj and dP Tj ,Tj and dS as in (16) as in (31) and (32) as in (34) and (35) , dQ Tj ,Tj , dV , dK Tj ,: Tj ,: Tj ,: , dQ Tj ,: as in (33) and (36) 9 10 11 12 13 15 16 21 22 23 24 end 25 Return the gradients dQ , dK , dV , dQ C, dK C, dV RLd As in [7], we first preprocess RL1 with = dO i,:O i,:. Let (N 1) be the saved for backward pass in Algorithm 1. Also, let be the vector saved for backward pass in Algorithm 1. Set dD (N 1) = 0dL. Then, we iterate over = 1, . . . , 1. In each iteration, we compute the corresponding gradients and update (k) to (k1) inversely as in the forward pass. Thus, for each k, before we launch kernels for the k-th off-diagonal blocks, we already have (k). k-th off-diagonal blocks. For any 0 1 k, we first compute as in (16). Recall that in the end of the forward pass (Algorithm 1), we have set + log(ℓ). Then, the as in (21) and ATj+k,Tj Tj+k,Tj 22 attention weights can be computed by Compute the gradient of attention weights Tj+k,Tj = exp (cid:0)ATj+k,Tk Tj+k (cid:1) . Then, update the gradient of Tj ,: as follows dP Tj+k,Tj = dO Tj+k,:V Tj ,:. dV Tj ,: dV Tj ,: + Tj+k,Tj dO Tj+k,:. Then, compute the gradients of attention weights dS Tj+k,Tj = Tj+k,Tj (cid:0)dP Tj+k,Tj Tj+k (cid:1) and dS Tj+k,Tj = SiLU(cid:0)S Tj+k,Tj (cid:1) Tj+k,Tj (cid:0)dP Tj+k,Tj Tj+k (cid:1) . Then, we update the gradients of Tj+k,: , Tj ,: through the backward pass of (16) as follows dQ Tj+k,: dQ Tj+k,: + dK Tj ,: dK Tj ,: + Tj ,: , dS Tj+k,Tj dS Tj+k,Tj Tj+k,: . (22) (23) (24) (25) (26) (27) Then, we update the gradients of as follows Tj+k,: , (k) :,Tj , Tj+k,: , Tj ,: , Tj+k,: through the backward pass of (21) + Tj+k,: (28) dQ Tj+k,: dQ Tj+k,: + dD (k) :,Tj dD (k) :,Tj + dV Tj+k,: dV Tj+k,: + dQ Tj ,: dQ Tj ,: + dK Tj+k,: dK Tj+k,: + Tj+k,: Tj+k,Tj dS D (k) :,Tj Tj+k,Tj dS Q Tj+k,: U Tj+k,: Q Tj ,: , where auxiliary matrices dS Tj+k,Tj = sigmoid (cid:18) Tj ,:K Tj+k,: + Tj ,Tj+k (cid:19) (cid:102)M Tj+k,Tj+k , (cid:16) (cid:17) dS Tj+k,Tj = Tj+k,:V Tj+k,: (cid:102)M Tj+k,Tj+k sigmoid (cid:18) Tj ,:K Tj+k,: + Tj ,Tj+k (cid:19) . 23 Then, we update the gradients of Tj+k1,: , Tj ,: , Tj+k1,: from the backward pass of (20) as follows dV Tj+k1,: dV Tj+k1,: + sigmoid (cid:18) Tj ,:K Tj+k1,: dQ Tj ,: dQ Tj ,: + GK Tj+k1,: dK Tj+k1,: dK Tj+k1,: + Tj ,: + Tj ,Tj+k (cid:19) (cid:16) dD (k) :,Tj (cid:17) (29) where the auxiliary matrix = (cid:18)(cid:16) (cid:17) dD (k) :,Tj"
        },
        {
            "title": "V U",
            "content": "Tj+k1,: (cid:19) sigmoid (cid:18) Tj ,:K Tj+k1,: + Tj ,Tj+k1 (cid:19) . After updating gradients, we set dD (k1) follows :,Tj dD (k) :,Tj and get (k1) :,Tj back from (k) :,Tj by reversing (20) as (k1) :,Tj = (k) :,Tj Tj+k1,: sigmoid (cid:18) Tj ,:K Tj+k1,: + Tj ,Tj+k1 (cid:19) . (30) Diagonal blocks. For any 0 1, we first compute compute the attention weights Tj ,Tj as in (19) and ATj ,Tj as in (16). Then, Compute the gradient of attention weights Tj ,Tj = exp (cid:0)ATj ,Tj Tj (cid:1) . Then, update the gradient of Tj ,: as follows dP Tj ,Tj = dO Tj ,:V Tj ,:. dV Tj ,: dV Tj ,: + Tj ,Tj dO Tj ,:. Then, compute the gradients of attention weights dS Tj ,Tj = Tj ,Tj (cid:0)dP Tj ,Tj Tj (cid:1) and dS Tj ,Tj = SiLU(cid:0)S Tj ,Tj (cid:1) Tj ,Tj (cid:0)dP Tj ,Tj Tj (cid:1) . (31) (32) (33) (34) (35) Then, we update the gradients of Tj ,: , Tj ,: , Tj ,: , Tj ,: through the backward pass of (19) as follows , Tj ,: Q Tj ,: U , Tj ,: Q Tj ,: dQ Tj ,: dQ Tj ,: + dV Tj ,: dV Tj ,: + dQ Tj ,: dQ Tj ,: + dK Tj ,: dK Tj ,: + 24 , , (36) where auxiliary matrices dS Tj ,Tj = sigmoid (cid:18) Tj ,:K Tj ,: + Tj ,Tj (cid:19) (cid:102)M Tj ,Tj , (cid:16) (cid:17) dS Tj ,Tj ="
        },
        {
            "title": "Q C",
            "content": "Tj ,:V Tj ,: (cid:102)M Tj ,Tj sigmoid (cid:18) Tj ,:K Tj ,: + Tj ,Tj (cid:19) . Then, the pseudo-code of backward pass is illustrated in Algorithm 3. The backward passs parallel scheme is naturally the reverse of the forward passs parallel scheme. We remark that when computing the gradients with respect to the block (Tj+k, Tj), we update both Tj+k,: Tj+k1,: receives contributions from two sources: (Tj+k, Tj) and (Tj+k1, Tj1). To prevent overlapping updates, we . Specifically, introduce two auxiliary variables for storing intermediate results in dV , namely d(cid:99)V in block (Tj+k, Tj), the update to dV is . After all gradients with respect to off-diagonal blocks have been computed, we obtain accumulated in d(cid:102)V the true gradient of dV by dV d(cid:99)V . The same procedure is applide to dK . , while the update to dV . Consequently, the block is accumulated in d(cid:99)V and d(cid:102)V and + d(cid:102)V Tj+k1,: Tj+k1,: Tj+k,: Efficient Inference with UQ-KV Cache Proof of (10). By (2), s = (cid:88) j=1 Since sj = 1 when > s, we have sigmoid (cid:18) k sj (cid:19) . s = (cid:88) j=s+1 Thus, for 1 < t, sigmoid (cid:18) k (cid:19) . s t1 = sigmoid (cid:18) k (cid:19) . Since t,t = 0, the last row of is all-zeros. This yields (10). 25 Algorithm 4: Prefilling Algorithm Require: , , , C, C, RLd 1 # Get UQ-KV Cache 2 Initialize = 0Ld 3 for = 0, . . . , 1 (in parallel) do 4 Load for = k, . . . , 1 (sequential) do Tk,: from HBM to on-chip SRAM On chip, compute AU Tk,Tj = sigmoid On chip, Tk,: Tk,: + AU Write Tk,: to HBM Tk,Tj"
        },
        {
            "title": "V U",
            "content": "Tj ,: 10 end 11 end 12 Save UQ-KV cache , , C, RLd 13 Call Algorithm 1 to get 14 Return RLd 5 7 8 9 Load Tj , Tj ,: , Tj ,: from HBM to on-chip SRAM (cid:18) Tk ,:K Tj ,: + Tk,Tj (cid:19) Algorithm 5: Decoding Algorithm (generating the (t + 1)-th token) Require: representation R1dmodel, UQ-KV cache t1, = tW = tW = tW = tW , , , , t , t1 , t1 R(t1)d = tW 1 Compute t = tW 2 Compute 3 Update as in (10) 4 Update 5 Update 6 Update 7 Compute t = [Q t1; ] t1; = [K ] = [V t1; ] = K R1t as in (3) t = 8 Compute 9 Compute = softmax (cid:0)s 10 Compute = tV 11 Return R1d and UQ-KV cache t, R1t as in (4) SiLU (cid:0)s R1d as in (6) (cid:1)(cid:1) R1t as in (5) , , t Rtd"
        },
        {
            "title": "E Experimental Details and Additional Results",
            "content": "E.1 Experimental Setup We give details of our experimental setup in this section. E.1.1 Model Architecture We use the improved Transformer architecture with SwiGLU [20], Rotary Positional Embeddings [22], and RMSNorm [29] following LLaMA [24]. More specifically, in each layer l, given the contextualized representations (l) RLdmodel where is the sequence length and dmodel is the hidden dimension, then, (l+1) is obtained by (l) = MultiHead-Attention(RMSNorm(X (l))) (l+1) = SwiGLU(RMSNorm(Y (l))), 26 3 dmodeldmodel are learnable parameters. where the SwiGLU(X ) = (cid:0)Swish(X 1)(X 2)(cid:1)W 3. Here, 1 Rdmodel 8 3 8 The function MultiHead-Attention is instantiated with either the standard multi-head causal attention or the multi-head CASTLE. 3 dmodel, 2 Rdmodel 8 3 dmodel, E.1.2 Model Configuration and Training Recipe We train models on four scales from scratch: small (0.16B), medium (0.35B), large (0.75B) and XL (1.3B), where the medium, large and XL baseline models follow the configurations of GPT-3 [3]. For the small setting, we increase the number of heads and the hidden dimension in the original GPT-3 configuration to better align parameter counts between standard causal attention and CASTLE. The configurations of the models can be found in Table 4. To ensure fair comparison between CASTLE and standard causal attention, we align the number of model parameters by adjusting the number of attention heads and keeping hidden dimensions and head dimensions invariant. This avoids changes to the representational upper bound of the models hidden states and the behavior of RoPE, both of which depend on the hidden dimension. As shown in Appendix B, the number of parameters in CASTLE matches that of standard causal attention when the number of heads 7 nstandard, where nCASTLE and nstandard denote the number of heads in satisfies the relation nCASTLE = 4 CASTLE and standard causal attention, respectively. For the small model, the baseline uses 14 heads. By setting CASTLE to 8 heads, we align the parameter counts. For the other settings, the baseline models use 16 heads. As 16 4 7 9.14, we choose 9 heads for CASTLE, resulting in slightly smaller number of parameters than the baseline. Table 4 Configurations and learning hyper-parameters (batch size in tokens and learning rate) of the models which we trained. Model Name nparams nlayers dmodel nheads Baseline-S CASTLE-S Baseline-M CASTLE-M Baseline-L CASTLE-L 160M 160M 353M 351M 756M 753M Baseline-XL CASTLE-XL 1.310B 1.304B 12 12 24 24 24 24 24 896 (=14 * 64) 896 1024 (=16 * 64) 1024 1536 (=16 * 96) 1536 2048 (=16 * 128) 2048 14 16 9 16 9 16 9 64 64 64 96 96 128 128 Batch Size Learning Rate 0.5M 6.0 10 0.5M 3.0 104 0.5M 2.5 104 0.5M 2.0 10 We adopt the training hyper-parameters of [6]. We use the AdamW optimizer [14] with β1 = 0.9, β2 = 0.95, weight decay rate coefficient 0.1, and gradient clipping coefficient 1.0. All models are trained with sequence length 2K and batch size 0.5M tokens. The small, medium, large and XL models use peak learning rates of 6.0 104, 3.0 104, 2.5 104 and 2.0 104, respectively. All models are trained with 2000 warmup steps, and then, the cosine scheduler decays the learning rate to 10% of the peak learning rate. All models are trained on the FineWeb-Edu dataset [15] for 50 billion tokens. The efficient parallel training algorithm of CASTLE (forward pass in Algorithm 1 and backward pass in Algorithm 3) is implemented in Triton [23]. E.2 Ablation Studies We conduct ablation studies to better understand the contributions of different design components in CASTLE. These studies address three key questions: Are causal keys necessary, or could lookahead keys alone suffice? Do the observed improvements arise from the mechanism of lookahead keys, or simply from increasing the number of keys? What is the role of the SiLU function in (5)? 27 We systematically investigate each question in the following sections. E.2.1 Ablations on Causal Keys As described in Section 2.2, CASTLE adopts hybrid design that partitions keys into two groups: causal keys and lookahead keys. If all lookahead keys are replaced with causal keys, CASTLE reduces to standard causal attention. Thus, the performance gains demonstrated in Section 3 can be attributed to the introduction of lookahead keys. natural follow-up question is whether causal keys are necessary, or if lookahead keys alone suffice. To investigate this, we construct variant of CASTLE in which all causal keys are removed. To ensure fair comparison, we adjust the configurations so that the total parameter count remains the same, as shown in Table 5. Table 5 Configurations of CASTLE and its variant without causal keys used in ablation study on causal keys. CASTLE TYPE nparams nlayers dmodel nheads CASTLE CASTLE w/o causal keys 120M 120M 12 12 768 768 6 7 64 64 The above 2 models are both trained on FineWeb-Edu for 25B tokens for efficiency, using the same learning hyper-parameters with the small models in Section E.1.2. Their training and validation loss and perplexity are presented in Table 6. Table 6 Training and validation loss and perplexity of CASTLE and its variant without causal keys. Each model is trained on FineWeb-Edu for 25B tokens. Train Eval Loss PPL Loss PPL CASTLE CASTLE w/o causal keys 2.913 3.006 18.417 20. 2.920 3.021 18.541 20.505 As shown in Table 6, removing causal keys results in clear degradation in performance. This demonstrates that causal keys are indispensable in CASTLE. While these results establish the necessity of both causal and lookahead keys, our current formulation in (5) employs one-to-one pairing of causal key and lookahead key. An alternative design could involve grouping multiple causal keys with single lookahead key, or vice versa. Exploring the optimal ratio between causal keys and lookahead keys is left for future work. E.2.2 Ablations on the Number of Keys As discussed in Section B, when CASTLE uses half as many heads as standard causal attention, its parameter count becomes 7 of the baseline. To maintain comparable parameter counts, we adjust the number of heads 8 accordingly. However, unlike the baseline where each head corresponds to one key, each head in CASTLE introduces two keysone causal key and one lookahead key. This design results in CASTLE models having 16, 18, 18, and 18 keys for the small, medium, large, and XL scales, respectively, compared to the corresponding baselines with 14, 16, 16, and 16 keys  (Table 4)  . Thus, CASTLE naturally uses slightly more keys than its baseline counterparts. natural question arises: are the observed improvements due to the introduction of lookahead keys, or simply from having more keys overall? To disentangle this effect, we construct CASTLE variants with only half as many heads as their baselines, ensuring that the total number of keys (nCausalKeys + nLookaheadKeys) matches the baselines. This adjustment results in CASTLE having notably smaller parameter count than the baselines. 28 For efficiency, we train the medium and XL variants on FineWeb-Edu for 25B tokens, using the same hyperparameters as in Section E.1.2. Results are reported in Table 8. Despite having clearly fewer parameters, both CASTLE-M-16 and CASTLE-XL-16 outperform their baselines: CASTLE-M-16 lags behind CASTLE-M by only 0.005 in validation loss, yet surpasses the baseline by 0.026; CASTLE-XL-16 trails CASTLE-XL by 0.008 in validation loss, while exceeding the baseline by 0.032. These findings confirm that CASTLEs advantage stems from its mechanism of incorporating lookahead keys, rather than from increasing the number of keys from 16 to 18. This further consolidates the advantage of CASTLE. Table 7 Configurations of baseline models, CASTLE, and its variants used in the ablation study on the number of keys. Baseline-M, Baseline-XL, CASTLE-M, and CASTLE-XL follow the same configurations as in Table 4. CASTLE-M-16 and CASTLE-XL-16 are constructed by reducing the number of heads in CASTLE-M and CASTLE-XL, respectively, so that the total number of keys (nLookaheadKeys + nCausalKeys) matches the number of keys of the corresponding baseline models. Model Name nparams nlayers dmodel nheads nLookaheadKeys + nCausalKeys Baseline-M CASTLE-M CASTLE-M-16 Baseline-XL CASTLE-XL CASTLE-XL-16 353M 351M 340M 1.310B 1.304B 1.260B 24 24 24 24 24 1024 (=16 * 64) 1024 1024 2048 (=16 * 128) 2048 2048 16 9 8 16 9 8 16 18 16 16 18 64 64 64 128 128 128 Table 8 Training and validation loss and perplexity of baseline models, CASTLE and CASTLE variants with the same number of keys as the baselines, after training for 25B tokens on FineWeb-Edu. The lowest loss and perplexity are shown in bold, and the second-lowest values are underlined. Train Eval nparams Loss PPL Loss PPL Baseline-M CASTLE-M CASTLE-M353M 2.740 351M 2.709 2.714 340M Baseline-XL CASTLE-XL CASTLE-XL-16 2.548 1.310B 1.304B 2.507 2.514 1.260B 15.483 15.018 15.093 12.779 12.267 12.349 2.742 2.711 2. 2.543 2.503 2.511 15.523 15.039 15.126 12.723 12.219 12.316 E.2.3 Ablations on SiLU function in (5) We examine the role of the SiLU function introduced in (5). For efficiency, ablations are conducted on three model sizes: small (0.16B), medium (0.35B), and large (0.75B). Validation perplexity and downstream evaluation results are reported in Table 9 (0-shot) and Table 10 (5-shot). Across all scales, the inclusion or exclusion of SiLU yields negligible differences in perplexity. However, models equipped with SiLU consistently achieve higher average accuracy on downstream benchmarks, relative to their counterparts without SiLU. These findings indicate that incorporating SiLU in (5) improves CASTLEs ability to generalize across tasks. Table 9 Ablation study on the SiLU function in (5). Validation perplexity and evaluation results (0-shot) for downstream tasks of different model scales are reported. CASTLE TYPE PPL ARC-C ARC-E BoolQ Hella. MMLU OBQA PIQA Wino. Avg. CASTLE-S CASTLE-S w/o SiLU 16.315 16.333 CASTLE-M 13.665 CASTLE-M w/o SiLU 13.676 CASTLE-L CASTLE-L w/o SiLU 11.840 11.821 26.19 25.34 30.20 29. 32.34 32.76 56.69 56.52 61.36 61.95 65.15 65.32 59.85 58.32 58.01 57. 57.65 55.35 36.28 36.91 43.24 43.75 47.87 47.72 23.00 23.05 25.34 23. 24.51 23.02 31.60 31.20 34.60 36.20 35.60 35.40 64.25 64.91 67.95 68. 70.78 69.86 52.25 52.25 52.64 52.17 53.51 53.20 43.76 43.56 46.67 46. 48.43 47.83 Table 10 Ablation study on the SiLU function in (5). Evaluation results (5-shot) for downstream tasks of different model scales are reported. CASTLE TYPE ARC-C ARC-E BoolQ Hella. MMLU OBQA PIQA Wino. Avg. CASTLE-S CASTLE-S w/o SiLU CASTLE-M CASTLE-M w/o SiLU CASTLE-L CASTLE-L w/o SiLU 26.02 26.54 32.17 31.40 37.37 34.47 54.25 55. 64.06 64.14 67.89 64.90 57.13 49.30 54.62 51.74 50.95 53.46 35.24 36. 43.47 44.05 47.71 48.42 25.22 24.55 25.22 26.09 26.11 25.18 29.80 29. 33.80 33.40 34.20 34.80 64.53 65.34 69.48 68.17 70.18 69.15 50.99 51. 52.49 52.09 54.06 56.91 42.90 42.34 46.91 46.39 48.56 48.41 E.3 Additional Loss Curves Figure 7 Training and validation loss curves of small models. Training loss curve is smoothened with moving window of 2000 training steps. Validation loss is evaluated every 100 training steps on 40M tokens, and its curve is smoothened by moving window of 20 evaluation intervals. As seen in Table 1 and in comparison with Figure 8, Figure 9 and Figure 5, CASTLE yields only marginal improvements over the baseline on small models. likely explanation is that this is because the benefit of lookahead keys may lie in helping models capture global dependencies, but small models are capacity-limited and can primarily extract local features, making global relations less useful at this scale. 30 Figure 8 Training and validation loss curves of medium models. Training loss curve is smoothened with moving window of 2000 training steps. Validation loss is evaluated every 100 training steps on 40M tokens, and its curve is smoothened by moving window of 20 evaluation intervals. After 50B training tokens, CASTLE-M achieves 0.0294 lower training loss and 0.0245 lower validation loss compared to Baseline-M. Figure 9 Training and validation loss curves of large models. Training loss curve is smoothened with moving window of 2000 training steps. Validation loss is evaluated every 100 training steps on 40M tokens, and its curve is smoothened by moving window of 20 evaluation intervals. After 50B training tokens, CASTLE-L achieves 0.0371 lower training loss and 0.0356 lower validation loss compared to Baseline-L."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Princeton University"
    ]
}