{
    "paper_title": "Trajectory Attention for Fine-grained Video Motion Control",
    "authors": [
        "Zeqi Xiao",
        "Wenqi Ouyang",
        "Yifan Zhou",
        "Shuai Yang",
        "Lei Yang",
        "Jianlou Si",
        "Xingang Pan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in video generation have been greatly driven by video diffusion models, with camera motion control emerging as a crucial challenge in creating view-customized visual content. This paper introduces trajectory attention, a novel approach that performs attention along available pixel trajectories for fine-grained camera motion control. Unlike existing methods that often yield imprecise outputs or neglect temporal correlations, our approach possesses a stronger inductive bias that seamlessly injects trajectory information into the video generation process. Importantly, our approach models trajectory attention as an auxiliary branch alongside traditional temporal attention. This design enables the original temporal attention and the trajectory attention to work in synergy, ensuring both precise motion control and new content generation capability, which is critical when the trajectory is only partially available. Experiments on camera motion control for images and videos demonstrate significant improvements in precision and long-range consistency while maintaining high-quality generation. Furthermore, we show that our approach can be extended to other video motion control tasks, such as first-frame-guided video editing, where it excels in maintaining content consistency over large spatial and temporal ranges."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 2 ] . [ 1 4 2 3 9 1 . 1 1 4 2 : r TRAJECTORY ATTENTION FOR FINE-GRAINED VIDEO MOTION CONTROL Zeqi Xiao1, Wenqi Ouyang1, Yifan Zhou1, Shuai Yang2, Lei Yang3, Jianlou Si3, Xingang Pan1 1S-Lab, Nanyang Technological University, 2Wangxuan Institute of Computer Technology, Peking University 3Sensetime Research {zeqi001, yifan006, wenqi.ouyang, xingang.pan}@ntu.edu.sg williamyang@pku.edu.cn {jianlousi,leiyang}@sensetime.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advancements in video generation have been greatly driven by video diffusion models, with camera motion control emerging as crucial challenge in creating view-customized visual content. This paper introduces trajectory attention, novel approach that performs attention along available pixel trajectories for fine-grained camera motion control. Unlike existing methods that often yield imprecise outputs or neglect temporal correlations, our approach possesses stronger inductive bias that seamlessly injects trajectory information into the video generation process. Importantly, our approach models trajectory attention as an auxiliary branch alongside traditional temporal attention. This design enables the original temporal attention and the trajectory attention to work in synergy, ensuring both precise motion control and new content generation capability, which is critical when the trajectory is only partially available. Experiments on camera motion control for images and videos demonstrate significant improvements in precision and long-range consistency while maintaining high-quality generation. Furthermore, we show that our approach can be extended to other video motion control tasks, such as first-frame-guided video editing, where it excels in maintaining content consistency over large spatial and temporal ranges."
        },
        {
            "title": "INTRODUCTION",
            "content": "Video generation has experienced remarkable advancements in recent years, driven by sophisticated deep learning models such as video diffusion models and temporal attention mechanisms (OpenAI, 2024; Chen et al., 2024; Wang et al., 2023a; Guo et al., 2023b). These innovations have enabled the synthesis of increasingly realistic videos, fueling fields in areas such as filmmaking (Zhao et al., 2023; Zhuang et al., 2024) and world modeling (OpenAI, 2024; Valevski et al., 2024). Video motion control, which aims to produce customized motion in video generation, has emerged as crucial aspect (Yang et al., 2023b; Ling et al., 2024; Ouyang et al., 2024; Ku et al., 2024; Zhao et al., 2023). Among various control signals, camera motion control has garnered increasing attention due to its wide applications in creating view-customized visual content. However, effectively conditioning generation results on given camera trajectories remains non-trivial. Researchers have explored several approaches to address this challenge. One method involves encoding camera parameters into embeddings and injecting them into the model via cross-attention or addition (Wang et al., 2024c; He et al., 2024; Bahmani et al., 2024). While straightforward, this approach often yields imprecise and ambiguous outputs due to the high-level constraints and implicit control mechanisms it employs. Another strategy involves rendering partial frames based on camera trajectories and using these either as direct input (Hu et al., 2024; Yu et al., 2024) or as optimization targets (You et al., 2024) for frame-wise conditioning. Although this method provides more explicit control, it often neglects temporal correlations across frames, leading to inconsistencies in the generated sequence. Project page at this URL. 1 Figure 1: Trajectory attention injects partial motion information by making content along trajectories consistent. It facilitates various tasks such as camera motion control on images and videos, and first-frame-guided video editing. Yellow boxes indicate reference contents. Green boxes indicate input frames. Blue boxes indicate output frames. In response to these limitations, recent methods have begun to address temporal relationships by leveraging 3D inductive biases (Xu et al., 2024; Li et al., 2024). These approaches focus on narrowed domains, utilizing specific settings such as row-wise attention (Li et al., 2024) or epipolar constraint attention (Xu et al., 2024). As we consider the trajectory of camera moving around scenes, it becomes apparent that certain parts of the moving trajectories of pixels, represented as sequence of 2D coordinates across frames, are predictable due to 3D consistency constraints. This observation raises an intriguing question: can we exploit such trajectories as strong inductive bias to achieve more fine-grained motion control? Revisiting the temporal attention mechanism, which is central to video models for synthesizing dynamic motions with consistent content, we can view the dynamics as pixel trajectories across frames. The temporal attention mechanism, with its generic attention design, functions by implicitly synthesizing and attending to these trajectories. Building on this observation, when parts of the trajectories are available, the attention along these trajectories can be modeled explicitly as strong inductive bias to produce controlled motion with consistent content. To this end, we propose trajectory attention that performs attention along the available trajectories across frames for fine-grained camera motion control. Instead of directly adapting the temporal attention to operate on trajectories, which yields suboptimal results in practice, we model trajectory attention as an auxiliary branch alongside the original temporal attention. This design is critical due to the distinct goals of these two attention mechanisms. Temporal attention, which must balance motion synthesis and content consistency, typically focuses on short-range dynamics and attends to adjacent frames within local window. In contrast, trajectory attention is designed to ensure longrange consistency across features along trajectory (see Fig. 2). The trajectory attention branch can inherit the parameters of the original temporal attention for efficient tuning, and its output is added to the output of temporal attention as residuals. This whole design offers several merits: 1) it allows better division of tasks: trajectory attention manages motion control and ensures longrange consistency along specified paths, while temporal attention synthesizes motion for the rest regions; 2) it can integrate seamlessly without modifying the original parameters; 3) it supports sparse trajectories, as the condition is injected moderately, meaning available trajectories do not have to cover all pixels. Our experiments on camera motion control for images and videos demonstrate that our designs significantly enhance precision and long-range consistency. As shown in Fig. 1, our approach leverages stronger inductive bias that optimizes the attention mechanism. This results in improved control precision while maintaining high-quality generation. The proposed trajectory attention can be extended to other video motion control tasks, such as first-frame-guided video editing. Existing techniques often struggle to maintain content consistency over large spatial and temporal ranges (Ku et al., 2024; Ouyang et al., 2024). In contrast, our methods ability to model long-range, consistent correspondences achieves promising results in these challenging scenarios. Moreover, the efficiency of our design allows for training with limited data and computational resources, making it generalizable to diverse contexts and frame ranges."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Video Diffusion Models. The field of video generation has seen significant advancements in recent years, especially in the area of video diffusion models (Ho et al., 2022; Guo et al., 2023b; Chen et al., 2023a; Wang et al., 2023b;a; OpenAI, 2024; Blattmann et al., 2023; Guo et al., 2023a; Chen et al., 2024; Hong et al., 2022). The core of motion modeling of video diffusion models is the temporal attention module. Some approaches (Guo et al., 2023b; Chen et al., 2023a; Wang et al., 2023b;a) decompose attention into spatial and temporal components, where temporal attention aligns features across different frames. Others (Yang et al., 2024c; OpenAI, 2024; Lab & etc., 2024) integrate spatial and temporal attention into unified mechanism, capturing both types of information simultaneously. While these methods rely on data-driven techniques to implicitly learn dynamic video priors within the attention mechanism, how to leverage such priors for explicit and precise motion control remains under-explored. Motion Control in Video Generation. Prior works have explored various control signals for video motion control (Guo et al., 2024; Niu et al., 2024; Yu et al., 2023; Chen et al., 2023b; Yang et al., 2024b; Zuo et al., 2024; Zhu et al., 2024a; Zhao et al., 2023; Chen et al., 2023c; Zhang et al., 2023b), including sketches (Wang et al., 2024b), depth maps (Wang et al., 2024b), drag vectors (Yin et al., 2023; Teng et al., 2023; Deng et al., 2023), human pose (Zhang et al., 2024; Zhu et al., 2024b), object trajectory (Qiu et al., 2024; Wang et al., 2024a; Wu et al., 2024; Gu et al., 2024), and features extracted from reference videos (Yatim et al., 2023; Xiao et al., 2024; Yang et al., 2023b; Ouyang et al., 2024; Ku et al., 2024). One important branch of video motion control is camera motion control, also known as novel view synthesis. In this regard, Wang et al. (2024c); He et al. (2024); Bahmani et al. (2024); Wu et al. (2024) utilize high-level condition signals by encoding camera pose parameters into conditional features. However, these methods often lack precision in capturing detailed temporal dynamics, as they impose weak constraints on the resulting motion. Hou et al. (2024) enables camera control by rendering incomplete warped views followed by re-denoising. Muller et al. (2024); Yu et al. (2024); You et al. (2024) render partial videos as guidance and leverage video generation models to inpaint the remaining frames. Despite these innovations, their approaches suffer from temporal inconsistency due to the lack of consideration for sequential coherence. Methods such as those proposed by Shi et al. (2024); Xu et al. (2024); Cong et al. (2023); Kuang et al. (2024) explicitly modify attention using optical flow or epipolar constraints. These solutions can be viewed as weaker variant of trajectory-consistent constraint. Our approach introduces plug-and-play trajectory attention mechanism for motion information injection. Thanks to the strong inductive bias that makes the best use of the attention mechanism, our method offers precise control over video generation, improving efficiency without the need for specially annotated datasets (like camera pose annotations). It enables enhanced motion control throughout the generation process while maintaining the fidelity of temporal dynamics."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "This section introduces trajectory attention for fine-grained motion control. We first outline video diffusion models with focus on temporal attention (Sec. 3.1), then adapt it for trajectory attention and discuss its limitations (Sec. 3.2). We present trajectory attention as an additional branch, with visualizations of its effectiveness (Sec. 3.3), and describe an efficient training pipeline (Sec. 3.4). 3.1 PRELIMINARY The core of video motion modeling lies in the temporal attention mechanism within video diffusion models, whether applied through decomposed spatial and temporal attention or full 3D attention, to capture robust motion priors. This paper demonstrates the decomposed setting, which is more widely used and has greater open-source availability. However, our design is also adaptable to full 3D attention, as will shown in the experimental results and appendix. typical video diffusion architecture for decomposed spatial-temporal attention includes convolutional layers, spatial attention blocks, and temporal attention blocks. The temporal attention operates as follows. Given an input latent feature RF HW C, where , H, , and represent the 3 Figure 2: Attention map visualization of temporal attention and trajectory attention. (a) Temporal attention tends to concentrate its weight on narrow, adjacent frame window. (b) In contrast, trajectory attention exhibits broader attention window, highlighting its capacity to produce more consistent and controllable results. Here, the attention map is structured with the frame number as the side length. The attention weights are normalized within the range of 0 to 1, where higher values (indicated by light yellow) represent stronger attention. Figure 3: Overview of the proposed motion control pipeline. Our method allows for conditioning on trajectories from various sources such as camera motion derived from single image, as shown in this figure. We inject these conditions into the model through trajectory attention, enabling explicit and fine-grained control over the motion in the generated video. number of frames, height, width, and channels, respectively, temporal attention operates along the frame dimension. The feature is first projected into query (Q), key (K), and value (V): = pq(Z), = pk(Z), = pv(Z), (1) where pq, pk, and pv are learnable projection functions. Temporal attention is then applied along the frame dimension as: = Softmax(QKT )V, (2) yielding the output latent feature Z. For simplicity, we omit the details like rescaling factor and multi-head operations. With large-scale training, temporal attention effectively captures dynamic and consistent video motions, making it natural candidate for motion control in video models. 3.2 TAMING TEMPORAL ATTENTION FOR TRAJECTORY ATTENTION As shown in Fig. 4, vanilla temporal attention operates on the same spatial position across different frames, where the coordinates in the attention form predefined trajectories across frames. Since temporal attention has already learned to model motion along pre-defined trajectories, natural extension is to tame temporal attention for additional trajectory attention. For example, given set of trajectories Tr, where each trajectory is represented by series of coordinates, we incorporate them into the temporal attention mechanism. 4 Algorithm 1: Trajectory-based sampling Input: Hidden states RF HW C, where is the number of frames, H, are the spatial dimensions, and is the number of channels. trajectories Tr RLF 2, where each trajectory specifies 2D locations. Trajectory masks RF L, where Mf,l {0, 1} indicates whether trajectory is valid at frame for trajectory l. 1 foreach trajectory = 1, . . . , do 2 Sample hidden states Zi = {Zf (xf,i, yf,i) = 1, . . . , } RF where (xf,i, yf,i) are the 2D coordinates from Tr[i] for each frame . 3 4 end 5 Stack sampled hidden states: Zs = Stack(Zi = 1, . . . , L) RF LC 6 Mask out invalid hidden states using M: Zt = Zs Output: Masked sampled hidden states Zt RF LC However, this straighwarpward adaptation often yields suboptimal results due to conflict between temporal and trajectory attention. Temporal attention is designed to ensure consistency along the trajectory while preserving the dynamism of feature representations. However, achieving both perfectly is challenging. Consequently, temporal attention often prioritizes natural dynamics at the expense of long-range consistency. This is evident in the attention statistics: as shown in Fig. 2(a), the learned temporal attention predominantly focuses on adjacent frames. In contrast, trajectory attention, given its known dynamics, aims solely to align features along the trajectory. This singular focus on alignment often clashes with the broader objectives of temporal attention. Simply adapting temporal attention to accommodate trajectory information can therefore introduce conflicts. Experimental results further demonstrate that, even with extensive training, the quality of motion control remains suboptimal when trajectory attention is naively integrated. 3.3 MODELING TRAJECTORY ATTENTION AS AN AUXILIARY BRANCH Figure 4: Visualization of vanilla temporal attention and trajectory attention. The above analysis reveals that temporal attention and trajectory attention should not share the same set of weights. Inspired by the recent success of Zhang et al. (2023a), we model temporal attention and trajectory attention into two-branch structure, where trajectory attention is responsible for injecting fine-grained trajectory consistent signal to the origin generation process. As illustrated in Fig. 3, trajectory attention and temporal attention share the same structure, as well as identical input and output shapes. The key difference lies in the process: we first use the given trajectories to sample features from the hidden states (Algorithm 1), then apply multi-head attention with distinct parameters, and finally project the results back to the hidden state format after framewise attention (Algorithm 2). To validate the purpose distinction, we compare the attention maps (softmax scores along the frame axis) of temporal and trajectory attention, based on the SVD model (Blattmann et al., 2023). As shown in Fig. 2(a) and (b), trajectory attention clearly provides broader attention window, enabling more consistent and controllable results. 3.4 TRAINING TRAJECTORY ATTENTION EFFICIENTLY As illustrated in Fig. 5, we initialize the weights of the QKV projectors with those from temporal attention layers to harness the motion modeling capabilities learned from large-scale data. Additionally, the output projector is initialized with zero weights to ensure gradual training process. The training objective follows the standard approach used in fundamental generation models. For instance, in the case of Stable Video Diffusion (Blattmann et al., 2023), the objective is: 5 Algorithm 2: Back projection Input: Hidden states after attention masks RF L. RF LC. trajectories Tr RLF 2. Trajectory 1 Initialize: Zp RF HW C, RF HW , Zp = 0, = 0 where and are the height and width of the spatial grid. 2 foreach = 1, . . . , do 3 t[i] RF to Zp at locations (xf,i, yf,i) from Tr[i]: Zp(f, xf,i, yf,i, :)+=Z Add Update count table at the same locations: U(f, xf,i, yf,i)+=M[f, i] t[i](f, :) 4 5 end 6 Normalize Zp element-wise for valid positions (U > 0): Zp(f, x, y, :) = Zp(f, x, y, :) U(f, x, y) for all (f, x, y) where U(f, x, y) > 0 Output: Back-projected hidden states Zp RF HW Figure 5: Training strategy for trajectory attention. To leverage the motion modeling capability learned from large-scale data, we initialize the weights of the QKV projectors with those from temporal attention layers. Additionally, the output projector is initialized with zero weights to ensure smooth and gradual training process. E[Dθ(x0 + n; σ, c) x02 2], (3) where Dθ represents the neural network, x0 denotes the latent features of the target videos, is the noise, is the condition signal, and σ is the variance parameter."
        },
        {
            "title": "4 FINE-GRAINED CONTROL OF VIDEO GENERATION",
            "content": "This section delves into the process of extracting trajectories for different task settings. While our primary focus is on camera motion control for both static images and dynamic video content, we also showcase the process of trajectory extraction for video editing. 4.1 CAMERA MOTION CONTROL ON IMAGES Algorithm 3 outlines the process of extracting trajectories, denoted as Tr, along with the corresponding validity mask from single image. Unlike prior approaches (Wang et al., 2024c; He et al., 2024), which rely on high-level control signals for video manipulation, our method explicitly models camera motion as trajectories across frames. This enables precise and accurate control of camera movement. 4.2 CAMERA MOTION CONTROL ON VIDEOS The process for camera motion control on videos is more complex than the process for images since the video itself has its own motion. We need to extract the original motion with point trajectory estimation methods like Karaev et al. (2023), then combine the original motion with camera motion to get the final trajectories. We show the details in Algorithm 4. Algorithm 3: Trajectory extraction from single image Input: Image RHpWp3, set of camera pose with intrinsic and extrinsic parameters,{K R33} and {E[R; t]}, where R33 representations the rotation part of the extrinsic parameters, and R31 is the translation part. The length of the camera pose equals frame number . Hp and Wp are the height and width of the pixel space 1 Estimate the depth map RHpWp from given camera pose parameters. 2 Get the translation of pixels RF HpWp2 based on using using D, K, and E. 3 Get trajecories Tr = + C, where RHpWp2 is pixel-level grid coordinates of image with shape Hp Wp. 4 Get valid trajectory mask for pixels that within the image space. Output: Trajectories Tr, Trajectory Masks Algorithm 4: Trajectory extraction from video Input: Video Frames RF HpWp3, set of camera pose with intrinsic and extrinsic parameters,{K R33} and {E[R; t]}. The lenght of camera pose equals to frame number 1 Estimate the depth map RF HpWp from given camera pose parameters. 2 Estimate point trajecotries RF L2 and the corresponding occlusion masks Mo. 3 Get the translation of pixels RF HpWp2 using D, and E. 4 Sample the translation of point trajectories Pt RF L2 from using P. 5 Get trajecories Tr = Pt + P. 6 Get valid trajectory mask = Mi Mo, where Mi is for pixels that within the image space. Output: Trajectories Tr, Trajectory Masks 4.3 VIDEO EDITING Video editing based on an edited first frame has gained popularity recently (Ouyang et al., 2024; Ku et al., 2024). The goal is to generate videos where the content of the first frame aligns with the edited version while inheriting motion from reference videos. Our method is well-suited for this task, as we leverage Image-to-Video generation models that use the edited first frame as conditioning input while incorporating trajectories extracted from the original videos to guide the motion."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTAL SETTINGS Datasets. We use MiraData (Ju et al., 2024) for training, large-scale video dataset with longduration videos and structured captions, featuring realistic and dynamic scenes from games or daily life. We sample short video clips and apply Yang et al. (2023a) to extract optical flow as trajectory guidance. In total, we train with 10k video clips. Implementation Details. We conducted our main experiments using SVD (Blattmann et al., 2023), employing the Adam optimizer with learning rate of 1e-5 per batch size, with mixed precision training of fp16. We only fine-tune the additional trajectory attention modules which inherit weights from the temporal modules. Our efficient training design allows for approximately 24 GPU hours of training (with batch size of 1 on single A100 GPU over the course of one day). We train trajectory attention on the 12-frame video generation modules and apply the learned trajectory attention to both 12-frame and 25-frame video generation models. Despite being trained on 12-frame videos, the trajectory attention performs effectively when integrated into the 25-frame model, demonstrating the strong generalization capability of our design. Metrics. We assessed the conditional generation performance using four distinct metrics: (1) Absolute Trajectory Error (ATE) (Goel et al., 1999), which quantifies the deviation between the estimated and actual trajectories of camera or robot; and (2) Relative Pose Error (RPE) (Goel et al., 1999), 7 Table 1: Qualitative comparison on image camera motion control. *: MotionI2V uses AnimateDiff (Guo et al., 2023b) while we use SVD (Blattmann et al., 2023) as the base models. Other methods use SVD as default. Setting Methods ATE (m, ) RPE trans (m, ) RPE Rot (deg, ) FID () 14 frames 16 frames 25 frames MotionCtrl Ours MotionI2V* Ours CameraCtrl NVS Solver Ours 1.2151 0.0212 0.0712 0.0413 0.0411 0.1216 0.0396 0.5213 0.0221 0.0471 0.0241 0.0268 0.0558 0. 1.8372 0.1151 0.2853 0.1231 0.3480 0.4785 0.1939 101.3 104.2 124.1 108.7 115.8 108.5 103. Figure 6: Qualitative comparisons for camera motion control on images. While other methods often exhibit significant quality degradation or inconsistencies in camera motion, our approach consistently delivers high-quality results with precise, fine-grained control over camera movements. Regions are highlighted in yellow boxes to reveal camera motion. For more comprehensive understanding, we highly recommend viewing the accompanying videos in the supplementary materials. which captures the drift in the estimated pose by separately calculating the translation (RPE-T) and rotation (RPE-R) errors. (3) Frechet Inception Distance (FID) (Heusel et al., 2017), which evaluates the quality and variability of the generated views. 5.2 CAMERA MOTION CONTROL ON SINGLE IMAGES We compare the results of camera motion control on single images with the methods proposed by Wang et al. (2024c); Shi et al. (2024); He et al. (2024). The evaluation is based on 230 combinations of diverse scenes and camera trajectories. To ensure fair comparison, our model is tested under varying settings due to the frame limitations of certain models (i.e., (Wang et al., 2024c) only releases 12-frame version). Table 1 summarizes the results, showing that our methods consistently achieve higher or comparable control precision in terms of ATE and RPE, along with strong fidelity as measured by FID, compared to other methods (Wang et al., 2024c; Shi et al., 2024; He et al., 2024; You et al., 2024). Although MotionCtrl (Wang et al., 2024c) generates slightly better results in terms of FID, it compromises significantly on control precision. Motion-I2V Shi et al. (2024), which uses flow-based attention, only allows frames to attend to the first frame, leading to quality issues in some cases. In contrast, our approach maintains better control precision while preserving generation quality. It also performs better over longer time ranges than other recent methods (He et al., 2024; You et al., 2024). We further provide qualitative results in Fig. 6, which is aligned with the conclusions in Table 1. 8 Table 2: Qualitative comparison on video camera motion control. Methods ATE (m, ) RPE trans (m, ) RPE Rot (deg, ) FID () NVS Solver Ours Ours (w. NVS Solver) 0.5112 0.3572 0.3371 0.3442 0.1981 0.1972 1.3241 0.7889 0.6241 134.5 129.3 112. Figure 7: Qualitative comparisons for camera motion control on videos. In the second row, we provide video frames after view warping as reference. Methods like NVS Solver (You et al., 2024) use frame-wise information injection but overlook temporal continuity, leading to inconsistent motion control, especially in frames farther from the first one. In contrast, our approach explicitly models attention across frames, which significantly benefits control precision. We highlight the control precision with yellow boxes, where our method aligns better with the reference. *: we integrate NVS Solvers capability to inject frame-wise information, achieving better video alignment with the original videos. 5.3 CAMERA MOTION CONTROL ON VIDEOS We compare the video synthesis performance of our method with You et al. (2024), who employ test-time optimization approach. Their method uses view-warped frames as optimization targets, injecting partial frame information into the generation process. However, it optimizes on per-frame basis, neglecting temporal coherence. As result, when large view changes occur, their method often struggles to follow the motion accurately and introduces spatial blur. In contrast, our method precisely handles large motions. Notably, the way You et al. (2024) injects frame information is orthogonal to our approach. By combining their optimization technique with our trajectory attention, we achieve higher fidelity in the generated results, as demonstrated in Table 2 and Fig. 7. 5.4 VIDEO EDITING Compared to previous first-frame guided editing methods (Ku et al., 2024; Ouyang et al., 2024), our approach explicitly models motion dynamics as trajectories across frames, enabling better content consistency over large spatial and temporal ranges. As shown in Fig. 8, while other methods struggle to maintain consistency after editing, our method successfully preserves the edited features throughout the entire sequence. 5.5 ABLATION ON TRAJECTORY ATTENTION DESIGNS To validate the effectiveness of our trajectory attention design, we conducted an ablation study, presented in Table 3 . We examined four types of implementations: 1) Directly applying temporal attention to trajectory attention, 2) Integrating trajectory attention into temporal attention with weight fine-tuning, 3) Utilizing an add-on branch for modeling trajectory attention, and 4) Inheriting weights from temporal attention (as illustrated in Fig. 5) The results in Table 3 indicate that the vanilla adaptation leads to significantly poor motion tracking and video quality, with some outputs exhibiting complete noise (we omit such invalid results during 9 Figure 8: Results on first-frame guided video editing. We compare our method with those from Ouyang et al. (2024); Ku et al. (2024). The results show that other methods struggle to maintain consistency after editing. In contrast, our method successfully preserves the edited features across frames, thanks to its ability to model trajectory consistency throughout the video. Table 3: Ablation on trajectory attention design. Methods ATE (m, ) RPE trans (m, ) RPE Rot (deg, ) FID () Vanilla + Tuning + Add-on Branch + Weight Inheriting 1.7812 0.3147 0.0724 0. 2.4258 0.3169 0.1274 0.0232 13.2141 1.5364 0.3824 0.1939 329.6 139.2 112.4 103.5 evaluation, otherwise calculating the statistic results is not feasible.). After fine-tuning the temporal weights, the implementation functions better but remains suboptimal. In contrast, using an add-on branch for trajectory attention markedly improves both motion control precision and video quality. Additionally, inheriting weights from temporal attention facilitates faster convergence and better overall performance compared to simply initializing attention weights randomly. Figure 9: Qualitative results on Open-Sora-Plan.(Lab & etc., 2024) By incorporating trajectory attention into the 3D attention module, we successfully enable camera motion control. 5.6 RESULTS ON FULL ATTENTION MODELS. Our method also has the potential to support full 3D attention using similar pipeline as shown in Fig. 3 and Fig. 5, with the key difference being the application of trajectory attention to the 3D attention module instead of the temporal attention. As demonstrated in Fig. 9, this enables diverse camera motion control in the generated results. For detailed implementation, please refer to the supplementary materials."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In conclusion, we introduced trajectory attention, novel approach for fine-grained camera motion control in video generation. Our method, which models trajectory attention as an auxiliary branch alongside temporal attention, demonstrates significant improvements in precision and longrange consistency. Experiments show its effectiveness in camera motion control for both images 10 and videos while maintaining high-quality generation. The approachs extensibility to other video motion control tasks, such as first-frame-guided video editing, highlights its potential impact on the broader field of video generation and editing."
        },
        {
            "title": "REFERENCES",
            "content": "Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al. Vd3d: Taming large video diffusion transformers for 3d camera control. arXiv preprint arXiv:2407.12781, 2024. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for highquality video generation. arXiv preprint arXiv:2310.19512, 2023a. Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. arXiv preprint arXiv:2401.09047, 2024. Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-Yi Lin, and Ming-Hsuan Yang. Motionconditioned diffusion model for controllable video synthesis. arXiv preprint arXiv:2304.14404, 2023b. Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models, 2023c. Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. Flatten: optical flow-guided attention for consistent text-to-video editing. arXiv preprint arXiv:2310.05922, 2023. Yufan Deng, Ruida Wang, Yuhao Zhang, Yu-Wing Tai, and Chi-Keung Tang. Dragvideo: Interactive drag-style video editing. arXiv preprint arXiv:2312.02216, 2023. Daniel Geng and Andrew Owens. Motion guidance: Diffusion-based image editing with differentiable motion estimators. arXiv preprint arXiv:2401.18085, 2024. Puneet Goel, Stergios Roumeliotis, and Gaurav Sukhatme. Robust localization using relative and absolute position estimates. In Proceedings 1999 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human and Environment Friendly Robots with High Intelligence and Emotional Quotients (Cat. No. 99CH36289), volume 2, pp. 11341140. IEEE, 1999. Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, Mike Zheng Shou, and Kevin Tang. Videoswap: Customized video subject swapping with interactive semantic point correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 76217630, 2024. Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: arXiv preprint arXiv:2311.16933, Adding sparse controls to text-to-video diffusion models. 2023a. Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023b. Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan arXiv preprint Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv:2404.02101, 2024. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 12 Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:8633 8646, 2022. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. Chen Hou, Guoqiang Wei, Yan Zeng, and Zhibo Chen. Training-free camera control for video generation. arXiv preprint arXiv:2406.10126, 2024. Teng Hu, Jiangning Zhang, Ran Yi, Yating Wang, Hongrui Huang, Jieyu Weng, Yabiao Wang, and Lizhuang Ma. Motionmaster: Training-free camera motion transfer for video generation, 2024. Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. arXiv preprint arXiv:2407.06358, 2024. Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. arXiv preprint arXiv:2307.07635, 2023. Max Ku, Cong Wei, Weiming Ren, Huan Yang, and Wenhu Chen. Anyv2v: plug-and-play framework for any video-to-video editing tasks. arXiv preprint arXiv:2403.14468, 2024. Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein. Collaborative video diffusion: Consistent multi-video generation with camera control. arXiv preprint arXiv:2405.17414, 2024. PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, April 2024. URL https://doi.org/10. 5281/zenodo.10948109. Peng Li, Yuan Liu, Xiaoxiao Long, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, Shanghang Zhang, Wenhan Luo, Ping Tan, et al. Era3d: High-resolution multiview diffusion using efficient row-wise attention. arXiv preprint arXiv:2405.11616, 2024. Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, and Yi Jin. Motionclone: Training-free motion cloning for controllable video generation. arXiv preprint arXiv:2406.05338, 2024. Norman Muller, Katja Schwarz, Barbara Rossle, Lorenzo Porzi, Samuel Rota Bul`o, Matthias Nießner, and Peter Kontschieder. Multidiff: Consistent novel view synthesis from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1025810268, 2024. Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, and Yinqiang Zheng. Mofavideo: Controllable image animation via generative motion field adaptions in frozen image-tovideo diffusion model. arXiv preprint arXiv:2405.20222, 2024. OpenAI. Video generation models as world simulators. https://openai.com/research/ video-generation-models-as-world-simulators, 2024. Wenqi Ouyang, Yi Dong, Lei Yang, Jianlou Si, and Xingang Pan. I2vedit: First-frame-guided video editing via image-to-video diffusion models. arXiv preprint arXiv:2405.16537, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Haonan Qiu, Zhaoxi Chen, Zhouxia Wang, Yingqing He, Menghan Xia, and Ziwei Liu. Freetraj: Tuning-free trajectory control in video diffusion models. arXiv preprint arXiv:2406.16863, 2024. Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In ACM SIGGRAPH 2024 Conference Papers, pp. 111, 2024. Yao Teng, Enze Xie, Yue Wu, Haoyu Han, Zhenguo Li, and Xihui Liu. Drag-a-video: Non-rigid video editing with point-based interaction. arXiv preprint arXiv:2312.02936, 2023. Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines, 2024. URL https://arxiv.org/abs/2408.14837. Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint arXiv:2402.01566, 2024a. Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023a. Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems, 36, 2024b. Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023b. Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pp. 111, 2024c. Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-to-video generation. NeurIPS, 2024. Zeqi Xiao, Yifan Zhou, Shuai Yang, and Xingang Pan. Video diffusion models are training-free motion interpreter and controller. arXiv preprint arXiv:2405.14864, 2024. Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Camera-controllable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. Lihe Yang, Lei Qi, Litong Feng, Wayne Zhang, and Yinghuan Shi. Revisiting weak-to-strong consistency in semi-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 72367246, 2023a. Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv preprint arXiv:2406.09414, 2024a. Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with user-directed camera movement and object motion. arXiv preprint arXiv:2402.03162, 2024b. Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender video: Zero-shot text-guided video-to-video translation. In SIGGRAPH Asia 2023 Conference Papers, pp. 111, 2023b. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024c. Danah Yatim, Rafail Fridman, Omer Bar Tal, Yoni Kasten, and Tali Dekel. Space-time diffusion features for zero-shot text-driven motion transfer. arXiv preprint arXiv:2311.17009, 2023. Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. Meng You, Zhiyu Zhu, Hui Liu, and Junhui Hou. Nvs-solver: Video diffusion model as zero-shot novel view synthesizer. arXiv preprint arXiv:2405.15364, 2024. 14 Jiwen Yu, Xiaodong Cun, Chenyang Qi, Yong Zhang, Xintao Wang, Ying Shan, and Jian Zhang. Animatezero: Video diffusion models are zero-shot image animators. arXiv preprint arXiv:2312.03793, 2023. Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, TienTsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 38363847, 2023a. Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023b. Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. arXiv preprint arXiv:2406.19680, 2024. Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-to-video diffusion models. arXiv preprint arXiv:2310.08465, 2023. Hanxin Zhu, Tianyu He, Anni Tang, Junliang Guo, Zhibo Chen, and Jiang Bian. Compositional 3daware video generation with llm director. Advances in Neural Information Processing Systems, 2024a. Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision (ECCV), 2024b. Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, and Yali Wang. Vlogger: Make your dream vlog. arXiv preprint arXiv:2401.09414, 2024. Yi Zuo, Lingling Li, Licheng Jiao, Fang Liu, Xu Liu, Wenping Ma, Shuyuan Yang, and Yuwei Guo. Edit-your-motion: Space-time diffusion decoupling learning for video motion editing, 2024. URL https://arxiv.org/abs/2405.04496."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 LIMITATIONS As shown in Fig. 20, our method has several limitations that require further exploration. Currently, it depends on external techniques such as Karaev et al. (2023); Yang et al. (2023a) for trajectory extraction. Investigating how to generate trajectories from more flexible inputs, such as text-based conditions, is promising direction for future research. Moreover, our method depends on the generative capabilities of the underlying foundation models. If these models face difficulties handling rapid motions, the effectiveness of trajectory-based control diminishes. Additionally, our method encounters challenges with 3D cycle consistency. For instance, performing 360-degree rotations would require further design adaptations. Furthermore, the control precision diminishes when the trajectories become too sparse since the auxiliary branch does not provide enough control information. A.2 DETAILS OF FULL-ATTENTION IMPLEMENTATION We implement trajectory attention in Open-Sora-Plan Lab & etc. (2024), DiT (Peebles & Xie, 2023) model with 3D attention. The trajectory attention is constructed and trained following the same procedure outlined in the main paper, with the key difference being that it is appended to the 3D full attention block instead of the temporal attention block  (Fig. 10)  . For other training details, we follow the setting in Lab & etc. (2024). Figure 10: Pipe for video diffusion models with 3D Attention. The key distinction with the pipeline in the main paper lies in applying trajectory attention to the 3D attention module, rather than to the temporal attention mechanism. A.3 DETAILS OF TASK PROCESS Camera Parameters. The intrinsic parameters describe the internal characteristics of the camera. These parameters define how the camera transforms 3D points in its coordinate system to 2D points on the image plane. The matrix R33 typically has the following structure: = (cid:34)fx 0 0 0 fy 0 (cid:35) cx cy (4) fx and fy: Focal lengths in the and directions, often in pixel units. cx and cy: Coordinates of the principal point (optical center) in the image plane. The intrinsic matrix encapsulates how pixel coordinates relate to normalized image coordinates. The extrinsic parameters define the cameras position and orientation in the world coordinate system. This involves: Rotation matrix R33: Represents the cameras orientation by rotating the world coordinate system to align with the camera coordinate system. Translation vector R31: Represents the position of the camera in the world coordinate system. These two components are combined to form the extrinsic matrix, which can be represented as: = [R t] Here, defines how the 3D space is rotated relative to the camera, while indicates the displacement of the cameras origin from the world coordinate origin. For random input image, we predefine the intrinsic parameters and use the given camera trajectory to generate the extrinsic parameters. Effects of intrinsic parameters. Since we cannot precisely estimate the intrinsic and extrinsic parameters from single image, we use predefined intrinsic parameters and some hyperparameters for extrinsic parameters. From our observations, these predefined parameters with statistics can effectively generate reasonable results. We can also adjust them accordingly. Specifically, we set cx and cy in the center of the image plane, and fx and fy to 260 (relative). As shown in Fig. 21, we illustrate cases with different focal lengths. Depth estimation. We use DepthAnythingV2Yang et al. (2024a) to estimate the depth maps from frames. Examples of estimated results are shown in Fig. 11 Figure 11: Depth estimation results. Translation computing. Based on the depth map, camera parameters of two views, we can get the translation of pixels shown in Alg. 5. Algorithm 5: Compute pixel translation Input: RHpWp : Depth map of the first view E1 R44: Extrinsic matrix of the first view E2 R44: Extrinsic matrix of the second view R33: Intrinsic matrix for both views Output: T12 RHpWp2: Transformed pixel coordinates between views // Compute relative transformation // Homogeneous pixel positions // Normalized positions in camera space ; 1 E2 E1 1 2 [0, . . . , Hp 1], [0, . . . , Wp 1] 3 X, meshgrid(x, y) 4 Phomo stack([X, Y, 1HpWp ]) ; 5 K1 Phomo ; 6 Dreshaped reshaped to (Hp, Wp, 1, 1) 7 Pworld Dreshaped 8 Pworld, homo concatenate([Pworld, 1HpWp ]) 9 world, homo Pworld, homo 10 world world, homo[:, :, : 3] 11 T12 12 return world Point trajectory extraction from videos. We use CoTracker Karaev et al. (2023) to extract point trajectories. An example is shown in Fig. 12. 17 Figure 12: Point trajectory estimation results. Table 4: Ablations on training datasets. Dataset Setting Training steps ATE (m, ) RPE trans (m, ) RPE Rot (deg, ) FID () 10k real-world 10k games 10k real-world +10k games 40k 40k 40k 0. 0.0421 0.0372 0.0232 0.0211 0.0233 0. 0.2139 0.1899 103.5 105.3 102.2 A. INPUT PROCESS. In Alg. 4 of the main paper, we present the process of combining the video point trajectories and camera motion trajectories. In Sec 4.3, we present the input process of video editing. To clarify this process, we further provide the visualization in Fig. 13 A.5 ABLATIONS ON TRAINING DATASETS. We initially selected 10k short real-world videos from Miradata Ju et al. (2024). To evaluate the impact of training domain diversity and dataset size, we conducted ablation experiments on various training datasets. As shown in Table 4, we did not observe substantial improvements when varying the training domains or increasing the dataset size. The evaluation setup matches the 25-frame version described in Table 1. Figure 13: Input process visualization. For all tasks, the inputs to the network are the first frame and the extracted trajectories. The usage of the first frame and the trajectories are identical to Fig. 3 in the main paper. The wrapped frames and the reference frames will not be used as inputs to the generation network. A.6 USING SYNTHETIC OPTICAL FLOW AS GUIDANCE Our method directly leverages optical flow to guide the generation process. Unlike previous approaches Geng & Owens (2024) requiring inference-time optimization, our method seamlessly integrates this guidance into the attention mechanism. In addition to generating intermediate frames by interpolating the optical flow, our approach achieves improved consistency, as demonstrated in Fig. 14. Figure 14: Using synthetic optical flow as guidance. Our method supports directly using optical flow to guide generation. Blue boxes indicate the optical flow. Yellow boxes indicate the reference image. A.7 MORE CHALLENGING CASES. As shown in Fig. 15, our method can also cover many challenging situations, like (a) video editing with multiple objects, (b) video editing with occlusions, and (c) diverse and rapid camera motions (i.e. zoom-in, zoom-out, and clockwise rotation.). A.8 EXPERIMENTS ON THE SPARSITY OF TRAJECTORY ATTENTION To assess the generalization capability of trajectory attention under varying levels of sparsity, we conduct experiments in two scenarios: using trajectories with reduced density and applying small region mask to the trajectories. As shown in Fig. 17, our method performs effectively in both settings. However, when the trajectory density is extremely sparse (below 1/32 resolution), the results become unstable. Moreover, we also find trajectory attention can be generalized to sparse hand-crafter trajectories, as shown in Fig. 18. A.9 COMPARISON ON CAMERA TRAJECTORIES For clearer understanding, we visualize the predicted trajectories in Fig. 16, illustrating results from five scenes with single camera trajectory. The figure shows that, thanks to the explicit modeling of camera motion, our methods estimated trajectories closely align with the ground truth. In contrast, methods like He et al. (2024) exhibit inconsistencies in some cases. 19 Figure 15: Examples of challenging situations. Our method effectively addresses complex scenarios, including (a) video editing involving multiple objects, (b) video editing in the presence of occlusions, (c) diverse and rapid camera movements, such as zooming in and out, as well as clockwise rotations, and (d) video editing with distinct object categories. Please note that yellow boxes indicate reference videos, green boxes indicate input frames and blue boxes indicate output results. A.10 TRAINING DATA CONSTRUCTION The training data consists of natural scenes with inherent camera and object movements. We estimate the optical flow and occlusion masks using the method proposed by (Yang et al., 2023a). Examples are shown in Fig. 19. A.11 MORE QUALITATIVE RESULT We strongly recommend viewing the attached webpage for more intuitive visualization. 20 Figure 16: Visualization of camera trajectories. The first row displays the estimated trajectories from our generation alongside the ground truth trajectories. The second row presents the estimated trajectories from CameraCtrl (denoted as CC) compared to the ground truth. The results indicate that our method aligns significantly better with the ground truth camera motion trajectories. Figure 17: Results on sparse trajectories. In (a), we show that trajectory attention remains robust even with relatively sparse trajectories. Even when the trajectory density is reduced to 1/16 of the original video resolution, it still performs well in motion control. In (b), we apply the trajectory mask to selectively use only portion of the trajectories, keeping the regions outside the mask static. The model accurately follows the motion within the small masked area. (c) If we apply sparse trajectories control to specific region (i.e., the dog region), the output results are more dynamic. Best viewed on the attached HTML file. 21 Figure 18: Applications on drag signals. Trajectory attention supports hand-crafted dragging trajectory. Row 1: origin videos. Row 2: dragged results. Figure 19: Visualization on the training data. The training data includes origin frames, predicted optical flow, and occlusion masks. Figure 20: Visualization on failure cases. Our method encounters challenges when dealing with extremely fast motions as well as complex and difficult-to-estimate motion patterns. Figure 21: Visualization on different intrinsic parameters."
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University",
        "Sensetime Research",
        "Wangxuan Institute of Computer Technology, Peking University"
    ]
}