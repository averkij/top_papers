{
    "paper_title": "Benchmarking Information Retrieval Models on Complex Retrieval Tasks",
    "authors": [
        "Julian Killingback",
        "Hamed Zamani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are incredible and versatile tools for text-based tasks that have enabled countless, previously unimaginable, applications. Retrieval models, in contrast, have not yet seen such capable general-purpose models emerge. To achieve this goal, retrieval models must be able to perform complex retrieval tasks, where queries contain multiple parts, constraints, or requirements in natural language. These tasks represent a natural progression from the simple, single-aspect queries that are used in the vast majority of existing, commonly used evaluation sets. Complex queries naturally arise as people expect search systems to handle more specific and often ambitious information requests, as is demonstrated by how people use LLM-based information systems. Despite the growing desire for retrieval models to expand their capabilities in complex retrieval tasks, there exist limited resources to assess the ability of retrieval models on a comprehensive set of diverse complex tasks. The few resources that do exist feature a limited scope and often lack realistic settings making it hard to know the true capabilities of retrieval models on complex real-world retrieval tasks. To address this shortcoming and spur innovation in next-generation retrieval models, we construct a diverse and realistic set of complex retrieval tasks and benchmark a representative set of state-of-the-art retrieval models. Additionally, we explore the impact of LLM-based query expansion and rewriting on retrieval quality. Our results show that even the best models struggle to produce high-quality retrieval results with the highest average nDCG@10 of only 0.346 and R@100 of only 0.587 across all tasks. Although LLM augmentation can help weaker models, the strongest model has decreased performance across all metrics with all rewriting techniques."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 3 5 2 7 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Benchmarking Information Retrieval Models on Complex\nRetrieval Tasks",
            "content": "JULIAN KILLINGBACK and HAMED ZAMANI, University of Massachusetts Amherst, USA Large language models (LLMs) are incredible and versatile tools for text-based tasks that have enabled countless, previously unimaginable, applications. Retrieval models, in contrast, have not yet seen such capable generalpurpose models emerge. To achieve this goal, retrieval models must be able to perform complex retrieval tasks, where queries contain multiple parts, constraints, or requirements in natural language. These tasks represent natural progression from the simple, single-aspect queries that are used in the vast majority of existing, commonly used evaluation sets. Complex queries naturally arise as people expect search systems to handle more specific and often ambitious information requests, as is demonstrated by how people use LLM-based information systems. Despite the growing desire for retrieval models to expand their capabilities in complex retrieval tasks, there exist limited resources to assess the ability of retrieval models on comprehensive set of diverse complex tasks. The few resources that do exist feature limited scope and often lack realistic settings making it hard to know the true capabilities of retrieval models on complex real-world retrieval tasks. To address this shortcoming and spur innovation in next-generation retrieval models, we construct diverse and realistic set of complex retrieval tasks and benchmark representative set of state-of-the-art retrieval models. Additionally, we explore the impact of LLM-based query expansion and rewriting on retrieval quality. Our results show that even the best models struggle to produce high-quality retrieval results with the highest average nDCG@10 of only 0.346 and R@100 of only 0.587 across all tasks. Although LLM augmentation can help weaker models, the strongest model has decreased performance across all metrics with all rewriting techniques. We explore the potential causes for poor performance, as well as what features the best performing models have and why they might help. Our findings suggest that there is still work to be done to improve the quality of retrieval models on complex search tasks. We hope that by sharing our unified complex evaluation set we can set new standard for complex retrieval evaluation and spur innovation in retrieval models. The data and artifacts are available at https://github.com/jfkback/crumb."
        },
        {
            "title": "1 Introduction\nInformation retrieval (IR) systems are fundamental to accessing and organizing the vast repositories\nof digital information available today. Traditional IR approaches have predominantly focused on\nmatching queries with documents through keyword overlap or basic semantic similarity. These\nsystems, while effective for simple search scenarios, frequently fall short when faced with complex\ninformation needs that contain multiple aspects [6, 38, 56]. In large part because these aspects\ncan often require nuanced understanding to properly untangle and weight while determining\nrelevance. This problem has become especially important recently, as the broad adoption of large\nlanguage models (LLMs) is changing how users expect information systems to work [61, 69]. As\nuser expectations for information access and retrieval systems increase, it is crucial to quantify and\nunderstand how well current state-of-the-art retrieval paradigms and models, such as LLM-based\ndense and sparse retrieval models, perform on a wide range of complex retrieval tasks. This paper\nprovides insights which highlight the limitations of the current state-of-the-art in information\nretrieval systems and helps inform the development of the next generation of IR technology.",
            "content": "The first step towards this goal is to prepare suitable comprehensive benchmark that focuses on diverse complex retrieval tasks. We believe that existing resources are not sufficient. For example, standard retrieval benchmarks provided by TREC ad hoc retrieval tracks [3, 64], TREC Web Tracks [12, 13], MSMARCO [43], and TREC Deep Learning Track [14, 15] predominantly feature keywordbased queries or simple natural language questions that generally have singular focus. Although these datasets have driven substantial improvements in retrieval models, the frequent reliance on Authors Contact Information: Julian Killingback, jkillingback@cs.umass.edu; Hamed Zamani, zamani@cs.umass.edu, University of Massachusetts Amherst, Amherst, Massachusetts, USA. 1 2 Julian Killingback and Hamed Zamani Paper Retrieval Multi-aspect Paper Criteria Queries Tip-of-the-Tongue Vague, Multi-detail Queries My goal is to develop learning model that can handle multiple tasks simultaneously. This proposed learning model will function as sub-model selector, meaning that when presented... Cops son needs blood transfusion from an inmate. cops son is very sick and needs certain type of blood transfusion. He finds out an inmate has the same type and once agreed the inmate..."
        },
        {
            "title": "Clinical Trial Retrieval\nPatient History Queries",
            "content": "On the way to school, Karen became fixated on the puzzle game on her phone! The game is played as follows. In each level, you have grid with rows and columns. Each cell originally contains... Patient is 45-year-old man with history of anaplastic astrocytoma of the spine complicated by severe lower extremity weakness and urinary retention s/p Foley catheter, high-dose steroids..."
        },
        {
            "title": "StackExchange QA\nCommunity Questions Requiring Reasoning",
            "content": "Imagine you have digital scale that can measure the weight of an infinite number of infinitely small digital dots. Each dot can either be on or off, and their weights are determined by specific pattern... Does the genetic sequence of SARS-CoV-2 end with 33 As? Looking at the DNA (or RNA?) sequence of the Covid-19 virus here: https://www.ncbi.nlm.nih.gov/nuccore/MN908947.3..."
        },
        {
            "title": "Legal QA\nLegal Queries with Geographic Constraints",
            "content": "SetOps Entity Queries with Set-based Operations Are eviction cases first heard in high court? In the state of Tennessee German spy comedy films, or 2000s comedy-drama mystery films. Fig. 1. An overview of our proposed benchmark: CRUMB. Each card is dedicated to one of the complex tasks in CRUMB. The card header includes the name of the task and short overview of the query type. The body of the card gives truncated example query. existing search systems to source queries results in collections that primarily capture the subset of queries where users expect existing systems to succeed [2, 9, 40, 61, 80]. This bias results in the failure to capture the true breadth and complexity that is inherent to real-world information-seeking behavior [61, 69]. The BEIR [60] benchmark attempted to aggregate diverse datasets to evaluate zero-shot capabilities of retrieval models. However, BEIR is still primarily composed of common retrieval tasks with limited query complexity. Additionally, some of BEIRs datasets have problems such as (1) using answers to questions as target documents despite answers for specific question generally having higher lexical overlap, (2) including tasks that are significantly different from standard retrieval tasks such as citation prediction and fact verification, and (3) standardizing the labels from original datasets in way that results in incorrect or unsupported labels [75]. More recently, Su et al. [56] introduced the BRIGHT benchmark to evaluate retrieval on reasoningintensive tasks. While BRIGHT addresses some limitations by focusing on reasoning-intensive retrieval tasks that often contain multiple requirements or sub-questions, the queries tend to have limited breadth and exclude many types of complex retrieval tasks. For example, complex queries where multiple requirements are combined using logical operations such as and, or, and not do not require reasoning and are thus missing from BRIGHT, but are an important group of complex queries that are relevant to many real-world search scenarios. Additionally, BRIGHT has some practical limitations, one major issue is that for the tasks derived from Stack Exchange each subject only has between 500-2000 documents making full-document evaluation unrealistic when compared to most real-world document collections. Additionally, the naive scraping and chunking approach results in many chunks solely containing webpage boilerplate or chunks without sufficient context. Julian Killingback and Hamed Zamani 3 In this paper, we introduce the Complex Retrieval Unified Multi-task Benchmark (CRUMB) an evaluation suite of eight retrieval tasks that are meticulously curated from existing datasets. An overview of the eight tasks can be seen in Figure 1. Each task has multiple aspects per query expressed in unique ways as well as additional dataset-specific features that make the collection diverse and well rounded with focus on realistic retrieval settings. These include: tip-of-the-tongue queries for movie retrieval, multi-aspect queries for scientific paper retrieval, set-based logical queries for entity retrieval, state-specific legal questions for statute retrieval, multi-constraint math problems as queries for theorem retrieval, varied Stack Exchange questions with related web pages, clinical trial search where patient histories are used as queries, and code retrieval where multi-constraint code problem is the query and the code snippets are the documents. Some unique features of these tasks include different vocabularies between queries and documents, highly technical terms, and numerical comparisons. Note that although we feature many types of retrieval tasks which have complex queries there will always be some that are not accounted for, our aim was to cover several different variations and include other features that might impact retrieval quality to get holistic view of how retrieval models perform on complex retrieval tasks. To promote the best retrieval performance, we use unified markdown format for documents and include headings in the chunked versions to provide context. Our formatting allows for the future study of retrieval models that use document structure and provides important context for retrieval models to perform well. We believe that there is significant value in unifying these datasets so that it is simple to test retrieval systems on broad range of complex tasks. Our selection of both datasets and the subset of data used in our final collection reflect substantial investment in choosing realistic retrieval tasks that have high-quality relevance judgments. Additionally, several of the original datasets lack standardized version that is well suited for modern retrieval models (e.g. the documents come in XML format with unnecessary fields) which our version solves. Using CRUMB, we evaluated diverse and representative set of state-of-the-art neural retrieval models to assess how they perform on complex tasks and derive several insights about where current models struggle most and what characteristics the best models share. We find that even the best models struggle on these complex tasks with the best average nDCG@10 of 0.346 and R@100 of 0.587. We find that models tend to struggle with precision metrics for the top-ranked documents when semantic and keyword overlap between queries and documents is low or when such overlaps are weak signals for relevance. For example, in the tip-of-the-tongue retrieval [6] task where queries often have minimal term overlap has one of the lowest nDCG values. The task SetOps which has queries featuring set-based operations, such as and and not, with entity pages as documents has the lowest maximum nDCG across all baseline models. Due to the set-based operations in the query, often documents with relevant terms or semantics are either irrelevant or only partially relevant. These observations are true for the best performing models, while in general the weaker models struggle on datasets with significant differences from common retrieval training datasets. For instance, Theorem Retrieval and Code Retrieval both have very different document vocabularies than common retrieval models and see poor performance from the less capable models. We find that model performance is most impacted by four factors: (1) the ability of the model to follow instructions, (2) the size of the model, (3) the diversity and difficulty of training data on which the model was trained and (4) what base model was used. Experiments with LLM-based query rewriting techniques show that rewriting tends to harm better performing models while bringing notable improvements to weaker models. This finding suggests that there is limit to the usefulness of query rewriting to improve performance on complex tasks, at least with current state-of-the-art rewriting techniques. In summary, the main contributions of this work include: 4 Julian Killingback and Hamed Zamani Table 1. Overview of dataset characteristics for the chunked-document (passage) version of CRUMB. is the number of evaluation queries, is the number of documents in the corpus. Avg D+ ð‘ ðµ and Avg D+ ðµ are the average number of relevant documents per query for non-binary and binary relevance judgments, respectively. Avg Qð‘â„Žð‘Žð‘Ÿ and Avg Dð‘â„Žð‘Žð‘Ÿ are the average character lengths of queries and documents."
        },
        {
            "title": "Dataset Name",
            "content": "Tip-of-the-tongue StackExchange Paper Retrieval Set-Ops Clinical Trial Legal QA Theorem Retrieval Code Retrieval 135 107 72 423 113 6,753 69 3,665 1,083,337 40,956 363,133 651,704 914,628 1,182,626 23,839 232,444 Avg D+ 6.92 2.20 90.72 29.91 391.57 2.04 1.99 29. ð‘ ðµ Avg D+ ðµ Avg Qð‘â„Žð‘Žð‘Ÿ Avg Dð‘â„Žð‘Žð‘Ÿ - - 9.67 - - - - - 726 876 951 47 740 148 415 837 2,535 2,661 1,258 1,411 1,743 2,704 874 511 Table 2. Overview of dataset characteristics for the full-document version of CRUMB. is the number of evaluation queries, is the number of documents in the corpus. Avg D+ ðµ are the average number of relevant documents per query for non-binary and binary relevance judgments, respectively. Avg Qð‘â„Žð‘Žð‘Ÿ and Avg Dð‘â„Žð‘Žð‘Ÿ are the average character lengths of queries and documents. ð‘ ðµ and Avg D+ Avg D+"
        },
        {
            "title": "Dataset Name",
            "content": "Clinical Trial Legal QA Set-Ops StackExchange Tip-of-the-tongue 113 6,753 423 107 135 375,580 296,274 325,505 5,035 231,852 ðµ Avg Qð‘â„Žð‘Žð‘Ÿ Avg Dð‘â„Žð‘Žð‘Ÿ ð‘ ðµ Avg D+ 4,032 740 - 10,312 148 - 2,812 - 47 25,529 876 - 10,168 726 - 150.63 1.50 8.22 1.07 1. The construction of complex retrieval benchmark composed of eight diverse complex retrieval tasks. Benchmarking wide range of top-performing retrieval models to uncover how they perform on complex tasks. An analysis of what qualities the best performing models have which allows them to do well on complex tasks. An analysis of what data features impact model performance the most."
        },
        {
            "title": "2.1 General Evaluation Collections for Retrieval\nPrevious work has used a collection of retrieval tasks to evaluate how retrieval models perform on\na range of tasks. These collections are generally preferred over a single task because they provide\na more complete picture of how retrieval models generalize. One of the most used evaluation\ncollections is BEIR [60] which was constructed to evaluate the zero-shot performance of retrieval\nmodels on various domains. BEIR is constructed by aggregating existing datasets, including datasets\nthat were existing retrieval datasets and those that originally were constructed for other tasks, but\nwhich were converted to retrieval datasets. Another popular collection with a broader scope is the\nMassive Text Embedding Benchmark (MTEB) [42]. MTEB is not just a retrieval dataset as it also\nconsiders other text embedding uses such as clustering and sentence similarity as well as more",
            "content": "Julian Killingback and Hamed Zamani 5 classic IR tasks such as retrieval and reranking. Similar to BEIR, MTEB does not create new datasets; instead, it unifies and curates existing datasets. MTEB has spurred follow-up work which includes second version which adds new tasks like long-document embedding and code retrieval [21]. As well as multilingual variant [21]. There have also been several language-specific embedding benchmarks, such as for Vietnamese [45] and Polish [46]. Although BEIR has been useful benchmark for generalization it has shortcomings which limit the validity of results when drawing general conclusions. major limitation is that several of the tasks are not strictly retrieval tasks such as citation prediction and fact verification, evaluating retrieval models on these tasks likely provides little insight as they are out of domain for the models. Additionally, several of BEIRs datasets include answer retrieval where the relevant document is an exact answer to question which acts as query. Although this might be valid retrieval task, it is likely far from realistic given that answers written for questions will likely include more similar terms which likely results in an overestimate of retrieval performance. Furthermore, the way BEIR converted some of the dataset labels to standard range resulted in incorrect labels as mentioned by Weller et al. [75]. As MTEB uses many retrieval tasks sourced from BEIR it inherits many of these problems. ATEB [23] is another embedding benchmark, like MTEB, it focuses on evaluating embeddings on range of tasks including ranking, classification, retrieval, and bi-text mining. Unlike MTEB, ATEB focuses on advanced NLP tasks that include reasoning-based retrieval and safety classification. ATEBs focus on advanced NLP tasks differs from our focus on complex retrieval tasks. Additionally, the ATEB reranking and retrieval tasks largely use existing NLP datasets in ways that produce unrealistic tasks. In contrast, our benchmark consists of realistic search tasks. Another recent collection MAIR [57], has large number of retrieval tasks sourced from existing datasets. Unlike MTEB and BEIR, MAIR has specific focus on instruction following with each task including handwritten instructions. MAIR shares several tasks with our proposed dataset, but it has some drawbacks. It includes many tasks from BEIR and thus includes many of the aforementioned problems. Furthermore, many of the tasks are also question and answer tasks that have the same lexical biases mentioned in the limitations of BEIR. As MAIR contains so many tasks each task has minimal attention without any standardization of documents. For instance, in the TREC Clinical Trial 2022 task their documents are the raw XML documents while for the HuggingfaceAPI task the documents are raw JSON files. Their documents are also not chunked, despite the large body of evidence that chunked documents result in better performance [41]. Additionally, their dataset focuses on instructions, so many of the datasets are not complex unlike our dataset. We also ensure that the datasets have human written queries and that our documents are in unified markdown format and include contextualized chunks to evaluate the best-case performance and provide standardized version for future users."
        },
        {
            "title": "2.2 Retrieval and Reranking Models for Hard Retrieval Tasks\nAlthough complex retrieval has not become a common genre of retrieval collections, several\nmodeling approaches have emerged to address complex queries in various forms. Some of these\ntechniques are limited to specific types of complex queries while other approaches are broader\napproaches that would likely be successful over a range of complex query types.",
            "content": "As our focus is on unified models which can generalize to varied complex tasks, we mostly focus on approaches that are not tightly-coupled to specific query type. We start with popular approach to doing retrieval on multi-part or complex queries, using an explicit decomposition step either before retrieval or between iterative retrieval steps. Malon and Bai [39] was one of the earliest works to explicitly generate intermediate questions to address multi-hop queries. While Lin et al. [37] used an LLM to decompose tip-of-the-tongue queries into various sections corresponding 6 Julian Killingback and Hamed Zamani to fields in the target document before retrieval. Vemuru et al. [63] uses part-of-speech parsing to construct \"query tree\" which can then be deconstructed into simpler queries to handle multi-hop queries, though it is unclear if their approach would work on complex queries that do not follow the specific multi-hop construction. Instruction following retrieval models has emerged as way to adapt retrieval models without further fine-tuning. As consequence, instruction-tuned models must understand subtle instructions and multiple relevance criteria in way that most retrieval and reranking models cannot. Early work in instruction-tuned model had task-level instructions to adapt models for new tasks, but did not consider query-level instructions [7, 55]. Weller et al. [72] proposed an instruction-tuned ranker that works on query-level instructions instead of task-level instructions. The recent dense-retrieval model Promptriever [73] uses large-scale instruction dataset and LLM backbone model to enable strong instructions following. Promptriever is the first retrieval model to show prompting behavior similar to generative LLMs. An interest in ranking models that can handle reasoning-heavy tasks has recently gained popularity. These reasoning-heavy tasks often require more than lexical or semantic matching, instead requiring reasoning steps to connect query to relevant document. Though not always explicitly complex, reasoning intensive retrieval tasks often contain several questions and constraints which makes them complex. recent work ReasonIR [53] focuses on creating synthetic reasoning data to enable dense-retrieval model to better handle reasoning-heavy retrieval tasks. The prior approaches largely focus on using training data to modify model behavior to enable better performance on complex tasks, but there have also been number of approaches to modify the traditional retrieval training techniques and model architecture to enable fundamentally more capable retrieval and reranking models. Inspired by generative reasoning models such as OpenAIs o3 model [1] and DeepSeeks R1 model [18], which make use of extended reasoning traces with verifiable rewards, several ranking models have incorporated reasoning. The reranking model Rank1 [75] uses reasoning traces from R1 to distill reranking reasoning to smaller LLMs such as Qwen 2.5 7B. Other approaches have used reinforcement learning such as Search R1 [28] and [54] for retrieval and for reranking [83]. Beyond reasoning, Hypencoder [31] which uses hypernetwork to enable query-specific neural networks has also been proposed as method to enable retrieval on more complex tasks."
        },
        {
            "title": "2.3.1 Evaluation Sets. One of the earliest examples of complex natural language queries is from the\nTREC Filtering Track [51] where given a question that often contained multiple aspects, participants\nhad to filter a document stream with the binary judgment relevant or irrelevant. Participants also\ngot human judgments to help update their scoring function making it different from most retrieval\ntasks where human judgments are not available.",
            "content": "Since then one of the most common types of complex query come from multi-hop tasks, where queries are specifically created to require information from multiple distinct documents. Some of the earliest multi-hop question answering (QA) datasets include QAngaroo [71], Complex Web Questions [58], and HotPotQA [77]. More recent multi-hop QA datasets have considered information in both tables and text [11]. Another type of complex query that has gained recent popularity is the instruction-following query. These queries generally contain more constraints than traditional queries which are expressed Julian Killingback and Hamed Zamani 7 as instructions. The idea of adding instructions to queries was first introduced by Su et al. [55] and Asai et al. [7] though both focus more on adaptability of retrieval models to different tasks and thus use task-level instructions. FollowIR [72] was the first paper to introduce densely annotated evaluation set with query-level instructions. Tip-of-the-tongue queries which come from users who remember aspects of an entity but forget the name (it is on the tip-of-their-tongue) are also generally complex by their nature. The term tip-of-the-tongue in the Information Retrieval context was first coined by Arguello et al. [6] which investigated tip-of-the-tongue queries for movie recommendation. Recently, tip-of-the-tongue has become TREC track [4, 5], which includes items from movies, landmarks, and celebrities. Other works that combine multiple aspects have been created in various domains and formats. Some have been built to specifically include logical operations such as and and or such as QUEST [38] and RoMQA [82]. Others have focused on constructing multi-aspect queries such as DORISMAE [65], which focuses on paragraph-length queries for scientific paper search."
        },
        {
            "title": "2.3.2 Collections. Although many datasets are complex, there are substantially fewer collections\nthat primarily feature complex retrieval tasks. Generally, the collections that do can be split into\nspecialized and non-specialized, where specialized collections focus on a specific topic whereas\nnon-specialized cover multiple topics.",
            "content": "For specialized evaluations, there are few collections which focus on topics that generally have multiple constraints. These include CoIR [34] for code-related retrieval tasks, MIRB [29] for mathematical information retrieval, and R2MED [33] for reasoning-heavy medical retrieval. Each of these collections includes several tasks with the majority featuring complex queries. Though these benchmarks are useful to evaluate models in these specific domains, the lack of breadth limits the generalization of findings on any specific dataset. Although multiple specialized collections could be used to evaluate system, this can require significant effort and can lead to data variants that make direct comparisons difficult between methods. To remedy the problems with specialized collections, some non-specialized collections have been proposed. One such collection is BIRCO [67] which combined five complex tasks to form unified collection. BIRCO specifically was constructed to evaluate LLM-based systems and as such they focus on making small enough dataset to make LLM-based evaluations tractable, due to this design choice, BIRCO is designed as reranking dataset with only small number of documents provided for each query. Although they argue the difficulty is similar to full retrieval, the limited number of documents makes evaluating recall at larger cut-offs meaningless, which is highly relevant for complex retrieval tasks where models often struggle with recall and where strong LLM rankers are able to make up for precision errors. Additionally, the majority of tasks only have single relevant query with an average of 8.4 relevant documents per query, this means evaluating recall with large number of relevant documents is not possible with BIRCO. The selected datasets have some additional shortcomings, WhatsThatBook [37] tip-of-the-tongue dataset for book retrieval uses Goodreads1 description for documents though these are often short and lack necessary details to satisfy the queries; RELIC [59] uses quotes from literature analysis papers as documents and the original passages with the quote masked as the queries, though this task is complex, it is not natural search task and is likely far out-of-domain for retrieval models. These datasets make BIRCO less reliable as an overall benchmark for complex retrieval. In contrast, we focus on high-quality and realistic datasets for complex retrieval tasks. BRIGHT [56] is one of the most popular existing complex collections with focus on reasoningintensive retrieval tasks. BRIGHT has three main task types: (1) given Stack Exchange question retrieve relevant document (2) given math or coding problem retrieve similar types of problems 1goodreads.com 8 Julian Killingback and Hamed Zamani (3) given math or coding question retrieve relevant theorem or relevant code documentation. BRIGHT is well-formulated and realistic dataset, but it has some limitations. First, the query types are limited to either Stack Exchange questions or code/math questions. Though these are complex, they are only subset of the types of meaningful queries that we would expect users to ask of advanced information access systems. Second, for the Stack Exchange set, the number of documents for each subject area is low with an average around 500. Though chunked version is provided which increases the number of documents, the low number of source documents makes the collection far less challenging in the full document scenario and means the chunked passages do not show the diversity present in real-world large-scale collections. In this work, we focus on collecting large set of diverse complex tasks which include several large collections with long documents and put an emphasis on tasks that cover various additional aspects beyond complexity. Additionally, we provide some additional benefits over BRIGHT, we provide large portion of our dataset in unified markdown format enabling research on how to handle semi-structured information. We use this structure to construct contextualized chunks, when possible, which is likely what would be used for real-world complex retrieval tasks if quality is desired, as preserving hierarchical context (like headers) within chunks helps to maintain semantic integrity and provides more complete information to the retrieval model, leading to more accurate relevance assessments [41]. Furthermore, BRIGHT provides no official development/validation set which makes it: (1) hard to tune retrieval systems (2) makes it difficult to use modern retrieval strategies which often take advantage of few-shot prompting approaches [17, 66]."
        },
        {
            "title": "3.1 Datasets\nPaper Retrieval Derived from DORIS-MAE [65] a test collection that contains 100 paragraph-\nlength multi-aspect queries that describe information needs when searching for scientific papers.\nEach query is human-written and motivated by analyzing a set of one or more initial papers and\nreconstructing the early thought processes of the original authors. The corpus is made up of paper\ntitles and abstracts. To facilitate data labeling, the authors break each query into various aspects\nand these aspects further into sub-aspects when appropriate. The authors then use GPT-3.5 to judge\nwhether a paper meets an aspect or sub-aspect. The judgments on the aspects and sub-aspects can",
            "content": "Julian Killingback and Hamed Zamani 9 then be used to get an overall relevance score. This dataset is complex by design as it is constructed specifically to contain multiple aspects. We remove all queries that do not have at least one document that satisfies all aspects (at least partially). This results in 79 queries. We provide two query relevance labels, one intended for binary metrics considers paper relevant if all aspects are met, the other set is sum of points where document receives 1 point for each aspect that it mostly fulfilled and 2 points for each aspect it completely fulfilled. SetOps Derived from QUEST [38] an entity retrieval dataset where queries contain implicit set relationships such as or, and, and not. The dataset is constructed using Wikipedia categories to find relevant entities for simulated query based on set of templates such as \"_ or _\" and \"_ but not _\". This query is then rewritten by crowd-source workers and scored for its naturalness and fluency. The documents used to represent each entity are the corresponding Wikipedia pages for category. Although the known entities in category and the query templates allow for an initial set of relevant documents, the information required by query may not be present in the text of the Wikipedia page. Thus, the authors have crowd-workers assess whether candidate page is relevant. This dataset is complex, as the queries often contain multiple requirements combined with set-based operations. Additionally, many of the \"single\" requirements contain multiple aspects on their own such as German spy comedy films. In this work, we only consider queries from the test collection in the book and film domains, as these are the only ones with human-relevance judgments. We further filter the queries, removing those that are not judged as having high naturalness and fluency. We consider an entity relevant if the average rater score is above 1.5 points where 1 point is given for likely relevant ratings and 2 points are given for definitely relevant ratings. We provide both full document and chunked document (passage) collection. The full document collection is just the Wikipedia page formatted in standard markdown. While the passage collection is set of passages formatted using the contextualized chunking strategy described in Section 3.2. Legal QA Derived from Reasoning Focused Legal Question Answering [81] question answering dataset focusing on questions in the legal domain that require reasoning. We use only the Housing QA task which focuses on retrieving relevant legal statutes given question about housing policy for specific state. For this dataset, we construct full documents using the chunks provided by the original dataset and add headings using metadata from the original dataset. We also produce chunked (passage) version following the chunking method from Section 3.2. This dataset is complex as there are always at least two distinct features to consider, first is the actual legal question and second is the state that is being asked about. Furthermore, as the questions are reasoning heavy, often there are multiple considerations, for instance, in the example shown in Table 13 the query Are eviction cases first heard in high court? In the state of Tennessee requires relevant document to address both where eviction cases are first heard and whether that is high court or not. As the full documents are the complete list of statutes for state, they are very long, and various chapters do not share topical relevance. For this reason, the documents released are split version of the original documents. We split them so that each chunk contains content that shares the first three levels of headings (e.g. Laws > Ordinances > Housing > Construction and Laws > Ordinances > Housing > Permitting would be in the same chunk). We found that three levels generally aligned with topical boundaries well. Tip-of-the-Tongue Derived from TREC Tip-of-the-Tongue 2023 track [5] this dataset contains tip-of-the-tongue (TOT) queries for movies and TV shows with Wikipedia pages as documents. TOT queries are queries where user knows of an item, but cannot remember the exact identifier such as title in the case of movie or TV show. As such, users tend to add as many details as they can remember, which often results in long queries with many details at varying levels of relevance 10 Julian Killingback and Hamed Zamani and certainty. For instance, the user might remember certain interior vividly, but only provides guess of when the movie might have been produced. As the entity identifier is not known and peoples memories are often hazy, these queries tend to share minimal keywords with the relevant query. natural consequence of this is also that queries have many aspects making them complex. We process the original collection provided by TREC into our standardized markdown format. The original corpus is provided in MediaWiki format, Wikipedias editing format, which includes templates that cannot be easily converted to markdown, since the exact way template is rendered depends on the templates internal logic. To address this, we use WikiMedias template expansion API to convert all templates to rendered HTML and then convert this to markdown. This results in clean markdown collection that includes the relevant information from the templates, which include all Wikipedia information boxes and many tables. After creating standardized markdown document, we use the chunking strategy outlined in Section 3.2. We assume that all chunks derived from relevant document are also relevant for our passage labels, though we recommend, and used while benchmarking, MaxP strategy [16] with the full document labels to not penalize models for failing to retrieve chunks that might not have relevant information. Theorem Retrieval Derived from BRIGHTs theorem retrieval dataset [56] this dataset has math problems as queries and math theorems as documents. The task is to retrieve theorems that are relevant to solving the math problem. We largely leave the dataset unchanged as the documents are short enough that they do not need to be chunked and there would be minimal benefit to adding markdown formatting given the minimal structure. As such, we only split the original test set into validation and test split. This dataset is complex, as the math problems generally have multiple constraints and considerations which are necessary to evaluate when finding the relevant theorem. StackExchange This task is derived from BRIGHTs questions from Stack Exchange which cover range of topics including Economics, Biology, Robotics, Programming (Stack Overflow), Sustainable Living, Earth Science, and Psychology. The original BRIGHT data was created by taking questions from various Stack Exchange communities and using links from relevant answers to find relevant web pages. Then additional searches using Google were conducted to find additional relevant web pages or hard-negative web pages. This dataset is complex as generally the Stack Exchange questions include multiple components, such as an example and question based on that example or multiple related questions. To create unified markdown collection, we used the original links collected by the BRIGHT authors to download the raw HTML for the web pages. We then used the boilerplate removal tool Trafilatura [8] to clean the HTML and convert it to markdown. For Wikipedia pages, which made up sizable subset of the collection, we directly downloaded the Wikipedia source using the WikiMedia API and converted this to markdown. Due to errors in accessing and converting the web pages as well as some missing document links, we were unable to completely recreate the BRIGHT corpus ourselves. To make up the difference, we used BRIGHTs original documents and converted them to our standard markdown to the best of our ability. Aligning the relevance judgments for the full documents was straightforward, but as BRIGHT provides chunk-level relevance judgments, we also wanted chunk-level relevance for our chunked collection. We found this difficult because if their chunk was split over two of our chunks it is not clear which chunk contains the relevant information without additional labeling. To remedy this problem, we only use queries from BRIGHT where all the original relevant chunks are fully encapsulated in one of our chunks. We used sub-string coverage to automate this process with manual checks for lower-overlap sub-string chunks to ensure the chunk was valid. This reduced the number of queries, but ensured valid relevance labels. Julian Killingback and Hamed Zamani 11 To increase difficulty and make evaluation simpler, we combined all Stack Exchange topics into single collection and corpus. We believe that this has very minimal chance of adding unlabeled positives given the significant differences in the topics. Code Retrieval Derived from the dataset APPS [24] which was created to measure the ability of generative models to solve code problems. APPS contains 10k code problems such as those from LeetCode2 as well as several reference solutions. To create retrieval dataset, we create corpus made up of the reference solutions and use the code problems as queries. We find that the solutions have minimal keyword overlap with the problems in large part because these problems are often solved quickly with short variable names and minimal documentation. This makes the dataset good measure of retrieval models ability to reason even with very minimal term overlap. This dataset is complex, as like Theorem Retrieval the code problems have multiple constraints and components which need to be considered to identify the correct solution document. As the documents are fairly short and have no clear way to chunk them while remaining comprehensible, we do not do any document chunking. For the queries, we only include the natural language problem description and input-output format without any provided examples. Clinical Trial Derived from the TREC Clinical Trial 2021 [49] and 2022 [50] Tracks this dataset uses patients medical history expressed in natural language as the query and tries to retrieve relevant clinical trials. For this dataset, we converted the original XML files to markdown and used the chunking approach described in Section3.2. We assume that all chunks derived from relevant document are also relevant, but as recommended for Tip-of-the-tongue we suggest using MaxP and the full-document labels for evaluation. This dataset is complex because the patient history used has many aspects about the patient. Additionally, relevant documents can require certain age range which provides useful measure of whether retrieval models can adhere to numeric constraints."
        },
        {
            "title": "3.2 Formatting and Chunking Strategy\nFor all datasets that had original documents that had natural divisions (e.g. not a single code\nblock) we formatted the documents in a uniform structured way using markdown. We provide the\ncomplete markdown document to enable long-context evaluation with structured documents. As\nmany retrieval models are unable to process longer documents (or unable to represent them well),\nwe also provide a passage version which features standardized chunks. For the chunked version we\ntake advantage of the markdown structure to include all the relevant headings for a paragraph or\nother content block, see Figure 2 for an example of what this looks like. Our chunking strategy\ncontinues to add content to a chunk until the total number of BERT [19] tokens in the chunk would\nexceed 512 if the content was added. If a piece of content is more than 512 tokens on its own, it\nwill be added as an individual chunk without truncation.",
            "content": "We believe that our formatting and chunking strategy is unique in that it provides unified structured format for documents. This enables future evaluation of retrieval models that use document structure and truer understanding of retrieval model performance on our chunked datasets. This is in contrast to several existing datasets, which often use new-line chunking without headings for context, which can result in incomplete information for retrieval, which results in worse performance [41]."
        },
        {
            "title": "3.3 Validation Data\nTo enable validation of trained models and enable the use of few-shot techniques that have become\ncommon practice in LLM augmented retrieval [25] we include a validation/dev set for each dataset.",
            "content": "2leetcode.com 12 Julian Killingback and Hamed Zamani Original Markdown Document # Main Title ## Section 1 This is the first paragraph of section 1. It contains some interesting information that spans multiple sentences. ### Subsection 1.1 This is paragraph in subsection 1.1. It provides more detailed information. Lets assume this paragraph is long enough to be its own chunk, or combined with the next one if its short. Another paragraph in subsection 1.1, continuing the discussion with further details and examples. ## Section 2 This is the first paragraph of section 2. It introduces new topic. # Main Title ## Section 1 This is the first paragraph of section 1. It contains some interesting information that spans multiple sentences. # Main Title ## Section 1 ### Subsection 1.1 This is paragraph in subsection 1.1. It provides more detailed information. Lets assume this paragraph is long enough to be its own chunk, or combined with the next one if its short. Generated Chunks # Main Title ## Section 1 ### Subsection 1.1 Another paragraph in subsection 1.1, continuing the discussion with further details and examples. ## Section 2 This is the first paragraph of section 2. It introduces new topic. Fig. 2. Example of our contextualized chunking strategy. The original markdown document content is segmented into chunks and each chunk prepended with its hierarchical header path. This approach preserves structural context for the retrieval models. Table 3. Overview of the instructions used for each dataset."
        },
        {
            "title": "Instruction",
            "content": "Tip-of-the-tongue Given description of an entity, retrieve the entity that is described StackExchange Paper Retrieval Set-Ops Clinical Trial Legal QA Theorem Retrieval Given math question, find theorems that are relevant to it Code Retrieval Given question, find relevant documents that can help answer it Given query, find relevant scientific papers which satisfy the requirements Given query, find entity pages which satisfy it Given patients medical history, find clinical trials that are relevant to them Given query, find relevant legal documents which satisfy the requirements Given code question, find code snippets that would answer it Validation data is taken from the same pool as the evaluation data though it is of course not included in the final evaluation data. We take 10% or 100 instances of the data, whichever is smaller, as the validation set for each task. This is notable difference from other common retrieval benchmarks such as BEIR [60] and BRIGHT [56], which do not include development sets for each of their tasks. This lack of development set limits the types of methods that can be evaluated e.g. few-shot prompt-based methods [17, 66]."
        },
        {
            "title": "4 Experimental Setup\nThis section details the setup for our experiments.",
            "content": "Julian Killingback and Hamed Zamani 13 Table 4. Overview of the instructions used while retrieving using the various query rewriting approaches. These instructions were only used for retrieval models that were trained with instructions."
        },
        {
            "title": "Instruction",
            "content": "Query-to-Answer Query-to-Doc Query-as-Reasoning-Trace Given question, information about what should be included in Given an answer to question find supporting/relevant items Given an item find similar items the answer, and draft answer find supporting/relevant items Table 5. The format used for each retrieval model when indexing/encoding the documents and encoding/searching the queries. All formats were from the original authors recommendations when available. If models were not trained with instructions, the query and document were provided without any further information."
        },
        {
            "title": "Document Formatting",
            "content": "<query> query: <instruction>n<query> BM25 Snowflake GTE Qwen 1.5B Instruct: <instruction>nQuery: <query> GTE Qwen 7B Instruct: <instruction>nQuery: <query> Lion SB 1B <query> Lion SB 7B <query> query: <query> <instruction> Promptriever Lion DS 1B <query> Lion DS 8B <query> <document> <document> <document> <document> <document> <document> passage: <document> <document> <document>"
        },
        {
            "title": "4.2 Benchmarking Models\nTo get a comprehensive picture of how existing neural retrieval models perform on complex retrieval\ntasks, we have assembled a diverse and representative collection of models. To achieve this goal,\nwe considered several criteria, concretely we wanted: (1) models that were high-performing based\non existing benchmarks (2) a range of model sizes (3) models trained on various types of data (4)\nmodels that used different retrieval paradigms. We also tried to pick models in a way that allowed\nfor meaningful comparisons between models.",
            "content": "14 Julian Killingback and Hamed Zamani"
        },
        {
            "title": "4.2.1 Retrieval Models. For general purpose strong retrieval models we use the retrieval subset of\nthe Massive Text Embedding Benchmark (MTEB) leaderboard [42] to select a few models across\na variety of model sizes. The first is Snowflake Arctic Embed L V2.0 (abbreviated Snowflake)\n[48] which is a dense retrieval model with 303M parameter (not including embedding parameters)\nand an embedding dimension of 1024. It is trained on a large collection of data including Stack\nExchange, S2ORC, and Wikipedia [78] and has one of the strongest scores on MTEB for a sub-0.5B\nmodel at the time of writing. The second model is GTE Qwen2 1.5B Instruct (abbreviated as GTE\nQwen 1.5B) [35] a dense retrieval model based on Qwen2 1.5B. It has an embedding size of 1536\nand uses bi-directional attention. Like other GTE models, GTE Qwen 1.5B is trained on a corpus of\n800 million weakly supervised pairs from multiple domains including question-answering, code,\nsocial media, and link-text-webpage pairs. It is then fine-tuned on retrieval datasets including MS\nMARCO [43] and NQ [32]. For its size, GTE Qwen is one of the best performing models on MTEB.\nThe third model is GTE Qwen2 7B Instruct (abbreviated as GTE Qwen 7B) [35] is identical to\nGTE Qwen 1.5B except that it uses the larger Qwen2 7B as the base model and has an embedding\ndimension of 4096. It is one of the best performing embedding models on MTEB.",
            "content": "To get sense of how models trained on the popular training dataset MSMARCO fare, we included several models which are only trained on MSMARCO (or derived datasets), but with variety in the model architecture. To investigate how sparse retrieval models compare to dense ones we use Lion-SB and Lion-DS [79] which are Llama 3 [20] based retrieval models that use sparse embeddings in the case of Lion-SB and dense embeddings in the case of Lion-DS. To investigate how size may impact sparse and dense retrieval we use Lion models in two sizes: 1B and 8B. All of the Lion models are trained with only MSMARCO data with fixed compute budget, meaning that large models are trained for fewer epochs. To investigate how prompting might help models generalize we use Promptriever Llama 3 8B Instruct (abbreviated as Promptriever) [73]. This model was trained with modified version of MSMARCO which added additional instruction and negative documents which makes it capable of responding to instructions to modify the inference-time behavior. We also include the term-based model BM25 [52] as reference."
        },
        {
            "title": "4.2.2 Query Rewriting and Expansion Approaches. Using LLMs to rewrite queries has become a\npopular method to improve retrieval performance [26, 56, 66]. For completeness, we include several\nLLM based query rewriting techniques to investigate how these impact model performance on\ncomplex tasks. As chain-of-thought (CoT) [70] prompts are generally known to produce better\noutcomes [26, 68, 70], which was verified by our early experiments, we use CoT whenever possible\nand do not provide zero-shot results.",
            "content": "We experiment with three query rewriting techniques: Query-to-Answer CoT Given query, we prompt the LLM to directly answer the question after an extended \"thinking\" period. The thinking information is not used. Query-to-Document CoT Given query, we prompt the LLM to provide document which provides an answer to the query after an extended \"thinking\" period. The query instruction provides the expected document type (e.g. piece of code, web page, etc.). The thinking trace is not used for retrieval only the final document. Query as Reasoning Trace Given query, the model is asked to understand the query, think about it, and produce complete answer. Unlike previous approaches, the entire reasoning process is used as part of the query. This is similar to the approach used in BRIGHT [56]. The prompts for each rewriting technique can be seen in Appendix A. Julian Killingback and Hamed Zamani"
        },
        {
            "title": "4.3 Retrieval Model Implementation\nFor all neural retrieval models, we used PyTorch [44] and Huggingface Transformers [76] for the\nimplementations. For the Snowflake Arctic Embed L V2.0 we used SentenceTransformers [47]. For\nthe other models, we followed the exact usage suggested by the model authors and directly used\nthe original code when necessary. For prompts, we followed the published prompt format for all\nmodels that included suggested prompt formats. For models that do not support prompting, we\ninclude only the query text. The prompt templates for all baseline models can be seen in Table 5.\nFor each task, we include a specific instruction for the models that support instructions. The\ninstructions for each task can be found in Table 3. For rewritten queries, we used a shared instruction\nbetween tasks that was unique to the rewriting approach. These instructions can be seen in Table 4.\nFor dense retrieval models, we used an exact nearest neighbor (i.e. flat index) implemented in\nPyTorch. For sparse retrieval models, we used an adapted version of the Numba optimized version\nreleased by Formal et al. [22]. For BM25, we used the implementation from the Pyserini library\n[36] with default parameters.",
            "content": "The results on the chunked-document (passage) version of CRUMB (the main results in this paper) were found by retrieving 2000 documents for each query. For datasets where chunking was employed and where the original dataset only included document-level relevance labels namely Tip-of-the-tongue, Clinical Trial, and SetOps we used MaxP [16] to produce the final retrieval list for all models. MaxP selects the maximum passage (i.e. chunk) from parent document and only keeps this maximum passage. We chose this approach as expecting models to retrieve all chunks from relevant document would likely add significant amounts of noise given that many chunks from relevant document may not be directly relevant to the query."
        },
        {
            "title": "5.1 Overall Performance",
            "content": "16 Julian Killingback and Hamed Zamani Table 6. Overview of baseline model performance for the chunked document (passage) version of CRUMB. We use three metrics: normalized discounted cumulative gain at 10 (nDCG@10), recall at 100 (R@100), and recall at 1000 (R@1000). The last row shows the average (mean) value of each metric across all the tasks. . 5 1 Q 0.156 0.489 0. 0.300 0.773 0.977 0.429 0.532 0.840 0.156 0.364 0.650 0.352 0.216 0.532 0.255 0.680 0.835 0.177 0.583 0. 0.302 0.293 0.529 0.266 0.491 0.749 7 Q 0.253 0.674 0.867 0.327 0.836 0.961 0.391 0.486 0. 0.227 0.484 0.771 0.370 0.258 0.625 0.428 0.821 0.863 0.374 0.723 0.921 0.395 0.417 0.666 0.346 0.587 0. a fl S 0.092 0.326 0.519 0.273 0.740 0.949 0.338 0.427 0.763 0.096 0.253 0.511 0.294 0.153 0. 0.181 0.561 0.775 0.078 0.325 0.647 0.100 0.082 0.184 0.182 0.358 0.590 1 o 0.096 0.311 0. 0.180 0.678 0.898 0.414 0.488 0.759 0.217 0.447 0.717 0.179 0.084 0.271 0.223 0.598 0.796 0.044 0.248 0. 0.072 0.092 0.247 0.178 0.368 0.607 8 o 0.193 0.496 0.741 0.197 0.696 0.902 0.424 0.492 0. 0.231 0.484 0.740 0.254 0.129 0.333 0.161 0.504 0.751 0.080 0.255 0.524 0.113 0.128 0.305 0.207 0.398 0. v t r 0.305 0.578 0.844 0.262 0.730 0.940 0.421 0.536 0.802 0.194 0.430 0.710 0.468 0.283 0. 0.300 0.703 0.834 0.204 0.511 0.764 0.390 0.402 0.658 0.318 0.522 0.772 1 o 0.040 0.193 0.415 0.223 0.649 0.895 0.429 0.450 0.774 0.156 0.316 0.555 0.291 0.150 0.369 0.162 0.506 0. 0.046 0.173 0.367 0.042 0.042 0.111 0.174 0.310 0.530 8 o 0.104 0.193 0. 0.185 0.578 0.859 0.457 0.517 0.820 0.156 0.351 0.590 0.301 0.145 0.364 0.116 0.425 0.687 0.033 0.124 0. 0.077 0.074 0.186 0.179 0.301 0.534 5 2 Metric / Dataset Tip-of-the-tongue nDCG@10 R@100 R@1000 0.011 0.052 0.141 StackExchange nDCG@10 R@100 R@ Paper Retrieval nDCG@10 R@100 R@1000 SetOps nDCG@10 R@100 R@1000 Clinical Trial nDCG@10 R@100 R@1000 Legal QA nDCG@10 R@100 R@1000 0.088 0.450 0.752 0.376 0.449 0.738 0.197 0.413 0.644 0.224 0.102 0.276 0.064 0.257 0. Theorem Retrieval nDCG@10 R@100 R@1000 0.021 0.114 0.399 Code Retrieval nDCG@10 R@100 R@"
        },
        {
            "title": "Average",
            "content": "nDCG@10 R@100 R@1000 0.041 0.046 0.108 0.128 0.235 0.447 Julian Killingback and Hamed Zamani 17 The main results can be seen in Table 6. general trend is that model performance is substantially lower than on common retrieval evaluation datasets such as BEIR [60] and TREC DL [15], suggesting that additional developments are needed to produce high-performance retrieval models on complex tasks. Looking at the models that perform best, we find that the clear standouts are GTE Qwen 7B, Promptriever, and GTE Qwen 1.5B. For their size, the Qwen models lead models in similar range. This is likely due to the substantial text-pair training data that is used during the first training stage for both models. As these data are diverse and vast they likely increases the models capacity to represent broader range of concepts, which could explain the improvement in domains far from traditional training datasets. Promptrievers high performance is interesting because unlike the GTE models it is trained on narrow retrieval dataset based on MSMARCO but with instructions added to queries and hard negative passages created to encourage instruction following. Compared to Lion DS 8B, which uses the same Llama-3 8B architecture and MSMARCO training dataset, Promptriever performs substantially better with close to double the nDCG@10 and sizable increase in both recall metrics. There are few differences beyond the training data for each model that might play role in this disparity. One substantial difference is that although both use the same architecture, Promptriever uses the instruction-tuned version of Llama-3 8B while Lion DS uses the base version. This could help Promptriever as during instruction tuning the model may be exposed to examples that are more similar to the complex retrieval tasks in CRUMB such as answering code questions. The knowledge gained from instruction tuning might be transferred to the retrieval model and thus improve performance. There are other differences between Promptriever and Lion DS, for one Lion uses knowledge distillation loss and contrastive loss while Promptriever only uses contrastive loss. The impact of knowledge distillation loss is demonstrated in the original Lion paper where the authors find that compared to contrastive loss, the addition of knowledge distillation reduces the models performance on out-of-domain tasks. There are two other notable differences between Promptriever and Lion DS 8B: Lion uses bi-directional attention while Promptriever uses causal attention. It seems unlikely that the bidirectional attention would result in decrease in performance, though perhaps it would require more time to adapt to, and thus results in worse performance. Although these factors likely play some role, the introduction of instructions seems like very important difference, especially as CRUMB contains such varied datasets. Having the ability to modify the search-time behavior would probably confer substantial advantage. Our analysis of the importance of task-specific instruction in Section 5.6 shows that instructions are important for performance, but they do not explain Promptrievers substantial performance gain over Lion DS 8B. It might be that the training data helps Promptriever beyond allowing it to follow instructions, as in addition to adding instructions, the training data include new hard negatives which are only negative due to an added requirement from the instructions. This requires the model to learn to judge relevance in way that might be more subtle than what is present in the original MSMARCO dataset and make Promptriever better suited for complex retrieval tasks. Note that the GTE Qwen models also use the instruction-tuned versions of Qwen as base models and are also trained with instructions, both of which likely play role in their success. Though like Lion they use bi-directional attention. The takeaway from the best performing models is that training data, unsurprisingly, plays significant role in the ability of retrieval models to perform well in varied and complex retrieval tasks. Interestingly, Promptriever shows that vast diverse set of training data may not be necessary for good performance and that instruction-tuning alone can have substantial impact on the capability of retrieval models. With that said, the performance of GTE Qwen suggests there are still benefits to training on large collection of diverse text pairs. Another takeaway is that larger models generally do better, with the two best performing models having seven billion or more parameters. In support of this conclusion, directly comparing GTE Qwen 1.5B and GTE Qwen 7B we can see notable 18 Julian Killingback and Hamed Zamani 0 1 @ n 0.3 0. 0.1 0 BM25 Qwen7B Qwen1.5B Snowflake GTE GTE LionSB1B LionSB8B Promptriever DS1B Lion DS8B Lion No Rewriting Query-to-Answer Query-to-Doc Query-as-Reasoning Fig. 3. Plot of average nDCG@10 on the chunked-document version of CRUMB with various query rewriting techniques. 0 0 0 1 @ 0. 0.6 0.4 0.2 0 BM25 Qwen1.5B Qwen7B Snowflake GTE GTE LionSB1B LionSB8B Promptriever DS1B Lion DS8B Lion No Rewriting Query-to-Answer Query-to-Doc Query-as-Reasoning Fig. 4. Plot of average R@1000 on the chunked-document version of CRUMB with various query rewriting techniques. increase in the average performance of the larger model; though the smaller Qwen does do better on Paper Retrieval."
        },
        {
            "title": "5.2.1 Tip-of-the-tongue. The low nDCG and recall values of BM25 suggest that there is very little\nterm overlap which is supported by the properties of tip-of-the-tongue queries where people try to",
            "content": "Julian Killingback and Hamed Zamani 19 recall features from memory without remembering key identifiers such as the title or actors names. There is substantial variance in both nDCG and recall across the various models. The general ranking is similar to the average with the notable exception that Promptriever has the highest nDCG value. The best model in terms of R@1000, GTE Qwen 7B, is able to achieve respectable 0.867 which is close to what state-of-the-art models achieve on datasets like TREC DL. The high recall contrasted with the low nDCG suggests that models struggle to find the correct item, but might be able to find reasonable candidates quite well. This highlights an important aspect of retrieval which is how plausible candidates scale as the granularity of relevance decreases. In this case, the model may not be able to exactly match the qualities of the query with the document, but might be able to find broader features which match such as genre, general time period, etc. By leveraging these broader features the candidate space might decrease significantly which explains the high recall as only small subset of the movies in the corpus fall into the intersection of these broad subsets. StackExchange. The nDCG values are fairly low, and BM25 is especially low, suggesting 5.2.2 that term matching is not able to easily distinguish relevant documents from irrelevant ones. The variance of nDCG values is not as high as other datasets, but still reasonably large. The variance of R@100 is substantial while R@1000 is fairly compressed. The high recall values suggest models are able to find reasonable set of candidate documents, but struggle to distinguish the subtle differences between relevant and hard negative documents. The low nDCG and high recall likely indicates similar phenomenon as in Tip-of-the-tongue where it is pretty easy to retrieve candidates which share broad features with relevant documents such as topic, but it is difficult for the models to differentiate relevant documents within this candidate pool. The construction of the dataset may make finding the broad pool of topically relevant candidates easier, as each query is meant to be distinct enough from other queries to ensure there would be no overlap in relevant documents, which likely results in minimal overlap in document topics."
        },
        {
            "title": "5.2.3 Paper Retrieval. This dataset is characterized by long multi-aspect queries with specialized\nterminology on scientific topics. Performance is relatively high across the board compared to other\ndatasets, with even BM25 achieving a respectable nDCG@10 of 0.376. This suggests that the aspects\nin the queries often contain keywords present in the relevant paper titles and abstracts. However,\nthe top neural models still provide a significant boost, with Lion DS 8B achieving the highest\nnDCG@10 of 0.457. This is one of the few datasets where the dense Lion models outperform their\nsparse counterparts, lending credence to the idea explored in Section 5.4 that dense embeddings\nare better at capturing the meaning of rare specialized terminology. The instruction-tuned models\nlike Promptriever and GTE Qwen also perform strongly, though worse than Lion DS 8B which\nmakes this the only dataset where this is true. This may be because the paragraph length query\nand abstract are similar to the passages in MSMARCO which allows the Lion models to be closer to\ntheir \"in-domain\" performance.",
            "content": "Set-Ops. The results of this dataset are unique in that BM25 outperforms several models 5.2.4 in both nDCG and recall. This suggests that term matching is an important feature which is not surprising, as many of the queries contain entity names such as books by Leo Tolstoy written in 1863. This would clearly explain why BM25 is able to do well on the recall metrics, but given that the queries also require set-based operation logical such as and, or, and not it is surprising that BM25 is able to do so well. This result might indicate that the neural retrieval models that do worse than BM25 fail in two ways: (1) they fail to represent the precise information in the queries and documents and (2) they fail to understand the set-based operations. Both seem plausible with prior 20 Julian Killingback and Hamed Zamani work on the ability of neural retrieval models to handle negations and other set-based operations [38, 62, 74] show that unless models are specifically trained they will often fail. Additional evidence for the importance of term matching can be seen in the performance of Lion SB 8B which takes second place beating out the overall silver medalist Promptriever. This is unusual as generally Promptriever is significantly better than the Lion models. The stronger performance from Lion may also be due to the similarity between the queries from SetOps and the MSMARCO queries used for training. Unlike many of the other tasks, the SetOps queries are fairly short and have an implicit information need that is similar to the queries found in MSMARCO. Overall, the performance is below average even for the best models. This suggests that models struggle with set-based logic in queries which is supported by existing work [38, 74] and that models may also struggle to represent the entity-dense information from both the queries and documents."
        },
        {
            "title": "5.2.5 Clinical Trial. In this task, models must match a patientâ€™s medical history to relevant clinical\ntrial descriptions, both of which are rich in specialized medical terminology. The results show that\nthis is a challenging task, with low recall scores across all models; even the best model, GTE Qwen\n7B, only achieves an R@1000 of 0.625. This indicates difficulty in even identifying a broad set of\ncandidate trials from the large corpus. It is worth noting that this is one of the larger datasets\nwith around 900k chunks and has by far the highest number of positive documents per query at\n391.5. It may be the case that having more positive documents makes recalling them all effectively\nmore difficult, as they may be more spread out in embedding space. Interestingly, although models\nstruggle with recall the best nDCG@10 (0.468) achieved by Promptriever is well above the average\nfor the other datasets. This may be because distinguishing relevant clinical trials requires paying\nattention to small details which may align well with the generated hard negatives that Promptriever\nis trained with. Although Promptriever is unique in how high its nDCG is, it is not unique in\nhaving a higher nDCG than the benchmark average. Several models score above their average,\nsuggesting that high precision is a general trend. This is interesting as in other datasets such as\nStackExchange and Tip-of-the-tongue, we see the opposite, models do well on recall but poorly\non nDCG. This might be explained by revisiting the number of positive documents and how they\nmight be distributed in embedding space. Consider that there are likely some kinds of clusters in\nembedding space with similar documents grouped together, when there are more positives these\nclusters have a higher likelihood of contain several positives for a given query. For high precision\nat a low k, such as 10, the model may only need to place the query embedding close to one of these\npositive clusters. Conversely, to get high recall the model would need to place multiple embeddings\nto be close to multiple clusters, which, as these are all single-embedding models, is impossible.\nAnother observation is that similar to the Paper Retrieval dataset, this is another domain where\ndense models (like Lion DS) show a competitive advantage over their sparse counterparts, likely\ndue to their ability to better handle the complex, domain-specific vocabulary. This may be due to\ntokenization which may split medical terms into several sub-word tokens, and thus reduce the\nprecision the model can express. This may be exacerbated by the property of medical terminology\nto share prefixes and suffixes (e.g. Hemophilia and Hemoglobin).",
            "content": "Legal QA. The Legal QA task requires matching natural language question about housing 5.2.6 issue to specific legal statute. Overall, the results seem to be above average especially in terms of recall. The low nDCG for BM25 suggests that term matching is not enough to distinguish relevant from irrelevant documents. This could be because questions are state specific, so relevant documents might exist for various states, not just the one specified in the question. It might also be because the questions require reasoning and thus cannot be solved with keyword matching alone. The model performances are inline with the average model rankings for the most part with Julian Killingback and Hamed Zamani 21 the GTE Qwen models and Promptriever leading the pack for both recall and nDCG. Overall, the results suggest strong models are pretty good at finding candidate documents and are better at distinguishing them from irrelevant documents than on some of the other datasets. This seems reasonable given the characteristics of the dataset. Compared to many of the other datasets the number of aspects is pretty small and one of them, the state being inquired about, is very simple to match with the documents. Additionally, the format is similar to those seen in common training datasets like MSMARCO and NQ [32, 43]. Further, the terminology is likely less specialized than in Paper Retrieval and Clinical Trial which likely further improves model performance."
        },
        {
            "title": "5.2.8 Code Retrieval. For the Code Retrieval task, models must retrieve code solutions given a\nnatural language problem description. As noted, there is often minimal keyword overlap between\nthe problem and the solution, making this a challenging complex retrieval problem; which is\nsupported by the low BM25 scores for both recall and nDCG. Again, we see that the Qwen models\nand Promptriever dominate and even outperform their average nDCG. In contrast, recall is below\naverage for every model. The low recall and high nDCG mirrors the trends from Clinical Trial\nwhich suggests models may be able to find some of the correct answers, but struggle to recall the\npotentially diverse set of relevant answers. Though the top three models do above average in terms\nof nDCG this is not true of the other models which underperform their averages often significantly.\nThe large range of nDCG might indicate that whatever is different about the top performing\nmodels is well suited for code-based retrieval. On this topic, it is notable that the best three models\nare also those which use instruction tuned LLMs as base models. This could indicate that the\nadditional instruction data used on these models makes them more capable of understanding\ncode. As instruction-tuning often includes a specific focus on code it seems reasonable that these\nmodels are better at understanding and thus retrieving code. Additionally, the GTE models are\nspecifically trained on code-text pairs which likely explains the slight performance edge over\nPromptriever though it is not as significant as in some of the other tasks such as Theorem Retrieval\nor StackExchange.",
            "content": "22 Julian Killingback and Hamed Zamani"
        },
        {
            "title": "5.3 Full Document Results\nThe results of the baseline models for the full-document variant of CRUMB are presented in Table\n7. As mentioned previously, we believe the passage version of CRUMB is more realistic and better\nreflects the general trends and capabilities of retrieval model performance. Thus, we only include a\nlimited analysis of the results on the full document collection with a focus on the change in model\nbehavior between the chunked and full document versions.",
            "content": "Before analyzing the results, it is worth noting that some datasets only have one corpus as the original documents do not require chunking. These datasetsPaper Retrieval, Theorem Retrieval, and Code Retrievalare included in both result tables with identical results, as they can be used in either collection without modification. Since their results are unchanged from the passage version, we focus our analysis on the datasets that differ between the two collections. The remaining datasets show general trends similar to the passage version in terms of model rankings and relative performance, with the top-performing models (GTE Qwen 7B, Promptriever, and GTE Qwen 1.5B) maintaining their lead. However, there are notable differences in absolute performance levels that provide insights into the trade-offs between chunking and full-document retrieval. For the datasets that had per-chunk labels in the passage variantLegal QA and StackExchange we observe interesting and contrasting patterns. StackExchange shows substantial improvement in the full document version, with nDCG@10 increasing from range of 0.088-0.327 in the short version to 0.129-0.561 in the long version. The recall improvements are even more dramatic, with R@1000 jumping from 0.752-0.977 to 0.869-1.000, with several models achieving perfect recall. This substantial improvement can be attributed to the significantly smaller document collection size (5k versus 41k documents), which makes finding relevant documents considerably easier as there are fewer topically similar but irrelevant documents to distinguish between. Legal QA presents more varied picture with mixed results across different models. The topperforming modelsGTE Qwen 7B, Promptriever, and GTE Qwen 1.5Bgenerally show improvements in the full document version, as does BM25 and Lion SB 1B. However, other neural models including Lion DS 1B, Lion DS 8B, Snowflake, and Lion SB 8B exhibit notable performance decreases. This suggests that while some models can effectively leverage the additional context and reduced corpus size, others struggle with the longer sequences that must be processed and represented. The remaining datasetsTip-of-the-tongue, SetOps, and Clinical Trialallow for the most direct comparison between the passage and document versions of CRUMB, as they only have documentlevel labels and were evaluated using MaxP aggregation on the chunked version. This comparison essentially examines whether chunking or using full documents produces better retrieval performance. For SetOps, we observe that nearly all neural models perform worse on the document collection compared to the chunked version. This aligns with the common wisdom that chunking is beneficial. The queries in SetOps are often fact-based and entity-rich, requiring the model to find specific piece of information within document. Chunking helps by isolating these facts into smaller, more focused passages. single embedding for long document, such as full Wikipedia page, may dilute or \"average out\" these crucial details, making it harder for the model to perform precise matching. On Tip-of-the-tongue, the results are mixed. An interesting trend is that for all small and large model pairsQwen 1.5B and Qwen 7B, Lion SB 1B and Lion SB 8B, and Lion DS 1B and Lion DS 8Bthe large models do worse in both nDCG and recall on the full document collections than on the passage collection. In contrast, the smaller models see an increase in both nDCG and recall. Looking beyond the pairs of models, the other unpaired models show similar trend, with the Julian Killingback and Hamed Zamani 23 larger Promptriever performing worse and the smaller Snowflake performing better. BM25 seems to be included in the smaller model camp and sees better performance. This trend is pretty odd given that generally larger models have both larger embedding sizes and are better at understanding long-context information. One possible interpretation is that chunking is better, but capable model is required to fully take advantage of the contextualized chunks. If the smaller models are not able to understand the contextualized chunking format, it may add noise to the chunked version, which is less prominent, and smaller ratio of the overall text, in the full document version. Another potential reason for this odd behavior is that larger models are able to fill in missing context with their parametric knowledge in way that smaller model may not be able to. If this were the case for the chunked collection, they may be able to better represent chunks than the smaller models. As for why the larger models would see lower performance on the full document collection, it could be that they have to do more compression, which hurts the representation quality, than they need to do for the chunked collection. Although these theories might explain this phenomenon, future experiments are needed to fully understand it. Clinical Trial does not show the same inverse scaling; instead nearly all models see boost to nDCG with the full documents, with the notable exception of the best performing models in both collections, Promptriever which sees lower nDCG. Promptriever also has decrease in recall though it is not the only model to have lower recall. Qwen 7B and Lion SB 8B also have lower recall with the full document collection, while the rest of the models have some improvement. This might suggest weaker recall-only version of the inverse performance trend from Tip-of-the-tongue expect that Lion DS 8B sees large improvement in recall. The variation in performance makes it hard to draw solid conclusion from this task. One clear takeaway from both Clinical Trial and some of the other tasks is that better performing models in the passage collection, especially large ones, tend to see performance degrade with full documents. There is no clear message from the comparison between the full documents and chunked documents. In some cases, chunking leads to clear improvements, while in other cases there is substantial degradation. While it may be useful to some extent to compare StackExchange and Legal QA the difference in labels makes the comparison more muddied than the other datasets which have document-level labels. Even with this benefit, the picture from the tasks with document-level label is still murky with range of outcomes across models and tasks. One observation is that the chunked versions of these collections always contain the highest nDCG and recall which indicates that chunking does seem to be optimal for the best performing models. This aligns with prior work that shows that chunking, generally, improves retrieval performance. The gain that many models show seems to contradict these findings and requires more investigation to fully understand. We leave this to future work."
        },
        {
            "title": "5.4 Comparison of Learned Dense and Sparse Retrieval Models\nBeyond exploring the top-performing models, our experiments reveal some other interesting\nfindings. Notably, comparing the sparse and dense Lion models, we find that there is a substantial\ndifference between the two. For instance, Lion SB 1B outperforms Lion DS 8B on both recall metric\nand is only 0.01 points worse on nDCG despite being less than a quarter of the size. The aggregated\nresults suggest that when trained in the same way, sparse retrieval models have a significant\nadvantage over their dense counterparts.",
            "content": "Inspecting the individual task performance, we find that the sparse models perform better in the majority of the datasets often by substantial amounts; for example in Tip-of-the-tongue, the sparse models have nDCG@10 values almost double the dense version with large increases in recall. The only datasets where the dense models performed better are the Clinical Trial and Paper Retrieval datasets. These datasets likely represent the datasets with the most specialized terminology, except 24 Julian Killingback and Hamed Zamani perhaps Theorem retrieval, which might indicate that sparse models struggle to represent rarer terminology."
        },
        {
            "title": "5.5 Impact of Query Rewriting\nThe results of the average nDCG and R@1000 with query rewriting techniques can be seen in Figure\n3 and Figure 4, respectively. The complete result table for each rewriting technique can be viewed\nin Table 8 (Query-to-Answer), Table 9 (Query-to-Doc), and Table 10 (Query-as-Reasoning-Steps).\nThe results show that rewriting techniques tend to help weaker models while hurting stronger\nmodels. For the best model, GTE Qwen 7B, all rewriting techniques harm nDCG@10 and R@1000.\nRewriting impacts recall and nDCG differently, only the bottom four models see any improvement\non R@1000 with any of the techniques while for nDCG@10 all but the strongest model has some\nimprovement or stay the same. Looking only at the models that see improved nDCG there are\ndifferences between the weaker and stronger models. Weaker models see gains with more of the\nrewriting techniques on average, while stronger models tend to only have one technique which\nhelps. The gains from rewriting techniques are also larger for weaker models.",
            "content": "Of the rewriting techniques, Query-to-Answer is the approach which improves nDCG@10 for the most models with it improving performance for five out of the eight models which saw improvements from rewriting. The Query-to-Doc is second bringing the improvement to two models and Query-as-Reasoning is last with improvements to only one model. Looking at R@1000 all four models that saw improvement got the most improvement from Query-to-Doc. These results are reasonable as directly providing an answer, when the rewriting model is correct and there are limited number of correct answers, would likely lead to high overlap with the relevant document, thus resulting in higher nDCG@10. Query-to-Doc on the other hand may introduce more noise in the form of context for the answer, formatting, background, etc. which hurts precision, but which adds useful key-words and concepts increasing recall. The minimal or lack of gains in recall for strong models might indicate that they already have enough general knowledge to match broad categories and topics which enables strong recall. Rewriting may hurt this ability by adding superfluous information or incorrect information which dilutes the original query intent. Additionally, the rewritten query may modify the distribution of the queries making them less like the query distributions the models were trained on which may contribute in the degraded performance. Julian Killingback and Hamed Zamani 25 Table 7. Overview of baseline model performance for the full-document version of CRUMB. We use three metrics: normalized discounted cumulative gain at 10 (nDCG@10), recall at 100 (R@100), and recall at 1000 (R@1000). The last row shows the average (mean) value of each metric across all the tasks. . 5 1 Q G 0.212 0.519 0.793 0.447 0.953 1.000 0.429 0.532 0.840 0.152 0.352 0.623 0.371 0.237 0.569 0.322 0.689 0. 0.177 0.583 0.899 0.302 0.293 0.529 0.302 0.520 0.762 7 Q 0.237 0.600 0.830 0.561 0.981 1. 0.391 0.486 0.802 0.215 0.457 0.722 0.403 0.279 0.620 0.457 0.766 0.872 0.374 0.723 0.921 0.395 0.417 0. 0.379 0.589 0.804 fl S 0.110 0.348 0.556 0.435 0.944 0.991 0.338 0.427 0.763 0.092 0.254 0. 0.343 0.178 0.421 0.087 0.442 0.699 0.078 0.325 0.647 0.100 0.082 0.184 0.198 0.375 0.597 1 o 0.140 0.378 0.652 0.306 0.850 0.967 0.414 0.488 0.759 0.197 0.432 0.703 0.233 0.104 0.280 0.281 0.653 0. 0.044 0.248 0.553 0.072 0.092 0.247 0.211 0.406 0.624 8 o 0.075 0.215 0.393 0.312 0.879 0. 0.424 0.492 0.770 0.204 0.390 0.634 0.278 0.132 0.332 0.101 0.345 0.591 0.080 0.255 0.524 0.113 0.128 0. 0.216 0.374 0.590 e p P 0.225 0.593 0.837 0.442 0.944 0.995 0.421 0.536 0.802 0.198 0.432 0. 0.428 0.264 0.577 0.330 0.690 0.864 0.204 0.511 0.764 0.390 0.402 0.658 0.330 0.546 0.776 n 0.043 0.222 0.437 0.357 0.855 1.000 0.429 0.450 0.774 0.140 0.279 0.509 0.331 0.164 0. 0.094 0.438 0.689 0.046 0.173 0.367 0.042 0.042 0.111 0.185 0.328 0.537 8 o 0.045 0.141 0.363 0.249 0.729 0.939 0.457 0.517 0.820 0.142 0.295 0.505 0.327 0.161 0.394 0.036 0.208 0. 0.033 0.124 0.370 0.077 0.074 0.186 0.189 0.301 0.522 5 2 Metric / Dataset Tip-of-the-tongue nDCG@10 R@100 R@1000 0.042 0.148 0. StackExchange nDCG@10 R@100 R@1000 Paper Retrieval nDCG@10 R@100 R@1000 SetOps nDCG@10 R@100 R@1000 Clinical Trial nDCG@10 R@100 R@ Legal QA nDCG@10 R@100 R@1000 0.129 0.505 0.869 0.376 0.449 0.738 0.202 0.408 0.655 0.260 0.119 0. 0.279 0.599 0.807 Theorem Retrieval nDCG@10 R@100 R@1000 0.021 0.114 0.399 Code Retrieval nDCG@10 R@100 R@"
        },
        {
            "title": "Average",
            "content": "nDCG@10 R@100 R@1000 0.041 0.046 0.108 0.169 0.298 0.520 26 Julian Killingback and Hamed Zamani Table 8. Overview of model performance using the Query-to-Answer CoT query rewriting approach on the chunked-document version of CRUMB. The Query-to-Answer CoT approach uses generated direct answer to the original query as the rewritten query. We use three metrics: normalized discounted cumulative gain at 10 (nDCG@10), recall at 100 (R@100), and recall at 1000 (R@1000). The last row shows the average (mean) value of each metric across all the tasks. . 5 1 Q 0.068 0.193 0.444 0.373 0.805 0.970 0.295 0.413 0.767 0.154 0.349 0. 0.329 0.225 0.519 0.306 0.708 0.841 0.413 0.739 0.892 0.252 0.261 0.502 0.274 0.462 0.694 7 Q G 0.114 0.341 0.726 0.359 0.815 0.960 0.280 0.387 0.748 0.163 0.371 0.672 0.347 0.245 0.549 0.413 0.805 0. 0.427 0.774 0.905 0.316 0.352 0.630 0.302 0.511 0.756 fl S 0.062 0.133 0.252 0.352 0.799 0. 0.254 0.357 0.687 0.098 0.229 0.484 0.306 0.188 0.384 0.307 0.700 0.836 0.372 0.717 0.831 0.091 0.097 0. 0.230 0.403 0.584 1 o 0.073 0.185 0.356 0.292 0.675 0.927 0.169 0.254 0.552 0.127 0.284 0. 0.148 0.061 0.176 0.266 0.644 0.814 0.411 0.669 0.877 0.061 0.080 0.237 0.193 0.357 0.561 8 o 0.090 0.222 0.415 0.327 0.698 0.923 0.150 0.246 0.540 0.134 0.288 0.534 0.196 0.096 0.230 0.232 0.609 0. 0.412 0.690 0.888 0.138 0.151 0.354 0.210 0.375 0.586 e p P 0.101 0.274 0.467 0.378 0.786 0. 0.325 0.426 0.778 0.167 0.354 0.614 0.349 0.253 0.539 0.358 0.732 0.849 0.422 0.725 0.849 0.375 0.403 0. 0.309 0.494 0.714 1 o 0.049 0.081 0.230 0.391 0.779 0.926 0.250 0.308 0. 0.142 0.267 0.464 0.311 0.191 0.407 0.302 0.682 0.819 0.425 0.696 0.850 0.118 0.116 0.263 0.248 0.390 0. 8 o 0.052 0.119 0.267 0.364 0.772 0.926 0.225 0.292 0.659 0.120 0.243 0. 0.294 0.165 0.362 0.215 0.572 0.775 0.379 0.636 0.851 0.170 0.165 0.349 0.227 0.370 0.580 5 2 Metric / Dataset Tip-of-the-tongue nDCG@10 R@100 R@1000 0.045 0.059 0.119 StackExchange nDCG@10 R@100 R@1000 Paper Retrieval nDCG@10 R@100 R@1000 SetOps nDCG@10 R@100 R@ Clinical Trial nDCG@10 R@100 R@"
        },
        {
            "title": "Legal QA",
            "content": "nDCG@10 R@100 R@1000 0.258 0.644 0.867 0.205 0.311 0.595 0.080 0.208 0.450 0.255 0.183 0.464 0.133 0.413 0. Theorem Retrieval nDCG@10 R@100 R@1000 0.315 0.584 0.790 Code Retrieval nDCG@10 R@100 R@"
        },
        {
            "title": "Average",
            "content": "nDCG@10 R@100 R@1000 0.082 0.094 0.214 0.172 0.312 0.521 Julian Killingback and Hamed Zamani 27 Table 9. Overview of model performance using the Query-to-Doc CoT query rewriting approach on the chunked-document version of CRUMB. The Query-to-Doc CoT approach uses generated relevant document created based on the original query as the rewritten query. We use three metrics: normalized discounted cumulative gain at 10 (nDCG@10), recall at 100 (R@100), and recall at 1000 (R@1000). The last row shows the average (mean) value of each metric across all the tasks. . 5 1 Q 0.060 0.207 0.548 0.387 0.809 0.961 0.324 0.434 0.747 0.163 0.327 0. 0.305 0.201 0.478 0.297 0.693 0.833 0.365 0.744 0.929 0.226 0.231 0.466 0.266 0.456 0.695 7 Q G 0.084 0.341 0.726 0.336 0.802 0.934 0.312 0.412 0.750 0.166 0.340 0.616 0.298 0.205 0.502 0.387 0.791 0. 0.367 0.788 0.915 0.289 0.327 0.601 0.280 0.501 0.738 fl S 0.061 0.215 0.459 0.371 0.780 0. 0.287 0.411 0.715 0.138 0.266 0.528 0.250 0.155 0.365 0.323 0.706 0.830 0.383 0.703 0.903 0.087 0.092 0. 0.237 0.416 0.619 1 o 0.041 0.156 0.385 0.241 0.623 0.906 0.171 0.266 0.538 0.117 0.259 0. 0.100 0.031 0.105 0.167 0.501 0.739 0.326 0.638 0.848 0.053 0.077 0.231 0.152 0.319 0.530 8 o 0.067 0.237 0.511 0.291 0.661 0.898 0.150 0.262 0.553 0.118 0.228 0.447 0.151 0.073 0.185 0.158 0.506 0. 0.329 0.611 0.851 0.127 0.142 0.328 0.174 0.340 0.566 e p P 0.100 0.267 0.630 0.333 0.776 0. 0.339 0.411 0.759 0.172 0.338 0.594 0.282 0.183 0.458 0.380 0.740 0.846 0.442 0.742 0.935 0.358 0.382 0. 0.301 0.480 0.724 1 o 0.061 0.200 0.333 0.367 0.762 0.920 0.299 0.383 0. 0.151 0.269 0.493 0.268 0.140 0.324 0.278 0.644 0.811 0.395 0.667 0.855 0.118 0.113 0.251 0.242 0.397 0. 8 o 0.079 0.222 0.385 0.304 0.704 0.924 0.260 0.328 0.679 0.134 0.248 0. 0.250 0.131 0.314 0.158 0.494 0.746 0.310 0.590 0.864 0.160 0.156 0.331 0.207 0.359 0.587 5 2 Metric / Dataset Tip-of-the-tongue nDCG@10 R@100 R@1000 0.051 0.119 0.185 StackExchange nDCG@10 R@100 R@1000 Paper Retrieval nDCG@10 R@100 R@1000 SetOps nDCG@10 R@100 R@ Clinical Trial nDCG@10 R@100 R@"
        },
        {
            "title": "Legal QA",
            "content": "nDCG@10 R@100 R@1000 0.258 0.652 0.885 0.332 0.388 0.750 0.149 0.279 0.505 0.284 0.189 0.459 0.093 0.355 0. Theorem Retrieval nDCG@10 R@100 R@1000 0.308 0.587 0.841 Code Retrieval nDCG@10 R@100 R@"
        },
        {
            "title": "Average",
            "content": "nDCG@10 R@100 R@1000 0.082 0.091 0.208 0.195 0.333 0.560 28 Julian Killingback and Hamed Zamani Table 10. Overview of model performance using the Query-as-Reasoning-Steps query rewriting approach on the chunked-document version of CRUMB. The Query-as-Reasoning-Steps approach uses reasoning trace as the query which includes what the original query is asking, what relevant document should contain, and an example relevant document. We use three metrics: normalized discounted cumulative gain at 10 (nDCG@10), recall at 100 (R@100), and recall at 1000 (R@1000). The last row shows the average (mean) value of each metric across all the tasks. . 5 1 Q 0.044 0.148 0.333 0.345 0.816 0.972 0.296 0.433 0.732 0.059 0.183 0. 0.409 0.250 0.560 0.286 0.708 0.843 0.333 0.786 0.986 0.184 0.180 0.381 0.244 0.438 0.653 7 Q G 0.160 0.519 0.793 0.336 0.768 0.944 0.304 0.399 0.730 0.098 0.246 0.526 0.382 0.252 0.558 0.408 0.806 0. 0.404 0.777 0.930 0.216 0.236 0.479 0.289 0.500 0.728 fl S 0.050 0.148 0.296 0.363 0.801 0. 0.285 0.440 0.724 0.045 0.136 0.352 0.345 0.175 0.378 0.338 0.728 0.843 0.342 0.724 0.937 0.113 0.103 0. 0.235 0.407 0.589 1 o 0.062 0.170 0.467 0.208 0.651 0.910 0.168 0.298 0.558 0.123 0.280 0. 0.077 0.022 0.076 0.181 0.543 0.745 0.322 0.659 0.880 0.008 0.032 0.174 0.144 0.332 0.542 8 o 0.119 0.333 0.578 0.288 0.687 0.903 0.161 0.257 0.535 0.131 0.265 0.496 0.133 0.055 0.147 0.187 0.550 0. 0.352 0.680 0.924 0.056 0.085 0.242 0.178 0.364 0.574 e p P 0.131 0.400 0.644 0.354 0.799 0. 0.368 0.486 0.800 0.148 0.348 0.620 0.406 0.264 0.559 0.389 0.768 0.856 0.417 0.725 0.931 0.354 0.375 0. 0.321 0.521 0.748 1 o 0.063 0.148 0.304 0.357 0.779 0.937 0.295 0.376 0. 0.109 0.209 0.393 0.289 0.119 0.294 0.333 0.716 0.836 0.341 0.645 0.860 0.059 0.072 0.189 0.231 0.383 0. 8 o 0.057 0.126 0.252 0.299 0.710 0.914 0.241 0.362 0.651 0.083 0.168 0. 0.127 0.041 0.130 0.186 0.536 0.749 0.261 0.599 0.825 0.089 0.102 0.250 0.168 0.330 0.514 5 2 Metric / Dataset Tip-of-the-tongue nDCG@10 R@100 R@1000 0.023 0.081 0.170 StackExchange nDCG@10 R@100 R@1000 Paper Retrieval nDCG@10 R@100 R@1000 SetOps nDCG@10 R@100 R@ Clinical Trial nDCG@10 R@100 R@"
        },
        {
            "title": "Legal QA",
            "content": "nDCG@10 R@100 R@1000 0.241 0.624 0.901 0.330 0.424 0.772 0.120 0.245 0.464 0.289 0.161 0.377 0.089 0.339 0. Theorem Retrieval nDCG@10 R@100 R@1000 0.291 0.670 0.854 Code Retrieval nDCG@10 R@100 R@"
        },
        {
            "title": "Average",
            "content": "nDCG@10 R@100 R@1000 0.054 0.062 0.156 0.180 0.326 0.541 Julian Killingback and Hamed Zamani"
        },
        {
            "title": "5.6 Impact of Instructions on Performance\nBoth of the top performing models, GTE Qwen 7B and Promptriever, are trained to follow instruc-\ntions. As each task has instructions outlining the objective, it is likely that these models modify their\nquery representation to better complete the task. To quantify the impact of instruction following on\nretrieval performance, we perform an experiment in which we replace the task-specific instructions\nwith the generic instruction Given a query, find relevant documents. This generic instruction provides\nno insight about the task and thus can help to gauge the importance of task-specific instructions in\nthe performance of the best models.",
            "content": "The results can be seen in Figure 5 and Figure 6, for nDCG@10 and R@1000 respectively. Both figures show how performance changes when task-specific instructions are swapped with generic instructions. Starting with Figure 5 we see that on average removing task-specific hurts nDCG@10 for both GTE Models and Promptriever. The amount varies with GTE Qwen 7B having the most profound decrease, followed by Promptriver, and GTE Qwen 1.5B. Interestingly, the difference in performance is highly dependent on the dataset, with Theorem Retrieval having the most overall performance decrease with generic instructions and Clinical Trial seeing gains with generic instructions for all models. Clinical Trial is the only task which sees all models improve with generic instructions, while Legal QA and StackExchange join Theorem Retrieval in worse performance across the board with generic instructions. Promptriever is the model that improves on the largest number of tasks when generic instructions are used which might suggest that its ability to follow instructions to changes its query-time behavior may not be the main factor in its improvement over the similar Lion models. Looking at the average performance with generic instructions, Promptriever only loses around 0.015 nDCG which still puts it well ahead of the Lion models. Moving to the recall changes in Figure 6, we find that on average both Qwen models are hurt when using generic instructions, while Promptriever is slightly better with generic instructions. We see that Legal QA and Theorem Retrieval both still have worse performance across all models with generic instructions, suggesting that these tasks see benefits with instruction regardless of the metric used. In general, the task-level trends are similar to those found with nDCG. In fact, for both Qwen models, when generic instructions hurt or help is exactly the same with R@1000 as it is for nDCG@10. Like the Qwen models, the recall changes for Promptriever largely follow the same trend as nDCG, but with flips on the StackExchange, Clinical Trial, and Code Retrieval tasks. Although for both Clinical Trial and Code Retrieval the change is pretty minimal and could be noise. The average slight gain with instructions does suggest that for Promptriever instructions are less helpful for improving recall. This makes intuitive sense, as trying to put relevant document in the top 10 positions has far less room for error than in the top 1000. Instructions likely reduce this error, but might not be fully needed for doing well on recall. In general, the impact of instructions varies by model and task. On average, the GTE Qwen models, especially the 7B variant, are most affected by change to generic instructions. Promptriever is surprisingly minimally affected by the switch to generic instructions, especially for recall, where there is actually slight gain with generic instructions. This may be in part because the instructions used for training Promptriever are longer and more specific than the ones we provided. Perhaps our instructions are more inline with the instructions used by GTE Qwen which could explain the differences between Promptriever and the Qwen models. Further, the GTE Qwen models were trained on far more varied data and thus are likely better able to adapt retrieval strategies for different domains based on the instructions. 30 Julian Killingback and Hamed Zamani 0 1 @ n Î” 0.05 0.00 0.05 0.10 0. Tip-of-the-tongue StackExchange PaperRetrieval SetOps"
        },
        {
            "title": "ClinicalTrial",
            "content": "LegalQA Theorem"
        },
        {
            "title": "Average",
            "content": "Promptriever GTE Qwen 1.5B GTE Qwen 7B Fig. 5. Comparison of change in nDCG@10 when generic instruction is used instead of task-specific instruction. The result is found by taking (ð‘›ð·ð¶ðºð‘”ð‘’ð‘›ð‘’ð‘Ÿð‘–ð‘ ð‘›ð·ð¶ðºð‘¡ð‘Žð‘ ð‘˜ ð‘ ð‘ð‘’ð‘ð‘– ð‘“ ð‘–ð‘ ), where ð‘›ð·ð¶ðºð‘”ð‘’ð‘›ð‘’ð‘Ÿð‘–ð‘ is the nDCG@10 with the generic instruction and ð‘›ð·ð¶ðºð‘¡ð‘Žð‘ ð‘˜ ð‘ ð‘ð‘’ð‘ð‘– ð‘“ ð‘–ð‘ is the nDCG@10 with the task-specific instruction."
        },
        {
            "title": "6.1 Where do Models Struggle?\nOur results in Table 6 reveal several patterns about how retrieval models handle complex tasks.\nThe consistently lower performance compared to standard benchmarks like BEIR and TREC DL\nTracks demonstrates that current retrieval models struggle when faced with queries that have\ncomplex information needs. This fact is highlighted by the substantially lower average nDCG@10\nof 0.346 for the best model (GTE Qwen 7B) than what these same models achieve on traditional\nbenchmarks.",
            "content": "It is clear that models on average perform poorly, but what are the underlying causes that lead to these bad results? One major takeaway is that where and how models struggle is not uniform across tasks and models. For instance, on the Code Retrieval task, the top three models have better nDCG than their average nDCG while the bottom models are far worse than their average. These groups of models tend to correlate through the tasks and metrics so we will do an analysis of each group of models separately to find what trends they each follow and what they struggle with."
        },
        {
            "title": "6.1.1 Where do the Stronger Models Struggle? Starting with the set of the best models, GTE\nQwen 7B, Promptriever, and GTE Qwen 1.5B we find that all of their nDCG scores are lower",
            "content": "Julian Killingback and Hamed Zamani 31 0.10 0.05 0.00 0. 0.10 0.15 0 0 0 1 @ Î” Tip-of-the-tongue StackExchange PaperRetrieval SetOps"
        },
        {
            "title": "ClinicalTrial",
            "content": "LegalQA Theorem"
        },
        {
            "title": "Average",
            "content": "Promptriever GTE Qwen 1.5B GTE Qwen 7B Fig. 6. Comparison of change in R@1000 when generic instruction is used instead of task-specific instruction. The result is found by taking (ð‘…ð‘”ð‘’ð‘›ð‘’ð‘Ÿð‘–ð‘ ð‘…ð‘¡ð‘Žð‘ ð‘˜ ð‘ ð‘ð‘’ð‘ð‘– ð‘“ ð‘–ð‘ ), where ð‘…ð‘”ð‘’ð‘›ð‘’ð‘Ÿð‘–ð‘ is the R@1000 with the generic instruction and ð‘…ð‘¡ð‘Žð‘ ð‘˜ ð‘ ð‘ð‘’ð‘ð‘– ð‘“ ð‘–ð‘ is the R@1000 with the task-specific instruction. than their average on Tip-of-the-tongue and SetOps. Promptriever also sees lower than average performance for Theorem Retrieval, Legal QA, StackExchange; GTE Qwen 1.5B also sees lower performance on Theorem Retrieval and Legal QA; while GTE Qwen 7B also has lower performance on StackExchange. Looking at the tasks where all models performed worse than average on nDCG, SetOps and Tipof-the-tongue, we find two very different datasets. Tip-of-the-tongue has long multi-part queries with minimal keyword overlap with relevant documents, while SetOps has the shortest queries in the collection which often contain exact or near exact terms from the documents. One thing that might explain the poor nDCG performance for both is that keyword matching is likely not enough to do well on both of them, but over reliance on keywords can lead to poor outcomes for both. In Tip-of-the-tongue, there may not always be lot of term overlap between the query and relevant document, but there are lot of terms which might show up in movie descriptions. Things like genres, time periods, etc. If model focuses too much on these superficial matches without understanding the larger picture the query is painting, it would often be led astray. SetOps seems to not fit in the same category as many of the terms, such as entities or time periods, are very similar between the query and documents, but as the queries have set operations often matching just these aspects would lead to incorrect documents. For example, if the query is movies from the 90s that arent action movies it is likely that many retrieval models would find action movies from the 90s if they are not able to understand or represent the negation. The property of often having misleading lexical or semantic overlap could account for the lower scores on these datasets. Another similarity is that both tasks use Wikipedia as corpus. Though this is likely not direct cause, it does mean that the documents are long articles that are split into multiple chunks. Another possible explanation for the models poor performance could be that both tasks have queries that 32 Julian Killingback and Hamed Zamani refer to several aspects of the document and that chunking could lead to relevant aspects being split into separate chunks. This might result in the model finding many chunks for relevant document, but due to the MaxP operation used only the maximum scoring chunk is retained, which might mean documents with only partial match coming from single chunk end up with similar scores to documents with multiple chunk matches. If this is the case, it may indicate that the retrieval models are more capable than they appear, but it still leaves practical problem of how to chunk documents and combine the scores of chunks in robust way. The other datasets in which the models perform poorly do not seem to have clear pattern, as each model seems to struggle in different ways. The exception is Promptriever that struggles on all of the datasets which one of the two Qwen models also struggles with. This might be caused by the lack of diversity in Promptrievers training data, as all three datasets require specific domain knowledge. Code Retrieval, Paper Retrieval, and Clinical Trial have higher than average nDCG across all the best models. These datasets share several characteristics with some of the datasets where models perform badly, such as being in specialized domains and requiring some reasoning to understand. One other observation is that these datasets make up three of the four datasets with the highest number of relevant documents per query. This might mean that the increase in nDCG is not necessarily because the models are stronger in these domains, but that the chance of retrieving something relevant is inherently higher. Moving to R@1000, we find that all three models underperform in Code Retrieval, Clinical Trial, and SetOps. Interestingly, these also make up three of the four datasets with the most relevant documents. potential cause which we mentioned in the per-task results might be that large number of relevant documents likely correlates with more diverse set of relevant documents which causes models to fail as they are only able to target subset of the relevant document space. Otherwise, the collections have minimal in common with each other. It is interesting that the only dataset which all models struggle with for both R@1000 and nDCG is SetOps. Perhaps the large number of possible \"distractor\" documents, which have certain attributes but not others, is to blame, as well as the inability of the models to understand the set-based logic. Each of the best models also struggled with one of these tasks Paper Retrieval, Tip-of-the-tongue, and Theorem Retrieval tasks. As each model had unique task that it struggled with, it is hard to draw many general conclusions. For reasons why each model may have struggled on these particular tasks you can refer to the task-result sections in Section 5.2. The datasets where all of the best models perform well are StackExchange and Legal QA. Both of these have small number of relevant documents which might contribute to the good performance. StackExchange also has fairly small document collection, which may also be contributing factor."
        },
        {
            "title": "6.1.2 Where do the Weaker Models Struggle? Looking at nDCG for the worse performing models,\nwe find that three tasks have below average performance across the board: Code Retrieval, Theorem\nRetrieval, and Tip-of-the-tongue. All three of these datasets have low term overlap and also have\nminimal semantic overlap. This suggests that the models may be overly dependent on semantic or\nlexical matching and are not able to perform higher-level reasoning-based or abstractive retrieval.\nAll but Lion SB 1B also performed below average on Legal QA which also has minimal semantic\nand lexical overlap. Additionally, all the non-sparse models also perform below average on Set Ops\nsuggesting that perhaps they struggle to properly represent the tail entities and aspects that appear\nin many of the queries.",
            "content": "Looking at R@1000 all models do worse than average on Code Retrieval and Clinical Trial with almost all doing worse on Theorem Retrieval with the exception of Snowflake. The worst datasets are shared between the best and worst performing models suggesting that these datasets might have some universally difficult features. Though the better performing models also all underperformed Julian Killingback and Hamed Zamani 33 on Set Ops while only Snowflake performed below average for the second-tier models. The below average performance on Theorem Retrieval for most of the models is interesting given that the other datasets where the models performed worse have larger number of relevant documents while Theorem Retrieval has one of the lowest. With that said, Theorem Retrieval is one of the farthest out of domain for models trained only on MSMARCO and likely features minimal term overlap which is likely major cause for the poor performance. Interestingly, all models but the learned sparse models also perform worse than average on Tip-of-the-tongue."
        },
        {
            "title": "6.3 Limitations of Dataset\nNo dataset is fully representative, and very few are without potential shortcomings. To help promote\nfuture work and be fully transparent, we will go over some of the limitations of our dataset. When\nconstructing our data we made choices to limit the scope to maintain feasibility and produce a\nfocused dataset. These limitations include:",
            "content": "English Only - Our dataset focuses only on tasks in the English language. This was for few reasons: (1) we (the authors) understand English so we can better evaluate the quality of both the datasets selected and the results of our experiments (2) some of the most commonly used evaluation sets are in English so there is historical precedent to use English (3) related to 34 Julian Killingback and Hamed Zamani this point most people in the academic community know English as it is the main language used in publications, which means it is likely the most universal language which allows more people to do analysis of the data (4) likely due to some of the prior reasons there are more data resources in English which made assembling this dataset easier. Uni-Modal - Our dataset focuses on text-only retrieval tasks. We made this choice for few reasons: (1) adding new modalities adds complexity to distributing data which might make it less accessible (2) although new approaches which support multi-modal inputs are becoming more common, the vast majority of retrieval systems are built for text only thus making multi-modal dataset would exclude large numbers of existing and future retrieval systems. Single-Hop - Many of the existing complex retrieval tasks fall into the multi-hop category, though all of the data in our datasets is single-hop. The reason for this is multi-hop questions often require an iterative approach where the steps - rewrite the query, retrieve - are repeated several times until the final answer is obtained. This process is useful for many real world search tasks, but it requires more complexity and as mentioned in the Uni-Modal bullet it would limit the types of models that could be used for our dataset. For this reason, we focused on tasks that could be solved with single retrieval. Note iterative systems can still be applied, they just are not required. Subset of Possible Complex Tasks - Our dataset only contains eight complex tasks, though this only accounts for small subset of all the potential complex retrieval tasks. This is because: (1) it is time consuming to standardize datasets especially given the strong emphasis we gave to produce high-quality document collections for each dataset (2) finding high quality datasets which are complex is hard and there is limited supply; although more datasets are produced every day, evaluating, standardizing, and running benchmarks on them is resource and time intensive (3) there are diminishing returns, more tasks likely mean there are overlaps in task features and thus less useful information is gained with each new task (4) as we would like our benchmark to be used by others, reducing the size of the dataset makes it more likely to be adopted and enables those with minimal computation resources to use it. These limitations are consequences of our decisions and desires for the dataset, there are also some inherent limitations that come from the datasets we choose to include and how we modified them for this benchmark. We will now elucidate any limitations we think those using our dataset should be aware of for each task: Tip-of-the-tongue - This dataset has minimal chance that document marked as relevant is not relevant due to how the data was collected. With that said, there may be chance that another document could plausibly also exactly match the description provided, though this is unlikely. This is because it is assumed only the actual document the user was looking for is correct even if others are equally plausible. Although the marked relative documents are likely correct, they may not always contain enough information to know they are relevant. Additionally, for the short document version the chunking may split relevant information across chunks which may impact retrieval. Also, the chunked version does not have meaningful per-chunk labels as we only have per-document labels, thus all chunks from document are considered relevant. In our experiments we used MaxP to overcome this limitation. For more details on why and how to use MaxP see Section 4.3. StackExchange - This data was derived from BRIGHT [56] so it inherits the issues from BRIGHT though these are minimal. major limitation is the small number of documents collected for each topic area, we partially remedy this by combining the document collections from all topics. However, this does increase the risk of an unlabeled positive, but we believe this risk is small, as each topic is pretty distinct so documents from one topic are unlikely to even be topically relevant to queries Julian Killingback and Hamed Zamani 35 from another topic. Another issue is that we created our own document collection to improve quality and unify the data format, this required aligning our collection with the original collection. For the full documents this was straightforward, but for the chunked collection it was more difficult. We tried to ensure our chunk-level labels were still valid by ensuring an original BRIGHT-chunk was fully encapsulated by one of our chunks. We only kept queries if all relevant chunks could be fully encapsulated which we believe makes our data fully valid. For more details see the section introduce the StackExchange task 3.1. Paper Retrieval - This dataset was derived from DORIS-MAE [65] and most limitations come from the method used for labeling the original dataset. Documents were labeled using GPT 3.5 by checking how many topics and subtopics from the deconstructed query were satisfied. Due to cost, every document was not checked for every query only limited candidate pool. The candidate pool used pooling approach which combined lexical and semantic retrieval models and citation information. The limited scope of judgments could mean that relevant papers are unlabeled and the labeled papers may also have biases towards certain types of models used to create the original candidate pool. Additionally, the authors also exclude papers used as reference when writing the queries from the candidate pool. These documents are likely highly relevant and may not be labeled as such. It is unclear from the released data whether these reference documents were removed entirely from the corpus or whether they are included in the judged documents. The labels produced by GPT 3.5 may also be incorrect at times, though the labels produced were closer to three-rater majority than an original set of individual human labels. This indicates the task may be quite difficult even for humans, but that GPTs ratings are likely reasonable. Beyond the limitation inherited from DORIS-MAE we also did additional filtering of queries and converted the topic and sub-topic judgments to query-level labels. The full procedure is described in Section 3.1. Though our approach is well motivated, it weighs all aspects equally in terms of importance and may be sensitive to slight errors in GPTs labels. Also, the choice to only mark documents as relevant in the binary setting when all aspects are met may be overly harsh and again may be sensitive to small errors in GPTs labeling. Though sensitive per-query, in aggregate these variances likely have minimal impact especially if GPTs mistakes are uniformly distributed. Despite these limitations, the various approaches to source documents for labeling likely minimizes bias and covers large quantity of relevant documents and as mentioned previously, GPTs labels correlated well with human raters suggesting that although imperfect its labels are likely good enough to produce meaningful results. SetOps - This dataset is based on QUEST [38] which uses Wikipedia categories to create \"atomic\" aspects which were combined to produce queries. Each category has set of documents associated with it which was used to find the initial set of relevant documents. In the subset of queries we selected, the relevant documents were also judged by human raters as relevant. Thus, the documents that are marked as relevant have high chance of being relevant and containing the necessary relevant information. Though there is still risk that there are some documents which were not under the initial categories and were excluded for the candidate pool. We believe the risk of this is likely small, and would likely have minimal bias for or against certain types of pages. The use of templates to produce queries based on categories results in some unusual combinations of query aspects. To mitigate this effect we only selecting queries that human raters judged as having high naturalness and fluency. Despite this, some queries may have odd combinations of aspects, but (1) it is hard to know what true distribution of users information needs would include so these queries may be plausible (2) even if they have information needs that are unrealistic they are still valid in testing systems on what is otherwise realistic search task. Clinical Trial - This dataset is based on the TREC Clinical Trial tracks and as such has fairly comprehensive labels. As TREC is experienced in producing high quality test collections the chance 36 Julian Killingback and Hamed Zamani of incorrect labels is low though there may still be some unlabeled relevant documents, these likely would not change the relative rankings of search systems. Our conversion of documents from the original XML to markdown may have resulted in some irregular formatting for edge-cases. The original documents also had some irregularities due to the formatting of the original Clinical Trial text, though this is likely realistic evaluation of real world text. Legal QA - The quality of query-document labels for this dataset is likely high as it is unlikely there are many sections in states statutes that are relevant in answering the same question as this kind of redundancy would make understanding the laws difficult. Still, there may be some sections that are relevant and unlabeled. The labeling was done by legal professionals so their judgments should be sound. To specify the states for each query we added natural language text at the end of each question which is the same for each query (with the exception of the state name changing). This added context may lack naturalness, but is similar to how additional requirements are often added to keyword queries so we believe it is reasonable inclusion. Theorem Retrieval - This dataset is derived from BRIGHTs Theorem retrieval [56] task. The queries are originally from the TheoremQA dataset [10] which the BRIGHT authors rewrite with GPT-4 and manually check to remove superficial term overlap. These queries are then paired with documents from ProofWiki using the name of the theorem which is provided by the original TheoremQA dataset. The authors look for theorems that contain the exact substring and that are in the top 10 results for BM25. The candidate documents are then scored using GPT-4. The authors show that GPT-4 has high correlation with human raters. The methods used by the BRIGHT authors may result in few potential problems, first the GPT rewritten queries may have issues though each query is manually checked which reduces this risk. Second, the alignment and labeling has room to produce some problems though the use of the proof name likely guarantees the relevant theorem is found in most cases. The authors also discard queries which have no relevant labels which would exclude queries where the approaches to find theorems failed. Overall, the approach has enough human checks to produce confident in the results. Code Retrieval - Based on the dataset APPs [24] this is the only task that does not come from an existing retrieval dataset. The original dataset was meant to test code generation but includes number of possible solutions along with code problems. To produce retrieval dataset we treat the code problems as queries and code solutions as relevant documents. As the solutions have all been verified to pass problem-specific tests they are known to be correct, with that said there may be chance one problems code solves one of the other problems. Although many problems use the same techniques, major focus with these types of coding problems is to obscure the exact technique that is necessary, thus the chance the inputs and outputs for two problems is identical and the underlying algorithm are identical is low. Another potential issue is that the code solutions may include problem specific terms that makes the task unrealistic. Our qualitative investigation found that most of the code solutions had minimal term overlap with the questions and usually used short uninformative variable names and did not include documentation. This is confirmed by the low nDCG@10 of BM25. We found one instance of document including the name of the problem, but only saw that one instance. Overall, we believe the nature of the original dataset makes the conversion to retrieval task valid due to the low likelihood of unlabeled positives and minimal lexical similarity between queries and documents."
        },
        {
            "title": "6.4 Citing Direction\nAs our benchmark consists of many other datasets that made our work possible, we ask that\nwhen using our benchmark, in addition to citing this paper, you also cite the original datasets. For\nconvenience, all the citations are cited here [5, 24, 38, 49, 50, 56, 65, 81].",
            "content": "Julian Killingback and Hamed Zamani"
        },
        {
            "title": "7 Conclusion\nIn this work, we compile a diverse and complex set of retrieval tasks to benchmark how a wide\nselection of powerful neural retrieval models perform on complex tasks. We find that across the\nboard even the best models struggle to produce satisfactory nDCG@10 values suggesting that\nadditional work is still needed to have general-purpose retrieval models for complex retrieval\ntasks. Recall is also often low, especially for tasks with several relevant documents. This finding\nimplies that even using powerful reranking models overall ranking performance will still be poor\nin many cases due to weak first-stage candidates. Our experiments give some direction for future\nimprovements and find some key attributes of the most successful models: (1) the best models use\nlarge backbone models that have been trained with vast quantities of data (2) they also use large\namounts of varied retrieval specific data to pretrain the retrieval models though the success of\nPromptriever shows that this is not always necessary for a high quality model (3) there is reason to\nbelieve that using an instruction tuned backbone LLM results in better performance than using the\nbase LLM, though more results are still needed to exactly quantify this effect (4) they are capable of\nmodifying their search behavior based on natural language instructions. We hope that by releasing\nCRUMB, a high-quality diverse and complex retrieval benchmark, we can provide a meaningful\nmeasure of retrieval progress and spur new innovations.",
            "content": "Acknowledgments The authors thank Sriharsha Hatwar for his substantial help in producing the data for the StackExchange task. He helped to download and process the web pages that were used for the document collection, as well as assisting with aligning the document chunks and manually verifying the alignment quality. We thank Varun Pininty and Madhav Jhawar for their help adding and checking citations, and Harrie Oosterhuis for his early feedback. Furthermore, we acknowledge the use of the large language model Gemini 2.5 Pro for various purposes throughout the development and drafting of this work, including assisting with figure creation, table creation and design, code generation, and paper feedback. This work was supported in part by the Center for Intelligent Information Retrieval, in part by NSF grant number 2402873, in part by the Office of Naval Research contract number N000142412612, and in part by the NSF Graduate Research Fellowships Program (GRFP) Award number 1938059. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. Prompts for LLM Query Rewriting This section provides the detailed prompts used for the LLM query rewriting techniques discussed in Section 4.2.2. Dataset Examples This section provides illustrative examples (query and relevant document chunk) for each dataset in our benchmark. References [1] 2025. https://openai.com/index/introducing-o3-and-o4-mini/ [2] Marwah Alaofi, Luke Gallagher, Dana Mckay, Lauren L. Saling, Mark Sanderson, Falk Scholer, Damiano Spina, and Ryen W. White. 2022. Where Do Queries Come From?. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, Madrid Spain, 28502862. doi:10.1145/3477495.3531711 [3] James Allan, Donna Harman, Evangelos Kanoulas, Dan Li, Christophe Van Gysel, and Ellen M. Voorhees. 2017. TREC 2017 Common Core Track Overview. In Proceedings of The Twenty-Sixth Text REtrieval Conference, TREC 2017, Julian Killingback and Hamed Zamani Query-as-Doc Prompt Follow the below steps to produce document that is relevant to the main information request based on what document type is requested Make sure to do each step: 1. Identify the type of document the user is trying to retrieve (and thus what should be output). 2. Analyze the request, what kinds of information is the user looking for? What kinds of documents would exist that would satisfy the request? 3. Do additional brainstorming about what relevant document would be. 4. You must do the brainstorming and analysis steps! Once the full analysis and brainstorming steps have been completed output ##final output followed by the final document (and no additional comments, information, or anything else) on new line which would be relevant based on the information request. The document should be in the style and format mentioned in the first line of the information request. For example, if the information request is looking for relevant Wikipedia pages the document should resemble the style and formatting (in markdown) of Wikipedia page. Include the same amount of text as real document would contain, do not include placeholders. Only output single document even if there are multiple that might be relevant. Information Request: <instruction> <query> Fig. 7. Prompt used for Query-as-Doc query rewriting. Query-as-Answer Prompt Follow the below steps to produce an answer to an information request make sure to do each step: 1. Analyze the request, what kinds of information is the user looking for? What kinds of information would complete answer have? 2. Do additional brainstorming about what might be relevant and what might not be. 3. You must do the brainstorming and analysis steps! Once the full analysis and brainstorming steps have been completed output ##final output followed by the final answer (and no additional comments, information, or anything else) on new line which should answer the information request. Information Request: <instruction> <query> Fig. 8. Prompt used for Query-as-Answer query rewriting. Gaithersburg, Maryland, USA, November 15-17, 2017 (NIST Special Publication, Vol. 500-324), Ellen M. Voorhees and Angela Ellis (Eds.). National Institute of Standards and Technology (NIST). https://trec.nist.gov/pubs/trec26/papers/ Overview-CC.pdf Julian Killingback and Hamed Zamani Query-as-Reasoning-Step Prompt Follow the below steps to produce an answer to an information request make sure to do each step: 1. Identify the essential problem in the information request. 2. Think step by step to reason about what should be included in relevant documents. 3. Draft an answer. Information Request: <instruction> <query> Fig. 9. Prompt used for Query-as-Reasoning-Step query rewriting. [4] Jaime Arguello, Samarth Bhargav, Fernando Diaz, Evangelos Kanoulas, To Eun Kim, Yifan He, and Bhaskar Mitra. 2025. Overview of the TREC 2024 Tip-of-the-Tongue Track. In Proceedings of the Thirty-Third Text REtrieval Conference. [5] Jaime Arguello, Samarth Bhargav, Fernando Diaz, Evangelos Kanoulas, and Bhaskar Mitra. 2023. Overview of the TREC 2023 Tip-of-the-Tongue Track. In The Thirty-Second Text REtrieval Conference Proceedings (TREC 2023), Gaithersburg, MD, USA, November 14-17, 2023 (NIST Special Publication, Vol. 500-xxx), Ian Soboroff and Angela Ellis (Eds.). National Institute of Standards and Technology (NIST). https://trec.nist.gov/pubs/trec32/papers/Overview_tot.pdf [6] Jaime Arguello, Adam Ferguson, Emery Fine, Bhaskar Mitra, Hamed Zamani, and Fernando Diaz. 2021. Tip of the Tongue Known-Item Retrieval: Case Study in Movie Identification. In CHIIR 21: ACM SIGIR Conference on Human Information Interaction and Retrieval, Canberra, ACT, Australia, March 14-19, 2021, Falk Scholer, Paul Thomas, David Elsweiler, Hideo Joho, Noriko Kando, and Catherine Smith (Eds.). ACM, 514. doi:10.1145/3406522. [7] Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. 2023. Task-aware Retrieval with Instructions. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 36503675. doi:10.18653/V1/2023.FINDINGS-ACL.225 [8] Adrien Barbaresi. 2021. Trafilatura: Web Scraping Library and Command-Line Tool for Text Discovery and Extraction. In Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations. Association for Computational Linguistics, 122131. https://aclanthology.org/2021.acl-demo.15 [9] Shariq Bashir and Andreas Rauber. 2011. On the relationship between query characteristics and IR functions retrieval bias. Journal of the American Society for Information Science and Technology 62, 8 (2011), 15151532. doi:10.1002/asi.21549 _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.21549. [10] Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. 2023. TheoremQA: Theorem-driven Question Answering Dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 78897901. doi:10.18653/v1/2023.emnlp-main.489 [11] Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Yang Wang. 2020. HybridQA: Dataset of Multi-Hop Question Answering over Tabular and Textual Data. In Findings of the Association for Computational Linguistics: EMNLP 2020, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 10261036. doi:10.18653/v1/2020.findings-emnlp.91 [12] Kevyn Collins-Thompson, Paul N. Bennett, Fernando Diaz, Charlie Clarke, and Ellen M. Voorhees. 2013. TREC 2013 Web Track Overview. In Proceedings of The Twenty-Second Text REtrieval Conference, TREC 2013, Gaithersburg, Maryland, USA, November 19-22, 2013 (NIST Special Publication, Vol. 500-302), Ellen M. Voorhees (Ed.). National Institute of Standards and Technology (NIST). http://trec.nist.gov/pubs/trec22/papers/WEB.OVERVIEW.pdf [13] Kevyn Collins-Thompson, Craig Macdonald, Paul N. Bennett, Fernando Diaz, and Ellen M. Voorhees. 2014. TREC 2014 Web Track Overview. In Proceedings of The Twenty-Third Text REtrieval Conference, TREC 2014, Gaithersburg, Maryland, USA, November 19-21, 2014 (NIST Special Publication, Vol. 500-308), Ellen M. Voorhees and Angela Ellis (Eds.). National Institute of Standards and Technology (NIST). http://trec.nist.gov/pubs/trec23/papers/overview-web.pdf [14] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2020. Overview of the TREC 2020 Deep Learning Track. In Proceedings of the Twenty-Ninth Text REtrieval Conference, TREC 2020, Virtual Event [Gaithersburg, Maryland, 40 Julian Killingback and Hamed Zamani Table 11. Example query and relevant passage for the Clinical Trial task."
        },
        {
            "title": "Query",
            "content": "Patient is 45yearold man with history of anaplastic astrocytoma of the spine complicated by severe lower extremity weakness and urinary retention s/p Foley catheter, highdose steroids, hypertension, and chronic pain. The tumor is located in the TL spine, unresectable anaplastic astrocytoma s/p radiation. Complicated by progressive lower extremity weakness and urinary retention. Patient initially presented with RLE weakness where his right knee gave out with difficulty walking and right anterior thigh numbness. MRI showed spinal cord conus mass which was biopsied and found to be anaplastic astrocytoma. Therapy included field radiation t10 l1 followed by 11 cycles of temozolomide 7 days on and 7 days off. This was followed by CPT 11 Weekly x4 with Avastin Q2 weeks/ 2 weeks rest and repeat cycle."
        },
        {
            "title": "Document",
            "content": "# Phase II Trial of Paclitaxel and Topotecan With Filgrastim in Patients With Recurrent or Refractory Glioblastoma Multiforme or Anaplastic Astrocytoma ## Eligibility gender: All minimum age: 18 Years maximum age: N/A healthy volunteers: No ## Conditions Brain and Central Nervous System Tumors ## Interventions Type: Biological, Name: filgrastim Type: Drug, Name: paclitaxel Type: Drug, Name: topotecan hydrochloride ## Criteria DISEASE CHARACTERISTICS: Biopsy proven glioblastoma multiforme or anaplastic astrocytoma Central pathologic review at DartmouthHitchcock Medical Center, including assay for tumor p53 expression No anaplastic oligodendroglioma No mixed oligodendroastrocytoma Recurrent or progressive disease following radiotherapy documented by CT or MRI within 2 weeks of entryPATIENT CHARACTERISTICS: Age: 18 and over Performance status: Karnofsky 60% USA], November 16-20, 2020 (NIST Special Publication, Vol. 1266), Ellen M. Voorhees and Angela Ellis (Eds.). National Institute of Standards and Technology (NIST). https://trec.nist.gov/pubs/trec29/papers/OVERVIEW.DL.pdf [15] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. 2020. Overview of the TREC 2019 deep learning track. CoRR abs/2003.07820 (2020). arXiv:2003.07820 https://arxiv.org/abs/2003.07820 [16] Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with Contextual Neural Language Modeling. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019, Benjamin Piwowarski, Max Chevalier, Ã‰ric Gaussier, Yoelle Maarek, Jian-Yun Nie, and Falk Scholer (Eds.). ACM, 985988. doi:10.1145/3331184.3331303 [17] Zhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang. 2023. Promptagator: Few-shot Dense Retrieval From 8 Examples. In The Eleventh International Julian Killingback and Hamed Zamani 41 Table 12. Example query and relevant passage for the Code Retrieval task."
        },
        {
            "title": "Query",
            "content": "On the way to school, Karen became fixated on the puzzle game on her phone! [Image] The game is played as follows. In each level, you have grid with rows and columns. Each cell originally contains the number 0. One move consists of choosing one row or column, and adding 1 to all of the cells in that row or column. To win the level, after all the moves, the number in the cell at the ith row and jth column should be equal to g_{i}, j. Karen is stuck on one level, and wants to know way to beat this level using the minimum number of moves. Please, help her with this task!"
        },
        {
            "title": "Document",
            "content": "import sys #sys.stdin=open(\"data.txt\") input=sys.stdin.readline n,m=list(map(int,input().split())) realg=[list(map(int,input().split())) for _ in range(n)] g=[[0]m for _ in range(n)] ans=[] # get differences f1=min([realg[0][i] for in range(m)]) for in range(m): for _ in range(realg[0][i]f1): ans.append(\"col % for in range(n): g[j][i]+=realg[0][i]f1 f2=min([realg[i][0] for in range(n)]) for in range(n): for _ in range(realg[i][0]f2): ans.append(\"row % for in range(m): g[i][j]+=realg[i][0]f2 [truncated for brevity] 42 Julian Killingback and Hamed Zamani Table 13. Example query and relevant passage for the Legal QA task."
        },
        {
            "title": "Query",
            "content": "Are eviction cases first heard in high court? In the state of Tennessee"
        },
        {
            "title": "Document",
            "content": "# 2021 Tennessee Code ## Title 29 Remedies and Special Proceedings ### Chapter 18 Forcible Entry and Detainer #### 2918106. Alternative Actions Where the action is to recover real property, ejectment, or forcible or unlawful entry or detainer may be brought. Code 1858, 2750; Shan., 4441; Code 1932, 8567; T.C.A. (orig. ed.), 231606. #### 2918107. Jurisdiction of General Sessions Judge All cases of forcible entry and detainer, forcible detainer, and unlawful detainer, may be tried before any one (1) judge of the court of general sessions of the county in which the acts are committed, who shall decide the particular case, and all questions of law and fact arising. Code 1858, 3346 (deriv. Acts 18411842, ch. 186, 1); Acts 1879, ch. 23; Shan., 5095; Code 1932, 9249; impl. am. Acts 1979, ch. 68, 3; T.C.A. (orig. ed.), 231607. #### 2918108. Original Jurisdiction of Circuit Court The action for the recovery of the possession of land, given in this chapter, may also be originally instituted in the circuit court, the same forms being substantially pursued as those prescribed, the process being issued by the clerk, the plaintiff first giving bond and security to answer costs and damages as provided in 2918111. Code 1858, 3366 (deriv. Acts 18411842, ch. 186, 8); Shan., 5115; Code 1932, 9270; T.C.A. (orig. ed.), 231608. Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview. net/forum?id=gmL46YMpu2J [18] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. CoRR abs/2501.12948 (2025). arXiv:2501.12948 doi:10.48550/ARXIV.2501.12948 [19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, 41714186. doi:10.18653/V1/N19-1423 [20] Abhimanyu Dubey et al. 2024. The Llama 3 Herd of Models. CoRR abs/2407.21783 (2024). arXiv:2407.21783 doi:10. 48550/ARXIV.2407. [21] Kenneth C. Enevoldsen, Isaac Chung, Imene Kerboua, MÃ¡rton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzeminski, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Diganta Misra, Shreeya Dhakal, Jonathan RystrÃ¸m, Roman Solomatin, Ã–mer Veysel Ã‡agatan, Akash Kundu, and et al. Julian Killingback and Hamed Zamani 43 Table 14. Example query and relevant passage for the Paper Retrieval task."
        },
        {
            "title": "Query",
            "content": "My goal is to develop learning model that can handle multiple tasks simultaneously. This proposed learning model will function as submodel selector, meaning that when presented with new task, it will determine the most suitable submodel to learn this task. Additionally, my learning model could also operate as submodel constructor. If it determines that no existing submodel can learn the task, it will generate innovative model architectures to construct new submodel suitable for the task. At present, am considering the application of combination of reinforcement learning and model architecture search algorithms to achieve this. To effectively evaluate the performance, require comprehensive benchmark that includes variety of datasets that can be used as different subtasks. My aim is for my learning model to achieve stateoftheart performance on this benchmark. In general, am open to any methodologies that enhance the performance and adaptability of my learning model across different problem domains."
        },
        {
            "title": "Document",
            "content": "# Continual Learning with Adaptive Weights (CLAW) ## Categories Machine Learning ## Abstract Approaches to continual learning aim to successfully learn set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying shared component and taskspecific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of tasktransfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves stateoftheart performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting. 2025. MMTEB: Massive Multilingual Text Embedding Benchmark. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. https://openreview.net/forum?id=zl3pfz4VCV [22] Thibault Formal, Benjamin Piwowarski, and StÃ©phane Clinchant. 2021. SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking. In SIGIR 21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021, Fernando Diaz, Chirag Shah, Torsten Suel, Pablo Castells, Rosie Jones, and Tetsuya Sakai (Eds.). ACM, 22882292. doi:10.1145/3404835.3463098 [23] Simeng Han, Frank Palma Gomez, Tu Vu, Zefei Li, Daniel Cer, Hansi Zeng, Chris Tar, Arman Cohan, and Gustavo Hernandez Abrego. 2025. ATEB: Evaluating and Improving Advanced NLP Tasks for Text Embedding Models. doi:10.48550/arXiv.2502.16766 arXiv:2502.16766 [cs]. [24] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. 2021. Measuring Coding Challenge Competence With APPS. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, Joaquin Vanschoren and Sai-Kit Yeung (Eds.). https://datasets-benchmarksproceedings.neurips.cc/paper/2021/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2.html 44 Julian Killingback and Hamed Zamani Table 15. Example query and relevant passage for the SetOps task."
        },
        {
            "title": "Query",
            "content": "German spy comedy films, or 2000s comedydrama mystery films."
        },
        {
            "title": "Document",
            "content": "# The Mimosa Wants to Blossom Too The Mimosa Wants to Blossom Too ''(German:Auch Mimosen wollen blÃƒÄ³hen'') is 1976 West German comedy spy film directed by Helmut Meewes and starring Curd JÃƒÄ³rgens, Eric Pohlmann and Horst Frank. The film's sets were designed by the art director Peter Rothe. Curd JÃƒÄ³rgens as Josef Popov Eric Pohlmann as Iwan Pederenko Horst Frank as Oberst Oschenko Susi Nicoletti as Emily Hopkins Heinz Reincke as Obdachloser Barbara Nielsen as Ludmilla Chiquita Gordon as Miss Ly Erich Padalewski as Mr. Gate Ljuba Welitsch as Lady Shots Harry Hardt as Sir Shots Bock, HansMichael & Bergfelder, Tim. The Concise CineGraph. Encyclopedia of German Cinema. Berghahn Books, 2009. [25] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot Learning with Retrieval Augmented Language Models. J. Mach. Learn. Res. 24 (2023), 251:1251:43. https://jmlr.org/papers/v24/23-0037.html [26] Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Bendersky. 2023. Query Expansion by Prompting Large Language Models. CoRR abs/2305.03653 (2023). arXiv:2305.03653 doi:10.48550/ARXIV.2305.03653 [27] Kalervo JÃ¤rvelin and Jaana KekÃ¤lÃ¤inen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst. 20, 4 (2002), 422446. doi:10.1145/582415. [28] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning. CoRR abs/2503.09516 (2025). arXiv:2503.09516 doi:10.48550/ARXIV.2503.09516 [29] Haocheng Ju and Bin Dong. 2025. MIRB: Mathematical Information Retrieval Benchmark. CoRR abs/2505.15585 (2025). arXiv:2505.15585 doi:10.48550/ARXIV.2505.15585 [30] Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre RamÃ©, Morgane RiviÃ¨re, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-Bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, GaÃ«l Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, RÃ³bert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, AndrÃ¡s GyÃ¶rgy, AndrÃ© Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucinska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Julian Killingback and Hamed Zamani Table 16. Example query and relevant passage for the StackExchange task."
        },
        {
            "title": "Query",
            "content": "Does the genetic sequence of SARSCoV2 end with 33 A's? Looking at the DNA (or RNA?) sequence of the Covid19 virus here: https:// www.ncbi.nlm.nih.gov/nuccore/MN908947.3 notice it ends in 33 a's. Does the virus really look like that, or is it some artifact of the sequencing process, or some sort of padding to round out numbers? Here's the last few lines: 29761 acagtgaaca atgctaggga gagctgccta tatggaagag ccctaatgtg taaaattaat 29821 tttagtagtg ctatccccat gtgattttaa tagcttctta ggagaatgac aaaaaaaaaa 29881 aaaaaaaaaa aaaaaaaaaa aaa"
        },
        {
            "title": "Document",
            "content": "Polyadenylation is the addition of poly(A) tail to an RNA transcript, typically messenger RNA (mRNA). The poly(A) tail consists of multiple adenosine monophosphates; in other words, it is stretch of RNA that has only adenine bases. In eukaryotes, polyadenylation is part of the process that produces mature mRNA for translation. In many bacteria, the poly(A) tail promotes degradation of the mRNA. It, therefore, forms part of the larger process of gene expression. The process of polyadenylation begins as the transcription of gene terminates. The 3Ã¢Ä‚Å¡most segment of the newly made premRNA is first cleaved off by set of proteins; these proteins then synthesize the poly(A) tail at the RNA's 3Ã¢Ä‚Å¡ end. In some genes these proteins add poly( A) tail at one of several possible sites. Therefore, polyadenylation can produce more than one transcript from single gene (alternative polyadenylation), similar to alternative splicing. The poly(A) tail is important for the nuclear export, translation and stability of mRNA. The tail is shortened over time, and, when it is short enough, the mRNA is enzymatically degraded. However, in few cell types, mRNAs with short poly(A) tails are stored for later activation by re polyadenylation in the cytosol. In contrast, when polyadenylation occurs in bacteria, it promotes RNA degradation. This is also sometimes the case for eukaryotic noncoding RNAs. mRNA molecules in both prokaryotes and eukaryotes have polyadenylated 3Ã¢Ä‚Å¡ends, with the prokaryotic poly(A) tails generally shorter and fewer mRNA molecules polyadenylated. Ju-yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim PÃµder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, ClÃ©ment Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry (Dima) Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and LÃ©onard Hussenot. 2025. Gemma 3 Technical Report. CoRR abs/2503.19786 (2025). arXiv:2503.19786 doi:10.48550/ARXIV.2503.19786 [31] Julian Killingback, Hansi Zeng, and Hamed Zamani. 2025. Hypencoder: Hypernetworks for Information Retrieval. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval (Padua, Italy) (SIGIR 25). Association for Computing Machinery, New York, NY, USA, 2372Ã¢Ä‚Åž2383. doi:10.1145/ 46 Julian Killingback and Hamed Zamani Table 17. Example query and relevant passage for the Theorem Retrieval task."
        },
        {
            "title": "Query",
            "content": "Imagine you have digital scale that can measure the weight of an infinite number of infinitely small digital dots. Each dot can either be on or off, and their weights are determined by specific pattern: the first dot weighs 1/4 gram, the second dot weighs 1/16 grams, the third dot weighs 1/64 grams, and so on, with each subsequent dot weighing exactly onefourth the weight of the previous dot. If you can create any combination of these dots being on (weighing their specified amount) or off (weighing nothing), and you can also add whole number of grams to the scale ( from set of weights that are whole grams), what would be the total possible weight measured by the scale? (specifically, Lebesgue measure of this sum)"
        },
        {
            "title": "Document",
            "content": "section{Lebesgue Measure is Diffuse} Tags: Measure Theory, Lebesgue Measure, Diffuse Measures begin{theorem} Let $lambda^n$ be Lebesgue measure on $R^n$. Then $lambda^n$ is diffuse measure. end{theorem} begin{proof} singleton $set {mathbf x} subseteq R^n$ is seen to be closed by combining: :Euclidean Space is Complete Metric Space :Metric Space is Hausdorff :Corollary to Compact Subspace of Hausdorff Space is Closed Hence by Closed Set Measurable in Borel SigmaAlgebra: :$set {mathbf x} in map BB {R^n}$ where $map BB {R^n}$ is the Borel $sigma$algebra on $R^n$. Write $mathbf + epsilon = tuple {x_1 + epsilon, ldots, x_n + epsilon}$ for $epsilon > 0$. Then: :$ds set {mathbf x} = bigcap_{m mathop in N} horectr {mathbf x} {mathbf + frac 1 m}$ where $horectr {mathbf x} {mathbf + dfrac 1 m}$ is halfopen $n$rectangle. {{handwavingjustify equality}} By definition of Lebesgue measure, we have (for all $m in N$): :$ds map {lambda^n} {horectr {mathbf x} {mathbf + frac 1 m} } = prod_{i mathop = 1}^n frac 1 = m^{n}$ From Characterization of Measures, it follows that: :$ds map {lambda^n} {set {mathbf x} } = lim_{m mathop to infty} m^{n}$ which equals $0$ from Sequence of Powers of Reciprocals is Null Sequence. Therefore, for each $mathbf in R^n$: :$map {lambda^n} {set {mathbf x} } = 0$ that is, $lambda^n$ is diffuse measure. {{qed}} [truncated for brevity] Julian Killingback and Hamed Zamani 47 Table 18. Example query and relevant passage for the Tip-of-the-tongue task."
        },
        {
            "title": "Query",
            "content": "Cop's son needs blood transfusion from an inmate . cop's son is very sick and needs certain type of blood transfusion. He finds out an inmate has the same type and once agreed the inmate takes advantage of the situation in the hospital and escapes. Then long chasing odyssey till the end when the kid finally gets cured"
        },
        {
            "title": "Document",
            "content": "# Desperate Measures (film) ## Plot Conner and Hawkins make their way to McCabe and convince him to let them inside so that Hawkins can attend to Matt. As McCabe watches Conner on the security cameras, he realizes that his nemesis is truly devoted father, and develops grudging respect for him. Conner intervenes when McCabe is about to ambush Cassidy and his SWAT team with set of tanks of cyclopropane. Cassidy is furious that Conner continues to aid an escaped convict, while McCabe is angry that Conner foiled his plan. He kidnaps Matt and descends to the sublevels of the building. Matt tries to wound McCabe to give his father better chance; impressed, McCabe spares Matt and leaves him at the hospital for Conner to find. McCabe then escapes into San Francisco, where he steals car. Conner chases McCabe to bridge, still needing him captured alive. Cassidy and his men arrive in helicopter and sniper opens fire. Conner again shields McCabe and is wounded in the arm. McCabe attempts to flee, but Conner is determined not to let him go. Conner wounds McCabe, sending him off the bridge and into the bay. Conner then dives in and saves him. Back in the hospital, wounded McCabe agrees to the transplant, which saves Matt's life. Even though his career is clearly over, Conner is overjoyed that his son will live. McCabe is informed by guard that the surgery went well. As the bed reclines upwards and McCabe looks at the guard menacingly, the guard suddenly realizes that his gun is gone. McCabe holds it over the guard and asks, \"What kind of car do you have?\". 3726302.3729983 [32] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: Benchmark for Question Answering Research. Transactions of the Association for Computational Linguistics 7 (2019), 452466. doi:10.1162/tacl_ a_00276 [33] Lei Li, Xiao Zhou, and Zheng Liu. 2025. R2MED: Benchmark for Reasoning-Driven Medical Retrieval. CoRR abs/2505.14558 (2025). arXiv:2505.14558 doi:10.48550/ARXIV.2505.14558 [34] Xiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia, Hao Zhang, Xinyi Dai, Yasheng Wang, and Ruiming Tang. 2025. CoIR: Comprehensive Benchmark for Code Information Retrieval Models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 2207422091. https://aclanthology.org/2025.acl-long.1072/ [35] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards General Text Embeddings with Multi-stage Contrastive Learning. CoRR abs/2308.03281 (2023). arXiv:2308.03281 doi:10.48550/ ARXIV.2308.03281 [36] Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: Python Toolkit for Reproducible Information Retrieval Research with Sparse and Dense Representations. In Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 48 2021). 23562362. Julian Killingback and Hamed Zamani [37] Kevin Lin, Kyle Lo, Joseph Gonzalez, and Dan Klein. 2023. Decomposing Complex Queries for Tip-of-the-tongue Retrieval. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 55215533. doi:10.18653/v1/2023.findingsemnlp. [38] Chaitanya Malaviya, Peter Shaw, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2023. QUEST: Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 1403214047. doi:10.18653/V1/2023.ACL-LONG.784 [39] Christopher Malon and Bing Bai. 2020. Generating Followup Questions for Interpretable Multi-hop Question Answering. CoRR abs/2002.12344 (2020). arXiv:2002.12344 https://arxiv.org/abs/2002.12344 [40] Yazdan Mansourian and Nigel Ford. 2007. Web searchers attributions of success and failure: an empirical study. Journal of Documentation 63, 5 (Sept. 2007), 659679. doi:10.1108/00220410710827745 [41] Carlo Merola and Jaspinder Singh. 2025. Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation. CoRR abs/2504.19754 (2025). arXiv:2504.19754 doi:10.48550/ARXIV.2504.19754 [42] Niklas Muennighoff, Nouamane Tazi, LoÃ¯c Magne, and Nils Reimers. 2023. MTEB: Massive Text Embedding Benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, Andreas Vlachos and Isabelle Augenstein (Eds.). Association for Computational Linguistics, 20062029. doi:10.18653/V1/2023.EACL-MAIN. [43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: Human Generated MAchine Reading COmprehension Dataset. In Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016 (CEUR Workshop Proceedings, Vol. 1773), Tarek Richard Besold, Antoine Bordes, Artur S. dAvila Garcez, and Greg Wayne (Eds.). CEUR-WS.org. https://ceurws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf [44] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas KÃ¶pf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlchÃ©-Buc, Emily B. Fox, and Roman Garnett (Eds.). 80248035. https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740Abstract.html [45] Loc Pham, Tung Luu, Thu Vo, Minh Nguyen, and Viet Hoang. 2025. VN-MTEB: Vietnamese Massive Text Embedding Benchmark. arXiv:2507.21500 [cs.CL] https://arxiv.org/abs/2507.21500 [46] Rafal Poswiata, Slawomir Dadas, and Michal Perelkiewicz. 2024. PL-MTEB: Polish Massive Text Embedding Benchmark. CoRR abs/2405.10138 (2024). arXiv:2405.10138 doi:10.48550/ARXIV.2405. [47] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. https://arxiv.org/abs/1908.10084 [48] Snowflake AI Research. 2024. Arctic embed: The best open source embedding models. Snowflake Blog. https://www. snowflake.com/blog/arctic-embed-open-source-embedding-models/ Accessed: June 11, 2025. See also arXiv:2412.04506 for technical details.. [49] Kirk Roberts, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, Kyle Lo, Ian Soboroff, Ellen M. Voorhees, Lucy Lu Wang, and William R. Hersh. 2021. Searching for scientific evidence in pandemic: An overview of TREC-COVID. J. Biomed. Informatics 121 (2021), 103865. doi:10.1016/J.JBI.2021.103865 [50] Kirk Roberts, Dina Demner-Fushman, Ellen M. Voorhees, Steven Bedrick, and William R. Hersh. 2022. Overview of the TREC 2022 Clinical Trials Track. In Proceedings of the Thirty-First Text REtrieval Conference, TREC 2022, online, November 15-19, 2022 (NIST Special Publication, Vol. 500-338), Ian Soboroff and Angela Ellis (Eds.). National Institute of Standards and Technology (NIST). https://trec.nist.gov/pubs/trec31/papers/Overview_trials.pdf [51] Stephen E. Robertson and Ian Soboroff. 2001. The TREC 2001 Filtering Track Report. In Proceedings of The Tenth Text REtrieval Conference, TREC 2001, Gaithersburg, Maryland, USA, November 13-16, 2001 (NIST Special Publication, Vol. 500-250), Ellen M. Voorhees and Donna K. Harman (Eds.). National Institute of Standards and Technology (NIST). http://trec.nist.gov/pubs/trec10/papers/filtering_track.pdf [52] S. E. Robertson and S. Walker. 1994. Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development Julian Killingback and Hamed Zamani 49 in Information Retrieval (Dublin, Ireland) (SIGIR 94). Springer-Verlag, Berlin, Heidelberg, 232Ã¢Ä‚Åž241. [53] Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen-tau Yih, Pang Wei Koh, and Luke Zettlemoyer. 2025. ReasonIR: Training Retrievers for Reasoning Tasks. CoRR abs/2504.20595 (2025). arXiv:2504.20595 doi:10.48550/ARXIV.2504.20595 [54] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. 2025. R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning. CoRR abs/2503.05592 (2025). arXiv:2503.05592 doi:10.48550/ARXIV.2503.05592 [55] Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2023. One Embedder, Any Task: Instruction-Finetuned Text Embeddings. In Findings of the Association for Computational Linguistics: ACL 2023, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 11021121. doi:10.18653/v1/2023.findings-acl.71 [56] Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan Ã–. Arik, Danqi Chen, and Tao Yu. 2025. BRIGHT: Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. https://openreview.net/forum?id=ykuc5q381b [57] Weiwei Sun, Zhengliang Shi, Wu Long, Lingyong Yan, Xinyu Ma, Yiding Liu, Min Cao, Dawei Yin, and Zhaochun Ren. 2024. MAIR: Massive Benchmark for Evaluating Instructed Retrieval. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, 1404414067. doi:10.18653/V1/2024.EMNLP-MAIN. [58] Alon Talmor and Jonathan Berant. 2018. The Web as Knowledge-Base for Answering Complex Questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), Marilyn Walker, Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, New Orleans, Louisiana, 641651. doi:10.18653/v1/N18-1059 [59] Katherine Thai, Yapei Chang, Kalpesh Krishna, and Mohit Iyyer. 2022. RELiC: Retrieving Evidence for Literary Claims. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 75007518. doi:10.18653/v1/2022.acl-long.517 [60] Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). https://openreview.net/forum?id=wCu6T5xFjeJ [61] Johanne R. Trippas, Sara Fahad Dawood Al Lawati, Joel Mackenzie, and Luke Gallagher. 2024. What do Users Really Ask Large Language Models? An Initial Log Analysis of Google Bard Interactions in the Wild. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang (Eds.). ACM, 27032707. doi:10.1145/3626772.3657914 [62] Coen van den Elsen, Francien Barkhof, Thijmen Nijdam, Simon Lupart, and Mohammad Aliannejadi. 2025. Reproducing NevIR: Negation in Neural Information Retrieval. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2025, Padua, Italy, July 13-18, 2025, Nicola Ferro, Maria Maistro, Gabriella Pasi, Omar Alonso, Andrew Trotman, and Suzan Verberne (Eds.). ACM, 33463356. doi:10.1145/3726302. 3730294 [63] Srihari Vemuru, Eric John, and Shrisha Rao. 2021. Handling Complex Queries Using Query Trees. doi:10.36227/techrxiv. 14845212.v [64] Ellen Voorhees. 2005. Overview of the TREC 2004 Robust Retrieval Track. doi:10.6028/NIST.SP.500-261 [65] Jianyou (Andre) Wang, Kaicheng Wang, Xiaoyue Wang, Prudhviraj Naidu, Leon Bergen, and Ramamohan Paturi. 2023. Scientific Document Retrieval using Multi-level Aspect-based Queries. In Advances in Neural Information Processing Systems, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc., 3840438419. https://proceedings.neurips.cc/paper_files/paper/2023/file/78f9c04bdcb06f1ada3902912d8b64ba-PaperDatasets_and_Benchmarks.pdf [66] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 610, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 94149423. doi:10.18653/V1/2023.EMNLP-MAIN.585 [67] Xiaoyue Wang, Jianyou Wang, Weili Cao, Kaicheng Wang, Ramamohan Paturi, and Leon Bergen. 2024. BIRCO: Benchmark of Information Retrieval Tasks with Complex Objectives. CoRR abs/2402.14151 (2024). arXiv:2402.14151 doi:10.48550/ARXIV.2402.14151 50 Julian Killingback and Hamed Zamani [68] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. (2023). https: //openreview.net/forum?id=1PL1NIMMrw [69] Albatool Wazzan, Stephen MacNeil, and Richard Souvenir. 2024. Comparing Traditional and LLM-based Search for Image Geolocation. In Proceedings of the 2024 ACM SIGIR Conference on Human Information Interaction and Retrieval, CHIIR 2024, Sheffield, United Kingdom, March 10-14, 2024, Paul D. Clough, Morgan Harvey, and Frank Hopfgartner (Eds.). ACM, 291302. doi:10.1145/3627508.3638305 [70] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-AbstractConference.html [71] Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing Datasets for Multi-hop Reading Comprehension Across Documents. Transactions of the Association for Computational Linguistics 6 (2018), 287302. doi:10.1162/tacl_a_00021 [72] Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn J. Lawrie, and Luca Soldaini. 2025. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, Luis Chiruzzo, Alan Ritter, and Lu Wang (Eds.). Association for Computational Linguistics, 1192611942. doi:10.18653/V1/2025.NAACL-LONG.597 [73] Orion Weller, Benjamin Van Durme, Dawn J. Lawrie, Ashwin Paranjape, Yuhao Zhang, and Jack Hessel. 2024. Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models. CoRR abs/2409.11136 (2024). arXiv:2409.11136 doi:10.48550/ARXIV.2409.11136 [74] Orion Weller, Dawn J. Lawrie, and Benjamin Van Durme. 2024. NevIR: Negation in Neural Information Retrieval. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024 - Volume 1: Long Papers, St. Julians, Malta, March 17-22, 2024, Yvette Graham and Matthew Purver (Eds.). Association for Computational Linguistics, 22742287. https://aclanthology.org/2024.eacl-long. [75] Orion Weller, Kathryn Ricci, Eugene Yang, Andrew Yates, Dawn J. Lawrie, and Benjamin Van Durme. 2025. Rank1: Test-Time Compute for Reranking in Information Retrieval. CoRR abs/2502.18418 (2025). arXiv:2502.18418 doi:10. 48550/ARXIV.2502.18418 [76] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, and Jamie Brew. 2019. HuggingFaces Transformers: State-of-the-art Natural Language Processing. CoRR abs/1910.03771 (2019). arXiv:1910.03771 http://arxiv.org/abs/1910.03771 [77] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium, 23692380. doi:10.18653/v1/D181259 [78] Puxuan Yu, Luke Merrick, Gaurav Nuti, and Daniel Campos. 2024. Arctic-Embed 2.0: Multilingual Retrieval Without Compromise. CoRR abs/2412.04506 (2024). arXiv:2412.04506 doi:10.48550/ARXIV.2412.04506 [79] Hansi Zeng, Julian Killingback, and Hamed Zamani. 2025. Scaling Sparse and Dense Retrieval in Decoder-Only LLMs. arXiv:2502.15526 [cs.IR] https://arxiv.org/abs/2502.15526 [80] Yan Zhang. 2008. Undergraduate students mental models of the Web as an information retrieval system. J. Assoc. Inf. Sci. Technol. 59, 13 (2008), 20872098. doi:10.1002/ASI.20915 [81] Lucia Zheng, Neel Guha, Javokhir Arifov, Sarah Zhang, Michal Skreta, Christopher D. Manning, Peter Henderson, and Daniel E. Ho. 2025. Reasoning-Focused Legal Retrieval Benchmark. In Proceedings of the 2025 Symposium on Computer Science and Law, CSLAW 2025, Munich, Germany, March 25-27, 2025. ACM, 169193. doi:10.1145/3709025.3712219 [82] Victor Zhong, Weijia Shi, Wen-tau Yih, and Luke Zettlemoyer. 2023. RoMQA: Benchmark for Robust, Multi-evidence, Multi-answer Question Answering. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 70557067. doi:10.18653/V1/2023.FINDINGS-EMNLP.470 [83] Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, and Guido Zuccon. 2025. Rank-R1: Enhancing Reasoning in LLM-based Document Rerankers via Reinforcement Learning. CoRR abs/2503.06034 (2025). arXiv:2503.06034 doi:10.48550/ARXIV.2503."
        }
    ],
    "affiliations": [
        "University of Massachusetts Amherst, USA"
    ]
}