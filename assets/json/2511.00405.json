{
    "paper_title": "UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings",
    "authors": [
        "Zhibin Lan",
        "Liqiang Niu",
        "Fandong Meng",
        "Jie Zhou",
        "Jinsong Su"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The remarkable success of multimodal large language models (MLLMs) has driven advances in multimodal embeddings, yet existing models remain inherently discriminative, limiting their ability to benefit from reasoning-driven generation paradigm. In this work, we pioneer the exploration of generative embeddings, unifying embedding tasks within a generative paradigm. We propose UME-R1, a universal multimodal embedding framework consisting of a two-stage training strategy: a cold-start supervised fine-tuning equips the model with reasoning capabilities and enables it to generate both discriminative and generative embeddings; a subsequent reinforcement learning enhances reasoning and further optimizes generative embedding quality. This pioneering work reveals four key insights: 1) generative embeddings unlock substantial performance gains over conventional discriminative embeddings by leveraging the powerful generative reasoning capabilities of MLLMs; 2) discriminative and generative embeddings are complementary, whose combined oracle performance far exceeding that of either alone; 3) RL can effectively enhance generative embeddings, establishing a scalable optimization paradigm.; 4) repeated sampling at inference boosts downstream task coverage (pass@k), highlighting the inference-time scalability potential of generative embeddings. Evaluated on the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual documents, UME-R1 significantly outperforms conventional discriminative embedding models and offers a foundation for more interpretable, reasoning-driven generative multimodal embeddings. Our code, models, and datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 5 0 4 0 0 . 1 1 5 2 : r Preprint UME-R1: EXPLORING REASONING-DRIVEN GENERATIVE MULTIMODAL EMBEDDINGS Zhibin Lan1 Liqiang Niu2 Fandong Meng2 1School of Informatics, Xiamen University, China, 2Pattern Recognition Center, WeChat AI, Tencent Inc, China, 3Shanghai Artificial Intelligence Laboratory, China lanzhibin@stu.xmu.edu.cn, jssu@xmu.edu.cn {poetniu, fandongmeng, withtomzhou}@tencent.com Jie Zhou2 Jinsong Su1,"
        },
        {
            "title": "ABSTRACT",
            "content": "The remarkable success of multimodal large language models (MLLMs) has driven advances in multimodal embeddings, yet existing models remain inherently discriminative, limiting their ability to benefit from reasoning-driven generation paradigm. In this work, we pioneer the exploration of generative embeddings, unifying embedding tasks within generative paradigm. We propose UME-R1, universal multimodal embedding framework consisting of two-stage training strategy: cold-start supervised fine-tuning equips the model with reasoning capabilities and enables it to generate both discriminative and generative embeddings; subsequent reinforcement learning enhances reasoning and further optimizes generative embedding quality. This pioneering work reveals four key insights: 1) generative embeddings unlock substantial performance gains over conventional discriminative embeddings by leveraging the powerful generative reasoning capabilities of MLLMs; 2) discriminative and generative embeddings are complementary, whose combined oracle performance far exceeding that of either alone; 3) RL can effectively enhance generative embeddings, establishing scalable optimization paradigm.; 4) repeated sampling at inference boosts downstream task coverage (pass@k), highlighting the inferencetime scalability potential of generative embeddings. Evaluated on the MMEBV2 benchmark across 78 tasks spanning video, image, and visual documents, UME-R1 significantly outperforms conventional discriminative embedding models and offers foundation for more interpretable, reasoning-driven generative multimodal embeddings. Our code, models, and datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recently, the field of multimodal embeddings has been significantly advanced by the remarkable success of multimodal large language models (MLLMs). For instance, VLM2Vec (Jiang et al., 2025) and MM-Embed (Lin et al., 2025) construct multimodal embedding models based on MLLMs. These models demonstrate superior performance across range of multimodal embedding tasks compared to traditional dual-encoder visionlanguage models like CLIP (Radford et al., 2021). In parallel, large reasoning models (LRMs) represented by GPT-4o (Hurst et al., 2024) and DeepSeek-R1 (Guo et al., 2025) have made breakthroughs in complex reasoning. distinctive feature of these models is the incorporation of the chain of thought (CoT) (Wei et al., 2022), which elicits step-by-step reasoning paths and typically produces more accurate and interpretable outputs. Building on this success, recent works (Shen et al., 2025b; Hong et al., 2025a) have extended these advances to MLLMs, substantially enhancing their performance on various multimodal tasks. However, multimodal embedding models have derived limited benefit from these advances. The key Work was done when Zhibin Lan was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. Corresponding author. Preprint reason is that existing MLLM-based multimodal embedding models are discriminative: they directly encode the multimodal input and extract the last tokens final hidden state as the embedding, without generating any new tokens. Naturally, this raises the question: How to make multimodal embedding model act as generative one? Several prior studies (Ouali et al., 2025; Yu et al., 2025a) have incorporated next-token prediction loss in training multimodal embedding models, demonstrating that it preserves generative capabilities while enhancing discriminative performance. Nevertheless, these approaches merely introduce additional data and losses during training. Ultimately, at inference, they remain discriminative, deriving representations from the last input token, which we refer to as discriminative embeddings. In this paper, we propose UME-R1, universal multimodal embedding framework that enables multimodal embedding models to produce either discriminative or generative embeddings on demand. First, we construct cold-start supervised fine-tuning (SFT) dataset by augmenting the original querytarget pairs used for embedding training with intermediate reasoning and summaries. During training, the contrastive loss is applied to embedding tokens that follow the summary, while an autoregressive next-token prediction loss is imposed on the reasoning and summary tokens. As result, the model learns to first generate intermediate reasoning and summary, and then produce representations from the subsequently generated embedding token; we term these as generative embeddings. Meanwhile, discriminative embeddings are preserved throughout training, allowing the model to flexibly output either type of embedding as needed. Interestingly, experiments reveal substantial gap between the oracle upper bound and current discriminative embeddings, indicating that there remains considerable room for improvement. We further ask: Can reinforcement learning with verifiable reward (RLVR) also be effective for generative embedding models? natural approach would assign positive reward if the similarity of given positive pair exceeds preset threshold, and no reward otherwise. However, since the degree of similarity varies among different pairs, this approach may render some pairs excessively difficult or easy, resulting in the problem of zero policy gradients (Yu et al., 2025b). To overcome this, we propose reward policy that considers ranking and similarity gaps simultaneously, and demonstrate that generative embedding models can also benefit from RLVR. Additionally, we find that repeated sampling can improve the coverage (i.e., pass@k) of generative embedding models, suggesting that embeddings also have the potential for inference-time scaling. Overall, we make the following four contributions: 1 Based on MMEB-V2 (Meng et al., 2025) training data, we build multimodal embedding cold-start SFT dataset with CoT annotations, and construct small-scale dataset for efficient RL training. 2 We propose UME-R1, framework designed to endow multimodal embedding models with the flexibility to switch between discriminative and generative embeddings. To the best of our knowledge, we are the first to explore generative embeddings, demonstrating the significant potential of unifying embeddings within generative paradigm. 3 We pioneer the successful application of rule-based RL to the multimodal embeddings task, which lacks standard best answers like math, by designing novel reward policy tailored to embeddings. 4 UME-R1 outperforms conventional discriminative embedding models on MMEB-V2, benchmark comprising 78 tasks across three visual modalities: video, image, and visual documents. Analysis of an oracle upper bound and pass@k indicates that UME-R1 retains significant potential for further improvement."
        },
        {
            "title": "2 DATASET CONSTRUCTION",
            "content": "To construct the training corpus for generative multimodal embeddings, as illustrated in Figure 1, we sample 50,000 instances from each of the 20 in-distribution datasets within MMEB (Jiang et al., 2025). Following VLM2Vec-V2 (Meng et al., 2025), we also incorporate the training instances from LLaVA-Hound (Zhang et al., 2025a), ViDoRe (Faysse et al., 2025b), and VisRAG (Yu et al., 2025c) datasets to cover video and visual-document modalities, yielding total of 1.76 million pairs. Subsequently, we employ the pure-thinking model GLM-4.1V-Thinking (Hong et al., 2025b) to generate CoT rationales for both the query and the target of each pair. We filter the data by excluding pairs that meet any of the following criteria: (1) contain extensive contiguous token repetition; (2) include reasoning that are excessively long (e.g., exceeding 8,192 tokens); or (3) produce responses that do not conform to the <think>...</think><answer> 2 Preprint Figure 1: Illustration of the pipeline for data construction. Specific prompts used for CoT annotation and the resulting data samples are presented in Appendix B. format. This filtering process results in final set of 1.46 million cold-start SFT pairs. For RL training, set of 11,136 pairs is balanced sampled from various datasets spanning the image, video, and visual-document modalities, prioritizing instances not included in the SFT data to avoid overly simple samples."
        },
        {
            "title": "3 UME-R1",
            "content": "3.1 PRELIMINARIES We adopt the formulation from VLM2Vec (Jiang et al., 2025) for discriminative multimodal embeddings task as follows: given query and its corresponding positive target t+, as well as set of negative targets = {t K}, the objective is to maximize similarity between and t+ over all and pairs. Here, both queries and targets can be text, image, or interleaved text-image. 1 , . . . , In practice, we sample mini-batch of querytarget pairs (q1, t1), . . . , (qN , tN ), where (qi, ti) forms the positive pair and all targets {tj = i} serve as negatives for qi. Formally, we optimize the model by minimizing the following InfoNCE loss function: Ldctr = 1 N (cid:88) i=1 log exp(cid:0)(πθ(qi) πθ(ti))/τ (cid:1) exp(cid:0)(πθ(qi) πθ(ti))/τ (cid:1) + (cid:80)N j=i exp(cid:0)(πθ(qi) πθ(tj))/τ (cid:1) . (1) where πθ() denotes the normalized representation of the last input token, derived from the MLLMs final-layer hidden state, and τ represents the temperature hyper-parameter. 3.2 ARCHITECTURE In this work, we introduce multimodal embedding model capable of producing both discriminative and generative embeddings. To obtain the generative embeddings, the model first generates distinct reasoning and summaries for each query and target. These outputs are then concatenated with the original input to produce the final generative representation. Note that the model can simultaneously yield discriminative embeddings without incurring additional computation. Specifically, we employ the following template to realize this process: Template for Discriminative and Generative Embeddings USER: <image> <video> {query/target} <disc emb> Represent the above input text, images, videos, or any combination of the three as embeddings. First output the thinking process in <think> </think> tags and then summarize the entire input in word or sentence. Finally, use the <gen emb> tag to represent the entire input. ASSISTANT: <think> {reasoning} </think> <answer> {summary} <gen emb> where <image> and <video> denote placeholders for the input image and video. As illustrated in Figure 2(a), the last-layer hidden states corresponding to the prompts <disc emb> token and the final model-generated <gen emb> token serve as the discriminative and generative embeddings, respectively. 3 Preprint Figure 2: Overview of UME-R1. UME-R1 introduces two-stage training framework for generative multimodal embedding. (a) Supervised fine-tuning uses query-target pairs with reasoning annotations to train the MLLM, enabling it to generate both discriminative and generative embeddings as well as to possess basic reasoning abilities. (b) RLVR continues to fine-tune the model using regular query-target pairs, encouraging it to generate reasoning trajectories that lead to more beneficial generative embeddings. 3.3 MODEL TRAINING We train the model in two stages, enabling it not only to generate discriminative embeddings but also to develop reasoning capabilities for producing stronger generative embeddings. Figure 2 illustrates the overall training process. Stage 1: Supervised Fine-tuning. In this initial stage, we perform SFT on the model using the multimodal embedding dataset constructed in Section 2, which incorporates the step-by-step reasoning processes. As shown in Figure 2(a), alongside the discriminative embedding training objective outlined in Section 3.1, we also include the following generative embedding training objectives: Lgctr = 1 (cid:88) log i= exp(cid:0)(πθ(qi, oq ) πθ(ti, ot i))/τ (cid:1) + (cid:80)N j=i exp(cid:0)(πθ(qi, oq ) πθ(tj, oq ))/τ (cid:1) . exp(cid:0)(πθ(qi, oq ) πθ(ti, ot i))/τ (cid:1) and ot (2) where oq denote the i-th reasoning trajectory and summary of the query and target, respectively. Compared to the original input, reasoning process and summarization provide more detailed and useful information, which often enhances the performance of the resulting generative embeddings. Furthermore, to endow the model with reasoning capabilities during inference, we apply nexttoken prediction loss over both the reasoning trajectories and summaries, formalized as Lce = 1 (cid:88) Lq (cid:88) log πθ (cid:0)oq i,j qi, oq i,<j i=1 j= 4 (cid:1) + Lt(cid:88) j=1 log πθ (cid:0)ot i,j ti, ot i,<j (cid:1) , (3) Preprint where Lq and Lt denote the lengths of the reasoning trajectories for the query and the target, respectively. Overall, the loss for the SFT stage is defined as follows: Lsf = Ldctr + Lgctr + Lce. (4) This stage of training not only equips the model to generate both discriminative and generative embeddings, but also lays the foundation for its reasoning abilities. Stage 2: Reinforcement Learning with Verifiable Reward. As illustrated in Figure 2(b), in this stage, we further refine the model πθ using Group Relative Policy Optimization (GRPO) (Shao et al., 2024). Unlike methods that rely on learned value function, GRPO utilizes the mean reward across multiple sampled outputs as its baseline. Specifically, for each input query q, it samples group of candidate responses {oi}G i=1 from the old policy πθold, and then optimizes the policy model πθ by maximizing the following objective: Lgrpo = qD,{oi}G i=1πθold (cid:34) 1 (cid:88) (cid:16) i= min (cid:16) πθ(oi q) πθold(oi q) Ai, clip (cid:18) πθ(oi q) πθold (oi q) , 1 ϵ, 1 + ϵ (cid:19) (cid:17) Ai βDKL(πθπref ) (cid:17) (cid:35) , (5) where denotes the training dataset, ϵ and β are hyper-parameters, and πref represents the reference model before optimization. Ai indicates the advantage of the i-th response, computed based on group of rewards {r1,. . . ,rG} corresponding to the outputs within each group: Ai = ri mean({r1, , rG}) std({r1, , rG}) . (6) Accordingly, we design the reward function to include two components: format rewards and embedding rewards, which we will now describe in detail. Format Reward. The use of this reward encourages the model to adhere to predefined template, ensuring that responses are well-structured and interpretable. Specifically, the model is required to perform reasoning within the <think> and </think> tags, provide summary after the <answer> tag, and finally generate the <gen emb> for obtaining the generative embedding. reward of 1 is granted for strict adherence to the template, while any deviation results in reward of 0. Embedding Reward. This component is used to evaluate the quality of the embeddings generated by the model. Since embeddings cannot be directly evaluated against standard answers as in mathematics, we evaluate them from two aspects: the ranking of positives among negatives, and the similarity gap between positives and negatives. Concretely, for each query with positive target t+ and negative target t, we sample group of responses {o+ j=1 corresponding to the positive target, another group {o }G j=1 corresponding to the negative target1. For the i-th sampled response oi of the query, we calculate its similarity scores with the positive targets as + = j=1, and with the negative targets as = {πθ(q, oi) πθ(t, {πθ(q, oi) πθ(t+, o+ j=1. The embedding reward for the i-th response oi sampled from the query is defined as follows: + topG(S + ) (cid:123)(cid:122) Ranking (cid:0)avg(S +) avg(S )(cid:1) , (cid:125) (cid:123)(cid:122) Similarity Gap Remb(oi) = )}G )}G }G (7) (cid:124) (cid:125) (cid:124) where topG() denotes the operation of selecting the top-G largest elements from input set. By optimizing this reward, the model learns to produce reasoning trajectories that are more conducive to generating high-quality generative embedding."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Training Details. Following VLM2Vec-V2 (Meng et al., 2025), we adopt Qwen2-VL-2B and Qwen2-VL-7B as backbone models. During the SFT stage, we train using the cold-start dataset con1For simplicity, only one negative target is illustrated; however, this method can extends to any number of negative targets in practice. 5 Preprint Table 1: Comparison of performance between baselines and UME-R1 on MMEB-V2. CLS: classification, QA: question answering, RET: retrieval, GD: grounding, MRET: moment retrieval, VDR: ViDoRe, VR: VisRAG, OOD: out-of-domain. Oracle denotes the case where the best result between generative and discriminative embeddings is picked. Detailed results can be found in Appendix C. Model Image Video VisDoc All CLS QA RET GD Overall CLS QA RET MRET Overall VDRv1 VDRv2 VR OOD Overall # of Datasets 10 10 12 36 5 5 5 3 10 4 6 4 24 ColPali-V1.3 (PaliGemma-3B) GME (Qwen2-VL-2B) GME (Qwen2-VL-7B) LamRA (Qwen2-VL-7B) LamRA (Qwen2.5-VL-7B) VLM2Vec (Qwen2-VL-2B) VLM2Vec (Qwen2-VL-7B) VLM2Vec-V2 (Qwen2-VL-2B) CAFe (LLaVA-OV-7B) DUME (Qwen2-VL-2B) DUME (Qwen2-VL-7B) 40.3 54.4 57.7 59.2 51.7 58.7 62.7 62.9 63.6 59.3 64.2 11.5 29.9 34.7 26.5 34.1 49.3 56.9 56.3 61.7 55.0 57.0 48.1 66.9 71.2 70.0 66.9 65.0 69.4 69.5 69.1 66.3 70.8 40.3 55.5 59.3 62.7 56.7 72.9 82.2 77.3 87.6 78.0 81.8 34.9 51.9 56.0 54.1 52.4 59.7 65.5 64.9 67.6 62.5 66. UME-R1 (Qwen2-VL-2B) UME-R1 (Qwen2-VL-7B) 64.8 67.1 62.8 69.2 67.6 71.9 77.2 84.9 66.6 71. UME-R1 (Qwen2-VL-2B) Ours UME-R1 (Qwen2-VL-7B) Ours 67.6 +2.8 69.1 +2.0 67.5 +4.7 73.2 +4.0 71.2 +3.6 74.8 +2.9 80.1 +2.9 87.4 +2.5 70.2 +3.6 74.2 +2. Baseline Models 26.7 34.9 37.4 39.3 32.9 33.4 39.1 39.3 35.8 37.7 32.9 44.3 48.6 47.0 +2.7 51.6 +3.0 21.6 25.6 28.4 24.3 23.2 20.6 29.0 28.8 34.4 17.1 8.6 25.5 32.4 38.2 34.6 37.6 33.0 40.6 38.5 39.5 30.0 28. 32.9 38.2 39.7 39.3 37.8 42.0 50.4 42.6 42.6 30.5 30.0 34.3 58.7 46.6 47.4 Ours 51.2 60.7 Oracle 58.7 +7.5 67.2 +6.5 37.2 +4.3 39.6 +1.4 48.8 +9.1 49.6 +10.3 28.2 33.9 38.6 35.2 33.7 29.0 34.0 34.9 42.4 33.2 29.4 42.2 47.5 47.9 +5.7 52.2 +4. 83.6 86.1 89.4 22.0 56.3 49.8 56.9 75.5 70.7 67.6 67.1 72.4 75.7 76.8 +4.4 79.7 +4.0 52.0 54.0 55.6 11.5 33.3 13.5 9.4 44.9 49.6 43.3 35.2 81.1 82.5 85.0 37.4 58.2 51.8 59.1 79.4 79.5 47.1 82.6 43.1 43.1 44.4 21.0 40.1 33.5 38.1 39.4 38.1 33.8 34. 71.0 72.7 75.2 23.9 50.2 41.6 46.4 65.4 63.9 52.8 60.3 44.4 54.1 57.8 40.4 47.4 47.0 52.3 58.0 60.6 52.7 55.9 46.2 50.5 79.2 83.7 37.2 37.6 63.9 67. 60.1 64.5 51.5 +5.3 55.8 +5.3 82.6 +3.4 86.0 +2.3 41.5 +4.3 40.7 +3.1 68.2 +4.3 70.8 +3.7 64.4 +4.3 68.1 +3. structed in Section 2, which is approximately two-thirds the size of the dataset used by VLM2VecV2. Consistent with the settings of VLM2Vec-V2, the temperature τ is set to 0.02, the batch size to 1,024 (achieved through gradient accumulation), and the number of training steps to 5K. Besides, the maximum sequence length is 12,288 tokens, and the learning rate is 5e-5. During the RL stage, the model is trained on approximately 11K pairs and uses the default GRPO hyperparameter settings: group size = 8, clipping parameter ϵ = 0.2, and KL-divergence coefficient β = 0.04. In this stage, we set the batch size to 256, the learning rate to 1e-6, and train for one epoch. Evaluation. We evaluate UME-R1 on MMEB-V2 (Meng et al., 2025), benchmark that extends MMEB-V1 (Jiang et al., 2025) by introducing 5 meta-tasks focused on video and visual document, covering total of 9 meta-tasks and 78 tasks. During inference, we use greedy search and set the maximum number of newly generated tokens to 8,192. Unless otherwise specified, we use generative embeddings for evaluation. Hit@1 is used as the evaluation metric for all video and image tasks, while NDCG@5 (Jarvelin & Kekalainen, 2002) is reported for visual document tasks. In addition, we compare several strong models on MMEB-V1, with the corresponding results presented in Appendix D. Baselines. We compare against several MLLM-based multimodal embedding models, including GME (Zhang et al., 2025b), ColPali (Faysse et al., 2025a), VLM2Vec (Jiang et al., 2025), LamRA (Liu et al., 2025a), CAFe (Yu et al., 2025a), and VLM2Vec-V2 (Meng et al., 2025). To ensure fair comparison and to clearly assess the role of generative embeddings, we evaluate model that performs contrastive learning exclusively on discriminative embeddings, using the same dataset and settings as ours. We refer to this model as DUME (discriminative UME). 4.2 MAIN RESULTS Table 1 presents performance comparison between UME-R1 and the Baseline on 78 tasks spanning three visual modalities: images, videos, and visual documents. The results show that UME-R1 consistently achieves the best performance in images and videos with the same backbone. Although ColPali and GME perform well on visual document retrieval, the former is specifically optimized for visual document tasks, while the latter uses large amount of closed-source data. In particular, compared to VLM2Vec-V2, UME-R1 achieves an overall improvement of 2.1 while using only twothirds of its training data. Compared to the discriminative embedding model DUME trained with the same amount of data, UME-R1 increases the total scores for images, videos, and visual documents Preprint Table 2: Ablation study of the RL stage on images, videos, and visual documents. # 1 2 3 4 5 Model UME-R1 (Qwen2VL-2B) w/o RL (UME) w/o similarity gap reward w/o ranking reward w/ threshold reward Image 66.6 65.2 1.4 65.2 1.4 66.0 0.6 65.6 1.0 Video 42.2 41.2 1.0 41.2 1.0 41.8 0.4 41.7 0.5 VisDoc 63.9 63.5 0.4 63.6 0.3 63.3 0.6 63.5 0.4 ALL 60.1 59.1 1.0 59.2 0.9 59.6 0.5 59.4 0.7 by 4.1, 9.0, and 11.1, respectively, fully demonstrating the effectiveness of generative embeddings. Comparative examples of generative and discriminative embeddings are provided in Appendix E. Since UME-R1 can flexibly choose discriminative or generative embeddings as needed, we report an oracle upper bound. For each test instance, the oracle selects the embedding mode that yields the best retrieval performance. Under the oracle setting, UME-R1-2B and UME-R1-7B achieve overall score improvements of 4.3 and 3.6, respectively. The results demonstrate that the oracle substantially outperforms using only generative embeddings, which means that in practical applications users can freely switch modes to obtain more satisfactory retrieval results. 4.3 ABLATION STUDY Impact of RL Stage and Reward Design on Model Effectiveness. As shown in Table 2, we study the effectiveness of different components in the RL stage across 78 tasks of MMEB-V2. From the second row, we observe that although the RL stage uses only small dataset for training with GRPO and does not incorporate contrastive learning, it still substantially improves model performance. This finding suggests that effective reasoning paths and summarization contribute to better embeddings. The results in the Rows 3 and 4 show that jointly considering ranking and similarity differences in the reward is essential. Ranking offers supervision that aligns more closely with downstream tasks, but for relatively easy samples, the ranking reward often saturates. In such cases, similarity differences help guide the model toward learning more effective reasoning paths. In addition, we explore using fixed threshold (set to 0.5) as the evaluation criterion for assigning rewards, where positive pairs exceeding the threshold receive reward of 1 and others receive 0. The results in Row 5 show that this approach is mainly beneficial for video tasks but provides limited improvement for other modalities. We attribute this to the varying similarity distributions across task categories, which make it difficult to define single fixed threshold. Developing an adaptive threshold for reward assignment may be promising solution. Video Image Model VisDoc Table 3: Comparison of UME and UME-R1 using only discriminative embeddings against DUME under the same training settings. Impact of Generative Embedding Training on Discriminative Embeddings. While UME-R1 is primarily designed for it also supports generative embeddings, discriminative embeddings. In this study, we investigate how the SFT stage and the RL stage affect the performance of discriminative embeddings. Table 3 reports the performance of 2B-parameter models DUME, UME (without RL training), and UME-R1. Under the same training settings, introducing generative embeddings and the next-token prediction objective during the SFT stage improves the overall score of discriminative embeddings across 78 tasks by 3 points. Notably, for visual document tasks, the improvement reaches 7.5 points, likely due to the limited amount of such data in the training set, suggesting that incorporating the generative embedding and the next-token prediction objective provides richer supervisory signals. Furthermore, UME-R1 achieves an additional 0.4-point improvement over UME in the overall score. Although the RL stage only optimizes the generative embeddings, it does not compromise the performance of the discriminative embeddings, indicating that the two types of embeddings do not conflict during training. DUME UME UME-R1 33.2 34.4 1.2 34.4 1.2 52.7 55.7 3.0 56.0 3.3 62.5 63.2 0.7 64.0 1.5 52.8 60.3 7.5 60.3 7.5 ALL 7 Preprint Figure 3: pass@k curves of UME-2B and UME-7B across multiple datasets. 4.4 DEEP ANALYSIS Potential of Generative Embeddings for Inference-Time Scaling. One of the key characteristics of generative reasoning models is their ability to scale at inference time, meaning that performance can be improved by allocating more computing resources. Motivated by this, we explore whether generative embeddings possess similar potential for inference-time scaling. To this end, we evaluate model coverage (pass@k) on four randomly selected test sets from the image and video modalities, each containing 128 randomly sampled examples. Pass@k considers problem solved if any of the sampled outputs is correct, thereby indicating the models ability to retrieve the correct result through multiple attempts. To reduce variance in coverage estimation, we apply the unbiased estimation formula proposed by Brown et al. (2024). As illustrated in Figure 3, both UME-R1-2B and UME-R1-7B yield improved embedding representations through repeated sampling, underscoring that generative embeddings also hold strong promise for inference-time scaling. Appendix presents visual illustrations of how repeated sampling affects retrieval results. Figure 4: Comparison between DUME, DUME+Gen, and UME-R1. DUME+Gen denotes the approach in which an external model first generates reasoning and summaries, followed by DUME to obtain the corresponding embeddings. External-Enhanced Discriminative Embeddings vs. Self-Generated Generative Embeddings. We further investigate an approach where an external reasoning model generates reasoning and summaries, subsequently encoded by discriminative embedding model to obtain representations. We evaluate whether this approach enhances performance and compare it with our proposed selfgenerated method. Concretely, we evaluate the 2B model on previously extracted test set, employing the 9B GLM-4.1V-Thinking (Hong et al., 2025a) as the external reasoning model. As shown in Figure 4, incorporating an external model can enhance discriminative embeddings on certain tasks, with improvements of 19.7 and 3.9 observed on K700 and MSVD, respectively. However, this approach may also degrade performance, exemplified by 12.3-point drop on CIRR. Importantly, UME-R1 consistently outperforms DUME+Gen, indicating that self-generated reasoning and summaries are more efficient and effective than even stronger external model for producing high-quality embedding representations. 8 Preprint"
        },
        {
            "title": "5 RELATED WORK",
            "content": "5.1 MULTIMODAL LARGE LANGUAGE MODEL Multimodal large language models (MLLMs) (OpenAI, 2023; Liu et al., 2023; Chen et al., 2023; Li et al., 2024; Wang et al., 2024) have achieved remarkable progress across wide range of multimodal understanding tasks. The emergence of Large Reasoning Models (LRMs), exemplified by GPT-4o (Hurst et al., 2024) and DeepSeek-R1 (Guo et al., 2025), has catalyzed the development of various strategies to elicit chain-of-thought (CoT) reasoning within MLLMs. Among the most prominent is the use of reinforcement learning with verifiable reward signals to enhance visual reasoning (Zhou et al., 2025a; Zhan et al., 2025; Liu et al., 2025b; Shen et al., 2025a). However, to our knowledge, no prior work has applied reinforcement learning with verifiable reward to embedding tasks, primarily due to such tasks are non-generative and do not have definitive answers. 5.2 UNIVERSAL MULTIMODAL EMBEDDINGS Universal multimodal embedding models aim to encode inputs of various modalities into vector representations, facilitating range of multimodal tasks such as image-text retrieval (Wu et al., 2021; Zhang et al., 2024a), automatic evaluation (Hessel et al., 2021), and retrieval-augmented generation (RAG) (Zhao et al., 2023). Early vision-language models (VLMs) (Radford et al., 2021; Jia et al., 2021; Zhai et al., 2023) primarily used dual-encoder architecture and were trained with contrastive learning on large-scale imagetext datasets. Although these models exhibited strong representational capabilities, they still suffered from deficiencies such as poor understanding of interleaved imagetext inputs and tendency to behave like bag-of-words (Yuksekgonul et al., 2023). To address these issues, VLM2Vec (Jiang et al., 2025) and MM-Embed (Lin et al., 2025) convert MLLMs into multimodal embedding models through contrastive learning, leveraging MLLMs strong multimodal understanding and inherent advantages in handling interleaved imagetext inputs. Given the limited scale of existing multimodal embedding datasets, MegaPairs (Zhou et al., 2025c) and GME (Zhang et al., 2025b) introduce automated data synthesis pipelines to generate large-scale pairs, thereby further improving the performance of MLLM-based multimodal embedding models. On the other hand, some works focus on negative sample selection or learning, for example, UniME (Gu et al., 2025a) filters out false negatives and easy negatives during training based on similarity, while LLaVE (Lan et al., 2025) and QQMM (Xue et al., 2025) estimate negative difficulty and weight negatives accordingly. Furthermore, B3 (Thirukovalluru et al., 2025) introduces hard negative mining method that leverages community detection to construct training batches enriched with in-batch negatives. Additionally, some studies explore how to preserve MLLMs generative strengths when converting them from generative to discriminative models. VladVA (Ouali et al., 2025) and CAFe (Yu et al., 2025a) combine contrastive objective with autoregressive language modeling to prevent catastrophic forgetting of the models generative abilities while enhancing their discriminative capabilities. Moreover, Ju & Lee (2025) design hierarchical prompts to elicit powerful discriminative embeddings from generative models in zero-shot manner. Despite these advances, existing MLLM-based embedding models remain limited to producing discriminative embeddings and therefore do not exploit MLLMs generative and reasoning capabilities. In contrast, UME-R1 can generate discriminative or generative embeddings on demand, demonstrating the substantial potential of harnessing MLLMs reasoning power for embedding tasks."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we pioneer the exploration of generative embeddings and propose UME-R1, universal multimodal embedding framework that unifies discriminative and generative embeddings. To support this, we construct an SFT dataset by augmenting existing multimodal embedding benchmarks with reasoning and summaries produced by thinking-capable MLLM. Fine-tuning on this dataset enables the model to produce both embedding types. We further apply reinforcement learning with reward function that incorporates similarity gaps and ranking, encouraging reasoning trajectories that enhance generative embeddings. Experiments on MMEB-V2, spanning 78 tasks across video, image, and visual document domains, show that generative embeddings yield significant gains over 9 Preprint discriminative ones. Finally, oracle and inference-time analyses suggest that UME-R1 holds substantial headroom for further improvement. Our work highlights three promising directions for future research: 1) developing mechanisms that allow the model to adaptively decide whether to produce discriminative or generative embeddings based on the input; 2) constructing more challenging RL datasets or designing more effective RL training strategies to encourage the model to produce reasoning and summaries that more conducive to embedding quality; and 3) exploring inference-time scaling techniques to further enhance the quality of generative embeddings. In general, UME-R1 establishes new direction for reasoningdriven generative multimodal embeddings and lays foundation for future research."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This work complies with the ICLR Code of Ethics and does not involve the collection of new human subject data or any personally identifiable information. All datasets used in this study are publicly available and widely adopted in the research community. Additionally, the constructed data in our experiments is derived from existing models and datasets, without introducing any new sensitive, private, or proprietary content. We have carefully ensured that our methodology and experiments comply with relevant ethical standards, including fairness, transparency, and reproducibility."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "To facilitate reproducibility, we will release code, datasets, and trained models used in this work. The code has already been included in the supplementary materials submitted with this paper. Detailed descriptions of the dataset construction, model architectures, and training procedures are provided in both the main text and the appendix. These resources are intended to enable other researchers to reproduce the results reported in this work and build upon our methods."
        },
        {
            "title": "REFERENCES",
            "content": "Bradley C. A. Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. CoRR, abs/2407.21787, 2024. Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, and Zhicheng Dou. mme5: Improving multimodal multilingual embeddings via high-quality synthetic data. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pp. 82548275. Association for Computational Linguistics, 2025a. Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/ R1-V, 2025b. Accessed: 2025-02-02. Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, Vinci, Lingpeng Kong, Qi Liu, and Baobao Chang. Rlvr in vision language models: Findings, questions and directions. Notion Post, Feb 2025c. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. CoRR, abs/2312.14238, 2023. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for In IEEE/CVF Conference on Computer Vision and Patcontrastive language-image learning. tern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp. 28182829. IEEE, 2023. 10 Preprint Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Celine Hudelot, and Pierre Colombo. Colpali: Efficient document retrieval with vision language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025a. URL https://openreview.net/forum?id=ogjBpZ8uSi. Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Celine Hudelot, and Pierre Colombo. Colpali: Efficient document retrieval with vision language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025b. Tiancheng Gu, Kaicheng Yang, Ziyong Feng, Xingjun Wang, Yanzhao Zhang, Dingkun Long, Yingda Chen, Weidong Cai, and Jiankang Deng. Breaking the modality barrier: Universal embedding learning with multimodal llms. CoRR, abs/2504.17432, 2025a. Tiancheng Gu, Kaicheng Yang, Ziyong Feng, Xingjun Wang, Yanzhao Zhang, Dingkun Long, Yingda Chen, Weidong Cai, and Jiankang Deng. Breaking the modality barrier: Universal embedding learning with multimodal llms. CoRR, abs/2504.17432, 2025b. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: In Marie-Francine Moens, Xuanjing reference-free evaluation metric for image captioning. Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 75147528. Association for Computational Linguistics, 2021. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006, 2025a. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pp. arXiv2507, 2025b. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Kalervo Jarvelin and Jaana Kekalainen. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst., 20(4):422446, 2002. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 49044916. PMLR, 2021. Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. E5-V: universal embeddings with multimodal large language models. CoRR, abs/2407.12580, 2024. Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. Vlm2vec: In The Thirteenth Training vision-language models for massive multimodal embedding tasks. International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=TE0KOzWYAF. Yeong-Joon Ju and Seong-Whan Lee. From generator to embedder: Harnessing innate abilities of multimodal llms via building zero-shot discriminative embedding model. arXiv preprint arXiv:2508.00955, 2025. Preprint Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, and Jinsong Su. Llave: Large language and vision embedding models with hardness-weighted contrastive learning. CoRR, abs/2503.04812, 2025. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. CoRR, abs/2408.03326, 2024. Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping languageimage pre-training with frozen image encoders and large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 1973019742. PMLR, 2023. Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. In The Thirteenth InterMm-embed: Universal multimodal retrieval with multimodal LLMS. national Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. Yikun Liu, Yajie Zhang, Jiayin Cai, Xiaolong Jiang, Yao Hu, Jiangchao Yao, Yanfeng Wang, and Weidi Xie. Lamra: Large multimodal model as your advanced retrieval assistant. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pp. 40154025. Computer Vision Foundation / IEEE, 2025a. Zheyuan Liu, Cristian Rodriguez-Opazo, Damien Teney, and Stephen Gould. Image retrieval on real-life images with pre-trained vision-and-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 21252134, October 2021. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. CoRR, abs/2503.01785, 2025b. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: visual question answering benchmark requiring external knowledge. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 3195 3204. Computer Vision Foundation / IEEE, 2019. Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. Rui Meng, Ziyan Jiang, Ye Liu, Mingyi Su, Xinyi Yang, Yuepeng Fu, Can Qin, Zeyuan Chen, Ran Xu, Caiming Xiong, Yingbo Zhou, Wenhu Chen, and Semih Yavuz. Vlm2vec-v2: Advancing multimodal embedding for videos, images, and visual documents. CoRR, abs/2507.04590, 2025. OpenAI. Gpt-4v(ision) system card, September 2023. URL https://cdn.openai.com/ papers/GPTV_System_Card.pdf. Yassine Ouali, Adrian Bulat, Alexandros Xenos, Anestis Zaganidis, Ioannis Maniadis Metaxas, Brais Martınez, and Georgios Tzimiropoulos. Vladva: Discriminative fine-tuning of lvlms. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pp. 41014111. Computer Vision Foundation / IEEE, 2025. Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. Preprint Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 87488763. PMLR, 2021. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. VLM-R1: stable and generalizable r1-style large vision-language model. CoRR, abs/2504.07615, 2025a. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. VLM-R1: stable and generalizable r1-style large vision-language model. CoRR, abs/2504.07615, 2025b. Raghuveer Thirukovalluru, Rui Meng, Ye Liu, Karthikeyan K, Mingyi Su, Ping Nie, Semih Yavuz, Yingbo Zhou, Wenhu Chen, and Bhuwan Dhingra. Breaking the batch barrier (B3) of contrastive learning via smart batch mining. CoRR, abs/2505.11293, 2025. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. CoRR, abs/2409.12191, 2024. Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. Uniir: Training and benchmarking universal multimodal information retrievers. In Ales Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gul Varol (eds.), Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part LXXXVII, volume 15145 of Lecture Notes in Computer Science, pp. 387404. Springer, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and Rogerio Feris. Fashion IQ: new dataset towards retrieving images by natural language feedback. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp. 1130711317. Computer Vision Foundation / IEEE, 2021. Youze Xue, Dian Li, and Gang Liu. Improve multi-modal embedding learning via explicit hard negative gradient amplifying. CoRR, abs/2506.02020, 2025. Hao Yu, Zhuokai Zhao, Shen Yan, Lukasz Korycki, Jianyu Wang, Baosheng He, Jiayi Liu, Lizhu Zhang, Xiangjun Fan, and Hanchao Yu. Cafe: Unifying representation and generation with contrastive-autoregressive finetuning. CoRR, abs/2503.19900, 2025a. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, WeiYing Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. DAPO: an open-source LLM reinforcement learning system at scale. CoRR, abs/2503.14476, 2025b. 13 Preprint Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, and Maosong Sun. Visrag: Vision-based retrieval-augmented generation on multi-modality documents. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025c. Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language In IEEE/CVF International Conference on Computer Vision, ICCV 2023, image pre-training. Paris, France, October 1-6, 2023, pp. 1194111952. IEEE, 2023. Yufei Zhan, Yousong Zhu, Shurong Zheng, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Vision-r1: Evolving human-free alignment in large vision-language models via visionguided reinforcement learning. CoRR, abs/2503.18013, 2025. Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, and Ming-Wei Chang. Magiclens: Self-supervised image retrieval with open-ended instructions. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024a. Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, and Ming-Wei Chang. Magiclens: Self-supervised image retrieval with open-ended instructions. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024b. Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander G. Hauptmann, Yonatan Bisk, and Yiming Yang. Direct preference optimization of video large multimodal models from language model reward. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pp. 694717. Association for Computational Linguistics, 2025a. Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. Bridging modalities: Improving universal multimodal retrieval by multimodal large language models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pp. 92749285. Computer Vision Foundation / IEEE, 2025b. Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Do Xuan Long, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, and Shafiq Joty. Retrieving multimodal information for augmented generation: survey. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pp. 47364756. Association for Computational Linguistics, 2023. Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1zeros aha moment in visual reasoning on 2b non-sft model. CoRR, abs/2503.05132, 2025a. Junjie Zhou, Yongping Xiong, Zheng Liu, Ze Liu, Shitao Xiao, Yueze Wang, Bo Zhao, Chen Jason Zhang, and Defu Lian. Megapairs: Massive data synthesis for universal multimodal retrieval. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pp. 1907619095. Association for Computational Linguistics, 2025b. Junjie Zhou, Yongping Xiong, Zheng Liu, Ze Liu, Shitao Xiao, Yueze Wang, Bo Zhao, Chen Jason Zhang, and Defu Lian. Megapairs: Massive data synthesis for universal multimodal retrieval. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), 14 Preprint Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pp. 1907619095. Association for Computational Linguistics, 2025c. Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 49955004, 2016. 15 Preprint"
        },
        {
            "title": "A USE OF LARGE LANGUAGE MODELS",
            "content": "In the preparation of this paper, we use Large Language Models (LLMs) solely to aid in writing and polishing the text, including improving clarity, grammar, and readability. LLMs are not used for generating scientific content, experimental design, analysis, or conclusions. All technical ideas, experiments, and results reported in this paper are entirely the work of the authors."
        },
        {
            "title": "B EXAMPLE OF DATA CONSTRUCTION",
            "content": "The prompt template for SFT CoT annotation is provided as follows: Prompt Template for Reasoning Annotation {query/candidate} The above input is query/candidate for retrieval. Carefully examine and analyze the above input (which may include text, images, videos, or any combination). Identify and describe the key elements present in the input, such as the main topic, important entities, relationships, context, and any notable features or details that contribute to the overall meaning. Finally, synthesize your analysis and reflection into single word or concise sentence that best captures the essence of the input for retrieval purposes. If the input is phrase or word, the summary is that word itself. We present examples of our constructed cold-start dataset in Figures 5, 6, and 7 to illustrate the typical querytarget pairs it contains. For RL training, we sampled roughly equal numbers of instances from each modality while ensuring balanced numbers across different datasets within each modality. In particular, for the image modality, pairs were drawn only from OK-VQA (Marino et al., 2019), ChartQA (Masry et al., 2022), CIRR (Liu et al., 2021), A-OKVQA (Marino et al., 2019), and Visual7W (Zhu et al., 2016), as the tasks in the other image datasets are relatively simple. 16 Preprint Figure 5: Example from the constructed cold-start dataset (Case 1). The orange part represents the original data, the blue part denotes the added prompt, the black part indicates the reasoning content, and the green part shows the summary. orange segments correspond to the original data, blue segments represent the added prompts, black segments capture the reasoning process, and green segments provide the summaries. 17 Preprint Figure 6: Example from the constructed cold-start dataset (Case 2). 18 Preprint Figure 7: Example from the constructed cold-start dataset (Case 3). 19 Preprint DETAILED SCORES OF MMEB-V2 Table 4: The detailed results of the baselines and UME-R1 the full MMEB-v2 benchmark. We only include the best version of each series of previous models in the table. Numbers in parentheses represent the task count for each category. ColPali v1.3 GME-7B LamRA-Qwen2.5-VL VLM2Vec-7B VLM2Vec-V2.0 CAFe-7B DUME-2B DUME-7B UME-R1-2B UME-R1-7B Avg - All (78 tasks) Avg - Image (36 tasks, Hit@1) Avg - Video (18 tasks, Hit@1) Avg - Visdoc (24 tasks, NDCG@5) I-CLS (10) I-QA (10) I-RET (12) I-VG (4) V-CLS (5) V-QA (5) V-RET (5) V-MR (3) VD-Vidore-V1 (10) VD-Vidore-V2 (4) VD-VisRAG (6) VD-OOD (4) ImageNet-1K N24News HatefulMemes VOC2007 SUN397 Place365 ImageNet-A ImageNet-R ObjectNet Country211 OK-VQA A-OKVQA DocVQA InfographicsVQA ChartQA Visual7W ScienceQA VizWiz GQA TextVQA VisDial CIRR VisualNews t2i VisualNews i2t MSCOCO t2i MSCOCO i2t NIGHTS WebQA FashionIQ Wiki-SS-NQ OVEN EDIS MSCOCO RefCOCO RefCOCO-Matching Visual7W-Pointing K700 SmthSmthV2 HMDB51 UCF101 Breakfast MVBench Video-MME NExTQA EgoSchema ActivityNetQA DiDeMo MSR-VTT MSVD VATEX YouCook2 QVHighlight Charades-STA MomentSeeker ViDoRe arxivqa ViDoRe docvqa ViDoRe infovqa ViDoRe tabfquad ViDoRe tatdqa ViDoRe shiftproject ViDoRe artificial intelligence ViDoRe energy ViDoRe government reports ViDoRe healthcare industry ViDoRe esg reports human labeled v2 ViDoRe biomedical lectures v2 multilingual ViDoRe economics reports v2 multilingual ViDoRe esg reports v2 multilingual VisRAG ArxivQA VisRAG ChartQA VisRAG MP-DocVQA VisRAG SlideVQA VisRAG InfoVQA VisRAG PlotQA ViDoSeek-page ViDoSeek-doc MMLongBench-page MMLongBench-doc 44.4 34.9 28.2 71.0 40.3 11.5 48.1 40.3 26.7 37.8 21.6 25.5 83.6 52.0 81.1 43.1 42.4 25.5 50.6 69.8 56.1 27.5 14.9 64.6 45.6 6.0 9.4 6.6 11.3 5.0 5.7 6.1 16.3 27.6 8.3 18.8 41.2 8.2 50.1 47.6 59.2 49.9 65.5 53.8 5.9 80.5 50.0 64.7 36.7 64.5 3.9 56.1 23.4 25.1 24.8 49.4 10.9 33.7 30.6 35.2 38.4 51.3 22.8 17.6 45.4 16.7 5.3 19.9 29.0 27.6 81.7 56.6 84.9 86.9 70.9 75.1 95.7 94.7 93.6 95.9 51.3 54.7 49.0 52.9 80.9 72.3 82.0 85.1 83.5 79.3 38.1 87.5 27.1 80. 57.8 56.0 38.4 75.2 57.7 34.7 71.2 59.3 37.4 50.4 28.4 37.0 89.4 55.6 85.0 44.4 64.6 50.5 53.6 80.3 69.5 39.1 41.2 83.9 69.0 24.8 33.2 21.0 41.4 20.3 17.8 22.2 28.0 39.0 76.9 46.8 60.8 54.9 79.7 83.6 71.2 57.7 67.6 91.4 37.8 78.2 75.1 96.0 31.4 60.9 78.4 66.5 39.7 30.6 47.9 54.7 14.3 46.6 39.2 53.6 46.8 65.6 26.4 31.8 49.7 24.9 9.1 59.5 14.0 37.4 86.9 57.5 91.6 94.6 74.1 96.8 99.6 95.3 98.8 99.3 63.4 49.5 54.2 55.4 87.4 86.1 89.7 92.6 88.6 76.5 32.6 90.3 36.9 85. 47.4 52.4 33.6 50.2 51.7 34.1 66.9 56.7 32.9 42.6 23.2 37.2 56.3 33.3 58.2 40.1 58.9 29.8 51.3 78.7 66.5 37.4 36.3 77.0 59.4 21.7 39.9 34.1 37.1 23.7 15.0 24.6 31.3 32.0 57.4 46.1 62.5 44.7 70.1 74.2 65.7 71.1 64.4 85.7 33.4 67.0 84.8 78.7 36.0 57.1 82.6 51.2 32.1 25.3 33.8 53.0 20.1 37.6 35.1 44.9 47.0 48.5 22.8 25.0 41.9 18.7 7.5 60.9 18.8 31.8 53.0 25.4 72.3 66.1 25.9 27.3 72.0 65.2 72.2 83.8 33.0 35.9 31.9 32.5 37.7 68.2 72.0 71.1 67.9 56.4 10.7 63.9 0.5 51. 52.3 65.5 33.7 46.4 62.7 56.9 69.4 82.2 39.1 30.0 29.0 38.9 56.9 9.4 59.1 38.1 80.1 79.7 69.7 80.7 77.4 37.4 58.1 73.9 40.1 29.8 56.8 47.3 89.7 60.0 56.9 52.7 38.5 39.9 55.1 71.6 81.9 51.1 80.5 81.2 77.2 73.9 67.6 88.3 17.1 62.3 66.5 85.7 75.7 87.6 84.6 81.0 35.5 32.1 42.2 61.8 23.8 28.5 27.8 20.3 21.8 51.4 29.3 34.5 46.7 25.5 9.0 57.7 19.8 39.3 60.2 34.7 70.4 78.2 27.6 38.6 67.7 60.4 61.8 69.9 6.8 5.1 13.9 11.9 52.6 57.7 60.6 54.7 66.0 62.7 16.3 69.4 0.4 28. 20 58.0 64.9 34.6 65.4 62.9 56.3 69.5 77.3 39.3 34.3 28.8 36.8 75.7 45.1 79.6 39.6 80.8 72.9 56.3 85.0 71.0 35.9 47.4 89.3 65.2 25.2 51.5 43.6 90.1 58.8 47.4 52.9 38.2 43.3 64.9 72.2 82.7 57.5 74.5 78.2 75.3 71.4 68.6 90.6 19.5 66.9 64.3 84.1 67.1 87.1 85.8 69.2 38.0 42.8 40.9 60.0 14.8 33.7 30.7 20.9 34.0 52.3 30.4 28.3 48.1 26.5 10.6 49.4 20.2 40. 80.6 44.9 83.7 89.2 43.8 60.8 88.5 86.5 85.0 92.2 45.6 44.3 43.0 46.6 76.9 83.7 88.1 84.1 82.3 75.9 29.1 79.0 15.8 63.0 60.6 67.6 42.4 63.9 63.6 61.7 69.1 87.6 35.8 58.7 34.4 39.5 70.7 49.6 79.5 38.1 77.3 83.2 78.7 89.8 79.9 45.0 55.2 88.0 22.5 16.7 67.3 63.8 79.2 53.3 48.8 52.5 65.4 43.8 65.7 76.8 82.7 60.4 69.5 79.4 75.4 73.1 66.7 89.3 39.0 61.2 60.8 71.3 84.7 89.4 83.0 93.2 40.1 35.8 46.9 39.6 16.6 48.9 46.0 62.4 60.0 76.0 37.8 36.5 56.4 32.0 9.5 58.4 18.7 41. 73.3 38.3 80.6 80.7 37.8 52.0 86.0 84.8 85.0 88.4 50.7 50.9 54.3 42.3 74.0 82.7 75.1 87.6 87.9 69.4 22.5 73.8 13.3 42.6 52.7 62.5 33.2 52.8 59.3 54.9 66.3 78.0 37.7 46.6 17.1 30.0 67.6 43.3 47.1 33.8 74.6 69.7 65.3 68.9 71.4 41.0 41.3 90.7 46.2 23.9 56.8 46.9 86.0 59.2 39.1 46.9 38.7 42.0 60.2 73.9 75.9 52.0 71.2 72.5 74.5 68.3 67.5 90.2 11.5 60.0 65.2 86.5 68.1 85.1 89.3 69.5 22.7 37.7 53.4 55.7 18.9 48.8 39.2 55.2 23.2 66.7 16.9 16.2 34.9 11.1 0.06 40.3 16.1 33. 68.7 33.6 74.5 78.3 35.3 61.8 74.3 78.4 83.0 88.2 48.0 39.8 44.1 41.1 35.8 47.2 35.3 61.3 64.7 38.5 20.0 69.5 10.4 35.4 55.9 66.4 29.4 60.3 64.2 57.0 70.8 81.8 32.9 47.4 8.6 28.0 67.1 35.2 82.6 34.9 76.6 77.2 79.6 85.5 74.6 41.9 48.6 88.8 44.8 24.7 61.6 51.4 86.3 62.3 49.8 52.1 45.5 44.3 46.9 69.9 75.7 51.6 76.9 82.3 77.1 71.2 69.6 90.3 20.5 70.6 70.5 92.8 72.3 86.8 85.1 83.1 27.3 25.1 42.6 48.8 20.8 47.4 40.2 48.6 50.4 50.2 0.10 0.10 28.8 13.8 0.00 29.4 15.8 38. 66.6 35.8 72.8 89.2 38.5 61.9 69.3 68.4 83.1 84.9 40.4 37.4 29.6 33.5 77.3 83.4 83.8 91.5 88.2 71.3 20.2 73.2 10.3 36.0 60.1 66.6 42.2 63.9 64.8 62.8 67.6 77.2 44.3 51.0 32.9 39.7 72.4 46.2 79.2 37.2 75.3 81.1 75.2 80.0 79.4 42.6 50.4 88.7 52.0 23.4 62.4 51.1 92.2 67.7 64.9 54.1 42.7 46.8 67.3 78.6 76.6 53.7 71.7 74.2 75.1 68.9 67.2 90.0 17.1 62.0 66.9 88.0 69.5 83.3 84.4 71.5 35.8 44.1 54.4 67.2 20.1 49.9 41.7 59.9 45.4 57.8 32.4 34.3 55.4 29.9 12.7 57.5 20.4 41. 73.9 37.9 76.2 86.1 40.6 66.8 85.9 83.3 82.6 90.8 50.2 46.2 45.7 42.7 74.3 86.0 75.6 87.1 84.4 68.0 21.2 75.9 11.9 39.7 64.5 71.3 47.5 67.1 67.1 69.2 71.9 84.9 48.6 60.7 38.2 39.3 75.7 50.5 83.7 37.6 80.4 82.3 79.0 90.8 80.3 46.8 53.9 90.1 42.3 25.0 71.7 58.7 93.8 79.2 75.1 55.2 53.7 51.6 69.3 83.5 80.7 55.3 76.8 82.0 78.3 71.4 68.1 90.9 23.4 72.5 71.4 92.0 72.7 91.4 91.1 84.2 42.8 50.4 58.3 70.0 21.5 58.2 47.3 69.6 52.4 76.0 40.0 38.9 60.8 32.6 18.5 54.9 21.9 41. 73.6 41.1 80.8 90.2 46.7 65.0 89.5 85.7 89.8 94.3 50.4 50.7 57.8 43.2 80.5 85.0 83.4 91.5 89.2 72.7 21.3 75.3 12.3 41.3 Preprint MMEB-V1 BENCHMARK SCORES Since MMEB-V1 has been widely adopted in prior work, in this section we also report the performance of UME-R1 alongside other baseline models on MMEB-V1. The results in Table 5 demonstrate that UME-R1 achieves the best overall score among models of the same size. Table 5: Results on the MMEB-V1 benchmark, which comprises total of 36 image embedding tasks. IND represents the in-distribution dataset, and OOD represents the out-of-distribution dataset. In UniIR, the FF and SF subscripts under CLIP or BLIP represent feature-level fusion and scorelevel fusion, respectively. CAFe-V1 indicates that the model is trained solely on the MMEB-V1 training data (contains only image data), whereas CAFe-V2 denotes that the model is trained on the MMEB-V2 training data. The best results are marked in bold, and the second-best results are underlined. Model Per Meta-Task Score Classification VQA Retrieval Grounding Average Score IND OOD Overall # of Datasets 10 10 4 20 16 36 CLIP (Radford et al., 2021) BLIP2 (Li et al., 2023) SigLIP (Zhai et al., 2023) OpenCLIP (Cherti et al., 2023) UniIR (BLIPF ) (Wei et al., 2024) UniIR (CLIPSF ) (Wei et al., 2024) Magiclens (Zhang et al., 2024b) Baseline Models 42.8 27.0 40.3 47.8 42.1 44.3 38.8 9.1 4.2 8.4 10.9 15.0 16.2 8.3 MLLM-based Baseline Models E5-V (Jiang et al., 2024) VLM2Vec (Qwen2-VL-2B) (Jiang et al., 2025) VLM2Vec (Qwen2-VL-7B) (Jiang et al., 2025) VLM2Vec-V2 (Qwen2-VL-7B) (Jiang et al., 2025) MMRet-7B (Zhou et al., 2025b) CAFe-V1-7B (Yu et al., 2025a) CAFe-V2-7B (Yu et al., 2025a) mmE5-11B (Chen et al., 2025a) LLaVE-2B (Lan et al., 2025) LLaVE-7B (Lan et al., 2025) UniME-4B (Gu et al., 2025b) UniME-7B (Gu et al., 2025b) UME-R1-2B UME-R1-7B UME-R1-2B UME-R1-7B 21.8 59.0 62.6 62.9 56.0 65.2 63.6 67.6 62.1 65.7 54.8 66.8 Ours 64.8 67.1 Oracle 67.6 69.1 4.9 49.4 57.8 56.3 57.4 65.6 61.7 62.8 60.2 65.4 55.9 66. 62.8 69.2 67.5 73.2 53.0 33.9 31.6 52.3 60.1 61.8 35.4 11.5 65.4 69.9 69.5 69.9 70.0 69.1 70.9 65.2 70.9 64.5 70.6 67.6 71.9 71.2 74. 51.8 47.0 59.5 53.3 62.2 65.3 26.0 19.0 73.4 81.7 77.3 83.6 91.2 87.6 89.7 84.9 91.9 81.8 90.9 77.2 84.9 80.1 87.4 37.1 25.3 32.3 39.3 44.7 47.1 31.0 14.9 66.0 72.2 68.8 68.0 75.8 72.8 72.3 69.4 75.0 68.2 74. 38.7 25.1 38.0 40.2 40.4 41.7 23.7 11.5 52.6 57.8 59.9 59.1 62.4 61.1 66.7 59.8 64.4 52.7 65.8 71.5 76.1 60.4 65.1 75.3 79.2 63.8 67. 37.8 25.2 34.8 39.7 42.8 44.7 27.8 13.3 60.1 65.8 64.9 64.1 69.8 67.6 69.8 65.2 70.3 64.2 70.7 66.6 71.3 70.2 74.2 21 Preprint"
        },
        {
            "title": "EMBEDDINGS",
            "content": "Figures 8, 9, 10, and 11 present several comparative examples of generative and discriminative embeddings. It can be observed that generative embeddings are capable of producing effective reasoning and summaries, thereby facilitating the generation of higher-quality embeddings. Figure 8: comparison of generative and discriminative embeddings is shown (Case 1). Green highlights denote content that positively impacts retrieval performance. 22 Preprint Figure 9: comparison of generative and discriminative embeddings is shown (Case 2). Preprint Figure 10: comparison of generative and discriminative embeddings is shown (Case 3). Figure 11: comparison of generative and discriminative embeddings is shown (Case 4). 24 Preprint"
        },
        {
            "title": "F EXAMPLE OF REPEATED SAMPLING",
            "content": "Figures 12, 13, and 14 illustrate the impact of different samplings on the reasoning and summarization of generative embeddings. Interestingly, we observe that for correctly retrieved examples, the generated reasoning and summaries tend to be more effective. This suggests that generative embeddings offer more interpretable approach, potentially allowing the quality of embeddings to be assessed through the generated reasoning and summaries. Figure 12: An example showing how repeated sampling leads to variations in model-generated reasoning and summaries, resulting in different retrieval outcomes (Case 1). The green segments indicate correct reasoning or summaries, while the red segments highlight incorrect ones. 25 Preprint Figure 13: An example showing how repeated sampling leads to variations in model-generated reasoning and summaries, resulting in different retrieval outcomes (Case 2). Preprint Figure 14: An example showing how repeated sampling leads to variations in model-generated reasoning and summaries, resulting in different retrieval outcomes (Case 3). 27 Preprint"
        },
        {
            "title": "G REWARD AND COMPLETION LENGTH VISUALIZATION",
            "content": "In this section, we present visualizations in Figures 15 and 16 illustrating the evolution of reward and completion length throughout training. We observe that for both the 2B and 7B models, the lowest reward value increases as training progresses. However, unlike other tasks, our reward does not exhibit strictly increasing trend. This is because our RL dataset consists of data from multiple modalities and sources, and follows the VLM2Vec-V2 strategy of using data from the same source within each batch to avoid overly trivial negatives. Due to substantial differences in similarity and difficulty across datasets, the rewards vary considerably between batches: rewards are relatively high when the batch is easier, but lower when the batch is more challenging. Consequently, the reward curve does not follow strictly monotonic upward trajectory. In addition, we observe that the completion length of the 2B model decreases as training progresses. This trend is consistent with the findings of Chen et al. (2025c), Chen et al. (2025b), and Peng et al. (2025) on small-scale MLLMs. possible explanation is that the reasoning capacity of the 2B model is limited, and excessively long reasoning may even impair its performance. Figure 15: Evolution of reward and generated completion length of UME-R1-2B during training. Figure 16: Evolution of reward and generated completion length of UME-R1-7B during training."
        }
    ],
    "affiliations": [
        "Pattern Recognition Center, WeChat AI, Tencent Inc, China",
        "School of Informatics, Xiamen University, China",
        "Shanghai Artificial Intelligence Laboratory, China"
    ]
}