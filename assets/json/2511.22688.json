{
    "paper_title": "Test-time scaling of diffusions with flow maps",
    "authors": [
        "Amirmojtaba Sabour",
        "Michael S. Albergo",
        "Carles Domingo-Enrich",
        "Nicholas M. Boffi",
        "Sanja Fidler",
        "Karsten Kreis",
        "Eric Vanden-Eijnden"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A common recipe to improve diffusion models at test-time so that samples score highly against a user-specified reward is to introduce the gradient of the reward into the dynamics of the diffusion itself. This procedure is often ill posed, as user-specified rewards are usually only well defined on the data distribution at the end of generation. While common workarounds to this problem are to use a denoiser to estimate what a sample would have been at the end of generation, we propose a simple solution to this problem by working directly with a flow map. By exploiting a relationship between the flow map and velocity field governing the instantaneous transport, we construct an algorithm, Flow Map Trajectory Tilting (FMTT), which provably performs better ascent on the reward than standard test-time methods involving the gradient of the reward. The approach can be used to either perform exact sampling via importance weighting or principled search that identifies local maximizers of the reward-tilted distribution. We demonstrate the efficacy of our approach against other look-ahead techniques, and show how the flow map enables engagement with complicated reward functions that make possible new forms of image editing, e.g. by interfacing with vision language models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 8 8 6 2 2 . 1 1 5 2 : r TEST-TIME SCALING OF DIFFUSIONS WITH FLOW MAPS Amirmojtaba Sabour1,2,3 Michael S. Albergo4,5,6 Carles Domingo-Enrich7 Nicholas M. Boffi8 Sanja Fidler1,2,3 Karsten Kreis1 Eric Vanden-Eijnden9,10 1NVIDIA, 2University of Toronto, 3Vector Institute, 4Harvard University, 5Kempner Institute, 6IAIFI, 7Microsoft Research, 8Carnegie Mellon University, 9Courant Institute, New York University, 10ML Lab at Capital Fund Management (CFM) amsabour@cs.toronto.edu, malbergo@fas.harvard.edu, carlesd@microsoft.com,nboffi@andrew.cmu.edu, fidler@cs.toronto.edu, kkreis@nvidia.com, eve2@nyu.edu Project Page: https://flow-map-trajectory-tilting.github.io/"
        },
        {
            "title": "ABSTRACT",
            "content": "A common recipe to improve diffusion models at test-time so that samples score highly against user-specified reward is to introduce the gradient of the reward into the dynamics of the diffusion itself. This procedure is often ill posed, as user-specified rewards are usually only well defined on the data distribution at the end of generation. While common workarounds to this problem are to use denoiser to estimate what sample would have been at the end of generation, we propose simple solution to this problem by working directly with flow map. By exploiting relationship between the flow map and velocity field governing the instantaneous transport, we construct an algorithm, Flow Map Trajectory Tilting (FMTT), which provably performs better ascent on the reward than standard testtime methods involving the gradient of the reward. The approach can be used to either perform exact sampling via importance weighting or principled search that identifies local maximizers of the reward-tilted distribution. We demonstrate the efficacy of our approach against other look-ahead techniques, and show how the flow map enables engagement with complicated reward functions that make possible new forms of image editing, e.g. by interfacing with vision language models. Figure 1: Test-time search can overcome model biases and reliably sample from regions of the distribution (e.g., precise clock times) that baselines fail to capture."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large scale foundation models built out of diffusions (Ho et al., 2020; Song et al., 2020) or flowbased transport (Lipman et al., 2022; Albergo & Vanden-Eijnden, 2022; Albergo et al., 2023; Liu Equal contribution. 1 et al., 2022) have become highly successful tools across computer vision and scientific domains. In this paradigm, performing generation amounts to numerically solving an ordinary or stochastic differential equation (ODE/SDE), the coefficients of which are learned neural networks. An active area of current research is how to best adapt these dynamical equations at inference time to extract samples from the model that align well with user-specified reward. For example, as shown in Figure 1, user may want to generate an image of clock with precise time displayed on it, which is often generated inaccurately without suitable adaptation of the generative process. These approaches, often collectively referred to as guidance, do not require additional re-training and as result are orthogonal to the class of fine-tuning methods, which instead attempt to adjust the model itself via an additional learning procedure to modify the quality of generated samples. While guidance-based approaches can often be made to work well in practice, most methods are somewhat ad-hoc, and proceed by postulating term that may drive the generative equations towards the desired goal. To this end, common approach is to incorporate the gradient of the reward model, which imposes gradient ascent-like structure on the reward throughout generation. Despite the intuitive appeal of this approach, typical rewards are defined only at the terminal point of generation i.e., over clean image rather than over the entire generative process. This creates need to predict where the current trajectory will land, in principle necessitating an expensive additional differential equation solve per step of generation. To avoid the associated computational expense of this nested solve, common practice is to employ heuristic approximation of the terminal point, such as leveraging one-step denoiser that can be derived from learned scoreor flow-based model. See Figure 2 for visualizations of the one-step denoiser across different noise levels. the In this paper, we revisit reward guidance problem from the perspective of flow maps, recently-introduced methodology for flow-based generative modeling that learns the solution operator of probability flow ODE directly rather than the associated drift (Boffi et al., 2024; 2025; Sabour et al., 2025; Geng et al., 2025). By leveraging simple identity of the flow map, we show that an implicit flow can be used to define reward-guided generative process as in the case of standard flow-based models. With access to the flow map in addition to the implicit flow, we can predict the terminal point of trajectory in single or few function evaluations, vastly improving the prediction relative to denoiser-based techniques see again Figure 2 and leading to significantly improved optimization of the reward. In addition, we highlight how to incorporate time-dependent weights throughout the generative process to account for the gradient ascents failure to equilibrate on the timescale of generation, leading to several new and effective ways to sample high-reward outputs. Figure 2: Comparison between different look-ahead methods. We visualize corrupted data for different levels of noise and show the outputs of 1-step denoiser, 1-step flow map, and 4-step flow map. Contributions. (i) We introduce Flow Map Trajectory Tilting (FMTT), principled inference time adaptation procedure for flow maps that effectively uses their look-ahead capabilities to accurately incorporate learned and complex reward functions in Monte Carlo and search algorithms. (ii) Using conditions that characterize the flow map, we show that the importance weights for this Jarzynski/SMC scheme reduce to remarkably simple formula. Our approach is theoretically grounded in controlling the thermodynamic length of the process over baselines, measure of the efficiency of the guidance in sampling the tilted distribution. (iii) We empirically show that FMTT has favorable test-time scaling characteristics that outperform standard ways of embedding rewards into diffusion sampling setups. (iv) To our knowledge, we demonstrate the first successful use of pretrained 2 Figure 3: Schematic overview of test-time adaptation of diffusions with flow map tilting. Using the look-ahead map Xt,1(xt) in the diffusion inside the reward, reward information can be principly used through the tilted trajectories (green lines). This allows us to perform better ascent on the reward, and the importance weights At take on remarkably simple form that can be used for both exactly sampling ˆρt and search for maximizers of ˆρt. vision-language models (VLMs) as reward functions for test-time scaling, allowing rewards to be specified entirely in natural language. We further show that the flow map is crucial for their success, substantially boosting the effectiveness of the search process when using these rewards. 1.1 RELATED WORK Flows and diffusions. Diffusion models (Song et al., 2020; Ho et al., 2020) and flow models (Lipman et al., 2022; Albergo & Vanden-Eijnden, 2022; Liu et al., 2022) are the backbone of efficient, state-of-the-art generative model for continuous data. They are learned by regressing the coefficients that appear in ordinary or stochastic differential equations that fulfill the transport of samples from one distribution to samples from another. Dual to the instantaneous picture of transport is the flow map (Song et al., 2023; Kim et al., 2024; Boffi et al., 2024; Geng et al., 2025; Sabour et al., 2025; Boffi et al., 2025), in which we learn not the coefficients in differential equation that needs to be integrated, but the arbitrary integrator itself. This enables few-step sampling. Our approach in this paper is to combine these perspectives to modify diffusions using the flow map. Test-time scaling for diffusions. Test-time scaling in diffusions refers to the line of work that tradeoff compute at inference time to improve the performance of model or align the generation with user specified reward (Ma et al., 2025). Certain works use the denoiser associated with the score model to perform this look-ahead on the dynamics (Wu et al., 2023a; Singhal et al., 2025; Zhang et al., 2025) or do not use any look-ahead at all (Mousavi-Hosseini et al., 2025; Skreta et al., 2025). However, as discussed later, there is little signal from the denoiser at early times in the generative trajectory. Other works rely on Monte Carlo search algorithms (Lee et al., 2025; Ramesh & Mardani, 2025), which monotonically increase the reward but reduce sample diversity. As we will see, many of these approaches are compatible with the flow map approach presented here."
        },
        {
            "title": "2 METHODOLOGY",
            "content": "We consider the task of generative modeling via continuous-time flow maps, wherein samples x0 Rd from base distribution with probability density function (PDF) ρ0 are mapped via diffeomorphism to samples x1 from the target PDF ρ1 known through empirical data. From there, we will detail how the instantaneous dynamics of this map can be directly adapted (without retraining) to sample tilted distribution favoring reward, i.e. to sample ˆρ1(x) = ρ1er(x)+ ˆF where r(x) is user specified reward function and ˆF = ln (cid:82) Rd ρ1(x)er(x)dx is normalization factor."
        },
        {
            "title": "2.1 BACKGROUND ON DYNAMICAL GENERATIVE MODELING",
            "content": "An effective means of instantiating the transport from the base PDF ρ0 to the target PDF ρ1 relies on formulating it as the solution to an ordinary differential equation (ODE) of the form xt = bt(xt) xt=0 ρt=0, (1) where bt : [0, 1] Rd Rd is velocity field that governs the transport and is adjusted so that the solutions to the ODE (1) satisfy xt=1 ρ1. Since the time dependent PDF ρt(x) of the solutions to (1) at time satisfies the continuity equation tρt = (btρt) ρt=0 = ρ0 (2) this requirement on bt implies that the solution to (2) is such that ρt=1 = ρ1. Associated with these dynamics is the two-time flow map Xs,t : [0, 1]2 Rd Rd, which satisfies Xs,t(xs) = xt s, [0, 1]. (3) That is, the map jumps along solutions of (1) from time to time t. Notably, if = 0 and = 1, we could produce sample under ρ1 in single step, though we have the freedom to use more if we so choose. This property, and the relation between the flow map and bt will be exploited below to devise principled adaptation procedure for Xs,t. Importantly, the flow map satisfies the Eulerian equation sXs,t(x) + bs(x) Xs,t(x) = 0, (4) which will play role in simplifying our analysis later. Equation (4) can be obtained by taking the total derivative of (3) with respect to s, using the ODE (1), and evaluating the result at xs = x. Stochastic Interpolants. One way to instantiate the generative models above is to construct PDF ρt that connects ρ0 to ρ1 and then learn the associated the velocity field bt that gives rise to this evolution. common strategy to construct such path and regress bt is that of stochastic interpolants (Albergo & Vanden-Eijnden, 2022; Albergo et al., 2023; Lipman et al., 2022; Liu et al., 2022), in which ρt is defined as the law of the stochastic process It(x0, x1) = αtx0 + βtx1 with (x0, x1) ρ(x0, x1), where ρ(x0, x1) is some coupling from which x0, x1 are drawn that marginalizes onto ρ0, ρ1 and αt, βt are scalar coefficients that satisfy α0 = β1 = 1 and α1 = β0 = 0. common choice is to use αt = 1 and βt = t, which we will use throughout for simplicity. Importantly, using these coefficients, the law of this process satisfies (2) with the velocity field bt(x) = E[ ItIt = x] = E[x1It] E[x0It], (5) where we used It = x1 x0 and E[It = x] denotes expectation over ρ(x0, x1) conditional on E[x0It = x], and It = x. By Steins identity, the score is given by st(x) = log ρt(x) = 1 1t using = E[ItIt = x] = (1 t)E[x0It] + tE[x1It], it can be expressed in terms of bt as st(x) = (tbt(x) x)(1 t)1. (6) The velocity field bt(x) is also the minimizer of simple quadratic objective (Lipman et al., 2022; Albergo & Vanden-Eijnden, 2022) which, once learned, can be translated into function for the score via (6). Using the score, the deterministic ODE can be converted to stochastic dynamics dxt = [bt(xt) + ϵtst(xt)] dt + 2ϵtdWt (7) where ϵt 0 is an arbitrarily tunable diffusion coefficient and dWt is an incremental Brownian motion (Albergo et al., 2023). The solutions to (7) sample the same PDF ρt as (1), as can be seen from the fact that the PDF of (7) satisfies the Fokker-Planck equation tρt = (btρt) + ϵt [stρt + ρt] (8) which reduces to (2) since stρt = ρt. The velocity field given in (5) is related to the two-time flow map Xs,t via the tangent identity (Kim et al., 2024; Boffi et al., 2025) lim st tXs,t(x) = bt(x), (9) 4 Figure 4: Qualitative results using VLM-based rewards. Prompts where the base model fails to generate aligned outputs are corrected by FMTT, with flow map look-ahead producing the most reliable improvements. which says that for infinitesimally small steps, the variation of the flow map output in time is characterized by the velocity. To automatically enforce the boundary condition Xs,s(x) = x, we can express the flow map as Xs,t(x) = + (t s)vs,t(x), (10) where vs,t(x) : Importantly, the velocity field is accessible directly from the flow map by using (9) on (10): [0, 1]2 Rd Rd is function of s, t, and defined through this relation. As such, training flow map model instead of just velocity model gives access to both the drifts in (1) and (7) as well as the any step model, i.e. the ability to look ahead our trajectory. vt,t(x) = bt(x). (11) 2.2 FIXING INFERENCE-TIME ADAPTATION OF DIFFUSIONS WITH FLOW MAPS contemporary question is how best to adapt the SDE (7) to tilt toward samples that score highly against time-dependent reward rt(x) satisfying r0 = 0 and rt=1 = so that the time-dependent tilted PDF ˆρt = ρtert(x)+ ˆFt satisfies ˆρt=0 = ρ0 and ˆρt=1 = ˆρ1 = ρ1er+ ˆF . One may want to sample exactly under the tilted PDF ˆρt = ρtert(x)+Ft for scientific applications, or one may want to track local maximizers of ˆρt for example in image generation procedures. This is useful for ensuring user prompts align with image content. Tilting the diffusion. natural way to modify the SDE (7) is to add the gradient of the timedependent reward rt to the score, i.e. use dxt = [bt(xt) + ϵtst(xt) + ϵtrt(xt)]dt + 2ϵtdWt, x0 ρ0 (12) To implement this change in practice, however, we face an issue: meaningful rt(x) is not readily available, as user-specified rewards are usually learned only on the data-distribution, i.e. at time = 1. One could think of several solutions to this problem: Naıve look-ahead: This amounts to using e.g. rt(x) = tr(x). Unfortunately, the gradient dynamics from tr(x) provides no clear signal at small times when xt is still far from the region where the reward r(x) is meaningful. Denoiser look-ahead: common workaround for the fact that the reward has no signal for most of the trajectory is to use the denoiser Dt(x) = E[x1It = x] to estimate where the sample would have gone. That is, instead of rt(x) = tr(x), we could instead use rt(x) = tr(Dt(x)). This strategy is tractable because the denoiser is readily available from the score. However, this still does not provide useful information early on in the dynamics, as the denoiser is only effective at producing samples close to the data distribution later in the evolution. Flow map look-ahead: Intuitively, the above dynamics are better if one works instead with the flow map defined in the previous section. Because the flow allows us to look ahead at any point on the trajectory, e.g. by taking xt at time 5 and computing Xt,1(xt), and because the velocity field associated to the flow map is accessible via limst tXs,t(x) = vt,t(x), we can instead sample with the following SDE: dxt = vt,t(xt)dt + ϵt [st(xt) + txtr(Xt,1(xt))] dt + 2ϵtdWt, x0 ρ (13) where rt(x) = tr(Xt,1)(x) makes use of the exact look-ahead to properly evaluate the reward for any t, even = 0. The above could be interpreted as continuous deformation of how the 1-step flow map would evolve under ascent on the reward."
        },
        {
            "title": "2.3 CORRECTING THE DYNAMICS FOR UNBIASED SAMPLING",
            "content": "While using the flow map composed with the reward makes possible the precise use of the reward for all times in the diffusion trajectory, in applications where it is important to exactly sample the tilted distribution ˆρt, the dynamics in (13) are not sufficient to fulfill this. This gives rise to the second issue, for any version of rt(x), with or without the flow map: The PDF associated with (12) is not the tilted density ˆρt. To see why this is true, note that we can explicitly compare the PDF ρt of xt to that of ˆρt. Indeed, the PDF ρt(x) of xt satisfies: ρt + (bt ρt) = ϵt ([st rt]ρt + ρt), (14) and we can show explicitly that ˆρt satisfies different, imbalanced equation which can be obtained by expanding ˆρt + (bt ˆρt): ˆρt + (bt ˆρt) = t(ert+ ˆFtρt) + (btert+ ˆFtρt) = (trt + ˆFt)ˆρt + bt rt ˆρt (15) where we used the FPE (8) with ϵt = 0 to get the last equality. Since ˆρt = (st + rt)ˆρt we can add the diffusion term ϵt ((st + rt)ˆρt + ˆρt) = 0 to (15) to arrive at ˆρt + (bt ˆρt) = ϵt ((st rt)ˆρt + ˆρt) + (bt rt + trt + ˆFt)ˆρt. (16) As we can see, the extra term (bt rt + trt + ˆFt)ˆρt on the RHS of this equation differentiates it from being the law of (12). Nonetheless, we can account for this term with weights At emerging as the solution of different differential equation coming from an adaptation of the Jarzynski equality (Jarzynski, 1997; Vaikuntanathan & Jarzynski, 2008): Proposition 2.1 (Jarzynskis estimator). Assume that r0 = 0 so that ρr SDE (12) with x0 ρr 0 = ρ0. Let xt solve the 0 and define (cid:90) At = (bs(xs) rs(xs) + srs(xs)) ds, 0 Then for all [0, 1] and any test function : Rd R, we have (cid:90) Rd h(x)ˆρt(x)dx = E[eAth(xt)] E[eAt] , (17) (18) where the expectations at the right-hand side are taken over the law of = (xt)t[0,T ]. The proof of this statement in Section A.1 relies on manipulating the augmented FPE of the joint PDF ft(x, a) of (xt, At). This relation ensures that the lag associated to naively using the gradient of the reward in the diffusion can be compensated for by reweighting the trajectories, and, in addition, these weights can be used to perform resampling of the trajectories as is done in Sequential Monte Carlo (SMC) and birth/death processes, as is depicted in Figure 3. Here, as the trajectories walk out, the walkers can be resampled using the importance weights, removing some and duplicating others. Simplicity of importance weights with the flow map. Interestingly, the importance weights in (17) take on remarkably simple form when we use as rt(x) the reward composed with the flow map, as stated in the following proposition 6 Figure 5: Qualitative comparison on three basic geometric rewards (symmetry, anti-symmetry, rotation invariance). The gradient-based methods that change the generative dynamics produce sharper images that satisfy the constraints more reliably than prior methods. Proposition 2.2 (Unbiased Flow Map Trajectory Tilting). Using the same notations as in Proposition 2.1, if rt(x) = r(Xt,1(x)), then the importance weights defined (17) reduce to At = (cid:90) 0 r(Xs,1(xs))ds. (19) This result is proven in Section A.2, and relies on simple modification of the proof of Proposition 2.1 and the Eulerian equation (4). Thanks to the flow map, the complicated derivatives appearing in (17) reduce to simply compounding the reward over the look-ahead trajectory. Reward-modified drift. variant of Proposition 2.1 makes use of not only augmenting the score with the gradient of the reward with the look-ahead, but also the drift itself. Because vt,t is the velocity field of stochastic interpolant and is related to the score via (6), we can replace the SDE given in (13) with dxt = [vt,t(xt)+χtrt(xt)] dt + ϵt [st(xt)+rt(xt)] dt + where (χt)t[0,1] is arbitrary, and the log-weight ODE with 2ϵtdWt x0 ρ0, (20) At = (cid:0)bt(xt) rt(xt)+trt(xt)+χt (cid:0)rt(xt)2 +rt(xt)+rt(xt), st(xt)(cid:1)(cid:1) dt (21) where A0 = 0. It is proven in Appendix A.3 that these equations also provide an unbiased sampler of the tilted distribution by ensuring (18) holds for any choice of (χt)t[0,1], and three choices besides the default χt 0 are studied. This further augments the sampling process toward the tilt and will prove useful in the experiments below. Characterizing the effectiveness of the flow map trajectory tilting As discussed above, the dynamics (13)-(19) and (35)-(21) are simulated via SMC, which involves system of particles, time discretization (tk)K k=1 and (random) number of resampling steps K. SMC algorithms naturally yield an unbiased estimate ˆZSMC of the normalization constant E[eAt], and low variance Var[ ˆZSMC] is proxy for an efficient sampling schedule, as it signals that the empirical distribution 7 Figure 6: Comparison of MNIST tilted sampling to generate digits that would be classified as zeros. Left: using (12)-(17) with no look-ahead. Center: Doing the same with the denoiser composed with the reward. Right: Doing the same with the flow map i.e. our method FMTT. FMTT has the lowest total discrepancy, and the smallest thermodynamic length. of the particles is close to ˆρ1. However, Var[ ˆZSMC] often takes exponentially high values, making it hard to approximate, and it depends on , K, and R. The following proposition introduces the thermodynamic length, quantity related to Var[ ˆZSMC] which does not suffer from these issues. informal). The variance Proposition 2.3 (Total discrepancy and thermodynamic length, Var[ ˆZSMC] can be expressed in terms of the number of particles , discretization steps K, and resampling steps R, and the total discrepancy D(T ) which depends on discretization schedk=0 and is computable in practice. For optimal , K(cid:112)D(T ) can be replaced by ule = (tk)K thermodynamic length Λ, which can be computed from the At updates and that is agnostic to , and R, and satisfies that Λ K(cid:112)D(T ). From sampling to search: making the most of rewards in practice. Notably, the importance weights defined in either (19) or (21) do not need to be used to perform exact sampling. They can also be used to perform various greedy search algorithms that search for samples with high reward. That is, we are free to use top-n sampling approaches in place of the resampling one would usually do in SMC. Unlike (Ma et al., 2025), this use of top-n still makes use of the gradient of the reward along the trajectory, while also making use of better signal thanks to the flow map."
        },
        {
            "title": "3 NUMERICAL EXPERIMENTS",
            "content": "Algorithm 1 details all the inference-time adaption techniques that we introduce. The base algorithm described in Proposition 2.2 corresponds to the choice ϵk = ϵtk and ηk = 0 for all = 0 : K, and Sampling = True (see Section C). 3.1 SAMPLING EXPERIMENTS ON MNIST To demonstrate that the thermodynamic length is meaningful diagnostic of the performance of the tilt, we compute the thermodynamic length on the problem of tilting an unconditional image generation model to class conditional one. This will allow us to show that the process driven by FMTT more efficiently samples the tilted distribution of interest. For sake of expediency to make the computation of the thermodynamic discrepancies and length calculable, we measure and compare these quantities on an MNIST experiment where the reward model is 0.1 times the likelihood that classifier assigns to an unconditionally generated image being zero. In Figure 9 we compare the three setups specified below equation (12) and we plot the average reward, which is the average log-likelihood of being zero for the generated samples, the average class entropy assigned by the classifier. For the three algorithms, the ground truth values fall within the standard error bars, and the lowest total discrepancy and thermodynamic length correspond to FMTT. Section contains discussion of these results and additional experiments on MNIST. 8 Table 1: Quantitative results on GenEval. NFE denotes the total number of function evaluations used during generation. Method Mean Single Obj. Two Obj. Counting Colors Position Attr. Binding NFE Diffusions + Flow Maps FLUX.1 [dev] Flow Map Gradient-Free Search FLUX.1 [dev] + Best-of-N Flow Map + Best-of-N Flow Map + Multi-best-of-N Flow Map + Beam Search Gradient-Based Search Flow Map + ReNO FMTT (Ours) FMTT - 1-step denoiser look-ahead FMTT - 4-step diffusion look-ahead 0.65 0.62 0.75 0.73 0.76 0.75 0.71 0.79 0.75 0.75 0.99 0. 0.99 1.00 1.00 1.00 0.98 1.0 0.99 0."
        },
        {
            "title": "3.2 TEXT-TO-IMAGE EXPERIMENTS",
            "content": "0.78 0.72 0.94 0.88 0.95 0.92 0.89 0.97 0.90 0.93 0.70 0.63 0.83 0.82 0.84 0.86 0.79 0.90 0.87 0. 0.78 0.80 0.86 0.85 0.85 0.85 0.89 0.91 0.87 0.89 0.18 0.19 0.26 0.25 0.26 0.29 0.20 0.30 0.26 0. 0.45 0.39 0.57 0.59 0.69 0.58 0.57 0.64 0.59 0.57 180 16 1440 128 1280 1200 1280 1400 350 We evaluate our approach on text-to-image generation using the 4-step distilled flow map from Align Your Flow (Sabour et al., 2025), trained by distilling the open-source FLUX.1-dev model (Labs, 2024). For these experiments, Sampling = False, Searching = True is used in Algorithm 1. We consider three categories of reward functions: 1) Human preference rewards capturing visual quality and text alignment, 2) Geometric rewards enforcing structural constraints such as symmetry or rotation invariance, 3) VLM-based rewards defined through natural language queries. As baselines, we compare against gradient-free and gradient-based methods. Gradient-free approaches such as Best-of-N (Chatterjee & Diaconis, 2018), Multi-Best-of-N (Lee et al., 2025), and beam search (Fernandes et al., 2025) rely on sampling and selection, and remain confined to the base models distribution. Gradient-based methods use reward gradients, but differ in how they apply them: ReNO (Eyring et al., 2024) performs gradient ascent in the initial noise latent space, keeping samples tied to the base distribution, whereas our FMTT algorithm (and its ablations) use the gradient to modify the generative process itself, enabling exploration beyond the models support and generation of out-of-distribution samples. Notably, the gradient of the reward used in FMTT efficiently gives meaningful signal for the whole trajectory thanks to the flow map, enably OOD sample generation for highly nuanced rewards. Human Preference Rewards. To quantitatively benchmark FMTT, we follow prior work (Eyring et al., 2024) and use linear combination of PickScore (Kirstain et al., 2023), HPSv2 (Wu et al., 2023b), ImageReward (Xu et al., 2023), and CLIPScore (Radford et al., 2021) as the reward and perform evaluation on GenEval (Ghosh et al., 2023), which consists of 550 object-centric prompts and measures the quality of generated images using pre-trained object detector. Results in Table 1. The base model, FLUX.1-dev, achieves strong scores due to its training on large amounts of objectcentric data. Distillation into 4-step flow map slightly reduces performance but significantly accelerates generation. simple best-of-N search on top of the flow map recovers this drop and surpasses the base diffusion model while remaining about 30% faster. More advanced search methods, such as multi-best-of-N or beam search, yield additional but modest gains. Using reward gradients with FMTT provides further small improvement. It is important to note that the base FLUX model has already been post-trained with human preference data. As result, optimizing for the same types of reward during inference cannot substantially shift its output distribution, which explains why improvements across methods remain limited. Finally, we ablate the use of the 4-step flow map look-ahead by comparing FMTT against variants using either 1-step denoiser or 4-step diffusion sampler. The flow map look-ahead consistently performs best, in line with our earlier findings. Geometric Transformation Rewards. Recall that FLUX.1-dev has already been trained on human preference data, so its output distribution is already biased toward high preference rewards. This explains why much of the improvement in the previous experiments could be achieved with simple best-of-N search, with slight additional boost being obtained when using gradient-based methods. This changes when the reward function is more specialized and achieves high values only in the long 9 Figure 7: Qualitative comparison on masked rewards. Only our flow map-based FMTT reliably satisfies the constraints, concentrating content in the unmasked regions. tails of the base models output distribution. An example is reward that enforces invariance under simple geometric transformations, defined as r(x) = d(x, (x)) where (x) : Rd Rd is transformation function and d(, ) is distance metric. For example, if is masking operator, this reward incentivizes blackening the masked regions which can be used as way to position elements in the scene. Similar rewards can be defined for symmetry, anti-symmetry, rotation, and so on. Figure 5 shows that the base model roughly aligns with these objectives but does not fully satisfy them (the small planets break symmetry, the cats eyes arent anti-symmetric, and the koi fish have different colors so is not rotation invariant). Prior methods such as multi-best-of-N (Lee et al., 2025) and ReNO (Eyring et al., 2024) also fail, either breaking constraints or producing blurry images. In contrast, our gradient-based variants directly modify the dynamics, producing sharper outputs that more reliably satisfy the constraints. For harder case, we evaluate the masked reward in Figure 7. FMTT with denoiser look-ahead produces darker images with higher rewards than the base model, but fails to move all content to the unmasked region. Using flow map look-ahead, however, successfully maximizes the reward, generating images that fully satisfy the constraint. VLMs as Judge. We explore using pretrained VLMs to judge our images. The setup is straightforward: we provide the generated image along with binary yes/no question, and define the reward as sigmoid(logits[Yes] logits[No]). This formulation allows rewards to be expressed entirely in natural language. Since some VLMs accept multiple image inputs, we can also define rewards that depend on comparisons between the generated image and additional context images. In our experiments, we use Skywork-VL Reward (Wang et al., 2025a) for single-image settings and Qwen2.5-VL-7B-Instruct (Bai et al., 2025) for multi-image applications. We evaluate our method on the UniGenBench++ (Wang et al., 2025b) benchmark using the Skywork-VL reward model. This benchmark contains 600 short English prompts, each requiring 4 generated images. The resulting images are scored by the UniGenBench evaluation model, finetuned variant of Qwen2.5-VL-72B (Bai et al., 2025), across multiple dimensions including entity layout, text rendering, world knowledge, and more. We ask the following question of the VLM: Is {PROMPT} correct caption for the image? Please answer no if the image is not in high definition (i.e., clear, sharp, not pixelated, and not blurry). summary of the results is visualized as performance-vs-compute scaling plot in Figure 8. The plot shows that, when comparing the flow map look-ahead, the 1-step denoiser, and standard best-of-N sampling, the denoiser lookahead offers little to no improvement over the best-of-N baseline. In contrast, FMTT with flow map look-ahead consistently provides larger gains at lower computational cost. We attribute the 10 Figure 8: Scaling results on UniGenBench++ showing overall score versus compute. FMTT significantly outperforms both Best-of-N and Multi-Best-of-N , achieving higher scores at comparable number of function evaluations. Note that FMTT with 1-step denoiser look-ahead fails in beating the Best-of-N baseline, demonstrating the unhelpfulness of reward gradients at blurry denoised states. denoisers weak performance to its heavily blurred predictions at early, high-noise timesteps, which fall far off the data manifold and result in unhelpful reward gradients. See Figure 2 for side-by-side comparison of the different look-ahead strategies across differnet noise levels. For full breakdown of the various metrics, please see Table 2. Separately from the UniGenBench++ evaluation, we also include additional qualitative examples in Figure 4. The figure highlights two prompts where the base model fails to produce text-aligned outputs. When the VLM is used as reward to judge whether the prompt is correct caption for the image, our FMTT algorithm generates outputs that match the input text much more accurately. For additional VLM-based experiments, including multi-image settings, please see Section E. One caveat is the possibility of reward hacking (Amodei et al., 2016), as the search procedure explicitly maximizes the VLM reward. To mitigate this, the yes/no questions must be written with enough detail to prevent the algorithm from exploiting loopholes. Discussion and examples in Section F. Conclusions. We have presented FMTT, using flow maps for improved test-time scaling of diffusions. We envision that FMTT can overcome the limitations of common image generation systems when nuanced control is required or challenging rewards are given, for instance by VLMs."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We thank Cheuk-Kit Lee, Peter Holderrieth, Andrej Risteski, Stephen Huan, Max Simchowitz, and Kevin Xie for helpful discussions. MSA is supported by Junior Fellowship at the Harvard Society of Fellows as well as the National Science Foundation under Cooperative Agreement PHY-2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental Interactions, http://iaifi.org/). This work has been made possible in part by gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh International Conference on Learning Representations, 2022. Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. 11 Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Nicholas M. Boffi, Michael S. Albergo, and Eric Vanden-Eijnden. Flow map matching: unifying framework for consistency models. arXiv preprint arXiv:2406.07507, 2024. Nicholas M. Boffi, Michael S. Albergo, and Eric Vanden-Eijnden. How to build consistency model: Learning flow maps via self-distillation. arXiv preprint arXiv:2505.18825, 2025. Sourav Chatterjee and Persi Diaconis. The sample size required in importance sampling. The Annals of Applied Probability, 2018. Nicolas Chopin, Sumeetpal S. Singh, Tomas Soto, and Matti Vihola. On resampling schemes for particle filters with weakly informative observations. The Annals of Statistics, 2022. Chenguang Dai, Jeremy Heng, Pierre E. Jacob, and Nick Whiteley. An invitation to sequential monte carlo samplers. Journal of the American Statistical Association, 2020. Luca Eyring, Shyamgopal Karthik, Karsten Roth, Alexey Dosovitskiy, and Zeynep Akata. Reno: Enhancing one-step text-to-image models through reward-based noise optimization. Neural Information Processing Systems (NeurIPS), 2024. Guilherme Fernandes, Vasco Ramos, Regev Cohen, Idan Szpektor, and Joao Magalhaes. Latent beam diffusion models for decoding image sequences. arXiv preprint arXiv:2503.20429, 2025. Zhengyang Geng, Mingyang Deng, Xingjian Bai, Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025. Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. arXiv preprint arXiv:2310.11513, 2023. Roger Grosse, Chris Maddison, and Russ Salakhutdinov. Annealing between distributions by averaging moments. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2013. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 2020. Christopher Jarzynski. Nonequilibrium equality for free energy differences. Physical Review Letters, APS, 1997. Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. arXiv preprint arXiv:2310.02279, 2024. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Picka-pic: An open dataset of user preferences for text-to-image generation. Advances in neural information processing systems, 2023. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Gyubin Lee, Bao Nguyen Truong, Jaesik Yoon, Dongwoo Lee, Minsu Kim, Yoshua Bengio, and Sungjin Ahn. Adaptive inference-time scaling via cyclic diffusion search. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2022. 12 Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2022. Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, et al. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. Alireza Mousavi-Hosseini, Stephen Y. Zhang, Michal Klein, and Marco Cuturi. Flow matching with semidiscrete couplings. arxiv preprint arXiv:2509.25519, 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning. PmLR, 2021. Vignav Ramesh and Morteza Mardani. Test-time scaling of diffusion models via noise trajectory search. arXiv preprint arXiv:2506.03164, 2025. Amirmojtaba Sabour, Sanja Fidler, and Karsten Kreis. Align your flow: Scaling continuous-time flow map distillation. arxiv preprint arXiv:2506.14603, 2025. Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, and Rajesh Ranganath. general framework for inference-time scaling and steering of diffusion models. arXiv preprint arXiv:2501.06848, 2025. Marta Skreta, Tara Akhound-Sadegh, Viktor Ohanesian, Roberto Bondesan, Alan Aspuru-Guzik, Arnaud Doucet, Rob Brekelmans, Alexander Tong, and Kirill Neklyudov. Feynman-kac correctors in diffusion: Annealing, guidance, and product of experts. arXiv preprint arXiv:2503.02819, 2025. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency Models. arXiv preprint arXiv:2303.01469, 2023. Saifuddin Syed, Alexandre Bouchard-Cˆote, Kevin Chern, and Arnaud Doucet. Optimised annealed sequential monte carlo samplers. arXiv preprint arXiv:2408.12057, 2024. Suriyanarayanan Vaikuntanathan and Christopher Jarzynski. Escorted free energy simulations: Improving convergence by reducing dissipation. Physical Review Letters, 2008. Xiaokun Wang, Peiyu Wang, Jiangbo Pei, Wei Shen, Yi Peng, Yunzhuo Hao, Weijie Qiu, Ai Jian, Tianyidan Xie, Xuchen Song, et al. Skywork-vl reward: An effective reward model for multimodal understanding and reasoning. arXiv preprint arXiv:2505.07263, 2025a. Yibin Wang, Zhimin Li, Yuhang Zang, Jiazi Bu, Yujie Zhou, Yi Xin, Junjun He, Chunyu Wang, Qinglin Lu, Cheng Jin, et al. Unigenbench++: unified semantic evaluation benchmark for text-to-image generation. arXiv preprint arXiv:2510.18701, 2025b. Luhuan Wu, Brian Trippe, Christian Naesseth, David Blei, and John Cunningham. Practical and asymptotically exact conditional sampling in diffusion models. Advances in Neural Information Processing Systems, 2023a. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-toimage synthesis. arXiv preprint arXiv:2306.09341, 2023b. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: learning and evaluating human preferences for text-to-image generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems, 2023. 13 Tao Zhang, Jia-Shu Pan, Ruiqi Feng, and Tailin Wu. Vfscale: Intrinsic reasoning through verifierfree test-time scalable diffusion model. arXiv preprint arXiv:2502.01989, 2025. 14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 16 17 17 20 22 22 24 24 25 26 26 28 31"
        },
        {
            "title": "Appendix",
            "content": "A Proofs . . . . . . . . . A.1 Proof of Proposition 2.1 . A.2 Proof of Proposition 2.2 . . A.3 Modifying the drift with the flow map reward . . A.4 Solving the SDE/ODE system numerically . . . . . A.4.1 On the best-performing approach in practice . . A.5 Lemmas used in Section A.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Analyzing the performance of test-time sampling algorithms . . . . . . B.1 Simulating the dynamics with SMC . . B.2 The incremental and total discrepancies . . B.3 The variance of the SMC normalization constant . B.4 Optimizing the annealing schedule to minimize the total discrepancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Implementation Details MNIST experiments Additional VLM-as-judge examples VLM Reward Hacking Complete UniGenBench++ Results"
        },
        {
            "title": "A PROOFS",
            "content": "A.1 PROOF OF PROPOSITION 2.1 Consider the coupled SDE/ODE: dxt = (cid:0)bt(xt) + ϵtst(xt) + ϵtrt(xt)(cid:1)dt + dAt = (cid:0)bt(xt) rt(xt) + trt(xt)(cid:1) dt, 2ϵtdWt, x0 ρ0, A0 = 0. (22) Let ft(x, a) be the joint probability density of (xt, At). Then, it satisfies the Fokker-Planck equation tft(x, a) = (cid:0)(cid:0)bt(x) + ϵtst(x) + ϵtrt(x)(cid:1)ft(x, a)(cid:1) (cid:0)(cid:0)bt(x) rt(x) + trt(x)(cid:1)ft(x, a)(cid:1) + ϵtxft(x, a), ft(x, a) = ρ0(x)δ0(a). Observe that if we let ρt(x) = (cid:82) ft(x, a)ea da, ρ satisfies (23) ρt(x) = = (cid:90) (cid:90) tft(x, a)ea da (cid:0) (cid:0)(cid:0)bt(x) + ϵtst(x) + ϵtrt(x)(cid:1)ft(x, a)(cid:1) (cid:0)(cid:0)bt(x) rt(x) + trt(x)(cid:1)aft(x, a) + ϵtxft(x, a)(cid:1)ea da = (cid:0)(cid:0)bt(x) + ϵtst(x) + ϵtrt(x)(cid:1) (cid:90) ft(x, a)ea da(cid:1) (cid:0)bt(x) rt(x) + trt(x)(cid:1) (cid:90) aft(x, a)ea da + ϵtx (cid:18) (cid:90) (cid:19) ft(x, a)ea da = (cid:0)(cid:0)bt(x) + ϵtst(x) + ϵtrt(x)(cid:1)ρt(x)(cid:1) + (cid:0)bt(x) rt(x) + trt(x)(cid:1)ρt(x) + ϵtx ρt(x). (24) where the fourth inequality holds through integration by parts: (cid:90) aft(x, a)ea da = [ft(x, a)ea] (cid:90) ft(x, a)aea da = (cid:90) ft(x, a)ea da = ρt(x). We can reinterpret equation (24) as stating that for any test function h, (cid:90) Rd h(x)ρt(x) dx = (cid:90) (cid:90) Rd ft(x, a)eah(x) da dx = E[eAth(xt)]. (25) (26) However, ρt is not normalized density for 0: if we integrate both sides of (24) over Rd, and we use the divergence theorem, we obtain that (cid:90) Rd ρt(x) dx = (cid:90) Rd (cid:0)bt(x) rt(x) + trt(x)(cid:1)ρt(x) dx, (cid:90) Rd ρ0(x) dx = 1 (27) Rd ρt(x) dx, and we define ˆρt(x) = ρt(x)/Ft, we obtain that If we define Ft = (cid:82) ˆρt(x) = (cid:0)(cid:0)bt(x) + ϵtst(x) + ϵtrt(x)(cid:1) ρt(x) Ft (cid:1) + (cid:0)bt(x) rt(x) + trt(x)(cid:1) ρt(x) Ft + ϵtx ρt(x) Ft tFt Ft ρt(x) Ft , and by (27), if we define ˆFt = log Ft, we have that ˆFt = tFt Ft (cid:90) = Rd (cid:0)bt(x) rt(x) + trt(x)(cid:1) ρt(x) Ft dx = (cid:90) Rd (28) (cid:0)bt(x) rt(x) + trt(x)(cid:1)ˆρt(x) dx. (29) 16 Plugging this into the right-hand side of (28) and substituting ˆρt(x) = ρt(x)/Ft yields PDE for ˆρt which matches (16): ˆρt(x) = (cid:0)(cid:0)bt(x) + ϵtst(x) + ϵtrt(x)(cid:1)ˆρt(x)(cid:1) + (cid:0)bt(x) rt(x) + trt(x) ˆFt (cid:1)ˆρt(x) + ϵtx ˆρt(x). (30) To show that equation (18) holds, we rely on (26) and the fact that Ft = (cid:82) E[eAt]: Rd (cid:82) ft(x, a)ea da dx = (cid:90) Rd h(x)ˆρt(x) dx ="
        },
        {
            "title": "1\nFt",
            "content": "(cid:90) Rd h(x)ρt(x) dx = E[eAth(xt)] E[eAt] . A.2 PROOF OF PROPOSITION 2. When rt(x) = tr(Xt,1(x)), the log-weight At defined in (17) satisfies the ODE dAt dt = bt(xt) xtr(Xt,1(xt)) + (cid:0)t r(Xt,1(xt))(cid:1) = tbt(xt) Xt,1(xt)r(Xt,1(xt)) + r(Xt,1(xt)) + tXt,1(xt) r(Xt,1(xt)) = tr(Xt,1(xt)) (cid:0)tXt,1(xt) + Xt,1(xt)bt(xt)(cid:1) + r(Xt,1(xt)), The Eulerian identity states that tXt,1(x) + Xt,1(x)bt(x) = 0. (31) (32) (33) To prove (33), we write 0 = s(Xs,1 Xt,s)(x) = sXs,1(Xt,s(x)) + Xs,1(Xt,s(x))sXt,s(x), we use that sXt,s(x) = b(Xt,s(x)), and we set = t. Plugging (33) into the right-hand side of (32) yields dAt dt = r(Xt,1(xt)), (34) which concludes the proof. A.3 MODIFYING THE DRIFT WITH THE FLOW MAP REWARD While the position and log-weight dynamics in Proposition 2.1 and Proposition 2.2 are remarkably simple to compute, they are not the only ones that can be used to sample from the tilted distribution. In particular, if we add an additional term χtrt(xt) to the position SDE, we can adjust the logweights dynamics so that the coupled system still lets us compute expectations according to the tilted distribution ˆρt for all [0, 1]. Namely, the reward-modified position SDE reads: dxt = [vt,t(xt)+χtrt(xt)] dt + ϵt [st(xt)+rt(xt)] dt + 2ϵtdWt x0 ρ0, (35) There are four choices of χt of particular interest: (i) Default dynamics: χt = 0. This choice recovers the dynamics introduced in Section 2.3. (ii) Tilted score dynamics: χt = ηt := αt( βt αt αt). When the dynamics (1) has been βt learned using stochastic interpolants, the score st(x) and the vector field bt(x), or equivalently vt,t(x), are related to each other via the equation bt(x) = αt( βt βt αt αt)st(x) + βt βt (36) which (6) is particular case of, when αt = 1 t, βt = t. The score of the tilted distribution at time is ˆst(x) = log ˆρt(x) = st(x) + rt(x), and if we replace st(x) by ˆst(x) in (36), we obtain bt(x) = αt( βt βt αt αt)ˆst(x) + βt βt 17 = vt,t(x) + αt( βt βt αt αt)rt(x). (37) (iii) Local tilt dynamics: χt = ϵt. If we let (x(k+1)hxkh) be the transition kernel for the Euler-Maruyama discretization of the SDE tilt dynamics arises dxt = (cid:0)vt,t(xt) + ϵtst(xt) + ϵtrt(xt)(cid:1)dt + (38) from tilting (x(k+1)hxkh) according to the rethe local the transition kernel r(x(k+1)hxkh) ward r(k+1)h(x(k+1)h), which yields (x(k+1)hxkh) exp(r(k+1)h(x(k+1)h)). By Lemma A.4, when 0, r(x(k+1)hxkh) can be viewed as the transition kernel for the SDE 2ϵtdWt, dxt = [vt,t(xt)+ϵtrt(xt)] dt + ϵt [st(xt)+rt(xt)] dt + which amounts to setting χt = ϵt in (35). (iv) Base dynamics: χt = ϵt. With this choice, the SDE (35) becomes 2ϵtdWt, dxt = [vt,t(xt)+ϵtst(xt)] dt + 2ϵtdWt, (39) (40) which does not involve the reward rt. The following proposition, which generalizes Proposition 2.1, shows the log-weight dynamics that we must use for generic reward multipliers χt, written in three different ways. Observe that Proposition A.1 (Unbiased Map Tilting with reward-modified vector field). Let (χt)t[0,1] be an arbitrary real valued function, and let rt be an arbitrary time-dependent reward. Let (xt, At) be solution of dxt = (cid:0)bt(xt) + χtrt(xt) + ϵtst(xt) + ϵtrt(xt)(cid:1)dt + dAt = (cid:0)bt(xt) rt(xt)+trt(xt)+χt (41) (cid:0)rt(xt)2 +rt(xt)+rt(xt), st(xt)(cid:1)(cid:1) dt, A0 = 0. (42) x0 ρ0, 2ϵtdWt, Then for all [0, 1] and any test function : Rd R, we have (cid:90) Rd h(x)ˆρt(x)dx = E[eAth(xt)] E[eAt] , (43) where the expectations at the right-hand side are taken over the law of (x, A). The equation (42) for At can be rewritten as dAt = (cid:0)bt(xt) rt(xt)+trt(xt)+χt χt 2ϵt rt(xt) dWt χt 2ϵt + (cid:0)rt(xt)2 +rt(xt), st(xt)(cid:1)(cid:1) dt rt(xt) dWt, A0 = 0. Equation (42) can be rewritten in third way as dAt = (cid:40) ds E[ers(X χ )rt(xt) χ )(cid:12) ds rs(X 0 ds (cid:12)X 0 (cid:0)2 t =xt s=t = xt](cid:12) (cid:12)s=t dt, E[ers(X χ )rt(xt) χ = xt](cid:12) (cid:12)s=t (cid:1) dt, if χt 0, if χt < 0, A0 = 0, where χ is solution of the SDE = (cid:0)bt(X χ and 0 is solution of the ODE dX dX χ ) + χtst(X χ = bt(X χ ) dt. )(cid:1)dt + (cid:112)2χtdWt, (44) (45) (46) Proof. (i) Proof of equation (43). When we replace bt(x) by bt(x) = bt(x) + χtrt(x), the analog of equation (15) is: ˆρt + (bt ˆρt) = t(ert+ ˆFtρt) + (btert+ ˆFtρt) = (trt + ˆFt)ˆρt + ert+ ˆFt tρt + (cid:0)bt + χtrt = (trt + ˆFt)ˆρt + bt rt ˆρt + χtrt2 ˆρt + χtert+ ˆFt (rtρt) = (cid:0)bt rt + trt + χt (cid:0)rt2 + rt + rt, st(cid:1) + ˆFt (cid:1)ˆρt, (cid:1) rt ˆρt + ert+ ˆFt ((bt + χtrt)ρt) 18 (47) where the third equality holds because tρt + (btρt) = 0 by the FPE (8) with ϵt 0, and in the fourth equality we used the definition st(x) = log ρt(x). Hence, when we replace bt by bt, the terms bt rt + trt get replaced by bt rt + trt + χt (cid:0)rt2 + rt + rt, st(cid:1). Given equation (47), the proof of the statement (43) is analogous to the proof of Proposition 2.1 in (cid:0)rt2 + Section A.1, simply replacing bt by bt + χtrt, and bt rt + trt by bt rt + trt + χt rt + rt, st(cid:1). We omit full proof. (ii) Proof of equation (44). We proceed to prove that (44) is equivalent to (42). Observe that the solution of equation (42) is At = (cid:90) 0 (cid:18) bt(xt) rt(xt)+trt(xt)+χτ (cid:0)rτ (xτ )2 +rτ (xτ )+rτ (xτ ), sτ (xτ )(cid:1) Next, we handle the term rt. Applying Lemma A.5, we obtain that (cid:90) (cid:90) χτ rτ (xτ ) dτ = τ χτ (r Xτ,1)(xτ ) dτ 0 = (cid:90) t τ χτ 2ϵτ 0 (r Xτ,1)(xτ ) dWτ (cid:90) t τ χτ 2ϵτ (r Xτ,1)(xτ ) dWτ , And plugging this back into (48) yields equation (44). (cid:19) dt. (48) (49) (iii) Proof of equation (45). Finally, we prove that (45) is equivalent to (42). We introduce the necessary concepts first. The semi-group for (time-inhomogeneous) Markov process is defined as Ps,tf (x) = E(cid:2)f (Xt) (cid:12) (cid:12) Xs = x(cid:3) , The (time-dependent) infinitesimal generator is defined by ds Pt,t+hft+h(x) ft(x) Ltft(x) = lim = 0 t. E[fs(Xs) Xt = x](cid:12) (cid:12)s=t. (50) For the process χ that solves (46), the infinitesimal generator takes the form Lχ Rχ We define the Hamilton-Jacobi-Bellman (HJB) residual Rχ ft(x) = tft(x) + (bt(x) + χtst(x)) ft(x) + χtft(x). (51) for the reward rt and the process χ as (x) = bt(x) rt(x)+trt(x)+χt(cid:0)rt(x)2 +rt(x)+rt(x) st(x)(cid:1). (52) The term HJB residual stems from the fact that the Hamilton-Jacobi-Bellman equation for timedependent reward rt and process χ can be expressed as Rχ We start with the case χt 0. The term in the right-hand side of (42) can be rewritten as follows: bt(x) rt(x) + trt(x) + χt (cid:0)rt(x)2 + rt(x) + rt(x), st(x)(cid:1) = Rχ (x) = 0 for all and x*. (x) = ert(x)[Lχ ert](x) = ert(x) ds (cid:12)s=t. (53) where the second equality holds by Lemma A.6, and the third equality holds by the definition of the infinitesimal generator in (50). This concludes the case χt 0. (cid:12)s=t = E[ers(X χ )rt(x) χ E[ers(X χ ) χ = x](cid:12) = x](cid:12) ds We move on to the case χt 0. The term in the right-hand side of (42) can be rewritten as follows: (cid:0)rt(x)2 + rt(x) + rt(x), st(x)(cid:1) bt(x) rt(x) + trt(x) + χt = 2(cid:0)bt(x) rt(x) + trt(x)(cid:1) (cid:0)bt(x) rt(x) + trt(x) + χt (cid:0)ers(X 0 (x) Rχ (x) = 2 = 2R0 (cid:12)X 0 = 2 )(cid:12) rs(X 0 ds is defined as Rχ where R0 and Rχ . =x s=t (cid:0)rt(x)2 + rt(x) + rt(x), st(x)(cid:1)(cid:1) E[ers(X χ )rt(xt) χ = x](cid:12) (cid:12)s=t (54) ds ds )rt(x)(cid:1)(cid:12) (cid:12)X 0 =x s=t )rt(xt) χ ds = x](cid:12) (cid:12)s=t E[ers(X χ with χ = 0, and the last equality holds by equation (53) applied to R0 *Note that in stochastic optimal control, the HJB equation is usually written for the value function Vt = rt. The following corollary particularizes Proposition A.1 to the case in which the time-dependent reward is built from the flow map Xt,1, thus generalizing Proposition 2.2. Corollary A.2 (Unbiased Map Tilting with reward-modified vector field from flow map). When rt(x) = tr(Xt,1(x)), equations (42), (44) and (45) simplify respectively to dAt = (cid:0) rt(xt) +χt (cid:0)rt(xt)2 +rt(xt)+rt(xt), st(xt)(cid:1)(cid:1) dt, A0 = 0. (55) dAt = (cid:0) rt(xt) χt 2ϵt + +χt (cid:0)rt(xt)2 +rt(xt), st(xt)(cid:1)(cid:1) dt rt(xt) dWt χt 2ϵt rt(xt) dWt, A0 = 0. (56) dAt = (cid:40) sE[ers(X χ (cid:0) 2rt(xt) )rt(xt) χ sE[ers(X χ = xt](cid:12) )rt(xt) χ (cid:12)s=t dt, = xt](cid:12) (cid:12)s=t (cid:1) dt, if χt 0, if χt < 0, A0 = 0, (57) Proof. Using the argument in Section A.2 yields bt(x) rt(x) + trt(x) = r(Xt,1(x)) = rt(x) . (58) Substituting this into equations (42), (44) and (45) yields the simplified forms. Remark A.3 (Setting χt = 0 in Proposition A.1 and Corollary A.2). Proposition A.1 and Corollary A.2 show that the default choice χt = 0 is the only one for which the evolution of At simplifies. Namely, when χt = 0, equations (42), (44) and (45) in Proposition A.1 simplify respectively to dAt = (cid:0)bt(xt) rt(xt)+trt(xt)(cid:1) dt, dAt = (cid:0)bt(xt) rt(xt)+trt(xt)(cid:1) dt, dAt = ds )(cid:12) rs(X 0 (cid:12)X 0 =xt s=t dt, and equations (55), (56), and (57) in Corollary A.2 become dAt = dAt = dAt = dt, rt(xt) rt(xt) )(cid:12) rs(X 0 dt, ds (cid:12)X 0 =xt s=t dt, (59) (60) (61) (62) (63) (64) A.4 SOLVING THE SDE/ODE SYSTEM NUMERICALLY We need to simulate the coupled dynamics on weights and positions numerically. The SDE (41) for xt can be solved via K-step Euler-Maruyama scheme with time discretization (tk)K k=0 and updates of the form Xk = Xk1 +(tk tk1)[btk1 (Xk1)+χtk1 rtk1 (Xk1) +ϵtk1 (stk1 (Xk1)+rtk1 (Xk1))] + (cid:112)2ϵk(tk tk1)ξk1, (65) where ξk1 (0, I). For generic χt, solving the dynamics for the log-weight At requires one of the following approaches, which correspond to equations (42), (44) and (45), respectively: (i) Estimating the Laplacian rt: If solving (42), we need to estimate the Laplacian rt(x), m=1 zm (cid:2)rt(x + which can be done using the Hutchinson trace estimator (cid:100)rt(x) = 1 2M ε εzm) rt(x εzm)(cid:3), where zm (0, I) or Rademacher, ε 1. Then, the log-weights can be simulated with Euler updates of the form: (cid:80)M Ak = Ak1 +(tk tk1)(cid:2)btk1 (Xk1) rtk1 (Xk1)+trtk1 (Xk1) +χtk1 (cid:0)rtk1 (Xk1)2 + (cid:92)rtk1 (Xk1) +rtk1 (Xk1), stk1 (Xk1)(cid:1)(cid:3) (66) 20 When χ(k1)h = 0, we do not need to compute the estimate the Laplacian, which makes the update much faster. (ii) Estimating the difference of forward and backward Itˆo integrals: If solving (44), we estimate (cid:90) tk tk1 χτ 2ϵτ rτ (Xk1) dWτ (cid:90) tk tk1 χtk (cid:115) tk tk1 2ϵtk Thus, the update reads rtk (Xk) ξk1 χtk rτ (xτ ) dWτ χτ 2ϵτ (cid:115) rtk1 (Xk1) ξk1. (67) tk tk1 2ϵtk1 Ak = Ak1 +(tk tk1) (cid:20) btk1 (Xk1) rtk1 (Xk1)+trtk1 (Xk1) +χtk1 (cid:0)rtk1 2 +rtk1 stk1 (cid:21) (cid:1)(Xk1) (cid:115) +χtk tk tk1 2ϵtk rtk (Xk1) ξn k1 χtk1 (cid:115) tk tk1 2ϵtk1 rtk1 (Xk1) ξn k1. (68) (iii) Estimating expectations of exp(rt): If solving (45), we approximate ds E[ers(X χ )rt(x) χ 1 (tk tk1) (cid:88) (cid:12)s=t = x](cid:12) (cid:18) exp rtk (x + (tk tk1)(btk1 (x) + χtk1 stk1 (x)) m=1 (cid:113) + 2χtk1 (tk tk1)ξ(m) k1) rtk1 (x) (cid:19) , ξ(m) k1 (0, I). (69) where we used finite-difference approximation of the derivative, and the Euler-Maruyama update corresponding to the SDE (46). Thus, for χ(k1)h 0 the update reads Ak = Ak1 + 1 M (cid:88) m=1 (cid:18) exp rtk (Xk1 +(tk tk1)(btk1 (Xk1)+χtk1 stk1 (Xk1)) + Similarly, (cid:113) 2χtk1 (tk tk1)ξ(m) k1) rtk1 (Xk1) (cid:19) , ξ(m) k1 (0, I). (70) )(cid:12) rs(X 0 ds Thus, for χ(k1)h 0 the update reads Ak = Ak1 +2(cid:0)rtk (Xk1 + (tk tk1)btk (Xk1)) rtk1 (Xk1)(cid:1) rtk (x + (tk tk1)btk (x)) rtk (x) tk tk1 =x s=t (cid:12)X . (71) 1 (cid:88) m=1 (cid:18) exp rtk (Xk1 +(tk tk1)(btk1 (Xk1)+χtk1 stk1 (Xk1)) +(cid:112)2ϵk(tk tk1)ξn k1) rtk1 (Xk1) (cid:19) , ξ(m) k1 (0, I). (72) Observe that for χ(k1)h = 0, equations (72) and (70) simplify to Ak = Ak1 +rtk (Xk1 + (tk tk1)btk (Xk1)) rtk1 (Xk1), (73) which makes the update much less expensive. 21 Observe that following Corollary A.2, when rt(x) = tr(Xt,1(x)) the terms btk1 (Xk1) rtk1 (Xk1)+trtk1 (Xk1) in (66) and (68) can be replaced by rtk1 (Xk1) tk1 . A.4.1 ON THE BEST-PERFORMING APPROACH IN PRACTICE When χt = 0, the approach (i) requires evaluating the gradient rt at points to obtain an estimate with variance Θ(1/M ), and the approach (iii) requires evaluating rt at points to obtain an estimate with variance Θ(1/M ). Meanwhile, the approach (ii) only requires evaluating the gradient rt at the iterates (Xk)K k=0. Hence, priori, the approach (ii) is preferable. However, when the reward functions is neural network, such as classifier, vision-language model, or CLIP model, the gradients are very noisy as is highly non-smooth: if we perturb sample with an amount of Gaussian noise small enough that the noiseless sample and the noised sample ˆx have very similar reward values, and compute cosine similarity of the gradients r(x) and r(ˆx), we generally obtain low values ( 0.5). The noisy gradients cause the difference of integrals in (67) to be poor, as the gradients rtk (Xk) and rtk1 (Xk1) are much more misaligned than they would be if the function was smoother. The noisy gradient issue also affect approach (i), but it does not affect approach (ii), which only relies on evaluations of rt but not its gradient. The issue can be mitigated by replacing the gradient evaluations rt by averages of the evaluations on noised samples, effectively estimating the gradient of rt convolved with Gaussian. However, running approach (ii) with this mitigation requires using several evaluations of rt anyway. In practice, we observe that approach (iii) performs best, and that is the approach we used in our MNIST experiments in Section 3 and in Section D. A.5 LEMMAS USED IN SECTION A.3 Lemma A.4 (Simulating the local tilt dynamics). Consider the Euler-Maruyama discretization of the SDE dxt = bϵ t(x) = bt(x)+ϵtst(x)+ϵtrt(x), which corresponds t(xt)dt+ to the conditional density 2ϵtdWt, where bϵ (x(k+1)hxkh) = exp (cid:0) x(k+1)h xkh hbϵ (4πhϵkh)d/2 kh(xkh)2/(2hϵkh)(cid:1) . (74) If we define the conditional density r(x(k+1)hxkh) (x(k+1)hxkh) exp(r(k+1)h(x(k+1)h)), in the limit 0, this conditional density can be written as r(x(k+1)hxkh) = exp (cid:0) x(k+1)h xkh h(bϵ kh(xkh) + ϵkhrkh(xkh))2/(2hϵkh)(cid:1) (4πhϵkh)d/2 which is the conditional density for the Euler-Maruyama discretization of the SDE dXt = (bϵ t(Xt) + ϵtrt(Xt))dt + 2ϵtdBt. Proof. We use discrete version of Itˆos lemma: r(k+1)h(x(k+1)h) rkh(xkh) + h(cid:0)trkh(xkh) + ϵkhrkh(xk)(cid:1) + rkh(xk), x(k+1)h xkh + O(h3/2). Thus, r(x(k+1)hxkh) (cid:18) exp (cid:18) exp x(k+1)h xkh hbϵ kh(xkh)2 + rkh(xk), x(k+1)h xkh + O(h3/2) (cid:19) 2hϵkh x(k+1)h xkh h(bϵ kh(xkh) + ϵkhrkh(xk))2 2hϵkh (cid:19) + O(h3/2) . 22 (75) , (76) (77) (78) (79) Lemma A.5. Assume that (xt)t[0,1] satisfies the SDE (41). We have that (cid:90) (cid:90) (cid:90) τ χτ (r Xτ,1)(xτ ) dτ = τ χτ 2ϵτ (r Xτ,1)(xτ ) dWτ τ χτ 2ϵτ (r Xτ,1)(xτ ) dWτ , (80) (81) (82) where the forward and backward Itˆo integrals are defined respectively as (cid:90) (cid:90) Hτ dWτ := lim π Hτ dWτ := lim π0 n1 (cid:88) k=0 n1 (cid:88) k=0 Htk (cid:0)Wtk+1 Wtk (cid:1), Htk+1 (cid:0)Wtk+1 Wtk (cid:1). Here, (Hs)tst is process adapted to the filtration induced by the Brownian motion such that E[(cid:82) Hs2 ds] < +, π = {t = t0 < t1 < < tn = t} is partition of [t, t] with mesh π = maxk(tk+1 tk), and the limits are L2 limits. Proof. By definition of the forward and backward Itˆo integrals, (cid:90) Hτ dWτ (cid:90) 0 Hτ dWτ = lim π0 0 n1 (cid:88) (cid:0)Htk+1 Htk (cid:1) (cid:0)Wtk+1 Wtk (cid:1) := [H, ]t, (83) k=0 where [H, ]t is known as the quadratic variation of and . Let us set Hτ = τ χτ 2ϵτ Xτ,1)(xτ ) = γτ (r Xτ,1)(xτ ), where we defined γτ = τ χτ 2ϵτ . By Itˆos lemma, (r d(γτ (r Xτ,1))(xτ ) (cid:18) = γt t(r Xt,1)(xτ ) + 2(r Xt,1)(xτ ) (cid:0)bt + ϵtst ϵtrt (cid:19) (cid:1)(xτ ) + ϵt 2(r Xt,1)(xτ ) dt + γt(r Xt,1)(xτ ) dt + 2ϵtγt2(r Xt,1)(xτ )dWt. (84) When we simplify the quadratic variation, only the stochastic term survives: [H, ]t = lim π0 = lim π0 n1 (cid:88) k=0 n1 (cid:88) k= (cid:112)2ϵtk γtk 2(r Xtk,1)(xtk )(Wtk+1 Wtk ), Wtk+1 Wtk (cid:112)2ϵtk γtk Tr(cid:0)2(r Xtk,1)(xtk )(cid:1)(tk+1 tk) = (cid:90) 0 2ϵτ γτ (r Xτ,1)(xτ ) dτ = (cid:90) 0 τ χτ (r Xτ,1)(xτ ) dτ, (85) which concludes the proof. Lemma A.6. The infinitesimal generator Lχ fulfill: defined in (51) and the HJB residual defined in (52) [Lχ ert](x) = ert(x)Rχ (x). Proof. Applying the Hopf-Cole transformation, we obtain that tert(x) = ert(x)trt(x) = ert(x)(cid:0)Rχ (x) bt(x) rt(x)χt(cid:0)rt(x)2 +rt(x)+rt(x) st(x)(cid:1)(cid:1). And observe that (bt(x) + χtst(x)) ert(x) + χtert(x) = ert(x)(cid:0)bt(x) rt(x)+χt(cid:0)rt(x)2 +rt(x)+rt(x) st(x)(cid:1)(cid:1). 23 (86) (87) (88) Plugging this into (87) yields tert(x) = ert(x)Rχ (x) (bt(x) + χtst(x)) ert(x) χtert(x). And plugging the expression of the infinitesimal generator in (51) into (89) yields 0 = ert(x)Rχ (x) [Lχ ert](x). (89) (90) ANALYZING THE PERFORMANCE OF TEST-TIME SAMPLING ALGORITHMS B.1 SIMULATING THE DYNAMICS WITH SMC The natural approach to handle the weights eAt in Proposition 2.2 and in Proposition A.1 is sequential Monte Carlo (SMC), which is implemented in Algorithm 1. Let = (tk)K k=0 be an annealing schedule satisfying 0 = t0 < < tK = 1. The SMC sampling procedure starts with particles X0 = (X 0 )n[N ] with wn 0 = 1. For each subsequent iteration [K], we produce Xk and wk by propagating, reweighting, and optionally resampling. In what follows, we use notation similar to the one of Syed et al. (2024). 0 ρ0, and initial weights w0 = (wn 0 )n[N ] drawn from the reference, Propagate. Evolve Xk1 forward with the Markov transition kernel Mtk1,tk (xk1) to obtain Xk = (X )n[N ], where Mtk1,tk (X For the dynamics of Proposition 2.2 and Proposition A.1, the Markov transition kernels are the Euler-Maruyama updates for the SDEs (13) and (35), respectively: k1)+rtk1 (X k1))](tk tk1) = k1). (91) X k1 + [btk1 (X + (cid:112)2ϵk(tk tk1)ξn k1 + [btk1 (X + (cid:112)2ϵk(tk tk1)ξn k1)+ϵtk1 (stk1 (X ξn k1 (0, I), k1)+χtk1 rtk1 (X ξn k1 (0, I), k1, k1, k = k1)+ϵtk1 (stk1 (X k1)+rtk1 (X k1))](tk tk1) Reweight. Update the weights using the incremental weight function gtk1,tk (xk1, xk): wk = (wn )n[N ], wn = wn k1, k1 gtk1,tk (X ) and weights wn k1, ). can be computed using the The incremental weight functions gtk1,tk (X framework of Section A.4: gtk1,tk (X An ) = exp (cid:0)An k1, k1 is computed for each particle using one of the approaches described An = exp (cid:0)An wn (95) (cid:1), (cid:1). k1 Here, the difference An in Section A.4. Resample (optional). On (possibly random) subset of iterations [K] determined by criterion depending on (Xk, wk), apply resampling step Xk resample(Xk, wk), to stabilize wk and favor propagation of particles with higher relative weight. Concretely, set an )n[N ] is random ancestor index vector with an , where ak = (an [N ] and P(an = wk) = wm j[N ] wj (cid:80) , [N ]. While in Algorithm 1 we allow for number of clones greater than one, for simplicity, the analysis we perform in this section is with = 1. 24 (92) (93) (94) 1 for all n. See (Chopin et al., 2022) for annealedAfter resampling, reset the weights via wn SMC-specific resampling schemes, and (Dai et al., 2020) for recent review of SMC samplers. The SMC procedure yields an unbiased estimate of the expectation (cid:82) Rd h(x)ˆρtk (x)dx for any test function and any [K]. Namely, if we let ρt be the unnormalized density as defined in Section A.1, recall that [k] denotes the subset of iterations where resampling step happens, and for we let wr be the weight prior to resetting to 1, we have that (cid:90) Rd h(x)ρtk (x)dx = (cid:20)(cid:18) (cid:89) rR[k1]"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) n=1 wn (cid:19) 1 (cid:88) (cid:21) h(X , ) wn n=1 (96) Setting 1 and recalling that ˆρtk (x) = ρtk (x)/ (cid:82) (cid:21) (cid:20) ˆZ (k) SMC h(X tk (cid:80)N 1 ) n=1 wn tk (cid:80)N n=1 wn tk 1 (cid:90) h(x)ˆρtk (x)dx = Rd E(cid:2) ˆZ (k) SMC (cid:3) Rd ρtk (x)dx, we obtain that , (cid:18) (cid:89) ˆZ (k) SMC = rR[k1]"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) n=1 wn (cid:19) (cid:18) 1 N (cid:88) (cid:19) . wn n=1 (97) If we set = K, we obtain that (cid:90) Rd h(x)ˆρ1(x)dx = (cid:20) ˆZSMC 1 (cid:80)N n=1 wn (cid:80)N 1 h(X ) n=1 wn (cid:3) E(cid:2) ˆZSMC (cid:21) , where ˆZSMC = (cid:89) rR 1 (cid:88) n=1 wn . (98) ˆZSMC is known as the SMC normalization constant. Observe that E[ ˆZSMC] = (cid:82) Rd ρ1(x)dx := Z. Thus, low Var[ ˆZSMC/Z] is proxy for good performance of the SMC procedure. In Section B.3 we reproduce result by Syed et al. (2024) that expresses Var[ ˆZSMC/Z] in terms of the parameters of the SMC procedure, using the concept of total discrepancy from Section B.2. B.2 THE INCREMENTAL AND TOTAL DISCREPANCIES Following Syed et al. (2024), we define the normalized incremental weight function Gt,t as Gt,t(x, x) = gt,t(x, x) E(X,X ) ˆρtMt,t [gt,t(X, )] (99) where gt,t is the unnormalized incremental weight function defined in (94) and Mt,t is the Markov transition kernel defined in (91). Given Gt,t from time to time t, the incremental discrepancy D(t, t) is defined as D(t, t) = log (cid:0)1 + Var(X,X ) ˆρtMt,t (cid:0)Gt,t(X, )(cid:1)(cid:1). (100) Given sequence of timesteps = (tk)K as the accumulated discrepancy between iterations and and D(T ) as the total discrepancy: k=0 with t0 = 0, tK = 1, tor k, define D(T , tk, tk) D(T , tk, tk) = (cid:88) k=k+1 D(tk1, tk ), D(T ) = D(T , 0, 1). (101) Observe that the incremental discrepancy can be expressed in terms of the first and second moments of gt,t(X, ): (cid:18) (cid:21)(cid:19) (cid:20) D(t, t) = log 1 + Var(X,X ) ˆρtMt,t gt,t(X, ) E(X ,X ) ˆρtMt,t [gt,t(X , )] (cid:2)gt,t(X, )2(cid:3) E(X,X ) ˆρtMt,t (cid:18) = log 1 + E(X,X ) ˆρtMt,t E(X ,X ) ˆρtMt,t [gt,t(X , )]2 = log E(X,X ) ˆρtMt,t (cid:2)gt,t(X, )2(cid:3) 2 log E(X,X ) ˆρtMt,t E(X ,X ) ˆρtMt,t [gt,t(X , )]2 (cid:2)gt,t(X, )(cid:3) (cid:2)gt,t(X, )(cid:3)2 (cid:19) 25 (102) We want to obtain consistent estimator for D(t, t). Following (Syed et al., 2024, Sec. 5.1), if we are using single SMC run with particles, and we let gn ), then the following estimator is consistent: = gtk1,tk (X k1, ˆDk = log ˆgk,2 2 log ˆgk,1 log ˆgk,0, where for {0, 1, 2}, ˆgk,i = (cid:88) wn (cid:0)gn (cid:1)i . n[N ] (103) To get consistent estimator using NR SMC runs, each with particles, we compute the normalization constant ˆZ (k,j) SMC in (97) for each SMC run = 1 : NR, and define ˆDk as in (103), but where ˆgk,i takes the form ˆgk,i = NR(cid:88) j=1 ˆZ (k,j) SMC and (n,j) , w(n,j) (cid:80) (cid:0)g(n,j) (cid:1)i n[N ] w(n,j) (cid:80) k1 n[N ] w(n,j) k1 , where g(n,j) = gtk1,tk (X (n,j) k1 , (n,j) ), (104) is the n-th particle and weight of j-th SMC run at iteration k. B.3 THE VARIANCE OF THE SMC NORMALIZATION CONSTANT The total discrepancy defined in equation (101) is related to the variance of the SMC normalization constant ZSMC via the following result, which was proven by Syed et al. (2024) as generalization of result of Dai et al. (2020): Theorem B.1 (Theorem 1, Syed et al. (2024)). Suppose that the following assumptions on the normalized incremental weights Gn ) defined in (99) hold: = Gtk1,tk (X k1, Assumption 1 (Integrability). For all [N ], [K], Gn has finite variance with respect to ˆρtk Mtk1,tk . Assumption 2 (Temporal indep.). For [N ], (Gn )t[T ] are independent. Assumption 3 (Particle indep.). For [K], (Gn )n[N ] are independent. Assumption 4 (Efficient local moves). For each [N ] and [k], Gn d= Gtk1,tk (Xk1, Xk), (Xk1, Xk) πtk1 Mtk1,tk . Assume also that > 1, D(T ) > 0. For every resample schedule TR = (tr)R unique 1 Reff E[R] such that (cid:32) ˆZSMC (cid:18) D(T ) Reff Reff 1. 1 Var exp (cid:33) = (cid:19) (cid:19) (cid:18) r=0, there exists (105) Moreover, Reff = 1 if and only if D(T , tr1, tr) a.s.= D(T ) for some [R], and Reff = E[R] if and only if is a.s. constant and D(T , tr1, tr) a.s.= D(T )/R for some [R]. Remark B.2. As remarked by Syed et al. (2024), Assumptions 14 constitute an idealized model similar to the one considered by other works in the area (Grosse et al., 2013; Dai et al., 2020). While Assumption 1 is weak, Assumptions 24 are not. Assumption 3 only holds when no resampling is performed, and Assumptions 24 only hold (approximately) when number of MCMC steps are interleaved with the SMC updates. However, Syed et al. (2024, Sec. 6.1) show that empirically, the scaling (105) is consistent with empirical observation. B.4 OPTIMIZING THE ANNEALING SCHEDULE TO MINIMIZE THE TOTAL DISCREPANCY As defined in (101), the total discrepancy depends not only on the continuous time dynamics for positions and weights, but also on the specific annealing schedule = (tk)K k=0. For fixed K, it is possible to characterize and find the annealing schedule that minimizes the total discrepancy. 26 Under technical regularity assumptions (see (Syed et al., 2024, Sec. 4.1)), the incremental discrepancy admits the asymptotic expansion Gt,t+t = 1 + St + o(t), and hence the local changes and variance of the incremental discrepancy Gt,t+t are encoded in St and its variance δ(t), defined as Gt,t Using this expansion, we can expand the incremental discrepancy as follows: δ(t) = Vart,t[St]. St = , (cid:12) (cid:12) (cid:12) (cid:12)t=t D(t, + t) = δ(t)t + O(t3). (106) Scheduler generators schedule generator is continuously twice-differentiable function φ : du φ(u) > 0. Given N, φ generates [0, 1] [0, 1] such that φ(0) = 0, φ(1) = 1, and φ(u) = an annealing schedule = (tk)K k=0 where tk = φ(uk), uk = . In the following, without loss of generality, we restrict our attention to schedules generated by schedule generator. By the mean value theorem, we have tk tk1 φ(uk) . Combining this with equation (106), we obtain D(tk1, tk) δ(φ(uk)) φ(uk)2 2 . (107) By summing over and using Riemann approximations, we can approximate D(T , tk, tk) and D(T ) in terms of E(φ, utk , utk ) and E(φ), defined as the integral of δ(φ(u)) φ(u)2, E(φ, u, u) = (cid:90) δ(φ(v)) φ(v)2 dv, E(φ) = E(φ, 0, 1). Proposition B.3 (Proposition 1, Syed et al. (2024)). Suppose Assumptions 5 to 8 hold. There exists CD(φ) > 0 such that, for k, [K], (cid:12) (cid:12) (cid:12) (cid:12) D(T , tk, tk) 1 E(φ, uk, uk) (cid:12) (cid:12) (cid:12) (cid:12) CD(φ) tk tk 3 . An immediate consequence of Proposition 1 is that, in the dense schedule limit as , the total discrepancy D(T ) is asymptotically equivalent to E(φ) . Hence, for fixed K, optimizing D(T ) with respect to is asymptotically equivalent to the following problem: min φ:[0,1][0,1] (cid:90) 1 0 δ(φ(u)) φ(u)2 du, s.t. (cid:90) 1 φ(u) du = 1. (108) Jensens inequality implies that (cid:90) 1 0 δ(φ(u)) φ(u)2 du (cid:112)δ(φ(u)) φ(u) du (cid:19)2 , (cid:18) (cid:90) 1 0 (109) with equality if and only if there exists constant Λ > 0 such that (cid:112)δ(φ(u)) φ(u) = Λ for a.e. in [0, 1]. Defining Λ(t) = (cid:90) 0 (cid:112)δ(u) du, (110) by the chain rule, we have equivalently that Λ = Λ(φ(t)) φ(t) = dt Λ(φ(t)) = Λ(φ(t)) = Λt = φ(t) = Λ1(Λt). (111) (cid:112)δ(u) du. Syed Setting = 1 in Λ(φ(t)) = Λt also implies that Λ = Λ(φ(1)) = Λ(1) = (cid:82) 1 et al. (2024) refer to Λ(t) and Λ as the local barrier and the global barrier associated to the SMC algorithm. We refer to Λ as the thermodynamic length associated to the algorithm. 0 change of the integration variable implies that for any schedule generator φ, Λ(φ(t)) = (cid:90) φ(t) (cid:112)δ(u) du = (cid:90) (cid:112)δ(φ(u)) φ(u) du, 0 = Λ(tk) = Λ(φ(uk)) = 0 (cid:90) uk (cid:112)δ(φ(u)) φ(u) du (cid:88) k=1 (cid:112)δ(φ(uk)) φ(uk) = (cid:88) k=1 (cid:112)D(tk1, tk), Λ = (cid:90) 1 0 (cid:112)δ(φ(u)) φ(u) du (cid:112)D(tk1, tk). (cid:88) k=1 (112) where the last equality holds by (107). This allows us to approximate the local and global barriers using (102) to compute the incremental discrepancy D(tk1, tk). Once we have an estimate ˆΛ of the barrier, Syed et al. (2024) propose to iteratively refine the annealing schedule by resetting tk ˆΛ1(ˆΛk/K). Observe that given the quantities (cid:80)k (cid:112)D(tk1, tk), we have that k=1 D(tk1, tk) and (cid:80)k k=1 (cid:88) k=1 D(tk1, tk) = D(T , 0, tk) 1 E(φ, u0, uk) 1 Λ(tk)2 1 (cid:18) (cid:88) k=1 (cid:112)D(tk1, tk) (cid:19)2 , Thus, the quantity (cid:18) (cid:80)k k=1 (cid:80)k (cid:112)D(tk1, tk) (cid:19)2 k=1 D(tk1, tk) (113) (114) should fall within [0, 1] and close to 1 when the annealing schedule is close to the optimal one."
        },
        {
            "title": "C IMPLEMENTATION DETAILS",
            "content": "The complete pseudocode of FMTT is given in Algorithm 1. 28 Algorithm 1 Inference-time adaptation of flow maps 1: Input: # simulation steps K, # resampling steps R, # particles , # particle clones C, time sequence (tk)k=0:K, sequences (ϵk)k=0:K and (χk)k=0:K = (0, I) i.i.d. Let 0 = (x0 ij)i=1:N,j=1:C, where (x0 2: for = 1 : , initialize x0 3: Clone the particles: 0 = (x0 4: if Sampling then for = 1 : and = 1 : C, initialize A0 5: for = 0 : 1 do 6: tk+1 tk 7: 8: for = 1 : and = 1 : do = xk ξk ij (0, I). xk+1 ij 2ϵktξk ij, ij) + χkrtk (xk ij + [vtk,tk (xk )i=1:N . ij = 0. ij)j=1:C are equal copies of x0 . ij) + ϵk(stk (xk Update particles ij))]t + ij) + rtk (xk 9: 10: 11: 12: 13: 14: 15: 16: if Sampling then Compute Ak+1 ij (70)-(72) from Section A.4. Note that simplifications occur when ξk 0 (Remark A.3). from Ak ij using approach (i) (66), approach (ii) (68) or approach (iii) end if end for if = 0 (mod K/R) and > 0 then if Sampling then Define probabilities pk = softmax(Ak) = (cid:0) exp(Ak Resample = (xk )i=1:N (cid:80)n j=1 pk (cid:80)C i=1 ij)/ (cid:80) ijδxk ij Resample / select particles ij)(cid:1) ij exp(Ak i=1:N,j=1:C i.i.d., or using Quasi-Monte Set Ak ij = 0. else if Searching then Select = (xk end if Clone the particles: (xk )i=1:N as the top-n samples among with respect to rtk (xk ij). ij)j=1:C i=1:N , where (xk ij)j=1:C are equal copies of xk . Carlo 17: 18: 19: 20: 21: end if 22: 23: end for 24: return"
        },
        {
            "title": "D MNIST EXPERIMENTS",
            "content": "We ran the MNIST experiments using flow map model that we trained from scratch, with stochastic interpolants αt = 1 t, βt = t. We use the unconditional flow map, which samples the distribution ρ1 of all MNIST digits. We run Algorithm 1 with = 128 particles and = 200 simulation steps. We ablate the following look-ahead approaches introduced in Section 2.2: Naive look-ahead / no look-ahead (NL): rt(x) = tr(x). Denoiser look-ahead (WD): rt(x) = tr(Dt(x)). Flow map look-ahead (FMTT): rt(x) = tr(Xt,1(x)). We set ϵt = αt = 1 t. We consider the four choices of χt described in Section A.3: Default: χt = 0. Tilted score dynamics: χt = ηt := αt( βt βt add an offset of 0.05 to βt in the denominator: χt = αt( 1.05(1t) t+0.05 . Local tilt dynamics: χt = ϵt = 1 t. Base dynamics: χt = ϵt = 1 + t. We run Sampling and Searching experiments. αt αt) = (1 t)(cid:0) 1t + 1(cid:1) = 1t βt+0.05 αt αt) = (1t)(cid:0) 1t . In practice, we t+0.05 +1(cid:1) = βt For Sampling experiments (Figure 9), we set r(x) = 0.1 log pθ(0x), where pθ is pre-trained MNIST classifier and log pθ(0x) is the probability that the classifier assigns to the digit 0 for the image x. Following Section 2, the target distribution that we aim to sample is the tilted probability distribution is ˆρ1(x) ρ1(x) exp (cid:0)0.1 log pθ(0x)(cid:1). We run the sampling experiments with = 1 clone. We perform the log-weight update in Line 10 using approach (iii) (see Section A.4.1 for discussion on the different approaches). We remark again that the choice χt = 0 is much faster computationally, as it does not require estimating expectations. For the other three choices of χt, we use = 400 noisy samples when computing (70) or (72). Instead of resampling every steps with fixed, we resample when the effective sample size, which is computed as (cid:80)N , falls below 0.85 . (cid:1) )2/(cid:0) (cid:80)N i=1 exp(Ak i=1 exp(Ak For Searching experiments (Figure 10), r(x) = 0.05 log pθ(0x). Note that in this case, there does not exist an explicit form for the target probability distribution, which is different for each algorithm. We run the simulation with = 2 clones, and do one resampling step after the 100th simulation step (in the middle of the generation). Sampling results In Figure 9, we show samples generated using each algorithm and plot four metrics. For each algorithm, the metrics were computed from 16 runs, each with = 128 particles, and the bars show the average value plus/minus the standard error. Average reward: this is the expected value of log pθ(0x) for the samples generated with the algorithm (without the multiplier 0.1). We also print the ground truth value, which we compute by sampling 400 128 = 51200 independent samples from the unconditional model, and reweighting them by the factor exp (cid:0)0.1 log pθ(0x)(cid:1) upon normalization. Class entropy: this is the average value of the entropy of the probability distribution (pθ(ix))9 i=0 induced by the classifier pθ for each generated sample x, i.e. (cid:80)9 i=0 log pθ(ix). We also print the ground truth value, which we compute by sampling 51200 samples and reweighting them accordingly. Total discrepancy: this is the quantity D(T ) = (cid:80)K k=1 D(tk1, tk) defined in equation (101), where D(tk1, tk) are computed using the procedure described in Section B.2. Thermodynamic length: this is the Λ = (cid:80)K k=1 (cid:112)D(tk1, tk) defined in (112) (strictly speaking, this is an approximation of the actual thermodynamic length). 30 (cid:12)X 0 We observe that the only χt choice for which the ground truth values for the average reward and the class entropy fall within the error bars is χt = 0. For this choice, the look-ahead approach that results in lower total discrepancy and thermodynamic length is FMTT. To understand why this is the case, recall that the evolution of the log-weights for χt = 0 is given by the ODE dAt dt = )(cid:12) ds rs(X 0 dt = 0, which implies that the total discrepancy and the thermodynamic length are zero, and it is easy to see that this is the only choice of rt for which they are zero. Thus, the total discrepancy and the thermodynamic length values can be regarded as measure of how close given rt is to := Xt,1. From their values in the first row of Figure 9, we deduce that the flow map look-ahead reward rt(x) = tr(Xt,1(x)) is the closest one to (x) = r(Xt,1(x)), which is unsurprising because they only differ by factor t. (x) := r(Xt,1(x)), then dAt . Observe that when rt(x) = =xt,s=t ds E[ers(X χ )rt(xt) χ For all other χt choices, the ground truth class entropy values fall outside of the error bars of the model. Moreover, for these other χt choices, the total discrepancy and thermodynamic length are substantially higher for FMTT than for NL and WD, and are lowest for WD. To understand why this is the case, observe that the evolution of the log-weights for χt = ηt and χt = ϵt is given by the ODE dAt dt = (cid:12)s=t. Note that when rt is the optimal time-dependent reward (x) := log E[exp(r1(X χ in the stochastic optimal control sense, i.e., rt(x) = = xt], we have that E[ers(X χ = xt] = 0 for all 0 1. Hence, for this choice of rt, the total discrepancy and the thermodynamic length are zero, and it is the only choice for which they are zero. Like before, the total discrepancy and the thermodynamic length values can be regarded as measure of how close given rt is to . From their values in the second and third row of Figure 9, we deduce that the denoiser look-ahead reward rt(x) = tr(Dt(x)) is the closest one to (x), and rt(x) = tr(Xt,1(x)) is the farthest apart. )rt(xt) χ = xt](cid:12) 1 ))X χ Searching results In Figure 10, we show samples generated using each algorithm and plot the average reward and average class entropy methods, which are also computed from 16 runs with = 128 particles each, as in the sampling case. In this case, there are no ground truth values to compare to. We observe that all else equal, FMTT with χt = ηt is the most effective choice in that it achieves the highest average reward and the lowest class entropy, hence sampling the digit 0 more often. Accordingly, FMTT with χt = ηt is the choice we make for our high dimensional search experiments throughout the paper. ADDITIONAL VLM-AS-JUDGE EXAMPLES As described in the paper, we use Qwen2.5-VL-7B-Instruct to define rewards expressed as yes/no questions over one or more context images. This makes it possible to cast diverse objectives as test-time search problems, including style consistency, character consistency, and multi-subject generation. Here, we demonstrate the style consistency case. The VLM receives both reference image and generated image and is asked whether they share the same art style. FMTT then optimizes this reward, producing generations more closely aligned with the reference style. Qualitative results are shown in Figure 11. Observe that rt(x) = (x) = r(Xt,1(x)) cannot be used in practice because we require that r0(x) is constant so that we can sample from ˆρ0 This holds by the path integral characterization of stochastic optimal control. 31 0 = χ η = χ ϵ = χ ϵ = χ Figure 9: Comparison of MNIST tilted sampling to generate digits that would be classified as zeros: the reward is r(x) = 0.1 log p(0x). The four rows correspond to the dynamics with different χt choices described in Section A.3."
        },
        {
            "title": "F VLM REWARD HACKING",
            "content": "As discussed in the paper, challenge of using VLMs (or any non-verifiable reward model) is the risk of the search process exploiting loopholes. This happens when the algorithm produces images that either act as adversarial examples for the VLM or satisfy the literal question without achieving the intended effect. Figure 12 shows such case. 32 0 = χ η = χ ϵ = χ ϵ = χ Figure 10: Comparison of MNIST greedy search to generate digits that would be classified as zeros: the reward is r(x) = 0.05 log p(0x). The four rows correspond to the dynamics with different χt choices described in Section A.3. Figure 12: VLM reward hacking. Instead of the clock being at 4:45, the search process finds way to cheat by writing the text 4:45 on the clock face and achieving high rewards from the VLM. 33 Figure 11: Style consistency via VLM-based rewards. Given reference image, FMTT produces images that better match its art style than the base model. We explored two solutions. The first is to craft the VLM prompt to be as verbose and unambiguous as possible, explicitly discouraging potential cheats. This works when only few edge cases exist, but becomes brittle when many (4+) conditions are needed, at which point the reward model grows opaque and the search converges to local maxima. The second approach is to decompose the binary question into several simpler sub-questions and define the reward as their sum. While this adds computational overhead by requiring multiple VLM inferences, it proved more robust in practice. For reference, to achieve the results in Figure 1, we used the following three questions: Is the hour hand pointing between 4 and 5? Is the minute hand pointing at 9? Is the second hand pointing at 12? COMPLETE UNIGENBENCH++ RESULTS Table 2 reports the complete per-dimension scores used by UniGenBench++, including entity layout, text rendering, world knowledge, spatial reasoning, and all other evaluation categories."
        },
        {
            "title": "LLM USAGE",
            "content": "In preparing this paper, we used large language models (LLMs) as assistive tools. Specifically, LLMs were used for (i) editing and polishing the text for clarity and readability, and (ii) generating some reference images that appear in some figures. All research ideas, experiments, and analysis were conducted by the authors. The authors take full responsibility for the content of this paper. 34 Table 2: Quantitative results on UniGenBench++. Entries show the percentage of images judged by the evaluation model to satisfy each criterion. The standard deviation is computed across the 4 images generated for every prompt. represents the number of distinct samples/particles used during generation. Method Diffusions FLUX.1 [dev] Gradient-Free Search FLUX.1 [dev] + Best-of-N - = 8 - = 16 - = 32 - = 64 - = 128 - = 256 Flow Map + Multi-best-of-N (10 rounds) - = 8 - = 16 - = 32 - = 64 - = 128 - = 256 Gradient-Based Search FMTT - 1-step denoiser look-ahead - = 4 - = 8 - = 16 - = 32 - = 64 - = 128 - = 256 FMTT - 1-step flow map look-ahead - = 16 - = 32 - = 64 - = 128 - = FMTT - 2-step flow map look-ahead - = 16 - = 32 - = 64 - = 128 - = 256 FMTT - 4-step flow map look-ahead - = 16 - = 32 - = 64 - = 128 - = 256 Mean Style World Knowledge Attribute Action Relationship Compound Grammar Logical Reasoning Entity Layout Text Generation NFE 61.78 3. 85.20 2.30 86.23 2.62 64.53 2.82 60.46 3.80 63.07 5.46 41.24 5. 60.96 2.95 24.08 3.63 67.16 4.32 30.17 5.77 180 65.40 2.56 67.57 1.53 69.18 0.87 70.93 1.38 72.77 1.26 73.75 1. 87.40 2.09 88.60 1.40 89.80 1.40 90.50 0.59 91.40 0.66 91.10 0.59 67.58 0.56 68.01 1.05 70.53 0.80 70.90 0.77 72.60 0.63 73.47 0.36 86.70 0.77 85.90 1.80 89.20 0.49 88.80 1.67 90.40 1.50 90.00 1.47 62.00 4.93 65.73 1.83 67.75 1.82 69.79 1.06 70.89 1.32 72.52 1.42 72.90 0.89 85.00 3.26 87.70 1.21 88.20 0.87 90.00 1.30 90.20 1.54 91.60 0.85 91.00 1.31 63.28 0.77 67.55 0.86 71.13 0.62 73.32 0.68 74.86 0. 85.10 0.95 88.60 0.82 89.60 0.63 91.90 0.52 91.90 0.77 64.73 0.74 68.04 1.34 71.46 0.40 74.31 0.78 75.55 0.53 87.40 2.09 89.60 0.57 90.10 1.07 91.20 1.20 91.90 0.77 65.90 0.45 68.12 0.62 71.48 0.29 74.32 0.32 75.14 0.80 86.90 1.21 89.00 1.00 89.80 0.72 90.90 0.71 91.40 1.18 89.72 1.75 90.82 1.30 90.82 1.30 90.98 0.82 91.77 0.45 92.88 0. 90.66 1.22 88.29 1.05 91.14 1.85 92.56 2.21 91.46 0.32 93.04 1.17 84.02 4.79 88.29 3.30 88.92 1.14 90.51 1.55 90.66 1.51 92.56 0.27 90.51 1.85 85.28 1.57 88.92 1.05 91.14 0.45 90.98 0.94 92.72 1.30 87.34 2.15 90.35 0.82 91.46 2.12 92.56 0.82 92.88 1.51 84.34 1.64 87.03 2.22 90.82 0.78 90.55 0.78 92.41 1.18 68.27 2.50 69.44 1.37 72.33 1.22 74.15 1.37 75.00 1.56 76.39 0. 62.26 3.64 64.92 2.33 66.06 1.30 67.97 0.73 70.15 1.38 71.58 1.83 70.41 1.06 72.33 1.97 74.04 1.67 73.18 1.49 75.75 1.33 77.35 1.24 67.40 1.87 67.97 1.24 70.06 2.99 70.63 1.53 70.91 0.87 75.00 2.55 68.02 4.38 70.81 3.10 72.97 1.98 75.00 0.66 74.24 2.36 75.51 1.54 71.32 2.05 71.57 1.52 73.73 2.57 73.98 2.34 79.70 2.06 76.27 0.92 47.29 1.80 52.71 2.48 54.64 1.59 56.96 4.13 60.95 2.20 61.86 3. 63.37 2.16 64.97 1.10 65.24 1.65 65.24 3.00 68.85 2.49 69.52 0.53 52.32 1.39 52.84 2.30 55.93 0.85 57.73 2.80 60.57 1.73 61.73 2.16 64.71 2.11 67.38 2.30 70.32 1.79 69.79 3.04 71.93 1.79 72.46 2.16 66.88 4.63 69.12 2.27 70.09 2.16 71.90 2.35 73.72 0.88 73.40 2.82 73.93 1.98 60.08 4.79 62.83 1.33 65.49 1.33 67.30 1.17 68.92 2.89 69.49 2.75 70.72 1.57 63.96 7.57 69.04 2.24 71.70 2.92 73.35 1.16 73.73 2.60 76.27 1.58 78.30 0. 42.91 5.79 47.81 3.54 49.23 3.45 52.71 1.56 56.06 1.80 58.12 2.92 60.31 0.82 58.96 3.99 62.83 3.04 64.57 3.77 67.38 1.81 66.58 1.58 69.52 2.30 69.65 3.37 67.09 2.58 68.06 1.69 73.40 2.17 76.39 2.01 77.78 1.89 60.93 1.55 65.97 1.46 67.87 2.26 71.29 1.46 71.86 0.60 67.13 1.31 70.94 0.83 75.25 1.98 75.63 1.61 77.41 2.17 45.36 3.75 51.93 1.17 57.35 1.52 59.79 1.67 62.50 1. 61.90 1.33 65.37 0.95 68.72 1.34 69.65 0.95 72.59 1.53 65.60 2.35 70.19 2.55 74.15 1.11 74.79 2.28 78.21 1.09 61.79 0.83 64.83 4.10 68.25 2.49 71.48 2.81 72.34 1.12 68.27 1.81 69.80 1.16 74.49 1.50 78.05 0.55 79.19 1.87 47.94 1.63 51.80 1.06 56.70 2.03 62.89 1.26 65.08 1.98 61.90 1.38 66.84 2.33 69.65 1.28 72.46 1.26 72.73 0. 68.27 1.55 70.51 1.17 72.86 0.48 78.31 0.56 77.03 1.72 63.97 1.43 65.02 2.56 69.39 0.69 70.63 0.78 71.39 2.16 69.92 1.81 71.07 1.83 74.49 2.19 76.65 1.19 79.19 1.48 49.23 1.65 52.19 1.33 56.83 2.20 60.44 1.72 65.08 3.29 63.90 0.96 66.84 0.38 69.12 0.95 74.20 1.38 72.46 2.32 29.36 3.24 31.19 2.34 31.88 1.00 38.30 4.12 41.97 2.86 45.18 3. 38.30 3.46 38.76 3.14 39.22 1.19 41.74 2.47 42.89 3.00 46.56 3.00 24.77 5.31 30.05 2.78 32.57 3.52 37.84 5.44 37.61 4.05 42.20 5.07 42.89 4.12 27.06 1.38 31.65 3.70 37.39 3.39 42.43 1.76 46.33 1.38 30.50 0.76 33.26 4.27 38.76 1.88 46.33 2.00 46.79 1.45 31.42 1.76 34.17 1.00 40.14 2.78 46.56 3.00 45.87 2.25 71.08 71.08 72.95 2.26 76.49 1.54 77.99 1.71 80.60 1.40 80.97 1. 76.49 1.63 77.05 1.43 79.29 2.44 79.29 3.23 78.73 2.87 79.66 2.14 69.59 7.13 72.57 0.97 76.49 3.05 77.05 2.07 78.36 3.03 81.53 2.55 81.53 1.70 69.22 1.43 75.37 3.34 77.05 2.66 80.04 1.93 81.72 2.14 71.27 3.10 73.69 2.50 77.99 2.87 80.78 2.32 81.53 1.10 75.37 0.53 74.81 2.14 78.92 0.97 80.22 0.83 80.41 1.10 35.06 5.66 36.49 4.01 39.37 4.55 42.82 3.29 44.54 5.71 44.83 3. 18.39 0.81 18.10 2.35 22.99 2.93 23.28 2.21 23.85 1.49 21.55 2.97 28.16 6.32 34.20 1.70 40.80 4.34 39.94 1.49 43.97 2.21 42.24 0.95 39.94 1.88 27.87 3.39 36.49 4.33 45.11 3.39 46.84 2.86 46.55 4.10 33.33 2.15 39.08 3.45 44.83 1.63 45.98 2.44 47.70 3.09 32.47 1.25 40.52 4.25 43.97 2.05 49.43 2.82 50.86 2.62 360 720 1440 2880 5760 320 640 1280 2560 5120 10240 180 360 720 1440 2880 5760 11520 240 480 960 1920 3840 360 840 1680 3360 6720 600 1560 3120 6240"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Courant Institute, New York University",
        "Harvard University",
        "IAIFI",
        "Kempner Institute",
        "ML Lab at Capital Fund Management (CFM)",
        "Microsoft Research",
        "NVIDIA",
        "University of Toronto",
        "Vector Institute"
    ]
}