{
    "paper_title": "Baking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable Single-stage Image-to-3D Generation",
    "authors": [
        "Yuanhao Cai",
        "He Zhang",
        "Kai Zhang",
        "Yixun Liang",
        "Mengwei Ren",
        "Fujun Luan",
        "Qing Liu",
        "Soo Ye Kim",
        "Jianming Zhang",
        "Zhifei Zhang",
        "Yuqian Zhou",
        "Zhe Lin",
        "Alan Yuille"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing feed-forward image-to-3D methods mainly rely on 2D multi-view diffusion models that cannot guarantee 3D consistency. These methods easily collapse when changing the prompt view direction and mainly handle object-centric prompt images. In this paper, we propose a novel single-stage 3D diffusion model, DiffusionGS, for object and scene generation from a single view. DiffusionGS directly outputs 3D Gaussian point clouds at each timestep to enforce view consistency and allow the model to generate robustly given prompt views of any directions, beyond object-centric inputs. Plus, to improve the capability and generalization ability of DiffusionGS, we scale up 3D training data by developing a scene-object mixed training strategy. Experiments show that our method enjoys better generation quality (2.20 dB higher in PSNR and 23.25 lower in FID) and over 5x faster speed (~6s on an A100 GPU) than SOTA methods. The user study and text-to-3D applications also reveals the practical values of our method. Our Project page at https://caiyuanhao1998.github.io/project/DiffusionGS/ shows the video and interactive generation results."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 2 ] . [ 1 4 8 3 4 1 . 1 1 4 2 : r Baking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable Single-stage Image-to-3D Generation Yuanhao Cai1, He Zhang2, Kai Zhang2, Yixun Liang3, Mengwei Ren2, Fujun Luan2, Qing Liu2, Soo Ye Kim2, Jianming Zhang2, Zhifei Zhang2, Yuqian Zhou2, Zhe Lin2, Alan Yuille1 1 Johns Hopkins University, 2 Adobe Research, 3 Hong Kong University of Science and Technology Figure 1. Generation results of our method. For objects, the prompt views are in the left dashed box. The generated novel views and Gaussian point clouds are depicted on the right. For scenes, our model can handle hard cases with occlusion and rotation, as shown in the dashed boxes of the third row. The text-to-3D demos are prompted by stable diffusion [59] and Sora [3] for objects and scenes, respectively."
        },
        {
            "title": "Abstract",
            "content": "Existing feed-forward image-to-3D methods mainly rely on 2D multi-view diffusion models that cannot guarantee 3D consistency. These methods easily collapse when changing the prompt view direction and mainly handle objectcentric prompt images. In this paper, we propose novel single-stage 3D diffusion model, DiffusionGS, for object and scene generation from single view. DiffusionGS directly outputs 3D Gaussian point clouds at each timestep to enforce view consistency and allow the model to generate robustly given prompt views of any directions, beyond object-centric inputs. Plus, to improve the capability and generalization ability of DiffusionGS, we scale up 3D training data by developing scene-object mixed training strategy. Experiments show that our method enjoys better generation quality (2.20 dB PSNR and 23.25 FID) and over 5 faster speed (6s on an A100 GPU) than SOTA methods. Project page: https://caiyuanhao1998. github.io/project/DiffusionGS/ 1. Introduction Image-to-3D generation is an important and challenging task that aims to generate 3D representation of scenes or objects given only single-view image. It has wide applications in AR/VR [30], film making [22], robotics [27, 89], animation [48, 57], gaming [40], and so on. Existing feed-forward image-to-3D methods are mainly two-stage [31, 37, 70, 72]. They firstly adopt 2D diffuFigure 2. Single-view object-level generation of our method on GSO [13], wild images, and text-to-images prompted by stable diffusion or FLUX. Our DiffusionGS can robustly handle hard cases with furry appearance, shadow, flat illustration, complex geometry, and specularity. sion model to generate blocked multi-view images and secondly feed the multi-view images into 3D reconstruction model. Without 3D model in the diffusion, these methods cannot enforce view consistency and easily collapse when the prompt view direction changes. Another less studied technical route [2, 67, 75] is to train 3D diffusion model with 2D rendering loss. Yet, these methods mainly rely on triplane neural radiance field (NeRF) [47]. The volume rendering of NeRF is time-consuming and the triplane resolution is limited, preventing the model from scaling up to larger scenes. In addition, current methods mainly study object-level generation using only object-centric datasets to train, while limits the model generalization ability and leaves larger-scale scene-level generation less explored. To address these issues, we propose novel single-stage 3D Gaussian Splatting (3DGS) [26] based diffusion model, DiffusionGS, for 3D object and scene generation from single view. Our DiffusionGS enforces 3D consistency of the generated contents by predicting multi-view pixel-aligned Gaussian primitives in every timestep. With the highly parallel rasterization and scalable imaging range, DiffusionGS enjoys fast inference speed of 6 seconds per asset and can be easily applied to large scenes. Since our goal is to build general and large-scale 3D generation model, it is critical to fully exploit existing 3D scene and object data. However, directly training with scene and object data may lead to non-convergence because of the large domain discrepancy. Thus, we propose scene-object mixed training strategy to handle this problem and learn general prior of geometry and texture. Our mixed training strategy adapts DiffusionGS to both object and scene datasets by controlling the distribution of the selected views, camera condition, Gaussian point clouds, and imaging depths. In particular, we notice previous camera conditioning method Plucker coordinate [55] shows limitations in capturing depth and 3D geometry. Hence, we design new camera conditioning method, Reference-Point Plucker Coordinates (RPPC), that encodes the point on each ray closest to the origin of the world coordinate system to help DiffusionGS better perceive the depth and 3D geometry across scene and object data. Finally, the mixed training weights are further finetuned on object or scene data to boost the performance. Figure 3. Single-view scene generation results of our method on the indoor (upper) and outdoor (lower) scenes with rotation and occlusion. Our contributions can be summarized as follows: We propose novel 3DGS-based diffusion model, DiffusionGS, for single-view 3D object and scene generation. We design scene-object mixed training strategy to learn more general prior from both 3D object and scene data. We customize new camera pose conditioning method, RPPC, to better perceive the relative depth and geometry. DiffusionGS yields more visually pleasant results and outperforms previous methods by 2.20 dB/23.25 and 2.91 dB/75.68 in PSNR/FID on objectand scene-level generation with fast speed of 6s on single A100 GPU. 2. Related Work 2.1. Diffusion Models for Image-to-3D Generation Diffusion models [20, 46, 59, 65, 66] are proposed for image generation and recently have been applied to 3D generation, which can be divided into four categories. The first category [18, 24, 51, 52, 62, 85, 90] uses direct supervision on 3D models such as point clouds or meshes, which are hard to obtain in practice. The second kind of methods [29, 33, 58, 69, 71, 78, 79, 88] use SDS loss [56] to distill 3D model from 2D diffusion. Yet, these methods require time-consuming per-asset optimization. The third category [5, 17, 38, 41, 42, 61] adds the camera poses as the input condition to finetune 2D diffusion model to render fixed novel views. Yet, these methods cannot guarantee 3D consistency and easily collapse when prompt view direction changes. The last category [2, 67, 75] trains 3D diffusion model with 2D rendering loss. However, these methods mainly based on triplane-NeRF suffer from the limited resolution of triplane and slow speed of volume rendering. 2.2. Gaussian Splatting 3DGS [26] uses millions of Gaussian ellipsoid point clouds to represent objects or scenes and render views with rasterization. It achieves success in 3D/4D reconstruction [4, 10, 14, 15, 32, 44, 73, 77, 81, 83, 92], generation [9, 19, 21, 29, 33, 39, 49, 79, 85, 90], inverse rendering [23, 34, 74], SLAM [25, 45, 76, 84], etc. For instance, GS-LRM [87] develops regression model to reconstruct 3DGS from four views with camera poses. Yet, deterministic methods cannot generate diverse samples and suffer from view misalignment issue when chained with 2D multi-view diffusion. 3. Method Fig. 4 depicts the pipeline of our method. Fig. 4 (a) shows the scene-object mixed training. For each scene or object, we pick up view as the condition, views as the noisy views to be denoised, and novel views for supervision. Then in Fig. 4 (b), the clean and noisy views are fed into our DiffusionGS to predicts per-pixel 3D Gaussian primitives. 3.1. DiffusionGS Preliminary of Diffusion. We first review denoising diffusion probabilistic model (DDPM) [20]. In the forward noising process, DDPM transforms the real data distribution x0 q(x) to standard normal distribution (0, I) by gradually applying noise to the real data x0 : q(xtx0) = (xt; αtx0, (1 αt)I) at every timestep [0, ], where αt are pre-scheduled hyper-parameters. Then xt is sampled by xt = αtx0 + 1 αtϵt, where ϵt (0, I). The denoising process reverses the forward process by gradually using neural network to predict ϵt. Similarly, 2D multiview diffusion [37, 38, 61, 70] generates novel views by denoising images or latents at multiple viewpoints. However, these 2D diffusions do not have 3D models, thus suffering from view misalignment and easily collapsing when the prompt view direction changes. We solve these problems by baking 3D Gaussians into the diffusion denoiser. Our 3D Diffusion. Different from the normal diffusion model that predicts noise, our DiffusionGS aims to recover clean 3D Gaussian point clouds. Thus, we design the denoiser to directly predict pixel-aligned 3D Gaussians [68] and be supervised at clean 2D multi-view renderings. As shown in Fig. 4 (b), the input of DiffusionGS in the training phase are one clean condition view xcon , , x(N ) RHW 3 and noisy views Xt = {x(1) } , x(2) t Figure 4. Pipeline. (a) When selecting the data for our scene-object mixed training, we impose two angle constraints on the positions and orientations of viewpoint vectors to guarantee the training convergence. (b) The denoiser architecture of DiffusionGS in single timestep. concatenated with viewpoint conditions vcon RHW 6 and = {v(1), v(2), , v(N )}. Denote the clean counterparts of the noisy views as X0 = {x(1) 0 , , x(N ) 0 }. The forward diffusion process adds noise to each view as 0 , x(2) = αtx(i) x(i) 0 + 1 αtϵ(i) , (1) where ϵ(i) (0, I) and = 1, 2, , . Then in each timestep t, the denoiser θ predicts the 3D Gaussians Gθ to enforce view consistency. As the number of original 3D Gaussians is not constant, we adopt the pixel-aligned 3D Gaussians [68] as the output, whose number is fixed. The predicted 3D Gaussians Gθ is formulated as Gθ(Xtxcon, vcon, t, V) = {G(k) (µ(k) , Σ(k) , α(k) , c(k) )}, (2) t = o(k) +u(k) R3. Specifically, µ(k) where 1 Ng and Ng = (N +1)HW is the number of per-pixel Gaussian G(k) . and are the height and width contains center position µ(k) of the image. Each G(k) R3, covariance Σ(k) R33 controlling its shape, an opacity α(k) characterizing the transmittance, and an RGB color c(k) d(k). o(k) and d(k) are the origin and direction of the k-th pixelaligned ray. The distance u(k) = w(k) u(k) unear + (1 w(k) where unear and uf ar are the nearest and farthest distances. w(k) is the weight to control u(k) is parameterand scaling matrix S(k) ized by rotation matrix R(k) . w(k) , and c(k) are directly extracted from the merged per-pixel Gaussian maps by splitting channels. Denoiser Architecture. As shown in Fig. 4 (b), the input images concatenated with the viewpoint conditions are patchified, linearly projected, and then concatenated with positional embedding to derive the input tokens of the is parameterized by . Σ(k) , R(k) , α(k) , S(k) )uf ar, (3) t Transformer backbone, which consists of blocks. Each block contains multi-head self-attention (MSA), an MLP, and two layer normalization (LN). Eventually, the output tokens are fed into the Gaussian decoder to be linearly projected and then unpatchified into per-pixel Gaussian maps ˆH = { ˆHcon, ˆH(1), , ˆH(N )}, where ˆHcon and ˆH(i) RHW 14. Then + 1 Gaussian maps are merged into the Gaussian point clouds Gθ in Eq. (2). The timestep condition controls the Transformer block and Gaussian decorder through the adaptive layer normalization mechanism [54]. Gaussian Rendering. As the ground truth of G(k) is not available, we use the 2D renderings to supervise Gθ. To this end, we formulate DiffusionGS to multi-view diffusion model. As aforementioned, 2D diffusion usually predicts the noise ϵt. Yet, noisy Gaussian point clouds do not have texture information and may degrade view consistency. To derive clean and complete 3D Gaussians, DiffusionGS is x0-prediction instead of ϵ-prediction. The denoised multi- (0,t), ˆx(2) view images ˆX(0,t) = {ˆx(1) (0,t)} are rendered by the dfferentiable rasterization function Fr as (0,t), , ˆx(N ) ext, M(i) (0,t) = Fr(M(i) ˆx(i) where 1 . M(i) matrix and intrinsic matrix of the viewpoint c(i). int, Gθ(Xtxcon, vcon, t, V)), ext and M(i) int denote the extrinsic (4) For each G(k) its 3D covariance Σ(k) Σ(k,i) at viewpoint c(i), the rasterization projects from the world coordinate system to R33 in the camera coordinate system as Σ(k,i) = J(i) W(i) Σ(k) W(i) (5) where J(i) R33 is the Jacobian matrix of the affine approximation of the projective transformation. W(i) R33 is the viewing transformation. The 2D projection is divided into non-overlapping tiles. The 3D Gaussians are assigned J(i) , without background and the camera rotates around this central object to capture multi-view images. The imaging range and depth are limited. In contrast, as depicted in the upper part of Fig. 4 (a), scene-level datasets have more dense image representations instead of blank background. The imaging range and depth are much wider. The distribution of viewpoints is usually trajectory of continuous motion, such as dolly in and out [91] or panning left and right [35]. To handle these issues, we design mixed training strategy that controls the distribution of selected views, camera condition, Gaussian point clouds, and imaging depth. Viewpoint Selecting. The first step of our mixed training is to select viewpoints. For better convergence of training process, we impose two angle constraints on camera positions and orientations to ensure the noisy views and novel views have certain overlaps with the condition view. The first constraint is on the angle between viewpoint positions. After normalization, this angle measures the distance of viewpoints. As the noisy views can only provide partial information, we control the angle θ(i) cd between the ith noisy view position and the condition view position, and the angle θ(i,j) dn between the i-th noisy view position and the j-th novel view position. Then the constraints are cd θ1, θ(i,j) θ(i) dn θ2, (9) where θ1 and θ2 are hyperparameters, 1 , and 1 . The position vector can be read from the translation of camera-to-world (c2w) matrix of the viewpoint. The second constraint is on the angle between viewpoint orientations. This angle also controls the overlap of different viewpoints. Denote the forward direction vectors of the condition view, the i-th noisy view, and the j-th novel view as zcon, (i) nv . Then the constraints are noise, and (j) zcon (i) noise zcon (i) noise cos(φ1), zcon (j) nv zcon (j) nv cos(φ2), (10) Where φ1 and φ2 are hyperparameters. is read from c2w. Reference-Point Pl ucker Coordinate. To offer the camera conditions, previous methods [7, 16, 64, 70, 75] adopt pixel-aligned ray embedding, plucker coordinates [55], concatenated with the image as input. As shown in Fig. 5 (a), the pixel-aligned ray embeddings are parameterized as = (od, d), where and are the position and direction of the ray landing on the pixel. Specifically, od represents the rotational effect of relative to d, showing limitations in perceiving the relative depth and geometry. To handle this problem, we customize Reference-Point Plucker Coordinate (RPPC) as the camera condition. As depicted in Fig. 5 (b), we use the point on the ray closest to the origin of the world coordinate system as the reference point to replace the moment vector, which can be formulated as = (o (o d)d, d) (11) Figure 5. Plucker ray vs. Reference-Point Plucker Coordinate. to the tiles where their 2D projections cover. For each tile, the assigned 3D Gaussians are sorted according to the view space depth. Then the RGB value at pixel (m, n) is derived by blending ordered points overlapping the pixel as ˆx(i) (0,t)(m, n) = c(j) σ(j) (cid:88) jN j1 (cid:89) (1 σ(l) ), l=1 (6) (z(l) where σ(l) = α(l) section 3D point, and (z(l) the corresponding 3D Gaussian distribution at z(l) ), z(l) , Σ(l) µ(l) µ(l) is the l-th intert ) is the possibility of , Σ(l) . Then we use the weighted sum, controlled by λ, of L2 loss and VGG-19 [63] perceptual loss LVGG between the multi-view predicted images ˆX(0,t) and ground truth X0 as the denoising loss Lde to supervise the 3D Gaussians Gθ as Lde = L2( ˆX(0,t), X0) + λ LVGG( ˆX(0,t), X0). (7) In the testing phase, our DiffusionGS randomly samples noise from standard normal distribution at timestep and then gradually denoise it step by step. The predicted ˆX(0,t) at each timestep is fed into the next timestep 1 to replace X0 in Eq. (1) for sampling Xt1 at each noisy view as t1 = αt1ˆx(i) x(i) (0,t) + (cid:112)1 αt1ϵ(i) t1. (8) Then we use the stochastic differential equation version DDIM [65] to facilitate the sampling speed by skipping some intermediate timesteps. Finally, the generated Gθ at = 0 in Eq. (2) can be used to render novel view images. 3.2. Scene-Object Mixed Training Strategy Existing 3D training data is relatively scarce and the cost of data annotation is expensive. Especially for the scene-level datasets, there are only 90K training samples [35, 91] and most of them only cover small viewpoint changes, which are not sufficient to learn strong geometry representations. Besides, the majority of object-level training data is synthetic [12]. As result, models [24, 31, 51, 70, 75] trained on object-level data often generate unrealistic textures, limiting the practice on real-camera images. To improve the capacity and generalization ability of our DiffusionGS, it is critical to make full use of both object and scene data. Yet, directly training 3D diffusion models with both object and scene datasets may introduce artifacts or lead to non-convergence because of the large domain discrepancy. As shown in the lower part of Fig. 4 (a), object-level datasets [12] usually have an object in the central position Figure 6. Visual comparison of object-level generation on ABO, GSO, real-camera image, and text-to-image prompted by FLUX. Our method can generate more fine-grained details with accurate geometry from prompt views of any directions. Zoom in for better view. Our RPPC satisfies the translation invariance assumption of the 4D light field [1]. Plus, compared to the moment vector, our reference point can provide more information about the ray position and the relative depth, which are beneficial for the diffusion model to capture the 3D geometry of scenes and objects. By skip connections, the reference-point information can flow through every Transformer block and the Gaussian decoder to guide the GS point cloud generation. Dual Gaussian Decoder. As the depth range varies across objectand scene-level datasets, we use two MLPs to decode the Gaussian primitives for objects and scenes in mixed training. As shown in Fig. 4 (b), for the object-level Gaussian decoder, the nearest and farthest distances [unear, uf ar] in Eq. (3) are set as [0.1, 4.2] and µ(k) is clipped into [1, 1]3. For the scene-level Gaussian decoder, [unear, uf ar] is set to [0, 500]. The two decoders are also controlled by the timestep embedding. In the finetuning phase, we just use single decoder while the other is removed. Overall Training Objective. Similar to the denoising loss Lde in Eq. (7), we compute L2 loss and perceptual loss with the same balancing hyperparameter λ on novel views. The novel view loss is denoted as Lnv. To encourage the distribution of 3D Gaussian point clouds of object-centric generation more concentrated, we design point distribution loss Lpd for training warm-up. Lpd is formulated as Lpd = [l(k) ( [l(k) l(k) (cid:113) Var(l(k) ) ] σ0 + [o(k)])], (12) where represents the mean value, l(k) = u(k) dk, Var denotes the variance, and σ0 is the target standard deviation. σ0 is set to 0.5. Then the overall training objective is = (Lde + Lnv) 1iter>iter0 + Lpd 1iteriter0 1object, (13) where 1iter>iter0 is conditional indicator function which equals 1 if the current training iteration (iter) is greater than the threshold (iter0). 1iteriter0 and 1object are similar. 4. Experiment Dataset. We use Objaverse [12] and MVImgNet [82] as the training sets for objects. We center and scale each 3D object of Objaverse into [1, 1]3, and render 32 images at random viewpoints with 50 FOV. For MVImgNet, we crop the object, remove the background, normalize the cameras, and center and scale the object to [1, 1]3. We preprocess 730K and 220K training samples in Objaverse and MVImgNet. For evaluation, we use the ABO [11] and GSO [13] datasets. Each evaluation instance has 1 input view and 10 testing views. We adopt RealEstate10K [91] and DL3DV10K [35] as the scene-level training datasets. RealEstate10K includes 80K video clips of indoor and outdoor real scenes selected from YouTube videos. We follow the same training/testing split of pixelSplat [6]. DL3DV10K contains 10510 videos of real-world scenarios, covering 96 complex categories. We select 6894 video clips as the training samples. For each video clip, we use SfM [60] to compute the camera poses. Implementation Details. We implement DiffusionGS by Pytorch [53] and train it with Adam optimizer [28]. To save GPU memory, we adopt mixed-precision training [50] with BF16, sublinear memory training [8], and deferred GS renMethod DreamGaussian [69] LGM [70] DMV3D [75] CRM [72] 12345++ [37] DiffusionGS (Ours) User Study Score Runing Time (s) 1.94 3.04 4.1 3.16 31.4 2.69 10 3.81 60.0 4.88 5.8 (a) User preference and runing time of different state-of-the-art single-view object-level generation methods and our DiffusionGS Method PSNR SSIM LPIPS FID Method PSNR SSIM LPIPS FID Method PSNR SSIM LPIPS FID LGM [70] GS-LRM [87] DMV3D [75] 16.01 18.78 23.69 0.7262 0.7974 0.8634 0.3255 0.2720 0.1131 86.32 123.55 32.28 LGM [70] GS-LRM [87] DMV3D [75] 14.27 17.70 20.82 0.7183 0.7950 0.8347 0.3003 0.2411 0.1289 75.55 112.96 33.48 PixelNeRF [80] PixelSplat [6] GS-LRM [87] 17.46 18.57 18. 0.5713 0.6202 0.6231 0.5525 0.4655 0.4549 159.52 102.07 91.55 DiffusionGS 25.89 0. 0.0965 9.03 DiffusionGS 22.07 0.8545 0. 11.52 DiffusionGS 21.63 0.6787 0.2743 15. (b) Object-level results on ABO [11] (c) Object-level results on GSO [13] (d) Scene-level results on Realestate10K [91] Table 1. User study and main quantitative results of single-view image-to-3D generation on ABO [11], GSO [13], and Realestate10K [91]. Figure 7. Visual comparison of single-view scene generation. For fairness, we re-train pixelNeRF [80], pixelSplat [6], and GS-LRM [87] with single input view and the same number of supervised views as our method. These deterministic models all yield blurry images and fail in novel view synthesis. In contrast, our method can robustly generate both indoor and outdoor scenes with occlusion and rotation. dering [86]. In mixed training, we use 32 A100 GPUs to train the model on Objaverse, MVImgNet, RealEstate10K, and DL3DV10K for 40K iterations at the per-GPU batch size of 16. Then we finetune the model on the objectand scene-level datasets with 64 A100 GPUs for 80K and 54K iterations at the per-GPU batch size of 8 and 16. The learning rate is linearly warmed up to 4e4 with 2K iterations and decades to 0 using cosine annealing scheme [43]. Finally, we scale up the training resolution from 256256 to 512512 and finetune the model for 20K iterations. 4.1. Comparison with State-of-the-art Methods Object-level Generation. Fig. 6 illustrates the visual comparison of object-level generation on ABO, GSO, realcamera image [36], and text-to-image prompted by FLUX. We compare our method with five representative state-ofthe-art (SOTA) methods including one-stage 3D diffusion DMV3D [75], three 2D multi-view diffusion-based methods (LGM [70], CRM [72], and 12345++ [37]), and an SDS-based method DreamGS [69]. Previous methods render over-smoothed images or distort 3D geometry. In contrast, our method robustly generates clearer novel views and perfect 3D geometry with prompt views of any directions, while preserving fine-grained details such as the lampshade in the first row. Even when the front view, which previous methods specialize in, is given (third and fourth row), our method still yields better view consistency by retaining the face details of the dolls. While the methods based on 2D multi-view diffusion introduce cracks, artifacts, and blur to the faces when stitching unaligned multi-view images. We conduct user study by inviting 25 people to score the visual quality of the generation results of 14 objects according to the 3D geometry, texture quality, and alignment with the prompt view. The user study score ranges from 1 (worst) to 6 (best). For each testing object, we display the prompt view and the generated novel views of different methods without names in shuffled order to human testers. Tab. 1a reports the results and running time at the size of 256256. Our method achieves the highest score while enjoying over 5 and 10 inference speed compared to the recent best 3D diffusion DMV3D and multi-view diffusionbased method 12345++. Tab. 1b and 1c show the quantitative results of object-level generation on the ABO and GSO datasets. DiffusionGS surpasses DMV3D by 2.2/1.25 dB in PSNR and 23.25/21.96 in FID score on ABO/GSO. We chain stable diffusion [59] or FLUX with DiffusionGS to perform text-to-3D in Fig. 1 and 2, our method can handle hard cases with furry appearance, shadow, flat illustration, complex geometry, and even specularity. Scene-level Generation. Since single-view scene generation is less studied. We train three SOTA scene reconstruction models including GS-LRM [87], pixelSplat [6], and pixelNeRF [80] with single input view and the same number of supervised views as DiffusionGS for fair comparison. Tab. 1d reports the quantitative results on RealEstate10K Figure 8. Visual analysis of scene-object mixed training and generation diversity. (a) When using our mixed training strategy, the model can generate more realistic textures for objects and more accurate structural contents for scenes such as the window and stove. (b) Three generated samples of our method with the same prompt view of the ikun doll in Fig. 1. We show four rendered views for each sample. Method Baseline + Our Diffusion + Lpd + Mixed Training + RPPC PSNR SSIM LPIPS FID 17.63 0.7928 0.2452 118. 20.57 0.8120 0.1417 47.86 20.94 0.8423 0.1218 28.41 21.73 0.8515 0.1196 17.79 22.07 0.8545 0.1115 11.52 Table 2. Ablation study. Results on the GSO [13] dataset are listed. test set. Our method significantly outperforms the SOTA method GS-LRM [87] by 2.91 dB in PSNR and 75.68 in FID score. Fig. 1, 3, and 7 depict the visual results of indoor and outdoor scene generation with occlusion and rotation. In Fig. 7, previous deterministic methods all render very blurry images. In contrast, our DiffusionGS can generate the detailed contents behind the obstacle and outside the prompt view. We chain Sora [3] with our DiffusionGS to perform text-to-scene. As shown in Fig. 1, when moving the camera, our DiffusionGS can reliably generate novel views for both indoor and outdoor scenes prompted by Sora. 4.2. Ablation Study Break-down Ablation. To study the effect of each component towards higher performance, we adopt the denoiser without timestep control as the baseline to conduct breakdown ablation. We train it on the object-level datasets with single-view input and the same amount of supervised views as DiffusionGS. Results on GSO [13] are reported in Tab. 2. The baseline yields poor results of 17.63 dB in PSNR and 118.31 in FID. When applying our diffusion framework, point cloud distribution loss Lpd in Eq.(12), sceneobject mixed training without the reference-point plucker coordinate (RPPC), and RPPC, the model gains by 2.94, 0.37, 0.79, 0.34 dB in PSNR and drops by 70.45, 19.45, 10.62, 6.27 in FID. Besides, the improvement of RPPC on RealEstate10K test set is 0.28 dB in PSNR and 7.09 in FID. These results suggest the effectiveness of our methods. Analysis of Mixed Training. We conduct visual analysis of our scene-object mixed training in Fig. 8 (a). For fair comparison, models are trained with the same number of iterations whether with or without mixed training. The upper row shows the effect on object-level generation. After using the mixed training, the textures of the cup become clearer and more realistic, and the artifacts on the back are reduced. The lower row depicts the effect on scene-level generation. When applying our mixed training, DiffusionGS can better capture the 3D geometry and generate more structural contents such as the window and stove of the kitchen. Analysis of Generation Diversity. We evaluate the generation diversity of DiffusionGS by changing the random seed with the same front prompt view ikun as shown in the second row of Fig. 1. Different generated samples are shown in Fig. 8 (b). DiffusionGS can generate different shapes and textures for 3D assets, such as the back of the ikun doll. 5. Conclusion In this paper, we propose novel 3DGS-based diffusion model, DiffusionGS, for single-stage object and scene generation from single view. Our DiffusionGS directly outputs 3D Gaussian point clouds at each timestep to enforce view consistency and only requires 2D renderings for supervision. In addition, we develop scene-object mixed training strategy with new camera conditioning method RPPC to learn general prior capturing better 3D geometry and texture representations. Experiments demonstrate that our DiffusionGS significantly outperforms the SOTA 3D diffusion model by 2.2/2.91 dB in PSNR and 23.25/75.68 in FID on object/scene generation while enjoying over 5 speed (6s) on single A100 GPU. The user study and text-to-3D application also reveal the practical values of our method."
        },
        {
            "title": "References",
            "content": "[1] Edward Adelson, James Bergen, et al. The plenoptic function and the elements of early vision. The MIT press, 1991. 6 [2] Titas Anciukeviˇcius, Zexiang Xu, Matthew Fisher, Paul Henderson, Hakan Bilen, Niloy Mitra, and Paul Guerrero. Renderdiffusion: Image diffusion for 3d reconstruction, inpainting and generation. In CVPR, 2023. 2, 3 [3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 1, 8 [4] Yuanhao Cai, Yixun Liang, Jiahao Wang, Angtian Wang, Yulun Zhang, Xiaokang Yang, Zongwei Zhou, and Alan Yuille. Radiative gaussian splatting for efficient x-ray novel view synthesis. In ECCV, 2024. 3 [5] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W. Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. Generative novel view synthesis with 3d-aware diffusion models. In ICCV, 2023. 3 [6] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In CVPR, 2024. 6, [7] Eric Ming Chen, Sidhanth Holalkere, Ruyu Yan, Kai Zhang, and Abe Davis. Ray conditioning: Trading photo-realism for photo-consistency in multi-view image generation. In ICCV, 2023. 5 [8] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. arXiv Training deep nets with sublinear memory cost. preprint arXiv:1604.06174, 2016. 6 [9] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin. Gaussianeditor: Swift and controllable 3d editing with gaussian splatting. In CVPR, 2024. 3 [10] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In ECCV, 2025. 3 [11] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In CVPR, 2022. 6, 7 [12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In CVPR, 2023. 5, 6 [13] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas McHugh, and Vincent Vanhoucke. Google scanned objects: highIn ICRA, quality dataset of 3d scanned household items. 2022. 2, 6, 7, [14] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, and Zhangyang Wang. Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps. In NeurIPS, 2024. 3 [15] Zhiwen Fan, Jian Zhang, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, et al. Large spatial model: In NeurIPS, End-to-end unposed images to semantic 3d. 2024. 3 [16] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul P. Srinivasan, Jonathan T. Barron, and Ben Poole. Cat3d: Create anything in 3d with multi-view diffusion models. arXiv, 2024. 5 [17] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi. Nerfdiff: Single-image view synthesis with nerf-guided distillation from 3d-aware diffusion. In ICML, 2023. 3 [18] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas Oguz. 3dgen: Triplane latent diffusion for textured mesh generation. arXiv preprint arXiv:2303.05371, 2023. 3 [19] Hao He, Yixun Liang, Luozhou Wang, Yuanhao Cai, Xinli Xu, Hao-Xiang Guo, Xiang Wen, and Yingcong Chen. Lucidfusion: Generating 3d gaussians with arbitrary unposed images. arXiv preprint arXiv:2410.15636, 2024. 3 [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 3 [21] Shoukang Hu and Ziwei Liu. Gauhuman: Articulated gaussian splatting from monocular human videos. arXiv preprint arXiv:, 2023. 3 [22] Xuekun Jiang, Anyi Rao, Jingbo Wang, Dahua Lin, and Bo Dai. Cinematic behavior transfer via nerf-based differentiable filming. In CVPR, 2024. 1 [23] Yingwenqi Jiang, Jiadong Tu, Yuan Liu, Xifeng Gao, Xiaoxiao Long, Wenping Wang, and Yuexin Ma. Gaussianshader: 3d gaussian splatting with shading functions for reflective surfaces. arXiv preprint arXiv:2311.17977, 2023. 3 [24] Heewoo Jun and Alex Nichol. ing conditional 3d implicit functions. arXiv:2305.02463, 2023. 3, Shap-e: GeneratarXiv preprint [25] Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang, Sebastian Scherer, Deva Ramanan, and Jonathon Luiten. Splatam: Splat, track & map 3d gaussians for dense rgb-d slam. arXiv preprint arXiv:2312.02126, 2023. 3 [26] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 2023. 2, 3 [27] Justin Kerr, Letian Fu, Huang Huang, Yahav Avigal, Matthew Tancik, Jeffrey Ichnowski, Angjoo Kanazawa, and Ken Goldberg. Evo-nerf: Evolving nerf for sequential robot grasping of transparent objects. In CoRL, 2022. 1 [28] Diederik P. Kingma and Jimmy Lei Ba. Adam: method for stochastic optimization. In ICLR, 2015. [29] Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, and Anurag Ranjan. Hugs: Human gaussian splats. arXiv preprint arXiv:2311.17910, 2023. 3 [30] Chaojian Li, Sixu Li, Yang Zhao, Wenbo Zhu, and Yingyan Lin. Rt-nerf: Real-time on-device neural radiance fields towards immersive ar/vr rendering. In Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design, pages 19, 2022. 1 [31] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with In sparse-view generation and large reconstruction model. ICLR, 2024. 1, 5 [32] Yanyan Li, Chenyu Lyu, Yan Di, Guangyao Zhai, Gim Hee Lee, and Federico Tombari. Geogaussian: Geometry-aware gaussian splatting for scene rendering. In ECCV, 2024. 3 [33] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching. arXiv preprint arXiv:2311.11284, 2023. [34] Zhihao Liang, Qi Zhang, Ying Feng, Ying Shan, and Kui Jia. Gs-ir: 3d gaussian splatting for inverse rendering. arXiv preprint arXiv:2311.16473, 2023. 3 [35] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In CVPR, 2024. 5, 6 [36] Isabella Liu, Linghao Chen, Ziyang Fu, Liwen Wu, Haian Jin, Zhong Li, Chin Ming Ryan Wong, Yi Xu, Ravi Ramamoorthi, Zexiang Xu, and Hao Su. Openillumination: multi-illumination dataset for inverse rendering evaluation on real objects. In NeurIPS, 2023. 7 [37] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. In CVPR, 2024. 1, 3, 7 [38] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In ICCV, 2023. 3 [39] Xian Liu, Xiaohang Zhan, Jiaxiang Tang, Ying Shan, Gang Zeng, Dahua Lin, Xihui Liu, and Ziwei Liu. Humangaussian: Text-driven 3d human generation with gaussian splatting. arXiv preprint arXiv:2311.17061, 2023. [40] Yang Liu, Xiang Huang, Minghan Qin, Qinwei Lin, and Haoqian Wang. Animatable 3d gaussian: Fast and highIn ACM quality reconstruction of multiple human avatars. MM, 2023. 1 [41] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. In CVPR, 2024. 3 [42] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, and Wenping Wang. Wonder3d: Single image to 3d using cross-domain diffusion. In CVPR, 2024. 3 [43] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In ICLR, 2017. 7 [44] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Tracking arXiv preprint Deva Ramanan. by persistent dynamic view synthesis. arXiv:2308.09713, 2023. 3 Dynamic 3d gaussians: [45] Hidenobu Matsuki, Riku Murai, Paul HJ Kelly, and Andrew Davison. Gaussian splatting slam. arXiv preprint arXiv:2312.06741, 2023. 3 [46] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2021. 3 [47] Mildenhall, PP Srinivasan, Tancik, JT Barron, Ramamoorthi, and Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 2 [48] Arthur Moreau, Jifei Song, Helisa Dhamo, Richard Shaw, Yiren Zhou, and Eduardo Perez-Pellitero. Human gaussian splatting: Real-time rendering of animatable avatars. In CVPR, 2024. 1 [49] Yuxuan Mu, Xinxin Zuo, Chuan Guo, Yilin Wang, Juwei Lu, Xiaofeng Wu, Songcen Xu, Peng Dai, Youliang Yan, and Li Cheng. Gsd: View-guided gaussian splatting diffusion for 3d reconstruction. In ECCV, 2024. [50] Sharan Narang, Gregory Diamos, Erich Elsen, Paulius Micikevicius, Jonah Alben, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. In ICLR, 2018. 6 [51] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. 3, 5 [52] Evangelos Ntavelis, Aliaksandr Siarohin, Kyle Olszewski, Chaoyang Wang, Luc Gool, and Sergey Tulyakov. AuIn NeurIPS, 2023. todecoding latent 3d diffusion models. 3 [53] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. 6 [54] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [55] Julius Plucker. Neue Geometrie des Raumes gegrundet auf die Betrachtung der geraden Linie als Raumelement von Julius Pluecker. Teubner, 1869. 2, 5 [56] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2023. 3 [57] Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, and Siyu Tang. 3dgs-avatar: Animatable avatars via deformable 3d gaussian splatting. In CVPR, 2024. 1 [58] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: Subject-driven text-to-3d generation. In ICCV, 2023. 3 [59] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 3, 7 [60] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016. 6 [61] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. [62] J. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 3d neural field generation using triplane diffusion. In CVPR, 2023. 3 [63] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICCV, 2014. 5 [64] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. In NeurIPS, 2021. 5 [65] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 3, 5 [66] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. [67] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Viewset diffusion:(0-) image-conditioned 3d generative models from 2d data. In ICCV, 2023. 2, 3 [68] Stanislaw Szymanowicz, Chrisitian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. In CVPR, 2024. 3, 4 [69] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. 3, 7 [70] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In ECCV, 2024. 1, 3, 5, 7 [71] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In NeurIPS, 2024. 3 [72] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun Zhu. Crm: Single image to 3d textured mesh with convolutional reconstruction model. In ECCV, 2024. 1, [73] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528, 2023. 3 [74] Tianyi Xie, Zeshun Zong, Yuxin Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. Physgaussian: PhysicsarXiv integrated 3d gaussians for generative dynamics. preprint arXiv:2311.12198, 2023. 3 [75] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, et al. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. In ICLR, 2024. 2, 3, 5, 7 [76] Chi Yan, Delin Qu, Dong Wang, Dan Xu, Zhigang Wang, Bin Zhao, and Xuelong Li. Gs-slam: Dense visual slam with 3d gaussian splatting. arXiv preprint arXiv:2311.11700, 2023. 3 [77] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv:2310.10642, 2023. [78] Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou Wang, Yueqi Duan, and Jun Zhu. Dreamreward: Text-to-3d generation with human preference. In ECCV, 2024. 3 [79] Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussian splatting with point cloud priors. arXiv preprint arXiv:2310.08529, 2023. 3 [80] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In CVPR, 2021. 7 [81] Heng Yu, Joel Julin, Zoltan A. Milacski, Koichiro Niinuma, and Laszlo A. Jeni. Cogs: Controllable gaussian splatting. In CVPR, 2024. 3 [82] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: largescale dataset of multi-view images. In CVPR, 2023. 6 [83] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In CVPR, 2024. 3 [84] Vladimir Yugay, Yue Li, Theo Gevers, and Martin Oswald. Gaussian-slam: Photo-realistic dense slam with gaussian splatting. arXiv preprint arXiv:2312.10070, 2023. 3 [85] Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, and Baining Guo. Gaussiancube: Structuring gaussian splatting using optimal transport for 3d generative modeling. In NeurIPS, 2024. [86] Kai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu, Eli Shechtman, and Noah Snavely. Arf: Artistic radiance fields. In ECCV, 2022. 7 [87] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. In ECCV, 2024. 3, 7, 8 [88] Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Y. Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, and William T. Freeman. PhysDreamer: Physics-based interaction with 3d objects via video generation. In ECCV, 2024. 3 [89] Allan Zhou, Moo Jin Kim, Lirui Wang, Pete Florence, and Chelsea Finn. Nerf in the palm of your hand: Corrective augmentation for robotics via novel-view synthesis. In CVPR, 2023. 1 [90] Junsheng Zhou, Weiqi Zhang, and Yu-Shen Liu. Diffgs: Functional gaussian splatting diffusion. In NeurIPS, 2024. 3 [91] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. In SIGGRAPH, 2018. 5, 6, [92] Zehao Zhu, Zhiwen Fan, Yifan Jiang, and Zhangyang Wang. Fsgs: Real-time few-shot view synthesis using gaussian splatting. In ECCV, 2024."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Hong Kong University of Science and Technology",
        "Johns Hopkins University"
    ]
}