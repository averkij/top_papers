{
    "paper_title": "Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression",
    "authors": [
        "Joykirat Singh",
        "Justin Chih-Yao Chen",
        "Archiki Prasad",
        "Elias Stengel-Eskin",
        "Akshay Nambi",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent thinking models solve complex reasoning tasks by scaling test-time compute, but this scaling must be allocated in line with task difficulty. On one hand, short reasoning (underthinking) leads to errors on harder problems that require extended reasoning steps; but, excessively long reasoning (overthinking) can be token-inefficient, generating unnecessary steps even after reaching a correct intermediate solution. We refer to this as under-adaptivity, where the model fails to modulate its response length appropriately given problems of varying difficulty. To address under-adaptivity and strike a balance between under- and overthinking, we propose TRAAC (Think Right with Adaptive, Attentive Compression), an online post-training RL method that leverages the model's self-attention over a long reasoning trajectory to identify important steps and prune redundant ones. TRAAC also estimates difficulty and incorporates it into training rewards, thereby learning to allocate reasoning budget commensurate with example difficulty. Our approach improves accuracy, reduces reasoning steps, and enables adaptive thinking compared to base models and other RL baselines. Across a variety of tasks (AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8% compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length drop compared to the best RL baseline. TRAAC also shows strong generalization: although our models are trained on math datasets, they show accuracy and efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH, and OptimalThinkingBench. Our analysis further verifies that TRAAC provides fine-grained adjustments to thinking budget based on difficulty and that a combination of task-difficulty calibration and attention-based compression yields gains across diverse tasks."
        },
        {
            "title": "Start",
            "content": "THINK RIGHT: LEARNING TO MITIGATE UNDER-OVER THINKING VIA ADAPTIVE, ATTENTIVE COMPRESSION Joykirat Singh1 Elias Stengel-Eskin2 Akshay Nambi3 Mohit Bansal1 1UNC Chapel Hill 2The University of Texas at Austin Justin Chih-Yao Chen1 Archiki Prasad1 3Microsoft Research 5 2 0 O 2 ] . [ 1 1 8 5 1 0 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent thinking models are capable of solving complex reasoning tasks by scaling test-time compute across various domains, but this scaling must be allocated in line with task difficulty. On one hand, short reasoning (underthinking) leads to errors on harder problems that require extended reasoning steps; but, excessively long reasoning (overthinking) can be token-inefficient, generating unnecessary steps even after reaching correct intermediate solution. We refer to this as under-adaptivity, where the model fails to modulate its response length appropriately given problems of varying difficulty. To address under-adaptivity and strike balance between underand overthinking, we propose TRAAC (Think Right with Adaptive, Attentive Compression), an online post-training RL method that leverages the models self-attention over long reasoning trajectory to identify important steps and prune redundant ones. TRAAC also estimates difficulty and incorporates it into training rewards, thereby learning to allocate reasoning budget commensurate with example difficulty. Our approach improves accuracy, reduces reasoning steps, and enables adaptive thinking compared to base models and other RL baselines. Across variety of tasks (AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute accuracy gain of 8.4% with relative reduction in reasoning length of 36.8% compared to the base model, and 7.9% accuracy gain paired with 29.4% length drop compared to the best RL baseline. TRAAC also shows strong generalization: although our models are trained on math datasets, they show accuracy and efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH, and OptimalThinkingBench. Our analysis further verifies that TRAAC provides fine-grained adjustments to thinking budget based on difficulty and that combination of task-difficulty calibration and attention-based compression yields gains across diverse tasks."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in thinking models have enabled language models to solve complex reasoning tasks (DeepSeek-AI et al., 2025; OpenAI et al., 2024; Team, 2025). These models extend the chainof-thought (CoT; Wei et al., 2023) paradigm with online reinforcement learning (RL; Shao et al., 2024), allowing them to refine intermediate solutions as well as sequentially scaling the number of tokens (i.e., compute) to arrive at the final answer. While such approaches show strong promise for harder problems in domains like mathematics, programming, and logical puzzles (Xie et al., 2025; Chen et al., 2025), their accuracy and utility remain capped by failure to regulate their reasoning length. On one hand, underthinking arises when models terminate too early on harder problems, yielding an incorrect final answer. On the other hand, overthinking occurs when models think excessively for simpler tasks, inflating test-time computation (Marjanovic et al., 2025; Wu et al., 2025; Cuadron et al., 2025), and reducing efficiency. This highlights the need for adaptive thinking (Saha et al., 2025; Chen et al., 2024; Snell et al., 2024; Aggarwal & Welleck, 2025), where models dynamically allocate thinking based on difficulty. We refer to the phenomenon of models misallocating thinking budget illustrated in Fig. 1 as under-adaptivity. Addressing under-adaptivity is crucial for improving both performance and effi1Codebase: https://github.com/joykirat18/TRAAC 1 Figure 1: Overthinking on easy problems wastes tokens by continuing computation after correct answer has been reached. On the other hand, underthinking on hard problems saves token budgets but fails to maintain accuracy. TRAAC addresses this trade-off by adapting to problem difficulty (estimated during training) through attention-based compression, enabling intelligent resource allocation while improving both accuracy and efficiency. ciency of long-thinking models, as dynamic reasoning effort allocation can enable better reasoning exploration in harder problems, while avoiding wasteful computation on problems requiring minimal reasoning. Prior work has generally addressed the overthinking end of under-adaptivity, i.e., improving thinking efficiency. This line of work employs supervised fine-tuning on compressed CoT (Xia et al., 2025), using user control signals such as early stopping during inference (Muennighoff et al., 2025), or RL methods with length penalties (Arora & Zanette, 2025; Hou et al., 2025). Other more adaptive work has employed budget-aware reward shaping with binary choice between thinking or not thinking (Zhang et al., 2025b). While such work can reduce token usage, its performance is typically bounded by the accuracy of the underlying model being trained, and often trades performance for efficiency. Our work aims to beat this trade-off and improve both efficiency and accuracy by providing finer-grained feedback through difficulty-adaptive compression, where the degree of compression is dynamically adapted to task difficulty to address under-adaptivity. To address these gaps, we introduce TRAAC (Think Right with Adaptive, Attentive Compression), GRPO-based (Shao et al., 2024) post-training method that incorporates an online, difficultyadaptive, attention-based compression module to adaptively prune the reasoning trajectory (an entire chain in Fig. 1) based on estimated task difficulty. Our method teaches the model to compress the context that it should pay attention to, such that it contains only relevant material without getting distracted or skewed in wrong directions (Weston & Sukhbaatar, 2023). Specifically, we compute the attention score averaged across layers and heads of the model for each reasoning step (illustrated as nodes in Fig. 1 (right)) from the </think> token and compress reasoning steps that are least attended to, based on the assumption that these are the least important tokens contributing to the final answer. During online training, the level of attention-compression is determined by task difficulty, as estimated by the pass rate during GRPO rollout, making the model more adaptive. For harder problems, TRAAC maintains low compression rate, allowing the model to extend its reasoning trajectory, which increases the likelihood of reaching the correct final answer. For easier problems, it applies higher compression rate to aggressively compress once the correct final answer is reached. We evaluate TRAAC on two strong off-the-shelf reasoning models, Qwen3-4B (Team, 2025) and Deepseek-Qwen-7B (DeepSeek-AI et al., 2025), across multiple benchmarks: AMC (AMC, 2023), AIME (AIME, 2024), GPQA-Diamond (Rein et al., 2023), BBEH (Big Bench Extra Hard; Kazemi et al., 2025), and OptimalThinkingBench (Aggarwal et al., 2025). Our experiments demonstrate that TRAAC consistently adapts to problem difficulty, yielding improvements in efficiency on simple tasks and stronger accuracy on complex tasks. Averaged across AMC, AIME, GPQA-D, and BBEH, TRAAC (Qwen3-4B) achieves an average absolute improvement of 8.4% in accuracy while relative reduction in reasoning length by 36.8% compared to the base model. When compared to the next-best performing baseline, AdaptThink (Zhang et al., 2025b), we achieve an average accuracy improvement of 7.9% and 29.4% efficiency gain. We test our TRAAC method on Opti2 Figure 2: Overview of TRAAC. Given problem, the model first generates rollouts, and the pass rate of these rollouts is used to estimate the problems difficulty (easy, medium, or hard). Next, the generated reasoning is fed back into the model, which is asked to compute the attention score of each reasoning token from </think>. During this attention-based compression step, we remove steps with lower scores. The degree of removal is determined by the estimated difficulty: easier problems undergo more aggressive compression. Finally, we compute the correctness and length rewards using the compressed reasoning trajectory, and these rewards are used to update the policy. malThinkingBench (Aggarwal et al., 2025), and find TRAAC improves by 7.36 points on Qwen3-4B and 12.55 points on Deepseek-Qwen-7B over the base model according to Aggarwal et al. (2025)s F1 metric designed to measure both performance and efficiency. Moreover, TRAAC is trained on math-specific dataset; evaluation on non-math benchmarks such as GPQA-D, BBEH, OverthinkingBench, and UnderthinkingBench shows its generalization ability. Among these OOD tasks, TRAAC shows an average improvement of 3% on Qwen3-4B, with maximum improvement of 6.8% on UnderthinkBench, along with an average 40% reduction in response length across OOD tasks. Our analysis and ablations demonstrate that through difficulty level calibration, TRAAC learns to dynamically adjust its compression ratio with lower compression on difficult tasks and higher compression on easier ones, which translates into performance gains across diverse difficulty tasks. Further analysis reveals that attention-based compression consistently outperforms other compression techniques like random and confidence-based compression. 2 TRAAC: THINK RIGHT WITH ADAPTIVE ATTENTIVE COMPRESSION In this section, we introduce our proposed TRAAC method in detail (also shown in Fig. 2). It is designed to mitigate under-adaptivity, which leads to resource misallocation during test-time. The main challenge lies in the efficient identification of low-importance tokens and making the attentionbased compression adaptive to the tasks difficulty. To this end, TRAAC employs an attention-based compression module that calibrates its degree of compression based on estimated task difficulty and prunes unnecessary reasoning steps while preserving essential information. 2.1 PROBLEM FORMULATION IN TRAAC TRAAC is based on Group Reward Policy Optimization (GRPO; Shao et al., 2024), which is an online reinforcement learning (RL) algorithm that extends Proximal Policy Optimization (Schulman et al., 2017) by eliminating the critic and instead estimating the baseline from group of sampled responses. Let πθ denote the policy model and the input query. Given q, the model generates an output = cat(r, a) where cat is the concatenate function, is the complete reasoning trajectory, and is the final answer, separated by the delimiter </think>. An attention-based compression module (described below) produces compressed reasoning trajectory: rcomp = C(r). At each 3 i=1, where each rollout yi = cat(ri, ai) (see training step, the model generates rollouts, {yi}N rollout arrow in Fig. 2). The advantage of each rollout is estimated using the standard GRPO objective (details in Appendix A.4). The task difficulty is estimated from these rollouts as the proportion of correct answers among the samples (Zhang & Zuo, 2025; Huang et al., 2025). We show this in Fig. 2 by classifying problem to easy, medium or hard based on d. Task difficulty is then used to (i) modulate the compression ratio applied to the reasoning trajectory r, and (ii) assign rewards to each rollout. The answer is regenerated based on the compressed trajectory and the advantage is estimated using both the original rollouts and their compressed counterparts. 2.2 ADAPTIVE, ATTENTIVE COMPRESSION MODULE The goal of the compression module is to identify and remove redundant reasoning steps by evaluating attention scores assigned to each token. Attention-Based Compression. To calculate the attention score assigned to each token, we pass the reasoning trajectory (full rollout in Fig. 2) through the initial policy model. As compared to other compression-based methods (Cheng et al., 2025; Lu et al., 2025), TRAAC does not rely on external models for annotating reasoning steps. To segment the reasoning trajectory into reasoning steps, we split it at special control tokens such as wait, alternative, Let me think again, etc (complete list Appendix A.3.2). For the current thinking models, </think> marks the end of reasoning trajectory, followed by the final answer. Choi et al. (2025) show that </think> attends to key reasoning steps that contain crucial information for deriving the final answer, therefore, for each token tj in the reasoning steps, its importance score is defined as the aggregated attention from the delimiter </think> across all layers and heads: sj = 1 LH (cid:88) (cid:88) ℓ= h=1 α(ℓ,h) </think>tj , </think>tj where is the number of layers, is the number of heads per layer, and α(ℓ,h) is the attention weight from </think> to token tj in head of layer ℓ. Table 6 presents an ablation study comparing attention-based compression with other pruning techniques. Before computing the attention score of each token, consistent with prior work (Muennighoff et al., 2025; Choi et al., 2025), we also append an auxiliary prompt Time is up. should stop thinking and now write summary containing all key steps required to solve the problem. at the end of the reasoning trajectory. This encourages the model to distill the reasoning process into its most salient steps, thereby enabling the delimiter token </think> to attend to the most informative parts of the reasoning trajectory (highlighted in green). As shown in Fig. 2 (bottom-right), the model assigns low attention scores to reasoning steps that do not contribute to the final correct answer (highlighted in red), effectively pruning unnecessary cyclic self-corrections and verification loops. Finally, the importance score of reasoning step Ck, consisting of tokens {tj}jCk , is then computed as the mean of its token-level scores: sCk = sj. Steps with lower importance scores are pruned, yielding the compressed reasoning comp. 1 jCk Ck trajectory ri (cid:80) Difficulty-Level Calibration. To address under-adaptivity, the pruning strategy is further adapted to task difficulty, i.e., for easier tasks, larger proportion of reasoning steps are removed, encouraging the model to condense its reasoning more aggressively (see compression on the right of Fig. 2). The difficulty of task is estimated based on the pass rate of each problem during rollout. From the estimated difficulty level, each problem set is categorized among three difficulty levels: easy, medium, and hard, with higher pass rate indicating easier problems and vice versa. Each category is assigned compression rate to determine the degree of redundant steps to prune from the reasoning trajectory, with higher compression for easier problems and lower compression for hard problems. In addition, to keep these constraints adaptive to the amount of redundancy in the steps, we calculate the uniformity of the attention score distribution. When the distribution of {sj} is close to uniform, indicating that no step or token within step stands out as significantly more important, the compression rate is reduced to avoid removing potentially useful reasoning steps. More details on calculating the uniformity score can be found in Appendix A.3.3. The difficulty estimate is further incorporated with the reward calculation described below. 4 2.3 REWARDS Following standard GRPO practice of having verifiable reward system (Shao et al., 2024), our setup comprises three different reward signals to guide the model to generate correct adaptive length responses based on the difficulty of the task: Correctness Reward (CR): high-weight reward is assigned to outputs that produce the correct final answer. high score over other rewards is used to ensure that correctness remains the primary optimization objective, regardless of the reasoning trajectory length. Format Reward: structure reward to ensure the presence of special delimiter tokens such as <think> and </think>, ensuring that trajectory and final answer are easily distinguishable. Length Reward (LR): To regulate the verbosity of the reasoning process, we define lengthbased reward that penalizes unnecessarily long reasoning traces while adapting to task difficulty. Based on our initial experiment, simply favoring shorter rollouts led to drastic decrease in response length along with model accuracy; therefore we introduce sigmoid-based smoothing mechanism that provides soft bonus (β) for rollouts beyond the median length. This prevents sharp drops in reward for slightly longer reasoning and helps stabilize training. During each training step, rollouts are partitioned into bins according to their calculated difficulty. As mentioned above, we use the pass rate of the rollouts to categorize them into three difficulty bins: easy, medium and hard. For each bin, we maintain different distribution Ld = {ℓ1, ℓ2, . . . , ℓm} for each difficulty category, where each ℓi denotes the reasoning length of rollout within that difficulty category d. Let ℓ be the length of the current rollout. The normalized length score is computed as: Lnorm = (Lmax ℓ)/max(Lmax Lmin, ϵ), where ϵ > 0 prevents division by zero and Lmin = min(L), Lmax = max(L). To avoid sharp cutoff around the median, we add smooth bonus term: (cid:16) β = 1/ 1 + exp (cid:16) ℓmedian(L) 0.1median(L) (cid:17) (cid:17) , where median(L) = median of the set. The final length reward becomes rlength = max(Lnorm, β). Note that length reward is only provided to rollout if it reaches final correct answer. Moreover, to ensure stability when calculating Lmin, Lmax, and medium(L), we maintain sliding window over the last 10 steps for each difficulty bin, thereby avoiding drastic fluctuations during training. The final reward for each rollout during GRPO training is the combination of correctness, format, and length rewards (c.f. range of each reward in Appendix A.5.2)."
        },
        {
            "title": "3 EXPERIMENTAL SETUP",
            "content": "Models. We adopt two reasoning models, DeepSeek-R1-Distill-Qwen-7B (DeepSeek-AI et al., 2025) (Deepseek-Qwen-7B) and Qwen3-4B (Team, 2025) as our base models. Datasets. We train the model using DAPO-Math-17k (Yu et al., 2025), math dataset that has verifiable answer. For evaluation, we use diverse set of benchmarks, including AIME (AIME, 2024), AMC (AMC, 2023), GPQA-D (Rein et al., 2023), OverthinkingBench/ UnderthinkingBench (Aggarwal et al., 2025),2 and Big Bench Extra Hard (BBEH) (Kazemi et al., 2025). Among the evaluation datasets, only AIME and AMC are math-specific, while the remaining benchmarks represent out-of-distribution settings. Further dataset details and their sizes are provided in Appendix A.1. Evaluation. For each evaluation run, we set temperature to 1.0, and the maximum response length is set to 10k. For each dataset, the mean accuracy and mean response length across 5 runs are reported. For the OverthinkingBench split, we also report the AUCOAA (Aggarwal et al., 2025), as used in their work. Intuitively, higher AUCOAA indicates that the model sustains stronger accuracy while minimizing unnecessary reasoning across thresholds. Following evaluation from Aggarwal et al. (2025) for computing the OptimalThinkingBench score, we combined the AUCOAA from OverthinkingBench and accuracy from UnderthinkingBench into single F1 score. Additional details on these metrics can be found in Appendix A.5.3. Training. During the GRPO rollout, we keep high temperature of 1.0 and sample 8 rollouts at each step. Due to computational constraints, we set the maximum response length to 10k (see 2For simplicity, we avoid using LLM as judge during evaluation, thus we only choose problems that can be verified automatically (i.e., MCQ and questions with numerical answer) in OverthinkingBench. 5 Table 1: Performance comparison of TRAAC with various baselines. Acc. means accuracy(%) and Len. represents the average response length (k). On average, TRAAC achieves the highest performance while substantially compressing the response length. Method AIME AMC GPQA-D BBEH Average Acc. Len. Acc. Len. Acc. Len. Acc. Len. Acc. Len. Base Model TokenSkip L1-Max LC-R1 Adapt Think TRAAC Base Model TokenSkip L1-Max LC-R1 Adapt Think TRAAC 27.64 5.84 30.11 13.48 36.63 45.45 33.71 24.94 31.01 6.07 38.88 38. 9.2 9.6 7.1 2.6 8.4 6.7 8.2 8.5 3.1 4.0 7.1 7.3 Qwen3-4B 68.19 27.71 63.61 56.38 72.77 79.52 7.0 8.7 5.8 1.7 5.8 4.2 45.18 32.32 43.23 26.67 44.04 47. 7.6 7.8 5.8 1.5 6.7 4.2 DeepSeek-R1-Distill-Qwen-7B 74.22 52.05 75.90 37.35 75.66 77.83 5.7 6.8 2.2 3.5 4.1 4.5 43.55 34.24 23.54 28.78 19.29 47.31 7.1 7.0 1.9 2.5 4.8 6. 18.28 11.91 14.91 12.35 7.87 20.59 10.61 6.30 13.43 9.09 6.17 11.55 6.7 7.2 5.0 1.9 6.2 4.3 5.9 6.4 2.1 1.7 5.2 5.2 39.8 19.4 38.0 27.2 40.3 48.2 40.5 29.4 36.0 20.3 35.0 43. 7.6 8.3 5.9 1.9 6.8 4.8 6.7 7.2 2.3 2.9 5.3 5.8 Appendix A.5.1 for other hyperparameter details). For difficulty calibration, we bin problems into easy, medium, and hard categories, assigning the categories decreasing compression scores. Baselines. We compare TRAAC with 5 strong baselines: (1) Base model: off-the-shelf reasoning model, (2) TokenSkip: An SFT based baseline as described by Xia et al. (2025) that fine-tunes the model over compressed CoT training data. (3) L1-Max: An RL framework proposed by Aggarwal & Welleck (2025) that optimizes for accuracy while adhering to user-specific length constraints. (4) LCWe used the constraint Think for maximum of 10000 tokens. during its training. R1: compression-based RL framework by Cheng et al. (2025) that uses an externally trained (5) AdaptThink: Different from the model to remove invalid portions of the thinking process. above baselines, AdaptThink is an adaptive RL framework described by Zhang et al. (2025b), that enables reasoning models to choose between thinking and no-thinking modes and poses it as constraint optimization problem that encourages the model to choose no-thinking while maintaining performance. Prompts used for all baselines in Appendix A.6.1."
        },
        {
            "title": "4 RESULT AND DISCUSSION",
            "content": "4.1 MAIN RESULTS TRAAC improves both performance and efficiency. Tables 1 show the performance of TRAAC compared to other baselines on AIME, AMC, GPQA-D, BBEH (Big Bench Extra Hard) benchmarks. TRAAC (Qwen3-4B) achieves an average accuracy improvement of 8.4% while reducing reasoning length by 36.8% compared to the base model. Similarly, TRAAC (Deepseek-Qwen-7B) improves accuracy by 3.3% with 13.4% reduction in length. When compared to the SFT baseline TokenSkip (Xia et al., 2025), TRAAC outperforms in terms of performance and efficiency for both models, Qwen3-4B and Deepseek-Qwen-7B. Similarly, L1-Max (Aggarwal & Welleck, 2025), an RL-based method that penalizes long responses, also focuses on efficiency gains, at slight cost of overall performance. Additionally, the compression-based RL framework LC-R1 (Cheng et al., 2025) improves the efficiency of the model at the cost of 12.6% drop for Qwen3-4B and 20.2% drop for Deepseek-Qwen-7B, when compared with base models, respectively. On average for Qwen3-4B, TRAAC outperforms L1-Max by 10.2% on Qwen3-4B and by 7.9% on Deepseek-Qwen-7B. Similarly, TRAAC also outperforms LC-R1 by 21% on Qwen3-4B and 23% on Deepseek-Qwen-7B. Moreover, given the same token budget, of approximately 7k, TRAAC (Qwen3-4B) on AIME outperforms L1-Max by 15%. These results highlight that, unlike methods that prioritize only efficiency, TRAAC simultaneously delivers both higher accuracy and shorter reasoning traces. 6 Table 2: Performance of TRAAC and various baselines on OptimalThinkingBench (OTB). For UnderthinkingBench we report the Acc: Accuracy(%), and Len: Average Response length(k). For OverthinkingBench, in addition to Acc. and Len. we also report the AUCOAA. Method OverthinkingBench UnderthinkingBench OTB Acc. Len. AUCOAA Acc. Len. F1 Base Model TokenSkip L1-Max LC-R1 Adapt Think TRAAC Base Model TokenSkip L1-Max LC-R1 Adapt Think TRAAC 90.02 78.15 87.22 78.62 68.83 89.79 78.45 57.03 73.18 76.08 73.41 81.81 Qwen3-4B 80.06 57.88 1.11 64.20 63.44 85.06 34.33 14.80 21.27 14.95 18.80 41. 1.2 3.5 0.9 0.3 8.2 0.6 DeepSeek-R1-Distill-Qwen-7B 0.9 3.9 1.0 0.9 0.4 1.0 72.38 40.77 66.01 69.81 70.72 72.89 12.69 8.55 20.07 7.16 13.13 22.30 7.1 7.9 6.3 1.3 6.0 4. 6.2 7.2 2.0 2.5 4.6 5.9 48.05 23.57 2.10 24.25 29.01 55.41 21.60 14.13 30.78 12.99 22.14 34.15 TRAAC generalizes across domains. Recall that for training, we used data from DAPO-Math17k (Yu et al., 2025), which is math reasoning dataset. In addition to math datasets, we also evaluate TRAAC on several out-of-domain (OOD) tasks, including GPQA-D, BBEH, OverthinkingBench, and UnderthinkingBench  (Table 2)  . Among these OOD tasks, TRAAC shows an average improvement of 3% on Qwen3-4B and 2.8% on Deepseek-Qwen-7B compared to the base model, with improvement as high as 6.8% on UnderthinkingBench, which covers 100 diverse reasoning tasks from Reasoning Gym (Stojanovski et al., 2025). In addition, TRAAC reduces reasoning tokens by 40% on Qwen3-4B and 20% on Deepseek-Qwen-7B, demonstrating substantially higher efficiency while also boosting accuracy across benchmarks. This indicates that TRAAC learns generalizable compression strategy that transfers from math to other reasoning domains. TRAAC learns to adaptively allocate token budget. Among the baselines in Tables 1 and 2, we also compare TRAAC against an adaptive RL method, AdaptThink (Zhang et al., 2025b), which teaches the model to use distinct thinking vs. non-thinking modes for hard and easy problems, respectively. On Qwen3-4B, TRAAC outperforms AdaptThink by 7.9% while also reducing tokens by 29.4%, highlighting that flexible adaptive strategy is more effective in handling diverse problem difficulties. Table 2 further tests on OverthinkingBench/UnderthinkingBench (Aggarwal et al., 2025). OverthinkingBench is designed to measure excessive use of thinking tokens on simple queries. On the other hand, UnderthinkingBench evaluates how necessary thinking is based on problem difficulty. Taken together, TRAAC improves overall F1 performance by 7.36% on Qwen34B, and 12.55% on Deepseek-Qwen-7B over base model, indicating that TRAAC enables the model to avoid both overthinking on simple problems and underthinking on complex ones(Aggarwal et al., 2025). Against AdaptThink, TRAAC achieves 26% gain on Qwen3-4B and 12% gain on DeepseekQwen-7B, underscoring its ability to adaptively allocate reasoning effort and adjust token budgets based on problem difficulty. On OverthinkingBench, we measure overthinking using the AUCOAA metric, which rewards models that solve very easy problems correctly while using minimal tokens (ideally 0). Compared to the base model, TRAAC (Qwen3-4B) improves AUCOAA by 5% and Deepseek-Qwen-7B by 0.5%. Relative to AdaptThink, TRAAC gains 21.6% for Qwen3-4B and 6.9% for Deepseek-Qwen-7B. 4.2 ABLATIONS AND ANALYSIS To understand the importance of each component of the training setup, we conducted an ablation study, removing each component of our method. Table 3 and Table 4 show the performance of these ablations compared with the base model. Specifically, we start with the base model and the ablations: (i) Base Model + CR: The base model trained with GRPO using only the correctness reward, (ii) 7 Base model + CR + LR: The base model trained with GRPO using both correctness and length rewards, but without difficulty-level calibration, (iii) Base model + CR + LR + Compression: The base model trained with GRPO using correctness and length rewards, along with the attention-based compression module, with no difficulty-level calibration. Our findings are as follows. Table 3: Ablation Results of TRAAC on Qwen3-4B tested across 4 datasets: AIME, AMC, GPQA-D, and BBEH. Each component addition adds to the previous setting. Method AIME AMC GPQA-D BBEH Average Acc. Len. Acc. Len. Acc. Len. Acc. Len. Acc. Len. Qwen3-4B Base Model + CR + LR + Compression TRAAC 27.64 44.36 37.84 38.37 45.45 9.2 7.9 4.5 8.1 6.7 68.19 77.35 77.35 75.90 79.52 7.0 5.5 2.4 5.5 4.2 45.18 46.29 44.06 46.40 47.21 7.6 5.7 2.3 6.2 4. 18.28 18.13 18.57 18.41 20.59 6.7 5.2 2.1 5.4 4.3 39.8 46.5 44.5 44.8 48.2 7.6 6.1 2.8 6.3 4.8 Table 4: Ablation Results of TRAAC (Qwen3-4B) on OptimalThinkingBench (OTB). Each component is additional to the previous setting. Method OverthinkingBench UnderthinkingBench OTB Acc. Len. AUCOAA Acc. Len. F1 Qwen3-4B Base Model + CR + LR + Compression TRAAC 90.02 90.02 90.94 90.12 89.79 1.2 0.9 0.4 0.9 0.6 80.06 78.86 75.86 80.41 85.06 34.33 37.06 29.62 36.51 41. 7.1 5.7 2.3 6.0 4.7 48.1 50.4 42.6 50.2 55.4 Combining difficulty-adaptiveness and attention-based compression is crucial for accuracy and efficiency. Table 3 shows that on Qwen3-4B, removing the difficulty-based calibration (Base Model + CR + LR + compression) reduces the average performance across AIME, AMC, GPQA-D, and BBEH by 3.4%, while also making the model less efficient by 23.8%. Similarly, on OptimalThinkingBench  (Table 4)  , we observe comparable degradation: the F1 score decreases by 5.2% when task-difficulty level calibration is removed and drops further by 7.6% when the attention-based compression module is also removed. These results highlight that combination of task-difficulty calibration and attention-based compression is crucial for achieving both high performance and efficiency gains across tasks. TRAAC adapts to task difficulty. To further understand the level of adaptivity of TRAAC compared to other methods, we plot the relative compression ratio and absolute accuracy gains (w.r.t. the base model) in Fig. 3 as function of task difficulty. Here, we rank tasks in order of increasing difficulty. We conduct these experiments on SuperGPQA (Team et al., 2025) benchmark to evaluate model knowledge and reasoning capabilities, which is stratified into easy, medium, and hard splits, and BBH (Big Bench Hard) (Suzgun et al., 2022) an easier version of BBEH. To get oracle difficulty ratings, we rank the datasets by the performance of frontier models on them (Kazemi et al., 2025; Team et al., 2025), with harder datasets being those with lower performance. From Fig. 3(a), we see that as the difficulty of the dataset increases from left to right, the compression rate steadily drops for TRAAC, underscoring its ability to compress more for easier tasks and less for difficult tasks. However, without task-difficulty level calibration for Base model + CR + LR + Compression, the compression rate remains roughly uniform across the tasks. Fig. 3(b) highlights the performance difference, and shows that even with more compression, TRAAC always maintains higher accuracy than Qwen3-4B + CR + LR + compression, reiterating the effectiveness of adapting to problem difficulty in TRAAC. Moreover, most of the accuracy gains stems from harder problems, indicating the average accuracy gains seen in Table 1 come from difficulty-adaptive thinking. Deepseek-Qwen7B results are shown in Appendix A.2 and follow similar trend as Qwen3-4B. 8 Figure 3: (a) Relative change in compression rate of TRAAC and Qwen3-4B + Compression compared to Qwen3-4B across varying problem difficulty. (b) Absolute accuracy drop of TRAAC and Qwen34B + Compression compared to Qwen3-4B across varying problem difficulty. Table 5: TRAAC with 15k training and test-time response length. For each dataset, Accuracy (%) and Response Length (k tokens) are reported. TRAAC scales with longer response length. During TRAAC training, we set maximum token budget of 10k. To test the scalability of our method, we increase the max response length for both training and testing to 15k. Table 5 shows the accuracy and average response length for AIME, AMC, and GPQA-D datasets, for the Qwen3-4B and TRAAC with increased token budget. Similar to the prior results, we see an average accuracy improvement of 3.5% and 23.4% efficiency gains. This underscores that scaling TRAAC still shows consistent gains for both accuracy and efficiency. 47.74 / 12.3 51.93 / 9.7 49.64 / 8.6 51.27 / 6.2 77.11 / 8.5 81.68 / 6.6 Qwen3-4B TRAAC GPQA-D AIME AMC Pruning Strategy AIME Table 6: Ablation on Qwen3-4B: comparing TRAAC with pruning random and least confident steps. For each dataset, Accuracy(%) / Response length (k) is reported. Attention-based compression identifies redundant steps effectively. To help understand the efficiency of the adaptive, attentive compression module, we replace the attention-based compression with random step compression or confidence-based compresinstead sion. At each training step, of using attention as metric, reasoning steps are pruned either randomly or steps with the least confidence (complete details on how confidence is calculated are in Appendix A.6). Table 6 compares TRAAC (Qwen3-4B) with random steps and least confidence. Relative to TRAAC, random step pruning shows an average of 11% accuracy drop, and similarly, pruning the least confidence steps leads to 7.25% accuracy drop. This highlights the efficacy of using attention-based compression in TRAAC. Random Steps Least Confidence TRAAC 29.54 / 6.5 32.35 / 5.8 45.45 / 6.7 42.94 / 3.2 47 / 3.0 47.2 / 4.2 66.74 / 4.1 71.08 / 3.4 79.52 / 4.2 GPQA-D AMC Table 7: Results of TRAAC as test time method using Qwen3-4B, compared to base model and TRAAC. For each dataset Accuracy(%) / Response length (k) are reported. Attention-based compression during test time inference. Similar to prior test-time approaches (Choi et al., 2025), we evaluated TRAAC as test-time method by adapting its compression module to operate during inference. Specifically, during decoding, once the model outputs </think>, we apply the attention-based compression module with static compression rate of 0.4 (same as medium difficulty task), pruning intermediate reasoning steps. Because task difficulty cannot be estimated at test time, this static compression rate is maintained throughout. If </think> is not produced, no compression is applied. After compression, the model is allowed to generate the final answer. Table 7 presents the results on Qwen3-4B, and compares this inference-only variant of TRAAC against both the base model and the fully trained TRAAC. On average across AIME, AMC 68.19 / 7.0 70.60 / 5.7 79.52 / 4.2 27.64 / 9.2 32.13 / 8.5 45.45 / 6.7 45.18 / 7.6 47.61 / 6.6 47.20 / 4.2 Base Test-Time TRAAC GPQA-D Method AIME AMC 9 and GPQA-D datasets, the fully trained TRAAC outperforms the inference-only variant by 7.28% in accuracy and 27.5% in efficiency, underscoring the benefit of incorporating the compression module during training. On GPQA-D, the test-time method achieves accuracy comparable to TRAAC but suffers from 38% efficiency drop. Finally, even the inference-only setup yields an average accuracy improvement of 3.11% and 12.65% efficiency gain over the base model, indicating that applying compression at test time already provides measurable accuracy gains, while training with compression further amplifies accuracy by 10.39% and efficiency by 36.7%."
        },
        {
            "title": "5 RELATED WORK",
            "content": "In the past years, reasoning performance of language models has vastly improved via the introduction of chain-of-thoughts (Wei et al., 2023), parallel scaling through self-consistency (Wang et al., 2023), and best-of-N sampling (Lightman et al., 2023). More recently, several works have found sequential scaling i.e., increasing the number of reasoning tokens to be the most effective approach (Muennighoff et al., 2025), especially when combined with online reinforcement learning or distillation from such models (Aggarwal & Welleck, 2025; Shao et al., 2024; DeepSeek-AI et al., 2025). Consequently, the area of efficient reasoning maintaining high performance from sequential scaling with minimal token usage has become central research focus (Chen et al., 2024; Marjanovic et al., 2025; Wu et al., 2025). To this end, prior works compress or prune chain-of-thoughts via early exiting (Zhang et al., 2025a; Fu et al., 2025), train models under pre-specified budgets (Aggarwal & Welleck, 2025), learn thoughts latently without generating them (Hao et al., 2025), use supervised finetuning to avoid overthinking (Xia et al., 2025; Cheng et al., 2025; Lu et al., 2025), or add length-based penalties for conciseness (Arora & Zanette, 2025; Hou et al., 2025). However, this line of work does not explicitly account for varying problem difficulty, instead relying on the model to learn to allocate budget implicitly; in contrast, TRAAC introduces difficulty-based supervision for budget allocation. Moreover, prior approaches typically address only overthinking reducing output length at the cost of performance drops whereas we tackle both overand underthinking. Improving both reasoning performance and efficiency requires more adaptive approach through explicit training. Prior work such as Zhang et al. (2025b) frames adaptivity as binary decision of whether to think, whereas we argue that for harder problems it must involve deciding how much to think and empirically outperform this baseline in Appendix 4.1. similar insight appears in planning, where Saha et al. (2025) show that mixing system 1 and system 2 reasoning within the same instance outperforms binary choice between them. Shen et al. (2025) pursue difficultyadaptive training via repeated sampling and offline preference optimization to prefer shorter responses. In contrast, TRAAC provides attention-based supervision in the compression module through online RL (DeepSeek-AI et al., 2025). Unlike concurrent work by Choi et al. (2025), who prune redundant tokens post hoc, our method adapts compression during training itself yielding difficultyaware reasoning and improved test-time efficiency without generating unnecessary tokens (c.f. comparison to test-time pruning in Table 7)."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduced TRAAC, post-training RL method that operates online and uses difficulty-adaptive, attention-based compression module. Through its adaptive attentive compression, TRAAC is able to prune its reasoning steps adaptively based on the task difficulty. TRAAC addresses the issue of under-adaptivity, which helps improve both performance and efficiency, as thinking longer on harder problems helps in better exploration, and thinking shorter on easier problems avoids wasting of testtime compute. Moreover, our method also shows strong generalizability, with evaluation done on various OOD tasks. Through our analysis and ablation, we further verify that our adaptive method can provide fine-grained adjustments to the thinking budget based on the difficulty of the problem, and combination of task-difficulty calibration and attention-based compression helped achieve both accuracy and efficiency gains."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was supported by NSF-AI Engage Institute DRL-2112635, NSF-CAREER Award 1846185, DARPA ECOLE Program No. HR00112390060, Capital One Research Award, Cisco 10 Research Award, and an Apple PhD Fellowship. The views contained in this article are those of the authors and not of the funding agency."
        },
        {
            "title": "REFERENCES",
            "content": "Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning, 2025. URL https://arxiv.org/abs/2503.04697. Pranjal Aggarwal, Seungone Kim, Jack Lanchantin, Sean Welleck, Jason Weston, Ilia Kulikov, and Swarnadeep Saha. Optimalthinkingbench: Evaluating over and underthinking in llms, 2025. URL https://arxiv.org/abs/2508.13141. AIME. American invitational mathematics examination, 2024. //artofproblemsolving.com/wiki/index.php/American Invitational Mathematics Examination. URL https: AMC. American mathematics competitions, 2023. URL https://maa.org/student-programs/ amc/. Daman Arora and Andrea Zanette. Training language models to reason efficiently, 2025. URL https://arxiv.org/abs/2502.04463. Jiangjie Chen, Qianyu He, Siyu Yuan, Aili Chen, Zhicheng Cai, Weinan Dai, Hongli Yu, Qiying Yu, Xuefeng Li, Jiaze Chen, Hao Zhou, and Mingxuan Wang. Enigmata: Scaling logical reasoning in large language models with synthetic verifiable puzzles, 2025. URL https://arxiv.org/abs/ 2505.19914. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. Zhengxiang Cheng, Dongping Chen, Mingyang Fu, and Tianyi Zhou. Optimizing length compression in large reasoning models, 2025. URL https://arxiv.org/abs/2506.14755. Daewon Choi, Jimin Lee, Jihoon Tack, Woomin Song, Saket Dingliwal, Sai Muralidhar Jayanthi, Bhavana Ganesh, Jinwoo Shin, Aram Galstyan, and Sravan Babu Bodapati. Think clearly: Improving reasoning via redundant token pruning, 2025. URL https://arxiv.org/abs/2507. 08806. Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, et al. The danger of overthinking: Examining the reasoning-action dilemma in agentic tasks. arXiv preprint arXiv:2502.08235, 2025. DeepSeek-AI et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language model to reason in continuous latent space, 2025. URL https: //openreview.net/forum?id=tG4SgayTtk. Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning. arXiv preprint arXiv:2504.01296, 2025. Shijue Huang, Hongru Wang, Wanjun Zhong, Zhaochen Su, Jiazhan Feng, Bowen Cao, and Yi R. Fung. Adactrl: Towards adaptive and controllable reasoning via difficulty-aware budgeting, 2025. URL https://arxiv.org/abs/2505.18822. 11 Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, and Orhan Firat. Big-bench extra hard, 2025. URL https://arxiv.org/abs/2502.19187. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Ximing Lu, Seungju Han, David Acuna, Hyunwoo Kim, Jaehun Jung, Shrimai Prabhumoye, Niklas Muennighoff, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, et al. Retro-search: Exploring untaken paths for deeper and efficient reasoning. arXiv preprint arXiv:2504.04383, 2025. Sara Vera Marjanovic, Arkil Patel, Vaibhav Adlakha, Milad Aghajohari, Parishad BehnamGhader, Mehar Bhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han L`u, Nicholas Meade, Dongchan Shin, Amirhossein Kazemnejad, Gaurav Kamath, Marius Mosbach, Karolina Stanczak, and Siva Reddy. Deepseek-r1 thoughtology: Lets think about llm reasoning, 2025. URL https://arxiv.org/abs/2504.07128. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. OpenAI et al. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof qa benchmark, 2023. URL https://arxiv.org/abs/2311.12022. Swarnadeep Saha, Archiki Prasad, Justin Chih-Yao Chen, Peter Hase, Elias Stengel-Eskin, and Mohit Bansal. System-1.x: Learning to balance fast and slow planning with language models, 2025. URL https://arxiv.org/abs/2407.14414. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, Zhaoxiang Liu, and Shiguo Lian. Dast: Difficulty-adaptive slow-thinking for large reasoning models, 2025. URL https://arxiv.org/abs/2503.04472. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/ 2408.03314. Zafir Stojanovski, Oliver Stanley, Joe Sharratt, Richard Jones, Abdulhakeem Adefioye, Jean Kaddour, and Andreas Kopf. Reasoning gym: Reasoning environments for reinforcement learning with verifiable rewards, 2025. URL https://arxiv.org/abs/2505.24760. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging bigbench tasks and whether chain-of-thought can solve them, 2022. URL https://arxiv.org/ abs/2210.09261. Team et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines, 2025. URL https://arxiv.org/abs/2502.14739. 12 Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language In The Eleventh International Conference on Learning Representations, 2023. URL models. https://openreview.net/forum?id=1PL1NIMMrw. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. Jason Weston and Sainbayar Sukhbaatar. System 2 attention (is something you might need too), 2023. URL https://arxiv.org/abs/2311.11829. Yuyang Wu, Yifei Wang, Ziyu Ye, Tianqi Du, Stefanie Jegelka, and Yisen Wang. When more is less: Understanding chain-of-thought length in llms. arXiv preprint arXiv:2502.07266, 2025. Heming Xia, Chak Tou Leong, Wenjie Wang, Yongqi Li, and Wenjie Li. Tokenskip: Controllable chain-of-thought compression in llms, 2025. URL https://arxiv.org/abs/2502.12067. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning, 2025. URL https://arxiv.org/abs/2502.14768. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. Anqi Zhang, Yulin Chen, Jane Pan, Chen Zhao, Aurojit Panda, Jinyang Li, and He He. Reasoning models know when theyre right: Probing hidden states for self-verification. arXiv preprint arXiv:2504.05419, 2025a. Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li. Adaptthink: Reasoning models can learn when to think, 2025b. URL https://arxiv.org/abs/2505.13417. Jixiao Zhang and Chunsheng Zuo. Grpo-lead: difficulty-aware reinforcement learning approach for concise mathematical reasoning in language models, 2025. URL https://arxiv.org/abs/ 2504.09696."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 DATASET DETAILS We evaluated the model on various benchmarks: AMC: All questions come from AMC12 2022, AMC12 2023, and have been extracted from the AOPS wiki page. Total Count: 83 AIME: All questions come from AIME 22, AIME 23, and AIME 24, and have been extracted directly from the AOPS wiki page. Total Count: GPQA-D: It is multiple-choice dataset covering physics, biology, and chemistry. Total Count: 198 BBEH: benchmark designed to push the boundaries of LLM reasoning evaluation. BBEH replaces each task in BBH with novel task that probes similar reasoning capability but exhibits significantly increased difficulty. Total Count: 460 OptimalThinkingBench: unified benchmark that jointly evaluates overthinking and underthinking in LLMs and also encourages the development of optimally-thinking models that balance performance and efficiency. Two sub benchmarks: OverthinkingBench, featuring simple queries mcq or numerical answers in 72 domains, and UnderthinkingBench, containing 11 challenging reasoning tasks from reasoningGyms. UnderthinkingBench count: 550, OverthinkingBench count: 607. BBH: suite of 23 challenging BIG-Bench tasks. Total Count: 2115 SuperGPQA: comprehensive benchmark designed to evaluate the knowledge and reasoning abilities of Large Language Models (LLMs) across 285 graduate-level disciplines. Each problem is also categorized as easy, medium and hard. 540 problems for each difficulty category, so the total count is 1620. To calculate the accuracy, we adopt Math-Verify.3 For UnderthinkingBench accuracy calculation, we used the evaluation scripts from Reasoning-Gym (Stojanovski et al., 2025) A.2 DEEPSEEK ABLATION AND ANALYSIS Table 8 and Table 9 present the ablation results for (i) Base Model + CR: The base model trained with GRPO using only the correctness reward, (ii) Base model + CR + LR: The base model trained with GRPO using both correctness and length rewards, but without difficulty-level calibration. Table 8: Ablation Results of TRAAC Deepseek-Qwen-7B tested across 4 datasets: AIME, AMC, GPQA-D, and BBEH. Each component addition adds to the previous method. Method AIME AMC GPQA-D BBEH Average Acc. Len. Acc. Len. Acc. Len. Acc. Len. Acc. Len. DeepSeek-R1-Distill-Qwen-7B Base Model + CR + LR TRAAC 33.71 35.81 32.73 38. 8.2 7.6 6.0 7.3 74.22 78.55 79.04 77.83 5.7 4.9 3.3 4.5 43.55 45.99 45.99 47.31 7.1 6.1 3.5 6.2 10.61 11.74 11.51 11. 5.9 5.1 2.7 5.2 40.5 43.0 42.3 43.8 6.7 5.9 3.9 5.8 A.3 COMPRESSION MODULE A.3.1 AUXILIARY PROMPT For every reasoning trajectory, auxiliary prompt was appended at the end of the trajectory. The prompt is: Time is up. should stop thinking and now write summary containing all key steps required to solve the problem.. 3Huggingface Math-Verify 14 Table 9: Ablation Results of TRAAC on Deepseek-Qwen-7B on OptimalThinkingBench (OTB). Each component addition adds to the previous method. Method OverthinkingBench UnderthinkingBench OTB Acc. Len. AUCOAA Acc. Len. F1 DeepSeek-R1-Distill-Qwen-7B Base Model + CR + LR TRAAC 78.45 79.51 78.06 81.81 0.9 0.8 0.4 1.0 72.38 73.36 72.61 72.89 12.69 17.05 14.69 22.30 6.2 5.7 3.0 5.9 21.6 27.7 24.4 34. A.3.2 SPECIAL TOKENS TO SPLIT TRAJECTORY TO CHUNKS Below is the list that is used to split each reasoning trajectory into multiple reasoning steps. split_tokens = [ \"Wait\", \"Alternatively\", \"Another angle\", \"Another approach\", \"But wait\", \"Hold on\", \"Hmm\", \"Maybe\", \"Looking back\", \"Okay\", \"Let me\", \"First\", \"Then\", \"Alright\", \"Compute\", \"Correct\", \"Good\", \"Got it\", \"I don't see any errors\", \"I think\", \"Let me double-check\", \"Let's see\", \"Now\", \"Remember\", \"Seems solid\", \"Similarly\", \"So\", \"Starting\", \"That's correct\", \"That seems right\", \"Therefore\", \"Thus\" ] A.3.3 UNIFORMITY SCORE Algorithm 1 presents the pseudocode for calculating the uniformity score, based on which the final compression rate is calculated. Algorithm 1: Calculating Eviction Percentage Based on Attention Uniformity Input: Step importance scores {s1, s2, . . . , sn}, target reduction τ (default: 0.25) Output: Eviction percentage [0, 1] Function CALCULATEUNIFORMITYSCORE({s1, . . . , sn}): if 1 then return 1.0; ; Clamp all si 0; (cid:80) si; if 0 then return 1.0; pi log(pi + ϵ) ; pi si/T ; (cid:80) Hmax log(n); if Hmax = 0 then return 1.0; return H/Hmax ; // Only one step perfectly uniform // Normalize to probability distribution // Entropy, ϵ = 1012 // Uniformity score in [0, 1] Function DETERMINEEVICTIONPERCENTAGE(u, τ ): if > 0.8 then return 0.0 ; τ (1 u) ; return min(e, 0.8) ; // High uniformity: keep all steps // Scale eviction by non-uniformity // Cap eviction at 80% CALCULATEUNIFORMITYSCORE({s1, . . . , sn}); DETERMINEEVICTIONPERCENTAGE(u, τ ); 15 A.4 GRPO DETAILS For each question q, group of responses {y1, y2, . . . , yN } is sampled from the old policy πold, and the policy model πθ is optimized by maximizing the following GRPO objective. JGRPO(θ) = 1 (cid:88) i=1 1 yi yi (cid:88) t=1 min (cid:20) πθ(yi(t)yi πold(yi(t)yi <t) <t) ˆAi,t, clip (cid:18) πθ(yi(t)yi πold(yi(t)yi <t) <t) , 1 ε, 1 + ε (cid:19) (cid:21) , ˆAi,t where ε is the clipping range hyperparameter, and ˆAi,t represents the advantage, computed based on the relative verifiable outcome based rewards of outputs within each group. A.5 EXPERIMENTAL DETAILS We adopt verl (Sheng et al., 2024) as the training framework. A.5.1 HYPERPARAMETERS Table 10: Hyperparameters used for training, evaluation, and difficulty calibration. Category Hyperparameter Value Number of rollouts Temperature top top Max response length clip ratio low clip ratio high kl loss coef Learning rate (LR) Number of rollouts Temperature top top Max response length Hard Medium Easy 8 1.0 1.0 -1.0 10k 0.20 0.28 0.001 1e8 1.0 1.0 -1.0 10k 5 0.20 0.40 0.60 Training Evaluation Difficulty Calibration A.5.2 TRAINING REWARD To ensure high weight on correctness relative to other components, we assign correctness reward of +4 if the final answer is correct and 0 otherwise. The format reward ranges from 0 to 1: score of 0.5 is given for the presence of the <think> and </think> tokens, and an additional 0.5 is awarded if every reasoning trajectory is properly enclosed within these tokens in the correct order. The length reward ranges from 0 to 2. The overall reward is computed as the sum of these components: Total Reward = Correctness Reward + Format Reward + Length Reward. A.5.3 EVALUATION METRICS For each of the dataset we compute the accuracy and the average response length. Specifically for OverthinkingBench we also compute the AUCOAA. This metric is based on Overthinking-Adjusted 16 Accuracy (OAA), which measures model correctness under limit on reasoning tokens. For threshold t, it is defined as OAAt = 1 (cid:88) i=1 (cid:0)Correctnessi I(ThinkTokensi < t)(cid:1), where Correctnessi {0, 1} indicates whether the i-th response is correct, and I() is the indicator function that enforces the thinking length constraint. AUCOAA = (cid:90) tmax 0 OAAt tmax dt 1 tmax tmax(cid:88) t=0 OAAt, where tmax is the maximum number of allowed thinking tokens. Furthermore, following the method from (Aggarwal et al., 2025), to compute the OptimalThinkingBench metric: F1 score we combine the AUCOAA from OverthinkingBench and Accuracy (Accut) from UnderthinkingBench into single F1 score: 1 = 2 AUCOAA Accut AUCOAA + Accut (1) We computed AUCOAA and 1 scores using our own implementation, since the evaluation scripts from Cheng et al. (2025) were not open-sourced. A.5.4 TRAINING PROMPT For each questions in the training set, instruction was provided: Lets think step by step and output the final answer within boxed{} A.6 CONFIDENCE BASED COMPRESSION Similar to attention compression, where score is calculated for each reasoning token, the confidence of the model is used to calculate the score, and based on the lowest average score, reasoning steps are removed. Algorithm 2 shows the pseudocode used to calculate the confidence of each token. Algorithm 2: Token Confidence Calculation Input: Top-k token log-probabilities = {ℓ1, ℓ2, . . . , ℓk} Output: Confidence score begin // Convert log-probabilities to probabilities pj exp(ℓj) for each ℓj ; // Normalize probabilities (cid:80)k j=1 pj ; pj pj/Z for each ; // Compute entropy of distribution (cid:80)k j=1 pj log(pj + ϵ) ; // Maximum entropy with tokens Hmax log(k) ; // Confidence is normalized inverse entropy 1 (H/Hmax) ; return A.6.1 BASELINE PROMPTS Below we define the instruction that was provided to each baseline model: Base Model: Lets think step by step and output the final answer within boxed{} 17 L1-Max: Lets think step by step and output the final answer within boxed{}. Think for maximum 10000 tokens. LC-R1: Please reason step by step, and put your final answer within boxed{} AdaptThink: No prompt, just the question TokenSkip: <im start>system assistant.<im end> <im start>user Please reason step by step, and put your final answer within boxed{}. question<eot id>0.5<eot id><im end> <im start>assistant helpful You are A.7 COMPUTE USED All training was done on 4*A100 (80GB)."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "The University of Texas at Austin",
        "UNC Chapel Hill"
    ]
}