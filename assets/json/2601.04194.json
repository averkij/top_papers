{
    "paper_title": "Choreographing a World of Dynamic Objects",
    "authors": [
        "Yanzhe Lyu",
        "Chen Geng",
        "Karthik Dharmarajan",
        "Yunzhi Zhang",
        "Hadi Alzayer",
        "Shangzhe Wu",
        "Jiajun Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord"
        },
        {
            "title": "Start",
            "content": "Yanzhe Lyu1,, Yunzhi Zhang1 Chen Geng1, Hadi Alzayer1,3 Karthik Dharmarajan1 Shangzhe Wu2 Jiajun Wu1 1Stanford University 2University of Cambridge 3University of Maryland 6 2 0 2 7 ] . [ 1 4 9 1 4 0 . 1 0 6 2 : r Figure 1. 4D scene motion generated by our method. We present CHORD, universal generative pipeline capable of animating scenes with multiple objects that interact with each other. Project page: https://yanzhelyu.github.io/chord"
        },
        {
            "title": "Abstract",
            "content": "Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing Equal contribution. Work was done when Y. Lyu was visiting student at Stanford University. Y. Lyu is currently with the University of Science and Technology of China. distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate diverse range of multibody 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https: //yanzhelyu.github.io/chord 1. Introduction Humans and other embodied agents live in 4D (3D + time) world, world composed of diverse range of dynamic objects, i.e., objects that can evolve, deform, or interact with other objects. Creating 4D motions for both object deformations and interactions is crucial when building 3D world models for robotics [48, 74] and embodied AI [36]. Traditionally, it has been challenging to generate such motions for scene composed of dynamic objects in their static snapshots because it requires extensive manual modeling and expert labor. Recent approaches [73] have attempted to learn such 4D generators purely from data in an end-to-end manner. However, most existing datasets [13] focus on the internal deformations and evolutions of an individual object with little to no coverage on their interactions, and 4D data describing both deformations of objects and object interactions is extremely rare. This scarcity on scene-level 4D dynamics has rendered existing data-driven approaches into only being capable of generating dynamics of single object. Inspired by the recent success of general-purpose video generative models, we use different approach to tackle this problem: distilling these scene motions from video generative models. At high level, we iteratively optimize the low-level Lagrangian deformations of each object. At each optimization step, we deform the 3D scene and render it from certain viewpoints, and let video generative models judge whether the deformation is plausible. Through this process, we essentially leverage video models as highlevel choreographer to plan the motions of individual objects and make them consistent with each other. Despite the promise of this distillation-based paradigm, getting plausible results with it has been challenging. Existing methods [4, 30, 64] mainly operate at the object level and often show noticeable artifacts in the generated motion. Two major obstacles hinder these approaches from working effectively in our setting: (1) 4D deformations are both spatially high-dimensional and temporally ill-regularized, and (2) the non-conventional architecture designs of modern video generative models are not compatible with existing distillation algorithms [53]. We address the first challenge by analyzing the inherent locality of 4D deformations: temporal deformation fields should be locally smooth in both space and time. To this end, we design coarse-to-fine 4D motion representation that injects hierarchical structures to both the spatial and the temporal domain. Spatially, we adopt bi-level control point-based representation that disentangles finegrained motion details from coarse transformations. Temporally, inspired by time-honored data structure in theoretical algorithm design, i.e. the Fenwick tree [15, 34], we store deformations in cumulative, range-based structure that implicitly enforces temporal coherence and improves the learnability of long-horizon motion. With these two innovations, our novel 4D representation is robust, stable, and supports generating diverse range of motions. The second challenge stems from modern video generative models being based on flow-based models [44]. These models are incompatible with the traditional distillation algorithms. Therefore, we propose novel strategy for distillation from modern rectified flow-based video generative models. We derive novel Score Distillation Sampling (SDS) [53] target for flow-based video diffusion models and analyze their noise pattern, thus enabling video models to effectively provide guidance to our 4D representation. By proposing these two innovations and the framework to choreograph object motion, we arrive at simple yet elegant solution to the challenging problem of generating 4Dconsistent motion of dynamic objects in scene. We name this pipeline CHORD, for CHOReographing Dynamic objects and scenes. CHORD is universal, versatile, and applicable across wide range of dynamic phenomena. We evaluate our framework on diverse dynamic objects and compare it against prior art and show clear advantages. Beyond visual generation, our pipeline also enables the robot manipulation in the physical world by generating physically-grounded Lagrangian deformation trajectories of real-world objects. We demonstrate this by leveraging the generated 3D trajectories to plan the motion of real robot and showing that they can guide zero-shot manipulation of diverse dynamic objects. In summary, our contributions are as follows: 1. 4D motion representation that combines Fenwick treeinspired cumulative temporal structure with hierarchical low-to-high DoF parameterization, making it well-suited for distillation-based 4D generation. 2. distillation strategy for modern flow-based video generative models to make SDS algorithms effective on generating 4D motions from 2D video generative models. 3. robust framework to generate physically-grounded 4D motions for diverse dynamic objects that are applied to learning real-world robotic manipulation policies. 2. Related Work Object-Level 4D Generation. Generating 4D consistent object deformations has been long-standing challenge in the community. Traditional approaches first determine category-specific kinematic models (i.e., rigging representations) [6, 8, 22, 43, 45, 51, 71, 88] and then generate motion based on them [20, 28, 42, 47, 52, 58, 60, 63, 77], which inherently limits these methods to constrained categories. Some methods [73, 82, 86] attempt to learn end-toend 4D generators from existing 4D object datasets [1214], but they struggle to generalize beyond humanoid characters since most existing datasets are dominated by animated human-like models. Other approaches [4, 19, 21, 29, 38, 40, 46, 49, 55, 56, 61, 64, 67, 75, 78, 79, 83, 85] avoid supervised learning by performing 4D reconstruction or distillation from video generative models, yet they typically yield minor and unrealistic motion due to the difficulty of optimizing high-dimensional 4D motion and the noise in the 2 guidance signals. Our framework addresses these limitations by assuming neither category-specific kinematic structure nor large-scale 4D datasets, and generates realistic 4D motion for arbitrary objects. Scene-Level 4D Generation. Scene-level 4D generation extends beyond the object-centric setting, introducing substantially more complexity and greater challenges. It must not only produce plausible object-level motion but also maintain motion consistency across multiple interacting objects. Therefore, existing methods often simplify the problem by restricting it to specific categories (e.g., humanobject interaction [25, 37, 68, 81]), enforcing physical constraints [9, 39, 41, 87], or conditioning on symbolic structures [3, 57]. Some approaches attempt to produce 4D scenes by reconstructing them from videos [10, 35, 66, 70] generated by video models, yet the resulting representation remains largely 2.5D and does not support full 360 view synthesis. Our approach is the first to tackle the challenging setting of generating scene-level 4D motion of objects without relying on any category-specific inductive bias. 4D Representations. key component in 4D generation pipelines is the selection of the underlying 4D representation. Early works use high-dimensional deformation fields to represent 4D scenes [19, 50, 54, 69]. They work well for reconstruction targets with dense inputs, but are not suitable for generative tasks with noisy supervision signals. Recent works explore reducing the dimensionality of 4D representations in the spatial domain [21, 22, 66, 72]. Our hierarchical 4D representation strengthens this idea by injecting lowdimensionalities and hierarchies in both spatial and temporal domains, which serves as backbone representation in our 4D generation framework. 3. Method Given 3D scene containing multiple dynamic objects represented by their static 3D snapshots, along with text prompt describing how the scene should change over time (e.g., man facing lamp with the prompt the man lowers the head of the lamp with his hand), our goal is to generate sequence of temporal deformations that drive the objects so that the resulting 3D animations aligns with the prompt. Figure 2 shows an overview of our method. We iteratively optimize 4D scene motion representation using guidance signals distilled from video generative model. In the following section, we detail the three main components in this framework: strategy for distillation from modern rectified flow-based video generative models (Sec. 3.2), robust and general 4D scene motion representation (Sec. 3.3), and regularization terms to ensure stable optimization (Sec. 3.4). 3 3.1. Preliminary: Score Distillation Sampling The Score Distillation Sampling method [53] was introduced to distill 3D assets from image diffusion models [24]. At each iteration, an image is rendered from the 3D asset parameterized by θ. Gaussian noise ϵ is then added to produce noisy image zτ , where the noise level τ is uniformly sampled from (0, 1). The noisy image zτ is subsequently fed into image diffusion model, which predicts noise ˆϵ. SDS updates θ with the following gradient: θLSDS(θ; z, y) = Eτ,ϵ (cid:20) w(τ ) (ˆϵ (zτ ; τ, y) ϵ) (cid:21) , θ (1) where w(τ ) is weighting function. Extending this idea to 4D generation follows the same principle: at each iteration, video is rendered from the 4D asset, blended with noise, and then passed through the diffusion model, which provides gradients to update the 4D representation. 3.2. Distilling from Rectified Flow Models The above-mentioned 4D SDS algorithm is conceptually simple, yet it is non-trivial to apply them to distill from modern video generative models. The major obstacle is the gap between the diffusion architecture used in the original SDS target and the Rectified Flow (RF)-based model architecture in modern video generative models, such as Wan 2.2 [65] used in our paper. To mitigate this architectural gap, we derive novel SDS target for RF models. Similar to the derivation of SDS gradients for diffusion models [53], we align the optimization objective with the models training loss and express the SDS update rule for RF models as: θLRFSDS(θ; z, y) = (cid:20) Eτ,ϵ w(τ ) (ˆv (zτ ; τ, y) ϵ + z) (cid:21) , θ (2) where τ is the noise level uniformly sampled from (0, 1), w(τ ) is the corresponding weight in the training schedule, ϵ is the added noise, zτ = (1 τ )z + τ ϵ denotes the noisy video, and ˆv (zτ ; τ, y) is the predicted velocity. domain-specific noise sampling strategy is critical for this target to work well on our objective of optimizing scene deformations. We observed that the deformations are prone to be generated at higher noise levels τ , as significant changes only happen when substantial noise is added. Based on this observation and the properties of w(τ ), instead of sampling τ uniformly, we perform sampling according to probability density function ˆw(τ ) = w(τ ) dτ w(τ ), which is the normalized form of w(τ ). With this modification in sampling strategy, the weighted (cid:82) 1 Figure 2. Overview. For the input meshes of given scene, we first convert them into 3D-GS representations to enable smooth gradient computation. The converted 3D-GS models are then used to initialize 4D representation (Sec. 3.3). We iteratively refine this 4D representation by sampling camera poses at each iteration, rendering the corresponding videos, and passing them to the video generation model to obtain optimization gradients (Sec. 3.2). Additionally, we compute regularization terms (Sec. 3.4) to enforce spatial and temporal smoothness during the optimization process. RFSDS update rule becomes: θLW-RFSDS(θ; z, y) = (cid:20) (ˆv (zτ ; τ, y) ϵ + z) Eτ ˆw(τ ),ϵ (cid:21) , θ (3) where the weighting term in RFSDS gradients defined in Eq. (16) is eliminated to ensures the invariance of the expectation of gradients. Empirically, this yields more realistic generated motion, as shown in Sec. 4.3. Practically, this noise sampling strategy is implemented with an annealing noise schedule [26, 62] during the optimization. At each optimization step out of entire iterations, we set τ to be fixed noise level τi, which is obtained by solving: h(τi) = 1 + 1 , (4) where h(τ ) = (cid:82) τ ˆw(t) dt is the cumulative distribution function (CDF) of ˆw(τ ). This creates an annealing schedule in which τ gradually decreases over training, enabling coarse motion to form early and allowing fine deformations to be refined in later iterations. 3.3. Hierarchical 4D Representation Most existing 4D representations are highly unstable to optimize with the W-RFSDS target described above. Therefore, we introduce hierarchical 4D representation that leverages natural locality of deformations in both spatial and temporal domain to stabilize the optimization process. Our representation is composed of two components: geometric representation of canonical shapes and 4D motion representation that deforms the canonical geometry in different frames. The canonical shape of our 4D representation is represented with 3D-GS [31]. Specifically, given mesh inputs, we convert them into 3D-GS models = {Gi}N i=1 by optimizing directly on multi-view images rendered from the meshes, where each Gi denotes converted 3D-GS model. Figure 3. Illustration of the hierarchical control point representation. We represent the deformation using spatial hierarchical structure. Coarse control points capture large-scale deformations, while fine control points refine local details. At time t, the canonical 3D geometry is deformed with set of deformation fields to represent the 4D motion of the 3D-GS scene S. The set of deformation fields at time is denoted by Dt = {T i=1, where }N denotes the deformation for object at time t. The 4D scene motion is represented with novel representation that injects hierarchical structures in both spatial and temporal domain, as detailed below. Spatial Hierarchy with Control Points. The deformation fields are spatially high-dimensional, and we reduce the dimensionality of this representation with hierarchical control point-based representation. Inspired by SC-GS [27], we represent with coarse level and fine level of control points sparse set of spatially-grounded blobs that controls local spatial region of deformations. The coarse level of control points roughly dictates how an object will deform, and the fine level adds more details to the deformation. Specifically, each control point is defined by mean and covariance matrix Σ, which together determine its radius of influence. In addition, each control point maintains sequence of deformations (Rt, Tt) in SE(3). The deformation of Gaussian is obtained by blending transformations from neighboring control points using linear blend skinning. For Gaussian (µ, q, S, C, o) Gi, we denote its nearest neighboring control points as . The deformed Gaussian at time is then computed as: (cid:88) µt = βk (cid:0)Rt k(µ pk) + pk + k (cid:1) , kN (cid:88) qt = ( kN βkrt k) q, (5) (6) R4 are the quaternion representations of rotawhere rt tion on control point k, and is the production of quaternions. Furthermore, βk in the formula denotes the blending weight of control point k, which is calculated through: βk = ˆβk (cid:80) lN ˆβl , ˆβk = exp (cid:18) (cid:104) 1 2 (µ pk) Σ (µ pk)T (cid:105)(cid:19) . (7) We optimize the bi-level sets of control points in coarse-to-fine manner, following the noise schedule defined in Eq. (4). When τ is large during the optimization process, substantial motion can be generated; however, the SDS gradients produced at such noise levels are often noisy. Conversely, when τ is annealed to lower value, the gradients become more stable but are less capable of producing substantial deformations. To accompany with the inherent nature of this optimization process, we only optimize the coarse level of control points at earlier iterations when τ is large, and we introduce the fine control points later, once τ becomes smaller, to append their residual deformations: µt final = µt + µt, final = qt qt qt (8) (9) where µt and qt denote the residual deformations from the fine layer of control points, computed in the same manner as in Eq. (5) and Eq. (6). After training, the deformation learned with Gaussians can be directly transferred to deform meshes. Concretely, we deform the mesh vertices using Eq. (5) by substituting the Gaussian means with the vertex positions. Temporal Hierarchy with the Fenwick Tree. We further observe that deformations of later frames are challenging to learn if (Rt, t) of frame are modeled independently from other frames. This can be explained by the fact that all deformations are initially initialized as zero vectors and the parameters of the first frame are kept frozen, leading to the significant deviation of deformations in later frames. To alleviate this issue, we represent the sequence of deformations for each control point (Rt, t) with the Fenwick tree, hierarchical data structure from theoretical algorithm design [18]. As illustrated in Figure 4, for each control point k, we maintain nodes Fk = {(r[j] j=1, where each node encodes the accumulated deformation over specific range of frames. This range-based decomposition allows deformations at different frames to share parameters , [j] )}T Figure 4. Illustration of the Fenwick Tree representation. Each node stores the cumulative deformation over temporal range, allowing nearby frames to share parameters and naturally enforcing temporal coherence. For example, (r[6] ) encodes the accumulated deformation from frames 56. Queries for frames 6 and 7 then compose their deformations from small, overlapping set of nodes, as shown in the figure. , [6] through overlapping intervals, greatly improving temporal coherence and enable the learning of long-horizon motion. The final deformation at frame is obtained by composing all relevant nodes: (cid:88) = jBIT(t) rt = norm( [j] , (cid:88) jBIT(t) r[j] ), (10) (11) where BIT(t) denotes the set of active nodes returned by the Fenwick query operation, and norm() ensures that the summed result forms valid quaternion. 3.4. Regularization We introduce two regularization terms to further stabilize the optimization process: temporal regularization loss to enforce smoothness over time and spatial regularization loss to encourage local spatial consistency. Temporal Regularization. When rendering the RGB video for computing the SDS gradients, we additionally render 3D flow map video from the same viewpoint, which is used for temporal regularization. To produce the flow map at frame t, we replace the color attribute of Gaussians in the 3D-GS rendering equation with µt , where µt denotes the mean of Gaussian at time t. After obtaining F, the temporal regularization loss is defined as: µt+1 Ltemp = (cid:88) (cid:88) F 2 2, (12) where the inner summation is over all pixels p, and Ft resents the rendered 3D flow at pixel and time t. Spatial Regularization. To ensure spatially uniform regularization, we generate uniformly distributed point cloud near the surface of each object i, deform it using the learned motion, and compute an As-Rigid-As-Possible (ARAP) rep5 loss [59] over the resulting sequence of deformed point clouds. Specifically, we first compute signed distance field (SDF) ϕi(x) from the mesh of object i. We then extract voxel centers near the surface as Si = {x ϕi(x) τ, Vs}, where Vs is the set of voxel centers on grid with voxel size s, and τ is predefined threshold. At each iteration, for every Si and timestamp t, we compute its deformed position xt using Eq. (5) (with µ replaced by x), thereby producing the deformed point set = {xt Si}. ARAP loss is then calculated as: LARAP = (cid:88) i,t,xSi,yNx ˆRx(xt yt)2 2, (13) where Nx denotes the set of the 10 nearest neighbors of in Si, and ˆRx is the estimated local rotation matrix at x. 4. Experiments We evaluate our proposed method on diverse dynamic scenes featuring multiple interacting objects. We compare our approach with several state-of-the-art baselines, each representing distinct category of methods. 4.1. 4D Scene Motion Generation We compare our method against state-of-the-art mesh animation approaches, as well as 4D reconstructions from camera-controlled video models. Specifically, we compare our approach with four baselines: Animate3D [30], AnimateAnyMesh [73], MotionDreamer [64], and TrajectoryCrafter [84]. Animate3D generates multi-view videos using multi-view video diffusion model and then performs 4D reconstruction on them. AnimateAnyMesh directly predicts mesh deformations using pretrained Rectified Flow model. MotionDreamer first generates video conditioned on the text prompt and rendering of the given mesh, and then animates the mesh by performing diffusion feature matching with the generated video. We present results from our reimplementation using Wan 2.2, and provide results obtained with DynamiCrafter [76] which was used in its original pipeline in the supplementary materials. TrajectoryCrafter is video generation model that redirects camera trajectories for monocular videos. We first generate video using Wan 2.2, then produce corresponding multiview videos with TrajectoryCrafter, and finally perform 4D reconstruction on the sampled videos. We select six scenes spanning diverse object categories for comparison: man petting dog, cat stepping on cushion, sealion nudging ball, block falling on trampoline, Two men shaking hands, and robot picking up block. We additionally include comparisons between our method and baseline approaches for singleobject mesh animation in the supplementary materials. Qualitative Comparisons. Part of the qualitative results are shown in Figure 5; please refer to the supplementary Table 1. Quantitative comparisons with baselines. We conduct user study on six scene animations to evaluate the performance. Additionally, we report the Semantic Adherence (SA) and Physical Commonsense (PC) metrics computed with VideoPhy-2 [5]. User Study VideoPhy-2 Alignment Realism SA PC Animate3D AnimateAnyMesh MotionDreamer (DC) MotionDreamer (Wan) TrajectoryCrafter 0.34% 1.01% 0.51% 0.84% 9.60% 0.51% 0.51% 0.84% 0.34% 10.44% 3.83 3.5 3.42 3.5 4.17 CHORD (Ours) 87.71% 87.37% 4.33 3.42 4.5 4.08 3.83 3. 4.25 materials for the complete set of results. Our method exhibits stronger prompt alignment and generates more natural motion compared to existing approaches. Animate3D and AnimateAnyMesh fail to generate results that align with the given prompts, as they have not been extensively trained on 4D data containing multiple objects. MotionDreamer suffers from severe artifacts due to errors in diffusion feature matching when fitting meshes. Although 4D reconstruction from videos sampled via TrajectoryCrafter yields motions that follow the prompts, the results suffer from strong temporal inconsistencies and unnatural dynamics due to discrepancies among videos generated under different camera trajectories. This highlights the necessity of distilling video model in our method. Quantitative Comparisons. We perform user study with 99 participants to compare the quality of our method with the baselines. Additionally, we utilize VideoPhy-2 [5] to automatically evaluate the rendered videos from two aspects: Semantic Adherence (SA) and Physical Commonsense (PC). As shown in Table 1, our method achieves the highest score in SA and the second-highest score in PC. Note that AnimateAnyMesh achieves the highest Physical Commonsense (PC) score due to its common failure mode, where objects remain statican outcome that aligns with physical commonsense but fails to follow the given prompt. 4.2. Extensions and Applications Beyond generating multi-object 4D motion, our framework naturally supports several extension and downstream uses. Long-Horizon Motion Generation. By using the last frame of the generated deformation as the input state for the subsequent generation process, we can extend our method to produce longer motion sequences. In Figure 1, we show an example motion sequence consisting of four actions. Real-world Object Animation. Since our method distills video generative model trained extensively on realworld video data, it is robust and can be applied to animate scanned real-world objects without concern for the gap between synthetic and real-world data, as shown in Figure 6. Robot Manipulation. We demonstrate that the dense ob6 Figure 5. Qualitative comparisons. We compare our approach with several mesh animation methods. Our method produces results that better align with the given prompts and exhibit more natural motion. In the figure, A3D refers to Animate3D [30], AAM denotes AnimateAnyMesh [73], MD represents MotionDreamer [64], and TC corresponds to 4D reconstruction results from videos generated by TrajectoryCrafter [84]. For additional comparisons and full animation results, please refer to our supplementary website. grasp planner [17] to propose grasp on the relevant object. Then, the robot either grasps the object or moves to pose for pushing the object, which is at an offset from the proposed grasp. Constrained by rigid attachment forward model, where relative transformations of the end-effector also apply to the initial points on the object, motion planner [32] solves for sequence of end-effector poses to minimize an objective consisting of transformed points to dense flow alignment, reachability, and pose smoothness costs. Figure 6. Real-world object animation results. ject flow generated by our method can be utilized as guidance for manipulation of rigid, articulated, and deformable objects, as shown in Figure 7. We first use an off-the-shelf 7 Figure 7. Robot manipulation guided by our generated dense object flow. Given our generated dense object flow, the robot either grasps or pushes the object of interest in manner that matches the flow. This allows effective manipulation of rigid objects (first row), articulated objects (second row), and deformable objects (third and fourth rows). Figure 8. Ablation on noise-level sampling strategy. Removing our noise-level sampling strategy leads to unnatural motion, such as the laptop appearing to float. 4.3. Ablation Studies Noise Level Sampling Strategy We compare the effectiveness of the noise-level sampling strategy (Sec. 3.2) against uniform noise sampling with weighting. As shown in Figure 8, unrealistic results emerge under uniform sampling due to insufficient coverage of noise levels that inject motion. In this case, the laptop appears to float above the table. 4D Representation. We study two key components of our 4D representation: the Fenwick tree for modeling deformation sequences and the hierarchical control-point structure. Results are shown in Figure 9. Removing the Fenwick tree leads to noticeable artifacts, as later frames become extremely difficult to learn when each deformation is modFigure 9. Ablations on components in the 4D representation. Removing the Fenwick Tree leads to severe artifacts in later frames; removing fine control points prevents detailed deformation; and removing coarse control points causes distortions. eled independently. Removing the fine control-point layer prevents the model from producing detailed motions (e.g., grasping). Conversely, starting with the fine layer from the beginning also introduces artifacts, since the noise injected at early iterations cannot be effectively smoothed without an initial coarse stage. Regularization. We show that the regularization losses are necessary. Removing them results in temporal flickering (e.g., the tail suddenly appearing when temporal regularization is removed) and visual artifacts (when spatial regularization is removed), as shown in Figure 10. 8 computer vision and pattern recognition, pages 55435552, 2016. 2 [9] Boyuan Chen, Hanxiao Jiang, Shaowei Liu, Saurabh Gupta, Yunzhu Li, Hao Zhao, and Shenlong Wang. Physgen3d: Crafting miniature interactive world from single image. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 61786189, 2025. 3 [10] Wen-Hsuan Chu, Lei Ke, and Katerina Fragkiadaki. Dreamscene4d: Dynamic multi-object scene generation from monocular videos. Advances in Neural Information Processing Systems, 37:9618196206, 2024. 3 [11] Blender Online Community. Blender - 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. 13 [12] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36:3579935813, 2023. [13] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. 2 [14] Yufan Deng, Yuhao Zhang, Chen Geng, Shangzhe Wu, and Jiajun Wu. Anymate: dataset and baselines for learning 3d object rigging. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 110, 2025. 2 [15] Knuth Donald et al. The art of computer programming. Sorting and searching, 3(426-458):4, 1999. 2 [16] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the 41st International Conference on Machine Learning, pages 1260612633, 2024. 13 [17] Hao-Shu Fang, Chenxi Wang, Hongjie Fang, Minghao Gou, Jirong Liu, Hengxu Yan, Wenhai Liu, Yichen Xie, and Cewu Lu. Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. IEEE Transactions on Robotics (T-RO), 2023. [18] Peter Fenwick. new data structure for cumulative frequency tables. Software: Practice and experience, 24(3): 327336, 1994. 5 [19] Quankai Gao, Qiangeng Xu, Zhe Cao, Ben Mildenhall, Wenchao Ma, Le Chen, Danhang Tang, and Ulrich Neumann. Gaussianflow: Splatting gaussian dynamics for 4d content creation. arXiv preprint arXiv:2403.12365, 2024. 2, 3 [20] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei Zhou. Learning neural volumetric representations of dynamic humans in minutes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 87598770, 2023. 2 [21] Chen Geng, Yunzhi Zhang, Shangzhe Wu, and Jiajun Wu. Birth and death of rose. In Proceedings of the Computer Figure 10. Ablations on regularization losses. Removing temporal regularization results in flickering, while removing spatial regularization results in distortions. 5. Conclusion We introduce robust, scalable, and versatile approach to generate scene-level 4D object motion given only 3D shapes as input. Our pipeline works effectively for diverse natural phenomena and opens new possibilities of scalable 4D generation with guidance from video generative models. It also enables downstream applications as we demonstrated in the robotics manipulation."
        },
        {
            "title": "References",
            "content": "[1] Sketchfab. https : / / sketchfab . com. Accessed: 2025-11-18. 13 [2] Noam Aigerman, Kunal Gupta, Vladimir G. Kim, Siddhartha Chaudhuri, Jun Saito, and Thibault Groueix. Neural jacobian fields: Learning intrinsic mappings of arbitrary meshes. ACM Transactions on Graphics (SIGGRAPH 2022), 2022. 13 [3] Sherwin Bahmani, Xian Liu, Wang Yifan, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, et al. Tc4d: In European Trajectory-conditioned text-to-4d generation. Conference on Computer Vision, pages 5372. Springer, 2024. [4] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David Lindell. 4d-fy: Text-to-4d generation using hybrid score disIn Proceedings of the IEEE/CVF Contillation sampling. ference on Computer Vision and Pattern Recognition, pages 79968006, 2024. 2 [5] Hritik Bansal, Clark Peng, Yonatan Bitton, Roman Goldenberg, Aditya Grover, and Kai-Wei Chang. Videophy-2: challenging action-centric physical commonsense evaluation in video generation. arXiv preprint arXiv:2503.06800, 2025. 6 [6] Volker Blanz and Thomas Vetter. Face recognition based on fitting 3d morphable model. IEEE Transactions on pattern analysis and machine intelligence, 25(9):10631074, 2003. 2 [7] BlenderKit. Blenderkit online asset library. https:// www.blenderkit.com. Accessed: 2025-11-18. 13 [8] James Booth, Anastasios Roussos, Stefanos Zafeiriou, Allan Ponniah, and David Dunaway. 3d morphable model learnt from 10,000 faces. In Proceedings of the IEEE conference on 9 Vision and Pattern Recognition Conference, pages 26102 26113, 2025. 2, of the Computer Vision and Pattern Recognition Conference, pages 61656177, 2025. 3 [22] Guangzhao He, Chen Geng, Shangzhe Wu, and Jiajun Wu. Category-agnostic neural object rigging. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2207822088, 2025. 2, 3 [23] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 13 [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems (NeurIPS), 2020. 3 [25] Di Huang, Xiaopeng Ji, Xingyi He, Jiaming Sun, Tong He, Qing Shuai, Wanli Ouyang, and Xiaowei Zhou. Reconstructing hand-held objects from monocular video. In SIGGRAPH Asia 2022 Conference Papers, pages 19, 2022. 3 [26] Yukun Huang, Jianan Wang, Yukai Shi, Boshi Tang, Xianbiao Qi, and Lei Zhang. Dreamtime: An improved optimization strategy for diffusion-guided 3d generation. In The Twelfth International Conference on Learning Representations (ICLR), 2024. 4 [27] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 42204230, 2024. [28] Zehuan Huang, Haoran Feng, Yangtian Sun, Yuanchen Guo, Yanpei Cao, and Lu Sheng. Animax: Animating the inanimate in 3d with joint video-pose diffusion models. arXiv preprint arXiv:2506.19851, 2025. 2 [29] Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, and Yao Yao. Consistent4d: Consistent 360 {deg} dynamic obarXiv preprint ject generation from monocular video. arXiv:2311.02848, 2023. 2 [30] Yanqin Jiang, Chaohui Yu, Chenjie Cao, Fan Wang, Weiming Hu, and Jin Gao. Animate3d: Animating any 3d model with multi-view video diffusion. Advances in Neural Information Processing Systems, 37:125879125906, 2024. 2, 6, 7, 13, 14, 15, 18 [31] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 4, 13 [32] Chung Min Kim*, Brent Yi*, Hongsuk Choi, Yi Ma, Ken Goldberg, and Angjoo Kanazawa. Pyroki: modular toolkit for robot kinematic optimization. In 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025. 7 [33] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the International Conference on Learning Representations (ICLR), 2014. [34] Sumith Kulal, Jiayuan Mao, Alex Aiken, and Jiajun Wu. Hierarchical motion understanding via motion programs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65686576, 2021. 2 [35] Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds. In Proceedings [36] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martın-Martın, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In Conference on Robot Learning, pages 8093. PMLR, 2023. 2 [37] Hongjie Li, Hong-Xing Yu, Jiaman Li, and Jiajun Wu. Zerohsi: Zero-shot 4d human-scene interaction by video generation. arXiv preprint arXiv:2412.18600, 2024. 3 [38] Zhiqi Li, Yiming Chen, and Peidong Liu. Dreammesh4d: Video-to-4d generation with sparse-controlled gaussianmesh hybrid representation. Advances in Neural Information Processing Systems, 37:2137721400, 2024. 2 [39] Zizhang Li, Hong-Xing Yu, Wei Liu, Yin Yang, Charles Herrmann, Gordon Wetzstein, and Jiajun Wu. Wonderplay: Dynamic 3d scene generation from single image and actions. arXiv preprint arXiv:2505.18151, 2025. 3 [40] Hanwen Liang, Yuyang Yin, Dejia Xu, Hanxue Liang, Zhangyang Wang, Konstantinos Plataniotis, Yao Zhao, and Yunchao Wei. Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion models. arXiv preprint arXiv:2405.16645, 2024. 2 [41] Yuchen Lin, Chenguo Lin, Jianjin Xu, and Yadong Mu. Omniphysgs: 3d constitutive gaussians for genarXiv preprint eral physics-based dynamics generation. arXiv:2501.18982, 2025. 3 [42] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, and Christian Theobalt. Neural actor: Neural free-view synthesis of human actors with pose control. ACM transactions on graphics (TOG), 40(6):116, 2021. 2 [43] Ruoshi Liu, Alper Canberk, Shuran Song, and Carl VonarXiv preprint drick. Differentiable robot rendering. arXiv:2410.13851, 2024. 2 [44] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations (ICLR), 2023. 2, [45] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: skinned multiperson linear model. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 851866. 2023. 2 [46] Ziqiao Ma, Xuweiyi Chen, Shoubin Yu, Sai Bi, Kai Zhang, Chen Ziwen, Sihan Xu, Jianing Yang, Zexiang Xu, Kalyan Sunkavalli, et al. 4d-lrm: Large space-time reconstruction model from and to any view at any time. arXiv preprint arXiv:2506.18890, 2025. 2 [47] Naureen Mahmood, Nima Ghorbani, Nikolaus Troje, Gerard Pons-Moll, and Michael Black. Amass: Archive In Proceedings of of motion capture as surface shapes. the IEEE/CVF international conference on computer vision, pages 54425451, 2019. 2 [48] Zhao Mandi, Yifan Hou, Dieter Fox, Yashraj Narang, Ajay Mandlekar, and Shuran Song. Dexmachina: Functional arXiv retargeting for bimanual dexterous manipulation. preprint arXiv:2505.24853, 2025. 2 10 [49] Linzhan Mou, Jiahui Lei, Chen Wang, Lingjie Liu, and Kostas Daniilidis. Dimo: Diverse 3d motion generation for arbitrary objects. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1435714368, 2025. [50] Keunhong Park, Utkarsh Sinha, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Steven Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pages 58655874, 2021. 3 [51] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Expressive body capture: 3d hands, Michael Black. In Proceedings of face, and body from single image. the IEEE/CVF conference on computer vision and pattern recognition, pages 1097510985, 2019. 2 [52] Sida Peng, Chen Geng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Implicit neural representations with structured latent codes for human body modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(8):98959907, 2023. 2 [53] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR. OpenReview.net, 2023. 2, 3, 13 [54] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1031810327, 2021. 3 [55] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142, 2023. [56] Jiawei Ren, Cheng Xie, Ashkan Mirzaei, Karsten Kreis, Ziwei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, Huan Ling, et al. L4gm: Large 4d gaussian reconstruction model. Advances in Neural Information Processing Systems, 37:5682856858, 2024. 2 [57] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. Novel view synthesis of human interactions from sparse multi-view videos. In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. 3 [58] Chaoyue Song, Xiu Li, Fan Yang, Zhongcong Xu, Jiacheng Wei, Fayao Liu, Jiashi Feng, Guosheng Lin, and Jianfeng Zhang. Puppeteer: Rig and animate your 3d models. arXiv preprint arXiv:2508.10898, 2025. 2 [59] Olga Sorkine and Marc Alexa. As-rigid-as-possible surface In Symposium on Geometry processing, pages modeling. 109116, 2007. 6 [60] Shih-Yang Su, Frank Yu, Michael Zollhofer, and Helge Rhodin. A-nerf: Articulated neural radiance fields for learning human shape, appearance, and pose. Advances in neural information processing systems, 34:1227812291, 2021. [61] Qi Sun, Zhiyang Guo, Ziyu Wan, Jing Nathan Yan, Shengming Yin, Wengang Zhou, Jing Liao, and Houqiang Li. Eg4d: Explicit generation of 4d object without score distillation. arXiv preprint arXiv:2405.18132, 2024. 2 [62] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. In The Twelfth International Conference on Learning Representations(ICLR), 2024. 4 [63] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit Bermano. Human motion diffusion model. arXiv preprint arXiv:2209.14916, 2022. 2 [64] Lukas Uzolas, Elmar Eisemann, and Petr Kellnhofer. Motiondreamer: Exploring semantic video diffusion features for zero-shot 3d mesh animation. In 2025 International Conference on 3D Vision (3DV), pages 893904. IEEE, 2025. 2, 6, 7, 13, 14, 15, 18 [65] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3, 13, 18 [66] Qianqian Wang, Vickie Ye, Hang Gao, Weijia Zeng, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 96609672, 2025. 3 [67] Yikai Wang, Xinzhou Wang, Zilong Chen, Zhengyi Wang, Fuchun Sun, and Jun Zhu. Vidu4d: Single generated video to high-fidelity 4d reconstruction with dynamic gaussian surfels. Advances in Neural Information Processing Systems, 37:131316131343, 2024. 2 [68] Bowen Wen, Jonathan Tremblay, Valts Blukis, Stephen Tyree, Thomas Muller, Alex Evans, Dieter Fox, Jan Kautz, and Stan Birchfield. Bundlesdf: Neural 6-dof tracking and In Proceedings of 3d reconstruction of unknown objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 606617, 2023. [69] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2031020320, 2024. 3 [70] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan Barron, and Aleksander Holynski. Cat4d: Create anything in 4d with multi-view video diffusion modIn Proceedings of the Computer Vision and Pattern els. Recognition Conference, pages 2605726068, 2025. 3 [71] Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. Magicpony: Learning arIn Proceedings of the ticulated 3d animals in the wild. 11 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 87928802, 2023. 2 in Neural Information Processing Systems, 37:4525645280, 2024. 2 [84] Mark Yu, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocuIn Proceedings of the lar videos via diffusion models. IEEE/CVF International Conference on Computer Vision (ICCV), pages 100111, 2025. 6, 7, 13, 14, 15, [85] Yu-Jie Yuan, Leif Kobbelt, Jiwen Liu, Yuan Zhang, Pengfei Wan, Yu-Kun Lai, and Lin Gao. 4dynamic: Text-to-4d generation with hybrid priors. arXiv preprint arXiv:2407.12684, 2024. 2 [86] Bowen Zhang, Sicheng Xu, Chuxin Wang, Jiaolong Yang, Feng Zhao, Dong Chen, and Baining Guo. Gaussian variation field diffusion for high-fidelity video-to-4d synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1250212513, 2025. 2 [87] Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, and William Freeman. Physdreamer: Physics-based interaction with 3d objects via video generation. In European Conference on Computer Vision, pages 388406. Springer, 2024. 3 [88] Silvia Zuffi, Angjoo Kanazawa, David Jacobs, and Michael J. Black. 3D menagerie: Modeling the 3D shape and pose of animals. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017. 2 [72] Zijie Wu, Chaohui Yu, Yanqin Jiang, Chenjie Cao, Fan Wang, and Xiang Bai. Sc4d: Sparse-controlled video-to-4d generation and motion transfer. In European Conference on Computer Vision, pages 361379. Springer, 2024. [73] Zijie Wu, Chaohui Yu, Fan Wang, and Xiang Bai. Animateanymesh: feed-forward 4d foundation model for arXiv preprint text-driven universal mesh animation. arXiv:2506.09982, 2025. 2, 6, 7, 13, 14, 15, 18 [74] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. Sapien: simulated part-based interactive environment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11097 11107, 2020. 2 [75] Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, and Varun Jampani. Sv4d: Dynamic 3d content generation with multi-frame and multi-view consistency. arXiv preprint arXiv:2407.17470, 2024. 2 [76] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating In Euopen-domain images with video diffusion priors. ropean Conference on Computer Vision, pages 399417. Springer, 2024. 6, 18 [77] Zhen Xu, Sida Peng, Chen Geng, Linzhan Mou, Zihan Yan, Jiaming Sun, Hujun Bao, and Xiaowei Zhou. Relightable and animatable neural avatar from sparse-view video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9901000, 2024. 2 [78] Zeyu Yang, Zijie Pan, Chun Gu, and Li Zhang. Diffusion2: Dynamic 3d content generation via score composition of arXiv preprint video and multi-view diffusion models. arXiv:2404.02148, 2024. [79] Chun-Han Yao, Yiming Xie, Vikram Voleti, Huaizu Jiang, and Varun Jampani. Sv4d 2.0: Enhancing spatio-temporal consistency in multi-view video diffusion for high-quality 4d generation. arXiv preprint arXiv:2503.16396, 2025. 2 [80] Vickie Ye, Ruilong Li, Justin Kerr, Matias Turkulainen, Brent Yi, Zhuoyang Pan, Otto Seiskari, Jianbo Ye, Jeffrey Hu, Matthew Tancik, and Angjoo Kanazawa. gsplat: An open-source library for gaussian splatting. Journal of Machine Learning Research, 26(34):117, 2025. 13 [81] Yufei Ye, Abhinav Gupta, Kris Kitani, and Shubham Tulsiani. G-hop: Generative hand-object prior for interacIn Proceedings of tion reconstruction and grasp synthesis. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19111920, 2024. 3 [82] Jiraphon Yenphraphai, Ashkan Mirzaei, Jianqi Chen, Jiaxu Zou, Sergey Tulyakov, Raymond Yeh, Peter Wonka, and Chaoyang Wang. Shapegen4d: Towards high qualarXiv preprint ity 4d shape generation from videos. arXiv:2510.06208, 2025. 2 [83] Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Laszlo Jeni, Sergey Tulyakov, and Hsin-Ying Lee. 4real: Towards photorealistic 4d scene generation via video diffusion models. Advances 12 A. Implementation Details A.1. Pipeline Implementation Details The 3D assets used in our experiments are downloaded from Sketchfab [1] and BlenderKit [7], and we construct the static scene snapshots in Blender [11]. Rendering of 3D-GS [31] for both mesh-based initialization and 4D optimization is performed using gsplat [80]. We adopt the Wan 2.2 (14B) image-to-video model [65] as our video generation model. All training is conducted at resolution of 832 464 (the default for Wan 2.2), and deformation sequences of 41 frames are optimized. Control points are initialized based on the center points of an occupancy grid. Specifically, for each object, we first compute signed distance field (SDF) ϕi(x) from its given mesh. We then extract the set of voxel centers within the object as Ii = {x ϕi(x) 0, Vs}, where Vs denotes all voxel center points in grid with voxel size s. Finally, we apply farthest point sampling followed by K-means clustering on Ii to determine the positions pk of the control points. We further initialize the scale in each control points covariance matrix Σk as the average distance to its three nearest neighboring control points, and set the initial rotation to the identity. For stable optimization, we keep pk fixed and only optimize Σk during training. In the training of the deformations, we additionally introduce split training schedule: at iteration 100, we reinitialize all deformations after 30 to the deformation at 30, which further facilitates stable learning for later frames. We use the log-linear learning rate schedule adopted in 3D-GS. The learning rate for the deformations stored in the Fenwick tree decays from 0.006 to 0.00006. The learning rate for the scales of the control points follows the same decay (from 0.006 to 0.00006), while the learning rate for rotations decays from 0.003 to 0.00003. The CFG [23] scale is linearly decayed from 25 to 12. The weight for the temporal regularization loss is decayed from 9.6 to 1.6, and the weight for the spatial regularization loss is decayed from 3000 to 300. The voxel size used for extracting the uniformly distributed point cloud in temporal regularization and for initializing control points is automatically determined via binary search such that the number of voxel centers near the surface satisfies Si 7500. Each asset is trained for 2,000 iterations with batch size of 4, requiring approximately 20 hours on an NVIDIA H200 GPU. A.2. Robot Manipulation Implementation Details For the objects used to generate dense object flow, we directly scanned the real objects in the pick banana and lower lamp cases and fed the scans into our pipeline. For the other cases, due to challenges in accurately scanning the objects, we instead measured their length statistics and created digital cousins with matching dimensions in Blender before inputting them into our pipeline. A.3. Baseline Implementation Details For Animate3D [30] and AnimateAnyMesh [73], we merge all objects in the scene into single mesh and directly input it into their pipelines. For MotionDreamer [64], we follow their setup and use Neural Jacobian Fields (NJF) [2] as the animation model, training separate NJF for each object. For robust 4D reconstruction of videos sampled from TrajectoryCrafter [84], we use coarse set of control points with Fenwick-treebased deformation sequence as the 4D representation. We additionally apply both temporal and spatial regularization losses during optimization. B. Derivation of SDS for Rectified Flow Models When sampling noise levels τ uniformly from U(0, 1), the training loss of Rectified Flow (RF) model [16, 44] is: LRF(θ; z, y) = Eτ U(0,1), ϵ (cid:104) w(τ ) (cid:13) (cid:13)ˆv(zτ ; τ, y) (ϵ z)(cid:13) (cid:13) 2(cid:105) , where ϵ (0, I) and zτ = (1 τ )z + τ ϵ is the linearly interpolated latent. Taking the derivative of LRF with respect to yields: zLRF(θ; z, y) = Eτ U(0,1), ϵ (cid:20) w(τ ) (cid:0)ˆv(zτ ; τ, y) (ϵ z)(cid:1) (cid:18) ˆv(zτ ; τ, y) (cid:19)(cid:21) + . (14) (15) Following the derivation style of Score Distillation Sampling (SDS) [53], we omit the term that backpropagates through , and apply the chain rule from back to the 4D representation parameters θ. This gives the RF-SDS the RF model, ˆv(zτ ;τ,y) gradient used in the main text: θLRFSDS(θ; z, y) = Eτ, ϵ (cid:20) w(τ ) (cid:0)ˆv(zτ ; τ, y) (ϵ z)(cid:1) θ (cid:21) . (16) 13 Figure 11. Qualitative comparisons on single mesh animation. We compare our approach with several mesh animation methods. In the figure, A3D refers to Our method produces results that better align with the given prompts and exhibit more natural motion. Animate3D [30], AAM denotes AnimateAnyMesh [73], MD represents MotionDreamer [64], and TC corresponds to 4D reconstruction results from videos generated by TrajectoryCrafter [84]. C. More Experiment Results In this section, we present additional experimental results for our method. 14 Metric Object Animate3D AnimateAnyMesh MotionDreamer (Orig) MotionDreamer (Wan) Prompt Alignment Motion Realism Cat Dog Hugging Robot Sea Lion Brick Avg Cat Dog Hugging Robot Sea Lion Brick Avg 0 0 1 0 1 0 0.3333 0 1 0 0 1 1 0. 2 3 0 0 0 1 1 0 2 0 1 0 0 0.5 0 0 0 1 1 1 0.5 1 1 0 1 2 0 0.8333 1 1 0 1 2 0 0.8333 1 0 0 1 0 0 0. TC 0 7 1 2 43 4 9.5 2 11 3 1 37 8 10.3333 Ours Total 96 88 97 95 52 93 86.8333 95 84 96 95 59 90 86. 99 99 99 99 99 99 99 99 99 99 99 99 99 99 Table 2. Raw results of the user study on generating scene-level 4D motion. We show the number of vote from each participant on which option they consider the best under certain metric. Metric Object Animate3D AnimateAnyMesh MotionDreamer (Orig) MotionDreamer (Wan) TC Ours Total Prompt Alignment Motion Realism Chest Lamp Scissors Sitting Walking Avg (raw) Chest Lamp Scissors Sitting Walking Avg (raw) 2 0 1 4 1 1.6 2 5 0 5 2 2. 0 1 0 1 0 0.4 0 0 1 1 2 0.8 1 2 1 1 3 1.6 1 2 7 2 1 2.6 0 1 1 0 0 0.4 1 0 0 0 0 0. 0 0 0 5 1 1.2 1 0 1 6 0 1.6 47 46 47 39 45 44.8 45 43 41 36 45 42 50 50 50 50 50 50 50 50 50 50 50 Table 3. User study results for quantitative comparison on single-object 4D motion generation. C.1. Comparison on Single Mesh Animation We further compare our method with baselines on the task of single-object mesh animation. The set of baselines follows the main paper: Animate3D [30], AnimateAnyMesh [73], MotionDreamer [64], and 4D reconstruction from videos generated by TrajectoryCrafter [84]. We evaluate all methods on five prompts: The lid of chest is closing, lamp is lowering its head, The blades of pair of scissors cross together, tiger is sitting down, tiger is walking. Qualitative results are shown in Figure 11. Our method consistently achieves better prompt alignment and produces more natural motion than existing approaches. For quantitative evaluation, we conducted user study with 50 participants comparing our results against all baselines: 89.6% of participants rated our method highest in prompt alignment, and 84% rated it highest in motion realism. These results further indicate the strength of our approach relative to existing methods. The full results are provided in Table 3. C.2. Full Table for User Study In Table 2 and Table 3, we provide the complete user study results, including the number of participants who preferred each method for each scene. Across all scenes, our method receives the highest preference in both prompt alignment and motion realism. C.3. Failure Cases Our failure cases mainly arise from two factors: (1) limitations of the underlying video generative model, and (2) the inability to handle objects that do not exist in the static snapshot but appear later in the motion sequence. Examples are shown in Figure 12. Our failure cases mainly arise from two factors: (1) limitations of the underlying video generative model, and (2) the inability to handle objects that do not exist in the static snapshot but appear later in the motion sequence. Examples are 15 Figure 12. Failure Cases. The failure in the first row is due to limitations of the video generative model: it cannot produce motion that matches the prompt, as evidenced by its inability to sample videos aligned with the described action. The failure in the second row arises because our method cannot generate objects that were not present in the initial static scene. As result, no liquid can appear when prompted, since the system cannot generate newly emerging objects. shown in Figure 12. We elaborate on them below. Video Generative Model Limitation. Because our approach distills from pretrained video generation model, its capabilities are inherently linked to those of the underlying model. If the generator cannot synthesize videos aligning with the prompt, our 4D optimization receives misleading gradients. In such cases, our method cannot generate the correct motion. This is shown in the first row of Figure 12, where the video model repeatedly fails to sample videos consistent with the prompt, leading our method to produce incorrect motion. Inability to Handle Newly Appearing Objects. Another limitation of our method is that it cannot handle objects that do not exist in the initial static snapshot. Our 4D representation only deforms the geometry present at the start, so any object that should appear later in the sequence cannot be created. When the prompt involves new objects entering the scene, the supervision asks for motion that the system cannot produce. In these cases, the optimization either omits the requested effect or yields incomplete motion, as illustrated in the second row of Figure 12, where no liquid appears because the system cannot introduce new geometry. D. Limitation and Future Work Although our method can generate dynamic scenes with highly realistic interactions among multiple objects, there remain several limitations that point to promising directions for future work. For the failure cases described in Sec. C.3, those arising from limitations of the underlying video generative model may be alleviated as video generation technology continues to improve. For failures caused by newly appearing objects that are not present in the initial static scene, potential solution is to incorporate module capable of generating new geometry during the optimization process. Apart from the failure cases, another limitation of our method is its extensive training time. In our observations, substantial portion of the runtime is spent backpropagating through the VAE [33]. promising future direction is to develop distillation strategy that avoids backpropagating through the VAE entirely. This may be feasible because our objective is to generate motion rather than RGB appearance, suggesting that full VAE gradients may not be strictly necessary for effective motion supervision. 16 Figure 13. Screenshot of the user study question on Prompt Alignment. E. User Study Template We provide screenshots of the user study interface in Figure 13 and Figure 14. Participants were asked to select the best, second-best, and third-best results among five methods. From left to right and top to bottom, the corresponding methods 17 Figure 14. Screenshot of the user study question on Motion Realism. are: Animate3D [30], AnimateAnyMesh [73], MotionDreamer [64] using DynamiCrafter [76], MotionDreamer using Wan 2.2 [65], 4D reconstruction from videos generated by TrajectoryCrafter [84], and our method."
        }
    ],
    "affiliations": [
        "Stanford University",
        "University of Cambridge",
        "University of Maryland"
    ]
}