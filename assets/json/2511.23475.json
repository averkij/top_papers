{
    "paper_title": "AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement",
    "authors": [
        "Zhizhou Zhong",
        "Yicheng Ji",
        "Zhe Kong",
        "Yiying Liu",
        "Jiarui Wang",
        "Jiasun Feng",
        "Lupeng Liu",
        "Xiangyi Wang",
        "Yanjia Li",
        "Yuqing She",
        "Ying Qin",
        "Huan Li",
        "Shuiyang Mao",
        "Wei Liu",
        "Wenhan Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 5 7 4 3 2 . 1 1 5 2 : r AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement Zhizhou Zhong1,2 Yicheng Ji2,3 Zhe Kong1 Yiying Liu2 Jiarui Wang2 Jiasun Feng2 Lupeng Liu2,4 Xiangyi Wang2,4 Yanjia Li2 Yuqing She2,4 Ying Qin4 Huan Li3 Shuiyang Mao2 Wei Liu2 Wenhan Luo1 1Hong Kong University of Science and Technology 2Video Rebirth 3Zhejiang University 4Beijing Jiaotong University Homepage: https://hkust-c4g.github.io/AnyTalker-homepage Code: https://github.com/HKUST-C4G/AnyTalker Figure 1. We propose AnyTalker, powerful audio-driven framework for interactive multi-person video generation. It can generate natural videos that are rich in gestures, lively emotions, and interactivity, and can freely generalize to arbitrary IDs or even non-human cases."
        },
        {
            "title": "Abstract",
            "content": "Recently, multi-person video generation has started to gain prominence. While few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multiProject leader. Corresponding author. person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformers attention block with novel identity-aware attention mechanism that iteratively processes identityaudio pairs, allowing arbitrary scaling of drivable identities. Betraining multi-person generative models demands sides, massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multiperson speaking patterns and refines interactivity with only few real multi-person clips. Furthermore, we contribute targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking favorable balance between data costs and identity scalability. 1. Introduction In the era of digital media, video creation has emerged as crucial component of media platforms. Podcasts, livestreaming sales, and entertainment programs often feature rich multi-person interactions, resulting in growing demand for multi-person video generation. Despite the advent of large-scale video generation models [4, 28, 59, 70], which have furnished robust backbone architectures for audio-driven talking video generation methods [7, 11, 14, 15, 24, 26, 31, 35, 69], enabling the creation of realistic lip movements for single subjects, these models still struggle to accommodate the complexities of multi-person interactions. This limitation has prompted multi-person driving approaches that scale to arbitrary numbers of persons, handle multi-stream signals, and enable differentiated control over each subject. Despite recent advances [23, 31, 63] that propose solutions for handling multi-stream audio signals, these methods typically require hundreds to thousands of hours of meticulously curated multi-person data, leading to prohibitive collection costs and limited reproducibility. Specifically, the challenge of gathering training data for multi-person scenarios is heightened by their complex dynamics, including turn-taking, role-switching, and nonverbal cues like eye gaze, which complicate data annotation. Most existing audio-visual datasets [9, 11, 33, 41, 65, 80] focus on single-person monologues or isolated facial animations, thereby limiting their applicability for training the audio-driven multi-person generation model. In addition, current multi-person driving approaches frequently struggle to model genuine interactivity, leading to unnatural results. To address the aforementioned challenges, we introduce an innovative driving framework named AnyTalker. Built on the pre-trained video diffusion model [59], AnyTalker achieves impressive multi-person video driving with remarkably low data costs and distinct emphasis on interactivity, as illustrated in Fig. 1. The central idea is to leverage low-cost single-person data for scalable multi-person video driving and refine interactivity with just small amount of multi-person data (as little as 12 hours). Specifically, AnyTalker supports extending the number of drivable identities (IDs) to arbitrary numbers, with guaranteed interactivity among all IDs. To accommodate multi-stream control signals, we design an extensible audio-to-face in-context attention mechanism supporting any number of IDs and audio inputs. The training process is divided into two stages based on the types of data used, with the number of speaker IDs in individual data samples evolving from one to many. In the first stage, we randomly concatenate single-person talking videos along the horizontal dimension to simulate multiperson talking scenarios, ensuring that the model acquires baseline capacity for multi-person speaking patterns. In the second stage, we fine-tune the model using small amount of multi-person data to enhance its interactive capabilities. Commonly used single-person talking head benchmarks [65, 80, 81] lack multi-person interactions, rendering them unsuitable for assessing multi-person generation methods. While InterActHuman [63] offered related benchmark, it focuses on single speakers, limiting its utility for interaction analysis. To address this limitation, we introduce meticulously annotated benchmark featuring videos of two individuals engaging in both speech and eye contact, with fine-grained labels that mark the speaking and listening intervals, thus facilitating the assessment of interactions during listening states. Additionally, we firstly introduce novel metric to evaluate interactivity by measuring the activity of eye keypoints during listening periods. The proposed benchmark and metric will fill the gap in the assessment of interactivity in multi-person generation methods, benefiting future research in this area. The main contributions are summarized as follows: (1) We present an extensible multi-stream processing architecture for multi-person generation that can scale drivable identities arbitrarily. (2) novel two-stage training pipeline is introduced for the model to learn multi-speaker speaking patterns from single-person data and achieve seamless inter-identity interactions via multi-person data refinement. (3) We propose new metric that quantitatively evaluates multi-person interactivity for the first time, accompanied by tailored benchmark dataset for thorough assessment. (4) Comprehensive experiments demonstrate that AnyTalker achieves state-of-the-art performance and strikes favorable balance among identity scalability, interactivity, lip synchronization, and data cost. 2. Related Work 2.1. Audio-driven Talking Video Generation Recent key advancements [19, 20, 44, 50, 75] in text-toimage fields have significantly catalyzed progress in downstream applications. Early efforts like EMO [55] extend pre-trained diffusion text-to-image models [50] to endto-end audio-driven virtual-human video generation [6, 8, 25, 34, 43, 60, 66, 73]. They typically integrate modules for temporal attention [17], identity control via RefFigure 2. (a) The architecture of AnyTalker, which incorporates novel multi-stream audio processing layer, Audio-Face Cross Attention, enables the handling of multiple facial and audio inputs. (b) The training of AnyTalker is divided into two stages: the first stage uses concatenated multi-person data derived from single-person data mixed with single-person data to learn accurate lip movements; the second stage employs authentic multi-person data to enhance the interactivity in generated videos. (c) The detailed implementation of Audio-Face Cross Attention, recursively callable structure that applies masking to the output using face masks. erenceNet [22, 30, 74, 82], and audio-conditioning using pre-trained audio processing models [1, 47, 51], with conditional signals injected via cascaded attention layers [58]. Video foundation models [4, 28, 59, 70, 76] enable end-toend, audio-driven virtual-human models [11, 15, 24, 26, 35, 40, 56, 69]. These works largely employ audio-injection practices from earlier methods [25, 55] and report breakthroughs in long-video generation [11, 69], full-body synthesis [35], driving non-human cases [14, 15], synchronized lip movements [24], and coherent hand movements [40]. Consequently, they surpass earlier GAN-based generation methods [16, 67, 78, 79] and image diffusion-based techniques [44, 50] in terms of both quality and controllability. However, most methods still target single-person scenarios; in multi-person scenarios, they tend to synchronize identical motions or lip movements across all speakers, with restricted multi-person interaction. 2.2. Multi-person Video Generation Multi-person video generation has advanced rapidly through specialized architectures, including portrait video generation [42, 62], dancing video generation [68], and talking video generation [23, 31, 63]. Within the audiodriven talking-head axis, Bind-your-Avatar [23] introduces fine-grained Embedding Router that binds who with what they speak. InterActHuman [63] trains mask predictor to identify which body regions to activate, enabling targeted control. MultiTalk [31] proposes Label Rotary Position Embedding [52] to address audioperson binding. All three approaches above rely on expensive data, ranging from hundreds to thousands of hours of multi-person talking data. Some models [7, 39] trained on single-person data generalize to multi-person driving through specialized controllers: HunyuanVideo-Avatar [7] leverages FaceAware Audio Adapter to activate attention across different characters selectively, and Playmate2 [39] uses token-level masking within classifier-free guidance framework to realize similar binding. Despite enabling multi-person outputs, these approaches might yield fragmented interactions across distinct characters, with limited interactivity between individuals. In this work, AnyTalker explores the potential of learning multi-person speaking patterns from single-person data and designs an extensible multi-stream audio processing attention architecture, achieving favorable balance among interactivity, identity scalability, and lip synchronization in the generated videos with low training data cost. 3. Method The overall framework of the proposed AnyTalker is illustrated in Fig. 2. AnyTalker inherits certain architectural components from the Wan I2V model [59]. To handle multi-stream audio and identity input, we introduce trated in Eq. (4) and Fig. 2 (c), it enables flexible processing of diverse audio and identity inputs, with summed outputs from each iteration yielding the final attention output. Audio Token Modeling. We employ Wav2Vec2 [1] to encode audio features. The first latent frame attends to all audio tokens, whereas each subsequent latent frame focuses only on local temporal window corresponding to four audio tokens. This structured alignment between video and audio streams is achieved by applying Temporal Attention Mask Mtemporal, as explicitly shown in Fig. 3 (a). Furthermore, to enable comprehensive information integration, each audio token faudio utilized in the AFCA computation is concatenated with face token fface encoded by ECLIP. This concatenation allows all video query tokens Qvideo to attend to different pairs of audio and face information effectively, as computed below: Kaf = Concat(faudio, fface) WK, Vaf = Concat(faudio, fface) WV , (2) Attnout = MHCA(Qvideo, Kaf, Vaf, Mtemporal). Here, MHCA denotes Multi-Head Cross Attention, while WK and WV represent the key matrix and value matrix, respectively. The attention output Attnout will later be refined by the face mask token, as described in Eq. (3). Face Token Modeling. The facial image is obtained by online cropping the first frame of the selected video clip using InsightFace [13] during training, while the facial mask Mface is precomputed offline to cover the maximum extent of the face mask across the entire video, i.e., the global face bounding box. This mask ensures that facial movements will never exceed this region, preventing the mask from incorrectly activating video tokens after the reshape and flatten operations shown in Fig. 3 (b), especially for videos with significant facial displacements. This mask, which shares the same dimensions as Attnout, can be directly employed for element-wise multiplication to compute the Audio-Face Cross Attention output, as formulated below: Mtoken = Patchify(Flatten(Mface)), AFCAout = Mtoken Attnout. (3) Consequently, the hidden state Hi of each I2V DiT block can be formulated as i = Hi + AFCA(1) out + + AFCA(n) out , (4) where represents the layer index of the attention block, and denotes the number of IDs. Note that all AFCAout terms are produced by the same AFCA layer with shared parameters. The AFCA computation is applied times iteratively, once for each individual. This architecture enables the number of drivable IDs to scale arbitrarily. Figure 3. (a) Mapping of video tokens to audio tokens, facilitated by custom attention mask. Every 4 audio tokens are bound to 1 video token, except for the first. (b) Mask token used for output masking in Audio-Face Cross Attention. specialized multi-stream processing structure, termed as the Audio-Face Cross Attention (AFCA), which will be further described in Sec. 3.2. Our training pipeline is divided into two stages, which are summarized in Sec. 3.3. 3.1. Preliminaries As DiT-based model, AnyTalker tokenizes the 3D VAE features fvideo through patchifying and flattening, whereas the text features ftext are generated by the T5 encoder [48]. Additionally, AnyTalker incorporates Reference Attention Layer, cross-attention mechanism that leverages the CLIP image encoder [46] ECLIP to extract features fref from the first frame of the video. Wav2Vec2 [1] is also applied to extract the audio feature faudio. The overall input features finput can be written as finput = [fvideo, ftext, fref , faudio]. (1) Consistent with the Wan model, all attention layers are connected to the final output FFN layer (omitted in Fig. 2). 3.2. Audio-Face Cross Attention To enable multi-person talking, the model must be capable of handling multi-stream audio inputs. Potential solutions may include the L-RoPE technique used in MultiTalk [31], which assigns unique labels and biases to different audio features. However, the range of these labels needs to be explicitly defined, which limits its scalability. Considering this, we design more extensible structure to drive multiple IDs and enable accurate control in scalable manner. As depicted in Fig. 2 (a) and (c), we introduce specialized structure named Audio-Face Cross Attention (AFCA). The structure can iterate through loop multiple times, contingent upon the number of input face-audio pairs. As illusFigure 4. Two video clips from InteractiveEyes with otion score (px): left shows original video, right shows cropped face and eye landmarks. Head turn toward the speaker or eyebrow raise will increase otion and Interactivity; sustained stillness keeps both low. 3.3. Training Strategy AnyTalker explores the potential of single-person data for learning multi-person speaking patterns, with low-cost single-person data comprising the majority of training data. Single-Person Data Pretraining. We train the model using both standard single-person data and synthetic two-person data generated by horizontal concatenation. With an equal 50% probability, each batch of data is randomly configured to either two-person or single-person mode, as depicted in Fig. 2 (b). In two-person mode, each sample within the batch is horizontally concatenated with the data at the next index, along with its corresponding audio. This approach keeps the batch size identical across the two modes for every data batch. Additionally, we have predefined several generic text prompts that describe dual people speaking when data concatenation happens. Although the above data construction strategy enhances the models ability to localize audio-visual features within local regions and learn speaking patterns of dual speakers, completely omitting the single-person data is not feasible. Doing so would significantly degrade the models performance on generating accurate lip movements, leading to unstable driving results, which we later discuss in Tab. 4. Multi-person Data Refinement. In the next stage, we refine the model using small amount of authentic multiperson data to enhance the interactivity within different IDs. Although our training data contains only interactions between two identities, we surprisingly find that our model equipped with the AFCA module naturally generalizes to scenarios with more than two IDs, as shown in Fig. 1. We speculate that this is because the AFCA mechanism enables learning general patterns of human interaction, including not only accurate lip-syncing to the audio but also listening and responsive behaviors to other IDs speaking actions. To construct high-quality multi-person training data, we construct rigorous quality control pipeline, using InsightFace [12] to ensure two faces in most frames, audio diarization [45] to separate audio and ensure there is only one or two speakers, optical flow [27] to filter excessive motion, and Sync scores [10] to pair audio with faces. More details about this pipeline are in the supplementary material. This pipeline yields total of 12 hours of high-quality dualperson data, which is small amount compared to previous methods [31, 39, 63]. As the design of AnyTalkers AFCA Layer inherently supports multi-ID inputs, two-person data is fed into the model in the same format as the concatenated data in the first stage, with no extra processing required. To summarize, the single-person data training process enhances the models lip-syncing capability and generation quality, while also learning generalized multi-person speaking pattern. Subsequently, lightweight multi-person data refinement compensates for the real interactions that cannot be fully covered by single-person data. 4. Interactivity Evaluation Despite progress, the prevailing evaluation benchmarks [65, 80, 81] for single-person talking head generation are inadequate for assessing natural interactions among characters. Although InterActHuman [63] introduces comparable benchmark, its test set is limited to scenarios with only one speaker, which is not conducive to evaluating interactions among multiple characters. To fill this gap, we have fore, evaluating during listening periods is more targeted and valuable. The lengths of the listening and speaking periods of each person are described in Fig. 5, denoted as L1, L2, L3, L4, respectively. To quantify the responsiveness of the generated avatars, we compute the average motion intensity during the listening phases L2 and L3: Interactivity = L2 otionL2 + L3 otionL3 L2 + L3 . (6) This metric measures the interactivity effectively in the generated multi-character videos. As Fig. 4 suggests, the proposed metric aligns well with human perception: static or sluggish eye movements receive low otion scores, while head turns and eyebrow raises increase the score thus indicating higher interactivity. Moreover, to avoid misjudging abnormal eye movements, we implemented an exclusion algorithm detailed in the supplementary materials. 5. Experiments Dataset. We expand single-person datasets [9, 11, 65, 80, 81] with internet-collected data, yielding roughly 1,000 hours for first-stage training, and also gather two-person conversation clips for the second-stage training, retaining only about 12 hours after filtering. Evaluations are conducted on two types of benchmarks: (i) standard talkinghead benchmarks HDTF [80] and VFHQ [65], and (ii) our self-collected multi-person conversation dataset (head-andbody, both identities speak). We then select 20 videos from each benchmark, rigorously ensuring that their identities do not appear in the training set. Implement Details. To comprehensively evaluate our method, we train two models of different sizes: Wan2.11.3B-Inp and Wan2.1-I2V-14B [59], which serve as the foundational video diffusion models for our experiments. In all stages, the text [48], audio [1], and image [46] encoders, as well as the 3D VAE, remain frozen. The DiT main network, including the newly added AFCA layers, has all its parameters open for training. Stage 1 pretrains at 2 105 learning rate; stage 2 fine-tunes at 5 106. All models are optimized with AdamW [36] on 32 NVIDIA H200 GPUs. Evaluation Metrics. For the single-person benchmark, we employ several commonly used metrics: the Frechet Inception Distance (FID) [18] and the Frechet Video Distance (FVD) [57] to assess the quality of the generated data, SyncC [10] to measure the synchronization between audio and lip movements, and ID similarity [12] calculated between the first frame and the remaining frames. For the multi-person benchmark, we evaluate from different dimensions. The newly introduced metric, termed Interactivity, serves as the primary metric for assessment. For the FVD metric, the calculation is similar to that in the single-person benchmark. For the Sync-C metric, we refine Figure 5. Listening and speaking periods of each speaker. curated collection of videos featuring two distinct identities, sourced from the web for evaluation purposes. 4.1. Dataset Construction We select interactive two-person videos to construct the video dataset, named InteractiveEyes. Two clips of these videos are illustrated in Fig. 4. Each video is approximately 10 seconds in duration and showcases exactly two faces throughout the entire segment. Furthermore, through meticulous manual process, we segment the audio of each video to ensure that the majority of the videos capture scenes of both individuals engaging in speaking and listening, as well as variety of rich eye interaction scenarios, as shown in Fig. 5. We have also ensured that each video includes instances of mutual gaze and head movements to provide authentic references. 4.2. Proposed Interactivity Metric In addition to this dataset, we introduce novel metric, the eye-focused Interactivity, designed to assess the natural interaction between speakers and listeners. Since eye interaction is fundamental and spontaneous behavior in conversational contexts, we use it as key indicator of interactivity. Drawing inspiration from the Hand Keypoint Variance (HKV) metric employed in CyberHost [34], we propose quantitative evaluation of the interaction by tracking the motion amplitude of eye keypoints. To achieve this, we define otion on the sequence of face-aligned eye keypoints extracted from the generated frames, where denotes the frame sequence and the eye keypoints. The otion is calculated as follows otion = 1 1 S1 (cid:88) j=1 1 E (cid:88) i=1 Ei,j+1 Ei,j . (5) Here, and denote the eye keypoint index and the frame index, while Ei,j denotes eye keypoints present in each frame. This formula intuitively computes the displacement and rotation of the eye region. We then calculate the motion during listening periods. The reason is, most generation methods perform well when activating the speaking subject, but the listening subject often appears rigid. ThereTable 1. Quantitative comparison with other competing methods on HDTF [80] and VFHQ [65] benchmark. Here, OmniHuman-1.5 [26] refers to its Master Mode version accessed via the JiMeng platform [5], which currently does not support multi-person generation. Method AniPortrait [64] FantasyTalking [61] StableAvatar [56] EchoMimic [8] Hallo3 [11] Sonic [24] OmniHuman-1.5 [26] MuitiTalk [31] AnyTalker-1.3B AnyTalker-14B Supported Generation Scope HDTF VFHQ multi-person body Sync-C FID FVD ID Sync-C FID FVD 3.44 3.97 4.11 5.23 7.53 7.81 7.23 8. 6.85 9.05 18.74 14.93 14.67 61.53 17.12 52.96 35.26 13.54 14.47 13.84 241.84 166.79 166.44 381.55 195.61 286.12 173.23 162.58 218.01 160.87 0.94 0.93 0.91 0.95 0.91 0.95 0.90 0. 0.91 0.94 2.63 3.57 3.53 4.87 6.32 7.71 7.67 7.77 5.81 7.79 28.54 24.83 22.91 58.72 41.26 36.68 35.36 24.25 21.88 20.99 269.24 272.13 275.73 486.75 371.24 385.37 283.39 243. 267.08 290.73 ID 0.95 0.94 0.88 0.86 0.91 0.89 0.90 0.94 0.91 0.94 Table 2. Quantitative comparison with other competing methods on the multi-person benchmark, InteractiveEyes. Method Interactivity Sync-C FVD Ground Truth Bind-Your-Avatar [23] MuitiTalk [31] AnyTalker-1.3B AnyTalker-14B 0. 0.45 0.49 0.97 1.01 6.01 3.03 6.88 4.56 6.99 0 695.58 500.03 467.84 424.15 its calculation as Sync-C to focus only on the lip synchronization during each characters speaking periods, thereby avoiding the influence of long listening segments on the final lip synchronization score, specifically, Sync-C = L1 Sync-CL1 + L4 Sync-CL4 L1 + L4 . (7) Here, L1 and L4 denotes the speaking phases depicted in Fig. 5. Comparsion Methods. We compare AnyTalker with several state-of-the-art talking video generation methods. For single-person generation, we compare with AniPortrait [64], EchoMimic [8], Hallo3 [11], Sonic [24], FantasyTalking [61], StableAvatar [56], OmniHuman-1.5 [26], and MultiTalk [31]. For multi-person generation, we choose Bind-Your-Avatar [23] and MultiTalk [31] for quantitative and qualitative comparison. 5.1. Comparison with SOTA methods Quantitative Comparison. To begin with, we compare AnyTalker with several single-person generation methods to verify its excellent single-person driving capability. The quantitative results are shown in Tab. 1. Despite not being specifically designed for driving talking faces, AnyTalker achieves the best or competitive results across all metrics. Moreover, the 1.3B model of AnyTalker significantly outperforms AniPortrait [64], EchoMimic [8], and StableAvatar [56] in terms of lip synchronization, even though they have similar number of parameters. These results demonstrate the excellent and comprehensive driving capabilities of the AnyTalker framework. Subsequently, we evaluate AnyTalkers ability to drive multiple IDs while maintaining both accurate lip synchronization and natural interactivity using the multi-person dataset, InteractiveEyes, described in Sec. 5, along with relevant metrics. In this comparison, we contrast AnyTalker with the available open-source multi-person driving methods, MultiTalk [31] and Bind-Your-Avatar [23]. The results depicted in Tab. 2 demonstrate that both the 1.3B and 14B models of AnyTalker achieve the best performance in terms of the Interactivity metric. Additionally, the 14B model achieves the best results across all metrics, thereby validating the effectiveness of our proposed training pipeline. We further illustrate AnyTalkers capability to generate videos rich in interactivity through quantitative evaluation. Qualitative Comparsion. We then select an authentic human input from the InteractiveEyes dataset and use an input generated by an AIGC mode [54], both accompanied by corresponding text prompts and dual audio streams, to conduct quantitative evaluation comparison using BindYour-Avatar [23], MultiTalk [31], and AnyTalker. As shown in Fig. 6, AnyTalker generates more natural videos with eye and head interactions compared to the other methods. MultiTalk exhibits weaker eye interaction, while Bind-YourAvatar tends to produce more static expressions. This trend further validates the effectiveness of the Interactivity metric proposed in Sec. 5. AnyTalker not only generates natural, two-person interactive speaking scenarios but also scales well to multiple IDs, as demonstrated in Fig. 1, where it effectively handles interactions among four IDs. The qualitative results of the single-person benchmarks [65, 80] will Figure 6. Qualitative comparison of multiple multi-person driving methods. With the same text prompt, reference images, and multiple audio streams as input, we compare the generation results of Bind-Your-Avatar, MultiTalk, and AnyTalker. The left case uses the input image from the InteractiveEyes dataset, while the right case uses the image produced by text-to-image generative model [54]. Table 3. Ablation study about AnyTalkers components on the HDTF dataset using the 1.3B model. Baseline denotes the model equipped solely with the basic audio attention layers. AFCA indicates the inclusion of the Audio-Face Cross Attention mechanism. Single indicates that authentic multi-person data is not utilized in this stage. Table 4. Ablation studies conducted on the InteractiveEyes dataset using the 1.3B model. RS denotes the use of authentic singleperson data in the first stage. CM indicates concatenated multiperson data in the first stage. RM represents authentic multiperson data in the second stage. Setting Metrics Sync-C FID FVD Baseline w/o AFCA w/o Mask Token w/o Concatenated Data AnyTalker-1.3B (Single) 5.42 6.71 5.84 6.21 6.97 13.01 14.97 14.81 14.73 15.58 170.96 207.47 193.78 202.01 166.27 ID 0.90 0.88 0.89 0.91 0.91 Setting Metrics RS CM RM Interactivity Sync-C FVD 0.55 0.47 0.58 0.71 0. 3.21 4.13 4.89 3.63 4.56 672.18 475.31 393.86 511.51 467.84 be included in the supplementary material. 5.2. Ablation Study Components. We conduct ablation studies on the three important components mentioned in Fig. 2, including AudioFace Cross Attention, Mask Token for attention output, and concatenated multi-person data. Starting from the full 1.3B model in the first stage, which utilizes only single-person data, we progressively remove the relevant components to evaluate their impact on lip synchronization, generation quality, and identity similarity in the generated videos. The results in Tab. 3 show that every component plays significant role, with concatenated multi-person data most beneficial to lip-movement accuracy. Although the completed model in the initial phase exhibits marginally higher FID score compared to the Baseline model, we attribute this to the Baseline models propensity for generating talking videos with minimal facial expressions or head movements. Conversely, the completed model generates more dynamic actions, which inherently influence the FID calculation to some degree. We regard this trade-off as natural consequence of the models design. Furthermore, the completed model demonstrates superior performance over the Baseline model across other evaluation metrics. Crucially, the firststage model already has the basic capability to drive multiple individuals, feature that the Baseline model lacks. Multi-Person Data. Subsequently, we focus on multiperson data and conduct more targeted ablation experiments on the InteractiveEyes benchmark. In the first stage, we create concatenated multi-person data using single-person data. As shown in Tab. 4, models utilizing this concatenated data outperform those without it across all evaluation dimensions, especially in lip synchronization and interactivity, highlighting the role of concatenated data in learning multi-person speaking patterns. It is worth noting that the results in the first row of the table also demonstrate the necessity of mixing single-person data in the first stage; otherwise, the generated results would be unstable. After fine-tuning with authentic multi-person data, models that use concatenated data in the first stage demonstrate even more superior performance, further proving their early adaptation to multi-person speaking patterns. Although fine-tuning with authentic multi-person data leads to slight decrease in lip synchronization, it significantly improves interactivity, which we consider reasonable trade-off. 6. Conclusion In this paper, we introduce AnyTalker, an audio-driven framework for generating multi-person talking videos. It presents an extensible multi-stream processing structure called Audio-Face Cross Attention that enables identity scaling while guaranteeing seamless cross-identity interactions. We further propose generalizable training strategy that maximally leverages single-person data through learning multiconcatenation-based augmentation for person speaking patterns. Additionally, we propose the first interactivity evaluation metric and tailored benchmark for comprehensive assessment. Extensive experiments suggest that AnyTalker balances lip synchronization, identity scalability, and interactivity in multi-person scenarios."
        },
        {
            "title": "References",
            "content": "[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised learning of speech representations. Advances in Neural Information Processing Systems, 33:1244912460, 2020. 3, 4, 6, 1 [2] Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea Tagliasacchi, David Lindell, and Sergey Tulyakov. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2287522889, 2025. 6 [3] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025. 6 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3 [5] ByteDance. Jimeng platform. https : / / jimeng . jianying . com / ai - tool / home ? type = digitalHuman, 2025. [6] Ming Chen, Liyuan Cui, Wenyuan Zhang, Haoxian Zhang, Yan Zhou, Xiaohan Li, Songlin Tang, Jiwen Liu, Borui Liao, Hejia Chen, et al. Midas: Multimodal interactive digitalhuman synthesis via real-time autoregressive video generation. arXiv preprint arXiv:2508.19320, 2025. 2 [7] Yi Chen, Sen Liang, Zixiang Zhou, Ziyao Huang, Yifeng Ma, Junshu Tang, Qin Lin, Yuan Zhou, and Qinglin Lu. Hunyuanvideo-avatar: High-fidelity audio-driven huarXiv preprint man animation for multiple characters. arXiv:2505.20156, 2025. 2, 3 [8] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and Chenguang Ma. Echomimic: Lifelike audio-driven portrait In Proanimations through editable landmark conditions. ceedings of the AAAI Conference on Artificial Intelligence, pages 24032410, 2025. 2, 7, 4 [9] Chung, Nagrani, and Zisserman. Voxceleb2: Deep speaker recognition. Interspeech 2018, 2018. 2, 6 [10] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Asian Conference on Computer Vision, pages 251263. Springer, 2016. 5, [11] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with video diffusion transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2108621095, 2025. 2, 3, 6, 7 [12] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep In Proceedings of the IEEE/CVF Conface recognition. ference on Computer Vision and Pattern Recognition, pages 46904699, 2019. 5, 6, 1 [13] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. Retinaface: Single-shot multiIn Proceedings of the level face localisation in the wild. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52035212, 2020. 4, 1, 2 [14] Qijun Gan, Ruizi Yang, Jianke Zhu, Shaofei Xue, and Steven Hoi. Omniavatar: Efficient audio-driven avatar video generation with adaptive body animation. arXiv preprint arXiv:2506.18866, 2025. 2, 3 [15] Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, et al. Wan-s2v: Audio-driven cinematic video generation. arXiv preprint arXiv:2508.18621, 2025. 2, 3 [16] Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. 3 [17] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textIn The to-image diffusion models without specific tuning. Twelfth International Conference on Learning Representations, 2024. [18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in Neural Information Processing Systems, 30, 2017. 6 [19] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 2, 3 [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. 2 [21] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 6 [22] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 3 [23] Yubo Huang, Weiqiang Wang, Sirui Zhao, Tong Xu, Lin Liu, and Enhong Chen. Bind-your-avatar: Multi-talkingcharacter video generation with dynamic 3d-mask-based embedding router. arXiv preprint arXiv:2506.19833, 2025. 2, 3, 7, [24] Xiaozhong Ji, Xiaobin Hu, Zhihong Xu, Junwei Zhu, Chuming Lin, Qingdong He, Jiangning Zhang, Donghao Luo, Yi Chen, Qin Lin, et al. Sonic: Shifting focus to global audio perception in portrait animation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 193203, 2025. 2, 3, 7, 4 [25] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, and Yanbo Zheng. Loopy: Taming audio-driven In The portrait avatar with long-term motion dependency. Thirteenth International Conference on Learning Representations, 2025. 2, 3 [26] Jianwen Jiang, Weihong Zeng, Zerong Zheng, Jiaqi Yang, Chao Liang, Wang Liao, Han Liang, Yuan Zhang, and Mingyuan Gao. Instilling an active Omnihuman-1.5: mind in avatars via cognitive simulation. arXiv preprint arXiv:2508.19209, 2025. 2, 3, 7, 6 [27] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In European Conference on Computer Vision, pages 1835. Springer, 2024. 5, 1 [28] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, 3 [29] Xianghao Kong, Hansheng Chen, Yuwei Guo, Lvmin Zhang, Gordon Wetzstein, Maneesh Agrawala, and Anyi Rao. Taming flow-based i2v models for creative video editing. arXiv preprint arXiv:2509.21917, 2025. 6 [30] Xianghao Kong, Qiaosong Qi, Yuanbin Wang, Anyi Rao, Biaolong Chen, Aixi Zhang, Si Liu, and Hao Jiang. Profashion: Prototype-guided fashion video generation with multiple reference images. arXiv preprint arXiv:2505.06537, 2025. [31] Zhe Kong, Feng Gao, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Xunliang Cai, Guanying Chen, and Wenhan Luo. Let them talk: Audio-driven multi-person conversational video generation. arXiv preprint arXiv:2505.22647, 2025. 2, 3, 4, 5, 7, 6 wei Xing. Latentsync: Taming audio-conditioned latent diffusion models for lip sync with syncnet supervision. arXiv preprint arXiv:2412.09262, 2024. 1, 2 [33] Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, et al. Openhumanvid: large-scale high-quality dataset for enhancing human-centric video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 77527762, 2025. 2 [34] Gaojie Lin, Jianwen Jiang, Chao Liang, Tianyun Zhong, Jiaqi Yang, Zerong Zheng, and Yanbo Zheng. Cyberhost: one-stage diffusion framework for audio-driven talking body generation. In The Thirteenth International Conference on Learning Representations, 2025. 2, 6 [35] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, Chao Liang, Yuan Zhang, and Jingtuo Liu. Omnihuman-1: Rethinking the scaling-up of one-stage conditioned human animation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1384713858, 2025. 2, 3 [36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6, 3 [37] Yanzuo Lu, Manlin Zhang, Andy Ma, Xiaohua Xie, and Jianhuang Lai. Coarse-to-fine latent diffusion for poseIn Proceedings of the guided person image synthesis. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64206429, 2024. [38] Yanzuo Lu, Yuxi Ren, Xin Xia, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Andy Ma, Xiaohua Xie, and Jian-Huang Lai. Adversarial distribution matching for diffusion distillation towards efficient image and video synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1681816829, 2025. 6 [39] Xingpei Ma, Shenneng Huang, Jiaran Cai, Yuansheng Guan, Shen Zheng, Hanfeng Zhao, Qiang Zhang, and Shunsi Zhang. Playmate2: Training-free multi-character audiodriven animation via diffusion transformer with reward feedback. arXiv preprint arXiv:2510.12089, 2025. 3, 5 [40] Rang Meng, Yan Wang, Weipeng Wu, Ruobing Zheng, Yuming Li, and Chenguang Ma. Echomimicv3: 1.3 parameters are all you need for unified multi-modal and multi-task human animation. arXiv preprint arXiv:2507.03905, 2025. 3 [41] Rang Meng, Xingyu Zhang, Yuming Li, and Chenguang Ma. Echomimicv2: Towards striking, simplified, and semibody human animation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 54895498, 2025. 2, 3, 4 [42] Xiangyu Meng, Zixian Zhang, Zhenghao Zhang, Junchao Liao, Long Qin, and Weizhi Wang. Identity-grpo: Optimizing multi-human identity-preserving video generation via reinforcement learning. arXiv preprint arXiv:2510.14256, 2025. 3 [43] Fatemeh Nazarieh, Zhenhua Feng, Diptesh Kanojia, Muhammad Awais, and Josef Kittler. Portraittalk: Towards customizable one-shot audio-to-talking face generation. arXiv preprint arXiv:2412.07754, 2024. [32] Chunyu Li, Chao Zhang, Weikai Xu, Jingyu Lin, Jinghui Xie, Weiguo Feng, Bingyue Peng, Cunjian Chen, and Wei- [44] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2, 3 [45] Alexis Plaquet and Herve Bredin. Powerset multi-class cross entropy loss for neural speaker diarization. In Proc. INTERSPEECH 2023, 2023. 5, 1, 2 [46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 4, 6, 3 [47] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech In Internarecognition via large-scale weak supervision. tional conference on machine learning, pages 2849228518. PMLR, 2023. 3, [48] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 4, 6, 1, 3 [49] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. Advances in Neural Information Processing Systems, 37:117340117362, 2024. 6 [50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1068410695, 2022. 2, 3 [51] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised pre-training for speech recognition. arXiv preprint arXiv:1904.05862, 2019. 3, 1, 2 [52] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 3 [53] Naoya Takahashi and Yuki Mitsufuji. Multi-scale multiband densenets for audio source separation. In 2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), pages 2125. IEEE, 2017. [54] Kolors Team. Kolors: Effective training of diffusion model for photorealistic text-to-image synthesis. arXiv preprint, 2024. 7, 8 [55] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions. In European Conference on Computer Vision, pages 244260. Springer, 2024. 2, 3 [56] Shuyuan Tu, Yueming Pan, Yinming Huang, Xintong Han, Zhen Xing, Qi Dai, Chong Luo, Zuxuan Wu, and Yu-Gang Jiang. Stableavatar: Infinite-length audio-driven avatar video generation. arXiv preprint arXiv:2508.08248, 2025. 3, 7, 4 [57] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. 2019. 6 [58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017. 3 [59] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3, 6 [60] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng Luo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei Yang. V-express: Conditional dropout for progressive training of portrait video generation. arXiv preprint arXiv:2406.02511, 2024. [61] Mengchao Wang, Qiang Wang, Fan Jiang, Yaqi Fan, Yunpeng Zhang, Yonggang Qi, Kun Zhao, and Mu Xu. Fantasytalking: Realistic talking portrait generation via coherent motion synthesis. Proceedings of the 33th ACM International Conference on Multimedia, 2025. 7, 3, 4 [62] Qiang Wang, Mengchao Wang, Fan Jiang, Yaqi Fan, Yonggang Qi, and Mu Xu. Fantasyportrait: Enhancing multicharacter portrait animation with expression-augmented diffusion transformers. arXiv preprint arXiv:2507.12956, 2025. 3 [63] Zhenzhi Wang, Jiaqi Yang, Jianwen Jiang, Chao Liang, Gaojie Lin, Zerong Zheng, Ceyuan Yang, and Dahua Lin. Interacthuman: Multi-concept human animation with layoutaligned audio conditions. arXiv preprint arXiv:2506.09984, 2025. 2, 3, 5 [64] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animation. arXiv preprint arXiv:2403.17694, 2024. 7 [65] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and Ying Shan. Vfhq: high-quality dataset and benchIn Proceedings of mark for video face super-resolution. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 657666, 2022. 2, 5, 6, 7, 4 [66] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei Zhang, Ce Liu, Jingdong Wang, Yao Yao, and Siyu Zhu. Hallo: Hierarchical audio-driven visual synthesis for portrait image animation. arXiv preprint arXiv:2406.08801, 2024. 2 [67] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. Vasa-1: Lifelike audio-driven talking faces generated in real time. Advances in Neural Information Processing Systems, 37:660684, 2024. [68] Jingyun Xue, Hongfa Wang, Qi Tian, Yue Ma, Andong Wang, Zhiyuan Zhao, Shaobo Min, Wenzhe Zhao, Kaihao Zhang, Heung-Yeung Shum, et al. Towards multiple character image animation through enhancing implicit decoupling. In The Thirteenth International Conference on Learning Representations, 2025. 3 [69] Shaoshu Yang, Zhe Kong, Feng Gao, Meng Cheng, Xiangyu Liu, Yong Zhang, Zhuoliang Kang, Wenhan Luo, Xunliang Cai, Ran He, et al. Infinitetalk: Audio-driven video generation for sparse-frame video dubbing. arXiv preprint arXiv:2508.14033, 2025. 2, 3 In Proceedings of high-resolution audio-visual dataset. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36613670, 2021. 2, 5, 6, 7, 4 [81] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. Celebvhq: large-scale video facial attributes dataset. In European Conference on Computer Vision, pages 650667. Springer, 2022. 2, 5, 6 [82] Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William Chan, Chitwan Saharia, Mohammad Norouzi, and Ira Kemelmacher-Shlizerman. Tryondiffusion: tale of two unets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 46064615, 2023. 3 [70] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, 2025. 2, [71] Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, and Wenhan Luo. Unic: Unified in-context video editing. arXiv preprint arXiv:2506.04216, 2025. 6 [72] Zixuan Ye, Huijuan Huang, Xintao Wang, Pengfei Wan, Di Zhang, and Wenhan Luo. Stylemaster: Stylize your video In Proceedings of with artistic generation and translation. the Computer Vision and Pattern Recognition Conference, pages 26302640, 2025. 6 [73] Phyo Thet Yee, Dimitrios Kollias, Sudeepta Mishra, and Abhinav Dhall. Synchrorama: Lip-synchronized and emotionaware talking face generation via multi-modal emotion embedding. arXiv preprint arXiv:2509.19965, 2025. 2 [74] Delong Zhang, Qiwei Huang, Yang Sun, Yuanliu Liu, WeiShi Zheng, Pengfei Xiong, and Wei Zhang. Learning implicit features with flow-infused transformations for realistic virtual try-on. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1873618745, 2025. 3 [75] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 2 [76] Ruihan Zhang, Borou Yu, Jiajian Min, Yetong Xin, Zheng Wei, Juncheng Nemo Shi, Mingzhen Huang, Xianghao Kong, Nix Liu Xin, Shanshan Jiang, Praagya Bahuguna, Mark Chan, Khushi Hora, Lijian Yang, Yongqi Liang, Runhe Bian, Yunlei Liu, Isabela Campillo Valencia, Patricia Morales Tredinick, Ilia Kozlov, Sijia Jiang, Peiwen Huang, Na Chen, Xuanxuan Liu, and Anyi Rao. Generative ai for film creation: survey of recent advances. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 62666278, 2025. [77] Ruicheng Zhang, Jun Zhou, Zunnan Xu, Zihao Liu, Jiehui Huang, Mingyang Zhang, Yu Sun, and Xiu Li. Zero-shot 3daware trajectory-guided image-to-video generation via testtime training. arXiv preprint arXiv:2509.06723, 2025. 6 [78] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learning realistic 3d motion coefficients for stylized audiodriven single image talking face animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86528661, 2023. 3 [79] Yue Zhang, Zhizhou Zhong, Minhao Liu, Zhaokang Chen, Bin Wu, Yubin Zeng, Chao Zhan, Yingjie He, Junxin Huang, and Wenjiang Zhou. Musetalk: Real-time high-fidelity video dubbing via spatio-temporal sampling. arXiv preprint arXiv:2410.10122, 2024. 3 [80] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 7. SyncNet Score Matrix in two-person data."
        },
        {
            "title": "Outline",
            "content": "Figure 8. Properties of training data. This supplementary material provides two sets of additional information on AnyTalker. A. Experimental Details Data-processing pipeline used during training Construction scheme for inference data Training hyper-parameters and other technical details Inference settings B. Extended Experiments More analysis of the Interactivity metric Additional experimental results Effectiveness of interactivity refinement A. Experimental Details A.1. Training Data Processing Auxiliary Models. We first introduce the auxiliary models used during the data processing stage. To segment sentences and chunk the audio track extracted from each video, we employ the pre-trained speaker-diarizationin conjunction with OpenAIs Whis3.1 model [45] per [47]. For vocal separation and vocal feature extraction, we use pre-trained models from Kim Vocal 2 [53] and Wav2Vec2 [1, 51]. For face detection and facial feature processing, we adopt RetinaFace [13] and ArcFace [12], both accessible via InsightFace. To measure audio-visual synchronization, we employ the pre-trained SyncNet model from LatentSync [32]. Text prompts are obtained with Gemini 2.5 Pro, and their features are extracted using the T5-encoder [48]. Finally, we filter videos with excessive camera motion by means of the CoTracker model [27]. Single-Person Data. valid single-person clip is 24-fps video that continuously exhibits one identifiable face and is accompanied by speech audio whose lip movements are perfectly synchronised with the soundtrack. We first eliminate hand-held recordings exhibiting pronounced camera shake or abrupt scene changes using CoTracker [27]. RetinaFace [13] is then applied to guarantee that most frame contains exactly one face. Videos that satisfy the above criteria are processed by speaker-diarization-3.1 [45] to obtain coarse sentence-level segments. Utterances longer than 5 are further subdivided into semantically coherent units with Whisper [47], after which any fragment containing overlapping speakers is discarded. The remaining clean single-speaker segments average 2 in duration. Because 2 clips cannot saturate GPU memory, we randomly concatenate consecutive segments to extend each sequence; the resulting clip lengths follow Gaussian distribution with mean 4 and standard deviation 0.5 s. Then, their vocal tracks are extracted with Kim Vocal 2 [53] and enhttps : / / huggingface . co / pyannote / speaker - diarization-3. https://github.com/deepinsight/insightface https : / / huggingface . co / ByteDance / LatentSync / https://github.com/openai/whisper https github com / / : . ultimatevocalremovergui / Anjok07 / https://deepmind.google/models/gemini/pro/ https://huggingface.co/Wan-AI/Wan2.1-I2V-14Btree/main https://huggingface.co/facebook/wav2vec2-base720P/tree/main/google/umt5-xxl 960h https://huggingface.co/facebook/cotracker Figure 9. Two cases from InteractiveEyes. Both of the speakers have speaking periods. Figure 10. Input cases from the benchmark dataset, from left to right: HDTF [80], VFHQ [65], and EMTD [41]. coded with Wav2Vec-2 [51], while audio-visual synchrony is scored by SyncNet [32]. Clips whose synchrony score lies below pre-defined threshold are removed from the training set. Finally, every retained clip is transcribed and annotated with frame-level face bounding boxes. We then use Gemini 2.5 Pro to generate textual descriptions for these clips and employ T5 encoder to extract text features. All input text is uniformly truncated or padded to exactly 512 tokens. After being preceded by the above pipeline, it yields approximately 1, 000 of high-quality 480P single-person data. Two-Person Data. Processing Two-person clips follows the single-person pipeline with three key modifications. 1. Face count: the video must contain exactly two faces in most frame. 2. Speaker activity: speaker-diarization-3.1 [45] is constrained to output only two valid states: (i) both speakers active or (ii) single speaker active. 3. Spatial consistency: the leftright spatial ordering of the two faces must remain unchanged throughout the entire clip; identity swapping is detected and rejected via InsightFace [13]. To establish the correct voiceface correspondence, we compute 2 2 SyncNet confidence matrix  (Fig. 7)  and requires the two largest scores to lie on the diagonal; otherwise, the clip is discarded. minimum synchrony threshold identical to the single-person setting is further applied. After filtering, approximately 12 of clean two-person data are retained. Data Concatenation. During the first-stage training, we horizontally concatenate randomly selected single-person clips. Because the original videos are extremely highresolution (many 2K/4K), naively resizing them along the height axis and stacking would yield faces that occupy only tiny fraction of the frame. To avoid this, we adopt special cropping strategy. First, we locate the face centre in each of the two candidate clips. For 480P footage, the frame size is (H, W)=(480,832), so we expand minimal crop of (480,416) around each face centre. We then apply further augmentation: while keeping the crop inside the original image, we randomly enlarge the window while preserving the 480/416 aspect ratio. The resulting crops contain much larger facial region, enabling the model to learn more accurate lip-to-audio mapping. Data Properties. Each processed sample is summarised by dictionary-style item that stores the absolute paths to the decoded video, cleaned audio, and pre-extracted features. During training, the data loader indexes all information through this item. Single-person and two-person clips share an identical key structure; the number of facespeech entries in the list unambiguously indicates whether the sample contains one or two speakers. Further details are illustrated in Fig. 8. A.2. Benchmark Data Processing Single-Person Data. We adopt HDTF [80] and VFHQ [65] as the single-speaker evaluation benchmarks. Both datasets are de facto standards for talking-head generation; their images are tightly cropped around the face region. Additional experiments on half-body sequences that include hands are reported in Sec. B.2. For each test video, we supply (i) single reference image of the subject and (ii) the corresponding speech segment. To respect GPU-memory constraints, we fix the audio length to 6 sa duration that all competing methods process without overflow. Twenty clips are randomly selected from each dataset, ensuring that none of the identities appear in the AnyTalker training set. Reference images from the three evaluation splits are visualised in Fig. 10. Two-Person Data. We provide additional details for InteractiveEyes, the test set introduced in Section 4 in the main text for two-speaker scenarios. Each video is shot with stable camera and contains exactly two faces in every frame; the duration is approximately 10 s. In 80 % of the cases, both speakers produce speech, while in the remaining 20 % only one speaker talks and the other maintains listening pose. To avoid segmentation errors, we do not apply speaker-diarization-3.1 [45]; instead, speaking intervals Figure 11. Four typical cases that will get high Interactivity score. Table 5. Quantitative result on EMTD [41] benchmark. Method Metrics Sync-C FID FVD FantasyTalking [61] EchoMimic v2 [41] MultiTalk [31] AnyTalker-1.3B AnyTalker-14B 3.76 6.27 8.38 5.83 8.45 67.66 63.43 64.71 56.01 50. 818.71 671.18 787.99 789.59 664.58 ID 0.76 0.76 0.79 0.74 0.77 are manually annotated so that the Interactivity metric proposed in the main paper can be computed accurately. Two representative cases are shown in Fig. 9. A.3. Implement Details Training Details. To comprehensively evaluate our method, we train two models of different sizes: Wan2.11.3B-Inp and Wan2.1-I2V-14B [59], which served as the foundational video diffusion models for our experiments. In all stages, the text [48], audio [1], and image [46] encoders, as well as the 3D VAE, remained frozen with their parameters unchanged. The DiT main network, including the newly added AFCA Layers, had all its parameters open for training. The first stage employs higher learning rate of 2 105 for pretraining, while the second stage uses lower learning rate of 5 106 for fine-tuning, incorporating warm-up strategy and optimized using the AdamW optimizer [36]. The 14B model is trained using 32 NVIDIA H200 GPUs. In the first stage, the global batch size is set to 32 and the model is trained for 2.4M steps. In the second stage, the batch size is adjusted to 16, and the model is trained for an additional 50K steps. The 1.3B model is trained using 8 NVIDIA H200 GPUs. The global batch size https://huggingface.co/alibaba-pai/Wan2.1-Fun1.3B-InP is maintained at 48 throughout the training process, with the total number of training steps being consistent with that of the 14B model. Inference Details. For all competing methods, we strictly follow the publicly released implementations and adopt their default recommended inference hyperparameters. Approaches that require textual input receive fixed prompt for the single-person benchmark: this person is talking. For the multi-person benchmark, the prompts are automatically generated by Gemini 2.5 Pro as described in Sec. A.1. AnyTalker performs inference with classifierfree guidance (CFG) [19] using guidance scale of 4.0; in the unconditional branch, both textual and audio features are set to zero. Face masks are extracted with InsightFace and uniformly dilated to provide slightly enlarged region for generation. B. Extended Experiments B.1. More analysis about Interactivity Metric Good Case. Fig. 11 visualises four representative listener behaviours that strongly signal conversational engagement: eyebrow raise, head nod, head turn, and gaze shift. The presence of such cues contributes substantially to the final Interactivity score. Because Interactivity is computed over the entire video, we do not report frame-level values in Fig. 11. As illustrated in Fig. 14, AnyTalker produces sequences rich in these interactive actions, and thus achieves comparatively high Interactivity rating. The Rubustness of Interactive Metric. Some generative baselines occasionally produce highly implausible motions. For example, Bind-Your-Avatar [23] generates an exaggerated lying-down action shown in Fig. 12. Without counter-measures, such artifacts would dramatically inflate the otion score even though they are unrelated to interactivity. We therefore introduce lightweight anomalysuppression rule: if the mean facial-landmark displacement Figure 12. bad case generated by Bind-Your-Avatar [23]. Figure 13. Qualitative results on HDTF [80] (left) and VFHQ [65] (right) benchmark. The positions of pronunciation have been highlighted in red and underlined. between two consecutive frames exceeds 10 pixels (all faces are pre-aligned to 256 256 canvas), the landmark positions are frozen until subsequent displacement below 10 pixels is observed. As demonstrated in the right two frames of Fig. 12, this simple clamping prevents the vast majority of abnormal movements from entering the Interactivity computation. B.2. Additional Experimental Results Quantitative Results on the EMTD Benchmark. EMTD [41] is half-body dataset that includes hands, as illustrated in Fig. 8. We compare AnyTalker against three methods capable of generating half-body sequences: EchoMimic v2 [41], FantasyTalking [61], and MultiTalk [31]. The evaluation protocol strictly follows the metrics described in Sec. 5.1 for the single-person benchmark. AnyTalker-14B achieves the best scores on three metrics and is only marginally behind MultiTalk on identity preservation (ID). These results demonstrate that AnyTalker is comprehensive framework that handles not only tight-face inputs but also half-body scenarios. Qualitative Result on Single-Person Benchmarks. As illustrated in Fig. 13, we present qualitative comparisons of EchoMimic [8], StableAvatar [56], Sonic [24], MulFigure 14. More Results generated by AnyTalker. Figure 15. Improvement in interaction among each identity after fine-tuning on authentic multi-person data (right). tiTalk [31], OmniHuman-1.5 [26], and AnyTalker-14B on the HDTF [80] and VFHQ [65] benchmarks. AnyTalker consistently produces clear dentition and accurate lip movement. More Multi-Person Results. Fig. 14 showcases additional multi-person animations generated by AnyTalker. The model gracefully handles broad spectrum of inputs: real photographs, AIGC images, and cartoons. It produces natural, context-appropriate interactions regardless of the number of identities involved. Further compelling examples are available in the videos on the project homepage. B.3. Effectiveness of Interactivity Refinement As shown in Fig. 15, refining interactivity with authentic multi-person data lets identities exchange significantly more natural eye contact. model trained solely on singleperson data can activate each face correctly, yet every identity remains blank whenever it is not speaking, an effect that looks highly unnatural in conversational scenes. B.4. Future Work At present, AnyTalker supports only rudimentary camera motions driven by textual prompts. Drawing inspiration from recent controllable video-generation approaches [37, 71, 72], we can incorporate additional conditional signals, such as camera trajectory [3]. By grafting lightweight, training-efficient modules [21, 29, 38, 49] into recent camera-trajectory-control techniques [2, 77], we expect to enrich the visual storytelling of the generated videos, automatically framing and tracking the active speaker without manual intervention."
        }
    ],
    "affiliations": [
        "Beijing Jiaotong University",
        "Hong Kong University of Science and Technology",
        "Video Rebirth",
        "Zhejiang University"
    ]
}