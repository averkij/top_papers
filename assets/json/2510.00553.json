{
    "paper_title": "On Predictability of Reinforcement Learning Dynamics for Large Language Models",
    "authors": [
        "Yuchen Cai",
        "Ding Cao",
        "Xin Xu",
        "Zijun Yao",
        "Yuqing Huang",
        "Zhenyu Tan",
        "Benyi Zhang",
        "Guiquan Liu",
        "Junfeng Fang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in reasoning capabilities of large language models (LLMs) are largely driven by reinforcement learning (RL), yet the underlying parameter dynamics during RL training remain poorly understood. This work identifies two fundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1 Dominance, where the top singular subspace of the parameter update matrix nearly fully determines reasoning improvements, recovering over 99\\% of performance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace evolves linearly throughout training, enabling accurate prediction from early checkpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the generalizability of these properties. More importantly, based on these findings, we propose AlphaRL, a plug-in acceleration framework that extrapolates the final parameter update using a short early training window, achieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning performance without extra modules or hyperparameter tuning. This positions our finding as a versatile and practical tool for large-scale RL, opening a path toward principled, interpretable, and efficient training paradigm for LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 3 5 5 0 0 . 0 1 5 2 : r Preprint On Predictability of Reinforcement Learning Dynamics for Large Language Models Yuchen Cai1, Ding Cao1, Xin Xu3, Zijun Yao2, Yuqing Huang1, Zhenyu Tan1, Benyi Zhang1, Guiquan Liu1, Junfeng Fang2 1USTC, 2NUS, 3HKUST {caiyuchen,caoding}@mail.ustc.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in reasoning capabilities of large language models (LLMs) are largely driven by reinforcement learning (RL), yet the underlying parameter dynamics during RL training remain poorly understood. This work identifies two fundamental properties of RL-induced parameter updates in LLMs: (1) Rank1 Dominance, where the top singular subspace of the parameter update matrix nearly fully determines reasoning improvements, recovering over 99% of performance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace evolves linearly throughout training, enabling accurate prediction from early checkpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the generalizability of these properties. More importantly, based on these findings, we propose AlphaRL, plug-in acceleration framework that extrapolates the final parameter update using short early training window, achieving up to 2.5 speedup while retaining >96% of reasoning performance without extra modules or hyperparameter tuning. This positions our finding as versatile and practical tool for large-scale RL, opening path toward principled, interpretable, and efficient training paradigm for LLMs. Our model and code will be available at: https://anonymous.4open.science/r/AlphaRL. Figure 1: Comparison between RL-trained models and their Rank-1% parameter update counterparts across five reasoning benchmarks. The results demonstrate that retaining only the Top 1% of the parameter update matrix is sufficient to recover the reasoning gains achieved by RL-trained models. More detailed experimental settings and results are exhibited in Section 2. Best viewed in color. Corresponding author: fjf@mail.ustc.edu.cn 1 Preprint Figure 2: Overview of our key findings and method. (a) Rank-1 Dominance: The majority of reasoning improvements induced by RL can be captured by the Rank-1 Subspace of the parameter update Î”ğ‘Š, which throughout the RL training process. (b) AlphaRL: Leveraging Rank-1 Linear Dynamics, AlphaRL predicts the trajectory of the Rank-1 Subspace, allowing models to reach final performance with fewer RL training steps. Best viewed in color."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have witnessed rapid advances in reasoning, development largely driven by reinforcement learning (RL) based training paradigm (OpenAI, 2025; Claude, 2025; Yang et al., 2025). These advances naturally motivate efforts to interpret RL-trained LLMs, leading to studies such as neuron attribution (Bogdan et al., 2025), circuit analysis (Qian et al., 2025), and sparse autoencoders (Galichin et al., 2025). While effective, these researches mainly focus on post-hoc interpretability (i.e., explaining the endpoints of training), leaving the RL process itself less explored (Wang et al., 2025b; Zhang et al., 2025; Feng et al., 2025). Understanding parameter dynamics during RL is essential: not only for optimizing RL paradigms, but also for shedding light on the emergence of reasoning capabilities. Hence, this work aims to reveal the black-box of RL process by addressing two fundamental questions: Do RL-guided parameter updates follow consistent patterns, and how do these patterns give rise to reasoning capabilities? To solve these, we conduct step-wise analysis of the parameter update matrix Î”ğ‘¾ (i.e., the parameter difference between the RL-trained model and the base model). After applying mathematical tools such as orthogonal subspace projection (Cai & Cao, 2024), we uncover striking phenomenon: performing singular value decomposition (SVD) (Koren et al., 2009) on Î”ğ‘¾ reveals that the top singular subspace, defined as the Rank-1 Subspace, almost entirely determines the reasoning gains from RL. That is, adding only the Rank-1 component of Î”ğ‘¾ to the base model is sufficient to recover nearly all of the RL-trained models reasoning improvements. More remarkably, this property holds not just at convergence, but at any intermediate step of RL training, as shown in Figure 2 (a). We formalize this finding as Property 1 (Rank-1 Dominance): Rank-1 Subspace of Î”ğ‘¾ determines the reasoning gains of LLMs throughout the RL training process. This property inspired us to probe how the Rank-1 Subspace evolves during RL training. By applying partial least squares (PLS) to track the dimension-wise trajectory of the Rank-1 Subspace across training steps, we observe an almost strictly linear upward trend, with the linear rate metric ğ‘…2 (Geladi & Kowalski, 1986) exceeding 0.96. Consequently, the Rank-1 Subspace at target step can be accurately forecast from short early window of training. We formalize this as Property 2 (Rank-1 Linear Dynamics): Rank-1 Subspace evolves approximately linearly with RL training process, yielding high predictability from early-stage checkpoints of RL process. To validate the generality, we conducted extensive experiments across diverse set of 8 models (ranging from 7B to 32B parameters, e.g., Qwen2.5 (Qwen et al., 2025) and Qwen3 (Yang et al., 2025)) trained with 7 distinct advanced training algorithms (e.g., RLOO (Ahmadian et al., 2024), GRPO (DeepSeek-AI et al., 2025) and DAPO (Yu et al., 2025)). Our analysis shows that, for Property 1, the Rank-1 Subspace alone recovers an average of 99.17% of the reasoning capability. For Property 2, (1) the linearity of Rank-1 Subspaces evolution exhibits an average ğ‘…2 of 0.914, and (2) predictions of its later state based on early-stage states achieve an average error of less than 2 Preprint 5%. Crucially, control experiments with alternative training paradigms like supervised fine-tuning and distillation on the same models yielded neither property, demonstrating that these phenomena are distinctive characteristics of the RL process for LLMs. Detailed experimental setups and results are presented in Sections 2 and 3. These findings provide actionable interpretability for RL in LLMs: since the Rank-1 Subspace governs RL-induced gains (Property 1) and evolves almost linearly over training (Property 2), the trajectory becomes effectively predictable. We therefore introduce AlphaRL, plug-in acceleration scheme. As shown in Figure 2 (b), for any given RL algorithm applied to any LLM, AlphaRL simply requires an early training window to calculate (1) the initial Rank-1 Subspace of Î”ğ‘¾ and (2) its linear growth rate. It then directly predicts the final parameter update that attains the target reasoning performance without running the full schedule. Experiments on the aforementioned models and RL algorithms demonstrate that AlphaRL achieves up to 2.5 acceleration while retaining >96% of the final reasoning capability. Detailed implementation and results are presented in Section 4. In summary, this paper uncover two laws of parameter dynamics in LLM training process, Rank-1 Dominance and Rank-1 Linear Dynamics, providing predictive lens on how RL yields reasoning gains. These findings suggest that the complex, multi-step optimization of RL may be governed by surprisingly simple and low-dimensional core mechanism. Hence, it not only challenges the blackbox view of RL, but also opens new avenues for bridging the gap between empirical scaling laws and theoretical understandings of how capabilities emerge. Building on these properties, we introduce AlphaRL, free lunch for RL acceleration: it requires no extra modules, hyperparameter tuning, or human intervention, and remains orthogonal to, thus multiplicatively compatible with, existing acceleration paradigms. This positions our finding as versatile and practical tool for large-scale RL, opening path toward principled, interpretable, and efficient training paradigm for LLMs."
        },
        {
            "title": "2 Dominance of Rank-1 Subspace (Property 1)",
            "content": "In this section, our objective is to analyze the effect of the Rank-1 Subspace of the parameter update matrix Î”ğ‘Š on reasoning gains. In Section 2.1, we first introduce the method for quantifying the contribution of the Rank-1 Subspace. Then, we exhibit experimental setups and main results In Section 2.3, we investigate the underlying causes of of Rank-1 dominance by in Section 2.2. exploring the norm of Î”ğ‘Š, the rank of Î”ğ‘Š, and model embeddings. 2.1 Rank-1 Subspace We first describe the method for quantifying the contribution of the Rank-1 Subspace of parameter updates Î”ğ‘¾ to reasoning gains. Specifically, performing SVD on Î”ğ‘¾, we have: ğ‘Ÿ Î”ğ‘¾ = ğœğ‘– ğ’–ğ‘–ğ’— ğ‘– , ğ‘Ÿ = rank(Î”ğ‘¾), (1) where ğœğ‘– are singular values and ğ’–ğ‘–, ğ’—ğ‘– are the corresponding singular vectors. The Rank-1 update matrix is then defined by retaining only the largest singular value ğœ1 and its vectors: ğ‘–=1 Î”ğ‘¾ (1) = ğœ1 ğ’–1ğ’— 1 . To ensure consistency in update strength, we rescale Î”ğ‘¾ (1) to match the L2 norm of Î”ğ‘¾: Î” Ë†ğ‘¾ (1) = ğ›¼ Î”ğ‘¾ (1) , ğ›¼ = Î”ğ‘¾ 2 Î”ğ‘¾ (1) 2 . (2) (3) The evaluation model is then obtained by adding Î” Ë†ğ‘¾ (1) to the base model. In addition, we also evaluate Rank-ğ‘˜% Subspace strategy, in which only the leading the top ğ‘˜% of singular subspaces are retained, in order to consistently study the collective effect of multiple subspaces. 2.2 Rank-1 Subspace & Reasoning gains Experiment Seting. Our experiments are conducted on eight models, covering five RL algorithms, including PPO (Schulman et al., 2017), RLOO (Ahmadian et al., 2024), GRPO (DeepSeek-AI et al., 3 Preprint Figure 3: (a) Performance under Rank-1 and Rank-ğ‘˜% Subspaces on MATH-500; (b) Performance of the Rank-1 Subspace across training. Best viewed in color. 2025), Dr.GRPO (Liu et al., 2025), and DAPO (Yu et al., 2025), as well as Distillation (DIST) (Hinton et al., 2015) and Supervised Fine-tuning (SFT) (Ye et al., 2025). We comprehensively evaluate the reasoning performance of these models on six reasoning benchmarks, including AIME24, AIME25 (Ye et al., 2025) and MATH-500 (Lightman et al., 2023), to verify the robustness and generality of our findings. More detailed settings are provided in Appendix A. Results on Fully Trained Models. Figure 1 presents the comparison with Rank-1% Subspace, while Figure 3 shows the results for the Rank-1 Subspace. For clearer presentation, we report the Relative Accuracy, defined as the ratio between the accuracy of the Rank-1 reconstructed model and that of the fully trained model. As shown in Figure 3 (a), even single Rank-1 Subspace is sufficient to recover performance close to that of the fully trained model; in RLOO, GRPO, and DAPO, it can even surpass the fully trained model. This indicates that RL updates are highly concentrated in few directions, with single Rank-1 Subspace capable of capturing and reproducing nearly all reasoning improvements. In contrast, SFT and DIST exhibit strong dependence on subspace rank, requiring more subspaces to achieve performance gains. Notably, unlike methods such as LoRA in SFT, which predefine subspace dimensionality prior to training, our finding holds under stricter condition: even after full-parameter RL training, reasoning improvements can still be almost entirely captured by only few subspace directions. Additional results are provided in Appendix 7. Results across the RL Process. We then examine the property of Rank-1 dominance throughout the RL training process, with results shown in Figure 3 (b). We observe that, during the early stages of training, the performance of the Rank-1 Subspace is slightly lower than that of the fully trained model; however, at later checkpoints, its performance can fully match the fully trained model. We hypothesize that this phenomenon arises because, in the early stages, update gradients are relatively dispersed and have not yet concentrated into stable subspace regions. As training progresses, the the RL update directions gradually converge and align with unified reasoning-enhancement pattern, and the Rank-1 Subspace has already captured the principal components of this pattern, thereby exhibiting stronger effectiveness at later stages. In general, these results demonstrate that the Rank-1 Subspace of Î”ğ‘Š remains the key factor driving reasoning improvements throughout the RL process. Ablation Study. After establishing the dominant role of the Rank-1 Subspace, we compare the relative contributions of different individual subspaces. As shown in Figure 4 (a), the Rank-1 Subspace significantly outperforms other subspaces, and its performance gradually decreases as the corresponding singular values decline, underscoring its central role in reasoning enhancement. Notably, several subspaces associated with relatively large singular values, although individually less effective than the Rank-1 Subspace, still contribute substantially to reasoning improvements. This indicates that, despite being orthogonal by construction, the functional contributions of these high-singular-value subspaces are largely aligned with the Rank-1 Subspace, collectively reflecting unified reasoning-enhancement pattern. Scaling Experiment. Finally, to better understand the impact of Rank-1 Subspace strength on reasoning improvements, we conducted an experiment with the scaling factor ğœ†, where the update rule was modified as: Î” Ë†ğ‘¾ (1) = ğœ† ğ›¼ Î”ğ‘¾ (1) . As shown in Figure 4 (b), performance increases rapidly with ğœ† and then saturates, peaking at ğœ† 0.7, slightly below the norm magnitude of the full update. This is consistent with the previous findings: the Rank-1 Subspace has captured the principal 4 Preprint Figure 4: (a) Effect of different single subspaces on performance; (b) Effect of scaling the Rank-1 Subspace updates on performance. Best viewed in color. components of the unified reasoning-enhancement pattern, with its core effect primarily determined by magnitude. At this level, the core contribution of the Rank-1 Subspace has been largely realized, and further increasing the scaling factor may result in diminishing returns. 2.3 Analysis of Rank-1 Dominance In the previous part, we experimentally observed and analyzed the contribution of the Rank-1 Subspace to reasoning gains. In this section, we attempt to further investigate the underlying causes of this phenomenon. To this end, we begin by comparing the update characteristics of different methods. Specifically, we compute the average L2 norm of Î”ğ‘¾ for RL and compare it with the average L2 norms of Î”ğ‘¾ for SFT and DIST. Additionally, we examine the proportion of the norm of the unscaled Rank-1 Subspace and the Rank-1% subspace relative to the total update norm Î”ğ‘¾. As illustrated in Figure 5 (a), the update norms for DIST and SFT are found to be one to two orders of magnitude larger than those for RL, indicating that they involve much larger parameter changes during training. In contrast, RL updates show higher degree of concentration, with the unscaled Rank-1 Subspace and Rank-1% Subspace occupying larger fraction of the total update norm. Distribution Shifts of Embedding Space. The previous experiments raise an intriguing question: why is RL, compared to SFT and DIST, able to achieve substantial reasoning improvements with only 1% or even less of the parameter update? To explore this, we analyze the impact of different training processes on token embeddings in LLMs. By applying PCA for dimensionality reduction and t-SNE for visualization, we observe that the embeddings of DIST and SFT exhibit noticeable global shifts, with DIST showing particularly large deviations for certain tokens, as shown in Figure 5 (b). In contrast, RL methods cause minimal distribution shift of the embedding space. This suggests that the updates in DIST and SFT are not merely adjustments in high-level reasoning pathways, but involve significant global modifications to the lower-level representation space. As result, even when utilizing the all update information, these methods struggle to consistently improve reasoning performance. In contrast, RL maintains the embedding space largely unchanged, with its reasoning improvements primarily driven by the optimization and adjustment of high-level information flow. Approximate Low-rank of Î”ğ‘¾. It is worth mentioning that, in the above experiments, we discovered universal approximate low-rank (Zhang, 2015) property of Î”ğ‘¾ in RL, which is completely absent in SFT and DIST. Due to space limitations, we provide detailed discussion of this phenomenon in Appendix B. Furthermore, we propose that the superior properties observed in RL-trained LLMs (e.g., minimizing catastrophic forgetting and improving generalization) may fundamentally arise from this low-rank structure, which plays pivotal role in the models ability to effectively retain and adapt learned knowledge. Additionally, we also observed in our experiments the unique role of Rank-1 in guiding the reasoning chain, where modifying small number of tokens achieves reasoning performance comparable to that of the fully trained model. We recommend interested readers to refer to the detailed results and analysis in Appendix C. 5 Preprint Figure 5: (a) L2 norm of updates across methods and the fraction of update information captured by the unscaled Rank-1 and Rank-1% Subspaces; (b) Effect of different update methods on the embedding layer, with the two embedding representations of the same token connected by gray lines. Best viewed in color."
        },
        {
            "title": "3 Linear Dynamics of Rank-1 Subspace (Property 2)",
            "content": "In the section 2, we analyzed the contribution of the Rank-1 Subspace to reasoning gains and established its dominant role in RL training. Here, we investigate the evolution of the Rank-1 Subspace during RL training process. 3.1 Exploring the Dynamics of Rank-1 Subspace To characterize the evolution of the Rank-1 Subspace during training, we collect the sequence of ğ’–1 vectors across ğ‘‡ checkpoints for each module: U1 = {ğ’– (ğ‘¡ ) ğ‘¡=1, which we refer to as the modules update trajectory. Since each ğ’– (ğ‘¡ ) lies in high-dimensional space, we apply PCA for dimensionality 1 reduction and then t-SNE for visualization. As shown in Figure 6 (a), the trajectories exhibit smooth, nearly linear patterns, with color gradients aligned to training progress, indicating the existence of stable update direction. Limited by space, we describe the perspective and interpretation underlying this procedure in Appendix D. 1 }ğ‘‡ Furthermore, to quantify whether there is similar linear relationship between this evolution and reasoning performance, we treat each modules Rank-1 trajectory ğ’– (ğ‘¡ ) as the independent variable 1 and the corresponding checkpoints accuracy ğ‘¦ on the reasoning dataset as the dependent variable, performing linear fitting using Partial Least Squares (PLS) regression (Geladi & Kowalski, 1986) and using ğ‘…2 as measure of linearity (calculation details can be found in Appendix D). As shown in Figure 6 (b), some modules even achieve ğ‘…2 values close to 1, indicating that the Rank-1 update directions are strongly correlated with reasoning performance and can be effectively modeled by fixed linear relationship. These experiments reveal the unique role of Rank-1 Subspace, where it acts as bridge during training, providing the foundation for the visual correlation between training steps and reasoning performance. 3.2 Rank-1 Linearity & Module Importance Although many modules exhibit high linearity in their Rank-1 Subspace (ğ‘…2 close to 1), we still observe modules with relatively low linearity. These modules often display fragmented and irregular trajectories, with frequent directional shifts and unstable relations to accuracy. This naturally raises two key questions: (1) Does linearity systematically correspond to the functional roles of modules? (2) Can module contributions to reasoning gains be quantified based on their linearity? To address the first question, we aggregate ğ‘…2 values across all modules and layers in Appendix 14. Results show that MLP modules, particularly those in mid-to-high layers, tend to achieve higher ğ‘…2. possible reason is that higher-layer modules are closer to the source of the reward signal, allowing them to better retain and utilize reasoning-related update directions. In contrast, self-attention modules generally exhibit lower ğ‘…2, suggesting noisier or partially redundant update signals. 6 Preprint Figure 6: (a) Projection visualization of U1 trajectories; (b) PLS regression reveals linear relationship between U1 and accuracy, with ğ‘…2 values indicating the strength of fit; (c) Sliding window analysis shows that as the window progresses from 1 to 7, the ğ‘…2 and prediction accuracy both exhibit decreasing trend. Best viewed in color. Based on this observation, we argue that the heterogeneity in linearity reflects differences in functional roles during reasoning. Modules with high ğ‘…2 and smooth monotonic trajectories are likely key regions where RL allocates effective capacity: after an initial exploratory phase, they converge around reasoning-enhancing update direction. Conversely, modules with low ğ‘…2 and irregular trajectories may be only weakly influenced by reward signals and more strongly driven by noisy gradients, preventing them from forming stable performance-related update directions. To validate the relation between ğ‘…2 and module contribution, we sort all modules in descending order of ğ‘…2 and select subsets using sliding window (window size roughly one-third of all modules, step size about one-seventh). For each window, only the Rank-1 updates of the selected modules are injected into the base model, while other modules remain unchanged. As shown in Figure 6 (c), performance gradually declines as the minimum ğ‘…2 in the window decreases. This demonstrates that ğ‘…2 effectively quantifies the functional role of module updates, providing reliable tool for systematically analyzing module-level contributions to performance during RL training."
        },
        {
            "title": "4 AlphaRL: A Free Acceleration for RL Training",
            "content": "The dominance of the Rank-1 Subspace established in Section 2, combined with its linear dynamics demonstrated in Section 3, directly motivates the RL acceleration algorithm: AlphaRL. It leverages the early-training dynamics of Rank-1 Subspace to predict the final parameter update matrix, bypassing the need for full training. Here we provide the detailed acceleration process and results. 4.1 Rank-1 Update Prediction As noted in Section 3, we observed linear relationship between the training trajectory and relative accuracy; however, since the ğ’– (ğ‘¡ ) 1 used in the construction are unit vectors, they do not capture the magnitude of the Rank-1 updates. To address this, we construct the scaled Rank-1 vectors, where each vector is scaled by the product of ğ›¼ (ğ‘¡ ) and ğœ (ğ‘¡ ) to represent the corresponding column of the 1 update matrix Î” Ë†ğ‘¾ (1) . We record the relative accuracy of the corresponding checkpoints and fit the vectors with their relative accuracies using single-component PLS regression, establishing linear relationship between them. Given target relative accuracy ğ‘¦, AlphaRL obtains the corresponding update vector through inversion. Finally, this update vector is combined with the left singular vector ğ’—1 to form the new Rank-1 update for each module. In this manner, each modules Rank-1 update is guided by its AlphaRL-predicted linear relationship between the scaled Rank-1 trajectory and accuracy. We use the models test accuracy on MATH-500 and set the target accuracy to ğ‘¦ = 1. The update vectors obtained through inversion are then applied to all datasets for evaluation. 4.2 Main Results We use models trained with RLOO, GRPO, and DAPO on Qwen3-8B-Base (Qwen et al., 2025). We then evaluate on six standard mathematical reasoning benchmarks: AIME24, AIME25 (Ye et al., Preprint Table 1: Performance on reasoning benchmarks at different training stages and their AlphaRLenhanced variants. The prediction is based on Rank-1 vectors and their corresponding accuracies extracted from checkpoints between the Base model and the current training stage. Within each method block, the highest score is highlighted in bold, and the second highest score is indicated with . Stage AIME24 AIME25 MATH MINERVA GPQA GSM8K Avg. DAPO for the Qwen3-8B Base Model Fully Trained Model Training 10% Training 40% Training 10%+AlphaRL Training 40%+AlphaRL 28.54 12.50 15.80 15.00 28.33 GRPO from the Qwen3-8B Base Model Fully Trained Model Training 10% Training 40% Training 10%+AlphaRL Training 40%+AlphaRL 26.40 9.17 15.83 12.50 22.25 RLOO from the Qwen3-8B Base Model Fully Trained Model Training 10% Training 40% Training 10%+AlphaRL Training 40%+AlphaRL 27.50 11.67 16.67 11.67 17.92 24.17 7.50 11.67 11.67 23. 21.67 8.33 14.17 13.25 18.13 18.33 8.33 14.17 14.17 18.33 80.95 70.25 77.60 76.45 80.50 78.25 64.65 72.25 67.60 78.45 78.25 57.25 72.75 60.45 76.00 44.02 32.07 37.07 40.46 43. 42.19 31.89 37.30 36.83 40.12 41.90 35.02 39.24 37.46 40.60 48.23 36.66 41.67 41.54 49.25 47.10 36.74 41.16 36.74 43.13 45.82 38.65 42.05 44.95 44.40 94.35 84.30 93.20 93.75 94. 93.50 85.35 91.25 91.35 91.75 95.10 83.50 93.75 93.75 93.80 53.38 40.55 46.30 46.47 53.31 51.52 39.36 45.30 43.43 49.42 50.82 39.89 46.44 43.74 48.52 Table 2: Different model outputs on the centroid geometry problem. Question: The medians ğ´ğ·, ğµğ¸, and ğ¶ğ¹ of triangle ğ´ğµğ¶ intersect at the centroid ğº. The line through ğº that is parallel to ğµğ¶ intersects ğ´ğµ and ğ´ğ¶ at ğ‘€ and ğ‘, respectively. If the area of triangle ğ´ğµğ¶ is 144, then find the area of triangle ğ¸ ğ‘ğº. Base Model Answer: To solve this problem, we need to understand the properties of medians and centroids 3 = 32 in triangle... Area of ğ¸ ğ‘ğº = 16 16 Fully Trained Model Answer: Alright, have this geometry problem in front of me, and need to find... Area of ğ¸ ğ‘ğº = 1 Rank-1 Answer: Alright, have this geometry problem in front of me, and need to find... Area of ğ¸ ğ‘ğº = 288 36 = 8. Final Answer: 8 . 3 . So, the area of triangle ğ¸ ğ‘ğº is 32 3 . 2 16 = 8. Final Answer: 8 . 3 = 48 3 2025), MATH-500 (Lightman et al., 2023), Minerva, and GPQA (Rein et al., 2023), with 32 sampled responses per question under temperature ğ‘‡ = 0.6, and we report average accuracy. Table 1 presents the reasoning performance across six reasoning benchmarks (AIME24, AIME25, MATH, MINERVA, GPQA, and GSM8K) at different training stages, with and without AlphaRLenhanced updates. The results show that AlphaRL significantly improves the models reasoning performance even at early stages (only 10% of the total training steps), achieving performance comparable to the non-AlphaRL model at 40% of training steps. By the 40% training stage, the AlphaRL-enhanced models almost reach the performance of fully trained models. For instance, RLOO and GRPO models achieve 96% of the reasoning performance of the fully trained model, surpassing the fully trained model on the MATH-500 dataset. For DAPO, AlphaRL reaches relative accuracy of 102% on the GPQA dataset. Furthermore, we present an example after acceleration in Table 2, which shows that the AlphaRL-enhanced model not only remains consistent with the Fully Trained Model in evaluation metrics but also exhibits largely similar patterns and reasoning approaches in the responses. 8 Preprint Overall, the AlphaRL-enhanced models exhibit significant improvements at all training stages. AlphaRL not only accelerates the training process but also maintains reasoning performance close to that of the fully trained model while reducing training time. On average, AlphaRL accelerates the training process by up to 2.5 times while retaining more than 95% of the reasoning performance, demonstrating its tremendous potential in improving both training efficiency and reasoning capability. 9 Preprint"
        },
        {
            "title": "5 Related Work",
            "content": "Reinforcement Learning for LLMs. Before the emergence of reasoning-capable models such as OpenAIs o1, reinforcement learning (RL) was primarily employed in RLHF to improve instructionfollowing and alignment with human preferences (Ouyang et al., 2022). More recently, RL with Verifiable Rewards (RLVR) has been proposed as an effective strategy to enhance reasoning in domains such as mathematics and programming (Lambert et al., 2025). OpenAIs o1 was the first to demonstrate that RL can incentivize large-scale reasoning, inspiring subsequent models such as DeepSeek-R1 (DeepSeek-AI et al., 2025), Kimi-K2 (Team et al., 2025), and Qwen3 (Yang et al., 2025). Among these, DeepSeek-R1 stands out for achieving strong reasoning capabilities via the online RL algorithm GRPO and for introducing the Zero RL paradigm, showing that reasoning can emerge even without conventional RL fine-tuning. Building on these advances, later approaches such as DAPO (Yu et al., 2025), VAPO (Yue et al., 2025b), and GSPO (Zheng et al., 2025) have further broadened the landscape of RL-based reasoning. Emergent Behaviors of Reinforcement Learning. Yue et al. (2025a) investigated the differences in sampling between base models and RL-fine-tuned models, showing that RL improves sampling efficiency for pass@1 but does not directly enhance reasoning ability. Cui et al. (2025) identified the phenomenon of entropy collapse in reinforcement learning, where rapid early convergence causes the model to become overly confident, prematurely degrading its exploratory capacity. Wang et al. (2025a) observed in chain-of-thought reasoning that high-entropy tokens often act as branching points defining multiple potential reasoning paths. Shenfeld et al. (2025) compared RL with supervised fine-tuning and found that RL better preserves the models original knowledge and capabilities. Finally, Feng et al. (2025) demonstrated in an intent detection task that RL significantly improves generalization compared to SFT, underscoring the value of RL in more challenging scenarios."
        },
        {
            "title": "6 Limitations and Future works",
            "content": "Despite revealing two simple yet generalizable laws of reinforcement learning in large language models, our study has certain limitations. The conclusions are primarily based on large-scale empirical observations, which uncover universal low-rank dynamics in RL training. However, these findings still lack rigorous theoretical foundations. Future work will incorporate techniques such as neuron attribution and causal tracing to build more formal theoretical models, thereby providing deeper insights into the underlying mechanisms. Furthermore, while AlphaRL demonstrates the feasibility of predicting later updates from early checkpoints to accelerate RL training, its effectiveness remains constrained by the design and stability of RL algorithms. Future directions include exploring more sophisticated nonlinear forecasting methods, combined with robust reward modeling and optimization strategies, to further enhance acceleration. In addition, AlphaRL may find application in high-cost scenarios such as large-scale agents or multimodal training, where reducing computational overhead is especially critical. Finally, the Rank-1 property can also be exploited for monitoring training dynamics and serving as reward signal for reverse optimization of the training process. Future research may explore combining Rank-1 regularities with high-rank corrections to develop more flexible low-rank control methods, thereby advancing the development of efficient reasoning models."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we uncover two fundamental laws of RL in LLMs: (1) Rank-1 Dominance, where reasoning improvements concentrate in the top singular direction, and (2) Rank-1 Linear Dynamics, where this direction evolves in predictable linear manner throughout training. Building on these insights, we introduce AlphaRL, plug-and-play acceleration method that leverages early checkpoints to forecast later updates, reducing computational cost while preserving reasoning performance close to full training. Overall, our findings provide new interpretability lens on how RL enhances reasoning and establish path toward more efficient and controllable parameter training for LLMs. 10 Preprint"
        },
        {
            "title": "References",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms, 2024. URL https://arxiv.org/abs/2402.14740. Paul C. Bogdan, Uzay Macar, Neel Nanda, and Arthur Conmy. Thought anchors: Which llm reasoning steps matter?, 2025. URL https://arxiv.org/abs/2506.19143. Yuchen Cai and Ding Cao. O-edit: Orthogonal subspace editing for language model sequential editing, 2024. URL https://arxiv.org/abs/2410.11469. Claude. claude-opus-4-1. https://www.anthropic.com/news/claude-opus-4-1/, 2025. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, and Ning Ding. The entropy mechanism of reinforcement learning for reasoning language models, 2025. URL https://arxiv.org/abs/2505.22617. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Zihao Feng, Xiaoxue Wang, Ziwei Bai, Donghang Su, Bowen Wu, Qun Yu, and Baoxun Wang. Improving generalization in intent detection: Grpo with reward-based curriculum sampling, 2025. URL https://arxiv.org/abs/2504.13592. Andrey Galichin, Alexey Dontsov, Polina Druzhinina, Anton Razzhigaev, Oleg Y. Rogov, Elena Tutubalina, and Ivan Oseledets. have covered all the bases here: Interpreting reasoning features in large language models via sparse autoencoders, 2025. URL https://arxiv.org/abs/ 2503.18878. Paul Geladi and Bruce R. Kowalski. tutorial. Analytica Chimica Acta, 185:117, 1986. https://doi.org/10. 1016/0003-2670(86)80028-9. URL https://www.sciencedirect.com/science/ article/pii/0003267086800289. least-squares regression: doi: ISSN 0003-2670. Partial 11 Preprint Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories, 2021. URL https://arxiv.org/abs/2012.14913. Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning, 2025. URL https://arxiv.org/abs/2504.11456. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network, 2015. URL https://arxiv.org/abs/1503.02531. Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):3037, 2009. doi: 10.1109/MC.2009.263. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2025. URL https://arxiv.org/abs/2411.15124. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt, 2023. URL https://arxiv.org/abs/2202.05262. OpenAI. Introducing introducing-gpt-oss/, 2025. gpt-oss. https://openai.com/zh-Hans-CN/index/ Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155. Chen Qian, Dongrui Liu, Haochen Wen, Zhen Bai, Yong Liu, and Jing Shao. Demystifying reasoning dynamics with mutual information: Thinking tokens are information peaks in llm reasoning, 2025. URL https://arxiv.org/abs/2506.02867. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. Rls razor: Why online reinforcement learning forgets less, 2025. URL https://arxiv.org/abs/2509.04259. 12 Preprint Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, pp. 12791297. ACM, March 2025. doi: 10.1145/3689031.3696075. URL http://dx.doi.org/10.1145/ 3689031.3696075. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang, Yao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qianqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu, Chenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin, Longhui Yu, Enming Yuan, Hongbang Yuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu, Weiyu Zhuang, and Xinxing Zu. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning, 2025a. URL https://arxiv.org/abs/ 2506.01939. Shuhe Wang, Shengyu Zhang, Jie Zhang, Runyi Hu, Xiaoya Li, Tianwei Zhang, Jiwei Li, Fei Wu, Guoyin Wang, and Eduard Hovy. Reinforcement learning enhanced llms: survey, 2025b. URL https://arxiv.org/abs/2412.10400. Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, and Xu Yang. On the generalization of sft: reinforcement learning perspective with reward rectification, 2025. URL https://arxiv.org/abs/2508.05629. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning, 2025. URL https://arxiv.org/abs/2502.03387. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Preprint Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, WeiYing Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/ abs/2503.14476. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, 2025a. URL https://arxiv.org/abs/2504.13837. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, and Lin Yan. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks, 2025b. URL https://arxiv.org/ abs/2504.05118. Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, and Bowen Zhou. survey of reinforcement learning for large reasoning models, 2025. URL https://arxiv.org/abs/2509.08827. Zhihua Zhang. The singular value decomposition, applications and beyond, 2015. URL https: //arxiv.org/abs/1510.08532. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization, 2025. URL https://arxiv.org/abs/2507.18071. 14 Preprint"
        },
        {
            "title": "Ethics Statement",
            "content": "This research does not involve human subjects, personal data, or other sensitive information. All data used are derived from publicly available benchmark datasets and model parameters. The study adheres to principles of research integrity and strictly follows the ICLR Code of Ethics. We declare that there are no potential conflicts of interest or ethical risks."
        },
        {
            "title": "Reproducibility Statement",
            "content": "To ensure the reproducibility of our results, we provide detailed descriptions of the experimental settings in the appendix, including the models used, training methods, hyperparameter configurations, and optimizer settings. All evaluations are conducted on publicly available datasets. We have already released the complete code, data processing scripts, and running instructions to facilitate verification and further exploration by the research community."
        },
        {
            "title": "Use of LLMs",
            "content": "We used large language models (LLMs) solely for language polishing and stylistic refinement of the manuscript. Specifically, LLMs were employed to improve clarity, grammar, and readability of the text, without altering the technical content, experimental results, or scientific claims. All ideas, methods, experiments, and conclusions presented in this paper are original contributions of the authors. 15 Preprint"
        },
        {
            "title": "A Experimental Setup",
            "content": "We begin by outlining our experimental setup and relevant definitions. Let ğœƒinit denote the parameters of pretrained base LLM. By applying training method ğ‘€, we obtain the updated parameters ğœƒfull. In our experiments, we consider the following methods: Distillation (DIST) (Hinton et al., 2015), Supervised Fine-Tuning (SFT) (Ye et al., 2025; Wu et al., 2025), PPO (Schulman et al., 2017), RLOO (Ahmadian et al., 2024; DeepSeek-AI et al., 2025), GRPO (DeepSeek-AI et al., 2025), Dr.GRPO (Liu et al., 2025) and DAPO (Yu et al., 2025). For DIST, we adopt the distilled model DeepSeek-R1-Distill-Qwen-7B and its base model Qwen2.5Math-7B. For SFT, we adopt Qwen3-8B-Base as the base model, trained on the DeepMath-103K dataset with the LlamaFactory1 framework. For PPO, we adopt the open-sourced OpenReasoner-Zero-7B and its base model Qwen2.5-7B. For Dr.GRPO, we adopt the open-sourced Qwen2.5-Math-7B-Oat-Zero and its base model Qwen2.5-Math-7B. For RLOO and GRPO, we adopt Qwen3-8B-Base as the base model, trained on the DAPO-Math-17k dataset with the Verl2 framework. For DAPO, we consider two variants: (i) ours (8B) starting from Qwen3-8B-Base, trained on DAPO-Math-17k with Verl; (ii) open-source (32B) DAPO-Qwen-32B trained from the base Qwen2.5-32B with DAPO-Math-17k. All of our training runs are conducted on 8 H800 80GB GPUs until the reward/loss converges. For Supervised Fine-Tuning (SFT), we adapt our training codebase with LlamaFactory (Sheng et al., 2025). We employ full-parameter training in Float16 precision, with the maximum sequence length set to 20,480 tokens. The training batch size is 1,024 and the mini-batch size is 8, corresponding to 256 gradient accumulation steps. The learning rate is set to 1 105 with warmup, and gradient clipping of 1.0 is applied. We monitor the training loss and terminate training once the loss decreases by less than 2101 over five consecutive steps. We conduct the SFT training experiment on Qwen38B-Base models, using the DeepMath-107K (He et al., 2025) dataset. The chat template for SFT is specified as: User: {question} Please reason step by step, and put your final answer within boxed{}. Assistant: {CoT} with <endoftext> serving as the EOS token, where {question} is replaced with the specific problem instance and {CoT} denotes the chain-of-thought reasoning and final answer provided in the dataset. By training on nearly 100K examples, the model achieves stable convergence, and the final checkpoint is adopted for subsequent experiments. For RLOO, GRPO, and DAPO, we adapt our training codebase with the Verl (Sheng et al., 2025) and follow the corresponding training setups. All methods share the same core configuration: the maximum prompt length is 2,048 tokens and the maximum response length is 20,480 tokens, yielding total budget of 22,528 tokens. We adopt training batch size of 512 and mini-batch size of 32, corresponding to 16 gradient accumulation steps in Float16 precision. Each prompt generates ğ‘›=8 outputs during rollout. The learning rate is set to 1 106 with warmup, and gradient clipping of 1.0 is applied. We monitor the average reward per training batch and terminate training once the reward fails to improve for five consecutive steps. In addition to the unified configuration described above, each method adopts specific hyperparameter settings in our experiments. For RLOO, we use low-variance KL loss with coefficient 0.001 and disable entropy regularization. For GRPO, we set both the high and low clipping ratios to 0.2 and apply KL loss with coefficient 0.001 following DeepSeek-AI et al. (2025). For DAPO, we employ techniques such as clip-higher, dynamic sampling, token-level policy gradient loss, and overlong reward shaping and apply the recommended hyperparameters from Yu et al. (2025): the clipping ratios are set to ğœ–low = 0.2 and ğœ–high = 0.28, KL divergence terms are removed entirely, and each training batch generates up to 512 3 candidate responses. 1https://github.com/hiyouga/LLaMA-Factory 2https://github.com/volcengine/verl 16 Preprint We perform RLVR experiments on Qwen3-8B-Base models, using the DAPO-Math-17K (Yu et al., 2025) dataset for training. For this dataset, we employ the built-in chat template, specified as: User: Solve the following math problem step by step. The last line of your response should be of the form Answer: $Answer (without quotes) where $Answer is the answer to the problem. {question} Remember to put your answer on its own line after \"Answer:\". Assistant: As in the SFT setting, <endoftext> serves as the EOS token, where {question} is replaced with the corresponding problem instance. We save the checkpoint after each training batch to enable subsequent evaluation experiments. Preprint In-depth Analysis of the Low-rank Phenomenon We propose that the low-rank yet effective update mechanism observed in reinforcement learning (RL) fine-tuning arises from several key factors. First, most RL fine-tuning methods adopt an on-policy strategy, sampling training data directly from the models own policy distribution. Shenfeld et al. (2025) suggest that this naturally biases the optimization process toward staying close to the base model in terms of KL divergence, favoring only minor corrections on top of its existing capabilities. Therefore, we argue that RL gradients do not introduce entirely new update directions, but rather reinforce signals already present during pretraining and instruction tuning. As result, parameter updates concentrate in few regions, exhibiting sparse, low-rank structure. Second, common stabilization mechanismssuch as KL regularization, logits clipping, and gradient clippingfurther constrain the magnitude and spread of parameter updates, thereby limiting, to some extent, the enrichment of update information. Importantly, our norm-based analysis (Figure 3(b)) demonstrates that even under such strong constraints, RL achieves substantial improvements in reasoning performance through limited updates. This suggests that performance gains do not rely on large-scale parameter drift but emerge from focused adjustments within small set of critical subspaces. Third, prior work shows that updating only about 20% of tokens suffices to match or even surpass full-token updates (Wang et al., 2025a), indicating that reasoning improvements primarily depend on small set of critical tokens rather than broad, global parameter modifications. These sparse, highimpact token updates may constitute the microscopic origin of the unified reasoning-enhancement pattern in RL: low-rank, highly structured updates effectively concentrate on key tokens, forming dominant update directions in parameter space. Finally, studies on RL generalization demonstrate that RL-fine-tuned models consistently outperform SFT-fine-tuned models in mitigating catastrophic forgetting and enhancing generalization (Shenfeld et al., 2025; Feng et al., 2025). Our analysis supports this view: RL leverages and reinforces existing gradient signals to activate latent model capabilities, with improvements primarily arising from concentrated adjustments in critical subspaces and minimal overall parameter drift. In contrast, SFT often requires learning task distributions that substantially deviate from the models intrinsic capabilities, necessitating larger-scale training data and frequently inducing parameter shifts that may lead to catastrophic forgetting. 18 Preprint Figure 7: Performance under Rank-1 and Rank-ğ‘˜% Subspace on MATH-500, AIME24, AIME25, MINERVA, GSM8K, and GPQA datasets. 19 Preprint Figure 8: Effect of RLOO, GRPO, Dr.GRPO and DAPO(32B) on the embedding layer, the two representations of the same token are connected with gray lines. 20 Preprint Figure 9: Performance of GRPO Rank-1 Subspace across different training checkpoints. Figure 10: Effect of different single subspace on performance. Preprint Figure 11: (a) Proportion of greedy versus non-greedy tokens in RL-generated reasoning chains (CoTs), evaluated with the Base Model; (b) Perplexity comparison of CoTs in the Base Model: RLgenerated CoTs versus those generated by the Base Model itself; (c) Relative positional distribution of non-greedy tokens in RL-generated CoTs, evaluated under the Base Model and the Rank-1 model. External Manifestations of Rank-1 Dominance Takeaway The Rank-1 Subspace captures key adjustments in the reasoning tokens, recovering the reasoning preferences of fully trained models. In the previous section, we discovered the naturally emerging low-rank property in RL updates and discussed its potential causes. In this section, we further analyze external manifestations of the Rank-1 Subspace, focusing on how it shapes model behavior. To investigate how RL training affects reasoning behavior, we conducted two experiments. For each problem, the RL-trained model first generated answers step by step using greedy strategy, i.e., selecting the token with the highest predicted probability at each step, thereby producing complete chain of thought. This chain was then fed token by token into the base model, and the base models greedy predictions were recorded at each step. Positions where the base models prediction matched the RL model were labeled as greedy, and all others as non-greedy. Figure 12: Word cloud of non-greedy tokens. These tokens appear in RL-generated reasoning chains but are not treated as greedily decoded tokens at the corresponding positions by the base model. As shown in Figure 11 (a), the proportion of non-greedy positions is substantially higher for the DIST and SFT methods compared to RL, indicating that these methods significantly alter the base models output distribution, whereas RL has comparatively limited effect. We further measured the perplexity of the base model on the chain-of-thought reasoning generated before and after RL training (both using greedy decoding). The results, shown in Figure 11 (b), reveal that RL training leaves perplexity largely unchanged, while DIST and SFT training lead to marked increase. These observations suggest that, unlike DIST and SFT, the reasoning trajectories reinforced by RL are not entirely newly created; rather, they correspond to latent patterns already present in the base model that can be activated. In other words, RL training primarily introduces signals at small number of critical positions, effectively activating and stabilizing these latent reasoning patterns, thereby enhancing reasoning capabilities without substantially altering the overall output distribution. 22 Preprint Figure 13: Using prefix tokens generated by the Rank-1 model to guide the base models reasoning. We analyzed the positions and characteristics of these non-greedy tokens and identified two predominant patterns in Figure 12. The first pattern corresponds to initial-answer tokens, such as \"Alright\" or \"Let\", which typically occur at the beginning of the generation and reflect the models initial understanding of the problem. The second pattern corresponds to reasoning-transition tokens, such as \"But\" or \"Wait\", which often appear at critical reasoning junctures and indicate adjustments or corrections of intermediate steps. Moreover, as shown in Figure 11 (c), injecting only the Rank-1 information significantly reduces the number of non-greedy tokens. This indicates that the Rank-1 Subspace reshapes token-level decoding preferences throughout the reasoning process. By adjusting small set of critical tokens, the Rank-1 information can activate and stabilize latent reasoning trajectories already present in the base model, gradually aligning them with the RL-enhanced reasoning patterns. To more precisely investigate where these critical adjustments exert their influence, we designed prefix token experiment. Specifically, we truncated the reasoning chains produced by the Rank-1 model to their first ğ‘› tokens and fed these prefixes into the base model to continue reasoning. As shown in Figure 13, using only the first 20 tokens - approximately the first sentence - allows the base model to approach the performance of the full parameter update. This result indicates that performance gains primarily arise from the early stage of the reasoning chain, where the problem is represented and the solution strategy is established. They further suggest that the base model inherently possesses strong reasoning capabilities, but its potential is not fully realized, partly because it fails to sample the tokens most critical for understanding the problem. By capturing this key sampling capability, the Rank-1 Subspace effectively activates and stabilizes latent reasoning trajectories, aligning the base models reasoning behavior more closely with that of the RL-trained model and thereby significantly enhancing performance. Overall, compared to DIST and SFT, RL induces only limited modifications to the models reasoning behavior. The Rank-1 Subspace accurately captures the reasoning preferences of the RL-trained modelparticularly the critical token-level adjustmentsthereby efficiently recovering reasoning capabilities that would otherwise require full-parameter RL training. Preprint Question The medians ğ´ğ·, ğµğ¸, and ğ¶ğ¹ of triangle ğ´ğµğ¶ intersect at the centroid ğº. The line through ğº that is parallel to ğµğ¶ intersects ğ´ğµ and ğ´ğ¶ at ğ‘€ and ğ‘, respectively. If the area of triangle ğ´ğµğ¶ is 144, then find the area of triangle ğ¸ ğ‘ğº. Base Model Answer: To solve the problem, we need to find the area of triangle ğ¸ ğ‘ğº given that the area of triangle ğ´ğµğ¶ is 144. Lets break down the problem step by step. Identify Key Properties: - The centroid ğº divides each median into ratio of 2 : 1, with the longer segment being closer to the vertex. - The line through ğº parallel to ğµğ¶ intersects ğ´ğµ at ğ‘€ and ğ´ğ¶ at ğ‘. Area Relationships: - Since ğº is the centroid, it divides the triangle into three smaller triangles of equal area. Each has area 144 3 = 48. - The line through ğº parallel to ğµğ¶ creates smaller triangle ğ´ğ‘€ ğ‘ that is similar to triangle ğ´ğµğ¶ with side ratio 1 3 . Area of Triangle ğ´ğ‘€ ğ‘: - Ratio of areas: (cid:16) 1 Area of Triangle ğ¸ ğ‘ğº: - By area decomposition, the final result is 16 . 9 . - So Area( ğ´ğ‘€ ğ‘) = 9 = 16. = 1 (cid:17) 2 3 RL Trained Model Answer: Alright, have this geometry problem in front of me, and need to find the area of triangle ğ¸ ğ‘ğº given that the area of triangle ğ´ğµğ¶ is 144. Lets break it down step by step. Given: Triangle ğ´ğµğ¶ with medians ğ´ğ·, ğµğ¸, and ğ¶ğ¹ intersecting at the centroid ğº. line through ğº parallel to ğµğ¶ intersects ğ´ğµ at ğ‘€ and ğ´ğ¶ at ğ‘. The area of triangle ğ´ğµğ¶ is 144. Find: The area of triangle ğ¸ ğ‘ğº. Wait, think theres typo in the problem. It mentions triangle ğ¸ ğ‘ğº, but Im not sure what point ğ¸ is. Looking back at the problem, it says the medians are ğ´ğ·, ğµğ¸, and ğ¶ğ¹, so ğ¸ is the midpoint of ğ´ğ¶. But the triangle were supposed to find the area of is ğ¸ ğ‘ğº, which would involve points ğ¸, ğ‘, and ğº. Maybe ğ‘ is the intersection of the line through ğº parallel to ğµğ¶ with ğ´ğ¶. Lets clarify that. First, lets visualize the triangle and label all the points: 1. Triangle ğ´ğµğ¶: triangle with vertices ğ´, ğµ, and ğ¶. 2. Medians: ğ´ğ·: Median from ğ´ to the midpoint ğ· of ğµğ¶. ğµğ¸: Median from ğµ to the midpoint ğ¸ of ğ´ğ¶. ğ¶ğ¹: Median from ğ¶ to the midpoint ğ¹ of ğ´ğµ. 3. Centroid ğº: The point where the medians intersect. It divides each median in 2:1 ratio. 4. Line through ğº parallel to ğµğ¶: This line intersects ğ´ğµ at ğ‘€ and ğ´ğ¶ at ğ‘. 5. Area of ğ´ğµğ¶: 144. We need to find the area of triangle ğ¸ ğ‘ğº. Step 1: Understanding the Centroid and Medians The centroid ğº divides each median in 2:1 ratio, with the longer part being between the vertex and the centroid. So, for median ğ´ğ·, ğ´ğº : ğº ğ· = 2 : 1. Similarly, ğµğº : ğºğ¸ = 2 : 1 and ğ¶ğº : ğºğ¹ = 2 : 1. Preprint Step 2: Line through ğº Parallel to ğµğ¶ We have line passing through ğº that is parallel to ğµğ¶. This line intersects ğ´ğµ at ğ‘€ and ğ´ğ¶ at ğ‘. Since the line is parallel to ğµğ¶, triangles ğ´ğ‘€ ğ‘ and ğ´ğµğ¶ are similar by the Basic Proportionality Theorem (Thales theorem). Step 3: Similarity of Triangles ğ´ğ‘€ ğ‘ and ğ´ğµğ¶ Because ğ‘€ ğ‘ ğµğ¶, triangle ğ´ğ‘€ ğ‘ is similar to triangle ğ´ğµğ¶. The ratio of similarity can be determined by the position of ğº. Since ğº is the centroid, it divides the median in 2:1 ratio. The distance from ğ´ to ğº is 2 3 of the median ğ´ğ·. The line ğ‘€ ğ‘ is parallel to ğµğ¶ and passes through ğº, so the height of triangle ğ´ğ‘€ ğ‘ from ğ´ is 2 3 of the height of triangle ğ´ğµğ¶ from ğ´. Therefore, the ratio of similarity between ğ´ğ‘€ ğ‘ and ğ´ğµğ¶ is 2 3 . Step 4: Area of Triangle ğ´ğ‘€ ğ‘ The area of similar triangles is the square of the ratio of their corresponding sides. So, the area of ğ´ğ‘€ ğ‘ is (cid:16) 2 times the area of ğ´ğµğ¶. (cid:17) 3 Area of ğ´ğ‘€ ğ‘ = (cid:19) 2 (cid:18) 2 3 144 = 4 144 = 64 Step 5: Locating Point ğ‘ Point ğ‘ is the intersection of the line ğ‘€ ğ‘ with ğ´ğ¶. Since ğ‘€ ğ‘ ğµğ¶, and ğ‘€ is on ğ´ğµ, ğ‘ divides ğ´ğ¶ in the same ratio as ğ‘€ divides ğ´ğµ. But we need to find the area of triangle ğ¸ ğ‘ğº. To do that, we need to understand the positions of points ğ¸, ğ‘, and ğº. Step 6: Coordinates Approach Maybe assigning coordinates to the points would help. Lets place triangle ğ´ğµğ¶ in coordinate plane. Let: Then: The centroid is: ğ´(0, 0), ğµ(2ğ‘, 0), ğ¶ (2ğ‘, 2â„) ğ· = (ğ‘ + ğ‘, â„), ğ¸ = (ğ‘, â„), ğ¹ = (ğ‘, 0) ğº = (cid:16) 2ğ‘+2ğ‘ 3 , 2â„ 3 (cid:17) Step 7: Equation of the Line through ğº Parallel to ğµğ¶ The slope of ğµğ¶ is: 2â„ 2(ğ‘ ğ‘) = â„ ğ‘ ğ‘ Equation of line through ğº parallel to ğµğ¶: ğ‘¦ 2â„ = â„ ğ‘ ğ‘ (cid:18) ğ‘¥ (cid:19) 2ğ‘ + 2ğ‘ Step 8: Finding Point ğ‘€ on ğ´ğµ On ğ´ğµ, ğ‘¦ = 0: 2â„ 3 (cid:18) ğ‘¥ (cid:19) 2ğ‘ + 2ğ‘ 3 = â„ ğ‘ ğ‘ 4ğ‘ 3 ğ‘¥ = , ğ‘€ = (cid:19) , 0 (cid:18) 4ğ‘ 3 25 Preprint Step 9: Finding Point ğ‘ on ğ´ğ¶ Equation of ğ´ğ¶: Intersect with line through ğº: ğ‘¦ = â„ ğ‘ ğ‘¥ 1 ğ‘ ğ‘¥ 2 = 1 ğ‘ ğ‘ (cid:18) ğ‘¥ (cid:19) 2ğ‘ + 2ğ‘ After manipulation, one finds contradictions. Lets switch method. Step 10: Vector Geometry Approach Let (cid:174)ğ´ = (cid:174)0, (cid:174)ğµ = (cid:174)ğ‘, (cid:174)ğ¶ = (cid:174)ğ‘. Centroid: Equation of line through ğº parallel to ğµğ¶: (cid:174)ğº = (cid:174)ğ‘ + (cid:174)ğ‘ 3 (cid:174)ğ‘Ÿ = (cid:174)ğ‘ + (cid:174)ğ‘ 3 + ğ‘¡ ( (cid:174)ğ‘ (cid:174)ğ‘) Intersection with ğ´ğµ : (cid:174)ğ‘Ÿ ğ´ğµ = ğ‘ (cid:174)ğ‘ gives: ğ‘¡ = 1 3 , ğ‘  = 2 3 , ğ‘€ = 3 (cid:174)ğ‘ Intersection with ğ´ğ¶ : (cid:174)ğ‘Ÿ ğ´ğ¶ = ğ‘¢ (cid:174)ğ‘ gives: ğ‘¡ = 1 3 , ğ‘¢ = 2 3 , ğ‘ = 2 3 (cid:174)ğ‘ Step 11: Coordinates of Points ğ‘€ = 2 3 (cid:174)ğ‘, ğ‘ = 2 3 (cid:174)ğ‘, ğ¸ = 1 2 (cid:174)ğ‘, ğº = (cid:174)ğ‘+ (cid:174)ğ‘ 3 Step 12: Area of Triangle ğ¸ ğ‘ğº (cid:174)ğ¸ ğ‘ = 1 6 (cid:174)ğ‘, (cid:174)ğ¸ ğ‘ (cid:174)ğ¸ğº = 1 (cid:174)ğ¸ğº = 2 (cid:174)ğ‘ (cid:174)ğ‘ 6 18 ( (cid:174)ğ‘ (cid:174)ğ‘) But Areağ´ğµğ¶ = 1 So: 2 (cid:174)ğ‘ (cid:174)ğ‘ = 144 = (cid:174)ğ‘ (cid:174)ğ‘ = 288 (cid:174)ğ¸ ğ‘ (cid:174)ğ¸ğº = 1 18 288 = 1 2 16 = 8 8 Area of ğ¸ ğ‘ğº: Final Answer: Top Rank-1 Answer: Alright, have this geometry problem in front of me, and need to find the area of triangle ğ¸ ğ‘ğº given that the area of triangle ğ´ğµğ¶ is 144. Lets break it down step by step. Understanding the Problem First, lets make sure understand all the components: Triangle ğ´ğµğ¶: triangle with vertices ğ´, ğµ, and ğ¶. 26 Preprint Medians ğ´ğ·, ğµğ¸, and ğ¶ğ¹: median of triangle is line segment joining vertex to the midpoint of the opposite side. So: ğ· is the midpoint of ğµğ¶. ğ¸ is the midpoint of ğ´ğ¶. ğ¹ is the midpoint of ğ´ğµ. Centroid ğº: The point where the three medians intersect. Its also the center of mass of the triangle. Line through ğº parallel to ğµğ¶: This line is parallel to side ğµğ¶ and passes through the centroid ğº. Points ğ‘€ and ğ‘: The intersections of this parallel line with sides ğ´ğµ and ğ´ğ¶, respectively. Area of triangle ğ´ğµğ¶: Given as 144. Goal: Find the area of triangle ğ¸ ğ‘ğº. Note that point ğ¸ is already defined as the midpoint of ğ´ğ¶, so triangle ğ¸ ğ‘ğº has vertices ğ¸, ğ‘, and ğº. Visualizing the Problem Its helpful to draw diagram. Sketch triangle ğ´ğµğ¶, mark midpoints ğ·, ğ¸, and ğ¹, draw the medians, locate the centroid ğº, and then draw the line through ğº parallel to ğµğ¶, intersecting ğ´ğµ at ğ‘€ and ğ´ğ¶ at ğ‘. (Diagram representation omitted for clarity) Properties of the Centroid The centroid divides each median in 2:1 ratio: ğ´ğº : ğº ğ· = 2 : 1, ğµğº : ğºğ¸ = 2 : 1, ğ¶ğº : ğºğ¹ = 2 : 1. Line Parallel to ğµğ¶ through ğº Since the line through ğº is parallel to ğµğ¶, triangles ğ´ğ‘€ ğ‘ and ğ´ğµğ¶ are similar. The ratio of similarity is determined by the position of ğº along the median. As ğº divides the median in 2 : 1 ratio, the height from ğ´ to ğ‘€ ğ‘ is 2 3 of the height from ğ´ to ğµğ¶. Therefore, the ratio of similarity is 2 3 . Area of Triangle ğ´ğ‘€ ğ‘ The area of similar triangles scales with the square of the ratio of corresponding sides: Area( ğ´ğ‘€ ğ‘) = (cid:19) (cid:18) 2 3 Area( ğ´ğµğ¶) = 4 9 144 = 64. Coordinates Approach To find the area of triangle ğ¸ ğ‘ğº, we assign coordinates: Midpoints: Centroid: ğµ(0, 0), ğ¶ (ğ‘, 0), ğ´(ğ‘, ğ‘) ğ· , 0(cid:17) (cid:16) ğ‘ , ğ¸ (cid:19) (cid:18) ğ‘ + ğ‘ 2 , ğ‘ 2 , ğ¹ (cid:19) (cid:18) ğ‘ 2 , ğ‘ 2 ğº (cid:19) (cid:18) ğ‘ + ğ‘ 3 , ğ‘ 3 Line through ğº parallel to ğµğ¶ (horizontal) intersects ğ´ğµ at ğ‘€ and ğ´ğ¶ at ğ‘. Equation of ğ´ğµ: ğ‘¦ = ğ‘ ğ‘ ğ‘¥ 27 Preprint Intersection with ğ‘¦ = ğ‘ 3 : ğ‘ğ‘ ğ‘¥ + ğ‘ğ‘ Equation of ğ´ğ¶: ğ‘¦ = ğ‘ ğ‘ğ‘ Intersection with ğ‘¦ = ğ‘ 3 : Thus, points of interest: ğ‘€ (cid:19) (cid:18) ğ‘ , ğ‘ 3 ğ‘ (cid:18) 2ğ‘ + ğ‘ 3 , ğ‘ (cid:19) ğ¸ (cid:19) (cid:18) ğ‘ + ğ‘ 2 , ğ‘ , ğ‘ (cid:18) 2ğ‘ + ğ‘ 3 , ğ‘ 3 (cid:19) , ğº (cid:19) (cid:18) ğ‘ + ğ‘ 3 , ğ‘ 3 Area of Triangle ğ¸ ğ‘ğº Using the shoelace formula: Area = 1 2 (cid:12) (cid:12) (cid:12) ğ‘¥1 (ğ‘¦2 ğ‘¦3) + ğ‘¥2 (ğ‘¦3 ğ‘¦1) + ğ‘¥3(ğ‘¦1 ğ‘¦2) (cid:12) (cid:12) (cid:12) Plugging in coordinates: ğ‘¥1 = ğ‘ + ğ‘ 2 , ğ‘¦1 = ğ‘ 2 , ğ‘¥2 = 2ğ‘ + ğ‘ , ğ‘¦2 = ğ‘ 3 , ğ‘¥3 = ğ‘ + ğ‘ 3 , ğ‘¦3 = ğ‘ 3 Compute: Area = 1 2 (cid:12) 0 + (cid:12) (cid:12) (2ğ‘ + ğ‘)ğ‘ + (ğ‘ + ğ‘)ğ‘ 18 (cid:12) (cid:12) (cid:12) = 1 2 ğ‘ğ‘ = ğ‘ğ‘ 36 Given Area( ğ´ğµğ¶) = 144: Therefore: Final Answer: 1 ğ‘ğ‘ = 144 = ğ‘ğ‘ = 288 Area(ğ¸ ğ‘ğº) = 288 36 = 8 8 Preprint"
        },
        {
            "title": "D Linear Projection Methods",
            "content": "Constructing Update Trajectories Motivated by prior interpretability studies (Geva et al., 2021; Meng et al., 2023), we interpret the tuple (ğ’–1, ğ›¼, ğœ1, ğ’—1) of the Rank-1 update Î” Ë†ğ‘¾ (1) as keyvalue operator. For any input ğ’‰, the Rank-1 update induces: Î”ğ’š (1) = Î” Ë†ğ‘¾ (1) ğ’‰ = ğ›¼ ğœ1 ğ’–1 ğ’—1, ğ’‰, (4) where ğ’—1 serves as the key, selecting the relevant input directions, ğ’–1 defines the value direction injected into the output space, and ğ›¼ğœ1 controls the magnitude of the update. To characterize the evolution of the dominant update direction during training, we collect the sequence of ğ’–1 vectors across ğ‘‡ checkpoints for each module: U1 = {ğ’– (ğ‘¡ ) which we refer to as the modules update trajectory. Since each ğ’– (ğ‘¡ ) resides in high-dimensional space, we first apply Principal Component Analysis 1 (PCA) to capture the top 50 principal components, retaining the most significant directions of variation. The vectors are then projected onto this 50-dimensional subspace, and t-SNE is subsequently applied to these projections to obtain two-dimensional, geometry-aware visualization of the trajectory. This procedure provides an interpretable representation of how the Rank-1 update direction evolves over the course of training. 1 }ğ‘‡ ğ‘¡=1, (5) Details of PLS regression For each module, we collect checkpoint-wise pairs, forming the set: = {(ğ’– (ğ‘¡ ) , ğ‘¦ (ğ‘¡ ) )}ğ‘‡ ğ‘¡=1, (6) where ğ’– (ğ‘¡ ) Rğ‘‘ is the Rank-1 left singular (value) vector extracted at checkpoint ğ‘¡, and ğ‘¦ (ğ‘¡ ) is the corresponding reasoning accuracy. The vectors are stacked row-wise into U1 Rğ‘‡ ğ‘‘, and each feature is standardized to zero mean and unit variance, yielding the design matrix U1. We then perform Partial Least Squares (PLS) regression with single latent component. PLS regression can be viewed as Ordinary Least Squares (OLS) applied in latent low-dimensional space: it first extracts the most predictive direction by maximizing the covariance with the response variable, and then fits the target values on this component using OLS. The resulting score vector is defined as: ğ’›1 = U1 ğ’˜1, (7) where ğ’˜1 identifies the direction in the standardized value space that is maximally predictive of accuracy. Accuracy is then regressed on this component via: with ( Ë†ğ›¼, Ë†ğ›½) estimated by OLS, i.e., by minimizing the sum of squared residuals: ğ‘¦ (ğ‘¡ ) = ğ›¼ ğ‘§ (ğ‘¡ ) 1 + ğ›½ + ğœ€ (ğ‘¡ ) , ( Ë†ğ›¼, Ë†ğ›½) = arg min ğ›¼,ğ›½ ğ‘‡ (cid:0)ğ‘¦ (ğ‘¡ ) (ğ›¼ğ‘§ (ğ‘¡ ) 1 + ğ›½)(cid:1) 2. ğ‘¡=1 The coefficient of determination is computed as: ğ‘…2 = 1 (cid:205)ğ‘‡ ğ‘¡=1 (cid:205)ğ‘‡ ğ‘¡=1 (cid:0)ğ‘¦ (ğ‘¡ ) Ë†ğ‘¦ (ğ‘¡ ) (cid:1) 2 (cid:0)ğ‘¦ (ğ‘¡ ) ğ‘¦(cid:1) 2 , Ë†ğ‘¦ (ğ‘¡ ) = Ë†ğ›¼ğ‘§ (ğ‘¡ ) 1 + Ë†ğ›½. (8) (9) (10) Here, ğ‘…2 quantifies the strength of the approximately linear coupling between the modules value trajectory and performance variation across checkpoints. In Section 4, AlphaRL perform the same computation but with the scaled vectors Ë†ğ’– (ğ‘¡ ) = ğ›¼ (ğ‘¡ ) ğœ (ğ‘¡ ) 1 ğ’– (ğ‘¡ ) instead of the raw vectors ğ’– (ğ‘¡ ) . 29 Preprint Figure 14: Heatmap of ğ‘…2 across MLP and self-attention components. Figure 15: t-SNE visualization of U1 trajectories under DAPO for Attn modules. 30 Preprint Figure 16: PLS regression visualization of U1 trajectories under DAPO for Attn modules. Preprint Figure 17: t-SNE visualization of U1 trajectories under DAPO for Attn modules. 32 Preprint Figure 18: PLS regression visualization of U1 trajectories under DAPO for Attn modules. Preprint Figure 19: t-SNE visualization of U1 trajectories under DAPO for Attn modules. 34 Preprint Figure 20: PLS regression visualization of U1 trajectories under DAPO for Attn modules. Preprint Figure 21: t-SNE visualization of U1 trajectories under DAPO for Attn modules. 36 Preprint Figure 22: PLS regression visualization of U1 trajectories under DAPO for Attn modules. Preprint Figure 23: t-SNE visualization of U1 trajectories under DAPO for MLP GATE modules. 38 Preprint Figure 24: PLS regression visualization of U1 trajectories under DAPO for MLP GATE modules. Preprint Figure 25: t-SNE visualization of U1 trajectories under DAPO for MLP UP modules. 40 Preprint Figure 26: PLS regression visualization of U1 trajectories under DAPO for MLP UP modules. Preprint Figure 27: t-SNE visualization of U1 trajectories under DAPO for MLP DOWN modules. 42 Preprint Figure 28: PLS regression visualization of U1 trajectories under DAPO for MLP DOWN modules."
        }
    ],
    "affiliations": [
        "HKUST",
        "NUS",
        "USTC"
    ]
}