{
    "paper_title": "Evaluating Intelligence via Trial and Error",
    "authors": [
        "Jingtao Zhan",
        "Jiahao Zhao",
        "Jiayu Li",
        "Yiqun Liu",
        "Bo Zhang",
        "Qingyao Ai",
        "Jiaxin Mao",
        "Hongning Wang",
        "Min Zhang",
        "Shaoping Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Intelligence is a crucial trait for species to find solutions within a limited number of trial-and-error attempts. Building on this idea, we introduce Survival Game as a framework to evaluate intelligence based on the number of failed attempts in a trial-and-error process. Fewer failures indicate higher intelligence. When the expectation and variance of failure counts are both finite, it signals the ability to consistently find solutions to new challenges, which we define as the Autonomous Level of intelligence. Using Survival Game, we comprehensively evaluate existing AI systems. Our results show that while AI systems achieve the Autonomous Level in simple tasks, they are still far from it in more complex tasks, such as vision, search, recommendation, and language. While scaling current AI technologies might help, this would come at an astronomical cost. Projections suggest that achieving the Autonomous Level for general tasks would require $10^{26}$ parameters. To put this into perspective, loading such a massive model requires so many H100 GPUs that their total value is $10^{7}$ times that of Apple Inc.'s market value. Even with Moore's Law, supporting such a parameter scale would take $70$ years. This staggering cost highlights the complexity of human tasks and the inadequacies of current AI technologies. To further investigate this phenomenon, we conduct a theoretical analysis of Survival Game and its experimental results. Our findings suggest that human tasks possess a criticality property. As a result, Autonomous Level requires a deep understanding of the task's underlying mechanisms. Current AI systems, however, do not fully grasp these mechanisms and instead rely on superficial mimicry, making it difficult for them to reach an autonomous level. We believe Survival Game can not only guide the future development of AI but also offer profound insights into human intelligence."
        },
        {
            "title": "Start",
            "content": "Jingtao Zhan1, Jiahao Zhao2, Jiayu Li1, Yiqun Liu1, Bo Zhang1, Qingyao Ai1, Jiaxin Mao2, Hongning Wang1, Min Zhang1, Shaoping Ma1 1Tsinghua University, 2Renmin University of China {zhanjt20@mails., yiqunliu@}tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "How does intelligence emerge? We propose that intelligence is not sudden gift or random occurrence, but rather crucial trait for species to survive through Natural Selection. Natural Selection requires species to find solutions within limited number of trial-and-error attempts. Building on this idea, we introduce Survival Game as framework to evaluate intelligence based on the number of failed attempts in trial-and-error process. Fewer failures indicate higher intelligence. When the expectation and variance of failure counts are both finite, it signals the ability to consistently find solutions to new challenges, which we define as the Autonomous Level of intelligence. Using Survival Game, we comprehensively evaluate existing artificial intelligence (AI) systems. Our results show that while AI systems achieve the Autonomous Level in simple tasks, they are still far from it in more complex tasks, such as vision, search, recommendation, and language. While scaling current AI technologies might help, this would come at an astronomical cost. Projections suggest that achieving the Autonomous Level for general tasks would require 1026 parameters. To put this into perspective, loading such massive model requires so many H100 GPUs that their total value is 4 107 times that of Apple Inc.s market value. Even with Moores Law, supporting such parameter scale would take 70 years. This staggering cost highlights the complexity of human tasks and the inadequacies of current AI technologies. To further investigate this phenomenon, we conduct theoretical analysis of Survival Game and its experimental results. Our findings suggest that human tasks possess criticality property. As result, Autonomous Level requires deep understanding of the tasks underlying mechanisms. Current AI systems, however, do not fully grasp these mechanisms and instead rely on superficial mimicry, making it difficult for them to reach an autonomous level. We believe Survival Game can not only guide the future development of AI but also offer profound insights into human intelligence. ... But Natural Selection, as we shall hereafter see, is power incessantly ready for action, and is as immeasurably superior to mans feeble efforts, as the works of Nature are to those of Art. ... Charles Darwin, Origin of Species, 1859. 5 2 0 2 3 ] A . [ 2 8 5 8 8 1 . 2 0 5 2 : r Corresponding author."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Work 3 Methodology"
        },
        {
            "title": "3.3 Classifying Intelligence into Three Levels . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "3.3.1 Infinity Assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.4 Approximation with Reference Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "3.4.1 Scale-Invariance Assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.2 Survival Game with References . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5 Comparison with Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Evaluation with Survival Game 4.1 Beginners Task: MNIST . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Vision . 4.3 Search . . . . . . . . . . . . . 4.4 Recommendation . 4.5 Language . . . . 4.5.1 Coding . . . 4.5.2 Mathematics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5.3 Question-Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5.4 Writing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.6 Revisiting Current AI Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Scaling in Survival Game 5.1 Fitting Failure Count Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Fitting Effect of Scaling . 5.3 Predicting Future . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Theoretical Analysis of Survival Game 6.1 Human Tasks exhibit Criticality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Modeling Survival Game as Self-Organized Criticality System . . . . . . . . . . . . . . . 7 Conclusions and Future Work Reproducibility Contributions 2 3 4 5 6 7 7 8 9 9 10 10 10 11 13 13 15 15 17 18 19 21 21 23 24 25 26 27"
        },
        {
            "title": "Introduction",
            "content": "How does intelligence emerge? We believe that intelligence is not an innate gift but rather necessity shaped by Natural Selection. The diversity of life forms we see today, including humans, animals, and plants, has addressed countless challenges imposed by the natural world. Natural Selection can be viewed as trial-anderror process, where subjects shall persistently explore, seek solutions in the face of uncertainty, and ultimately prevail. The subjects must strive to solve the challenges, experimenting again and again until they succeed. If they cannot find solution, they fail the test and thus do not survive. Inspired by Natural Selection, we propose Survival Game to evaluate intelligence. Similar to how species find way to survive through trial and error in Natural Selection, Survival Game evaluates intelligence by counting the number of failures before finding correct solutions in trial-and-error process. Fewer failures correspond to higher intelligence. The number of failures is discrete random variable, and smaller expectations and variances of the failure count indicate higher intelligence. If expectations and variances are infinite, the subjects can never find the correct solutions and thus do not survive in the game. Based on the convergence of the expectations and variances, Survival Game divides intelligence into three levels: Limited, Capable, and Autonomous. If both the expectation and variance diverge, the subject is at the Limited Level. At this level, the subject is comparable to blindly enumerating possible solutions. If both the expectation and variance converge, the subject reaches the Autonomous Level. At this level, the subject can stably find the correct solution with only few trials, thereby being able to autonomously operate at an affordable cost. As we can see, the results of the Survival Game have clear physical meaning about the subjects intelligence level. The Survival Game can be applied to any task and any species. In this paper, we are particularly interested in artificial intelligence (AI) systems. Therefore, we conduct Survival Game on state-of-the-art AI systems available today. The results demonstrate that system with better modeling of the task can reach higher level of intelligence. Current AI technologies can reach the Autonomous Level on simple tasks like handwritten digit recognition. However, they are mostly at Limited Level on more complex tasks, including vision, search, recommendation, and language. This indicates that most AI systems are at preliminary stage: they are unable to substantially narrow down the range of possible answers and their performance is comparable to brute-force enumeration. This indicates that directly applying these AI technologies can result in very high costs and serious errors, so they cannot operate autonomously, and human supervision is essential. These findings challenge conclusions from previous studies (Biever, 2023; Aharoni et al., 2024; Mei et al., 2024), which suggest that AI has already reached very high level of intelligence. Figure 1: Experimental Results of Survival Game in General Domain. Left: Results suggest that larger models achieve better performance. Right: Results suggest that achieving Autonomous-Level Intelligence requires an unimaginable parameter scale. In Survival Game, the intelligence score exhibits log-linear relationship with the scale of AI systems. If we assume this relationship continues to hold, we can predict the scale required to achieve Autonomous-Level intelligence, as shown in Figure 1. The projection suggests that, for general language tasks, an AI system would need parameter size of 1026 to reach the Autonomous Level. To put this scale into perspective, this is equivalent to 105 times the total number of neurons in all of humanitys brains combined. Loading model of this size onto H100 GPUs would necessitate 5 1015 H100 cards, cost equivalent to 4 107 times the market value of Apple Inc. If hardware development continues to follow Moores Law, it would take 70 years of progress to support the development of such large model. These results suggest that attempting to solve human tasks with current AI technology is extremely difficult, if not impossible. Why is the Autonomous Level so difficult to achieve for current AI systems? We conduct theoretical analysis and demonstrate that the root cause lies in the complexity of human tasks and the inadequacies of current AI technologies. Specifically, we leverage self-organized criticality (SOC) (Bak, 2013) to analyze Survival Game. Results suggest that many human tasks exhibit criticality property: even slight changes in the environment require entirely different responses. To successfully operate these tasks, it is important to fully understand their mechanisms. However, current AI systems do not fully grasp this complex mechanism and instead leverage superficial imitation: They memorize answers to some questions and attempt to solve new questions through exploration. Although scaling AI systems can make the exploration more effective and improve imitation performance, lack of full understanding of the underlying mechanism results in unimaginable costs. The structure of this paper is as follows. In Section 2, we review related works on intelligence evaluation to provide broad context for our method. In Section 3, we present Survival Game and show how it measures intelligence and categorizes it into three levels. In Section 4, we extensively evaluate existing AI systems using Survival Game. Section 5 empirically explores how scaling improves Intelligence. In Section 6, we provide theoretical analysis of Survival Game to gain deep understanding of the nature of human tasks and current AI. Finally, in Section 7, we conclude the paper and outline potential directions for future research."
        },
        {
            "title": "2 Related Work",
            "content": "Defining test for intelligence is fundamental issue. For AI research, it allows us to understand, apply, and develop AI technology. More broadly, it enables us to gain deep understanding of intelligence, leading to profound insight into both humanity and the natural world. In 1950, Alan Turing proposed the Imitation Game to test whether machine can possess human intelligence. Since then, it has been highly influential in the AI field. Many researchers have developed methods to practically implement or further improve Imitation Game. In this section, we review some of the most influential approaches. Imitation Game, aka Turing Test (Turing, 1950): Intelligence is the ability to imitate human responses convincingly in text-based conversation. If human evaluator cannot reliably distinguish between machine and human based on their answers, the machine is considered intelligent. Total Turing Test (Harnad, 1991): It is an extended version of the Turing Test that assesses machines ability to interact with the world in human-like way. It goes beyond text-based conversations to include physical interaction and sensory perception. Chinese Room Argument (Searle, 1999): It argues that the Imitation Game only evaluates syntactics and yet AI should also understand semantics, such as knowing the actual meaning of each word. Lovelace Test (Bringsjord et al., 2003): It argues that intelligence is about creativity. For example, AI should be able to originate art, music, or poetry. Reverse Turing Test (Baird et al., 2003): Instead of asking whether machine can act like human, it asks whether an AI can differentiate between humans and machines. Universal Intelligence (Legg & Hutter, 2007): Beyond the conversation task in Imitation Game, it measures an agents ability to achieve goals in wide range of environments. Winograd Schema Challenge (Levesque et al., 2012): It tests whether AI can identify the antecedent of an ambiguous pronoun in statement. It requires world knowledge and contextual understanding. General intelligence (Goertzel, 2014): It defines intelligence as the ability to achieve wide range of goals and handle new problems in different contexts and environments. Visual Turing Test (Geman et al., 2015): It adds the visual understanding ability to the Imitation Game. It tests whether AI can answer complex questions about images. Economical Value (OpenAI, 2018): It tests whether AI can be highly autonomous system that outperforms humans at most economically valuable work. 4 The Modern Turing Test (Suleyman, 2023): It argues intelligence is to make meaningful impact to the real world. It tests whether AI can make $1 million on retail web platform in few months with just $100,000 investment. Outperforming Humans (Morris et al., 2024): It defines different levels of intelligence by how many humans AI can outperform. For example, Competent AI outperforms 50% skilled adults and virtuoso AI outperforms 99% skilled adults. We observe that almost all previous works attempted to define intelligence by determining which human-like tasks that machine must accomplish in order to be considered intelligent. However, there is significant variation in the choice of the tasks, as different researchers hold different perspectives on what constitutes intelligence. We can see that these approach approaches are inherently subjective, which manifests in three key ways: Subjective (Human-Centric) View of Intelligence: Many of these tests utilize human intelligence as an upper bound for AI and evaluate whether AI can approach this bound. For example, Imitation Game (Turing, 1950) evaluates machines ability to replicate human behavior; OpenAI (2018) defines intelligence as outperforming humans at economically valuable work; Morris et al. (2024) defines intelligence level by how many humans AI can outperform. Nevertheless, if AI surpasses humans in certain tasks, these evaluation methods are no longer applicable. Subjective Choice of Tasks: These researchers believe that intelligence is general property rather than something tied to specific tasks. Researchers have sought to define tasks that best reflect intelligence, making these tasks increasingly complex to measure ever more sophisticated forms of intelligence. However, this approach is inherently subjective: different researchers emphasize different aspects of intelligence, preventing consensus. For instance, Harnad (1991) chooses physical tasks; Bringsjord et al. (2003) argues creative tasks; Suleyman (2023) adopts economic tasks; Morris et al. (2024) suggest cognitive tasks. The belief that intelligence is independent of tasks, yet simultaneously trying to define it through single universal task, leads to contradictions. Subjective Evaluation Framework: These tests rely heavily on subjective measures of how well an AI system imitates human behavior. However, defining what constitutes good imitation and the threshold at which intelligence emerges is highly ambiguous. For example, Winograd Schema Challenge (Levesque et al., 2012) is considered defeated because AI achieved 90% accuracy (Kocijan et al., 2023); Imitation Game (Turing, 1950) is considered defeated because current chatbot successfully fooled human evaluators 40% of the time (Biever, 2023); Modern Turing Test (Suleyman, 2023) will be defeated if AI makes $1 million. However, these thresholds are not well-defined and may differ among researchers. Since there is no universally accepted standard, the conclusions will be inconsistent. This defect makes it difficult to translate these tests into reliable evaluation methods for real-world AI applications. In contrast, our proposed Survival Game has clear physical meaning and well-defined statistical basis. It is inherently an objective way to evaluate intelligence. Before we further elaborate on the differences between Survival Game and related studies, we will first introduce Survival Game in Section 3 and then continue the comparison in Section 3.5."
        },
        {
            "title": "3 Methodology",
            "content": "We propose Survival Game, framework to evaluate intelligence via trial-and-error process. Its core concept is to test how well subject can autonomously explore and find solutions. In the following subsections, we first revisit Natural Selection and formalize it as Survival Game. Then, we extend Survival Game as Survival Game to quantify intelligence at any task. Next, we interpret the results of Survival Game into three levels of intelligence. Furthermore, we propose an approximation method to apply Survival Game in costly tasks. Finally, we discuss how Survival Game differs from previous studies. 3.1 Natural Selection as Trial-and-Error Test Natural Selection is an intuitive way to test intelligence. If subject passes Natural Selection, it signifies that this subject possesses the intelligence to operate autonomously and can sustain itself without external guidance. The process of Natural Selection is extraordinarily complex, involving competition between species, 5 genetic mutations, etc. Rather than delving into these intricate details, we simplify Natural Selection into trial-and-error test as follows: Definition 3.1 Imagine species with sufficiently large population. Its individuals stand in line outside room. sign at the entrance warns them that once inside, they will face critical question. One by one, the individual enters the room and gives answers. An incorrect answer makes the individual vanish, while correct answer lets it survive. One survivor can mark the species as having passed the test. Despite the simplification, this trial-and-error test captures the essence of Natural Selection. Throughout history, nature has posed countless challenges to humankind. When asked how to survive predators, the intelligent among us answered fire and tools. When faced with the threat of starvation, the intelligent among us developed agriculture. When confronted with disease, the intelligent among us advanced medicine. Civilization itself has been forged through these relentless trials and errors. Based on the description of the above trial-and-error process, we can translate it into mathematical terms to make it clearer. Definition 3.2 Let be the population size of species. Let represent the number of individuals who fail before the correct answer is found. takes values in the range of 0 , where = 0 means the first individual answers correctly, while = means that all individuals fail. If at least one individual succeeds, i.e., < , the species passes the game. We can see that the number of failures, X, is direct measure of species survival intelligence. The smaller the value of X, the less effort the species needs to solve problems. Inspired by this, our proposed Survival Game will similarly measure intelligence. 3.2 Measuring Intelligence with Survival Game Based on the trial-and-error process in Natural Selection, we introduce Survival Game, which evaluates intelligence by the number of failure attempts in this process. To ensure robust evaluation result, Survival Game models failure counts as discrete random variable and uses statistical metrics for evaluation. The modifications are two-fold: Modeling Failure Count as Discrete Random Variable: One task can involve numerous variations, and failure counts may be very different across these variations. For instance, consider testing subjects ability to solve mathematical problems. small change in the numbers or the context of the problem could lead to significant shift in the subjects failure counts. Similarly, when task is classifying images, different pictures can result in substantial fluctuations in performance. Therefore, the variability within the task can cause the results to be unstable. To account for this variability, Survival Game models failure count as discrete random variable, which allows us to handle the variations across task variants effectively. Statistical Criteria for Evaluation: Population size serves as threshold value in the trial-and-error test for Natural Selection. It directly affects the conclusion. The larger the value of , the more attempts are available to the subject, and consequently, the higher the likelihood of success. Yet, for tasks other than survival, the notion of what constitutes an appropriate can vary from one researcher to another. This variability in determining an appropriate leads to inconsistencies in the conclusions. Therefore, Survival Game does not use pre-defined threshold for measurement. It quantifies intelligence as the distribution of failure count. lower probability of large failure count suggests higher intelligence. With these statistical improvements, we formally define Survival Game as follows: Definition 3.3 (Survival Game) Let subject perform certain task through continuous trial and error until finding the correct solution. is random variable representing the number of failures before the subject finds the correct solution. Then, serves as the measure of this subjects intelligence on the task. Smaller expectations and variances of correspond to higher intelligence. Smaller expectations and variances indicate that the subject can achieve success with fewer failures and thus is more intelligent. This definition allows us to assess intelligence in any given task. We can choose to evaluate intelligence in narrow tasks such as answering domain-specific questions, or we can test subject across diverse and complex tasks to determine whether it exhibits general intelligence, such as memorizing every information on the Internet. The measurement of Survival Game has clear physical meaning: it signifies how well subject can reliably find solutions for given task on its own. It is worth noting that Survival Game assumes that subjects must keep trying until they succeed even if the cases are very difficult. For easy cases where subjects can answer correctly without trial and error, the contribution to the failure count is zero. We can see that Survival Game essentially ignores easy cases where subjects can answer correctly right away and instead focuses on difficult cases that require repeated trials and errors. For example, in image classification, Survival Game focuses on images that the AI model initially misclassifies. It examines how many trial-and-error attempts are needed before achieving the correct classification. This emphasis on trial and error differentiates the Survival Game from existing evaluation methods based on accuracy. In real-world applications, if task is highly sensitive to errors, such as high-risk decision-making scenarios like autonomous driving, Survival Game provides better reflection of whether an AI model can be trusted. Additionally, in highly intellectual tasks that require AI to go through trial and error to find solution, such as proving mathematical theorems or optimizing agent workflows, Survival Game metric directly corresponds to computational cost and shall better reflect AIs applicability. 3.3 Classifying Intelligence into Three Levels In this subsection, we analyze the distribution of failure counts obtained from the Survival Game to gain clear understanding of the subjects level of intelligence. First, we introduce an Infinity Assumption to define the least intelligent scenario. Based on this, we then propose three levels of intelligence. Finally, we explain how to classify subjects into these three intelligence levels based on the distribution of their failure counts. 3.3.1 Infinity Assumption What situation represents subject having almost no intelligence related to the task? Imagine scenario in an Survival Game where monkey sits in front of computer and types to see if it can produce Shakespeares works. If it deviates from Shakespeares works, we let it attempt again. The failure count refers to the number of attempts before success. The monkey has no understanding of human language and just types randomly. In theory, since the human vocabulary is finite and Shakespeares works are also of limited length, the monkey could use an enumeration method, blindly trying all possible combinations of words. Even though most of these combinations are completely nonsensical to us, the monkey can eventually type out Shakespeares works. However, this blind, exhaustive enumeration shows that the subject lacks any real intelligence. It is also disconnected from practical reality because the cost of such an exhaustive search would far exceed any reasonable resource limitations, much like how it is completely unrealistic to expect monkey to eventually produce Shakespeares works. Shakespeare did not create his works by randomly typing and waiting for greatness to emerge. Instead, he produced the masterpieces through intentional creativity within the limitations of human life. Therefore, when the failure count approaches the cost of exhaustive enumeration, it almost certainly indicates that the subject has no intelligence related to the task. We note that the high cost of blind enumeration closely resembles the mathematical concept of infinity. In mathematics, infinity describes scenario where quantity is beyond the scale we can measure or endure. For example, when measuring objects on Earth, we can assume the distance from the Sun to the Earth is infinite, and based on this assumption, we treat sunlight as parallel rays. This is because, compared to the size of objects on Earth, the distance between the Sun and Earth is so vast that it can be approximated as infinity. This allows us to use the property of parallel sunlight to help with measurement tasks. The concept of infinity in mathematics is way of thinking in terms of limits and approximations. While infinity does not directly exist in the physical world, it helps us understand and describe extremely large quantities and allows us to handle them more conveniently. In the case of the Survival Game, the characteristic of blind enumeration aligns closely with the concept of infinity. In theory, blind enumeration can eventually lead to the correct solution, but the cost of doing so far exceeds the available resources or our willingness. Therefore, we can model the cost of blind enumeration in the Survival Game as infinity and thus can better interpret the results of the test. We propose the following Infinity Assumption: Failure count approaches infinity if it approaches the cost of blindly enumerating all possibilities. In other words, failure count is finite if it is much smaller than the cost of exhaustive enumeration. Under this mathematical assumption, infinity serves as clear criterion for determining whether intelligence is present. When the failure count is finite, it indicates that the subject has excluded many possibilities in advance and is consciously engaging in trial and error, ultimately achieving success. At this point, the subject truly demonstrates intelligence in this task. This mathematical assumption allows us to clearly distinguish between different levels of intelligence. 7 3.3.2 Three Intelligence Levels The above Infinity Assumption links intelligence with infinity. It enables us to clearly define different levels of intelligence in mathematical terms. Based on this, we compare the statistical measures of failure count with infinity and define three levels of intelligence: Limited Level: subject belongs to this category if the expectation of failures is infinite: E(X) . At this intelligence level, the subject is comparable to blindly enumerating all possible outcomes. The cost for the subject to autonomously solve the task is unacceptable in real-world scenarios. It requires external supervision to improve itself and reliably operate within the task. Capable Level: subject belongs to this category if the expectation of failures is finite, but the variance remains infinite: E(X) < , Var(X) . At this intelligence level, the subject is, in principle, capable of solving the given task. However, the number of failures vary drastically across different cases. Its performance is highly unpredictable and failures can still occur frequently. As result, autonomous operation is risky, and external supervision remains necessary to ensure reliability. Autonomous Level: subject belongs to this category when both the expectation and variance of failures are finite: E(X) < , Var(X) < . Subjects at this level can reliably find solutions for the given task. They may operate autonomously without relying on external supervision. If subject reaches the Autonomous Level, it can reliably find solutions with affordable trials and errors. If we imagine that the subject will use the correct solutions as supervision signals to improve itself, the Autonomous Level implies that the subject no longer requires external supervision to provide correct answers. Instead, it can rely solely on their attempts to find the solution. In this way, the subject can independently generate supervision data and improve itself to further reduce the failure counts. In AI, this process is similar to reinforcement learning, where the system autonomously explores solutions and uses the results to update itself. If the subject has not reached the Autonomous Level, it is almost infeasible to find solutions on its own. More precisely, subjects at the Limited Level require an infinite number of attempts, which is completely beyond reasonable limits, while subjects at the Capable Level are very unstable in finding the solution. These factors make it challenging for the system to autonomously explore solutions and instead necessitate external supervision. 3.3.3 Decay Rate Classification Before presenting how to practically determine intelligence levels, let us revisit the Infinity Assumption. Although infinity does not exist in the physical world, this does not prevent us from treating certain quantities, which far exceed our capacity to measure or endure, as if they were infinite. In the case of the Survival Game, the total number of possible solutions is finite, but as described in the Infinity Assumption, we lack the resources or willingness to blindly enumerate all of them. Therefore, the Infinity Assumption treats the number of possible solutions as if it were infinite. Based on the Infinity Assumption, the distribution of the failure count can be seen as extending from 0 to infinity. Therefore, we can assess the convergence of the expectation and variance according to the distribution of the failure count. Note that the convergence of expectation and variance is determined by the tail behavior of the probability density function. Let be discrete random variable, and (X) be the discrete probability density function. The convergence of E(X) and Var(X) completely rely on how fast (X) decays at the tail. Only if (X) is sufficiently small for big values will the expectation and variance be finite. Since the decay rate of failure count determines the convergence of its expectation and variance, it also directly determines the subjects intelligence level. In this way, we connect the intelligence level to the decay rate of failure count. To examine the decay rate, we introduce power law as reference distributions for comparison. Power law 1/xα has the following properties: When α 2, both expectation and variance are infinite. When 2 < α 3, expectation is finite but variance is infinite. When α > 3, both expectation and variance are finite. Therefore, we compare the decay rate of failure count (X) with x2 and x3, and propose the following classification method to determine the intelligence level: If (X) decays more slowly than x2, both expectation and variance are infinite. The subject is at the Limited Level. 8 Figure 2: Decay Rate Classification: Log-log plot of failure counts (x-axis) vs. probability (y-axis). The intelligence level is determined based on which region the distribution of the subject falls in. If (X) decays faster than x2 but more slowly than x3, expectation is finite but variance is infinite. The subject is at the Capable Level. If (X) decays faster than x3, both expectation and variance are finite. The subject is at the Autonomous Level. practical way to visualize this comparison is to plot (X) alongside these two reference power-law functions on log-log scale. On such plot, the reference functions appear as straight lines, allowing for an intuitive comparison of decay rates. As shown in Figure 2, the two reference distributions divide the graph into three distinct regions, corresponding to Limited Level, Capable Level, and Autonomous Level, from top to bottom. We can easily determine the intelligence level of the subject by examining which region (X) falls in. 3.4 Approximation with Reference Answers Note that Survival Game requires to determine whether each attempt made by subject is correct. Yet verifying correctness for every attempt can be expensive in some tasks. Consider task where the test subject is to prove mathematical theorem. The subject provides proof with each attempt. However, for complex mathematical theorems, the proofs can be very long and intricate, and the cost of verifying the correctness of each proof is extremely high. In such cases, directly applying Survival Game may make the process prohibitively expensive. Therefore, we propose an approximation method to address this problem. We will first introduce the underlying assumption and then formalize the approximation method. 3.4.1 Scale-Invariance Assumption In situations where it is difficult to verify the correctness of each trial, we can adopt an alternative approach: counting the failure attempts before the subject arrives at predefined reference answer. For example, when testing whether subject can prove mathematical theorems, we do not evaluate whether each of its outputs constitutes valid new proof. Instead, we check whether it can produce known proof. To support the validity of such reference-based evaluation, we propose Scale-Invariance Assumption: The number of failures before finding any solution follows power-law distribution. The number of failures before finding any solution is linearly related to the number of failures before finding particular solution. Under the given assumption, we can prove that the number of failures before finding any solution and the number of failures before finding particular solution have the same failure decay rate. Therefore, they yield the same result in our Decay Rate Classification. The proof is as follows: (X = x) = Cxα, xmin. (1) (kX = x) = (X = x/k) = C(x/k)α = Ckαxα xα. (2) Thus, the power-law distribution retains its functional form under linear scaling. If the failure count follows the power law, linear transformation does not change the power-law formulation and the exponent. The first part of the Scale-Invariance Assumption is empirically and theoretically supported. Specifically, Section 5 will show that the failure count is close to power-law distribution. In Section 6, we theoretically 9 analyze the cause of such phenomenon. We demonstrate that this is because human tasks exhibit criticality property. The second part of the Scale-Invariance Assumption requires further investigation. Whether it is linearly correlated depends on the relationship between references, tasks, and subjects. In our experiments, we use human-written answers as references. For example, when assessing whether the model can write mathematical proofs, we use human-written proofs as references. When evaluating whether the model can generate highquality legal opinions, we use legal opinions written by human judges. Similarly, when assessing whether the model can produce excellent literary works, we use human literary works as references. In these cases, we assume that the number of failed attempts to arrive at feasible solution is linearly related to the number of attempts to reach these reference solutions. We have not verified its correctness for now and will verify it in the future. 3.4.2 Survival Game with References Based on the Scale-Invariance Assumption, we propose variation named Survival Game with References. It avoids the need for direct correctness verification while keeping the core of Survival Game. In those tasks where the cost of correctness verification is high, it uses reference answer and measures the number of failed attempts before producing the reference answer. The validity of this method is supported by the following theorem: Theorem 3.4 (Survival Game with References) Let be discrete random variable representing the number of failure attempts before finding predefined reference answer. is an upper bound estimation of the real failure counts. If the Scale-Invariance Assumption holds, the failure decay rate of is accurate. This approach eliminates the need for verifying every attempt and instead examines failure counts until reaching known reference answer. It is low-cost realization of Survival Game and reflects an upper bound of the subjects errors. If the Scale-Invariance Assumption holds, this theorem shows that we can exactly evaluate the intelligence level in an efficient way. 3.5 Comparison with Related Work Since we have introduced Survival Game, we can pick up our discussion from Section 2. In contrast to the subjective tests in prior studies, Survival Game provides an objective way to evaluate intelligence: Objective (Species-Agnostic) View of Intelligence: We define intelligence not by its similarity to humans, but by the ability to pass test akin to Natural Selection. Any entity that can independently find solutions demonstrates intelligence, regardless of whether it is human, artificial, or another species. Even humans may not necessarily be at the Autonomous Level in some tasks, and the test is always applicable no matter whether AI surpasses humans. Objective Choice of Tasks: We recognize that intelligence is inherently task-dependent. Unlike previous approaches that attempt to define universal intelligence, Survival Game does not prescribe any specific task. Instead, it allows researchers to evaluate intelligence in any task of interest, ensuring that the definition of intelligence remains grounded in the actual demands of given task. Objective Evaluation Framework: The Survival Game is mathematically well-defined and does not rely on any hyperparameters. Its conclusions are based on clear statistical criteria rather than subjective assessments. This ensures that evaluations remain consistent across different studies and applications, making it robust and practical tool for assessing intelligence in real-world settings. It is important to note that while we argue that intelligence is inherently task-dependent and should be evaluated within specific tasks, this does not prevent researchers from using Survival Game as framework for assessing Artificial General Intelligence (AGI). From our point of view, an AGI system should reach the Autonomous Level in at least every basic human task. Therefore, to evaluate general intelligence, researchers can construct diverse set of tasks and apply Survival Game on each of them."
        },
        {
            "title": "4 Evaluation with Survival Game",
            "content": "In this section, we evaluate state-of-the-art AI systems with Survival Game. We adopt wide range of tasks, including vision, search, recommendation, and language. 10 Figure 3: Experimental Results of Survival Game in handwritten digit recognition task (MNIST). The red line is power law curve drawn based on the distribution of the models data points. Results suggest that system reaches higher-level of intelligence if it better models the task. Quantify AIs failures: We calculate the number of failures based on the scores output by the AI system. More precisely, for given task, existing AI systems output score for each potential answer. For instance, an image classification model assigns score to each class; search engine model predicts relevance score for each document; recommendation system assigns score to each product; and language model outputs score to each word. higher score represents higher possibility the AI system predicts that this is the correct answer. We rank the answers based on the models output score from highest to lowest. This ranking list is the models attempt sequence, and the failure count equals the position of the reference answer minus one. The following presents the evaluation results of these models across various tasks. We will see that current models only reach the Autonomous Level in simple tasks and are at the Limited Level in most complex tasks. At the end of this section, we revisit existing AI techniques and show that these techniques are exactly developed in the context of Limited-Level intelligence. 4.1 Beginners Task: MNIST MNIST (Deng, 2012) is handwritten digit recognition task. It consists of collection of images depicting the digits 0-9, written by different people. The task is for an AI system to correctly identify the digit in each image. Many people consider MNIST to be relatively simple task, and it is often used as an introductory challenge for beginners to experiment with various AI algorithms. As such, we also start with this task to test whether Survival Game can effectively distinguish different types of AI algorithms. We used three AI algorithms: linear classifier, multilayer perceptron (MLP) classifier (Haykin, 1994), and convolutional neural network (CNN). Neither the linear classifier nor the MLP classifier takes into account the specific characteristics of the task; they both flatten the 2D image into 1D vector and perform transformations on this vector to do the classification. The transformation for the linear classifier is linear, while the MLP classifier introduces non-linear activation functions. In contrast, CNN is equipped with deeper understanding of images. It uses convolution to capture local features and employs multiple layers to extract abstract semantic information. Therefore, from the perspective of task modeling, CNN performs more in-depth modeling compared to both the MLP and the linear classifier. We train the three models on MNISTs training data. To ensure the stability of the results, we run the experiments with 10 different random seeds and then average the failure count distribution. The experimental results are shown in Figure 3. The blue dots are failure count distribution, and the red line is power law for reference. The gray, green, and yellow regions represent Limited Level, Capable Level, and Autonomous Level, respectively. As we can see, Survival Game clearly distinguishes these three different methods: The linear classifier falls within the Linear Level region, the MLP classifier falls within the Capable Level region, and CNN approaches the Autonomous Level region. Therefore, the better the system models the task, the higher its intelligence level. This suggests that the Survival Game is effective at evaluating the intelligence level of an AI system. 4.2 Vision In this subsection, we test whether current AI models can effectively execute complex vision tasks. We select two types of tasks for evaluation. The first is an image classification task. Given an image, the model needs to identify what animal or object is present and categorize it appropriately. For this task, we use widely Figure 4: Experimental Results of Survival Game in Computer Vision. The three rows correspond to three different datasets. Figures in different columns correspond to different models. The red line is power law curve drawn based on the distribution of the models data points. Its exponent roughly represents the models failure decay rate. Results show that models are at Limited Level. recognized dataset, ImageNet-1K (Deng et al., 2009). The second task is more complex: given natural language description, the model should find the corresponding image from large set of images. Compared to image classification, this task requires the model to understand the meaning of long natural language description and have deeper understanding of complex images. We use two popular datasets for this task: MS COCO (Lin et al., 2014) and Flickr30k (Plummer et al., 2015). We evaluate state-of-the-art AI models currently available in the field. In the first image classification task, we use CLIP model (Radford et al., 2021) and MAE models of various sizes (He et al., 2022). CLIP is widely used for visual tasks, such as text-to-image generation. The MAE models are among the bestperforming on ImageNet. For the second task, we select several top-performing models from the relevant task leaderboard (Ilharco et al., 2021), namely DFN-VIT-L, ConvN-XXL, and EVA01-G. These models are not only large in parameter size but also in the size of the training data. They represent the best models in the field. The experimental results are shown in Figure 4. The first row shows the results of the image classification task, with different images corresponding to different models. We can see that all models are at Limited Level. As we use larger MAE model, the decay rate increases and data points gradually approach the Capable Level. In the two subsequent rows, we show the results for the MS COCO and Flickr30k datasets. Different images in the same row correspond to different models. The results show that even the most advanced models today are at Limited Level, with decay rates around 1.7 or below, far from Capable Levels threshold of 2. We can also see similar trend as observed in the first row: the larger the model, the closer it is to the Capable Level. But the marginal improvements diminish gradually. The fact that these models are at Limited Level points to clear physical meaning: if these models are to find out the answers to vision-related task when they are wrong in the first place, the model would, in statistical terms, need an infinite number of guesses to get it right. In other words, the model not only makes incorrect predictions but also regards the correct answer as completely wrong. If it tries to solve the task, it will try many incorrect answers before finally outputting the correct one. Therefore, we should not place blind trust in visual models results. Instead, we should provide supervision and guidance to ensure their reliability. 4.3 Search Next, we evaluate the performance of text search models. Text search should be familiar to many people. It has widespread applications in search engines like Google, Bing, and Baidu. Given query, the text search model ranks the candidate documents in order of relevance from highest to lowest. We regard this ranking list as its attempt sequence when applying Survival Game. We use diverse range of datasets. We synthesize basic dataset so that readers can have better understanding of the task. We use Wikipedia as the raw data and construct text search task with its titles and documents. Given title, the search model ranks all the documents and should put the corresponding document at the top of the ranking list. The number of failure attempts is equal to how many incorrect documents are ranked higher than the correct ones. Besides this synthetic dataset, we also use many real-world search datasets. We adopt two web search datasets, MS MARCO (Bajaj et al., 2016) and T2Ranking (Xie et al., 2023). The former is in English and the latter is in Chinese. Both were derived from real user queries on search engines. They are widely used to benchmark the effectiveness of text search models. We also use datasets from finance domain and social platforms: FiQA (Maia et al., 2018), CqadupStack (Hoogeveen et al., 2015), and Quora (Iyer et al., 2012). FiQA requires the model to find the relevant answers to financial questions. CqadupStack and Quora are released by StackExchange and Quora social platforms, respectively. Given query, they require models to find duplicate queries. We use three distinct search models for evaluation. The first is BM25 (Robertson & Jones, 1976), popular model that was proposed decades ago. It is based on exact match and term frequency weighting. We implement it with Anserini toolkit (Yang et al., 2017). The second is dense retrieval (Karpukhin et al., 2020; Reimers & Gurevych, 2019), which represents both the query and the documents as semantic vectors and ranks them based on vector similarity. We use two open-sourced models from BGE (Xiao et al., 2023) since they are top performers on the related leaderboard. The two models vary in size, and we denote them as DR Small and DR Base. The third is cross-encoder (Nogueira & Cho, 2019), which takes both the query and the document as input and uses attention mechanisms to model their interaction. In this way, it captures more nuanced matching signals and predicts relevance more accurately. We use two strong open-sourced models. On the English dataset, we use MiniLM cross-encoder (Reimers & Gurevych, 2019). On the Chinese dataset, we use BGE cross-encoder (Xiao et al., 2023). The experimental results are shown in Figure 5. The first two rows show the performance of the Wikipedia synthetic dataset and the English web search dataset, respectively. The third row shows the performance on other datasets. We can see that on all datasets and for all text search models, the performance remains at the Limited Level. On the synthetic Wikipedia dataset, the current models performance is close to the Capable Level. On other real-world datasets, the models are far from the Capable Level. Besides, from the results in the first two rows, as the models become larger and more complex, their decay rate increases and data points move closer to the Capable Level. Limited Level has clear physical meaning in the text search scenario: when user submits query and the right document is not ranked at the top, the right document is likely to be ranked at the end of the list. The user needs to read, in statistical terms, infinite irrelevant documents before reaching the document they are looking for. In other words, when search model makes mistake, it is almost completely unable to correct itself. This highlights the complexity of the text search task and the inadequacy of current search technologies. It inspires us that we cannot simply rely on search engines to seek information. 4.4 Recommendation After examining the results of search engines, lets turn our attention to another widely used AI application: recommendation systems. Recommendation systems predict what user likes based on past behavior and profile information. These systems have extensive applications in areas such as e-commerce, short videos, etc. We adopt many real-world datasets from wide range of domains. We use the Amazon Beauty dataset (He & McAuley, 2016) to represent users preference in e-commerce recommendations. It focuses on skincare product recommendations on the Amazon platform. We use MovieLens (Harper & Konstan, 2015) to represent movie recommendations. It is constructed based on user ratings of movies. We use Steam dataset (Kang & McAuley, 2018) to represent users preference in game recommendations. It recommends games to players on the Steam platform. We use Douban Book (Zhu et al., 2020, 2019) to represent book recommendations. Douban is popular Chinese internet platform and this dataset is to recommend books to users. We use Douban Music (Zhu et al., 2020, 2019) to represent users preference in music recommendations. It is also collected from the Douban platform and is to recommend music to users. Finally, we use Gowalla dataset (Cho et al., 13 Figure 5: Experimental Results of Survival Game in Text Search. The first two rows show performance on the synthetic Wiki task and the web search task. The final row shows performance of the cross-encoder on another four tasks. The red line is power law curve drawn based on the distribution of the models data points. Its exponent roughly represents the models failure decay rate. Results indicate that all models are at the Limited Level. 2011) to represent location recommendations. Gowalla is location-based online social network application where users share their check-in location. The dataset is to recommend places users might like to visit. We test four widely recognized recommendation methods. The first is popularity-based recommendation method. As the name suggests, it ranks items based on their popularity and recommends them accordingly. Although it is straightforward, it is effective and commonly used in real-world applications. The other three methods are sequential recommendation models: GRU4Rec (Hidasi et al., 2016; Hidasi & Karatzoglou, 2018), SASRec (Kang & McAuley, 2018), and ComiRec (Cen et al., 2020). They differ in architecture. GRU4Rec employs recurrent neural networks to build user profiles based on the interaction history. SASRec uses attention mechanisms to model how past interactions influence future preferences. ComiRec captures users diverse interests with dynamic modeling approach. The experimental results are shown in Figure 6. The first row shows product recommendations, and the second row shows movie recommendations. The third row shows the performance of SASRec across different domains. According to the results, on all datasets and for all models, data points fall within the Limited Level region and are far from the Capable Level region. The estimated failure decay rate is even lower than 1, meaning that the distribution of failure counts has very heavy tail. In other words, the recommendation system has to try lot of times before finding the item users like. We believe this poor performance originates from the nature of recommendations. Recommendations do not require explicit input from users and rely solely on historical interactions. Such lack of explicit information input makes predictions very difficult. This result has clear physical meaning: When recommendation model makes mistake, it is almost impossible for it to find the correct product through continuous attempts. For users, it means that users will see, in statistical terms, infinite uninterested items before being presented with something they are truly interested in. If user is disappointed every time they see an item they are not interested in, the current recommendation system will disappoint them countless times. 14 Figure 6: Experimental Results of Survival Game in Recommendation System. The first two rows show performance on Product Recommendation and Movie Recommendation. The final row shows the performance of SASRec on another four recommendation tasks. The red line is power law curve drawn based on the distribution of the models data points. Its exponent roughly represents the models failure decay rate. Results indicate that models are at Limited Level. 4.5 Language We have assessed AI models in vision, search, and recommendation tasks. Now, we proceed to language tasks. Some studies claim that large language models have already achieved exceptionally high-level intelligence and passed the Turing Test (Biever, 2023; Aharoni et al., 2024; Mei et al., 2024). With Survival Game, we can examine their intelligence levels and re-think this conclusion. We will use four tasks for comprehensive evaluation, including coding, mathematics, question answering, and writing. Experimental Setup: We input the question to large language models and examine the models correctness in predicting the answer. The answer written by humans is regarded as the correct one. If the answer contains more than one word, such as writing math proof or long passage, we concatenate the question and the first answer words as the models input and evaluate the performance in predicting the + 1-th answer word. The number of failure attempts equals the number of words that are scored higher than the correct one. For datasets where the answers need to follow fixed format, such as multiple-choice questions or calculating number, we provide several examples before the actual question to prompt the model about the required answer format. If we use examples, we will indicate that this is an m-shot result in the figure title. This approach helps the model respond in the specified format and improves accuracy (Brown et al., 2020). We evaluate state-of-the-art large language models, including Qwen2.5 series (Yang et al., 2024a,b), Deepseek V2 16B (DeepSeek-AI, 2024), and Llama3 72B (Dubey et al., 2024). They are state-of-the-art models at their scale and are even competitive compared to models with much larger scale (Guo et al., 2025). Although we cannot run models with more parameters due to limited resources, we will extrapolate our results to larger scale in Section 5. 4.5.1 Coding We test models ability to write code. Code has clear structure, which makes it easier to predict compared to natural language. We use three widely recognized coding benchmarks. All three are designed for beginner-level programming tasks. The first is HumanEval (Chen et al., 2021a). It provides the function signature as well Figure 7: Experimental Results of Survival Game in Coding. The three rows correspond to three datasets, and figures in different columns correspond to different models. The red line is power law curve drawn based on the distribution of the models data points. Its exponent roughly represents the models failure decay rate. The results indicate that models are approaching Capable Level. as docstring and requires subjects to write the function body. The second is MBPP (Austin et al., 2021). It requires subjects to write functions based on natural language description. Answers for both HumanEval and MBPP are function definitions. The third is CRUXEval (Gu et al., 2024). It requires subjects to understand function and infer its output for given input. The answer is usually code object, such as string or list. The experimental results are shown in Figure 7. The three rows present results on HumanEval, MBPP, and CRUXEval, respectively. We can see that models with more parameters are closer to the Capable Level. For 70B models, few data points are already within the Capable Level region, yet long tail of data points still falls at the Limited Level region. Therefore, although current models are relatively strong and approaching the Capable Level in coding, they are mostly at the Limited Level. It means that they cannot reliably find correct solutions through trial and error for basic coding questions. Thus, human supervision is essential. 4.5.2 Mathematics Next, we test models in another structured domain, namely mathematics. We use three popular datasets. The first is CMath (Wei et al., 2023). It is Chinese dataset that focuses on elementary school-level math problems. It requires subjects with the ability of addition and subtraction. The second is GSM8K (Cobbe et al., 2021). It is an English dataset with similar problems to CMath. For these two datasets, the correct answer that the model needs to output is number, usually no more than two digits. The third is MATH competition dataset (Hendrycks et al., 2021). It contains complex math problems derived from math competitions. The answers to these math problems are usually long text, such as mathematical proof or the step-by-step process of solving the problem. The experimental results are shown in Figure 8. From top to bottom, the rows correspond to CMath, GSM8K, and MATH. We can see that on the first two datasets, models are far away from the Capable Level. Thus, current models can hardly perform basic addition and subtraction. In contrast, results in the third row suggest that models are relatively strong in Math Competitions. The data points are approaching the Capable Level. In summary, the models have difficulty solving simple elementary school math problems, yet they perform much better on complex competition-level math problems. This reflects significant difference between AI 16 Figure 8: Results of Survival Game on Mathematics. The first two rows test the model on simple addition and subtraction of two-digit numbers, while the last row tests whether the model can provide the solution process for math competition problem. The red line is power law curve drawn based on the distribution of the models data points. Its exponent roughly represents the models failure decay rate. Results suggest that models are better at reasoning through complex problems than performing simple addition and subtraction. and human intelligence. Besides, we also observe that as the model size increases, there is clear trend of moving closer to the Capable Level. Therefore, we should exercise caution when using large language models to solve mathematical problems. Although they might solve some complex math questions, they still make significant errors on basic math problems that are easy for humans. In general, current models are at Limited Level. This means that they require large amount of trials before finding the correct solutions. Thus, it is always necessary to validate their outputs. 4.5.3 Question-Answering Next, we examine the models ability in the Question Answering (QA) task. We select three widely used datasets: MMLU-Pro (Wang et al., 2024), Natural Questions (NQ) (Lee et al., 2019; Kwiatkowski et al., 2019), and Trivia QA (Joshi et al., 2017). MMLU-Pro consists of multiple-choice questions across various fields such as mathematics, chemistry, law, etc. The model needs to choose one answer from ten options. NQ is dataset of real-world questions about factual information. TriviaQA is similar to NQ. Answers in both datasets are only several words long. The experimental results are shown in Figure 9. The three rows represent MMLU-Pro, NQ, and TriviaQA, respectively. From the results, we observe that all four models are at Limited Level. In MMLU-Pro, the models failure decay rate is less than 1. In NQ and TriviaQA, the performance is slightly better than in MMLU-Pro, but the models are still far from reaching the Capable Level. Furthermore, we can see that as the model size increases, the decay rate also increases, gradually moving toward the Capable Level. However, the marginal gains diminish: there is significant improvement when going from 0.5B to 16B, but then the progress slows down. This suggests that the improvement is sublinear with respect to model size. Results reflect that question-answering systems can make serious mistakes. In some cases, the systems regard the correct answer as completely incorrect. As result, we cannot fully trust current question-answering systems, and it is crucial to verify the accuracy of their outputs. 17 Figure 9: Experimental Results of Survival Game in Question-Answering. The three rows correspond to three datasets, and figures in different columns correspond to different models. The red line is power law curve drawn based on the distribution of the models data points. Its exponent roughly represents the models failure decay rate. Results indicate that all models remain at Limited Level. 4.5.4 Writing Now, we evaluate general writing ability. Based on many human-written articles, we examine whether current systems can also write like humans. During the evaluation, we use the first words as the input and examine whether subjects can accurately predict the + 1-th word. The number of failure attempts equals the number of words scored higher than the + 1-th word written by humans. To ensure the model has sufficient context to make its prediction, we only consider cases where the input prefix is long enough, such as when 1, 000. Since those AI systems are trained on large amounts of human data to mimic humans writing, we believe it is appropriate to adopt humans next token as reference. First, we test model performance in different domains. Domains include Wikipedia, code (from Github), patent backgrounds, scientific papers (from ArXiv), medical articles (from PubMed), community QA, and legal texts. Most of the domain data is from Pile (Gao et al., 2020). For the legal domain, we use legal texts from France, China, and the US. China and France follow civil law systems with codified legal texts, and we examine whether models accurately memorize them. The data is from Naudet (2024) and Wang (2023). The US follows common law system, and the test examines whether models can write legal opinions by federal and state courts. The data is from the FreeLaw subset in Pile (Gao et al., 2020). The experimental results are shown in Figure 10. The first row compares the performance of different models on Wikipedia, while the second and third rows show results on other domains. We can see that all models are at Limited Level. The first row illustrates that as model size increases, the slope of the performance curve becomes steeper and the data points move closer to the Capable Level region. Besides, we notice that on the French and Chinese law datasets, models are also at the Limited Level even though the two datasets simply test their memorization ability of regulations. This suggests that memorizing legal texts is not as simple as it sounds. Overall, current models are still in the early stages in terms of writing. It is important to carefully validate their outputs. Next, we examine whether this result holds across different languages. We test the language modeling capabilities in English, Chinese, Spanish, German, French, Japanese, Italian, Portuguese, and Polish. We use the C4 dataset (Raffel et al., 2020), which consists of large number of articles from the internet and already 18 Figure 10: Results of Survival Game for Writing in Different Domains. The first row shows language modeling results on Wikipedia. The other two rows present QWen2.5 72Bs writing performance in other domains. The red line is power law curve drawn based on the distribution of the models data points. Its exponent roughly represents the models failure decay rate. Results indicate that all models are at Limited Level. categorizes them into different subsets based on the languages. This dataset is commonly used to train and test large language models. The experimental results are shown in Figure 11. The first row shows the performance of different models in Polish. The second and third rows show the results for other languages. We can see that across all languages, models are at the Limited Level. It further demonstrates that current language models are at the Limited Level regardless of the language they use. 4.6 Revisiting Current AI Techniques In previous subsections, we evaluate current AI systems in areas such as vision, search, recommendation, and language. We can see that AI remains at the Limited Level. Although this insight was not widely recognized before this study, we find that it has already profoundly impacted existing AI technologies. In other words, current AI technologies are exactly developed in the context of Limited-Level intelligence. We begin by establishing connection between AI technology and Survival Game through the concept of loss. Loss plays crucial role in AI, especially in deep learning, as it quantifies the degree of error made by an AI system. The smaller the loss, the more advanced the AI system is considered. We can see that the concept of loss is very similar to the concept of failure count in Survival Game, where failure count quantifies the extent of subjects errors. The smaller the failure count, the more intelligent the subject is considered. Therefore, we can treat failure count as form of loss, which we will refer to as Survival Game Loss in this subsection. Survival Game Loss has strong physical meaning and naturally reflects the performance of AI systems. Yet, most advanced AI systems are stuck at the Limited Level and Survival Game Loss diverges, making directly adopting this loss infeasible. In the following, we demonstrate that many current AI technologies are profoundly related to the divergence of Survival Game Loss, even though these technologies were not explicitly designed or used with this awareness in mind. Figure 11: Results of Survival Game for Writing in Different Languages. The first row shows language modeling results in Polish. The other two rows present QWen2.5 72Bs writing performance in different languages. The red line is power law curve drawn based on the distribution of the models data points. Its exponent roughly represents the models failure decay rate. Results show that all models lie withat Limited Level. Hard Negative Sampling: Hard negative sampling is widely used optimization technique in many AI fields, including vision (Shrivastava et al., 2016), search (Zhan et al., 2021), recommendation (Ding et al., 2020), and language (Kalantidis et al., 2020) tasks. It penalizes the models top-k most incorrect predictions (i.e., hard negatives), rather than punishing all of its wrong predictions (i.e., random negatives). Researchers explain its effectiveness with various hypotheses, such as increasing gradient magnitudes (Xiong et al., 2021), bootstrapping the training data (Shrivastava et al., 2016), simulating an easy-to-hard curriculum learning process (Chen et al., 2021b), etc. However, from the perspective of Survival Game Loss, its effectiveness becomes easy to understand. Since Hard Negative Sampling focuses on top-k errors, its loss can be seen as min(Survival Game Loss, k). By truncating Survival Game Loss with k, this approach ensures convergence. This truncation operation gives up on the difficult cases where models fail more than times before finding the correct solutions. It only optimizes performance in easy cases where the failure count can be smaller than k. Ignoring poor performance in difficult cases enables the model to focus on improving accuracy in simple cases. This approach is effective for Limited-Level intelligence, but if the model could reach the Autonomous Level, there would be no need to ignore difficult cases, and this method would not be so effective. An interesting story about hard negative sampling is that we studied its effectiveness years ago and demonstrated its effectiveness in ignoring difficult cases (Zhan et al., 2021). Yet, it is only now that we realize how deeply it relates to the essence of intelligence. Cross-Entropy Loss: Cross-entropy loss is commonly used loss function, widely applied across tasks such as vision (Oord et al., 2018; Radford et al., 2021) and language (Radford et al., 2019; Izacard et al., 2021). Earlier researchers provided heuristic explanations for its effectiveness, such as making the neural networks embedding distribution more uniform (Wang & Isola, 2020) and automatically weighting different negatives (Chen et al., 2020). Yet, from the perspective of Survival Game Loss, its role becomes clearer. It can be seen as using log(Survival Game Loss) as the loss. log transformation leads to better convergence. For instance, if Survival Game Loss follows power-law distribution with an exponent between 1 and 2, its expectation does not converge, but the log transformation does. In this way, Cross-Entropy loss helps address 20 Figure 12: Distribution of Failure Count in Language Tasks. Failure count ranges from 10 to 100. Results show that data closely follows power law across different domains and model sizes. the divergence of Survival Game Loss and thus makes the training process more effective. Nevertheless, if models were at the Autonomous Level, it would not be so effective since the loss would already be convergent. Reinforcement Learning (RL): RL (Kaelbling et al., 1996) lets AI systems explore solutions themselves and rewards them when they succeed. It is similar to how animals learn. However, its application is limited because the training cost is prohibitively high (Dulac-Arnold et al., 2021). We can explain this high training cost based on the convergence of Survival Game Loss at Limited Level. The cost of RL is closely related to the number of failed attempts, which is indeed Survival Game Loss. Therefore, the cost is infinite at Limited Level, making RL infeasible. Recently, DeepSeek-R1 (Guo et al., 2025) shows that RL can be applied in mathematical and coding tasks. This is because current advanced models are approaching the Capable Level in the two areas, as reflected by our previous experiments in Section 4.5.1 and 4.5.2. Models at Capable Level are more likely to find correct answers and their cost in the RL process is much lower. Yet for many other tasks, such as writing, current models are still far from the Capable Level, making RL difficult to apply. We can see that Survival Game provides deep insights for understanding AI techniques. Although earlier researchers mainly designed algorithms based on heuristics without the knowledge of this test, Survival Game effectively reveals the fundamental reasons behind their success. With the guidance of Survival Game, we believe researchers will easily design more advanced AI techniques in the future."
        },
        {
            "title": "5 Scaling in Survival Game",
            "content": "In this section, we make predictions about the model size needed to achieve the Autonomous Level in language tasks. We begin by introducing the empirical relationship between model size and decay rate in Survival Game. Using this relationship, we will extrapolate to even larger model sizes, making predictions about the future trajectory of AI development. 5.1 Fitting Failure Count Distribution In this subsection, we aim to quantitatively characterize the performance of different models in Survival Game. Take look at the experimental results from the previous section, such as Figure 10 and 11. The distribution of failure count is close to straight line in log-log plot, especially for failure count between 10 and 100. When the failure count exceeds 100, the data points become scattered. When the failure count is less than 10, 21 Figure 13: Impact of Model Size on Failure Decay Rate in Language Tasks. The x-axis represents model size and the y-axis represents failure decay rate, both on log scale. The results show that the relationship between model size and failure decay rate approximates straight line on log-log plot. the distribution is curve that bends downward with an increasingly steeper slope. Since straight line in log-log plot suggests power law distribution, this observation suggests that the failure count distribution can be approximated by power law, especially when the failure count is neither too small nor too large. Therefore, we use the power law to fit the distribution of the failure count and directly use the exponent obtained from the power law fit as the subjects decay rate. In this way, we further quantify the subjects intelligence from the distribution of failure count to fitted number of its decay rate. Thus, we no longer use log-log plot to see where the distribution falls as suggested in Section 3.3.3, but instead, directly compare the fitted decay rate with 2 and 3. If the decay rate is less than 2, the subject is classified as Limited Level. If the decay rate is between 2 and 3, the subject is classified as Capable Level. If the decay rate is greater than 3, the subject is classified as Autonomous Level. We validate the fitting quality of this approach in Figure 12. The task is language writing tasks, and the three rows correspond to Wikipedia, code, and general domains, respectively. We evaluate models of different sizes, ranging from 0.5B to 72B. The x-axis represents the failure count, and the y-axis represents the frequency. Both axes are on logarithmic scale. We fit the distribution within the failure count range of 10 to 100. We can see that data points closely follow power law distribution across domains and model sizes. The R2 values for the fits are marked on the figure and are very close to 1, suggesting that the fitting quality is near perfect. Thus, we use this setup to obtain the decay rate in this section. 5.2 Fitting Effect of Scaling Now, we empirically examine how model sizes relate to failure decay rate. We conduct experiments with models from Qwen 2.5 series (Yang et al., 2024a,b). The model sizes range from 0.5B to 72B. We use these models because they are state-of-the-art in terms of their respective parameter sizes and can even approach the performance of models that are ten times larger (Guo et al., 2025). Using such advanced models allows us to draw conclusions that represent the current cutting-edge technology. Figure 13 illustrates the impact of model size on failure decay rate. The x-axis represents the model size, and the y-axis represents the fitted decay rate, both on logarithmic scale. The circle markers represent the performance of the Qwen models. In general, these points approximately follow straight line. Among these tasks, the points are the closest to straight line in Knowledge QA (NQ and Trivia QA) and Coding tasks. In other tasks, we observe that the curve formed by the data points begins to bend downward as the model size becomes larger. This suggests that increasing the model size with the current training techniques will result in sub-log-linear improvement rate. If this phenomenon holds, we would seriously overestimate the performance when we use straight line on the log-log plot to predict the decay rate of larger models. Yet, from an optimistic perspective, we can attribute this slowing growth trend to the limitations of current training methods. Specifically, it is enough to use current training techniques and data size for training small models. As result, the improvements when model sizes are small fall along the straight line on the log-log plot. However, current data and techniques are limited when the model size reaches 32B or 72B. Consequently, the improvements fall short of expectations. Optimistically, if future researchers make breakthroughs in training techniques, the performance of these large models could still return to the straight line on the log-log plot. Besides the results of QWen 2.5, we also show the performance across different model architectures in Figure 1. The models include GPT2 (Radford et al., 2019), OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), Llama-1 (Touvron et al., 2023a), Llama-2 (Touvron et al., 2023b), Llama-3 (Dubey et al., 2024), Phi-2 (Javaheripi et al., 2023), GLM4 (GLM et al., 2024), DeepSeek-V2 (DeepSeek-AI, 2024), BaichuanM1 (Baichuan, 2024), Mistral (Jiang et al., 2023), QWen2.5 (Yang et al., 2024b). We can see that across different model architectures, larger models tend to perform better, forming an approximately linear trend. This is evident in models such as the OPT series, Llama 3 & 3.2 series. This observation aligns with our idea of fitting straight line. Additionally, we observe that more recently released models tend to achieve better performance over time. Therefore, optimistically speaking, although the fitted line slightly overestimates the performance of large models, their performance are expected to improve over time. Moreover, we can see that among these models, Qwen2.5 demonstrates state-of-the-art performance. As result, we use Qwen2.5 as benchmark to represent the current cutting-edge level. In this paper, we adopt this optimistic perspective. We model the effect of model size on failure decay rate as straight line on the log-log plot and assume that this linear approximation is still valid when researchers train models of larger scales. We proceed with this optimistic fitting approach into the next subsection, where we predict the development of scaling AI in the future. 5.3 Predicting Future We optimistically assume that, as the model size increases, the failure decay rate improves along straight line on log-log plot. We fit the straight line based on current models and extrapolate the straight line to predict models with larger scales in the future. Figure 14 shows the results of this extrapolation. The x-axis represents the model size, and the y-axis represents the failure decay rate, both on logarithmic scale. We calculate the model size required to enable the decay rate to 3, which corresponds to the Autonomous Level. The results indicate that for structured tasks, such as mathematics, law, and coding, the required parameter size is around 1018. These tasks are governed by clear rules and formats, which facilitate learning for AI systems and require fewer parameters. On the other hand, more complex tasks, such as knowledge-based question answering, patent applications, and writing medical or academic papers, require parameter size around 1021. These tasks are more intricate, demand specialized knowledge, and require sophisticated reasoning abilities. Thus, AI systems need to be very large to grasp them. Finally, we adopt general task that requires the model to comprehend all the information on the Internet. Specifically, we use the C4 English dataset (Raffel et al., 2020), which contains high-quality English corpus from the Internet. We test whether models can memorize the information by asking models to predict the next word given all previous words. According to the prediction results, the required parameter size for Autonomous Level is astonishing, around 1026. Achieving such scale with current hardware is virtually impossible. In the case of general language tasks, the required scale is on the order of 1026. This number is even 5 orders of magnitude higher than the total number of neurons in all of humanitys brains combined. Specifically, the number of neurons in single human brain is around 1011, and considering the global population is approximately 1010, the total number of neurons in all human brains is about 1021. This is only 105 of the scale needed for the AI model. From this perspective, building such large AI model would be like creating machine with computational complexity far greater than the total computational capacity of the human species. If we ignore any computational costs, such as training and inference, and just focus on loading this massive model onto H100 GPUs, heres the calculation: Since the memory of an H100 GPU is 80GB, we would need 5 1015 GPUs. Based on the cost of H100 GPU ($30,000) and the market value of Apple Inc. ($3.7 trillion) in February 2025, the total value of these GPUs would be equivalent to 4 107 times the market value of Apple Inc. As we can see, without breakthroughs in hardware and AI technology, it is infeasible to afford scaling for Autonomous-Level intelligence. 23 Figure 14: Prediction for Higher Intelligence Levels in Language Tasks. The x-axis represents model size, and the top axis shows the estimated time based on Moores Law. The y-axis represents the failure decay rate. Different colors indicate different intelligence levels. The results suggest that it will take several decades to achieve Autonomous Level in language tasks. If hardwares like GPU and CPU continue to improve with the rate suggested by Moores Law, we can predict the time when sufficiently large models can be developed. Moores Law states that the performance of chips doubles approximately every 18 months. As chip performance doubles, we can also double the size of AI systems without too much cost. Assuming Moores Law continues to hold, and taking the current maximum trainable model size as 1 trillion parameters, we can forecast the maximum trainable model size at each time in the future. In Figure 14, the top x-axis shows the predicted timeline. The results suggest that for structured tasks such as mathematics, law, and coding, it will take approximately 30 more years before sufficiently large models can be trained to achieve Autonomous Level. For more complex tasks, such as question answering, patent applications, and writing medical or academic papers, we project that it will take about 40 years. Finally, for general tasks, which require models to handle the full breadth of knowledge across various domains, we anticipate that it will take 70 years to train sufficiently large models. This projection provides timeline for the future of AI development. Nevertheless, model with larger scale require more training data and corresopnding training techniques. Even with such hardware improvement, it is still important to achieve breakthrough in stably training large models with only limited data."
        },
        {
            "title": "6 Theoretical Analysis of Survival Game",
            "content": "In previous sections, we present the experimental results of current AI systems. Results demonstrate that many AI systems are stuck at the Limited Level on complex human tasks and that achieving the Autonomous Level requires extremely high parameter size. This raises an intriguing question: Why is the Autonomous Level so difficult to achieve for current AI systems? To address this, we turn to the framework of Self-Organized Criticality (SOC) (Bak et al., 1987), complexity theory in Physics. SOC describes systems in criticality state where small perturbations can trigger large-scale changes. We propose that Survival Game is deeply related to SOC and that drawing this connection provides insights into the nature of human tasks and AI. In the following, we will first discuss how Survival Game relates to SOC, then model Survival Game with SOC framework. 24 6.1 Human Tasks exhibit Criticality SOC refers to phenomenon within complex systems where the system tends to settle into stable state and yet is inherently sensitive to perturbations. On one hand, the system exhibits self-organizing property: when disturbance occurs, the system adapts and re-establishes new equilibrium. On the other hand, the system is always in criticality state where even the slightest perturbation in any single part can trigger cascading changes throughout the entire system. In this way, the system remains in delicate balance: it is in stable state but slight perturbation can result in massive reorganization throughout the entire system. This property has been observed in many natural phenomena, such as earthquakes (Turcotte et al., 1985), forest fires (Malamud et al., 1998), proteins (Phillips, 2014), and neuronal avalanches (Chialvo, 2010). We propose that human tasks exhibit criticality, which leads to Survival Game as SOC system. In Survival Game, the questions posed and the corresponding correct answers form system. The self-organizing nature of this system is evident in the way that different questions correspond to different correct answers. We can imagine the correspondence between question and its answer as dynamic process. When question and the correct answer are given, the system is in stable state. When certain parts of the question are modified, the answer should undergo self-organizing modification process and eventually evolve into new correct answer, thus bringing the entire system back to stability. The systems criticality arises from the nature of human tasks. For example, minor alteration in mathematical question can drastically change the approach required to solve it. This criticality property requires the test subject to address subtle differences in the question that can lead to significantly distinct answers. Merely memorizing answers for several specific cases does not help because small change results in entirely different answers. We believe this is the cause of why so many AI systems are at Limited Level. AI systems might simply memorize some answers and yet human tasks are not friendly to memorization because of criticality property. In the following, we will show several examples to help illustrate how human tasks exhibit criticality. Physics: Physics exhibits criticality. In physics, even seemingly similar problems can lead to vastly different solutions depending on the initial conditions. For instance, consider question about matters state or superconductivity. The answer relies crucially on whether the temperature is above or below threshold. Similarly, if we ask about physical laws, the appropriate theory, quantum mechanics or classical physics, depends on the scale. This phenomenon is ubiquitous in physics: small variations in initial conditions can lead to fundamentally different results. By incorporating such questions into an Survival Game, we naturally create system with SOC property. The test subjects must adapt to subtle changes in initial conditions. Otherwise, their responses would be entirely wrong. If participant relies solely on memorizing answers for some specific conditions, they will struggle to apply their knowledge to new, subtly altered questions. In such cases, the failure attempts are likely to approach infinity, as the answers will deviate drastically from the memorized solutions. Mathematics: Mathematics exhibits criticality. Mathematics embodies the very essence of SOC. Take Fermats Last Theorem as an example. It asserts that there are no integer solutions to the equation xn + yn = zn for any integer > 2. At first glance, the change of might seem like minor adjustment and does not affect the essence of the problem. However, the theorems sensitivity to the value of is profound. As soon as grows larger, the problem becomes much more complex. The theorem with small values was proven soon, but it took more than two centuries before the general case was finally proven. Many math problems show such criticality property that seemingly small modifications can alter the entire landscape of solutions. Thus, when mathematical problems are used in Survival Game, subjects must be acutely aware of the details and select the appropriate mathematical tools. Subjects that do not possess this ability will take infinite attempts to arrive at the correct answer. Law: Legal issues exhibit criticality. In law, seemingly minor differences in behavior can lead to vastly divergent legal consequences. For instance, suspects actions might determine whether the charge is premeditated murder or voluntary manslaughter, whether it was excessive self-defense or justifiable defense, or even whether they are guilty or innocent. Such significant shifts in legal outcomes can arise from differences in behavior that, on the surface, might appear negligible. When legal questions are incorporated into an Survival Game, they require the subjects to discern these nuances and reason the outcomes. If subject has only memorized conclusions for specific cases, they will struggle in new scenarios and will inevitably make infinite attempts before arriving at the right answers. Medicine: Medicine exhibits criticality. In medicine, small, seemingly insignificant changes in physiological parameters can result in dramatic shifts in treatment plans. For example, in cancer treatment, subtle genetic differences between patients can lead to vastly different responses to immunotherapy (Hwang et al., 2020; Figure 15: Modeling Survival Game as complex network. Nodes represent concepts and edges represent interdependence. The network is also sandpile model (Bak et al., 1987). Martínez-Jiménez et al., 2023). In diabetes management, minor fluctuations in blood sugar levels may cause significant organ damage, necessitating precise adjustments in treatment (Zhang et al., 2019). Although the differences in physiological parameters may appear minimal, the underlying medical phenomena and the corresponding treatments can vary greatly. Therefore, answering medical questions requires grasping the underlying principles. If the subjects simply rely on memorizing specific examples, they will make catastrophic errors in new cases. These human tasks exhibit criticality and make Survival Game exhibit SOC. Criticality is typical symbol of complexity: Since slight change in the question can result in an entirely new answer, successfully operating the task requires full understanding of the underlying mechanism. Otherwise, subjects will be completely wrong about the correct answer. It will be difficult for them to arrive at the right one through trial and error. This is indeed the case for current AI systems as shown in our experiments. They remain at the Limited Level where the number of trials is infinite. This suggests that most AI systems do not fully understand the mechanism. Even if they are trained on enormous data and have memorized all of it, it is not enough for tasks with criticality properties. 6.2 Modeling Survival Game as Self-Organized Criticality System We have qualitatively explored how Survival Game exhibits SOC property on human tasks. Now, we will adopt quantitative perspective to validate this hypothesis. We will see that it closely resembles typical SOC model and exhibits very similar experimental phenomena. In Survival Game, both questions and answers inherently consist of many basic concepts. Since answers change when questions change, these concepts are intricately coupled. Because of the criticality property of human tasks, even small change in one concept can trigger significant changes in many other concepts. We can imagine this interconnection as network graph, as shown in Figure 15. Each node represents concept, and the edges between the nodes represent the couplings between these concepts. Slightly changing the question is akin to disturbing node. When node becomes unstable, it is activated and is likely to make its neighbors also unstable. This might eventually result in cascading activation throughout the network. Such changes throughout the entire network imitate how seemingly small changes in the question can lead to very different answers in Survival Game. This conceptual network is very similar to typical SOC system, namely the sandpile model (Bak et al., 1987). It has been used to model complex systems in the natural world. In the sandpile model, each node accumulates sand. To disturb the network is to add little more sand on node. When the amount of sand exceeds certain threshold, the sand topples from this node. The toppled sand is distributed to its neighbors, and thus the neighbors sand may also topple. This cascade of toppling sometimes results in an avalanche throughout the entire network. We believe that this sand avalanche effect is close to the cascading activation of concepts in our conceptual network for Survival Game. Therefore, we further implement our conceptual network as sandpile model. Specifically, slight change in question is equivalent to adding small amount of sand to node. The degree to which the answer changes after altering the question is analogous to the avalanche size triggered by adding sand. Through this correspondence, we model Survival Game as sandpile model. The validity of this modeling can be confirmed by testing whether the sandpile model accurately reflects the characteristics of Survival Game as observed in real-world applications. Modeling Survival Game as sandpile model can explain why the failure count distribution of current AI systems resembles power law. In real-world experiments, we empirically show that failure count distribution is very close to power law, as described in Section 5.1. However, the underlying reason is mystery. Now, we can investigate this phenomenon based on our abstraction of Survival Game, namely the sandpile model. 26 Figure 16: Simulation Results of the sandpile model with grid topology in 2, 3, and 4 dimensions. We examine the distribution of sand avalanche size after disturbance. The x-axis represents the avalanche size, and the y-axis shows the frequency. The distribution of avalanche size approximates power law. Figure 16 shows the simulated distribution of avalanche size in the sandpile model. Based on our analogy, this also corresponds to the distribution of how much answers change after altering the questions in Survival Game. The distribution follows power law, which closely matches the failure count distribution of AI systems. This actually reflects that current AI systems rely on memorization and exploration to solve new problems. Here is the reason. If AI relies on memorization and exploration, it means that when faced with new question in Survival Game, it recalls similar questions it has memorized and, based on the remembered answers, explores potential solutions. As result, the number of explorations correlates with the distance between the answers. In contrast, if the AI system genuinely understood the test questions, the distance between answers would not correlate with the number of failure attempts. In fact, the mechanics of the sandpile model are clear, and an AI system that truly grasps these mechanics could directly compute the stable state without any failure attempt. Therefore, the phenomena indicate that AI systems do not fully understand the mechanics of the task and instead rely on memorization and exploration to find answers. Modeling Survival Game as sandpile model also helps to illuminate how scaling works. From our experiments in Section 5.2, we empirically observe that scaling model size can improve the decay rate and thereby improve intelligence. Yet, the underlying reason is mystery. However, we find that the dimensionality of the sandpile topology has similar effect and can explain this phenomenon. More precisely, both theoretical analyses (Dhar, 2006; Zachariou et al., 2015) and our simulations in Figure 16 show that the power law exponent of the sandpile model increases as the topology becomes high-dimensional. This is because higher-dimensional topology creates more connections between nodes, enabling the network to stabilize more quickly when disturbed. We can draw the analogy between the sandpiles stabilization through these connections and intelligence systems solving new problems through exploration. When we scale an intelligent system, we expand the problem-solving space to higher-dimensional level and construct new paths between concept nodes. These new paths enable current intelligence systems to explore new solutions from memorized ones more quickly. This is akin to how sandpile model stabilizes itself through new connections in higher dimensionality. If we regard these new paths for exploration as the new connections in SOC systems, the effects of scaling can be explained: scaling makes exploration more effective. In summary, Survival Game can be modeled as sandpile model. It helps us gain deep understanding of the nature of human tasks and current AI. The sandpile model is complex system where little more sand can result in an avalanche throughout the entire landscape. Similarly, Survival Game with human tasks are also complex where small change in environments requires completely different responses. No matter whether it is to predict the state of the sandpile model or find the correct solution in Survival Game, it is necessary to fully understand the underlying mechanisms. Yet, most AI systems do not understand these mechanisms and rely on superficial imitation, such as memorization and exploration. They exhibit typical power law with small exponent. Scaling is able to transform the exploration space to higher dimensionality and makes the exploration process more effective. Nevertheless, if AI lacks full understanding of underlying mechanisms, SOC property will make it extremely difficult to achieve the Autonomous Level of intelligence."
        },
        {
            "title": "7 Conclusions and Future Work",
            "content": "In this paper, we introduce Survival Game. It is inspired by Natural Selection and quantifies the intelligence of any subject in any task. It demonstrates three advantages. 27 Firstly, the test offers clear physical meaning and well-defined mathematical framework. It categorizes intelligence into three levels and enables deep understanding of the subjects intelligence. Secondly, Survival Game provides roadmap for the future development of AI. It shows clear relationship to the scale of AI systems, which enables us to project the time to reach high-level intelligence. Finally, Survival Game helps reveal the nature of human tasks and AI. It suggests that human tasks exhibit criticality properties and AIs superficial manner to solve human tasks. We believe that future efforts should focus on three key areas. Firstly, since current AI technologies are mostly at the Limited Level, we need to identify appropriate application scenarios and design an effective human supervision framework. Secondly, since Survival Game connects the model size to the intelligence levels, we can use Survival Game to plan the roadmap of future AI development. Finally, linking Survival Game to SOC theory shows promising results and more efforts shall be made in this direction. This will not only help design more effective AI models but also deepen our understanding of humans ourselves."
        },
        {
            "title": "Reproducibility",
            "content": "Code is open-sourced at https://github.com/jingtaozhan/IntelligenceTest."
        },
        {
            "title": "Contributions",
            "content": "Jingtao Zhan initiated the project, proposed Survival Game, conducted experiments, and used SOC for theoretical analysis. Jiahao Zhao contributed to the experiments by conducting the text search task. Jiayu Li contributed to the experiments by conducting the recommendation system task. Yiqun Liu supervised the project, engaged in multiple discussions, and guided the researchers in exploring the fundamental principles underlying the observed phenomena. Bo Zhang participated in multiple discussions, provided important advice, and pointed out the similarity between the experimental results and phenomenon in SOC systems. Qingyao Ai, Jiaxin Mao, Hongning Wang, Min Zhang, and Shaoping Ma were involved in the early-stage discussions and provided invaluable feedback. Jingtao Zhan drafted the manuscript, and all other authors contributed important insights and suggestions to improve the writing."
        },
        {
            "title": "References",
            "content": "Eyal Aharoni, Sharlene Fernandes, Daniel Brady, Caelan Alexander, Michael Criner, Kara Queen, Javier Rando, Eddy Nahmias, and Victor Crespo. Attributions toward artificial agents in modified moral turing test. Scientific reports, 14(1):8458, 2024. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Baichuan. Baichuan-m1-14b. https://github.com/baichuan-inc/Baichuan-M1-14B, 2024. Henry Baird, Allison Coates, and Richard Fateman. Pessimalprint: reverse turing test. International Journal on Document Analysis and Recognition, 5:158163, 2003. Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268, 2016. Per Bak. How nature works: the science of self-organized criticality. Springer Science & Business Media, 2013. Per Bak, Chao Tang, and Kurt Wiesenfeld. Self-organized criticality: An explanation of the 1/f noise. Phys. Rev. Lett., 59:381384, Jul 1987. doi: 10.1103/PhysRevLett.59.381. URL https://link.aps.org/ doi/10.1103/PhysRevLett.59.381. Celeste Biever. Chatgpt broke the turing test-the race is on for new ways to assess ai. Nature, 619(7971): 686689, 2023. Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/10.5281/ zenodo.5297715. If you use this software, please cite it using these metadata. Selmer Bringsjord, Paul Bello, and David Ferrucci. Creativity, the turing test, and the (better) lovelace test. The Turing test: the elusive standard of artificial intelligence, pp. 215239, 2003. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Yukuo Cen, Jianwei Zhang, Xu Zou, Chang Zhou, Hongxia Yang, and Jie Tang. Controllable multi-interest framework for recommendation. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 29422951. ACM, 2020. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021a. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 15971607. PmLR, 2020. Yudong Chen, Xin Wang, Miao Fan, Jizhou Huang, Shengwen Yang, and Wenwu Zhu. Curriculum metalearning for next poi recommendation. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 26922702, 2021b. Dante Chialvo. Emergent complex neural dynamics. Nature physics, 6(10):744750, 2010. Eunjoon Cho, Seth Myers, and Jure Leskovec. Friendship and mobility: user movement in location-based social networks. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 10821090, 2011. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. DeepSeek-AI. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model, 2024. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141142, 2012. Deepak Dhar. Theoretical studies of self-organized criticality. Physica A: Statistical Mechanics and its Applications, 369(1):2970, 2006. Jingtao Ding, Yuhan Quan, Quanming Yao, Yong Li, and Depeng Jin. Simplify and robustify negative sampling for implicit collaborative filtering. Advances in Neural Information Processing Systems, 33:10941105, 2020. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 29 Gabriel Dulac-Arnold, Nir Levine, Daniel Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal, and Todd Hester. Challenges of real-world reinforcement learning: definitions, benchmarks and analysis. Machine Learning, 110(9):24192468, 2021. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Donald Geman, Stuart Geman, Neil Hallonquist, and Laurent Younes. Visual turing test for computer vision systems. Proceedings of the National Academy of Sciences, 112(12):36183623, 2015. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. Ben Goertzel. Artificial General Intelligence: Concept, State of the Art, and Future Prospects. Journal of Artificial General Intelligence, 01 2014. doi: 10.2478/jagi-2014-0001. Alex Gu, Baptiste Rozière, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I. Wang. Cruxeval: benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Stevan Harnad. Other bodies, other minds: machine incarnation of an old philosophical problem. Minds and Machines, 1:4354, 1991. Maxwell Harper and Joseph Konstan. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis), 5(4):119, 2015. Simon Haykin. Neural networks: comprehensive foundation. Prentice Hall PTR, 1994. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022. Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In proceedings of the 25th international conference on world wide web, pp. 507517, 2016. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. Balázs Hidasi and Alexandros Karatzoglou. Recurrent neural networks with top-k gains for session-based recommendations. In Proceedings of the 27th ACM international conference on information and knowledge management, pp. 843852, 2018. Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based recommendations with recurrent neural networks. 2016. Doris Hoogeveen, Karin M. Verspoor, and Timothy Baldwin. Cqadupstack: benchmark data set for community question-answering research. In Proceedings of the 20th Australasian Document Computing Symposium (ADCS), ADCS 15, pp. 3:13:8, New York, NY, USA, 2015. ACM. ISBN 978-1-4503-4040-3. doi: 10.1145/2838931.2838934. URL http://doi.acm.org/10.1145/2838931.2838934. Sohyun Hwang, Ah-Young Kwon, Ju-Yeon Jeong, Sewha Kim, Haeyoun Kang, Joonsuk Park, Joo-Hang Kim, Ok Jin Han, Sun Min Lim, and Hee Jung An. Immune gene signatures for predicting durable clinical benefit of anti-pd-1 immunotherapy in patients with non-small cell lung cancer. Scientific reports, 10(1):643, 2020. 30 Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/zenodo.5143773. If you use this software, please cite it as below. Iyer, Shankar lease: First-Quora-Dataset-Release-Question-Pairs. Nikhil Dandekar, and Kornél Csernai."
        },
        {
            "title": "Question",
            "content": "2012. pairs,"
        },
        {
            "title": "URL",
            "content": "rehttps://quoradata.quora.com/ dataset quora"
        },
        {
            "title": "First",
            "content": "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021. Mojan Javaheripi, Sébastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio César Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, et al. Phi-2: The surprising power of small language models. Microsoft Research Blog, 1(3):3, 2023. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. triviaqa: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv e-prints, art. arXiv:1705.03551, 2017. Leslie Pack Kaelbling, Michael Littman, and Andrew Moore. Reinforcement learning: survey. Journal of artificial intelligence research, 4:237285, 1996. Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard negative mixing for contrastive learning. Advances in neural information processing systems, 33:2179821809, 2020. Wang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM), pp. 197206. IEEE, 2018. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. Vid Kocijan, Ernest Davis, Thomas Lukasiewicz, Gary Marcus, and Leora Morgenstern. The defeat of the winograd schema challenge. Artificial Intelligence, 325:103971, 2023. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019. Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 60866096, 01 2019. doi: 10.18653/v1/P19-1612. Shane Legg and Marcus Hutter. Universal intelligence: definition of machine intelligence. Minds and machines, 17:391444, 2007. Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740755. Springer, 2014. 31 Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. Www18 open challenge: Financial opinion mining and question answering. In Companion Proceedings of the The Web Conference 2018, WWW 18, pp. 19411942, Republic and Canton of Geneva, CHE, 2018. International World Wide Web Conferences Steering Committee. ISBN 9781450356404. doi: 10.1145/3184558.3192301. URL https://doi.org/10.1145/3184558. 3192301. Bruce Malamud, Gleb Morein, and Donald Turcotte. Forest fires: an example of self-organized critical behavior. Science, 281(5384):18401842, 1998. Francisco Martínez-Jiménez, Peter Priestley, Charles Shale, Jonathan Baber, Erik Rozemuller, and Edwin Cuppen. Genetic immune escape landscape in primary and metastatic cancer. Nature Genetics, 55(5): 820831, 2023. Qiaozhu Mei, Yutong Xie, Walter Yuan, and Matthew Jackson. turing test of whether ai chatbots are behaviorally similar to humans. Proceedings of the National Academy of Sciences, 121(9):e2313925121, 2024. Meredith Ringel Morris, Jascha Sohl-dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement Farabet, and Shane Legg. Levels of agi for operationalizing progress on the path to agi, 2024. URL https://arxiv.org/abs/2311.02462. Louis Brulé Naudet. The laws, centralizing legal texts for better use. https://huggingface.co/ datasets/HFforLegal/laws, 2024. Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert. arXiv preprint arXiv:1901.04085, 2019. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. OpenAI. OpenAI Charter, 2018. URL https://openai.com/charter. Accessed February 24, 2025. JC Phillips. Fractals and self-organized criticality in proteins. Physica A: Statistical Mechanics and Its Applications, 415:440448, 2014. Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pp. 26412649, 2015. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.10084. Stephen Robertson and Sparck Jones. Relevance weighting of search terms. Journal of the American Society for Information science, 27(3):129146, 1976. John Searle. The chinese room, 1999. Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors with online hard example mining. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 761769, 2016. Mustafa Suleyman. The coming wave: technology, power, and the twenty-first centurys greatest dilemma. Crown, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. DL Turcotte, RF Smalley Jr, and Sara Solla. Collapse of loaded fractal trees. Nature, 313(6004):671672, 1985. A. M. Turing. Computing machinery and intelligence. Mind, 59(236):433460, 1950. ISSN 00264423. URL http://www.jstor.org/stable/2251299. Tao Wang. Chinese law and regulations. https://huggingface.co/datasets/twang2218/ chinese-law-and-regulations, 2023. Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International conference on machine learning, pp. 99299939. PMLR, 2020. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. Cmath: Can your language model pass chinese elementary school math test?, 2023. Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. C-pack: Packaged resources to advance general chinese embedding, 2023. Xiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv, Ting Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li, Haitao Li, Yiqun Liu, et al. T2ranking: large-scale chinese benchmark for passage ranking. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 26812690, 2023. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=zeFrfgyZln. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024b. Peilin Yang, Hui Fang, and Jimmy Lin. Anserini: Enabling the use of lucene for information retrieval research. In Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval, pp. 12531256, 2017. Nicky Zachariou, Paul Expert, Misako Takayasu, and Kim Christensen. Generalised sandpile dynamics on artificial and real-world directed networks. PloS One, 10(11):e0142685, 2015. 33 Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. Optimizing dense retrieval model training with hard negatives. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 15031512, 2021. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. Zhen-Ye Zhang, Ling-Feng Miao, Ling-Ling Qian, Ning Wang, Miao-Miao Qi, Yu-Min Zhang, Shi-Peng Dang, Ying Wu, and Ru-Xing Wang. Molecular mechanisms of glucose fluctuations on diabetic complications. Frontiers in endocrinology, 10:640, 2019. Feng Zhu, Chaochao Chen, Yan Wang, Guanfeng Liu, and Xiaolin Zheng. Dtcdr: framework for dual-target cross-domain recommendation. In Proceedings of the 28th ACM international conference on information and knowledge management, pp. 15331542, 2019. Feng Zhu, Yan Wang, Chaochao Chen, Guanfeng Liu, and Xiaolin Zheng. graphical and attentional framework for dual-target cross-domain recommendation. In IJCAI, volume 21, pp. 39, 2020."
        }
    ],
    "affiliations": [
        "Renmin University of China",
        "Tsinghua University"
    ]
}