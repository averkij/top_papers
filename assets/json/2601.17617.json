{
    "paper_title": "Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests",
    "authors": [
        "Jingjie Ning",
        "JoÃ£o Coelho",
        "Yibo Kong",
        "Yunfan Long",
        "Bruno Martins",
        "JoÃ£o MagalhÃ£es",
        "Jamie Callan",
        "Chenyan Xiong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LLM-powered search agents are increasingly being used for multi-step information seeking tasks, yet the IR community lacks empirical understanding of how agentic search sessions unfold and how retrieved evidence is used. This paper presents a large-scale log analysis of agentic search based on 14.44M search requests (3.97M sessions) collected from DeepResearchGym, i.e. an open-source search API accessed by external agentic clients. We sessionize the logs, assign session-level intents and step-wise query-reformulation labels using LLM-based annotation, and propose Context-driven Term Adoption Rate (CTAR) to quantify whether newly introduced query terms are traceable to previously retrieved evidence. Our analyses reveal distinctive behavioral patterns. First, over 90% of multi-turn sessions contain at most ten steps, and 89% of inter-step intervals fall under one minute. Second, behavior varies by intent. Fact-seeking sessions exhibit high repetition that increases over time, while sessions requiring reasoning sustain broader exploration. Third, agents reuse evidence across steps. On average, 54% of newly introduced query terms appear in the accumulated evidence context, with contributions from earlier steps beyond the most recent retrieval. The findings suggest that agentic search may benefit from repetition-aware early stopping, intent-adaptive retrieval budgets, and explicit cross-step context tracking. We plan to release the anonymized logs to support future research."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 2 ] . [ 1 7 1 6 7 1 . 1 0 6 2 : r Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests Jingjie Ning jening@andrew.cmu.edu Carnegie Mellon University Pittsburgh, PA, US Yunfan Long justinlo@andrew.cmu.edu Carnegie Mellon University Pittsburgh, PA, US JoÃ£o Coelho jmcoelho@andrew.cmu.edu Carnegie Mellon University Pittsburgh, PA, US Yibo Kong yibok@andrew.cmu.edu Carnegie Mellon University Pittsburgh, PA, US Bruno Martins bruno.g.martins@tecnico.ulisboa.pt Instituto Superior TÃ©cnico, University of Lisbon Lisbon, Portugal JoÃ£o MagalhÃ£es jm.magalhaes@fct.unl.pt NOVA LINCS, NOVA University Lisbon Caparica, Portugal Jamie Callan callan@cs.cmu.edu Carnegie Mellon University Pittsburgh, PA, US Chenyan Xiong cx@cs.cmu.edu Carnegie Mellon University Pittsburgh, PA, US Abstract LLM-powered search agents are increasingly being used for multistep information seeking tasks, yet the IR community lacks empirical understanding of how agentic search sessions unfold and how retrieved evidence is used. This paper presents large-scale log analysis of agentic search based on 14.44M search requests (3.97M sessions) collected from DeepResearchGym, i.e. an open-source search API accessed by external agentic clients. We sessionize the logs, assign session-level intents and step-wise query-reformulation labels using LLM-based annotation, and propose Context-driven Term Adoption Rate (CTAR) to quantify whether newly introduced query terms are traceable to previously retrieved evidence. Our analyses reveal distinctive behavioral patterns. First, over 90% of multi-turn sessions contain at most ten steps, and 89% of interstep intervals fall under one minute. Second, behavior varies by intent. Fact-seeking sessions exhibit high repetition that increases over time, while sessions requiring reasoning sustain broader exploration. Third, agents reuse evidence across steps. On average, 54% of newly introduced query terms appear in the accumulated evidence context, with contributions from earlier steps beyond the most recent retrieval. The findings suggest that agentic search may benefit from repetition-aware early stopping, intent-adaptive retrieval budgets, and explicit cross-step context tracking. We plan to release the anonymized logs to support future research. These authors contributed equally to this work. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. SIGIR 26, Melbourne Naarm, Australia 2026 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/26/07 https://doi.org/XXXXXXX.XXXXXXX Keywords Agentic Search, Query Log Analysis, Deep Research, Search Intent ACM Reference Format: Jingjie Ning, JoÃ£o Coelho, Yibo Kong, Yunfan Long, Bruno Martins, JoÃ£o MagalhÃ£es, Jamie Callan, and Chenyan Xiong. 2026. Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests. In Proceedings of The 49th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 26). ACM, New York, NY, USA, 12 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nInformation retrieval is shifting from human-initiated search into\nagentic search [2, 30, 40, 51], where LLM-powered agents plan\nand execute multi-step information seeking with retrieval tools.\nInstead of issuing a single query and consuming a ranked list, an\nagent iteratively reformulates queries, retrieves evidence, and up-\ndates subsequent queries based on retrieved context. While agent\ncapabilities are increasingly demonstrated on controlled bench-\nmarks [19, 29, 49], the benchmark scores alone do not reveal how\nagentsâ€™ queries evolve across steps, or how retrieved context shapes\nsubsequent queries.",
            "content": "The aforementioned questions matter for practical system design. Agents may waste retrieval budget through repetitive or overly narrow reformulations, fail to explore alternative facets, or under-use evidence accumulated across steps. Understanding session structure can inform query-policy control, and quantifying evidence reuse can guide budget allocation and evaluation design. As agents consume results programmatically, leaving no observable trace of what they found useful, the logs lack implicit feedback signals (e.g., direct clicks) that anchor traditional behavioral inference. This creates measurement gap. We can observe sequences of queries and retrieved evidence, but it remains unclear how sessions unfold, how behavior differs by intent, what reformulation moves dominate, and whether agents incorporate evidence from earlier steps. SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Ning et al. We provide practical design takeaways from the logs, including repetition-aware stopping, intent-adaptive retrieval budgeting, and cross-step context tracking. We plan to release an anonymized version of the logs to support reproducibility (details in Section 3)."
        },
        {
            "title": "2 Related Work",
            "content": "Human Search Behavior and Log Analysis: Large-scale query logs have long been used to study search behavior in the wild [8, 16, 41], offering scalable, behavior-grounded signals for characterizing session dynamics and query reformulation beyond what offline benchmarks capture. core theme is within-session learning. Eickhoff et al. [10] trace how newly introduced terms relate to evidence observed before reformulation (e.g., SERP snippets and visited pages), alongside complementary work on interpreting implicit feedback such as clicks and dwell time [1, 12, 20]. Exploratory search and navigation studies further document differences in branching and interaction patterns across users and information needs [28, 43, 48], while sessionization analyses examine how sessions begin and end in practice [21]. We adopt this evidence-traceability perspective for autonomous agents and operationalize it using retrieved evidence text, enabling systematic comparisons of agent behaviors across intents and guiding design choices such as retrieval budgeting and cross-step context management. While small line of work compares humans and agents directly [46, 47, 56], these comparisons are often task-specific or simulation-based, motivating complementary large-scale log analyses of how autonomous agents search across sessions in the wild. LLM Interaction Platforms and Usage Logs: Recent efforts analyze large-scale interaction data from LLM systems and evaluation platforms. Chatbot Arena (LMSYS LLM Arena) aggregates pairwise preference votes [6], and LMSYS-Chat-1M releases one million multi-model conversations collected in the wild [55]. OpenAI reports how people use ChatGPT at scale [33], and Anthropic presents privacy-preserving analyses of millions of Claude conversations to characterize economic task usage [14]. SciArena extends the Arena protocol to scientific literature-grounded tasks and provides corresponding benchmark [53]. These works capture usage and preference signals, but typically do not expose tool-level retrieval traces (queries, evidence, step-wise search decisions) needed to study agentic search behavior and within-session evidence reuse. Agentic Search Modeling, Benchmarks, and Infrastructures: Recent systems enabling LLMs to plan multi-step interactions with retrieval tools have shifted IR toward agentic workflows [2, 30, 40, 51]. Benchmarks for tool-using agents include WebShop [50], WebArena [57], AgentBench [27], and large-scale tool-use evaluation such as ToolLLM/ToolBench [37]. DeepResearchGym (DRGym) provides an open-source sandbox with reproducible search API and evaluation protocol for deep research systems [7]. Early analyses have begun to formalize agent behaviors. Jin et al. [19] link beneficial reasoning patterns to gains on GAIA [29] and WebWalker [49]; complementary efforts propose taxonomies and risk frameworks, such as ST-WebAgentBench [24] and the Agentic AI Security Scoping Matrix [5]. However, benchmark scores alone provide limited visibility into how agents search in practice, and prior humanagent Figure 1: Intenttrajectory structure of agentic search logs. To address this gap, we analyze agentic search at two complementary levels: what the agent is trying to accomplish in session (session-level intent), and how the agent pursues that goal through step-wise search actions (trajectory-level query reformulation). Figure 1 illustrates this structure on short example session. To operationalize these levels, we develop measurement framework with three components: (1) LLM-based annotation pipelines that assign interpretable intent and trajectory labels to sessions and step-pairs following standard taxonomies; (2) offline replay of logged queries to reconstruct the retrieved evidence available at each step; and (3) Context-driven Term Adoption Rate (CTAR), metric we introduce to quantify whether newly introduced query terms can be lexically traced to retrieved evidence, including contributions from steps beyond the most recent retrieval. We apply this framework to logs collected from DeepResearchGym (DRGym) [7], i.e. research-oriented reproducible search API accessed by external agentic clients. With permission from the DRGym organizers, we study 14.44M logged search requests spanning six months, which we sessionize into 3.97M sessions. This provides an at-scale view of autonomous agents operating in the wild under shared retrieval backend. Our results answer the practical questions raised above about budget waste, reformulation behavior, and evidence reuse in multistep agentic search. We find that over 90% of multi-turn sessions contain at most ten steps, and 89% of inter-step intervals fall under one minute. Retrieval depth (i.e. the number of documents requested per query) is largely static, suggesting agents treat it as fixed parameter rather than adapting within sessions. Behavior also varies by intent. Fact-seeking sessions exhibit the highest repetition, which increases over time, indicating that agents can enter near-duplicate loops when retrieval is unproductive, while sessions requiring reasoning sustain broader exploration throughout. We also find that agents frequently incorporate terms from previously retrieved evidence, with measurable contributions from earlier steps beyond the most recent retrieval. Our contributions can be summarized as follows: We provide large-scale behavioral characterization of agentic search from reproducible search infrastructure (14.44M search requests, 3.97M sessions), offering an at-scale view of autonomous agents operating in the wild. We introduce CTAR, metric for quantifying evidence-conditioned query evolution, and use it to demonstrate cross-step evidence reuse beyond the most recent retrieval. Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia (a) Geographic distribution of requests. (b) Pairwise query cos similarity (100k sample). (c) Query frequency distribution (loglog). Figure 2: Representativeness and diversity of the DRGym logs. comparisons suggest differences in query breadth and context use that are difficult to diagnose without session-level traces [46, 47, 56]. Most prior work focuses on benchmarking and system design, whereas we measure behavior at scale from real logs, and quantify evidence traceability via CTAR."
        },
        {
            "title": "3.1 DRGym Log Overview\nDRGym serves requests from external agentic clients, capturing\ndiverse usage patterns and interaction styles. The API is model-\nagnostic; although logs do not record the underlying agent im-\nplementation, requests originate from independent client systems\noperating under the same retrieval infrastructure.",
            "content": "The backend performs dense retrieval [17, 22] over two largescale English corpora, ClueWeb22-A-EN [34] and FineWeb [35]. The DRGym paper describes retrieval API with /search endpoint for ranked retrieval [7]. Operating over static web snapshots (instead of changing live index) enables re-issuing queries under fixed corpora for consistent retrieval behavior across experiments. Consistent with this design, each log entry records the timestamped query and request parameters  (Table 1)  , including retrieval depth and an ANN search-budget parameter [17]. For privacy, IP addresses are anonymized and used only for coarse client-level aggregation (grouping, sessionization, and country-level reporting), never for user identification or fine-grained geolocation. Scale and Coverage: The logs span 2025-06 to 2025-12 and contain 14.44 million requests. After preprocessing and sessionization (Section 3.2), we obtain 3.97M sessions. Requests originate from 558 anonymized client IPs across 25 countries. Figure 2a shows the geographic coverage, with the largest shares from China and the United States, followed by Hong Kong and Iceland. Overall traffic is substantial, peaking at 2.49 million requests in single week. Semantic Diversity: Figure 2b plots the pairwise cosine-similarity distribution for 100k randomly sampled queries using Qwen3Embedding-0.6B [52]. The distribution (mean=0.12) lies close to Table 1: Fields recorded in the search_logs table. Field Description id ip_address query_text num_of_docs complexity dataset timestamp Auto-incremented unique identifier for each request. Client IP address (anonymized for analysis). Query string submitted to the API. Number of documents requested for retrieval. ANN retrieval complexity controlling search budget. Corpus used for retrieval (ClueWeb22 or FineWeb). Timestamp of the logged request. the random-vector baseline (mean0 for uniformly distributed vectors), indicating that queries are semantically diverse rather than clustered around repeated themes. The slight rightward shift likely reflects shared information-seeking phrasing, not semantic redundancy. For reference, Qwen3 uses cosine similarity > 0.7 to mark semantically related pairs during training [52]. Query-Level Repetition: Figure 2c shows long-tailed query frequency distribution. Most distinct queries are quite rare, while only small set of queries repeats often. In particular, 53.89% of distinct queries occur at most three times, including 38.38% singleton queries, and the top-10 and top-100 most frequent queries account for only 0.59% and 1.51% of all requests, respectively. Taken together, the broad geographic coverage, low average semantic similarity, and long-tailed frequency spectrum suggest that the DRGym stream reflects realistic and diverse mix of information needs, rather than narrow set of repeatedly executed prompts. To further validate that this diversity is not an artifact of surfacelevel paraphrasing of popular benchmarks, we next quantify the overlap between logged queries and public benchmarks. We measure semantic overlap between log queries and four commonly used agentic benchmarks: GAIA [29], FRAMES [23], HLE [36], and WebWalkerQA [49]. We consider sample of 1 million queries from the logs, and encode all benchmark queries using Qwen3-Embedding-0.6B. Then, we count log queries exceeding cosine similarity threshold of 0.7 to any benchmark query. As shown in Table 2, benchmark-similar queries constitute less than 0.4% of the sample across all four benchmarks combined. Overlap is lowest for GAIA, and highest for WebWalker, whose web traversal queries more closely resemble natural search formulations. Overall, these results suggest the logs reflect diverse open-ended usage rather than concentrated benchmark execution. SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Ning et al. Table 2: Semantic overlap between agentic benchmarks and 1M log query sample (cosine similarity 0.7). Benchmark Bench. Queries Hits in Sample % of Sample GAIA FRAMES HLE WebWalkerQA Total 103 824 2,158 680 3,765 27 879 616 2,219 3, 0.00% 0.09% 0.06% 0.22% 0.37% To support reproducibility and enable further research, we will release the cleaned and anonymized logs associated with this study. Prior to release, we will remove direct identifiers (e.g., IP addresses) and apply standard PII scrubbing on free-text fields, releasing only the fields needed to reproduce our analyses with anonymized session IDs. This release has already received formal approval from the DRGym organizers. We will document the anonymization procedure and residual risks in the dataset card, following prior largescale LLM interaction log releases and privacy-preserving analyses [14, 55]. After the initial publication, the dataset will be updated as additional logs are collected and validated."
        },
        {
            "title": "3.2 Log Preprocessing and Sessionization\nWe first remove malformed entries (e.g., empty queries), internal\ntesting traffic, and outlier repetition bursts, before segmenting the\nremaining stream into sessions.",
            "content": "Although standard sessionization often relies on fixed time-gap heuristics, agentic requests can arrive in fast parallel patterns [31], making pure temporal cutoff unreliable. We therefore sessionize with semantic-continuity criterion combined with an explicit temporal constraint. Concretely, for each IP we maintain active sessions and assign an incoming query to the most semantically continuous active session when the continuity score exceeds threshold; otherwise we start new session. We additionally impose 10-minute hard cutoff between consecutive queries within session, reflecting faster interaction loops than the conventional 30-minute rule for human logs [21, 41]. Among classifier-predicted continuous pairs, only 0.92% have gaps exceeding 10 minutes. The aforementioned pipeline yields 3.97M sessions. Manual spotchecks confirm that the resulting sessions are generally coherent. Full procedural details (i.e., continuity model, thresholds, and validation) are provided in Appendix A."
        },
        {
            "title": "4 Methodology\nTo address the questions motivating this work, we require measure-\nments at two levels, which are session-level intent (what type of\ninformation need drives the session) and trajectory-level reformula-\ntion (how queries change from step to step). We also require a way\nto assess whether retrieved evidence influences subsequent queries.\nFor intent and trajectory labeling, we use standard LLM-as-a-judge\npipelines [25, 54]. For evidence influence, we introduce a new met-\nric, namely the Context-driven Term Adoption Rate (CTAR).",
            "content": "We segment the log into sessions S, where each session ğ‘  = (ğ‘1, . . . , ğ‘ ğ‘  ) is an ordered sequence of timestamped queries. Retrieval depth is denoted by ğ¾, corresponding to the logged parameter num_of_docs. We analyze behavior at three granularities: global (corpus-wide), session-level (intent-conditioned), and trajectorylevel (adjacent query pairs within session ğ‘ğ‘˜ ğ‘ğ‘˜+1)."
        },
        {
            "title": "4.1 LLM-based Intent and Trajectory Labeling",
            "content": "Session-Level Intent: Different information needs may induce different search strategies. For instance, user seeking factual answer may behave differently from one debugging procedure or reasoning through complex question. To test whether agentic search exhibits such intent-conditioned structure, we label each session with an intent category. We adopt three-way taxonomy from web search goal modeling [4, 10, 39]: Declarative (fact retrieval), Procedural (method execution), and Reasoning (complex synthesis). Since ğ‘1 is often already reformulation, we assign intent from the whole session text. Trajectory-Level Reformulation: Intent alone does not reveal how agents iterate within session. An agent might narrow its query, broaden it, pivot to related facet, or retry with near-identical phrasing. These reformulation patterns have implications for retrieval efficiency: excessive repetition wastes budget, while lack of exploration may leave relevant facets unexamined. To capture these dynamics, we label each adjacent query pair (ğ‘ğ‘˜ ğ‘ğ‘˜+1) with trajectory type grounded in prior reformulation taxonomies [3, 15]: Specialization (narrowing by adding constraints), Generalization (broadening by relaxing constraints), Exploration (within-topic facet pivots) and Repetition (identical or near-duplicate reformulations). Representative examples are provided in Appendix C. Implementation: We implement labeling with gpt-5-nano [32]. We annotate multi-turn sessions with ğ‘  [2, 10] for intent (one label per session) and all adjacent pairs for trajectories (one label per pair). We focus on this range because our sessionization analysis (Section 5) reveals that it covers 90.32% of all multi-turn traffic, representing the core behavior of current agents. To assess labeling robustness, we compare labels from two models (gpt-5-nano and gemini-3-flash-preview [13]) on 2000-pair random subset, achieving 95.15% agreement; the remaining disagreements are spread across categories rather than concentrated in any single label. Prompts are provided in Appendix D. Unless otherwise noted, analyses from Section 6 onward use random subset of labeled multi-turn sessions under the annotation budget (excluding singlequery sessions and outlier long-tail sessions, as described in Section 5). We also compute auxiliary metrics used throughout the paper, each defined at first use with summary in Appendix B."
        },
        {
            "title": "4.2 Context-driven Term Adoption Rate (CTAR)\nAgentic search iterates between retrieval and query formulation.\nEvidence returned at step ğ‘˜ may shape how the agent revises the\nnext query. Yet agentic logs provide no direct signal of what the\nagent actually attended to in retrieved documents, making evidence\nuse hard to observe. We therefore ask a more tractable traceability\nquestion: when the agent introduces new query terms at step ğ‘˜+1,\ndo those terms appear in the evidence it has already seen? This\naligns with evidence-traceability perspectives in human log analy-\nsis [10] and searching-as-learning research [9, 38, 45], but has not\nbeen systematically studied for autonomous agents.",
            "content": "Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Table 3: Session-level descriptive statistics by intent type. Formulas for less standard metrics are in Appendix B. Statistic Mean Median Metric (Sample / Ratio) Session Length Retrieval Depth (ğ¾) Query Length (whitespace terms) Initial-Final Gap Total Duration (s) Step Latency (s) Declarative (fact-seeking) Procedural (how-to) Reasoning (analytical) (99.8k / 88.64%) 4.03 7.70 7.59 0.21 40.00 17.00 (4.5k / 3.96%) 3.81 37.34 10.58 0.22 26.00 13.00 (8.3k / 7.41%) 4.03 24.99 12.69 0.28 31.00 14.00 We formulate this idea through Context-driven Term Adoption Rate (CTAR), i.e. the fraction of newly introduced query terms that can be lexically traced to retrieved evidence. We use exact-match tracing rather than semantic similarity because it is interpretable without threshold tuning, robust across domains and query styles, and conservative, i.e., semantic variants would typically yield higher rates by crediting paraphrases and near matches. Let ğ‘‡ ğ‘’ğ‘Ÿğ‘šğ‘  (ğ‘¥) denote the set of unique, lowercased, and nonstopword tokens that can be extracted from text ğ‘¥. For trajectory (ğ‘ğ‘˜ ğ‘ğ‘˜+1), the set of newly introduced terms is: ğ‘ ğ‘’ğ‘¤ğ‘‡ ğ‘’ğ‘Ÿğ‘šğ‘  (ğ‘ğ‘˜+1, ğ‘ğ‘˜ ) = ğ‘‡ ğ‘’ğ‘Ÿğ‘šğ‘  (ğ‘ğ‘˜+1) ğ‘‡ ğ‘’ğ‘Ÿğ‘šğ‘  (ğ‘ğ‘˜ ). (1) Let ğ¸ğ‘˜ denote the textual evidence available at step ğ‘˜. Since raw logs do not store retrieved documents, we reconstruct ğ¸ğ‘˜ by querying the DRGym API (which guarantees reproducibility) using the original logged parameters. We consider two context definitions: ğ¶last ğ‘˜ = ğ‘‡ ğ‘’ğ‘Ÿğ‘šğ‘  (ğ¸ğ‘˜ ), ğ‘˜ (cid:216) (2) ğ¶agg ğ‘˜ = ğ‘‡ ğ‘’ğ‘Ÿğ‘šğ‘  (ğ¸ğ‘– ). (3) ğ‘–=1 CTAR is the fraction of new terms appearing in the chosen context: CTAR() ğ‘˜ = (cid:12) (cid:12) (cid:12) ğ‘ ğ‘’ğ‘¤ğ‘‡ ğ‘’ğ‘Ÿğ‘šğ‘  (ğ‘ğ‘˜+1, ğ‘ğ‘˜ ) ğ¶ () ğ‘˜ ğ‘ ğ‘’ğ‘¤ğ‘‡ ğ‘’ğ‘Ÿğ‘šğ‘  (ğ‘ğ‘˜+1, ğ‘ğ‘˜ ) (cid:12) (cid:12) (cid:12) , () {last, agg}. (4) In the previous equation, if ğ‘ ğ‘’ğ‘¤ğ‘‡ ğ‘’ğ‘Ÿğ‘šğ‘  (ğ‘ğ‘˜+1, ğ‘ğ‘˜ ) = or ğ¶ () ğ‘˜ = , we set CTAR() ğ‘˜ = 0. In summary, CTAR quantifies the degree to which query evolution is lexically grounded in retrieved evidence. CTARğ‘™ğ‘ğ‘ ğ‘¡ captures adoption from the immediately preceding step, while CTARğ‘ğ‘”ğ‘” captures adoption from any prior step, enabling us to test whether agents integrate evidence across the full session or rely only on the most recent retrieval. Comparing the two allows quantifying the contribution of earlier context, and identifying session types or phases where evidence integration breaks down."
        },
        {
            "title": "5 Aggregate Session Statistics",
            "content": "Session Length and Structural Composition: The corpus comprises 3.97M unique search sessions with skewed length distribution (Figure 3, left). Nearly half (47.77%) are single-query sessions (likely one-off API calls or simple lookups), with manual inspection suggesting that these are predominantly Declarative or Procedural queries. Among multi-turn sessions, 90% have length 10, indicating that most multi-turn sessions in the log resolve within few iterations, though some extend considerably further. Figure 3: Left: distribution of session length (number of queries per session). Right: distribution of step-wise time intervals between consecutive requests. For reference, human web search logs are slightly shorter on average (1.7 queries per session) and include substantially larger fraction of single-query sessions (77.6%) [10, 41], suggesting that agentic systems engage in more extended information-seeking episodes. We focus subsequent analyses on multi-turn sessions. Temporal Dynamics and Interaction Speed: Within-session intervals are short for most steps, where 56.12% fall within 010 seconds, and 89.21% are under one minute (Figure 3, right). Intervals are heavy-tailed, reflecting occasional long-latency steps due to system or pipeline delays. For reference, prior human log studies report median dwell times of several minutes for knowledge-acquisition intents [10]. While dwell time and session duration are not directly comparable to our inter-step intervals, the contrast highlights the faster iterative pacing typical of agentic search. Retrieval Depth and Parameter Stability: Retrieval depth is concentrated at fixed values ğ¾ {1, 5, 10}, with only 8.36% of sessions using other values. Furthermore, only 1.35% of sessions vary ğ¾ across steps. Since DRGym supports 1 ğ¾ 100, this suggests that many agents treat retrieval count as effectively hard-coded rather than adapted within session."
        },
        {
            "title": "6 Intent-Conditioned Session Behavior\nUsing the LLM-as-a-judge pipeline described in Section 4, we label\neach multi-turn session as either being Declarative (fact-seeking),\nProcedural (how-to or step-by-step tasks), or Reasoning (compar-\native, analytical or multi-hop questions). Declarative dominates\n(88.64%), followed by Reasoning (7.41%) and Procedural (3.96%).\nTable 3 summarizes the session-level behavior, reporting medians",
            "content": "SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Ning et al. Table 4: Descriptive statistics by trajectory type. Formulas for less standard metrics are in Appendix B. Metric Specialization Generalization Exploration Repetition Statistic (Sample / Ratio) (73.8k / 21.76%) (30.6k / 9.02%) (125.8k / 37.07%) (109.1k / 32.15%) Mean Dense Similarity Jaccard Similarity Result Overlap Delta Query Length Median Step Latency (s) Narrowing Broadening Facet pivots Near-duplicates 78.18% 40.13% 22.58% +1.47 7.00 80.07% 44.66% 23.47% -2.42 8.00 55.08% 25.10% 7.35% +0. 14.00 96.70% 81.63% 78.23% -0.08 6. for time-based measures due to heavy tails and means for count and semantic measures. Beyond standard length and timing, we characterize sessions with two additional measures. Retrieval Depth summarizes per-step ğ¾ at the session level, and Initial-Final Gap measures semantic drift from first to last query 1 cos(ğ‘1, ğ‘ ğ‘  ) using Qwen3-Embedding0.6B [52]. Formal definitions are in Appendix B. Declarative sessions use the shallowest retrieval yet incur the highest interaction costs. This suggests that limited evidence per step forces agents into more iterations to locate and verify information. The pattern contrasts with human fact-finding, where users issue fewer and shorter queries [8, 10, 41]. Agents instead phrase queries as full constraint-bearing questions, consistent with iterative verification behavior [19]. Procedural sessions show the opposite pattern. Deeper retrieval accompanies more semantically stable progression, suggesting that broader evidence coverage reduces the need for iterative refinement. Queries within these sessions are longer than Declarative ones, consistent with prior studies of procedural search reporting similar characteristics [10]. Finally, we observe that Reasoning sessions match Declarative in turn count but differ in how queries evolve. They show the largest semantic drift and longest queries, while retrieval depth is moderate. The distinguishing signal for Reasoning lies in within-session query reformulation rather than session duration or retrieval depth."
        },
        {
            "title": "7.1 Trajectory Types and Properties\nWe follow the labeling procedure in Section 4.1, classifying each\nadjacent pair (ğ‘ğ‘˜ â†’ ğ‘ğ‘˜+1) as Specialization (narrowing by adding\nconstraints), Generalization (broadening by relaxing constraints),\nExploration (within-topic facet pivots), or Repetition (identical or\nnear-duplicate reformulations). Table 4 summarizes trajectory prop-\nerties, and Table 5 reports their usage. We interpret trajectories\nwith three stability measures: Dense Similarity is cosine similar-\nity between consecutive query embeddings (Qwen3-Embedding-\n0.6B [52]); Jaccard Similarity is lexical overlap over lowercase,",
            "content": "Table 5: Distribution of trajectory types across all step-wise transitions within sessions of each intent."
        },
        {
            "title": "Trajectory",
            "content": "Specialization Generalization Exploration Repetition"
        },
        {
            "title": "Session Intent",
            "content": "Declarative Procedural Reasoning 26.35% 7.89% 47.57% 18.19% 27.99% 10.12% 39.04% 22.85% 21.12% 9.07% 36.12% 33.69% whitespace-tokenized word sets; and Result Overlap is the Jaccard overlap between retrieved document identifier sets for consecutive queries (definitions in Appendix B). The Drill-Down Bias: Across intents, agents mainly tighten constraints via local edits or pivot across nearby facets, while explicit broadening is consistently the least-used move (under 11%; Table 5). This imbalance suggests that agents are more comfortable focusing on local neighborhood of the query space rather than stepping back to relax constraints and reconsider alternatives. Exploration is common (roughly 3648%), but pivots tend to induce larger evidence turnover and slower transitions, making them costlier than incremental refinement  (Table 4)  . Together, these signatures suggest that when progress stalls, agents often continue local edits rather than deliberately broadening and re-planning, motivating improved controller policies and training signals. Intent Differences: Although all intents share the same move vocabulary, they exhibit different interaction patterns  (Table 5)  . Declarative sessions are most prone to retry-like behavior, with Repetition at about one-third of moves, consistent with agents re-issuing near-duplicate queries when evidence remains elusive. Reasoning sessions, in contrast, sustain the highest pivoting (Exploration near 48%) with lower retrying, suggesting broader search over sub-questions. Procedural sessions more often combine pivots with subsequent constraint tightening, aligning with an explore then refine workflow in step-by-step tasks. Stability as Diagnostic: Table 4 reveals stability spectrum. Repetition largely preserves retrieved results (Result Overlap 78%), whereas Exploration induces major evidence turnover (Result Overlap 7%). Specialization and Generalization fall between these extremes, typically preserving part of the retrieved context while Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Figure 4: Step-wise trajectory distribution trends for the first 10 steps across different task intents. Each sub-figure illustrates the evolving proportions of Specialization, Generalization, Exploration, and Repetition as the search session progresses. Figure 5: Declarative retry-loop example dominated by near-duplicate reformulations. steering the query. This suggests that sustained high-stability runs, especially Repetition, signal stagnation, whereas Exploration and Specialization more often accompany evidence-seeking progress. Figure 5 illustrates the contrast in real session. The agent briefly generalizes and then immediately re-specializes (Q1Q2Q3), and the interaction then transitions into high-stability retry loop (Q3Q8) where the intent remains largely unchanged despite minor wording edits. This example helps explain why Repetition is prominent in Declarative tasks and highlights an intervention point. Detecting such loops can trigger strategy switch (e.g., to Exploration) to break the cycle. We connect these regimes to evidence reuse signals (e.g., term adoption) in Section 7.3. Pacing Implications: Move types also differ in end-to-end pacing between requests  (Table 4)  . Exploration is slower (median of 14.0s) than minor reformulations such as Repetition (6.0s), consistent with facet pivots inducing larger evidence turnover and higher processing cost. This makes strategy selection consequential, when pivots are expensive, agents may default to cheaper local edits unless they learn when refinement is no longer productive. Although explicit broadening is rare, it is often followed by re-specialization in the transition dynamics (Section 7.2), consistent with broadening acting as brief reset rather than sustained re-planning."
        },
        {
            "title": "7.2 Temporal Dynamics of Search Strategies\nWe next study how query-reformulation strategies evolve over a\nsession. Figure 4 traces the step-wise trajectory composition over\nthe first 10 steps for each intent, and Figure 6 summarizes how\nmoves transition from one step to the next.",
            "content": "Figure 6: Trajectory transition matrix (row-wise normalized). Trends over Steps: Strategy use shifts over time. Early steps mix facet pivots, retries, and constraint adjustments, then diverge by task type (Figure 4). Declarative sessions gradually concentrate on retries, consistent with late-stage stagnation; Procedural sessions maintain substantial pivoting but increasingly emphasize refinement; and Reasoning sessions sustain pivoting with consistently low retrying. We report the full step-wise proportions in Figure 4 and focus here on higher-level directional shifts. Transition Mechanisms: Figure 6 helps explain these trends by showing which moves persist as runs and which act as resets. Exploration and Repetition often form multi-step runs, consistent with the step-wise patterns in Figure 4 where pivoting and retrying persist across consecutive steps. In contrast, broadening frequently acts as brief reset, nearly half of Generalization moves are followed by Specialization, suggesting that agents relax constraints momentarily before re-introducing them. Case Study - The Reset-then-Refine Pattern: Figure 7 illustrates reset-then-refine sequence. The agent specializes broad topic by adding constraints (Napoleon campaigns Italy 1796), then generalizes by removing them (a shorter, broader query), and respecializes toward different facet (Egyptian expedition). Querylength changes match our definitions, where Specialization tends to lengthen queries, while Generalization shortens them  (Table 4)  . This is consistent with Generalization acting as lightweight backtracking to switch refinement branches rather than sustained broadening. SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Ning et al. Figure 7: reset-then-refine example: Specialization Generalization Specialization."
        },
        {
            "title": "7.3 Context-Driven Term Adoption Rate (CTAR)\nThe previous sections show that agents frequently pivot, refine, and\nretry across steps, and that these strategies shift over a sessionâ€™s\nlifespan. A central question is whether such multi-step behavior is\nevidence-grounded: if agents do not incorporate retrieved context\nwhen reformulating queries, multi-step search may degenerate into\na sequence of effectively independent single-shot queries. Thus, we\nmeasure context traceability by testing whether new query terms\nintroduced at step ğ‘˜+1 appear in the evidence context up to step ğ‘˜.\nWe use CTAR as defined in Section. 4.2 and report both last-step\nand aggregated variants.",
            "content": "Evidence of Context Grounding and Recency Effect: The CTAR analysis indicates that substantial fraction of newly introduced query terms can be lexically traced to retrieved evidence. Table 6 reports the mean CTAR under aggregated versus last-step context. Key observations are that overall more than half of newly introduced query terms are present in the aggregated evidence context (mean CTAR = 54.35%), aggregated context adds +5.81 percentage points over last-step evidence suggesting strong reliance on recent evidence with additional benefit from earlier steps [26], and CTAR varies substantially by trajectory type, with Specialization and Exploration much higher than Repetition (78.35%/69.59% vs. 20.92% under aggregated context). CTAR is intentionally lexical traceability measure (based on token overlap), rather than semantic variant, to avoid embeddingmodel dependence and uninterpretable similarity-threshold choices. As result, low CTAR score does not necessarily imply that the agent ignores evidence. Conversely, high CTAR score indicates that new terms are explicitly present in the context, but it does not by itself establish causality. Multi-step Context Contribution: We further examine how CTAR varies when tracing new terms against evidence from different historical steps within session. Figure 8 summarizes the CTAR measured against progressively older evidence contexts. For example, value of 70.93% for Spec at previous step 1 means that, for Specialization transitions, 70.93% of newly introduced terms in ğ‘ğ‘˜+1 also appear in the evidence retrieved at the immediately preceding step ğ¸ğ‘˜ (i.e., the Last Step context in Table 6); previous Table 6: Mean CTAR under Aggregated vs. Last-step Evidence. Trajectory type Specialization Generalization Exploration Repetition Overall Aggregated 78.35% 52.95% 69.59% 20.92% 54.35% Last-step 70.93% 49.44% 60.14% 19.77% 48.54% Figure 8: CTAR scores across previous steps. step 2 analogously traces against ğ¸ğ‘˜ 1, and so on. The curves suggest that for trajectory types with higher CTAR, term adoption remains non-trivial beyond the immediate previous step, indicating that historical context can contribute to query reformulation, in addition to the most recent evidence. Taken together, these results suggest that the agents query reformulation behavior is frequently consistent with evidence-grounded term adoption, where new query terms are often directly traceable to retrieved context, with strong reliance on the most recent step and additional contributions from earlier steps."
        },
        {
            "title": "8 Discussion and Implications for Agent Design\nOur findings suggest that agentic search is not merely a sequence\nof queries but a structured process of state transitions and evidence\nintegration, with three implications for agent design:",
            "content": "Repetition as Potential Stall Signal: In Declarative sessions, repetition increases to 42.68% by Step 9 (Figure 4), indicating that agents can enter near-duplicate loops when retrieved evidence does not diversify. Repetition can therefore be treated not only as redundancy but also as practical stall signal. Systems may benefit from early-detection guards that trigger strategy switch (e.g., forcing Generalization move) or escalation to human-in-theloop when lexical repetition exceeds threshold [11, 42]. Intent-Adaptive Resource Allocation: Retrieval depth is largely rigid, with 91.64% of requests using ğ¾ {1, 5, 10}, despite intentdependent needs (e.g., deeper retrieval for Procedural than Declarative). This suggests retrieval is often treated as static hyperparameter. Future architectures may adopt intent-aware budgeting that adjusts compute and retrieval depth across intents and steps, rather than relying on fixed ğ¾ choices [18]. Evidence Grounding as an Audit Signal: The gain in CTAR from aggregated context (+5.81 pp over the last-step) shows that agents actively synthesize historical evidence. Crucially, the sharp contrast in grounding between progress-oriented moves like Specialization (78.35% CTAR), and moves with lower CTAR like Repetition (20.92% CTAR), suggests that low evidence adoption is associated with retry loops. This motivates explicit context management modules that not only cache history but prioritize high-utility terms from earlier steps, to guide subsequent query formulation and reduce ungrounded exploration [44]. Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia"
        },
        {
            "title": "9 Conclusions\nWe study agentic search behavior in the wild through 14.44M DR-\nGym requests [7], converting raw logs into sessions and analyz-\ning session-level structure and step-wise query transitions. Our\ncentral takeaway is that multi-step agentic search exhibits intent-\nconditioned reformulation patterns that expose where agents make\nprogress versus stall (Sections 5â€“7). Importantly, our diversity anal-\nyses in Section 3 show that queries from this log is not simply\nrepetitions of popular benchmarks; instead, they cover a broad\nrange of semantic content, supporting the ecological validity of the\nbehavioral patterns we report. In particular, trajectories reveal a\ndrill-down bias. Agents favor local refinement and facet pivots over\ndeliberate broadening or backtracking. Declarative sessions exhibit\nsalient retry loops, while Procedural and Reasoning sessions show\ndistinct progression signatures (Sections 6â€“7). To study how multi-\nstep querying incorporates retrieved content without clicks, we\nintroduce CTAR and find that newly introduced query terms often\nalign with retrieved context across steps, suggesting non-trivial\ncross-step evidence reuse (Section 7.3).",
            "content": "These results suggest several implications for designing more reliable agentic IR systems. First, the move distributions, transition dynamics, and case studies in Section 7 highlight recurring pattern where agents often overuse inexpensive local edits rather than switching strategies. This motivates controller policies that detect high-stability loops and trigger structured interventions (e.g., forced facet pivots, purposeful backtracking, or adaptive retrieval budgeting). Second, our intent-conditioned analyses indicate that single global reformulation policy may be suboptimal; intent-aware guardrails can better balance refinement, exploration, and retrying under different tasks (Section 6). Third, CTAR offers lightweight audit signal for whether multi-step querying is evidence-driven, and can support memory designs that preserve useful context while reducing redundant retries (Section 7.3). Finally, we see several promising directions for follow-up work enabled by this study. Beyond releasing the DRGym-derived dataset and the analysis protocol, future research can connect reformulation moves to downstream answer quality, develop learning objectives that explicitly reward productive backtracking and evidencegrounded progress, and explore how retrieval parameters and context management should adapt over the course of session. More broadly, we hope these measurements and case-driven diagnostics provide foundation for intent-aware control and training of agentic search systems under reproducible retrieval settings. SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Ning et al. Log Sessionization Procedure This section describes our log sessionization pipeline, including the semantic continuity model used to link adjacent queries and the per-IP online assignment rules used to form sessions. Step 1: Train semantic continuity model. (1) Training pairs: We randomly sample 200K queries and pair each query with the nearest-in-time query from the same IP. (2) Pair labels: We label each pair as same-session vs. different-session using an LLM-as-a-judge prompt with gpt-5-nano-2025-08-07 (Appendix D.1). (3) Pair representation: We encode each query with Qwen3-Embedding0.6B [52] and use the resulting embeddings to construct fixed dense feature vector for each query pair as input to the downstream neural classifier. (4) Classifier: We train 3-layer MLP to output continuity score in [0, 1]. The hidden-layer dimensions are 1024, 512, and 256, and the model achieves held-out accuracy of 0.9419. Step 2: Sessionize online per IP (with validation). (1) Per-IP assignment: We process queries in order and maintain active sessions for each IP. For an incoming query ğ‘ğ‘¡ , we score it against each active session using the sessions most recent query and assign ğ‘ğ‘¡ to the highest-scoring session if the score 0.5; otherwise we start new session. (2) Temporal hard cutoff: Beside the continuity score, if the gap to the candidate sessions last query exceeds 10 minutes, we start new session. (3) Sanity check: We manually inspect 100 random sessions for coherence; after excluding four unusually long sessions, the remaining sessions are centered on single objective. Auxiliary Metric Definitions This section defines the auxiliary metrics used throughout our analyses and provides their formal notation and formulas for reproducibility  (Table 7)  . Notation: For session ğ‘  = (ğ‘1, . . . , ğ‘ğ‘  ), let vğ‘¡ denote the dense embedding of query ğ‘ğ‘¡ (and cos(, ) the cosine similarity). ğ‘Šğ‘¡ is the set of normalized tokens from ğ‘ğ‘¡ after lowercasing and stopword-aware tokenization WS_tok(). ğ·ğ‘¡ is the set of retrieved evidence returned for query ğ‘ğ‘¡ (at the logged retrieval depth). Table 7: Summary of auxiliary metrics used in our analyses. Metric Formula InitialFinal Gap Gap(ğ‘  ) = 1 cos(cid:0)v1, vğ‘  Dense Sim. (cid:1) DenseSim(ğ‘ğ‘¡ , ğ‘ğ‘¡ +1 ) = cos(cid:0)vğ‘¡ , vğ‘¡ +1 Jac(ğ‘ğ‘¡ , ğ‘ğ‘¡ +1 ) = ğ‘Šğ‘¡ ğ‘Šğ‘¡ +1 ğ‘Šğ‘¡ ğ‘Šğ‘¡ +1 Jaccard Sim. Result Overlap Overlap(ğ‘ğ‘¡ , ğ‘ğ‘¡ +1 ) = ğ·ğ‘¡ ğ·ğ‘¡ +1 ğ·ğ‘¡ ğ·ğ‘¡ +1 Representative Query Examples This section presents real representative query examples from our logs to help interpret our intent  (Table 8)  and trajectory  (Table 9)  labels. Table 8: Representative Queries for Intent Categories. Intent Declarative Procedural Reasoning Example Queries 1. Who owns Handi-Snacks? 2. Definition of home food store 3. Ansel Adams residences in Yosemite National Park 1. Best instructions for homemade reading shelf 2. Practical driving tips for beginners 3. How to change constitution? 1. Why is gas so expensive? 2. Does advertising help or harm us? 3. Why are minority rights important? Table 9: Representative Transitions Trajectories. Each entry illustrates step-wise reformulation (ğ‘ğ‘˜ ğ‘ğ‘˜+1). Type Specialization Generalization Exploration Repetition Step 1 2 1 2 1 2 1 2 Example Queries Recent climate data Durban Average temperature in Durban 2025 Key events that ended Hitlers dictatorship Hitlers dictatorship Headquarters of Oberoi Hotels Parent company of Oberoi Hotels Handi-Snacks parent company Who owns Handi-Snacks LLM-as-a-judge Prompts and Parsing Details This section provides the exact LLM-as-a-judge prompts for reproducibility. D.1 Query-pair Continuity Judgment Prompt [ SYSTEM ] You label query pairs for DeepResearch search agent . The agent fans out several queries to answer ONE user question . Answer YES if both queries would naturally be used for the same research task or user question ( same core topic ) , even if they cover different aspects or levels of detail . Answer NO if they would answer clearly different questions , even if they share broad words like 'WWII ', ' health ', or ' economy '. [ USER ] Query 1: << query1 >> Query 2: << query2 >> For DeepResearch agent that fans out queries to answer ONE user question , would these two queries belong to the same research task ? Answer YES or NO only . D.2 Session-level Intent Classification Prompt [ SYSTEM ] You are an expert search intent classifier . [ USER ] \" Session Queries : << joined_queries >> Classify the user intent of this session into exactly ONE of these three categories : 1. Declarative : Asking for simple facts , definitions , entity attributes , or lists (e.g., ' who is ', ' what is ', ' release date ') . 2. Procedural : Asking for steps , methods , tutorials , or guides (e. g. , ' how to ' , ' guide for ', ' fix error ') . 3. Reasoning : Asking for comparisons , planning , analysis , multi - hop reasoning , or creative generation (e.g., ' difference between ', ' best plan for ', ' why is ') . Output ONLY the category name ( Declarative , Procedural , or Reasoning ) . [ SYSTEM ] You are an expert search behavior analyst . [ USER ] Query 1 ( Previous ): << PLACEHOLDER : q_k >> Query 2 ( Current ): Analyze the search behavior evolution from Query 1 to Query 2 for an autonomous << PLACEHOLDER : q_ {k +1} > > agent . Classify the transition into exactly ONE of these four categories : 1. Specialization ( Vertical Deepening ): Query 2 is MORE specific than Query 1 by adding constraints / details ( q2 subset q1 ). (e.g., ' apple ' -> ' green apple nutritional value ') . 2. Generalization ( Vertical Broadening ): Query 2 is MORE general than Query 1 by removing constraints / abstracting ( q2 supset q1 ). (e.g., ' green apple nutritional value ' -> ' benefits of fruits ') . 3. Exploration ( Horizontal Expansion within the same domain / task ): Query 2 is NOT simply more specific or more general . It shifts to different aspect / subtopic / related entity but still within the same overall topic / domain . (e.g., ' green apple nutritional value ' -> ' green apple recipes ' or ' MRI Scans ' -> 'CT Scans ') . 4. Repetition ( Stationary ): Query 2 is semantically equivalent to Query 1. It is paraphrase , reformatting , or synonym replacement with NO significant change in intent (e .g., ' green apple value ' -> ' nutritional value of green apple ') . Output ONLY the category name ( Specialization , Generalization , Exploration , or Repetition ). (cid:1) D.3 Step-wise Trajectory Classification Prompt , ğ‘Šğ‘¡ = WS_tok(lower(ğ‘ğ‘¡ ) ) Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia References [1] [2] [3] [4] [5] Eugene Agichtein, Eric Brill, and Susan Dumais. 2006. Improving web search ranking by incorporating user behavior information. In International Conference on Research and Development in Information Retrieval (SIGIR). Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through SelfReflection. In International Conference on Learning Representations (ICLR). Paolo Boldi, Francesco Bonchi, Carlos Castillo, and Sebastiano Vigna. 2011. Query reformulation mining: models, patterns, and applications. Information Retrieval. Andrei Broder. 2002. taxonomy of web search. SIGIR Forum. Aaron Brown and Matt Saner. 2025. The Agentic AI Security Scoping Matrix: framework for securing autonomous AI systems. AWS Security Blog. Published: 21 Nov 2025. Accessed: 29 Dec 2025. (2025). https://aws.amazon.com/cn/blogs /security/the-agentic-ai-security-scoping-matrix-a-framework-for-securin g-autonomous-ai-systems/. [6] Wei-Lin Chiang et al. 2024. Chatbot Arena: An Open Platform for Evaluating [7] [8] [9] [10] LLMs by Human Preference. (2024). arXiv: 2403.04132. JoÃ£o Coelho et al. 2025. DeepResearchGym: Free, Transparent, and Reproducible Evaluation Sandbox for Deep Research. (2025). arXiv: 2505.19253. Susan Dumais, Robin Jeffries, Daniel M. Russell, Diane Tang, and Jaime Teevan. 2014. Understanding User Behavior Through Log Data and Analysis. Ways of Knowing in HCI. Carsten Eickhoff, Sebastian Dungs, and Vu Tran. 2015. An Eye-Tracking Study of Query Reformulation. In Conference on Research and Development in Information Retrieval (SIGIR). Carsten Eickhoff, Jaime Teevan, Ryen White, and Susan Dumais. 2014. Lessons from the Journey: Query Log Analysis of Within-session Learning. In International Conference on Web Search and Data Mining (WSDM). [17] [18] [13] [15] [12] [14] [16] [11] Henry A. Feild, James Allan, and Rosie Jones. 2010. Predicting Searcher Frustration. In International Conference on Research and Development in Information Retrieval (SIGIR). Steve Fox, Kuldeep Karnawat, Mark Mydland, Susan Dumais, and Thomas White. 2005. Evaluating implicit measures to improve web search. ACM Transactions on Information Systems. Google. 2025. Gemini 3 Developer Guide (model id: gemini-3-flash-preview). Google AI for Developers Documentation. Gemini 3 models in preview; model IDs listed in documentation. (2025). Retrieved Jan. 18, 2026 from https://ai.goo gle.dev/gemini-api/docs/gemini-3. Kunal Handa et al. 2025. Which Economic Tasks are Performed with AI? Evidence from Millions of Claude Conversations. (2025). arXiv: 2503.04761. Jeff Huang and Efthimis N. Efthimiadis. 2009. Analyzing and evaluating query reformulation strategies in web search logs. In Conference on Information and Knowledge Management (CIKM). Bernard J. Jansen, Amanda Spink, Judy Bateman, and Tefko Saracevic. 1998. Real life information retrieval: study of user queries on the web. SIGIR Forum. Suhas Jayaram Subramanya, Fnu Devvrit, Harsha Vardhan Simhadri, Ravishankar Krishnawamy, and Rohan Kadekodi. 2019. DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on Single Node. In Advances in Neural Information Processing Systems. Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong Park. 2024. Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). Jiahe Jin, Abhijay Paladugu, and Chenyan Xiong. 2025. Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them. (2025). arXiv: 2510.06534. Thorsten Joachims, Laura Granka, Bing Pan, Helene Hembrooke, and Geri Gay. 2005. Accurately interpreting clickthrough data as implicit feedback. In International Conference on Research and Development in Information Retrieval (SIGIR). Rosie Jones and Kristina Lisa Klinkner. 2008. Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs. In Conference on Information and Knowledge Management (CIKM). Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for OpenDomain Question Answering. (2020). arXiv: 2004.04906. Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. 2025. Fact, Fetch, and Reason: Unified Evaluation of Retrieval-Augmented Generatio. In Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL). Ido Levy, Ben wiesel, Sami Marreed, Alon Oved, Avi Yaeli, and Segev Shlomov. 2025. ST-WebAgentBench: Benchmark for Evaluating Safety and Trustworthiness in Web Agents. In Workshop on Computer Use Agents (ICML). [19] [20] [21] [22] [23] [24] [26] [27] [25] Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu. 2024. LLMs-as-Judges: Comprehensive Survey on LLM-based Evaluation Methods. (2024). arXiv: 2412.05579. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the Middle: How Language Models Use Long Contexts. Transactions of the Association for Computational Linguistics. Xiao Liu et al. 2023. AgentBench: Evaluating LLMs as Agents. (2023). arXiv: 2308.03688. Gary Marchionini. 2006. Exploratory search: from finding to understanding. Communications of the ACM. GrÃ©goire Mialon, ClÃ©mentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. GAIA: benchmark for General AI Assistants. (2023). arXiv: 2311.12983. Reiichiro Nakano et al. 2022. WebGPT: Browser-assisted question-answering with human feedback. (2022). arXiv: 2112.09332. Lunyiu Nie, Nedim Lipka, Ryan A. Rossi, and Swarat Chaudhuri. 2025. FlashResearch: Real-time Agent Orchestration for Efficient Deep Research. (2025). arXiv: 2510.05145. [30] [31] [28] [29] [32] OpenAI. 2025. GPT-5 nano Model. OpenAI API Documentation. Accessed: [33] [34] [35] [36] [37] [38] 2025-12-29. (2025). https://platform.openai.com/docs/models/gpt-5-nano. OpenAI. 2025. How People Use ChatGPT. Tech. rep. OpenAI. Arnold Overwijk, Chenyan Xiong, Xiao Liu, Cameron VandenBerg, and Jamie Callan. 2022. ClueWeb22: 10 Billion Web Documents with Visual and Semantic Information. (2022). arXiv: 2211.15848. Guilherme Penedo, Hynek KydlÃ­Äek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. 2024. The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale. (2024). arXiv: 2406.17557. Long Phan, Alice Gatti, Ziwen Han, et al. 2025. Humanitys Last Exam. (2025). https://arxiv.org/abs/2501.14249 arXiv: 2501.14249. Yujia Qin et al. 2023. ToolLLM: facilitating large language models to master 16,000+ real-world APIs. (2023). arXiv: 2307.16789. Soo Young Rieh, Kevyn Collins-Thompson, Preben Hansen, and Hye-Jung Lee. 2016. Towards searching as learning process: review of current perspectives and future directions. Journal of Information Science. [39] Daniel E. Rose and Danny Levinson. 2004. Understanding user goals in web [45] [41] [40] [43] [42] search. In International Conference on World Wide Web (WWW). Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language Models Can Teach Themselves to Use Tools. (2023). arXiv: 2302.047 61. Craig Silverstein, Hannes Marais, Monika Henzinger, and Michael Moricz. 1999. Analysis of very large web search engine query log. SIGIR Forum. Jaime Teevan, Eytan Adar, Rosie Jones, and Michael A. S. Potts. 2007. Information re-retrieval: repeat queries in yahoos logs. In Conference on Research and Development in Information Retrieval (SIGIR). Jaime Teevan, Christine Alvarado, Mark S. Ackerman, and David R. Karger. 2004. The perfect search engine is not enough: study of orienteering behavior in directed search. In Conference on Human Factors in Computing Systems (CHI). [44] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving Retrieval with Chain-of-Thought Reasoning for KnowledgeIntensive Multi-Step Questions. In Annual Meeting of the Association for Computational Linguistics (ACL). Kelsey Urgo and Jaime Arguello. 2022. Learning assessments in search-aslearning: survey of prior work and opportunities for future research. Information Processing and Management. Zhefan Wang, Ning Geng, Zhiqiang Guo, Weizhi Ma, and Min Zhang. 2025. Human vs. Agent in Task-Oriented Conversations. (2025). arXiv: 2509.17619. Zora Zhiruo Wang, Yijia Shao, Omar Shaikh, Daniel Fried, Graham Neubig, and Diyi Yang. 2025. How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations. (2025). arXiv: 2510.22780. Ryen W. White and Steven M. Drucker. 2007. Investigating behavioral variability in web search. In International Conference on World Wide Web (WWW). Jialong Wu et al. 2025. WebWalker: Benchmarking LLMs in Web Traversa. In Annual Meeting of the Association for Computational Linguistics. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents. In International Conference on Neural Information Processing Systems (NeurIPS). Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. (2023). arXiv: 2210.03629. Yanzhao Zhang et al. 2025. Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models. (2025). arXiv: 2506.05176. Yilun Zhao et al. 2025. SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks. (2025). arXiv: 2507.01001. [47] [48] [46] [50] [49] [51] [52] [53] SIGIR 26, July 2024, 2026, Melbourne Naarm, Australia Ning et al. [54] [55] Lianmin Zheng et al. 2023. Judging LLM-as-a-judge with MT-bench and Chatbot Arena. In International Conference on Neural Information Processing Systems (NeurIPS). Lianmin Zheng et al. 2024. LMSYS-Chat-1M: Large-Scale Real-World LLM Conversation Dataset. In International Conference on Learning Representations (ICLR). [56] [57] Jianan Zhou, Fleur Corbett, Joori Byun, Talya Porat, and Nejra van Zalk. 2025. Psychological and behavioural responses in human-agent vs. human-human interactions: systematic review and meta-analysis. (2025). arXiv: 2509.21542. Shuyan Zhou et al. 2023. Webarena: realistic web environment for building autonomous agents. (2023). arXiv: 2307.13854."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Instituto Superior TÃ©cnico, University of Lisbon",
        "NOVA LINCS, NOVA University Lisbon"
    ]
}