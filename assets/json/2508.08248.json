{
    "paper_title": "StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation",
    "authors": [
        "Shuyuan Tu",
        "Yueming Pan",
        "Yinming Huang",
        "Xintong Han",
        "Zhen Xing",
        "Qi Dai",
        "Chong Luo",
        "Zuxuan Wu",
        "Yu-Gang Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current diffusion models for audio-driven avatar video generation struggle to synthesize long videos with natural audio synchronization and identity consistency. This paper presents StableAvatar, the first end-to-end video diffusion transformer that synthesizes infinite-length high-quality videos without post-processing. Conditioned on a reference image and audio, StableAvatar integrates tailored training and inference modules to enable infinite-length video generation. We observe that the main reason preventing existing models from generating long videos lies in their audio modeling. They typically rely on third-party off-the-shelf extractors to obtain audio embeddings, which are then directly injected into the diffusion model via cross-attention. Since current diffusion backbones lack any audio-related priors, this approach causes severe latent distribution error accumulation across video clips, leading the latent distribution of subsequent segments to drift away from the optimal distribution gradually. To address this, StableAvatar introduces a novel Time-step-aware Audio Adapter that prevents error accumulation via time-step-aware modulation. During inference, we propose a novel Audio Native Guidance Mechanism to further enhance the audio synchronization by leveraging the diffusion's own evolving joint audio-latent prediction as a dynamic guidance signal. To enhance the smoothness of the infinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy that fuses latent over time. Experiments on benchmarks show the effectiveness of StableAvatar both qualitatively and quantitatively."
        },
        {
            "title": "Start",
            "content": "StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation Shuyuan Tu1 Yueming Pan3 Yinming Huang1 Xintong Han4 Zhen Xing1 Qi Dai2 Chong Luo2 Zuxuan Wu1 Yu-Gang Jiang1 1Fudan University 2Microsoft Research Asia 3Xian Jiaotong University 4Hunyuan, Tencent Inc. https://francis-rings.github.io/StableAvatar 5 2 0 2 1 1 ] . [ 1 8 4 2 8 0 . 8 0 5 2 : r Figure 1. Audio-driven avatar videos generated by StableAvatar, showing its power to synthesize infinite-length ID-preserving videos. The presented videos last over 3 minutes (FPS=30). Frame-X refers to the X-th frame of the synthesized avatar video."
        },
        {
            "title": "Abstract",
            "content": "Current diffusion models for audio-driven avatar video generation struggle to synthesize long videos with natural audio synchronization and identity consistency. This paper presents StableAvatar, the first end-to-end video diffusion transformer that synthesizes infinite-length highquality videos without post-processing. Conditioned on reference image and audio, StableAvatar integrates tailored training and inference modules to enable infinite-length video generation. We observe that the main reason preventing existing models from generating long videos lies in their audio modeling. They typically rely on third-party off-theshelf extractors to obtain audio embeddings, which are then directly injected into the diffusion model via cross-attention. Since current diffusion backbones lack any audio-related priors, this approach causes severe latent distribution error accumulation across video clips, leading the latent distribution of subsequent segments to drift away from the optimal distribution gradually. To address this, StableAvatar introduces novel Time-step-aware Audio Adapter that prevents error accumulation via time-step-aware modulation. During inference, we propose novel Audio Native Guidance Mechanism to further enhance the audio synchronization by leveraging the diffusions own evolving joint audio-latent prediction as dynamic guidance signal. To enhance the smoothness of the infinite-length videos, we introduce DyFigure 2. Quantitative comparisons in quality drift. a-b on the x-axis is the frame range from the a-th frame to the b-th frame. namic Weighted Sliding-window Strategy that fuses latent over time. Experiments on benchmarks show the effectiveness of StableAvatar both qualitatively and quantitatively. 1. Introduction Diffusion models [12, 21, 24, 25, 32, 40, 47, 51, 53, 59 63, 72, 7577] have significantly inspired research in audiodriven avatar video generation [6, 10, 14, 27, 30, 41, 67, 69, 71, 81]. In particular, audio-driven avatar video generation aims to synthesize natural human-centric videos with synchronized facial expressions and body movements based on reference image and audio, offering diverse applications in film production and virtual assistants. However, current approaches are limited to generating short avatar videos of less than 15 seconds, and when they attempt to generate videos longer than 15 seconds, significant body distortions and appearance inconsistencies occur, particularly in facial regions. This severely restricts their practical applications. To address this issue, several methods have explored consistency preservation for avatar video generation [10, 14, 27, 30], yet limited effort has been devoted to tackling the underlying essence of the problem. These strategieswhether leveraging motion frames [78] or adopting various shifted-window mechanisms during inferenceonly partially improve the smoothness of long videos and fail to fundamentally mitigate quality degradation in infinite-length avatar videos. One could alternatively split long audio into segments, process each independently, and then concatenate them to form continuous video. However, this approach introduces inconsistencies and abrupt transitions between segments. Thus, for audio-driven avatar video generation, end-to-end synthesis of infinite-length avatar videos while ensuring high fidelity remains an extremely challenging task. In light of this, we propose StableAvatar, consisting of dedicated modules for both training and inference to maintain ID consistency and audio synchronization for audiodriven high-quality and infinite-length avatar video generation, as shown in Fig. 3. We first observe that the primary limitation preventing previous models from synthesizing infinite-length videos lies in their audio modeling. They simply use third-party off-the-shelf extractor [1] to obtain audio embeddings, and then directly inject them into Video Diffusion Transformer via crossattention. As current diffusion backbones lack any audiorelated priors, this approach results in severe latent distribution error accumulation across video clips, leading the latent distribution of subsequent segments to gradually drift away from the target distribution. To tackle this, StableAvatar feeds audio embeddings to Video Diffusion Transformer with novel Timestep-aware Audio Adapter that dramatically reduces error accumulation over clips. In particular, initial audio embeddings (Query) sequentially crossattend with initial patchified latents (Key and Value), followed by affine modulation with timestep embeddings to obtain the refined audio embeddings. As timestep embeddings are strongly correlated with latents, this potentially forces the diffusion to model the joint audio-latent feature distribution at each timestep, effectively mitigating latent distribution error accumulation due to the lack of audio priors. Following [14, 30, 69], the refined audio embeddings (Key and Value) are injected into the diffusion model via cross-attention with latents (Query). During inference, to further enhance the audio synchronization and facial expression, StableAvatar introduces novel Audio Native Guidance Mechanism to replace the conventional Classify-Free-Guidance (CFG) [23]. As the refined audio embeddings are also inherently dependent on the latents, rather than solely relying on external audio signals, our guidance directly manipulates the diffusions sampling distribution, steering the generation towards the joint audio-latent distribution and enabling the diffusion model to refine its outputs throughout the denoising process. StableAvatar further proposes dynamic weighted slidingwindow denoising strategy that fuses latents over time to improve the smoothness in long avatar video generation. As shown in Fig. 1 and Fig. 2, while the latest audiodriven avatar video generation model OmniAvatar [14] suffers from dramatic face/body distortion and color drift, StableAvatar accurately animates the reference with natural lip synchronization driven by audio while preserving the overall consistency, even in the infinite-length video generation scenario (3500+ frames in single pass). In conclusion, our contributions are as follows: (1) We propose novel Timestep-aware Audio Adapter to force the diffusion model to capture joint audio-latent features, thereby significantly reducing latent distribution error accumulation during audio injection, making StableAvatar the first video diffusion transformer that generates infinitelength audio-driven avatar video end-to-end. (2) novel training-free Audio Native Guidance Mechanism replaces the conventional CFG to further enhance audio synchronization, while dynamic weighted sliding-window denoising strategy improves the smoothness of synthesized long videos. (4) Experimental results on benchmark datasets show the superiority of StableAvatar over the SOTA. Notably, based on Wan2.1-1.3B model [66], we surpass previous Wan2.1-14B-based models in video quality for longvideo generation. 2. Related Work Video Generation. The capacity for diversity and high fidelity in diffusion models [12, 21, 24, 25, 40, 42, 47, 51, 53, 58, 63] has sparked significant interest in their potential for video generation. Early video diffusion models [3, 5, 18, 49, 5961, 70, 74] primarily leverage the U-Net architecture for video generation by adding temporal layers to pre-trained image diffusion models for joint spatiotemporal modeling. Recent works [2, 26, 29, 66] replace the U-Net with the Diffusion-in-Transformer (DiT) architecture [44] for stronger scalability. Inspired by previous works [14, 30, 69], we utilize Wan2.1 [66] as the backbone. Audio-Driven Avatar Video Generation. Audio-driven avatar video generation aims to animate reference human image based on given audio. Early works [8, 15, 17, 43, 67, 79, 81] first convert audio into features [52, 56], and then use motion-to-video model (GAN [16]) projects these features into videos. Recently, some studies have applied diffusion models to this field. Most works [7, 27, 28, 31, 54, 71] 2 Figure 3. Architecture of StableAvatar. (a) refers to the structure of the Audio Adapter. Embeddings from the Image Encoder and Text Encoder are injected to each block of DiT. Given the audio, we extract the audio embeddings utilizing Wav2Vec. To model joint audio-latent representations, the audio embeddings are fed into the Audio Adapter, and its outputs are injected into the DiT via cross-attention. leverage the diffusion model to integrate audio cues with lips, which only supports head movement. CyberHost [33] and FantasyTalking [69] aim to animate half-body avatars. EMO2 [55] and EchomimicV2 [41] use hand pose sequences to improve half-body video quality. OmniHuman [34] applies mixed data training strategy for scaling up. HunyuanVideo-Avatar [6] and MultiTalk [30] aim to synthesize multi-character animation. OmniAvatar [14] supports full-body avatar video generation. However, previous works suffer from face/body distortion and color drift, particularly when generating long avatar videos. While some works [10, 14, 27, 30, 78] propose strategies for long video generation, their approaches are only activated during inference and primarily improve smoothness, without addressing the underlying cause of distortionlatent error accumulation. StableAvatar addresses these issues and performs infinite-length high-quality avatar video generation. Long Video Generation Strategy. Generating long videos [4, 19, 37, 38, 50, 57, 65, 80] is extremely challenging due to temporal complexity and the need for content consistency. Nuwa-XL [80] and StreamingT2V [20] both adopt short-long memory blocks to ensure long video consistency. Gen-L-Video [68] and FreeNoise [45] extend videos by merging overlapping sub-segments with sliding window. FreeLong [38] and FreeLong++ [37] blend global and local video features for consistency. However, audio-driven avatar video generation cannot directly apply the above methods, as they are designed for Text-to-Video tasks and exhibit significant domain gap with audio. 3. Method Illustrated in Fig. 3, StableAvatar is based on the commonly used Wan2.1 [66] following previous works [14, 30, 69]. The audio is first fed to Wav2Vec [1] to gain audio embeddings, which are subsequently refined for reducing latent distribution error accumulation via our Audio Adapter. The refined audio embeddings are then fed to the denoising DiT. More details are described in Sec. 3.1. Following [66], reference image is processed through the diffusion model via two pathways: (1) Concatenated with zero-filled frames along the temporal axis and transformed into latent code by frozen 3D VAE Encoder [66]. The latent code is further concatenated with compressed video frames and binary masks (the first frame is 1 and all subsequent frames are 0) in the channel axis. (2) Encoded by the CLIP Image Encoder [46] to obtain image embeddings, which are fed to each image-audio cross-attention block of denoising DiT, to modulate the synthesized appearance. We replace the original input video frames with random noise during inference, while the other inputs stay the same. We propose novel Audio Native Guidance to replace conventional CFG [23] for further facilitating lip synchronization and facial expression, as detailed in Sec. 3.2. We introduce dynamic weighted sliding-window denoising strategy that fuses latent over time to enhance the video smoothness during long video generation, as detailed in Sec. 3.3. 3.1. Timestep-aware Audio Adapter Our goal is to generate infinite-length avatar videos under the guidance of the audio while preserving the content consistency. Previous works [6, 10, 14, 27, 30, 41, 69] exhibit significant face and body distortions, along with color drift, in avatar videos longer than 15 seconds. This issue is attributed to their audio modeling, which directly injects the third-party off-the-shelf audio embeddings [1] into diffusion models via cross-attention. Current diffusion backbones [3, 29, 66] lack audio-related priors, causing significant latent distribution errors to accumulate across video clips when injecting audio embeddings into the diffusion. This results in the latent distribution of subsequent segments gradually drifting away from the optimal distribution. To address this, we propose novel Timestep-aware Audio Adapter, in which the audio embeddings go through multiple affine modulations and cross-attention blocks to interact with timestep embeddings and latents as shown in Fig. 3. Concretely, following [30, 54], we first use Wav2Vec [1] to extract the raw audio embeddings a. As the current state is influenced by preceding and succeeding audio frames, we then concatenate them proximal to the current frames, gaining audio embeddings embaud: embi aud = Concat(aik, ..., ai, ..., ai+k), (1) where 2k +1 is the context length. We further feed embaud to our Audio Adapter for addressing the error accumulation issue. Given timestep, following [66], the DiT uses two successive MLP layers to obtain overall timestep embeddings R1D and projected timestep embeddings e0 R6D. refers to the latent dimension. In diffusion pretraining, latents are tightly coupled with the timestep embeddings. Each timestep embedding corresponds to distinct latent distribution, revealing strong correlation between the latents and the timestep embeddings. Due to this strong correlation, applying timestep-aware affine modulations to embaud can implicitly bridge the joint relationship between embaud and latents zi, enabling the diffusion model to more effectively capture joint audio-latent features, thereby overcoming the scarcity of audio priors. The modulations (scale and shift) are the same as those in the DiT for domain consistency: embλ embγ aud = LN(MLP(embaud), aud = e0[2] (embλ aud (1 + e0[1]) + e0[0]) + embλ aud, (2) where LN() is Layer Norm. MLP() aims to project embaud onto the latent dimension. To further explicitly enhance joint audio-latent modeling, we perform cross-attention between embγ aud (Query) and zt (Key and Value), with the outputs modulated by e0 as follows: emb aud = CAttn(LN(embγ embη aud = MLP(LN(emb aud), zt) + LN(embγ aud) (1 + e0[4]) + e0[3]), aud), (3) emb aud = emb aud + e0[5] embη aud, where CAttn(x, y) refers to cross-attention operation, in which is the Query and is the Value and Key. To more comprehensively establish the joint relationship between latents and audio representations, emb aud is modulated by to gain the refined audio embeddings at: = Repeat(e) + r, at = MLP(emb aud (1 + e[1]) + e[0]), (4) 4 where Repeat() and refer to duplicating along the channel dimension twice and learnable parameters. We ultimately inject at into the DiT via cross attention: zt = CAttn(zt, at) + CAttn(zt, embimg), (5) where embimg refers to the image embeddings. 3.2. Audio Native Guidance To further enhance audio synchronization and facial expression, we propose novel Audio Native Guidance mechanism to replace the conventional CFG [23], which does not take joint audio-latent relationship into consideration. We modify the denoising score function [53] to steer the denoising process forward in way that maximizes both audio synchronization and naturalness. In particular, according to our Audio Adapter, at depends on the latents and the given audio. Thus, we treat at as an additional prediction of the DiT, guiding the diffusion model to capture the joint audiolatent distribution, conditioned on the external signal and model parameters. The denoising process is given by: pθ([zt, at]A) pθ([zt, at]A)pθ(A[zt, at])αpθ(atzt, A)β, (6) where pθ(), pθ(), and refer to the modified sampling process, original sampling process, and audio. α and β are guidance scales. Following Bayes Theorem, we obtain: pθ([zt, at]A)( pθ([zt, at]A)( pθ([zt, at], A) pθ(zt, A) )β )α( pθ([zt, at], A) pθ(zt, at) pθ([zt, at]A)pθ(A) pθ(zt, at) )α( pθ([zt, at]A) pθ(ztA) )β. (7) As pθ(A) is constant probability, we remove it as follows: pθ([zt, at]A)( pθ([zt, at]A) pθ(zt, at) )α( pθ([zt, at]A) pθ(ztA) )β. We further convert Eq. 8 to the score function format: (1 + α + β)θ log pθ([zt, at]A) αθ log pθ([zt, at]) βθ log pθ(ztA). (8) (9) Thus, the inference formulation can be described as: (1 + α + β)D([zt, at], y, I, A; θ) αD([zt, at], y, I, ; θ) βD([zt, ], y, I, A; θ), (10) where D(), y, and refer to the diffusion model, text prompt, and reference image. Notably, and are not guidance factors, as we find that incorporating and into the guidance substantially increases GPU resource consumption and does not significantly improve visual quality. The Audio Native Guidance mechanism regards at as an additional prediction target for the diffusion model, allowing the model to be guided by the joint audio-latent distribution, thereby ensuring that audio and latents are strongly correlated during the denoising process. It significantly reduces distribution error accumulation in audio-driven video generation, even when the base model lacks audio priors. Algorithm 1 Dynamic Weighted Sliding-Window Strategy 4. Experiments , D(), (l < L), m, y, aud , , z[0,L] 1: Input: emb[0,L] 2: for {T, . . . , 1} do 3: 4: is denoising steps start index = 0 is overlap length between windows is video clip/window length end index = + z[0,L] previous end index eprev = are noised latents is the reference image is the text log1p(x) is log(x + 1) aud , t, y, I) , emb[s,e] 5: 6: while do z[s,e] t1 = D(z[s,e] 7: if = 0 and = : 8: 9: 10: = np.linspace(0, 1, num samples=m) = np.log1p(w (np.exp(1) 1)) = wmin(w) z[s,s+m] t1 if < L: = z[s,s+m] + (1 w) z[eprev m,eprev ] t1 t1 Its for the edge case of the final clip max(w)min(w) 11: 12: 13: 14: 15: 16: 17: return z[0,L] else: break 0 eprev = e, = + (l m) = min(s + l, L) 3.3. Dynamic Weighted Sliding-Window Strategy To improve the smoothness of synthesized long avatar videos, we further propose Dynamic Weighted SlidingWindow Strategy (DWSW) during inference. Compared with the previous sling-window denoising strategy [27], overlapping latents between adjacent windows are fused using sliding window mechanism, where the fusion weights follow logarithmic interpolation based on the relative frame indices, as described in the Algorithm 1 and Fig. 8. is the VAE-compressed total video length. The fused latents are injected back into both adjacent windows, ensuring that both boundaries of the central window consist of blended features. Leveraging logarithmic weighting function introduces progressive smoothing effect in the transitions between video clips. Early stages experience more pronounced weight variation, while later stages exhibit subtle shifts, resulting in seamless continuity across video clips. 3.4. Training We use the reconstruction loss to train our model, with trainable components including attention modules of Dit and an Audio Adapter. We introduce face masks Mf ace and lip masks Mlip, extracted by Mediapipe [39] from the input video frames to enhance the modeling of face regions: = Eθ((zgt zθ) Mf ace2), 0.4 < 0.5 Eθ((zgt zθ) Mlip2), Eθ((zgt zθ) (1 + Mf ace + Mlip)2), otherwise 0.5 (11) where zgt and zε refer to diffusion latents and denoised latents, respectively. is random variable uniformly distributed over the interval [0, 1]. This piecewise objective separately supervises lip synchronization and facial expression, enabling more targeted learning. 5 4.1. Implementation Details Our training dataset is composed of three parts: Hallo3 [10], Celebv-HQ [83], and videos collected from the internet, totaling 1200 hours of video. Following previous works [10, 14, 30, 69], we evaluate our model on HDTF [82] and semibody AVSpeech [13]. Since previous works do not opensource their testing datasets, we randomly select 100 videos (5-20 seconds long) from HDTF and AVSpeech, respectively. We conduct additional experiments on 100 unseen videos (2-5 minutes long), referred to the Long100, selected from the internet to assess the robustness of our model during long avatar animation. Our DiT uses pre-trained weights of Wan2.1-I2V 1.3B [36, 66], while the Audio Encoder is trained from scratch. Our model is trained for 20 epochs on 64 NVIDIA A100 80G GPUs, with batch size of 1 per GPU. We set learning rate=1e-5, α = 4.5, and β = 3.0. 4.2. Comparison with State-of-the-Art Methods Quantitative results. Regarding metrics, we use FID [22] and FVD [64] to assess the quality of synthesized images and videos. We further use the Q-align model [73] to evaluate the video quality (IQA) and aesthetic metrics (ASE). Sync-C [9] and Sync-D [9] are utilized to assess the synchronization of lips with audio. CSIM [81] evaluates the cosine similarity between the facial embeddings of two images. We compare with recent audio-driven avatar video generation models, including GAN-based models (SadTalker [81]) and diffusion-based models (SD-based: AniPortrait [71], EchoMimic [7]; SVD-based: Sonic [27]; CogVideo-5B-based: Hallo3 [10]; HunyuanVideo-13Bbased: HunyuanAvatar [6]; Wan-14B-based: FantasyTalking [69], MultiTalk [30], OmniAvatar [14]). Based on previous studies that assess quantitative results using the selfdriven and reconstruction approach, we perform quantitative comparisons with the above competitors on HDTF [82], AVSpeech [13], and Long100. Notably, all competitors are trained on our dataset before evaluating on Long100 to ensure fair comparison. The results are shown in Table 1. We observe that, even though all competitors encounter significant drop in performance for long video generation, StableAvatar still surpasses them regarding face quality, video fidelity, and lip synchronization while maintaining relatively high single-frame quality. Specifically, Wan2.1-1.3B-based StableAvatar outperforms the leading competitor, Wan2.1-14B-based OmniAvatar, by 80.3% and 85.2% in CSIM and Sync-C on Long100. Qualitative Results. The qualitative results are shown in Fig. 4. Notably, each audio lasts 3+ minutes and is filled with intricate rhythm patterns, while the references include intricate details of appearances. We only display selected frames from the last 2 minutes for brevity. EchoMimic [7] Table 1. Quantitative comparisons on HDTF/AVSpeech/Long100. In the table elements / / c, a, b, and refer to the result on the HDTF, AVSpeech, and Long100, respectively. The average video duration of HDTF and AVSpeech is 10 seconds, while Long100 is 3 minutes. Model FID FVD CSIM Sync-C Sync-D IQA ASE SadTalker [81] Aniportrait [71] Sonic [27] EchoMimic [7] Hallo3 [10] FantasyTalking [69] HunyuanAvatar [6] MultiTalk [30] OmniAvatar [14] 53.12/120.57/194.88 46.35/118.86/190.66 62.17/187.42/278.40 63.43/108.13/178.12 44.31/98.14/170.44 46.74/80.01/175.78 52.16/77.53/172.94 46.94/75.67/175.52 41.79/72.56/168. 567/1468/2261 537/1524/2095 552/2051/2877 593/1123/1885 438/987/1724 479/823/1789 625/868/1743 446/804/1768 424/744/1621 0.821/0.802/0.423 0.815/0.796/0.415 0.843/0.825/0.451 0.849/0.837/0.456 0.845/0.834/0.462 0.863/0.853/0.468 0.866/0.859/0.472 0.868/0.861/0.465 0.862/0.857/0.471 7.25/4.13/3.21 3.88/1.95/1.13 8.16/6.04/4.03 5.44/4.59/3.64 6.58/5.16/4.42 3.42/2.98/1.92 7.20/6.74/4.34 7.53/4.88/4.12 7.50/6.78/4.45 9.86/9.72/11.16 10.84/11.58/12.87 7.65/8.94/10.23 9.27/9.78/10.74 8.64/9.62/9.92 12.15/11.422/11.78 8.39/8.28/10.07 8.02/9.59/10.18 8.26/8.05/9.62 3.12/2.40/2.31 3.82/2.28/2.10 3.28/2.24/2.08 3.64/2.97/2.31 3.50/3.38/2.35 3.55/3.24/2.42 3.56/3.63/2.46 3.54/3.72/2.42 3.55/3.74/2.51 2.14/1.43/1.38 2.41/1.31/1.22 2.05/1.36/1.13 2.23/1.82/1.43 2.11/1.96/1.36 2.28/1.89/1.48 2.25/2.21/1.52 2.15/2.25/1.45 2.27/2.29/1. Ours 38.14/68.12/57.18 375/640/504 0.875/0.872/0.849 8.15/7.56/8.24 6.94/7.85/6. 3.90/3.79/3.84 2.46/2.32/2.39 Table 2. Ablation study on core components. MF and SW are motion frame [10] and conventional sliding-window [27], which are both based on w/o Audio Adapter. Table 3. Ablation study on audio modeling. w/o Random modulation and w/o CAttn refer to replace timesteps with random learnable parameters and remove cross-attention in our Audio Adapter. Method FVD CSIM Sync-C Sync-D IQA ASE Method FVD CSIM Sync-C Sync-D IQA ASE w/o Audio Adapter w/o Guidance w/o DWSW w/ MF [10] w/ SW [27] Ours 1802 866 718 2043 1854 0.457 0.822 0.845 0.402 0.438 0.849 3.95 7.48 8.17 3.69 3.77 8.24 10.96 8.36 6.85 10.82 10.72 6. 2.34 3.74 3.79 2.28 2.31 3.84 1.40 2.31 2.36 1.32 1.35 2.39 exhibits face/body distortion and clothing changes, while the rest of competitors can accurately modify the reference lip movements within the first 15 seconds of the video. However, when the video duration exceeds 15 seconds, all competitors suffer from audio-lip synchronization issues, blurry noises, face distortion, and color drift. In particular, Hallo3 [10] and HunyuanAvatar [6] suffer from severe face distortion and audio-lip synchronization issues, with the lips moving randomly. Meanwhile, FantasyTalking [69], MultiTalk [30], and OmniAvatar [14] struggle with color drift, body/face distortion, and audio-lip synchronization issues. In contrast, our StableAvatar accurately animates images based on the given audio while preserving reference identities even after generating 3500+ frames, highlighting the superiority of our model in identity retention and in generating vivid, infinite-length avatar videos. Length Discussion. Fig. 2 shows that the quality drift in our StableAvatar remains negligible as the frame count increases, especially when compared to previous models. Theoretically, our StableAvatar is capable of synthesizing hours of video without significant quality degradation. 4.3. Ablation Study Audio Adapter. We conduct an ablation study to demonstrate the contributions of core components in StableAvatar, as shown in Table 2 and Fig. 5. Notably, all quantitative ablation studies are on the Long100 dataset. We can see that removing the core components significantly degrades performance, particularly in CSIM and Sync-C/D, highlighting that our components significantly enhance both video w/o Audio Adapter w/o Modulation w/ Random modulation w/o CAttn Ours 1802 1340 1186 1218 504 0.457 0.637 0.632 0. 0.849 3.95 5.25 5.16 6.37 8.24 10.96 9.48 9.76 8.52 6.79 2.34 3.39 3.50 3. 3.84 1.40 1.98 2.07 2.23 2.39 fidelity while preserving high ID consistency in long avatar video generation. In contrast, previous long video generation strategies (w/ MF [10] and w/ SW [27]) still suffer from dramatic appearance inconsistency and color drift, as they only basically tackle the video smoothness issue. We further conduct an ablation study regarding audio modeling, as shown in Table 3 and Fig. 6 (a). By analyzing the results, we can gain the following observations: (1) w/o Aduio Adapter dramatically degrades the video fidelity and lip synchronization. The plausible reason is that current diffusion backbones lack audio-related priors, and directly injecting audio embeddings from third-party off-the-shelf extractors into diffusion leads to significant latent error accumulation across video clips, gradually deteriorating the overall long video quality. (2) w/o Modulation/CAttn both relatively degrade the video quality. The underlying reason is that timestep-aware modulation bridges the joint audiolatent modeling, as latents and timesteps are strongly correlated. CAttn explicitly introduces latents into audio modeling, but without timestep modulation for audio embeddings, making it challenging for the model to effectively model the joint latent-audio space. Thus, timestep-aware modulation and CAttn are complementary, which is also evidenced by w/ Random modulation results. (3) StableAvatar can significantly refine the face quality while maintaining high video fidelity in long video generation since our model enables joint audio-latent modeling, reducing latent distribution error accumulation across video clips. Error Accumulation. We conduct an ablation study on error accumulation, as shown in Table 4 and Fig. 6(b). and pick the 1st-200th and 3500th-3700th frames for evalFigure 4. Qualitative comparisons with state-of-the-art methods. More examples can be found in the supplementary material. Table 4. Ablation study on the error accumulation. Adapter and refer to our Audio Adapter and Audio Native Guidance. Method FVD CSIM Sync-C Sync-D CIEDE Baseline(A) Baseline(B) w/ Adapter(A) w/ Adapter(B) w/ Adapter+G(A) w/ Adapter+G(B) 865 2388 723 912 478 0.836 0.405 0.829 0.818 0.846 0.853 7.66 3.78 8.23 7.95 8.28 8. 7.82 10.47 6.74 6.92 6.65 6.83 0.536 2.318 0.196 0.831 0.166 0. the 3500th-3700th frames. The main reason is that raw audio embeddings have conflicts with original backbone priors, triggering latent distribution error at each clip. As the number of generated frames increases, error accumulation becomes more severe, and the shift of the subsequent distribution from the target distribution grows larger. (2) w/ Adapter relatively maintains video fidelity even in the later frame intervals. It indicates that our Audio Adapter helps the diffusion model to overcome the scarcity of audio priors via timestep-aware modulation, thereby tackling the error accumulation issue. (3) Our guidance can also ensure the video quality stability in the long video generation to some extents, as it can further alleviate the latent error at each Figure 5. Ablations on core components of StableAvatar. Figure 6. Ablation study on audio modeling. uation, respectively. CIEDE [48] measures the extent of color drift. Baseline removes all our audio-related components. We have the following observations: (1) Baseline suffers from significant video quality deterioration in 7 Table 7. User preference of StableAvatar compared with other competitors. Higher indicates users prefer more to our model. Method L-A A-A B-A I-A Hallo3 [10] FantasyTalking [69] HunyuanAvatar [6] MultiTalk [30] OmniAvatar [14] 97.4% 98.1% 95.2% 98.9% 98.6% 95.8% 94.9% 98.1% 96.2% 94.5% 94.6% 97.5% 95.5% 95.2% 94.2% 96.4% 94.6% 95.6% 93.8% 95.8% models [14, 30, 69] in face quality and lip synchronization, highlighting its superiority in long avatar video generation. Full Body Avatar Videos. We conduct qualitative experiment on our StableAvatar in full/half-body avatar animation. The results are shown in Sec. A.6 of the Supp. Each protagonist in the reference image interacts with an object, such as an instrument or an apple. We can observe that our StableAvatar can handle full/half-body avatar animation in high fidelity while preserving identities even during intensive object interactions. Multi-Avatar Animation. We experiment on audio-driven multi-avatar animation, as shown in Sec. A.7 of the Supp. We can see that our model is capable of animating multiple individuals following the given audio. Cartoon Avatars. To validate the robustness of our StableAvatar, we experiment on audio-driven cartoon avatar animation, as shown in Sec. A.8 of the Supp. We can observe that our model can synthesize natural cartoon avatar videos with rich facial expressions based on the given audio. User Study. We conduct user study on 30 selected videos to evaluate the human preference between our StableAvatar and other competitors. The participants are basically university students and faculty. In each case, participants are first presented with the reference image and the audio. Then we provide two videos (one is generated by our StableAvatar and the other is synthesized by competitor) in random orders. Participants are then asked to answer the following questions: L-A/A-A/B-A/I-A: Which one has better lip/appearance/background/ID alignment with the audio/reference. Table 7 shows the superiority of our model regarding subjective evaluation. 5. Conclusion In this paper, we proposed StableAvatar, video diffusion transformer with dedicated modules for training and inference to synthesize infinite-length high-quality avatar StableAvatar first utilized off-the-shelf models videos. To overcome the scarcity to gain audio embeddings. of audio priors of diffusion backbones, StableAvatar introduced an Audio Adapter to refine audio embeddings. In inference, to further enhance lip synchronization with audio, StableAvatar introduced an Audio Native Guidance Mechanism to replace conventional ClassifyFree-Guidance. To improve the long videos smoothness, further proposed dynamic weighted StableAvatar Figure 7. Ablation study on long video generation strategies. Table 5. Ablation study on the guidance. w/ CFG replaces our Audio Native Guidance with Classify-Free-Guidance (CFG). Method FVD CSIM Sync-C Sync-D IQA ASE w/o Guidance w/ CFG [23] Ours 866 822 532 0.822 0.828 0.853 7.48 7.62 8. 8.36 7.91 6.83 3.74 3.78 3.84 2.31 2.33 2. Table 6. Ablation study on long avatar video generation methods. Method FVD CSIM Sync-C Sync-D IQA ASE Motion Frame [10] Sliding Window [27] Ours 772 698 532 0.836 0.842 0. 8.12 8.18 8.20 6.98 6.89 6.83 3.72 3.77 3. 2.23 2.31 2.39 (4) Our audio-related components ensure that even clip. after generating 3500+ frames, the video consistency and fidelity remain stable without significant degradation, certainly addressing the accumulation issue. Audio Native Guidance. To validate the significance of our Audio Native Guidance, we conduct an ablation regarding different strategies. The results are in Table 5 and Fig. 7. While conventional CFG only regards each external condition as an individual signal which is independent of latents, our guidance considers audio embeddings as correlated with the latents, accounting for the joint audio-latent distribution, thereby further facilitating the lip synchronization/naturalness and video fidelity. Long Video Strategy. We conduct comparison between our DWSW and other types of long avatar video generation strategies, as shown in Table 6 and Fig. 7. We can see that both motion frame [10, 30] and conventional sliding-window [27] fail to eliminate the jitter caused by the connection between video clips. By contrast, our DWSW leverages logarithmic interpolation to dynamically assign weights to different context windows, significantly mitigating the impact of video clip connection. More ablation studies are in Sec. A.4 of the Supp. 4.4. Applications and User Study Speed and GPU Resource. We compare the inference speed and GPU memory consumption between our StableAvatar and previous models, as shown in Sec. A.5 of the Supp. With approximately 50% of the memory consumption and 10 times the inference speed of the leading competitor OmniAvatar [14], our Wan2.1-1.3B-based StableAvatar significantly outperforms previous Wan2.1-14B-based results across sliding-window strategy. various datasets demonstrated the superiority of our model in producing infinite-length high-quality avatar videos."
        },
        {
            "title": "References",
            "content": "[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised learning of speech representations. Advances in neural information processing systems, 2020. 2, 3, 4 [2] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. 2 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3 [4] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, and Tero Karras. Generating long videos of dynamic scenes. In NIPS, 2022. 3 [5] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 2 [6] Yi Chen, Sen Liang, Zixiang Zhou, Ziyao Huang, Yifeng Ma, Junshu Tang, Qin Lin, Yuan Zhou, and Qinglin Lu. Hunyuanvideo-avatar: High-fidelity audio-driven huarXiv preprint man animation for multiple characters. arXiv:2505.20156, 2025. 1, 3, 5, 6, 8, 2 [7] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and Chenguang Ma. Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions. In AAAI, 2025. 2, 5, [8] Kun Cheng, Xiaodong Cun, Yong Zhang, Menghan Xia, Fei Yin, Mingrui Zhu, Xuan Wang, Jue Wang, and Nannan Wang. Videoretalking: Audio-based lip synchronization for talking head video editing in the wild. In SIGGRAPH Asia, 2022. 2 [9] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In ACCV, 2016. 5, 1 [10] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with video diffusion transformer. In CVPR, 2025. 1, 2, 3, 5, 6, 8 [11] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, 2019. 1 [12] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. 1, 2 [13] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William Freeman, and Michael Rubinstein. Looking to listen at the cocktail party: speaker-independent audio-visual model for speech separation. arXiv preprint arXiv:1804.03619, 2018. 5 [14] Qijun Gan, Ruizi Yang, Jianke Zhu, Shaofei Xue, and Steven Hoi. Omniavatar: Efficient audio-driven avatar video generation with adaptive body animation. arXiv preprint arXiv:2506.18866, 2025. 1, 2, 3, 5, 6, 8 [15] Yuan Gong, Yong Zhang, Xiaodong Cun, Fei Yin, Yanbo Fan, Xuan Wang, Baoyuan Wu, and Yujiu Yang. Toontalker: Cross-domain face reenactment. In ICCV, 2023. 2 [16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 2020. 2 [17] Jiazhi Guan, Zhanwang Zhang, Hang Zhou, Tianshu Hu, Kaisiyuan Wang, Dongliang He, Haocheng Feng, Jingtuo Liu, Errui Ding, Ziwei Liu, et al. Stylesync: High-fidelity generalized and personalized lip sync in style-based generator. In CVPR, 2023. 2 [18] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toIn ICLR, image diffusion models without specific tuning. 2024. [19] William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. Flexible diffusion modeling of long videos. In NIPS, 2022. 3 [20] Roberto Henschel, Levon Khachatryan, Hayk Poghosyan, Daniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. In CVPR, 2025. 3 [21] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 1, 2 [22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NIPS, 2017. 5 [23] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2, 3, 4, 8 [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 1, [25] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. JMLR, 2022. 1, 2 [26] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Cogvideo: Large-scale pretraining for and Jie Tang. text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2 [27] Xiaozhong Ji, Xiaobin Hu, Zhihong Xu, Junwei Zhu, Chuming Lin, Qingdong He, Jiangning Zhang, Donghao Luo, Yi Chen, Qin Lin, et al. Sonic: Shifting focus to global audio perception in portrait animation. In CVPR, 2025. 1, 2, 3, 5, 6, 8 9 [28] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, and Yanbo Zheng. Loopy: Taming audio-driven portrait avatar with long-term motion dependency. arXiv preprint arXiv:2409.02634, 2024. 2 [29] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, 3 [30] Zhe Kong, Feng Gao, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Xunliang Cai, Guanying Chen, and Wenhan Luo. Let them talk: Audio-driven multi-person conversational video generation. arXiv preprint arXiv:2505.22647, 2025. 1, 2, 3, 4, 5, 6, [31] Chunyu Li, Chao Zhang, Weikai Xu, Jinghui Xie, Weiguo Feng, Bingyue Peng, and Weiwei Xing. Latentsync: Audio conditioned latent diffusion models for lip sync. arXiv eprints, pages arXiv2412, 2024. 2 [32] Quanhao Li, Zhen Xing, Rui Wang, Hui Zhang, Qi Dai, and Zuxuan Wu. Magicmotion: Controllable video generation with dense-to-sparse trajectory guidance. arXiv preprint arXiv:2503.16421, 2025. 1 [33] Gaojie Lin, Jianwen Jiang, Chao Liang, Tianyun Zhong, Jiaqi Yang, Zerong Zheng, and Yanbo Zheng. Cyberhost: one-stage diffusion framework for audio-driven talking body generation. In ICLR, 2025. 3 [34] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. Omnihuman-1: Rethinking the scaling-up of one-stage conditioned human animation models. arXiv preprint arXiv:2502.01061, 2025. 3 [35] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 1 [36] Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Gen Li, Siyu Zhou, Qian He, and Xinglong Wu. Phantom: Subject-consistent video generation via crossmodal alignment. arXiv preprint arXiv:2502.11079, 2025. [37] Yu Lu and Yi Yang. Freelong++: Training-free long video arXiv preprint generation via multi-band spectralfusion. arXiv:2507.00162, 2025. 3 [38] Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. In NIPS, 2024. 3 [39] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, ChuoLing Chang, Ming Guang Yong, Juhyun Lee, et al. Mediapipe: framework for building perception pipelines. arXiv preprint arXiv:1906.08172, 2019. 5 [40] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2021. 1, 2 [41] Rang Meng, Xingyu Zhang, Yuming Li, and Chenguang Ma. Echomimicv2: Towards striking, simplified, and semi-body human animation. In CVPR, 2025. 1, [42] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, 2021. 2 [43] Youxin Pang, Yong Zhang, Weize Quan, Yanbo Fan, Xiaodong Cun, Ying Shan, and Dong-ming Yan. Dpe: Disentanglement of pose and expression for general video portrait editing. In CVPR, 2023. 2 [44] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 2 [45] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169, 2023. 3 [46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 2 [48] Gaurav Sharma, Wencheng Wu, and Edul Dalal. The ciede2000 color-difference formula: Implementation notes, supplementary test data, and mathematical observations. Color Research & Application: Endorsed by Inter-Society Color Council, The Colour Group (Great Britain), Canadian Society for Color, Color Science Association of Japan, Dutch Society for the Study of Color, The Swedish Colour Centre Foundation, Colour Society of Australia, Centre Francais de la Couleur, 30(1):2130, 2005. 7 [49] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2 [50] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: continuous video generator with the price, image quality and perks of stylegan2. In CVPR, 2022. 3 [51] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 1, 2 [52] Linsen Song, Wayne Wu, Chaoyou Fu, Chen Change Loy, and Ran He. Audio-driven dubbing for user generated contents via style-aware semi-parametric synthesis. IEEE Transactions on Circuits and Systems for Video Technology, 33(3): 12471261, 2022. [53] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. 1, 2, 4 [54] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions. In ECCV, 2024. 2, 4 [55] Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, and Liefeng Bo. Emo2: End-effector guided audio-driven avatar video generation. arXiv preprint arXiv:2501.10687, 2025. 3 [56] Luan Tran and Xiaoming Liu. Nonlinear 3d face morphable model. In CVPR, 2018. 2 10 [57] Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, JiaBin Huang, and Johannes Kopf. Consistent view synthesis with pose-guided diffusion models. In CVPR, 2023. 3 [58] Shuyuan Tu, Qi Dai, Zuxuan Wu, Zhi-Qi Cheng, Han Hu, and Yu-Gang Jiang. Implicit temporal modeling with learnable alignment for video recognition. In ICCV, 2023. 2 [59] Shuyuan Tu, Qi Dai, Zhi-Qi Cheng, Han Hu, Xintong Han, Zuxuan Wu, and Yu-Gang Jiang. Motioneditor: Editing video motion via content-aware diffusion. In CVPR, 2024. 1, [60] Shuyuan Tu, Qi Dai, Zihao Zhang, Sicheng Xie, Zhi-Qi Cheng, Chong Luo, Xintong Han, Zuxuan Wu, and Yu-Gang Jiang. Motionfollower: Editing video motion via lightweight score-guided diffusion. arXiv preprint arXiv:2405.20325, 2024. [61] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. Stableanimator: Highquality identity-preserving human image animation. In CVPR, 2025. 2 [62] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, Zuxuan Wu, and Yu-Gang Jiang. Stableanimator++: Overcoming pose misalignment and face arXiv preprint distortion for human image animation. arXiv:2507.15064, 2025. [63] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Plug-and-play diffusion features for text-driven Dekel. image-to-image translation. In CVPR, 2023. 1, 2 [64] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 5 [65] Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal. Mcvd-masked conditional video diffusion for prediction, generation, and interpolation. In NIPS, 2022. [66] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3, 4, 5 [67] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng Luo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei Yang. V-express: Conditional dropout for progressive training of portrait video generation. arXiv preprint arXiv:2406.02511, 2024. 1, 2 [68] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264, 2023. 3 [69] Mengchao Wang, Qiang Wang, Fan Jiang, Yaqi Fan, Yunpeng Zhang, Yonggang Qi, Kun Zhao, and Mu Xu. Fantasytalking: Realistic talking portrait generation via coherent motion synthesis. arXiv preprint arXiv:2504.04842, 2025. 1, 2, 3, 5, 6, 8 [70] Weimin Wang, Jiawei Liu, Zhijie Lin, Jiangqiao Yan, Shuo Chen, Chetwin Low, Tuyen Hoang, Jie Wu, Jun Hao Magicvideo-v2: MultiLiew, Hanshu Yan, et al. arXiv preprint stage high-aesthetic video generation. arXiv:2401.04468, 2024. [71] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animation. arXiv preprint arXiv:2403.17694, 2024. 1, 2, 5, 6 [72] Zejia Weng, Xitong Yang, Zhen Xing, Zuxuan Wu, and Genrec: Unifying video generation arXiv preprint Yu-Gang Jiang. and recognition with diffusion models. arXiv:2408.15241, 2024. 1 [73] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023. 5, 1 [74] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning In of image diffusion models for text-to-video generation. CVPR, 2023. 2 [75] Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and Yu-Gang Jiang. Simda: Simple diffusion adapter for efficient video In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 7827 7839, 2024. [76] Zhen Xing, Qi Dai, Zejia Weng, Zuxuan Wu, and YuGang Jiang. Aid: Adapting image2video diffusion models for instruction-guided video prediction. arXiv preprint arXiv:2406.06465, 2024. [77] Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. survey on video diffusion models. ACM Computing Surveys, 57(2):142, 2024. 1 [78] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei Zhang, Ce Liu, Jingdong Wang, Yao Yao, and Siyu Zhu. Hallo: Hierarchical audio-driven visual synthesis for portrait image animation. arXiv preprint arXiv:2406.08801, 2024. 2, 3 [79] Fei Yin, Yong Zhang, Xiaodong Cun, Mingdeng Cao, Yanbo Fan, Xuan Wang, Qingyan Bai, Baoyuan Wu, Jue Wang, and Yujiu Yang. Styleheat: One-shot high-resolution editable talking face generation via pre-trained stylegan. In ECCV, 2022. 2 [80] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. 3 [81] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learning realistic 3d motion coefficients for stylized audiodriven single image talking face animation. In CVPR, 2023. 1, 2, 5, 6 [82] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with highresolution audio-visual dataset. In CVPR, 2021. 5 [83] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. CelebVHQ: large-scale video facial attributes dataset. In ECCV, 2022. 5, 1 12 Figure 8. The pipeline of our DWSW. A. Supplementary Material A.1. Preliminaries The diffusion model includes forward diffusion process and reverse denoising process. In the forward process, inspired by Rectified Flow [35], the Gaussian noise is progressively added to the data sample x0 pdata from the particular data distribution pdata: xt = (1 t)x0 + te, (12) where [0, 1] refers to the timestep and is sampled from standard-normal distribution (0, 1). The data sample x0 is ultimately converted into Gaussian noise xT (0, 1) after diffusion forward steps. In the reverse process, the diffusion model εθ(xt, t) is trained to predict the velocity = dxt/dt conditioned on the noisy latents xt and the timestep t. The MSE loss is applied to train ε(): = Ex0,ε,t((cid:13) (cid:13)v εθ(xt, t)(cid:13) (cid:13) 2). (13) Figure 9. Examples from Long100. Table 8. Ablation study on different weight assignment. Model FVD CSIM Sync-C Sync-D IQA ASE Fixed weights Uniformed gap Ours 705 594 532 0.838 0.847 0.853 8.08 8. 8.20 7.18 6.94 6.83 3.73 3.82 3.84 2.25 2. 2.39 close-up shots against varied indoor and outdoor settings. In contrast to existing open-source testing datasets (HDTF and AVSpeech), our Long100 contains relatively complicated audio rhythms and intricate protagonist appearances. Moreover, the average duration of Long100 (2-5 minutes) is significantly longer than existing open-source testing datasets (2-60 seconds). Long100 also involves multiple humanobject interactions, such as instruments, making it more challenging to maintain identity consistency. A.2. Sling Window Details A.4. Additional Ablation on DWSW 8 illustrates detailed pipeline of our Dynamic Fig. Weighted Sliding Window Denoising Strategy (DWSW). A.3. Details of Dataset Regarding the training dataset, our training dataset consists of three parts, including Hallo3 [10], Celebv-HQ [83], and collected videos from the internet (BilBil, YouTube, and TikTok). We leverage SyncNet [9] and Q-Align [73] to filter for higher-quality videos by assessing lip-sync accuracy with audio and video fidelity. We also employ InsightFace [11] to filter out videos with facial confidence score below 0.9. Ultimately, we obtain the refined training dataset, containing approximately 1200 hours of videos. In terms of the testing dataset, we select 100 unseen videos (2-5 minutes long, FPS=30) from the internet to construct the testing dataset Long100. Some examples are shown in Fig. 9. The sources of videos come from numerous social media platforms, including YouTube, TikTok, and BiliBili. These videos showcase individuals across ethnicities, genders, portrayed in full-body, half-body, and We further conduct an ablation study on the weight assignment of our Dynamic Weighted Sliding Window Denoising Strategy (DWSM),as shown in Table 8. Fixed weights set all weights of context window to 0.5 and Uniformed gap refers to the weights of each element in the context window following an arithmetic progression. We can observe that our logarithmic weighting function achieves the best performance, indicating that the logarithmic function prioritizes the fusion of elements from the previous context window in the early stages, while progressively and smoothly incorporating both context windows in the later stages, thereby facilitating the overall long video smoothness. A.5. Speed and GPU Resource. We compare the inference speed and GPU memory consumption between our StableAvatar and previous models, as shown in Table 9. Despite being built on significantly smaller backbone (Wan2.1-1.3B), StableAvatar achieves superior face quality and lip synchronization compared to prior Wan2.1-14B-based models [14, 30, 69], while requir1 the protagonist wears mask, making it particularly challenging for the model to perform accurate lip synchronization with audio. Moreover, the reference protagonists in the fourth row and the sixth row of Fig. 16 face the camera at an angle, making it dramatically difficult for the avatar animation model to preserve ID consistency. It is noticeable that our StableAvatar can accurately manipulate the lip, facial expression, and body gesture in the reference image while preserving high-quality identity consistency, even in specific cases involving head shaking and external object interaction. Moreover, StableAvatar supports simultaneously animating the avatar and background following the given audio via text prompt description. For example, in the reference images in the second row of Fig. 13 and the sixth row of Fig. 16, StableAvatar utilizes text prompts to animate contextual elementsfor instance, making the scenery outside car window shift as the vehicle starts moving, or generating dynamic ocean waves along the coastline. A.10. Limitation and Future Work Fig. 18 shows one failure case of our StableAvatar. In the given reference image, the protagonist is non-human creature (fantastical creature), which has significant appearance and structure discrepancies with the common human. Our StableAvatar struggles to find its lip, thereby failing to synchronize its lip with the audio. One potential solution is to introduce an additional reference network to explicitly capture the semantic details of the reference images. This part is left as future work. A.11. Ethical Concern Our StableAvatar can animate the given reference image based on the given audio, which can be implemented in various fields, including virtual reality and digital human creation. However, the potential misuse of this model, particularly for creating misleading content on social media platforms, is concern. To mitigate this, it is essential to use sensitive content detection algorithms. Table 9. Comparisons in inference latency and GPU resources. Inference latency (minutes) and GPU memory are measured for generating 81 frames at 480832 resolution. Method CSIM Sync-C Sync-D GPU Mem Speed Hallo3 [10] FantasyTalking [69] HunyuanAvatar [6] MultiTalk [30] OmniAvatar [14] Ours 0.462 0.468 0.472 0.465 0.471 0.849 4.42 1.92 4.34 4.12 4.45 8. 9.92 11.78 10.07 10.18 9.62 6.79 49.8G 44.7G 26.6G 49.21G 40.83G 18.4G 13.72 16.40 21.98 23.0 20.31 2. ing only half the memory and achieving tenfold increase in inference speed over the leading competitor OmniAvatar [14]. This demonstrates its strong advantage in longform avatar video generation. A.6. Full/Half-Body Avatar Video Results We perform qualitative experiment in audio-driven full/half-body avatar animations, as shown in Fig. 10. Each reference protagonist interacts with an object, such as an apple or piano, making it more challenging to preserve appearance consistency and lip synchronization with the given audio. We can see that our model is capable of synthesizing full/half-body avatar videos, even involving interactions with external objects. A.7. Multi-Avatar Animation To demonstrate the robustness of our StableAvatar, we experiment on particular reference image involving multiple protagonists, as shown in Fig. 11. We can see that our StableAvatar is also capable of handling audio-driven multipleavatar animations while preserving the original identity and achieving high video fidelity. A.8. Cartoon Avatar Video Generation To validate the diversity of our StableAvatar, we perform experiments on two reference images involving cartoon protagonists, as shown in Fig. 12. We can observe that our StableAvatar can also handle audio-driven cartoon avatar video generation, highlighting the diversity of our StableAvatar. A.9. Long Avatar Video Results Fig. 13, Fig. 14, Fig. 15, Fig. 16, and Fig. 17 show additional audio-driven long avatar videos synthesized by our StableAvatar. Notably, each audio lasts over 3 minutes, and with an FPS of 30, this results in 5400+ synthesized frames. We only display selected frames from the last 10 seconds for brevity. Each case contains complex protagonists appearance and an intricate audio rhythm pattern. We can see that our StableAvatar can perform wide range of audio-driven avatar animation while simultaneously preserving the protagonists appearance, background, and identity, even after synthesizing 5000+ frames. For example, in the reference image in the third row of Fig. 13, 2 Figure 10. Full/Half-body avatar animation results. The images with red borders are the reference images. Figure 11. Audio-driven multiple-avatar animation results. The images with red borders are the reference images. 3 Figure 12. Audio-driven cartoon avatar animation results. The images with red borders are the reference images. Figure 13. Audio-driven long avatar video results (1/5). The images with red borders are the reference images. 4 Figure 14. Audio-driven long avatar video results (2/5). The images with red borders are the reference images. Figure 15. Audio-driven long avatar video results (3/5). The images with red borders are the reference images. 5 Figure 16. Audio-driven long avatar video results (4/5). The images with red borders are the reference images. 6 Figure 17. Audio-driven long avatar video results (5/5). The images with red borders are the reference images. Figure 18. One failure case of our StableAvatar. The images with red borders are the reference images."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Hunyuan, Tencent Inc.",
        "Microsoft Research Asia",
        "Xian Jiaotong University"
    ]
}