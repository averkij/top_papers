{
    "paper_title": "Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning",
    "authors": [
        "Chengqi Lyu",
        "Songyang Gao",
        "Yuzhe Gu",
        "Wenwei Zhang",
        "Jianfei Gao",
        "Kuikun Liu",
        "Ziyi Wang",
        "Shuaibin Li",
        "Qian Zhao",
        "Haian Huang",
        "Weihan Cao",
        "Jiangning Liu",
        "Hongwei Liu",
        "Junnan Liu",
        "Songyang Zhang",
        "Dahua Lin",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning abilities, especially those for solving complex math problems, are crucial components of general intelligence. Recent advances by proprietary companies, such as o-series models of OpenAI, have made remarkable progress on reasoning tasks. However, the complete technical details remain unrevealed, and the techniques that are believed certainly to be adopted are only reinforcement learning (RL) and the long chain of thoughts. This paper proposes a new RL framework, termed OREAL, to pursue the performance limit that can be achieved through \\textbf{O}utcome \\textbf{RE}w\\textbf{A}rd-based reinforcement \\textbf{L}earning for mathematical reasoning tasks, where only binary outcome rewards are easily accessible. We theoretically prove that behavior cloning on positive trajectories from best-of-N (BoN) sampling is sufficient to learn the KL-regularized optimal policy in binary feedback environments. This formulation further implies that the rewards of negative samples should be reshaped to ensure the gradient consistency between positive and negative samples. To alleviate the long-existing difficulties brought by sparse rewards in RL, which are even exacerbated by the partial correctness of the long chain of thought for reasoning tasks, we further apply a token-level reward model to sample important tokens in reasoning trajectories for learning. With OREAL, for the first time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL, being on par with 32B models. OREAL-32B also surpasses previous 32B models trained by distillation with 95.0 pass@1 accuracy on MATH-500. Our investigation also indicates the importance of initial policy models and training queries for RL. Code, models, and data will be released to benefit future research\\footnote{https://github.com/InternLM/OREAL}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 1 8 7 6 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Exploring the Limit of Outcome Reward\nfor Learning Mathematical Reasoning",
            "content": "Chengqi Lyu1 Songyang Gao1 Yuzhe Gu1,2 Wenwei Zhang1 Jianfei Gao1 Kuikun Liu1 Ziyi Wang1 Shuaibin Li1 Qian Zhao1 Haian Huang1 Weihan Cao1 Jiangning Liu1 Hongwei Liu1 Junnan Liu1 Songyang Zhang1 Dahua Lin1,3,4 Kai Chen1 1Shanghai AI Laboratory 2Shanghai Jiao Tong University 3MMLab, The Chinese University of Hong Kong 4HKGAI under InnoHK {lvchengqi,gaosongyang,guyuzhe,zhangwenwei,chenkai}@pjlab.org.cn"
        },
        {
            "title": "Abstract",
            "content": "Reasoning abilities, especially those for solving complex math problems, are crucial components of general intelligence. Recent advances by proprietary companies, such as o-series models of OpenAI, have made remarkable progress on reasoning tasks. However, the complete technical details remain unrevealed, and the techniques that are believed certainly to be adopted are only reinforcement learning (RL) and the long chain of thoughts. This paper proposes new RL framework, termed OREAL, to pursue the performance limit that can be achieved through Outcome REwArd-based reinforcement Learning for mathematical reasoning tasks, where only binary outcome rewards are easily accessible. We theoretically prove that behavior cloning on positive trajectories from best-of-N (BoN) sampling is sufficient to learn the KL-regularized optimal policy in binary feedback environments. This formulation further implies that the rewards of negative samples should be further reshaped to ensure the gradient consistency between positive and negative samples. To alleviate the long-existing difficulties brought by sparse rewards in RL, which are even exacerbated by the partial correctness of the long chain of thought for reasoning tasks, we further apply token-level reward model to sample important tokens in reasoning trajectories for learning. With OREAL, for the first time, 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL, being on par with 32B models. OREAL-32B also surpasses previous 32B models trained by distillation with 95.0 pass@1 accuracy on MATH-500. Our investigation also indicates the importance of initial policy models and training queries for RL. Code, models, and data will be released to benefit future research1."
        },
        {
            "title": "Introduction",
            "content": "Solving complex problems with reasoning capability forms one of the cornerstones of human cognition - cognitive ability that artificial general intelligence must ultimately master [1, 2]. Among various problem domains, the mathematical problem emerges as particularly compelling experimental paradigm for AI research [36], owing to its relatively well-defined structure and availability of precise binary correctness feedback based on the verifiable final answers. Recent advances in large language models (LLMs) have achieved remarkable progress in mathematical reasoning by the chain-of-thought technics [79], in which the LLMs are elicited to produce series of intermediate reasoning steps before providing the final answers to the problem. However, as most of the capable models (e.g., the o-series models by OpenAI [10]) are developed by proprietary Equal contribution Corresponding author 1https://github.com/InternLM/OREAL Figure 1: Overall performance between OREAL-32B and some competitive baselines. companies, there is no clear pathway to develop state-of-the-art reasoning models. Some recent work shows that distillation [11, 12] is sufficient to obtain high performance given the accessibility to existing best or near best AI models, reinforcement learning (RL) is believed to be more fundamental approach and has exhibited potential [13] to advance beyond the intelligence boundary of current AI models, using the most capable open-source foundation models (DeepSeek-V3-base [14], inter alia). However, fundamental challenges of sparse reward in RL persist and are even exacerbated in mathematical reasoning tasks that mainly rely on the chain of thought technics with LLMs [7]: the evaluation of intermediate reasoning steps is labor intensive [15] and its accurate automation approach is still under-explored, thus, the only reliable reward is based on the outcome (correctness of final answer), which is inherently binary and sparse when faced with more than 2000 tokens in the long reasoning trajectories [13, 16]. Existing approaches have attempted to estimate the advantages or values of reasoning steps by search [17, 18] or value function-based credit assignment [19, 20], yet, their performance remains unsatisfactory in comparison with the distilled models [13]. This paper aims to conquer the above challenges and proposes simple framework, termed OREAL, to push the limit of Outcome REwArd-based reinforcement Learning for mathematical reasoning tasks. OREAL is grounded in the unique characteristics of mathematical reasoning tasks that binary outcome feedback creates an environment where all positive trajectories are equally valid. We first establish that behavior cloning on BoN-sampled positive trajectories is sufficient to achieve KL-regularized optimality, which emerges from the analysis that the positive trajectory from BoN sampling converges to distribution independent of the sample number. For learning on negative samples, OREAL reveals the necessity of reward shaping to maintain consistent gradient estimation between sampling and target distributions. Such mechanism compensates for BoNs under-sampling of negative gradients, and enables difficulty-adaptive optimization over both successful and failed trajectories. Another intrinsic property of mathematical reasoning tasks is the partial correctness in long reasoning chains, which further imposes the learning difficulty of sparse rewards when only binary outcome reward is available at each iteration of RL training. Thus, OREAL adopts lightweight credit assignment scheme through token-level reward model trained using outcome rewards. This mechanism automatically estimates step-wise importance weights by decomposing trajectory advantages, enabling focused learning of critical reasoning steps or errors. The integration of these components yields theoretically sound framework that effectively bridges the gap between sparse binary feedback and dense policy optimization requirements for mathematical reasoning tasks. Extensive experimental results show that OREAL effectively improves the mathematical reasoning capability of LLMs. At the 7B parameter scale, to the best of our knowledge, OREAL-7B is the first to obtain the pass@1 accuracy on MATH-500 [21] to 91.0 using RL instead of distillation, which even exceeds QwQ-32B-Preview [22] and o1-mini [10]. OREAL also improves DeepSeekR1-Distilled-Qwen-7B from 92.8 to 94.0 pass@1 accuracy, being on par with the previous best 32B 2 models. For the 32B model, OREAL-32B outperforms all previous models (Figure 1), both distilled and RL-based, obtaining new state-of-the-art results with 95.0 pass@1 accuracy on MATH-500."
        },
        {
            "title": "2 Methods",
            "content": "To obtain deeper understanding of the challenges when applying reinforcement learning (RL) for solving math word problems, we first analyze the formulation of RL and the intrinsic properties of underlying binary feedback environments (2.1), and establish theoretical foundation for our optimization framework about how to learn from positive samples (2.2) and failure trials (2.3). To further conquer the learning ambiguity brought by outcome rewards to the partially correct long reasoning chains, we adopt new strategy to estimate the importance of tokens for learning (2.4). 2.1 Preliminary When adopting large language model (LLM) for mathematic reasoning, the input to the LLM policy is textual math problem that prompts the LLM to output multi-step reasoning trajectory consisting of multiple tokens as actions. During RL training, common practices [6, 23] conduct sampling on the LLM to produce multiple reasoning trajectories, assign binary feedback (0/1 reward) based solely on the correctness of their final answer, and perform corresponding policy optimization using the sampled trajectories with reward. Policy Optimization. Consider Markov Decision Process (MDP) defined by the tuple (S, A, P, r, γ), where is finite state space (e.g., contextual steps in mathematical reasoning), is the action space (i.e. the token space of LLMs), (ss, a) specifies the state transition dynamics, : is the reward function, and γ [0, 1) denotes the discount factor. In this section, we focus on KL-regularized policy optimization, which maximizes the expected cumulative returns while regularizing the policy πθ(s) toward reference policy π0(s). The objective function is formulated as: J(θ) Esρ0,aπθ(s) [Qπθ (s, a)] α Esρ0 [DKL (πθ(s)π0(s))] (1) with the state-action value function Qπ(s, a) = Eπ vanilla policy π. This objective admits closed-form solution for optimal policy π: k=0 γkr(st+k, at+k) st = s, at = a(cid:3) under (cid:2)(cid:80) π(as) = π0(as) exp (Qπ(s, a)/α) Z(s) , (2) where Z(s) = Eaπ0(s) [exp (Qπ(s, a)/α)] is the partition function that ensures normalization. Best-of-N (BoN) Sampling. As common and efficient strategy to sample multiple reasoning trajectories from LLMs, Best-of-N sampling selects the trajectory with maximal reward among independent rollouts from π0 to enhance policy performance. Formally, given candidate actions {a(i)}n i=1 π0(s), the chosen action is = arg maxa(i) Q(s, a(i)). This strategy effectively leverages the exploration-exploitation trade-off through parallel sampling [24, 25]. Binary Feedback under Outcome Supervision. Though reasoning trajectory usually contains multiple reasoning steps with thousands of tokens, there lacks an efficient approach to automatically label the correctness of each token or reasoning step in math reasoning tasks. Thus, practical way is to parse the final answer from the reasoning trajectory [13, 26], evaluate its correctness based on rules or models, and then provide an outcome reward at the end of the trajectory as below: R(st) = (cid:26)1 if is the end step and the answer is correct otherwise, (3) which intrinsically treats the correct trajectory equally for learning. Moreover, the reward signal is severely sparse when compared to the thousands of tokens and does not provide any signal of progress or correctness for intermediate steps. The resulting reward distribution of trajectories is also different from that of the dense reward function constructed through preference pairs in traditional RL for large language models [27], which induces more appropriate optimization framework for mathematical reasoning tasks, discussed in the next section. 3 2.2 Learning from Positive Samples Building upon the reward equivalence principle stated in Eq. 3, we first formalize key probabilistic characteristic of BoN sampling: Lemma 2.1. Let π(θ, s) be distribution over parameters θ and trajectory s, where each is associated with binary reward R(s) {0, 1}. Define Esπ(θ,)[R(s) = 1] > 0. Consider the BoN sampling: = n0 and sample {s1, s2, . . . , sn} i.i.d. from πθ. BoN selects uniformly from the subset with R(si) = 1. We have that, The probability of selecting is converge to π(θ,s) , which is independent of n. The proof follows directly from the union law of BoN sampling (BoNn+m = BoN2(BoNm, BoNn)) and the trivial distinguishability of 0 1 rewards. This result reveals that for problems with attainable positive responses, we are using BoN generator with an arbitrary sampling budget to construct the positive training samples. To quantify the distributional divergence induced by BoN sampling, prior work [2830] has analyzed the KL divergence between the BoN distribution πBoN and the original policy π. For continuous trajectory spaces S, the BoN distribution admits the explicit form: πBoN(s) = [P (s)]n1 π(s), (4) where (s) denotes the cumulative distribution function (CDF) associated with π(s). The corresponding KL divergence is given by KL(πBoN π) = log 1 . The KL divergence expression possesses two crucial properties, as , the KL(πBoN π) is strictly increasing in for 1, and converge to log , covering the entire positive real axis. This implies that for any prescribed KL divergence constraint ϵ > 0, there exists BoN parameterization for approximating the optimal policy, and can be simply sampled with the minimal n(ϵ) satisfying that n(ϵ) = arg min EsπBoN [R(s)]. BoNBoN [31] empirically shows that BoN sampling achieves the optimal win rate under fixed KL constraint by exhaustive search over the positive support. Therefore, the behavior cloning on BoN-selected positive samples directly learns the analytic solution to Eq. 1. Intuitively, since every correct answer is preferred identically in the outcome-supervised sense, we only need to sample until we get positive example, whose generating probability distribution will be the same as randomly picking from arbitrarily large numbers of samples. Based on the theoretical understanding established, we formulate the first component of the learning objective in OREAL by incorporating KL-constrained max-likelihood-objective over positive examples obtained through sampling: L1(θ) = EsD+ [ log πθ(s)] (cid:124) (cid:125) (cid:123)(cid:122) Positive example alignment , +β KL(πθ πold) (cid:125) (cid:123)(cid:122) Policy constraint (cid:124) where D+ denotes the set of positive trajectories selected via BoN sampling from RL rollouts. 2.3 Learning from Negative Samples As established in Section 2.2, direct behavioral cloning on positive responses can effectively recover the policy distribution. BOND [32] proposes estimating Jeffreys divergence [33] for the BoN strategy to train with both positive and negative samples, and demonstrates that signals from unsuccessful trajectories provide critical information about decision boundaries and failure modes. In this section, we will discuss the relationship between the BoN (Best-of-N) distribution and the optimization objective defined in Eq. 1, then elucidate the necessity of reward reshaping when training with negative samples. Notably, while Eq. 4 shares structural similarities with Eq. 2, its application to mathematical reasoning tasks with binary feedback requires reformulation. Specifically, the transformed BoN distribution can be expressed as (cid:20) πbon(s) = π(s) R(s) 1 (1 p)n + (1 R(s)) (1 p)n (cid:21) , (5) which reveals fundamental differences between the BoN distribution and the original sampling distribution. Consider scenario where two correct and two incorrect solutions are sampled, yielding an empirical accuracy of 50%. However, the probability of selecting negative samples under Bestof-4 becomes (0.5)4 = 6.25%, significantly lower than the original distribution. This discrepancy necessitates reward shaping to maintain consistency between our optimization target and the expected return under the BoN distribution. Building on BoN-RLBs [34] application of the log-likelihood trick for BoN-aware policy gradients, we analyze the reward shaping technic for negative samples to maintain gradient consistency with Section 2.2. With expectation return follow the definition in Lemma 2.1. The policy gradient under the BoN distribution can be derived as θJbon = Esπbon() [R(s)θ log πbon(s)] (cid:20) R(s)θ = Esπbon() (cid:18) ID+(s) log π(s) 1 (1 p)n + ID(s) log π(s)(1 p)n1 (cid:19)(cid:21) , (6) where ID+(s) and ID (s) denote indicator functions for positive and negative sample sets respectively. Notably, these indicators are independent of policy parameters θ. Given Esπbon[ID+(s)] = 1 (1 p)n, we derive the gradient components as Esπbon (cid:18) (cid:20) θ ID+(s) log π(s) (cid:19)(cid:21) 1 (1 p)n = n(1 p)n1Esπ,sD+ [θ log π(s)] . Similarly, we also have Esπbon (cid:2)θ (cid:0)ID (s) log π(s)(1 p)n1(cid:1)(cid:3) = n(1 p)nEsπ,sD [θ log π(s)] . This derivation reveals that when assigning unit reward (R(s) = 1) to positive samples, gradient consistency requires reshaping negative sample rewards to R(s) (1 p)R(s). Based on this reward shaping, we can construct policy optimization both on positive and negative samples for optimal policy. To obtain the parameter 1 which can be linked to the Monte-Carlo (MC) advantage estimation, we can simply estimate that probability by calculating the expected accuracy on the sample space by counting small number of responses. In this paper we apply similar setting to RLOO [35], namely RRLOO(s) = R(s) 1 s=s R(s) for unbiased mean reward and train with policy gradient. The second part of our OREAL objective is then formulated as below: (cid:20) 1 (cid:80) (cid:21) L2(θ) = EsS (1 p) log + βKL(πθ πold), πθ(s) πold(s) where = Pθπ[R(θ) = 1], is the failed subset generated by policy model, and represents the preprocessing for advantage scores to serve as generalized form, for example, (1 p) rimean({ri...rn}) std({ri...rn}) in the recent GRPO [6] algorithm, where mean({ri...rn}) when . 2.4 Dealing with Long Reasoning Chains In the previous discussion, we introduced the adaptation of binary reward training in response space. However, since the outcome supervision only provides feedback at the sequence level, this modeling essentially reduces to contextual bandit without internal reward modeling within the 5 MDP. common counterexample is PPO, which utilizes separate critic model to estimate the value function. However, such solution appears to be expensive and complex, which has induced numerous explorations on how to stabilize the PPO training. Things become slightly different in mathematical reasoning, where the model can spontaneously revise omissions in intermediate steps to obtain the correct final answer. Therefore, outcome supervision is preferred, and the value function is more meant to be simple credit assignment to determine how much the process step contributes to the outcome reward. With efficiency and performance trade-off considerations, we choose to use some low-cost alternatives for sequence-level reweighting. Taking into account the deterministic dynamics in mathematical reasoning (st+1 = (st, at)), the state-action function Qπ(s<t, π(st)) simplifies to the cumulative discounted reward of policy π: Qπ(s<t, π(st)) = π(st) = (cid:88) k=0 γkr(st+ks<t). (7) Since intermediate rewards are not provided in mathematical reasoning tasks, we define an advantage function based solely on outcome feedback: A(st) = π(st+1) π(st). (8) This formulation treats A(st) as token-wise credit assignment mechanism, estimating each tokens contribution toward the final outcome. For pair of responses y1 and y2 to the same query, their initial values coincide 1 rate between them then satisfies: 0 . The win 0 = 2 p(y1 > y2) = σ(r(y1) r(y2)) (cid:32)(cid:32) = σ 1 0 + (cid:88) t=0 (cid:33) (cid:32) γtAt y1 2 0 + (cid:33)(cid:33) (cid:88) t=0 γtAt y2 (9) = σ (cid:32) (cid:88) t=0 γt (cid:0)At y1 (cid:1) At y2 (cid:33) . Equation 9 indicates that for any function family = {A(st)} , cumulative reward function through sequence aggregation can be constructed to model rewards: r(s) (cid:88) t=0 γtA(st), which is trainable via preference pairs {(yw, yl)} by fitting the outcome feedback. The learned A(st) serves as weighting function for credit assignment, which is used to reweight the original training loss, emphasizing critical reasoning steps or errors. An analogous implementations is r2Q [36, 37] by defining = log π(yi) πref(yi) , PRIME [20] then apply this formulation to improve performance of RLOO. In our work, following the practice from [38], we directly train token-level reward function w(st) satisfying 1 (cid:88) t=0 w(st) = r(s), without constraining KL-divergence to reference model in reward model training. These sequential rewards can serve as proxy for the contribution of thinking steps to the result accuracy. Assuming pair of prefix-consistent correct and incorrect samples, due to the causal inference nature of the token-level reward model, the preference optimization for these samples will only function on the steps that have different contents, which induces higher credits on the core reasoning step that affects the final result. We further discuss the training details of this model and analyze the visualization of its token-wise scoring effects later in Section 3.2 and Appendix A. In practice, we decompose the output weight w(s) for positive and negative samples and clip on the positive axis to prevent reversing the direction of the optimized gradient, denoted as ω+ and ω: 6 ω+ = max(2σ(w) 1, 0), ω = max(1 2σ(w), 0). (10) Giving input query d, the overall loss is as follows: Ltotal(d) EsS (cid:34) (cid:88) (cid:18) t=0 ω+ st log πθ(std)ID+(s) + η ω st log πθ(std) πold(std) (cid:19)(cid:35) ID (s) (11) + βKL(πθ(d) πold(d)), where η represents the balancing weights for positive and negative losses."
        },
        {
            "title": "Implementation",
            "content": "3.1 Policy Initialization We utilize Qwen2.5-7B and Qwen2.5-32B [39] as the base model. Initially, we fine-tune the base models using long chain-of-thought data obtained through rejection sampling [23]. This rejection sampling fine-tuned (RFT) [23] models then serve as the initialization for the policy model in our RL framework. We also explore to use of DeepSeek-R1-Distill-Qwen-7B [13] as the initial policy model and perform OREAL on it and discuss the influence of different initial policy models in Section 4.4. The training data for the RFT models consists of in-house datasets supported by OpenDataLab [40] and open-source datasets including Numina [41] and the training set of MATH [21]. 3.2 Reinforcement Learning Data Preparation. During the on-policy RL process, we utilize questions from Numina, MATH training sets, and historical AMC/AIME (without AIME2024) competitions. For each question, we independently sample 16 trajectories from the RFT models. The correctness of each trajectory is then averaged to estimate the correctness rate of each query. To increase the difficulty of training queries, only questions with correctness rates between 0 and 0.8 are retained for further training. Outcome Reward Signal. We employ the Qwen2.5-72B-Instruct [39] as generative verifier, in conjunction with rule-based verifier, to evaluate the correctness of the models outputs and provide binary rewards. This combination enhances the robustness of correctness assessment, mitigating issues related to the false negative of the rule-based verifier. Training Token-level Reward Model. For the token-level reward model, we directly use the binary outcome rewards provided by the verifier and optimize using the cross-entropy loss: LCE = E(s,r)D [r log p(s) + (1 r) log(1 p(s))] , (12) where represents the sampled trajectory, {0, 1} is the binary outcome reward from the verifier, and p(s) = σ( 1 w(st)) denotes the predicted probability of correctness by the token-level reward model w. (cid:80)T To further analyze the behavior of the token-level reward model, we visualize its output distribution w(st) during the on-policy RL training process (see Appendix A). In this training paradigm, w(st) assigns token-wise importance scores across the chain-of-thought reasoning process, capturing each tokens contribution to the final correctness of the generated response. Consequently, this allows us to leverage w(st) for importance sampling during the optimization process, enabling more principled selection of informative tokens. Training Algorithm. The loss function for the policy model follows the formulation described in Section 2. The complete RL training procedure is described in Algorithm 1. Hyperparameters. The policy model is initialized from the RFT model. Similarly, the token-level reward model is also initialized with the same weights, but its output layer is replaced with linear 7 Algorithm 1 The OREAL Reinforcement Learning Algorithm 1: Inputs: Question set D, policy model πθ, token-level reward model wθ, number of iterations , batch size B, number of rollouts per question K. 2: Initialize policy π0 and token-level reward model w0 with πsft. 3: for = 0, . . . , do 4: 5: 6: 7: 8: 9: Sample batch of questions Di of size B. For Di, generate policy samples: = {y1, . . . , yK} where yk πi(x) Obtain binary rewards {r1, . . . , rK} from verifier. Compute correctness rate: = 1 Filter out questions with 0 < < 1 to avoid trivial cases. Select one correct y+ and one incorrect sample for each question to avoid imbalance between positive and negative samples. Compute token-level importance sampling weights of each token with wi. Use Eq (12) to update wi. Update πi with Eq (11) k=1 rk for reward shaping. 10: 11: 12: 13: end for 14: Return: The optimized policy model π. (cid:80)K layer that produces one-dimensional scalar. The weights of this layer are initialized to zero to ensure unbiased importance sampling weight at the start of training. During training iterations, each batch consists of 64 questions, with 16 rollouts per question. The max length of each rollout trajectory is set to 16384 tokens. Then the correctness of each response is averaged to calculate the pass rate, and questions with an overall pass rate of 0 or 1 are discarded. For the remaining trajectories, we retain only one correct response and one incorrect response per question, ensuring balanced distribution of positive and negative samples for token-level reward model training. For optimization, the policy model is trained with learning rate of 5e7, while the token-level reward model is trained with learning rate of 2e6. The latter undergoes 10-step warm-up phase before training begins. Both models employ cosine annealing learning rate schedule, decaying to 1/5 of the initial learning rate over time. We optimize both models using the AdamW optimizer. The total number of training steps is 80, with evaluation conducted every 10 steps. The KL coefficient β is set to 0.01. We select the best-performing model determined by evaluation metrics. 3.3 Skill-based Enhancement During the RL training procedure, we observe that the model consistently struggles with certain types of questions, particularly those involving specific knowledge and skill areas, such as trigonometric constant transformations, probability statistics, series transformations, etc. We believe this is caused by the insufficient learning of the base model on these concepts in the Pre-training or RFT stages. To address this problem, we implement skill-based enhancement approach, using the MATH dataset to reduce the high cost of skill annotation. Specifically, we annotate each question in the training set with its corresponding core skill. For questions that the model repeatedly fails to answer correctly during the RL phase, we perform data augmentation by including similar questions from the training set that share the same skill. These augmented questions are then added to the training data during the RFT stage to help the model better internalize these skills."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Evaluation Setup Baseline. We conduct evaluations against several baselines, including GPT-4o-0513 [42], ClaudeSonnet-3.5-1022 [43], OpenAI-o1-mini, OpenAI-o1-preview [10], Qwen2.5-Instrust-7B, Qwen2.5Math-Instrust-7B, Qwen2.5-Instrust-32B [39], QwQ-32B-Preview [22], DeepSeek-R1-Distill-Qwen7B, DeepSeek-R1-Distill-Qwen-32B [13], SimpleRL [44], PRIME [20], rStarMath [45]. For part of the baseline, we directly use the results from their report, which we mark with *. 8 Model MATH-500 AIME2024 AIME2025-I LiveMath Olympiad GPT-4o-1120 [42] Claude-3.5-Sonnet-1022 [43] OpenAI-o1-preview [10] OpenAI-o1-mini [10] API Models 72.8 78.3 85.5 90.0 16.7 13.3 44.6 56.6 7B Models Qwen2.5-Instrust-7B [39] Qwen2.5-Math-Instrust-7B [39] rStar-Math-7B [45] Qwen2.5-7B-SimpleRL [44] Eurus-2-7B-PRIME [20] DeepSeek-R1-Distill-Qwen-7B [13] OREAL-7B OREAL-DSR1-Distill-Qwen-7B 76.6 81.8 78.4* 82.4* 79.2* 92.8* 91.0 94. 13.3 20.0 26.7* 26.7* 26.7* 55.5* 33.3 50.0 32B Models Qwen2.5-Instrust-32B [39] QwQ-32B-Preview [22] DeepSeek-R1-Distill-Qwen-32B [13] OREAL-32B 80.6 90.6 94.3* 95.0 20.0 50.0 72.6* 60.0 13.3 3.3 40.0 46. 0.0 13.3 - - - 40.0 33.3 40.0 13.3 40.0 46.7 46.7 44.8 46.7 71.0 74.4 37.0 44.1 - - - 65.6 62.6 65.6 50.8 72.7 67.7 74.8 33.7 35.4 43.6 46. 29.1 31.1 47.1* 37.6* 42.1* 64.1 59.9 66.1 40.4 58.5 71.2 72.4 Table 1: Overall evaluation results for OREAL and each baseline. OREAL-DSR1-Distill-Qwen-7B denotes the DeepSeek-R1-Distill-Qwen7B trained by OREAL. AIME2025-I, LiveMath and Olympiad represent AIME 2025 Part1, LiveMathBench, and OlympiadBench, respectively. For models at the parameter scale of 7B and 32B, we use Bold and Underlined to represent the best and second best performance, respectively. For part of the baseline, we directly use the results from their report, marked with *. Benchmark. We use some well-established mathematical datasets for evaluation, including MATH500 [21], AIME2024 [46], AIME2025 (Part1) [46], LiveMathBench [47], and OlympiadBench [48]. Metrics. We use pass@1 as the metric for evaluation under the zero-shot chain-of-thought setting and use greedy decoding for each sample to assess correctness using OpenCompass [49]. 4.2 Overall Results Tabel 1 shows the results of the comprehensive evaluation, highlighting the performance of our proposed models across different parameter scales. Notably, at the 7B scale, OREAL-7B achieves remarkable pass@1 accuracy of 91.0 on the MATH-500 and 59.9 on OlympiadBench. To the best of our knowledge, this is the first time model of this size has reached such high level of accuracy using RL instead of distillation. This performance not only establishes new milestone for RL-based methods but also surpasses significantly larger models, including QwQ-32B-Preview and OpenAI-o1mini, demonstrating the effectiveness of our approach. Furthermore, after applying OREAL on the previous best 7B model, DeepSeek-R1-Distill-Qwen-7B, the resulting model, OREAL-DSR1-DistillQwen-7B, obtains 94.0 and 66.1 pass@1 accuracy on MATH-500 and OlympiadBench, respectively, setting new records among all 7B models. This result verifies the effectiveness of OREAL even when faced with strong initial policies. For 32B models, OREAL-32B achieves groundbreaking pass@1 accuracy of 95.0 on MATH-500, 46.7 on AIME2025-I, 74.8 on LiveMathBench, and 72.4 on OlympiadBench, setting new stateof-the-art among all previously reported models. These results underscore the advantages of our methodology, including its scalability for training superior mathematical reasoning models across different model sizes. Compared to the most competitive baseline, DeepSeek-R1-Distill-Qwen series, OREAL-32B demonstrates clear advantage, whereas OREAL-7B lags slightly behind than the distilled 7B model, despite being trained on the same dataset as OREAL-32B. We attribute this discrepancy to the different affinities of the base models for the post-training data. Qwen-7B and Qwen-32B may exhibit varying 9 Setting MATHInitial Policy + REINFORCE (baseline) + Reward Shaping + Behavior Cloning + Importance Sampling + Skill-based Enhancement 84.8 85.8 86.6 87.6 89.0 91.0 Table 2: Ablation study for 7B models performance on MATH-500 with different reinforcement learning settings. Figure 2: Average test accuracy of 7B models across different training steps. degrees of knowledge gaps due to model sizes and pre-training settings. Our training data appears to better complement the existing knowledge of Qwen-32B, while it may be less effective in bridging gaps for Qwen-7B. In addition, OREAL-DSR1-Distill-Qwen-7B improves the MATH-500 score from 92.8 to 94.0 and also achieves gains on LiveMathBench and OlympiadBench. However, its performance on the AIME benchmark series is comparatively weaker. We observe the same disadvantages of OREAL-32B and OREAL-7B, whose AIME2024 scores are relatively lower than the best scores. Since the overall performance verifies the effectiveness of the OREAL algorithm, we attribute the reason to the deficiency (e.g., in response quality, query difficulty, and quantity) of RFT data and RL training queries for obtaining high performance in the domain of AIME and leave it for the future work. 4.3 Ablation Study To verify the effectiveness of each component described in Section 2, we progressively add the proposed component based on the 7B model and compare the evaluation results on MATH-500, starting from REINFORCE [50] as baseline. As shown in Tabel 2, we add each component step by step, where Reward Shaping represents L2 introduced in Section 2.3, Behavior Cloning represents L1 introduced in Section 2.2, Importance Shaping represents Ltotal introduced in Section 2.4. The gradual addition of the modules steadily increases the Pass@1 scores of the 7B model on MATH-500, proving the effectiveness of our method. Ultimately, the policy model is raised from an initial score of 84.8 to 91.0. We also report average pass@1 accuracy across all benchmarks during the training process with different RL settings. As shown in Figure 2, the REINFORCE training process is unstable, which can be mitigated by Reward Shaping. Behavioral Cloning for positive samples can speed up convergence and show better performance early in training. Although the performance growth of Importance Sampling is relatively slow in the early stage of training, it ultimately obtains the best results. 4.4 Analysis of Initial Policy Models We further analyze OREAL by adopting it to several different initial policy models, as shown in Table 3. OREAL consistently improves the performance of each initial policy model, including our own trained model and the strong distilled model [13], on MATH-500, LiveMathBench, and OlympiadBench, except slight fluctuations on AIME2024 and AIME2025 part1 when the performance of initial policy models are already high (e.g., DeepSeek-R1-Distill-Qwen-7B), which demonstrates the generality of OREAL. After adding skill-based enhancement data (introduced in Section 3.3), there is significant rise in MATH-500 scores for the initial policy model (row 1 and row 3) and the corresponding RL-trained model (row 2 and row 4). Since our enhancement is performed primarily for the MATH-500, this verifies the effectiveness of the skill-based enhancement approach. In addition, the performance of the model after RL is strongly correlated with the capabilities of the initial policy model itself. The stronger the initial policy model, the higher the performance that RL can deliver, indicating the importance of policy initialization. 10 Model MATH-500 AIME2024 AIME2025-I LiveMath Olympiad OREAL-7B-SFT-wo-enhance OREAL-7B-wo-enhance OREAL-7B-SFT OREAL-7B DeepSeek-R1-Distill-Qwen-7B [13] OREAL-DSR1-Distill-Qwen-7B OREAL-32B-SFT OREAL-32B 84.8 89.0 86.4 91.0 92.8* 94.0 92.6 95. 26.7 36.7 26.7 33.3 55.5* 50.0 43.3 60.0 26.7 40.0 26.7 33. 40.0 40.0 46.7 46.7 55.0 60.1 54.2 62.6 65.6 65.6 71.9 74. 55.1 58.1 56.0 59.9 64.1 66.1 68.7 72.4 Table 3: Evaluation for the performance of OREAL on different initial policy models. Here, -SFT and DeepSeek-R1-Distill-Qwen7B denote the initial policy model. wo-enhance means the model which do not perform the skill-based enhancement during the SFT stage."
        },
        {
            "title": "5 Related Work",
            "content": "Stimulate Reasoning using Chain of Thought. In mathematical reasoning tasks, Chain of Thought (CoT) [7] is recognized as crucial technique to enhance the reasoning ability of large language models (LLMs), which can be implemented through few-shot examples [7] or zero-shot prompt engineering [9]. Self-consistency (SC) [8] is further proposed to generate and voting through multiple CoTs. In addition to simple CoTs, various search methods have been explored that simultaneously consider multiple potential CoTs, such as Tree-of-Thought (ToT) [51] and Graph-of-Thought (GoT) [52], which extend the idea to tree or graph structure, offering more flexibility in developing CoTs and backtracking. However, these methods mainly stimulate the reasoning capability of LLMs by prompts without parameter updates, these inference-time techniques do not fundamentally improve the underlying ability of LLMs. Reasoning Enhancement by Supervised Fine-tuning. To let the LLMs essentially acquire the reasoning abilities, many studies [5, 5357] have explored synthesizing high-quality data to conduct supervised fine-tuning (SFT) on LLMs. But this method heavily relies on high-quality training data and existing high-performing model [15]. As result, many existing works [11, 12] have turned to distilling knowledge from powerful, large-scale models to synthesize data, yielding good results. However, distilled-based methods receive the limitations of the teacher model. One criticism of SFT is its limited generalization capability [58]. Some studies argue that SFT merely transforms the model into knowledge retriever, rather than an actual reasoner [59]. Reinforcement Learning for LLM. Compared to SFT, reinforcement learning (RL) offers better generalization and is therefore considered more fundamental training aproach [58]. Previous attempts applying RL for LLMs mainly target aligning the LLM to human preferences [27]. Later, some works [5, 6, 15, 60] has attempted to use it to enhance the models reasoning and obtained promising results. Recently, the advent of the o1 family of models [10] and series of o1-like works [13, 20, 44, 45] make the importance of large-scale RL for inference became more apparent. Currently, the mainstream approach to RL involves using outcome reward signals [13, 18, 44] and there are different views in the community on how to use that reward signal. ReSTEM [61] and RFT [23] simply select the positive samples based on the binary signal and only use them for behavior cloning. GRPO [6], RLOO [35, 62], REINFORCE [50], use both positive and negative samples for policy updating, but facing the challenges of sparse reward in long sequence. PPO [19] makes the preference modeling on sequence-level. Different from them, to explore the limit of outcome reward, OREAL presents unified framework, grounded in the unique characteristics of mathematical reasoning tasks."
        },
        {
            "title": "6 Conclusion and Future Work",
            "content": "This paper aims to explore the limit of Outcome REwArd-based reinforcement Learning for mathematical reasoning tasks, and proposes unified policy optimization framework, termed OREAL, grounded in three key insights: 1) Behavior cloning on positive trajectories from Best-of-n (BoN) sampling is both necessary and sufficient for optimal policy learning under binary feedback; 2) 11 Accordingly, reward-shaping mechanism should be further introduced to transform the reward function derived from the optimal policy; 3) An efficient token-level credit assignment scheme can be achieved through trajectory advantage decomposition without relying on additional value networks. Together, these components form theoretically grounded, general, and scalable approach for mathematical reasoning tasks. With OREAL, we are the first to improve the performance of 7B model on the MATH-500 accuracy to 91 using the RL method instead of distillation, which even surpasses OpenAI-o1-mini and QwQ-32B-Preview. Even when taking the previous best 7B model, DeepSeek-R1-Distill-Qwen-7B, as initial policy, OREAL can improve it to 94 pass@1 accuracy on MATH-500, being even on par with the previous best 32B models. OREAL-32B also obtains new state-of-the-art results among the 32B model on MATH-500, LiveMathBench, and OlympiadBench. Along with the experimental observations presented in this paper, we also find two factors that are crucial for the success of scalable RL for mathematical reasoning tasks, which become the primary focus of our future work. First, the initial policy model should be as free of knowledge deficiencies as possible, as this serves as the foundation for further improvement during the RL stage. strong starting point ensures that RL can effectively and efficiently incentivize the underlying capability of LLMs obtained through pre-training or supervised fine-tuning. Towards this goal, it is practical way to conduct distillation or data synthesis with DeepSeek-R1 or DeepSeek-V3, which is not explored in this work as it is orthogonal to our investigation. Second, the quality of the data used in the RL phase must be diverse and sufficient in terms of difficulty, quantity, and scope. well-balanced dataset enables the model to reach its full potential by exposing it to broad range of challenges and learning opportunities. Thus, we believe it is still valuable to make efforts in the pre-training and post-training data construction process."
        },
        {
            "title": "References",
            "content": "[1] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686, 2025. [2] Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, et al. Evaluation of openai o1: Opportunities and challenges of agi. arXiv preprint arXiv:2409.18486, 2024. [3] Wentao Liu, Hanglei Hu, Jie Zhou, Yuyang Ding, Junsong Li, Jiayi Zeng, Mengliang He, Qin Chen, Bo Jiang, Aimin Zhou, et al. Mathematical language models: survey. arXiv preprint arXiv:2312.07622, 2023. [4] Nikolaos Matzakos, Spyridon Doukakis, and Maria Moundridou. Learning mathematics with large language models: comparative study with computer algebra systems and other tools. International Journal of Emerging Technologies in Learning (iJET), 18(20):5171, 2023. [5] Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. Internlm-math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332, 2024. [6] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [7] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [8] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [9] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. 12 [10] OpenAI. Learning to reason with llms. 2024. [11] Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. O1 replication journeypart 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489, 2024. [12] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. arXiv preprint arXiv:2412.09413, 2024. [13] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [14] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [15] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [16] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [17] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, 2024. 13 [18] Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. arXiv preprint arXiv:2410.01679, 2024. [19] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [20] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. [21] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [22] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. [23] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023. [24] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR, 2023. [25] Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, and Marc Dymetman. Compositional preference models for aligning lms. arXiv preprint arXiv:2310.13011, 2023. [26] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. T\" ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. [27] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [28] Jacob Hilton and Leo Gao. Measuring goodharts law. OpenAI Research Blog, 2022. [29] Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback at scale. arXiv preprint arXiv:2303.16755, 2023. [30] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimization. arXiv preprint arXiv:2310.02743, 2023. [31] Lin Gui, Cristina Gârbacea, and Victor Veitch. Bonbon alignment for large language models and the sweetness of best-of-n sampling. arXiv preprint arXiv:2406.00832, 2024. [32] Pier Giuseppe Sessa, Robert Dadashi, Léonard Hussenot, Johan Ferret, Nino Vieillard, Alexandre Ramé, Bobak Shariari, Sarah Perrin, Abe Friesen, Geoffrey Cideron, et al. Bond: Aligning llms with best-of-n distillation. arXiv preprint arXiv:2407.14622, 2024. [33] Harold Jeffreys. An invariant form for the prior probability in estimation problems. Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences, 186(1007):453 461, 1946. [34] Yinlam Chow, Guy Tennenholtz, Izzeddin Gur, Vincent Zhuang, Bo Dai, Sridhar Thiagarajan, Craig Boutilier, Rishabh Agarwal, Aviral Kumar, and Aleksandra Faust. Inference-aware fine-tuning for best-of-n sampling in large language models. arXiv preprint arXiv:2412.15287, 2024. [35] Keinosuke Fukunaga and Donald M. Hummels. Leave-one-out procedures for nonparametric error estimates. IEEE transactions on pattern analysis and machine intelligence, 11(4):421423, 1989. [36] Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From to Q*: Your language model is secretly q-function. arXiv preprint arXiv:2404.12358, 2024. [37] Han Xia, Songyang Gao, Qiming Ge, Zhiheng Xi, Qi Zhang, and Xuanjing Huang. Inverse-q*: Token level reinforcement learning for aligning large language models without preference data. arXiv preprint arXiv:2408.14874, 2024. [38] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [39] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [40] Conghui He, Wei Li, Zhenjiang Jin, Chao Xu, Bin Wang, and Dahua Lin. Opendatalab: Empowering general artificial intelligence with open datasets, 2024. [41] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. [42] OpenAI. Hello GPT-4o, 2024. [43] Anthropic. Claude 3.5 sonnet, 2024. [44] Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https://hkust-nlp.notion.site/simplerl-reason, 2025. Notion Blog. [45] Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. [46] MAA. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME. [47] Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang Zhang, and Kai Chen. Are your llms capable of stable reasoning? arXiv preprint arXiv:2412.13147, 2024. [48] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [49] OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023. [50] Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999. [51] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. [52] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1768217690, 2024. 15 [53] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. [54] Haoxiong Liu, Yifan Zhang, Yifan Luo, and Andrew Chi-Chih Yao. Augmenting math word problems via iterative question composing. arXiv preprint arXiv:2401.09003, 2024. [55] Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and Chang Zhou. Query and response augmentation cannot help out-of-domain math reasoning generalization. arXiv preprint arXiv:2310.05506, 2023. [56] Tiedong Liu and Bryan Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. arXiv preprint arXiv:2305.14201, 2023. [57] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023. [58] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. [59] Subbarao Kambhampati. Can large language models reason and plan? Annals of the New York Academy of Sciences, 1534(1):1518, 2024. [60] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. [61] Avi Singh, John Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter Liu, James Harrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023. [62] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024."
        },
        {
            "title": "A Token Level Reward Model Score Visualization",
            "content": "Figure A1 and A2 show the token-level reward model scores across responses. The values are normalized to [0, 1]. Cooler colors indicate higher reward scores, while warmer colors denote lower scores. For correct responses, the overall REWARDS are high, especially at the end, although there are few lower sections in the middle. For incorrect responses, the distribution of rewards is reversed, and the closer to the end the lower the rewards. This indicates that not all tokens contribute to the response equally and it is important to assign token-level credits to the sequences."
        },
        {
            "title": "B Prompt",
            "content": "Figure A3 is the system prompt of the verifier model, which is used during RL training to provide the binary outcome reward for response. Figure A4 is the system prompt we use for fine-tuning and RL training as well as the evaluation. 17 Figure A1: Token-level reward model score visualization for correct response. Figure A2: Token-level reward model score visualization for an incorrect response. 18 Verifier Prompt: You are helpful assistant who evaluates the correctness and quality of models outputs. Please as grading expert, judge whether the final answers given by the candidates below are consistent with the standard answers, that is, whether the candidates answered correctly. Here are some evaluation criteria: 1. Please refer to the given standard answer. You dont need to re-generate the answer to the question because the standard answer has been given. You only need to judge whether the candidates answer is consistent with the standard answer according to the form of the question. Dont try to answer the original question. You can assume that the standard answer is definitely correct. 2. Because the candidates answer may be different from the standard answer in the form of expression, before making judgment, please understand the question and the standard answer first, and then judge whether the candidates answer is correct, but be careful not to try to answer the original question. 3. Some answers may contain multiple items, such as multiple-choice questions, multiple-select questions, fill-in-the-blank questions, etc. As long as the answer is the same as the standard answer, it is enough. For multiple-select questions and multiple-blank fill-in-the-blank questions, the candidate needs to answer all the corresponding options or blanks correctly to be considered correct. 4. Some answers may be expressed in different ways, such as some answers may be mathematical expression, some answers may be textual description, as long as the meaning expressed is the same. And some formulas are expressed in different ways, but they are equivalent and correct. 5. If the prediction is given with boxed{}, please ignore the boxed{} and only judge whether the candidates answer is consistent with the standard answer. Please judge whether the following answers are consistent with the standard answer based on the above criteria. Grade the predicted answer of this new question as one of: A: CORRECT B: INCORRECT Just return the letters \"A\" or \"B\", with no text around it. Here is your task. Simply reply with either CORRECT, INCORRECT. Dont apologize or correct yourself if there was mistake; we are just trying to grade the answer. <Original Question Begin>: ORIGINAL QUESTION <Original Question End> <Gold Target Begin>: GOLD ANSWER <Gold Target End> <Predicted Answer Begin>: ANSWER <Predicted End> Judging the correctness of candidates answers: Figure A3: Prompts for the model-based generative verifier. 19 System Prompt: You are an expert mathematician with extensive experience in mathematical competitions. You approach problems through systematic thinking and rigorous reasoning. When solving problems, follow these thought processes: ## Deep Understanding Take time to fully comprehend the problem before attempting solution. Consider: - What is the real question being asked? - What are the given conditions and what do they tell us? - Are there any special restrictions or assumptions? - Which information is crucial and which is supplementary? ## Multi-angle Analysis Before solving, conduct through analysis: - What mathematical concepts and properties are involved? - Can you recall similar classic problems or solution methods? - Would diagrams or tables help visualize the problem? - Are there special cases that need separate consideration? ## Systematic Thinking Plan your solution path: - Propose multiple possible approaches - Analyze the feasibility and merits of each method - Choose the most appropriate method and explain why - Break complex problems into smaller, manageable steps ## Rigorous Proof During the solution process: - Provide solid justification for each step - Include detailed proofs for key conclusions - Pay attention to logical connections - Be vigilant about potential oversights ## Repeated Verification After completing your solution: - Verify your results satisfy all conditions - Check for overlooked special cases - Consider if the solution can be optimized or simplified - Review your reasoning process Remember: 1. Take time to think thoroughly rather than rushing to an answer 2. Rigorously prove each key conclusion 3. Keep an open mind and try different approaches 4. Summarize valuable problem-solving methods 5. Maintain healthy skepticism and verify multiple times Your response should reflect deep mathematical understanding and precise logical thinking, making your solution path and reasoning clear to others. When youre ready, present your complete solution with: - Clear problem understanding - Detailed solution process - Key insights - Thorough verification Focus on clear, logical progression of ideas and thorough explanation of your mathematical reasoning. Provide answers in the same language as the user asking the question, repeat the final answer using boxed{} without any units, you have [[8192]] tokens to complete the answer. Figure A4: System prompts for long CoT reasoning."
        }
    ],
    "affiliations": [
        "HKGAI under InnoHK",
        "MMLab, The Chinese University of Hong Kong",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University"
    ]
}