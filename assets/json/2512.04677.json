{
    "paper_title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
    "authors": [
        "Yubo Huang",
        "Hailong Guo",
        "Fangtai Wu",
        "Shifeng Zhang",
        "Shijie Huang",
        "Qijun Gan",
        "Lin Liu",
        "Sirui Zhao",
        "Enhong Chen",
        "Jiaming Liu",
        "Steven Hoi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications."
        },
        {
            "title": "Start",
            "content": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length Yubo Huang1,2 Hailong Guo1,3 Lin Liu2 Qijun Gan4 Sirui Zhao2, Fangtai Wu1,4 Shifeng Zhang Shijie Huang1 Enhong Chen2, Jiaming Liu1, Steven Hoi1 5 2 0 2 ] . [ 2 7 7 6 4 0 . 2 1 5 2 : r 1 Alibaba Group 2 University of Science and Technology of China 3 Beijing University of Posts and Telecommunications 4 Zhejiang University snake1124@mail.ustc.edu.cn liujiaming.ljl@alibaba-inc.com Corresponding authors. Project Leader. Figure 1. We propose Live Avatar, powerful real-time streaming model capable of infinitely long audio-driven avatar generation, producing lifelike avatars that talk, react, and persist seamlessly over hours."
        },
        {
            "title": "Abstract",
            "content": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system codesigned framework that enables efficient, high-fidelity, and infinite-length avatar generation using 14-billionparameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications. Project page: https://liveavatar.github.io 1. Introduction Audio-driven avatar generation, the synthesis of photorealistic human face video conditioned solely on an input audio stream, is foundational technology for interactive digital communication. Its applications are expansive, ranging from virtual reality and live streaming to digital assistants. The demand for systems capable of producing high-fidelity, expressive, and real-time avatars has driven significant recent advancements, particularly with the rise of diffusion models for high-fidelity video synthesis[2, 17, 40]. Despite their success in setting new benchmarks for visual quality, deploying these powerful generative models in real-time, streaming environments faces fundamental and conflicting challenges. The first challenge is the real-time fidelity dilemma. Large-scale diffusion models[14] provide unmatched visual detail but suffer from high inference latency. Their inherent reliance on the sequential denoising computation leads to latency profile that is fundamentally incompatible with interactive scenarios requiring low latency and high frame rates (e.g., 20 FPS). The complex trade-off between model capacity, visual quality, and practical execution speed remains critical bottleneck for deployment. The second challenge is the long-horizon consistency. Applications demanding infinite-length or long-horizon generation necessitate continuous temporal stability. Existing methodologies are highly susceptible to limited expression or the gradual accumulation of identity drift and distracting color artifacts over prolonged synthesis periods as revealed in some works[20, 38], severely compromising the avatars coherence and the viewers experience. To address these critical challenges, we propose Live Avatar, novel training and inference framework. Live Avatar is designed to fundamentally enable large diffusion models (up to 14 billion parameters) for real-time, streaming, and infinite-length audio-driven avatar generation without compromising visual fidelity. Our work successfully resolves the long-standing tension between high visual quality, model complexity, and practical execution speed through an algorithm-system co-design approach. The core contributions of Live Avatar are as follows: Causal, Streamable Adaptation Framework. We employ Self-Forcing Distribution Matching Distillation to distill the knowledge from non-causal model into causal, few-step student architecture. This framework is designed to ensure the models efficiency for deployment on our subsequent distributed inference engine. Furthermore, during distillation, we inject random noise into the Key-Value cache of historical frames to enhance robustness in long-horizon video generation. This strategy not only mitigates quality degradation in long-term generation but also encourages the student model to draw upon"
        },
        {
            "title": "Method",
            "content": "stream real time inf-len size Hallo3[6] StableAvatar[38] Wan-s2v[15] Ditto[24] InfiniteTalk[47] OminiAvatar[13] Live avatar(ours) (cid:37) (cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:34) (cid:37) (cid:34) (cid:37) (cid:34) (cid:34) (cid:37) (cid:34) 5B 1.3B 14B 0.2B 14B 14B 14B Table 1. Comparison of state-of-the-art audio-driven avatar generation methods. Live Avatar simultaneously achieves streaming, real-time, and infinite-length generation with high-fidelity, largescale (14B) diffusion model. the static identity features provided by the sink frame. Timestep-forcing Pipeline Parallelism (TPP). We introduce novel distributed inference execution paradigm that physically decouples the sequential denoising steps across multiple devices. By assigning each device to fixed timestep, we create fully pipelined inference engine. This approach breaks the autoregressive denoising bottleneck inherent to diffusion models, achieving predictable low latency and guaranteeing 20 FPS real-time streaming performance on only 5H800 GPUs. Rolling Sink Frame Mechanism (RSFM). To tackle the challenge of long-term consistency, we propose RSFM. This mechanism dynamically recalibrates the generated sequence by periodically reintroducing and synchronizing the initial appearance frame during the generation process. RSFM effectively suppresses the accumulation of identity drift and color artifacts, ensuring high stability and visual coherence for streaming avatar video generation. 2. Related Works Streaming and Long Video Generation. Streaming and long video generation require efficient management of both computation and memory resources. CausVid [50] applies distillation and KV cache to accelerate generation and enable streaming, but suffers from over-exposure artifacts caused by error accumulation over long sequences. Diffusion Forcing [3] introduces varied noise levels to sequential targets, supporting longer videos. Self Forcing [19] addresses exposure bias by conditioning on previously generated frames during training, resolving train-inference mismatches. Self-Forcing++ [7] extends DMD to support video generation beyond base model duration. LongLive [46] uses KV-recache mechanism and short window attention for streaming and long-tuning. Rolling Forcing [26] proposes rolling window joint denoising and attention sink mechanisms for reduced error and improved coherence. StreamDiT [21] supports streaming and interactive generation using window attention and multistep distillation. While these approaches improve video quality and temporal consistency for long video generation, none achieve realtime, streaming long video generation with large-scale diffusion models. Audio-driven Avatar Video Generation. Audio-driven subject consistency avatar video generation requires Early works such as and effective motion control. Wav2Lip [34] employ GANs for accurate lip-syncing, while SadTalker [52] predicts 3D motion coefficients from audio for flexible video control. With the success of diffusion models in video synthesis, several studies [37, 51] adapt diffusion frameworks and ReferenceNet architectures for audio-driven avatar generation, and DiT-based models are especially effective [2, 17, 22, 40]. Recently, more DiT-based approaches [8, 27, 32] have emerged, and some works [16, 24, 42, 53, 55] focus on head video generation due to the complexity of human motion. In addition, recent advances in autoregressive modeling for images, such as MAR [23], have inspired multimodal video generation methods [1, 4, 45] that combine autoregressive and diffusion strategies. Nevertheless, further progress is still required in maintaining identity consistency and achieving high video quality over long-form, audio-driven avatar video generation. Diffusion Distillation. Diffusion model distillation has emerged as pivotal technique for accelerating video generation. Classical distillation approaches include trajectory distillation[12], Consistency Models[28, 36, 54], and Distribution Matching Distillation (DMD)[30, 31, 49], all of which have demonstrated consistent benefits across diverse video generation models. In the context of streaming video generation, DMD has attracted broader attention. Notably, methods such as CausVID[50], Self-Forcing[7, 19], and LongLive[46] employ DMD for few-step distillation: they first generate long videos and subsequently extract temporal segments for DMD-based distillation. This strategy not only achieves significant sampling speedup but also substantially enhances the generation quality of minute-long videos. Recent works reveal that DMD can function analogously to reinforcement learning, where pretrained diffusion model serves as specialized reward signal during optimization[29], which may be the reason for the effectiveness of distillation on streaming video diffusion models. 3. Preliminaries 3.1. Video Diffusion Models Video diffusion models generate high-fidelity video sequences by progressively denoising from Gaussian prior xT (0, I), following the reverse process of forward diffusion. In this work, we adopt the flow matching [25], where noisy latents at time are constructed as xt = (1 st) x0 + st xT , st [0, 1] (1) is scheduling function that controls the interpolation between clean and noise. The model is trained to predict the target velocity = xT x0 leading to the standard mean squared error objective: = Ex0,xT ,t (cid:2)vθ(xt, t, c) (xT x0)2 2 (cid:3) (2) (3) where denotes conditional inputs such as text embeddings. To reduce computational cost, most modern video diffusion models operate in compressed latent space. We use causal 3D VAE to encode input videos into temporallatent representations, ensuring that future frames do not leak during training. Text conditioning is achieved through pre-trained language model that produces contextual embeddings fed into the diffusion backbone. 3.2. Distribution Matching Distillation Distribution Matching Distillation (DMD)[49] aims to distill pre-trained teacher diffusion model into student model that operates with fewer sampling steps. Let pθ,t(xt) denote the distribution induced by the few-step student model = Gθ(z), and let pdata,t(xt) represent the corresponding ground-truth distribution produced by the teacher diffusion model at time step t. The primary objective of DMD is to minimize the distribution, i.e., reverse KullbackLeibler (KL) divergence, between these two distribu- (cid:1)(cid:3) . The gra- (cid:2)DKL tions at each time step t: Et dient of DMD loss is given by: (cid:0)pθ,t pdata,t θLDMD = Et,z (cid:20) (sreal(xt, t) sfake,ϕ(xt, t)) Gθ(z) (4) where xt = Ψ(ˆx, t) is the noise scheduler, ˆx = Gθ(z) is the data prediction of the distilled model, sreal and sfake,ϕ denote the score functions corresponding to the pre-trained teacher diffusion model and the student generator, respectively. θ (cid:21) The theoretical foundation of DMD is limited to singlestep distillation. To address this limitation, TDM[31] establishes trajectory-aware distribution matching framework that naturally extends to multi-step distillation, thereby achieving superior performance compared to DMD2[48]. sreal, sfake,ϕ, and Gθ(z) are all initialized from the pretrained teacher model induced by vθ. The training proceeds by alternately updating sfake,ϕ and Gθ(z), in which sfake,ϕ is trained on samples generated by the current student generator Gθ(z), and Gθ(z) is trained using the DMD loss defined above. For multi-step distillation, DMD first performs Figure 2. The Live Avatar Training Framework. (a) Stage 1 Diffusion Forcing Pretraining, showing the block-wise noise setup and the applied attention masks. (b) Stage 2 Self-Forcing Training, where the noise level consistency between the KV cache and the noisy latents is enforced for distillation. Ψ xt multi-step sampling trajectory using the student generator: Gθ ˆxt1 Ψ xt2 xtN . Then, at each training iteration, random intermediate state xti from this trajectory is selected and used in place of pure noise as the starting point for the DMD training procedure.[48] Gθ ˆxt2 4. The Live Avatar Framework In this section, we present the technical details of Live Avatar. We first detail the model architecture in Sec.4.1, following overall training framework in Sec.4.2 and Fig. 2. The inference framework and Timestep-forcing Pipeline are demonstrated in Sec.4.3. We also investigate long video generation in Section 4.4 and propose the rolling sink frame to address identity inconsistency and color artifacts in longterm generation. 4.1. Model Architecture In order to enable streaming video generation, the Live Avatar adopt autoregressive generation by factorizing the joint distribution Bi t1 = vθ(Bi t, B(iw):(i1) , I, ai, ti) (cid:125) (cid:123)(cid:122) kv cache (cid:124) (5) combining diffusion-based frame synthesis with causal dependencies across chunks. in Eq.5 are blocks of consecutive noisy frame latents. In our work, we set the number of frame latents to 3; Here, the denotes the rolling sink frame, which provides the appearance; ai and ti are the audio embedding and prompt embedding for the i-th block respectively. The underscore denotes the denoising step, and the superscript denotes the frame index. Note that in the model, the kv cache B(iw):(i1) and the noisy block Bi share the same noise level, this is crucial design that improves generation quality and maximize the inference speed, which will be illustrated in Sec.4.3. 4.2. Model Training Our overall training framework is illustrated in Figure 2, which consists of two main stages: 1) Stage 1, Diffusion Forcing Pretraining with block-wise independent noise scheduling and causal local attention mask, and 2) stage 2, Self-Forcing Distribution Matching Distillation with KV cache perturbation. In the first stage, we perform diffusion-forcing pretraining to ensure stable training in the next distillation stage. We apply causal masks during training following CausVid[50] and X-Streamer[45]. The masking strategy treats all frames within block as fully visible (i.e., intra-block full attenFigure 3. visual illustration of Timestep-forcing Pipeline Parallelism (TPP). After warm-up fills the pipeline, all GPUs denoise simultaneously in the fully pipelined stage, turning the sequential diffusion chain into an asynchronous spatial pipeline. For example, GPU2 always performs the t2 t1 step: it reuses its local KV cache (very fast) and sends only the latent to GPU3 (fast). tion), while enforcing causality between blocks, each block can only attend to features from earlier blocks(i.e., interblock causal attention). In the second stage, we distill the knowledge of bidirectional teacher model into causal, few-step generator suitable for efficient streaming avatar video synthesis. Our approach builds upon the Self-forcing paradigm [19], involving three key models during training: (1) fixed bidirectional diffusion teacher (Real Score Model), which represents the target distribution; (2) dynamically updated bidirectional model (Fake Score Model) that tracks the distribution of the student generators outputs; and (3) causal, autoregressive generator that is initialized from the first stage model and optimized for few steps denoising. primary technical challenge arises from the need to generate long-sequence latent inputs for the bidirectional score models in fashion that is compatible with causal, block-wise streaming generation. To address this, the causal generator produces the video sequence autoregressively, denoising one block at time while conditioning on previous blocks. After each block is synthesized, the clean latent is stored, and the sequence of these latents forms the input trajectory for both Real and Fake Score Models. Specifically, we follow self-forcing[19] and apply holistic distributionmatching loss in an online manner. To ensure tractable memory usage during training, we adopt computation graph truncation as introduced in the self-forcing. Maintaining long-sequence coherence and stability is further facilitated by introducing Key-Value (KV) caching strategy within the causal architecture. At each denoising step, KV activations are cached and propagated across blocks, preserving temporal context. Specifically, during the denoising of each block, the model attends to the KV cache from previous blocks at the same timestep, following the practice introduced in [27]. To further boost robustness, we inject random noise into the historical KV cache, which we refer to as History Corrupt1. This noise injection encourages the model to distinguish between dynamic motion from recent history and the static identity cues contributed by the sink frame, ultimately supporting stable, high-quality video streaming over arbitrary durations, thereby improving temporal coherence and reducing artifact accumulation across the video sequence. 4.3. Timestep-forcing Pipeline Parallelism Deploying large video generation models in real-time settings remains challenging due to the inherent sequential structure of diffusion-based sampling. Our distilled 14B model, for example, reaches only 5 FPS on single GPU and 6 FPS under conventional 4-GPU sequential parallelization. In contrast, existing real-time streaming methods, such as CausVid [50] and LongLive [46], achieve higher frame rates by substantially reducing model capacityoften relying on lightweight 1.3B models and aggressive quantizationat the cost of generation fidelity. This establishes long-standing tradeoff between model quality and real-time performance that has yet to be resolved. To overcome the sequential bottleneck of diffusion sampling, we introduce Timestep-forcing Pipeline Parallelism (TPP) (illustrated in Figure 3), which assigns each GPU fixed timestep ti and partitions the denoising steps across devices. Each GPU repeatedly performs its designated transformation ti ti1, converting the Sequential diffusion chain into an asynchronous spatial pipeline. Through this reparameterization, throughput is determined by the single denoise forward of the model rather than the sum of all diffusion steps, effectively removing the inference-time sampling bottleneck. TPP enables real-time 1Details of History Corrupt are provided in the supplementary materials. streaming generation at over 20 FPS while preserving the capacity and quality benefits of multi-step diffusion. TPP operates in two stages. During warm-up, the first block is propagated through all timesteps to fill the pipeline, which completes quickly given the small number of sampling steps. Once filled, the system enters the fully pipelined streaming phase: each GPU repeatedly performs its assigned denoising step, passes the latent to the next device, and immediately processes the next block, achieving maximal parallel throughput. Each GPU maintains its own rolling KV cache. This design both enables TPP and aligns with the base-models pretraining-time setting, which helps stabilize generation quality [27]. At each denoising step, GP Ui reuses its local KV cache, attending only to the KV cache within its predefined local window. This operation is entirely localrequiring no inter-GPU communicationand thus remains extremely fast and efficient. After finishing its step, GP Ui passes only the latent to GP Ui+1, keeping the communication cost negligible. To prevent pipeline bottlenecks, the VAE decoding stage is offloaded to an additional dedicated GPU, which consumes the clean latent and outputs synchronized video chunks. 4.4. Rolling Sink Frame for long video generation Existing talking-avatar systems exhibit pronounced degradation over long, streaming runsmanifesting as identity drift, color shifts, and temporal instability [38]. In practice we run inference in rolling-KV-cache [19], which extends the temporal horizon but does not by itself prevent collapse. We attribute these long-horizon failures to two internal phenomena. First, inference-mode drift: the conditioning pattern at inference (e.g., the RoPE-relative positioning between the sink frame and current target blocks) gradually diverges from the training-time setup, weakening identity cues. Second, distribution drift: the distribution of generated frames progressively deviates from normal, realistic video distributions, likely driven by persistent factors that continuously push the rolling generation toward unrealistic outputs. To address these challenges, we propose the Rolling Sink Frame Mechanism (RSFM), unified approach that maintains single, dynamically aligned sink frame throughout generation to ensure long-term consistency and fidelity. To counteract distribution drift, we propose Adaptive Attention Sink (AAS)2: immediately after the first block is generated, we replace the original sink frame with the models own first generated frame (VAE-encoded) and use that frame as the persistent sink for subsequent conditioning. By keeping the sink frame within the models learned generation distribution, AAS mitigates the persistent factors that would otherwise push the video toward unrealistic 2Details of AAS and Rolling RoPE are provided in the supplementary materials. color, exposure, or style deviations, while preserving identity cues. To mitigate inference-mode drift, we introduce Rolling RoPE2, dynamic position-alignment mechanism for the sink frame. The sink frame is permanently cached in KV cache and its temporal offset is adjusted via controllable RoPE shift so that its relative position to the current noisy states remains consistent with training. This dynamic RoPE alignment lets the model continuously reference identity features from sink frame without rigidly constraining local motion, thereby stabilizing long-range identity and structural fidelity. 5. Experiments 5.1. Experimental Settings Implementation Details The overall model architecture is borrowed from WanS2V [15], but the framepack is removed for conciseness. In stage 1, we initialize from its weights. In stage 2, the teacher score and fake score branches are initialized from Wan-S2V, while the student is initialized from Stage 1. All training and inference are performed at fixed resolution of 720400 and 84 frames. Experiments are conducted on 128 NVIDIA H800 GPUs, with 25 and 2.5 steps for stage 1 and stage 2, respectively, totaling about 500 GPU days. The per-GPU batch size is 1. To handle the high memory demand of Self-Forcing training, we adopt FSDP with gradient accumulation to reduce memory consumption. The learning rate is 1e-5 for the student and 2e-6 for the fake score. We group every 3 frames into block, set the KV cache length to 4 blocks, and use single rolling sink frame. We train models with LoRA, whose rank and alpha are set to 128 and 64, respectively. Datasets We train our model on the AVSpeech[10] dataset, large-scale audio-visual corpus comprising numerous video clips of human subjects with synchronized audio. To ensure data quality and maintain consistency with prior work, we follow the data preprocessing protocol established by OmniAvatar[13]. Additionally, we filter the raw data, retaining only video segments longer than 10 seconds for training. This process yields final training set of 400,000 high-quality samples. To evaluate our models outof-domain (OOD) generalization, we created synthetic benchmark named GenBench. This test set was generated using Gemini-2.5 Pro, Qwen-Image[43], and CosyVoice[9]. It is composed of two subsets: GenBench-ShortVideo, comprising 100 test samples with an approximate duration of 10 seconds, and GenBench-LongVideo, which contains 15 test videos, each exceeding 5 minutes in duration. The benchmark is designed to be challenging, featuring wide diversity of character styles (photorealistic humans, animated characters, and anthropomorphic non-humans) and visual compositions, including frontal and profile views, as well Table 2. Quantitative comparisons of our methods with state-of-the-art methods. Dataset Model Metrics GenBench-ShortVideo GenBench-LongVideo ASE IQA Sync-C Sync-D Dino-S FPS Ditto[24] Echomimic-V2[33] Hallo3[6] StableAvatar[38] OmniAvatar[13] WanS2V[14] Ours Ditto[24] Hallo3[6] StableAvatar[38] OmniAvatar[13] WanS2V[14] Ours 3.31 2.82 3.12 3.52 3.53 3.36 3.44 2.90 2.65 3.00 2.36 2.63 3. 4.24 3.61 3.97 4.47 4.49 4.29 4.35 4.48 4.04 4.66 2.86 3.99 4.73 4.09 5.57 4.74 3.42 6.77 5.89 5.69 3.98 6.18 1.97 8.00 6.04 6.28 10.76 9.13 10.19 11.33 8.22 9.08 9.13 10.57 9.29 13.57 7.59 9.12 8. 0.99 0.79 0.94 0.93 0.95 0.95 0.95 0.98 0.83 0.94 0.66 0.80 0.94 21.80 0.53 0.26 0.64 0.16 0.25 20.88 21.80 0.26 0.64 0.16 0.25 20.88 as half-body and full-body shots. This variety allows for robust assessment of the models performance on unseen data. Evaluation Metrics To evaluate our model, we assess frame-level image quality using FID[18] and overall video coherence with FVD[39]. For audio-visual synchronization, we measure the correspondence between lip movements and the input audio using Sync-C and Sync-D metrics[5]. Additionally, we employ the Q-align[44] model to evaluate the final videos perceptual quality (IQA) and aesthetic appeal (ASE). For our custom Out-of-Distribution (OOD) test set, we only compute Sync-C, Sync-D, IQA, and ASE, as ground truth videos are unavailable. 5.2. Comparison with Existing Methods We compare Live Avatar against current state-of-theart open-sourced audio-driven avatar generation approaches, including Ditto[24], Echomimic-V2[33], StableAvatar[38], OmniAvatar[13], and WanS2V[14]. Quantitative results on relevant objective metrics are presented in Table 2. In terms of visual quality, our method achieves competitive performance in ASE and IQA, comparable with OmniAvatar and Stable-Avatar and performs better than Ditto, Echomimic-V2 and WanS2V. Notably, although our approach is built upon WanS2V and employs step distillation, it demonstrates superior base visual quality compared to WanS2V. This observation aligns with findings in prior work utilizing DMD, where the DMD loss has been interpreted as specialized reinforcement learning objective that effectively optimizes both aesthetic appeal and foundational visual fidelity[29, 31]. Regarding audio-visual synchronization, our method also performs favorably, better than Ditto, Echomimic-V2 and StableAvatar and trailing only slightly behind WanS2V and OmniAvatar. This result is understandable given that step distillation inherently incurs some loss of dynamic temporal information[11]. The advantage of inference speed over WanS2V and OmniAvatar is the most favorable. More significantly, our method exhibits pronounced advantage in long-duration video generation. As shown in Table 2, which presents results for 7-minute-long videos, our approach substantially outperforms all baselines across almost every evaluation metric, especially on ASE and IQA. Qualitative visualizations in Figure 4 further corroborate this finding: existing methods, especially OmniAvatar and WanS2V, suffer from noticeable degradation in visual quality over extended durations, whereas our method consistently maintains high base-level visual fidelity throughout the entire sequence. Table 3. Ablation Study on Inference Efficiency. TTFF measures the end-to-end time-to-first-frame, which includes the VAE decoding time. Note that SP4GP indicates sequence parallelism of 4 GPUs. Methods #GPUs NFE FPS TTFF w/o TPP w/o TPP, w/ SP4GP w/o VAE Parallel w/o DMD Ours 2 5 4 5 5 5 4 80 4 4.26 5.01 10.16 0.29 20.88 3.88 3.24 4.73 45. 2.89 5.3. Ablation Study Study of Inference efficiency We analyze the impact of the Live Avatar model architecture and inference strategies on generation latency, considering the following variants: (1) Timestep-forcing Pipeline Parallelism (TPP), (2) disabling multi-GPU parallelism (denoted as w/ SP4GP ), (3) disFigure 4. Qualitative comparisons with state-of-the-art methods. Table 4. Ablation Study on Long Video Generation. Methods Metrics ASE IQA Sync-C Dino-S w/o AAS w/o Rolling RoPE w/o History Corrupt Ours 3.13 3.38 2.90 3.38 4.44 4.71 3.88 4.73 6.25 6.29 6.14 6.28 0.91 0.86 0.81 0.93 abling VAE parallelization, and (4) using Diffusion-forcing alone without DMD. We evaluate inference efficiency along two dimensions: (i) frames-per-second (FPS), defined as the average number of video frames generated per second, and (ii) the time-tofirst-frame (TTFF), i.e., the end-to-end latency to generate the first output frame. The corresponding results are summarized in Table 3. As shown in the table, DMD yields the most significant improvement in both FPS and TTFF, primarily due to its substantial reduction in the number of function evaluations (NFE). If TPP is discarded, there is remarkable throughput reduction to only 4 FPS. Moreover, employing sequence parallelism across GPUs provides only marginal gains in efficiency, which may stem from the fact that the inference sequence length per latent frame block is short and the computational cost is no longer the dominant factor in overall latency. Consequently, the VAE decoding latency constitutes non-negligible bottleneck for real-time generation, making parallelizing the VAE decoding critical design choice. Study on effectiveness of Rolling Sink Frame on long video generation Rolling Sink Frame is key inference technique enabling high-quality long-duration video generation. As shown in Table 4, removing both the Adaptive Attention Sink and the History Corrupt strategy leads to sharp decline in ASE and IQA scores, indicating that these components are crucial for maintaining perceptual and temporal consistency over minute-scale generations. Furthermore, discarding Rolling RoPE results in drop in the DINOv2 similarity metric. This suggests that Rolling RoPE plays an essential role in preserving semantic consistency between the current frame and the sink frame, thereby safeguarding global semantic attributes, especially identity, throughout extended sequences. 5.4. User Study Prior work such as THEval [35] has shown that popular metrics for talking-avatar evaluation (e.g., Sync-C) often diverge from human perception, as models can exploit them by exaggerating lip motion. To bridge this gap, we conduct double-blind user study with 20 participants, who rate generated videos from all methods on Naturalness, Synchronization, and Consistency. Here, Synchronization refers to the holistic audiovisual coherence [41]including facial expressions, gestures, and posture transitionsrather than the frame-level lip alignment measured by Sync-C. The final scores are averaged across participants and normalized to 0100 scale for comparison. As summarized in Table 5, although OmniAvatar achieves the best objective metrics, it is rated significantly lower by human evaluators, confirming that over-optimized lip movements harm perceptual naturalness. In contrast, our method attains the most balanced scores across all criteria, suggesting better alignment with human perception of natural and coherent motion. EchoMimic V2, which relies on fixed hand-landmark templates, shows degraded scores as it struggles to generalize to diverse body poses, often resulting in distorted geshances FPS but does not reduce TTFF, thereby limiting interactive responsiveness. In addition, the strong dependence on RSFM may compromise long-term temporal consistency in complex scenarios. Future work will focus on reducing latency and improving temporal coherence. Figure 5. Qualitative Comparison of Our Model, OmniAvatar, and Echomimic-v2. Results are sampled from GenBench-ShortVideo. tures. In contrast, our method is consistently preferred by human evaluators, indicating stronger alignment with human perception of natural and coherent motion. The results of the visualization are presented in 5 . Table 5. User study results on perceptual evaluation (higher is better). Each score denotes the mean normalized user rating. Model Naturalness Synchronization Consistency Ditto [24] EchoMimic-V2 [33] Hallo3 [6] StableAvatar [38] OmniAvatar [13] WanS2V [14] Ours 78.2 60.3 78.5 68.7 71.1 84.3 86.3 40.5 71.1 69.2 70.8 78.5 85.2 80.6 90.2 38.7 89.3 88.9 90.8 92.0 91.1 6. Conclusion In this work, we present Live Avatar, novel algorithmsystem co-designed framework that effectively bridges the gap between high-fidelity audio-driven avatar generation and real-world deployment requirements. By simultaneously addressing the two core challenges, i.e. real-time inference latency and long-horizon visual consistency, Live Avatar enables, for the first time, efficient streaming inference with large-scale diffusion models of up to 14 billion parameters without compromising visual realism or expressiveness. Specifically, through key techniques, our system achieves stable real-time streaming at 20 FPS using only 5 H800 GPUs. Meanwhile, the Rolling Sink Frame Mechanism effectively suppresses identity drift and color artifacts that typically accumulate during prolonged generation, significantly enhancing visual coherence and stability. To conclude, Live Avatar is capable of real-time, infinitelength, high-quality audio-driven avatar video generation, laying solid foundation for the practical deployment of next-generation interactive digital humans. Limitation The proposed TPP inference strategy enLive Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Overview of Supplementary Material This supplementary document provides comprehensive details, additional experiments, and implementation specifics to support the findings in the main paper. The content is organized as follows: Section 9: Additional Experimental Results. We investigate the impact of KV cache noise levels  (Table 6)  , evaluate our models long-horizon autoregressive extrapolation up to 10000s (Table 7, Figure 9), and further provide additional comparision against State-of-The-Art on AV-Speech test set  (Table 8)  . Section 10: Additional Experimental Setting. We detail the hardware configurations, inference configurations, and specific definitions for runtime metrics used in our benchmarking. Section 11: Algorithm Details. We provide complete pseudocode (Algorithm 1, 2, 3 and 4) for our methods, including the History Corrupt and Block-wise Gradient Accumulation training strategies, and detailed inference procedures for singleGPU setups (incorporating AAS and Rolling RoPE) and multi-GPU TPP. We further include visualizations (Figures 6, 7, and 8) to illustrate the mechanisms of different inference settings. Section 12: Additional Visual Results. We showcase further qualitative examples to demonstrate the temporal consistency and visual fidelity of our method. Section 13: Ethical Consideration. We discuss the potential societal impacts, privacy concerns, and responsible usage guidelines for our audio-driven avatar generation framework. 8. LLM USAGE STATEMENT. We use LLMs (e.g., Gemini-2.5 and GPT-5) to polish our paragraphs. 9. Additional Experimental Results. 9.1. Does the Noise Level of KV Caches Matter? Prior streaming systems such as CausVid [50], LongLive[46], and Self-Forcing [19] typically use clean KV caches, an intuitive choice given their higher information content. However, recent diffusion-based avatar models (e.g., TalkingMachine, StableAvatar) implicitly rely on same-timestep (same-noise) KV caches, without discussing why this design works or whether it is preferable. This motivates our study of simple question: does the noise level of the KV caches actually matter for streaming diffusion models? To answer this, we compare two settings: (1) Timestep-forcing, where each GPU handles one timestep and every noisy latent attends only to the KV cache of the same noise level. (2) Clean-kv-cache, where all latents attend to unified clean cache that must be generated via an extra forward pass (Algorithm 2 in [50]), introducing an additional 1 NFE. Since the clean cache for frame t+1 depends on the fully denoised output of frame t, each frame must wait for the previous one to finish before denoising can begin. Consequently, autoregressive generation cannot exploit pipeline parallelism and is limited to conventional sequential parallelism across GPUs, leading to substantial efficiency degradation. All experiments use four H800s and report both efficiency (FPS, TTFF) and quality (ASE, IQA, Sync-C, Sync-D, Dino-S). Results in Table 6 show that timestep-forcing (which also enable TPP) consistently outperform clean-kv-cache not only in efficiencyas discussed abovebut also in perceptual quality. We attribute the quality improvement to models pretraining: diffusion video models primarily learn cross-frame attention under same-timestep conditions, making noise-aligned In addition, the TPP setting is far more scalable, as KV features closer match to the learned denoising distribution. it avoids all inter-GPU KV communication, whereas the clean-KV alternative incurs substantial synchronization cost and higher latency. 9.2. Extending Autoregressive Generation to 10,000 Seconds. To rigorously evaluate the long-horizon autoregressive capability of our model, we construct stress test far exceeding the temporal range seen during training. Although the model is trained exclusively on 5-second clipsand its RoPE positions Table 6. Quantitative comparisons of timestep-forcing with traditional clean-kv-cache. Dataset Model Metrics #GPUs NFE ASE IQA Sync-C Sync-D Dino-S FPS TTFF GenBench-ShortVideo GenBench-LongVideo clean-kv-cache timestep-forcing clean-kv-cache timestep-forcing 4 4 4 4 5 5 4 3.26 3.44 3.05 3.38 4.01 4.35 4.44 4.73 5.69 5. 6.11 6.28 9.53 9.13 9.10 8.81 0.93 0.95 0.90 0.94 5.01 20. 5.01 20.88 3.24 2.89 3.24 2.89 during training are randomly shifted only within short-range window of few minuteswe extend inference to an extreme 10,000-second horizon. Each audio input in GenBench-LongVideo ( 10 minutes per sample) is repeated to match this duration, ensuring valid conditioning over the full sequence while avoiding degenerate periodicity. The model then performs fully autoregressive, block-wise generation like Self-Forcing [19], relying entirely on accumulated KV caches and our Rolling-RoPE strategy throughout the 10,000-second rollout. This setup intentionally exposes the model to RoPE indices over two orders of magnitude larger than those encountered in training (10k seconds corresponds to RoPE positions around 40k), regime where existing methods typically suffer severe attention degradation, ID drift, or visual collapse. Self-Forcing++ [7] demonstrates video generation of roughly 4 minutes, representing the longest horizon reported in prior work. In contrast, our model shows no observable quality decay or identity instability over the entire 10,000-second sequence. As shown in Table 7, perceptual quality (ASE, IQA), audiovisual synchronization (sync-C), and semantic consistency (DINO-s) remain nearly unchanged across segments sampled at 010 s, 100110 s, 10001010 s, and 1000010010 s. Figure 9 provides representative case, demonstrating consistent identity and visual fidelity even at the 10k-second horizon. Together, these results indicate that our long-video generation strategiesAAS, history corruption, and RollingRoPEallow the model to stably extrapolate far beyond its training regime, achieving an unprecedented 10,000-second autoregressive rollout without quality degradation. Table 7. Ablation Study on Long Video Generation. Methods Metrics ASE IQA Sync-C Dino-S 0-10s 100-110s 1000-1010s 10000-10010s 3.37 3.38 3.37 3.38 4.72 4.71 4.69 4.71 6.20 6.44 5.98 6. 0.94 0.93 0.93 0.93 9.3. Additional Comparison with Existing Methods. Although we have already provided comprehensive comparisons on GenBench in the main paper, we further evaluate the robustness of our method within its training domain, AVSpeech. Specifically, we hold out 5% of the original training videos and randomly sample 50 clips (510 seconds each) from this subset as an unseen test set. We report the same metrics used in the main evaluation; additionally, since ground-truth videos are available for this test set, we include FID and FVD to assess distribution alignment more thoroughly. The results are presented in Table 8. The results in Table 8 show that our method achieves competitive or superior performance across most metrics on the in-domain AVSpeech evaluation. Notably, OmniAvatar is also trained on AVSpeech and therefore serves as strong indomain baseline, yet our method remains competitive under this setting. Together with the results reported on multiple public benchmarks in the main paper, this additional experiment verifies that our approach performs reliably within its training domain and alleviates potential concerns about relying solely on benchmark-specific evaluations. 10. Additional Evaluation Details. Inference Configuration. All methods are evaluated on single H800 node under identical hardware conditions. To ensure fair comparison, we utilize multi-GPU parallel inference for all methods where the official open-source code provides apFigure 6. Illustration of different inference settings. Horizontally, each row follows the spatial denoising order from low to high SNR; vertically, each column shows the autoregressive rollout over frames. Each small rectangle denotes the latent of block, and the number inside represents its block index. Solid arrows indicate direct sink frame passing, whereas dashed arrows indicate KV-cache passing. Red marks indicate the components modified relative to the baseline. (a) Baseline with fixed sink frame and standard rolling-kv-cache. (b) AAS with the sink replaced by the first generated latent. (c) Timestep-forcing with each noisy latent attending only to KV caches of the same timestep. (d) Ours with both AAS and timestep-forcing. Figure 7. Visualization of the proposed Rolling-RoPE mechanism. Horizontally, each row follows the spatial denoising order from low to high SNR; vertically, each column shows the autoregressive rollout over frames. Each small rectangle denotes the latent of block, and the number inside represents its block index and its RoPE index, respectively. Red marks indicate the components modified relative to the baseline. Solid arrows denote sink-frame passing, sparse dashed arrows denote standard KV-cache passing, and dense dashed arrows indicate RoPE updates, where each block is reassigned updated positional embeddings. Rolling-RoPE dynamically increases the RoPE index of the sink frame throughout AR rollout, keeping the sink frames RoPE index slightly larger than that of the current noisy block, ensuring stable and appropriate relative positional distance throughout AR rollout. Table 8. Quantitative comparisons of our methods with other methods."
        },
        {
            "title": "Metrics",
            "content": "AV-Speech FID FVD ASE IQA Sync-C Sync-D Dino-S FPS Ditto[24] Echomimic-V2[33] Hallo3[6] StableAvatar[38] OmniAvatar[13] WanS2V[14] Ours 46.27 176.74 138.40 98.32 50.42 73.68 64.00 660.01 2059.81 1412.93 730.12 570.32 642.48 532.52 2.21 1.88 1.87 2.14 2.16 2.20 2. 3.75 3.29 3.35 3.55 3.68 3.71 3.84 4.84 4.07 4.50 5.72 6.04 4.90 4.64 9.05 9.38 9.99 9.01 8.37 9.02 9.35 0.99 0.78 0.93 0.93 0.95 0.95 0.95 21.80 0.53 0.26 0.64 0.16 0.25 20.88 propriate scripts. For methods that lack support for parallel acceleration, specifically EchoMimic V2 and Ditto, inference is conducted on single H800 GPU. Regarding resolution, we use fixed resolution of 720400 for all models except Hallo3, which does not support arbitrary aspect ratios or resolutions. For Hallo3, we crop both the input and the ground-truth frames to 512512. Furthermore, EchoMimic is excluded from the long-video comparative experiments, as its reliance on per-frame skeleton templates prevents it from effectively performing long-duration inference. Runtime Metrics. For runtime evaluation, we report two key efficiency metrics. FPS (Frame Per Second) is measured from the moment the inference pipeline is initialized and include the full end-to-end cost: the diffusion models denoising time, VAE decoding time, and any additional CPU-side processing. Time-to-First-Frame (TTFF) accounts for the total latency from audio signal arrival to visual output, calculated as the sum of (i) the random arrival latencythe waiting time between the arrival of an audio interaction signal and the next frame boundaryfollowed by (ii) the full denoising latency of the first frame and (iii) its VAE decoding cost. Note that random arrival latency depends on the output frame rate, and thus TTFF is inherently coupled with the FPS of the method. All FPS measurements for all methods are evaluated on the GenBench-LongVideo benchmark, where long-sequence testing provides more stable and accurate runtime estimation. 11. Algorithm Details. We provide detailed pseudocode for completeness. As shown in Algorithm 1, our self-forcing DMD training follows [19] but removes the additional forward pass used to refresh the KV cache after denoising. This ensures that the model is consistently trained with noisy KV states, aligning the training process with the actual autoregressive inference process and improving robustness to error accumulation. We refer to this strategy as History Corrupt. Due to the substantial memory footprint of DMD training, we additionally implement lightweight memory-reduction strategy using block-wise gradient accumulation (Algorithm 2). We partition the backward graph by blocks and accumulate the resulting gradients across multiple steps, which preserves training behavior while significantly reducing peak memory usage, enabling even single-node H800 training. The single-GPU inference procedure is provided in Algorithm 3. It builds upon the rolling-KV-cache inference algorithm from [19] with the addition of AAS and Rolling RoPE, as described in the main text. Figure 6 and Figure 7 provides visual comparison of the different inference strategies, highlighting key operations such as AAS and Rolling RoPE. The figure also illustrates the rope update performed during autoregressive generation, where the sink frame is continuously re-assigned with updated RoPE. The multi-GPU TPP inference is detailed in Algorithm 4, which outlines the parallel execution procedure with minimal computation overlap and communication overhead. Figure 8 further visualizes the computation and waiting time of each Figure 8. Multi-GPU Parallel Inference Timeline. This chart visualizes the computation and waiting periods for each GPU. The two distinct white gaps on the left represent the initial warmup phase (including the secondary warmup for AAS). Following these, the majority of the processing time is dedicated to DiT computation (shown in red), reflecting high utilization and stable frame rates. Sporadic white gaps (idle time) appearing thereafter are present due to rate fluctuations, but their rarity ensures negligible impact on performance. GPU. After the initial warmup (including second warmup required for AAS), the majority of each GPUs time is devoted to DIT computation (red), demonstrating high utilization and stable frame rates. 12. Additional Visualize results. We provide additional qualitative examples to further illustrate the models long-horizon generation capability. As shown in Figure 9, our method maintains stable identity, consistent lip movements, and coherent visual appearance when extrapolating videos far beyond the training horizon. For three different subjects, the generated frames at 10s, 100s, 1000s, and 10000s remain visually aligned with the reference image and follow the audio-driven motion patterns without exhibiting temporal drift or degradation. These results highlight the robustness of our approach in producing coherent long-duration talking-face videos. Figure 9. Visualization of the generated video at 10 s, 100 s, 1000 s, and 10000 s, demonstrating the models strong capability in longhorizon temporal extrapolation. Algorithm 1 Self-Forcing DMD with History Corrupt θ ) Require: Timesteps {t1, . . . , tT } Require: Number of video frames Require: Conditions of frames C1:N (including audio,text,ref image) Require: Generator Gθ (returns KV embeddings via GKV 1: loop 2: 3: 4: 5: 6: 7: 8: 9: 10: Initialize model output Xθ [] Initialize KV cache KV [] Sample Uniform(1, 2, . . . , ) for = 1, . . . , do Initialize xi for = T, . . . , do if = then tT (0, I) tj ; tj, KV, Ci) Enable gradient computation Set kvi, ˆxi θ (xi 0 GKV Xθ.append(ˆxi 0) Detach kvi from gradient graph KV.append(kvi) else Disable gradient computation Set ˆxi 0 Gθ(xi Sample ϵ (0, I) tj1 Ψ(ˆxi Set xi 0, ϵ, tj1) tj ; tj, KV, Ci) end if end for end for Update θ via distribution matching loss 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: end loop Noisy KV Cache 13. Ethical consideration. Our work focuses on enabling real-time and long-horizon audio-driven avatar generation, which naturally raises concerns related to privacy, consent, and potential misuse. All identity data used in training and evaluation is collected with permission, and our method does not store or reconstruct unauthorized identities. While high-fidelity avatars may be susceptible to impersonation risks, our system is intended solely for legitimate telepresence and interactive applications. We encourage responsible deployment practices such as access control and watermarking to prevent malicious use. Algorithm 2 Self-Forcing DMD with History Corrupt and Block-wise Gradient Accumulation θ ) Require: Timesteps {t1, . . . , tT } Require: Number of video frames Require: Conditions of frames C1:N (including audio,text,ref image) Require: Generator Gθ (extra returns KV embeddings via GKV 1: loop 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: Initialize model output Xθ [] Initialize noisy latent cache Xcache [] Initialize KV cache KV [] Sample Uniform(1, 2, . . . , ) Disable gradient computation for = 1, . . . , do Initialize xi for = T, . . . , do if = then tT (0, I) tj ; tj, KV, Ci) θ (xi 0, kvi from gradient graph Set kvi, ˆxi 0 GKV tj , ˆxi Detach xi Xcache.append(xi Xθ.append(ˆxi 0) KV.append(kvi) tj ) else tj ; tj, KV, Ci) 0 Gθ(xi Set ˆxi Sample ϵ (0, I) tj1 Ψ(ˆxi Set xi 0, ϵ, tj1) end if end for end for for = N, . . . , 1 do Initialize temporary model output Xi [] for = 1, . . . , do if = then Set xi ts Xcache[j] Enable gradient computation Set ˆxi 0 Gθ(xi ts ; ts, KV, Ci) Xi.append(ˆxi 0) Disable gradient computation else Xi.append(Xθ[j]) end if end for Accumulate gradient of θ via DMD loss KV.pop(i) 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: end loop end for Update θ Handle Partial Gradient Free Memory Algorithm 3 Single-GPU AR Inference with AAS and Rolling RoPE (extra returns KV embeddings) θ Require: Per-timestep KV caches, each with size Require: Timesteps {t1, . . . , tT } Require: Number of generated frames Require: Conditions of frames C1:N (including audio,text) Require: Ref image Require: Flow-Matching Model vKV Require: Rolling RoPE transform Φ() Require: VAE Decoder VAE() 1: Initialize model output Xθ [] 2: Initialize KV caches {KV1, . . . , KVT } [] 3: Initialize Rolling Sink Frame Sink 4: Initialize dt 1/T 5: for = 1, . . . , do 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: end for 20: return Xθ θ (xi; tj, KVj, Ci, Φ(Sink)) Set ˆvi j, kvi vKV Set xi xi + ˆvi jdt if KVj = then KVj.pop(0) end for Xθ.append(VAE(xi)) if = 1 then Initialize xi (0, I) for = T, . . . , 1 do end if KVj.append(kvi j) Sink xi end if RoPE Update AAS Update Algorithm 4 TPP with AAS and Rolling RoPE (extra returns KV embeddings) θ else xi dist.recv() Initialize xi (0, I) end if if k=T+1 then Require: GPU Index Require: Per-timestep KV caches, each with size Require: Timesteps {t1, . . . , tT } Require: Number of generated frames Require: Conditions of frames C1:N (including audio,text) Require: Ref image Require: Flow-Matching Model vKV Require: Rolling RoPE transform Φ() Require: VAE Decoder VAE() 1: Initialize model output Xθ [] 2: Initialize KV cache KV [] 3: Initialize Rolling Sink Frame Sink 4: Initialize dt 1/T 5: for = 1, . . . , do if k=1 then 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: end for 31: return Xθ end if KV.append(kvi) Set xi xi + ˆvi kdt dist.send(xi) end if k, kvi vKV Set ˆvi if KV = then KV.pop(0) Xθ.append(VAE(xi)) if = 1 then Sink Xθ[i] dist.broadcast(Sink) end if continue Sink dist.recv() if i=2 then end if else θ (xi; tT k+1, KV, Ci, Φ(Sink)) VAE Device Broadcast AAS DiT Device Update AAS"
        },
        {
            "title": "References",
            "content": "[1] Tenglong Ao. Body of her: preliminary study on end-to-end humanoid agent. arXiv preprint arXiv:2408.02879, 2024. 3 [2] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 2, 3 [3] Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024. 2 [4] Ming Chen, Liyuan Cui, Wenyuan Zhang, Haoxian Zhang, Yan Zhou, Xiaohan Li, Songlin Tang, Jiwen Liu, Borui Liao, Hejia Chen, et al. Midas: Multimodal interactive digital-human synthesis via real-time autoregressive video generation. arXiv preprint arXiv:2508.19320, 2025. 3 [5] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Computer VisionACCV 2016 Workshops: ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II 13, pages 251263. Springer, 2017. [6] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with video diffusion transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2108621095, 2025. 2, 7, 9, 4 [7] Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, and Cho-Jui Hsieh. Self-forcing++: Towards minute-scale high-quality video generation. arXiv preprint arXiv:2510.02283, 2025. 2, 3 [8] Fangyu Du, Taiqing Li, Ziwei Zhang, Qian Qiao, Tan Yu, Dingcheng Zhen, Xu Jia, Yang Yang, Shunshun Yin, and Siyuan Liu. Rap: Real-time audio-driven portrait animation with video diffusion transformer. arXiv preprint arXiv:2508.05115, 2025. 3 [9] Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, Fan Yu, Huadai Liu, Zhengyan Sheng, Yue Gu, Chong Deng, Wen Wang, Shiliang Zhang, Zhijie Yan, and Jingren Zhou. Cosyvoice 2: Scalable streaming speech synthesis with large language models, 2024. [10] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William Freeman, and Michael Rubinstein. Looking to listen at the cocktail party: speaker-independent audio-visual model for speech separation. arXiv preprint arXiv:1804.03619, 2018. 6 [11] Xiangyu Fan, Zesong Qiu, Zhuguanyu Wu, Fanzhou Wang, Zhiqian Lin, Tianxiang Ren, Dahua Lin, Ruihao Gong, and Lei Yang. Phased dmd: Few-step distribution matching distillation via score matching within subintervals. arXiv preprint arXiv:2510.27684, 2025. 7 [12] Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel. One step diffusion via shortcut models. arXiv preprint arXiv:2410.12557, 2024. 3 [13] Qijun Gan, Ruizi Yang, Jianke Zhu, Shaofei Xue, and Steven Hoi. Omniavatar: Efficient audio-driven avatar video generation with adaptive body animation. arXiv preprint arXiv:2506.18866, 2025. 2, 6, 7, 9, 4 [14] Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, Ke Sun, Linrui Tian, Guangyuan Wang, Qi Wang, Zhongjian Wang, Jiayu Xiao, Sheng Xu, Bang Zhang, Peng Zhang, Xindi Zhang, Zhe Zhang, Jingren Zhou, and Lian Zhuo. Wan-s2v: Audio-driven cinematic video generation, 2025. 2, 7, 9, 4 [15] Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, et al. Wan-s2v: Audio-driven cinematic video generation. arXiv preprint arXiv:2508.18621, 2025. 2, 6 [16] Ying Guo, Xi Liu, Cheng Zhen, Pengfei Yan, and Xiaoming Wei. Arig: Autoregressive interactive head generation for real-time conversations. arXiv preprint arXiv:2507.00472, 2025. [17] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. 2, 3 [18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 7 [19] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 2, 3, 5, 6, [20] Jianwen Jiang, Weihong Zeng, Zerong Zheng, Jiaqi Yang, Chao Liang, Wang Liao, Han Liang, Yuan Zhang, and Mingyuan Gao. Omnihuman-1.5: Instilling an active mind in avatars via cognitive simulation. arXiv preprint arXiv:2508.19209, 2025. 2 [21] Akio Kodaira, Tingbo Hou, Ji Hou, Masayoshi Tomizuka, and Yue Zhao. Streamdit: Real-time streaming text-to-video generation. arXiv preprint arXiv:2507.03745, 2025. 2 [22] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [23] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. 3 [24] Tianqi Li, Ruobing Zheng, Minghui Yang, Jingdong Chen, and Ming Yang. Ditto: Motion-space diffusion for controllable realtime talking head synthesis. arXiv preprint arXiv:2411.19509, 2024. 2, 3, 7, 9, 4 [25] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [26] Kunhao Liu, Wenbo Hu, Jiale Xu, Ying Shan, and Shijian Lu. Rolling forcing: Autoregressive long video diffusion in real time. arXiv preprint arXiv:2509.25161, 2025. 2 [27] Chetwin Low and Weimin Wang. Talkingmachines: Real-time audio-driven facetime-style video via autoregressive diffusion models. arXiv preprint arXiv:2506.03099, 2025. 3, 5, 6 [28] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. 3 [29] Weijian Luo. Diff-instruct++: Training one-step text-to-image generator model to align with human preferences. arXiv preprint arXiv:2410.18881, 2024. 3, 7 [30] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct: universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36:7652576546, 2023. 3 [31] Yihong Luo, Tianyang Hu, Jiacheng Sun, Yujun Cai, and Jing Tang. Learning few-step diffusion models by trajectory distribution matching. arXiv preprint arXiv:2503.06674, 2025. 3, 7 [32] Dechao Meng, Steven Xiao, Xindi Zhang, Guangyuan Wang, Peng Zhang, Qi Wang, Bang Zhang, and Liefeng Bo. Mirrorme: Towards realtime and high fidelity audio-driven halfbody animation. arXiv preprint arXiv:2506.22065, 2025. 3 [33] Rang Meng, Xingyu Zhang, Yuming Li, and Chenguang Ma. Echomimicv2: Towards striking, simplified, and semi-body human animation, 2025. 7, 9, 4 [34] KR Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, and CV Jawahar. lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM international conference on multimedia, pages 484492, 2020. 3 [35] Nabyl Quignon, Baptiste Chopin, Yaohui Wang, and Antitza Dantcheva. Theval. evaluation framework for talking head video generation, 2025. 8 [36] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023. 3 [37] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions. In European Conference on Computer Vision, pages 244260. Springer, 2024. 3 [38] Shuyuan Tu, Yueming Pan, Yinming Huang, Xintong Han, Zhen Xing, Qi Dai, Chong Luo, Zuxuan Wu, and Yu-Gang Jiang. Stableavatar: Infinite-length audio-driven avatar video generation. arXiv preprint arXiv:2508.08248, 2025. 2, 6, 7, 9, 4 [39] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 7 [40] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3 [41] Mengchao Wang, Qiang Wang, Fan Jiang, Yaqi Fan, Yunpeng Zhang, Yonggang Qi, Kun Zhao, and Mu Xu. Fantasytalking: Realistic talking portrait generation via coherent motion synthesis. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 98919900, 2025. [42] Zhongjian Wang, Peng Zhang, Jinwei Qi, Guangyuan Wang Sheng Xu, Bang Zhang, and Liefeng Bo. Omnitalker: Real-time textdriven talking head generation with in-context audio-visual style replication. arXiv e-prints, pages arXiv2504, 2025. 3 [43] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. 6 [44] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023. 7 [45] You Xie, Tianpei Gu, Zenan Li, Chenxu Zhang, Guoxian Song, Xiaochen Zhao, Chao Liang, Jianwen Jiang, Hongyi Xu, and Linjie Luo. X-streamer: Unified human world modeling with audiovisual interaction. arXiv preprint arXiv:2509.21574, 2025. 3, 4 [46] Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, et al. Longlive: Real-time interactive long video generation. arXiv preprint arXiv:2509.22622, 2025. 2, 3, 5, 1 [47] Shaoshu Yang, Zhe Kong, Feng Gao, Meng Cheng, Xiangyu Liu, Yong Zhang, Zhuoliang Kang, Wenhan Luo, Xunliang Cai, Ran He, et al. Infinitetalk: Audio-driven video generation for sparse-frame video dubbing. arXiv preprint arXiv:2508.14033, 2025. 2 [48] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024. 3, 4 [49] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. [50] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2296322974, 2025. 2, 3, 4, 5, 1 [51] Haojie Yu, Zhaonian Wang, Yihan Pan, Meng Cheng, Hao Yang, Chao Wang, Tao Xie, Xiaoming Xu, Xiaoming Wei, and Xunliang Cai. Lliaenabling low-latency interactive avatars: Real-time audio-driven portrait video generation with diffusion models. arXiv preprint arXiv:2506.05806, 2025. 3 [52] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86528661, 2023. 3 [53] Dingcheng Zhen, Shunshun Yin, Shiyang Qin, Hou Yi, Ziwei Zhang, Siyuan Liu, Gan Qi, and Ming Tao. Teller: Real-time streaming audio-driven portrait animation with autoregressive motion generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2107521085, 2025. 3 [54] Kaiwen Zheng, Yuji Wang, Qianli Ma, Huayu Chen, Jintao Zhang, Yogesh Balaji, Jianfei Chen, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Large scale diffusion distillation via score-regularized continuous-time consistency. arXiv preprint arXiv:2510.08431, 2025. 3 [55] Yongming Zhu, Longhao Zhang, Zhengkun Rong, Tianshu Hu, Shuang Liang, and Zhipeng Ge. Infp: Audio-driven interactive head generation in dyadic conversations. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1066710677, 2025."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Beijing University of Posts and Telecommunications",
        "University of Science and Technology of China",
        "Zhejiang University"
    ]
}