{
    "paper_title": "VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding",
    "authors": [
        "Jiapeng Shi",
        "Junke Wang",
        "Zuyao You",
        "Bo He",
        "Zuxuan Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents VideoLoom, a unified Video Large Language Model (Video LLM) for joint spatial-temporal understanding. To facilitate the development of fine-grained spatial and temporal localization capabilities, we curate LoomData-8.7k, a human-centric video dataset with temporally grounded and spatially localized captions. With this, VideoLoom achieves state-of-the-art or highly competitive performance across a variety of spatial and temporal benchmarks (e.g., 63.1 J&F on ReVOS for referring video object segmentation, and 48.3 R1@0.7 on Charades-STA for temporal grounding). In addition, we introduce LoomBench, a novel benchmark consisting of temporal, spatial, and compositional video-question pairs, enabling a comprehensive evaluation of Video LLMs from diverse aspects. Collectively, these contributions offer a universal and effective suite for joint spatial-temporal video understanding, setting a new standard in multimodal intelligence."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 1 0 9 2 7 0 . 1 0 6 2 : r VideoLoom: Video Large Language Model for Joint Spatial-Temporal Understanding Jiapeng Shi1, Junke Wang1, Zuyao You1, Bo He2, Zuxuan Wu1, 1Fudan University, 2University of Maryland, College Park"
        },
        {
            "title": "Abstract",
            "content": "This paper presents VideoLoom, unified Video Large Language Model (Video LLM) for joint spatial-temporal understanding. To facilitate the development of fine-grained spatial and temporal localization capabilities, we curate LoomData-8.7k, human-centric video dataset with temporally grounded and spatially localized captions. With this, VideoLoom achieves state-of-the-art or highly competitive performance across variety of spatial and temporal benchmarks (e.g., 63.1 &F on ReVOS for referring video object segmentation, and 48.3 R1@0.7 on Charades-STA for temporal grounding). In addition, we introduce LoomBench, novel benchmark consisting of temporal, spatial, and compositional videoquestion pairs, enabling comprehensive evaluation of Video LLMs from diverse aspects. Collectively, these contributions offer universal and effective suite for joint spatial-temporal video understanding, setting new standard in multimodal intelligence. Correspondence: zxwu@fudan.edu.cn Website: https://github.com/JPShi12/VideoLoom"
        },
        {
            "title": "Introduction",
            "content": "Recent years have witnessed the rapid development of Multimodal Large Language Models (MLLMs) [1, 2, 7, 26, 53], extending their scope from static image understanding [6, 38, 39, 43, 56] to dynamic video comprehension [34, 35, 42, 46, 55, 70]. Video Large Language Models (Video LLMs), which integrate spatial perception with temporal reasoning, have demonstrated strong generalization and competitive performance across wide range of multimodal benchmarks. More recently, increasing efforts have been devoted to equipping Video LLMs with fine-grained understanding capabilities, such as temporal grounding [19, 23, 36, 50, 58], referring video segmentation [15, 37, 62, 67], and object tracking [3, 63, 75]. Despite these achievements, most existing models still focus on either temporal or spatial dimension in isolation, limiting their ability to holistically interpret complex spatial-temporal events in real-world scenarios. While joint spatial-temporal understanding represents promising direction for Video LLMs, several critical challenges still remain. First and foremost, fundamental limitation is the scarcity of high-quality datasets with fine-grained spatial-temporal annotations. Most existing datasets provide either temporal (e.g., event segments [29, 74]) or spatial labels (e.g., object trajectories [10, 51]), but rarely both. straightforward practice is to jointly train on both types of datasets, but inconsistencies in annotation formats and data distributions often lead to unstable training and hinder the model from establishing coherent spatialtemporal associations. In addition, spatial and temporal video tasks inherently demand different input granularities, i.e., spatial tasks typically require higher resolutions to capture fine-grained details [62, 67], while temporal tasks depend Corresponding authors. 1 on denser frame sampling to model motion dynamics [18, 50]. Under fixed computational budgets, it is difficult to balance both requirements, making joint spatialtemporal modeling within single framework inherently challenging. To address the above issues, we first introduce LoomData-8.7k, novel dataset with consistent spatial and temporal annotations. LoomData-8.7k sources videos from ActivityNet [4] and is annotated using an automatic pipeline. Specifically, we first segment each untrimmed video into multiple shots and then identify the main characters in the initial shot. Based on this, we track trajectories and generate corresponding action descriptions for each character. This character-centric, shot-guided automatic annotation pipeline provides richer spatial references and complete temporal coverage, enabling detailed and coherent spatialtemporal understanding. With this, we further introduce VideoLoom, simple yet effective video large language model (Video LLM) for joint spatialtemporal understanding. To accommodate both capabilities within single framework, we combine multi-frame inputs that capture temporal dynamics with high-resolution keyframe inputs that preserve fine-grained spatial details. Two types of visual tokens, i.e., fast tokens and slow tokens, are introduced to balance temporal coverage and spatial precision. The former are generated from up to 128 frames uniformly sampled across the entire video span, providing global temporal context with low token density per frame. The latter are extracted from 5 keyframes, each allocated higher token density to encode spatial details at high resolution. These SlowFast visual tokens are interleaved with language instructions to form the input sequence of the Video LLM, enabling coherent and efficient spatialtemporal reasoning over the entire video. To comprehensively evaluate the spatialtemporal understanding capability of Video LLMs, we also propose LoomBench, benchmark comprising 130 videos and over 1,400 question-answering pairs spanning temporal grounding and spatial segmentation. Unlike existing datasets that assess these dimensions separately [10, 29], LoomBench consists of carefully designed questions that require models to perform grounding and segmentation simultaneously. Experiment results demonstrate that VideoLoom achieves new state-of-the-art on wide range of video understanding benchmarks, including spatial benchmarks (e.g., 51.7 &F on MeVIS [10], 63.1 &F on ReVOS [62]) and temporal ones (e.g., 48.3 R1@0.7 on Charades-STA [14], 7.3 SODA_c on YouCook2 [74], 63.3 HIT@1 on QVHighlights [31]). Comparisons with existing Video LLMs on LoomBench further validate the effectiveness of VideoLoom in unified spatial-temporal comprehension."
        },
        {
            "title": "2.1 Spatial-Temporal Video Datasets",
            "content": "Existing video datasets for spatial-temporal understanding can generally be categorized into two separate types: temporal-focused and spatial-focused. The temporal-focused datasets, e.g., for dense captioning [29, 74] or temporal grounding [14], provide descriptions or queries aligned with timestamps but typically lack spatial annotations. In contrast, the spatial-focused datasets focus on spatial localization through segmentation masks [10, 28, 51, 62] or trajectory annotations [11, 25, 45], but do not include detailed temporal locations of actions. Few datasets focus on atomic actions, featuring coarse-grained spatial-temporal tubelets [17, 72], yet constrained by extremely short durations, typically around 10 seconds. Additionally, current datasets rely on costly manual annotations with brief captions of objects or events, lacking detailed positional references and temporal coverage. Collectively, these factors hinder the training of Video LLMs with spatial-temporal comprehension. To bridge this gap, we introduce LoomData-8.7k, providing both fine-grained temporal annotations and mask-level spatial tracklets for long-form videos at scale, enabling more comprehensive spatial-temporal modeling."
        },
        {
            "title": "2.2 Video Large Language Models",
            "content": "Recent advancements in MLLMs reveal clear trend in visual comprehension from basic image [38, 43, 65] to video stream [20, 21, 35, 57, 68]. While early models primarily focus on coarse-grained tasks such as captioning 2 Figure 1 Illustration of the designed data annotation pipeline, comprising four stages: shot partition, spatial mask annotation, shot merging, and temporal action annotation. During spatial mask annotation, main characters and their complete tracklets are identified. In temporal action annotation, actions of characters are temporally grounded with visual prompts. and retrieval [52, 61], there is growing need for fine-grained understanding that captures precise object interactions and temporal dynamics. Within this landscape, Video LLMs designed for fine-grained video understanding can be broadly categorized into two directions: temporal-focused and spatial-focused models. The former, such as TimeChat [50] and TRACE [18], are trained on timestamp-aware instruction data to develop the temporal localization capabilities. Spatial models, on the other hand, focus on grounding visual regions in the format of trajectories [15, 62, 67]. While both directions address critical aspects of video understanding, neither is sufficient in isolation. Some works [24, 33, 54, 60] begin to model the spatial-temporal clues in video simultaneously, yet they remain confined to specific tasks or coarse-grained perception (e.g., sparse spatial bounding boxes). In this paper, we propose unified Video LLM, VideoLoom, that accommodates both fine-grained temporal understanding and spatial perception within single framework."
        },
        {
            "title": "3.1 LoomData-8.7k",
            "content": "We develop an automatic annotation pipeline that leverages multiple visual foundation models to detect and associate the temporal actions and spatial locations of main characters. As shown in Fig. 1, the pipeline comprises four main stages: (i) shot partition, (ii) spatial mask annotation, (iii) shot merging, and (iv) temporal action annotation. Shot Partition. We first partition each video into several shots using PySceneDetect [5] and KTS [47]. PySceneDetect identifies scene boundaries by detecting scene changes between adjacent frames, while KTS captures event transitions. We combine both by sequentially ordering the timestamps of all transition points to achieve accurate shot partition for different scenes. simple filtering strategy is then applied by merging shots shorter than 1 second and discarding videos exceeding 10 shots. Spatial Mask Annotation. For each video, we use GroundingDINO [40] to detect the person category in the center frame of the longest shot, and only keep the bounding box with the highest score as the main 3 Figure 2 Overview of VideoLoom Architecture. Two key designs are: (a) MLLM-SAM2 Architecture, where MLLM and SAM2 are connected via [SEG] token, unifying temporal understanding and spatial perception. (b) SlowFast Tokens, where input videos are encoded as SlowFast visual tokens to model spatial-temporal representations. character. With this region box, detailed description of its appearance (e.g., clothing and attributes) is generated by Pix2Cap [65]. After this, we employ SAM2 [49] to track the main character throughout the video to produce the initial mask tracklet. We then complete the cross-shot tracklet in grounding-tracking paradigm. GroundingDINO is also applied to re-annotate this character in the center frame of shots without tracklet, based on the description. SAM2 then conducts mask tracking on these shots to fill the missing masks, yielding complete tracklet of the main character. Finally, we perform manual verification step to refine redundantly tracked shots and remove incorrectly tracked videos. Shot Merging. As temporal event may span multiple shots (e.g., different camera angles), we merge all adjacent shots annotated with tracklets to obtain temporally consistent annotations. Temporal Action Annotation. With the merged shots and dense trajectories, we then generate detailed, timestamp-aligned action descriptions for main characters in each video. Specifically, we place unique numerical IDs on video frames sampled at 2 FPS in the manner of NumPro [59], and then employ Set-of-Marks (SoM) [64] to overlay an instance ID directly onto the segmentation masks of main characters. These sampled frames, along with both visual prompts, are fed into Gemini2.5pro [8] to produce fine-grained action descriptions aligned with the frame IDs. We annotate the training set of ActivityNet [4] with the above pipeline, resulting in 8,710 shots featuring both timestamp-aligned action descriptions and dense spatial masks. On average, each video has duration of 102.2 seconds and includes 6.0 shots, while the temporal descriptions average 41.3 words. For additional statistics, please refer to Sec. C.2."
        },
        {
            "title": "3.2 VideoLoom",
            "content": "With the above dataset, we further propose VideoLoom, unified Video LLM to unlock joint spatial-temporal understanding capabilities. Specifically, taking language query and video consisting of frames RN HW 3 as input, where and denote the height and width of each frame respectively, VideoLoom aims to generate an answer text that contains the required timestamp information, or predict trajectory in the format of segmentation masks RN HW : O, = VideoLoom(T, ). (1) Below, we introduce the SlowFast visual tokens which capture spatialtemporal information at different granularities in Sec. 3.2.1, the MLLM-SAM2 architecture which integrates these tokens for unified spatialtemporal modeling in Sec. 3.2.2, and the loss functions in Sec. 3.2.3."
        },
        {
            "title": "3.2.1 SlowFast Visual Tokens",
            "content": "Temporal understanding typically requires processing large number of frames [23, 50], whereas spatial perception demands higher-resolution inputs [67]. To accommodate both, we introduce two types of visual tokens, i.e., fast tokens and slow tokens, which respectively encode dense low-resolution frames with temporal bindings and sparse high-resolution keyframes with rich spatial details. Specifically, we sparsely sample Ns high-resolution keyframes and assign tokens for each frame to form Ns slow tokens. Meanwhile, we also densely sample Nf frames across the entire video. Both are fed fast tokens, where denotes the spatial to visual encoder [7] to obtain Ns slow tokens and Nf R2 downsampling ratio."
        },
        {
            "title": "3.2.2 MLLM-SAM2 Architecture\nOverview. We integrate InternVL3 [76], a multimodal large language model (MLLM), with SAM2 [49],\na video segmentation and tracking model, to support both spatial and temporal tasks within a unified\nframework. InternVL3 takes SlowFast visual tokens and text prompts as inputs, producing text responses,\ntimestamps, and a [SEG] token embedding. SAM2 then utilizes this [SEG] token to generate corresponding\nsegmentation masklets. The overall architecture is illustrated in Fig. 2.",
            "content": "MLLM for temporal understanding tasks. Our MLLM consists of visual encoder, visual projection layer, and an LLM. The sampled frames are input to the visual encoder and then mapped into visual tokens by the visual projection layer. Unlike previous work using absolute timestamps [50, 69] or special time tokens [18, 24], we interleave unique frame IDs between visual tokens to indicate temporal order. The complete token sequence is used as input to the LLM, which models the spatial-temporal visual features and generates text token predictions according to text queries. Note that for timestamp-related queries, the LLM outputs corresponding frame IDs in text responses to indicate temporal locations. SAM2 for spatial understanding tasks. Given the keyframes sampled for slow tokens, we input them to SAM2 to predict spatial trajectories. [SEG] token is used to connect MLLM with SAM2 mask decoder, providing the mask decoder with rich target information and prompting it to generate masks in the keyframes. We then propagate these masks to the entire video using visual memory [67]."
        },
        {
            "title": "3.2.3 Loss Functions",
            "content": "VideoLoom is trained in an end-to-end manner with the following objective: = λtextLtext + λmaskLmask, (2) where Ltext denotes the standard cross-entropy loss for text generation, and Lmask indicates the segmentation loss combining per-pixel binary cross-entropy (BCE) loss and DICE loss [44]. λtext and λmask are balancing hyper-parameters."
        },
        {
            "title": "3.3 LoomBench",
            "content": "We curate LoomBench, new benchmark designed to jointly evaluate the spatial and temporal understanding capabilities of Video LLMs. Specifically, we apply the automatic annotation pipeline described in Sec. 3.1 to the validation set of ActivityNet [4] to generate preliminary annotations. These annotations are then manually verified and refined to further improve quality and consistency. For each video shot, we prompt LLaMA3.1 [16] to generate three types of questions based on the action descriptions of the main characters: When, Where, and Combined. As result, LoomBench contains 130 videos, with an average of 4.2 temporal shots per video and an average shot length of 17.6 seconds. visualization example is shown in Fig. 3. When/Where questions respectively target the action timestamps and the person masks of each segment, focusing on the evaluation of temporal understanding and spatial perception. Following existing bench5 Figure 3 Visualization of the QA pairs in LoomBench. Three types of QA are shown: When targets the action timestamps given query and the whole video, Where targets the person masklet given query and certain video segment, while Combined directly targets the tracklet segment corresponding to the query. marks [14, 29], we adopt R1@0.5 and temporal IoU (tIoU) as evaluation metrics for When questions. For Where questions, we use the &F metric [10, 51], which averages region similarity (J ) and contour accuracy (F). LoomBench contains 541 When and 487 Where questions. Combined questions such as Where is the person when he/she is doing something? extend beyond the scope of existing datasets and enable more comprehensive evaluation of unified spatialtemporal understanding. We annotate 456 Combined questions in LoomBench. The standard &F metric computes the difference between predicted masklets and groundtruth across all video frames. However, for Combined questions, the duration of the queried tracklet constitutes only small fraction of the entire video (on average, 20.9%), making &F dominated by background frames without mask annotations. These backgrounds can inflate &F scores and undermine their reliability for evaluation. To address this issue, we propose Bidirectional Foreground &F, which computes &F within the temporal intervals of both the predicted and groundtruth foreground masks, and then takes their harmonic mean: &F bi-f ore = (Jp + Fp) (Jg + Fg) (Jp + Fp) + (Jg + Fg) (3) where Jp = JLoc(P )(P, G), Jg = JLoc(G)(P, G), Fp = FLoc(P )(P, G), and Fg = FLoc(G)(P, G). , denote the predicted and groundtruth masks, and the function Loc extracts the temporal span of masklet. Accordingly, Jp refers to the score computed over the temporal segment of predicted masklet, and so on. For more analysis on &F bi-f ore, please refer to Sec. D.1."
        },
        {
            "title": "4.1 Experimental Setup\nTraining data: Our training data can be categorized into four types: 1) image question answering (QA),\nwhich includes LLaVA-665k [39]. 2) image segmentation data, comprising standard referring expression\nsegmentation datasets [27, 66] and grounding conversation generation (GCG) data [48]. 3) video segmentation\ndata, including RefYTVOS [51], MeVIS [10], and ReVOS [62]. 4) video temporal instruction data, consisting of\nCharades-STA [14], YouCook2 [74], and QVHighlights [31]. The proposed LoomData-8.7k is converted into\nboth referring video object segmentation (VOS) and temporal grounding formats for joint training.",
            "content": "Implementation details: We choose InternVL3 [76] as our foundation MLLM and SAM2 [49] as the segmentation module. special token [SEG] is added for the mask generation following LISA [30]. The input frames are resized to 448448 and 10241024 for the MLLM and SAM2 visual encoders, respectively. The number of slow visual tokens per frame is set to 256, and the downsampling ratio is kept at 4, resulting 6 Method Charades YouCook2 QVHL Method R1@0.5 R1@0.7 F1 mAP HIT@1 MeVIS YTVOS ReVOS &F &F &F TimeChat-7B [50] VTG-LLM-7B [19] TRACE-7B [18] TimeSuite-7B [69] HawkEye-7B* [58] UniTime-7B* [36] 46.7 57.2 61.7 67. 58.3 75.3 23.7 33.4 41.4 43. 28.8 56.9 3.4 11.0 19.5 21.7 3.6 13.4 20.6 24.1 6.7 35.5 31.8 31.8 27.0 - - - - - - - - - - - 37.9 41.3 51.5 55. - - TrackGPT-7B [75] VISA-7B [62] ViLLa-6B [73] GLUS-7B [37] Sa2VA-8B [67] VRS-HQ-7B [15] VRS-HQ-13B [15] VideoLoom-8B 70.0 48. 7.3 41.5 33.6 27.5 63.3 VideoLoom-8B 40.1 43.5 49.4 51.3 46.9 50.6 50.9 51.7 56.4 61.5 67.5 67.3 70.7 70.4 71. 71.3 43.6 46.9 57.0 54.9 57.6 59.1 60.0 63.1 Table 1 Performance comparison on diverse temporal understanding benchmarks, * denotes models specifically designed for TVG. Table 2 Performance comparison on ref-VOS. in 16 fast tokens per frame. Up to 128 frames are uniformly sampled for fast tokens, while only 5 keyframes are encoded as slow tokens. We use the XTuner [9] codebase for training and evaluation, finetuning only the mask decoder and LLM module while keeping the visual encoder frozen. The LLM is adapted via LoRA [22], with learning rate of 4 105. The loss weights λtext and λmask are both set to 1. We train VideoLoom for one epoch with global batch size of 64. All experiments are facilitated on 8 NVIDIA H20 GPUs with 96 GB of memory."
        },
        {
            "title": "4.2 Main Results\nComparison on Temporal Benchmarks. We evaluate our model on a wide range of temporal tasks, including\ntemporal video grounding (TVG), dense video captioning (DVC), and video highlight detection (VHD), for a\ncomprehensive assessment of its temporal understanding capabilities.",
            "content": "The comparison with existing Video LLMs is reported in Tab. 1. VideoLoom achieves state-of-the-art or competitive performance across TVG, DVC, and VHD, e.g., 48.3 R1@0.7 on Charades-STA and 63.3 HIT@1 on QVHighlights, surpassing both unified models, e.g., TimeSuite [69], and task-specific models, e.g., HawkEye [58]. This highlights the strong temporal understanding capabilities of our method. Although VideoLoom lags behind UniTime [36] on Charades-STA, we attribute this to the much larger amount of grounding data used in their training and the complex inference procedure involving recursive localization. Comparison on Spatial Benchmarks. For spatial understanding in videos, we evaluate our method on referring Video Object Segmentation (VOS) task on RefYTVOS [51], MeVIS [10], and ReVOS [62]. &F is chosen as the metric. The results in Tab. 2 show that VideoLoom even outperforms tracking-oriented Video LLMs on all these benchmarks, achieving 51.7 on MeVIS, 71.3 on RefYTVOS, and 63.1 on ReVOS in terms of &F. This superior performance showcases the effectiveness of our method for fine-grained spatial understanding. Additionally, we also evaluate VideoLoom on image benchmarks, including RefCOCO [27], RefCOCO+ [27], and RefCOCOg [66] for referring segmentation, and Grand-f [48] for Grounded Conversation Generation (GCG). We adopt cIoU, AP50, and mIoU as the measurement metrics. The comparison results in Tab. 3 demonstrate that VideoLoom achieves the best results on all datasets, further demonstrating its strong spatial capabilities. Method RC cIoU RC+ cIoU RCg cIoU GCG AP50 mIoU VRS-HQ-7B [15] LISA-7B [30] OMG-LLaVA-7B [71] GLaMM-7B [48] Sa2VA-8B [67] VideoLoom-8B 73.5 74.9 78.0 79.5 81.6 83.4 61.7 65.1 69.1 72.6 76.2 79. 66.7 67.9 72.9 74.2 78.7 81.4 - - 29.9 30.8 31.0 34.1 - - 65.5 66.3 - 68. Table 3 Performance comparison on image segmentation benchmarks. 7 Comparison on LoomBench. Finally, we evaluate the joint spatial-temporal comprehension capability on the proposed LoomBench. For comparison, we design strong baseline that first adopts TimeSuite-7B [69] to localize the relevant clip in the given video, and then applies Sa2VA-8B [67] to segment the masks based on the user query (denoted as TimeSuite + Sa2VA). We incorporate tIoU and &F bi-f ore to evaluate Combined questions. As shown in Tab. 4, VideoLoom outperforms the above baseline by clear margin on Combined questions (+16.2 and +15.4 in terms of tIoU and &F bi-f ore). This not only validates the effectiveness of our model on this task, but also underscores the necessity of joint spatialtemporal understanding for comprehensive video comprehension. In addition, we also evaluate VideoLoom on When and Where questions, demonstrating robust performance in both temporal comprehension and spatial perception. tIoU &F tIoU &F bi-f ore - 27.6 - - 25.4 - TimeSuite-7B [69] Sa2VA-8B [67] TimeSuite+Sa2VA VideoLoom-8B Table 4 Performance comparison on LoomBench. - 86.1 - 23.1 - - - - 33.7 Combined Method Where When 87.2 37.9 41.6 49. 39.7 R"
        },
        {
            "title": "4.3 Ablation Studies",
            "content": "In this section, we conduct extensive ablation experiments using InternVL2.5-4B [7], lightweight MLLM, as our backbone to study the contribution of different components. Effects of SlowFast Visual Tokens. We build different variants to study the effects of SlowFast visual tokens: 1) using only slow tokens to train on spatial tasks, 2) using only fast tokens to train on temporal tasks, 3) using slow or fast tokens and train on both tasks jointly, 4) using fast tokens for temporal and slow tokens for spatial tasks, and 5) using both slow and fast tokens and train on both tasks. Results of all configurations are compared in Tab. 5. Setting Charades YouCook2 QVHL MeVIS YTVOS ReVOS R1@0.5 R1@0.7 mIoU F1 mAP mIoU &F Spatial (Slow) Temporal (Fast) Joint (Slow) Joint (Fast) Joint (Slow/Fast) Joint (SlowFast) - 66.1 38.8 63.3 62. 66.2 - 41.4 17.7 39.0 39.0 43.0 - 55.8 38.6 54.3 54.0 56.5 - 6.6 0.8 6.5 6. 7.0 - 30.3 4.8 28.6 26.4 30.3 - 26.8 19.1 26.2 24.2 25.8 - 52.4 42.2 54.8 47. 57.2 46.8 - 47.4 44.6 47.6 50.0 &F &F Ref. &F Rea. &F 59.5 69.1 - - 58.9 68.7 56.8 66.2 58.8 68.9 56.7 - 56.0 53.6 56.0 62.3 - 61.8 60.0 61. 70.0 62.5 57.6 60.0 Table 5 Ablation experiments on SlowFast visual tokens. Using either slow or fast tokens alone leads to substantial performance degradation on spatial or temporal tasks, respectively. The joint (Slow/Fast) setting, which assigns fast tokens for temporal and slow tokens for spatial, yields more balanced results across all datasets, though still with noticeable drop compared to the specialized single-task models. When SlowFast tokens are employed, the model achieves consistent improvements across nearly all benchmarks, surpassing standalone spatial or temporal models by 4.8 mIoU on QVHighlights and 3.2 &F on MeVIS. This demonstrates that the proposed SlowFast token design effectively unifies both tasks and enables coherent spatialtemporal understanding within single framework. Effects of LoomData-8.7K. Tab. 6 demonstrates the effectiveness of LoomData-8.7K in improving spatialtemporal understanding. We use VideoLoom trained on existing spatial and temporal datasets as the baseline. To eliminate the influence of additional VQA data, we include them only for fair comparison. The results show that with LoomData-8.7K, our model achieves an improvement of +5.0 &F bi-f ore in joint spatialtemporal understanding, along with consistent gains across all benchmarks, including spatial, temporal, and general visual comprehension (VideoMME [13], MME [12], MMBench [41], and SEED-Bench [32]). These 8 Dataset TVG VHD YTVOS ReVOS VMME MME MMBench SEED R1@0.5 mIoU mAP &F &F Baseline +VQA +LoomData 66.2 66.3 67.8 56.5 56. 57.4 25.8 26.0 26.3 70.0 70.3 70.3 60.0 59. 60.6 Acc 50.7 54.2 54.7 P./R. 492/115 1684/623 1699/ Acc 79.0 80.9 81.1 Acc 73.9 74.7 75. LoomBench tIoU &F bi-f ore 28.1 29.8 34.6 36.9 34.8 41.9 Table 6 Ablation experiments on Training data. results demonstrate that LoomData-8.7K could provide high-quality supervision for joint spatialtemporal understanding, with consistent spatial trajectories and temporal annotations. Effects of Base Models. To evaluate the impact of different base models on spatialtemporal understanding, we conduct experiments using various MLLMs as backbones. As shown in Tab. 7, VideoLoom achieves higher performance with InternVL2.5-8B [7] compared to its smaller InternVL2.5-4B counterpart, indicating that larger languagevision models provide stronger multimodal representations for spatialtemporal reasoning. When equipped with the more advanced InternVL3-8B [76], further improvements are observed under comparable model capacities. These results demonstrate that VideoLoom continues to benefit from advancements in underlying MLLMs, showing strong scalability and the potential for even better spatialtemporal understanding as foundation models evolve. TVG VHD ReVOS mIoU mAP &F tIoU &F bi-f ore 57.4 56.4 59.8 InternVL2.5-4B [7] InternVL2.5-8B [7] InternVL3-8B [76] Table 7 Ablation experiments on Model size and type. 60.6 62.0 63.1 41.9 47.2 49. 34.8 40.2 41.6 26.3 27.1 27.5 LoomBench Backbone"
        },
        {
            "title": "4.4 Visualizations",
            "content": "We present qualitative visualizations of VideoLoom across multiple spatialtemporal understanding datasets in Fig. 4. The first row illustrates that our model accurately localizes events along the temporal dimension, demonstrating its superior temporal modeling. The following two rows show its capability to perform object segmentation conditioned on diverse types of textual references (e.g., concise descriptions, reasoning-based queries). Figure 4 Visualization of the predictions by VideoLoom on different spatial-temporal understanding tasks. From top to down, we show the visualization results of video temporal grounding on Charades-STA [14], referring VOS on MeVIS [10], and reasoning VOS on ReVOS [62]. Additionally, Fig. 5 provides qualitative examples across the three question types from LoomBench, further illustrating the strong joint spatialtemporal understanding capability of VideoLoom. For instance, in the 9 query Where is the person in dark clothing when he throws the pink frisbee into the air, and the dog leaps to catch it, VideoLoom first localizes the relevant temporal segment corresponding to the throwing action and then accurately identifies the spatial region of the person within that interval. This example demonstrates its ability to reason across both time and space, linking dynamic actions to precise spatial localization within unified framework. Figure 5 Visualization of VideoLoom on LoomBench for When, Where, and Combined questions."
        },
        {
            "title": "5 Conclusion",
            "content": "This work presents the VideoLoom suite to advance joint spatial-temporal understanding. It comprises three key components: 1) LoomData-8.7k, human-centric dataset that provides both timestamp-aligned action descriptions and fine-grained spatial masks. 2) VideoLoom, unified Video LLM equipped with MLLM-SAM2 architecture to generate both temporal locations and spatial masks. and 3) LoomBench, novel benchmark designed to evaluate Video LLMs across diverse question types, When, Where, and Combined, for comprehensive assessment of spatial-temporal understanding. Extensive experiments on range of spatial and temporal benchmarks demonstrate that VideoLoom achieves strong performance and establishes new state-of-the-art results across multiple tasks. While already significantly reducing manual effort and enabling scalable annotation, the proposed annotation pipeline still involves multiple stages with interdependent components. In the future, we plan to further automate this process by integrating stronger multimodal foundation models and agents for both annotation generation and verification, aiming to further improve the efficiency and reliability."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shĳie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shĳie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Zheng Zhang, and Mike Zheng Shou. One token to seg them all: Language instructed reasoning segmentation in videos. NeurIPS, 2024. [4] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In CVPR, 2015. [5] Brandon Castellano. Pyscenedetect: Automated video scene detection. https://github.com/Breakthrough/ PySceneDetect, 2022. [6] Yitong Chen, Lingchen Meng, Wujian Peng, Zuxuan Wu, and Yu-Gang Jiang. Comp: Continual multimodal pre-training for vision foundation models. arXiv preprint arXiv:2503.18931, 2025. [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [8] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [9] XTuner Contributors. Xtuner: toolkit for efficiently fine-tuning llm. https://github.com/InternLM/xtuner, 2023. [10] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. Mevis: large-scale benchmark for video segmentation with motion expressions. In ICCV, 2023. [11] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sĳia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling. Lasot: high-quality benchmark for large-scale single object tracking. In CVPR, 2019. [12] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. In NeurIPS, 2025. [13] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In CVPR, 2025. [14] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In ICCV, 2017. [15] Sitong Gong, Yunzhi Zhuge, Lu Zhang, Zongxin Yang, Pingping Zhang, and Huchuan Lu. The devil is in temporal token: High quality video reasoning segmentation. In CVPR, 2025. [16] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [17] Chunhui Gu, Chen Sun, David Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vĳayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, et al. Ava: video dataset of spatio-temporally localized atomic visual actions. In CVPR, 2018. [18] Yongxin Guo, Jingyu Liu, Mingda Li, Qingbin Liu, Xi Chen, and Xiaoying Tang. Trace: Temporal grounding video llm via causal event modeling. arXiv preprint arXiv:2410.05643, 2024. [19] Yongxin Guo, Jingyu Liu, Mingda Li, Dingxin Cheng, Xiaoying Tang, Dianbo Sui, Qingbin Liu, Xi Chen, and Kevin Zhao. Vtg-llm: Integrating timestamp knowledge into video llms for enhanced video temporal grounding. In AAAI, 2025. [20] Songhao Han, Wei Huang, Hairong Shi, Le Zhuo, Xiu Su, Shifeng Zhang, Xu Zhou, Xiaojuan Qi, Yue Liao, and Si Liu. Videoespresso: large-scale chain-of-thought dataset for fine-grained video reasoning via core frame selection. In CVPR, 2025. [21] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding. In CVPR, 2024. [22] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 2022. [23] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. In CVPR, 2024. [24] De-An Huang, Shĳia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. Lita: Language instructed temporal-localization assistant. In ECCV, 2024. [25] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: large high-diversity benchmark for generic object tracking in the wild. TPAMI, 2019. [26] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [27] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In EMNLP, 2014. [28] Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video object segmentation with language referring expressions. In ACCV, 2019. [29] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In ICCV, 2017. [30] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In CVPR, 2024. [31] Jie Lei, Tamara Berg, and Mohit Bansal. Detecting moments and highlights in videos via natural language queries. In NeurIPS, 2021. [32] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In CVPR, 2024. [33] Hongyu Li, Jinyu Chen, Ziyu Wei, Shaofei Huang, Tianrui Hui, Jialin Gao, Xiaoming Wei, and Si Liu. Llava-st: multimodal large language model for fine-grained spatial-temporal understanding. In CVPR, 2025. [34] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. [35] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In ECCV, 2024. [36] Zeqian Li, Shangzhe Di, Zhonghua Zhai, Weilin Huang, Yanfeng Wang, and Weidi Xie. Universal video temporal grounding with generative multi-modal large language models. NeurIPS, 2025. [37] Lang Lin, Xueyang Yu, Ziqi Pang, and Yu-Xiong Wang. Glus: Global-local reasoning unified into single large language model for video segmentation. In CVPR, 2025. [38] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. [39] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. [40] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In ECCV, 2024. [41] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2024. [42] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. [43] Lingchen Meng, Jianwei Yang, Rui Tian, Xiyang Dai, Zuxuan Wu, Jianfeng Gao, and Yu-Gang Jiang. Deepstack: Deeply stacking visual tokens is surprisingly simple and effective for lmms. NeurIPS, 2024. [44] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 3DV, 2016. 12 [45] Matthias Muller, Adel Bibi, Silvio Giancola, Salman Alsubaihi, and Bernard Ghanem. Trackingnet: large-scale dataset and benchmark for object tracking in the wild. In ECCV, 2018. [46] Wujian Peng, Lingchen Meng, Yitong Chen, Yiweng Xie, Yang Liu, Tao Gui, Hang Xu, Xipeng Qiu, Zuxuan Wu, and Yu-Gang Jiang. Inst-it: Boosting multimodal instance understanding via explicit visual prompt instruction tuning. In NeurIPS, 2025. [47] Danila Potapov, Matthĳs Douze, Zaid Harchaoui, and Cordelia Schmid. Category-specific video summarization. In ECCV, 2014. [48] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In CVPR, 2024. [49] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [50] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In CVPR, 2024. [51] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation network with large-scale benchmark. In ECCV, 2020. [52] Nina Shvetsova, Anna Kukleva, Xudong Hong, Christian Rupprecht, Bernt Schiele, and Hilde Kuehne. Howtocaption: Prompting llms to transform video annotations at scale. In ECCV, 2024. [53] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [54] Jiankang Wang, Zhihan Zhang, Zhihang Liu, Yang Li, Jiannan Ge, Hongtao Xie, and Yongdong Zhang. Spacevllm: Endowing multimodal large language model with spatio-temporal video grounding capability. arXiv preprint arXiv:2503.13983, 2025. [55] Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, Zuxuan Wu, and Yu-Gang Jiang. Chatvideo: tracklet-centric multimodal and versatile video understanding system. arXiv preprint arXiv:2304.14407, 2023. [56] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023. [57] Junke Wang, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, and Yu-Gang Jiang. Omnivid: generative framework for universal video understanding. In CVPR, 2024. [58] Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, and Dongyan Zhao. Hawkeye: Training video-text llms for grounding text in videos. arXiv preprint arXiv:2403.10228, 2024. [59] Yongliang Wu, Xinting Hu, Yuyang Sun, Yizhou Zhou, Wenbo Zhu, Fengyun Rao, Bernt Schiele, and Xu Yang. Number it: Temporal grounding videos like flipping manga. In CVPR, 2025. [60] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Dehghan. Slowfast-llava: strong training-free baseline for video large language models. arXiv preprint arXiv:2407.15841, 2024. [61] Yifan Xu, Xinhao Li, Yichun Yang, Desen Meng, Rui Huang, and Limin Wang. Carebench: fine-grained benchmark for video captioning and retrieval. arXiv preprint arXiv:2501.00513, 2024. [62] Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios Gavves. Visa: Reasoning video object segmentation via large language models. In ECCV, 2024. [63] Cheng-Yen Yang, Hsiang-Wei Huang, Wenhao Chai, Zhongyu Jiang, and Jenq-Neng Hwang. Samurai: Adapting segment anything model for zero-shot visual tracking with motion-aware memory. arXiv preprint arXiv:2411.11922, 2024. 13 [64] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. [65] Zuyao You, Junke Wang, Lingyu Kong, Bo He, and Zuxuan Wu. Pix2cap-coco: Advancing visual comprehension via pixel-level captioning. arXiv preprint arXiv:2501.13893, 2025. [66] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In ECCV, 2016. [67] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. arXiv preprint arXiv:2501.04001, 2025. [68] Yuqian Yuan, Hang Zhang, Wentong Li, Zesen Cheng, Boqiang Zhang, Long Li, Xin Li, Deli Zhao, Wenqiao Zhang, Yueting Zhuang, et al. Videorefer suite: Advancing spatial-temporal object understanding with video llm. In CVPR, 2025. [69] Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, et al. Timesuite: Improving mllms for long video understanding via grounded tuning. arXiv preprint arXiv:2410.19702, 2024. [70] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. [71] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, and Shuicheng Yan. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding. NeurIPS, 2024. [72] Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng Liu, and Lianli Gao. Where does it exist: Spatio-temporal video grounding for multi-form sentences. In CVPR, 2020. [73] Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, and Hengshuang Zhao. Villa: Video reasoning segmentation with large language model. In ICCV, 2025. [74] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In AAAI, 2018. [75] Jiawen Zhu, Zhi-Qi Cheng, Jun-Yan He, Chenyang Li, Bin Luo, Huchuan Lu, Yifeng Geng, and Xuansong Xie. Tracking with human-intent reasoning. arXiv preprint arXiv:2312.17448, 2023. [76] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weĳie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "Appendix",
            "content": "A Overview Our supplementary includes the following sections: Sec. B: Model details. Details for VideoLoom design, implementation and training data. Sec. C: LoomData details. Details for manual verification, and statistics for LoomData-8.7k. Sec. D: More experiment results. Analysis on Bidirectional Foreground &F, and additional performance evaluation. Sec. E: More visualization. More visualization of our dataset and results. Sec. F: Prompt design. Prompt for temporal action annotation and LoomBench construction."
        },
        {
            "title": "B Model Details",
            "content": "B.1 More Details about VideoLoom Interleaved Input. For temporal modeling, we interleave temporal information, i.e., unique frame IDs, with fast visual tokens. Specifically, we insert frame IDs, e.g., \"This sampled frame id is 26\", after the fast tokens of the corresponding frames, leading to an interleaved sequence. We then concatenate this token sequence with the slow tokens as input to the LLM: = [F1; ID1; ...; FNf ; IDNf ; S1; ...; SNs]. (4) where IDj, Fj, Sk denote the ID text tokens, fast tokens, and slow tokens, while Nf and Ns for the count of frames with fast and slow tokens. By directly using numerical text of frame IDs to represent temporal positions, temporal understanding is transformed into language instruction QA, aligning with the general capabilities of MLLMs. [SEG] token. To generate masks for keyframes, SAM2 [49] only needs to activate visual encoder and mask decoder. Given the keyframes sampled for slow tokens, we extract visual features fv using the visual encoder, which provides pixel-level details for trajectory prediction. The SAM2 mask decoder is connected to MLLM via [SEG] token contained in the text output. Since MLLM performs fine-grained spatial-temporal modeling with SlowFast tokens, the [SEG] token captures rich target information under segmentation queries. The hidden states of the [SEG] token, denoted as hseg, pass through an MLP projection layer to form target embedding. This embedding serves as novel visual prompt for SAM2, fed into the mask decoder with the visual features fv to generate masks Mv for the keyframes: Mv = SAM2(fv, MLP(hseg)). (5) B.2 Additional Implemental Details Tab. 8 lists hyperparameters for one-stage tuning. Specifically, for the number of frames for fast tokens, we adopt different settings across datasets based on video duration, with maximum of 128 frames. For Charades-STA [14], where videos typically last around 30 seconds, we sample 64 frames for fast tokens. For YouCook2 [74], where videos often exceed 2 minutes in length, we uniformly sample 128 frames. For QVHighlights [31], annotated in 2-second intervals, we sample frames at 2 FPS, typically yielding around 75 frames. For spatial datasets [10, 51, 62], which provide annotated frame sequences, we uniformly sample up to 64 frames. 15 Hyperparameter Epochs Batch size Learning rate Weight decay AdamW β Max sequence length for MLLM Number of fast tokens per frame Number of slow tokens per frame Frame resolution for MLLM Frame resolution for SAM2 Value 1 4e-5 0.05 (0.9, 0.999) 8192 16 256 448 448 1024 Number of frames for fast tokens Number of frames for slow tokens 128 5 Dataset LLaVA [39] RefCOCO [27] RefCOCO+ [27] RefCOCOg [66] Grand-f [48] (Auto Annotated) Grand-f [48] (Human Annotated) Charades-STA [14] YouCook2 [74] QVHighlights [31] LoomData for VTG Ref-YTVOS [51] MeVIS [10] ReVOS [62] LoomData for refVOS Item count Repeats 665K 17K 17K 17K 196K 1K 12.4K 1.2K 6.9K 8.7K 3.5K 1.6K 1.7K 8.7K 1 4 4 4 1 4 10 4 4 12 12 12 4 Table 8 Hyperparameters for one-stage tuning. Table 9 Training datasets, item counts, and repeat times. B.3 Training Data We present all the datasets for training and report their item counts and repeat times in Tab. 9. Finally, VideoLoom is jointly trained for 1,315K iterations and achieves advanced performance on all these tasks."
        },
        {
            "title": "C LoomData Details",
            "content": "C.1 Details about Manual Verification Here we introduce the simple manual verification process in the pipeline, explaining how to implement filtering and correction of complete tracklets after spatial mask annotation. This approach involves two rounds of simple judgments to minimize manual involvement: In the first round, we primarily focus on filtering out videos with missing annotations, as completing the missing tracklets requires extensive manual annotation, which is not scalable. Specifically, we display the annotation on the middle frame of the longest shot (i.e., the key frame where we initially identify the main character) as reference, and then display the middle frames of the shots without tracklets in turn. We then manually determine whether there is an unlabeled main character in these frames, discarding the video if one is found, as shown in Fig. 6. (i). Figure 6 Examples of manual verification. (i) In the first round, we filter out videos with missing annotations. (ii) In the second round, we filter out videos with incorrect annotations and remove redundant annotations from the shots of the retained videos. The second round of verification focuses on the shots with tracklets, where we filter out videos with incorrect annotations and remove redundant annotations from the retained shots. For shot with tracklets, we define incorrect annotations as the presence of the main character but the mask labeled to other objects, and 16 redundant annotations as the absence of the main character but the mask labeled to other objects. We discard entire videos containing incorrectly labeled shots and remove annotations from redundantly labeled shots to make simple revision of the video, as shown in Fig. 6. (ii). Specifically, we continue to display the annotation of the middle frame of the longest shot for reference purposes and display the middle frames of the shots with tracklets in turn. We then manually determine whether the annotations on these frames are incorrect, redundant, or correct to carry out the corresponding operations. C.2 Statistics for LoomData-8.7k Tab. 10 compares our constructed dataset, LoomData-8.7k, with existing spatial-temporal datasets. For the first time, LoomData achieves joint annotation of temporal timestamps and spatial masks on nearly 2-minute videos. LoomData enables fine-grained temporal partition, with each video containing an average of 6.0 segments with tracklets, comparable to current spatial-temporal datasets. Compared to temporal datasets, which only roughly label overlapping temporal locations, LoomData achieves complete temporal partition of the videos while providing more detailed description. Compared to spatial datasets, LoomData achieves mask-level annotation while ensuring instance consistency across the entire video. Fig. 7 shows the distribution of shot lengths and normalized shot center timestamps (by video duration). LoomData contains shots of widely varying lengths. Over 50% of shots are concentrated in the range from 5 to 15 seconds, while only few exceed 30 seconds. These shots are almost evenly distributed across the videos, suggesting that LoomData suffers less from temporal bias. Dataset #Videos Avg #Segments Avg #Tracklets Avg Len (sec) Segment/Video Temporal Ann. Box Ann. Mask Ann. Charades-STA [14] ANet Captions [29] RefYTVOS [51] MeVIS [10] VidSTG [72] LoomData 5,338 10,024 3,471 1,662 5,563 1, 6.8 3.7 - - 6.5 6.0 - - 1.9 4.3 5.0 6.0 8.1/30.6 36.2/117.6 - - 9.7/28.0 15.0/102. Table 10 Comparison with existing spatial-temporal datasets. Figure 7 Distribution of shot lengths (left) and normalized (by video duration) center timestamps (right). The shots vary widely in length, and they distribute almost evenly along the videos."
        },
        {
            "title": "D More Experiment Results",
            "content": "D.1 Analysis on Bidirectional Foreground J&F We propose new evaluation metric, Bidirectional Foreground &F, for assessing the joint spatial-temporal understanding of Combined questions on LoomBench. In this section, we first demonstrate the necessity with experimental results under varying queried segment lengths. We then present the specific values of each component of this metric to provide an in-depth assessment. 17 The Necessity of &F bi-f ore. The Combined questions are divided by the percentage of length of the queried segment over the entire video, into three categories: 0-20%, 20-60%, and 60-100%. We then provide comparative analysis of the standard &F and our proposed &F bi-f ore in Tab. 11. Metric 0-20% 20-60% 60-100% Standard &F &F bi-f ore 88.9 47.6 77.7 50.8 41.0 37. All 83.3 49.1 Table 11 Comparison between standard &F and &F bi-f ore on Combined questions of LoomBench, under varying queried segment lengths. We can see that &F bi-f ore performs stably under variable-length queried segments, while the standard &F increases significantly with shorter lengths, resulting in substantial gap between 0-20% and 60-100%. However, VideoLoom does not demonstrate superiority in short segments. On the contrary, it is significantly more challenging to perform spatial-temporal localization for short segments over the whole video. This is due to the calculation of &F. For segments without masklets, i.e., background segments, when the predicted mask is None, the value of &F reaches 1 (100%). When computed over the entire video, &F is significantly influenced by the easily predicted background segments, leading to inflated values and excessive sensitivity to the proportion of foreground queries, which prevents correct assessment of spatial-temporal capabilities. Referring VOS [10, 51, 62] adopts standard &F as evaluation metrics because videos in existing datasets are often foreground throughout (up to 60% or more, as indicated by Tab. 11 showing close values for the two metrics on segments with length of 60-100%), which is notably different from LoomBench. To effectively evaluate performance on LoomBench, we utilize the Bidirectional Foreground &F metric, thereby avoiding extensive computation on background segments and ensuring accurate assessment for spatial-temporal comprehension. In-depth Comparison of Components. We present the specific values of each component of &F bi-f ore in Tab. 12, including Jp, Fp, &F computed over the predicted masklet, and Jg, Fg, &F computed over the groundtruth. The experimental results demonstrate that VideoLoom outperforms the baseline, which consists of TimeSuite [69] and Sa2VA [67], across all metrics. Additionally, it is evident that the metric scores computed over the predicted masklet are higher than those computed over the groundtruth, highlighting the superior precision of the model predictions, though notable gap remains in recall. Method TimeSuite+Sa2VA VideoLoom-8B Jp 47.0 58.1 Fp 48.9 60. &F 48.0 59.3 Jg 25.4 41.1 Fg 26.6 42. &F &F bi-f ore 26.0 41.9 33.7 49.1 Table 12 Detailed results of VideoLoom on Combined questions of LoomBench. D.2 Ablation on Non-Human Categories To demonstrate the generalizability of VideoLoom on human and non-human categories, we conduct ablation experiments on RefDavis17 [28], benchmark for referring VOS, in zero-shot setting. We divide the classes of objects and report the results separately in Tab. 13. With or without LoomData, the performance of segmentation on human class surpasses that of the nonhuman classes. Moreover, incorporating our constructed LoomData leads to notable enhancement in the segmentation of the human class (+2.3 &F), while also benefiting the segmentation of non-human classes (+2.3 &F). This suggests that, although our data primarily targets the human class, detailed textual descriptions contribute to the comprehension of semantics across various categories. Consequently, VideoLoom demonstrates the ability to generalize to any category and greatly benefits from the human class annotations provided by LoomData. 18 Method w/o LoomData Ours 73.0 75.4 Human 82.0 84.3 &F 77.5 79.8 63.1 65.7 Non-Human 72.3 74.3 &F 67.7 70.0 67.5 70.0 All 76.6 78.7 &F 72.1 74.3 Table 13 Ablation experiments on Human and Non-human categories of RefDavis17 [28]. D.3 Detailed Comparison with Sa2VA We conduct fair comparison with Sa2VA [67], the model most closely aligned with our approach. Following Sa2VA, we employ InternVL2.5-4B [7] as the MLLM backbone to report our results in Table 14. We can see that VideoLoom significantly surpasses Sa2VA on MeVIS [10] and also achieves competitive performance on RefYTVOS [51], highlighting its superior motion capture and reasoning capabilities. Sa2VA [67] InternVL2.5-4B VideoLoom InternVL2.5-4B Table 14 Comparison with Sa2VA [67] using the same backbone. MeVIS_u MeVIS Backbone 55.9 60.9 46.4 50.6 71.3 70.3 Method YTVOS &F &F &F"
        },
        {
            "title": "E More Visualization",
            "content": "E.1 Visualization of Full Annotation To visualize the annotation results of our pipeline, we present an example of the complete spatial-temporal annotation for randomly selected video in Fig. 8. This annotation fully captures the timestamp-aligned actions and mask-level locations of the main characters. Figure 8 An example of the complete spatial-temporal annotation of video. E.2 Qualitative Results and Failure Cases We present additional qualitative results of VideoLoom across multiple spatial-temporal tasks. As illustrated in Fig. 9, VideoLoom can follow diverse spatial-temporal instructions and establish solid baseline across different tasks. However, in complex joint understanding scenarios (e.g., when querying sub-actions or the n-th occurrence), it occasionally generates inaccurate spatial-temporal locations, as shown in Fig. 10. This issue likely arises from limitations in temporal action grounding. When confronted with lengthy queries, the model struggles to identify complete temporal intervals spanning the entire motion sequence, which may lead to misaligned spatial-temporal localization. We plan to explore this issue further in future work. Figure 9 Additional qualitative results of VideoLoom on diverse spatial-temporal tasks. 20 Figure 10 Failure cases of VideoLoom on LoomBench, e.g., when querying sub-actions or the n-th occurrence."
        },
        {
            "title": "F Prompt Design",
            "content": "F.1 Prompt for Temporal Action Annotation Prompt engineering plays vital role in guiding Gemini2.5pro [8] to generate detailed and specific action descriptions aligned with frame IDs for video shots. The prompt utilized is illustrated in Fig. 11. To ensure clarity and precision, we first outline the task of generating instance-level descriptions of actions and appearances using visual prompts from SoM [64] and NumPro [59]. The former annotates instance-level IDs on the main character, while the latter sequentially labels unique frame IDs on each frame. Next, we provide series of instructions, including Frame Range Division, Description Content, Writing Style, and Output Format. These guidelines ensure concise, distinct, and formatted output with complete temporal coverage, avoiding irrelevant descriptions. Finally, we provide an example output and specify the number of sampled frames for the shot, ensuring alignment between the descriptions and the frame IDs. As result, Gemini2.5pro generates clear and accurate instance-level descriptions of the main character based on the visual content of the current shot, under the guidance of our carefully designed prompt. F.2 Prompt for LoomBench Construction We prompt LLaMA3.1 [16] to generate When, Where, and Combined questions based on annotations produced by our pipeline, and we show the prompt in Fig. 12. We first define the task to create detailed and context-aware questions from video shot descriptions explicitly. Next, we specify the requirements for each of the three question types, emphasizing that timestamps should not appear in Combined or When questions. Finally, we present concrete example to clarify the form of the questions further. Based on each shot description, LLaMA3.1 subsequently generates three categories of questions, both detailed and context-aware, to construct the LoomBench. 21 Figure 11 Instruction format for guiding Gemini2.5pro [8] to generate detailed and distinct action descriptions, the italicized part are placeholders for the text inputs. Figure 12 Instruction format for guiding LLaMA3.1 [16] to generate three types of questions to construct LoomBench, the italicized part are placeholders for the text inputs."
        }
    ],
    "affiliations": [
        "Fudan University",
        "University of Maryland, College Park"
    ]
}