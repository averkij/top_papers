{
    "paper_title": "LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation",
    "authors": [
        "Donald Shenaj",
        "Ondrej Bohdal",
        "Mete Ozay",
        "Pietro Zanuttigh",
        "Umberto Michieli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in image generation models have enabled personalized image creation with both user-defined subjects (content) and styles. Prior works achieved personalization by merging corresponding low-rank adaptation parameters (LoRAs) through optimization-based methods, which are computationally demanding and unsuitable for real-time use on resource-constrained devices like smartphones. To address this, we introduce LoRA$.$rar, a method that not only improves image quality but also achieves a remarkable speedup of over $4000\\times$ in the merging process. LoRA$.$rar pre-trains a hypernetwork on a diverse set of content-style LoRA pairs, learning an efficient merging strategy that generalizes to new, unseen content-style pairs, enabling fast, high-quality personalization. Moreover, we identify limitations in existing evaluation metrics for content-style quality and propose a new protocol using multimodal large language models (MLLM) for more accurate assessment. Our method significantly outperforms the current state of the art in both content and style fidelity, as validated by MLLM assessments and human evaluations."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 ] . [ 1 8 4 1 5 0 . 2 1 4 2 : r LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation Donald Shenaj,,* Ondrej Bohdal Mete Ozay Samsung R&D Institute (SRUK) Pietro Zanuttigh University of Padova Umberto Michieli"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in image generation models have enabled personalized image creation with both user-defined subjects (content) and styles. Prior works achieved personalization by merging corresponding low-rank adaptation parameters (LoRAs) through optimization-based methods, which are computationally demanding and unsuitable for real-time use on resource-constrained devices like smartphones. To address this, we introduce LoRA.rar, method that not only improves image quality but also achieves remarkable speedup of over 4000 in the merging process. LoRA.rar pre-trains hypernetwork on diverse set of content-style LoRA pairs, learning an efficient merging strategy that generalizes to new, unseen content-style pairs, enabling fast, high-quality personalization. Moreover, we identify limitations in existing evaluation metrics for contentstyle quality and propose new protocol using multimodal large language models (MLLM) for more accurate assessment. Our method significantly outperforms the current state of the art in both content and style fidelity, as validated by MLLM assessments and human evaluations. Figure 1. LoRA.rar employs hypernetwork to predict zero-shot merging coefficients for content and style LoRAs, enabling highquality, real-time merging. Additionally, we introduce an MLLM judge to evaluate content-style fidelity. 1. Introduction The advent of text-to-image generation models based on denoising diffusion [19] allowed for significant improvements in output quality. Furthermore, recently there has been growing interest in personalized image generation [34, 36], where users can generate images that depict particular subjects or styles by providing just few reference images. key enabler of this personalization breakthrough is Low-Rank Adaptation (LoRA) [20], parameter-efficient method that achieves high-quality personalization using only few training samples. This innovation has spurred extensive model sharing on open-source platforms like Civitai [7] and Hugging Face [21], making pre-trained LoRAs readily available. The accessibility of these models has fueled interest in combining them to create images of personal subjects *Research completed during internship at Samsung R&D Institute UK in various styles. For instance, user might apply concept (i.e., subject) LoRA trained on few photos of their pet, and combine it with downloaded style LoRA to render their pet in an artistic style of their choice. Averaging the LoRA weights can work acceptably when the subject and style share significant visual characteristics, but typically requires fine-tuning of the merging coefficients (i.e., the coefficients used to combine LoRAs) for more distinct subjects and styles. ZipLoRA [36] introduces an approach that directly optimizes merging coefficients through customized objective function tailored to each subject-style LoRA combination. However, ZipLoRAs reliance on optimization for each new combination incurs substantial computational cost, typically taking minutes to complete. This limitation restricts its practicality for real-time applications on resource-constrained devices like smartphones. Achieving comparable or superior quality with respect to 1 ZipLoRA while enabling real-time merging (i.e., in under second) would make such technology far more accessible for deployment on lightweight devices. In this paper, we introduce method named LoRA.rar that trains hypernetwork to learn merging coefficients for arbitrary subject and style LoRAs. Our hypernetwork is pretrained on curated dataset of LoRAs. During deployment, it generalizes to unseen subject-style combinations, generating merging coefficients instantly via single forward pass and thus removing the need for retraining. In cases where users prefer to input images rather than LoRAs, image-toLoRA encoding methods like DiffLoRA [42] can transform reference images into LoRAs. An overview of our approach and examples of generated images are shown in Fig. 1. Our main contributions are as follows: 1. We propose LoRA.rar, which pre-trains small 0.5Mparameters hypernetwork to predict merging coefficients for arbitrary subject and style LoRAs. 2. LoRA.rar offers fast and lightweight solution for merging subject-style LoRAs to generate images of any subject in any style. It generalizes seamlessly to unseen subject-style combinations at test time without requiring fine-tuning, unlike ZipLoRA. 3. We analyze the limitations of existing metrics (CLIP-I, CLIP-T, DINO) in assessing the fidelity of joint subjectstyle generation. To address this, we introduce MARS2, new metric based on Multimodal Large Language Models (MLLMs), which aligns closely with user preferences and enables scalability of quantitative studies. 4. We show that LoRA.rar consistently outperforms existing merging strategies and ZipLoRA in subject-style personalization. 2. Related Work Subject-Conditioned Image Generation has been extensively explored in recent years. DreamBooth [34] fine-tunes the entire generative model on reference images for subject fidelity. Several techniques have been proposed to mitigate extensive training. Textual Inversion [11] optimizes token embeddings for subject encoding, with extensions such as [1, 18, 38, 39, 48] enhancing flexibility. However, these methods face limitations in scaling to multiple concepts. Another line of work optimize specific network parts or employing specialized tuning, such as CustomDiffusion [25], LoRA [8, 20, 42], SVDiff [17], and DreamArtist [9]. In particular, LoRA has gained popularity for its training efficiency and adaptability to multiple concepts, making it widely used approach for subject conditioning. More recently, techniques for zero-shot personalization aim to avoid fine-tuning by either (i) using separate conditioning encoders (encoder-based approaches) [6, 12, 26 29, 40, 50, 55, 56]; or (ii) utilizing features from the generative models backbone to guide generation (encoder-free methods) [32, 53]. However, these methods often require extensive additional storage, limiting their applicability in resource-constrained environments. Subjectand Style-Conditioned Image Generation. Beyond subject conditioning, many works tackle styleconditioned generation, such as StyleGAN [23], StyleDrop [37] and DreamArtist [9]. However, these methods lack the ability to handle both subject and style conditioning jointly. Recent approaches addressing this challenge include CustomDiffusion [25], which learns multiple concepts through expensive joint training but struggles to disentangle style from subject, and HyperDreamBooth [35], which generates personal subjects with good style editability via textual prompt. B-LoRA [10] proposes layer-wise LoRA tuning pipeline for either content or style. Notably, ZipLoRA [36], the closest reference for our work, merges pre-trained subject and style LoRAs via test-time optimization to discover the optimal merging coefficients. Concurrent work such as RB-Modulation [33] uses style descriptor for modulation without LoRAs. FreeTuner [45] and Break-for-Make [44] further explore style disentanglement with separate content and style encoders or training subspaces. Our method focuses on efficient zero-shot merging of subject and style LoRAs aiming for high-quality subject preservation while preserving text editability. Model Merging is an increasingly popular way to enhance the abilities of foundational models in both language [16] and vision domains [47]. The simplest technique is direct arithmetic merge that averages the weights of multiple finetuned models [41]. Despite its simplicity, it can improve performance and enable multi-tasking to at least certain extent [22]. Following [41], diverse strategies have been proposed. TIES [46] mitigates interference between parameters of different models due to different signs, while DARE [51] drops some parameters and rescales the remaining ones to reduce redundancy and interference. DARE-TIES [13] combines DARE and TIES, and demonstrates successful merging in complex scenarios. LoRA Merging for Image Generation has recently gained attention. Mix-of-Show [14] and LoRA-Composer [49] merge the concept of each LoRA in the output image for multi-concept generation (instead of subject-style). Further, they require custom version of LoRAs, hindering wide compatibility. ZipLoRA [36] merges standard LoRAs, focusing on subject-style generation through parameter optimization. However, this approach requires several minutes per merge at test time, limiting its usability in real-time scenarios. Our LoRA.rar builds upon ZipLoRAs foundations, targeting both more efficient solution and improved results. Hypernetworks, or networks generating the parameters of other networks [4, 15], have found diverse use cases. Hypernetworks are used to generate LoRA parameters in [3], while [2] uses them for model aggregation in federated learning. 2 Figure 2. Methods Overview. LoRA.rar pre-trains hypernetwork that dynamically generates merging coefficients for new, unseen content-style LoRA pairs at deployment. In contrast, existing solutions are limited by either costly test-time training, as with ZipLoRA, or produce lower-quality outputs, as with conventional merging strategies. In image generation, HyperDreamBooth [35] designs hypernetwork to efficiently generate model weights based on single reference image. In our work, we utilize hypernetwork to predict merging coefficients for subject-style LoRA fusion, enabling efficient, high-quality joint personalization without extensive optimization overhead. 3. Method Our objective is to design and train hypernetwork that predicts weighting coefficients to merge content and style LoRAs. Using set of LoRAs, we train this hypernetwork to produce suitable merging coefficients for unseen content and style LoRAs at the deployment stage. We start by formulating the problem in Sec. 3.1, detailing how LoRAs are applied to the base model and outlining the limitations of the current state-of-the-art approach. In Sec. 3.2, we describe the construction of the LoRA dataset used to train and evaluate our solution. Sec. 3.3 discusses the structural design of our hypernetwork, followed by an overview of the training procedure in Sec. 3.4. 3.1. Problem Formulation We use pre-trained image generation diffusion model with weights W0 and LoRA parameters with weight update matrix . For simplicity, we consider one layer at time. model that uses LoRA is denoted as DL = with weights W0 + , where operation Figure 3. ZipLoRAs Merging Coefficients mc, ms for randomly selected columns of the LoRA weight update matrices. The coefficients are visibly different for various combinations of content and style scenarios, showing the need for adaptive solutions. means we apply LoRA to the base model D. To specify content and style, we use LoRAs Lc (content) and Ls (style) with respective weight update matrix Wc and Ws. Our objective is to merge Lc and Ls into Lm, producing matrix Wm that combines content and style coherently in generated images. The merging operation can vary, from simple averaging to advanced techniques like ZipLoRAs and ours. learning column-wise merging coefficients mc and ms for Wc and Ws, respectively, as: Wm = mc Wc + ms Ws, where represents element-by-column multiplication. ZipLoRA takes gradient-based approach, Although ZipLoRA achieves high-quality results, it requires training these coefficients from scratch for each content-style pair, with distinct coefficients for different com3 Content and Style LoRAs to merge Rows Wc wi wi Hypernetwork Wc Ws Input Layer 1 Input Layer 2 Rows Ws Columns Mini-batch Output Layer mc, ms for each Figure 4. Hypernetwork Structure. The hypernetwork has multiple input layers, each matching the dimensionality of corresponding layers in the generative model. Here, an example layer with dimensions matching Input Layer 1 is shown. Content and style LoRAs are concatenated and input into the hypernetwork, which then predicts the columnwise merging coefficients for each specified layer. binations, as shown in Fig. 3. With ZipLoRA performing 100 gradient updates per pair, real-time performance is unfeasible, particularly on resource-constrained devices. Our goal is to outperform ZipLoRAs image quality while accelerating merging coefficient generation time by orders of magnitude for unseen content-style pairs. To accomplish this, we pre-train hypernetwork that predicts adaptive merging coefficients on the fly, enabling fast, high-quality merging in single feed-forward pass. An overview is shown in Fig. 2. 3.2. LoRA Dataset Generation To train our hypernetwork, we first build dataset of LoRAs. Content LoRAs are trained on individual subjects from the DreamBooth dataset [34], and style LoRAs are trained on various styles from the StyleDrop / ZipLoRA [36, 37] datasets. Each LoRA is generated using the DreamBooth protocol. training LoRA dataset We {Lval {Ltrain and validation test {Ltest } sets. During training and evaluation, we sample content-style LoRA pairs. The hypernetwork is trained on the training sets, with hyperparameters and design choices tuned on the validation sets. The test sets are reserved to assess performance on novel content-style pairs. the }, }, {Ltest into }, {Lval split }, {Ltrain }, 3.3. Hypernetwork Structure Our hypernetwork, H, takes two LoRA update matrices as inputs: Wc Rmn for content and Ws Rmn for style, and predicts column-wise merging coefficients mc Rn and ms Rn. Given the high dimensionality of each update matrix, flattening them directly as input would be impractical. To address this, we assume that the merging coefficient for each column can be predicted independently. For each column i, we extract the respective content and style columns, wi = Ws[:, i], s] R2m to form the and concatenate them as [wi input features for the hypernetwork. We treat different columns as minibatch, allowing for efficient parc = Wc[:, i] and wi c, wi , allel processing. The full hypernetwork input is thus concat(W , dim = 1) Rn2m. To accommodate the various LoRA matrix sizes within the diffusion model D, we designed with separate input layers tailored to each unique matrix size, each mapped to shared hidden dimension. In our case, the hypernetwork uses two input layers with ReLU non-linearities and shared output layer to predict merging coefficients for each column. Since different rows are treated as mini-batch, overall the hypernetwork outputs 2n coefficients, one for each column of content and style LoRAs: mc, ms = H(Lc, Ls). (1) These coefficients are used to merge the LoRAs Lc and Ls, resulting in the merged LoRA Lm with update matrix Wm: Wm = mc Wc + ms Ws. (2) Fig. 4 provides an overview of how content and style LoRAs predict merging coefficients. Notably, we apply hypernetwork-guided merging for query and output LoRAs, while we use simple averaging for key and value LoRAs. This configuration empirically outperformed other tested options, as detailed in the Supp. Mat. 3.4. Hypernetwork Training }, {Ltrain We train the hypernetwork by sampling content-style LoRA pairs from the training set {Ltrain }. The hypernetwork generates merging coefficients, which are then used to compute merging loss Lmerge that updates the parameters of H. We adopt the merging loss Lmerge from [36], which includes terms that ensure both content and style fidelity, while also encouraging orthogonality between content and style merging coefficients. Specifically, Lmerge is defined as: Lmerge =(D Lm)(xc, pc) (D Lc)(xc, pc)2 +(D Lm)(xs, ps) (D Ls)(xs, ps)2 +λmc ms, (3) where xc, xs are the noisy latents, and pc, ps are the text prompts for content and style reference images respectively [36]. The term λ controls the strength of the orthogonalitypromoting regularization term. The training process is formalized in Algorithm 1. Architecture choices for the hypernetwork, as well as hyperparameters, are optimized on the validation set, and the final evaluation is conducted on the test set, when the hypernetwork simply predicts the merging coefficients for new content and style LoRAs Lc {Ltest }, Ls {Ltest }. c 4. Joint Subject-Style Evaluation Metrics In this section we discuss how to evaluate personalized image generation methods across diverse subjects and styles. 4 Algorithm 1 Hypernetwork training. Reference Direct merge LoRA.rar (Ours) Require: # training steps , learning rate η, base model D, train- } ing dataset of content and style LoRAs {Ltrain }, {Ltrain 1: Initialize hypernetwork 2: for = 1, . . . , do 3: 4: 5: 6: } }, Ls {Ltrain Sample content and style LoRAs from the training set: Lc {Ltrain Predict merging coefficients mc, ms = H(Lc, Ls) Obtain merged LoRA Lm with weight update matrix Wm computed via Eq. (2) Compute Lmerge using Eq. (3) Update ηHLmerge 7: 8: end for Limitations of Existing Metrics. Developing reliable metrics that align with user preferences is crucial for scaling text-to-image models, especially when direct feedback is unavailable. Metrics like CLIP-I, CLIP-T, and DINO [34] are widely used for single-concept personalization (i.e., personalizing to either style or subject) in benchmarks as DreamBooth [34], DreamBench++ [30], and ImagenHub [24]. However, these metrics may not reliably evaluate joint subject-style personalization, as illustrated in Fig. 5. Specifically, the CLIP-I score tends to favor style fidelity, often overlooking accurate representation of the subject (top of Fig. 5), while the DINO score prioritizes the original subject replication overlooking stylistics integration (bottom of Fig. 5). CLIP-T, typically used for text alignment, supports subject recontextualization but is less suited to style-content prompts like [c] <class name> in [s] style. Here [c] is unique rare token identifier for content, <class name> is the class name following [34], and [s] is short description of the style as in StyleDrop [37]. Evaluation via Multimodal Large Language Models (MLLMs). To overcome the limitations of conventional metrics, we propose leveraging MLLMs for evaluation. LLMs have shown high effectiveness in evaluating text-based outputs [54], and their application has recently been extended to multimodal tasks involving both text and images [5]. For example, recent works [52, 57] have successfully utilized MLLMs to determine whether generated images meet specified criteria, such as color or object presence. Among specialized MLLM judge models, LLaVA-Critic [43] stands out for its accuracy in assessing output quality in multimodal contexts. In this work, we use LLaVA-Critic to evaluate whether generated images accurately represent the intended subject (content) and style. Our protocol is as follows: the MLLM judge first assesses if each generated image meets the specified style and content independently. For clarity, we provide reference images for both style and content along with detailed evaluation prompts. Binary ratCLIP-I: 0.817 DINO: 0.180 CLIP-T: 0.319 CLIP-I: 0.695 DINO: 0.293 CLIP-T: 0.334 CLIP-I: 0.445 DINO: 0.801 CLIP-T: 0.259 CLIP-I: 0.671 DINO: 0.411 CLIP-T: 0.298 Figure 5. Limitation of Existing Metrics. Top: CLIP-I is maximized when the style image (shown in the small upper right thumbnail) content is replicated. Bottom: DINO is maximized when the generated image has no style transfer. Figure 6. MLLM Judge Evaluation. Generated images are checked separately for content and style. We mark the image as correct if both are approved. ings are used for both style and content evaluations, with an image deemed correct, i.e., final score of 1, only if it fulfills both criteria, and 0 otherwise. When there are multiple reference images, we consider generated image accurate if the MLLM model identifies it as correct for more than half of the reference images. We call the new metric MARS2: Multimodal Assistant Rating Subject&Style. The process is illustrated in Fig. 6, with more details in the Supp. Mat. For each content-style pair, we generate multiple images and evaluate both the average and best sample quality according to the MLLM judge. This dual evaluation not only facilitates fair comparison with existing literature but also provides flexibility for users in downstream applications, allowing them to select the most preferred sample. Human Evaluation. To complement automated metrics, we also conduct human evaluations on subset of generated images, comparing our results with those from ZipLoRA, the primary competitor. We consider two cases: 1) randomly select one generated sample from each approach for every test 5 content-style LoRA pair; 2) take best sample, i.e. accepted by the MLLM model (if there are multiple samples with correct style and content, randomly choose one of them). For unbiased feedback, we anonymize method names, asking evaluators to rate whether our solution produces images that are better, similar, or worse than the baseline. This human evaluation offers insights into real-world user preferences and serves as qualitative validation of our approach. 5. Experiments Baselines. We compare our approach to several established methods, including: joint training of both content and style via Dreambooth [34]; direct merging of LoRA weights [41]; general model merging techniques such as DARE [51], TIES [46], and DARE-TIES [13]; and ZipLoRA [36], which is specifically designed for merging subject and style LoRAs. ZipLoRA has also been compared in [36] with strategies such as StyleDrop [37], Custom Diffusion [25] and Mix-ofshow [14]. These methods, however, have been shown to perform less effectively while being computationally costly, so we exclude them from further comparison in this work. Implementation Details. All experiments use the SDXL v1 Stable Diffusion model [31], following the setup in [36]. For subject-specific LoRAs, we adopt rare unique token identifiers as in [34]. in contrast, style LoRAs are fine-tuned using text description identifiers, following [37], where these were found more effective for style representation. Base LoRAs are trained as in [36], for 1000 fine-tuning steps, with batch size 1, learning rate of 5 105 and rank of 64. The text encoder remains frozen during training. The hypernetwork used is two-layer MLP with two separate input layers of size 1280 and 2560, followed by ReLU activation function, shared hidden layer of size 128, and two outputs. We train our hypernetwork for 100 different {Lc, Ls} combinations (totalling 5000 steps), with λ = 0.01, learning rate 0.01 and the AdamW optimizer. For ZipLoRA, we use training setup of 100 steps with the same λ and learning rate. The DARE, TIES, and DARE-TIES baselines are evaluated with uniform weights and density of 0.5. For joint training, we used multi-concept variant of Dreambooth LoRA as in [36]. In all experiments, 50 diffusion inference steps are used. Datasets. Our hypernetwork is trained on set of LoRAs rather than images. Subject LoRAs are fine-tuned on images from the DreamBooth [34] dataset, with style LoRAs are fine-tuned on images from the StyleDrop / ZipLoRA [36, 37] datasets. Specifically, our datasets includes 30 subjects (each with 45 images) and 26 styles (each represented by single image). For training, validation, and testing, we split the subjects into 20-5-5 and styles into 18-3-5 (see the Supp. Mat. for details), yielding total of 360 subject-style LoRA combinations for hypernetwork training, quantity shown to be sufficient for robust performance. Evaluation Details. For our MLLM-based metric, MARS2,"
        },
        {
            "title": "Average case Best case",
            "content": "Joint Training [34] Direct Merge [41] DARE [51] TIES [46] DARE-TIES [13] ZipLoRA [36] LoRA.rar (ours) 0.53 0.40 0.34 0.43 0.30 0.58 0.71 0.84 0.76 0.72 0.80 0.60 1.00 1.00 Table 1. MLLM Evaluation. Ratio of generated images with the correct content and style on the combinations of test subjects and styles according to our new metric MARS2. Our solution leads to better images compared to existing approaches. we use the LLaVA-Critic 7b model [43]. The prompts used for the MLLM model are detailed in the Supp. Mat. For human evaluation, evaluators are presented with content and style reference images alongside randomly ordered outputs from each method. Each of our 25 evaluators assesses 25 pairs of images comparing two approaches. An example task precedes the evaluation to clarify the assessment criteria (see Supp. Mat.). Evaluators choose the option that best reflects target content and style, with choices between Option 1, Comparable, and Option 2. 5.1. Quantitative Analysis We quantitatively assess the performance of our solution and the baselines in three main ways: (1) through our MLLMbased MARS2 metric as described in Sec. 4; (2) using standard CLIP-I, CLIP-T, and DINO metrics, that suffer from the already discussed limitations; and (3) via human evaluation study on subset of generated samples. 1) MLLM Evaluation Results are presented in Table 1. Our solution consistently outperforms all methods, including ZipLoRA, in both content and style accuracy. For the best sample (selected by MLLM from 10 generated images as one with correct style and content if available), both our solution and ZipLoRA achieve perfect accuracy, indicating that users can reliably choose preferred outputs when multiple samples are available. Across all generated images, our solution performs better on average than ZipLoRA, likely benefitting from its capacity to leverage knowledge learned from diverse content-style LoRA combinations. 2) Standard Metrics Evaluations are reported in Table 2. We include this analysis for informational purposes only. As explained in Sec. 4, these metrics (DINO, CLIP-I, CLIP-T) are not optimal for the joint subject-style personalization task. Specifically, DINO (CLIP-I) is maximized when the subject (style) reference images are copied without meaningful integration, so more attention should be given to MLLM and human evaluation results. 3) Human Evaluation results are reported in Fig. 7. This evaluation was conducted on subset of generated sam6 CLIP-I DINO CLIP-T ZipLoRA LoRA.rar (ours) Joint Training [34] Direct Merge [41] DARE [51] TIES [46] DARE-TIES [13] ZipLoRA [36] LoRA.rar (ours) 0.623 0.657 0.630 0.620 0.618 0.643 0. 0.764 0.747 0.576 0.592 0.559 0.741 0.643 0.329 0.305 0.360 0.358 0.355 0.334 0.344 Table 2. Standard Metrics. LoRA.rar attains similar results, but these metrics are inadequate for joint subject-style changes. Time to predict merging coeffs # Parameters # Attempts to good* image Extra memory at test time 158s 1.5M 2.55 4GB 0.037s 0.49M 2.28 0GB Table 3. Footprint Analysis. Our LoRA.rar is more than 4000 faster and uses 3 fewer parameters than ZipLoRA , despite using hypernetwork. *: good image is accepted by MARS2. : value for one subject-style pair only. Figure 7. Human Evaluation for generated images sampled randomly or according to MARS2. More than 75% respondents consider LoRA.rar comparable or better than ZipLoRA. ples as described earlier and focused on ZipLoRA as the primary comparison baseline, given the time constraints of manual assessment. The results indicate that our solution compares favorably with ZipLoRA, confirming that our generated images are typically either better or comparable in quality. Furthermore, our solution can operate in real-time for new subject-style combinations, unlike ZipLoRA. 5.2. Qualitative Analysis We conduct qualitative analysis of LoRA.rar by: (1) comparing the images generated by LoRA.rar with those produced by competing methods, and (2) analyzing the diversity of images generated by LoRA.rar across concepts and styles. Comparison against state of the art is shown in Fig. 8. The results demonstrate that LoRA.rar excels in capturing fine details across various styles, consistently producing highquality images. While ZipLoRA also generates high-quality images, LoRA.rar outperforms it in terms of overall fidelity to both content and style. limitation of ZipLoRA is in too realistic generation, e.g., the teapot in 3D rendering style is immersed in photorealistic scene, and the wolf plushie in oil painting is not painting. Other approaches show less consistent results, i.e., direct merge is able to produce teapot or stuffed animal in 3D rendering style (with minor inaccuracies), but fails at generating flat cartoon illustrations, where there is no one-to-one mapping of content and style. DARE, TIES, DARE-TIES do not produce satisfactory results, either the style or the concept are incorrect, or none. Joint training, presents improved results compared to direct merge, but has the same limitations. Reliability across different subject-style pairs is shown in Fig. 9, where LoRA.rar consistently works well across diverse combinations of concepts and styles, highlighting its versatility and effectiveness. 5.3. Additional Analyses We provide detailed analysis of resource usage in Table 3. Our findings highlight the efficiency and scalability of LoRA.rar in comparison to ZipLoRA: 1) Runtime Efficiency: our solution generates the merging coefficients over 4, 000 times faster than ZipLoRA on an NVIDIA 4090, achieving real-time performance. While ZipLoRA requires 100 training steps for each concept-style pair, LoRA.rar generates merging coefficients in single forward pass (per layer) using pre-trained hypernetwork. 2) Parameter Storage: ZipLoRA needs to store the learned coefficients for every combination of concept and style for later use. LoRA.rar only needs to store the hypernetwork, which has 3 times fewer parameters than single ZipLoRA combination. 3) Sample Efficiency: on average, LoRA.rar requires fewer attempts than ZipLoRA to produce high-quality image that aligns with both content and style2.28 attempts for LoRA.rar versus 2.55 for ZipLoRA. This improvement reflects LoRA.rars enhanced accuracy in generating visually coherent outputs without extensive retries, further optimizing resource usage and user experience. 4) Memory Consumption at Test Time: LoRA.rar is efficient in terms of memory, which is dominated by the generative model (15GB), with negligible overhead for our approach, while ZipLoRA requires additional 4GB (19GB totally). Finally, we analyze the merging coefficients learned by LoRA.rar in Fig. 10. LoRA.rar learns non-trivial adaptive merging strategy, with diverse coefficients (including also some negative values). This adaptability allows LoRA.rar to flexibly combine content and style representations, likely contributing to its superior performance. ZipLoRA, instead, mostly converges to an adaptive yet binary merging strategy (see Fig. 3). This more rigid merging strategy may limit its capacity to finely integrate details across styles and subjects, further underscoring the advantage of LoRA.rars approach in generating images with accurate content and style. [C] teapot in [S] style 3D Rendering Oil Painting Watercolor Painting Flat Cartoon Illustration Glowing [C] stuffed animal in [S] style 3D Rendering Oil Painting Watercolor Painting Flat Cartoon Illustration Glowing Joint Training Direct Merge DARE TIES DARETIES ZipLoRA LoRA.rar (ours) Joint Training Direct Merge DARE TIES DARETIES ZipLoRA LoRA.rar (ours) Figure 8. Qualitative Comparison. LoRA.rar generates better images than other merging strategies, including ZipLoRA. Styles Concepts Figure 10. LoRA.rars Merging Coefficients mc, ms for randomly selected columns of the LoRA weight update matrices. LoRA.rar learns non-trivial strategy with superior performance. leverages hypernetwork to generate coefficients for merging content and style LoRAs. By training on diverse contentstyle LoRA pairs, our method can generalize to new, unseen pairs. Our experiments show that LoRA.rar consistently outperforms existing methods in image quality, as assessed by both human evaluators and an MLLM-based judge specifically designed to address the challenges of joint content-style personalization. Crucially, LoRA.rar generates the merging coefficients in real time, bypassing the need for test-time optimization used by state-of-the-art methods. Figure 9. LoRA.rar Evaluation across different subject-style combinations. Our solution consistently produces good results. 6. Conclusion In this work, we introduced LoRA.rar, novel method for joint subject-style personalized image generation. LoRA.rar"
        },
        {
            "title": "References",
            "content": "[1] Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and Amit H. Bermano. Domainagnostic tuning-encoder for fast personalization of text-toimage models. In SIGGRAPH Asia 2023 Conference Papers, 2023. 2 [2] Marc Bartholet, Taehyeon Kim, Ami Beuret, Se-Young Yun, and Joachim Buhmann. Non-linear fusion in federated learning: hypernetwork approach to federated domain generalization. arXiv preprint arXiv:2402.06974, 2024. 2 [3] Taha Ceritli, Savas Ozkan, Jeongwon Min, Eunchung Noh, Cho Jung Min, and Mete Ozay. study of parameter efficient fine-tuning by learning to efficiently fine-tune. In EMNLP Findings, 2024. 2 [4] Vinod Kumar Chauhan, Jiandong Zhou, Ping Lu, Soheila Molaei, and David Clifton. brief review of hypernetworks in deep learning. Artificial Intelligence Review, 57(9), 2024. 2 [5] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In International Conference on Machine Learning, 2024. 5 [6] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and William Cohen. Subject-driven text-to-image generation via apprenticeship learning. In Advances in Neural Information Processing Systems, 2024. 2 [7] Civitai. Civitai: The Home of Open-Source Generative AI. https://civitai.com/, 2024. Accessed: November 2024. 1 [8] Clonesofimo. text-toimage diffusion finetuning. https://github.com/ cloneofsimo/lora, 2022. Accessed: November 2024. 2 Low-rank adaptation for fast [9] Ziyi Dong, Pengxu Wei, and Liang Lin. Dreamartist: Towards controllable one-shot text-to-image generation via positivenegative prompt-tuning. arXiv preprint arXiv:2211.11337, 2022. 2 [10] Yarden Frenkel, Yael Vinker, Ariel Shamir, and Daniel CohenOr. Implicit style-content separation using b-lora. In European Conference on Computer Vision, 2024. 2 [11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In International Conference on Learning Representations, 2022. 2 [12] Rinon Gal, Moab Arar, Yuval Atzmon, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Encoder-based domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics (TOG), 42(4), 2023. 2 [13] Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. Arcees mergekit: toolkit for merging large language models. arXiv preprint arXiv:2403.13257, 2024. 2, 6, [14] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, WUYOU XIAO, Rui Zhao, Shuning 9 Chang, Weijia Wu, Yixiao Ge, Ying Shan, and Mike Zheng Shou. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. In Advances in Neural Information Processing Systems, 2023. 2, 6 [15] David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In International Conference on Learning Representations, 2017. 2 [16] Hasan Abed Al Kader Hammoud, Umberto Michieli, Fabio Pizzati, Philip Torr, Adel Bibi, Bernard Ghanem, and Mete Ozay. Model merging and safety alignment: One bad model spoils the bunch. In EMNLP Findings, 2024. 2 [17] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameIn Proceedings of the ter space for diffusion fine-tuning. IEEE/CVF International Conference on Computer Vision, 2023. 2 [18] Shaozhe Hao, Kai Han, Shihao Zhao, and Kwan-Yee Wong. Vico: Plug-and-play visual condition for personalized text-toimage generation. arXiv preprint arXiv:2306.00971, 2023. [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020. 1 [20] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021. 1, 2 [21] HuggingFace. Hugging Face The AI community building the future. https://huggingface.co/, 2024. Accessed: November 2024. 1 [22] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In International Conference on Learning Representations, 2023. 2 [23] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019. 2 [24] Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. Imagenhub: Standardizing the evaluation of conditional image generation models. In International Conference on Learning Representations, 2024. [25] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of textto-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 2, 6 [26] Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-toIn Advances in Neural Inimage generation and editing. formation Processing Systems, 2024. 2 [27] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. arXiv preprint arXiv:2307.11410, 2023. [28] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-g: Generating images in context In International with multimodal large language models. Conference on Learning Representations, 2024. [29] Maitreya Patel, Sangmin Jung, Chitta Baral, and Yezhou Yang. λ-eclipse: Multi-concept personalized text-to-image diffusion models by leveraging clip latent space. arXiv preprint arXiv:2402.05195, 2024. 2 [30] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: human-aligned benchmark for personalized image generation. arXiv preprint arXiv:2406.16855, 2024. [31] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In International Conference on Learning Representations, 2024. 6 [32] Senthil Purushwalkam, Akash Gokul, Shafiq Joty, and Nikhil Naik. Bootpig: Bootstrapping zero-shot personalized image generation capabilities in pretrained diffusion models. arXiv preprint arXiv:2401.13974, 2024. 2 [33] Litu Rout, Yujia Chen, Nataniel Ruiz, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Rb-modulation: Training-free personalization of diffusion models using stochastic optimal control. arXiv preprint arXiv:2405.17401, 2024. 2 [34] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven genIn Proceedings of the IEEE/CVF Conference on eration. Computer Vision and Pattern Recognition, 2023. 1, 2, 4, 5, 6, 7 [35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2, 3 [36] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani. Ziplora: Any subject in any style by effectively merging loras. In European Conference on Computer Vision, 2024. 1, 2, 4, 6, [37] Kihyuk Sohn, Lu Jiang, Jarred Barber, Kimin Lee, Nataniel Ruiz, Dilip Krishnan, Huiwen Chang, Yuanzhen Li, Irfan Essa, Michael Rubinstein, Yuan Hao, Glenn Entis, Irina Blok, and Daniel Castro Chin. Styledrop: Text-to-image synthesis of any style. In Advances in Neural Information Processing Systems, 2023. 2, 4, 5, 6 [38] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. Key-locked rank one editing for text-to-image personalization. In ACM SIGGRAPH 2023 Conference Proceedings, 2023. 2 [39] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in text-to-image generation. arXiv preprint arXiv:2303.09522, 2023. 2 [40] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. 2 [41] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International conference on machine learning, 2022. 2, 6, 7 [42] Yujia Wu, Yiming Shi, Jiwei Wei, Chengwei Sun, Yuyang Zhou, Yang Yang, and Heng Tao Shen. Difflora: Generating personalized low-rank adaptation weights with diffusion. arXiv preprint arXiv:2408.06740, 2024. 2 [43] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. LlavaarXiv critic: Learning to evaluate multimodal models. preprint arXiv:2410.02712, 2024. 5, [44] Yu Xu, Fan Tang, Juan Cao, Yuxin Zhang, Oliver Deussen, Weiming Dong, Jintao Li, and Tong-Yee Lee. Break-for-make: Modular low-rank adaptations for composable content-style customization. arXiv preprint arXiv:2403.19456, 2024. 2 [45] Youcan Xu, Zhen Wang, Jun Xiao, Wei Liu, and Long Chen. Freetuner: Any subject in any style with training-free diffusion. arXiv preprint arXiv:2405.14201, 2024. 2 [46] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models. In Advances in Neural Information Processing Systems, 2024. 2, 6, 7 [47] Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng Tao. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities. arXiv preprint arXiv:2408.07666, 2024. 2 [48] Jianan Yang, Haobo Wang, Yanming Zhang, Ruixuan Xiao, Sai Wu, Gang Chen, and Junbo Zhao. Controllable textual inversion for personalized text-to-image generation. arXiv preprint arXiv:2304.05265, 2023. 2 [49] Yang Yang, Wen Wang, Liang Peng, Chaotian Song, Yao Chen, Hengjia Li, Xiaolong Yang, Qinglin Lu, Deng Cai, Boxi Wu, et al. Lora-composer: Leveraging low-rank adaptation for multi-concept customization in training-free diffusion models. arXiv preprint arXiv:2403.11627, 2024. 2 [50] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2 [51] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as free lunch. In International Conference on Machine Learning, 2024. 2, 6, [52] Yuchen Zeng, Wonjun Kang, Yicong Chen, Hyung Il Koo, and Kangwook Lee. Can MLLMs perform text-to-image in-context learning? In Conference on Language Modeling, 2024. 5 [53] Yu Zeng, Vishal Patel, Haochen Wang, Xun Huang, TingChun Wang, Ming-Yu Liu, and Yogesh Balaji. Jedi: Jointimage diffusion models for finetuning-free personalized text10 to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2 [54] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge In Advances in Neural with mt-bench and chatbot arena. Information Processing Systems, 2023. 5 [55] Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, and Tong Sun. Customization assistant for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [56] Yufan Zhou, Ruiyi Zhang, Kaizhi Zheng, Nanxuan Zhao, Jiuxiang Gu, Zichao Wang, Xin Eric Wang, and Tong Sun. Toffee: Efficient million-scale dataset construction for subject-driven text-to-image generation. arXiv preprint arXiv:2406.09305, 2024. 2 [57] Yongshuo Zong, Ondrej Bohdal, and Timothy Hospedales. Vl-icl bench: The devil in the details of benchmarking multimodal in-context learning. arXiv preprint arXiv:2403.13164, 2024. 5 11 LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "This document includes some additional material that was not possible to include in the main paper. Sec. A1 presents additional details regarding both MLLM-based and human evaluation, further information on image generation prompts, and it also includes dataset attribution and partitioning details. Sec. A2 shows additional results for our approach such as recontextualization output generations and thorough ablation study on the hypernetwork design. Sec. A3 outlines limitations of our approach and discusses its societal impact. A1. Additional Details A1.1. MLLM-based Evaluation Evaluation Prompts. We show the prompts we have used with our MLLM-based MARS2 metric using the LLaVACritic-7b model. The subject assessment prompt is shown in Fig. A1, while the style assessment prompt is in Fig. A2. Subject Assessment Prompt System Prompt You are helpful assistant. User Prompt Your task is to identify if the test image shows the same subject as the support image. Style Assessment Prompt System Prompt You are helpful assistant. User Prompt Your task is to identify if the test image shows the subject in {style} style. An example image in the {style} style is provided. Example image in the {style} style: {Image} Test image: {Image} The example image shows an illustration of the {style} style and the details of the subject are expected to be different. Do not check similarity with the subject. Is the test image in the {style} style? Answer with Yes or No only. Figure A2. Style Assessment Prompt. Prompt used to evaluate the style on generated images via our MLLM-based metric MARS2. Support image: {Image} Test image: {Image} Pay attention to the details of the subject, it should for example have the same color. However, the general style of the image may be different. Does the test image show the same subject as the support image? Answer with Yes or No only. Figure A1. Subject Assessment Prompt. Prompt used to evaluate the subject fidelity on generated images via our MLLM-based metric MARS2. 1 We test separately for correctness of the generated subject and style as we have found such approach to be more robust. We have also manually checked how accurate the MLLM model is in assessing the correctness of the subject and style, taken singularly, and found the quality to be suitable for the task. We show examples of how the MLLM judge assesses various generated images in terms of the subject or style in Fig. A3. In the first and second row, the generated images reproduce the reference subject in the reference style and, therefore, are correctly accepted by the MLLM judge. Images in third and fifth rows reproduce generic cat (e.g., white rather than gray) in the correct style, hence the MLLM judge accepts the style but not the subject preservation. The teapot in the fourth row is preserved in the generated image, but the style is incorrect (e.g., more similar to an oil painting rather than watercolor painting). Figure A4. Example Case for Evaluators. Example used to teach human evaluators how to evaluate the generated images. In this example, the participant should select Option 2 as better, because the generated image in Option 2 represents the target subject in the target style. Option 1 follows the style, but generates random cat instead. the best scenario, we gathered all the images that satisfied both subject and style according to the MLLM judge and then selected one randomly among thosethere was always at least one such example for each approach. We introduced and explained the task to the evaluators via the example shown in Fig. A4 and the following textual instruction: Your task is to evaluate which of two generated images better represents the given subject and style or if they are similarly good. You are provided with an image showing the subject (e.g. black cat) and an image showing the image style (e.g. van Gogh style painting), and two generated images such as in the example below. In this example you would select option 2 as better because it shows cat that looks like the one in the subject image, and both images follow the style.. The evaluation was done via web app that shows the images and lets the participant click on button saying which option is better among: Option 1, Similar, Option 2. A1.3. Additional Experimental Details for Prompts Used for Image Generation. The prompts the main paper used to generate the images qualitative and quantitative results are of the form: [c] <class name> in [s] style. For [c] we used the rare token used to train the content LoRAs and for <class name> we used the same name as DreamBooth [34]. Finally, for [s] we used the short text description as in StyleDrop, in particular it corresponds to the style name that we assigned (after removing the number, if present). The full list of names is detailed in Sec. A1.4. A1.4. Additional Dataset Details We use the style images from the datasets collected by StyleDrop / ZipLoRA [36, 37], while the subject images are taken from the DreamBooth [34] dataset. Note that these datasets do not contain any human subjects data or personally identifiable information. We provide image attributions below for each image that we used in our experiments. We refer readers to manuscripts and project websites of StyleDrop, Figure A3. MLLM Judge Assessment Samples. This figure illustrates how the MLLM judge evaluates generated images for subject and style alignment. First column: examples of generated images. Second and third columns: reference subject and style, respectively. Green boxes indicate that the MLLM judge confirms the generated image aligns with the reference subject or style, whereas red boxes denote mismatch. A1.2. Human Evaluation Study As part of the human evaluation study, we asked 25 participants to compare two generated images at time, given reference subject and style images. The images are generated by either our approach or ZipLoRA, and they are randomly ordered in each pair. We test 25 subject-style combinations with one pair of generated images for each. The combinations are also randomly ordered. We consider two scenarios, one where we use randomly generated images and one where we take the best images as judged by the MLLM judge. In"
        },
        {
            "title": "Train",
            "content": "backpack, backpack dog, berry bowl, candle, cat #1, colorful sneaker, dog #1, dog #5, dog #6, dog #7, duck toy, fancy boot, grey sloth plushie, monster toy, pink sunglasses, poop emoji, rc car, red cartoon, robot toy, shiny sneaker, vase"
        },
        {
            "title": "Validation",
            "content": "dog #2, dog #3, clock, bear plushie"
        },
        {
            "title": "Test",
            "content": "dog #8, cat #2, wolf plushie, teapot, can 3D rendering #1, 3D rendering #3, abstract rainbow, black statue, cartoon line drawing, flat cartoon illustration #1, glowing 3D rendering, kid crayon drawing, line drawing, melting golden rendering, oil painting #3, sticker, watercolor painting #2, watercolor painting #4, watercolor painting #5, watercolor painting #6, watercolor painting #7, wooden sculpture 3D rendering #2, oil painting #1, watercolor painting #1 3D rendering #4, oil painting #2, watercolor painting #3, flat cartoon illustration #2, glowing Table A1. Dataset partitioning. Contents and styles LoRAs train/validation/test splits. Contents [c] Styles [s] Figure A5. Test set samples. Subject and styles of the test set in our data partitioning. Contents [c] Styles [s] Figure A6. Validation set samples. Subject and styles of the validation set in our data partitioning. ZipLoRA and DreamBooth for more detailed information about the usage policy and licensing of these images. Attribution for Style Reference Images StyleDrop project webpage provides the image attribution information here. In particular, we used the following 20 styles: S1 (3D rendering #1), S2 (watercolor painting #1), S3 (3D rendering #3), S4 (sticker), S5 (flat cartoon illustration #2), S6 (watercolor painting #5), S7 (flat cartoon illustration #1), S8 (melting golden rendering), S9 (kid crawyon drawing), S10 (wooden sculpture), S11 (oil painting #3), S12 (watercolor painting #7), S13 (watercolor painting #6), S14 (oil painting #1), S15 (line drawing), S16 (oil painting #2), S17 (abstract rainbow colored flowing smoke wave design), S18 (glowing), S19 (glowing 3D rendering), S20 (3D rendering #4). Additionally, we also used 6 styles from ZipLoRA (linked as hyper3 links): S21 (3D rendering #2), S22 (watercolor painting #2), S23 (watercolor painting #3), S24 (watercolor painting #4), S25 (cartoon line drawing), S26 (black statue). Attribution for Subject Reference Images The DreamBooth project webpage provides the image attribution information here. Specifically, the sources of the content images that we used in our experiments are as follows (linked as hyperlinks): C1, C2, C3, C4, C5, C6, C7, C8, C9, C10, C11, C12, C13, C14, C15, C16, C17, C18, C19, C20, C21, C22, C23, C24, C25, C26, C27, C28, C29, C30. Dataset Partitioning There are 30 subjects and 26 styles overall. We split the subjects and styles randomly, but with the constraint that there is good representation of different subjects and styles in each split as some subjects and styles are similar to each other. For example we aimed at avoiding only testing on different dogs or only on painting styles. We split the subjects and styles into training, validation and test splits as shown in Tab. A1. In Fig. A5 and Fig. A6 we show images taken from the test and validation sets respectively (used to train the test and validation LoRAs). A2. Additional Results A2.1. MLLM Results per Subject and Style We provide results of MLLM evaluation for each test subject and style in Fig. A7. We report the results for both the average case as well as the best case. The results indicate that there are certain subjects and styles that are more challenging than others, for example the can subject or the glowing style. We also see that LoRA.rar and ZipLoRA are in general significantly more successful than the other approaches, and they can be successful also in cases where other approaches typically fail, for example in the case of the wolf plushie subject. A2.2. Ablation Study on Hypernetwork We conducted an ablation study on the hypernetwork design by exhaustively exploring all possible configurations to determine which components should have their merging coefficients predicted by the hypernetwork We used the validation set and MLLM judge for this investigation, and we report the results in Table A2. We observe that the best results are obtained by Query, Output case that we have used; however, few other combinations also achieve good results such as Query, Key, Output; Query, Value and Value. Average case Best case"
        },
        {
            "title": "Key\nValue\nQuery\nOutput",
            "content": "Key, Value Key, Query Key, Output Query, Value Query, Output Value, Output Query, Key, Value Query, Value, Output Query, Key, Output Key, Value, Output Query, Key, Value, Output 0.28 0.43 0.28 0.39 0.40 0.31 0.44 0.42 0.48 0.29 0.41 0.23 0.49 0. 0.23 0.75 0.83 0.75 0.83 0.92 0.75 0.75 0.83 0.92 0.58 0.83 0.33 0.83 0.50 0.50 Table A2. Ablation study via MLLM Evaluation. Ratio of generated images with the correct content and style on the combinations of validation subjects and styles according to our MARS2metric. A2.3. Additional Qualitative Results In Fig. A8 and Fig. A10 we report recontextulization analysis for different subjects and styles, demonstrating the effectiveness of our approach. A3. Discussion A3.1. Limitations Our approach exhibits certain limitations with specific subjects, particularly the can. This limitation is shared by the other tested model merging methods as well. The can subject is especially challenging because generative models struggle to accurately render text on objects (as we can see in Fig. A9), and also because this subject is significantly different from the ones used for training the hypernetwork. To improve robustness, we argue that we could train the hypernetwork on larger and more varied set of LoRAs. Figure A7. MLLM Evaluation per Test Subject and Style. Ratio of generated images with the correct content and style according to our metric MARS2. Our solution leads to better images compared to existing approaches. 4 [C] dog . . . . . . in [S] style . . . playing with ball . . . catching frisbie . . . wearing hat . . . with crown . . . riding bicycle . . . sleeping . . . in boat . . . driving car [C] stuffed animal . . . . . . in [S] style . . . playing with ball . . . catching frisbie . . . wearing hat . . . with crown . . . riding bicycle . . . sleeping . . . in boat . . . driving car Figure A8. Recontextualization Output Generations. Generated outputs using various prompts for the contents dog2 and wolf plushie. Subject Style LoRA.rar Output A3.2. Societal Impact Our work makes it possible to generate personalized images that follow given style and show given subject, for example ones pet in watercolor painting style. In particular we make generating personalized images significantly more accessible than before as our solution can be deployed on smartphones, enabling real-time merging of LoRA parameters needed for the personalization. However, this brings risks that are shared with image generative models and image editing methods in general. These solutions can be used for creating deceptive content, and with our method it is even easier than before. Addressing the risks of misuse is an ongoing research priority in generative AI. Figure A9. Limitation example. Example of challenging generation case, where the generated text and logo are not accurate. Furthermore, we note that while the MLLM judge is useful for the task of assessing generated images in terms of content and style, it is not perfect and, for example, it may overlook small details specific to the subjects. [C] dog . . . . . . in [S] style . . . playing with ball . . . catching frisbie . . . wearing hat . . . with crown . . . riding bicycle . . . sleeping . . . in boat . . . driving car [C] cat . . . . . . in [S] style . . . playing with ball . . . catching frisbie . . . wearing hat . . . with crown . . . riding bicycle . . . sleeping . . . in boat . . . driving car Figure A10. Recontextualization Output Generations. Generated outputs using various prompts for the contents dog8 and cat2."
        }
    ],
    "affiliations": [
        "Samsung R&D Institute (SRUK)",
        "University of Padova"
    ]
}