{
    "paper_title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields",
    "authors": [
        "Xinhang Liu",
        "Yuxi Xiao",
        "Donny Y. Chen",
        "Jiashi Feng",
        "Yu-Wing Tai",
        "Chi-Keung Tang",
        "Bingyi Kang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Effective spatio-temporal representation is fundamental to modeling, understanding, and predicting dynamics in videos. The atomic unit of a video, the pixel, traces a continuous 3D trajectory over time, serving as the primitive element of dynamics. Based on this principle, we propose representing any video as a Trajectory Field: a dense mapping that assigns a continuous 3D trajectory function of time to each pixel in every frame. With this representation, we introduce Trace Anything, a neural network that predicts the entire trajectory field in a single feed-forward pass. Specifically, for each pixel in each frame, our model predicts a set of control points that parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at arbitrary query time instants. We trained the Trace Anything model on large-scale 4D data, including data from our new platform, and our experiments demonstrate that: (i) Trace Anything achieves state-of-the-art performance on our new benchmark for trajectory field estimation and performs competitively on established point-tracking benchmarks; (ii) it offers significant efficiency gains thanks to its one-pass paradigm, without requiring iterative optimization or auxiliary estimators; and (iii) it exhibits emergent abilities, including goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion. Project page: https://trace-anything.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 2 0 8 3 1 . 0 1 5 2 : r Trace Anything: Representing Any Video in 4D via Trajectory Fields Xinhang Liu1,2, Yuxi Xiao1,3, Donny Y. Chen1, Jiashi Feng1, Yu-Wing Tai4, Chi-Keung Tang2, Bingyi Kang1 1ByteDance Seed, 2HKUST, 3Zhejiang University, 4Dartmouth College"
        },
        {
            "title": "Abstract",
            "content": "Effective spatio-temporal representation is fundamental to modeling, understanding, and predicting dynamics in videos. The atomic unit of video, the pixel, traces continuous 3D trajectory over time, serving as the primitive element of dynamics. Based on this principle, we propose representing any video1 as Trajectory Field : dense mapping that assigns continuous 3D trajectory function of time to each pixel in every frame. With this representation, we introduce Trace Anything, neural network that predicts the entire trajectory field in single feed-forward pass. Specifically, for each pixel in each frame, our model predicts set of control points that parameterizes trajectory (i.e., B-spline), yielding its 3D position at arbitrary query time instants. We trained the Trace Anything model on large-scale 4D data, including data from our new platform, and our experiments demonstrate that: (i) Trace Anything achieves state-of-the-art performance on our new benchmark for trajectory field estimation and performs competitively on established point-tracking benchmarks; (ii) it offers significant efficiency gains thanks to its one-pass paradigm, without requiring iterative optimization or auxiliary estimators; and (iii) it exhibits emergent abilities, including goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion. We will release the code, the model weights and the data platform. Correspondence: Bingyi Kang Project Page: trace-anything.github.io"
        },
        {
            "title": "Introduction",
            "content": "Understanding dynamic scenes requires more than disjoint reconstruction of 3D space at each time step; it demands modeling how the scene evolves in both space and time. central challenge toward spatial intelligence is to develop 4D video representation that captures the underlying spacetime dynamics in way that is geometrically grounded and scalable. Rather than relying on additional estimators such as depth, flow, or tracking, or on heavy per-scene optimization, we observe that the atomic elements of video, its pixels, naturally trace out 3D trajectories in the world, which acts as the atomic primitive of dynamics. Recognizing this, we propose Trajectory Fields, versatile 4D representation for any video that associates each pixel in each frame with parametric 3D trajectory, as illustrated in Figure 2. Unlike prior 4D reconstruction methods that produce disjoint per-frame point clouds [32, 50, 69] and rely on estimated optical flow or 2D 1Here, any video extends beyond monocular videos to include image pairs or even unordered unstructured image collections that capture dynamic scenes. 1 Figure 1 Any video can be represented in 4D with Trajectory Field, dense mapping assigning each pixel in each frame to parametric 3D trajectory. We propose Trace Anything, neural network that predicts the trajectory field with single forward pass. tracks to build cross-frame correspondences, Trajectory Fields offer more direct and compact way to model scene dynamics. Building on this representation, we propose Trace Anything, feed-forward neural network that estimates trajectory fields directly from video frames. As shown in Figure 1, with single forward pass over all input frames, it predicts stack of control point maps for each frame, defining spline-based parametric trajectories for every pixel. This design brings three key advantages: (i) its one-pass scheme eliminates intermediate estimators and iterative global alignment, (ii) it predicts all trajectories (per pixel per frame) jointly in shared world coordinate system, and (iii) it generalizes across diverse inputs, including monocular videos, image pairs, and unordered photo sets. To support training and evaluation at scale, we develop Blender-based platform featuring diverse environments, moving characters, and camera trajectories. It produces photo-realistic renderings with dense annotations, including 2D/3D trajectories, depth, semantics, flow, and camera poses. From this platform, we release (i) the Trace Anything dataset10,000+ videos (120 frames each) for training trajectory field estimation models, and (ii) the Trace Anything benchmark 200 curated videos for evaluating models ability to capture motion jointly across all frames. Figure 2 Given the input frames (left), trajectory field represents the video at the atomic level, mapping each pixel in each frame to 3D trajectory, expressed as parametric curve (right). Trained with our new dataset, Trace Anything achieves state-of-the-art results on our trajectory field benchmark and performs competitively on established point tracking benchmarks, while offering significant efficiency gains. Moreover, our paradigm also reveals new capabilities for spatial reasoning, including motion forecasting, spatio-temporal fusion, and goal-conditioned manipulation. In summary, our contributions are: We propose Trajectory Fields as an atomic-level and versatile 4D video representation, grounded in principled formulation. We present Trace Anything, feed-forward network that predicts trajectory fields without requiring extra estimators or per-scene optimization. We develop synthetic data platform for large-scale training and benchmarking of trajectory field estimation. Extensive experiments on existing and new benchmarks demonstrate competitive accuracy, faster inference, and new capabilities."
        },
        {
            "title": "2 Related Work",
            "content": "(Dynamic) 3D scene reconstruction. Reconstructing 3D structure from multi-view images is long-standing problem in computer vision. Classical Structure-from-Motion (SfM) pipelines [1, 21, 49] proceed in sequential stages: feature extraction, image matching, triangulation, relative pose estimation, and global bundle adjustment. Deep learning has improved individual components [10, 48] yet stage-wise pipelines remain prone to error accumulation. DUSt3R [58] addressed this by directly predicting 3D pointmaps from image pairs. Fast3R [65], VGGT [55], π3 [59] and MapAnything [24] further relaxed the pairwise assumption with all-to-all attention, enabling joint reasoning over all frames and avoiding O(N 2) pairwise inference. However, both traditional and learning-based methods generally assume static scenes and sufficient camera baselines, leading to degraded performance in dynamic settings. To handle monocular videos with dynamics, MegaSAM [32] integrates optimization-based SLAM, while Monst3R [69], POMATO [70], Easi3R [5], St4RTrack [15], and Dynamic Point Maps [50] extend DUSt3R-style networks to dynamic scenes. These methods typically generate disjoint per-frame point clouds, relying on optical flow or 2D tracks for cross-frame correspondences, and their pairwise inference often requires costly per-scene optimization for global alignment. In contrast, Trace Anything directly estimates trajectory fields that produce dynamic point clouds with cross-frame correspondences, sharing the feed-forward spirit of Yang et al. [65] and [55] and performing one-pass inference over all frames. Point tracking. Particle Video [47] first introduced long-range particle trajectories in videos. Early deep learning methods [11, 12, 20] approached this with global matching and local refinement. CoTracker [23] leveraged transformer-based architecture to enable tracking through occlusions, followed by works [6, 28] that improved efficiency with 4D correlation volumes. CoTracker3 [22] further leveraged unlabeled data to boost performance. 3D point tracking remains comparatively new. OmniMotion [57] addressed the task via test-time optimization, while SpatialTracker [63] introduced the first feed-forward 3D tracker by combining 2D tracking with monocular depth priors. DELTA [38] achieved dense 3D tracking using transformer with upsampling for high-resolution outputs. Concurrently, SpatialTrackerV2 [64] scaled training across real and synthetic data, and St4RTrack [15] and POMATO [70] extended 3D reconstruction models for tracking via joint optimization. Unlike prior approaches, our method bypasses monocular depth estimation and 2D trackers and directly predicts dense 3D trajectories in feed-forward manner. 4D representations for NVS. large class of 4D representations has been developed for novel view synthesis (NVS) in dynamic scenes, aiming to deliver immersive effects such as bullet time. Since Neural Radiance Fields (NeRF) [37] introduced implicit volumetric representations, many extensions have incorporated temporal dynamics. One class of methods [17, 30, 31, 62] directly conditions the radiance field on time, treating density and color as continuous functions of space and time. Another class [39, 40, 43, 52, 68] maps observations to canonical space and models dynamics via deformation fields. Grid-based approaches [2, 4, 16, 33, 56] discretize the 4D volume into compact planar factors for efficiency. Also in this line of work, Wang et al. [54] proposed neural trajectory fields, with different formulation and purpose than trajectory fields in this study. More recently, 3D Gaussian Splatting (3DGS) [25] has been extended to dynamics [29, 35, 61, 66, 67], 3 Figure 3 Trace Anything pipeline. Input frames are processed by geometric backbone consisting of an image encoder and fusion transformer. The control point head outputs dense control point maps Pi RDHW 3, where P(k) i,u,v is the k-th control point for pixel (u, v) in frame Ii. These define continuous 3D trajectories xi,u,v(t) via cubic B-splines, yielding 4D reconstruction. improving rendering quality and speed. These efforts focus on photorealistic appearance and typically assume precomputed camera poses or point clouds. Our work is orthogonal: we propose geometry-centric paradigm that directly infers trajectory fields from raw videos, emphasizing accurate 3D motion modeling. Integrating NVS with our paradigm, e.g., using trajectory fields to initialize dynamic 3DGS models, is promising future direction."
        },
        {
            "title": "3 Method",
            "content": "The atomic elements of video, its pixels, naturally trace out 3D trajectories in the world, forming the primitive units of dynamics. Recognizing this, we aim to model dynamic scenes through trajectory fields, 4D representation that encodes each pixel in each frame as continuous 3D trajectory over time. In the following, we first formalize trajectory fields in Section 3.1, then present Trace Anything in Section 3.2, feed-forward network designed to estimate them, and finally describe the overall training scheme in Section 3.3. In this section, we define field as mapping from domain to codomain , : , where may be discrete or continuous space, and may represent scalars, vectors, or functions. We provide preliminaries on fields in Section and on parametric curves in Section B."
        },
        {
            "title": "3.1 Problem Formulation\nWe formalize trajectory fields, a 4D representation of dynamic 3D scenes in a video. Let {Ii}N\nbe a collection\nof N RGB frames, where each Ii ∈ R3×H×W captures the scene at different time steps or viewpoints. A\ntrajectory field is defined as",
            "content": "i=1 : [N ] [H] [W ] C([0, 1], R3), (i, u, v) (cid:55) xi,u,v() (1) where [N ], [H], and [W ] denote the discrete sets of frame indices and pixel coordinates, respectively, and xi,u,v : [0, 1] R3 is continuous 3D trajectory for pixel (u, v) in frame Ii. The domain is = [N ][H][W ], and the codomain is = C([0, 1], R3), the space of continuous functions from [0, 1] to R3. Each trajectory xi,u,v(t) is parameterized as spline-based curve with control points, defined as Pi RDHW 3, (2) 4 i,u,v R3 is the k-th control point for pixel (u, v) in frame Ii, with {0, 1, . . . , 1}. Given basis where P(k) functions {ϕk(t)}D1 k=0 , the trajectory is xi,u,v(t) = D1 (cid:88) k=0 P(k) i,u,vϕk(t). (3) The form of the basis functions {ϕk(t)}D1 k=0 we use cubic B-splines with clamped knots as detailed in Section B. depends on the type of parametric curve. In our implementation, Figure 2 illustrates this formulation of trajectory fields. For any pixel from any frame, its 3D coordinate at any time [0, 1] can be obtained with Equation (3). This fundamentally differs from existing 4D reconstruction methods that predict per-frame disjoint point clouds and establish cross-frame correspondences via estimated optical flow or 2D tracks. Ideally, two conditions should hold: (C1) pixels in static regions collapse to degenerate trajectories; (C2) corresponding pixels from different frames map to the same 3D trajectory."
        },
        {
            "title": "3.2 Network Architecture\nBuilding on the formulation in Section 3.1, we propose a feed-forward network, Trace Anything (Figure 3),\nwhich predicts trajectory fields directly from video or unstructured image sets. For each frame, it outputs\ncontrol point maps defining parametric curves over time, enabling trajectory field estimation in a single pass.\nThis design eliminates reliance on depth or optical flow and avoids per-scene iterative optimization, providing\na compact, efficient approach to modeling 4D scenes.",
            "content": "Geometric backbone. We build Trace Anything upon feed-forward geometric backbone, similar in spirit to recent models [55, 65]. Each frame is first tokenized by an image encoder, followed by fusion transformer that integrates spatio-temporal context across views through interleaved frame-wise and global attention layers. For sequential video input, we additionally inject temporal index embeddings, while the architecture remains compatible with unordered or unstructured image collections. Control Point Head. Built on the backbone features, the control point head outputs dense control point maps Pi RDHW 3 for each input frame Ii. Each pixel (u, v) has control points {P(k) , compactly parameterizing its 3D trajectory. Predictions are in shared world coordinate system, with an optional local CP head estimating control points in each frames local camera system. The head also predicts per-control-point confidence scores Σ(k) i,u,v for confidence-weighted training and filtering uncertain estimates at inference. i,u,v}D1 k=0 , continuous trajectories Curve evaluation. Given the predicted control points and basis functions {ϕk(t)}D1 k=0 xi,u,v(t) are obtained via Equation (3). At evaluation time, the trajectory can be queried at any [0, 1]. In particular, xi,u,v(0) = D1 (cid:88) k=0 P(k) i,u,v ϕk(0) = P(0) i,u,v, xi,u,v(1) = D1 (cid:88) k=0 P(k) i,u,v ϕk(1) = P(D1) i,u,v , (4) where () holds for cubic B-splines with clamped knots or for Bézier bases. To obtain the 3D coordinates of pixel from frame evaluated at the acquisition time of another frame j, we substitute the corresponding temporal parameter tj into its trajectory: Xij(u, v) = xi,u,v(tj). In most cases, tj is obtained from metadata or frame order. Otherwise, an auxiliary timestamp head predicts normalized timestamps ˆtj [0, 1]. As special case, evaluating each trajectory at frame is own acquisition time ti recovers the 3D point map for frame Ii: (5) Xi(u, v) = xi,u,v(ti). (6) Trace Anything outputs the trajectory field with single network inference for all frames, avoiding pairwise inference and subsequent global alignment, while being self-contained and independent of external estimators for monocular depth, optical flow, or 2D tracks."
        },
        {
            "title": "3.3 Training Scheme",
            "content": "To train Trace Anything, we directly supervise the accuracy of predicted trajectories. Intuitively, trajectory predicted from frame should, when evaluated at the timestamp of another frame j, land exactly at its ground-truth 3D location at frame js acquisition time. Trajectory loss. For pixel (u, v) in frame i, the predicted 3D position evaluated at tj is Xij(u, v) (Equation (5)), while the corresponding ground truth is Xgt ij(u, v). We define the loss as ℓij(u, v) = (cid:13) (cid:13)Xij(u, v) Xgt Confidence adjustment. To account for the varying reliability of predicted trajectories across pixels and control points, we incorporate confidence adjustment. For each control point, the network predicts scalar confidence ˆΣ(k) i,u,v > 0 alongside its 3D coordinates. The confidence associated with Xij(u, v) is then aggregated using the same basis functions as in Equation (3): (7) ij(u, v)(cid:13) 2 2. (cid:13) ˆΣij(u, v) = D1 (cid:88) k=0 ˆΣ(k) i,u,v ϕk(tj). The final confidence-adjusted loss then becomes Ltraj-conf = 1 Ω (cid:88) (cid:88) (cid:105) (cid:104) ˆΣij(u, v) ℓij(u, v) + α log ˆΣij(u, v) , (i,j) (u,v)Ω (8) (9) where Ω denotes valid pixels with ground-truth supervision. This adjustment downweights uncertain predictions while discouraging overconfident ones. Timestamp supervision. When ground-truth timestamps are available, we directly supervise Timestamp Head with an L1 regression loss: Ltime = 1 (cid:88) i=1 (cid:12) (cid:12)ˆti ti (cid:12) (cid:12). (10) Static regularization. To encourage condition (C1), pixels in static regions should map to overlapped 3D control points. We enforce this by minimizing the variance of their control points: Lstatic = 1 Ωstatic (cid:88) (cid:16) Var (i,u,v)Ωstatic {P(k) i,u,v}D1 k=0 (cid:17) . (11) Rigidity regularization. For pixels segmented as belonging to the same rigid region, their trajectories should preserve internal distances across control points. Equivalently, the pairwise distance between any two pixels p, within rigid segment should remain constant across control points. We enforce this by minimizing the variance of their distances: Lrigid = 1 Ωrigid (cid:88) (p,q)Ωrigid Var (cid:16)(cid:8)P(k) P(k) 2 (cid:9)D1 k= (cid:17) . (12) Correspondence regularization. To encourage condition (C2), pixels with known cross-frame correspondences should share identical control points. Let Ωcorr be the set of matched pixels ((i, u, v), (j, u, v)). We penalize discrepancies between their control-point sequences: Lcorr = 1 Ωcorr (cid:88) ((i,u,v), (j,u,v))Ωcorr 1 D1 (cid:88) k=0 (cid:13) (cid:13)P(k) i,u,v P(k) j,u,v (cid:13) 2 2. (cid:13) (13) Final objective. The overall loss combines the core trajectory supervision with the above regularization terms: = Ltraj-conf + λtimeLtime + λstaticLstatic + λrigidLrigid + λcorrLcorr. (14) 6 Figure 4 Sample renderings from our data platform."
        },
        {
            "title": "4 Trace Anything Data Platform",
            "content": "Data-driven modeling of dynamic scenes is limited by the lack of large-scale datasets with dense, reliable annotations. Existing synthetic datasets and generators [13, 19, 20, 36, 71] are typically small and biased toward rigid motion, with sparse or short-term annotations, which are insufficient for realistic scene understanding and diverse dynamics. To address this, we develop scalable 4D Scene Data Platform in Blender that synthesizes photo-realistic dynamic scenes with dense ground-truth annotations. Trace Anything dataset. Using our platform, we build dataset whose primary purpose is to train the Trace Anything model on trajectory field estimation. The current release contains about 10K unique scenes, each with 120 annotated frames. The collection spans wide range of settings and motions, with examples shown in Figure 4. The dataset exhibits diversity across multiple aspects: (i) Environment diverse indoor and outdoor backgrounds from public asset libraries and procedural generation [44, 45]; (ii) Dynamics articulated human characters and movable objects with both rigid and non-rigid motion; (iii) Camera motion smooth trajectories sampled around active regions to mimic natural filming. Rendered RGB videos are paired with per-pixel 2D/3D trajectories, depth maps, camera poses, semantic masks, which facilitate the training scheme introduced in Section 3.3. Since our platform is fully programmable, it can be easily extended with new assets, domains, or annotation modalities to support future research. Trace Anything benchmark. To evaluate the task of trajectory field estimation, we construct benchmark consisting of 200 videos, each with 120 frames. key difference from established point tracking datasets [26] lies in the evaluation protocol: point tracking benchmarks evaluate estimated trajectories only for pixels sampled from the first frame (first-to-all), whereas our benchmark evaluates trajectories for pixels sampled from all frames (all-to-all). This requires models not only to follow motion from single starting frame, but also to jointly capture dynamics across the entire sequence. In addition, our benchmark provides denser trajectory annotations, covering more pixels per framel, and evaluates in world coordinates, requiring models to reason about both global geometry and motion."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we evaluate our method across series of challenging settings, demonstrating its competitive accuracy, faster inference, and novel capabilities. Please refer to the appendix for additional experimental results, and to our project page for videos and interactive demos."
        },
        {
            "title": "5.1 Experimental Details",
            "content": "We generate training data using Kubric [19] and our proposed 4D scene data engine. Specifically, we render 20K videos with 24 frames each using Kubric, with half containing continuous camera motion and the other half discrete camera motion, and over 10K videos with 120 frames each from our engine. While Kubric equips models with preliminary ability to capture rigid object motion, it is largely limited to rigid dynamics and 7 Figure 5 Video-based trajectory field estimation on DAVIS [41]. Trace Anything predicts trajectory fields that can yield dynamic point cloud sequences and dense 3D trajectories, while remaining robust to complex non-rigid motion and occlusions. textured backgrounds. Our engine complements this with diverse non-rigid object motions and more complex, varied environments. Our released model uses an image encoder and fusion transformer initialized with Fast3R [65] pretrained weights, while the CP heads are randomly initialized. For the choice of parametric curves, our released model adopts B-splines, as detailed in Section B. All models are trained on images at resolution of 512 pixels on the longest side, using AdamW [34] with learning rate of 0.0001 and cosine annealing schedule. In the first stage, we train on 20K Kubric videos; in the second stage, we use mixture of 20K Kubric videos and 10K from our engine. We adopt batch size of 1, with each batch sampling up to 30 frames. Training takes 7.22 days on 32 NVIDIA A100 80GB GPUs. To enable efficient large-scale training, we leverage FlashAttention [7, 8] for improved time and memory efficiency, and apply DeepSpeed ZeRO Stage 2 [46], which partitions optimizer states, moment estimates, and gradients across machines."
        },
        {
            "title": "5.2 Trajectory Field Estimation",
            "content": "We present qualitative results of trajectory field estimation on videos and image pairs. Qualitative comparisons are provided in Section C.2. Video to trajectory field. For computational efficiency, we uniformly subsample long sequences to fewer than 60 frames. Figure 5 shows qualitative results on DAVIS [41], covering diverse dynamic scenes. Our predictions faithfully reconstruct both dynamic and static components of the scene, yielding dense, pixel-level 3D trajectories. These trajectories capture motions ranging from near-rigid transformations, such as toy train moving along track, to highly non-rigid deformations, such as humans or animals in motion, while also handling severe occlusions and preserving global scene structure. 8 Figure 6 Image-pair-based trajectory field estimation (goal-conditioned manipulation) on Bridge [53]. Given an initial and goal image, Trace Anything predicts trajectory field that interpolates the 3D motion of both the robot arm and manipulated objects. We further show the projected 2D trajectories (see Section C.1 and Figure for details). Figure 7 Qualitative results with image pairs as input. Image pair to trajectory field. Our approach can also infer trajectory fields directly from image pairs, effectively reconstructing the implied spatio-temporal dynamics and interpolating intermediate motion. For this experiment, we use BridgeData V2 [53], large and diverse dataset of robotic manipulation behaviors. Image pairs are sampled from video sequences with temporal gap of 1020 frames. As illustrated in Figure 6 and Figure 7, given an initial image of scene and goal image specifying the desired outcome, our model predicts trajectory field that captures plausible 3D trajectories of both objects and agents involved. These 9 Figure 8 Trajectory fields and camera poses estimated from an unstructured, unordered image set. No sequence information is provided to the model. trajectories can also be re-projected with estimated camera poses to yield 2D trajectories (see Section C.1 and Figure for details). In the context of robot learning, this naturally aligns with goal-conditioned manipulation, where predicted trajectories can be interpreted as feasible robot end-effector motions [3]. Unstructured image set to trajectory field. Beyond videos or image pairs, our method also handles unstructured, unordered image sets, setting not addressed by prior work. The inputs lack both temporal ordering and continuous camera motion, yet our framework by design can also cope with such challenging cases. As shown in Figure 8, our method predicts plausible trajectory fields and camera poses under these conditions. For clarity, we present the input images in chronological order, although no sequence information is provided to the model."
        },
        {
            "title": "5.3 Quantitative Evaluation\nWe quantitatively evaluate trajectory field estimation on the Trace Anything benchmark, introduced in\nSection 4. In contrast to established point tracking benchmarks, which evaluate trajectories only from the",
            "content": "10 Table 1 Quantitative results on video-based trajectory field estimation. CA is reported in 102 and SDD in 103. Best in bold, second-best underlined. Method CoTracker3 + VGGT DELTA SpaTrackerV2 MonsT3R St4RTrack POMATO Easi3R Trace Anything EPEmix EPEsta EPEdyn CA 7.83 6.24 7.24 8.77 9.37 6.78 5. 0.461 0.384 0.291 0.258 0.247 0.254 0.302 0.518 0.404 0.296 0.316 0.278 0.272 0.308 0.555 0.425 0.366 0.330 0.370 0.308 0.324 SDD Runtime (s) 1.67 1.75 1.51 1.74 1.76 1.44 1.55 197.4 231.6 178.4 99.1 22.5 81.8 130.9 0. 0.218 0.295 5.09 1.06 2.3 Table 2 Quantitative results on image-pair-based trajectory field estimation. CA is reported in 102 and SDD in 103. Best in bold, second-best underlined. Method SEA-RAFT + VGGT RAFT-3D MASt3R MonST3R St4RTrack POMATO Easi3R Trace Anything EPEmix EPEsta EPEdyn CA 18.22 17.50 33.99 20.10 15.33 19.58 20.41 0.226 0.281 0.220 0.206 0.211 0.181 0.284 0.193 0.219 0.181 0.167 0.202 0.137 0. 0.427 0.324 0.328 0.346 0.325 0.320 0.323 SDD Runtime (s) 0.77 0.98 1.70 1.25 0.63 0.84 0.91 1.91 0.37 2.39 2.51 1.39 4.75 5.08 0.135 0.106 0. 12.41 0.54 0.20 first frame, our protocol requires all-to-all predictions: every pixel in every frame must be associated with complete trajectory spanning the entire sequence. Evaluation is conducted in two settings: (i) video-based inference, where models process 30-frame video clips, and (ii) image-pair-based inference, where trajectories are estimated from two frames sampled 5 frames apart. All evaluations were conducted using single NVIDIA A100 GPU. We present other quantitative results in Section C.3 and ablation study in Section C.4. Metrics. We evaluate reconstruction accuracy using end-point error (EPE). Specifically, EPEmix, EPEsta, and EPEdyn measure the average 3D end-point error over all points, static points, and dynamic points, respectively. To further verify whether the conditions C1 and C2 introduced in Section 3 are satisfied, we introduce two complementary metrics. Static Degeneracy Deviation (SDD) quantifies the temporal jitter of trajectories in static regions, where smaller values indicate better compliance with C1. Correspondence Agreement (CA) measures how consistently dynamic trajectories are predicted from corresponding pixels in different source frames, with lower values indicating better compliance with C2. Baselines. For video-based inference, we compare against state-of-the-art dynamic scene reconstruction and point tracking approaches, including CoTracker3 [22] (lifted to 3D using VGGT [55]), DELTA [38], SpaTrackerV2 [64], MonsT3R [69], St4RTrack [15], POMATO [70] and Easi3R [5]. For image-pair inference, we compare against the optical flow method SEA-RAFT [60] (lifted to 3D with VGGT [55]), the scene flow method RAFT-3D [51], and several 3D reconstruction approaches, including MASt3R [27], MonST3R [69], St4RTrack [15], POMATO [70] and Easi3R [5]. Results. Quantitative results are shown in Tables 1 and 2. Trace Anything achieves the best performance across all metrics, substantially reducing end-point errors in both static and dynamic regions while also attaining the lowest SDD and CA, indicating stronger compliance with consistency conditions. Moreover, it runs over an order of magnitude faster than optimization-based approaches, underscoring the advantage of our one-pass design. Runtime breakdown. As shown in Figure 9, our approach exhibits total runtime that scales approximately linearly with the number of frames. The fusion transformer is the most time-consuming stage, followed by 11 Figure 9 Stage-wise runtime vs. number of input frames. Figure 10 Velocity-based forecasting. Per-pixel trajectories are extrapolated by tangent continuation, with reconstructed trajectories in red and extrapolated ones in blue. Figure 11 Spatio-temporal fusion. The trajectory field can be leveraged to fuse observations of the dynamic entity across different frames into canonical frame. image encoding and curve evaluation. With single-pass inference and no per-scene optimization or external estimators, it exhibits clear efficiency advantage, as shown in Tables 1 and 2."
        },
        {
            "title": "5.4 Emergent Capabilities",
            "content": "Trajectory Field representation and Trace Anything model exhibit emergent capabilities that competing approaches do not support. Velocity-based forecasting. The trajectory field inherently encodes 3D point velocities, per-pixel trajectories can be extrapolated by tangent continuation, allowing dense motion forecasting without additional predictors, as shown in Figure 10. Instruction-based forecasting. With natural language instructions as input, we leverage image or video generation models to produce future states conditioned on the instructions, and then apply Trace Anything to lift these forecasts into 2D trajectory fields. In Figure 12, we forecast robot actions conditioned on different instructions. We use Seedance 1.0 [18] to generate videos of future states for different instructions, and then apply Trace Anything to predict the trajectory fields from the generated videos. 12 Figure 12 Instruction-based forecasting. Future states are generated using Seedance 1.0. Spatio-temporal fusion. In Figure 11, predicted trajectory fields enable dynamic entities observed across multiple frames to be consistently fused back into common canonical frame. This provides mechanism for aggregating partial observations over time, overcoming occlusions and view changes by aligning pixels to common reference."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced Trajectory Fields, 4D representation that encodes each pixel of each frame as 3D trajectory, and Trace Anything, feed-forward model that predicts trajectory fields from input frames, eliminating auxiliary estimators and per-scene optimization. To support large-scale learning and evaluation, we developed synthetic data platform. Experiments show that Trace Anything delivers competitive accuracy and inference efficiency, while exhibiting new capabilities."
        },
        {
            "title": "References",
            "content": "[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven Seitz, and Richard Szeliski. Building rome in day. Communications of the ACM, 54(10):105112, 2011. [2] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf, Matthew OToole, and Changil Kim. HyperReel: High-fidelity 6-DoF video with ray-conditioned sampling. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [3] Homanga Bharadhwaj, Roozbeh Mottaghi, Abhinav Gupta, and Shubham Tulsiani. Predicting point European Conference on Computer Vision (ECCV), pages 306324. Springer, 2024. from internet generalizable enables videos tracks robot manipulation. Track2act: In [4] Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 130141, 2023. [5] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and Anpei Chen. Easi3r: Estimating disentangled motion from dust3r without training. IEEE/CVF International Conference on Computer Vision (ICCV), 2025. [6] Seokju Cho, Jiahui Huang, Jisu Nam, Honggyu An, Seungryong Kim, and Joon-Young Lee. Local all-pair correspondence for point tracking. In European Conference on Computer Vision (ECCV), pages 306325. Springer, 2024. [7] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. [8] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems (NeurIPS), 35:1634416359, 2022. [9] Carl de Boor. Practical Guide to Splines. Springer, 1978. [10] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest point detection and description. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 224236, 2018. [11] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria Recasens, Lucas Smaira, Yusuf Aytar, Joao Carreira, Andrew Zisserman, and Yi Yang. Tap-vid: benchmark for tracking any point in video. Advances in Neural Information Processing Systems (NeurIPS), 35:1361013626, 2022. [12] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and AnIn drew Zisserman. Tapir: Tracking any point with per-frame initialization and temporal refinement. IEEE/CVF International Conference on Computer Vision (ICCV), pages 1006110072, 2023. [13] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In IEEE/CVF International Conference on Computer Vision (ICCV), pages 27582766, 2015. [14] Gerald Farin. Curves and Surfaces for CAGD: Practical Guide. Morgan Kaufmann, 2002. [15] Haiwen Feng, Junyi Zhang, Qianqian Wang, Yufei Ye, Pengcheng Yu, Michael Black, Trevor DarSimultaneous 4d reconstruction and tracking in the world. rell, and Angjoo Kanazawa. IEEE/CVF International Conference on Computer Vision (ICCV), 2025. St4rtrack: [16] Sara Fridovich-Keil, Giacomo Meanti, Explicit Benjamin Recht, Angjoo Kanazawa. and appearance. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1247912488, 2023. Frederik Rahbæk Warburg, radiance fields in space, K-planes: time, and In [17] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic monocular video. In IEEE/CVF International Conference on Computer Vision (ICCV), pages 57125721, 2021. [18] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [19] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, 14 Hsueh-Ti (Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: scalable dataset generator. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [20] Adam Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In European Conference on Computer Vision (ECCV), pages 5975. Springer, 2022. [21] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. [22] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudo-labelling real videos. arXiv preprint arXiv:2410.11831, 2024. [23] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In European Conference on Computer Vision (ECCV), pages 1835. Springer, 2024. [24] Nikhil Keetha, Norman Müller, Johannes Schönberger, Lorenzo Porzi, Yuchen Zhang, Tobias Fischer, Arno Knapitsch, Duncan Zauss, Ethan Weber, Nelson Antunes, Jonathon Luiten, Manuel Lopez-Antequera, Samuel Rota Bulò, Christian Richardt, Deva Ramanan, Sebastian Scherer, and Peter Kontschieder. MapAnything: Universal feed-forward metric 3D reconstruction, 2025. arXiv preprint arXiv:2509.13414. [25] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (TOG), 42(4):114, 2023. [26] Skanda Koppula, Ignacio Rocco, Yi Yang, Joe Heyward, Gabriel Brostow, and Carl Doersch. Advances in Neural Information Processing Systems (NeurIPS), 37:8214982165, 2024. Tapvid-3d: benchmark for Joao Carreira, Andrew Zisserman, in 3d. tracking any point [27] Vincent Leroy, Yohann Cabon, and Jérôme Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision (ECCV), pages 7191. Springer, 2024. [28] Hongyang Li, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, and Lei Zhang. Taptr: Tracking any point with transformers as detection. In European Conference on Computer Vision (ECCV), pages 5775. Springer, 2024. [29] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 85088520, 2024. [30] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 64986508, 2021. [31] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 64986508, 2021. [32] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast and robust structure and motion from casual dynamic videos. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [33] Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang, Pedro Miraldo, Suhas Lohit, and Moitreya ChatterIn jee. Gear-nerf: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1966719679, 2024. free-viewpoint rendering and tracking with motion-aware spatio-temporal sampling. [34] Ilya Loshchilov, Frank Hutter, et al. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 5(5):5, 2017. [35] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. arXiv preprint arXiv:2308.09713, 2023. 15 [36] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 40404048, 2016. [37] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Nerf: Representing scenes as neural radiance fields for view synthesis. Jonathan T. Barron, Ravi RamamoorIn thi, and Ren Ng. European Conference on Computer Vision (ECCV), 2020. [38] Tuan Duc Ngo, Peiye Zhuang, Chuang Gan, Evangelos Kalogerakis, Ying Lee, and Chaoyang Wang. International Conference on Learning Representations (ICLR), 2025. Delta: Dense efficient [39] Keunhong Park, Utkarsh Sinha, Jonathan Barron, Sergey Tulyakov, Hsinlong-range 3d tracking for any video. Sofien Bouaziz, Dan Goldman, In radiance fields. Steven Seitz, and Ricardo Martin-Brualla. IEEE/CVF International Conference on Computer Vision (ICCV), pages 58655874, 2021. Nerfies: Deformable neural [40] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Ricardo Martin-Brualla, and Steven Seitz. Hypernerf: higher-dimensional representation for topologically varying neural radiance fields. ACM Transactions on Graphics (TOG), 2021. [41] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 724732, 2016. [42] Les Piegl and Wayne Tiller. The NURBS Book. Springer, 1997. [43] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages for dynamic scenes. 1031810327, 2021. [44] Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei, Mingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen, Beining Han, Yihan Wang, Alejandro Newell, Hei Law, Ankit Goyal, Kaiyu Yang, In Infinite photorealistic worlds using procedural generation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1263012641, 2023. and Jia Deng. [45] Alexander Raistrick, Hongyu Wen, Meenal Parakh, Deng. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Lingjie Mei, Karhan Kayan, David Yan, Yiming Zuo, Beining Han, and Jia Stamatis Alexandropoulos, Lahav Lipson, Zeyu Ma, In June generation. 2178321794, scenes using procedural Infinigen indoors: Photorealistic indoor pages [46] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. [47] Peter Sand and Seth Teller. Particle video: Long-range motion estimation using point trajectories. International Journal of Computer Vision (IJCV), 80(1):7291, 2008. [48] Paul-Edouard novich. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 49384947, 2020. Malisiewicz, graph feature matching with Sarlin, Superglue: Andrew networks. Daniel Learning and neural DeTone, Tomasz [49] Johannes Schonberger revisited. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 41044113, 2016. Structure-from-motion Jan-Michael Frahm. and RabiIn In [50] Edgar Sucar, Zihang Lai, Eldar Insafutdinov, and Andrea Vedaldi. Dynamic point maps: versatile representation for dynamic 3d reconstruction. arXiv preprint arXiv:2503.16318, 2025. [51] Zachary Teed and Jia Deng. rigid-motion embeddings. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 83758384, 2021. Scene flow using Raft-3d: In [52] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Christoph Lassner, and Christian Theobalt. Non-rigid neural radiance fields: Reconstruction and novel view synthesis of dynamic scene from monocular video. In IEEE/CVF International Conference on Computer Vision (ICCV), pages 1295912970, 2021. 16 [53] Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, Philippe HansenEstruch, Quan Vuong, Andre He, Vivek Myers, Kuan Fang, Chelsea Finn, and Sergey Levine. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning (CoRL), 2023. [54] Chaoyang Wang, Ben Eckart, Simon Lucey, and Orazio Gallo. Neural trajectory fields for dynamic novel view synthesis. arXiv preprint arXiv:2105.05994, 2021. [55] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [56] Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yanshun Zhang, Yingliang Zhang, Minye Wu, In Jingyi Yu, and Lan Xu. Fourier plenoctrees for dynamic radiance field rendering in real-time. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1352413534, 2022. [57] Qianqian Wang, sander Holynski, IEEE/CVF International Conference on Computer Vision (ICCV), pages 1979519806, 2023. Yen-Yu Chang, and Noah Snavely. Ruojin Cai, Tracking Li, everywhere Zhengqi everything Bharath Hariharan, once. all at AlekIn [58] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2069720709, 2024. [59] Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, and Tong He. π3: Scalable permutation-equivariant visual geometry learning, 2025. URL https://arxiv.org/abs/2507.13347. [60] Yihan Wang, Lahav Lipson, and Jia Deng. Sea-raft: Simple, efficient, accurate raft for optical flow. In European Conference on Computer Vision (ECCV), pages 3654. Springer, 2024. [61] Guanjun Wu, Taoran Yi, Qi Tian, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 4d gaussian splatting for and Xinggang Wang. Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, rendering. real-time dynamic scene [62] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiance fields for free-viewpoint video. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 94219431, 2021. [63] Yuxi Xiao, and Xiaowei IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2040620417, 2024. Spatialtracker: space. pixels Shangzhan Zhang, Tracking Nan Xue, any 2d Sida Peng, 3d in Yujun Shen, In Qianqian Wang, Zhou. [64] Yuxi Xiao, Jianyuan Wang, Nan Xue, Nikita Karaev, Yuri Makarov, Bingyi Kang, Xing Zhu, tracking made easy. Hujun Bao, Yujun Shen, and Xiaowei Zhou. IEEE/CVF International Conference on Computer Vision (ICCV), 2025. Spatialtrackerv2: 3d point [65] Jianing Yang, Alexander Sax, Kevin J. Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska In Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2025. [66] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. International Conference on Learning Representations (ICLR), 2023. [67] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. formable IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2033120341, 2024. high-fidelity monocular reconstruction. gaussians dynamic scene for 3d DeIn [68] Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu, Yingliang Zhang, representation. Editable free-viewpoint video using layered neural Lan Xu, ACM Transactions on Graphics (TOG), 40(4):118, 2021. and Jingyi Yu. [69] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. International Conference on Learning Representations (ICLR), 2025. [70] Songyan Zhang, Yongtao Ge, Jinyuan Tian, Guangkai Xu, Hao Chen, Chen Lv, and Chunhua Pomato: Marrying pointmap matching with temporal motion for dynamic 3d reconstruction. Shen. IEEE/CVF International Conference on Computer Vision (ICCV), 2025. [71] Yang Zheng, Adam Harley, Gordon Wetzstein, and Leonidas long-term point Guibas. IEEE/CVF International Conference on Computer Vision (ICCV), pages 1985519865, 2023. large-scale Pointodyssey: for tracking. Bokui Shen, synthetic dataset In"
        },
        {
            "title": "Contents",
            "content": "1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.1 Experimental Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.2 Trajectory Field Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.3 Quantitative Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.4 Emergent Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 3 4 4 5 6 7 7 8 10 12 13 19 Parametric Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 Additional Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1 2D Trajectories, Dynamic Masks, Scene Flow, and Camera Poses . . . . . . . . . . . . . . . . C.2 Qualitative Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Additional Quantitative Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . LLM Usage Declarations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 21 21 21 22"
        },
        {
            "title": "A Fields",
            "content": "A field is mapping defined on domain , either continuous space (e.g., Rn) or discrete space (e.g., Zn), to codomain , which may be scalar space (e.g., R), vector space (e.g., R3), or function space (e.g., C(N )). Formally, the field is given by For instance, the radiance field introduced in [37] maps 3D coordinate R3 and viewing direction S2 (the unit sphere) to density σ R+ and color R3. This is expressed as : V. (15) As discussed in Section 3.1, the trajectory field introduced in this work is defined as : R3 S2 R+ R3, (x, d) (cid:55) (σ, c). : [N ] [H] [W ] C([0, 1], R3), (i, u, v) (cid:55) xi,u,v(), 19 (16) (17) where [N ], [H], and [W ] denote the discrete sets of frame indices and pixel coordinates, respectively, and xi,u,v : [0, 1] R3 is continuous 3D trajectory for pixel (u, v) in frame Ii. The domain is = [N ][H][W ], and the codomain is = C([0, 1], R3), the space of continuous functions from [0, 1] to R3."
        },
        {
            "title": "B Parametric Curves",
            "content": "Our trajectory field representation assigns each pixel to 3D trajectory, expressed as parametric curve. In computer graphics, parametric curves are essential for modeling smooth trajectories and surfaces in applications like geometric design and animation [14, 42]. spline-based parametric curve x(t) : [0, 1] R3 maps parameter [0, 1] to 3D space, defined as x(t) = n1 (cid:88) k=0 Pkϕk(t), (18) where Pk R3 are control points and ϕk(t) are basis functions. As widely used class, Bézier curves use Bernstein polynomials as basis functions. Bézier curve of degree with + 1 control points P0, P1, . . . , Pd R3 is defined as x(t) = (cid:88) i=0 PiBi,d(t), Bi,d(t) = (cid:19) (cid:18)d ti(1 t)di, (19) where Bi,d(t) are Bernstein polynomials [14]. Bézier curves interpolate the first and last control points but lack local control, as adjusting one control point affects the entire curve. B-spline curves, in contrast, provide local control through knot vector that defines the parameter intervals where basis functions are active. B-spline curve of degree with control points P0, P1, . . . , Pn1 R3 is defined as x(t) = n1 (cid:88) i=0 PiNi,p(t), (20) where Ni,p(t) are B-spline basis functions determined by knot vector via the Cox-de Boor recursion formula [9]. In our implementation of Trace Anything, we employ cubic B-splines (p = 3) with clamped, non-uniform knot vectors to parameterize trajectories xi,u,v(t). Each segment is defined by four control points, corresponding to the cubic degree (p = 3). trajectory is defined as xi,u,v(t) = n1 (cid:88) k=0 P(k) i,u,vNk,3(t), (21) where P(k) i,u,v R3 are control points indexed by i, u, v, and Nk,3(t) are cubic B-spline basis functions determined by knot vector = [t0, t1, . . . , tm1]. The basis functions are computed via the Cox-de Boor recursion formula: Nk,0(t) = 1 1 0 if tk < tk+1 for < + p, if tk tk+1 for = + p, otherwise, Nk,p(t) = tk tk+p tk Nk,p1(t) + tk+p+1 tk+p+1 tk+1 Nk+1,p1(t), (22) (23) for = 1, 2, 3, with non-zero denominators assumed. For = 4, 7, 10 control points, we define knot vectors with multiplicity 4 at = 0 and = 1 to ensure interpolation of the first and last control points (xi,u,v(0) = P(0) , i,u,v 20 xi,u,v(1) = P(n1) i,u,v ). The knot vectors tn are defined as: tn = [0, 0, 0, 0, 1, 1, 1, 1] [0, 0, 0, 0, 0.5, 0.5, 0.5, 1, 1, 1, 1] [0, 0, 0, 0, 1/3, 1/3, 1/3, 2/3, 2/3, 2/3, 1, 1, 1, 1] if = 4, if = 7, if = 10. (24) Internal knots have multiplicity up to 3, ensuring 0-continuity between segments. Knot differences are precomputed for efficient evaluation. Confidence values are interpolated alongside 3D coordinates P(k) using i,u,v the same basis functions ϕk(t) = Nk,3(t), enabling uncertainty-aware trajectory modeling."
        },
        {
            "title": "C Additional Experimental Results",
            "content": "In this section, we present additional experimental results. Please also refer to the supplementary materials for video results, including the presented features, interactive visualization demos, and qualitative comparisons. C.1 2D Trajectories, Dynamic Masks, Scene Flow, and Camera Poses The outputs of Trace Anything can naturally yield 2D trajectories, dynamic masks, scene flow, and camera poses. 2D trajectories. Given the predicted per-pixel 3D trajectories, and with known or estimated camera parameters, we can project them into the image plane to obtain 2D trajectories. In Figure A, we overlay the projected 2D trajectories on the first input frame. We also demonstrate this feature in Figure 6. Dynamic masks. Our method effectively disentangles static and dynamic components. After Trace Anything predicts control points, we compute the variance over the control-point set associated with each pixel; thresholding this per-pixel variance yields dynamic mask that cleanly separates static from dynamic regions, as illustrated in Figure B. Scene flow. Given an input image pair, the scene flow can be obtained as the difference between the two In Figure C, we present 4D reconstruction together with the endpoints of the predicted trajectories. estimated scene flow from an image pair in the Spring dataset. To highlight robustness under long-range motion, the two images are chosen from non-consecutive frames. Camera poses. Since Equation (6) provides world-coordinate point map for each image, we follow Yang et al. [65] (Sec. 4.2) to estimate focal length, rotation, and translation. Our method handles both continuous camera motion in videos and discrete poses from unordered image sets. As shown in Figure D, it correctly recovers camera motion even in dynamic scenesfor example, forward camera movement with perpendicular object motion, or objects in free fall captured by an unordered image set. In the second example, we present the input images in chronological order, although no temporal information is provided to the model. C.2 Qualitative Comparison We provide qualitative comparisons of reconstructed point clouds on DAVIS [41]. As shown in Figure E, our method better preserves fine object details (e.g., the elephants tail and the flamingos neck), correctly handles complex motion, and disentangles static and dynamic objects. Please refer to the supplementary videos for clearer visual comparisons. C.3 Additional Quantitative Comparison Out-of-distribution input. To evaluate our model under out-of-distribution conditions, we construct an additional benchmark from PointOdyssey [71], consisting of 50 videos of 30 frames each. Our model has never been trained or fine-tuned on PointOdyssey. As shown in Table A, our method maintains advantages across all metrics as well as inference efficiency. 3D tracking. Although our primary task is trajectory field estimation, our method achieves strong results on 3D tracking without taskor dataset-specific fine-tuning. We quantitatively compare against other approaches Table Quantitative results on out-of-distribution data. CA is reported in 102 and SDD in 103. Best in bold, second-best underlined. Method St4RTrack POMATO Easi3R EPEmix EPEsta EPEdyn CA 9.82 6.24 7.10 0.243 0.319 0.307 0.325 0.397 0. 0.269 0.344 0.368 SDD Runtime (s) 1.70 1.72 1.99 19.9 84.1 125.1 Trace Anything 0.256 0. 0.319 6.19 1.37 2.3 on the TAPVid-3D [26] benchmark. For each subset of TAPVid-3D (ADT, DriveTrack, and PStudio), we sample 50 videos of 60 frames each, using every other frame as input, and report APD3D (average percent of points within threshold, measuring spatial accuracy) and AJ (average Jaccard, capturing both spatial and occlusion correctness). Table Quantitative results on 3D tracking. Best in bold, second-best underlined. ADT DriveTrack PStudio Method APD3D AJ APD3D AJ APD3D AJ Runtime (s) VGGT + CoTracker St4RTrack POMATO SpaTracker Trace Anything 8.9 15.2 18.2 18.3 20.5 9.7 13.4 13.6 17.4 15.6 6.2 8.5 11. 16.0 15.5 5.4 7.4 7.8 10.1 9.6 8.6 7.2 12.2 16.2 16.3 5.8 6.9 8.3 10. 10.8 172.4 18.9 69.2 191.1 2.1 In Table B, we present these quantitative results. Notably, SpaTracker [63] is designed and trained for 3D tracking. Our approach remains competitive, surpassing it on some metrics and running orders of magnitude faster, as SpaTracker is limited to fixed number of query points per run, whereas our model performs per-pixel tracking in single forward pass. C.4 Ablation Study Table presents ablation studies on our Trace Anything benchmark, evaluating both the choice of geometric backbone and the type of parametric curve. For the geometric backbone, we compare the effect of initializing the image encoder and fusion transformer with different pretrained models, including Fast3R [65], VGGT [55], and None (following the Fast3R architecture but with random initialization). For the parametric curve types, we evaluate polynomial curves1 as well as Bézier and B-spline curves with varying numbers of control points. Table Ablation study on Trace Anything benchmark. CA is reported in 102 and SDD in 103. Best in bold, second-best underlined. Backbone Curve Type None Fast3R Fast3R Fast3R Fast3R Fast3R Fast3R VGGT B-Spline (10 control points) Polynomial (degree 3) Bezier (4 control points) Bezier (10 control points) B-Spline (4 control points) B-Spline (7 control points) B-Spline (10 control points) B-Spline (10 control points) EPEmix EPEsta EPEdyn CA 8.17 9.19 0.472 0.619 0.299 0.238 0.281 0. 0.416 0.582 0.271 0.224 0.264 0.229 0.505 0.673 0.312 0.319 0.330 0.317 2.3 1.8 SDD Runtime (s) 1.08 1.10 1.11 1.08 1.08 1.11 1.7 2.5 1.7 2.1 2.3 7. 5.08 6.13 6.01 5.81 5.09 6.11 0.234 0.236 0.218 0.221 0.295 0.276 1.06 1.07 1Although our method restricts parametric curves to control-pointbased ones such as Bézier and B-spline, we experimented with polynomial curves during the early development phase. 22 As shown in Table C, polynomial curves underperform because their parameters lack the clear geometric and physical interpretability. In contrast, B-spline curves with ten control points achieve the best overall performance, and accuracy generally improves as the number of control points increases. For the backbone, training without pretrained initialization struggles to converge. Compared with Fast3R, VGGT yields modest gains on certain metrics but incurs substantially higher runtime. Nonetheless, we observe VGGT can be beneficial in settings that demand fine structural detail or involve large-baseline scenarios. Based on these results, we adopt Fast3R with B-spline curves (10 control points) as the default configuration in Trace Anything."
        },
        {
            "title": "D Limitations",
            "content": "Since Trace Anything is trained for trajectory field estimation, we rely on synthetic data to obtain dense Incorporating partial annotations. This inevitably introduces domain gap with real-world scenarios. annotations from real data may help bridge this gap and represents promising direction for future work. Our parametric curve representation, with limited number of control points, has restricted expressive power for highly complex motions. In such cases, we mitigate the issue by clipping trajectories into fixed window sizes or downsampling frames. However, these strategies may fail in scenarios such as repeated back-and-forth motion, and performance also degrades as the number of frames increases. more fundamental solution likely requires training with larger-scale datasets with high quality. As the first attempt at dense per-pixel trajectory field estimation, our approach offers efficiency advantages but may be less precise than sparse 3D tracking methods [63, 64]. Incorporating fine-grained point-level estimation from such methods into our framework could be an interesting direction for future research."
        },
        {
            "title": "E LLM Usage Declarations",
            "content": "We declare that Large Language Models (LLMs) were used in limited capacity during the preparation of this manuscript. Specifically, LLMs were employed for grammar checking, word choice refinement, and typo correction. All core technical contributions, experimental design, analysis, and conclusions are entirely our own. The use of LLMs did not influence the scientific methodology, result interpretation, or theoretical contributions of this research. 23 Figure Projected 2D trajectories overlaid on the first input frame. 24 Figure Dynamic mask estimation. Figure 4D reconstruction and scene flow from single image pair. From non-consecutive image pair in the Spring dataset, our method recovers both the 4D reconstruction and the scene flow, with and components color-coded for visualization. 25 Figure Estimated camera poses over the 4D reconstruction. 26 Figure Qualitative comparison on DAVIS [41]. Our method better recovers fine details and handles complex motion while disentangling static and dynamic objects."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Dartmouth College",
        "HKUST",
        "Zhejiang University"
    ]
}