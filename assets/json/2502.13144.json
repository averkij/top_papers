{
    "paper_title": "RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning",
    "authors": [
        "Hao Gao",
        "Shaoyu Chen",
        "Bo Jiang",
        "Bencheng Liao",
        "Yiang Shi",
        "Xiaoyang Guo",
        "Yuechuan Pu",
        "Haoran Yin",
        "Xiangyu Li",
        "Xinbang Zhang",
        "Ying Zhang",
        "Wenyu Liu",
        "Qian Zhang",
        "Xinggang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing end-to-end autonomous driving (AD) algorithms typically follow the Imitation Learning (IL) paradigm, which faces challenges such as causal confusion and the open-loop gap. In this work, we establish a 3DGS-based closed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS techniques, we construct a photorealistic digital replica of the real physical world, enabling the AD policy to extensively explore the state space and learn to handle out-of-distribution scenarios through large-scale trial and error. To enhance safety, we design specialized rewards that guide the policy to effectively respond to safety-critical events and understand real-world causal relationships. For better alignment with human driving behavior, IL is incorporated into RL training as a regularization term. We introduce a closed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS environments. Compared to IL-based methods, RAD achieves stronger performance in most closed-loop metrics, especially 3x lower collision rate. Abundant closed-loop results are presented at https://hgao-cv.github.io/RAD."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 4 4 1 3 1 . 2 0 5 2 : r RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning Hao Gao1, Shaoyu Chen1,2, Bo Jiang1, Bencheng Liao1, Yiang Shi1, Xiaoyang Guo2 Yuechuan Pu2 Haoran Yin2 Xiangyu Li2 Xinbang Zhang2 Ying Zhang2 Wenyu Liu1 Qian Zhang2 Xinggang Wang1,(cid:0) 1 Huazhong University of Science & Technology 2 Horizon Robotics https://hgao-cv.github.io/RAD"
        },
        {
            "title": "Abstract",
            "content": "Existing end-to-end autonomous driving (AD) algorithms typically follow the Imitation Learning (IL) paradigm, which faces challenges such as causal confusion and the In this work, we establish 3DGSopen-loop gap. based closed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS techniques, we construct photorealistic digital replica of the real physical world, enabling the AD policy to extensively explore the state space and learn to handle out-of-distribution scenarios through large-scale trial and error. To enhance safety, we design specialized rewards that guide the policy to effectively respond to safety-critical events and understand real-world causal relationships. For better alignment with human driving behavior, IL is incorporated into RL training as regularization term. We introduce closed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS environments. Compared to IL-based methods, RAD achieves stronger performance in most closed-loop metrics, especially 3 lower collision rate. Abundant closed-loop results are presented at https://hgao-cv.github. io/RAD. Figure 1. Different training paradigms of end-to-end autonomous driving (AD). 1. Introduction End-to-end autonomous driving (AD) is currently trending topic in both academia and industry. It replaces modularized pipeline with holistic one by directly mapping sensory inputs to driving actions, offering advantages in system simplicity and generalization ability. Most existing end-toend AD algorithms [2, 11, 17, 22, 28, 38, 43, 49] follow the Imitation Learning (IL) paradigm, which trains neural network to mimic human driving behavior. However, Intern of Horizon Robotics. Project lead. (cid:0) Corresponding author. despite their simplicity, IL-based methods face significant challenges in real-world deployment. One key issue is causal confusion. IL trains network to replicate human driving policies by learning from demonstrations. However, this paradigm primarily captures correlations rather than causal relationships between observations (states) and actions. As result, IL-trained policies may struggle to identify the true causal factors behind planning decisions, leading to shortcut learning [8], e.g., merely extrapolating future trajectories from historical ones [23, 47]. Furthermore, since IL training data predominantly consists of common driving behaviors and does not adequately cover long-tailed distributions, IL-trained poli1 cies tend to converge to trivial solutions, lacking sufficient sensitivity to safety-critical events such as collisions. Another major challenge is the gap between openloop training and closed-loop deployment. IL policies are trained in an open-loop manner using well-distributed driving demonstrations. However, real-world driving is closed-loop process where minor trajectory errors at each step accumulate over time, leading to compounding errors and out-of-distribution scenarios. IL-trained policies often struggle in these unseen situations, raising concerns about their robustness. straightforward solution to these problems is to perform closed-loop Reinforcement Learning (RL) training, which requires driving environment that can interact with the AD policy. However, using real-world driving environments for closed-loop training poses prohibitive safety risks and operational costs. Simulated driving environments with sensor data simulation capabilities [6, 14] (which are required for end-to-end AD) are typically built on game engines [7, 39] but fail to provide realistic sensor simulation results. In this work, we establish 3DGS-based [18] closedloop RL training paradigm. Leveraging 3DGS techniques, we construct photorealistic digital replica of the real world, where the AD policy can extensively explore the state space and learn to handle out-of-distribution situations through large-scale trial and error. To ensure effective responses to safety-critical events and better understanding of real-world causations, we design specialized safetyrelated rewards. However, RL training presents several critical challenges, which this paper addresses. One significant challenge is the Human Alignment Problem. The exploration process in RL can lead to policies that deviate from human-like behavior, disrupting the smoothness of the action sequence. To address this, we incorporate imitation learning as regularization term during RL training, helping to maintain similarity to human driving behavior. As illustrated in Fig. 1, RL and IL work together to optimize the AD policy: RL enhances IL by addressing causation and the open-loop gap, while IL improves RL by ensuring better human alignment. Another major challenge is the Sparse Reward Problem. RL often suffers from sparse rewards and slow convergence. To alleviate this issue, we introduce dense auxiliary objectives related to collisions and deviations, which help constrain the full action distribution. Additionally, we streamline and decouple the action space to reduce the exploration cost associated with RL. To validate the effectiveness of our approach, we construct closed-loop evaluation benchmark comprising diverse, unseen 3DGS environments. Our method, RAD, outperforms IL-based approaches across most closed-loop metrics, notably achieving collision rate that is 3 lower. The contributions of this work are summarized as follows: We propose the first 3DGS-based RL framework for training end-to-end AD policy. The reward, action space, optimization objective, and interaction mechanism are specially designed to enhance training efficiency and effectiveness. We combine RL and IL to synergistically optimize the AD policy. RL complements IL by modeling the causations and narrowing the open-loop gap, while IL complements RL in terms of human alignment. We validate the effectiveness of RAD on closedloop evaluation benchmark consisting of diverse, unseen 3DGS environments. RAD achieves stronger performance in closed-loop evaluation, particularly 3 lower collision rate, compared to IL-based methods. 2. Related Work Dynamic Scene Reconstruction. Implicit neural representations have dominated novel view synthesis and dynamic scene reconstruction, with methods like UniSim [46], MARS [44], and NeuRAD [40] leveraging neural scene graphs to enable structured scene decomposition. However, these approaches rely on implicit representations, leading to slow rendering speeds that limit their practicality in real-time applications. In contrast, 3D Gaussian Splatting (3DGS) [18] has emerged as an efficient alternative, offering significantly faster rendering while maintaining high visual fidelity. Recent works have explored its potential for dynamic scene reconstruction, particularly in autonomous driving scenarios. StreetGaussians [45], DrivingGaussians [51], and HUGSIM [50] have demonstrated the effectiveness of Gaussian-based representations in modeling urban environments. These methods achieve superior rendering performance while maintaining controllability by explicitly decomposing scenes into structured components. However, these works primarily leverage 3DGS for closedloop evaluation. In this work, we incorporate 3DGS into an RL training framework. End-to-End Autonomous Driving. Learning-based planning has shown great potential recently due to its datadriven nature and impressive performance with increasing amounts of data. UniAD [12] demonstrates the potential of end-to-end autonomous driving by integrating multiple perception tasks to enhance planning performance. VAD [16] further explores the use of compact vectorized scene representations to improve efficiency. series of works [3, 9, 20, 24, 33, 42, 43, 49] also adopt the singletrajectory planning paradigm and further enhance planning performance. VADv2 [2] shifts the paradigm towards multimode planning by modeling the probabilistic distribution of planning vocabulary. Hydra-MDP [22] improves the scorFigure 2. Overall framework of RAD. RAD takes three-stage training paradigm. In the perception pre-training, ground-truths of map and agent are used to guide instance-level tokens to encode corresponding information. In the planning pre-training stage, large-scale driving demonstrations are used to initialize the action distribution. In the reinforced post-training stage, RL and IL synergistically finetune the AD policy. ing mechanism of VADv2 by introducing additional supervision from rule-based scorer. SparseDrive [38] explores an alternative BEV-free solution. DiffusionDrive [28] proposes truncated diffusion policy that denoises an anchored Gaussian distribution to multi-mode driving action distribution. Most of the end-to-end methods follow the datadriven IL-training paradigm. In this work, we present an RL-training paradigm based on 3DGS. Reinforcement Learning. Reinforcement Learning is promising technique that has not been fully explored. AlphaGo [36] and AlphaGo Zero [37] have demonstrated the power of Reinforcement Learning in the game of Go. Recently, OpenAI O1 [32] and Deepseek-R1 [4] have leveraged Reinforcement Learning to develop reasoning abilities. Several studies have also applied Reinforcement Learning in autonomous driving [1, 13, 31, 41, 48]. However, these studies are based on non-photorealistic simulators (such as CARLA [6]) or do not involve end-to-end driving algorithms, as they require perfect perception results as input. To the best of our knowledge, RAD is the first work to train an end-to-end AD agent using Reinforcement Learning in photorealistic 3DGS environment. 3 3. RAD 3.1. End-to-End Driving Policy The overall framework of RAD is depicted in Fig. 2. RAD takes multi-view image sequences as input, transforms the sensor data into scene token embeddings, outputs the probabilistic distribution of actions, and samples an action to control the vehicle. BEV Encoder. We first employ BEV encoder [21] to transform multi-view image features from the perspective view to the Birds Eye View (BEV), obtaining feature map in the BEV space. This feature map is then used to learn instance-level map features and agent features. Map Head. Then we utilize group of map tokens [2527] to learn the vectorized map elements of the driving scene from the BEV feature map, including lane centerlines, lane dividers, road boundaries, arrows, traffic signals, etc. Agent Head. Besides, group of agent tokens [15] is adopted to predict the motion information of other traffic participants, including location, orientation, size, speed, and multi-mode future trajectories. Image Encoder. Apart from the above instance-level map and agent tokens, we also use an individual image encoder [5, 10] to transform the original images into image longitudinal action ay: π(ax s) =softmax(MLP(ϕ(Eplan, Escene) + Enavi + Estate)), π(ay s) =softmax(MLP(ϕ(Eplan, Escene) + Enavi + Estate)), (1) where Eplan, Enavi, Estate, and the output of MLP are all of the same dimension (1 D). The planning head also outputs the value functions Vx(s) and Vy(s), which estimate the expected cumulative rewards for the lateral and longitudinal actions, respectively: Vx(s) = MLP(ϕ(Eplan, Escene) + Enavi + Estate), Vy(s) = MLP(ϕ(Eplan, Escene) + Enavi + Estate). (2) The value functions are used in RL training (Sec. 3.5). 3.2. Training Paradigm We adopt three-stage training paradigm: perception pre-training, planning pre-training, and reinforced posttraining, as shown in Fig. 2. Perception Pre-Training. Information in the image is sparse and low-level. In the first stage, the map head and the agent head explicitly output map elements and agent motion information, which are supervised with ground-truth labels. Consequently, map tokens and agent tokens implicitly encode the corresponding high-level information. In this stage, we only update the parameters of the BEV encoder, the map head, and the agent head. Planning Pre-Training. In the second stage, to prevent the unstable cold start of RL training, IL is first performed to initialize the probabilistic distribution of actions based on large-scale real-world driving demonstrations from expert drivers. In this stage, we only update the parameters of the image encoder and the planning head, while the parameters of the BEV encoder, map head, and agent head are frozen. The optimization objectives of perception tasks and planning tasks may conflict with each other. However, with the training stage and parameters decoupled, such conflicts are mostly avoided. Reinforced Post-Training. In the reinforced post-training, RL and IL synergistically fine-tune the distribution. RL aims to guide the policy to be sensitive to critical risky events and adaptive to out-of-distribution situations. IL serves as the regularization term to keep the policys behavior similar to that of humans. We select large amount of risky dense-traffic clips from collected driving demonstrations. For each clip, we train an independent 3DGS model that reconstructs the clip and serves as digital driving environment. As shown in Fig. 3, we set parallel workers. Each worker randomly samples Figure 3. Post-training. workers parallelly run. The generated rollout data (st, at, rt+1, st+1, ...) are recorded in rollout buffer. Rollout data and human driving demonstrations are used in RLand IL-training steps to fine-tune the AD policy synergistically. tokens. These image tokens provide dense and rich scene information for planning, complementary to the instancelevel tokens. Action Space. To accelerate the convergence of RL training, we design decoupled discrete action representation. We divide the action into two independent components: lateral action and longitudinal action. The action space is constructed over short 0.5-second time horizon, during which the vehicles motion is approximated by assuming constant linear and angular velocities. Under this assumption, the lateral action ax and longitudinal action ay can be directly computed based on the current linear and angular velocities. By combining decoupling with limited temporal scope and simplified motion model, our approach effectively reduces the dimensionality of the action space, accelerating training convergence. Planning Head. We use Escene to denote the scene representation, which consists of map tokens, agent tokens, and image tokens. We initialize planning embedding denoted as Eplan. cascaded Transformer decoder ϕ takes the planning embedding Eplan as the query and the scene representation Escene as both key and value. The output of the decoder ϕ is then combined with navigation information Enavi and ego state Estate to output the probabilistic distributions of the lateral action ax and the 4 3DGS environment and begins rollout, i.e., the AD policy controls the ego vehicle to move and iteratively interacts with the 3DGS environment. After the rollout process of this 3DGS environment ends, the generated rollout data (st, at, rt+1, st+1, ...) are recorded in rollout buffer, and the worker will sample new 3DGS environment for another round of rollout. As for policy optimization, we iteratively perform RLtraining steps and IL-training steps. For RL-training steps, we sample data from the rollout buffer and follow the Proximal Policy Optimization (PPO) framework [35] to update the AD policy. For IL-training steps, we use real-world driving demonstrations to update the policy. After fixed number of training steps, the updated AD policy is sent to every worker to replace the old one, to avoid distribution shift between data collection and optimization. We only update the parameters of the image encoder and the planning head. The parameters of the BEV encoder, the map head, and the agent head are frozen. The detailed RL design is presented below. 3.3. Interaction Mechanism between AD Policy and 3DGS Environment In the 3DGS environment, the ego vehicle acts according to the AD policy. Other traffic participants act according to real-world data in log-replay manner. simplified kinematic bicycle model is employed to iteratively update the ego vehicles pose at every seconds as follows: t+1 = xw xw t+1 = yw yw t+1 = ψw ψw + vt cos (ψw + vt sin (ψw vt + ) t, ) t, tan (δt) t, (3) and yw where xw denote the position of the ego vehicle relative to the world coordinate; ψw is the heading angle that defines the vehicles orientation with respect to the world x-coordinate; vt is the linear velocity of the ego vehicle; δt is the steering angle of the front wheels; and is the wheelbase, i.e., the distance between the front and rear axles. , ay During the rollout process, the AD policy outputs actions , ay (ax ) for 0.5-second time horizon at time step t. We derive the linear velocity vt and steering angle δt based on (ax ). Based on the kinematic model in Eq. 3, the pose of the ego vehicle in the world coordinate system is updated ) to pt+1 = (xw from pt = (xw Based on the updated pt+1, the 3DGS environment computes the new ego vehicles state st+1. The updated pose pt+1 and state st+1 serve as the input for the next iteration of the inference process. t+1, ψw t+1, yw , ψw , yw t+1). The 3DGS environment also generates rewards (Sec. 3.4) according to multi-source information (including trajectories of other agents, map information, the expert trajectory of the ego vehicle, and the parameters of Gaussians), 5 Figure 4. Example diagram of four types of reward sources. (1): Collision with dynamic obstacle ahead triggers reward rdc. (2): Hitting static roadside obstacle incurs reward rsc. (3): Moving onto the curb exceeds the positional deviation threshold dmax, triggering reward rpd. (4): Drifting toward the adjacent lane exceeds the heading deviation threshold ψmax, triggering reward rhd. which are used to optimize the AD policy (Sec. 3.5). 3.4. Reward Modeling The reward is the source of the training signal, which determines the optimization direction of RL. The reward function is designed to guide the ego vehicles behavior by penalizing unsafe actions and encouraging alignment with the It is composed of four reward compoexpert trajectory. nents: (1) collision with dynamic obstacles, (2) collision with static obstacles, (3) positional deviation from the expert trajectory, and (4) heading deviation from the expert trajectory: = {rdc, rsc, rpd, rhd}. (4) As illustrated in Fig. 4, these reward components are triggered under specific conditions. In the 3DGS environment, dynamic collision is detected if the ego vehicles bounding box overlaps with the annotated bounding boxes of dynamic obstacles, triggering negative reward rdc. Similarly, static collision is identified when the ego vehicles bounding box overlaps with the Gaussians of static obstacles, resulting in negative reward rsc. Positional deviation is measured as the Euclidean distance between the ego vehicles current position and the closest point on the expert trajectory. deviation beyond predefined threshold dmax incurs negative reward rpd. Heading deviation is calculated as the angular difference between the ego vehicles current heading angle ψt and the expert trajectorys matched heading angle ψexpert. deviation beyond threshold ψmax results in negative reward rhd. Any of these events, including dynamic collision, static collision, excessive positional deviation, or excessive heading deviation, triggers immediate episode termination. Because after such events occur, the 3DGS environment typically generates noisy sensor data, which is detrimental to RL training. 3.5. Policy Optimization In the closed-loop environment, the error in each single step accumulates over time. The aforementioned rewards are not only caused by the current action but also by the actions of the preceding steps. The rewards are propagated forward with Generalized Advantage Estimation (GAE) [34] to optimize the action distribution of the preceding steps. Specifically, for each time step t, we store the current state st, action at, reward rt, and the estimate of the value (st). Based on the decoupled action space, and considering that different rewards have different correlations to lateral and longitudinal actions, the reward rt is divided into and longitudinal reward ry lateral reward rx : + rpd rx = rsc ry = rdc . + rhd , (5) Similarly, the value function (st) is decoupled into two components: Vx(st) for the lateral dimension and Vy(st) for the longitudinal dimension. These value functions estimate the expected cumulative rewards for the lateral and longitudinal actions, respectively. The advantage estimates ˆAx are then computed as follows: and ˆAy δx = rx + γVx(st+1) Vx(st), = ry δy + γVy(st+1) Vy(st), (cid:88) ˆAx = (γλ)lδx t+l, (6) l=0 (cid:88) (γλ)lδy t+l, ˆAy = l=0 and δy where δx are the temporal difference errors for the lateral and longitudinal dimensions, γ is the discount factor, and λ is the GAE parameter that controls the trade-off between bias and variance. To further clarify the relationship between the advantage estimates and the reward components, we decompose ˆAx and ˆAy based on the reward decomposition in Eq. 5 and the advantage estimation in Eq. 6. Specifically, we derive the following decomposition: + ˆApd = ˆAsc = ˆAdc , + ˆAhd , ˆAx ˆAy (7) where ˆAsc lisions, ˆApd sitional deviations, ˆAhd imizing heading deviations, and ˆAdc mate for avoiding dynamic collisions. is the advantage estimate for avoiding static colis the advantage estimate for minimizing pois the advantage estimate for minis the advantage estiThese advantage estimates are used to guide the update of the AD policy πθ, following the PPO framework [35]. By leveraging the decomposed advantage estimates ˆAx and ˆAy , we can independently optimize the lateral and longitudinal dimensions of the policy. This is achieved by defining separate objective functions LCLIP (θ) for each dimension, as follows: (cid:16) (θ) and LCLIP (cid:17)(cid:105) (cid:104) ρx ˆAx , clip(ρx , 1 ϵx, 1 + ϵx) ˆAx (θ) = Et LPPO min ,"
        },
        {
            "title": "LPPO\ny",
            "content": "(θ) = Et (cid:104) min (cid:16) ρy ˆAy , clip(ρy , 1 ϵy, 1 + ϵy) ˆAy (cid:17)(cid:105) , LPPO(θ) = LPPO (θ) + LPPO (θ), (8) = πθ(ax st) where ρx st) is the importance sampling ratio πθold (ax st) for the lateral dimension, ρy st) is the importance sampling ratio for the longitudinal dimension, ϵx and ϵy are small constants that control the clipping range for the lateral and longitudinal dimensions, ensuring stable policy updates. = πθ(ay πθold (ay The clipped objective function LPPO(θ) prevents excessively large updates to the policy parameters θ, thereby maintaining training stability. 3.6. Auxiliary Objective RL usually faces the challenge of sparse rewards, which makes the convergence process unstable and slow. To speed up convergence, we introduce auxiliary objectives that provide dense guidance to the entire action distribution. The auxiliary objectives are designed to penalize undesirable behaviors by incorporating specific reward sources, including dynamic collisions, static collisions, positional deviations, and heading deviations. These objectives are computed based on the actions ax,old selected by the old AD policy πθold at time step t. To facilitate the evaluation of these actions, we separate the probability distribution of the action into four parts: and ay,old πdec = (cid:88) πθ(ay st), πacc = πleft = πright = <ay,old ay (cid:88) >ay,old ay (cid:88) <ax,old ax (cid:88) >ax,old ax πθ(ay st), πθ(ax st), πθ(ax st). (9) Here, πdec actions, πacc tion actions, πleft ward steering actions, and πright ability of rightward steering actions. represents the total probability of deceleration represents the total probability of accelerax represents the total probability of leftrepresents the total probx RL:IL CR DCR SCR DR PDR HDR ADD Long. Jerk Lat. Jerk 0:1 1:0 2:1 4:1 8:1 0.229 0.143 0.137 0.089 0.125 0.211 0.128 0.125 0.080 0.116 0.018 0.015 0.012 0.009 0.009 0.066 0.080 0.059 0.063 0.084 0.039 0.065 0.050 0.042 0. 0.027 0.015 0.009 0.021 0.039 0.238 0.345 0.274 0.257 0.323 3.928 4.204 4.538 4.495 5.285 0.103 0.085 0.092 0.082 0.115 Table 1. Ablation on RL-to-IL step mixing ratios in the reinforced post-training stage. Dynamic Collision Auxiliary Objective. The dynamic collision auxiliary objective adjusts the longitudinal control action ay based on the location of potential collisions relIf collision is detected ahead, ative to the ego vehicle. the policy prioritizes deceleration actions (ay ); if collision is detected behind, it encourages acceleration actions (ay ). To formalize this behavior, we define directional factor fdc: > ay,old < ay,old > ax,old the policy promotes rightward corrections (ax ); if it deviates rightward, it promotes leftward corrections (ax ). We formalize this with directional factor fpd: < ax,old fpd = (cid:40) 1 1 if ego vehicle deviates leftward, if ego vehicle deviates rightward. (14) The auxiliary objective for positional deviation correcfdc = (cid:40) 1 1 if the collision is ahead, if the collision is behind. tion is: (10) Lpd(θx) = Et (cid:104) ˆApd fpd (πright πleft ) (cid:105) , (15) The auxiliary objective for dynamic collision avoidance is defined as: Ldc(θy) = Et (cid:104) ˆAdc fdc (πdec πacc ) (cid:105) , (11) where ˆAdc avoidance. is the advantage estimate for dynamic collision Static Collision Auxiliary Objective. The static collision auxiliary objective adjusts the steering control action ax based on the proximity to static obstacles. If the static obstacle is detected on the left side, the policy promotes rightt > ax,old ward steering actions (ax ); if the static obstacle is detected on the right side, it promotes leftward steering actions (ax ). To formalize this behavior, we define directional factor fsc: < ax,old fsc = (cid:40) 1 1 if static obstacle is on the left, if static obstacle is on the right. (12) The auxiliary objective for static collision avoidance is where ˆApd estimates the advantage of trajectory alignment. Heading Deviation Auxiliary Objective. The heading deviation auxiliary objective adjusts the steering control action ax based on the angular difference between the ego vehicles current heading and the experts reference heading. If the ego vehicle deviates counterclockwise, the policy promotes clockwise corrections (ax ); if it deviates clockwise, it promotes counterclockwise corrections < ax,old (ax ). To formalize this behavior, we define direct tional factor fhd: (cid:40) 1 1 if ego vehicle deviates clockwise, if ego vehicle deviates counterclockwise. > ax,old fhd = (16) The auxiliary objective for heading deviation correction is then defined as: Lhd(θx) = Et (cid:104) ˆAhd fhd (πright (cid:105) πleft ) , (17) where ˆAhd is the advantage estimate for heading alignment. Overall Auxiliary Objectives. The overall auxiliary objectives are weighted sum of the individual objectives: defined as: Lsc(θx) = Et (cid:104) ˆAsc fsc (πright (cid:105) πleft ) , (13) Laux(θ) =λ1Ldc(θy) + λ2Lsc(θx)+ λ3Lpd(θx) + λ4Lhd(θx), (18) where ˆAsc avoidance. is the advantage estimate for static collision where λ1, λ2, λ3, and λ4 are weighting coefficients that balance the contributions of each auxiliary objective. Positional Deviation Auxiliary Objective. The positional deviation auxiliary objective adjusts the steering control action ax based on the ego vehicles lateral deviation from the expert trajectory. If the ego vehicle deviates leftward, Optimization Objective. The final optimization objective combines the clipped PPO objective with the auxiliary objective: L(θ) = LPPO(θ) + Laux(θ). (19) 7 ID 1 2 3 4 5 Heading Dynamic Collision Collision Deviation Deviation Position Static CR DCR SCR DR PDR HDR ADD Long. Jerk Lat. Jerk 0.172 0.238 0.146 0.151 0.166 0.089 0.154 0.217 0.128 0.142 0.157 0.080 0.018 0.021 0.018 0.009 0.009 0.009 0.092 0.090 0.060 0.069 0.048 0. 0.033 0.045 0.030 0.042 0.036 0.042 0.059 0.045 0.030 0.027 0.012 0.021 0.259 0.241 0.263 0.303 0.243 0.257 4.211 3.937 3.729 3.938 3.334 4.495 0.095 0.098 0.083 0.079 0.067 0.082 Table 2. Ablation on reward sources. The table shows the impact of different reward components on performance. ID PPO Obj. 1 2 3 4 5 6 7 8 Dynamic Col. Heading Dev. Auxiliary Obj. Auxiliary Obj. Auxiliary Obj. Auxiliary Obj. Position Dev. Static Col. CR DCR SCR DR PDR HDR ADD Long. Jerk Lat. Jerk 0.249 0.178 0.137 0.169 0.149 0.128 0.187 0.089 0.223 0.163 0.125 0.151 0.134 0.119 0.175 0. 0.026 0.015 0.012 0.018 0.015 0.009 0.012 0.009 0.077 0.151 0.157 0.075 0.063 0.066 0.077 0.063 0.047 0.101 0.145 0.042 0.057 0.030 0.056 0.042 0.030 0.050 0.012 0.033 0.006 0.036 0.021 0.021 0.266 0.301 0.296 0.254 0.324 0.254 0.309 0.257 4.209 3.906 3.419 4.450 3.980 4.102 5.014 4. 0.104 0.085 0.071 0.098 0.086 0.092 0.112 0.082 Table 3. Ablation on auxiliary objectives. The table shows the impact of different auxiliary objectives on performance. 4. Experiments 4.1. Experimental Settings Dataset and Benchmark. We collect 2000h of expert human driving demonstrations in the real physical world. We get ground-truths of maps and agents in these driving demonstrations through low-cost automated annotation pipeline. We use the map and agent labels as supervision for the first-stage perception pre-training. For the secondstage planning pre-training, we use the odometry information of the ego vehicle as supervision. For the third-stage reinforced post-training, we select 4305 critical dense-traffic clips of high collision risks from collected driving demonstrations and reconstruct these clips into 3DGS environments. Of these, 3968 3DGS environments are used for RL training, and the other 337 3DGS environments are used as closed-loop evaluation benchmarks. Metric. We evaluate the performance of the AD policy using nine key metrics. Dynamic Collision Ratio (DCR) and Static Collision Ratio (SCR) quantify the frequency of collisions with dynamic and static obstacles, respectively, with their sum represented as the Collision Ratio (CR). Positional Deviation Ratio (PDR) measures the ego vehicles deviation from the expert trajectory with respect to position, while Heading Deviation Ratio (HDR) evaluates the ego vehicles consistency to the expert trajectory with respect to the forward direction. The overall deviation is quantified by the Deviation Ratio (DR), defined as the sum of PDR and HDR. Average Deviation Distance (ADD) quantifies the mean closest distance between the ego vehicle and the expert trajectory before any collisions or deviations occur. Additionally, Longitudinal Jerk (Long. Jerk) and Lateral Jerk (Lat. Jerk) assess driving smoothness by measuring acceleration changes in the longitudinal and lateral directions. CR, DCR, and SCR mainly reflect the policys safety, and ADD reflects the trajectory consistency between the AD policy and human drivers. 4.2. Ablation Study To evaluate the impact of different design choices in RAD, we conduct three ablation studies. These studies examine the balance between RL and IL, the role of different reward sources, and the effect of auxiliary objectives. RL-IL Ratio Analysis. We first analyze the effect of different RL-to-IL step mixing ratios (Tab. 1). pure IL policy (0:1) results in the highest CR (0.229) but the lowest ADD (0.238), indicating strong trajectory consistency but poor safety. In contrast, pure RL policy (1:0) significantly reduces CR (0.143) but increases ADD (0.345), suggesting improved safety at the cost of trajectory deviation. The best balance is achieved at 4:1 ratio, which results in the lowest CR (0.089) while maintaining relatively low ADD (0.257). Further increasing RL dominance (e.g., 8:1) leads to deteriorated ADD (0.323) and higher jerk, implying reduced trajectory smoothness. Reward Source Analysis. We analyze the influence of different reward components (Tab. 2). Policies trained with only partial reward terms (e.g., ID 1, 2, 3, 4, 5) exhibit higher collision rates (CR) compared to the full reward setup (ID 6), which achieves the lowest CR (0.089) while maintaining stable ADD (0.257). This demonstrates that well-balanced reward function, incorporating all reward"
        },
        {
            "title": "Method",
            "content": "CR DCR SCR DR PDR HDR ADD Long. Jerk Lat. Jerk VAD [17] GenAD [49] VADv2 [2] RAD 0.335 0.341 0.270 0.089 0.273 0.299 0.240 0. 0.062 0.042 0.030 0.009 0.314 0.291 0.243 0.063 0.255 0.160 0.139 0.042 0.059 0.131 0.104 0.021 0.304 0.265 0.273 0.257 5.284 11.37 7.782 4. 0.550 0.320 0.171 0.082 Table 4. Closed-loop quantitative comparisons with other IL-based methods on the 3DGS dense-traffic evaluation benchmark. Figure 5. Closed-loop qualitative comparisons between the IL-only policy and RAD in 3DGS environments. Rows 1-2 correspond to Yield to Pedestrians. Rows 3-4 correspond to Unprotected Left-turn. terms, effectively enhances both safety and trajectory consistency. Among the partial reward configurations, ID 2, which omits the dynamic collision reward term, exhibits the highest CR (0.238), indicating that the absence of this term significantly impairs the models ability to avoid dynamic obstacles, resulting in higher collision rate. Our ablation studies highlight the importance of combining RL and IL, using comprehensive reward function, and implementing structured auxiliary objectives. The optimal RL-IL ratio (4:1) and the full reward and auxiliary setups consistently yield the lowest CR while maintaining stable ADD, ensuring both safety and trajectory consistency. Auxiliary Objective Analysis. Finally, we examine the impact of auxiliary objectives (Tab. 3). Compared to the full auxiliary setup (ID 8), omitting any auxiliary objective increases CR, with significant rise observed when all auxiliary objectives are removed. This highlights their collective role in enhancing safety. Notably, ID 1, which retains all auxiliary objectives but excludes the PPO objective, results in CR of 0.187. This value is higher than that of ID 8, indicating that while auxiliary objectives help reduce collisions, they are most effective when combined with the PPO objective. 4.3. Comparisons with Existing Methods As presented in Tab. 4, we compare RAD with other endto-end autonomous driving methods in the proposed 3DGSbased closed-loop evaluation. For fair comparisons, all the methods are trained with the same amount of human driving demonstrations. The 3DGS environments for the RL training in RAD are also based on these data. RAD achieves better performance compared to IL-based methods in most metrics. Especially in terms of CR, RAD achieves 3 lower collision rate, demonstrating that RL helps the AD policy learn general collision avoidance ability. 9 4.4. Qualitative Comparisons We provide qualitative comparisons between the IL-only AD policy (without reinforced post-training) and RAD, as shown in Fig. 5. The IL-only method struggles in dynamic environments, frequently failing to avoid collisions with moving obstacles or manage complex traffic situations. In contrast, RAD consistently performs well, effectively avoiding dynamic obstacles and handling challenging tasks. These results highlight the benefits of closed-loop training in the hybrid method, which enables better handling of dynamic environments. Additional visualizations are included in the Appendix (Fig. A1). 5. Limitation and Conclusion In this work, we propose the first 3DGS-based RL framework for training end-to-end AD policy. We combine RL and IL, with RL complementing IL to model the causations and narrow the open-loop gap, and IL complementing RL in terms of human alignment. This work also has some limitations. Currently the used 3DGS environments are running in non-reactive manner, i.e., other traffic participants do not react according to ego vehicles behavior but act with log replay. And the effect of 3DGS still has room for improvement, especially for rendering non-rigid pedestrians, unobserved views, and low-light scenarios. Future works will focus on solving these problems and scaling up RL to the next level."
        },
        {
            "title": "Acknowledgement",
            "content": "We would like to acknowledge Qingjie Wang, Yongjun Yu, Zehua Li, Peng Wang, Nuoya Zhou, Songlin Yang, Ruiqi Wang, Tianheng Cheng, Changze Li, Zhe Chen, and Tong Qin for discussion and assistance."
        },
        {
            "title": "References",
            "content": "[1] Dian Chen, Vladlen Koltun, and Philipp Krahenbuhl. Learning to drive from world on rails. In ICCV, 2021. 3 [2] Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, Qing Xu, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Vadv2: End-to-end vectorized autonomous driving via probabilistic planning. arXiv preprint arXiv:2402.13243, 2024. 1, 2, 9 [3] Zhili Chen, Maosheng Ye, Shuangjie Xu, Tongyi Cao, and Qifeng Chen. Ppad: Iterative interactions of prediction and In ECCV, planning for end-to-end autonomous driving. 2024. 2 [4] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [5] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3 [6] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In Conference on robot learning, pages 116. PMLR, 2017. 2, 3 [7] Epic Games. Unreal engine. unrealengine.com/, 1998. 2 https : / / www . [8] Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2020. [9] Xunjiang Gu, Guanyu Song, Igor Gilitschenski, Marco Pavone, and Boris Ivanovic. Producing and leveraging online map uncertainty in trajectory prediction. In CVPR, 2024. 2 [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2016. 3 [11] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. CVPR2023, 2022. 1 [12] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In CVPR, 2023. 2 [13] Yihan Hu, Siqi Chai, Zhening Yang, Jingyu Qian, Kun Li, Wenxin Shao, Haichao Zhang, Wei Xu, and Qiang Liu. Solving motion planning tasks with scalable generative model. In ECCV, 2024. 3 [14] Applied Intuition. Carsim. https://www.carsim. com/, 2023. [15] Bo Jiang, Shaoyu Chen, Xinggang Wang, Bencheng Liao, Tianheng Cheng, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, and Chang Huang. Perceive, interact, predict: Learning dynamic and static clues for end-to-end motion prediction. arXiv preprint arXiv:2212.02181, 2022. 3 [16] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. In ICCV, 2023. 2 [17] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. ICCV, 2023. 1, 9 [18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2 [19] Diederik Kingma and Jimmy Ba. Adam: method for arXiv preprint arXiv:1412.6980, stochastic optimization. 2014. 12, [20] Yingyan Li, Lue Fan, Jiawei He, Yuqi Wang, Yuntao Chen, Zhaoxiang Zhang, and Tieniu Tan. Enhancing end-to-end autonomous driving with latent world model. arXiv preprint arXiv:2406.08481, 2024. 2 [21] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. Bevformer: 10 Learning birds-eye-view representation from multi-camera arXiv preprint images via spatiotemporal transformers. arXiv:2203.17270, 2022. 3 [22] Zhenxin Li, Kailin Li, Shihao Wang, Shiyi Lan, Zhiding Yu, Yishen Ji, Zhiqi Li, Ziyue Zhu, Jan Kautz, Zuxuan Wu, et al. Hydra-mdp: End-to-end multimodal planning with multitarget hydra-distillation. arXiv preprint arXiv:2406.06978, 2024. 1, 2 [23] Zhiqi Li, Zhiding Yu, Shiyi Lan, Jiahan Li, Jan Kautz, Tong Lu, and Jose Alvarez. Is ego status all you need for openloop end-to-end autonomous driving? In CVPR, 2024. 1 [24] Zhiqi Li, Zhiding Yu, Shiyi Lan, Jiahan Li, Jan Kautz, Tong Lu, and Jose Alvarez. Is ego status all you need for openloop end-to-end autonomous driving? In CVPR, 2024. 2 [25] Bencheng Liao, Shaoyu Chen, Xinggang Wang, Tianheng Cheng, Qian Zhang, Wenyu Liu, and Chang Huang. Maptr: Structured modeling and learning for online vectorized hd map construction. arXiv preprint arXiv:2208.14437, 2022. [26] Bencheng Liao, Shaoyu Chen, Bo Jiang, Tianheng Cheng, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Lane graph as path: Continuity-preserving path-wise modeling for online lane graph construction. arXiv preprint arXiv:2303.08815, 2023. [27] Bencheng Liao, Shaoyu Chen, Yunchi Zhang, Bo Jiang, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Maptrv2: An end-to-end framework for online vectorized hd map construction. arXiv preprint arXiv:2308.05736, 2023. 3 [28] Bencheng Liao, Shaoyu Chen, Haoran Yin, Bo Jiang, Cheng Wang, Sixu Yan, Xinbang Zhang, Xiangyu Li, Ying Zhang, Qian Zhang, and Xinggang Wang. Diffusiondrive: Truncated diffusion model for end-to-end autonomous driving. arXiv preprint arXiv:2411.15139, 2024. 1, 3 [29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In ICCV, 2017. 12 [30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 12, [31] Yiren Lu, Justin Fu, George Tucker, Xinlei Pan, Eli Bronstein, Rebecca Roelofs, Benjamin Sapp, Brandyn White, Aleksandra Faust, Shimon Whiteson, Dragomir Anguelov, and Sergey Levine. Imitation is not enough: Robustifying imitation with reinforcement learning for challenging driving scenarios. In IROS, 2023. 3 [32] OpenAI. Openai o1. https://openai.com/o1/, 2024. 3 [33] Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multimodal fusion transformer for end-to-end autonomous driving. 2021. 2 [34] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. 6 [36] David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 2016. [37] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. Nat., 2017. 3 [38] Wenchao Sun, Xuewu Lin, Yining Shi, Chuang Zhang, Haoran Wu, and Sifa Zheng. Sparsedrive: End-to-end autonomous driving via sparse scene representation. arXiv preprint arXiv:2405.19620, 2024. 1, 3 [39] Unity Technologies. Unity. unity.com/, 2005. 2 https : / / https : / / [40] Adam Tonderski, Carl Lindstrom, Georg Hess, William Ljungbergh, Lennart Svensson, and Christoffer Petersson. Neurad: Neural rendering for autonomous driving. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1489514904, 2023. 2 [41] Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde. learning for urban End-to-end model-free reinforcement driving using implicit affordances. In CVPR, 2020. 3 [42] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. In CVPR, 2024. [43] Xinshuo Weng, Boris Ivanovic, Yan Wang, Yue Wang, and Marco Pavone. Para-drive: Parallelized architecture for realtime autonomous driving. In CVPR, 2024. 1, 2 [44] Zirui Wu, Tianyu Liu, Liyi Luo, Zhide Zhong, Jianteng Chen, Hongmin Xiao, Chao Hou, Haozhe Lou, Yuan-Hao Chen, Runyi Yang, Yuxin Huang, Xiaoyu Ye, Zike Yan, Yongliang Shi, Yiyi Liao, and Hao Zhao. Mars: An instanceaware, modular and realistic simulator for autonomous driving. ArXiv, 2023. 2 [45] Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, and Sida Peng. Street gaussians: Modeling dynamic urban scenes with gaussian splatting. In European Conference on Computer Vision, 2024. 2 [46] Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma, Anqi Joyce Yang, and Raquel Urtasun. Unisim: neural closed-loop sensor simulator. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2 [47] Jiang-Tian Zhai, Ze Feng, Jihao Du, Yongqiang Mao, JiangJiang Liu, Zichang Tan, Yifu Zhang, Xiaoqing Ye, and Jingdong Wang. Rethinking the open-loop evaluation of endarXiv preprint to-end autonomous driving in nuscenes. arXiv:2305.10430, 2023. 1 [35] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 5, [48] Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, and Luc Van Gool. End-to-end urban driving by imitating reinforcement learning coach. 2021. 3 11 [49] Wenzhao Zheng, Ruiqi Song, Xianda Guo, Chenming Zhang, and Long Chen. Genad: Generative end-to-end autonomous driving. In ECCV, 2024. 1, 2, 9 [50] Hongyu Zhou, Longzhong Lin, Jiabao Wang, Yichong Lu, Dongfeng Bai, Bingbing Liu, Yue Wang, Andreas Geiger, Hugsim: real-time, photo-realistic and Yiyi Liao. and closed-loop simulator for autonomous driving. arXiv preprint arXiv:2412.01718, 2024. 2 [51] Xiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, and Ming-Hsuan Yang. Drivinggaussian: Composite gaussian splatting for surrounding dynamic autonomous driving scenes. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2163421643, 2023. 2 A. Appendix A.1. Action Space Details Here, we provide more comprehensive explanation of the action space design. To ensure stable control and efficient learning, we define the action space over short time horizon of 0.5 seconds. The ego vehicles movement is modeled using discrete displacements in both the lateral and longitudinal directions. Lateral Displacement. The lateral displacement, denoted as ax, represents the vehicles movement in the lateral direction over the 0.5-second horizon. We discretize this dimension into Nx options, symmetrically distributed around zero to allow leftward and rightward movements, with an additional option to maintain the current trajectory. The set of possible lateral displacements is: ax {dx min, . . . , 0, . . . , dx max}. (A1) In our implementation, we use Nx = 61, with dx 0.75 m, dx uniformly. min = max = 0.75 m, and intermediate values sampled Longitudinal Displacement. The longitudinal displacement, denoted as ay, represents the vehicles movement in the forward direction over the 0.5-second horizon. Similar to the lateral component, we discretize this dimension into Ny options, covering range of forward displacements, including an option to maintain the current position: ay {0, . . . , dy max}. (A2) In our setup, we use Ny = 61, with dy mediate values sampled uniformly. max = 15m, and interA.2. Implementation Details In this section, we summarize the training settings, configurations, and hyperparameters used in our approach. , ay Planning Pre-Training. The action space is discretized using predefined anchors = {(ax i=1,j=1. Each anchor corresponds to specific steering-speed combination within the 0.5-second planning horizon. Given the ground truth vehicle position at = 0.5 denoted as pgt = (px gt), we implement normalized nearest-neighbor matching over predefined anchor positions: )}Nx,Ny gt, py ˆi = arg min ˆj = arg min (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) dx ax min max dx dx min ay 0 dy max 0 gt dx px min max dx dx min (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 py gt 0 dy max 0 . (cid:13) (cid:13) (cid:13) (cid:13)2 , (A3) Based on the matched anchor indices (ˆi, ˆj), we formulate the imitation learning objective as dual focal loss [29]: LIL = Lfocal(π(ax s),ˆit) + Lfocal(π(ay s), ˆjt), (A4) where Lfocal is focal loss for discrete action classification, and π(ax s) and π(ay s) are predicted action distributions from Eq. 1. Training configurations. We provide detailed hyperparameters for the two main stages, Planning Pre-Training and Reinforced Post-Training, in Tab. A1 and Tab. A2, respectively. config learning rate learning rate schedule optimizer optimizer hyper-parameters weight decay batch size training steps planning head dim Planning Pre-Training 1e-4 cosine decay AdamW [19, 30] β1, β2, ϵ = 0.9, 0.999, 1e-8 1e-4 512 30k 256 Table A1. Hyperparameters used in RAD Planning Pre-Training stage. A.3. Metric Details We evaluate the performance of the autonomous driving policy using eight key metrics. Dynamic Collision Ratio (DCR). DCR quantifies the frequency of collisions with dynamic obstacles. It is defined as: DCR = Ndc Ntotal , (A5) where Ndc is the number of clips in which collisions with dynamic obstacles occur, and Ntotal is the total number of clips. config learning rate learning rate schedule optimizer optimizer hyper-parameters weight decay RL worker number RL batch size IL batch size GAE parameter clipping thresholds deviation threshold planning head dim value function dim Reinforced Post-Training 5e-6 cosine decay AdamW [19, 30] β1, β2, ϵ = 0.9, 0.999, 1e-8 1e-4 32 32 128 γ = 0.9, λ = 0.95 ϵx = 0.1, ϵy = 0.2 dmax = 2.0m, ψmax = 40 256 256 Table A2. Hyperparameters used in RAD Reinforced PostTraining stage. Average Deviation Distance (ADD). ADD quantifies the mean closest distance between the ego vehicle and the expert trajectory during time steps when no collisions or deviations occur. It is defined as: ADD ="
        },
        {
            "title": "1\nTsaf e",
            "content": "Tsaf (cid:88) t=1 dmin(t), (A11) where Tsaf represents the total number of time steps in which the ego vehicle operates without collisions or deviations, and dmin(t) denotes the minimum distance between the ego vehicle and the expert trajectory at time step t. Finally, Longitudinal Jerk (Long. Jerk) and Lateral Jerk (Lat. Jerk) quantify the smoothness of vehicle motion by measuring acceleration changes. Longitudinal jerk is defined as: Jlong = , (A12) d2vlong dt2 Static Collision Ratio (SCR). SCR measures the frequency of collisions with static obstacles and is defined as: where vlong represents the longitudinal velocity. Similarly, lateral jerk is defined as: Jlat = d2vlat dt2 , (A13) where vlat is the lateral velocity. These metrics collectively capture abrupt changes in acceleration and steering, providing comprehensive measure of passenger comfort and driving stability. A.4. More Qualitative Results This section presents additional qualitative comparisons across various driving scenarios, including detours, crawling in dense traffic, traffic congestion, and U-turn maneuvers. The results highlight the effectiveness of our approach in generating smoother trajectories, enhancing collision avoidance, and improving adaptability in complex environments. SCR = Nsc Ntotal , (A6) where Nsc is the number of clips with static obstacle collisions. Collision Ratio (CR). CR represents the total collision frequency, given by: CR = DCR + SCR. (A7) Positional Deviation Ratio (PDR). PDR evaluates the ego vehicles adherence to the expert trajectory in terms of position. It is defined as: DR = Npd Ntotal , (A8) where Npd is the number of clips in which the positional deviation exceeds predefined threshold. Heading Deviation Ratio (HDR). HDR assesses orientation accuracy by computing the proportion of clips where heading deviations surpass predefined threshold: HDR = Nhd Ntotal , (A9) where Nhd is the number of clips where the heading deviation exceeds the threshold. Deviation Ratio (DR). captures the overall deviation from the expert trajectory, given by: DR = DR + HDR. (A10) Figure A1. More Qualitative Results. Comparison between the IL-only policy and RAD in various driving scenarios: Detour (Rows 1-2), Crawl in Dense Traffic (Rows 3-4), Traffic Congestion (Rows 5-6), and U-turn(Rows 7-8)."
        }
    ],
    "affiliations": [
        "Horizon Robotics",
        "Huazhong University of Science & Technology"
    ]
}