{
    "paper_title": "Guided Decoding and Its Critical Role in Retrieval-Augmented Generation",
    "authors": [
        "Özgür Uğur",
        "Musa Yılmaz",
        "Esra Şavirdi",
        "Özay Ezerceli",
        "Mahmut El Huseyni",
        "Selva Taş",
        "Reyhan Bayraktar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The integration of Large Language Models (LLMs) into various applications has driven the need for structured and reliable responses. A key challenge in Retrieval-Augmented Generation (RAG) systems is ensuring that outputs align with expected formats while minimizing hallucinations. This study examines the role of guided decoding in RAG systems, comparing three methods, Outlines, XGrammar, and LM Format Enforcer, across different multi-turn prompting setups (0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates, and output quality, we provide insights into their performance and applicability. Our findings reveal how multi-turn interactions influence guided decoding, uncovering unexpected performance variations that can inform method selection for specific use cases. This work advances the understanding of structured output generation in RAG systems, offering both theoretical insights and practical guidance for LLM deployment."
        },
        {
            "title": "Start",
            "content": "Guided Decoding and Its Critical Role in Retrieval-Augmented Generation Özgür Ugur, Musa Yılmaz, Esra Savirdi, Özay Ezerceli, Mahmut El Huseyni, Selva Tas, Reyhan Bayraktar Newmind AI Istanbul, Türkiye {ougur, myilmaz, esavirdi, oezerceli, mehussieni, stas, rbayraktar}@newmind.ai 5 2 0 2 8 ] . [ 1 1 3 6 6 0 . 9 0 5 2 : r AbstractThe integration of Large Language Models (LLMs) into various applications has driven the need for structured and reliable responses. key challenge in Retrieval-Augmented Generation (RAG) systems is ensuring that outputs align with expected formats while minimizing hallucinations. This study examines the role of guided decoding in RAG systems, comparing three methods, Outlines, XGrammar, and LM Format Enforcer, across different multi-turn prompting setups (0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates, and output quality, we provide insights into their performance and applicability. Our findings reveal how multi-turn interactions influence guided decoding, uncovering unexpected performance variations that can inform method selection for specific use cases. This work advances the understanding of structured output generation in RAG systems, offering both theoretical insights and practical guidance for LLM deployment. Keywordsretrieval-augmented generation, guided decoding, large language models, structured output, outlines, xgrammar, lm format enforcer, finite-state machines, context-free grammar, hallucination reduction I. INTRODUCTION The rapid rise of Large Language Models (LLMs) has transformed natural language processing, enabling applications across diverse domains such as question-answering, content generation, and conversational systems. However, persistent challenge lies in ensuring that LLM outputs adhere to specific structural formats, critical requirement for practical applications like data integration, API compatibility, and automated workflows. Retrieval-Augmented Generation (RAG), introduced by [1], enhances LLMs by incorporating external knowledge retrieval, thereby improving factual accuracy and contextual relevance. Although RAG addresses some limitations of standalone LLMs, it does not inherently guarantee structured output, which remain essential to meet user-defined constraints in real-world scenarios. Recent research underscores this gap; authors in [2] highlight the industrys growing demand for user-centered, restricted LLM outputs. To bridge this gap, guided decoding backends have emerged as promising solution, restricting LLM output to predefined formats or grammars. These methods leverage techniques such as finite-state machines, pushdown automata, or character-level enforcement to ensure compliance with structural requirements. For example, authors introduces Outlines [3], method that employs finite-state machines for efficient and structured text generation. Similarly, other approaches such 979-8-3315-6655-5/25/$31.00 2025 IEEE as XGrammar [4] and LM Format Enforcer [5] offer flexible mechanisms to enforce complex formats like JSON or domainspecific schemas, enhancing LLM utility in structured contexts. II. RELATED WORK The development of RAG and guided decoding methods builds on rich foundation of research aimed at enhancing the capabilities of LLMs. This section reviews prior studies relevant to our investigation, focusing on RAG, the challenge of structured outputs, and the guided decoding techniques evaluated in this study. Structured Outputs in LLMs. The importance of structured outputs has gained increasing attention as LLMs are deployed in practical settings requiring specific formats, such as JSON, YAML, or domain-specific schemas. Industry insights [2] highlight that unconstrained outputs often fall short in tasks like data processing and API integration. Therefore, enforcing structural constraints is essential, making guided decoding critical area of study. Guided Decoding Methods. Guided decoding backends constrain LLM outputs to predefined formats, addressing the limitations of unconstrained generation. One notable approach is Outlines, proposed in [3], which leverages finite-state machines (FSMs) to guide text generation efficiently, ensuring compliance with regular grammars while maintaining computational scalability. This method is particularly suited for applications that require predictable structured output. For complex structures like context-free grammars (e.g., JSON or code), XGrammar uses pushdown automata to enforce syntax, offering flexibility and precision. In contrast, LM Format Enforcer [5] applies strict character-level constraints, with its practical use detailed in Gats publicly available implementation despite lacking formal paper. While guided decoding enforces strict structure, it can be computationally intensive. To address this limitation, speculative methods such as Ranked Speculative Decoding (RSD) improve efficiency by using draft models and reward-based token selection, speeding up generation without sacrificing quality, especially for long texts [6]. Monitor-Guided Decoding for Code Completion. Modular Guided Decoding (MGD) uses static analysis to improve code generation, boosting compilation rates and enabling smaller models to outperform larger ones. It generalizes across languages and coding constraints [7]. III.GUIDED DECODING IMPACT ON RAG PERFORMANCE A. Experiment Setup We conducted experiments using high-performance inference engine powered by vLLM, with xgrammar as the default backend for guided decoding. This setup is designed for structured output generation, such as JSON schema or regexbased decoding. Other supported backends include lm format enforcer and outlines. Initially, vLLM v0 was employed for its compatibility with structured decoding frameworks. vLLM v1 introduced issues, restricting decoding to the xgrammar:no_fallback mode, which generates errors with unsupported schemas. Consequently, vLLM v0 remains our preferred implementation. Future updates to vLLM v1 are expected to address these limitations. The experiment utilized the OpenAI-compatible server models Qwen2.5-72B-Instruct and LLaMA-3.3-70B-Instruct. The setup involved four stages: 1) 2) 3) 4) Retrieving Relevant Documents: Using RAG to extract query-specific context. Defining Model Input: Setting structured instructions, prompts, and response formats. Configuring Guided Decoding: Testing Outlines, XGrammar, and LM Format Enforcer under identical conditions. Evaluating Multi-Turn Conversations: Assessing performance across 0-turn, 1-turn, and 2-turn scenarios. B. MultiTurn Algorithm Algorithm 1 MultiTurn RAG Eval add example turns usr_ex \"rag ctx: {ctx} query: {q}\" asst_ex \"resp: {r} doc ids: {truth_id}\" chat_hist chat_hist + [usr_ex, asst_ex] end for usr_eval \"rag ctx: {ctx} query: {q}\" chat_hist chat_hist + [usr_eval] model_resp GetModelResp(chat_hist) resp_ids ExtractIDs(model_resp) result Eval(truth_id, resp_ids) return result 1: function MULTITURNEVAL(dataset, n) chat_hist [systemprompt] 2: for 1 to do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end function 15: function EVAL(truth_ids, resp_ids) 16: 17: 18: 19: 20: 21: } 22: 23: end function corr [i for in resp_ids if in truth_ids] [i for in resp_ids if not in truth_ids] tp (corr > 0 and p = 0) return { success : tp, hallucination : p > 0 regex The algorithm 1 describes the multi-turn RAG evaluation process, illustrating how it operates at different levels of depth of conversation. The algorithm takes as input dataset of contexts, queries, and reference document identifiers, with parameter specifying the number of exemplar turns. For n=0, it uses only the system prompt and the evaluation query; for n=1,2, it prepares the corresponding exemplar exchanges demonstrating the expected discourse patterns and the reference citation methodology. The framework constructs conversation history with exemplar exchanges followed by the evaluation query. For each query, it retrieves the corresponding RAG context containing identifiers. The model response is ground truth document evaluated by comparing its extracted document references with the identifiers in the RAG context. response is considered successful when it references at least one correct document identifier while avoiding hallucinated references. This framework enables systematic analysis of how conversation history depth affects retrieval accuracy and hallucination rates, providing insights into multiturn RAG system behavior. C. Guided Decoding Methods 1) FSM-Based Outlines This approach leverages finite-state machines (FSMs) for efficient text generation, guaranteeing structural validity with O(1) complexity per token. It is especially well-suited to domains that require strict syntactic or semantic constraints, such as legal and technical documentation. The comments on the algorithm suggest its application in broader context, particularly in parsing. By applying Algorithm 1 to each string in set using combined FSMs for each parse state, it becomes possible to determine parser configurations. These configurations include the Pushdown Automaton (PDA) states, corresponding FSM states, and potential terminal symbols. The analogy extends to using the pre-image of the PDAs transition map to identify PDA stack values that can read the PDA states and terminal symbol sets of parser configuration. FSM Representation of Constraints. Outlines represent regular expressions and context-free grammars (CFGs) as finite-state machines, where states correspond to valid prefixes of structured sequence. The FSM tracks valid transitions, determining which tokens can legally follow given sequence. This eliminates the need for exhaustive vocabulary filtering. Efficient Vocabulary Indexing. To accelerate constraint enforcement, Outlines precomputes mapping from FSM states to valid tokens. This mapping denoted as σ : P(V ), enables constant-time token validity checks. Unlike naive approaches that iterate over all vocabulary tokens per step, Outlines retrieves valid tokens in O(1) time on average. [8] Token Sampling with FSM Constraints. During inference, the Outlines method modifies token sampling by applying FSM constraints to ensure structured outputs. The FSM tracks the current state and dynamically determines the valid token set. The next token is sampled from constrained probability distribution, adhering to structural rules. This method is applicable to various formats, such as floating-point numbers, programming syntax, and structured data like JSON and XML. 2) XGrammar (Pushdown Automata-Based) XGrammar is high-performance engine that accelerates LLMs by 100 using precomputed token masks, persistent execution stack, and parallel grammar processing, supporting real-time generation with broad compatibility. Vocabulary Partitioning and Token Mask Optimization: Tokens are classified as context-independent or contextdependent, with an adaptive cache to reduce memory and speed up validation. Persistent Execution Stack: Manages parsing states efficiently, enabling fast branching and minimal memory overhead. Pushdown Automata Optimization: Improves CFG parsing by inlining rules and reducing ambiguity. Parallel Mask Generation with LLM Inference: Runs grammar processing parallel to GPU-based inference, minimizing latency. 3) LM Format Enforcer LM Format Enforcer ensures adherence to predefined formats by filtering token probabilities, allowing only compliant tokens. It integrates with local LMs to improve reliability and consistency. Unlike rigid methods, it offers flexible enforcement that preserves the models formatting style, dynamically evaluating valid token sequences to balance compliance with autonomy, ensuring high output quality. D. Dataset The dataset used in this study contains metadata spanning multiple dialogue turns. Although we report total of 750 samples, only 507 are publicly accessible on Hugging Face owing to privacy restrictions. The dataset can be accessed via our Hugging Face repository 1. Table I: Dataset Overview Across Different Turns Metric Total Ref. Unique Ref. Total Samples 0-Turn 4909 3614 750 1-Turn 2482 1955 375 2-Turns 1622 1310 IV. RESULTS AND DISCUSSION We present result graphs that illustrate the differences between the three guided decoding methods. Our format several addressed implementation in handling these challenges specific to Turkish legal documents. The specialized (doc_id)document_id(/doc_id) required precise enforcement, with all methods showing significant improvement as conversation turns increased. Despite the complexity of agglutinative Turkish morphology and specialized legal vocabulary, all guided decoding approaches maintained high semantic quality (judge scores consistently higher than 91). Complex references in Turkish legal documents (e.g., \"344.0321.DOR.2021_1630505603_page_623\") complex structures 1https://huggingface.co/datasets/newmindai/siu-rag-data were increasingly well-handled in multi-turn scenarios, with false positive rates dropping dramatically from hundreds to single digits. Table II: E2E Generation Time per Sample (sec) Backend Outlines XGrammar LM Format Enforcer LLaMA-3.3-70B-Instruct 30.642 30.282 30.534 Qwen2.5-72B-Instruct 50.766 50.784 51. As shown in Table II, LLaMA-3.3-70B-Instruct processes fewer tokens per sample than Qwen2.5-72B-Instruct, which supports larger inputs and produces more extensive outputs in multiturn contexts. This indicates that Qwen2.5-72B-Instruct is optimized for longer, more complex queries, whereas LLaMA-3.3-70B-Instruct delivers faster responses on simpler tasks. Furthermore, as multiturn complexity grows, generation time increases proportionally. In these scenarios, LLaMA-3.370B-Instruct remains more time-efficient per sample, while Qwen2.5-72B-Instruct maintains higher throughput with larger token sets. Structured Output Performance Across Conversational Scenarios. Table III and Figure 1 present comparative analysis of false positive rates for guided decoding methods across 0-, 1-, and 2-turn conversational settings. In zeroturn interactions, LM Format Enforcer (LMF) consistently achieved the lowest false positive rates (0.49% for Qwen2.572B-Instruct, 3.06% for Llama-3.3-70B-Instruct), outperforming Outlines and XGrammar. As conversational depth increased, all methods demonstrated improved performance, with LMF maintaining superior robustness: achieving the lowest rates in 1-turn (0.73% and 0.33%) and 2-turn (0.30% and 0.06%) contexts for Qwen2.5-72B-Instruct and Llama-3.370B-Instruct, respectively. Notably, Outlines and XGrammar exhibited marked gains in multi-turn settings, particularly with Qwen2.5-72B-Instruct, underscoring the benefits of conversational context in reducing structural output errors. Table III: False Positive Rates of Guided Decoding Model Qwen2.5-72B-Instruct Llama-3.3-70B-Instruct Turns 0-Turn 1-Turn 2-Turns 0-Turn 1-Turn 2-Turns Outlines 0.65% 0.32% 0.18% 3.20% 0.24% 0.48% XGrammar 0.61% 0.41% 0.12% 3.08% 0.53% 0.31% LMF 0.49% 0.73% 0.30% 3.06% 0.33% 0.06% Few-turn prompting significantly improved reliability, particularly in 1-turn scenarios, where explicit examples clarified the desired output structure. Outlines and XGrammar benefited the most, while LM Format Enforcer struggled with the added complexity of 2-turn prompting. Among the methods, Outlines balanced flexibility and enforcement effectively, XGrammar offered strong performance and efficiency, and LM Format Enforcer ensured strict structural compliance but often at the cost of usability. Together, guided decoding and few-turn prompting complement each other, ensuring structured and factual outputs in RAG systems. In RAG scenario with 10 million chunks and 100,000 unique queries, each potentially linked to distinct references, the decoding algorithm significantly affects reference accuracy. VI.CONCLUSION Guided decoding is critical for reliable LLM deployments. This study underscores the importance of combining structured prompting with guided decoding to optimize RAG systems. Integrating external retrieval with decoding strategies that ensure format adherence enhances factual accuracy and structural reliability. Multi-turn prompting further improves control over generation while maintaining consistency with applicationspecific requirements. These findings highlight the need for structured decoding and prompting to advance LLM accuracy, usability, and integration in high-stakes applications. This study explored the impact of decoding strategies on reference accuracy in large language models, comparing XGrammar with flexible methods like Outlines and LM Format Enforcer. Experiments on Qwen2.5-72B-Instruct and LLaMA3.3-70B-Instruct show that adaptive strategies significantly reduce reference loss, especially in multi-turn settings."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "This study is supported by GSI Attorney Partnership. The authors would also like to express their gratitude for the valuable insights and support provided throughout the research process. REFERENCES [1] Lewis, P. et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. (2021), https://arxiv.org/abs/2005.11401 [2] Liu, M., Liu, F., Fiannaca, A., Koo, T., Dixon, L., Terry, M. & Cai, C. We Need Structured Output: Towards User-centered Constraints on Large Language Model Output. Extended Abstracts Of The CHI Conference On Human Factors In Computing Systems. pp. 1-9 (2024,5), http://dx.doi.org/10.1145/3613905.3650756 [3] Willard, B. & Louf, R. Efficient Guided Generation for Large Language Models. (2023), https://arxiv.org/abs/2307.09702 [4] Dong, Y., Ruan, C., Cai, Y., Lai, R., Xu, Z., Zhao, Y. & Chen, T. XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models. (2024), https://arxiv.org/abs/2411.15100 the output [5] Noamgat Noamgat/LM-Format-enforcer: Enforce forregex etc) of language model. GitHub., (JSON schema, mat https://github.com/noamgat/lm-format-enforcer [6] B. Liao et al., Reward-Guided Speculative Decoding for Efficient LLM Reasoning. 2025. [Online]. Available: https://arxiv.org/abs/2501.19324 [7] Agrawal, L. A., Svyatkovskiy, A., Sundaresan, N., & Allamanis, M. (2023). Guiding language models of code with global context using monitors. arXiv preprint arXiv:2306.10763. [8] Lew, A. K., Zhi-Xuan, T., Grand, G. & Mansinghka, V. K. Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs. arXiv preprint arXiv:2306.03081 (2023). [9] G. P. by B. and R. Hat, Structured Decoding in vLLM: gentle introduction, vLLM Blog, Jan. 14, 2025. https://blog.vllm.ai/2025/01/14/structdecode-intro.html [10] Li, J., Li, J., Wang, Y., Chang, Y., & Wu, Y. (2025). StructFlowBench: Structured Flow Benchmark for Multi-turn Instruction Following. arXiv preprint arXiv:2502.14494. Figure 1. Performance of guided decoding backends across multi-turn scenarios For Qwen2.5-72B-Instruct, replacing LM Format Enforcer with XGrammar resulted in 1,600 additional missed references in the zero-turn setting and 4,000 more in the oneturn case. The performance drop suggests that XGrammars format handling introduces notable degradation in single-turn and multi-turn contexts. Similarly, for LLaMA-3.3-70B-Instruct, XGrammar led to 134 additional misses in zero-turn and 2,000 in one-turn, relative to the top-performing decoding strategies such as Outlines and LM Format Enforcer. These findings show that in large-scale RAG systems, decoding strategy is critical for ensuring factual consistency and reference accuracy. Default decoding methods can fall short in high-recall tasks, leading to grounding errors. Outlines propose enhanced control over the output, which can substantially improve the fidelity of references. V. LIMITATIONS Regex & Character Support: Outlines limited regex support, lacking advanced features, restricts complex text processing. Its character constraints also hinder non-ASCII handling, reducing applicability in multilingual domains. Generation Flexibility: Unlike LM Format Enforcer Outlines does not accommodate beam search or batched generation, reducing flexibility for tasks that require varied outputsampling strategies [9]. Additionally, its partial JSON potentially yielding suboptimal outputs when slight deviations are acceptable [10]. Structured-Output Adaptability.: The of optional-field support limits adaptability in production workflows that require flexible structured-generation methods. in JSON outputs absence further Moreover, effective grammar enforcement should follow the models reasoning to ensure logical outputs. While tools like XGrammar and Outlines offer immediate syntax checks, LM Format Enforcer lacks support for delayed enforcement aligned with generation dynamics. XGrammars reliance on manual rule specification and its computational overhead underline the need for more adaptive, lightweight grammaticalenforcement approaches."
        }
    ],
    "affiliations": [
        "Newmind AI Istanbul, Türkiye"
    ]
}