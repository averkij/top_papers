{
    "paper_title": "What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge",
    "authors": [
        "Yosub Shin",
        "Michael Buriek",
        "Boris Sobolev",
        "Pavel Bushuyeu",
        "Vikas Kumar",
        "Haoyang Xu",
        "Samuel Watson",
        "Igor Molybog"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning."
        },
        {
            "title": "Start",
            "content": "What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge Yosub Shin 1 Michael Buriek 1 2 Boris Sobolev 1 3 Pavel Bushuyeu 1 Vikas Kumar 1 Haoyang Xu 1 Samuel Watson 1 Igor Molybog 1 6 2 0 2 6 1 ] A . [ 1 2 2 9 0 1 . 1 0 6 2 : r Abstract We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for VisionLanguage Reasoning (DCVLR) challenge (DCVLR Organizers, 2025), which isolates dataset selection by fixing the model and training protocol. Using compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning. 1. Introduction Visionlanguage models (VLMs) are increasingly expected to perform faithful, multi-step reasoning over combined visual and textual inputs. While recent progress has emphasized model scaling and training algorithms, growing body of work in reasoning fine-tuning shows that performance can be strongly influenced by which examples are used for supervision, even when model architecture and optimization remain fixed. In such settings, fine-tuning often operates in saturation regime: most gains arise from relatively small number of carefully selected examples, while additional data primarily stabilizes learned behaviors rather than introducing fundamentally new capabilities. 1University of Hawaii at Manoa, Honolulu, HI, USA 2PwC, USA 3Cisco, USA. Correspondence to: Yosub Shin <yosubs@hawaii.edu>. Preprint. January 19, 2026. 1 This paper studies data curation for multimodal reasoning through the lens of the NeurIPS 2025 Data Curation for VisionLanguage Reasoning (DCVLR) challenge. DCVLR provides tightly controlled experimental setting in which teams curate limited dataset to fine-tune fixed base model under shared training recipe, and are evaluated on diverse benchmark suite comprising both known-in-advance and held-out tasks. These constraints intentionally isolate data curation as the primary axis of variation, but they also shape the regime in which empirical conclusions should be interpreted. Our team placed first in DCVLR using compact (1kexample) curated dataset derived primarily from the Walton Multimodal Cold Start corpus. Beyond the competition outcome, our goal in this paper is not to optimize leaderboard score, but to understand which aspects of data curation materially affect reasoning performance under DCVLR-style constraints. Through controlled post-competition analyses, we examine the roles of dataset alignment, example difficulty, dataset size, and several commonly used diversity heuristics, and identify which factors consistently drive gains versus those that provide limited or no benefit in this regime. Three empirical findings emerge. First, difficulty-based filtering is the dominant lever for improving multimodal reasoning accuracy: examples that are challenging but learnable yield substantially higher gains than either very easy or extremely difficult samples. Second, once sufficiently aligned base dataset is chosen, increasing dataset size does not reliably improve mean performance under the fixed DCVLR training protocol; instead, larger datasets primarily reduce run-to-run variance, consistent with prior observations in reasoning fine-tuning. Third, the diversity mechanisms we evaluateincluding clustering-based balancing, class-level reweighting, and synthetic data mixingdo not improve performance beyond difficulty filtering, and in several cases degrade accuracy. Importantly, these results should be interpreted within the design choices of the DCVLR benchmark. The organizers deliberately fixed the training recipe after sweeping hyperparameters on baseline dataset, prioritizing fairness and Title Suppressed Due to Excessive Size computational feasibility in setting where evaluation of reasoning models is expensive. Our findings do not argue against this design choice; rather, they highlight that conclusions about data scaling and diversity are inherently coupled to the chosen fine-tuning regime. Contributions. This paper makes the following contributions: We present systematic analysis of data curation strategies for multimodal reasoning under the DCVLR setting, reframing competition result as controlled empirical study. We show that difficulty-based example selection applied to well-aligned base dataset is the primary driver of performance gains, while increasing dataset size mainly reduces variance rather than improving average accuracy. We report negative results for several commonly used diversity heuristics, demonstrating that they provide no additional benefit beyond difficulty filtering in this regime. We discuss implications of these findings for the interpretation and design of data curation benchmarks for reasoning models under fixed training protocols. 2. Background: DCVLR as Data Curation"
        },
        {
            "title": "Problem",
            "content": "The Data Curation for VisionLanguage Reasoning (DCVLR) challenge was designed to isolate the impact of dataset selection on multimodal reasoning performance. Participants were asked to curate limited number of training examples to fine-tune fixed base model, with all teams evaluated under shared training and evaluation pipeline. This design sharply constrains the experimental degrees of freedom, making DCVLR useful testbed for studying data curation, while simultaneously imposing limitations that shape the conclusions one can draw. 2.1. Benchmark Suite and Known vs. Held-Out Tasks The DCVLR evaluation suite consists of ten benchmarks spanning diverse forms of multimodal reasoning, including text-heavy academic question answering, mathematics, physics subdomains, and general visual reasoning (Elachqar et al., 2025). subset of these benchmarks was released to participants during development, while others were held out until final evaluation. In particular, benchmarks such as LiveXivTQA (Shabtay et al., 2025) and VMCBenchDEV (Zhang et al., 2025) were known in advance and thus could influence dataset selection decisions, whereas Omni3DBench (Marsili et al., 2025) and the Yale Physics subsets (Feng et al., 2025) were not accessible during curation. This split creates an inherent tension between alignment and generalization. Data curation strategies that emphasize alignment with known benchmarks may achieve strong leaderboard performance, but risk over-specialization to observed distributions. Conversely, strategies that emphasize broad coverage may sacrifice performance on heavily weighted in-distribution benchmarks. In our analysis, we interpret results through this lens by contrasting behavior on known-in-advance benchmarks with performance on heldout evaluations. 2.2. Fixed Training Protocol and Evaluation Cost To ensure fairness and comparability across submissions, the DCVLR organizers fixed the training recipe for all teams. Hyperparameters were selected by the organizers through sweeps on baseline dataset and then applied uniformly to all curated submissions. Participants were not permitted to modify optimization settings, learning rate schedules, or training duration. This choice reflects deliberate tradeoff. Reasoning-focused VLMs are expensive to evaluate, and sweeping over multiple training configurations for each submission would be computationally prohibitive at competition scale. Fixing the training protocol enables reliable ranking under realistic resource constraints, but also implies that observed performance reflects the interaction between curated data and specific fine-tuning regime. 2.3. DCVLR as Saturation-Regime Evaluation Prior work on reasoning fine-tuning has shown that improvements often saturate quickly, with most gains arising from relatively small number of informative examples (Wang et al., 2025b; Oumi, 2025a). As we show later, DCVLR exhibits similar saturation behavior in terms of mean performance. In addition, our DCVLR dataset-size ablations show that increasing dataset size under the fixed training protocol primarily improves stability (reducing run-to-run variability) rather than consistently improving average accuracy. We therefore view DCVLR as evaluating data curation in saturation regime, where the central questions concern example selection, difficulty, and alignment rather than raw data scale. This perspective informs both our experimental design and our interpretation of negative results for data scaling and diversity-based heuristics. Taken together, these characteristics make DCVLR valuable but specialized benchmark. It enables controlled study of data selection effects for multimodal reasoning, while requiring care in extrapolating conclusions beyond the fixed 2 Title Suppressed Due to Excessive Size training and evaluation regime imposed by the competition. 3. Problem Setup and Candidate Data Sources In this section, we describe the data curation problem faced by participants in DCVLR, the candidate datasets provided by the organizers, and the rationale behind our choice of starting point. The goal is not to exhaustively survey all submissions, but to clarify the design space that teams operated in and to motivate the dataset choices examined in later sections. 3.1. Data Curation Constraints DCVLR participants were allowed to submit curated dataset of up to 10,000 multimodal reasoning examples to fine-tune fixed base model. Apart from this dataset, all other aspects of training and evaluation were held constant, including the model architecture, optimization hyperparameters, and evaluation protocol. As result, dataset selection, filtering, and composition constituted the primary degrees of freedom available to participants. Within this constraint, participants faced several practical trade-offs. Larger datasets may reduce stochastic variance during fine-tuning, but require sacrificing selectivity under fixed data budget. Conversely, smaller datasets allow aggressive filtering for quality or difficulty, but may lead to less stable outcomes across runs. These trade-offs motivate our later analyses of dataset size and difficulty thresholds. 3.2. Organizer-Provided Baseline Datasets To support the challenge, the organizers released several baseline datasets reflecting different data curation philosophies, broadly spanning synthesis-based and filtration-based approaches (Oumi, 2025d) (Figure 2). Synthesis-based datasets generate new multimodal reasoning examples designed to target specific reasoning skills or patterns, often emphasizing compositionality or coverage. Filtration-based datasets instead select subsets from larger existing corpora using heuristics intended to prioritize reasoning quality or relevance. Among these baselines, Walton Multimodal Cold Start occupies distinctive position (Oumi, 2025e). Derived from larger corpus (Wei et al. (2025)) and filtered by the organizers, Walton was designed as bootstrap dataset for multimodal reasoning, emphasizing stable visual grounding and foundational reasoning behaviors. In the official baselines, Walton demonstrated strong performance on LiveXivTQA, heavily weighted and text-intensive benchmark in the DCVLR evaluation suite. Recent cold-start reinforcement learning for multimodal reasoning provides complementary evidence for bootstrapping from limited data (Wei et al., 2025). Other baselines, such as MM-Open-R1 and MMMathInstruct (Oumi, 2025c;b), emphasize alternative reasoning styles or domains, including explicit chain-ofthought supervision and mathematical problem solving. LIMO VL follows language-informed synthesis pipeline motivated by recent less is more findings for reasoning supervision (Wang et al., 2025b; Oumi, 2025a), and related code-guided supervision for math reasoning appears in MathCoder-VL (Wang et al., 2025a). While these datasets offer complementary coverage, they exhibited weaker transfer to LiveXivTQA under the fixed DCVLR training protocol. 3.3. Rationale for Starting from Walton Our data curation strategy begins from Walton Multimodal Cold Start. This choice was motivated by two complementary forms of alignment: alignment between the training dataset and the target benchmark, and alignment between the training dataset and the base model itself. First, Walton exhibits strong distributional alignment with in the LiveXivTQA, which carries substantial weight DCVLR evaluation and was available during development. Figure 3 shows PCA projection of embeddings for LiveXivTQA together with several organizer-provided baseline datasets. Walton embeddings lie noticeably closer to the LiveXivTQA cluster than alternatives such as MM-Open-R1 and MM-MathInstruct, indicating stronger benchmark-level alignment. This observation is consistent with Waltons superior LiveXivTQA transfer among organizer baselines and motivates its use as starting corpus. Second, Walton is also well aligned with the pretrained Qwen2.5-VL base model (Bai et al., 2025). We provide additional evidence for this in Appendix A, where we show that the base model achieves higher accuracy on LiveXivTQA questions that are more Walton-like in embedding space. Together, these analyses suggest that fine-tuning on Walton reinforces regions of the task distribution that the model already represents effectively, enabling efficient gains under limited data budget. 3.4. Relation to Other Submissions Table 1 summarizes the data curation strategies adopted by several top-performing DCVLR submissions, highlighting both shared design patterns and meaningful differences. While implementations vary across teams, number of consistent trends emerge that help contextualize our approach. First, most competitive submissions rely primarily on filtration-based curation rather than large-scale synthetic data generation. Teams typically begin from existing multimodal corporasuch as Walton or other multi-source datasetsand apply filtering heuristics to prioritize exam3 Title Suppressed Due to Excessive Size Figure 1. Overview of our dataset curation pipeline for the submission, from base datasets through filtering, augmentation, and final sampling. Figure 2. DCVLR baseline curation strategies grouped into synthesis-based and filtration-based approaches. The Walton Multi-modal Cold Start filtration baseline provides strong bootstrap corpus and motivates our difficulty-driven refinement. ples with higher expected learning signal. Fully synthetic pipelines are less common among top submissions, and when used, are often paired with additional reward-based or failure-based filtering. Second, nearly all top submissions incorporate an explicit or implicit difficulty signal. These signals range from basemodel failure rates (ours, MICV), to error or fail scores produced by substantially larger teacher models (ZhuYun), to reward-based or complexity-based criteria (AthenaRC, KDDI). This convergence suggests shared intuition across teams that challenging-but-learnable examples are central to improving reasoning performance under limited data budgets. Third, diversity control mechanisms are widely employed but heterogeneous in form, including category balancing, embedding-based clustering, and image-domain heuristics. Notably, these mechanisms are typically used in conjunction with difficulty-aware filtering rather than as primary drivers of performance. Across teams, curated dataset sizes tend to cluster near the upper submission limit (approximately 10k examples), although smaller curated subsets are explored in some cases. Finally, many submissions leverage some form of expert reasoning distillation, often using large proprietary models to generate or judge reasoning traces. This reflects common strategy of transferring structured reasoning behavior into the fixed base model through carefully curated supervision. Our approach differs primarily in emphasizing aggressive difficulty-based filtering on strongly aligned base dataset, resulting in substantially smaller final dataset. In the remainder of the paper, we analyze which of these common design choices materially affect performance under the DCVLR training regime, and which appear to provide limited or no benefit in practice. 4. Data Curation Methodology This section describes the data curation pipeline used in our submission and subsequent analyses (Figure 1). We focus on the procedural aspects of dataset construction, de4 Team Base Curation Difficulty Diversity Distill. Size Keywords Title Suppressed Due to Excessive Size Walton Multi-source Images Filtering Model failure rate Filtering Synthetic Reward score Filtering Us AFIE (2025) AthenaRC (2025) ZhuYun & PJLab (2025) Multi-source Blackwell (2025) MICV (2025) KDDI Research (2025) Table 1. Comparison of dataset curation strategies used by top DCVLR submissions. Most approaches favor difficulty-aware filtering with expert reasoning distillation over large-scale synthetic generation. Category-balanced Category sampling Multi-LLM judges 10K 10K 10K 10K 10K Weak-model failures 100 / 1K Complexity + diversity Qwen2.5-VL Implicit (images) Qwen3-235B CoT Dedup + img-dep. gpt-5 / Gemini Domain balance Category balance Gemini-2.5 Pro Embedding clusters Existing CoTs Difficult filtering, failure-based Reject sampling, refinement Synthetic instr., reward Fail ranking, image-grounded Multi-domain, curriculum Open + synth. Hybrid Multi-source Existing Fail-score (72B) Task tiering Error mining CoT complexity gpt-4o / VLM CoT Filtering Filtering Implicit (source) 1K For each Walton example, we run the base model (Qwen2.5VL-7B-Instruct) multiple times using non-deterministic decoding. We use temperature 0.7, top-p 0.9, and = 16 rollouts per question. We record whether the model produces the correct final answer on each run. Examples that the model answers correctly and consistently are treated as easy, while examples with low or unstable accuracy are treated as more difficult. This definition captures examples that are challenging for the current model while remaining grounded in the target task distribution. It also avoids reliance on external heuristics or human annotation, making it practical under the DCVLR constraints. 4.3. Difficulty-Based Filtering Using the difficulty signal described above, we construct filtered subsets of Walton by retaining examples below specified accuracy threshold. Intuitively, this procedure prioritizes examples where the model frequently fails or produces inconsistent reasoning, approximating form of failure-driven difficult-example mining. We explore multiple difficulty thresholds in later ablations, ranging from moderately difficult to extremely difficult examples. Unless otherwise specified, our primary experiments use moderate difficulty range that balances learning signal and stability. Importantly, difficulty filtering is applied independently of dataset size, allowing us to study its effect in isolation. 4.4. Optional Data Augmentation and Diversity Controls In addition to difficulty filtering, we explore several commonly used data augmentation and diversity mechanisms motivated by prior work (Muennighoff et al., 2025) and by strategies adopted in other DCVLR submissions. These include: Synthetic data augmentation: incorporating additional multimodal reasoning examples generated by large language or visionlanguage models, optionally with rewritten chain-of-thought supervision. Figure 3. Embedding projection via PCA for LiveXivTQA and three DCVLR baseline datasets (Walton, MM-Open-R1, MMMathInstruct), using Qwen2.5-VL-7B-Instruct representations. ferring all evaluation and comparisons to later sections. The methodology is intentionally simple: we begin from an aligned base dataset, apply difficulty-aware filtering, optionally incorporate additional data sources or diversity controls, and construct final dataset under fixed size budget. 4.1. Base Dataset We start from Walton Multimodal Cold Start, one of the organizer-provided baseline datasets (Oumi, 2025e). As discussed in Section 3, Walton exhibits strong alignment with LiveXivTQA and provides stable visual grounding and foundational reasoning behaviors. Using Walton as the base dataset allows us to focus our curation efforts on selecting informative examples rather than correcting for distribution shift. 4.2. Difficulty Definition Our primary curation signal is example difficulty. Rather than proposing new definition, we adopt difficulty notion consistent with prior work on reasoning fine-tuning. Specifically, difficulty is measured by the inconsistency of the base models answers under stochastic decoding. 5 Title Suppressed Due to Excessive Size Clustering-based diversity: selecting examples to balance representation across embedding clusters or semantic categories. Category-level balancing: enforcing uniform sampling across coarse task or domain labels. These mechanisms are typically applied on top of the base Walton dataset and, when used, are combined with difIn the case of ficulty filtering rather than replacing it. CoSyn-400k (Yang et al., 2025), we do not use the synthetic dataset in isolation. Instead, we mix CoSyn with Walton at controlled ratios (e.g., 9:1 Walton:CoSyn in our final submission), and additionally perform ablations that vary the CoSyn composition relative to Walton. We treat CoSyn as complementary data source whose effect is evaluated through mixture and composition studies rather than as standalone replacement for Walton. We emphasize that our goal is not to exhaust the space of possible diversity strategies, but to evaluate representative and commonly adopted heuristics under controlled conditions. Appendix provides implementation details for the augmentation and diversity constructions. 4.5. Final Dataset Construction The final curated dataset is obtained by sampling from the filtered pool under fixed size budget. We apply lightweight post-processing steps, including deduplication and length-based filtering, to ensure compatibility with the fixed training pipeline. Final dataset construction follows the organizer-provided data-preproc tool (Penfever, 2025). Dataset size is treated as an explicit experimental variable in later sections, allowing us to disentangle the effects of example selection from those of scale. This modular pipeline enables controlled ablations on difficulty, dataset size, and diversity, which we analyze in the following sections. 5. Experimental Setup and Evaluation This section describes the training, evaluation, and experimental protocols used in our analyses. All experiments adhere strictly to the official DCVLR training and evaluation pipeline, unless otherwise noted. Our goal is to ensure that observed differences arise from data curation choices rather than confounding factors related to optimization or evaluation. 5.1. Model and Training Protocol We use the official DCVLR base model, Qwen2.5-VL-7BInstruct, for all experiments. Fine-tuning is performed using the fixed training recipe provided by the DCVLR organizers, including optimizer type, learning rate schedule, batch size, number of training steps, and sequence length. These hyperparameters were selected by the organizers through sweeps on baseline dataset and then applied uniformly to all submissions. We do not modify the training algorithm, optimization settings, or stopping criteria across experiments. As result, differences in performance across runs can be attributed solely to differences in curated training data. 5.2. Evaluation Benchmarks Models are evaluated on the full DCVLR benchmark suite, which comprises ten multimodal reasoning benchmarks spanning academic question answering, mathematics, physics, and general visual reasoning. As discussed in Section 2, subset of benchmarks (including LiveXivTQA and VMCBench-DEV) was known in advance, while others (such as Omni3DBench and the Yale Physics subsets) were held out until final evaluation. We report both aggregate DCVLR scores and perbenchmark accuracy where relevant. In particular, we distinguish between LiveXivTQA, which is closely aligned with several organizer-provided datasets, and the remaining benchmarks, which serve as partially out-of-distribution evaluations. 5.3. Evaluation Cost and Replication Evaluating reasoning-focused VLMs is computationally expensive due to long context lengths and multi-step generation. To balance reliability and computational feasibility, we follow the official DCVLR evaluation protocol, which uses large validation sets to reduce statistical noise. Unless otherwise stated, each experiment is evaluated using single training run, consistent with competition practice. For selected ablationsparticularly those involving dataset sizewe repeat training with different random seeds to estimate run-to-run variance. When such repetitions are performed, we report mean performance and standard deviation across runs. 5.4. Dataset Variants and Ablation Design Our experiments span multiple curated dataset variants constructed using the methodology in Section 4. These include: Difficulty-filtered subsets of Walton at varying thresholds. Dataset size ablations ranging from few hundred examples up to the DCVLR maximum of 10k examples. Mixtures of Walton and CoSyn-400k at controlled ratios, including the 9:1 Walton:CoSyn configuration used in our final submission. Diversity-controlled variants using clustering-based or 6.3. Effect of Dataset Size Title Suppressed Due to Excessive Size category-level balancing heuristics. All dataset variants are trained and evaluated under identical conditions. This controlled setup enables direct comparison of the effects of difficulty, dataset size, and data composition, which we analyze in the following section. 6. Results We now present empirical results analyzing the effects of difficulty-based filtering, dataset size, and data composition under the DCVLR training protocol. Unless otherwise noted, all results follow the experimental setup described in Section 5. 6.1. Overall Competition Outcome Our final submission, consisting of 1k-example dataset derived primarily from Walton with difficulty-based filtering and small proportion of CoSyn augmentation, achieved the highest aggregate score in the DCVLR challenge. Table 2 reports per-benchmark accuracy for the base model, the organizer-provided Walton 10k baseline, and our curated 1k dataset. While leaderboard performance motivated our initial exploration, our focus here is on understanding which data curation choices contributed meaningfully to this outcome and which did not. 6.2. Effect of Difficulty-Based Filtering We first analyze the impact of difficulty-based filtering on reasoning performance. Table 3 shows accuracy as function of difficulty threshold when training on filtered subsets of Walton. We observe that examples of intermediate difficultythose the base model fails on frequently but not uniformlyyield the largest performance gains. Filtering out very easy examples provides substantial improvement over unfiltered training, while retaining extremely difficult examples leads to diminished or unstable performance. This behavior is consistent with prior observations in reasoning finetuning, where challenging but learnable examples provide the strongest training signal. Across multiple thresholds, difficulty-based filtering consistently outperforms unfiltered baselines at comparable dataset sizes, establishing difficulty as the dominant factor in our data curation pipeline. 7 We next study the effect of dataset size by training on randomly subsampled Walton datasets ranging from few hundred examples to the DCVLR maximum of 10k examples. Figure 4 summarizes performance as function of dataset size, showing aggregated accuracy alongside disaggregated benchmark results. For LiveXivTQA (Figure 4(b)), which is closely aligned with the Walton distribution, performance stabilizes once the dataset size reaches approximately 1k examples. Increasing the dataset size beyond this point does not consistently improve mean accuracy, but does reduce run-to-run variance, indicating greater training stability. In contrast, performance on non-LiveXivTQA benchmarks (Figure 4(c)) exhibits different trend. As dataset size increases, accuracy on these held-out or less aligned benchmarks degrades mildly but consistently. This divergence highlights specializationgeneralization trade-off: scaling an aligned dataset reinforces performance on in-distribution benchmarks, while offering limited or negative transfer to less aligned tasks. These results support the view of DCVLR as operating in saturation regime, where additional data primarily stabilizes learned behaviors rather than expanding reasoning capabilities. 6.4. Effect of Diversity and Synthetic Data Mixing Finally, we evaluate the impact of several diversity-oriented strategies, including clustering-based selection, categorylevel balancing, and mixing Walton with synthetic CoSyn400k data at controlled ratios. Figure 5 summarizes results for representative diversity ablations. None of the diversity-based variants outperform the Walton-only difficulty-filtered dataset, which remains the strongest configuration. Several diversity-controlled datasets achieve performance comparable to or slightly above randomly subsampled Walton baseline, but consistently fall short of the difficulty-filtered reference. Moreover, subset of diversity heuristics performs worse than even the Walton 1k random subsample, indicating that enforcing diversity constraints can actively suppress useful learning signal when applied without regard to example difficulty. In the case of CoSyn-400k, we evaluate mixtures of Walton and CoSyn at varying ratios (Figure 6), including the 9:1 Walton:CoSyn configuration used in our final submission. While low-ratio CoSyn augmentation does not substantially degrade performance, it also does not improve upon the Walton-only difficulty-filtered baseline. Increasing the proTable 2. Per-benchmark accuracy (%) for the base model, the Walton 10k fine-tuned model, and our curated 1k dataset. Title Suppressed Due to Excessive Size"
        },
        {
            "title": "Benchmark",
            "content": "# Samples Base Walton 10k Ours 1k VMCBench DEV (Zhang et al., 2025) LiveXivTQA (Shabtay et al., 2025) OlympiadBench (He et al., 2024) Omni3DBench (Marsili et al., 2025) Atomic (Feng et al., 2025) Electro (Feng et al., 2025) Mechanics (Feng et al., 2025) Optics (Feng et al., 2025) Quantum (Feng et al., 2025) Statistics (Feng et al., 2025) Overall (weighted) 1000 7913 5929 501 200 242 221 158 236 240 16640 79.8 56.8 13.4 34.8 8.5 8.3 9.0 7.0 5.9 17. 38.4 78.4 76.7 10.6 34.2 8.0 8.3 8.1 6.3 7.6 12.5 46.7 78.2 74.5 11.7 32.6 11.5 7.4 8.1 8.9 6.8 14.2 46."
        },
        {
            "title": "Subset",
            "content": "k range Seed 1 Seed 2 Seed 3 Mean Std"
        },
        {
            "title": "Range",
            "content": "Super-difficult Moderately difficult Easy 0.486 0.490 0.414 Table 3. Ablation on Walton difficulty thresholds (overall accuracy). Each setting samples 1k Walton examples and repeats sampling with three random seeds. Difficulty is measured by correct out of 16 stochastic decoding passes (higher = easier). [0.446, 0.486] [0.490, 0.494] [0.414, 0.460] 0.472 0.023 0.491 0.002 0.440 0.024 3 0 8 15 0.446 0.490 0. 0.484 0.494 0.446 portion of CoSyn consistently leads to performance degradation across both aligned and non-aligned benchmarks. Taken together, these results show that, under the DCVLR training protocol, commonly used diversity heuristics and synthetic data augmentation do not provide additional benefit beyond difficulty-based filtering on an aligned base dataset, and in some cases are detrimental. Notably, combining diversity constraints on top of difficulty-based filtering consistently underperforms using either approach in isolation, indicating that stacking these signals can further suppress effective learning signal rather than complement it. 7. Discussion and Implications We discuss the implications of our findings for data curation in multimodal reasoning and for the interpretation of DCVLR-style benchmarks. Our goal is not to propose universal prescriptions, but to situate our results within the broader literature on reasoning fine-tuning and to clarify what conclusions can and cannot be drawn under the constraints imposed by the competition. 7.1. Difficulty as the Primary Curation Signal Across all experiments, difficulty-based filtering emerges as the most effective driver of performance improvements. This result aligns with prior work on reasoning finetuning, which shows that carefully selected, challengingbut-learnable examples provide disproportionately strong learning signal. In the DCVLR setting, difficulty filtering consistently outperforms unfiltered training and dominates other curation heuristics when applied to an aligned base dataset. that the gains from Importantly, our results suggest difficulty-based filtering cannot be explained solely by dataset size. In our experiments, difficulty-filtered subsets of Walton outperform randomly subsampled Walton datasets of comparable size, indicating that the observed improvements arise from example selection rather than increased data volume. This reinforces the view that, in saturation regime, example selection is more consequential than increasing data volume. 7.2. Specialization, Alignment, and Dataset Scaling The dataset size ablation reveals clear specialization generalization trade-off. For LiveXivTQA, which is closely aligned with Walton, increasing dataset size stabilizes performance but does not improve mean accuracy beyond approximately 1k examples. In contrast, performance on less aligned benchmarks degrades as the aligned dataset scales. This behavior suggests that scaling an aligned dataset reinforces in-distribution behaviors at the expense of broader generalization. Such specialization is not inherently undesirable in competition setting where certain benchmarks are heavily weighted, but it underscores the importance of inter8 Title Suppressed Due to Excessive Size (a) Overall accuracy (b) LiveXivTQA (c) Non-LiveXivTQA Figure 4. Accuracy vs. dataset size for overall performance, the aligned LiveXivTQA benchmark, and the aggregate of non-LiveXivTQA benchmarks. Mean accuracy plateaus beyond 1k samples, while variance decreases with scale. preting aggregate scores in light of benchmark composition rather than treating them as uniform measures of general reasoning ability. 7.3. Limits of Diversity and Synthetic Augmentation Despite their prevalence in prior work and among top DCVLR submissions, diversity-oriented heuristics do not improve performance in our experiments once difficulty filtering is applied. Moreover, stacking diversity constraints on top of difficulty filtering consistently underperforms using either signal in isolation, indicating that these heuristics can interact negatively rather than complement one another. Similarly, mixing Walton with synthetic CoSyn-400k data does not yield gains beyond the difficulty-filtered baseline, and increasing the proportion of synthetic data leads to consistent degradation. These findings do not imply that diversity or synthetic data are intrinsically unhelpful, but rather that, under fixed training recipe in saturation regime, such signals do not provide additive benefit when applied naÄ±vely. 7.4. Implications for Data Curation Benchmarks Our results highlight several considerations for the design and interpretation of data curation benchmarks for reasoning models. First, when fine-tuning operates in saturation regime, benchmarks primarily evaluate example selection rather than data scaling. Second, aggregate scores can obscure specialization effects when aligned and held-out benchmarks are combined. Finally, commonly used curation heuristics may not compose additively, making careful ablation and prioritization essential. 9 Title Suppressed Due to Excessive Size Figure 5. Diversity-oriented ablations on the Walton base dataset. Across clustering, category balancing, and related heuristics, none improve upon difficulty-only filtering. We emphasize that DCVLRs design choicesparticularly the use of fixed training protocolrepresent reasonable trade-off between fairness and computational feasibility. Our observations should therefore be viewed as guidance for interpreting results within this regime, rather than as criticism of the benchmark design itself. 8. Limitations and Future Work This study has several limitations. First, we do not vary the training algorithm or optimization hyperparameters, and therefore cannot determine whether alternative training regimes could better exploit larger or more diverse datasets. Second, our diversity experiments focus on limited set of commonly used heuristics and do not exhaust the space of possible diversity or augmentation strategies. Third, our analysis is conducted within the specific constraints of DCVLR, including fixed base model and evaluation suite. While our findings are consistent with prior observations in reasoning fine-tuning, their generality beyond this setting remains an open question. Future work could explore data curation under alternative training regimes, explicitly disentangle alignment and generalization objectives, or investigate diversity mechanisms that introduce qualitatively different reasoning behaviors. Extending this analysis to other models, benchmarks, and evaluation protocols would further clarify the conditions under which different data curation strategies are effective. 9. Acknowledgments This work was supported by the National Science Foundation NRT-AI 2244574. This work used cloud GPU resources at NCSA Delta cluster through allocation number CIS240027 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants # 2138259, # 2138286, # 2138307, # 2137603, and # 2138296. 10 Title Suppressed Due to Excessive Size Figure 6. Impact of Dataset Mixture Ratios on Accuracy. Performance trends across varying ratios of Walton-difficult to CoSyn-rewritten traces. While 100% Walton-difficult baseline achieves the highest accuracy, increasing CoSyn proportions leads to consistent decline in performance (excluding local 50/50 outlier), indicating distribution mismatch between CoSyns text-heavy traces and the multimodal requirements of the DCVLR benchmark. The technical support and advanced computing resources from University of Hawaii Information Technology Services Research Cyberinfrastructure, funded in part by the National Science Foundation CC* awards # 2201428 and # 2232862 are gratefully acknowledged. This material is based upon work supported by the National Science Foundation CISE Graduate Fellowships under Grant # 2313998. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation."
        },
        {
            "title": "References",
            "content": "AFIE. Dcvlr technical report, URL https://huggingface.co/ Technical report. 2025. datasets/Moutozf01/dcvlr_dataset_ 1005_bytes_refine_shuffled_11000/ blob/main/Instructions.pdf. AthenaRC. Dcvlr technical report. Technical report, 2025. https://huggingface.co/datasets/ URL 11 ilsp/dcvlr/blob/main/technical_report. pdf. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., Zhong, H., Zhu, Y., Yang, M., Li, Z., Wan, J., Wang, P., Ding, W., Fu, Z., Xu, Y., Ye, J., Zhang, X., Xie, T., Cheng, Z., Zhang, H., Yang, Z., Xu, H., and Lin, J. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. Blackwell. Dcvlr methodology report. Technical URL https://huggingface. report, 2025. co/datasets/ArkaMukherjee/ reasoning-10k-v2/blob/main/ methodology.pdf. DCVLR Organizers. Data curation for visionlanguage reasoning (dcvlr). Project website, 2025. URL https: //dcvlr-neurips.github.io/. Elachqar, O., Feuer, B., Tripathi, R., Zhang, Y., Hulkund, N., Nguyen, T., Shabtay, N., Udandarao, V., Wang, X., Webb, S., Koukoumidis, E., Schmidt, L., Xie, Title Suppressed Due to Excessive Size and S., Yeung-Levy, S., Liang, P., Beery, S., Dcvlr competition results: Data Gkioxari, G. curation for vision-language reasoning. Oumi Blog, 2025. URL https://blog.oumi.ai/p/ dcvlr-competition-results-data-curation. Shabtay, N., Polo, F. M., Doveh, S., Lin, W., Mirza, M. J., Chosen, L., Yurochkin, M., Sun, Y., Arbelle, A., Karlinsky, L., and Giryes, R. Livexiv multi-modal live benchmark based on arxiv papers content, 2025. URL https://arxiv.org/abs/2410.10783. Feng, K., Zhao, Y., Liu, Y., Yang, T., Zhao, C., Sous, J., and Cohan, A. Physics: Benchmarking foundation models on university-level physics problem solving, 2025. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., et al. Olympiadbench: challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems, 2024. KDDI Research. Dcvlr technical report. Technical report, 2025. URL https://huggingface.co/ datasets/k423kaggle/submission/blob/ main/technical_report.pdf. Marsili, D., Agrawal, R., Yue, Y., and Gkioxari, G. Visual agentic ai for spatial reasoning with dynamic api, 2025. MICV. Dcvlr technical report. Technical report, 2025. https://huggingface.co/datasets/ URL Chuanbiao/CoT_1/blob/main/technical_ report.pdf. Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Li, F.-F., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand`es, E., and Hashimoto, T. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. Oumi. Blog Less (data) is more for fine-tuning. post, 2025a. URL https://blog.oumi.ai/p/ small-data-is-all-you-need. Oumi. Mm-mathinstruct Hugging Face dataset, 2025b. //huggingface.co/datasets/oumi-ai/ MM-MathInstruct-to-r1-format-filtered. (filtered). URL https: to r1 format Oumi. Multimodal open r1 8192 (filtered, mid-ic). URL https: Hugging Face dataset, 2025c. //huggingface.co/datasets/oumi-ai/ multimodal-open-r1-8192-filtered-mid-ic. Oumi. Training frontier Blog post, 2025d. URL https://blog.oumi.ai/p/ training-frontier-reasoning-vlms. reasoning vlms. Oumi. Walton multimodal cold start Hugging Face dataset, 2025e. //huggingface.co/datasets/oumi-ai/ walton-multimodal-cold-start-r1-format. (r1 format). URL https: Penfever. URL data-preproc. data-preproc. GitHub repository, 2025. https://github.com/penfever/ Wang, K., Pan, J., Wei, L., Zhou, A., Shi, W., Lu, Z., Xiao, H., Yang, Y., Ren, H., Zhan, M., and Li, H. Mathcoder-vl: Bridging vision and code for enhanced multimodal mathematical reasoning. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, 2025a. URL https://openreview.net/forum? id=nuvtX1imAb. Wang, X. et al. LIMO: Less is more for reasoning, 2025b. Wei, L., Li, Y., Zheng, K., Wang, C., Wang, Y., Kong, L., Sun, L., and Huang, W. Advancing multimodal reasoning via reinforcement learning with cold start, 2025. Yang, Y., Patel, A., Deitke, M., Gupta, T., Weihs, L., Head, A., Yatskar, M., Callison-Burch, C., Krishna, R., and Kembhavi, A. Scaling text-rich image understanding via code-guided synthetic multimodal data generation, 2025. Zhang, Y., Su, Y., Liu, Y., Wang, X., Burgess, J., Sui, E., Wang, C., Aklilu, J., Lozano, A., Wei, A., et al. Automated generation of challenging multiple-choice questions for vision language model evaluation, 2025. ZhuYun and PJLab. Dcvlr technical report. Technical report, 2025. URL https://huggingface.co/ datasets/Dream000/high_quality_data_ 10k_multisource/blob/main/technical_ report.pdf. A. Additional Alignment Analysis A.1. Alignment Between Training Dataset and the Base Model For each LiveXivTQA example, we compute k-nearest neighbors (k = 32) against the union of Walton and MMOpen-R1 embeddings extracted from the base Qwen2.5VL-7B-Instruct model. We then measure the fraction of neighbors originating from Walton and bin questions by this fraction. Figure 7 shows that the base model is consistently more accurate on LiveXivTQA questions that are more Walton-like. This indicates that the pretrained model is already better adapted to distribution resembling Walton, helping explain why difficulty-filtered Walton examples yield efficient improvements under small data budget. Title Suppressed Due to Excessive Size proportions to study their effect. B.2. Clustering-Based Diversity Selection For clustering-based diversity, we operate on the Walton dataset. Each Walton example is embedded using the base Qwen2.5-VL-7B-Instruct model by extracting the final-layer hidden representation after prepending the input with the associated image and question text. These embeddings are then clustered using k-means. To construct diverse subset under fixed dataset size budget, we sample across clusters using balanced allocation scheme with the following components: Per-cluster caps: We compute cluster sizes and enforce maximum number of samples per cluster. Square-root allocation: The total target dataset size is allocated across clusters with weights proportional to cluster size, allowing larger clusters to contribute more examples without dominating the sample. Controlled randomness: Allocation uses Dirichletbased weighting followed by multinomial sampling, with tunable parameter controlling the degree of determinism. Budget redistribution: Any unused allocation budget is redistributed to clusters that remain below their caps. This procedure is designed to promote embedding-level diversity while maintaining broad coverage of the underlying data distribution. B.3. Category-Level Balancing via MSC Labels For category-level balancing, we adopt the Mathematics Subject Classification (MSC) scheme introduced in prior work on reasoning fine-tuning (e.g., S1 (Muennighoff et al., 2025)). When applicable, each Walton example is assigned coarse-grained MSC category. Using these labels, we construct category-balanced subsets by enforcing uniform sampling or capped sampling across categories, thereby reducing the influence of highly overrepresented categories. Figure 8 illustrates the category distribution of Walton under the MSC scheme. In exploratory ablations, removing dominant categories can yield modest improvements over random sampling, while more aggressive category removal degrades performance. Because these effects are small and sensitive to category choice, we focus in the main paper on the construction procedure rather than interpreting category-specific outcomes. Overall, these appendix results document representative implementations of commonly used diversity and augmentaFigure 7. Absolute accuracy on LiveXivTQA vs. Waltoncoverage (mean fraction of Walton examples among kNN neighbors, computed over LiveXivTQA questions). B. Details of Diversity and Augmentation"
        },
        {
            "title": "Strategies",
            "content": "This appendix describes the construction procedures for the diversity and augmentation variants evaluated in the main paper. These methods are included for completeness and reproducibility; the main paper focuses on their empirical impact rather than methodological novelty. B.1. Synthetic Data Augmentation via CoSyn-400K To evaluate synthetic data augmentation, we sample multimodal reasoning examples from the CoSyn-400K dataset. Each CoSyn example consists of an image, question, short reasoning trace, and ground-truth answer. In preliminary inspection, we found that the original CoSyn reasoning traces are substantially shorter and less explicit than the chain-of-thought supervision typically used in reasoning fine-tuning. To address this mismatch, we rewrite the CoSyn reasoning traces using GPT-4o while preserving the original question, image, and final answer. Specifically, for each sampled CoSyn example, we prompt GPT-4o with the following instruction: Rewrite clear, step-by-step reasoning trace. Ensure the final answer is EXACTLY the given ground-truth answer. Put the final answer within boxed{...}. This rewriting procedure produces longer, more explicit reasoning traces that more closely resemble standard chainof-thought supervision. Rewritten CoSyn examples are not used in isolation. Instead, they are mixed with Walton examples at controlled ratios (e.g., 9:1 Walton:CoSyn in our final submission), and we perform ablations over mixture 13 Title Suppressed Due to Excessive Size Figure 8. Distribution of Mathematics Subject Classification (MSC) categories in the Walton dataset. tion heuristics. While each method is principled in isolation, none outperform difficulty-based filtering on an aligned base dataset under the DCVLR training protocol."
        }
    ],
    "affiliations": [
        "Cisco",
        "PwC",
        "University of Hawaii at Manoa"
    ]
}