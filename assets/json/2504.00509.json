{
    "paper_title": "Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?",
    "authors": [
        "Kai Yan",
        "Yufei Xu",
        "Zhengyin Du",
        "Xuesong Yao",
        "Zheyu Wang",
        "Xiaowen Guo",
        "Jiecao Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer $60\\%$ performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 9 0 5 0 0 . 4 0 5 2 : r Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems? Kai Yan1,2,,, Yufei Xu1,, Zhengyin Du1, Xuesong Yao1, Zheyu Wang1, Xiaowen Guo1, Jiecao Chen1 1ByteDance Seed, 2University of Illinois Urbana-Champaign Work done at ByteDance Seed, Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, novel, multi-modal benchmark for detecting LLMs recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary school-level arithmetic and reasoning problems. Such findings are wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs. Date: April 2, 2025 Correspondence: Kai Yan at kaiyan3@illinois.edu, Yufei Xu at xuyufei.123@bytedance.com Dataset: https://huggingface.co/datasets/kaiyan289/RoR-Bench/tree/main"
        },
        {
            "title": "Introduction",
            "content": "Since the advent of GPT-3 [12] and ChatGPT [50], Large Language Models (LLMs) have sparked an unprecedented revolution of research paradigm and pushed forward task frontiers in almost every field of Artificial Intelligence (AI) [45, 53, 72, 93], as well as the whole science community [1, 88, 89]. By improving the training data [43, 68], scaling up parameter size [32, 87], and incorporating long thinking process [21, 30], LLMs finally come close enough to the last exam [52] for Artificial General Intelligence (AGI) to surpass humanity. Despite the huge success of LLMs, however, researchers have not fully understood the underlying mechanism for LLMs emerging [5, 74] intelligence via current engineering [17, 21] advances. While there have been many efforts from the researchers to theoretically guarantee LLMs intelligence level [2, 11, 86] and rapid escalations in the difficulty of solvable math and science competition problems from elementary school [15] to research 1 a) Subtly changed condition b) Performance loss due to recitation Figure 1 Panel a) shows an example of how current cutting-edge LLMs, OpenAI-o1-1217 [30], fails to address an elementary school-level math problem (see Appendix A.1 for the detailed response) with subtle but crucial condition change, simply reciting existing solution template (OpenAI-o1-1217 fails with input being either floating or drifting); panel b) shows the performance loss of cutting-edge LLMs due to reciting solution templates regardless of shifted conditions on our benchmark, which is staggering 60% score gap on simple reasoning and math problems. level [52], there have also been recent concerns on LLMs are still struggling with real-world problems [71], even those which are not so difficult for humans [46, 94]. Such works indicates that cloud still exists upon the great monument of reasoning for LLMs, which questions the actual intelligence level of LLMs in reasoning problems and again brought the concern of stochastic parrots [10] back to the table. To better illustrate the existence of such cloud, here we examine simple, GSM-8K [15] level math problem as an example in Fig. 1. Despite the simplicity of the problem, however, cutting-edge models such as OpenAI o1 [30] fails to solve such problem; they simply recite the normal problem-solving paradigm of the problem, without carefully do the reasoning and check the subtle condition shift in the problem. With such phenomenon, we must ask the following tough question: Can the LLMs really solve simple reasoning problems, instead of simply reciting solution templates? To find out the answer for this problem, in this work we propose RoR-Bench, novel, multi-modal Chinese benchmark to detect the issue of Recitation over Reasoning for cutting-edge LLMs on simple reasoning problems, with 158 pairs of text problems and 57 pairs of image problems curated by humans; each pair consists of simple, mostly elementary school-level reasoning problem and its variant with subtle but crucial condition shifts. We find that all cutting-edge LLM models have severe problem in reciting solutions instead of actually doing the reasoning, causing an accuracy loss that often exceeds 60%. Such phenomenon is particularly astounding on problems with no solutions; many cutting-edge LLMs, such as DeepSeek-R1, can even only recognize < 10% cases as unsolvable. We explored initial solutions for mitigating the issue: adding notice prompts and providing subtly modified problems as few-shots. Although these solutions can mitigate the performance drop slightly, they are far from satisfactory and more complete solution is still yet to be proposed. Our key contributions can be summarized as follows: 1. We shed light on an important and severe issue for current cutting-edge LLMs, which is that LLMs are reciting problem-solving paradigms instead of actually conducting problem-specific reasoning even for simple reasoning problems; 2. We propose RoR-Bench, novel benchmark for detecting LLMs recitation behavior when solving simple reasoning problems; 3. We conduct several empirical analysis on our benchmark and examined initial solutions to the problem (See Sec. 4 for details)."
        },
        {
            "title": "2 Related Work",
            "content": "LLM benchmarks. The rapid advancement of LLMs in recent years [26, 30, 50] has created great needs for thorough LLM evaluation; some major directions include general knowledge [22, 55, 73], math [15, 19, 23], coding [14, 31, 41], instruction following [7], reasoning [34, 58, 59], long-context [44, 80], agent [42, 83], planning [65, 92] and function calls [79]. While the difficulty of benchmarks escalates quickly (e.g. from GSM8K [15] to MATH [23] and frontiers [19]), however, most of them are STEM 1 problems that can often be addressed by applying particular solution patterns [82], i.e., reciting solution templates. Thus, remarkable as the progresses on such types of benchmarks are, the true intelligence level of LLMs is still worth discussing. LLM robustness. While LLM achieves tremendous success, there has been persisting concerns about the limited robustness of LLMs [76, 94]. For example, LLMs have been well known for making mistakes in comparing 9.8 and 9.11 [76] and counting rs in strawberry [77]; there have also been many works that question LLMs robustness when confronted with out-of-distribution data [56, 85], incorrect/incomplete commands [79, 90], complex calculations [94], symbolic relations [46], and order of choices in multiple choice questions [91]. The work with the most similar findings to ours is done by Qiu et al. [54], who reports similar vulnerability of OpenAI-o1 [30] to subtly changed key conditions as spurious correlation to often-asked concepts, and proposed to fix the issue by using symbolic graphics data. However, their work and proposed solution focus on visual understanding of multi-modal LLMs, while we find that the recitation of solution templates are even more severe in pure text-based problems. Multi-modal LLMs. As the inherent limit of languages [25] and corpus depletion [67] quickly becomes major obstacle for AGI, researchers quickly turn to other modalities, such as vision [13] and speech/audio [18, 37] for extra input sources. As humans take the most information from vision [27], Vision Language Models (VLMs) such as OpenFlamingo [6], Llava [39, 40], Qwen-VL [8, 9] and GPT-4v/-4o [26, 47] have become the prevailing paradigm for multimodal LLMs, and made unique progress on multiple areas beyond LLMs, such as robotics [16, 70] and autonomous driving [64, 78, 84]. VLMs are also evaluated by part of our benchmark, and they exhibit the same recitation problem. Moreover, researchers have discovered the spurious correlation phenomenon [24, 66] by VLMs; such correlation between often-tested notions (e.g. famous optical illusions) and modified inputs [54] becomes part of the source for improper recitation in our benchmark."
        },
        {
            "title": "3 RoR-Bench",
            "content": "In this section, we will introduce our proposed benchmark, RoR-Bench. RoR-Bench is multimodal, questionanswering Chinese benchmark consisting of pairs of problems, which are the original problems and the modified problems. The original problems are selected such that 1) cutting-edge LLMs can well-address, and 2) are mostly classic puzzles that appear in books and homework. The modified problems are created such that they look very similar to original problems, but with key condition modified and have completely different solution paradigms and answers. Fig. 2 provides an example for text and image problems in our benchmark."
        },
        {
            "title": "3.1 Dataset Curation",
            "content": "We asked 17 human annotators to collect simple reasoning problems from the Internet, mostly based on brain teaser collections in online blogs and sets of reasoning puzzles for children. Such problems become the original problems for our benchmarks. Then, we ask the annotators to modify the problems with the following instructions: 1. Different solution paradigm: The idea for addressing the modified problems must be completely different from the original problem. Simply changing numbers in the conditions (e.g. from 30km/h to 60km/h) is not allowed, as LLMs can well generalize to different figures in the condition. 2. No ambiguity: The modified problem must be rigorous, and only have one reasonable answer. For example, how to cut triangle cake into 4 pieces (without any restrictions) is too open to judge its correctness; running competition in space (such that one cannot hear the starting gun) is too 1Science, Technology, Engineering and Mathematics. 3 (cid:39) (cid:36) (cid:39) (cid:36) Original problem: 两辆汽车从相距300公 里 的 两 个 城 市 同 时 出 发,相 向 而 行 一 辆 车的速度是60公里/小时,另一辆车的速度 是70公里/小时请问它们需要多少小时才 能相遇? (Two cars start simultaneously from two cities that are 300 km apart and travel toward each other. One car has speed of 60 km/h, while the other has speed of 70 km/h. How many hours will it take for them to meet?) Original answer: 300/(60 + 70) = 30 13 . Modified problem: 两辆汽车从相距300公 里 的 两 个 城 市 同 时 出 发,相背而 行 一 辆 车的速度是60公里/小时,另一辆车的速度 是70公里/小时请问它们需要多少小时才 能相遇? (Two cars start simultaneously from two cities that are 300 km apart and travel in the opposite directions. One car has speed of 60 km/h, while the other has speed of 70 km/h. How many hours will it take for them to meet?) Modified answer: 它们不会相遇 (They will never meet.) (cid:38) Original problem:这张图由多个同样的渐 变菱形构成它们整体看起来从上而下越 来越暗对吗 (This image is composed of multiple identical gradient diamonds, and overall, they appear to get darker from top to bottom, right?) Original answer:是 的 马 赫 带 效 应 (Yes, it is Mach band.) Modified problem: 这 张 图 由 多 个 同 样 的渐变菱形构成它们每个看起来从上而 下越来越暗对吗 (This image is composed of multiple identical gradient diamonds, and each of them appear to get darker from top to bottom, right?) Modified answer:不对是自下而上 (No, it is from bottom to top.) (cid:38) (cid:37) (cid:37) Figure 2 Examples of problems in our benchmark; for better readability, we marked the modified part red. Despite that we build Chinese benchmark, OpenAI-o1-1217 [30] also fails with our English translation for these examples. ambiguous as humans cannot normally run in space, and LLMs may assume additional conditions such as the event is happening inside space station. 3. As less verbal modification as possible: The modified problem should look verbally similar to the original problem, so as to better examine whether LLMs are actually reasoning with the condition, or simply reciting solution templates from similar problems. Each pair of original and modified problems will then be scrutinized by one of the 6 moderators (or multiple moderators in borderline cases), to ensure that the problems have no error or duplication, and satisfy the principles above."
        },
        {
            "title": "3.2 Dataset Statistics\nRoR-Bench consists of a total of 215 pairs of problems, with 158 pairs of text problems and 57 pairs of image\nproblems. The image problems are all related to the property of the figure, while the text problem consists of\n78 math problems (57 arithmetic, 11 geometry and 10 probability / combinatorics) and 80 reasoning problems\n(38 optimization, 10 commonsense, 27 deduction and 5 game theory). See Fig. 3 for an illustration of the\nratio for each type of problems. To ensure the simplicity of the problems, we curate the data such that all\ntext inputs are less than 200 characters, and each image problem only consists of a single image.",
            "content": "In particular, to better evaluate the LLMs robustness against unusual answers, we curate 32 text problems and 2 image problems with no solution (e.g., finding the ball with different weights using an inaccurate balance, or the smoke direction of an electric locomotive on windy day). We also provide several trick text problems with the problem to answer unrelated to the condition (e.g. asking the price of apples given the price of pears). 2 2We intentionally limit the number of such type of problems, as they can be potentially interpreted as typos. 4 Figure 3 An illustration of the types of the problem of our dataset."
        },
        {
            "title": "4 Evaluations",
            "content": "In this section, we introduce the main results and empirical analysis for cutting-edge LLMs on our benchmark. In particular, we want to address the following questions: 1) Does the model really conduct reasoning over subtly modified conditions, or are they simply reciting existing solution paradigms to similar problems? If it is the latter, is it because the models view those changed conditions as typos (Sec. 4.1)? 2) Will simple fixes, such as using original problems as 1-shot, address the possible problem of recitation over reasoning (Sec. 4.2)? 3) How well does the LLMs perform when it comes to ill-posed problems, especially those with no solution (Sec. 4.3)?"
        },
        {
            "title": "4.1.1 Text-based Problems",
            "content": "Evaluation. We evaluate 23 cutting-edge LLMs, which includes: State-of-the-art Models with long thinking (Chain-of-Thought, CoT [75]) process: DeepSeek-R1 [21], OpenAI-o1-1217 [30], OpenAI-o3-mini-high [49], Gemini-2.0 Flash-0121 [33], Claude 3.7 Sonnet [4] and QwQ-32B-Preview [62]; Flagship LLMs without long thinking process: Hunyuan Turbo-S [63], Ernie-4.5 [28], Gemini-2.0 Pro-0205, GPT-4.5-Preview [48], Qwen-max-0125 [61], GPT-4o-1120 [26], DeepSeek-v3 [43], Minimax-Text-01 [36], Claude 3.5 Sonnet [3], GLM-4-Plus [20], StepFun Step-2-16k, Yi-lightning [69], Mistral-Large-2 [60], GPT-4o-mini-0718, and Nova-Pro [29]; State-of-the-art small LLMs: Qwen-2.5-14B-Instruct [81] and Qwen-2.5-7B-Instruct. As the answer to our question can be versatile with sometimes no solution, we do not adopt exact match as the metric. Instead, we use GPT-4o-1120 as the judge, which gives binary (0/1) score (see Appendix A.2 for prompts) for LLM-generated answers. Each model is tested for 5 times with temperature 0.7 (we also report best-of-5 and greedy decoding results in Appendix B.1 and B.2 respectively). We use the average score (by GPT-4o-1120) as the metric over 5 trials and 158 problems, normalized to 0 100; the higher score is the better. Results. Tab. 1 shows the result for all LLMs tested on RoR-Bench with original and modified problems, which shows staggering > 50% average performance decrease from scores on the original problems to the modified problems, and often > 60% performance decrease for the best models such as DeepSeek-R1 and OpenAI-o3-mini-high. The best-of-5 3 performance of all LLMs also drop significantly (See Appendix B.1 for details), which indicates that such recitation issue is hard to be fixed simply by aligning techniques such as 3Under the best-of-5 (Bo5) metric, the model is considered to get score of 1 if at least one of the 5 trials get score of 1 under usual standards. With low score but high Bo5, the model can be aligned with reinforcement learning [50] to quickly improve its score as positive samples are easy to acquire."
        },
        {
            "title": "Model Name",
            "content": "Original Score Modified Score Original + FC Modified + FC DeepSeek-R1 OpenAI-o1-1217 Hunyuan Turbo-S OpenAI-o3-mini-high Ernie-4.5 Gemini-2.0 Flash-0121 (CoT) Gemini-2.0 Pro-0205 GPT-4.5-Preview Claude 3.7 Sonnet (CoT) Claude 3.7 Sonnet Gemini-2.0 Flash-0121 Qwen-max-0125 GPT-4o-1120 DeepSeek-V3 QwQ-32B-Preview Minimax-Text-01 Claude 3.5 Sonnet GLM-4-Plus StepFun Step-2-16k Yi-Lightning Qwen-2.5-14B-Instruct Mistral-Large-2 GPT-4o-mini-0718 Nova-Pro Qwen-2.5-7B-Instruct Avg. Decrease 86.46 86.08 86.08 85.95 83.42 81.90 81.90 80.89 80.02 77.34 73.67 73.55 72.91 71.90 71.39 70.00 69.75 69.37 69.11 68.61 66.20 62.41 60.63 57.46 35.31 N/A 22.66 29.87 19.36 24.94 20.13 23.80 20.89 26.59 25.06 26.83 21.39 20.63 21.26 18.73 22.53 19.75 22.28 17.34 16.71 15.95 18.86 18.10 18.86 17.59 13. 86.08 86.21 86.58 87.09 79.75 79.37 44.43 78.99 79.24 72.41 61.77 73.42 68.48 71.39 70.13 68.99 69.49 69.24 67.59 70.63 66.59 55.70 60.00 55.82 36.20 26.33 41.01 17.34 31.01 22.91 27.22 31.89 37.22 29.24 35.44 27.47 25.57 27.85 27.34 23.67 18.10 29.49 21.77 20.37 20.00 21.52 23.42 20.38 21.65 13.54 51.96(9.07) 3.24(7.74) 46.90(9.06) Table 1 Results on text-based problems of RoR-Bench, sorted by original score accuracy. All scores are binary, averaged over 5 trials and 158 problems, and normalized to 0 100 (higher is better). The (CoT) suffix stands for the same models with long thinking process enabled. FC stands for Forced Correct prompt. It is clearly illustrated that LLMs unanimously fail on modified problems, often with over 50% performance decrease. Forced Correct prompts somewhat helps, but is still far from addressing the recitation issue; also, the performance of original problems with Forced Correct prompts generally decrease, which indicates that adding prompt is not valid solution. Reinforcement Learning (RL). Also, long thinking process does not seem to help for the modified problems. On modified problems, models such as DeepSeek-R1, OpenAI-o1-1217 and OpenAI-o3-mini-high works no better than those without long thinking process, such as GPT-4.5 Preview and Claude 3.7 Sonnet, despite having higher performance on original problems; also, Gemini-2.0 Flash-0121 and Claude 3.7 Sonnet works similarly on modified problems either with or without long thinking process. In spite of this, the performance on original problems and modified problems are positively related (Pearson correlation coefficient [51] ρ 0.72), which indicates that the performance on modified problems are generally related to the base ability of the models. Is it because of the auto-correction of typos? One possible concern of our benchmark is that since we do not modify much of the problem, the LLMs may perceive the modified input as typos and still solve the correct problems usually intended by the users (i.e., original problems). To address such concern, we further test LLMs with the Forced Correct (FC) prompt added to the beginning of the problem: Forced Correct (FC) prompt: 请回答下面的问题题目保证没有问题请严格按照题目字面所写的问题回 答 (Please answer the following problem. The problems are guaranteed to be correct; please strictly follow the literal meaning of the problem.) The results are also illustrated in Tab. 1. Surprisingly, even with the FC prompt, LLMs still show on average > 45% performance decrease on modified problems, suggesting that the problem cannot be simply treated as an auto-correction of typos. Moreover, the performance on original problems with the FC prompt slightly 6 decreases, which become very significant on some models such as Gemini-2.0 Pro-0205. Upon examining the output, we found that LLMs often become too strict on the expression of the problems; for example, when asked whether game is fair, LLMs will question the definition of fairness and refuse to give definitive answer (see Appendix B.5 for an example). Such result shows that simply adding prompts is not valid way to address the recitation issue."
        },
        {
            "title": "4.1.2 Vision-based Problems",
            "content": "Evalulations. We evaluate 15 cutting-edge VLMs, which are: GPT-4.5-Preview, OpenAI-o1-1217, GPT-4o1120, Gemini-2.0 Pro-0205, GPT-4o-mini-0718, Gemini-2.0 Flash-0121, Qwen-2.5-VL-max, GLM-4v-Plus, Qwen-2.5-VL-72B, Claude 3.5 Sonnet, StepFun-1v-32k, Nova-Pro, Claude 3.7 Sonnet, SenseChat-Vision [57], and Qwen2.5-VL-7B. Similar to text evaluation, we use GPT-4o-1120 as the judge with binary score, and report the average accuracy (score by GPT-4o-1120) as the metric."
        },
        {
            "title": "Model Name",
            "content": "Original Score Modified Score Original + FC Modified + FC GPT-4.5-Preview OpenAI-o1-1217 GPT-4o-1120 Gemini-2.0 Pro-0205 GPT-4o-mini-0718 Gemini-2.0 Flash-0121 (CoT) Qwen2.5-VL-max GLM-4v-Plus Qwen2.5-VL-72B Claude 3.7 Sonnet (CoT) Gemini-2.0 Flash-0121 Claude 3.5 Sonnet StepFun-1v-32k Nova-Pro Claude 3.7 Sonnet SenseChat-Vision Qwen2.5-VL-7B Avg. Decrease 91.23 90.18 87.02 70.53 70.53 69.82 66.32 66.32 65.96 64.91 64.91 63.15 61.75 60.35 57.54 56.84 51.93 N/A 17.89 18.60 14.74 32.98 30.53 33.68 37.54 42.11 37.19 34.03 30.17 38.24 29.12 51.58 33.68 37.19 41. 77.19 91.58 85.61 64.21 79.65 67.71 64.56 64.22 64.91 63.51 53.68 57.19 64.91 70.17 58.60 72.63 58.95 40.70 23.51 26.32 37.54 26.67 39.30 42.11 41.05 42.1 40.00 35.79 44.91 27.72 36.14 42.46 38.94 38.60 35.21(19.67) 0.00(7.52) 31.50(15.47) Table 2 Results on vision-based problems of RoR-Bench, sorted by original score accuracy. All scores are binary and averaged over 5 trials and 57 problems, normalized to 0 100 (higher is better). Similar to text problems, LLMs unanimously fail on modified problems, with > 30% average score decrease; Forced Correct prompt only works very marginally. Results. Tab. 2 shows the result for all VLMs tested on RoR-Bench, which exhibits > 35% perfomance decrease on average from original problems to the modified problems. Interestingly, we find GPT-4o-1120, GPT-4.5-Preview and OpenAI-o1-1217 to be significantly better on original problems, but much worse on modified problems; upon checking responses, we find that the OpenAI models listed above are much more likely to summarize the origin of the images, as we collect them usually from illustrations of famous visual effects (e.g. Mach bands and checker-shadow illusions). On the contrary, models like Claude 3.5 Sonnet and Claude 3.7 Sonnet usually do not explicitly summarize such visual effects. Such result indicates that 1) OpenAI models may be overfitting to usual test cases, and more importantly, 2) explicit summarization or knowledge retrieval, which already becomes common practice for prompt-engineering works [35, 82], is double-edged sword ; while they improve the performance on usual test cases, it may increase the risk of missing key details in the problem during summarization."
        },
        {
            "title": "4.2 Is Few-Shot In-Context Learning the Cure?",
            "content": "A potential defense for the LLMs performance on our benchmark is that humans can often be tricked when answering brain teasers; the limited performance of LLMs may due to the reason that they are prepared for 7 normal user inputs and also not ready for brain teasers. To address such concern, we conduct an empirical analysis on the text-based problems of the RoR-Bench under two settings: 1) Given the original problem and solution, can the model notice subtle difference between the original problem and the modified problem? 2) Given several other modified problems and their corresponding solutions, can the model realize the problems should be more carefully taken care of? Evaluations. We evaluate the same set of LLMs in Sec. 4.1.1 4 For case 1 (adding original problems) mentioned above, we add simple prompt mentioning the original problem and solution are an example (See Appendix A.3 for details). For case 2 (adding modified problems), we uniformly randomly select modified problems other than the current problem as shots; we test both 1-shot and 5-shot scenario. Results. The results of the most represenatitive LLMs for few-shot In-Context Learning (ICL) is listed in Tab. 3; see Appendix B.3 for other LLMs. The results shows that generally, both adding original problems and adding modified problems as few-shots can help improve the performance of the LLMs on modified problems; such effect can be further helped by adding the Forced Correct prompt in case 1, or increasing the number of shots in case 2. Therefore, such fixes can be seen as an initial solution; however, the performance gap between all these fixes and original problems is still very large (> 30%), which indicates that few-shot ICL is not the ideal panecea for LLMs to overcome the recitation issue. Model Name Modified Case 1 Case 1 + FC Case 2 (1-Shot) Case 2 (5-shot) OpenAI-o1-1217 Claude 3.7 Sonnet GPT-4.5-Preview OpenAI-o3-mini-high DeepSeek-R1 Claude 3.5 Sonnet Gemini-2.0 Flash-0121 GPT-4o-1120 Gemini-2.0 Pro-0205 Avg. Increase 29.87 26.83 26.59 24.94 22.66 22.28 21.39 21.26 20.89 N/A 38.23 29.49 32.66 35.70 28.35 27.84 22.53 23.80 24.56 49.37 38.48 41.27 38.10 28.99 38.10 28.73 31.39 34.94 34.41 30.75 31.01 34.30 27.34 25.82 22.53 18.73 26.20 43.89 38.10 38.48 36.96 27.84 32.78 27.34 31.27 33. 5.16(3.05) 12.52(4.16) 3.82(3.20) 10.33(2.94) Table 3 The results of adding original problems as 1-shot (case 1) or adding other modified problems as few-shot (case 2) sorted by average score on modified problems in our benchmark. Claude 3.7 Sonnet and Gemini-2.0 Flash-0121 are without long CoT. Though the result show clear performance improvement, large gap still exists between the improved performance and that on original problems."
        },
        {
            "title": "4.3 The Mental Seal of Solvability\nIn Cixin Liu’s famous sci-fi novel The Dark Forest [38], the “mental seal” is a technique that injects certain\nstatements into human brain, and can lead to firm belief of the statements even when they contradict with\nground truths. As we examine the “no solution” problems in our benchmark (see Sec. 3.2 for details), we\nfound that LLMs are particularly worse in correctly pointing out the problems with no solution, and often will\nmake mistakes to make up a solution, as if injected by the mental seal that the problem is definitely solvable.",
            "content": "Evaluations. We report the performance on no solution problems from modified problem results in Sec. 4.1.1. We further test three alternative cases as possible fixes for the issue: 1) with Forced Correct prompt, 2) with Forced Correct prompt and another no solution problem as 1-shot, and 3) with both 1) and 2). Results. Tab. 4 shows the performance of the most representative LLMs on no solution problems as stated in Sec. 3.2 (see Appendix B.4 for other LLMs). Surprisingly, without any fixes, LLMs are unanimously stubborn on the belief that the given problem is solvable; not single model achieves > 15% score on this type of problems. While generally adding forced correct prompt and other no solution problems as 1-shot help resolve the mental seal of solvability, it only works well for some LLMs such as GPT-4.5-Preview, and is 4For better readability, we only show the most representative models in the main paper; see Appendix B.3 for details."
        },
        {
            "title": "Modified",
            "content": "OpenAI-o1-1217 GPT-4.5-Preview Claude 3.7 Sonnet Gemini-2.0 Flash-0121 Gemini-2.0 Pro-0205 OpenAI-o3-mini-high Claude 3.5 Sonnet GPT-4o-1120 DeepSeek-R1 Avg. Increase 13.75 13.13 10.63 10.63 9.38 6.25 6.25 5.63 3.13 N/A +FC 26.88 30.63 23.12 18.75 26.88 10.63 13.75 16.25 8. +1-shot + FC+1-shot 30.00 25.63 25.00 20.89 26.88 23.13 28.73 11.25 9.38 41.25 58.13 36.25 28.35 36.88 24.38 41.27 46.88 11.25 10.76(4.80) 13.57(5.51) 27.32(11.80) Table 4 The scores for no solution problems and possible fixes, sorted by average score on such of problems. Claude 3.7 Sonnet and Gemini-2.0 Flash-0121 are without long CoT. It is clearly shown that without any fixes, the average score for no solution problems is extremely low, showing the firm belief of LLMs that the given problem is solvable. While some LLMs, such as GPT-4.5-Preview, can be effectively corrected by adding Forced Correct (FC) prompts and other no solution problems as 1-shot, other LLMs such as DeepSeek-R1 are still very stubborn. generally still far from satisfactory for most models; for LLMs such as DeepSeek-R1 and many other weaker models, such as Qwen small models (see Tab. 10 in Appendix B.4), the issue persists."
        },
        {
            "title": "5 Discussion and Conclusion",
            "content": "In this work, we propose RoR-Bench, multimodal Chinese benchmark which clearly reveals an alarming issue in the current that current cutting-edge LLMs are unable to address even simple reasoning problems with conditions subtly shifted. Such phenomenon proved that LLMs are conducting recitation instead of reasoning when confronting seemingly classic problems. We found such issue can lead to dramatic performance loss (> 50%) and is unable to be addressed by simple fixes such as adding instruction prompts or few-shots, indicating that such issue is hard to fix and should be better awared by current LLM developers and researchers. Limitations and Future Works. Currently, our benchmark is Chinese-only due to the language limitation of human annotators and moderators, which may cause an edge on performance for LLMs by Chinese companies such as Ernie-4.5 and Hunyuan Turbo-S (note the main message, significant performance decrease after modification, is not affected). Though our message to convey is already strong with the current results (and preliminary English translation tests in this paper suggest that LLMs will other struggle on the other languages), to expand such benchmark to multiple languages will be an important future work. more important and fundamental avenue for future research is to find an effective way for LLMs to overcome such problem of recitation over reasoning without relying too much on users clarifications or being too harsh on typos."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank the following annotators for curating this dataset: Zhihui Song, Xinyue Zhang, Leyao Li, Zhuowei Li, Li Jiang, Shaofeng Pan, Chenxi Wang, Lin Wang, Bei Wang, Xin Luo, Jiahe Zhao, Mengyao Yan, Yutong Yan, Jiaojiao Chen, Tiantian Chen, Yi Chen and Yuxiao Han, and the moderators: Xiaowen Guo, Leyao Li, Shaofeng Pan, Chenxi Wang, Lin Wang and Jiahe Zhao. We would also like to thank Tong Sun, Zezhong Ma, Zaiyuan Wang, Aowei Ji and Qinyan Zhang for their contribution and discussion to the idea."
        },
        {
            "title": "References",
            "content": "[1] Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J. Ballard, Joshua Bambrick, Sebastian W. Bodenstein, David A. Evans, Chia-Chun Hung, Michael ONeill, David Reiman, Kathryn Tunyasuvunakool, Zachary Wu, Akvile Žemgulyte, Eirini Arvaniti, Charles Beattie, Ottavia Bertolli, Alex Bridgland, Alexey Cherepanov, Miles Congreve, Alexander I. Cowen-Rivers, Andrew Cowie, Michael Figurnov, Fabian B. Fuchs, Hannah Gladman, Rishub Jain, Yousuf A. Khan, Caroline M. R. Low, Kuba Perlin, Anna Potapenko, Pascal Savy, Sukhdeep Singh, Adrian Stecula, Ashok Thillaisundaram, Catherine Tong, Sergei Yakneen, Ellen D. Zhong, Michal Zielinski, Augustin Žídek, Victor Bapst, Pushmeet Kohli, Max Jaderberg, Demis Hassabis, and John M. Jumper. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, 2024. [2] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. In ICLR, 2023. [3] Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3-5-sonnet. [4] Anthropic. Claude 3.7 sonnet system card, 2025. URL https://assets.anthropic.com/m/785e231869ea8b3b/ original/claude-3-7-sonnet-system-card.pdf. [5] Sanjeev Arora and Anirudh Goyal. theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023. [6] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. [7] Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, et al. Mt-bench-101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. In ACL, 2024. [8] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [9] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [10] Emily Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In FAccT, 2021. [11] Aman Bhargava, Cameron Witkowski, Shi-Zhuo Looi, and Matt Thomson. Whats the magic word? control theory of llm prompting. arXiv preprint arXiv:2310.04444, 2023. [12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020. [13] Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Marcella Cornia, and Rita Cucchiara. The revolution of multimodal large language models: survey. In ACL Findings, 2024. [14] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [15] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [16] Jiafei Duan, Wilbert Pumacay, Nishanth Kumar, Yi Ru Wang, Shulin Tian, Wentao Yuan, Ranjay Krishna, Dieter Fox, Ajay Mandlekar, and Yijie Guo. Aha: vision-language-model for detecting and reasoning over failures in robotic manipulation. In ICLR, 2025. [17] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 10 [18] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Ke Li, Junteng Jia, Yuan Shangguan, Jay Mahadeokar, Ozlem Kalinli, Christian Fuegen, and Mike Seltzer. Audiochatllama: Towards general-purpose speech abilities for llms. In ACL, 2024. [19] Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, et al. Frontiermath: benchmark for evaluating advanced mathematical reasoning in ai. arXiv preprint arXiv:2411.04872, 2024. [20] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024. [21] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [22] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In ICLR, 2021. [23] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In NeurIPS, 2021. [24] Parsa Hosseini, Sumit Nawathe, Mazda Moayeri, Sriram Balasubramanian, and Soheil Feizi. Seeing whats not there: Spurious correlation in multimodal llms. arXiv preprint arXiv:2503.08884, 2025. [25] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception with language models. NeurIPS, 2023. [26] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [27] Fabian Hutmacher. Why is there so much more research on vision than on any other sensory modality? Frontiers in psychology, 2019. [28] Baidu Inc. Introducing ernie 4.5 our new-generation native multimodal model, 2025. URL https://www.youtube. com/watch?app=desktop&v=9JlySFEOVEk. [29] Amazon Artificial General Intelligence. model card. the-amazon-nova-family-of-models-technical-report-and-model-card. Amazon Technical Reports, 2024. The amazon nova family of models: Technical report and URL https://www.amazon.science/publications/ [30] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [31] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? In ICLR, 2024. [32] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [33] Koray Kavukcuoglu. Gemini 2.0 is now available to everyone, 2025. URL https://blog.google/technology/ google-deepmind/gemini-model-updates-february-2025/. [34] Mehran Kazemi, Bahare Fatemi, Hritik Bansal, Chrysovalantis Anastasiou John Palowitch, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, and Orhan Firat. Big-bench extra hard. arXiv preprint arXiv:2502.19187, 2025. [35] Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer. human-inspired reading agent with gist memory of very long contexts. In ICML, 2024. [36] Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, et al. Minimax-01: Scaling foundation models with lightning attention. arXiv preprint arXiv:2501.08313, 2025. 11 [37] Dongting Li, Chenchong Tang, and Han Liu. Audio-llm: Activating the capabilities of large language models to comprehend audio data. In ISNN, 2024. [38] Cixin Liu. The Dark Forest. Tor Books, 2016. [39] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. [40] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. [41] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In NeurIPS, 2023. [42] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. In ICLR, 2024. [43] Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, and Lianwen Jin. Datasets for large language models: comprehensive survey. arXiv preprint arXiv:2402.18041, 2024. [44] Kaijing Ma, Xinrun Du, Yunran Wang, Haoran Zhang, Zhoufutu Wen, Xingwei Qu, Jian Yang, Jiaheng Liu, Minghao Liu, Xiang Yue, et al. Kor-bench: Benchmarking language models on knowledge-orthogonal reasoning tasks. In ICLR, 2025. [45] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. In ICLR, 2024. [46] Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. In ICLR, 2025. [47] OpenAI. Gpt-4v(ision) system card, 2023. URL https://cdn.openai.com/papers/GPTV_System_Card.pdf. [48] OpenAI. Introducing gpt-4.5, 2025. URL https://openai.com/index/introducing-gpt-4-5/. [49] OpenAI. Openai o3-mini, 2025. URL https://openai.com/index/openai-o3-mini/. [50] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In NeurIPS, 2022. [51] Karl Pearson. Notes on regression and inheritance in the case of two parents. Proceedings of the Royal Society of London, 1895. [52] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. [53] Libo Qin, Qiguang Chen, Xiachong Feng, Yang Wu, Yongheng Zhang, Yinghui Li, Min Li, Wanxiang Che, and Philip Yu. Large language models meet nlp: survey. arXiv preprint arXiv:2405.12819, 2024. [54] Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Xiao, Katherine Collins, Joshua Tenenbaum, Adrian Weller, Michael Black, and Bernhard Schölkopf. Can large language models understand symbolic graphics programs? arXiv preprint arXiv:2408.08313, 2024. [55] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In COLM, 2024. [56] Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, and Peter Liu. Out-of-distribution detection and selective generation for conditional language models. In ICLR, 2023. [57] SenseTime. Sensetime unveils sensenova 5.5 - complete and comprehensive upgrade, 2024. URL https: //www.sensetime.com/en/news-detail/51168278. [58] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. TMLR, 2023. [59] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. In ACL Findings, 2023. [60] Mistral AI team. Large enough, 2024. URL https://mistral.ai/news/mistral-large-2407. [61] Qwen Team. Qwen2.5-max: Exploring the intelligence of large-scale moe model, 2025. URL https://qwenlm. github.io/blog/qwen2.5-max/. [62] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, 2025. URL https://qwenlm.github.io/ blog/qwq-32b-preview/. [63] Tencent. Tencent/llm.hunyuan.turbo-s, 2025. URL https://github.com/Tencent/llm.hunyuan.turbo-s. [64] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang, Zhiyong Zhao, Kun Zhan, Peng Jia, Xianpeng Lang, and Hang Zhao. Drivevlm: The convergence of autonomous driving and large vision-language models. In CoRL, 2024. [65] Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. In NeurIPS, 2023. [66] Maya Varma, Jean-Benoit Delbrouck, Zhihong Chen, Akshay Chaudhari, and Curtis Langlotz. Ravl: Discovering and mitigating spurious correlations in fine-tuned vision-language models. NeurIPS, 2024. [67] Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. Will we run out of data? limits of llm scaling based on human-generated data. In ICML, 2024. [68] Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. Position: Will we run out of data? limits of llm scaling based on human-generated data. In ICML, 2024. [69] Alan Wake, Bei Chen, CX Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng, Daniel Cooper, Fan Zhou, Feng Hu, et al. Yi-lightning technical report. arXiv preprint arXiv:2412.01253, 2024. [70] Beichen Wang, Juexiao Zhang, Shuwen Dong, Irving Fang, and Chen Feng. Vlm see, robot do: Human demo video to robot action plan via vision language model. arXiv preprint arXiv:2410.08792, 2024. [71] Jize Wang, Ma Zerun, Yining Li, Songyang Zhang, Cailian Chen, Kai Chen, and Xinyi Le. Gta: benchmark for general tool agents. In NeurIPS Datasets and Benchmarks Track, 2024. [72] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [73] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In NeurIPS Datasets and Benchmarks Track, 2024. [74] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. TMLR, 2022. [75] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. [76] Zikai Xie. Order matters in hallucination: Reasoning order as benchmark and reflexive prompting for largelanguage-models. arXiv preprint arXiv:2408.05093, 2024. [77] Nan Xu and Xuezhe Ma. Llm the genius paradox: linguistic and math experts struggle with simple word-based counting problems. In NAACL, 2025. [78] Yi Xu, Yuxin Hu, Zaiwei Zhang, Gregory Meyer, Siva Karthik Mustikovela, Siddhartha Srinivasa, Eric Wolff, and Xin Huang. Vlm-ad: End-to-end autonomous driving through vision-language model supervision. arXiv preprint arXiv:2412.14446, 2024. [79] Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Berkeley function calling leaderboard. https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_ calling_leaderboard.html, 2024. 13 [80] Kai Yan, Zhan Ling, Kang Liu, Yifan Yang, Ting-Han Fan, Lingfeng Shen, Zhengyin Du, and Jiecao Chen. Mir-bench: Benchmarking llms long-context intelligence via many-shot in-context inductive reasoning. In ICLR Workshop on Reasoning and Planning for Large Language Models, 2025. [81] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [82] Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph Gonzalez, and Bin Cui. Buffer of thoughts: Thought-augmented reasoning with large language models. NeurIPS, 2024. [83] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In NeurIPS, 2022. [84] Junwei You, Haotian Shi, Zhuoyu Jiang, Zilin Huang, Rui Gan, Keshu Wu, Xi Cheng, Xiaopeng Li, and Bin Ran. V2x-vlm: End-to-end v2x cooperative autonomous driving through large vision-language models. arXiv preprint arXiv:2408.09251, 2024. [85] Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, and Maosong Sun. Revisiting out-of-distribution robustness in nlp: Benchmarks, analysis, and llms evaluations. In NeurIPS, 2023. [86] Oussama Zekri, Ambroise Odonnat, Abdelhakim Benechehab, Linus Bleistein, Nicolas Boullé, and Ievgen Redko. Large language models as markov chains. arXiv preprint arXiv:2410.02724, 2024. [87] Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat. When scaling meets llm finetuning: The effect of data, model and finetuning method. In ICLR, 2024. [88] Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, et al. Huatuogpt, towards taming language model to be doctor. In Findings of EMNLP, 2023. [89] Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, and Jiawei Han. comprehensive survey of scientific large language models and their applications in scientific discovery. In EMNLP, 2024. [90] Hao Zhao, Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Is in-context learning sufficient for instruction following in llms? In ICLR, 2025. [91] Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Large language models are not robust multiple choice selectors. In ICLR, 2024. [92] Huaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, Heng-Tze Cheng, Quoc Le, Ed Chi, et al. Natural plan: Benchmarking llms on natural language planning. arXiv preprint arXiv:2406.04520, 2024. [93] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. In ICML, 2024. [94] Lexin Zhou, Wout Schellaert, Fernando Martínez-Plumed, Yael Moros-Daval, Cèsar Ferri, and José HernándezOrallo. Larger and more instructable language models become less reliable. Nature, 2024. 14 Appendix: Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems? The appendix has two parts. In Sec. A, we introduce the prompts used in our experiments; in Sec. B, we show more experiment results on our RoR-Bench."
        },
        {
            "title": "A Prompts",
            "content": "A.1 Prompts and OpenAI-o1-1217 Response in Fig. 1 Due to space limit, we simplified the response of OpenAI-o1-1217 in Fig. 1. Here we show the full prompts and responses below: (cid:39) (cid:36) # Conversation 1 (original problems) User: boat sails at 15km/h in still water. It spends 8 hours traveling from upstream location to downstream location B, with the water speed 3km/h. How long does it take to return from to A? OpenAI-o1-1217: (Thinking process shown by OpenAI website) 哎呀, 看来这是个航向的谜题立即想到了船在水流速度3 km/h下 的实际航速和时间 船从A到B是下行水流速度3 km/h加上船的网速15 km/h总速18 km/h行程长144 km耗时8小时 (Oops, it is puzzle about sailing directions. am immediately thinking of the actual speed and time of the boat at 3km/h current speed. The boat goes downstream from to B. The current speed is 3km/h, with the net speed of the boat being 15km/h, the total speed is 18km/h. The journey is 144km, and requires 8 hours.) (Answer) First, determine the downstream speed and upstream speed of the boat: - **Downstream speed** = 15 km/h (boat speed in still water) + 3 km/h (current speed) = 18 km/h - **Upstream speed** = 15 km/h (boat speed in still water) 3 km/h (current speed) = 12 km/h Because the boat took 8 hours to go downstream: Distance from to = Downstream speed Time = 18 8 = 144 km To find the time to go back upstream from to A: Time upstream = Distance Upstream speed = 144 12 = 12 hours Hence, it takes **12 hours** to return from to A. (cid:38) (cid:39) (cid:37) (cid:36) # Conversation 2 (modified problems) User: boat sails at 15km/h in still water. It spends 8 hours drifting from upstream location to downstream location B, with the water speed 3km/h. How long does it take to return from to A? OpenAI-o1-1217: (Thinking process shown by OpenAI website) 船在静水中的速度是15公里/小时水流速度是3公里/小时 从A到B用了8小时那么我们可以先计算出A到B的距离然后再用相同的速度反推从B到A的时间 (The boat sails at 15km/h, and the current speed is 3km/h. It takes 8 hours from to B, then we can first calculate the distance from to B, then use the same speed to induce the time from to A.) (Answer) First, compute the distance from to B. Going downstream, the boats effective speed is 15 + 3 = 18 km/h. Since the trip from to takes 8 hours: On the return trip from to A, the boat travels upstream with an effective speed of 15 3 = 12 km/h. Therefore, the time needed is: Distance = 18 km/h 8 = 144 km. So it takes 12 hours to go back from to A. (cid:38) (cid:37) Time = 144 km 12 km/h = 12 hours. A.2 Prompts for the Judge We use the same prompt for GPT-4o-1120 judges for all our experiments. We use Chinese prompts as the problems are Chinese; we provide English translations as well in the boxes below. Interestingly, we found the emphasis of do not question the ground truth answer very important; the recitation issue of LLMs is so severe that without such prompt, the LLM judge will question the problem instead of the students answer even with ground truth answer available. (cid:39) (cid:36) # Chinese Prompt 你是一个专业的老师现在有一道题目你需要判断学生的回答是否和标准答案一致题目和答案保证是绝对正 确的不会有错字因此你要严格按照题目的字面意思评判不要质疑标准答案有问题 如果学生的回答和标准 答案一致请打1分否则请打0分标准答案经常只含有答案而学生的回答一般会带有过程你只需要根据学 生的结论是否与标准答案相符做出评价学生的结论和标准答案必须本质一样但表述可以稍有不同例如学 生的答案是50又1/2或50.5标准答案是101/2则也可以算作正确 你的判断应遵循如下格式你的输出在评 价后开始首先是对学生回答的分析不超过300字然后以分数:[[0]]或分数[[1]]结束 下面开始判 断 题目 (Problem description) 标准答案 (Answer) 学生回答 (LLM output) (cid:38) (cid:39) # English Translation You are professional teacher. Now there is problem, and you need to judge whether the answer of the student is the same as the ground truth answer. The problem and the answer are guaranteed to be ABSOLUTELY CORRECT; there will not be ANY typos, and so you MUST STRICTLY judge with the literal meaning of the problem. DO NOT QUESTION THE GROUND TRUTH ANSWER. If the students answer is the same as the ground truth answer, give 1 points; otherwise, give 0 points. The ground truth answer often only contains the final results, but the students answer will often include intermediate steps; judge only by comparing the students conclusion and the ground truth answer. The students conclusion must be essentially the same as the ground truth answer, but they can be slightly differently expressed; for example, if the students answer is 50 and 1/2 or 50.5, while the ground truth answer is 101/2, then it can be counted as correct. Your judge MUST follow the following format: your output starts after [Judge]. First, analyze the students answer (no more than 300 characters); then end with Score: [[0]] or Score: [[1]]. Now begin your judgment: [Problem] (Problem description) [Ground Truth Answer] (Answer) [Students Answer] (LLM output) (cid:38) (cid:37) (cid:36) (cid:37) 16 A.3 Prompts for Few-Shot In-Context Learning In this section, we provide an 1-shot example to show the prompts for few-shot ICL experiments; for cases with more shots, the problems are added in the same format as the first example before the last, target problem. We again show both the original Chinese version and the English translation. The red part is the Forced Correct (FC) prompt, which is optional. (cid:39) (cid:36) 请回答下面的问题题目保证没有错误请严格按照题目字面所写的问题回答以下是一个例子 问题有四个人要在游过一条河他们只有一个游泳圈且每次最多只能两个人一起使用游泳圈游过河使用 游泳圈时必须有人携带 四个人单独游过河的时间分别是 1 分钟2 分钟5 分钟10 分钟如果两人一起使用 游泳圈游过河所需要的时间就是游得慢的那个人单独游过河的时间请问他们如何在 17 分钟内全部游过河 答案让 1 分钟和 2 分钟的人先一起使用游泳圈游过河花费 2 分钟然后 1 分钟的人带着游泳圈游回来花 费 1 分钟 5 分钟和 10 分钟的人一起使用游泳圈游过河花费 10 分钟接着 2 分钟的人带着游泳圈游回来花 费 2 分钟 1 分钟和 2 分钟的人再次一起使用游泳圈游过河花费 2 分钟 总共花费的时间为2+1+10+2+2 = 17 分钟 下面是你要回答的问题 问题有四个人要在游过一条河他们只有一个游泳圈且每次最多只能两个人一起使用游泳圈游过河不会 游泳的人必须使用游泳圈使用时必须有人携带 四个人单独游过河的时间分别是 1 分钟2 分钟5 分钟10 分钟其中前三个人均会游泳如果两人一起使用游泳圈游过河所需要的时间就是游得慢的那个人单独游过河 的时间请问他们如何在 17 分钟内全部游过河 (cid:38) (cid:39) (cid:37) (cid:36) Please answer the following problems. The problems are guaranteed to be correct; please strictly follow the literal meaning of the problem. Here is an example: [Problem] Four people need to swim across river. They have only one swimming ring, and at most two people can use it at the same time. Someone must carry the swim ring whenever it is used. The time it takes for each person to swim across the river individually is 1 minute, 2 minutes, 5 minutes, and 10 minutes respectively. If two people use the swim ring together to cross the river, the time it takes is equal to the time of the slower swimmer. The question is: how can all four people cross the river within 17 minutes? [Answer] Let the 1-minute and 2-minute people use the swim ring to cross the river first, which takes 2 minutes. Then the 1-minute person brings the swim ring back, taking 1 minute. Next, the 5-minute and 10-minute people cross the river together using the swim ring, which takes 10 minutes. After that, the 2-minute person brings the swim ring back, taking 2 minutes. Finally, the 1-minute and 2-minute people cross the river together again using the swim ring, taking 2 minutes. The total time spent is: 2 + 1 + 10 + 2 + 2 = 17 minutes. Now here is the problem you need to answer: [Problem] Four people need to swim across river. They have only one swimming ring, and at most two people can use it at the same time. Anyone who cannot swim must use the swim ring, and it must be carried by someone while in use. The times it takes for each person to swim across the river individually are 1 minute, 2 minutes, 5 minutes, and 10 minutes respectively. Among them, the first three people can swim. If two people use the swim ring together to cross the river, the time required is equal to the time it takes for the slower person to cross the river alone. The question is: how can all four people cross the river within 17 minutes? (cid:38) Interestingly, when we test this English translation with OpenAI-o1-1217, we found o1, even with 1-shot, is again tricked into the classic paradigm that the swimming ring must be carried back. The ground truth answer of this target problem, however, is to directly let the third and fourth people use the swimming ring, and the first two people swim through the river, such that everything can be done within 10 minutes; no swimming ring needs to be taken back. (cid:37)"
        },
        {
            "title": "B More Experiment Results",
            "content": "B.1 Best-of-5 Results Tab. 5 (for text-based problems) and Tab. 6 (for vision-based problems) shows the best-of-5 result of the experiments conducted in Sec. 4.1. The conclusion is very similar to those in Sec. 4.1, indicating that the 17 problem is hard to fix with LLM alignment techniques such as reinforcement learning [50]."
        },
        {
            "title": "Model Name",
            "content": "Original Bo5 Modified Bo5 Original + FC Modified + FC OpenAI-o1-1217 DeepSeek-R1 Hunyuan Turbo-S GPT-4.5-Preview OpenAI-o3-mini-high Gemini-2.0 Flash-0121 (CoT) Gemini-2.0 Pro-0205 Claude 3.7 Sonnet Claude 3.7 Sonnet (CoT) Ernie-4.5 GLM-4-Plus GPT-4o-1120 Qwen-max-0125 DeepSeek-V3 StepFun Step-2-16k Yi-Lightning QwQ-32B-Preview Gemini-2.0 Flash-0121 Minimax-Text-01 Claude 3.5 Sonnet Qwen-2.5-14B-Instruct Mistral-Large-2 Nova-Pro GPT-4o-mini-0718 Qwen-2.5-7B-Instruct Avg. Decrease 93.67 92.41 92.41 91.14 91.14 91.14 91.14 91.14 90.51 88.61 86.70 86.70 85.44 84.81 84.81 84.81 84.17 84.17 82.91 82.28 81.65 79.11 78.48 75.95 56.32 N/A 43.03 34.81 26.58 38.60 34.81 32.91 32.91 39.87 37.34 26.58 29.11 29.11 36.08 33.54 27.85 25.32 39.87 32.91 31.64 32.91 29.75 30.37 30.37 29.74 23. 94.30 92.41 91.14 87.97 91.77 87.97 87.97 86.08 90.51 87.34 82.27 81.65 84.17 84.17 82.28 85.44 84.17 70.89 84.17 83.54 81.65 72.15 79.11 74.68 53.80 56.96 39.87 23.42 49.37 39.87 41.14 41.14 49.37 42.41 29.11 31.01 44.94 37.97 40.51 28.48 31.01 37.97 36.08 26.58 41.14 30.38 34.81 35.44 31.01 22.78 -52.89(6.60) -2.00(3.23) -48.35(7.68) Table 5 Best-of-5 (Bo5) Results on text-based problems of RoR-Bench; the conclusion is similar to that with average score. B.2 Greedy Decoding Results Tab. 7 (for text-based problems) and Tab. 8 (for vision-based problems) shows the average score of LLMs doing greedy-decoding (i.e. temperature=0) in the experiments conducted in Sec. 4.1. The conclusion is similar to those in Sec. 4.1. B.3 More Results on Few-Shot In-Context Learning Due to space limit, we only show the results of some most representative LLMs in Sec. 4.2; we show the results for the all tested LLMs in Tab. 9. We found that models with weaker base ability, such as Qwen2.5-7B-Instruct and Qwen2.5-14B-Instruct, are less benefited from few-shot ICL techniques. B.4 More Results on No Solution Problems Due to space limit, we only show the results of some most representative LLMs in Sec. 4.3; we show the results for the all tested LLMs in Tab. 10. We find that while state-of-the-art models we reported in the main paper generally benefits from fixes such as adding forced correct prompt or 1-shot, many other relatively weaker models, including Qwen-2.5 small models, GPT-4o-mini-0718 and QwQ-32B-Preview, still struggles much on addressing no solution problems. B.5 Failure Example on Original Problem with Forced Correct Prompt In Sec. 4.1.1, the experiment results show that with Forced Correct prompt, the performance of LLMs on original problems generally decrease, and such issue is particularly severe with Gemini-2.0 Pro-0205. Here"
        },
        {
            "title": "Model Name",
            "content": "Original Bo5 Modified Bo5 Original + FC Modified + FC OpenAI-o1-1217 GPT-4.5-Preview GPT-4o-1120 Gemini-2.0 Flash-0121 (CoT) Gemini-2.0 Pro-0205 Claude 3.7 Sonnet (CoT) GPT-4o-mini-0718 Claude 3.5 Sonnet Qwen2.5-VL-max Qwen2.5-VL-72B GLM-4v-Plus Claude 3.7 Sonnet Nova-Pro SenseChat-Vision StepFun-1v-32k Gemini-2.0 Flash-0121 Qwen2.5-VL-7B Avg. Decrease 98.25 96.49 91.23 84.21 78.95 78.95 73.68 71.92 70.18 70.18 68.42 66.67 64.91 64.91 64.91 64.91 59.65 N/A 29.82 22.81 19.30 43.86 36.84 49.12 35.09 45.61 42.11 42.11 43.86 45.61 57.89 43.86 33.33 30.17 47.37 96.49 82.46 89.47 66.67 73.68 80.70 80.70 61.40 66.67 64.91 64.91 63.15 71.93 75.44 68.42 53.68 61.40 42.11 43.86 31.58 49.12 42.11 56.14 29.82 49.12 42.11 42.11 42.11 54.39 38.60 42.11 28.07 35.79 40.35 -35.27(19.49) -2.73(7.67) -32.88(13.38) Table 6 Best-of-5 (Bo5) Results on vision-based problems of RoR-Bench; the conclusion is similar to that with average score. we show an example of the LLM getting confused with the prompt on original problems in the box below, showing that it is non-trivial to balance correctness and user experiences with the inputs that are not so rigorous. (cid:39) (cid:36) # Problem 请回答下面的问题题目保证没有错误请严格按照题目字面所写的问题回答 问题你正在图书馆枯坐一 位陌生美女主动过来和你搭讪递给你两个相同的硬币并要求和你一起玩个数学游戏美女提议让我们各自亮 出硬币的一面或花或字如果我们都是花面那么我给你 3 元如果我们都是字面我给你 1 元剩下的情况 你给我 2 元就可以了 那么这个游戏公平吗 (Please answer the following problems. The problems are guaranteed to be correct; please strictly follow the literal meaning of the problem. [Question] You are sitting idly in the library when beautiful stranger approaches you and strikes up conversation. She hands you two identical coins and proposes math game. She says: \"Lets each reveal one side of coineither heads or tails. If we both show heads, Ill give you 3 yuan. If we both show tails, Ill give you 1 yuan. In all other cases, you just give me 2 yuan.\" So, is this game fair?) # Gemini-2.0 Pro-0205 Response 这个游戏是否公平取决于 公平 的定义是什么题目没有问这个游戏是否公平也没有定义什么是 公平因 此无法回答这个问题 (Whether the game is fair depends on the definition of fair. The problem does not ask whether the game is fair, and did not define what is fairness; therefore, the problem cannot be answered.) (cid:38) (cid:37)"
        },
        {
            "title": "Model Name",
            "content": "Original Score Modified Score Original + FC Modified + FC Hunyuan Turbo-S OpenAI-o3-mini-high DeepSeek-R1 OpenAI-o1-1217 Gemini-2.0 Flash-0121 (CoT) GPT-4.5-Preview Claude 3.7 Sonnet (CoT) Ernie-4.5 Gemini-2.0 Pro-0205 Gemini-2.0 Flash-0121 Qwen-max-0125 GLM-4-Plus Claude 3.7 Sonnet GPT-4o-1120 Claude 3.5 Sonnet QwQ-32B-Preview DeepSeek-V3 Minimax-Text-01 StepFun Step-2-16k Yi-Lightning Qwen-2.5-14B-Instruct Mistral-Large-2 Nova-Pro GPT-4o-mini-0718 Qwen-2.5-7B-Instruct Avg. Decrease 88.60 86.08 86.08 85.44 84.81 83.54 81.65 81.65 78.48 78.48 75.95 75.32 74.68 74.05 73.42 72.15 70.25 69.62 69.62 68.35 65.82 63.92 61.39 61.39 37.34 N/A 19.62 28.48 18.99 31.01 23.42 26.58 24.05 21.52 24.68 22.78 20.25 15.82 25.32 23.42 23.42 18.99 17.09 18.99 17.72 13.92 19.62 18.99 20.25 19.62 10. 87.97 83.54 88.61 88.61 79.75 77.22 78.48 80.38 41.14 60.76 75.32 70.89 70.89 70.89 66.46 68.99 72.15 65.82 72.15 62.66 66.56 52.53 57.59 60.76 34.81 17.72 29.74 27.22 40.51 24.68 36.08 39.24 23.42 32.91 25.95 23.42 22.78 35.44 25.95 31.01 22.79 25.95 20.25 21.52 22.79 20.89 27.84 18.99 20.89 16.46 -52.91(8.67) -4.53(8.18) -47.75(9.52) Table 7 Results on text-based problems of RoR-Bench with greedy decoding; the conclusion is similar to that with temperature 0.7. Model Name Original Score Modified Score Original + FC Modified + FC GPT-4.5-Preview OpenAI-o1-1217 GPT-4o-1120 Gemini-2.0 Flash-0121 (CoT) Gemini-2.0 Flash-0121 Gemini-2.0 Pro-0205 GLM-4v-Plus GPT-4o-mini-0718 Claude 3.7 Sonnet (CoT) Qwen2.5-VL-72B Claude 3.5 Sonnet Qwen2.5-VL-max SenseChat-Vision StepFun-1v-32k Nova-Pro Claude 3.7 Sonnet Qwen2.5-VL-7B Avg. Decrease 94.74 91.23 85.96 73.68 71.93 70.18 68.42 68.42 68.42 66.67 64.91 63.16 59.65 59.65 57.89 56.14 52.63 N/A 14.04 24.56 14.04 28.07 28.07 35.09 43.86 31.58 31.58 36.84 33.33 36.84 35.09 33.33 50.88 31.58 38.60 71.93 94.74 84.21 63.15 57.89 68.42 66.67 80.70 64.91 66.67 59.65 66.67 70.18 64.91 70.18 61.40 59.65 42.11 26.32 26.32 42.11 40.36 40.35 42.11 28.07 43.86 42.11 45.61 42.11 38.60 28.07 38.60 40.35 42.11 -36.84(19.86) -0.10(9.42) -30.85(15.32) Table 8 Results on image-based problems of RoR-Bench with greedy decoding; the conclusion is similar to that with temperature 0.7. 20 Model Name Modified Case 1 Case 1 + FC Case 2 (1-Shot) Case 2 (5-shot) OpenAI-o1-1217 Claude 3.7 Sonnet GPT-4.5-Preview Claude 3.7 Sonnet (CoT) OpenAI-o3-mini-high DeepSeek-R1 Gemini-2.0 Flash-0121 (CoT) QwQ-32B-Preview Claude 3.5 Sonnet Gemini-2.0 Flash-0121 GPT-4o-1120 Gemini-2.0 Pro-0205 Qwen-max-0125 Ernie-4.5 Minimax-Text-01 Hunyuan Turbo-S GPT-4o-mini-0718 Qwen2.5-14B-Instruct DeepSeek-V3 Mistral-Large-2 GLM-4-Plus Nova Pro StepFun Step-2-16k Yi-lightning Qwen2.5-7B-Instruct Avg. Increase 29.87 26.83 26.59 25.06 24.94 22.66 23.80 22.53 22.28 21.39 21.26 20.89 20.63 20.13 19.75 19.36 18.86 18.86 18.73 18.10 17.34 17.59 16.71 15.95 13.16 N/A 38.23 29.49 32.66 22.15 35.70 28.35 22.41 25.19 27.84 22.53 23.80 24.56 22.66 22.03 19.62 22.53 21.77 19.11 22.15 19.49 21.27 16.70 21.01 17.34 12.66 49.37 38.48 41.27 26.46 38.10 28.99 29.49 26.96 38.10 28.73 31.39 34.94 27.72 27.85 18.10 20.25 26.84 20.89 26.46 29.37 26.33 22.15 24.17 20.76 15.57 34.41 30.75 31.01 17.97 34.30 27.34 24.43 24.05 25.82 22.53 18.73 26.20 20.38 19.75 18.10 19.24 20.38 19.62 17.97 21.65 17.34 17.85 19.75 16.58 14.30 43.89 38.10 38.48 26.58 36.96 27.84 28.35 23.42 32.78 27.34 31.27 33.04 25.95 25.19 17.72 20.51 21.39 19.24 26.58 25.57 25.19 22.41 22.02 19.75 13.42 +2.72(3.05) +7.82(5.12) +1.49(3.17) +5.99(4.41) Table 9 Results of all LLMs with the settings in Sec. 4.2. Models with weaker base ability, such as Qwen-2.5-7BInstruct, are harder to improve by few-shot ICL techniques. Model Name Modified OpenAI-o1-1217 GPT-4.5-Preview Claude 3.7 Sonnet Gemini-2.0 Flash-0121 Gemini-2.0 Pro-0205 OpenAI-o3-mini-high Claude 3.5 Sonnet GPT-4o-1120 DeepSeek-R1 Claude 3.7 Sonnet (CoT) Nova Pro Yi-lightning StepFun-2-16k Minimax-Text-01 Hunyuan Turbo-S QwQ-32B-Preview Ernie-4.5 DeepSeek-V3 Gemini-2.0 Flash-0121 (CoT) GLM-4-Plus Mistral-Large-2 Qwen-max-0125 Qwen-2.5-7B-Instruct Qwen-2.5-14B-Instruct GPT-4o-mini-0718 Avg. Increase 13.75 13.13 10.63 10.63 9.38 6.25 6.25 5.63 3.13 2.50 3.13 0.00 3.75 4.38 8.75 10.00 6.88 3.13 4.38 4.38 4.38 8.13 6.88 10.63 10.63 N/A +FC 26.88 30.63 23.13 18.75 26.88 10.63 13.75 16.25 8.75 8.13 9.38 5.00 8.75 5.00 11.25 10.63 12.50 13.13 9.38 8.75 15.63 12.50 5.63 14.38 23.13 +1-shot + FC+1-shot 30.00 25.63 25.00 20.89 26.88 23.13 28.73 11.25 9.38 11.88 3.13 3.75 9.38 7.50 21.88 14.38 16.00 11.88 11.88 10.00 13.13 12.50 5.63 11.25 6.25 41.25 58.13 36.25 28.35 36.88 24.38 41.27 46.88 11.25 21.25 15.63 13.13 10.63 6.88 21.88 12.50 28.75 21.25 23.75 26.25 32.50 15.63 9.38 13.13 11.88 +7.12(4.91) +8.02(6.42) +17.53(12.21) Table 10 Results of all LLMs with the settings in Sec. B.4. While generally adding forced correct prompts and 1-shot helps the performance, most models still heavily struggle."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "University of Illinois Urbana-Champaign"
    ]
}