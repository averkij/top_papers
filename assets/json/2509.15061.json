{
    "paper_title": "Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue",
    "authors": [
        "Xingyao Lin",
        "Xinghao Zhu",
        "Tianyi Lu",
        "Sicheng Xie",
        "Hui Zhang",
        "Xipeng Qiu",
        "Zuxuan Wu",
        "Yu-Gang Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The ultimate goal of embodied agents is to create collaborators that can interact with humans, not mere executors that passively follow instructions. This requires agents to communicate, coordinate, and adapt their actions based on human feedback. Recently, advances in VLAs have offered a path toward this goal. However, most current VLA-based embodied agents operate in a one-way mode: they receive an instruction and execute it without feedback. This approach fails in real-world scenarios where instructions are often ambiguous. In this paper, we address this problem with the Ask-to-Clarify framework. Our framework first resolves ambiguous instructions by asking questions in a multi-turn dialogue. Then it generates low-level actions end-to-end. Specifically, the Ask-to-Clarify framework consists of two components, one VLM for collaboration and one diffusion for action. We also introduce a connection module that generates conditions for the diffusion based on the output of the VLM. This module adjusts the observation by instructions to create reliable conditions. We train our framework with a two-stage knowledge-insulation strategy. First, we fine-tune the collaboration component using ambiguity-solving dialogue data to handle ambiguity. Then, we integrate the action component while freezing the collaboration one. This preserves the interaction abilities while fine-tuning the diffusion to generate actions. The training strategy guarantees our framework can first ask questions, then generate actions. During inference, a signal detector functions as a router that helps our framework switch between asking questions and taking actions. We evaluate the Ask-to-Clarify framework in 8 real-world tasks, where it outperforms existing state-of-the-art VLAs. The results suggest that our proposed framework, along with the training strategy, provides a path toward collaborative embodied agents."
        },
        {
            "title": "Start",
            "content": "Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue Xingyao Lin1,2, Xinghao Zhu3, Tianyi Lu1, Sicheng Xie1,2, Hui Zhang1, Xipeng Qiu1,2, Zuxuan Wu1,2, Yu-Gang Jiang1 5 2 0 2 9 1 ] . [ 2 1 6 0 5 1 . 9 0 5 2 : r Abstract The ultimate goal of embodied agents is to create collaborators that can actively interact with humans, not mere executors that passively follow instructions. This requires agents to communicate, coordinate, and adapt their actions based on human feedback. Recently, advances in Vision-Language-Action models (VLAs) have offered promising path toward this goal. However, most current VLA-based embodied agents operate in simple, one-way mode: they receive an instruction and execute it without any feedback or clarification. This passive approach fails in real-world scenarios where instructions are often ambiguous. In this paper, we address this problem with the Ask-to-Clarify framework. Our framework first resolves ambiguous instructions by asking clarifying questions in multi-turn dialogue. Then it generates low-level actions for realworld embodied tasks in an end-to-end manner. Specifically, the Ask-to-Clarify framework consists of two key components, one Vision-Language model (VLM) for collaboration and one diffusion model for action generation. We also introduce connection module that generates conditions for the diffusion model based on the output of the VLM. This module adjusts the observation using the instructions to create better and more reliable conditions. We train our framework with twostage knowledge-insulation training strategy. First, we fine-tune the collaboration component using specific ambiguity-solving interactive dialogue data to handle ambiguous instructions. Then, we integrate the action component while keeping the collaboration component frozen. This preserves the interaction abilities while fine-tuning the diffusion expert to generate lowlevel actions. The unique training strategy guarantees our framework can first ask questions for ambiguous instructions, then generate low-level actions end-to-end. During inference, signal detector functions as router that helps our framework seamlessly switch between asking questions and taking actions. We evaluate the Ask-to-Clarify framework in 8 real-world tasks, where it significantly outperforms existing state-of-theart VLAs. The results suggest that our proposed framework, along with the training strategy, provides new path toward building truly collaborative embodied agents. I. INTRODUCTION Embodied agents are intelligent systems designed to perceive and act within the physical world. Unlike systems that only solve abstract problems in virtual space, embodied agents must navigate the complexity and unpredictability of real-world environments [1]. In real-world scenarios, embodied agents face an extra challenge not present in virtual environments collaboration with humans. Collaboration behavior refers to the agents ability to communicate, coordinate, and adapt its actions based on human feedback. denotes the corresponding author 1College of Computer Science and Artificial Intelligence, Fudan University, Shanghai, China. 2Shanghai Innovation Institute, Shanghai, China. 3Mechanical Systems Control Lab, UC Berkeley, California, USA. Fig. 1: Difference between the executor and the collaborator. The executor passively follows instructions, which may lead to failure. In contrast, the collaborator actively communicates with the user to understand the intended instruction and successfully complete the task. Thus, the ultimate goal of embodied agents is to create collaborators that can actively interact with humans in the real-world environment, not mere executors that passively follow instructions. For decades, central goal in robotics has been to create such versatile and adaptive agents [2, 3, 4]. Recently, the emergence of large-scale pre-trained VisionLanguage Models has offered promising path forward. key focus of current research is leveraging these powerful VLMs to create VLAs. By building on the vast, internet-scale knowledge of VLMs, VLAs have shown great potential for developing generalist embodied agents in the real world. However, most current VLA-based embodied agents operate as passive executors in simple, one-way mode. They receive an instruction and execute it without any feedback or clarification, which limits their applicability in real-world scenarios. Consider common household scenario: person asks robot to Give me the mug when multiple mugs are on the table. An ideal agent would resolve this ambiguity by interacting with the user to identify the correct mug. In contrast, current agents are likely to pick one at random without asking for clarification or simply fail to execute. This limitation prevents them from being truly collaborative and reliable partners in real-world settings. To address this ambiguity problem, researchers have begun to explore collaborative embodied agents. Prior works [5, 6, 7] study human-robot interaction to resolve ambiguity in the instruction. For example, ASK-TO-ACT [7] fine-tunes VLM to ask questions to resolve ambiguity and control the robot with set of oracle high-level actions. DialFRED [5] enables an agent to actively ask questions to the human user to better complete its task in the simulation environment. TEACh [6] introduces dataset of human-human, interactive dialogues to complete household tasks in simulation. Despite this progress, existing solutions have some limthey primarily operate in simulation enviitations. First, ronments, not the real world. Second, they rely on highlevel actions, such as MoveRight and PickUpObj. This reliance on high-level actions is insufficient for complex tasks that require fine-grained manipulation, like those in household or factory settings. Furthermore, high-level actions require an additional planner to translate commands into robot translation. In contrast, low-level actions allow for direct control and end-to-end learning, potentially enabling the discovery of novel and more efficient skills. In this paper, we address these limitations and build collaborative embodied agent by proposing the Ask-toClarify framework. The framework first resolves ambiguous instructions by asking clarifying questions in multi-turn dialogue and then generates low-level actions end-to-end for embodied tasks. Instead of passively following commands, our framework actively engages with human users. When faced with ambiguity, it asks clarifying questions, effectively interacting with the user in real-time. This transforms the agent from mere executor into collaborator. Figure 1 illustrates the difference between the executor and the collaborator. As the figure shows, an executor passively follows an instruction even if it is ambiguous, which may lead to failure. On the contrary, the collaborator could actively interact with the human user to deduce the correct instruction from the ambiguous one before completing the task. Specifically, the Ask-to-Clarify framework consists of two key components, one VLM for collaboration and one diffusion model for action generation. For the smooth connection of these components, we introduce connection module that generates conditions for the diffusion model based on the output of the VLM. This module creates better and more reliable conditions by adjusting the observation using the instructions. We design two-stage knowledgeinsulation training strategy for our framework. In stage 1, the framework acquires the ability to clarify the ambiguous instructions by asking questions. In stage 2, the framework learns how to generate low-level actions end-to-end for real-world embodied tasks. This unique training strategy guarantees our framework can first ask questions to clarify the ambiguity in multi-turn dialogue, then generate lowlevel actions in an end-to-end manner. In inference time, signal detector enables our framework to seamlessly switch between asking questions and taking actions. We evaluate the Ask-to-Clarify framework in 8 real-world tasks and compare it with state-of-the-art VLAs, such as π0 [8], π0-FAST [9] and OpenVLA-OFT [10]. Our framework significantly outperforms these VLAs across all tasks. The results suggest that our proposed framework, along with the training strategy, opens up new path for building truly collaborative embodied agents. In summary, our contributions are threefold: We introduce new task and framework for collaborative embodied agents. The task requires the embodied agent to first ask clarifying questions to solve ambiguity in instructions, then execute the instruction successfully. The framework provides solution to this task by using VLM for asking questions and diffusion model for executing. We introduce connection module as smoother link between the two components to create more reliable conditions for the diffusion model based on the output of the VLM. We propose two-stage knowledge-insulation training strategy for our framework. This strategy first empowers the framework to interact with humans, then enables the framework to perform embodied tasks while preserving the interaction ability by insulating the VLM of the framework during stage 2 training. We conduct extensive real-world experiments to evaluate our framework. The results demonstrate its effectiveness and reveal insights into the design of the Askto-Clarify framework. II. RELATED WORK A. Embodied Agents for Manipulation Recent works have explored using Vision-Language models (VLMs) to build Vision-Language-Action models (VLAs). These models learn to map robot observations to actions and also benefit from large-scale pretraining on visionlanguage data from the web [11, 12, 10, 13, 14, 8, 9, 15, 16]. One line of work focuses on creating autoregressive VLAs. These models can sequentially predict action tokens and use different tokenizers to decode action tokens into low-level actions. OpenVLA [12] maps continuous robot actions to the discrete tokens of the VLMs original tokenizer. π0-FAST [9] applies the discrete cosine transform followed by byte-pair encoding to compress action chunks into tokens. Although these VLAs could be fine-tuned efficiently from pretrained VLMs, their autoregressive action generation is often too slow for high-frequency robot control. Another set of work introduces an action expert to explicitly decouple action generation from VLMs. This separation enables faster response time and smoother real-world deployment. These works adopt two distinct architectures: VLA with hierarchical or parallel action expert. For VLA with hierarchical action expert, they separate high-level cognition from low-level action generation in serial manner. The VLM backbone transmits features to the action expert to convey information derived from visual, textual, and robot state inputs. For example, DexVLA [17] introduces diffusion-based action expert and designs an embodiment curriculum learning strategy to support efficient training. CogACT [18] introduces the diffusion transformer [19] as an action model, and it also proposes an adaptive action ensemble strategy for smoother and more efficient movement trajectories. For VLA with parallel action expert, the VLM backbone and the action expert operate in parallel and interact to exchange information during inference. For example, π0 [8] initializes its backbone weights from pretrained VLM, and it trains another set of independent weights, i.e., the flowmatching action expert, to handle robot-specific inputs and action generation. GraspVLA [20] proposes dual-system model via Progressive Action Generation, which unifies autoregressive perception tasks and flow-matching action generation into Chain-of-Thought process. B. Agents that Collaborate with Humans Robots operating in human spaces must be able to engage in natural language interaction. Prior works [5, 6, 21, 7, interaction using 22, 23, 24, 25, 26] study human-robot interactive dialogues. These methods allow the agent to actively ask questions to the human user. The agent then uses the additional information from the users response to better complete its task. For example, ASK-TO-ACT [7] fine-tunes VLM to resolve ambiguity through asking clarification questions and control the robot using oracle high-level actions. DialFRED [5] allows an agent to actively ask questions to the human user to improve performance in the simulation environment. TEACh [6] introduces dataset of interactive human-human dialogues to train embodied agents for completing household tasks in simulation. ASK4HELP [21] proposes methods that allow embodied agents to request and use expert assistance when needed. However, most existing approaches control the robot with high-level actions [26, 7, 5, 6, 21], e.g., MoveRight and PickUpObj. This reliance on high-level actions is insufficient for complex tasks that require precise manipulation, as often seen in household or industrial environments. Moreover, high-level actions require an external planner to convert them into robot translation. In contrast, low-level actions allow for direct control and end-to-end learning. This enables the embodied agent to potentially discover novel and more efficient skills. In this paper, we introduce new framework for collaborative embodied agents that interact with humans and generate low-level actions in an end-to-end manner. III. METHOD The ultimate goal of embodied agents is to create collaborators, not mere executors [1]. Current embodied agents function as executors, operating in simple, one-way mode: they execute the received instruction without any feedback or clarification. [12, 27, 13]. This one-way approach fails to address natural ambiguities in human instructions [7]. To resolve these ambiguities, we need bi-directional feedback loop. We therefore propose Ask-to-Clarify, new framework with two-stage knowledge-insulation training strategy [15, 17]. Our framework first asks questions to clarify the ambiguity in instruction. Then it generates low-level actions in an end-to-end manner. Through this interaction, it deduces correct instructions from human responses and completes the task more accurately. Our framework consists of two main components. The collaboration component is initialized from standard VLM [28] trained on vision-language tasks, while the action component is initialized from pre-trained action expert [29]. To smoothly link the two components, we add connection module that adjusts observations based on instructions [30], which guarantees more reliable conditions for the action component. We train the Ask-to-Clarify framework using our two-stage knowledge-insulation training strategy. We first fine-tune the collaboration component to enable the ambiguity-solving ability with specific dialogue data. Next, we integrate the action component while keeping the collaboration component frozen. This preserves the interaction abilities while fine-tuning the diffusion expert to generate low-level actions. In inference time, our framework seamlessly switches between asking and action with the help of the signal detector. In this section, we first define the embodied task with bi-directional feedback loop in Section III-A. Next, we introduce the two-stage knowledge-insulation training strategy in Section III-B. Finally, we present our inference pipeline with the signal detector in Section III-C. A. Task Definition Picture an embodied agent performing tasks by following human instructions in home environment. For example, if single mug is on desk, the instruction Give me the mug is unambiguous. The robot can simply execute the command. However, the task becomes ambiguous if multiple mugs are present. In this scenario, capable robot should ask the human user for clarification. Nevertheless, existing VLAs lack this Ask-to-Clarify ability, which hinders their deployment in real-world human spaces. To evaluate whether embodied agents can ask questions to resolve ambiguity and deduce the correct one based on the answers, we propose new task. This task requires the agent to complete two steps. First, the agent needs to interact with the human users to deduce the correct instruction from the ambiguous one. Second, it must execute the correct instruction successfully. This task requires agents to ask questions to clarify the ambiguous instruction. It better reflects real-world scenarios compared to current tasks that only require the embodied agent to operate in simple, one-way mode. Formally, given an ambiguous instruction Ia and visual observation O, the agent engages in multi-turn dialogue to resolve the ambiguity. At each step i, it generates clarification question Qi = fAsktoClarify(Ia, O, A<i) and receives an answer Ai from the user. After rounds, the agent infers the correct instruction Ic = fAsktoClarify(Ia, Q1:n, A1:n), and then executes low-level actions a1:T = fAsktoClarify(Ic, O) to complete the task. B. Training Strategy To train our framework for collaborative embodied agents that interact with humans and generate low-level actions in an end-to-end manner, we adopt two-stage knowledgeinsulation training strategy. This training strategy can first Fig. 2: The Ask-to-Clarify framework. We train the Ask-to-Clarify framework using two-stage knowledge-insulation strategy. First, we fine-tune the collaboration component (VLM) with ambiguity-solving dialogue data to enable clarification ability. Next, we freeze the collaboration component and integrate the action component (diffusion expert), fine-tuning the diffusion expert to generate low-level actions while preserving interaction ability. The signal token (Sig. token) is supervised during training as part of the VLMs output, but is only used at inference time. empower the framework with ambiguity-solving ability, then enable the model to generate low-level actions in an end-toend manner, while preserving the ambiguity-solving ability from being forgotten through knowledge insulation. Ambiguity-solving Ability The ambiguity-solving ability of the Ask-to-Clarify framework is learned from targeted ambiguity-solving interactive dialogue data. We first collect images of multiple objects that differ from each other in some but not all attributes, e.g., two blocks that differ only in color. Then we use LLMs to generate the ambiguous instruction, the question-answer pairs, and the correct instruction based on the collected images. Finally, we use this generated content to construct our dialogue data. This process creates our ambiguity-solving interactive dialogue dataset. We train the collaboration component of Ask-to-Clarify using the interactive dialogue data. To guarantee our framework could seamlessly switch between asking and acting, we add new signal tokens to the pre-trained VLM during this stage of training. Therefore, the Ask-to-Clarify framework learns to use <AMBG> to denote that the instruction is ambiguous and forward to ask clarified questions. Once the ambiguity is solved through dialogue, the Ask-to-Clarify framework uses <NOT AMBG> to mark the correct instruction it has generated and takes this instruction as input. Upon receiving the correct instruction, the Ask-to-Clarify framework gives out <ACT> or <REJ> based on the instruction and observation, i.e., only execute when the target object is in the observation else refuse to act. In this training stage, we focus solely on the VLM by decoupling the action component. We freeze the vision encoder and fine-tune only the LLM part of the VLM to take advantage of the knowledge learned during VLM pretraining. By doing so, we efficiently adapt the collaboration component to ask clarifying questions and deduce correct instructions from ambiguous ones in embodied tasks. End-to-end Action Generation In this stage, we provide way of end-to-end action generation through our hierarchical framework while preserving the ambiguity-solving ability learned from the last stage by knowledge insulation. This aims to build an embodied agent that can engage in dialogue to clarify the ambiguous instruction before taking action. We implement knowledge insulation by freezing the collaboration component of the Ask-to-Clarify framework. This prevents catastrophic forgetting of dialogue abilities during embodied training. We call this approach knowledge insulation because we protect knowledge by isolating its container (the VLM) during training. To smoothly connect the VLM and the diffusion model, we introduce connection module to create reliable conditions for the latter. The module first extracts the instruction and observation from the VLMs output and then modulates the observation using the instruction. The output of the module obtains comprehensive information about both the environment and the humans intention. Thus, it provides the optimal condition for the diffusion expert, enabling successful action generation. For training data, we use expert demonstrations with correct instructions. This equips the Ask-to-Clarify framework with real-world embodied capabilities. Through this two-stage knowledge-insulation training strategy, we train collaborative embodied agent that can clarify the knowledge insulation in multi-turn dialogue and perform real-world embodied tasks. Our training strategy is illustrated in Figure 2. C. Inference the collaboration component of Ask-to-Clarify first evaluates its ambiguity. For correct instructions, it outputs <ACT> to proceed with execution. For ambiguous instructions, which are common in real-world human environments, the collaboration component outputs questions marked by <AMBG> and waits for the human user to respond. Each round of questions and answers is appended to the dialogue history, which is used as input for the collaboration component in the next round. After several rounds of question-answering, the collaboration component infers the correct instruction through dialogue and marks it with <NOT_AMBG>. The detector would extract the correct instruction from the VLMs output and feed it back to the VLM. The VLM would output <ACT> or <REJ>, depending on whether the target object in the correct instruction is in the observation. The working flow of the signal detector can be found in Figure 3. IV. EXPERIMENTS Fig. 3: The signal detector of the Ask-to-Clarify framework. The signal detector functions as router for the seamless switching of the Ask-to-Clarify framework between collaboration and actions. During inference, our Ask-to-Clarify framework acts as collaborative embodied agent. It first engages in ambiguitysolving dialogue with the human user, then executes realworld tasks based on the correct instruction inferred from the dialogue. To seamlessly connect the ambiguity-solving ability of the VLM and the embodied ability of the diffusion expert, we introduce training-free signal detector that serves as router during inference. This detector receives the signal token at the end of the VLMs output and determines subsequent movements based on that token. We introduce four signal tokens during the first stage of training, <AMBG>, <NOT AMBG>, <ACT> and <REJ>. These tokens cover all possible scenarios in real-world, ambiguous, embodied tasks. Upon receiving the instruction, Fig. 4: Examples of our tasks. The figure shows one task for each general type: Put the Apple on the plate (top), Pour the water from the Green cup onto the plate (middle), Stack the Yellow block on top of the Blue block (bottom). The Ask-to-Clarify framework is designed to act as collaborative embodied agent. To build such an embodied agent, we introduce two-stage knowledge-insulation training strategy to effectively train the Ask-to-Clarify framework, which consists of the collaboration component and action component. The collaboration component is capable of interacting with humans, and the action component generates actions in an end-to-end manner. To achieve smooth connection between the collaboration and action components, we use connection module that allows the collaboration output to better guide the action component. To evaluate our framework, we conduct series of realworld experiments. First, we conduct real-world experiments to evaluate whether our framework acquires the Ask-toClarify ability during training and to assess its overall performance. These experiments require the agent to first resolve ambiguity through dialogue with user, and then Method π0 [8] π0-FAST [9] OpenVLA-OFT [10] Ours Put the Object on the plate Pour the water from the Color cup onto the plate Stack the Color1 block on top of the Color2 block Apple 19/20 0/20 0/20 19/20 Peach 18/20 0/20 0/20 20/20 Orange Average 18/20 0/20 0/20 18/ 91.7% 00.0% 00.0% 95.0% Red 18/20 0/20 0/20 19/20 Green 19/20 0/20 0/20 20/20 White Average (Blue, Yellow) (Yellow, Blue) Average 19/20 0/20 0/20 20/20 93.3% 00.0% 00.0% 98.3% 14/20 0/20 0/20 18/20 9/20 0/20 0/20 18/20 57.5% 00.0% 00.0% 90.0% TABLE I: Overall comparison with baselines. We use ambiguous instructions for our framework while directly using correct instructions for baselines. execute the task based on the clarified instruction. Next, we perform ablation studies on our key designs, namely the training strategy and the connection module. Furthermore, we design experiments about the collaboration ability of our framework. Finally, we test the robustness of our Askto-Clarify framework in out-of-the-box environments, i.e., under low lighting conditions and with distractors present. A. Experimental Setup Robot We perform real-world experiments using an xArm 7 [31], which has 7 DoFs and 1-DoF gripper. We control the xArm using the official xArm API [32]. We use RealSense D435 for the wrist and third-view camera. Tasks We design set of real-world tasks to evaluate our framework. These tasks strictly follow the definition in Section III-A. In each task, the model must first infer the correct instruction from the ambiguous one and then generate low-level actions accordingly. We created 8 specific tasks, which fall into 3 general types: Put the fruit on the plate Put the Object on the plate, where Object {Apple, Peach, Orange} (3 tasks). Pour the water from the cup onto the plate Pour the water from the Color cup onto the plate, where Color {Red, Green, White} (3 tasks). Stack the blocks together Stack the Color1 block on top of the Color2 block, where (Color1, Color2) {(Blue, Yellow) , (Yellow, Blue)} (2 tasks). Training Data For stage 1 training, we use Qwen3-235BA22B [33] to generate ambiguity-solving interactive dialogue data. For stage 2 training, We manually collect 10 demonstrations for each of the 8 tasks through teleoperation [34] using Meta Quest 3. Implementation Details We use Qwen2-VL-2BInstruct [28] as the collaboration component and ScaleDPHuge [29] as the action component. We set chunk size of 50 timesteps for our framework. The details of training are illustrated in Table II. Learning rate Batch size #Epoch #Training params Data type Stage 1 Stage 2 1e-5 2e-5 128 64 50 Dialogue 1.5B 978M Embodied TABLE II: Training details of the Ask-to-Clarify framework. We use Qwen2-VL-2B-Instruct [28] as the collaboration component and ScaleDP-Huge [29] as the action component. In stage 1, we fine-tune all the parameters of the LLM in the collaboration component. In stage 2, we fine-tune the entire action component while insulating the collaboration component. B. Real-world Evaluation Comparison with Baselines We compare the Ask-toClarify framework with π0, π0-FAST and OpenVLA-OFT across 8 real-world tasks. As detailed in Table I, our framework achieves significantly higher success rate across all 8 tasks. Specifically, it obtains strong average success rates of 95.0%, 98.3%, and 90.0% on the 3 main task types. In our evaluation, we apply ambiguous instructions to the Ask-to-Clarify framework, requiring it to clarify the correct instruction through dialogue with human users. In contrast, all baselines are directly provided with correct instructions. We attribute this superior performance to our frameworks unique design. It effectively combines VLM for collaboration, diffusion model for action generation, and connection module to link the VLM and the diffusion model. The results indicate our framework is capable of engaging in dialogue to solve ambiguity by asking clarifying questions, then generating actions end-to-end to perform embodied tasks with high reliability. Apple Orange Peach Apple 1 98.44% 98.83% Orange 98.44% 1 99.22% Peach 98.83% 99.22% 1 TABLE III: The similarity matrix. We save the conditions of OpenVLA-OFT facing different instructions. We average all conditions through time under the same instruction. Furthermore, we observe that some state-of-the-art baselines are unable to complete our tasks. It appears that only the VLA with parallel action expert (π0) could perform the task reliably, while the autoregressive VLA (π0-FAST) and the VLA with hierarchical action expert (OpenVLAOFT) could not. As an autoregressive VLA, π0-FAST fails likely because it lacks an action expert. However, OpenVLAOFT, which uses diffusion expert, also performs poorly. To investigate this phenomenon, we compute the cosine similarity of the conditions to the diffusion expert when OpenVLA-OFT is faced with different instructions, such as Put the Apple on the plate, Put the Peach on the plate, Put the Orange on the plate. The result is shown in Table III and all three conditions are extremely similar, which is responsible for the poor performance of OpenVLAOFT. Nevertheless, our framework also adopts hierarchical architecture while establishing strong performance across all tasks. We attribute this to our connection module, which adjusts observations based on instructions, thus generating distinct and reliable conditions for the diffusion expert. Vision encoder LLM Modulation module Action component (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) Put the Apple on the plate Put the Orange on the plate Put the Peach on the plate Ambg. 0/10 0/10 9/10 0/10 Corr. 0/10 0/10 9/10 0/10 Ambg. 0/10 0/10 10/10 0/10 Corr. 0/10 0/10 8/10 0/10 Ambg. 0/10 0/10 9/10 0/10 Corr. 0/10 0/10 10/10 0/10 TABLE IV: Performance comparison of different training strategies in stage 2. We evaluate the Ask-to-Clarify when training different parts of the framework, such as the vision encoder, the LLM, the connection module, and the action component in stage 2. We use both ambiguous instructions (Ambg.) and correct instructions (Corr.) for evaluation. Training Strategy We evaluate the Ask-to-Clarify trained under different training strategies during stage 2. We use both ambiguous instructions (Ambg.) and correct instructions (Corr.) for testing. As we can see in Table IV, only when both the knowledge-insulation training strategy and the connection module are applied (line 3) does the framework exhibit the full Ask-to-Clarify capability. The reason is twofold. First, the knowledge-insulation training strategy is crucial because continuing to train the VLM in stage 2 can interfere with the ambiguity-solving ability it learned in stage 1. However, simply freezing the VLM weakens the link between the VLM and the diffusion model. Therefore, we introduce the connection module to compensate for this, re-establishing strong connection between the two components. The results suggest both our connection module and training strategy are essential to the effectiveness of our framework. Ambiguity-Solving Ability Present Absence Ours π0 [8] + Qwen2-VL-72B [28] inherent extern 27/30 25/30 30/30 30/30 TABLE V: Experiments about the collaboration ability. Collaboration Ability We design an experiment to specifically evaluate the inherent ambiguity-solving ability of our framework. We create two simple scenarios: Present, where an object targeted in dialogue is on table, and Absence, where it is not. In both cases, the agents task is to first use dialogue to determine the objects status and then act accordingly. To establish strong performance baseline, we compare our framework against π0 augmented with powerful, external VLM (Qwen2-VL-72B [28]). As shown in Table V, our framework performs exceptionally well. In the Present scenario, it successfully identifies the target object and completes the task in most trials. In the Absence scenario, it correctly refuses to execute the instruction with similarly high success rate. Crucially, the performance of our integrated framework is on par with the baseline that required an external VLM. These results show that our framework can resolve instruction ambiguity on its own, while existing methods rely on external assistance. Low-Light Conditions We evaluate the Ask-to-Clarify framework under challenging visual conditions. We create low-light environment, i.e., turn off 50% of the lights while keeping other conditions the same as we collect the training data. It can be observed in Table VI that the performance of both the Ask-to-Clarify framework and π0 drops, but the impact is much smaller for our framework. Its success rate Method π0 [8] π0 [8] + low-light Ours Ours + low-light Stack the Color1 block on top of the Color2 block (Blue, Yellow) (Yellow, Blue) Average 14/20 4/20 18/20 17/20 9/20 5/20 18/20 15/20 57.5% 22.5% 90.0% 80.0% TABLE VI: Experiments under low-light conditions. Put the Apple on the plate π0 [8] π0 [8] + distractors Ours Ours + distractors 19/20 13/20 19/20 16/ 95.0% 65.0% 95.0% 80.0% TABLE VII: Experiments with distractors present. decreases only slightly, from 90.0% to 80.0%. In contrast, π0s success rate drops sharply from 57.5% to 22.5%. This could be due to the fact that our framework keeps the VLMs vision encoder frozen, preserving its general robustness. The baseline π0, however, requires finetuning its entire model to work on our tasks. This makes it overfit to the training conditions and less robust to visual changes. The result suggests that our framework is robust for low-light conditions. Distractors We test the robustness of the Ask-to-Clarify framework in the presence of visual distractors. Specifically, we place both an apple and pomegranate on the table, which appear strikingly similar under our cameras. As we can see from Table VII, our framework shows more robust ability with distractor present. It maintains high success rate of 80.0%, while π0s performance falls to 65.0%. The results indicate our framework can better handle confusing objects, common challenge in real-world scenarios. V. CONCLUSION This paper proposes the Ask-to-Clarify framework, which serves as collaborative framework that first resolves ambiguous instructions by asking clarification questions in multi-turn dialogue and then generates low-level actions endto-end for real-world embodied tasks. Our framework is built on two main components. The collaboration component is standard VLM, while the action component is pre-trained diffusion expert. To connect these components smoothly, we also introduce connection module. This module adjusts visual observations based on the language instruction, leading to more reliable conditions for the diffusion model. We train the Ask-to-Clarify framework using our two-stage knowledge-insulation training strategy. This strategy preserves the interaction ability of the VLM while fine-tuning the diffusion-based action expert to generate lowlevel actions, which eventually lead to end-to-end action generation ability. During inference, signal detector serves as router and enables our framework to seamlessly switch between asking questions and taking actions. To evaluate our framework, we conduct series of real-world experiments. The results indicate our framework is capable of engaging in dialogue to clarify the ambiguous instruction, then generating actions end-to-end to perform embodied tasks with high reliability. Furthermore, we design set of ablation studies to reveal insights into the design of our framework. Future Work In this work, we implement the connection module using Feature-wise Linear Modulation (FiLM) [30] for its proven simplicity and effectiveness. However, this connection module is critical component of our framework, and exploring alternative designs presents rich avenue for future research. One promising direction is to employ CLIP-style contrastive loss. This would explicitly train the module to align the visual observation with the language instruction, enforcing strong semantic condition for the diffusion expert. Alternatively, cross-attention mechanism could serve as the connection module. Here, the instruction would act as query to dynamically attend to and extract the most task-relevant visual features from the observation, offering more flexible and fine-grained fusion approach. REFERENCES [1] A. M. Turing, Computing machinery and intelligence, in Parsing the Turing test: Philosophical and methodological issues in the quest for the thinking computer, 2007. 1, 3 [2] L. Kunze, N. Hawes, T. Duckett, M. Hanheide, and T. Krajnık, Artificial intelligence for long-term robot autonomy: survey, RAL, 2018. 1 [3] J. Duan, S. Yu, H. L. Tan, H. Zhu, and C. Tan, survey of embodied ai: From simulators to research tasks, TETCI, 2022. [4] S. Franklin, Autonomous agents as embodied ai, Cybernetics & Systems, 1997. 1 [5] X. Gao, Q. Gao, R. Gong, K. Lin, G. Thattai, and G. S. Sukhatme, Dialfred: Dialogue-enabled agents for embodied instruction following, RAL, 2022. 2, 3 [6] A. Padmakumar, J. Thomason, A. Shrivastava, P. Lange, A. Narayan-Chen, S. Gella, R. Piramuthu, G. Tur, and D. Hakkani-Tur, Teach: Task-driven embodied agents that chat, in AAAI, 2022. 2, 3 [7] R. Ramrakhya, M. Chang, X. Puig, R. Desai, Z. Kira, and R. Mottaghi, Grounding multimodal llms to embodied agents that ask for help with reinforcement learning, arXiv preprint arXiv:2504.00907, 2025. 2, 3 [8] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter et al., π0: vision-language-action flow model for general robot control, arXiv preprint arXiv:2410.24164, 2024. 2, 3, 6, [9] K. Pertsch, K. Stachowicz, B. Ichter, D. Driess, S. Nair, Q. Vuong, O. Mees, C. Finn, and S. Levine, Fast: Efficient action tokenization for vision-language-action models, arXiv preprint arXiv:2501.09747, 2025. 2, 6 [10] M. J. Kim, C. Finn, and P. Liang, Fine-tuning visionlanguage-action models: Optimizing speed and success, arXiv preprint arXiv:2502.19645, 2025. 2, 6 [11] D. Ghosh, H. R. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, T. Kreiman, C. Xu, J. Luo et al., Octo: An open-source generalist robot policy, in RSS, 2024. 2 [12] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. P. Foster, P. R. Sanketi, Q. Vuong et al., Openvla: An open-source vision-language-action model, in CoRL, 2025. 2, 3 [13] B. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid et al., Rt-2: Visionlanguage-action models transfer web knowledge to robotic control, in CoRL, 2023. 2, 3 [14] A. ONeill, A. Rehman, A. Maddukuri, A. Gupta, A. Padalkar, A. Lee, A. Pooley, A. Gupta, A. Mandlekar, A. Jain et al., Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0, in ICRA, 2024. 2 [15] D. Driess, J. T. Springenberg, B. Ichter, L. Yu, A. Li-Bell, K. Pertsch, A. Z. Ren, H. Walke, Q. Vuong, L. X. Shi et al., Knowledge insulating vision-language-action models: Train fast, run fast, generalize better, arXiv preprint arXiv:2505.23705, 2025. 2, [16] P. Intelligence, K. Black, N. Brown, J. Darpinian, K. Dhabalia, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai et al., π0.5: vision-language-action model with open-world generalization, arXiv preprint arXiv:2504.16054, 2025. 2 [17] J. Wen, Y. Zhu, J. Li, Z. Tang, C. Shen, and F. Feng, Dexvla: Vision-language model with plug-in diffusion expert for general robot control, arXiv preprint arXiv:2502.05855, 2025. 2, 3 [18] Q. Li, Y. Liang, Z. Wang, L. Luo, X. Chen, M. Liao, F. Wei, Y. Deng, S. Xu, Y. Zhang et al., Cogact: foundational vision-language-action model for synergizing cognition and action in robotic manipulation, arXiv preprint arXiv:2411.19650, 2024. 2 [19] W. Peebles and S. Xie, Scalable diffusion models with transformers, in ICCV, 2023. 2 [20] S. Deng, M. Yan, S. Wei, H. Ma, Y. Yang, J. Chen, Z. Zhang, T. Yang, X. Zhang, H. Cui et al., Graspvla: grasping foundation model pre-trained on billion-scale synthetic action data, arXiv preprint arXiv:2505.03233, 2025. [21] K. P. Singh, L. Weihs, A. Herrasti, J. Choi, A. Kembhavi, and R. Mottaghi, Ask4help: Learning to leverage an expert for embodied tasks, in NeurIPS, 2022. 3 [22] L. Sun, D. K. Jha, C. Hori, S. Jain, R. Corcodel, X. Zhu, M. Tomizuka, and D. Romeres, Interactive planning using large language models for partially observable robotic tasks, in ICRA, 2024. 3 [23] S. Xie, H. Cao, Z. Weng, Z. Xing, H. Chen, S. Shen, J. Leng, Z. Wu, and Y.-G. Jiang, Human2robot: Learning robot actions from paired human-robot videos, arXiv preprint arXiv:2502.16587, 2025. 3 [24] X. Chen, S. Zhang, P. Zhang, L. Zhao, and J. Chen, Asking before acting: Gather information in embodied decision making with language models, arXiv preprint arXiv:2305.15695, 2023. 3 [25] H. Liu, A. Chen, Y. Zhu, A. Swaminathan, A. Kolobov, learning from verbal and C.-A. Cheng, Interactive robot correction, arXiv preprint arXiv:2310.17555, 2023. 3 [26] A. Z. Ren, A. Dixit, A. Bodrova, S. Singh, S. Tu, N. Brown, P. Xu, L. Takayama, F. Xia, J. Varley et al., Robots that ask for help: Uncertainty alignment for large language model planners, in CoRL, 2023. [27] C. Chi, Z. Xu, S. Feng, E. Cousineau, Y. Du, B. Burchfiel, R. Tedrake, and S. Song, Diffusion policy: Visuomotor policy learning via action diffusion, The International Journal of Robotics Research, 2023. 3 [28] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge et al., Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution, arXiv preprint arXiv:2409.12191, 2024. 3, 6, 7 [29] M. Zhu, Y. Zhu, J. Li, J. Wen, Z. Xu, N. Liu, R. Cheng, C. Shen, Y. Peng, F. Feng et al., Scaling diffusion policy in transformer to 1 billion parameters for robotic manipulation, arXiv preprint arXiv:2409.14411, 2024. 3, 6 [30] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville, Film: Visual reasoning with general conditioning layer, in AAAI, 2018. 3, 8 [31] xarm 7, https://www.xarm.cc/products/xarm-7-2020. 6 [32] xArm Developer, xarm python sdk, https://github.com/ xArm-Developer/xArm-Python-SDK. [33] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv et al., Qwen3 technical report, arXiv preprint arXiv:2505.09388, 2025. 6 [34] A. Iyer, Z. Peng, Y. Dai, I. Guzey, S. Haldar, S. Chintala, and L. Pinto, Open teach: versatile teleoperation system for robotic manipulation, arXiv preprint arXiv:2403.07870, 2024."
        }
    ],
    "affiliations": [
        "College of Computer Science and Artificial Intelligence, Fudan University, Shanghai, China",
        "Mechanical Systems Control Lab, UC Berkeley, California, USA",
        "Shanghai Innovation Institute, Shanghai, China"
    ]
}