{
    "paper_title": "Exploring Federated Pruning for Large Language Models",
    "authors": [
        "Pengxin Guo",
        "Yinong Wang",
        "Wei Li",
        "Mengting Liu",
        "Ming Li",
        "Jinkai Zheng",
        "Liangqiong Qu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LLM pruning has emerged as a promising technology for compressing LLMs, enabling their deployment on resource-limited devices. However, current methodologies typically require access to public calibration samples, which can be challenging to obtain in privacy-sensitive domains. To address this issue, we introduce FedPrLLM, a comprehensive federated pruning framework designed for the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs to calculate a pruning mask matrix based on its local calibration data and share it with the server to prune the global model. This approach allows for collaborative pruning of the global model with the knowledge of each client while maintaining local data privacy. Additionally, we conduct extensive experiments to explore various possibilities within the FedPrLLM framework, including different comparison groups, pruning strategies, and the decision to scale weights. Our extensive evaluation reveals that one-shot pruning with layer comparison and no weight scaling is the optimal choice within the FedPrLLM framework. We hope our work will help guide future efforts in pruning LLMs in privacy-sensitive fields. Our code is available at https://github.com/Pengxin-Guo/FedPrLLM."
        },
        {
            "title": "Start",
            "content": "Pengxin Guo1 Yinong Wang1 Wei Li2 Mengting Liu3 Ming Li4 Jinkai Zheng5 Liangqiong Qu1 1The University of Hong Kong 3Sun Yat-sen University 2Southern University of Science and Technology 5Hangzhou Dianzi University 4Guangming Laboratory 5 2 0 2 9 ] . [ 1 7 4 5 3 1 . 5 0 5 2 : r {guopx,u3011648}@connect.hku.hk, li.wei.ml.619@gmail.com, liumt55@mail.sysu.edu.cn, ming.li@u.nus.edu, zhengjinkai3@hdu.edu.cn, liangqqu@hku.hk"
        },
        {
            "title": "Abstract",
            "content": "LLM pruning has emerged as promising technology for compressing LLMs, enabling their deployment on resource-limited devices. However, current methodologies typically require access to public calibration samples, which can be challenging to obtain in privacy-sensitive domains. To address this issue, we introduce FedPrLLM, comprehensive federated pruning framework designed for the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs to calculate pruning mask matrix based on its local calibration data and share it with the server to prune the global model. This approach allows for collaborative pruning of the global model with the knowledge of each client while maintaining local data privacy. Additionally, we conduct extensive experiments to explore various possibilities within the FedPrLLM framework, including different comparison groups, pruning strategies, and the decision to scale weights. Our extensive evaluation reveals that one-shot pruning with layer comparison and no weight scaling is the optimal choice within the FedPrLLM framework. We hope our work will help guide future efforts in pruning LLMs in privacy-sensitive fields. Our code is available at https://github.com/Pengxin-Guo/FedPrLLM."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) [4, 23, 1] have revolutionized the field of natural language processing by demonstrating remarkable capabilities across various tasks. However, their increasing size leads to significant hardware requirements, limiting real-world deployment. To address this, research has focused on compact LLMs through compression techniques, such as pruning [13, 5, 22], knowledge distillation [7, 29], quantization [27, 21], and low-rank factorization [34, 20]. Among these, pruning has emerged as promising method to reduce resource demands by selectively removing redundant parameters while preserving performance [13, 5]. Typically, LLM pruning methods can be broadly classified into structured pruning, which removes entire substructures within LLMs, such as neurons [13, 12, 2], layers [26], or even entire transformer blocks [6], and unstructured pruning, which removes individual weights from the models weight matrices based on certain criteria [5, 22, 33, 30, 28]. This work focuses on unstructured pruning, as it tends to achieve higher compression rates and maintain better model performance compared to structured pruning [5, 11, 26, 33]. Despite advances in LLM unstructured pruning methods, these approaches usually rely on access to public calibration data to guide the pruning process [5, 22, 33, 30, 28]. Specifically, they require calibration samples to evaluate the importance of the model weights in order to determine the pruning mask matrix for pruning models. However, in many real-world scenarios, such as healthcare, finance, Corresponding author. Preprint. Under review. and personalized services, the data used for pruning might be private and cannot be shared due to privacy regulations and concerns. Federated Learning (FL) [15, 32, 31, 9, 8], which utilizes collaborative and decentralized training of models across multiple institutions without sharing personal data externally, offers promising solution to this challenge. Integrating FL with LLM pruning allows each client to calculate local pruning mask matrix based on its private calibration data and share it with the server. The server then aggregates these mask matrices into an aggregated mask matrix and selects the top-k values (the most clients want to prune) to derive final pruning mask matrix for pruning the global model. Despite its ability to protect data privacy, three unresolved challenges within this framework hinder practical deployment. Challenge 1: How to compare parameters? When selecting the top-k values, critical ambiguity arises: Should parameter importance be compared across the entire layer or within each respective row or column (corresponding to layer, row, and column comparisons, respectively)? Previous centralized LLM pruning work [22] has highlighted the importance of using proper comparison group for pruning LLMs, yet no study explores this in federated scenarios. Challenge 2: To scale or not scale for retained parameters. Beyond simply determining which parameters to prune via majority voting (i.e., selecting top-k values), the FL aggregated mask matrix reveals critical hidden signal: how strongly each parameter is disfavored across clients. Consider two surviving parameters - one narrowly retained (pruned by 10/100 clients) and another unanimously preserved (pruned by 0/100 clients). Traditional pruning treats both equally, maintaining their original magnitudes despite their differing consensus levels. However, this ignores critical insight: the former parameter, though retained, exhibits weaker consensus across clients. This observation raises fundamental question: Rather than simply employing binary masking, could we leverage the FL aggregated mask matrix to guide continuous weight adjustment, where retained parameters are scaled down proportionally based on their pruning frequency? Challenge 3: Is iterative pruning worth the cost? LLM pruning is typically performed layer-bylayer recursively to avoid error accumulation [5, 22, 33]. As result, in FL, this necessitates either one-shot pruning (clients compute all layer mask matrices and share them with the server in one go) or iterative pruning (clients send the mask matrices to the server layer by layer in an iterative manner). While iterative pruning allows for refining the local model promptly, it incurs prohibitive communication costs for deep LLMs. This raises an unstudied question: Does iteratively refining the local model improve accuracy enough to justify its massive communication overhead? To address these challenges, we formalize the first systematic study on federated LLM pruning and empirically evaluate three core design choices through unified FedPrLLM framework (Figure 1): Q1. Comparison Group: Which comparison group is more effective: layer, row, or column? Q2. Weight Scaling: Should we scale the model weights of the retained parameters? Q3. Pruning Strategy: Does iterative pruning outperform one-shot pruning? We dedicated thousands of GPU hours to benchmark federated pruning for LLMs, conducting extensive experiments across 6 open-source LLMs, 3 sparsity ratios, 3 comparison groups, 2 pruning strategies on 3 common datasets. From these efforts, we have developed practical list of key insights for federated pruning of LLMs: 1). Layer comparison is simple yet effective. Among the three comparison groupslayer, row, and column comparisonslayer comparison stands out as the simplest and most effective method, regardless of the local pruning methods comparison group. 2). Scaling weights performs worse than expected. Though the FL aggregated mask matrix, which reveals how strongly each parameter is disfavored across clients, could be used to scale the retained parameters for continuous weight adjustment, its performance is inferior to that of not scaling them. 3). Iterative pruning offers no benefit. While iterative pruning allows for prompt refinement of the local model, it incurs significant communication overhead, and its performance is comparable to that of one-shot pruning, offering no additional advantages. We hope our findings will help guide future efforts in federated pruning for LLMs and inform best practices for deploying LLMs under federated scenarios in real-world applications. We summarize our contributions as follows: 2 Figure 1: Top). Research questions alongside the corresponding findings and experimental scenarios. Bottom). The FedPrLLM framework. 1 Each client calculates pruning mask matrix Mi using its calibration dataset Di. 2 Clients send the mask matrices Mi to the server. 3 The server aggregates these mask matrices Mi to obtain an aggregated mask matrix ˆM = (cid:80)m i=1 Mi. 4 Top-k values are selected from the aggregated mask matrix ˆW to derive the final mask matrix M. 5 Prune the global model using the mask matrix as follows: ˆW = (1 M), where denotes element-wise 6 Scale the model weights of the retained parameters using the aggregated mask multiplication. matrix ˆM as follows: ˆW (m ˆM) (if needed). 7 The server broadcasts the mask matrix to each client (for iterative pruning). The dashed arrow indicates that this operation is optional; step 6 is used for weight scaling, while 7 is used for iterative pruning. Note that this visualization is primarily for one-shot pruning, which requires only one communication round. For iterative pruning, multiple communication rounds will occur between steps 2 and 7, and the layer index is omitted here. We introduce FedPrLLM, comprehensive federated pruning framework designed for the privacy-preserving compression of LLMs, which incorporates various possibilities for integrating FL with LLM pruning. We conduct an extensive evaluation of FedPrLLM, providing practical insights into effective federated pruning techniques for LLMs, based on thousands of GPU hours invested in multiple open-source LLMs, various sparsity ratios, comparison groups, and datasets. We identify that layer comparison is simple yet effective, scaling weights offers no benefits and may worsen performance, and that one-shot pruning is as effective as iterative pruning while reducing communication costs."
        },
        {
            "title": "2.1 LLM Pruning",
            "content": "LLM pruning can be broadly classified into structured pruning [13, 12, 2, 26, 6] and unstructured pruning [5, 22, 33, 30, 28], and in this work, we focus on the latter. Unstructured pruning involves removing individual weights from the models weight matrices based on certain criteria while maintaining its performance as much as possible [5, 22, 33, 30, 28]. It is usually achieved by minimizing the discrepancy square error between the dense and pruned model layer-by-layer recursively. Specifically, for an uncompressed linear layer with weights Wl Rdr, the objective for unstructured pruning can usually be formulated as: arg min Ml WlXl (Wl (1 Ml))Xl2 2 s.t. Ml0 k, (1) where Xl is the input to l-th linear layer (also referred to as calibration data), Ml {0, 1}dr is the pruning mask matrix we aim to derive, denotes element-wise multiplication, 0 is the l0-norm (e.g., the number of non-zero elements), and represents the number of pruned weights determined by the pruning ratio. 3 The differences between previous pruning methods primarily lie in the design of the pruning metrics and the comparison groups used to derive the pruning mask matrix [5, 22, 33]. Pruning metrics refer to how the importance of each model weight is identified, while comparison groups denote the selection of groups for comparing these weights, including layer comparison, row comparison, and column comparison. For example, SparseGPT [5] utilizes the Hessian Matrix inverse, i.e., (cid:20) (cid:21) , as the pruning metric, employing layer comparison to determine the pruning W2 diag((XTX+λI)1) ij mask matrix for pruning, along with subsequent weight scaling. Wanda [22] adopts the magnitudes of model weights multiplied by the corresponding input activations, i.e., Wij Xj2, as the pruning metric and chooses row comparison. RIA [33] integrates relative importance within Wanda, resulting (Xj2)0.5, while utilizing layer comparison. in new pruning metric, i.e., (cid:17) (cid:16) Wij (cid:80) Wj + Wij (cid:80) Wi"
        },
        {
            "title": "2.2 Federated Learning",
            "content": "Federated Learning (FL) [15, 32, 31, 9, 8, 10] is decentralized approach where multiple clients collaboratively train model while keeping their data localized. The objective is to minimize global loss function: L(Θ) = 1 (cid:88) i=1 ℓi(Θ), (2) where Θ denotes the model weight, is the total number of clients, and ℓi(Θ) is the local loss function for client . In FL, each client trains its model using its local dataset, aiming to find: ˆΘi = arg minΘ ℓi(Θ). After local training, clients send their model updates Θi to central server, which aggregates these updates to update the global model: Θ Θ + 1 i=1 Θi. The local training and server aggregation process typically requires multiple communication rounds until the model converges or reaches the predefined maximum number of rounds. (cid:80)m"
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "In the federated pruning scenario for LLMs, multiple clients aim to collaboratively prune an LLM while ensuring that their local calibration data remains private. Formally, let represent the model parameters of the LLM that we aim to prune. Each client possesses private calibration dataset denoted as Di, which is used for calculating the pruning mask matrices during the local pruning process. These mask matrices are then shared with the server to prune the LLM."
        },
        {
            "title": "3.2 FedPrLLM",
            "content": "In this section, we first introduce the overall workflow of the comprehensive FedPrLLM framework, as illustrated at the bottom of Figure 1, and then discuss the various possibilities within it. Specifically, during local pruning, each client calculates pruning mask matrix Mi {0, 1}Wi using its calibration dataset Di (step 1). This mask matrix determines which weights are pruned (Mij = 1) and which are retained (Mij = 0). The decision on which weights to prune or retain is based on an importance criterion derived from the calibration data, such as the magnitudes of model weights multiplied by the corresponding input activations used in Wanda [22]. After calculating the pruning mask matrix, each client shares only the mask matrix Mi with the central server (step 2). This approach ensures that no local model parameters or private calibration data are transmitted, thereby minimizing communication overhead and preserving data privacy. Upon receiving the pruning mask matrices Mi from all clients, the server sums them to obtain an aggregated mask matrix ˆM = (cid:80)m i=1 Mi (step 3) and then selects the top-k values to create the final mask matrix (step 4) for pruning the global model (step 5). In the following, we will discuss various possibilities within the FedPrLLM framework, including different comparison groups, the decision to perform weight scaling, and the choice between one-shot and iterative pruning."
        },
        {
            "title": "3.2.1 Comparison Group",
            "content": "When selecting the top-k values from the aggregated mask matrix ˆM to derive the final pruning mask matrix M, three comparison groups can be considered (step 4): layer comparison, row comparison, and column comparison. In layer comparison, the comparison group consists of all elements within layer, allowing us to choose the top-k values across the entire layer. Conversely, in row (or column) comparison, the comparison group is defined by each individual row (or column), enabling the selection of the top-k values within each respective row (or column). The visualization of these comparison groups is shown in Figure 1. Thus, given that multiple comparison groups could be chosen, which comparison group is more effective for federated pruning of LLMs?"
        },
        {
            "title": "3.2.2 Weight Scaling",
            "content": "After obtaining the final mask matrix M, it can be used to effectively prune the dense model using (1 M), where denotes element-wise multiplication (step 5). This operation removes the weights corresponding to the masked parameters (i.e., Mij = 1), resulting in sparser model ˆW. Then, beyond merely determining which parameters to prune via majority voting (i.e., selecting top-k values), the aggregated mask matrix ˆM reveals critical hidden signal: how strongly each parameter is disfavored across clients. Consider two surviving parameters - one narrowly retained (pruned by 10/100 clients) and another unanimously preserved (pruned by 0/100 clients). Traditional pruning treats both equally, maintaining their original magnitudes despite their differing consensus levels. However, this ignores critical insight: the former parameter, though retained, exhibits weaker consensus across clients. To this end, the aggregated mask matrix ˆM could be further applied to scale down the retained parameters using the formula ˆW (m ˆM) (step 6, if needed). This approach corresponds to locally pruning the model and then sharing the pruned model with the server, which aggregates them using the FedAvg algorithm [15]. However, will the weight scaling improve the performance of federated pruning for LLMs? m"
        },
        {
            "title": "3.2.3 One-shot vs. Iterative Pruning",
            "content": "Since LLMs are usually pruned layer-by-layer recursively [5, 22, 33], federated pruning for LLMs can be naturally categorized into two types: one-shot pruning and iterative pruning. In one-shot pruning, each client calculates the pruning mask matrices for all layers and then sends them to the server, resulting in only one communication round. In contrast, iterative pruning involves sending the pruning mask matrices to the server layer by layer. Specifically, after calculating the pruning mask matrix for one layer, it is uploaded to the server for aggregation. The server then combines these matrices into global mask matrix for pruning the model at that layer and broadcasts the global mask matrix back to each client for local pruning of that layer (step 7, the layer index is omitted here). This process is carried out layer by layer and involves multiple communication rounds, resulting in higher communication costs compared to one-shot pruning. Therefore, given the significant communication costs associated with iterative pruning, will iterative pruning outperform one-shot pruning? One-shot and iterative pruning differ because, when calculating the pruning mask matrix for layer + 1 locally, the calibration data Xl+1 is derived from the output of layer l, which has already been pruned. Since the weights of the local pruned model for layer vary between using Mi (one-shot pruning) and (iterative pruning), this leads to different outputs for layer and, consequently, varying calibration data Xl+1, resulting in distinct pruning mask matrices for layer + 1."
        },
        {
            "title": "4 Experiments",
            "content": "Our experiments are designed to answer the following research questions that are important for the practical pruning of LLMs under federated scenario. Q1. Which comparison group is more effective: layer, row, or column? Q2. Should we scale the model weights of the retained parameters? Q3. Does iterative pruning outperform one-shot pruning? 5 Table 1: WikiText2 perplexity of pruned LLMs under 50% sparsity ratio."
        },
        {
            "title": "Dense",
            "content": "Centralized Local-only"
        },
        {
            "title": "FedPrLLM",
            "content": "Compar."
        },
        {
            "title": "Prune",
            "content": "Stra."
        },
        {
            "title": "Scaling",
            "content": "- - - - - - One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column - - -"
        },
        {
            "title": "LLaMA",
            "content": "LLaMA-2 LLaMA-3 7B 5.67 7.25 7.44 7.32 7.30 1524.28 7.30 7.30 1822.89 7.48 7.47 1708.41 7.46 7.46 1985. 13B 5.09 6.15 6.33 6.19 6.20 9282.09 6.19 6.20 6884.15 6.36 6.36 10819.42 6.35 6.35 6692.91 30B 4. 5.24 5.34 5.24 5.25 501.88 5.24 5.24 996.57 5.35 5.35 824.50 5.34 5.34 939.62 7B 5.11 6.46 6.63 6.48 6.48 20528.41 6.48 6.48 77245.84 6.67 6.67 18084.02 6.67 6.67 66911. 13B 4.57 5.58 5.72 5.61 5.61 5309.48 5.62 5.61 5430.81 5.75 5.75 5914.91 5.75 5.74 5268.71 8B 7. 11.00 11.39 11.02 11.02 311468.53 11.12 11.11 189134.78 11.75 11.75 276031.34 11.86 11.87 41996."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "We implement FedPrLLM in PyTorch [18] and use the Hugging Face Transformers library [25] to handle models and datasets. We evaluate the FedPrLLM on the three most widely adopted LLM model families: LLaMA 7B/13B/30B [23], LLaMA-2 7B/13B [24] and LLaMA-3 8B [17]. For each model under consideration, we focus on pruning the linear layers (skipping the first embedding layer and the final classification head), which account for around 99% of the total LLM parameters. We employ unstructured sparsity and impose uniform sparsity ratio for all linear layers. For the calibration data, following [5, 22, 28, 33], we use 128 samples from the C4 dataset [19], with each sample containing 2048 tokens. For FedPrLLM, we set the number of clients to 64, resulting in each client having only 2 calibration samples. For each client, we adopt Wanda [22] to perform local pruning and calculate the pruning mask matrix. Apart from the proposed FedPrLLM framework, we further implement two baselines for comparison: (1) Local-only, where each client prunes the model locally using its private calibration data, and (2) Centralized, where the server prunes the model with all calibration data, which could be considered as an upper bound for the pruning performance under FL setting. Following previous works on LLM compression [5, 28, 33], we measure the performance of pruned models in language modeling and evaluate their perplexity on the held-out WikiText2 [16] validation set, C4 [19] validation data, and PTB [14]. All experiments are conducted on NVIDIA L40S GPUs."
        },
        {
            "title": "4.2 Main Results",
            "content": "To answer the research questions above, we conducted extensive experiments to evaluate FedPrLLM along with two baselines across 6 open-source LLMs, 3 sparsity ratios, 3 comparison groups, 2 pruning strategies on 3 common datasets. The experimental results for the 50% sparsity ratio are shown in Tables 1, 2, and 3, while results for higher sparsity ratios (e.g., 60% and 70%) are shown in Tables 5, 6, and 7 in Appendix."
        },
        {
            "title": "4.2.1 Which Comparison Group is More Effective?",
            "content": "As discussed above, various comparison groups can be used to select top-k values from the aggregated mask matrix to derive the final mask matrix for pruning the global model, including layer comparison, row comparison, and column comparison. Thus, which comparison group is the most effective? According to the results in Tables 1, 2, and 3, we observe that layer comparison and row comparison achieve comparable performance, both significantly surpassing column comparison. To investigate why column comparison performs much worse than the others, we noted that the local pruning method we used (i.e., Wanda [22]) adopts row comparison, meaning the local pruning mask matrix"
        },
        {
            "title": "Dense",
            "content": "Centralized Local-only"
        },
        {
            "title": "Dense",
            "content": "Centralized Local-only"
        },
        {
            "title": "FedPrLLM",
            "content": "Table 2: C4 perplexity of pruned LLMs under 50% sparsity ratio. Compar."
        },
        {
            "title": "Prune",
            "content": "Stra."
        },
        {
            "title": "Scaling",
            "content": "- - - - - - One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column - - -"
        },
        {
            "title": "LLaMA",
            "content": "LLaMA-2 LLaMA-3 13B 6.79 8.14 8.37 8.22 8.22 10616.94 8.22 8.22 8567.66 8.40 8.40 13744.66 8.41 8.41 6860. 30B 6.12 7.28 7.52 7.39 7.39 512.27 7.39 7.39 779.01 7.57 7.57 895.18 7.57 7.57 724.28 7B 7. 8.94 9.16 9.01 9.01 9631.37 9.01 9.02 11658.80 9.21 9.21 11440.51 9.22 9.22 10355.87 13B 6.51 8.03 8.31 8.18 8.19 5075.92 8.19 8.18 4804.46 8.39 8.39 5189.73 8.39 8.39 4657. 8B 12.34 18.38 18.92 18.32 18.32 200257.70 18.43 18.38 112192.42 19.45 19.45 90476.94 19.58 19.60 44469.52 7B 7. 9.34 9.59 9.43 9.43 893.05 9.44 9.44 1050.26 9.64 9.64 887.34 9.64 9.65 1242.31 Table 3: PTB perplexity of pruned LLMs under 50% sparsity ratio. Compar."
        },
        {
            "title": "Prune",
            "content": "Stra."
        },
        {
            "title": "Scaling",
            "content": "- - - - - - One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Column Iterative One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column - - -"
        },
        {
            "title": "LLaMA",
            "content": "13B 28.09 36.41 37.57 36.57 36.61 22138.56 36.54 36.55 17610.52 37.70 37.72 29140.28 37.69 37.60 15189.83 LLaMA-2 LLaMA30B 23.51 26.64 27.13 26.69 26.64 713.56 26.68 26.64 1158.75 27.27 27.25 982.59 27.27 27.27 1178.40 7B 50. 96.99 108.66 102.71 101.85 14256.86 102.72 103.66 13401.63 112.52 112.17 12021.08 112.95 113.32 10208.03 13B 56.51 86.83 91.92 88.26 88.31 7392.64 88.38 88.94 6941.72 92.90 93.07 7801.23 92.58 92.61 5220. 8B 13.30 20.69 21.43 20.56 20.55 407313.84 20.55 20.60 168643.04 22.21 22.21 264723.12 22.39 22.41 39172.53 7B 41. 80.12 86.25 80.31 80.71 4463.92 81.22 81.26 4061.96 87.97 88.35 4557.48 87.28 87.61 6929.83 Mi derived from each client is based on row comparison. We hypothesize that this is the reason for the poorer performance of column comparison, as the comparison group used in FedPrLLM conflicts with that of the local pruning method. To validate this, we further change the comparison group in the local pruning method (i.e., Wanda [22]) to layer comparison and column comparison to evaluate the performance of the FedPrLLM framework with one-shot pruning and no weight scaling. The results on WikiText2 are shown in Table 4, while results for other datasets are presented in Table 8 in Appendix. From these results, we see that when the comparison group in the local pruning method (i.e., Wanda [22]) is changed to layer comparison, only the layer comparison used in FedPrLLM performs well, while row comparison performs poorly and column comparison performs even worse. Similarly, when the local pruning methods comparison group is changed to column comparison, only the layer and column comparisons perform normally, while row comparison performance is poor. Note that when the comparison group in the local pruning method (i.e., Wanda [22]) is changed to column comparison, it degrades to the magnitude-based pruning method, rendering the performance irrelevant to calibration samples, which results in the performance of Centralized and Local-only being the same [22]. These results demonstrate our hypothesis that the conflict between the local and server comparison groups leads to worse performance, while the layer comparison used in FerPrLLM consistently achieves good results, regardless of the comparison group used for the local pruning method. Therefore, we conclude that: Takeaway 1: Layer comparison is simple yet effective. Table 4: WikiText2 perplexity of pruned LLMs under 50% sparsity ratio when changing the comparison group for the local pruning method. FedPrLLM adopts one-shot pruning and no weight scaling. Local Compar. Compar."
        },
        {
            "title": "LLaMA",
            "content": "LLaMA-2 LLaMA-"
        },
        {
            "title": "Group",
            "content": "Centralized Local-only - - 7B 7.94 8.16 13B 6.57 6. 30B 5.47 5.58 7B 7.38 7.56 13B 5.92 6. 8B 12.04 12."
        },
        {
            "title": "Layer\nRow\nColumn",
            "content": "7.98 31.85 1749.59 6.60 10.08 10183.32 5.48 11.33 541.62 7.38 39.07 25258.16 5.95 124.08 5503.91 12.09 17.51 336255. Centralized Local-only - -"
        },
        {
            "title": "Layer\nRow\nColumn",
            "content": "8.86 8.86 8.86 138.54 8.86 7.68 7.68 7.68 100.80 7.68 5.67 5.67 5.67 49.17 5. 10.41 10.41 10.41 764.32 10.41 6.38 6.38 6.38 2580.88 6.38 83.67 83.67 83.67 400.95 83."
        },
        {
            "title": "4.2.2 Should We Scale the Model Weights of the Retained Parameters?",
            "content": "The aggregated mask matrix ˆM indicates the number of clients that wish to prune parameter, which allows it to be used for scaling the model weights of the retained parameters to (m ˆM) . This approach corresponds to locally pruning the model and then sharing the pruned model with the server, which aggregates them using the FedAvg algorithm [15]. However, will weight scaling be beneficial for the federated pruning of LLMs? From the results in Tables 1, 2, and 3, we observe that the performance with weight scaling is worse than that without weight scaling across all comparison groups and pruning strategies. It indicates that scaling weights offers no benefit and may even worsen performance. This may be due to the fact that locally pruned models do not perform well, and applying the FedAvg algorithm [15] to aggregate these pruned model weights leads to subpar performance. Therefore, we conclude that: Takeaway 2: Scaling weights performs worse than expected."
        },
        {
            "title": "4.2.3 Does Iterative Pruning Outperform One-shot Pruning?",
            "content": "Since LLMs are usually pruned layer-by-layer recursively [5, 22, 33], federated pruning for LLMs can be naturally categorized into two types: one-shot pruning and iterative pruning. Given the significant communication costs associated with iterative pruning, will it outperform one-shot pruning? The comparison results are provided in Tables 1, 2, and 3. These results indicate that the performance of iterative pruning and one-shot pruning is comparable, regardless of the comparison groups and pruning strategies. However, since iterative pruning introduces significant communication costs without any performance improvement, we conclude that: Takeaway 3: Iterative pruning offers no benefit."
        },
        {
            "title": "4.3 Sensitivity Analysis",
            "content": "In this section, we conduct sensitivity analyses on the number of clients and calibration samples in FedPrLLM to better understand its effectiveness in pruning LLMs within federated scenario. We use FedPrLLM, which employs layer comparison, one-shot pruning, and no weight scaling, to conduct the analysis under 50% sparsity ratio."
        },
        {
            "title": "4.3.1 Impact of Client Numbers",
            "content": "It is worth noting that the number of clients influences the performance of FL algorithms [9, 10]. In this section, we investigate the effect of client numbers on the federated pruning of LLMs. We use total of 128 calibration samples and vary the number of clients from 64 to 2, resulting in an 8 increase in the calibration samples allocated to each client. Specifically, when the number of clients is 64, each client has only 2 calibration samples; when the number of clients is reduced to 2, each client has 64 calibration samples. The experimental results are shown in Figure 2. From this figure, we observe that FedPrLLM consistently outperforms Local-only pruning across various numbers of clients, demonstrating the effectiveness of the federated pruning algorithm. (a) WikiText2 (b) C4 (c) PTB Figure 2: The effect of different client numbers on federated pruning LLMs."
        },
        {
            "title": "4.3.2 Impact of the Number of Calibration Samples",
            "content": "We further investigate the impact of pruning LLMs in federated scenario with varying numbers of calibration samples, as shown in Figure 3. Specifically, we change the total number of calibration samples from 128 to 4 while keeping the number of clients equal to half of that. As shown in Figure 3, we observe that with different numbers of calibration samples, FedPrLLM consistently outperforms Local-only pruning, which again shows the effectiveness of the federated pruning method. (a) WikiText2 (b) C4 (c) PTB Figure 3: The effect of the number of calibration samples on federated pruning LLMs."
        },
        {
            "title": "5 Related Work",
            "content": "There is one work that attempts to conduct LLM pruning in an FL scenario [3]. Specifically, it enables clients to locally prune their models based on private data and send the pruned models to the server for aggregation. The server averages the pruned models using the FedAvg algorithm [15] and prunes the model to satisfy the predefined sparsity rate based on an aggregated mask matrix. This method can be viewed as specific case within our FedPrLLM framework, i.e., iterative pruning with weight scaling. However, our extensive evaluations reveal that this approach is not optimal."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce FedPrLLM, comprehensive federated pruning framework designed for the privacy-preserving compression of LLMs, incorporating various possibilities for integrating FL with LLM pruning. To identify the optimal operation within this framework, we invested thousands of GPU hours exploring these possibilities, including different comparison groups, pruning strategies, and the decision to scale weights. Our extensive evaluation reveals that one-shot pruning with layer comparison and no weight scaling is the optimal choice within the FedPrLLM framework. We hope our work will help guide future efforts in pruning LLMs in privacy-sensitive fields."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. SliceGPT: Compress large language models by deleting rows and columns. In The Twelfth International Conference on Learning Representations, 2024. [3] Guangji Bai, Yijiang Li, Zilinghan Li, Liang Zhao, and Kibaek Kim. Fedspallm: Federated pruning of large language models. arXiv preprint arXiv:2410.14852, 2024. [4] Tom Brown. Language models are few-shot learners. arXiv preprint ArXiv:2005.14165, 2020. [5] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pages 1032310337. PMLR, 2023. [6] Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Dan Roberts. The unreasonable ineffectiveness of the deeper layers. In The Thirteenth International Conference on Learning Representations, 2025. [7] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. MiniLLM: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations, 2024. [8] Pengxin Guo, Runxi Wang, Shuang Zeng, Jinjing Zhu, Haoning Jiang, Yanran Wang, Yuyin Zhou, Feifei Wang, Hui Xiong, and Liangqiong Qu. Exploring the vulnerabilities of federated learning: deep dive into gradient inversion attacks. arXiv preprint arXiv:2503.11514, 2025. [9] Pengxin Guo, Shuang Zeng, Wenhao Chen, Xiaodan Zhang, Weihong Ren, Yuyin Zhou, and Liangqiong Qu. new federated learning framework against gradient inversion attacks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 1696916977, 2025. [10] Pengxin Guo, Shuang Zeng, Yanran Wang, Huijie Fan, Feifei Wang, and Liangqiong Qu. Selective aggregation for low-rank adaptation in federated learning. In The Thirteenth International Conference on Learning Representations, 2025. [11] Shwai He, Guoheng Sun, Zheyu Shen, and Ang Li. What matters in transformers? not all attention is needed. arXiv preprint arXiv:2406.15786, 2024. [12] Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, and Tuo Zhao. Losparse: Structured compression of large language models based on low-rank and sparse approximation. In International Conference on Machine Learning, pages 2033620350. PMLR, 2023. [13] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:2170221720, 2023. [14] Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank: Annotating predicate argument structure. In Human Language Technology: Proceedings of Workshop held at Plainsboro, New Jersey, March 8-11, 1994, 1994. [15] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera Arcas. In Artificial Communication-efficient learning of deep networks from decentralized data. intelligence and statistics, pages 12731282. PMLR, 2017. [16] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. [17] AI Meta. Introducing meta llama 3: The most capable openly available llm to date. Meta AI, 2024. [18] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, highperformance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchéBuc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [20] Rajarshi Saha, Varun Srivastava, and Mert Pilanci. Matrix compression via randomized low rank and low precision factorization. Advances in Neural Information Processing Systems, 36, 2023. [21] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137, 2023. [22] Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter. simple and effective pruning approach for large language models. In The Twelfth International Conference on Learning Representations, 2024. [23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [24] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [25] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. [26] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared LLaMA: Accelerating language model pre-training via structured pruning. In The Twelfth International Conference on Learning Representations, 2024. [27] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 3808738099. PMLR, 2023. [28] Peng Xu, Wenqi Shao, Mengzhao Chen, Shitao Tang, Kaipeng Zhang, Peng Gao, Fengwei An, Yu Qiao, and Ping Luo. BESA: Pruning large language models with blockwise parameter-efficient sparsity allocation. In The Twelfth International Conference on Learning Representations, 2024. [29] Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. survey on knowledge distillation of large language models. arXiv preprint arXiv:2402.13116, 2024. [30] Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Gen Li, AJAY KUMAR JAISWAL, Mykola Pechenizkiy, Yi Liang, Michael Bendersky, Zhangyang Wang, and Shiwei Liu. Outlier weighed layerwise sparsity (OWL): missing secret sauce for pruning LLMs to high sparsity. In Forty-first International Conference on Machine Learning, 2024. [31] Shuang Zeng, Pengxin Guo, Shuai Wang, Jianbo Wang, Yuyin Zhou, and Liangqiong Qu. Tackling data heterogeneity in federated learning via loss decomposition. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 707717. Springer, 2024. 11 [32] Junyuan Zhang, Shuang Zeng, Miao Zhang, Runxi Wang, Feifei Wang, Yuyin Zhou, Paul Pu Liang, and Liangqiong Qu. Flhetbench: Benchmarking device and state heterogeneity in federated learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1209812108, 2024. [33] Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, and Carlo Vittorio Cannistraci. Plug-and-play: An efficient post-training pruning method for large language models. In The Twelfth International Conference on Learning Representations, 2024. [34] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection. In International Conference on Machine Learning, pages 6112161143. PMLR, 2024."
        },
        {
            "title": "A Additional Experimental Results",
            "content": "Table 5: WikiText2 perplexity of pruned LLaMA, LLaMA-2, and LLaMA-3 models. Sparsity Method Group Compar. Prune Stra. Weight Scaling 0% Dense Centralized Local-only - - - - - - 50% FedPrLLM Layer One-shot One-shot Row Column One-shot Iterative Layer Iterative Row Iterative Column One-shot Layer One-shot Row Column One-shot Iterative Layer Iterative Row Iterative Column Centralized Local-only - - - - 60% FedPrLLM One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column Centralized Local-only - - - - 70% FedPrLLM One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column - - - - - - - LLaMA LLaMALLaMA-3 7B 5.67 7.25 7.44 7.32 7.30 1524.28 7.30 7.30 1822.89 7.48 7.47 1708.41 7.46 7.46 1985.40 10.71 11. 10.76 10.77 2861.56 10.87 10.85 3154.68 12.14 12.16 3785.85 12.27 12.24 2189.53 87.42 104.15 83.12 81.97 17281.43 89.25 92.29 19791.05 136.50 136.09 20505.56 174.95 182.73 8607.36 13B 5.09 6.15 6. 6.19 6.20 9282.09 6.19 6.20 6884.15 6.36 6.36 10819.42 6.35 6.35 6692.91 8.74 9.38 8.80 8.80 11190.34 8.88 8.90 7824.46 9.77 9.77 17163.16 9.85 9.86 6032.71 53.48 67.13 55.92 56.99 13045.16 55.48 57.18 10323.63 94.90 95.86 11695.06 102.78 99.32 11707.00 30B 4.10 5.24 5.34 5.24 5.25 501.88 5.24 5.24 996.57 5.35 5.35 824.5 5.34 5.34 939.62 6.55 6.96 6.65 6.64 1047.94 6.65 6.64 2250.97 7.10 7.09 1770.89 7.12 7.13 2626.57 17.30 23. 18.73 18.67 2670.43 18.65 18.23 3935.54 31.62 31.48 3032.65 31.12 30.87 3145.32 7B 5.11 6.46 6.63 6.48 6.48 20528.41 6.48 6.48 77245.84 6.67 6.67 18084.02 6.67 6.67 66911.49 10.03 10. 10.08 10.08 14737.65 10.17 10.18 18849.20 11.53 11.53 15180.33 11.90 11.87 16081.73 72.38 80.39 13B 4.57 5.58 5.72 5.61 5.61 5309.48 5.62 5.61 5430.81 5.75 5.75 5914.91 5.75 5.74 5268. 7.92 8.55 8.01 8.03 5385.33 8.05 8.05 6556.50 8.98 9.00 5401.19 9.07 9.06 6227.41 45.94 51.79 8B 7.46 11.00 11. 11.02 11.02 311468.53 11.12 11.11 189134.78 11.75 11.75 276031.34 11.86 11.87 41996.95 25.81 27.47 25.48 25.64 382319.37 26.21 25.98 65475.84 30.34 30.44 608169.33 30.94 31.08 165510.73 92.20 108.35 70.92 70.61 31238.51 79.27 72.60 23090.20 93.89 93.36 31485.38 94.49 96.37 36254172.00 44.98 44.66 12206.74 45.89 45.68 7857.41 64.34 63.98 10875.86 62.07 62.51 9604. 102.88 102.13 458666.00 100.37 93.13 355916.56 123.92 124.65 831352.18 116.97 120.19 1034635.56 13 Table 6: C4 perplexity of pruned LLaMA, LLaMA-2, and LLaMA-3 models. Sparsity Method Group Compar. Prune Stra. Weight Scaling 0% Dense Centralized Local-only - - - - - - 50% FedPrLLM One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column Centralized Local-only - - - - 60% FedPrLLM One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column Centralized Local-only - - - - 70% FedPrLLM One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column - - - - - - - LLaMA LLaMA-2 LLaMA-3 7B 7.34 9.34 9. 9.43 9.43 893.05 9.44 9.44 1050.26 9.64 9.64 887.34 9.64 9.65 1242.31 13.72 14.69 13.80 15.26 2149.09 13.92 13.86 2981.52 15.24 15.26 3336.72 15.46 15.42 1825.82 85.84 96.47 81.95 82.02 15276.62 83.52 86.77 18149.76 116.61 117.29 19380.0 142.08 145.15 7664.62 13B 6.79 8.14 8.37 8.22 8.22 10616.94 8.22 8.22 8567.66 8.40 8.40 13744.66 8.41 8.41 6860.69 11.22 11.91 11.23 12.24 11488.68 11.37 11.38 10375.02 12.24 12.24 19430.46 12.54 12.54 6669.63 53.35 63. 52.55 53.51 14041.01 57.22 55.98 13537.18 77.99 78.84 10934.98 85.91 84.68 15985.50 30B 6.12 7.28 7.52 7.39 7.39 612.27 7.39 7.39 779.01 7.57 7.57 895.18 7.57 7.57 724.28 9.16 9. 9.29 9.79 993.56 9.32 9.30 1752.73 9.80 9.79 1520.32 9.86 9.87 1865.50 18.80 22.48 19.24 19.22 2059.83 19.15 19.20 2874.83 26.30 26.29 2336.68 27.17 27.00 2685.76 7B 7.03 8.94 9. 9.01 9.01 9631.37 9.01 9.02 11658.80 9.21 9.21 11440.51 9.22 9.22 10355.87 13.64 14.68 13.77 15.60 12252.16 13.84 13.85 16673.62 15.61 15.60 14613.11 16.15 16.10 16167.12 84.16 82.96 13B 6. 8.03 8.31 8.18 8.19 5075.92 8.19 8.18 4804.46 8.39 8.39 5189.73 8.39 8.39 4657.88 11.39 12.17 11.40 12.75 4606.43 11.52 11.53 5289.35 12.74 12.75 4547.54 13.01 13.01 5057.57 58.56 67.09 8B 12.34 18.38 18.92 18.32 18.32 200257.70 18.43 18.38 112192.42 19.45 19.45 90476.94 19.58 19.60 44469.52 43.02 45.25 42.61 50.37 837570.62 44.24 43.77 62234.32 50.28 50.37 622715.25 51.47 51.48 145341.28 136.66 161. 81.40 81.59 39339.21 92.51 84.99 21704.32 104.86 104.51 32034.07 103.02 102.61 27805842.0 59.87 59.97 11306.11 60.46 60.86 7166.78 79.82 79.76 11360.57 79.25 79.41 8041.09 158.08 157.87 398674.93 162.29 144.71 346598.5 184.11 184.11 345798.53 177.97 182.36 1031318.56 14 Table 7: PTB perplexity of pruned LLaMA, LLaMA-2, and LLaMA-3 models. Sparsity Method Group Compar. Prune Stra. Weight Scaling 0% Dense Centralized Local-only - - - - - - 50% FedPrLLM One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column Centralized Local-only - - - - 60% FedPrLLM One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column Centralized Local-only - - - - 70% FedPrLLM One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column One-shot Layer Row One-shot Column One-shot Iterative Layer Iterative Row Iterative Column - - - - - - - 7B 41.15 80.12 86.25 80.31 80.71 4463.92 81.22 81.26 4061.96 87.97 88.35 4557.48 87.28 87.61 6929. 193.10 208.48 187.00 186.10 5604.92 191.22 190.60 6785.79 216.09 215.50 7600.58 220.22 220.16 4242.84 698.79 782.42 737.07 718.37 18649.81 721.31 734.43 28179.23 839.42 830.33 26556.95 887.36 896.85 8660.95 LLaMA 13B 28.09 36.41 37.57 36.57 36.61 22138.56 36.54 36.55 17610.52 37.70 37.72 29140.28 37.69 37.60 15189.83 71.66 82.24 74.66 74.64 31222.37 72.90 73.74 13234.02 91.63 91.60 41079.65 90.60 90.58 11345.68 299.42 412. 366.28 369.65 18136.88 355.21 349.63 17249.42 484.11 483.58 21627.29 469.70 454.31 18472.69 30B 23.51 26.64 27.13 26.69 26.64 713.56 26.68 26.64 1158.75 27.27 27.25 982.59 27.27 27.27 1178.40 34.94 37. 35.38 35.47 1338.25 35.83 35.77 1903.66 38.22 38.25 1910.36 38.79 38.74 2133.62 110.70 144.90 120.33 118.24 3180.23 113.31 113.65 3967.48 188.18 187.11 3383.87 173.86 172.48 3246.05 LLaMA-2 LLaMA-3 7B 50.20 96.99 108.66 102.71 101.85 14256.86 102.72 103.66 13401.63 112.52 112.17 12021.08 112.95 113.32 10208.03 363.71 409.47 339.79 337.69 28046.95 368.87 367.56 24022.75 429.58 428.87 18249.40 427.12 428.36 29512.89 1902.56 1780. 13B 56.51 86.83 91.92 88.26 88.31 7392.64 88.38 88.94 6941.72 92.90 93.07 7801.23 92.58 92.61 5220.64 220.81 271.49 241.14 242.96 7553.32 237.45 235.51 8125.57 293.11 294.44 7601.34 283.34 282.20 7113. 735.73 863.50 1521.25 1557.08 49646.82 1675.79 1757.10 29254.5 1633.85 1641.92 54429.17 1789.42 1740.04 11427895.00 793.55 792.08 12010.97 775.69 767.13 10233.18 890.27 891.27 14951.70 858.48 879.50 8037.55 8B 13.30 20.69 21. 20.56 20.55 407313.84 20.55 20.60 168643.04 22.21 22.21 264723.12 22.39 22.41 39172.53 52.42 55.39 52.61 52.61 322022.84 53.78 53.25 46139.19 60.49 60.48 416094.71 61.25 61.55 133467.18 131.13 152.97 156.63 154.72 466632.84 146.27 133.92 314505.62 174.11 172.74 239612.84 162.24 168.51 738685.56 Table 8: Perplexity of pruned LLMs under 50% sparsity ratio when changing the comparison group for the local pruning method. FedPrLLM adopts one-shot pruning and no weight scaling. Local Compar. Compar. LLaMA LLaMA-2 LLaMAGroup Dataset Method Group Centralized Local-only - - 7B 7.94 8.16 13B 6.57 6.74 Layer Column WikiText2 C4 PTB WikiText2 C4 PTB FedPrLLM Layer Row Column 7.98 31.85 1749.59 6.60 10.08 10183.32 Centralized Local-only - - FedPrLLM Layer Row Column Centralized Local-only - - 10.28 10.56 10.34 34.90 975. 92.84 99.13 8.63 8.90 8.71 12.35 12605.58 43.47 45.34 FedPrLLM Layer Row Column 91.99 284.19 3976.21 43.59 109.14 28144.48 Centralized Local-only - - FedPrLLM Layer Row Column Centralized Local-only - - FedPrLLM Layer Row Column Centralized Local-only - - 8.86 8.86 8.86 138.54 8.86 14.10 14.10 14.10 155.15 14.10 108.37 108.37 7.68 7. 7.68 100.80 7.68 11.20 11.20 11.20 87.03 11.20 47.17 47.17 30B 5.47 5. 5.48 11.33 541.62 7.59 7.86 7.72 12.75 553.85 27.25 27.87 27.25 110.46 711.16 5.67 5. 5.67 49.17 5.67 8.06 8.06 8.06 48.19 8.06 29.22 29.22 7B 7.38 7. 13B 5.92 6.06 8B 12.04 12.43 7.38 39.07 25258.16 5.95 124.08 5503. 12.09 17.51 336255.96 10.24 10.52 10.32 29.79 13950.23 306.71 338.70 305.79 1886.94 14131.82 10.41 10. 10.41 764.32 10.41 17.90 17.90 17.90 222.47 17.90 4567.49 4567.49 4567.49 21323.02 4567.49 8.49 8. 8.63 207.57 4899.58 119.17 136.88 124.27 480.24 7134.88 6.38 6.38 6.38 2580.88 6.38 9.57 9. 9.57 5135.37 9.57 115.68 115.68 115.68 1075.71 115.68 19.18 19.64 19.09 28.05 129415.62 23.14 23. 22.85 44.71 293147.84 83.67 83.67 83.67 400.95 83.67 30.88 30.88 30.88 327.77 30.88 240.14 240. 240.14 928.73 240.14 FedPrLLM Layer Row Column 108.37 1060.91 108.37 47.17 394.57 47.17 29.22 239.91 29."
        }
    ],
    "affiliations": [
        "Guangming Laboratory",
        "Hangzhou Dianzi University",
        "Southern University of Science and Technology",
        "Sun Yat-sen University",
        "The University of Hong Kong"
    ]
}