{
    "paper_title": "Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats",
    "authors": [
        "Chen Ziwen",
        "Hao Tan",
        "Kai Zhang",
        "Sai Bi",
        "Fujun Luan",
        "Yicong Hong",
        "Li Fuxin",
        "Zexiang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose Long-LRM, a generalizable 3D Gaussian reconstruction model that is capable of reconstructing a large scene from a long sequence of input images. Specifically, our model can process 32 source images at 960x540 resolution within only 1.3 seconds on a single A100 80G GPU. Our architecture features a mixture of the recent Mamba2 blocks and the classical transformer blocks which allowed many more tokens to be processed than prior work, enhanced by efficient token merging and Gaussian pruning steps that balance between quality and efficiency. Unlike previous feed-forward models that are limited to processing 1~4 input images and can only reconstruct a small portion of a large scene, Long-LRM reconstructs the entire scene in a single feed-forward step. On large-scale scene datasets such as DL3DV-140 and Tanks and Temples, our method achieves performance comparable to optimization-based approaches while being two orders of magnitude more efficient. Project page: https://arthurhero.github.io/projects/llrm"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 1 ] . [ 1 1 8 7 2 1 . 0 1 4 2 : r LONG-LRM: LONG-SEQUENCE LARGE RECONSTRUCTION MODEL FOR WIDE-COVERAGE GAUSSIAN SPLATS Chen Ziwen1 Hao Tan2 Kai Zhang2 Sai Bi2 Fujun Luan2 Yicong Hong2 Li Fuxin1 Zexiang Xu2 1Oregon State University 2Adobe Research"
        },
        {
            "title": "ABSTRACT",
            "content": "We propose Long-LRM, generalizable 3D Gaussian reconstruction model that is capable of reconstructing large scene from long sequence of input images. Specifically, our model can process 32 source images at 960540 resolution within only 1.3 seconds on single A100 80G GPU. Our architecture features mixture of the recent Mamba2 blocks and the classical transformer blocks which allowed many more tokens to be processed than prior work, enhanced by efficient token merging and Gaussian pruning steps that balance between quality and efficiency. Unlike previous generalizable 3D GS models that are limited to taking 14 input images and can only reconstruct small portion of large scene, Long-LRM reconstructs the entire scene in single feed-forward step. On large-scale scene datasets such as DL3DV-140 and Tanks and Temples, our method achieves performance comparable to optimization-based approaches while being two orders of magnitude more efficient. Project page: https://arthurhero.github.io/projects/ llrm/ Figure 1: We introduce Long-LRM, novel Gaussian reconstruction model capable of reconstructing large real scene from long sequence of up to 32 input images, with wide viewing coverage at resolution of 960 540, in just 1.3 seconds. Notably, as feed-forward generalizable model, Long-LRM can achieve instant large-scale GS reconstruction with high rendering quality comparable to (and, as shown in the figure, sometimes even surpassing) the optimization-based 3D Gaussian splatting (3D GS), which requires over 13 minutes for optimization."
        },
        {
            "title": "INTRODUCTION",
            "content": "3D reconstruction from multi-view images is fundamental problem in computer vision, with applications ranging from 3D content creation, VR/AR, to autonomous driving and robotics. Recently, Research done when Chen Ziwen was an intern at Adobe Research 1 NeRF (Mildenhall et al., 2021) and various radiance field-based methods (Muller et al., 2022; Xu et al., 2022; Chen et al., 2022; Barron et al., 2023) have shown great potential in reconstructing high-quality 3D scenes from set of posed images via differentiable rendering. However, these models are slow to reconstruct and not generalizable to unseen scenes, as they require optimization for each scene independently. While 3D Gaussian splatting (GS) (Kerbl et al., 2023) has significantly advanced the reconstruction and rendering efficiency, it still typically requires at least 10 minutes to optimize for each scene and can not achieve an instant reconstruction. Recently, generalizable 3D GS models (Szymanowicz et al., 2023; Tang et al., 2024) have been proposed to enable fast feed-forward GS reconstruction, avoiding per-scene optimization. Several methods (Charatan et al., 2024; Zhang et al., 2024; Liu et al., 2024a; Chen et al., 2024) have shown promising scene-level reconstruction results on real 3D captures by regressing per-pixel Gaussian primitives. In particular, GS-LRM (Zhang et al., 2024), following the principles of 3D large reconstruction models (LRMs) (Hong et al., 2024; Li et al., 2023; Wang et al., 2023) and leveraging densely self-attention-based transformer (Vaswani, 2017) without using 3D inductive biases such as epipolar attention or sweeping volumes, has achieved state-of-the-art novel-view rendering quality on multiple challenging datasets. However, the previous generalizable GS models are designed to handle small number of input images (typically 14) with limited viewing coverage, thus are incapable of reconstructing large real-world scenes, which require wide view span and at least dozens of images. In such cases, per-scene optimization-based methods were still the only viable option. Our goal is to enable fast and accurate GS reconstruction of large scenes with wide viewing coverage through direct feed-forward network prediction. To this end, we propose Long-LRM, novel GSbased LRM that is able to handle long-sequence input and achieve high-quality 3D GS reconstruction of large scenes from as many as 32 widely-displaced multi-view images at 960540 resolution within only 1.3 seconds on single A100 80G GPU. As shown in Fig. 1, the photorealistic novel-view renderings produced by our approach has quality comparable to or even better than 3D GS (Kerbl et al., 2023) that takes over 10 minutes for per-scene optimization. Specifically, as inspired by GS-LRM, we patchify the multi-view input images into sequence of patch tokens and consider the task of GS reconstruction as sequence-to-sequence translation to regress pixel-aligned Gaussian primitives. However, unlike GS-LRM that focuses on 2-4 input images, our input setting with 32 960540 images corresponds to an extremely long token sequence about 250K context length (considering patch size of 88) which is highly challenging for dense transformers (as used by GS-LRM) due to their quadratic time complexity. Note that this length is even larger than many modern large language models (LLM), such as LLama3 (Dubey et al., 2024) with context length of 128K. To address this challenge, we leverage the recent advancements of state space models (SSMs) (Gu & Dao, 2023), designed to handle long-context reasoning efficiently with linear complexity. In particular, we propose novel LRM architecture that combines Mamba2 (Dao & Gu, 2024) blocks with transformer blocks, enabling efficient sequential long-context reasoning while preserving critical global context. Additionally, we introduce token merging module to further reduce the number of tokens in the middle of the network processing, along with Gaussian pruning step to encourage efficient use of the dense per-pixel Gaussians. These combined designs allow us to train our LongLRM using similar computational resources to GS-LRM, while successfully scaling up the input sequence length and achieving over 10 faster training on long-sequence inputs, enabling fast, high-quality, wide-coverage reconstruction of large real scenes (see Tab. 2). We train our Long-LRM on the recent DL3DV dataset (Ling et al., 2024), which comprises approximately 10K diverse indoor and outdoor scenes. We evaluate our model on both the DL3DV test set and the Tanks and Temples dataset (Knapitsch et al., 2017), using 32 input images for each scene. The results show that our direct feed-forward reconstruction achieves comparable novel view synthesis quality to the per-scene optimization results of 3D GS, while substantially reducing the reconstruction time by two orders of magnitude (1.3 seconds vs. 13 minutes). Our approach is the first feed-forward GS solution for wide-coverage scene-level reconstruction and the first to enable large-scale GS scene reconstruction in seconds."
        },
        {
            "title": "2 RELATED WORK",
            "content": "3D Reconstruction. Many traditional and learning-based 3D reconstruction methods have been focusing on pure geometry reconstruction, where surface meshes (Murez et al., 2020; Sun et al., 2021; Bozic et al., 2021; Stier et al., 2021) or depth maps (Zbontar & LeCun, 2016; Schonberger et al., 2016; Yao et al., 2018; Cheng et al., 2020; Kar et al., 2017; Duzceker et al., 2021; Sayed et al., 2022) are the target output. These methods usually involve explicit feature matching along the epipolar lines, followed by the prediction of TSDF or depth values performed by the neural networks. In contrast, we adopt the recent 3D GS representation for joint geometry and appearance reconstruction, allowing for photo-realistic novel view synthesis. Neural reconstruction and rendering. Instead of directly predicting the surface geometry, NeRF (Mildenhall et al., 2021) proposes to leverage differentiable volume rendering to regress novel-view images, supervised with rendering loss. This implicit way of reconstruction eliminates the need for hard-to-obtain ground-truth 3D supervision while producing visually pleasing reconstruction results. However, NeRF reconstruction requires optimizing its network for each scene independently, taking hours or even days for reconstruction. Follow-up works have introduced advanced neural scene representations (Barron et al., 2023; Muller et al., 2022; Chen et al., 2022; Xu et al., 2022; Tancik et al., 2023; Barron et al., 2022) and largely improved the time and memory efficiency with 3D Gaussian splatting (Kerbl et al., 2023) notably reducing the reconstruction time to dozens of minutes while keeping high reconstruction quality and enabling real-time rendering, but these methods are still unable to achieve instant prediction. We aim to build scalable feed-forward reconstruction model, capable of achieving instant 3D GS reconstruction in seconds. Generalizable NeRF and 3D GS. Previous attempts to develop generalizable NeRF models have primarily relied on classical projective geometric structures, such as epipolar lines (Yu et al., 2021; Wang et al., 2021; Liu et al., 2022; Suhail et al., 2022) or plane-sweep cost volumes (Chen et al., 2021; Johari et al., 2022; Lin et al., 2022; Zhang et al., 2022), to aggregate multi-view features from nearby views for local NeRF estimation. Recently, similar designs have been adapted to enable feed-forward scene-level 3D GS reconstruction with generalizable models (Charatan et al., 2024; Chen et al., 2024; Liu et al., 2024a). However, since both epipolar geometry and plane-sweep volumes depend on significant overlap between input views, these GS-based methods (as well as most prior NeRF-based methods) are limited to local reconstructions from small number (14) of narrow-baseline inputs. On the other hand, GS-LRM (Zhang et al., 2024) avoids these 3D-specific structural designs and adopts an attention-based transformer, achieving state-of-the-art performance in this domain. However, GS-LRM still focuses on solving the problem of local reconstruction from just 24 views. In contrast, our model supports feed-forward 3D GS reconstruction from 32 input images, enabling complete reconstruction of large scenes. Efficient models for long sequences. Transformer-based 3D large reconstruction models (LRMs) have emerged (Hong et al., 2024; Li et al., 2023; Xu et al., 2023; Wang et al., 2023; Wei et al., 2024; Xie et al., 2024; Zhang et al., 2024), enabling high-quality 3D reconstruction and rendering from sparse-view inputs. While transformers dominate various AI fields due to their flexibility with input modalities and scalability in model sizes, their quadratic time complexity makes them extremely slow when handling long sequences, often requiring thousands of GPUs for parallel computing (Dubey et al., 2024). Efficient architectures such as linear attention (Katharopoulos et al., 2020) and structured state space model (SSM) (Gu et al., 2021) have been proposed in NLP to deal with large corpus of text. Mamba (Gu & Dao, 2023), variant of SSM, offers significant improvements by computing state parameters from each input in the sequence and has been successfully extended to tackle vision tasks (Zhu et al., 2024; Liu et al., 2024b; Lieber et al., 2024; Huang et al., 2024). Mamba2 (Dao & Gu, 2024) further restricts the state matrix and expands state dimensions, showing performance comparable to transformers on multiple language tasks. However, empirical studies (Waleffe et al., 2024) indicate that transformers still outperform Mamba2 in in-context learning and long-context reasoningboth critical for 3D reconstruction. Inspired by the findings and hybrid models from Waleffe et al. (2024) and Jamba (Lieber et al., 2024), we propose to apply hybrid architecture combining transformer and Mamba2 blocks for long-sequence 3D GS reconstruction, achieving balance between training efficiency and reconstruction quality (see Tab. 2). 3 Figure 2: Long-LRM takes up to 32 input images along with their Plucker ray embeddings as model input, which are then patchified, linearly transformed, and concatenated into token sequences. These tokens are processed through an optional token merging module, followed by sequence comprising Mamba2 blocks (7) and Transformer block (1). This entire processing structure is repeated three times (3) to ensure effective handling of the long-sequence inputs and comprehensive feature extraction. Fully processed, the tokens are unpatchified and decoded into Gaussian parameters, followed by Gaussian pruning to generate the final 3D GS representation. The bottom section of the figure illustrates the resulting novel view synthesis and wide-coverage Gaussian reconstruction, demonstrating Long-LRMs capability to handle extensive view coverage and produce high-quality, photorealistic reconstructions."
        },
        {
            "title": "3 METHOD",
            "content": "We present our Long-LRM method in this section. We give an overview in Sec. 3.1, the implementation details of the Mamba2 blocks in Sec. 3.2 and additional designs for memory reduction (e.g., token merging) in Sec. 3.3. We end with discussion of the training objectives in Sec. 3.4 that help the model to effectively converge."
        },
        {
            "title": "3.1 OVERALL ARCHITECTURE",
            "content": "As shown in Fig. 2, we follow prior work (Xu et al., 2023; Wei et al., 2024; Zhang et al., 2024) to tokenize the channel-wise concatenated RGB images and Plucker rays. Similar to GS-LRM (Zhang et al., 2024), we view the per-pixel GS prediction as sequence-to-sequence mapping. But crucially, we use hybrid of Mamba2 blocks and transformer blocks, following the studies in Waleffe et al. (2024) and Lieber et al. (2024), for better scalability to higher resolution and denser views, while GS-LRM solely builds upon transformer blocks. In our implementation, each hybrid block consists of 7 Mamba blocks and one transformer block, which we empirically observe to be balanced configuration. For the transformer blocks, we use global self-attention, as done in recent LRMs (Wei et al., 2024; Zhang et al., 2024). We detail our implementation of Mamba2 blocks in Sec. 3.2. token merging stage is optionally injected before the hybrid block to further speed up the processing, which is detailed later in Sec. 3.3. We decode per-pixel Gaussian parameters from the output tokens in the same way as GS-LRM. But we apply additional training-time and test-time pruning of the extremely dense Gaussians to improve efficiency at high resolution and increased views."
        },
        {
            "title": "3.2 MAMBA2 BLOCK",
            "content": "A Mamba block (Gu & Dao, 2023), similar to transformer block, processes token sequence of shape LD by mixing the token information, and outputs token sequence of the same shape. For sequence of length L, transformer block has computational complexity of O(L2) while Mamba effectively reduces it to O(L). Thus, it is suitable for the dense reconstruction task in our Long-LRM. 4 Being variant of SSM, Mamba at its core processes each input token by formula ht = Aht1 + Bxt yt = Cht (1) (2) where is the hidden state, is the output token, is the sequence index, and A, B, are parameters. Different from previous work (Gu et al., 2021), Mamba computes A, B, from the input with linear layer instead of storing them as model parameters. Its worth noting that similar to transformer block, Mamba block can be highly parallelized in terms of computation for leveraging the massive GPU compute power, which is one core factor driving its increasing popularity. The novel Mamba2 (Dao & Gu, 2024) block improves over Mamba by further restricting the state matrix to be scalar times identity structure, allowing the usage of efficient block multiplication and expansion to larger state dimensions, showing performance comparable to transformers on multiple language tasks. However, since the Mamba2 block is designed for language tasks, it only scans through the tokens in one direction, which is suboptimal for images. Following Vision Mamba (Zhu et al., 2024), we take bi-directional scans over the concatenated token sequence. Specifically, we first compute the state parameters from the input using one linear layer; then we run the SSM block in both forward and backward directions on the token sequence. Finally, we sum up the output tokens from the two scans before going through another linear layer. We also did some preliminary exploration of more complex scan patterns as in VMamba (Liu et al., 2024b) and LocalMamba (Huang et al., 2024), but we observed substantial decrease in speed, and hence we decided not to adopt them."
        },
        {
            "title": "3.3 TOKEN MERGING AND GAUSSIAN PRUNING",
            "content": "Boosting up input view number and image resolution can drastically increase the token sequence length. With 32 960540 images and patch size 8, the length can reaches about 260k, highly challenging even for linear-complexity models like Mamba. Empirically, we also find even the all-Mamba2 variant of our model runs out of memory under our highest resolution setting (see Tab. 2). To further reduce memory usage, we propose to merge the tokens in the middle of the model as well as to prune the Gaussians before rendering novel views. Token merging achieves fine-to-coarse effect similar to the traditional multi-level CNN encoders and effectively reduces token sequence length down to 1/4. We first reshape the token sequence from LD back to D where is the original patch size. Then, we apply channel-wise 22 2D convolution with stride 2, resulting in output shape 2p D, where is the new token dimension that can differ from the original one. Finally, we reshape it back to 4 where each token now has an effective patch size of 2p. In our ablation studies (Tab. 2), we find our token merging design does not sacrifice much reconstruction quality, while significantly reducing memory usage and increasing training speed. 2p Even with token merging, our per-pixel Gaussian prediction still brings us an enormous quantity of Gaussians at the end (17 million for 32 images with resolution 960540), which is likely more than we need for high-quality reconstruction due to the overlap between the input view frustums. To encourage the model to use compact set of Gaussians, we apply punishment on the opacity of all Gaussians (detailed in Sec. 3.4). With the effective reduction in the number of visible Gaussians, we can thus simply prune away certain percentage of Gaussians with low opacity. Empirically, we find no difference in rendering quality if we prune away Gaussians with opacity below 0.001. Beside pruning during inference, we also apply the Gaussian pruning to the 960540 resolution training. We keep fixed-number Gaussians instead of using opacity threshold to ensure near-constant training memory usage. O.w., the training can go out of memory for some scenes."
        },
        {
            "title": "3.4 TRAINING OBJECTIVES",
            "content": "Lastly, we illustrate the training objectives for Long-LRM. Rendering loss. Following previous work (Zhang et al., 2024), we use combination of Mean Squared Error (MSE) loss and Perceptual loss Limage ="
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) (cid:16) i="
        },
        {
            "title": "MSE",
            "content": "(cid:16) , Ipred Igt (cid:17) + λ Perceptual (cid:16) , Ipred Igt (cid:17)(cid:17) (3) 5 to supervise the quality of the rendered images, where λ is set to 0.5. While training solely with rendering loss can achieve competitive visual quality to our final model (see Sec. 5.2), we further introduce two regularization terms to improve training stability and inference efficiency. Depth regularization for training stability. Training instability is well-known curse for large-scale training. In our task, we observe that the instability comes from the difficulty of optimizing the Gaussian positions. With rendering loss only, the model will produce ill-posed Gaussians known as floaters, which does not lie on the actual 3D surface common issue for novel view synthesis (see the black floaters in Fig. 1). To stabilize training, we add scale-invariant depth loss (cid:88) (cid:16) (cid:17) Ldepth = Smooth-L"
        },
        {
            "title": "Dda",
            "content": "i , Dpred (4)"
        },
        {
            "title": "1\nM",
            "content": "i=1 is the disparity map predicted by DepthAnything (Yang et al., 2024), and Dpred where Dda is the disparity map obtained from the predicted position of the per-pixel Gaussians. Following Yang et al. (2024), we normalize the disparity maps by subtracting their medians t(di) and then dividing by (cid:80) di t(di). This soft depth supervision their mean absolution deviation from the medians effectively helps reduce the chance of the training divergence. 1 HW Opacity regularization for inference efficiency. Since our per-pixel prediction strategy renders dense set of Gaussians, to encourage an efficient use of the Gaussians, we apply small L1 regularization on the opacity Lopacity ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 oi (5) where the opacity values are between 0 and 1. Intuitively, L1 can encourage the sparsity of the regularized terms (Tibshirani, 1996). We empirically observe that adding this loss can drastically push the percentage of Gaussians with opacity above 0.001 from 99% down to around 40% (see Tab. 4). With these near-zero opacity Gaussians, we can perform Gaussian pruning as discussed above in Sec. 3.3 and both reduce the Gaussian splatting loading time and increase the rendering speed for better model serving experience. This regularization also enables the extreme 960540 resolution training where in-training pruning is used. Overall training loss. Our total loss is thus the rendering loss and the weighted regularization loss terms discussed above: = Limage + λopacity Lopacity + λdepth Ldepth (6) where we set λopacity = 0.1 and λdepth = 0.01."
        },
        {
            "title": "4.1 DATASETS",
            "content": "DL3DV (Ling et al., 2024) is recently published large-scale, real-world scene dataset for 3D reconstruction and novel view synthesis. It features diverse variety of scene types, with both indoor and outdoor captures. It consists of two parts: DL3DV-10K is the training split, consisting of 10,510 high-resolution videos1, each accompanied by 200300 keyframes with camera pose annotation (obtained from COLMAP (Schonberger et al., 2016)); DL3DV-140 Benchmark is the test split, containing 140 test scenes. We train our model on DL3DV-10K and evaluate on the DL3DV-140 Benchmark. We also perform zero-shot inference on Tanks and Temples (Knapitsch et al., 2017), another real-world scene dataset for novel view synthesis. It also contains 200300 keyframes with camera pose annotation (obtained from COLMAP) for each scene. Following previous work (Kerbl et al., 2023; Liu et al., 2024a), we use the train and the truck scene from Tanks and Temples. 4."
        },
        {
            "title": "IMPLEMENTATION AND EXPERIMENT DETAILS",
            "content": "Architecture Details. Our model consists of 24 blocks in total, with every 7 Mamba2 blocks followed by 1 transformer block, repeating 3 times. We start with patch size 8 and token dimension 1We trained on the first 7K of the training split as the remaining data had not been released at the time of our experiments. 6 256. We perform token merging at the beginning of the 9th block, with patch size expanded to 16 and token dimension expanded to 1024. For Mamba2 blocks, we use state dimension 256, an expansion rate 2 and head dimension 64. For transformer blocks, we use head dimension 64 and an MLP dimension ratio of 4. We use the FlashAttentionV2 (Dao, 2024) implementation which optimizes the GPU IO utilization for long sequences. Training Settings. Directly training the model on high-resolution images is extremely inefficient; therefore we opt for an low-to-high-resolution curriculum training schedule, with three training stages, using image resolutions of 256256, 512512 and 960540. Specifically, in the 1st stage, training images are resized so the shorter side is 256 and then centercropped to square. For the training view selection, we first randomly pick consecutive subsequence ranging from 64 frames to 128 frames, then uniformly sample 32 images as input and sample 8 images as target. Input and target are sampled independently and thus they can overlap. We randomly shuffle the input view order with probability 0.5 and reverse the input view order also with probability 0.5. We train with peak learning rate of 4E4 and the AdamW optimizer (Loshchilov & Hutter, 2017) with weight decay of 0.05. The learning rate is linearly warmed up in the first 2K steps and then cosine decayed. We use batch size of 256, and train for 60K steps. In the 2nd stage, we resize and crop the images to 512512, decrease the peak learning rate to 4E5, and train the model for 10K steps at batch size 64. The view selection protocol remains the same. In the last stage, we resize the images to 960540 without square cropping, expand the view selection sampling range to the entire sequence (about 200300 frames for DL3DV), and keep training the model for another 10K steps at batch size 64. We perform Gaussian pruning in this stage to save GPU memory usage, where we only keep top 40% of the Gaussians ranked by opacity plus 10% randomly sampled from the rest. We augment the FOV of the images by randomly center-cropping the images to 0.771.0 of the original size and resize back, in order to fit broader range of camera models. We optionally finetune model with 16 images as input. Evaluation Settings. During evaluation, our goal is to reconstruct the scene captured by the entire video sequence. Following previous work (Barron et al., 2022; Kerbl et al., 2023), we uniformly pick every 8-th image of the sequence as the test split. From the rest of the sequence, we use K-means clustering (based on camera positions and directions) for choosing the input views to ensure the coverage of the scene. The number of clusters is set of the number of input views. We simply choose the cameras closest to the cluster centers as the input split. We use an image resolution of 960 540 during the evaluation. We perform Gaussian pruning during evaluation by only keeping the top 50% of the Gaussians with highest opacity values, where 50% is safe range with negligible quality loss."
        },
        {
            "title": "4.3 RESULTS",
            "content": "We encourage the readers to check our project page (https://arthurhero.github.io/ projects/llrm/) for video and interactive renderings of our output 3D GS assets. Our approach achieves wide-coverage scene-level 3D Gaussian splatting reconstruction from up to 32 high-resolution input images, and to the best of our knowledge, we are the only method capable of this. While several recent works, including pixelSplat (Charatan et al., 2024), MVSplat (Chen et al., 2024), MVSGaussian (Liu et al., 2024a) and GS-LRM (Zhang et al., 2024), aim to handle scene-level GS reconstruction, their models are all designed for limited number (14) of input images, with pixelSplat and MVSplat even showing results only at 256256 resolution. Moreover, pixelSplat, MVSplat and MVSGaussian incorporate traditional 3D inductive biases, such as epipolar projection and cost volumes, in their model architectures, which typically expect narrow view coverage with large overlaps in input images for correspondence reasoningmaking them unsuitable for our setting with many wide-coverage input views. Additionally, naively extending pixelSplat, MVSplat, MVSGaussian and GS-LRM to train with many more input views at higher resolutions quickly lead to out-of-memory issues and cannot be easily achieved without significant modifications and further research. Therefore, in this section, we focus on comparing with the original optimization-based 3D Gaussian splatting (Kerbl et al., 2023). In addition, we also carefully compare with GS-LRM, which can be seen as an ablated version of our model without Mamba2 blocks, in terms of input scalability, quality 7 Figure 3: Qualitative comparisons between our Long-LRM and 3D Gaussian splatting (3D GS) across different scenes, reconstructed from 32 input images at 960 540 resolution. The left column showcases the wide-coverage Gaussian reconstruction achieved by Long-LRM, while the right column shows results from 3D GS. Our approach maintains high-quality reconstruction with competitive or even superior PSNR values (e.g., 25.71 vs. 24.13, 28.09 vs. 27.92), demonstrating the ability to generate accurate details and fewer artifacts in challenging regions. The red ellipses highlight areas where 3D GS struggles with artifacts or inaccuracies, whereas Long-LRM produces cleaner and more photorealistic outputs, effectively handling the wide range of input views."
        },
        {
            "title": "Input\nViews",
            "content": "Method FeedForward Time DL3DV-140 Tanks&Temples PSNR SSIM LPIPS PSNR SSIM LPIPS 16 32 3D GS30k Ours 3D GS30k Ours 13min 0.7sec 13min 1.3sec 21.20 22.40 0.708 0.730 23.60 23. 0.779 0.775 0.264 0.302 0.213 0.262 16.76 17.17 0.598 0.541 18.10 18. 0.688 0.590 0.334 0.426 0.269 0.375 Table 1: Quantitative comparison to 3D Gaussian splatting optimization. Feed-forward column indicates whether the method performs zero-shot feed-forward prediction. Time refers to the total inference/optimization time for all views in the test split for each scene. The image resolution is 960 540. and efficiency later in Sec. 5.2. In Table 1, we show the quantitave comparison results with the optimization-based 3D GS on two real-world large-scene datasets: DL3DV-140 Benchmark (Ling et al., 2024) and Tanks and Temples (Knapitsch et al., 2017). We show results under the sparser 16 input-view setting as well as the 32 input-view setting. We show that our model is capable of reconstructing an unseen novel scene from long-sequence input in feed-forward manner within as little time as 1.3 seconds, 600 faster than 3D GS optimization (13 minutes for 30K steps). Reconstruction quality-wise, our feed-forward reconstruction results are comparable with 3D GS with 30K optimization steps. Our model takes lead in terms of PSNR (with the gap larger in the sparser 16-view setting: +1.2 for DL3DV-140 and +0.4 for Tanks and Temples), while 3D GS performs better in terms of LPIPS. We speculate this is because 3D GS optimization is much stronger at directly copying the input images into the reconstructed scene with these many optimization steps, and thus can render images with local color distribution extremely similar to the test images; however, without any prior knowledge in 3D geometry, it can easily overfit to the input views when the input is sparse (see the floaters in Fig. 1 and 3). We also provide qualitative comparisons between our feed-forward reconstruction and 3D GS optimization results in Fig. 3. More visualization and interactive results can be found on our website and in Appendix."
        },
        {
            "title": "Image\nSize",
            "content": "Batch Size / GPU"
        },
        {
            "title": "Block Type",
            "content": "Token Merge Patch Size Token Dimension #Param Iteration Time (sec) GPU Memory (GB) PSNR 4 256 16 100K 256 4 60K 10K 32 32 960540 1 1 Transformer (GS-LRM) Mamba2 {7M1T}3 {7M1T}3 Transformer (GS-LRM) Mamba2 {7M1T}3 {7M1T}3 Transformer (GS-LRM) Mamba2 {7M1T}3 {7M1T} / / / 8 8 8 1024 1024 1024 @9 8 16 256 1024 / / / 8 8 1024 1024 1024 @9 8 16 256 1024 / / / 8 8 8 1024 1024 1024 @9 8 16 256 327M 190M 206M 162M 327M 190M 206M 162M 327M 190M 206M 162M 2.3 2.8 2.6 1.9 14.5 6.0 7.1 3.5 50.5 7.4 11.5 4. 10K All other variants are out of memory. {7M1T}3 @9 8 16 256 1024 162M 12.6 44 35 35 68 70 70 25 44 62 64 23 53 21.13 19.82 21.58 21.25 too slow 24.28 26.82 25.62 too slow 24.83 28.16 27. 27.32 Table 2: Ablation studies on model architecture. We study how the model architecture affects training time and memory efficiency as well as the reconstruction quality. All variants have 24 blocks in total. {7M1T}3 refers to our 7 Mamba2 blocks + 1 Transformer block, repeating 3 times model architecture. @9 means the token merging happens at the beginning of the 9th block. Models are trained on DL3DV-10K and evaluated on DL3DV-140 Benchmark. The 512-resolution models are finetuned from the checkpoints of their 256-resolution counterparts, and the 960-resolution from the 512-resolution checkpoints. We study how the model architecture variants scale with long input image sequence for both training efficiency and reconstruction quality. As shown in Table 2, we consider 4 experimental setups with different sequence lengths: 1. sparse low-resolution (Input Views=4, Image Size=256), 2. dense low-resolution (Input Views=32, Image Size=256), 3. dense high-resolution (Input Views=32, Image Size=512), 4. dense ultra-resolution (Input Views=32, Image Size=960540) 2. The results of different model architecture under the same setup are presented within Table block (i.e., every four rows). We study four model variants: all transformer blocks (row 1; equivalent to GS-LRM), all Mamba2 blocks (row 2), hybrid blocks but without token merging (row 3), and hybrid blocks with token merging (row 4; our final model). All variants have 24 blocks in total. We illustrate the number of model parameters (#Param), the training iteration time, the GPU memory usage, and PSNR reconstruction metric in the last four columns. The detailed experimental setup of this ablation study can be found in Appendix. We next highlight the key observations. Comparisons to Transformer. Transformers performance is comparable to our model under the 4-view 256-resolution (the 1st block in Tab. 2) setting. However, its training speed explodes for larger visual inputs, either with dense view or high resolutions. In our 3rd experiment setup (32 views with 2Note that here the terminology of sparse, dense, low, high, ultra are all relative. We use these terminology for simplicity and clarity. 9 resolution 512), the per-iteration training time with batch size=1 can go up to 50.5 seconds, which is unaffordable to train. This is due to the quadratic time complexity of transformers. Comparisons to Mamba2. The Mamba2 variant shows more manageable increase in time as the input scales up but leads to noticeable decline in reconstruction quality compared to other variants. For instance, in the 256-resolution, 4-view setting (1st block in Tab 2 ), the Mamba2 variant exhibits 1.8 PSNR drop compared to our hybrid model (row 3). This performance gap widens with longer sequences, reaching 2.5 PSNR for 32 views (2nd block in Tab 2) and 3.3 PSNR at 512 resolution (3rd block in Tab 2). This decrease in quality is possibly due to Mambas purely state-based design, which struggles to capture long-range dependencies effectively. Effectiveness of Token Merging. Comparing with transformer and Mamba2, our hybrid variant (third row in each block) gets the best of both worlds the reconstruction quality (in terms of PSNR) comparable to transformer and the speed (in terms of Iteration Time) comparable to Mamba2. On top of it, with the token merging design (last row in each block), our final model successfully reduces both time and memory usage down to 1/3 in the 512512 setting, without sacrificing too much reconstruction quality. Token merging with Gaussian pruning also further enables scaling up to 960 540 resolution with stable reconstruction, where all other variants are out-of-memory."
        },
        {
            "title": "Loss Type",
            "content": "PSNR % Gaussians w/ opacity>0.001 4 256 rendering-only +opacity +opacity+depth 20.43 20.96 21. 99.2 68.3 70.1 Table 3: Ablation studies on training objectives. We study how the opacity loss and the depth supervision affect the reconstruction quality as well as the Gaussian usage. Impact of the regularization terms. In Tab. 3, we show the impact of the two regularization terms introduced in Sec. 3.4: the opacity loss and the depth supervision. From the table, we see that adding the opacity loss can significantly reduce the number of visible Gaussians (% of Gaussians with opacity above 0.001), while having negligible impact on model rendering performance. The depth supervision help improve the rendering quality by guiding the floater Gaussians to the position of the true surfaces. We observe it also slightly lifts the number of visible Gaussians, which is reasonable because now the model can drive the floater Gaussians to their correct positions instead of simply deleting them by assigning them low opacity values. Also due to this, training with depth supervision significantly reduces the chance of gradient explosions in our experiments."
        },
        {
            "title": "Image\nSize",
            "content": "Input Sampling Range (frame) w/ opacity loss % Gaussians w/ opacity>0.001 4 4 32 32 32 256256 256256 256256 512512 960540 16 16 64 128 64 128 200 99.2 68.3 41.8 34.1 33.3 Table 4: Gaussian usage impacted by opacity loss and input size. We observe the opacity loss can effectively reduce the number of visible Gaussians, and our model learns to adapt to more compact set of Gaussians when the input views get denser. Impact of opacity loss on Gaussian Usage. In Tab. 4, we show how the opacity loss and the input size affects the Gaussian usage (percentage of Gaussians with opcacity > 0.001). Comparing row-1 and row-2, we observe that the opacity regularization loss introduced in Sec. 3.4 can effectively reduce the number of high-opacity Gaussians shown in the last column. Furthermore, our model learns to adaptively use different number of Gaussians when the input varies (as shown in Tab. 4). As the image resolution increases (hence more per-pixel Gaussians predicted), the chance that multiple pixels can be covered by the same Gaussian increases as well, and thus the percentage of Gaussian usage decreases. However, as the sampling range (i.e., the maximum difference in frame indices of the input views, shown as Input Sampling Range) increases, the overlap between input views decreases, and thus the model needs to retain more Gaussians to keep reconstruction quality, resulting in negligible drop in Gaussian usage in the last row."
        },
        {
            "title": "6 CONCLUSIONS",
            "content": "In this work, we introduce Long-LRM, novel model for fast and scalable 3D Gaussian splatting reconstruction. By combining Mamba2 and transformer blocks, along with token merging and Gaussian pruning, Long-LRM can instantly reconstruct wide-coverage 3D GS scene from 32 images at high resolution of 960 540 in just 1.3 seconds, leading to high rendering quality comparable to optimization-based methods such as 3D Gaussian splatting. Our approach is the first feed-forward GS solution for wide-coverage scene-level reconstruction."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We thank Nathan Carr and Kalyan Sunkavalli for their support and helpful discussions. We thank Lu Ling for the support in using the DL3DV dataset."
        },
        {
            "title": "REFERENCES",
            "content": "Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 54705479, 2022. Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1969719705, 2023. Aljaz Bozic, Pablo Palafox, Justus Thies, Angela Dai, and Matthias Nießner. Transformerfusion: Monocular rgb scene reconstruction using transformers. Advances in Neural Information Processing Systems, 34:14031414, 2021. David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian In Proceedings of the splats from image pairs for scalable generalizable 3d reconstruction. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1945719467, 2024. Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1412414133, 2021. Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European Conference on Computer Vision (ECCV), 2022. Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. arXiv preprint arXiv:2403.14627, 2024. Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, and Hao Su. Deep stereo using adaptive thin volume representation with uncertainty awareness. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 25242534, 2020. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=mZn2Xyh9Ec. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 11 Arda Duzceker, Silvano Galliani, Christoph Vogel, Pablo Speciale, Mihai Dusmanu, and Marc Pollefeys. Deepvideomvs: Multi-view stereo on video with recurrent spatio-temporal fusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1532415333, 2021. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d, 2024. Tao Huang, Xiaohuan Pei, Shan You, Fei Wang, Chen Qian, and Chang Xu. Localmamba: Visual state space model with windowed selective scan. arXiv preprint arXiv:2403.09338, 2024. Mohammad Mahdi Johari, Yann Lepoittevin, and Francois Fleuret. Geonerf: Generalizing nerf with geometry priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1836518375, 2022. Abhishek Kar, Christian Hane, and Jitendra Malik. Learning multi-view stereo machine. Advances in neural information processing systems, 30, 2017. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 51565165. PMLR, 2020. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36(4):113, 2017. Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214, 2023. Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: hybrid transformermamba language model. arXiv preprint arXiv:2403.19887, 2024. Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia 2022 Conference Papers, pp. 19, 2022. Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2216022169, 2024. Tianqi Liu, Guangcong Wang, Shoukang Hu, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, and Ziwei Liu. Fast generalizable gaussian splatting reconstruction from multi-view stereo. arXiv preprint arXiv:2405.12218, 2024a. Yuan Liu, Sida Peng, Lingjie Liu, Qianqian Wang, Peng Wang, Christian Theobalt, Xiaowei Zhou, and Wenping Wang. Neural rays for occlusion-aware image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 78247833, 2022. Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba: Visual state space model, 2024b. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 12 Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4): 115, 2022. Zak Murez, Tarrence Van As, James Bartolozzi, Ayan Sinha, Vijay Badrinarayanan, and Andrew Rabinovich. Atlas: End-to-end 3d scene reconstruction from posed images. In Computer Vision ECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part VII 16, pp. 414431. Springer, 2020. Mohamed Sayed, John Gibson, Jamie Watson, Victor Prisacariu, Michael Firman, and Clement Godard. Simplerecon: 3d reconstruction without 3d convolutions. In European Conference on Computer Vision, pp. 119. Springer, 2022. Johannes Schonberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14, pp. 501518. Springer, 2016. Noah Stier, Alexander Rich, Pradeep Sen, and Tobias Hollerer. Vortx: Volumetric 3d reconstruction with transformers for voxelwise view selection and fusion. In 2021 International Conference on 3D Vision (3DV), pp. 320330. IEEE, 2021. Mohammed Suhail, Carlos Esteves, Leonid Sigal, and Ameesh Makadia. Generalizable patch-based neural rendering. In European Conference on Computer Vision, pp. 156174. Springer, 2022. Jiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3d reconstruction from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1559815607, 2021. Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. arXiv preprint arXiv:2312.13150, 2023. Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, et al. Nerfstudio: modular framework for neural radiance field development. In ACM SIGGRAPH 2023 Conference Proceedings, pp. 112, 2023. Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: arXiv preprint Large multi-view gaussian model for high-resolution 3d content creation. arXiv:2402.05054, 2024. Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 58(1):267288, 1996. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mambabased language models. arXiv preprint arXiv:2406.07887, 2024. Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, and Kai Zhang. Pf-lrm: Pose-free large reconstruction model for joint pose and shape prediction. arXiv preprint arXiv:2311.12024, 2023. Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 46904699, 2021. Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. Meshlrm: Large reconstruction model for high-quality mesh. arXiv preprint arXiv:2404.12385, 2024. Desai Xie, Sai Bi, Zhixin Shu, Kai Zhang, Zexiang Xu, Yi Zhou, Soren Pirk, Arie Kaufman, Xin Sun, and Hao Tan. Lrm-zero: Training large reconstruction models with synthesized data. arXiv preprint arXiv:2406.09371, 2024. Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 54385448, 2022. Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, et al. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. arXiv preprint arXiv:2311.09217, 2023. Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1037110381, 2024. Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In Proceedings of the European conference on computer vision (ECCV), pp. 767783, 2018. Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 45784587, 2021. Jure Zbontar and Yann LeCun. Stereo matching by training convolutional neural network to compare image patches. Journal of Machine Learning Research, 17, 2016. Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. arXiv preprint arXiv:2404.19702, 2024. Xiaoshuai Zhang, Sai Bi, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. Nerfusion: Fusing radiance In Proceedings of the IEEE/CVF Conference on fields for large-scale scene reconstruction. Computer Vision and Pattern Recognition, pp. 54495458, 2022. Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024."
        },
        {
            "title": "A MORE RESULTS",
            "content": "We show more results of our Long-LRM on large-scale scenes using 32 wide-coverage input views at 960 540 image resolution in Fig. 4. For more visual results with rendered long-trajectory videos, please refer to our project webpage (https://arthurhero.github.io/projects/ llrm/)."
        },
        {
            "title": "B OTHER IMPLEMENTATION DETAILS",
            "content": "Following GS-LRM (Zhang et al., 2024), we normalize the camera poses by setting the world coordinate frame to the mean camera pose of the input views and scaling all input camera locations to fit within the [1, 1]3 bounding box. When rendering, we set the near plane of the splatting frustum at 0.01. We found that normalizing the camera poses causes Gaussians near the camera to be cut off if the default value of 0.2 from 3D GS (Kerbl et al., 2023) is used. However, omitting depth supervision makes the training unstable, so we retain the near plane at 0.2 for the ablation studies without the depth supervision in Tab. 3."
        },
        {
            "title": "C EXPERIMENTAL DETAILS FOR MODEL ARCHITECTURE ABLATION STUDIES",
            "content": "In Table 2, we present the model architecture ablation studies with different length of input sizes. We train all variants on DL3DV-10K and evaluate on DL3DV-140. The number of training steps are empirically decided based on the model convergence, and set to be the same. We study the model behavior under four different settings: 4 input views at 256256, 32 input views at 256256, and 32 input views at 512512, and our extreme setting: 32 input views at 960540. For these ablation studies, we use shorter frame range during evaluation for fair comparisons among each experiments. In details, we choose the first 96 frames from the original video frame sequence, then uniformly sample 8 test views. The training 4 to 32 training views are then uniformly sampled from the rest views, i.e., not overlapping to the testing views. We kept the same set of training and testing views for different experimental setups. The input images are resized and center-cropped to squares except for the last row."
        },
        {
            "title": "D LIMITATIONS",
            "content": "We now briefly discuss the limitations. While we successfully scaled the model to support 32 high-resolution views and achieved wide-coverage large-scale GS reconstruction, we observed only marginal performance improvements when further increasing the number of input views. Specifically, increasing the input to 64 views only lead to less than 1 dB PSNR improvement. Notably, 64 high-res images correspond to extremely long sequences, exceeding 500k in context length, which presents significant challenge for current sequence processing models. Addressing this limitation will require future work to better manage ultra-long sequences. Additionally, since the entire DL3DV training set contains images with fixed wide field of view (FOV), we found that our model struggles to generalize on test sets with significant FOV variations (e.g., the MipNeRF360 dataset with much smaller FOV). We suspect this limitation is due to the use of Mamba2 blocks, as differing FOVs can alter the meaning of tokens at different positions. Developing models that can generalize effectively across varying FOVs may require more diverse datasets with range of various FOVs, at scale similar to DL3DV. 15 Figure 4: Demonstration of our Long-LRMs novel view synthesis capabilities. The left column illustrates the wide-coverage Gaussian reconstruction achieved by our model, while the right columns show high-quality synthesized novel views from different perspectives. These examples demonstrate Long-LRMs ability to handle diverse and complex scenes, accurately reconstructing fine-level details, and generating photorealistic views from multiple angles, effectively capturing both geometric and appearance variations across different scenes."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Oregon State University"
    ]
}