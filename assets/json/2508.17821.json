{
    "paper_title": "Limitations of Normalization in Attention Mechanism",
    "authors": [
        "Timur Mudarisov",
        "Mikhail Burtsev",
        "Tatiana Petrova",
        "Radu State"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper investigates the limitations of the normalization in attention mechanisms. We begin with a theoretical framework that enables the identification of the model's selective ability and the geometric separation involved in token selection. Our analysis includes explicit bounds on distances and separation criteria for token vectors under softmax scaling. Through experiments with pre-trained GPT-2 model, we empirically validate our theoretical results and analyze key behaviors of the attention mechanism. Notably, we demonstrate that as the number of selected tokens increases, the model's ability to distinguish informative tokens declines, often converging toward a uniform selection pattern. We also show that gradient sensitivity under softmax normalization presents challenges during training, especially at low temperature settings. These findings advance current understanding of softmax-based attention mechanism and motivate the need for more robust normalization and selection strategies in future attention architectures."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 1 2 8 7 1 . 8 0 5 2 : r a"
        },
        {
            "title": "Limitations of Normalization in Attention Mechanism",
            "content": "Timur Mudarisov University of Luxembourg Luxembourg timur.mudarisov@uni.lu Tatiana Petrova University of Luxembourg Luxembourg tatiana.petrova@uni.lu Mikhail Burtsev London Institute for Mathematical Sciences mb@lims.ac.uk Radu State University of Luxembourg Luxembourg radu.state@uni.lu"
        },
        {
            "title": "Abstract",
            "content": "This paper investigates the limitations of the normalization in attention mechanisms. We begin with theoretical framework that enables the identification of the models selective ability and the geometric separation involved in token selection. Our analysis includes explicit bounds on distances and separation criteria for token vectors under softmax scaling. Through experiments with pre-trained GPT-2 model, we empirically validate our theoretical results and analyze key behaviors of the attention mechanism. Notably, we demonstrate that as the number of selected tokens increases, the models ability to distinguish informative tokens declines, often converging toward uniform selection pattern. We also show that gradient sensitivity under softmax normalization presents challenges during training, especially at low temperature settings. These findings advance current understanding of softmax-based attention mechanism and motivate the need for more robust normalization and selection strategies in future attention architectures."
        },
        {
            "title": "Introduction",
            "content": "The attention mechanism [1, 7, 15, 8] has become fundamental component of modern deep learning. Since its popularisation in the Transformer [16], attention has powered state-of-the-art systems in machine translation, text generation [3], and multimodal reasoning [10]. Yet the same softmax rule that enables differentiable focus also introduces chronic failure mode: as the context length grows, attention weights collapse toward 1/L, phenomenon we call vanishing attention. The resulting gradients are too small for effective learning, especially in long-context settings [4, 19]. Architectural work-arounds sparse windows, locality-sensitive hashing, or compressed memoriesreduce compute but do not eliminate the collapse, and may even worsen it [5, 14, 2, 9]. principled understanding of why softmax fails in this regime is still missing. Our view: attention as capacity-limited retriever (selector). We revisit normalisation in attention from first principles and show that softmax, and indeed any length-independent normaliser, possesses an intrinsic capacity limit. Our contributions are: 1. Distance bound. We derive non-asymptotic upper bound on the representation distance between selected and non-selected tokens (Theorem 1). The bound proves that once the top-N set grows proportionally to L, the distance must collapse, formalising the softmax bottleneck [18]. Preprint. Under review. 2. Geometric bound. Under mild spherical assumptions we show that no more than 80% of the top-N tokens can be simultaneously distinguished in Euclidean space (Theorem 2). This quantifies hard limit on what single head can represent. 3. Gradient bound. We bound the Jacobian norm of general normaliser (Lemma 2); specialised to softmax it recovers the classic 1/(4T ) instability and shows why aggressive temperature scaling trades separability for optimisation difficulty. 4. Empirical validation. Experiments on GPT-2 [13] confirm all three predictions: distance collapse, separability saturation, and 1/T gradient growth. Our analysis frames attention as selector with finite resolution: it works well while the active set is small fraction of the context, then degrades predictably. This viewpoint explains the empirical success of recent alternativesSparsemax, Scalable-Softmax, Self-Adjusted Softmaxand suggests concrete design rules we distil in the discussion. More broadly, our theoretical tools provide diagnostics for deciding when head has reached its intrinsic limit and when architectural or normalisation changes are warranted. By bridging closed-form theory with large-scale experiments, we provide both deeper understanding of normalisation in attention and practical guidelines for building robust, long-context Transformer models."
        },
        {
            "title": "2 Theoretical Analysis",
            "content": "Consider sequence of token embeddings = {xi}L i=1, where each embedding xi Rd is d-dimensional vector. We begin by reviewing the classical self-attention mechanism introduced by Vaswani et al. [16]: qm = fq(xm, m), kn = fk(xn, n), vn = fv(xn, n), (1) where qm, kn, and vn denote the query, key, and value vectors, respectively. The attention weights are computed using these query and key vectors as follows: am,n = exp (cid:0)q mkn/T (cid:1) / (cid:88) j=1 exp (cid:0)q mkj/T (cid:1) , (2) where the parameter is known as the temperature, typically set to = (as recommended in [16]). To extend the scope of our analysis beyond the standard softmax normalization, we introduce more general normalization framework: am,n = (q mkn, θ)/ (cid:88) j=1 (q mkj, θ), (3) where : R1+c is smooth positive function parameterized by θ, which can include parameters such as temperature or the number of tokens. For convenience, we denote the inner product mkn as lm,n, referring to it as the logit associated with the token pair (m, n). In this work, we provide detailed theoretical examination of the general normalization framework introduced in Equation (3). We focus on several critical aspects: (1) general limitations associated with softmax-type normalization; (2) the influence of normalization on token separation; (3) geometric insights into token separation; (4) connections between normalization and training dynamics. We begin by addressing the general limitations of softmax normalization. As highlighted by [17], one critical limitation is the phenomenon of vanishing attention weights. Intuitively, when the number of tokens increases, the normalization procedure (e.g., softmax) distributes attention weight across many tokens, causing the weights for individual tokens to become extremely small. This issue hampers the models ability to clearly differentiate between relevant and irrelevant tokens. Formally, consider set of logits {l1, . . . , lL} and their corresponding attention weights {α1, . . . , αL}, computed from the logits using normalization function. We present the following general results regarding these normalized attention weights (see Appendix ?? for full proofs): 2 Lemma 1. Consider the normalization scheme defined by Equation (3) with smooth function (li, θ) that does not explicitly depend on the number of tokens (i.e., θ). Assume also that the logits are bounded, li [a, a]. Then, the normalized attention weights satisfy: C1 αi C2 , (4) where the constants C1 and C2 do not depend on L. This lemma implies that for any normalization of function independent of token count, the attention weights inevitably become uniformly small (on the order of 1/L) as the context size grows. As result, the mechanism loses the ability to effectively highlight specific important tokens when processing long sequences. The proof relies on the boundedness and continuity of the function (li, θ) on the compact interval [a, a]. Due to these properties, both the numerator and denominator are bounded by constants independent of L. Hence, when normalized by the summation over all tokens, each weight naturally scales as 1/L. Full mathematical details are provided in Appendix ??. Corollary 1. In the special case of softmax normalization (Equation (2)), with temperature parameter , the attention weights satisfy: (cid:18) exp (cid:19) 2 T"
        },
        {
            "title": "1\nL",
            "content": "αi"
        },
        {
            "title": "1\nL",
            "content": "exp (cid:18) 2 (cid:19) , (5) where = q2k2. This corollary highlights how the magnitude of the query and key vectors directly impacts the distribution of attention weights. Specifically, the bounds show that unless the vectors have sufficiently large magnitudes (relative to temperature ), the attention weights remain close to uniform. Thus, increasing vector magnitudes can partially mitigatebut not eliminatethe vanishing attention problem. These results demonstrate fundamental limitation of softmax-type normalization: as the sequence length grows, each attention weight αi shrinks toward O(1/L). In turn, the model struggles to assign appreciably larger weights to genuinely informative tokens, blurring the distinction between relevant and irrelevant embeddings. This loss of focus impairs the models ability to exploit salient information within long input sequences and ultimately hampers effective learning. 2.1 Distance analysis We next study how normalization influences the total deviationthe discrepancy between learned representation and the underlying conditional distribution. Yang et al. [18] describe the softmax bottleneck: the low rank of the logit matrix limits the models ability to represent the true conditional distribution, thereby increasing total deviation. This deviation is not consequence of insufficient model capacity or sub-optimal optimization, but rather of restrictions imposed by the final softmax projection layer. To quantify the role of normalization in separating informative from non-informative tokens, let and = {x1, . . . , xL}, xi Rd denote the attention weights and their corre- {α1, . . . , αL} sponding token embeddings, respectively. We focus on the top-N tokens with the largest weights; their indices are collected in IN = {i1, . . . , iN }. The context vector derived from these tokens is (cid:88) = αi xi. (6) iIN Our quantity of interest is the cumulative distance between this context vector and all non-selected embeddings: = d(cid:0)X XIN , s(cid:1) = (cid:88) (cid:13) (cid:13)αi xi s(cid:13) (cid:13)2, iIIN (7) where = {1, . . . , L}, 2 is the Euclidean norm, and XIN = {xi1, . . . , xiN } is the set of selected embeddings. Intuitively, smaller value of indicates that many low-weight tokens lie close to the high-weight aggregate s, signalling reduced separability and greater likelihood of attention dilution. 3 Next, we distinguish two settings for the weight vector {αi}. In the first, the weights are treated as fixed; in the second, we analyse random-selection scenario in which the index set IN is drawn uniformly at random from all subsets of size : IN (I, ), = {1, . . . , L}. In this random case, the attention weights on the non-selected tokens vary with the draw of IN . We seek the expected cumulative distance = EIN (cid:2) d(cid:3), (8) where the expectation is taken over the uniform choice of IN . We now adopt classical metric-learning perspective: the above quantities quantify the Euclidean separation margin between selected and non-selected token embeddings. The next theorem formalises these observations by providing explicit upper bounds on the distance in both the fixed and random top-N settings. Theorem 1. For the representation distance defined in (7) the following bounds hold. 1. Fixed top-N set. If IN is fixed, (1 αN ) d1 + max jIN xj2 (cid:2)αN (L ) (1 αN )(cid:3), where d1 = max /IN , jIN xi xj2 and αN = (cid:88) iIN αi. 2. Uniformly random top-N set. When IN is sampled uniformly from all size , = L (cid:88) i=1 (cid:13) (cid:0)αi + (cid:13) (cid:13) L1 (cid:1)xi (cid:13) (cid:13) (cid:13)2 + ε, (cid:19) (cid:18) subsets of where = (cid:88) i=1 αixi and ε (cid:16) 1 2 1 (cid:17) (cid:88) i=1 1 (cid:80) xj2 j=i α2 2 (cid:80) j=i αjxj (cid:13) (cid:13)αixi . (cid:13) (cid:13)2 Intuitively, for fixed top-N set, the first term scales the largest out-of-set distance d1 by the total weight (1 αN ) carried by the remaining tokens, while the second term accounts for how much those low-weight tokens can still perturb the context vector, proportional to the norm of the largest selected embedding. When the top-N indices are chosen uniformly at random, each token is excluded with probability (L )/L. Replacing the random indicator variables by their expectations yields the main sum; the residual ε captures the Jensen gap and is small whenever the individual weights αi are not too concentrated. Full derivations are provided in Appendix ??. Theorem 1 relates the distance to key parameters (N , L, and the weights αi). To see how the bound behaves in two extreme regimes, we state the following corollary. Corollary 2. (i) Fixed L. When is held constant and grows, (cid:88) i=1 αixi2. (ii) Fixed L, L. If is fixed and the top set expands to the full sequence, i.e. L, 0. (9) When is small relative to the sequence length, most tokens are excluded and the expected distance is dominated by the individual contributions αixi2 of those low-weight tokens. In contrast, as approaches L, the context vector eventually incorporates all embeddings, so the distance between and the (now empty) set of non-selected tokens vanishes. 2.2 Geometrical interpretation We now recast the analysis in geometric terms, focusing on the spatial arrangement of the token embeddings xi. When these vectors lie close together, the model struggles to separate informative from non-informative tokens, hindering training. To quantify how many tokens can be reliably distinguished, we work under two standard geometric assumptions: 4 Assumption 1 (Uniform spherical distribution). Each embedding lies uniformly on ddimensional sphere of radius : xi U(cid:0)Sd1(M )(cid:1), = 1, . . . , L, where Sd1(M ) denotes the (d 1)-sphere of radius . In practice, we normalize embeddings so that they lie on this sphere. Assumption 2 (Minimum pairwise separation). There exists fixed lower bound on the distance between any two embeddings: min i=j xi xj2 = δ > 0, ensuring that no two tokens collapse onto one another. In experiments, we set δ to the empirical minimum pairwise distance. Under these assumptions, we can bound the number of embeddings that fall within specified neighbourhood of the context vector, thereby quantifying the models effective resolution. Let IN = {i1, . . . , iN } denote the indices of the selected tokens and recall the context vector = (cid:88) αixi. iIN Fix tolerance radius > 0 and consider the closed Euclidean ball Br(s) centred at s. We say that selected embedding xi is geometrically distinguishable if its rescaled version αixi remains within Br(s) while every non-selected embedding satisfies αjxj / Br(s) for / IN . The count of such distinguishable embeddings is Ns = #(cid:8) IN : αixi s2 r(cid:9), (10) and the ratio Ns/N measures the fraction of non-noise vectors the model can reliably separate (see Fig. 1 for an illustration). Our objective is to bound the expectation E[Ns] and hence provide explicit upper and lower limits on the models effective resolution as function of r, , and under Assumptions 12. Figure 1: Illustrative example of the geometric separation. Left: Token embeddings lie on circle. Middle:Okay. After scaling by their attention weights αi, both attended (magenta stars) and nonattended (black dots) points move toward the origin. Right: Only the selected tokens that remain inside the ball Br(s) (shaded) are deemed distinguishable. In the previous section, we examined separation in terms of Euclidean distances. The geometric approach introduced here, by contrast, focuses on directional separability and reframes the problem as one of metric learning. Now, we present the main result (for the proof, see Appendix ??): Theorem 2. Under Assumptions 12, the fraction of geometrically distinguishable embeddings satisfies 1 1 rN (cid:88) iIN ξi E[Ns] 1 (cid:88) (cid:104) exp (rξi)2 16M (cid:105) , iIN where = 2 (cid:88) ξ2 α + (cid:16) 2 δ2 2 (cid:17) (cid:88) αjαk. jIN j=i j,kIN j=k=i 5 (11) (12) The quantity ξi measures how widely the other 1 selected embeddings, rescaled by their weights, are spread around xi. If ξi is small, most selected points cluster near the context vector s, so many of them fall inside the ball Br(s) and become distinguishable. The lower bound in (11) subtracts from ξi; the tighter the cluster (smaller ξi), the one penalty proportional to the cumulative spread (cid:88) closer the ratio E[Ns]/N is to 1. Conversely, the upper bound shows that once is smaller than ξi for many i, the exponential term decays rapidly, implying very few embeddings remain separable under the chosen radius. 2.3 Gradient Sensitivity of Attention The results above show that language model must sharply distinguish informative from noninformative tokens; in other words, the attention weight distribution should be as selective as possible. Refining that distribution, however, exposes second difficulty: gradient sensitivity during training."
        },
        {
            "title": "Consider two nearly identical logit vectors",
            "content": "ℓ(1) = (0, . . . , 0, a, + ε), ℓ(2) = (0, . . . , 0, + 2ε, a), so that their Euclidean distance satisfies ℓ(1) ℓ(2)2 = 5 ε. Let α(1), α(2) be the corresponding softmax weights (Equation (2)) and denote by ℓα the Jacobian of the softmax map. first-order expansion gives (cid:13)α(1) α(2)(cid:13) (cid:13) (cid:13)2 (cid:13) (cid:13)ℓα(1) (ℓ(1) ℓ(2))(cid:13) (cid:13)2 2 ε , because the largest two logits swap order and the associated softmax gradient scales as 1/T . Hence, even though the logits differ by only O(ε), the output distribution can change by O(ε/T ). For sufficiently small temperature (a common tactic for sharpening attention) this factor becomes large, making the gradient step volatile and potentially destabilising optimisation. This example highlights fundamental trade-off: stronger normalisation (smaller , or any variant that steepens the softmax) improves token separability but simultaneously amplifies gradient variance, complicating training of deep transformers. Let have closer look at how normalisation affects gradient descent. For the general scheme in (3), let {li}L i=1 the resulting attention weights. We characterise the magnitude of the Jacobian lα. i=1 be the logits and {αi}L Lemma 2. With the notation above, lα2 min (cid:26) 2 (cid:18) 1 minj (lj, θ) + 2 L2 minj 2(lj, θ) (cid:19) (cid:27) 2 , (13) where 2 = max F (li, θ) and 2 = max (li, θ). The upper bound grows when any logit lj pushes (lj, θ) toward zero, because both terms inside the (lj, θ). Thus, sharply peaked normalisationsoftmax with parentheses scale inversely with min a small temperature, for instance, induces large Jacobian norms, signalling high gradient sensitivity during training. Corollary 3. For the softmax normalisation in (2), (cid:13)lα(cid:13) (cid:13) (cid:13)2 min (cid:26) 1 4T , (cid:27) 2 . smaller temperature sharpens the softmax but simultaneously inflates the Jacobian norm (scaling as 1/T ), making the attention distribution highly sensitive to even tiny logit perturbations and thus harder to train stably. 6 (a) (b) (c) Figure 2: Distance statistics validate Theorem 1. (a) With = 5, both the true distance (green) and its expectation (blue) grow roughly linearly in L; the red upper bound is safe but conservative. (b) With = 1024, increasing beyond 20 yields diminishing returns: the distance plateaus while the bound tightens. (c) Critical top-N obtained by KS test (α = 0.01); fewer than 6 % of the tokens need to be selected before the empirical and expected distances become statistically indistinguishable."
        },
        {
            "title": "3 Experiments",
            "content": "We evaluate our theoretical findings on the publicly available GPT-2 model family1 [13]. All text is tokenised with byte-pair encoding (BPE) [6] as implemented in the Hugging Face transformers library. Unless otherwise stated, the input consists of consecutive chapters from War and Peace by Leo Tolstoy (public domain), providing long-form prose well beyond the models context window. For every layer and attention head, we extract the full attention matrix RLL and the associated query, key, and value tensors, enabling direct comparison with our distance and geometry metrics. Implementation details, hyperparameters, and reproducibility scripts are included in Appendix ??. 3.1 Distance analysis We now test the non-asymptotic bounds of Theorem 1. Implementation details appear in Appendix ??. Two complementary experiments are performed: 1. Scaling with sequence length. Fix = 5 and vary {32, . . . , 1024}. 2. Scaling with top-N . Fix = 1024 and vary {1, 5, 10, 20, 100}. 1We report results for the 124 parameter version; qualitatively identical trends were observed for larger variants. 7 For each configuration we compute, across all 144 GPT-2 heads/layers, (i) the true distance (7), (ii) the expectation term of Theorem 1, and (iii) the analytic upper bound. In addition, we estimate critical top-N value: the smallest for which the empirical and expected distance distributions are indistinguishable under two-sample KolmogorovSmirnov test (α = 0.01). Results summarised in Fig. 2 support following key observations. (i) For the distance scales linearly with sequence length, exactly as Corollary 2(i) predicts. (ii) As approaches 100, the true and expected distances converge while the upper bound tightensevidence for the collapse of Eq. (9). (iii) The critical-N curve grows sub-linearly ( 0.06L), confirming that only small subset of tokens can be separated before attention behaves as if weights were uniform. These empirical trends corroborate the theoretical claim that softmax normalisation retains discriminative power only when the active set is small fraction of the context; larger values chiefly add noise. 3.2 Geometric Separability We now quantify how many of the highest-weight tokens remain geometrically distinguishable according to Theorem 2. For each sequence we set = min /IN αixi2, so that every non-selected token lies outside the ball Br(s). Using GPT-2 embeddings normalised as in Assumptions 12, we compute the empirical ratio Ns/N (Definition (10)) across all heads and layers and compare it with the analytic bounds of Theorem 2; results are summarised in Fig. 3. Across the full model, the proportion of distinguishable tokens declines up to 16 and then saturates between 0.7 and 0.85. The exponential upper bound tracks the empirical maxima closely, showing the theorem is tight on the high side, whereas the lower bound is intentionally conservative. Thuseven with idealised spherical embeddingssoftmax attention cannot cleanly separate more than about four-fifths of the tokens it selects; adding further tokens mainly dilutes the representation without improving geometric resolution. 3.3 Gradient sensitivity We next explore how the softmax temperature and logit perturbation scale ε affect the stability of the attention map. For each headlayer pair we evaluate the finite-difference Jacobian norm g(T, ε) = 1 ε (cid:13) (cid:13) (cid:13)α ℓ+εℓ α ℓ (cid:13) (cid:13) (cid:13)2 , ℓ2 = 1, which approximates ℓα2. Full implementation details are provided in Appendix ??. Figure 4 shows the maximum value of g(T, ε) across all 144 heads/layers of GPT-2 for ε {103, 101, 10}. For < 0.1 the empirical curves follow the theoretical 1/T trend of Corollary 3; smaller ε values yield larger gradients because the sharpest logits dominate the variation. Once 1 all curves collapse and drop by two orders of magnitude, indicating much improved robustness to logit perturbations but concomitant loss of selectivity. These findings confirm the trade-off already highlighted by our theory: sharper softmax improves token separability yet 8 Figure 3: Geometric separability saturates at 7085%. For increasing topN , the empirical fraction of distinguishable embeddings Ns/N (green boxes) quickly plateaus; roughly one-fifth of selected tokens remain outside Br(s). Figure 4: Gradient sensitivity decays as 1/T. Maximum finitedifference Jacobian norm g(T, ε) for three perturbation magnitudes (coloured curves, loglog scale). The dashed black curve is the theoretical bound min{1/(4T ), 2} from Corollary 3. inflates gradient variance, whereas higher temperatures stabilise training at the cost of blurrier attention."
        },
        {
            "title": "4 Discussion",
            "content": "Softmax remains the default normalisation in modern Transformers because it is simple, differentiable and lends probabilistic interpretation to the weights [16]. Its limitations, however, are now well established: it produces dense distributions, its peak value drops as the context grows (vanishingattention effect), and its Jacobian explodes when the temperature is driven low, yielding unstable gradients. Three main research lines have emerged to address these issues. 1. Sparsity-inducing rules such as Sparsemax and αEntmax replace the softmax exponential with projections onto simplex, producing exact zeros and lowering entropy [11]. 2. Length-aware rescales such as ScalableSoftmax scale logits by log so that the maximum attention weight stays roughly constant as the sequence grows, alleviating the vanishingattention problem [12]. 3. Gradient-controlled variants (e.g. Self-Adjusted Softmax) re-weight logits according to the layer-wise dynamic range, keeping the Jacobian spectrum in healthy band [20]. Our work complements these efforts by explaining, in closed form, why the above modifications are necessary. Where previous papers introduced new rule and showed empirical gains, we derived nonasymptotic bounds (Theorems 12) and gradient limits (Lemma 2) that apply to any normalisation function (, θ). In particular: The distance bound shows that when grows proportionally to L, the representation distance necessarily collapses to zero, formalising the empirical softmax bottleneck noted by [18]. Our GPT-2 experiment (Fig. 2, middle) confirms the predicted plateau and justifies why sparse or length-aware rules can improve long-context performance. The geometric bound states that even under optimistic spherical assumptions no more than 80% of the top-N tokens can stay inside the selective ball  (Fig. 3)  . This explains why empirical studies report that multiple heads are required to cover distinct parts of the context: single head cannot make every important token simultaneously salient. The reason behind this idea is pretty simple. Assuming the independence of the heads as attention mechanism parts, we can conclude that for given separability level p, heads can cover up to 1 (1 p)H of the top-N tokens. Under our analysis, we see that for = 0.8 it is needed approximatelly = 3 to cover up the 99% of information. The gradient bound recovers the familiar 1/(4T ) law but also shows how any normaliser with vanishing minimum value will inherit the same instability. Figure 4 demonstrates that GPT-2 operates close to the theoretical limit when < 101, validating the usefulness of gradient-controlled variants such as SA-Softmax. The combined theoryexperiment picture suggests three practical guidelines. 1. Keep the active set small. The critical-N curve in Fig. 2 grows roughly like 0.06L; selecting more tokens yields vanishing returns and erodes separability. Top-k or sparse attention should be preferred when k. 2. Monitor attention entropy. rising entropy or drop in the empirical Ns/N ratio is an early sign that head has saturated its geometric capacity; adding additional heads or switching to length-aware normaliser can restore separability. 3. Avoid overly sharp softmax. Lowering below 101 increases Jacobian norms without increasing separability (Figs. 3, 4). Practitioners should instead use normalisers that decouple selectivity from gradient health (e.g. Sparsemax, SS-Max, or SA-Softmax). Our analysis assumes embeddings are priori L2-normalised and roughly isotropic. Real models may violate these assumptions, and future work should extend the geometric bound to non-spherical distributions. Another promising direction is to design length-adaptive, gradient-controlled normaliser that inherits the best properties of Sparsemax (sparsity), SS-Max (length awareness) and SA-Softmax (stable Jacobians) while admitting proofs analogous to Theorems 12."
        },
        {
            "title": "5 Conclusions",
            "content": "This work provides theoretical and empirical analysis of normalisation in attention mechanisms beyond the classical softmax. We derived two non-asymptotic bounds (Theorems 1 and 2) that link token separability to sequence length L, selection size , and the embedding geometry, and we established general Jacobian bound (Lemma 2) that explains the well-known trade-off between sharpness and gradient stability. Experiments on GPT-2 confirmed all three predictions: the representation distance collapses once grows proportionally to L; no more than 80% of the selected tokens can be geometrically distinguished, even under ideal spherical embeddings; the empirical Jacobian norm tracks the theoretical 1/(4T ) law, saturating at low temperature. Taken together, these results recast softmax attention as selective but capacity-limited aggregator: it discriminates well only while the active set is small fraction of the context. The analysis also clarifies why recently proposed normalisersSparsemax, Scalable-Softmax, and Self-Adjusted Softmaxoffer complementary benefits: they relax one or more of the intrinsic limits quantified here. (1) limit the top-k set to sub-linear function of the context length; (2) Practical takeaways. monitor attention entropy or the Ns/N ratio during training; and (3) prefer length-aware or sparsityinducing normalisers over aggressive temperature scaling. Overall, our experiments shows several limitations providing the practical diagnostic idea: 1. When the level of geometrical separability drops to 70 80%, this signifies that the head has saturated its geometric capacity. 2. The temperature limitations shows that making the distribution sharp makes the Jacobian norm exploding. The teoretical and practical analysis shows that its better to avoid using the 0.1. Future directions. Immediate extensions include (i) relaxing the spherical-embedding assumption, (ii) analysing multi-query and multi-head interactions, and (iii) designing single normalisation rule that is simultaneously length-adaptive, sparse, and gradient-stable. We hope the quantitative framework introduced here will serve as benchmark for such developments and, more broadly, for principled improvements to longcontext transformers. Overall, the present study provides theoretical footing for the growing body of work that modifies softmax, and furnishes quantitative tools for diagnosing when given attention head has reached its intrinsic limit."
        },
        {
            "title": "References",
            "content": "[1] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. [2] I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer, 2020. [3] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020. [4] R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers, 2019. [5] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, and R. Salakhutdinov. Transformer-XL: Attentive language models beyond fixed-length context. In A. Korhonen, D. Traum, and L. Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 29782988, Florence, Italy, July 2019. Association for Computational Linguistics. 10 [6] P. Gage. new algorithm for data compression. Users J., 12(2):2338, Feb. 1994. [7] A. Graves, G. Wayne, and I. Danihelka. Neural turing machines, 2014. [8] A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka, A. Grabska-Barwinska, S. G. Colmenarejo, E. Grefenstette, T. Ramalho, J. Agapiou, et al. Hybrid computing using neural network with dynamic external memory. Nature, 538(7626):471476, 2016. [9] N. Kitaev, Łukasz Kaiser, and A. Levskaya. Reformer: The efficient transformer, 2020. [10] J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks, 2019. [11] A. F. T. Martins and R. F. Astudillo. From softmax to sparsemax: sparse model of attention and multi-label classification, 2016. [12] K. M. Nakanishi. Scalable-softmax is superior for attention, 2025. [13] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. guage models are unsupervised multitask learners. https://cdn.openai.com/better-language-models/language_models_are_ unsupervised_multitask_learners.pdf. LanOpenAI Blog, 1(8):9, 2019. [14] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap. Compressive transformers for long-range sequence modelling, 2019. [15] S. Sukhbaatar, J. Weston, R. Fergus, et al. End-to-end memory networks. Advances in neural information processing systems, 28, 2015. [16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [17] P. Veliˇckovic, C. Perivolaropoulos, F. Barbero, and R. Pascanu. softmax is not enough (for sharp out-of-distribution), 2024. [18] Z. Yang, Z. Dai, R. Salakhutdinov, and W. W. Cohen. Breaking the softmax bottleneck: high-rank rnn language model, 2018. [19] M. Zaheer, G. Guruganesh, A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, and A. Ahmed. Big bird: Transformers for longer sequences, 2021. [20] C. Zheng, Y. Gao, G. Chen, H. Shi, J. Xiong, X. Ren, C. Huang, X. Jiang, Z. Li, and Y. Li. Self-adjust softmax, 2025."
        }
    ],
    "affiliations": [
        "London Institute for Mathematical Sciences",
        "State University of Luxembourg",
        "University of Luxembourg"
    ]
}