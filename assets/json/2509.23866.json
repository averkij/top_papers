{
    "paper_title": "Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation",
    "authors": [
        "Pengxiang Li",
        "Zechen Hu",
        "Zirui Shang",
        "Jingrong Wu",
        "Yang Liu",
        "Hui Liu",
        "Zhi Gao",
        "Chenrui Shi",
        "Bofei Zhang",
        "Zihao Zhang",
        "Xiaochuan Shi",
        "Zedong YU",
        "Yuwei Wu",
        "Xinxiao Wu",
        "Yunde Jia",
        "Liuyu Xiang",
        "Zhaofeng He",
        "Qing Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language model (VLM) based GUI agents show promise for automating complex desktop and mobile tasks, but face significant challenges in applying reinforcement learning (RL): (1) slow multi-turn interactions with GUI environments for policy rollout, and (2) insufficient high-quality agent-environment interactions for policy learning. To address these challenges, we propose DART, a Decoupled Agentic RL Training framework for GUI agents, which coordinates heterogeneous modules in a highly decoupled manner. DART separates the training system into four asynchronous modules: environment cluster, rollout service, data manager, and trainer. This design enables non-blocking communication, asynchronous training, rollout-wise trajectory sampling, and per-worker model synchronization, significantly improving the system efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput, and 5.5* environment utilization. To facilitate effective learning from abundant samples, we introduce an adaptive data curation scheme: (1) pre-collecting successful trajectories for challenging tasks to supplement sparse success in online sampling; (2) dynamically adjusting rollout numbers and trajectory lengths based on task difficulty; (3) training selectively on high-entropy steps to prioritize critical decisions; (4) stabilizing learning via truncated importance sampling for policy mismatch between policy rollout and updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task success rate, a 14.61% absolute gain over the base model, and 7.34% higher than open-source SOTA. We will fully open-source our training framework, data, and model checkpoints via computer-use-agents.github.io/dart-gui, which we believe is a timely contribution to the open-source community of agentic RL training."
        },
        {
            "title": "Start",
            "content": "Preprint EFFICIENT MULTI-TURN RL FOR GUI AGENTS VIA DECOUPLED TRAINING AND ADAPTIVE DATA CURATION Pengxiang Li1,2, Zechen Hu3, Zirui Shang1,2, Jingrong Wu3, Yang Liu2, Hui Liu3, Zhi Gao1,2(cid:66),Chenrui Shi1,2, Bofei Zhang2, Zihao Zhang3, Xiaochuan Shi3, Zedong Yu2,4, Yuwei Wu1,5(cid:66), Xinxiao Wu1,5, Yunde Jia5, Liuyu Xiang4, Zhaofeng He4, Qing Li2(cid:66) 1Beijing Institute of Technology 2State Key Laboratory of General Artificial Intelligence, BIGAI 3DataCanvas 4Beijing University of Posts and Telecommunications 5Shenzhen MSU-BIT University https://computer-use-agents.github.io/dart-gui"
        },
        {
            "title": "ABSTRACT",
            "content": "Vision-language model (VLM) based GUI agents show promise for automating complex desktop and mobile tasks, but face significant challenges in applying reinforcement learning (RL): (1) slow multi-turn interactions with GUI environments for policy rollout, and (2) insufficient high-quality agent-environment interactions for policy learning. To address these challenges, we propose DART, Decoupled Agentic RL Training framework for GUI agents, which coordinates heterogeneous modules in highly decoupled manner. DART separates the training system into four asynchronous modules: environment cluster, rollout service, data manager, and trainer. This design enables non-blocking communication, asynchronous training, rollout-wise trajectory sampling, and per-worker model synchronization, significantly improving the system efficiency: 1.6 GPU utilization for rollout, 1.9 training throughput, and 5.5 environment utilization. To facilitate effective learning from abundant samples, we introduce an adaptive data curation scheme: (1) pre-collecting successful trajectories for challenging tasks to supplement sparse success in online sampling; (2) dynamically adjusting rollout numbers and trajectory lengths based on task difficulty; (3) training selectively on high-entropy steps to prioritize critical decisions; (4) stabilizing learning via truncated importance sampling for policy mismatch between policy rollout and updating. On the OSWorld benchmark, DART-GUI-7B achieves 42.13% task success rate, 14.61% absolute gain over the base model, and 7.34% higher than open-source SOTA. We will fully open-source our training framework, data, and model checkpoints, which we believe is timely contribution to the community of agentic RL. 5 2 0 2 8 2 ] . [ 1 6 6 8 3 2 . 9 0 5 2 : r Figure 1: Overview of the Decoupled Agentic RL Training (DART) framework for GUI agents. Equal contribution. (cid:66) Corresponding author. 1 Preprint"
        },
        {
            "title": "INTRODUCTION",
            "content": "The rapid advancement of large language models (LLMs) (OpenAI, 2025; DeepSeek-AI, 2025; Gao et al., 2024a) and vision-language models (VLMs) Bai et al. (2025); Wang et al. (2024); Shen et al. (2025); Gao et al. (2024b); Li et al. (2025) has accelerated the development of autonomous agents capable of understanding and interacting with graphical user interfaces (GUIs) (Liu et al., 2024; Hong et al., 2024). Such GUI agents (OpenAI, 2025; Bai et al., 2025; Guo et al., 2025b; Fu et al., 2025a) hold significant potential for automating complex desktop and mobile tasks by processing screenshots and natural language instructions. While reinforcement learning (RL) has proven effective for enhancing the reasoning and exploration capabilities of LLMs/VLMs in various domains (Wang et al.; Ye et al., 2025), its application to GUI agents remains particularly challenging. GUI tasks typically involve long-horizon, multi-turn interactions that require maintaining context across dozens of states and actions, making RL training processes prohibitively slow and inefficient. Recent attempts to apply RL for GUI agents (Lu et al., 2025; Yang et al., 2025; Xi et al., 2025) have yielded only modest improvements (2%4% on the OSWorld benchmark (Xie et al., 2024)), falling short of closed-source counterparts (Anthropic, 2025b; OpenAI, 2025; Wang et al., 2025a). We identify two primary bottlenecks: First, the tightly-coupled nature of current RL pipelines, where action prediction, environment interaction, data management, and model updates occur sequentially, creates significant idle time, especially given the long episode lengths typical of GUI tasks. Second, the inherent diversity in task difficulty leads to imbalanced learning: agents may overfit to simpler tasks while struggling to explore successful trajectories for more complex ones. Additionally, the sparse reward signals and the presence of critical decision points within long trajectories can introduce noise and instability during training. To overcome these limitations, we introduce DART, Decoupled Agentic RL Training framework that decouples the RL process into four specialized, asynchronous modules: environment cluster, rollout service, data manager, and trainer. This design enables non-blocking communication and parallel execution, allowing continuous policy updates alongside ongoing environment interactions. By deploying distributed rollout workers and rollout-level trajectory sampling, DART increases resource utilization, achieving 1.6 improvement in GPU utilization for rollout, 1.9 increase in training throughput, and 5.5 boost in environment utilization compared to coupled baselines. To further enhance the quality and efficiency of learning, we propose an adaptive data curation strategy that operates at multiple granularities. At the task and trajectory levels, we pre-collect successful trajectories for challenging tasks and dynamically adjust the number of rollouts and maximum trajectory length based on real-time success rates. At the step level, we prioritize training on high-entropy steps, identified as critical decision points, within long trajectories. Finally, at the token level, we incorporate truncated importance sampling term to mitigate distribution shift caused by the inference engine and stabilize policy updates. This curated approach ensures that the agent focuses on the most informative experiences, leading to more robust and efficient learning. We evaluate our method by training DART-GUI-7B, GUI agent model initialized from UI-TARS1.5-7B (Qin et al., 2025), on the OSWorld benchmark. DART-GUI-7B achieves 42.13% task success rate, representing 14.61% absolute gain over the base model and 7.34% improvement over the previous open-source state-of-the-art. As illustrated in Figure 1, our framework enables stable performance improvement throughout training, even when exploring with shorter trajectories, and demonstrates superior resource efficiency. Our main contributions are as follows: (1) We propose DART, novel decoupled RL framework that significantly enhances training efficiency for GUI agents, achieving 1.6 higher GPU utilization for rollout, 5.5 better environment utilization, and 1.9 higher training throughput. (2) We introduce an adaptive data curation scheme that optimizes learning at the task, trajectory, step, and token levels, leading to more effective policy updates. (3) We develop DART-GUI-7B, stateof-the-art open-source GUI agentic model that achieves superior performance on OSWorld. To promote reproducibility and advance research in agentic RL, we will fully open-source our training framework, model checkpoints, and curated datasets."
        },
        {
            "title": "2 RELATED WORK",
            "content": "GUI Agents The development of GUI agents has evolved along three architectural paradigms, each addressing different trade-offs between robustness and generalization. Structured agents (Deng et al., 2 Preprint 2023; Gur et al., 2023; Lai et al., 2024) leverage metadata such as APIs or accessibility trees to provide semantic clarity and resilience to layout changes, though their effectiveness is inherently bounded by metadata quality and availability. Visual agents (Xu et al., 2024a; Lu et al., 2024; Cheng et al., 2024) directly process raw screenshots through multimodal LLMs, enabling broader applicability across diverse interfaces but introducing sensitivity to visual variations. Hybrid approaches (Wu et al., 2024; Gou et al., 2024; He et al., 2024) synthesize both modalities, achieving superior grounding performance through complementary information fusion that mitigates the limitations of each individual approach. The training paradigms for GUI agents have shifted from supervised fine-tuning (SFT)(Lin et al., 2024; Qin et al., 2025; Wang et al., 2025e; Zhang et al., 2025), which suffers from limited generalization in complex scenarios, to reinforcement learning approaches that learn from environmental feedback. Early RL methods like GUI-R1(Luo et al., 2025) and InfiGUI-R1 (Liu et al., 2025) adopted offline training without real-time interaction, struggling with distribution shift and multi-turn reasoning. Recent online RL frameworks address these limitations through various strategies: ARPO (Lu et al., 2025) extends GRPO for multi-turn interactions, ZeroGUI (Yang et al., 2025) automates task and reward generation via VLMs, while ComputerRL (Lai et al., 2025) designs asynchronous architectures for training API-equipped GUI Agents. Our work presents fully open-sourced reinforcement learning framework specifically designed for GUI agents, integrating the decoupled asynchronous framework with adaptive data curation to achieve both superior performance and community accessibility. Agentic RL Reinforcement learning has emerged as powerful paradigm for enhancing the reasoning and decision-making capabilities of large language models, evolving from preferencebased to outcome-based training approaches. While early RLHF methods (Ouyang et al., 2022; Rafailov et al., 2023) relied on expensive human preference annotations that provided only indirect supervision signals, recent advances have shifted toward reinforcement learning with verifiable rewards (RLVR) like GRPO (Guo et al., 2025a) and DAPO (Yu et al., 2025), where automatic and scalable reward signals are derived from concrete task outcomes such as mathematical correctness or code execution success. GSPO (Xu et al., 2024b) further improves training stability and efficiency through sequence-level policy optimization that better handles the credit assignment problem in long sequences. The computational demands of RL have driven the development of asynchronous architectures that decouple different training components for improved efficiency. Building on successful applications in game AI (Vinyals et al., 2019; Berner et al., 2019), recent frameworks have adapted asynchronous training for language models: AREAL (Fu et al., 2025b) separates rollout generation from model training to maximize GPU utilization while employing staleness-aware PPO to maintain training stability; ROLL (Wang et al., 2025c) provides comprehensive RL library supporting multi-model pipelines and flexible resource scheduling for large-scale LLM training. These frameworks focus on general text-based or multimodal tasks with relatively dense rewards and shorter interaction sequences. We present decoupled RL framework specifically designed for GUI agent training, addressing the unique challenges of long-horizon multi-modal interactions."
        },
        {
            "title": "3 DART: DECOUPLED AGENTIC RL TRAINING FRAMEWORK",
            "content": "3.1 FORMULATION the time step t, We formulate GUI tasks as sequential decision-making process. At given the current visual state st (a screenshot of GUI) and the interaction history ht = {(smax(1,tm), tmax(1,tm), amax(1,tm)), . . . , (st1, rt1, at1)} of previous steps, where denotes the thought for reasoning and denotes the action (such as clicking on specific UI element or entering text), along with the task τ , the agent generates new thought rt and an executable action at. Executing the action at leads to new visual state st+1 (an updated screenshot). This interaction loop continues, with the agent repeatedly observing the environment, producing thought and actions, and receiving updated observations until either termination condition is met (e.g., the task is completed or fails) or the maximum number of steps is reached. We parameterize the GUI agent using policy model πθ (i.e., an VLM) that generates thoughts and actions based on the current 3 Preprint Figure 2: Overall architecture of our framework. The Rollout Service interacts with multiple environments in parallel to generate trajectories, which are managed and delivered to the Trainer for policy updates. Updated actors are synchronized back to the Rollout Service, enabling scalable and efficient asynchronous learning. Implementation techniques are annotated within the figure. state and historical context: optimal thought and action selected by the policy model. = arg maxrt,at πθ(atτ, ht, st), where , and represents the 3.2 ARCHITECTURE Our RL framework contains four decoupled modules: Trainer, Data Manager, Env Cluster, and Rollout Service, where none of them will be blocked by other modules, as shown in Figure 2. We set up hundreds of real desktop environments in Env Cluster, and design Rollout Service to load multiple policy models. The Env Cluster receives sequence of tasks from the Data Manager, and samples trajectories for each task. The Rollout Service dynamically assigns idle workers to produce thoughts and actions for different rollouts in parallel. We store sampled trajectories and corresponding rewards in the Data Manager. When trajectories of one task are finished, the Data Manager filters and passes them to the Trainer based on predefined rules for policy updates. Finally, the updated policy model is synchronized back to the Rollout Service, enabling scalable and highly efficient asynchronous learning. Key interactions among these modules are as follows. 3.3 ASYNCHRONOUS TRAINER To improve GPU and environment utilization, we decouple the Trainer from the trajectory rollout process, which avoids the blocking between training and rollout. The Trainer operates asynchronously, receiving filtered trajectories from the Data Manager and performing step-wise GRPO updates. The updated model weights are synchronized to the Rollout Service, enabling continuous training while new trajectories are sampled simultaneously. θold Step-wise GRPO. We adopt step-wise Group Relative Policy Optimization (GRPO) (Shao et al., 2024) to train our GUI agent. For each task τ , we sample trajectories T1, T2, . . . , TN using the current policy πRollout of the Rollout Service, where the i-th trajectory has length Li and consists of state-thought-action pairs: Ti = {(si,j, ri,j, ai,j)}Li j=1. Each trajectory receives reward Ri, and we decompose each trajectory into individual steps and group all steps from the same task for advantage computation. Specifically, we create step group = {(hi,j, si,j, ri,j, ai,j, Ri) [1, ], [1, Li]}, where each step (si,j, ri,j, ai,j) is combined with its history hi,j and the trajectory-level reward Ri. We denote hi,j, si,j, ri,j, ai,j as h, s, r, for simplicity. The step-wise GRPO objective is formulated as (θ) = E(h,s,a,R)D (cid:20) θ min (cid:18) πTrain θ πTrain old (ah, s) (ah, s) A, clip (cid:18) πTrain θ πTrain old (ah, s) (ah, s) , 1 ϵlow, 1 + ϵhigh (cid:19) (cid:19) where the advantage is computed as β DKL (cid:0)πTrain θ (ah, s) πRef θ (ah, s)(cid:1) (cid:21) , (1) Ai,j = Ri σR , = 1 (cid:80) (h,s,a,R)D R, = 1 σ2 (cid:80) (h,s,a,R)D(R R)2. 4 Preprint Figure 3: Visualization of sampling timelines for 4 tasks in two batches (denoted by different colors) with rollout number = 4, batch size = 2, and total of 8 environments. Each bar represents the timeline of rollout execution of task on one environment. 3.4 ROLLOUT-WISE SAMPLING Solving practical GUI tasks (such as OSWorld) typically consists of dozens of steps and lasts for tens of minutes. Thus, sampling efficiency often becomes major bottleneck. When sampling, tasks within the same batch may vary significantly in difficulty, and even for the same task, minor environmental variations across different executions may produce trajectories of vastly different lengths(Wang et al., 2025d). Under conventional batch-wise sampling (Figure 3 (a)), this heterogeneity causes substantial resource underutilization: environments that complete early remain idle while GPUs stay underused until all trajectories in the batch finish. Task-wise sampling (Figure 3 (b)) partially addresses this issue but still requires waiting for entire tasks to complete before resuming new sampling, limiting overall efficiency. We implement rollout-wise sampling ( Figure 3 (c)), where an individual trajectory serves as the minimal scheduling unit. Once an environment completes rollout, it immediately launches the next sampling request without waiting for others. This fine-grained scheduling significantly improves environment utilization and maximizes GPU throughput. Furthermore, rather than statically assigning environments to fixed rollout workers, our decoupled training framework introduces dynamic model service pool for load balancing. All GPUs are pooled into the shared Rollout Service, with incoming requests being distributed to works based on current device utilization. This design provides two engineering advantages: (1) all environments communicate to the Rollout Rervice through unified interface, simplifying system architecture and coordination; and (2) balanced GPU workloads minimize idle time and bottlenecks, resulting in faster inference and higher overall throughput. 3.5 PER-WORKER MODEL SYNCHRONIZATION Model synchronization presents critical bottleneck in asynchronous GUI agent RL. Traditional approaches rely on global synchronization: when the Trainer completes training iteration, all rollout workers halt operations and wait for every GPU device to receive the updated weights before resuming sampling. As shown in Figure 4 (a), this creates system-wide downtime where environments sit idle and GPUs remain underutilized during each update. We introduce per-worker model update to eliminate this bottleneck through staggered parameter distribution. Instead of synchronized model updates across all rollout workers simultaneously, we gradually refresh model weights on rollout workers. It means that when one worker is updating model weights, the others continue serving inference requests with its current model version. Figure 4 (b) illustrates how this maintains continuous service availability: environments never experience complete blocking. This approach delivers two key advantages: dramatically improved sampling throughput through reduced idle time, and seamless model version transitions that preserve ongoing rollout stability."
        },
        {
            "title": "4 MULTI-LEVEL ADAPTIVE DATA CURATION FOR GUI TASKS",
            "content": "4.1 PERFORMANCE-AWARE TASK ROLLOUT Fixed sampling strategies in reinforcement learning waste computational resources by treating all tasks equally. Easy tasks receive excessive sampling while difficult tasks lack sufficient exploration. 5 Preprint Figure 4: Timeline comparison between all-worker and per-worker model updating for 4 GPUs and 80 environments. The timelines depict idle periods for the GPUs and environments across two model updates. Different model versions are represented by varying shades of color. For per-worker model updating, the device number per worker is set to 2. We propose performance-aware task rollout that dynamically adjusts both sampling frequency and trajectory length based on each tasks learning progress. Dynamic Rollout Frequency We continuously monitor each tasks success rate and adjust its sampling frequency accordingly. As shown in Figure 5, when task achieves high success rates (above 0.6), we reduce its rollout frequency from 8 to lower values, preventing overfitting on already-solved tasks. Tasks with low success rates maintain maximum sampling to ensure adequate learning opportunities. This strategy reallocates computational resources from well-learned tasks to challenging ones, improving overall training efficiency. Dynamic Trajectory Length We set task-specific trajectory length limits based on the historical maximum length of successful completions. Figure 5: Dynamic rollout Instead of using fixed limit for all tasks, each task receives its own with task success rate. length threshold derived from its successful trajectories. This prevents wasting computation on hopeless long trajectories while allowing sufficient exploration for tasks that genuinely require more steps. For instance, simple clicking tasks might terminate after 10 steps, while complex multi-application tasks can extend to 50 steps. This adaptive approach optimizes the balance between thorough exploration and computational efficiency for individual tasks. 4.2 EXPERIENCE POOL OF TRAJECTORIES Training on challenging tasks poses significant obstacle in reinforcement learning due to extremely low success rates, which results in insufficient positive trajectories in rollouts. It is common that all trajectories of task fail, providing no effective learning signal for policy improvement. This severe imbalance between simple tasks and difficult tasks results in training instability, preventing the model from learning correct behavioral patterns. To address this limitation, we introduce an Experience Pool that serves as repository of high-quality successful trajectories for challenging tasks, enabling dynamic supplementation for difficult tasks during training to ensure balanced learning signals. Specifically, we pre-populate the Experience Pool by collecting and storing high-quality successful trajectories through preliminary sampling. During training, when the system detects that all trajectories in the current task fail, it automatically triggers the pool sampling mechanism to randomly retrieve successful trajectory and incorporate it into the training batch. This design guarantees that every training task contains at least one positive sample while maintaining reasonable balance between exploration and experience replay, thereby preventing performance degradation on challenging tasks and improving overall training stability. 4.3 HIGH-ENTROPY-DRIVEN STEP OPTIMIZATION Inspired by Wang et al. (2025b), who showed that training exclusively on high-entropy tokens that \"act as critical forks that steer the model towards diverse reasoning pathways\" can drive effective reinforcement learning, we extend this insight to multi-turn GUI agent training. Low entropy means non-critical steps for GUI task, and using such steps may cause instability in training. We design an entropy-based step selection mechanism that identifies and prioritizes the top 80% high-entropy steps for training, thereby encouraging exploration in critical decision-making moments. 6 Preprint (cid:80)rt+at i=1 For the t-th step in trajectory, we calculate the step-level entropy Ht as the average entropy across all tokens generated in the concatenated thought rt and action at sequence: Ht = 1 v=1 pt,i,v log pt,i,v represents the token-level entropy, rt+at with pt,i,v = πθ(vτ, ht, st, o<i) being the probability of generating the token v. During training, we modify the GRPO objective to include only steps whose entropy is at least larger than 20% steps within the group. This makes reinforcement learning focuses on uncertain steps in GUI navigation. Ht,i, where Ht,i = (cid:80)V 4.4 DISTRIBUTION ALIGNMENT FOR OOD TOKENS During training, the discrepancy between quantization strategies employed by the Rollout Service and the Trainer leads to significant differences in their generated policy distributions. In addition, the pre-collected trajectories also has different distributions from the current model. To solve this issue, we follow Yao et al. (2025) and incorporate truncated importance sampling weight min (cid:18) πTrain θold πRollout θold (ah,s) (ah,s) , (cid:19) into the training objective in Eq.(1) to mitigate this gap. By reweighting the gradient contributions based on the probability ratio between the two distributions, we utilize unbiased learning for our decoupled framework, enabling stable training. The final objective is JHE(θ) = E(h,s,a,R)D (cid:34) I[Ht τ 0.2 ] (cid:18) min (ah, s) (ah, s) (cid:19) , θ min (cid:32) πTrain θ πTrain old (cid:18) πTrain old πRollout old (cid:33) (cid:19) (ah, s) (ah, s) (cid:19)(cid:35) A, (2) , 1 ϵlow, 1 + ϵhigh β DKL (cid:0)πTrain θ (ah, s) πRef θ (ah, s)(cid:1) clip (cid:18) πTrain θ πTrain old (ah, s) (ah, s) where I[] is the indicator function, and τ 0. is the threshold larger than 20% entropy in the group."
        },
        {
            "title": "5 EXPERIMENT",
            "content": "5.1 SETTINGS Experimental Setup We evaluate our approach on OSWorld-Verified (Xie et al., 2024), comprehensive benchmark for assessing multimodal autonomous agents in realistic computer environments. For our training corpus, we adopt the sampling methodology proposed by Lu et al. (2025), selecting representative subset of 203 tasks from the OSWorld benchmark. We use the results of the evaluation scripts from the OSWorld as rewards for the RL. Evaluation Protocol We follow the OSWorld evaluation framework (Xie et al., 2024), which uses execution-based validation scripts to assess task completion. Each trajectory receives reward score in [0, 1] based on programmatic verification of the final system state against predefined success criteria. Implementation We adopt UI-TARS-1.5-7B (Qin et al., 2025) as the baseline for our policy model. Unlike approaches that rely on multi-agent systems, agent workflows, or agents equipped with additional APIs/tools, we focus on enhancing the capabilities of single VLM agent through reinforcement learning, aiming to improve the models inherent decision-making abilities without external scaffolding. Based on decoupled agentic RL training (DART) and the data curation scheme, we obtain the DART-GUI-7B model. More details about training and RL framework can be found in Appendix A.4. 5.2 MAIN RESULTS Table 1 presents results on the OSWorld benchmark across 10 diverse applications. Notably, Decoupled Agentic RL Training (DART)-GUI-7B demonstrates superior sample efficiency compared to both open-source and closed-source models. DART-GUI-7B achieves 42.13% overall success rate with only 30 maximum steps, establishing new state-of-the-art among open-source models and showing 12.71% improvement over the baseline UI-TARS-1.5-7B (27.52% with 100 steps). It also achieves comparable performance to Claude-4-Sonnet (41.39% with 100 steps) and outperforming models like OpenAI CUA o3 (23.00% with 100 steps). This significant gain validates the effectiveness 7 Preprint Table 1: Results on the OSWorld benchmark. Max Steps indicates the maximum number of agentenvironment interactions allowed. Bold values denote the best performance among open-source models. For brevity, LibreOffice Calc, Impress, and Writer are abbreviated as calc, impress, and writer, respectively. Our results are obtained through evaluation on self-deployed devices using the official codebase and Docker environment. * means self-reported results in the method. Model Max Steps chrome gimp calc impress writer multi_apps os thunderbird vlc vs_code Overall Task Success Rate (%) OpenAI CUA o3 (OpenAI, 2025) OpenAI CUA o3 (OpenAI, 2025) TianXi-Action-7B (Tang et al., 2025) OpenAI CUA (OpenAI, 2025) OpenAI CUA (OpenAI, 2025) Claude-3-7-Sonnet (Anthropic, 2025a) Claude-3-7-Sonnet (Anthropic, 2025a) DeepMiner-Mano-7B (Fu et al., 2025a) Seed1.5-VL-250717 (Guo et al., 2025b) Claude-4-Sonnet (Anthropic, 2025b) UI-TARS-250705 (Wang et al., 2025a) Claude-4-Sonnet (Anthropic, 2025b) Qwen2.5-VL-32B (Bai et al., 2025) Qwen2.5-VL-72B (Bai et al., 2025) ZeroGUI* (Yang et al., 2025) UI-TARS-72B-dpo (Qin et al., 2025) OpenCUA-7B (Wang et al., 2025e) UI-TARS-72B-dpo (Qin et al., 2025) UI-TARS-1.5-7B (Qin et al., 2025) UI-TARS-1.5-7B (Qin et al., 2025) OpenCUA-7B (Wang et al., 2025e) ARPO* (Lu et al., 2025) GUI-Owl-7B (Ye et al., 2025) OpenCUA-32B (Wang et al., 2025e) OpenCUA-32B (Wang et al., 2025e) UI-TARS-1.5-7B DART-GUI-7B (Ours vs. Baseline) 50 100 50 100 50 100 50 100 100 100 100 50 100 100 15 50 100 100 50 100 50 15 15 50 100 100 30 - 21.74 13.04 36.83 33.61 36.87 54.26 52.09 39.13 56.52 47.74 56.43 54.26 8.70 4.35 - 33.24 36.14 32.61 32.79 38.34 38.61 - 41.22 43.39 39. Closed-source Model 38.46 38.46 55.77 48.08 34.62 42.31 38.46 69.23 50.00 50.00 50.00 50.00 3.85 0.00 - 61.54 47.44 73.08 53.85 51.92 43.59 - 65.38 69.23 66.67 8.51 10.64 6.38 18.09 14.89 21.28 31.91 27.66 34.78 29.79 40.43 31.91 4.26 10.64 38.24 24.47 29.70 31.83 36.09 42.47 48.91 42.47 55.30 46.72 Open-Source Model 0.00 6.38 - 12.77 10.64 6.38 8.51 9.57 13.22 - 17.02 13.48 18.44 0.00 0.00 - 25.45 31.90 23.81 39.31 38.21 32.60 - 19.06 38.28 37.60 Baseline 21.74 30.43 54.35 23.91 26.09 47.83 43.48 56.52 56.52 52.17 60.87 60.87 8.70 8.70 - 43.48 27.54 34.78 41.30 39.13 33.33 - 52.17 40.58 36.23 11.83 16.53 6.60 15.61 15.81 19.62 17.66 17.20 15.35 27.86 14.66 28. 2.15 3.23 - 6.71 9.87 8.29 8.60 8.94 12.11 - 9.68 14.93 16.21 37.50 62.50 38.22 54.17 70.83 45.83 50.00 50.00 39.13 45.83 41.67 45.83 8.33 16.67 - 33.33 42.87 37.50 25.54 31.25 43.47 - 50.00 53.62 55.07 20.00 26.67 43.33 56.67 66.67 66.67 53.33 73.33 73.33 66.67 66.67 73.33 6.67 13.33 - 33.33 40.00 60.00 46.67 40.00 42.22 - 66.67 53.33 46.67 17.59 39.18 31.85 38.21 11.76 24.88 23.53 35.29 35.29 32.82 44.00 41. 0.00 5.88 - 23.53 29.41 17.65 24.41 22.44 28.31 - 29.41 25.49 33.33 21.74 39.13 67.39 60.87 69.57 56.52 56.52 78.26 56.52 69.57 52.17 60.87 8.70 4.35 - 47.83 44.93 52.17 52.17 47.83 47.10 - 65.22 55.07 63.31 17.17 23.00 29.81 30.50 31.30 35.57 35.83 40.16 40.18 41.39 41.84 43.88 3.88 4.99 20.2 25.88 26.60 26.84 27.25 27.52 28.20 29.9 32.11 34.14 34.79 38. 51.92 9.57 38.21 39.13 8.94 31. 40.00 22.44 47.83 27.52 Ours 60.86 19.15 52.09 13.75 25.00 9.58 10.59 21. 48.80 76.92 16.69 7.75 62.50 31.25 60.00 20.00 42.13 69.57 39.30 16.86 21.74 14. of our decoupled asynchronous RL framework and multi-level data curation strategies that focuses on critical decision points. DART-GUI-7B shows consistent improvements across all applications, with particularly strong gains in complex system-level tasks: OS tasks improve by 31.25% (62.50% vs. 31.25%), LibreOffice Writer by 21.73% (60.86% vs. 39.13%), and Thunderbird by 20.00% (60.00% vs. 40.00%). These applications involve longer interaction sequences and diverse action spaces, highlighting our frameworks ability to handle long-horizon tasks with sparse rewards effectively. 5.3 EFFICIENCY ANALYSIS We evaluate our decoupled frameworks efficiency gains across three key metrics, as shown in Table 2. Our framework achieves substantial improvements: training throughput nearly doubles from 22.6 to 43.6 actions/min (1.9), environment utilization increases dramatically from 12.2% to 67.7% (5.5), and GPU utilization improves from 29.6% to 46.7%(1.6). These gains stem from our decoupled designs elimination of system-wide blocking. Environments continuously generate rollouts without waiting for batch completion, enabling immediate trajectory generation upon task completion. Worker-wise model updates avoid global synchronization, allowing the GPU service to continuously perform inference while some are updating models. This asynchronous operation minimizes idle time across all components, demonstrating that decoupling is essential for efficient GUI agent RL at scale. Table 2: Efficiency improvements of our decoupled framework compared to non-decoupled baseline. System Setup Non-Decoupled Decoupled (Ours) Improvement Training Throughput (actions/min) Env Util. (%) GPU Util. (%) 22.6 43.6 1.9 12.2 67.7 5.5 29.6 46.7 1.6 Table 3: Ablation study of the data curation scheme. DR stands for dynamic rollout, DTL for dynamic trajectory length, HE for high-entropy-driven step selection, and DA for distribution alignment. Baseline w/ DR w/ DTL w/ HE w/ DA Ours Pass@1 (%) 28.67 50.90 66.11 68.33 70.55 72. 8 Preprint Figure 6: (a) Dynamic rollout frequency vs. model accuracy across epochs. (b) Dynamic trajectory length vs. model accuracy over training. (c) Impact of experience trajectory pool on accuracy. (d) Performance comparison with and without distribution alignment. 5.4 ABLATION We conduct ablation studies on subset of 45 tasks from the training set to evaluate the four levels of our data curation scheme. Baseline means only using the decoupled RL training framework, and Ours means we apply the whole data curation scheme is added. Dynamic Rollout and Dynamic Trajectory Length. We evaluate the impact of our mechanisms on training efficiency. As shown in Table 3, both approaches effectively improves the baseline performance. As shown in Figure 6(a) and (b), both strategies effectively reduce computational overhead as the model improves. Dynamic Rollout demonstrates that as accuracy increases, the average rollout frequency decreases from 8.0 to 5.0 per task. Similarly, Dynamic Trajectory Length shows that as performance improves from, the average trajectory length drops from 30 to less than 10 steps, as the model learns to complete tasks more efficiently. This confirms that our adaptive mechanisms successfully accelerate training by eliminating redundant computation while maintaining exploration on challenging tasks. Experience Pool of Trajectories. We evaluate the impact of the experience pool on training performance for set of 22 challenging tasks which initially exhibits success rate of 0%. As illustrated in Figure 6(c), initially the model fails to sample any correct trajectories, resulting in 0% success rate at the first step. During training, by dynamically incorporating successful trajectories from the pool when all online rollouts fail, the model progressively improves its performance, reaching 46% by the later steps. This demonstrates that the Experience Pool effectively mitigates the sparse positive signal problem and stabilizes learning on tasks with extremely low natural success rates. High-entropy-driven Step Selection. We evaluate the impact of our high-entropy-driven step selection on agent performance. As shown in Table 3, compared to the baseline without high-entropy step prioritization, this approach improves success rates from 28.67% to 68.33% compared to the baseline, demonstrating its effectiveness. Distribution Alignment. We examine the distribution alignment in stabilizing multi-turn VLM RL. As shown in Figure 6(d), incorporating rollout log probabilities as importance weights provides three key benefits: (1) maintains training stability with consistent 70% accuracy throughout training, (2) achieves higher peak performance (78% vs. 55%), and (3) prevents catastrophic collapse that occurs in the baseline, which drops from 55% to near 0% after step 60, common issue in agent RL. We also observe that our method improves performance over the baseline, reaching 70.55% in Table 3. By reweighting gradients according to the probability ratio between rollout and current distributions, our method effectively mitigates distribution shift, enabling stable learning in long-horizon GUI tasks."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we have introduced an efficient reinforcement learning (RL) method to address key challenges in training GUI agents powered by vision-language models (VLMs). Our approach overcomes two major limitations in current RL frameworks for GUI tasks. The proposed decoupled RL framework can speed up the training process, and the data curation scheme can improve the quality of training data for multi-turn agent-environment interactions. In this case, we significantly improved both GPU and environment utilization and further ensure better agent performance. Our experimental results on OSWorld demonstrate the efficacy of our approach, achieving 42.13% task success rate, outperforming existing open-source models and the baseline by substantial margins. The ablation studies highlight the critical role of the data curation scheme in optimizing the RL process. This work presents promising direction for improving the efficiency and success of RL-based GUI agents and provides valuable insights for future developments in this field. 9 Preprint"
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We have made several efforts to ensure that the results reported in this paper are reproducible. For the proposed decoupled RL framework, we provide detailed descriptions of the overall architecture and algorithmic components in Section 3, and of the adaptive multi-level data curation and exploration strategies in Section 4. Our experimental setup and evaluation protocols are described in Section 5.1, including dataset configurations, environment settings, and performance metrics. The implementation details of the training framework, such as the Trainer, Rollout Service, Environment Cluster, and Data Manager, are provided in Section 5.1, with additional specifics in Appendices A.1 A.2. All ablation studies and efficiency analyses are reported in Sections 5.2 5.4, with implementation details and additional examples provided in Appendices A.3 A.6. These include descriptions of action spaces, system prompts, and extended experimental results, which together provide sufficient information for reproducing the reported performance. Additionally, source code, configuration files, and pretrained models will be made available to facilitate full reproducibility."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Claude 3.7 Sonnet and Claude Code. Technical report, Anthropic, 2025a. URL https: //www.anthropic.com/news/claude-3-7-sonnet. System Card. 12, 13, 14. Anthropic. Claude-4 Sonnet. Technical report, Anthropic, 2025b. URL https://www. anthropic.com/news/claude-4. System Card. 14. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. Brendan Burns, Brian Grant, David Oppenheimer, Eric Brewer, and John Wilkes. Borg, omega, and kubernetes. In ACM Queue, volume 14, pp. 7093. ACM, 2016. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. Tianyu Fu, Anyang Su, Chenxu Zhao, Hanning Wang, Minghui Wu, Zhe Yu, Fei Hu, Mingjia Shi, Wei Dong, Jiayao Wang, Yuyang Chen, Ruiyang Yu, Siran Peng, Menglin Li, Nan Huang, Haitian Wei, Jiawei Yu, Yi Xin, Xilin Zhao, Kai Gu, Ping Jiang, Sifan Zhou, and Shuo Wang. Mano report. arXiv preprint arXiv:2509.17336, 2025a. Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, et al. Areal: large-scale asynchronous reinforcement learning system for language reasoning. arXiv preprint arXiv:2505.24298, 2025b. Zhi Gao, Yuntao Du, Xintong Zhang, Xiaojian Ma, Wenjuan Han, Song-Chun Zhu, and Qing Li. Clova: closed-loop visual assistant with tool usage and update. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1325813268, 2024a. 10 Preprint Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaojian Ma, Tao Yuan, Yue Fan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, and Qing Li. Multi-modal agent tuning: Building vlm-driven agent for efficient tool usage. arXiv preprint arXiv:2412.15606, 2024b. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025b. Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1428114290, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, et al. Autowebglm: large language model-based web navigating agent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 52955306, 2024. Hanyu Lai, Xiao Liu, Yanxiao Zhao, Han Xu, Hanchen Zhang, Bohao Jing, Yanyu Ren, Shuntian Yao, Yuxiao Dong, and Jie Tang. Computerrl: Scaling end-to-end online reinforcement learning for computer use agents. arXiv preprint arXiv:2508.14040, 2025. Pengxiang Li, Zhi Gao, Bofei Zhang, Yapeng Mi, Xiaojian Ma, Chenrui Shi, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, et al. Iterative tool usage exploration for multimodal agents via step-wise preference tuning. arXiv preprint arXiv:2504.21561, 2025. Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for generalist gui agent. In NeurIPS 2024 Workshop on Open-World Agents, volume 1, 2024. Xiao Liu, Bo Qin, Dongzhu Liang, Guang Dong, Hanyu Lai, Hanchen Zhang, Hanlin Zhao, Iat Long Iong, Jiadai Sun, Jiaqi Wang, et al. Autoglm: Autonomous foundation agents for guis. arXiv preprint arXiv:2411.00820, 2024. Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239, 2025. Fanbin Lu, Zhisheng Zhong, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Arpo: End-to-end policy optimization for gui agents with experience replay. arXiv preprint arXiv:2505.16282, 2025. Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent. arXiv preprint arXiv:2408.00203, 2024. Preprint Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. OpenAI. Computer-using agent (cua): Model for gui interaction and task automation. Research preview / API documentation, March 2025. URL https://openai.com/index/ computer-using-agent/. Powering Operator; computer-use-preview model; accessed via Responses API; performance: OSWorld 38.1% for computer tasks, WebArena 58.1%, WebVoyager 87% :contentReference[oaicite:0]index=0. OpenAI. OpenAI O3 and O4-Mini System Card. Technical report, OpenAI, 2025. URL https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/ o3-and-o4-mini-system-card.pdf. System Card. 14. Oracle Corporation. MySQL Database Management System, 2024. URL https://www.mysql. com/. Accessed: 2024-09-24. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Liang Tang, Shuxian Li, Yuhao Cheng, Yukang Huo, Zhepeng Wang, Yiqiang Yan, Kaer Huang, Yanzhe Jing, and Tiaonan Duan. Sea: Self-evolution agent with step-wise reward for computer use. arXiv preprint arXiv:2508.04037, 2025. Oriol Vinyals, Igor Babuschkin, Wojciech Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. nature, 575(7782):350354, 2019. Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, et al. Ui-tars-2 technical report: Advancing gui agent with multi-turn reinforcement learning. arXiv preprint arXiv:2509.02544, 2025a. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025b. Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye HAO, Jun Wang, and Kun Shao. Distrl: An asynIn The chronous distributed reinforcement learning framework for on-device control agent. Thirteenth International Conference on Learning Representations. Weixun Wang, Shaopan Xiong, Gengru Chen, Wei Gao, Sheng Guo, Yancheng He, Ju Huang, Jiaheng Liu, Zhendong Li, Xiaoyang Li, et al. Reinforcement learning optimization for large-scale learning: An efficient and user-friendly scaling library. arXiv preprint arXiv:2506.06122, 2025c. 12 Preprint Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Haotian Yao, Ziwei Chen, Qizheng Gu, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, and Tao Yu. Opencua: Open foundations for computer-use agents, 2025d. URL https://arxiv.org/abs/2508.09123. Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, et al. Opencua: Open foundations for computer-use agents. arXiv preprint arXiv:2508.09123, 2025e. Yufei Wang, Zhanyi Sun, Jesse Zhang, Zhou Xian, Erdem Biyik, David Held, and Zackory Erickson. Rl-vlm-f: reinforcement learning from vision language foundation model feedback. In Proceedings of the 41st International Conference on Machine Learning, pp. 5148451501, 2024. Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, et al. Agentgym-rl: Training llm agents for long-horizon decision making through multi-turn reinforcement learning. arXiv preprint arXiv:2509.08755, 2025. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024a. Zheng Xu, Xu Dai, Shaojun Wei, Shouyi Yin, and Yang Hu. Gspo: graph substitution and parallelization joint optimization framework for dnn inference. In Proceedings of the 61st ACM/IEEE Design Automation Conference, pp. 16, 2024b. Chenyu Yang, Shiqian Su, Shi Liu, Xuan Dong, Yue Yu, Weijie Su, Xuehui Wang, Zhaoyang Liu, Jinguo Zhu, Hao Li, et al. Zerogui: Automating online gui learning at zero human cost. arXiv preprint arXiv:2505.23762, 2025. Feng Yao, Liyuan Liu, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Your efficient rl framework secretly brings you off-policy rl training, August 2025. URL https: //fengyao.notion.site/off-policy-rl. Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, et al. Mobile-agent-v3: Foundamental agents for gui automation. arXiv preprint arXiv:2508.15144, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Bofei Zhang, Zirui Shang, Zhi Gao, Wang Zhang, Rui Xie, Xiaojian Ma, Tao Yuan, Xinxiao Wu, SongChun Zhu, and Qing Li. Tongui: Building generalized gui agents by learning from multimodal web tutorials. arXiv preprint arXiv:2504.12679, 2025. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. 13 Preprint"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 BROADER IMPACT Our work advances GUI automation through efficient reinforcement learning, offering benefits in accessibility, productivity, and evaluation. DART-GUI can assist users with disabilities, streamline repetitive tasks, and enable more thorough UI validation. By releasing our framework and models openly, we lower the barrier for researchers and developers while reducing environmental impact through efficient training. At the same time, we recognize risks such as unauthorized access or privacy concerns and emphasize the importance of responsible use with proper safeguards. A.2 LLM USAGE STATEMENT We acknowledge the use of LLMs as writing assistance tool during the preparation of this manuscript. The LLMs were utilized exclusively for improving language quality, including grammar correction, and enhancing clarity. All scientific contributions, including research conceptualization, methodology, experimental design, data analysis, and interpretation of results were conducted solely by the human authors. The LLMs did not generate any original research ideas, hypotheses, or substantive scientific content. The authors assume full responsibility for the accuracy, integrity, and originality of all content presented in this work, including any portions where language was refined with LLM assistance. A.3 SYSTEM PROMPT AND ACTION SPACE We follow the system prompt of the baseline model UI-TARS-1.5-7B(Qin et al., 2025), as shown in Figure 7. System Prompt for GUI Agent You are GUI agent. You are given task and your action history, with screenshots. You need to perform the next action to complete the task. ## Output Format Thought: ... Action: ... ## Action Space click(start_box=<box_start>(x1,y1)<box_end>) left_double(start_box=<box_start>(x1,y1)<box_end>) right_single(start_box=<box_start>(x1,y1)<box_end>) drag(start_box=<box_start>(x1,y1)<box_end>, end_box=<box_start>(x3,y3)<box_end>) hotkey(key=) type(content=) # If you want to submit, use \"n\" at the end scroll(start_box=<box_start>(x1,y1)<box_end>, direction=down or up or right or left) wait() # Sleep for 5s and take screenshot finished(content=xxx) # Use escape characters , \", ## Note Use {language} in Thought part. Write small plan and finally summarize your next action (with its target element) in one sentence in Thought part. My computers password is password, feel free to use it when you need sudo rights. ## User Instruction {instruction} Figure 7: System prompt template for DART-GUI agent with action space definition and output format specifications. 14 Preprint A.4 MORE IMPLEMENTATION DETAILS Trainer. The training pipeline utilizes Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023) via verl (Sheng et al., 2024) for distributed training across 8 NVIDIA H100 GPUs. The learning rate is set to 1 106 with KL divergence regularization coefficient of β = 0.1. Following DAPO (Yu et al., 2025), dynamic clipping boundaries are configured with ϵlow = 0.2 and ϵhigh = 0.28. The rollout policy scaling parameter is set to = 1. Rollout Service. Model deployment employs vLLM (Kwon et al., 2023) as the rollout service, incorporating load balancing mechanisms to distribute workload across devices. Worker-wise Model Syncing is implemented to enable non-blocking operation. Each worker is allocated 2 NVIDIA H100 GPUs. The sampling temperature is set to 1.0, with maximum of 30 interaction steps per episode. The initial rollout num is configured as nrollout = 8. Env Cluster. We use Kubernetes(K8s) Burns et al. (2016) to orchestrate 180 parallel Ubuntu Docker containers serving as environment instances. Each environment operates independently, receiving actions from agents and returning screenshots as observations. Figure 8 shows the dashboard of our cluster. Figure 8: Dashboard of the distributed Ubuntu env cluster. Data Manager.We build centralized Data Manager built on MySQL Oracle Corporation (2024) that handles data storage and coordination across the entire training pipeline. The database architecture, illustrated in Figure 9 and summarized in Table 4, comprises 11 interconnected tables organized into four functional categories. For model management, the Data Manager tracks model checkpoints and versions through the checkpoint, current_model, and model_registry tables, enabling seamless model The data management subsystem, consisting of datasets, versioning and deployment. dataset_usage_events, rollout_run, and rollout_chunk tables, maintains comprehensive records of trajectories, their usage patterns, and associated rewards. Each trajectory is uniquely identified and linked to its corresponding run, task, and model version. For the Trainer, the Data Manager ensures balanced training by monitoring trajectory outcomes through the reward field in multiple tables. When sampling training data, the Data Manager guarantees each task contains at least one successful trajectory (positive reward) and one failed trajectory (negative or zero reward). If all sampled trajectories for given task fail, the Data Manager queries the datasets and rollout_run tables to retrieve positive trajectories from previously collected data, using the trajectory_id and task_id as keys to maintain task consistency. The dataset_usage_events table tracks these data access patterns, recording each usage with timestamps and model versions to ensure reproducibility. Additionally, the trainable_group table aggregates trajectories ready for training, while the update_model_task table manages the model update pipeline, coordinating between checkpoints and deployment stages. 15 Preprint Table 4: Database tables categorized by functionality. Category Model Management Data Management Training Inference Total Table Count Tables 3 4 2 2 checkpoint, current_model, model_registry datasets, dataset_usage_events, rollout_run, rollout_chunk trainable_group, update_model_task inference_node, inference_tasks checkpoint id (BIGINT) PK name (VARCHAR) version (VARCHAR) run_id (VARCHAR) status (VARCHAR) path (VARCHAR) config_yaml (TEXT) created_at (TIMESTAMP) updated_at (TIMESTAMP) + 6 more fields... current_model id (BIGINT) PK checkpoint_id (BIGINT) version (VARCHAR) path (VARCHAR) status (VARCHAR) created_at (TIMESTAMP) updated_at (TIMESTAMP) activated_by (VARCHAR) remark (VARCHAR) model_registry id (BIGINT) PK task_id (VARCHAR) name (VARCHAR) version (VARCHAR) status (VARCHAR) path (VARCHAR) type (VARCHAR) replicas (INTEGER) + 7 more fields... datasets id (INTEGER) PK trajectory_id (VARCHAR) run_id (VARCHAR) task_id (VARCHAR) model_version (VARCHAR) reward (FLOAT) used (INTEGER) created_at (TIMESTAMP) dataset_usage_events id (BIGINT) PK trajectory_id (VARCHAR) run_id (VARCHAR) model_version (VARCHAR) used_delta (INTEGER) event_type (ENUM) created_at (TIMESTAMP) rollout_run id (BIGINT) PK run_id (VARCHAR) trajectory_id (VARCHAR) task_id (VARCHAR) trace_id (VARCHAR) reward (DOUBLE) model_version (VARCHAR) + 4 more fields... inference_node id (BIGINT) PK pod_name (VARCHAR) model_registry_id (BIGINT) task_id (VARCHAR) status (VARCHAR) started_at (TIMESTAMP) updated_at (TIMESTAMP) + 2 more fields... inference_tasks id (INTEGER) PK task_id (VARCHAR) statefulset_name (VARCHAR) namespace (VARCHAR) batch_size (INTEGER) replicas (INTEGER) status (ENUM) + 7 more fields... update_model_task id (BIGINT) PK checkpoint_id (BIGINT) path (VARCHAR) version (VARCHAR) type (VARCHAR) status (TINYINT) priority (TINYINT) + 5 more fields... trainable_group trajectory_id (VARCHAR) PK task_id (VARCHAR) run_id (VARCHAR) reward (DOUBLE) model_version (VARCHAR) create_at (DATETIME) rollout_chunk id (BIGINT) PK trajectory_id (VARCHAR) run_id (VARCHAR) chunk_index (INTEGER) json_path (VARCHAR) Legend: Primary Key Foreign Key Model Task Data Flow Training Figure 9: Database Schema Visualization showing the relationships between 11 tables. Golden cells indicate primary keys, green cells indicate foreign keys. Blue arrows represent model management relationships, green arrows show task dependencies, red dashed arrows indicate data flow between rollout and dataset tables, and purple arrows represent training data connections. 16 Preprint A.5 VISUALIZATION Key Steps of Tasks. We visualize the comparison between the baseline and our model on several tasks, demonstrating that through our RL training, our model can make correct actions at critical steps, thereby achieving successful trajectories. The comparative analysis clearly shows the improvement in decision-making at pivotal moments, as illustrated in Figure 10 and Figure 11. Visualization of Extremely Difficult Tasks. By leveraging pre-collected successful trajectories from the trajectory pool for extremely difficult tasks (where pass@32 failed), our trained model demonstrates the ability to generate correct trajectories on these challenging tasks. We visualize the critical steps that truly determine the success or failure of trajectories in these tasks and provide detailed analysis of the underlying reasons, as shown in Figure 12 and Figure 13. A.6 FAILURE CASES We also visualize representative failure cases to highlight the limitations of our model. These examples demonstrate situations where DART-GUI-7B makes mistakes at key steps, preventing successful task completion, as illustrated in Figure 14. 17 Preprint Figure 10: Case study comparing UI-TARS-7B and DART-GUI-7B on configuring line wrapping in VS Code. UI-TARS-7B exhibits reasoning error by modifying the unrelated HTML > Format: Wrap Line Length option, whereas DART-GUI-7B correctly locates and sets the Editor: Word Wrap Column parameter to the desired value.2 18 Preprint Figure 11: Case study comparing UI-TARS-7B and DART-GUI-7B on editing text in LibreOffice. UI-TARS-7B makes grounding error by selecting both and 2 in H2O, whereas DART-GUI7B correctly highlights only the 2 for conversion into subscript. 19 Preprint Figure 12: Case study on an extremely difficult LibreOffice Impress task. The task requires configuring dual-slide display settings. The baseline model (top) incorrectly clicks \"Slide Show\" in the menu, leading to task failure. Our DART-GUI-7B model (bottom), trained with successful trajectories from the trajectory pool, correctly selects \"Tools\" to access the preferences panel where dual-slide display can be configured. This demonstrates our models ability to learn from rare successful trajectories and solve previously intractable tasks through RL training. 20 Preprint Figure 13: Case study on an extremely difficult bookmark saving task. The task requires saving webpage to the bookmarks bar for quick access. The baseline model (top) makes critical error by clicking \"Done\" without changing the bookmark folder from the default \"All Bookmarks\" to \"Bookmarks bar,\" resulting in task failure. Our DART-GUI-7B model (bottom) correctly identifies the need to switch the folder dropdown to \"Bookmarks bar\" before confirming, successfully completing the task. This demonstrates our models ability to understand subtle but crucial UI requirements that determine task success, learned through RL training on rare successful trajectories. 21 Preprint Figure 14: Failure cases of DART-GUI-7B. (a) For the task of enabling the Do Not Track feature in Chrome, the model incorrectly clicks the Site settings option instead of the Third-party cookies option required to access the relevant privacy control. (b) For the task of opening two workspaces simultaneously in VS Code, the model attempts Ctrl+click sequence, but due to action space limitations, the execution corresponds to sequentially pressing Ctrl and clicking without holding Ctrl, which deselects the first workspace and leaves only the second one selected."
        }
    ],
    "affiliations": [
        "Beijing Institute of Technology",
        "Beijing University of Posts and Telecommunications",
        "DataCanvas",
        "Shenzhen MSU-BIT University",
        "State Key Laboratory of General Artificial Intelligence, BIGAI"
    ]
}