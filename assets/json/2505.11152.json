{
    "paper_title": "Learning Dense Hand Contact Estimation from Imbalanced Data",
    "authors": [
        "Daniel Sungho Jung",
        "Kyoung Mu Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Hands are essential to human interaction, and understanding contact between hands and the world can promote comprehensive understanding of their function. Recently, there have been growing number of hand interaction datasets that cover interaction with object, other hand, scene, and body. Despite the significance of the task and increasing high-quality data, how to effectively learn dense hand contact estimation remains largely underexplored. There are two major challenges for learning dense hand contact estimation. First, there exists class imbalance issue from hand contact datasets where majority of samples are not in contact. Second, hand contact datasets contain spatial imbalance issue with most of hand contact exhibited in finger tips, resulting in challenges for generalization towards contacts in other hand regions. To tackle these issues, we present a framework that learns dense HAnd COntact estimation (HACO) from imbalanced data. To resolve the class imbalance issue, we introduce balanced contact sampling, which builds and samples from multiple sampling groups that fairly represent diverse contact statistics for both contact and non-contact samples. Moreover, to address the spatial imbalance issue, we propose vertex-level class-balanced (VCB) loss, which incorporates spatially varying contact distribution by separately reweighting loss contribution of each vertex based on its contact frequency across dataset. As a result, we effectively learn to predict dense hand contact estimation with large-scale hand contact data without suffering from class and spatial imbalance issue. The codes will be released."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 2 5 1 1 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Learning Dense Hand Contact Estimation from\nImbalanced Data",
            "content": "Daniel Sungho Jung1 Kyoung Mu Lee1,2 1IPAI, 2Dept. of ECE & ASRI, Seoul National University, Korea {dqj5182, kyoungmu}@snu.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Hands are essential to human interaction, and understanding contact between hands and the world can promote comprehensive understanding of their function. Recently, there have been growing number of hand interaction datasets that cover interaction with object, other hand, scene, and body. Despite the significance of the task and increasing high-quality data, how to effectively learn dense hand contact estimation remains largely underexplored. There are two major challenges for learning dense hand contact estimation. First, there exists class imbalance issue from hand contact datasets where majority of samples are not in contact. Second, hand contact datasets contain spatial imbalance issue with most of hand contact exhibited in finger tips, resulting in challenges for generalization towards contacts in other hand regions. To tackle these issues, we present framework that learns dense HAnd COntact estimation (HACO) from imbalanced data. To resolve the class imbalance issue, we introduce balanced contact sampling, which builds and samples from multiple sampling groups that fairly represent diverse contact statistics for both contact and non-contact samples. Moreover, to address the spatial imbalance issue, we propose vertex-level class-balanced (VCB) loss, which incorporates spatially varying contact distribution by separately reweighting loss contribution of each vertex based on its contact frequency across dataset. As result, we effectively learn to predict dense hand contact estimation with large-scale hand contact data without suffering from class and spatial imbalance issue. The codes will be released."
        },
        {
            "title": "Introduction",
            "content": "From infancy, humans rely on physical contact to perceive and interact with the surrounding environment. Among the various mediums of contact, hands play predominant role, facilitating essential tasks and enabling effective communication. Consequently, developing robust hand contact estimation model is crucial for advancing our understanding of hand interactions and addressing various challenges [13, 33] that requires accurate hand contact estimation. Over the recent years, we have witnessed significant advances in large-scale interaction datasets involving hands. Hand-object interaction datasets [18, 5, 4, 14, 15, 11, 34, 23] focused on capturing hand grasps of objects. Hand-hand interaction datasets [52, 38, 37] considered two hand interaction from single person. Hand-face interaction dataset [48] investigated on interaction between hand and deformable face. Hand-scene interaction datasets [17, 21, 51] extended the scope of interaction to include environments featuring the ground, walls, and large-scale objects. Hand-body interaction dataset [56] covered broad range of interaction with human body as part of human-human interaction. Despite the wide range of diversity in the hand interaction datasets with contact annotations [18, 5, 4, 14, 15, 11, 34, 23, 38, 52, 48, 16, 21, 56], to the best of our knowledge, there have been very few attempts to build hand contact estimation model trained on such diverse hand contact. Based on such diverse sets of hand contact data, we aim to build an effective dense hand contact estimation model. Preprint. Under review. There are two major issues for training dense hand contact estimation. First, there exists class imbalance between contact and non-contact class. Human participants of the hand interaction datasets are often instructed to perform specific actions such as picking up an object [5, 14, 34] and pointing to specific regions of objects [23] or other hands [38]. As most of such actions require precise control with interaction by finger tips, most of the other hand regions (e.g., dorsal area) are overlooked and excluded from being in contact. In Figure 1, we provide comparison of occurrence of contact and non-contact in major hand contact datasets for all vertices of MANO hand model [47]. This observation shows severe class imbalance issue of hand contact from existing hand contact datasets. Numerically, DexYCB [5] dataset has roughly 2.7:1 ratio of non-contact to contact samples. More severe InterHand2.6M [38] dataset has huge imbalance ratio of 19.5:1. Such imbalance is also shown in Decaf [48] dataset with 21.7:1 ratio. As stated by numerous studies on data imbalance [3, 6, 45, 24], such data imbalance causes poor performance on underrepresented classes, which is in fact the contacting cases in our scenario. Second, the spatial imbalance issue of hand contact data exists throughout the majority of hand contact datasets. This imbalance arises from the fact that hand interaction predominantly involves the fingertips, which provide high degree of freedom in both movement and rotation. Due to such high maneuverability and the precise nature of actions from motion capture datasets, most of hand contact from the datasets are significantly skewed to finger tip regions. As shown in Figure 1, the heatmap for hand contact from hand contact datasets are severely skewed towards finger tips. Due to such spatial imbalance issue of hand contact datasets, models directly trained on the datasets struggle to generalize on diverse contact patterns across hand. Figure 1: Two challenges for dense hand contact estimation in the wild. First, hand contact datasets suffer from class imbalance, as the majority of samples contain no contact. Second, spatial imbalance arises because contact points are predominantly concentrated at the fingertips. Due to these issues, models trained on such data struggle to generalize to diverse contact patterns across the hand. Therefore, we propose HACO, framework that learns dense hand contact estimation from imbalanced data. Initially, HACO is trained on large-scale, assorted dataset composed of 14 datasets, shown in Table 1, that contains various hand interactions to leverage the power of large-scale training [2, 46, 22, 55, 44]. However, this large-scale dataset still exhibits inherent imbalance in terms of both class and spatial distribution. To address these issues, we introduce two strategies designed to fully exploit the potential of such large-scale data. First, we introduce Balanced Contact Sampling (BCS), which mitigates class imbalance in hand contact datasets by dividing the dataset into multiple sampling groups, each representing different statistics of contact and non-contact instances. To ensure fair representation of all contact statistics, we compute contact balance score that measures how much each hand contact sample deviates from the dataset-wide average. This metric enables us to assess whether each sample is under-represented or over-represented within the original dataset and to adjust the sampling ratios accordingly, ensuring all contact types are well represented. Second, we present the Vertex-Level Class-Balanced (VCB) loss, which applies spatially varying weights to each vertex to address the spatial imbalance issue in hand contact datasets. Inspired by the core idea of class-balanced (CB) loss [6], which re-weights samples based on class frequency, we adapt this concept to the spatial domain by computing separate loss weight for each vertex based on how well contact and non-contact cases are represented at that vertex in the dataset. While the original CB loss applies global class-level weighting, our method performs fine-grained, vertex-level weighting, allowing the model to focus on under-represented contact patterns in spatially aware manner. 2 Table 1: Dataset configuration. We leverage 14 datasets with various hand interaction. Dataset Interaction Domain # of images # of subjects # of objects ObMan [18] DexYCB [5] MOW [4] HO3D [14] H2O3D [15] ARCTIC [11] HOI4D [34] H2O [23] InterHand2.6M [38] HIC [52] PROX [16] RICH [21] Decaf [48] Hi4D [56] Rigid Object Rigid Object Rigid Object Rigid Object Rigid Object + Hand Synthetic Real / Indoor Real / Outdoor Real / Indoor Real / Indoor Articulated Object + Hand Real / Indoor / Studio Rigid Object + Hand Rigid Object + Hand Hand Hand Scene Scene Face (Whole-Body) Whole-Body Real / Indoor Real / Indoor Real / Indoor / Studio Real / Indoor Real / Indoor / Studio Real / Outdoor Real / Indoor Real / Indoor / Studio 152K 582K 0.5K 78K 76K 2.1M 2.4M 571K 2.6M 36K 100K 540K 100K 11K 20 10 450 10 5 10 4 4 27 1 20 22 8 40 8 20 121 10 10 11 16 8 - - - - - - As result, HACO achieves state-of-the-art performance across diverse hand contact scenarios, including hand-object, hand-hand, hand-scene, and hand-body interactions, consistently outperforming existing methods. Our key contributions are as follows: We introduce HACO, novel framework that addresses the data imbalance problem in hand contact datasets and enables effective learning of dense hand contact estimation from large-scale data. To mitigate class imbalance in existing hand contact datasets, we propose Balanced Contact Sampling (BCS), which constructs multiple sampling groups to fairly represent diverse contact statistics. To handle spatial imbalance, we present the Vertex-Level Class-Balanced (VCB) loss, which computes vertex-specific weighting factor based on the dataset-wide contact distribution. In the end, HACO achieves substantial performance gains over prior methods across wide range of hand contact scenarios."
        },
        {
            "title": "2 Related works",
            "content": "Dense hand contact estimation. Most of the existing methods that explore in dense hand contact estimation are devoted to dense human-scene contact estimation [17, 21, 51]. Given an image of human-scene interaction, such methods estimate contactness for each vertex of 3D human model (i.e., SMPL [35], SMPL-X [43]). POSA [17] is based upon the idea that the human-scene contact is largely influenced by the 3D pose of human. Hence, the work employs conditional variational autoencoder framework [49] to learn human-scene contact with the 3D vertex position of posed human as condition. BSTRO [21] takes another approach for estimating human-scene contact, which directly relies on visual input with Transformer-based architecture [9]. To train the model, they proposed RICH dataset [21] that annotated dense human-scene contact by capturing 3D geometry of body and scene with multi-view laser scanner in outdoor environment. Lastly, to improve generalization ability of human-scene contact estimation, DECO [51] focused on how to annotate human-scene contact in the wild. They introduced simple yet effective contact annotation method that simply asks crowdsourced annotators to paint 3D human model (i.e., SMPL [35]) in neutral T-pose, which significantly enhanced the potential of scaling contact data. Despite numerous attempts on dense human-scene contact estimation, there lacks in-depth investigation on dense hand contact estimation. Furthermore, we found that dense hand contact estimation has unique challenges compared to human-scene contact estimation. First, due to class imbalance of hand contact datasets, the distribution of the datasets are often skewed towards non-contact class, making it non-trivial to train hand contact estimation from the datasets. Second, there exists spatial imbalance issue of hand contact datasets where majority of hand contact occurs in finger tips with insufficient data for hand contact in rest of the regions (e.g., dorsum). We introduce HACO, dense hand contact estimation method that effectively tackles such data imbalance issues of hand contact datasets. 3 Figure 2: Overall pipeline of HACO. Our method encodes input image as image tokens with ViT backbone after patch embedding layers. Given the image tokens along with positional embeddings and contact token, multiple layers of self-attention Transformer and cross-attention Transformer produce an output token. Lastly, the output token is further processed with linear layer and added with contact initialization that passes sigmoid layer to output final hand contact prediction. 3D hand and object interaction reasoning. Dense hand contact reveals key spatial and semantic relationship between 3D hand and object. Initially, dense hand contact was studied in the field of grasping with specific tasks such as grasp contact prediction [1], contact-guided hand and object pose refinement [13], and contact-conditioned grasp generation [32]. Such works made tremendous impact on robotics, especially how robots can naturally grasp an object. However, their dense hand contact were mostly predicted from 3D geometry of hand and object without exploring how to directly predict the dense hand contact from visual input. Recently, the field of 3D hand and object reconstruction showed potential on the effectiveness of contact for improving both reconstruction accuracy and natural interaction between hand and object. CHOI [20] proposed contact conditioned implicit reconstruction model for 3D hand-held object that utilizes contact as major cue for predicting 3D object surface geometry. EasyHOI [33] introduced prior-guided hand-object interaction optimizer that refines initial 3D hand and object using heuristically determined contact regions to improve interaction quality. Our HACO is applicable to these downstream tasks as long as visual input is available. To demonstrate HACOs effectiveness, we show that our predicted dense contact improves 3D grasp optimization over geometry-driven contact estimation from ContactOpt [13], and 3D reconstruction over previously used heuristic contact regions in EasyHOI [33]."
        },
        {
            "title": "3 Method",
            "content": "3.1 Model architecture Given an RGB image R3HW , where and denote the height and width of the image, we first embed the image into multiple tokens with patch embedding layer and process these tokens using Vision Transformer (ViT) [10]. Following ViTPose [54] and HaMeR [44], we reshape the tokens and formulate the image feature R12801612. Additionally, we build contact token, which acts as query token for hand contact estimation. To inject image feature into the contact token, we process the image feature and positional embedding with consecutive self-attention Transformer and cross-attention Transformer with contact token as query and image feature as keyvalue pair. Afterwards, the output contact token is processed with linear layer. In order to resemble residual layers [19] that stabilize training, we add contact initialization to the output of linear layer. The contact initialization is learnable embedding that naturally learns the most effective initial contact estimation during training. Lastly, sigmoid layer converts the final contact logits RV into the estimated dense hand contact probabilities, where = 778 is MANO hand vertices [47]. 3.2 Balanced contact sampling hand contact dataset = {Hi = (vi, ci)}N i=1 consists of hand instances, where each Hi contains the 3D hand vertices vi RV 3 and binary vertex contact vector ci {0, 1}V . As illustrated in Figure 1, the dataset exhibits strong class imbalance, with many hand instances lacking contact. To alleviate this issue, we propose Balanced Contact Sampling (BCS), which constructs sampling bins = {B1, B2, . . . , BK} consisting of groups of hand instances. Each bin Bk is formed by binning samples based on their contact balance scores si, which reflect how 4 each hands contact ci deviates from the dataset-wide average c. To compute si, we first define the dataset-wide contact mean as = 1 i=1 ci, which represents the contact probability at each vertex across all samples. The contact balance score si for each sample is then defined as: 1 i (1 c) c(cid:1) (cid:0)c si = (cid:80)N (1) where higher scores correspond to hand instances Hi whose contact patterns deviate more from the dataset-wide average, enabling balanced grouping across diverse contact statistics that span both contact and non-contact samples. Based on the distribution of contact balance scores {si}N i=1, we partition the dataset into bins B1, . . . , BK by applying non-linear binning. We first compute the minimum and maximum contact balance scores as smin = mini si and smax = maxi si, and define K+1 bin edges using logarithmic spacing scheme controlled by curvature parameter β > 0 (e.g., β = 5): τk = smin + (smax smin) log(1 + β xk) log(1 + β) , xk = , = 0, . . . , K. (2) This results in bin edges {τ0, τ1, . . . , τK} that allocate finer resolution to higher contact scores. Each group Bk is then constructed by selecting hand instances whose contact balance scores fall within the corresponding interval: Bk = {Hi si [τk1, τk)} (3) Due to the skewed distribution of scores si, the bins Bk may differ in size. To equalize their contribution, we apply stratified resampling to ensure each bin contains the same number of hand instances in the final training set. In the end, this binning strategy enables more accurate grouping and fair sampling of contact-rich hand instances that are underrepresented in the original dataset D, improving the representational balance of the sampling bins B. 3.3 Vertex-level class balanced loss As illustrated in Figure 1, hand contact datasets exhibit severe spatial imbalance, with contact heavily concentrated at the fingertips. To address this issue, we propose vertex-level class-balanced (VCB) loss, which builds upon the class-balanced sigmoid binary cross-entropy loss [6]. We first formulate hand contact estimation as binary classification problem. simple binary cross-entropy (BCE) loss is defined as ℓBCE(y, p) = log(p) (1 y) log(1 p), where {0, 1} is the ground-truth label and [0, 1] is the predicted probability. When applied over all hand vertices , the overall BCE loss becomes: LBCE = ℓBCE(yv, pv), (4) (cid:88) 1 vV where we denote ground-truth contact for vertex as yv {0, 1} and predicted contact probability for vertex as pv = σ(z) with sigmoid function as σ and predicted logit as z. From the BCE loss, class-balanced (CB) loss [6] introduces weighting factor αc for each class c, assigning greater importance to under-represented classes. In our case, the class corresponds to the binary contact label {0, 1}. Accordingly, we denote the class-balanced weighting factor as αy. Simply, we can write CB loss as follows: LCB = 1 (cid:88) vV αyℓBCE(yv, pv). (5) Specifically, the weighting factor αy is defined using the effective number of samples E(y) 1 E(y) 1 β 1 βny αy = = . , where (6) Here, ny denotes the number of occurrences of class y, and β [0, 1) is hyperparameter that controls the influence of sample count on the weighting. When the contact class appears frequently in the dataset, the contact frequency value ny becomes large, which increases the effective number E(y) = 1βny . As result, the CB loss down1β weights the contribution of frequent classes and helps mitigate class imbalance in dense hand contact datasets. and results in smaller class-balanced weight αy = 1 E(y) Nevertheless, CB loss [6] is defined at the class level, based on the binary contact label {0, 1}. If we directly apply CB loss, the weighting factor αy takes only two distinct values, one for contact and one for non-contact, and is applied uniformly across all spatial regions. In the DexYCB dataset [5], contact and non-contact vertices account for approximately 76.86% and 23.14% of all vertex labels, respectively. Accordingly, the weighting factor αy is computed using sample counts ny = (0.7686 n, 0.2314 n), where is the total number of contact labels in the dataset. However, this count ny is applied uniformly across all vertices, without considering spatial variation. For example, fingertip vertices are much more likely to be in contact, while dorsal hand vertices are rarely in contact. As result, the class-balanced weight αy and effective sample number E(y) do not accurately reflect the local distribution of contact and non-contact at each vertex. To address this limitation, we extend the CB loss formulation to operate at the vertex level. We introduce the vertex-level class-balanced (VCB) loss, which computes spatially varying weighting factor αyv and effective number of samples E(yv) for each vertex v. To account for the spatial imbalance in hand contact datasets, we simply assign separate weighting factor αyv to each vertex, depending on the local distribution of contact labels. Given the number of samples nyv for contact class at vertex v, the weighting factor is computed as: αyv ="
        },
        {
            "title": "1\nE(yv)\nn",
            "content": "= 1 β 1 βnyv . Using this spatially varying weight, the vertex-level class-balanced loss is defined as: 1 αyv ℓBCE(yv, pv). LVCB = (cid:88) vV (7) (8) This spatial adaptivity enables the VCB loss to effectively mitigate the spatial imbalance inherent in hand contact datasets. 3.4 Final outputs and loss functions C}N The final dense hand contact output supports multi-level supervision through projections to coarser resolutions. From the dense contact logits regressed by the final linear layer, we build multi-level hand contact predictions = {Ci = vi i=1 for = 4 and vi {778, 336, 84, 21} where vi RviV is regressor that maps full MANO vertices into coarse representations. We then apply sigmoid layer to convert the logits into contact probabilities. To train HACO, we apply three loss functions: the vertex-level class-balanced (VCB) loss for multi-level hand contact supervision, regularization loss, and smoothness loss. Please refer to Section 3.3 for details on the VCB loss, and to the Appendix for the regularization and smoothness losses. Briefly, the regularization loss is defined as the L1 loss between the dense contact prediction and the dataset-wide mean ground-truth contact, encouraging the prediction to stay close to the average. The smoothness loss measures how predicted contact and non-contact regions are spatially disconnected. This encourages HACO to predict small number of large contact regions rather than many small, fragmented ones. We set the weights of the VCB loss, regularization loss, and smoothness loss to 1, 0.1, and 1, respectively."
        },
        {
            "title": "Implementation details",
            "content": "PyTorch [42] is used for implementation. Our backbone is initialized with the pre-trained weights of publicly released HaMeR [44]. Following HaMeR, we apply data augmentations including random scaling, cropping, and rotation. To improve robustness to degraded inputs, we additionally perform low-resolution, noise, and blur augmentations. We use the AdamW optimizer [36] with learning rate of 105 and mini-batch size of 24. For stable convergence, the learning rate is reduced by factor of 0.9 after 5 and 10 epochs. We train HACO for 10 epochs on single NVIDIA A6000 GPU."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Datasets We select 14 datasets with diverse hand interactions with ObMan [18], DexYCB [5], MOW [4], HO3D [14], H2O3D [15], ARCTIC [11], HOI4D [34], H2O [23] for hand-object interaction, InterHand2.6M [38], HIC [52] for hand-hand interaction, PROX [16], RICH [21] for hand-scene 6 interaction, and Decaf [48], Hi4D [56] for hand-body interaction. To reduce redundancy from large video datasets, we employ sampling ratio of 5, 10, 5 for HOI4D [34], InterHand2.6M [38], and Decaf [48] dataset, respectively. Note that this is separate from the Balanced Contact Sampling in Section 3.2. For ARCTIC [11] dataset, we select samples captured from an egocentric viewpoint, while for HOI4D [34], we use samples involving rigid objects due to their uniqueness. Since the 3D hand annotations in PROX [16] are inaccurate in the qualitative split, we use only the quantitative subset. For the RICH dataset, we follow the official split used in BSTRO [21] for fair comparison. In total, this results in 840K images with ground-truth dense hand contact labels. For details on our method on extracting ground-truth dense hand contact labels, please refer to Appendix. 5.2 Evaluation metrics To evaluate dense hand contact estimation, we compute precision, recall, and F1-score. However, recall and F1-score are undefined for fully non-contact samples. Thus, we skip fully non-contacting samples during evaluation. We also assess contact quality on two downstream tasks: 3D hand grasp optimization [13] and 3D hand-object reconstruction [33]. For 3D hand grasp optimization [13], we evaluate Intersection Volume (Inter Vol.), Mean Per-Joint Position Error (MPJPE), and contact metrics. For 3D hand-object reconstruction, we compare HACO using Mean Per-Vertex Position Error (MPVPE), MPJPE, CDho, F-5ho, and F-10ho against the contact method in EasyHOI. F-5ho and F-10ho denote F-scores of reconstructed 3D hand and object using 5mm and 10mm thresholds. 5.3 Ablation study Effectiveness of balanced contact sampling. In Table 2, the proposed balanced contact sampling strategy significantly enhances performance across all metrics. Compared to the model trained without sampling, our method improves 1.0% in precision, 16.6% in recall, and 10.4% in F1-score. The substantial boost in recall, in particular, highlights the effectiveness of balanced sampling in mitigating class imbalance by exposing the model to more positive contact instances. Overall, these results demonstrate that our sampling strategy plays critical role in improving the models ability to detect contact accurately and comprehensively. Table 2: Ablation on balanced contact sampling strategy on MOW [4] dataset. w/o sampling w/ sampling (Ours) Precision Recall F1-Score 0.542 0. 0.520 0.525 0.481 0.531 Methods Methods Table 3: Comparison of various loss-based techniques for data imbalance on MOW [4] dataset. Effectiveness of vertex-level class balanced loss. In Table 3, the proposed VCB loss achieves the best overall performance by effectively addressing data imbalance at finer granularity. Unlike CB loss, which applies class balancing at the hand level by weighing positive and negative samples across all vertices, VCB loss operates at the per-vertex level, allowing the model to learn contact patterns from individual vertex more accurately. This vertex-wise weighting enables the model to better capture subtle and spatially varying contact signals, especially in under-represented regions, leading to improved recall without sacrificing precision. As result, VCB loss yields the highest F1-score among all loss-based methods, demonstrating that resolving class imbalance locally, rather than globally, is key to effective dense contact estimation. CE loss L1 loss L2 loss Focal loss [30] CB loss [6] CB Focal loss [6] LDAM loss [3] Asymmetric loss [45] Poly loss [24] VCB loss (Ours) 0.530 0.521 0.531 0.518 0.484 0.522 0.532 0.484 0.528 0.525 0.348 0.413 0.352 0.409 0.465 0.392 0.293 0.440 0.371 0.531 0.294 0.392 0.298 0.387 0.534 0.360 0.224 0.479 0.324 0.632 Precision Recall F1-Score Effectiveness of large scale data configuration. Our final model benefits significantly from the inclusion of all four interaction types: hand-object (HO), hand-hand (HH), hand-body (HB), and handscene (HS). As shown in Table 4, performance improves consistently across all evaluated datasets as more interaction types are incrementally incorporated, with the full combination achieving the highest overall F1-scores. Removing HO leads to substantial drop in recall, further highlighting its essential role in capturing diverse contact patterns. Although HB and HH alone contribute modestly, their 7 Table 4: Comparison of various training dataset configurations on MOW [4], HIC [52], RICH [21], and Hi4D [56]. HO: Hand-object, HH: Hand-hand, HS: Hand-scene, HB: Handbody. Test dataset Train dataset Precision Recall F1-Score MOW [4] HIC [52] RICH [21] Hi4D [56] HS HS+HH HS+HH+HB HS+HH+HB+HO (Ours) HS HS+HO HS+HO+HB HS+HO+HB+HH (Ours) HO HO+HH HO+HH+HB HO+HH+HB+HS (Ours) HS HS+HO HS+HO+HH HS+HO+HH+HB (Ours) 0.416 0.290 0.430 0.525 0.070 0.096 0.186 0.181 0.551 0.554 0.554 0.743 0.562 0.610 0.596 0.781 0.089 0.045 0.150 0.632 0.005 0.257 0.409 0. 0.547 0.522 0.581 0.570 0.072 0.503 0.513 0.923 0.101 0.052 0.163 0.531 0.009 0.136 0.230 0.278 0.492 0.471 0.490 0.528 0.095 0.496 0.498 0. addition enhances recall and stabilizes overall performance, indicating their complementary effects. Omitting HS results in reduced F1-scores despite strong contributions from the other components, suggesting its importance in separating hand contacts from background context. Ultimately, only the full configuration achieves strong and balanced performance across all metrics and datasets, demonstrating that each interaction type contributes uniquely and that leveraging the full diversity is crucial for learning accurate and generalizable contact representations. 5.4 Comparison with state-of-the-art methods Dense hand contact estimation. Table 5 presents comparison between our HACO and stateof-the-art approaches of POSA [17], BSTRO [21], and DECO [51] on the MOW [4] dataset. Methods Precision Recall F1-Score Table 5: Comparison with SOTA methods of hand contact estimation on MOW [4] dataset. Our method achieves substantial improvements across all metrics, significantly outperforming the strongest prior method, DECO [51]. While POSA [17] shows limited performance due to its sole reliance on pose priors [12] without image evidence, BSTRO [21] suffers from the lack of largescale, diverse contact data and does not address class or spatial imbalance during training. DECO [51] improved over earlier methods, but performance remains constrained by the lack of diverse supervision and mechanisms for handling imbalance. In contrast, HACO addresses these challenges by training on diverse contact configurations, initializing contact prediction with learned prior, and tackling class and spatial imbalance through targeted loss design. Figure 3 further highlights HACOs qualitative superiority, showing more precise and anatomically plausible contact patterns across scenarios, such as fingertip and palmar contact when holding mic, partial dorsal contact when dicing knife, and thumbindex contact when grasping pencil. Thus, our method shows superior performance over prior SOTA methods, demonstrating its potential as strong model for dense hand contact estimation by addressing key challenges limiting earlier approaches. POSA [17] BSTRO [21] DECO [51] 0.101 0.112 0.197 0.128 0.126 0.235 0.134 0.204 0.246 HACO (Ours) 0.525 0.632 0.531 3D hand grasp optimization. Table 6 compares our HACO with the prior state-of-the-art, DeepContact [13], for contact-guided 3D grasp optimization using predicted hand-object poses from HFL-Net [31] on the DexYCB [5] dataset. Despite DeepContact having access to full 3D hand and object meshes for contact estimation, our method achieves comparable or superior results using only image input. HACO attains lower MPJPE, indicating more accurate hand articulation, achieving higher precision, recall, and F1-score, reflecting better contact quality and coverage. Overall, 8 Figure 3: Qualitative comparison of dense hand contact estimation with POSA [17], BSTRO [21], and DECO [51] on MOW [4] dataset. We highlight exemplar regions where HACO outperform previous methods. Note that we only predict right hand contact. these results show that HACO enables more effective and robust contact-aware grasp optimization, even without access to full 3D hand and object geometry, highlighting its practical applicability in real-world scenarios. Table 6: Comparison of various contact estimation methods for 3D hand grasp optimization on DexYCB [5] dataset. Methods Inter Vol. MPJPE Precision Recall F1-Score DeepContact [13] 29. 37.155 HACO (Ours) 29.264 36.520 0.522 0. 0.830 0.877 0.612 0.666 3D hand and object reconstruction. Table 7 compares the performance of EasyHOI [33] between using its original contact estimation module and using HACO, keeping all other components unchanged. This isolates the impact of the contact on reconstruction performance. HACO outperforms the original across all metrics, including MPVPE, MPJPE, Chamfer Distance (CDho), and F-scores. These consistent gains suggest that HACO produces more accurate and physically plausible contact predictions, directly improving hand-object alignment and reconstruction quality. Figure 4 further supports this, showing that the 3D reconstruction with contact from HACO more precisely captures interaction between the hand and the pen, unlike EasyHOIs heuristic-based approach. Table 7: Comparison of various contact estimation methods for 3D hand and object reconstruction with EasyHOI [33] on MOW [4] dataset. PVE and PJE refer to MPVPE and MPJPE on vertices and joints between estimated and groundtruth 3D hand, respectively. Methods PVE PJE CDho F-5ho F-10ho EasyHOI [33] 21.254 20.973 8.338 0.120 HACO (Ours) 21.093 20.845 8.186 0.122 0.230 0. Figure 4: Qualitative comparison of 3D handobject reconstruction on MOW [4] dataset."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose HACO, novel and powerful method that learns dense hand contact estimation from imbalanced data. For class imbalance and spatial imbalance in hand contact datasets, we propose balanced contact sampling and vertex-level class balanced loss. As result, our HACO outperforms previous methods by significant margin on dense hand contact estimation and shows effectiveness on 3D grasp optimization and 3D hand and object reconstruction."
        },
        {
            "title": "A Appendix",
            "content": "In this appendix, we provide additional technical details and experimental results that were omitted from the main manuscript due to space constraints. The contents are summarized below: A.1. Details of loss functions A.2. Details of contact initialization A.3. Details of dense hand contact labels A.4. More examples of class imbalance issue A.5. More examples of spatial imbalance issue A.6. Quantitative results with different backbones A.7. Computational requirements A.8. More qualitative results A.9. Limitations and societal impacts A.1 Details of loss functions Smoothness loss. We penalize spatially isolated or fragmented contact predictions by leveraging the mesh topology of the hand. Let pv [0, 1] denote the predicted contact probability after the sigmoid for each vertex , and let {0, 1}V be sparse adjacency matrix representing vertex connectivity in the mesh. Here, Avu = 1 indicates that vertex is connected to vertex v, and 0 otherwise. For each vertex , we compute the aggregated contact prediction over its neighbors as: (cid:88) ˆpv = Avu pu, and similarly compute the aggregated non-contact prediction as: uV ˆqv = (cid:88) uV Avu (1 pu), (9) (10) where ˆpv and ˆqv represent the neighborhood-smoothed estimates for contact and non-contact probabilities, respectively. To assess spatial inconsistency, we compute the discrepancy between each vertexs prediction and its neighborhood average. Specifically, we define the isolation score as: sv = pv ˆpv + (1 pv) ˆqv . (11) This term penalizes contact predictions that sharply differ from those of their neighbors, whether in the contact or non-contact region. To prevent the isolation score from disproportionately penalizing spatially large or confident regions, we normalize the total discrepancy by the sum of neighborhood weights: (cid:88) nv = Avu = ˆpv + ˆqv, (12) uV which corresponds to the degree of vertex in the mesh. This normalization accounts for how many vertices contribute to the isolation score at each location, ensuring that the loss measures average inconsistency rather than accumulating errors over larger neighborhoods. As result, the model is encouraged to produce smooth contact maps without being biased against large, spatially coherent contact regions. We then define the final smoothness regularization loss as: (cid:18) Liso = log 1 + (cid:80) vV sv vV nv + ϵ (cid:80) (cid:19) , (13) where ϵ is small constant added for numerical stability. This loss encourages smooth transitions and penalizes sharp discontinuities in the prediction map. By suppressing highly localized contact signals and promoting coherent spatial clusters, it guides the model to favor fewer but larger contact regions, better reflecting real-world hand interaction patterns. Regularization loss. To prevent the model from predicting arbitrarily skewed contact distributions, we introduce global regularization loss that encourages predictions to stay close to the dataset-wide 1 mean contact pattern. Let pv [0, 1] denote the predicted contact probability after the sigmoid for each vertex , and let pv [0, 1] denote the dataset-wide average contact probability at vertex v, computed over all training samples. We then define the regularization loss as: Lreg = 1 (cid:88) vV pv pv , (14) which is an L1 loss between the predicted contact probabilities and the mean distribution. This encourages the model to avoid degenerate or overly confident predictions that deviate significantly from typical contact patterns, providing soft global constraint during training. A.2 Details of contact initialization To stabilize the training of dense hand contact estimation of HACO, we propose contact initialization technique that utilizes learnable embedding to learn the most effective initial contact for dense hand contact estimation. In Table A1, our proposed contact initialization outperforms all other initialization methods, achieving the superior performance in precision, recall, and F1-score. Unlike other approaches that rely on fixed priors or simplistic assumptions, our method introduces contact representation learned from input data, enabling more accurate and robust initialization during inference. This formulation captures the most useful initial prediction compared to static or handcrafted methods, leading to gains across all metrics. Such consistent improvement demonstrates that learning contact directly from large-scale datasets, rather than relying on pre-defined numbers or dataset averages, is crucial for precise contact estimation. Table A1: Comparison of various contact initialization on MOW [4] dataset. Methods Precision Recall F1-Score No initialization No contact Full contact Mean of ObMan [18] Mean of DexYCB [5] Mean of MOW [4] Mean of IH26M [38] Ours 0.508 0.514 0.511 0.520 0.520 0.511 0.522 0.525 0.589 0.532 0.551 0.610 0.588 0.600 0.567 0.632 0.503 0.482 0.492 0.522 0.512 0.515 0.505 0. A.3 Details of dense hand contact labels Following the previous works on dense human-scene contact estimation [17, 21, 40], we employ distance-based thresholding to generate ground-truth dense hand contact labels. This is implemented using the Trimesh library [7]. Given ground-truth hand mesh and the mesh of the interacting entity (i.e., an object for hand-object interaction, another hand for hand-hand interaction, face for hand-face interaction, the environment for hand-scene interaction, or the body for hand-body interaction), we construct proximity query from the interacting mesh and compute the closest surface point on it for each vertex of the hand mesh. Contact is then determined by thresholding the Euclidean distance between each hand vertex and its corresponding closest point. The resulting contact labels are binary values per vertex, indicating whether each vertex is in contact. To set these thresholds, we manually inspected the ground-truth meshes and the resulting contact labels produced under fixed threshold values. We observed that the accuracy of 3D annotations varies significantly across datasets, making single threshold unsuitable for all cases. uniform threshold often led to incorrect labeling and semantically implausible contact regions. Based on our analysis, we set the contact distance threshold to 1 cm for the following datasets: ObMan [18], DexYCB [5], HO3D [14], H2O3D [15], ARCTIC [11], HOI4D [34], H2O [23], and PROX [16]. For the MOW [4] dataset, we used threshold of 3.5 cm due to its lower mesh fidelity. For InterHand2.6M [38] and HIC [52], we adopted smaller threshold of 0.5 cm to better capture fine-grained hand-hand interactions. Although most datasets produced reasonable contact labels with 1 cm threshold, MOW required larger value of 3.5 cm due to coarse mesh annotations generated through optimization on in-the-wild 2 Figure A1: More examples of class imbalance between contact and non-contact on H2O [23], H2O3D [15], HIC [52], MOW [4], ObMan [18], PROX [16], RICH [21], Hi4D [56]. images. For hand-hand interaction datasets, the 1 cm threshold, which is approximately the width of finger, often resulted in inaccurate and semantically implausible contact labels. To address this, we adopted smaller threshold of 0.5 cm. For datasets with existing annotated contact labels, such as RICH [21], Decaf [48], and Hi4D [56], we directly used the provided ground-truth annotations. A.4 More examples of class imbalance issue Figure A1 presents the class imbalance issue across datasets [23, 15, 52, 4, 18, 16, 21, 56] omitted from the main paper due to space constraints. In general, all datasets exhibit severe class imbalance, with the majority of samples corresponding to non-contact regions. Notably, some datasets such as H2O and HIC display highly skewed ratios of 10.57:1 and 58.41:1, respectively, between non-contact and contact samples. It is important to note that contact and non-contact counts are computed at the vertex level rather than per hand instance. While addressing class imbalance may appear straightforward, the sampling process must not only balance class proportions but also preserve the diversity of contact configurations present in the dataset. To tackle this challenge, we introduce Balanced Contact Sampling (BCS), which constructs multiple sampling groups that fairly represent both contact and non-contact. Please refer to the main paper for further details on the BCS. A.5 More examples of spatial imbalance issue We visualize the dataset-wide mean contact maps for all dense hand contact datasets used in training HACO in Figure A2 and Figure A3. Among hand-object interaction datasets, DexYCB [5], HO3D [14], H2O3D [15], ARCTIC [11], HOI4D [34], and H2O [23] exhibit significant spatial imbalance, with ground-truth contact concentrated predominantly on the fingertips. ObMan [18] and MOW [4] display more promising contact distributions, including high contact probability on the hypothenar region. However, ObMan is synthetic dataset with domain gap towards real images, and MOW, while showing diverse palmar contact patterns, lacks dorsal contact and contains fewer than 1,000 samples, making it insufficient for large-scale training. Among hand-hand interaction datasets, HIC [52] shows severe spatial imbalance, with contact limited to small fingertip regions and moderate values between fingers. In contrast, InterHand2.6M [38] offers more favorable contact distribution, covering nearly all hand regions except for small area on the dorsum. Hand-scene interaction datasets [16, 21] tend to involve large contact areas, primarily on the palmar side. However, interactions are often limited to flat surfaces such as the ground or walls, resulting in low diversity of contact types despite broad coverage. The hand-face interaction dataset Decaf [48] also exhibits poor spatial diversity due to its limited action types, such as poking or touching the chin. For hand-body interactions, Hi4D [56] includes extensive contact regions, but most interactions involve full palmar contact during actions such as hugging or patting. As visualized in the dataset-wide mean contact heatmaps, spatial imbalance is prevalent issue across existing dense hand contact datasets. To address this, we propose the vertex-level class-balanced (VCB) loss, which mitigates spatial imbalance by reweighting the loss at each vertex based on its mean contact frequency across the dataset. Please refer to the main paper for further details on the VCB loss. Table A2: Comparison of various backbone models on MOW [4] dataset. Model HACO HACO HACO HACO HACO Backbone ViT-H [10] FPN [29] ViT-L [10] ViT-B [10] ViT-S [10] HACO HRNet-W48 [50] HACO HRNet-W32 [50] HACO ResNet-152 [19] HACO ResNet-101 [19] ResNet-50 [19] HACO ResNet-34 [19] HACO ResNet-18 [19] HACO Pretrained Precision Recall F1-Score HaMeR [44] HandOccNet [41] ImageNet [8] ImageNet [8] ImageNet [8] ImageNet [8] ImageNet [8] ImageNet [8] ImageNet [8] ImageNet [8] ImageNet [8] ImageNet [8] 0.525 0. 0.513 0.498 0.519 0.494 0.492 0.516 0.503 0.508 0.494 0.496 0.632 0.551 0.616 0.666 0.571 0.587 0. 0.604 0.589 0.596 0.616 0.598 0.531 0.473 0.519 0.526 0.495 0.491 0.486 0.509 0.499 0.493 0.493 0.485 A.6 Quantitative results with different backbones We evaluate the performance of HACO with various backbone architectures, keeping the rest of the pipeline fixed. As shown in Table A2, our original model using ViT-H [10] pretrained on HaMeR [44] achieves the highest F1-score, demonstrating the strongest overall performance. ViTbased backbones consistently outperform convolutional alternatives, with ViT-B achieving the highest recall and ViT-S maintaining strong precision. ViT-L also performs competitively, achieving strong balance between precision and recall. This highlights the effectiveness of Transformer [53]-based architectures for dense contact estimation, likely due to their global receptive fields and capacity to model long-range dependencies. To assess the benefit of pretraining on related tasks, we additionally train and evaluate variant with FPN [29] backbone pretrained with HandOccNet [41], 3D hand mesh reconstruction model. This configuration yields the lowest F1-score among all backbones, suggesting that reconstruction-specific pretraining alone is insufficient for accurate hand contact prediction. HRNet models, widely used in 3D human mesh reconstruction [27, 28, 26, 57, 25], achieve moderate performance in our setting, with F1-scores below 0.5 despite their high-resolution design. ResNet backbones [19] exhibit wide performance range, with deeper variants such as ResNet-152 performing relatively well, while lighter variants such as ResNet-18 offer improved efficiency at the cost of reduced accuracy. Overall, these results indicate that the performance gains of our final HACO model arise from the combination of the strong representational capacity of the ViT backbone and task-specific knowledge transferred from 3D hand mesh reconstruction. All model variants will be publicly released. A.7 Computational requirements Table A3 reports the computational requirements of HACO with various backbone configurations. The original HACO model, listed in the first row, shows the highest computational cost, particularly in training memory, which exceeds 26GB. This is primarily due to the memory overhead associated with storing gradients while fine-tuning the ViT-H [10] backbone pretrained on HaMeR [44]. To support broader adoption in downstream tasks such as 3D hand grasp generation [13] and hand-object reconstruction [33], we provide additional HACO variants with reduced computational demand. We first evaluate the FPN [29] backbone from HandOccNet [41]. Although FPN is known to be computationally expensive due to its multi-scale feature hierarchy, it remains significantly more efficient than ViT-H. We also test lighter ViT variants such as ViT-B and ViT-S, which offer favorable trade-offs by maintaining moderate memory usage while achieving inference speeds exceeding 60 fps. We further evaluate HRNet [50] backbones, including HRNet-W48 and HRNet-W32. Despite their moderate memory consumption, both models exhibit low inference speeds, falling below 20 fps. Finally, we include ResNet-based [19] variants ranging from lightweight models such as ResNet-18 4 to deeper configurations such as ResNet-152. These models generally show low memory usage, fewer parameters, and high runtime efficiency. In particular, ResNet-18 achieves an inference speed exceeding 100 fps while maintaining computational cost under 3 GFLOPs. Table A3: Computational requirements of various backbone configurations. Backbone Train Memory (MB) Model Memory (MB) Params. (M) Speed (fps) GFLOPs Model HACO HACO HACO HACO HACO ViT-H [54] FPN [29] ViT-L [54] ViT-B [54] ViT-S [54] HACO HRNet-W48 [50] HACO HRNet-W32 [50] HACO ResNet-152 [19] HACO ResNet-101 [19] HACO ResNet-50 [19] HACO ResNet-34 [19] HACO ResNet-18 [19] A.8 More qualitative results 26,157 6,887 16,649 7,767 4, 11,296 9,026 9,252 7,471 5,919 4,163 3,785 2,629 230 1,305 469 218 468 334 395 337 264 219 671.10 59.56 341.91 122.84 56.34 122.37 86.13 103.05 87.40 68.41 56.75 46.64 54.26 66.71 65.79 96.80 86. 10.52 17.14 29.84 41.14 73.25 80.85 111.23 125.63 9.94 62.87 18.54 5.10 23.48 12.55 15.93 11.07 6.21 5.03 2. Additional qualitative comparisons of dense hand contact estimation between HACO and prior methods, including POSA [17], BSTRO [21], and DECO [51], are shown in Figures A4, A5, and A6, which correspond to the MOW [4], Hi4D [56], and both HIC [52] and RICH [21] datasets, respectively. Overall, HACO consistently outperforms existing approaches by substantial margin. Unlike prior methods that frequently fail to predict contact even when the hand is clearly interacting, HACO reliably captures accurate and dense hand contact. Among the baselines, DECO demonstrates stronger performance than POSA and BSTRO, likely as result of being trained on in-the-wild datasets with more diverse contact distributions. However, it remains to be affected by class imbalance, resulting in high rate of false negatives. BSTRO is more severely affected by this imbalance, frequently predicting no contact across the entire hand and producing overly coarse contact regions concentrated in the palmar area. This behavior likely stems from its reliance on human-scene interaction datasets during training. POSA also exhibits frequent false negatives and fails to detect contact even in visibly interacting regions, likely due to its dependence on pose prior rather than direct contact supervision with visual input. A.9 Limitations and societal impacts Limitations. Our HACO supports dense hand contact estimation across wide range of interaction types, including hand-object, hand-hand, hand-face, hand-scene, and hand-body interactions. However, hands frequently engage in self-contact under certain poses (e.g., contact between the thumb and index finger in an \"okay\" gesture) or during whole-body motions [39]. We intentionally exclude self-contact cases, as including them could negatively impact downstream applications. For example, ContactOpt [13] may incorrectly optimize grasps between the hand and an object if self-contact is misinterpreted as external contact. As result, HACO currently does not support dense hand contact estimation in self-contact scenarios. Nevertheless, we consider modeling self-contact promising direction for future research, particularly for applications in the metaverse (e.g., AR/VR) and action recognition. Societal impacts. The proposed method has broad potential for applications involving hand interaction analysis, including AR/VR, robotics, and behavioral understanding. However, given the inherently human-centered nature of the task, there is risk of misuse in areas such as mass surveillance or privacy infringement. We strongly discourage the deployment of this system in applications that may compromise human rights or personal privacy, and urge practitioners to consider ethical implications when applying HACO to downstream tasks. 5 Figure A2: Dataset-wide dense hand contact mean. These heatmaps show average hand contact of ground-truths from ObMan [18], DexYCB [5], MOW [4], HO3D [14], H2O3D [15], ARCTIC [11], HOI4D [34] dataset. 6 Figure A3: Dataset-wide dense hand contact mean. These heatmaps show average hand contact of ground-truths from H2O [23], InterHand2.6M [38], HIC [52], PROX [16], RICH [21], Decaf [48], Hi4D [56] dataset. 7 Figure A4: Qualitative comparison of dense hand contact estimation with POSA [17], BSTRO [21], and DECO [51] on MOW [4] dataset. We highlight exemplar regions where HACO outperform previous methods. Note that we only predict right hand contact. 8 Figure A5: Qualitative comparison of dense hand contact estimation with POSA [17], BSTRO [21], and DECO [51] on Hi4D [56] dataset. We highlight exemplar regions where HACO outperform previous methods. Note that we only predict right hand contact. 9 Figure A6: Qualitative comparison of dense hand contact estimation with POSA [17], BSTRO [21], and DECO [51] on HIC [52] and RICH [21] dataset. We highlight exemplar regions where HACO outperform previous methods. Note that we only predict right hand contact."
        },
        {
            "title": "References",
            "content": "[1] Samarth Brahmbhatt, Cusuh Ham, Charles Kemp, and James Hays. ContactDB: Analyzing and predicting grasp contact via thermal imaging. In CVPR, 2019. [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020. [3] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. In NeurIPS, 2019. [4] Zhe Cao, Ilija Radosavovic, Angjoo Kanazawa, and Jitendra Malik. Reconstructing hand-object interactions in the wild. In ICCV, 2021. [5] Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov, Ankur Handa, Jonathan Tremblay, Yashraj Narang, Karl Van Wyk, Umar Iqbal, Stan Birchfield, et al. DexYCB: benchmark for capturing hand grasping of objects. In CVPR, 2021. [6] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In CVPR, 2019. [7] Dawson-Haggerty et al. Trimesh. [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In CVPR, 2009. [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional Transformers for language understanding. In NAACL, 2019. [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [11] Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed Kocabas, Manuel Kaufmann, Michael Black, and Otmar Hilliges. ARCTIC: dataset for dexterous bimanual hand-object manipulation. In CVPR, 2023. [12] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios Tzionas, and Michael Black. Collaborative regression of expressive bodies using moderation. In 3DV, 2021. [13] Patrick Grady, Chengcheng Tang, Christopher Twigg, Minh Vo, Samarth Brahmbhatt, and Charles Kemp. ContactOpt: Optimizing contact to improve grasps. In CVPR, 2021. [14] Shreyas Hampali, Mahdi Rad, Markus Oberweger, and Vincent Lepetit. HOnnotate: method for 3D annotation of hand and object poses. In CVPR, 2020. [15] Shreyas Hampali, Sayan Deb Sarkar, Mahdi Rad, and Vincent Lepetit. Keypoint Transformer: Solving joint identification in challenging hands and object interactions for accurate 3D pose estimation. In CVPR, 2022. [16] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and Michael Black. Resolving 3D human pose ambiguities with 3D scene constraints. In ICCV, 2019. [17] Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios Tzionas, and Michael Black. Populating 3D scenes by learning human-scene interaction. In CVPR, 2021. [18] Yana Hasson, Gul Varol, Dimitrios Tzionas, Igor Kalevatykh, Michael Black, Ivan Laptev, and Cordelia Schmid. Learning joint reconstruction of hands and manipulated objects. In CVPR, 2019. [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 11 [20] Junxing Hu, Hongwen Zhang, Zerui Chen, Mengcheng Li, Yunlong Wang, Yebin Liu, and Zhenan Sun. Learning explicit contact for implicit reconstruction of hand-held objects from monocular images. In AAAI, 2024. [21] Chun-Hao Huang, Hongwei Yi, Markus Höschle, Matvey Safroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel Scharstein, and Michael Black. Capturing and inferring dense full-body human-scene contact. In CVPR, 2022. [22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment Anything. In ICCV, 2023. [23] Taein Kwon, Bugra Tekin, Jan Stühmer, Federica Bogo, and Marc Pollefeys. H2O: Two hands manipulating objects for first person interaction recognition. In ICCV, 2021. [24] Zhaoqi Leng, Mingxing Tan, Chenxi Liu, Ekin Dogus Cubuk, Jay Shi, Shuyang Cheng, and Dragomir Anguelov. PolyLoss: polynomial expansion perspective of classification loss functions. In ICLR, 2022. [25] Jiefeng Li, Siyuan Bian, Chao Xu, Zhicun Chen, Lixin Yang, and Cewu Lu. HybrIK-X: Hybrid analytical-neural inverse kinematics for whole-body mesh recovery. TPAMI, 2025. [26] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu, and Youliang Yan. CLIFF: Carrying location information in full frames into human pose and shape estimation. In ECCV, 2022. [27] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end human pose and mesh reconstruction with Transformers. In CVPR, 2021. [28] Kevin Lin, Lijuan Wang, and Zicheng Liu. Mesh graphormer. In ICCV, 2021. [29] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, 2017. [30] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In ICCV, 2017. [31] Zhifeng Lin, Changxing Ding, Huan Yao, Zengsheng Kuang, and Shaoli Huang. Harmonious feature learning for interactive hand-object pose estimation. In CVPR, 2023. [32] Shaowei Liu, Yang Zhou, Jimei Yang, Saurabh Gupta, and Shenlong Wang. ContactGen: Generative contact modeling for grasp generation. In ICCV, 2023. [33] Yumeng Liu, Xiaoxiao Long, Zemin Yang, Yuan Liu, Marc Habermann, Christian Theobalt, Yuexin Ma, and Wenping Wang. EasyHOI: Unleashing the power of large models for reconstructing hand-object interactions in the wild. In CVPR, 2025. [34] Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, and Li Yi. HOI4D: 4D egocentric dataset for category-level human-object interaction. In CVPR, 2022. [35] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. SMPL: skinned multi-person linear model. ACM TOG, 2015. [36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. [37] Gyeongsik Moon, Shunsuke Saito, Weipeng Xu, Rohan Joshi, Julia Buffalini, Harley Bellan, Nicholas Rosen, Jesse Richardson, Mallorie Mize, Philippe De Bree, et al. dataset of relighted 3D interacting hands. In NeurIPS, 2024. [38] Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori, and Kyoung Mu Lee. InterHand2.6M: dataset and baseline for 3D interacting hand pose estimation from single RGB image. In ECCV, 2020. [39] Lea Muller, Ahmed AA Osman, Siyu Tang, Chun-Hao Huang, and Michael Black. On self-contact and human pose. In CVPR, 2021. 12 [40] Hyeongjin Nam, Daniel Sungho Jung, Gyeongsik Moon, and Kyoung Mu Lee. Joint reconstruction of 3D human and object via contact-based refinement Transformer. In CVPR, 2024. [41] JoonKyu Park, Yeonguk Oh, Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee. HandOccNet: Occlusion-robust 3D hand mesh estimation network. In CVPR, 2022. [42] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. [43] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael Black. Expressive body capture: 3D hands, face, and body from single image. In CVPR, 2019. [44] Georgios Pavlakos, Dandan Shan, Ilija Radosavovic, Angjoo Kanazawa, David Fouhey, and Jitendra Malik. Reconstructing hands in 3D with Transformers. In CVPR, 2024. [45] Tal Ridnik, Emanuel Ben-Baruch, Nadav Zamir, Asaf Noy, Itamar Friedman, Matan Protter, and Lihi Zelnik-Manor. Asymmetric loss for multi-label classification. In ICCV, 2021. [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [47] Javier Romero, Dimitrios Tzionas, and Michael Black. Embodied hands: Modeling and capturing hands and bodies together. ACM TOG, 2017. [48] Soshi Shimada, Vladislav Golyanik, Patrick Pérez, and Christian Theobalt. Decaf: Monocular deformation capture for face and hand interactions. ACM TOG, 2023. [49] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In NeurIPS, 2015. [50] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation. In CVPR, 2019. [51] Shashank Tripathi, Agniv Chatterjee, Jean-Claude Passy, Hongwei Yi, Dimitrios Tzionas, and Michael Black. DECO: Dense estimation of 3D human-scene contact in the wild. In ICCV, 2023. [52] Dimitrios Tzionas, Luca Ballan, Abhilash Srikantha, Pablo Aponte, Marc Pollefeys, and Juergen Gall. Capturing hands in action using discriminative salient points and physics simulation. IJCV, 2016. [53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. [54] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. ViTPose: Simple Vision Transformer baselines for human pose estimation. In NeurIPS, 2022. [55] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth Anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024. [56] Yifei Yin, Chen Guo, Manuel Kaufmann, Juan Jose Zarate, Jie Song, and Otmar Hilliges. Hi4D: 4D instance segmentation of close human interaction. In CVPR, 2023. [57] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng Li, Liang An, Zhenan Sun, and Yebin Liu. PyMAF-X: Towards well-aligned full-body model regression from monocular images. TPAMI, 2023."
        }
    ],
    "affiliations": [
        "IPAI, Dept. of ECE & ASRI, Seoul National University, Korea"
    ]
}