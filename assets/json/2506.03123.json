{
    "paper_title": "DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video Generation",
    "authors": [
        "Zhengyao Lv",
        "Chenyang Si",
        "Tianlin Pan",
        "Zhaoxi Chen",
        "Kwan-Yee K. Wong",
        "Yu Qiao",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Models have achieved remarkable results in video synthesis but require iterative denoising steps, leading to substantial computational overhead. Consistency Models have made significant progress in accelerating diffusion models. However, directly applying them to video diffusion models often results in severe degradation of temporal consistency and appearance details. In this paper, by analyzing the training dynamics of Consistency Models, we identify a key conflicting learning dynamics during the distillation process: there is a significant discrepancy in the optimization gradients and loss contributions across different timesteps. This discrepancy prevents the distilled student model from achieving an optimal state, leading to compromised temporal consistency and degraded appearance details. To address this issue, we propose a parameter-efficient \\textbf{Dual-Expert Consistency Model~(DCM)}, where a semantic expert focuses on learning semantic layout and motion, while a detail expert specializes in fine detail refinement. Furthermore, we introduce Temporal Coherence Loss to improve motion consistency for the semantic expert and apply GAN and Feature Matching Loss to enhance the synthesis quality of the detail expert.Our approach achieves state-of-the-art visual quality with significantly reduced sampling steps, demonstrating the effectiveness of expert specialization in video diffusion model distillation. Our code and models are available at \\href{https://github.com/Vchitect/DCM}{https://github.com/Vchitect/DCM}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 3 2 1 3 0 . 6 0 5 2 : r DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video Generation Zhengyao Lv2,3 Chenyang Si1 Tianlin Pan1,4 Zhaoxi Chen5 Kwan-Yee K. Wong2 Yu Qiao3 Ziwei Liu5 1Nanjing University 2The University of Hong Kong 3Shanghai Artificial Intelligence Laboratory 4University of Chinese Academy of Sciences cszy98@gmail.com chenyang.si@nju.edu.cn 5S-Lab, Nanyang Technological University pantianlin23@mails.ucas.ac.cn zhaoxi001@ntu.edu.sg kykwong@cs.hku.hk yu.qiao@siat.ac.cn ziwei.liu@ntu.edu.sg Figure 1. Comparison of visual results between our DCM (4 steps), the original HunyuanVideo, and other competing methods (left). Comparison of latency and VBench score across different methods (right). Latency is measured on two A100 GPUs under the video synthesis configuration of 129 frames at 1280 720 resolution."
        },
        {
            "title": "Abstract",
            "content": "Diffusion Models have achieved remarkable results in video synthesis but require iterative denoising steps, leading to substantial computational overhead. Consistency Models have made significant progress in accelerating diffusion models. However, directly applying them to video diffusion models often results in severe degradation of temporal consistency and appearance details. In this paper, by analyzing the training dynamics of Consistency Models, we identify key conflicting learning dynamics during the distillation process: there is significant discrepancy in the optimization gradients and loss contributions across different timesteps. This discrepancy prevents the distilled student model from achieving an optimal state, leading to compromised temporal consistency *Equal Contribution. Project Leader Corresponding Author. and degraded appearance details. To address this issue, we propose parameter-efficient Dual-Expert Consistency Model (DCM), where semantic expert focuses on learning semantic layout and motion, while detail expert specializes in fine detail refinement. Furthermore, we introduce Temporal Coherence Loss to improve motion consistency for the semantic expert and apply GAN and Feature Matching Loss to enhance the synthesis quality of the detail expert. Our approach achieves state-of-the-art visual quality with significantly reduced sampling steps, demonstrating the effectiveness of expert specialization in video diffusion model distillation. Our code and models are available at https://github.com/Vchitect/DCM. 1. Introduction Diffusion Models [8] have achieved remarkable progress in image and video synthesis [41, 44, 59]. However, they require multiple iterations to model the probability flow Ordinary Differential Equation (ODE) [48] and rely on increasingly large denoising networks, resulting in substantial computational overhead that limits their practicality in real-world applications. To mitigate this constraint, Consistency Distillation [50] has emerged as an efficient knowledge distillation framework to reduce the sampling timesteps. It leverages pretrained diffusion model as the teacher and trains student model to directly map any point along the ODE trajectory to the same solution, thereby ensuring the selfconsistency property. Despite enabling few-step sampling, it often struggles with visual quality, particularly in challenging video synthesis, leading to distorted layouts, unnatural motion, and degraded details. To ground this inherent issue, we analyzed the sampling dynamics of video diffusion models, as shown in Fig. 2 (a). Our key observations are that the differences between adjacent steps are substantial in the early stages of sampling, whereas the changes become more gradual in the later stages. This discrepancy arises because the early steps primarily focus on synthesizing semantic layout and motion, while the later steps emphasize refining fine details. These findings suggest that the student model may learn different patterns and exhibit distinct learning dynamics when trained on high-noise and low-noise samples. We visualized the magnitude and gradient of the consistency loss during the distillation process and observed significant differences between high and low noise levels, as shown in Fig. 2 (b). This variation indicates that jointly distilling single student model to capture both semantic layout and fine-detail synthesis may introduce optimization interference, potentially leading to suboptimal results. To validate this assumption, we trained two expert denoisers. We first divide the ODE trajectory of the pretrained model into two phases: the semantic synthesis phase and the detail refinement phase. We then train two distinct student expert denoisers, each responsible for fitting one of these sub-trajectories. During inference, we dynamically select the corresponding expert denoiser based on the noise level of samples to predict the next position in the ODE trajectory. The results demonstrate that the combination of the two student expert denoisers achieves better performance, thereby confirming the validity of our hypothesis. However, this straightforward baseline involves training two student models which is not efficient enough. To further enhance parameter efficiency, we analyze the parameter differences between the two expert denoisers and identify that the primary differences lie in 1) embedding layers where the input parameters include timesteps, and 2) the linear layers within the attention layers. Based on this insight, we propose parameter-efficient Dual-Expert Consistency Model (DCM). Specifically, we first train semantic expert denoiser on the semantic synthesis trajectory. We then freeze this expert and introduce new set of timestepdependent layers, incorporating LoRA [11] into the linear layers of the attention blocks. Subsequently, we fine-tune these newly added layers on the detail refinement trajectory. In this manner, we decouple the optimization of the two expert denoisers with minimal additional parameters and computational cost, achieving visual results comparable to those obtained with two separate experts. Given the differing training dynamics of the semantic and detail expert denoisers, we introduce distinct optimization objectives beyond the original consistency loss. To enforce temporal coherence in the semantic expert denoiser, we introduce Temporal Coherence Loss, which guides it to capture motion variations across frames. To enhance the fine-grained content synthesized by the detail expert denoiser, we introduce generative adversarial (GAN) [4] loss and incorporate Feature Matching loss. Specifically, we alternately optimize the student model and the discriminator in the feature space, encouraging the generator to synthesize visual content that aligns with the output distribution of the teacher model. The Feature Matching term enhances supervision over intermediate features, thereby stabilizing the GAN training. Our proposed DCM accelerates sampling while preserving both semantic and detail quality, as shown in Fig. 1. In summary, our contributions are as follows: We analyze the training dynamics of Consistency Models and identify key conflict in the distillation process: discrepancies in loss contributions and optimization gradients across noise levels hinder optimal learning, leading to suboptimal visual quality. We propose parameter-efficient Dual-Expert Consistency Model that decouples the expert denoisers distillation, mitigating the conflict and improving visual quality with minimal parameter and computational cost. To enhance visual quality, we introduce Temporal Coherence Loss for the semantic expert and GAN loss with Feature Matching term for the details expert, improving both temporal consistency and detail quality. 2. Related Work 2.1. Diffusion Models For Video Synthesis Video Diffusion Models have witnessed rapid advancements with diffusion models [2, 8, 9]. Building on the Diffusion Transformer (DiT) [41] pre-training, notable breakthrough is the development of high-fidelity video diffusion models [10, 1820, 38, 40, 51, 59]. However, scaling these models for long videos incurs significant training and inference costs. Recently, LTX-Video [7] designed Video-VAE that achieves high compression ratio for efficient self-attention. Pyramid flow [13] introduced unified pyramidal flow matching algorithm for efficient video genFigure 2. Visualization of the video synthesis process and the trend of loss variation. (a) In the early stages of sampling, the results change significantly and rapidly, whereas in the later stages, the changes become gradual and smooth. (b) During distillation, the loss and gradient norm of the student model exhibit significant differences between samples with high and low noise levels. erative modeling. Moreover, while efficient fast diffusion samplers [15, 25, 29, 30, 48] reduce inference steps, further reduction often severely degrades performance. Diffusion distillation offers promising way to further minimize sampling steps while maintaining the visual quality. 2.2. Diffusion Model Distillation Diffusion distillation [34] aims to distill knowledge from pre-trained diffusion models to student models, reducing inference cost. Prior works can be generally classified into two categories based on their distillation mechanisms. Trajectory-preserving distillation methods exploit the fact that diffusion models learn an Ordinary Differential Equation (ODE) trajectory and aim to predict the exact teacher output in fewer steps. Among the earliest studies on diffusion distillation, Luhman et al. [31] and DSNO [64] proposed training the student model using noise-image pairs precomputed by the teacher model with an ODE solver. Progressive distillation [39, 45] reduces the final number of sampling steps by iteratively applying the distillation process to halve the number of sampling steps of previous model. Instaflow [26, 27] progressively learns straighter flows, enabling accurate one-step predictions over larger distances. Consistency models [24, 28, 32, 33, 49, 50, 65], BOOT [5] and TRACT [1] learn to map samples along the ODE trajectory to another point to achieve selfconsistency. The consistency trajectory model [16] was designed to mitigate discretization inaccuracies and accumulated estimation errors in the multistep consistency model sampling. PCMs [53] phase the ODE trajectory into several sub-trajectories and only enforce the self-consistency property on each sub-trajectory, thus alleviating the limitations of CMs. Trajectory-preserving distillation enables stable optimization but can degrade visual quality, leading to blurriness or distortions when sampling with fewer steps. Distribution-matching distillation methods bypass the ODE trajectory and aim to train the student model to generate samples whose distribution aligns with that of the teacher diffusion model. Some methods reduce the distribution gap between the student and teacher models through adversarial training [3, 14, 23, 37, 46, 47, 57, 58]. Other methods [35, 36, 6062, 66] achieve diffusion distillation by score distillation [42]. Notably, DMD [61] aligns the one-step generator with the distribution of teacher model by minimizing an approximate KL divergence, whose gradient is the difference between the target and synthetic distribution score functions. Recently works [17, 22, 43] have also tried to integrate the advantages of trajectory-preserving and distribution-matching methods. Hyper-SD [43] introduced the trajectory-segmented consistency distillation and used DMD [61] for one-step generation enhancement. Previous works have primarily focused on distilling image synthesis diffusion models, with some efforts [21, 54, 63] extending to the distillation of small-scale video synthesis models [6, 55]. However, these methods are limited to synthesizing low-resolution and short-sequence Seaweed-APT [23] proposed adversarial postvideos. tuning against real data following diffusion pre-training for one-step high-resolution 2-second duration video generation. recent work [62] extended DMD [61] for video synthesis in four sampling steps. Due to the inherent complexity of video synthesis and the increasing model scale, research on diffusion distillation for video synthesis remains limited, and its performance is yet to be fully explored. 3. Methodology 3.1. Preliminary Diffusion Model is generative framework with forward and reverse process. In the forward process, noise is progressively added to clean data x0 pdata(x0), degrading the signal: q(xtx0) = (xt; αtx0, 1 αtI), (1) where {αt}T t=1 controls the noise schedule. The reverse process, typically parameterized by UNet or transformer ϵθ, is trained to predict the noise: LDM = Ex,ϵN (0,1),t (cid:2)ϵ ϵθ(xt, t)2 2 (cid:3). (2) Figure 3. Comparison of the visual quality of denoiser variants trained at different noise level samples. By optimizing two expert denoisers to decouple the distillation process into semantic learning and detail learning, and combining them during inference, we achieve the best quantitative and qualitative visual results. During inference, clean sample x0 can be recovered through iterative denoising: p(xt1xt) = (xt1; µθ(xt, t), Σθ(xt, t)), (3) where µθ and Σθ are learned parameters. Consistency Distillation utilizes pre-trained model ϵθ as the teacher FT to distill its knowledge into student model FS initialized with ϵθ, allowing for faster sampling with fewer steps [50]. Specifically, consistency distillation trains the student model FS to directly map any point xtn on the solution trajectory of the ODE solver Φ to its endpoint xtend . The learning objective can be formulated as: LCD = Ex,tn Φ(xtn , FS(xtn , tn, c), tend) (ˆxtn1, tn1, c), tend)2 2. Φ(ˆxtn1, (4) Here, is the exponential moving average (EMA) of FS and ˆxtn1 is the next point on the ODE solution trajectory computed by the teacher model FT : ˆxtn1 = Φ(xtn , FT (xtn, tn, c), tn1). (5) Consistency distillation has garnered widespread attention and research [32, 43, 53] due to its ease of stable training. 3.2. Suboptimal Solution in Consistency Distillation Although consistency distillation has demonstrated promising results in class-conditioned image synthesis and text-toimage model distillation, it falls short in more challenging video synthesis diffusion models, where issues arise such as distorted layouts, unnatural motion, and degraded details. Due to the limited capacity of the student model, consistency distillation struggles to address these issues simultaneously. By tracking the video synthesis process, we find that in the early stages of sampling, the sampling results vary significantly and rapidly, whereas in the later stages, the transitions become more gradual and smooth, as shown in Fig. 2 (a). In the early stages of sampling, rapid changes establish the semantic layout and motion of the video, while in the later stages, the model gradually refines the details with smooth adjustments. These findings imply that the student model may learn different patterns with distinct training dynamics when distilled with high-noise and low-noise samples. By visualizing the trends of consistency loss and gradient norm during distillation, we find significant differences in the student model when distilled with highand low-noise samples, as shown in Fig. 2 (b). This suggests that jointly optimizing student model for both semantic and fine-grained details synthesis may introduce inefficient optimization, constraining its fitting capacity and leading to suboptimal performance. To validate our hypothesis, we conducted an experiment on the HunyuanVideo [18] text-to-video diffusion model. Specifically, we divided the ODE solution trajectory (xN , xN 1, ..., x1, x0) of the pre-trained model into two sub-trajectories, using tκ as the boundary (we set = 50 and κ = 37 by default). The first part ({xti}N i=κ) primarily focuses on synthesizing the semantic layout and motion, while the second part ({xtj }κ j=0) emphasizes semantic refinement and high-quality detail generation. As shown in Fig 3 (a), we optimized two distinct student models, semantic expert denoiser FSemE and details expert denoiser FDetE, Figure 4. The training process of DCM consists of two stages. In the semantic learning stage, we train SemE on high-noise samples with consistency loss and temporal coherence loss as the learning objectives. In the detail learning stage, we initialize DetE with the weights of SemE and introduce set of time-dependent layers and LoRA. DetE is then trained on low-noise samples, where only the newly added layers and LoRA are updated. The learning objectives in this stage include consistency loss, GAN loss, and Feature Matching loss. learned to model semantics and details, each outperforming VCM in their respective aspects. This validates our hypothesis, demonstrating that decoupled optimization is superior to jointly training single model for both tasks. 3.3. Parameter-efficient Dual-Expert Distillation While training two expert denoisers improves video quality, it significantly increases model parameters and GPU memory consumption during inference. Through the analysis of parameter similarity between the two expert denoisers, we found that the primary differences in model parameters lie in the embedding layers Ψ where the input parameters include timesteps and the linear layers within the attention layers Λ, as illustrated in Fig. 5. Based on the above observations, we propose the parameter-efficient Dual-Expert distillation strategy, as illustrated in Fig. 4. Specifically, our training scheme is divided into two stages. 1) Initialize the semantic expert denoiser FSemE with the teacher model FT and optimize all its parameters on the sub-trajectory {xti}N i=κ. 2) Use the optimized FSemE as the initialization of FDetE and freeze it. Then add new set of timestep-dependent embedding layers Ψ and LoRA [11] Λ of attention blocks. Optimize the newly added parameters (Ψ and Λ) on its sub-trajectory {xtj }κ j=0. In this way, we significantly reduce the number of parameters required for decoupling the optimization process of semantic modeling and detail learning, with minimal computational cost, while maintaining the visual quality of the synthesized videos. Figure 5. Weight difference distribution between expert denoisers. We employ the normalized L1 distance to quantify the difference between the weights. to fit each sub-trajectory: LSemE = Ex,tm[tκ,tN ]Φ(xtm , FSemE(xtm, tm, c), tκ) SemE(ˆxtm1 , tm1, c), tκ)2 2, (6) Φ(ˆxtm1, LDetE = Ex,tn[t0,tκ]Φ(xtn , FDetE(xtn , tn, c), t0) DetE(ˆxtn1, tn1, c), t0)2 2. Φ(ˆxtn1, (7) Expert denoiser FSemE is optimized to synthesize coherent semantic layouts and motion, while expert denoiser FDetE learns to generate high-quality details. During inference, we dynamically switch the expert denoiser based on the sampling stage. To assess the impact of each expert denoiser and the effectiveness of decoupled training, we evaluate four variants: a) VCM: The vanilla consistency model is used throughout the sampling process. b) SemE + VCM: SemE is applied in the first sub-trajectory, transitioning to VCM in the second. c) VCM + DetE: VCM is applied initially, followed by DetE. d) SemE + DetE: SemE and DetE are integrated for the sampling process. According to the Fig. 3 (b) and (c), SemE and DetE have respectively 3.4. Expert-specific Optimization Objective In addition to the consistency objectives mentioned in Eq. 6 and Eq. 7, we also designed expert-specific optimization objectives for the semantic expert denoiser FSemE and details expert denoiser FDetE, as shown in Fig. 4. Temporal Coherence Loss To enhance the temporal coherence in the video synthesized by the semantic expert denoiser FSemE, we introduce the Temporal Coherence Loss LT C, which emphasizes and guides the FSemE to focus on the variations and motion at corresponding positions among different frames: xtκ = Φ(xtm , FSemE(xtm , tm, c), tκ), ˆxtκ = Φ(ˆxtm1 , l:L xtκ LT = (xtκ SemE(ˆxtm1, tm1, c), tκ), 0:Ll) (ˆxtκ 0:Ll)2 2. l:L ˆxtκ (8) Here xtκ l:L represents the video latents from the l-th to the L-th channel along the temporal axis. This temporal coherence loss encourages the semantic expert denoiser FSemE to preserve consistent motion and spatial relationships between frames, ensuring more fluid videos synthesis. Generative Adversarial Loss The effectiveness of the generative adversarial (GAN) [4] loss in high-quality detail synthesis has been validated in many distribution-matching distillation methods. We introduce the GAN loss into the training of the details expert denoiser and incorporate Feature Matching loss to stabilize the training. We first obtain xt0 and ˆxt0 with the details expert denoiser FDetE, teacher model FT and ODE solver Φ: HunyuanVideo has 13 billion parameters, and CogVideoX has 2 billion parameters. Since most prior distillation methods for diffusion models have not been applied to video synthesis, we follow the official implementations of LCM [32] and PCM [53] to implement these two methods on the selected base models as baselines for comparison. Implementation Details For HunyuanVideo, we selected trajectories with 50 Euler steps and used the default sampling parameters from diffusers. The distillation was conducted on 129-frame video sequences at resolution of 1280 720 with batch size of 6. For the semantic expert denoiser, we performed 1000 iterations of distillation with learning rate of 1e 6, while for the details expert denoiser, we trained for 1000 iterations with learning rate of 5e 6. For CogVideoX-2B, we selected trajectories with 50 DDIM steps. The distillation was conducted on 29-frame video sequences at resolution of 720 480 with batch size of 4. In the first-stage fine-tuning, we distilled for approximately 1000 steps, while in the second-stage fine-tuning, we distilled for around 500 steps, both with learning rate of 1e 6. All experiments were conducted on 24 NVIDIA A100 80GB GPUs. Evaluation Metrics For video quality evaluation, we use VBench [12] as our assessment metric. VBench is comprehensive benchmark suite for video generative models, designed to align closely with human perception and offer valuable insights from multiple perspectives. Additionally, we conducted user study to help evaluate the visual quality of the generated videos. xt0 = Φ(xtn, FDetE(xtn, tn, c), t0), ˆxt0 = Φ(ˆxtn1, DetE(ˆxtn1 , tn1, c), t0), (9) 4.2. Main Results Then we perform the forward process and apply noise to them to obtain fake sample xf ake and real sample xreal with Eq. 1. We use frozen teacher model as the feature extraction backbone Ω, extracting intermediate features with fixed stride for calculating the GAN loss and Feature Matching loss LF . During training, we iteratively update the parameters of FDetE and the discriminator head fD: LF = Ex,tn Ω(xf ake) Ω(xreal)2 2 , LG = Ex,tn [1 fD(Ω(xf ake))] + LF , LD = Ex,tn [fD(Ω(xf ake))] + Ex,tn[1 fD(Ω(xreal))]. (10) The integration of the GAN loss in combination with Feature Matching loss provides robust framework for training the details expert denoiser FDetE, stabilizing its learning process and improving the quality of detail synthesis. 4. Experiments 4.1. Experimental Setup Backbones and Baselines We utilize HunyuanVideo [18] and CogVideoX [59] as the base models for distillation. The Quantitative Comparison Table 1 presents the quantitative comparison of our method with LCM and PCM on HunyuanVideo and CogVideoX. We generate videos using the prompts provided by VBench to evaluate their performance in terms of semantic alignment and visual quality. It can be observed that on HunyuanVideo, our method achieves VBench score comparable to the baseline with 4step sampling, significantly outperforming LCM and PCM. In terms of efficiency, our method incurs nearly identical latency cost per inference step compared to LCM and PCM. Qualitative Comparison Fig. 6 presents comparison of the videos generated by our method and those produced by the original model, LCM and PCM. The results demonstrate that our method maintains high semantic and detail quality in synthesized videos while reducing the number of inference steps. Additional qualitative results are provided in the supplementary material for further reference. User Study To further evaluate the effectiveness of our method, we conduct human evaluation to assess the perceived visual quality of the generated videos. Specifically, we randomly select 30 videos for each model. During the Figure 6. Visual quality comparison of different methods. Differences are highlighted in boxes. evaluation, each rater is presented with text prompt along with two videos generated by different distillation methods, displayed in randomized order to eliminate bias. Following the protocol of human preference evaluation in HunyuanVideo [18], the professional raters are asked to choose the video they perceive to have superior text alignment, motion quality, and visual quality. Each sample is evaluated by fifty independent raters, and the aggregated voting results are summarized in Table 2. As one can see, compared to other distillation methods, the raters significantly prefer the videos generated by our method. 4.3. Ablation Study To thoroughly evaluate both the effectiveness of our method, we conduct extensive ablation studies based on HunyuanVideo, as shown in Table 3. All experiments were conducted on 29-frame videos with resolution of 1280 720. Inference is performed with 4 sampling timesteps. Figure 7. efficient distillation. Impact of optimization decoupling and parameterEffect of Optimization Decoupling (OD) Through Experiments (1) and (2), we observe that decoupling the optiTable 1. Comparison of efficiency and visual quality of different methods. The latency of HunyuanVideo was measured on two A100 GPUs, and that of CogVideoX on single A100 GPU."
        },
        {
            "title": "Step",
            "content": "Lat.(Sec.)"
        },
        {
            "title": "Hunyuan\nLCM\nPCM\nOurs\nLCM\nPCM\nOurs\nCogVideoX\nLCM\nPCM\nOurs\nLCM\nPCM\nOurs",
            "content": "50 4 4 4 8 8 8 50 4 4 4 8 8 8 1504.5 120.68 120.89 121.52 242.80 242.96 244.72 76.50 3.22 3.23 3.31 6.42 6.42 6.58 VBench Total Quality Semantic 83.87 85.00 79.34 80.33 80.83 78.32 80.93 81.94 76.90 83.83 85.12 78.67 81.49 82.35 78.03 81.63 82.78 77.00 83.86 85.00 79.32 80.59 81.93 75.23 78.88 80.07 74.12 79.09 80.33 74.14 79.99 81.35 74.56 79.34 80.64 74.21 79.70 80.98 74.60 80.26 81.57 75.03 Table 2. User preference study. The numbers represent the percentage of raters who favor the videos synthesized by our method. Figure 8. Impact of temporal coherence loss. Figure 9. Impact of the GAN loss and Feature Matching term. Method comparison HunyuanVideo CogVideoX Ours vs. LCM Ours vs. PCM 82.67% 77.33% 75.33% 72.67% Table 3. Impact of different components of our method."
        },
        {
            "title": "Variants\nTC",
            "content": "PE OD GF (1) (2) (3) (4) (5) (6) VBench Total Quality Semantic 80.30 80.74 78.36 83.08 84.20 78.59 83.03 84.16 78.53 83.42 84.63 78.63 83.71 84.99 78.59 83.80 85.10 78.62 mization of semantic and detail modeling significantly improves the semantic and quality scores of the synthesized videos. As shown in Fig. 7, the optimized decoupled model synthesizes videos with better semantic and detail quality, where the motion of characters and facial details appear more natural. Effect of Parameter-Efficient dual-expert distillation (PE) Through Experiments (2) and (3), we observe that compared to simply decoupling the optimization into two separate model training processes, the parameter-efficient Dual-Expert distillation significantly reduces both the parameters and memory requirements, with minimal computational overhead, while preserving visual quality. The last two rows of Fig. 7 also demonstrate that our parameterefficient Dual-Expert method does not result in significant degradation in visual quality. Effect of Temporal Coherence Loss (TC) By comparing Figure 10. Impact of different κ. Experiments (3) and (4), or (5) and (6), we observe that the introduction of Temporal Coherence loss improves the quality scores of the synthesized videos. As shown in Fig. 8, the introduction of the TC loss enables more natural motion in the video and enhances its consistency. Effect of GAN and Feature Matching Loss (GF) By comparing Experiments (3) and (5), or (4) and (6), we observe that the introduction of GAN Loss improves the quality scores of the synthesized videos. As shown in Fig. 9, the introduction of the GAN loss and Feature Matching term enhances the realism of the details in the synthesized video. Selection of κ In this paper, we determine the value of κ based on the inference process. Fig. 10 (left) illustrates the L1 distance between adjacent time-step sampling results in HunyuanVideo during the sampling process. It can be observed that from approximately step 37 onward, the L1 Distance decreases to very small value. We interpret this as the point where the semantic content and layout are established, and the remaining steps focus on synthesizing highfrequency details. Therefore, we set κ = 37 as the default value. To evaluate the impact of different κ values, we experimented with κ = 28, 35, 37, 39, 46. As shown in Fig. 10 (right), the results indicate that as κ deviates from the transition point between semantic synthesis and detail synthesis, the video quality gradually deteriorates. It further validates the effectiveness of our optimization decoupling strategy. 5. Conclusion and Discussion In this paper, we identify key optimization conflict in consistency distillation for video synthesis: there exists significant discrepancy in the optimization gradients and loss contributions across different timesteps. Distilling the entire ODE trajectory into single student model fails to balance these aspects, leading to degraded motion consistency and coarse synthesis quality. To address this issue, we propose parameter-efficient Dual-Expert distillation framework that decouples semantic learning from fine-detail refinement. Additionally, we introduce Temporal Coherence loss to enhance motion consistency for the semantic expert and apply GAN and Feature Matching loss to improve synthesis quality for the detail expert. Our method significantly reduces sampling steps while achieving stateof-the-art visual quality, demonstrating the effectiveness of expert specialization in video diffusion model distillation. Limitation Although our method achieves favorable results with 4 steps inference, it still struggles to produce satisfactory outcomes with fewer steps (e.g., 2) due to limited training data and iterations. We will further explore highquality synthesis with fewer steps in future work."
        },
        {
            "title": "References",
            "content": "[1] David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter TalTract: Denoising diffusion models bott, and Eric Gu. arXiv preprint with transitive closure time-distillation. arXiv:2303.04248, 2023. 3 [2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 2 [3] Dar-Yen Chen, Hmrishav Bandyopadhyay, Kai Zou, and Yi-Zhe Song. Nitrofusion: High-fidelity single-step diffusion through dynamic adversarial training. arXiv preprint arXiv:2412.02030, 2024. 3 [4] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 2, 6 [5] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and Joshua Susskind. Boot: Data-free distillation of denoising diffusion models with bootstrapping. In ICML 2023 Workshop on Structured Probabilistic Inference {&} Generative Modeling, 2023. 3 [6] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 3 [7] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. 2 [8] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 2 [9] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 2 [10] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2 [11] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2, 5 [12] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [13] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video arXiv preprint arXiv:2410.05954, generative modeling. 2024. 2 [14] Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park. Distilling diffusion models into conditional gans. In European Conference on Computer Vision, pages 428447. Springer, 2024. 3 [15] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. 35:2656526577, 2022. 3 [16] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. 3 [17] Jonas Kohler, Albert Pumarola, Edgar Schonfeld, Artsiom Sanakoyeu, Roshan Sumbaly, Peter Vajda, and Ali Thabet. Imagine flash: Accelerating emu diffusion models with arXiv preprint arXiv:2405.05224, backward distillation. 2024. 3 [18] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, 4, 6, 7, [19] Kuaishou. Kling, 2024. [20] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. 2 [21] Shanchuan Lin and Xiao Yang. Animatediff-lightning: arXiv preprint Cross-model diffusion distillation. arXiv:2403.12706, 2024. 3 [22] Shanchuan Lin, Anran Wang, and Xiao Yang. SdxlProgressive adversarial diffusion distillation. lightning: arXiv preprint arXiv:2402.13929, 2024. 3 [23] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial post-training for one-step video generation. arXiv preprint arXiv:2501.08316, 2025. [24] Hongjian Liu, Qingsong Xie, Zhijie Deng, Chen Chen, Shixiang Tang, Fueyang Fu, Zheng-jun Zha, and Haonan Lu. Scott: Accelerating diffusion models with stochastic consistency distillation. arXiv preprint arXiv:2403.01505, 2024. 3 [25] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2202.09778, 2022. 3 [26] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3 [27] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusionbased text-to-image generation. In The Twelfth International Conference on Learning Representations, 2023. 3 [28] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. 3 [29] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. [30] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. 3 [31] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. 3 [32] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 3, 4, 6 [33] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinario Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: universal stable-diffusion acceleration module. arXiv preprint arXiv:2311.05556, 2023. 3 [34] Weijian Luo. comprehensive survey on knowledge distillation of diffusion models. arXiv preprint arXiv:2304.04262, 2023. 3 [35] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct: universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36:7652576546, 2023. [36] Weijian Luo, Zemin Huang, Zhengyang Geng, Zico Kolter, and Guo-jun Qi. One-step diffusion distillation through score implicit matching. arXiv preprint arXiv:2410.16794, 2024. 3 [37] Yihong Luo, Xiaolong Chen, Xinghua Qu, Tianyang Hu, and Jing Tang. You only sample once: Taming one-step text-toimage synthesis by self-cooperative diffusion gans. arXiv preprint arXiv:2403.12931, 2024. 3 [38] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 2 [39] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. 3 [40] OpenAI. Sora, 2024. 2 [41] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 1, 2 [42] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [43] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. arXiv preprint arXiv:2404.13686, 2024. 3, 4 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1 [45] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. 3 [46] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion In SIGGRAPH Asia 2024 Conference Papers, distillation. pages 111, 2024. 3 [47] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. Conference on Computer Vision, pages 87103. Springer, 2024. [48] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2, 3 and Stefano Ermon. arXiv preprint [49] Yang Song and Prafulla Dhariwal. niques for training consistency models. arXiv:2310.14189, 2023. 3 Improved techarXiv preprint [50] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya arXiv preprint Consistency models. Sutskever. arXiv:2303.01469, 2023. 2, 3, 4 [51] Genmo Team. Mochi 1. https://github.com/ genmoai/models, 2024. 2 [52] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1 [66] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024. [53] Fu-Yun Wang, Zhaoyang Huang, Alexander William Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu arXiv preprint Liu, et al. arXiv:2405.18407, 2024. 3, 4, 6 Phased consistency model. [54] Fu-Yun Wang, Zhaoyang Huang, Weikang Bian, Xiaoyu Shi, Keqiang Sun, Guanglu Song, Yu Liu, and Hongsheng Li. Animatelcm: Computation-efficient personalized style video generation without personalized video data. In SIGGRAPH Asia 2024 Technical Communications, pages 15. 2024. 3 [55] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 3 [56] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. 1 [57] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion gans. arXiv preprint arXiv:2112.07804, 2021. 3 [58] Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-image generIn Proceedings of the IEEE/CVF ation via diffusion gans. Conference on Computer Vision and Pattern Recognition, pages 81968206, 2024. 3 [59] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 2, [60] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. arXiv preprint arXiv:2405.14867, 2024. 3 [61] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66136623, 2024. 3 [62] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast causal video generators. arXiv preprint arXiv:2412.07772, 2024. 3 [63] Yuanhao Zhai, Kevin Lin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Chung-Ching Lin, David Doermann, Junsong Yuan, and Lijuan Wang. Motion consistency model: Accelerating video diffusion with disentangled motion-appearance distillation. arXiv preprint arXiv:2406.06890, 2024. 3 [64] Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of difIn International confusion models via operator learning. ference on machine learning, pages 4239042402. PMLR, 2023. 3 [65] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue and Tat-Jen Wang, Changxing Ding, Dacheng Tao, Cham. Trajectory consistency distillation. arXiv preprint arXiv:2402.19159, 2024. 3 DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Further implementation details Stage division and expert switching. During inference, we empirically observe that evenly dividing the total steps between the two experts produces favorable results. With 8 or 4 total steps, we assign 4 or 2 steps to each expert, respectively. These steps are uniformly sampled within each sub-trajectory. 7. Additional Results 7.1. Compatibility with other acceleration techniques DCM accelerates generation via sampling step reduction and is compatible with other methods like low precision computation and sparse modeling. For example, integrating SVG [56] (which leverages the sparsity of 3D full attention), yields an additional 1.33 speedup on top of DCMHunyuan while maintaining high fidelity (VBench 83.79%). 7.2. Generality of DCM DCM addresses discrepancies in loss and gradient contributions across noise levelsa problem inherent to consistency distillation itself, not from any specific model architecture. Beyond HunyuanVideo [18] and CogVideoX [59], we further apply it to the recent WAN2.1-T2V [52]. DCM significantly accelerates inference while preserving comparable visual quality, as evidenced by VBench scores (baseline: 83.2%, DCM: 82.9%). 7.3. Visualization of the sampling process To further verify the effectiveness of our method in semantic and detail synthesis, we visualize the results of each sampling step in 4-step sampling process on HunyuanVideo. As shown in Fig. 11 and Fig. 12, our method achieves better performance in both semantic layout and fine details compared to competing methods. 7.4. More visual comparison results The additional visual comparison results for HunyuanVideo are presented in Fig. 13, Fig. 14 and Fig. 15. More visual results of CogVideoX are shown in Fig. 16. The results indicate that our method maintains reliable fidelity across diverse models, styles, and content in video synthesis while also achieving acceleration. Figure 11. Visualization of the sampling process of different methods. Figure 12. Visualization of the sampling process of different methods. Figure 13. Visual quality comparison of different methods. Figure 14. Visual quality comparison of different methods. Figure 15. Visual quality comparison of different methods. Figure 16. Visual quality comparison of different methods."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "S-Lab, Nanyang Technological University",
        "Shanghai Artificial Intelligence Laboratory",
        "The University of Hong Kong",
        "University of Chinese Academy of Sciences"
    ]
}