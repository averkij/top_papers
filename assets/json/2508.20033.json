{
    "paper_title": "DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis",
    "authors": [
        "Liana Patel",
        "Negar Arabzadeh",
        "Harshit Gupta",
        "Ankita Sundar",
        "Ion Stoica",
        "Matei Zaharia",
        "Carlos Guestrin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The ability to research and synthesize knowledge is central to human expertise and progress. An emerging class of systems promises these exciting capabilities through generative research synthesis, performing retrieval over the live web and synthesizing discovered sources into long-form, cited summaries. However, evaluating such systems remains an open challenge: existing question-answering benchmarks focus on short-form factual responses, while expert-curated datasets risk staleness and data contamination. Both fail to capture the complexity and evolving nature of real research synthesis tasks. In this work, we introduce DeepScholar-bench, a live benchmark and holistic, automated evaluation framework designed to evaluate generative research synthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv papers and focuses on a real research synthesis task: generating the related work sections of a paper by retrieving, synthesizing, and citing prior research. Our evaluation framework holistically assesses performance across three key dimensions, knowledge synthesis, retrieval quality, and verifiability. We also develop DeepScholar-base, a reference pipeline implemented efficiently using the LOTUS API. Using the DeepScholar-bench framework, we perform a systematic evaluation of prior open-source systems, search AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that DeepScholar-base establishes a strong baseline, attaining competitive or higher performance than each other method. We also find that DeepScholar-bench remains far from saturated, with no system exceeding a score of $19\\%$ across all metrics. These results underscore the difficulty of DeepScholar-bench, as well as its importance for progress towards AI systems capable of generative research synthesis. We make our code available at https://github.com/guestrin-lab/deepscholar-bench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 3 3 0 0 2 . 8 0 5 2 : r DeepScholar-Bench: Live Benchmark and Automated Evaluation for Generative Research Synthesis Liana Patel, Negar Arabzadeh, Harshit Gupta, Ankita Sundar, Ion Stoica, Matei Zaharia, Carlos Guestrin Stanford University, UC Berkeley lianapat@stanford.edu, negara@berkeley.edu, gharshit@stanford.edu, ankitasun@berkeley.edu, istoica@cs.berkeley.edu, matei@berkeley.edu, guestrin@stanford.edu DeepScholar-Bench Repository ABSTRACT The ability to research and synthesize knowledge is central to human expertise and progress. An emerging class of systems promises these exciting capabilities through generative research synthesis, performing retrieval over the live web and synthesizing many discovered sources into long-form, cited summaries. However, evaluating such systems remains an open challenge: existing questionanswering benchmarks focus on short-form factual responses, while expert-curated datasets risk staleness and data contamination. Both fail to capture the complexity and evolving nature of real research synthesis tasks. In this work, we introduce DeepScholar-bench, live benchmark and holistic, automated evaluation framework designed to evaluate generative research synthesis. DeepScholarbench draws queries from recent, high-quality ArXiv papers and focuses on real research synthesis task: generating the related work sections of paper by retrieving, synthesizing, and citing prior research. We develop an automated evaluation framework that holistically assesses performance across three key dimensions, knowledge synthesis, retrieval quality, and verifiability, using metrics that show strong agreement with expert human judgments. We also develop DeepScholar-base, reference pipeline for generative research synthesis, implemented efficiently using the LOTUS API. Using the DeepScholar-bench framework, we perform systematic evaluation of prior open-source systems, search AIs with open-source and strong proprietary models, OpenAIs DeepResearch, and DeepScholar-base. We find that DeepScholar-base establishes strong baseline, attaining competitive or higher performance than prior open-source systems, search AIs and OpenAIs DeepResearch. We also find that DeepScholar-bench remains far from saturated, with no system exceeding score of 19% across all metrics. These results underscore both the difficulty and the importance of DeepScholar-bench as foundation for progress toward AI systems capable of generative research synthesis. We make our benchmark code and data available at https://github.com/guestrinlab/deepscholar-bench."
        },
        {
            "title": "1 INTRODUCTION\nA crucial underpinning of human knowledge and innovation is the\nability of human experts to research and synthesize known facts and\nnew findings to support others in comprehending, verifying and\nbuilding upon existing work. Recently, new systems designed for\ngenerative research synthesis offer exciting capabilities and promises",
            "content": "to automate challenging research synthesis tasks, which often require laborious hours of literature search, reading, and writing from human experts. These systems include wide array of both commercial DeepResearch offerings, including ones from OpenAI [7], Gemini [3], Anthropic [2], Grok [5], and Perplexity [8], as well as open-source methods, such as STORM [39], DeepResearcher [54], and OpenScholar [19]. These systems push the frontier of AI capabilities, demonstrating promising performance on existing factuality and question-answering benchmarks [25, 32, 48, 49]. Yet, as this new class of systems emerges, key question remains: how should we benchmark and evaluate generative research synthesis? The progress of these systems requires benchmarks that carefully evaluate their critical capabilities. Specifically, these systems provide three core functions: (1) retrieval, typically from large, complex, and constantly-evolving corpus, such as the live web, to collect key information (2) knowledge synthesis, to generate coherent long-form answers that surface key facts, integrating general knowledge and findings from many retrieved sources, and (3) verifiability, providing citations that allow readers to trace each stated claim in the synthesized answer to reputable source from the retrieved set. The ideal benchmark must holistically evaluate across all three of these dimensions, while providing realistic and challenging research synthesis task, which the community can reliably and repeatedly benchmark on over time. Unfortunately, existing benchmarks fall-short of these goals. Many prior works evaluate generative research synthesis systems using existing question answering benchmarks [2226, 28, 32, 43, 4749, 51, 52], which do not reflect realistic research synthesis tasks and instead focus on questions with short-form, easily-verifiable answers, making them severely limited for this setting. These question-answering benchmarks do not capture the complexity of long-form answers synthesized from many sources, key component of research synthesis. To address this limitation, several recent works [11, 19, 55] instead leverage expert-curated datasets with open-ended research questions and exemplar answers. Unfortunately, these benchmarks quickly become stale and outdated as new information emerges. Furthermore, these datasets risk data contamination as new models are trained on snapshots of the web, including public datasets. The prohibitive expense of curating, maintaining, and updating expert-curated benchmarks further limits their utility towards realistic, scalable evaluation. To address this gap, we introduce DeepScholar-Bench, live benchmark dataset and holistic evaluation framework tailored Patel et al. (a) Open-source systems, Search AIs, and DeepScholar-base, each with Llama-4-Scout-17B-16E-Instruct model. (b) Search AIs with proprietary models (o3, Claude-opus-4, Gemini2.5-pro), OpenAI DeepResearch, and DeepScholar-base (GPT4.1, Claude-opus-4) Figure 1: Performance comparison of generative research synthesis systems on DeepScholar-Bench. On the left (a), we show the performance of prior open-source systems, including DeepScholar, STORM, and OpenScholar, our DeepScholar baseline, and Search AI, each run with the open-source Llama-4-Scout-17B-16E-Instruct model. On the right (b), we show performance of Search AIs with strong proprietary modelso3, Claude-opus-4, and Gemini-2.5-proas well as OpenAIs commercial DeepResearch and DeepScholar-base (GPT4.1 + Claude-opus-4), one of our reference pipelines. Full evaluation results appear in Section 5. to tracking progress on generative research synthesis, as well as DeepScholar-Base, reference method, which we hope will further future research. Our benchmark provides live dataset with real research synthesis tasks, as well as holistic, automated evaluation strategy. We design our benchmark dataset based on fundamental research synthesis task reflected in thousands of research papers each year, where peer-reviewed manuscripts must concisely summarize existing work within subfield, detailing and citing key prior works, before discussing new contributions. Specifically, the DeepScholar-bench task requires system to generate related work section for given academic paper by retrieving sources from the Web and summarizing key prior work within field of study. This task allows us to assess the key capabilities of research synthesis while ensuring that our evaluation reflects real, high-quality research. To source realistic and challenging queries, we develop an automated data pipeline that curates high-quality, recent ArXiv papers across diverse scientific domains. We plan to continuously re-run our data pipeline to provide an evolving dataset of live queries. To assess performance on DeepScholar-bench, we develop an automated evaluation framework, which holistically measures the performance of system answers across the three key dimensions: knowledge synthesis, retrieval quality and verifiability. Evaluating the accuracy of our long-form synthesis task is inherently difficult since each query admits many possible answers, lacking straightforward notion of correctness. Moreover, developing an automated evaluation requires reliable metrics that exhibit high agreement with expert human annotators, another significant challenge. To address these challenges, our holistic evaluation assesses each system response across seven key metrics  (Table 1)  , which permit many possible correct answers, often leveraging human-written exemplars from our dataset. Specifically, on the knowledge synthesis dimension, we evaluate generated answers Organization, using pairwise comparisons to human exemplars, and Nugget Coverage, assessing the efficiency of the generated response in capturing key information and facts. To assess retrieval quality, we measure the Relevance Rate of retrieved sources, the Document Importance of DeepScholar-Bench: Live Benchmark and Automated Evaluation for Generative Research Synthesis Table 1: Summary of Evaluation Metrics. Metric Description Knowledge Synthesis framework and reference pipeline support the progress of new systems, and we believe that resolving DeepScholar-bench represents critical milestone towards more capable AI systems. Overall, our main contributions are the following: Organization Nugget Coverage assesses organization and coherence of system answer assesses the answers coverage of essential facts Retrieval Quality Relevance Rate measures avg. relevance among all referenced sources Document Importance measures how notable referenced sources Reference Coverage are, using citation counts assesses the referenced sets coverage of key, important references Verifiability Citation Precision Claim Coverage measures percent of cited sources that support their accompanying claim measures percent of claims that are fully supported by cited sources sources, according to citation counts of each reference, and Reference Coverage, by assessing the generated reports coverage of notable important references recovered from the human-written exemplar. To assess the Verifiability of each report, we measure its Citation Precision, whether each citation supports the given claim, and Citation Coverage, measuring whether each claim is fully supported by the cited sources. Our human agreement study validates that these automated metrics are effective, demonstrating strong agreement between our model-based judges and expert human annotators. Using the DeepScholar-bench framework, we systematically evaluate the performance of existing systems, including open-source research synthesis systems, search AIs with strong proprietary models, OpenAI DeepResearch, and DeepScholar-base. Figure 1 demonstrates that all of these existing methods exhibit significant opportunity for improvement, with no baseline exceeding score of .19 across all metrics. Furthermore, on several key metrics, including Nugget Coverage, Reference Coverage and Document Importance, each evaluated methods performance remains well below .45, reflecting the inherent difficulty of the DeepScholar-bench task, which requires systems to navigate the live web, reasoning about the relevance and importance of documents as well as surfacing key facts into concise final answer. Notably, OpenAIs DeepResearch offers strong performance relative to other baselines, outperforming many prior methods on knowledge synthesis and retrieval quality, with scores of .392 on Nugget Coverage, .187 on Reference Coverage and .124 on Document Importance; however, it struggles to provide strong verifiability relative to many other methods. We also find that DeepScholar-base, relatively simple reference pipeline implemented efficiently using the LOTUS API [14, 34], consistently improves upon performance of prior open-source systems and search AIs, as shown in Figure 1a, as well as achieving competitive performance and up to 6.3 higher verifiability compared to OpenAIs DeepResearch, as shown in Figure 1b. Nevertheless, DeepScholar-bench remains far from saturated, representing exciting opportunities for further work. We hope that our benchmark We propose DeepScholar-bench, live benchmark dataset with real research synthesis tasks and an automated evaluation that holistically assesses performance across three key dimensions: knowledge synthesis, retrieval quality, and verifiability. Our analysis demonstrates that our DeepScholarbench evaluation metrics are robust, exhibiting strong human agreement scores. We develop DeepScholar-base, reference pipeline for generative research synthesis, providing strong baseline with competitive or higher performance compared to prior opensource systems, search AIs, and OpenAIs DeepResearch. We perform systematic evaluation of existing baselines on DeepScholar-bench, including open-source systems, search AIs, OpenAIs DeepResearch, and DeepScholar-base. We find that DeepScholar-bench is far from saturated, demonstrating significant opportunity for improvement, with no baseline achieving score greater than .19 across all metrics."
        },
        {
            "title": "2 THE DEEPSCHOLAR DATASET\nWe study the task of generating a related works section of an aca-\ndemic paper, a fundamental research synthesis task. We choose\nthis task for two key reasons. First, this task is a real research task\nperformed by academic experts, allowing our benchmark to reflect\nrealistic, difficult and useful queries. Second, the online availability\nof diverse, high-quality academic papers allows us to develop an\nautomated dataset construction pipeline that we can continuously\nrun to obtain new queries over time.",
            "content": "We construct our dataset by scraping papers from ArXiv [1], which continuously posts thousands of new pre-print papers across wide array of scientific domains each week. We formalize our dataset task as follows: given description, 𝑑 of paper, the goal is to retrieve set of relevant sources, 𝑆, and generate related works sections, 𝑊 , for the paper by synthesizing and citing the retrieved documents. We briefly overview our automated data collection framework (Section 2.1) and describe the dataset instantiation (Section 2.2) used in our evaluation (Section 5)."
        },
        {
            "title": "2.1 Automated Data Collection Framework\nOur automated data collection framework aims to achieve the fol-\nlowing design goals:",
            "content": "(1) Inclusion of diverse paper topics across wide variety of research domains. (2) Focus on recent research papers, both to provide realistic, timely benchmark queries and to control data contamination when benchmarking models trained on snapshots of the web. (3) Control for quality of the scraped ArXiv papers and extracted data Figure 2 provides an overview of our dataset pipeline, which collects and extracts metadata about each paper (e.g., the title, abstract, and ArXiv link), the papers related works section, and information Patel et al. Figure 2: DeepScholarBench Overview. To curate our dataset with real and challenging research tasks, we scrape recent, highquality ArXiv papers from diverse domains, and extracting key attributes from each paper through an automated data pipeline that can easily be re-run. Our dataset task is to generate related works section given information about paper, such as its title and abstract. The DeepScholar-bench evaluation framework then holistically measures performance of generated reports on three key dimensions: knowledge synthesis, retrieval quality and verifiability. on each citation from the papers related works section. Our data collection pipeline extracts this information by scraping and selectively filtering ArXiv papers according to number of configured settings. Specifically, the pipeline loads papers from list of configured ArXiv domain categories (e.g., cs.ML) and filters paper according to the configured publication-date range. To avoid possible data contamination arising from multiple ArXiv versions, some of which may have been released prior to the configured publication-date range, we exclusively include v1 ArXiv papers. To control for paper quality, our pipeline optionally provides configuration setting which filters out papers which are not listed as \"accepted\" or \"published\" at conference within the papers comment metadata, which often lists updates to the papers status. We also disclude papers that do not have an explicit \"Related Works\" section and .bib file, containing well-formatted bibliography entries. For each paper, we then extract the Related Works section, from both the LaTex files, and PDF file, if available. We clean the extracted LaTex section to remove figures, sub-figures, labels and comments. We also extract all citations found in the related work section from the LaTex .bib file. For each citation we use the ArXiv API and the OpenAlex API to recover more detailed information, such as abstracts, authors, and links for ArXiv and non-ArXiv references respectively."
        },
        {
            "title": "2.2 Dataset Description and Statistics\nWe now briefly summarize the dataset we use in our evaluation\nin Section 5 which represents an instantiation of our automated\ndata collection pipeline. Our datasets take ArXiv papers with a\npublication date between April and June, following the April 5th,\n2025 release date of the Llama-4 models [13], the main open-source\nmodel we benchmark in our evaluation. Our dataset consist of",
            "content": "papers scraped from diverse set of 18 disticnt ArXiv domains, including, cs.AI, cs.CV, cs.DB, cs.LG, cs.AR, cs.CG, cs.DC, cs.DS, cs.IR. To control for quality, we filter out papers not accepted at conference, and we additionally exclude papers with related works sections longer than 1,000 words. Our final dataset instantiation includes 63 ArXiv papers, each providing single query and expertwritten exemplar for our benchmark. We make our scripts available to allow others to configure different datasets, and we plan to update our dataset to provide continual evaluation with recent queries. Our experiments leverage the abstract of each paper as the papers description 𝑑, provided to each baseline system as context within the query. We analyze the human-written exemplars from our dataset, and we find that, on average, each related work section contains 23 unique references, and we find over 63% of all cited references on ArXiv."
        },
        {
            "title": "FRAMEWORK",
            "content": "Evaluating research synthesis systems is challenging due to the complexity of the task and the variability of possible correct answers. Research synthesis systems generate complex, long-form reports, which are difficult to evaluate and lack notion of \"ground truth\" correctness. The task we consider departs significantly from traditional question answering and RAG-based evaluations [22 24, 26, 28, 43, 52], requiring carefully designed and holistic evaluation framework. An exemplar research report must retrieve important relevant sources, synthesize an informative and wellorganized answer, and provide appropriate references allowing readers to verify and re-trace facts. Our holistic evaluation framework thus analyzes three key dimensions, providing an automated, DeepScholar-Bench: Live Benchmark and Automated Evaluation for Generative Research Synthesis scalable approach for each: knowledge synthesis (Section 3.1), retrieval quality (Section 3.2), and verifiability (Section 3.3). While our experiments in Section 5 evaluate one specific task, our evaluation framework may extend to wide range of research synthesis tasks [11, 19, 39, 55], which exhibit similar properties and challenges. We provide an overview of our evaluation framework in this section and further details and analysis in Appendix Section 8.4. relevance judgment task, demonstrating their effectiveness on traditional IR datasets. Building on these works, we adopt an LLM-based approach for assigning graded relevance scores from 0 to 2 to each generated research report using the prompt in Box 8 in Appendix. For each retrieved set, 𝑆, for given query, we compute the average document relevance over the set, following the below formula:"
        },
        {
            "title": "3.2 Retrieval Quality\nWhile traditional information retrieval (IR) evaluations typically\nleverage gold labels for document relevance scores and a controlled\ncorpus [12, 33, 41], the research synthesis task we study in this\nwork differs substantially. An expert-written report section and its\nsources provide one reasonable retrieved set, but there may be many\npossible alternative sets that are likewise high-quality. Moreover,\nlive web search is a key component of research synthesis tasks and\nobtaining gold relevance labels over this corpus is prohibitively\nexpensive. To address these challenges, our evaluation measures\nthree components of the retrieved set: the relevance rate, reference\ncoverage of key sources, and document importance.\nRelevance Rate. We asses the relevance of each retrieved docu-\nment, following the Cranfield model [46], which is standard in IR\nevaluations and considers relevance of individual documents given\na query, independent of other documents. Due to the significant\ncost of obtaining human-annotated relevance judgments, recent\nworks [19, 20, 38, 42, 45] study leveraging an LLM-as-a-judge for",
            "content": "𝑅𝑅(𝑆) = 1 2𝑆 𝑠 𝑆 𝑅𝑒𝑙 (𝑠), where 𝑅𝑒𝑙 (𝑠) is the graded relevance score of source 𝑠. We validate the agreement between LLMs and human annotators for this task in our experiments in Section 5. Reference Coverage. We introduce metric to measure the reference coverage of the retrieved set for each report. key challenge in measuring this value is in defining set of important references for each generated report, that good research report should cite. To build this set, we take all references from the high-quality, human-written exemplar and label each as either \"important\" or \"not-important\", considering \"not-important\" reference as one that could be omitted or substituted by different reference. We find that LLM-based judge is effective in assigning these labels. For given report, we then compute its reference coverage by taking the ratio of the number of important references in the system-generated report to the number of important references in the human-written exemplar, following the below formula, where 𝐸 is the set of \"important\" references from the human-written exemplar: 𝑅𝐶 (𝑆, 𝐸) = 1 𝐸 𝑠 𝑆 𝐼 [𝑠 𝐸]. Document Importance. While the above relevance and coverage metrics assess topical matches between the retrieved set and the user query, an ideal research synthesis system must also retrieve notable and important sources. Exemplar human-written reports typically contain ample references of primary-sources and highlycited academic publications. While the ideal notion of document importance depends on the particular task and user, in our task, we adopt the following metric by considering the number of citations that each reference retrieved by the system has. We consider the median number of citations per reference over the set of all retrieved sources, 𝑆, provided by given baseline. We compute document importance as the ratio of this value for the given baseline compared to the median citations per reference over the set of sources, 𝑆 provided by the human-written exemplars, following the formula below: 𝐷𝐼 (𝑆, 𝑆) = 𝑚𝑖𝑛 (cid:32) median(cid:8)num-cites(𝑥)𝑥 𝑆 (cid:9) median(cid:8)num-cites(𝑥 )𝑥 𝑆(cid:9) (cid:33) , 1 , where num-cites(𝑥) is the number of citations for source 𝑥. We set an upper-bound of one, although in practice, we find this ratio to remains far below one for all measured generative research synthesis systems. Patel et al. Figure 3: Overview of DeepScholar-base. The system iteratively writes queries and performs web search, before passing the search results through series of semantic operators using the LOTUS system for LLM-based data-processing, including filtering step to discard irrelevant sources, top-k ranking step to re-rank the most relevant sources, and final aggregation step to generate the final report from all remaining sources."
        },
        {
            "title": "4 DEEPSCHOLAR-BASE\nWe introduce DeepScholar-base, an open-source reference pipeline\ndesigned to perform generative research synthesis. Figure 3 pro-\nvides an overview of our method. Given a user’s query, DeepScholar-\nbase iteratively generates web-search queries, summarizes the search\nresults in each round before generating a new query. The system\nthen post-processes the search results leveraging a series of seman-\ntic operators [34], which we implement efficiently using the LOTUS\nAPI [14]. This includes a semantic filtering step, which leverages\nan LLM to filter out irrelevant source documents, followed by a",
            "content": "semantic top-k which performs an LLM-based ranking over the documents based on their relevance to the user query. Finally, we perform semantic aggregation over the final source documents to generate the final report. We provide further details of each step of our reference pipeline in Appendix Section 8.3."
        },
        {
            "title": "5 EXPERIMENTAL RESULTS\nIn this section, we evaluate recent state-of-the-art generative re-\nsearch systems as well as DeepScholar-base on DeepScholar-bench.\nOverall, we find the following:",
            "content": "Existing baselines for generative research synthesis, including strong open-source LLM systems, search AIs, and commercial systems, demonstrate significant room for improvement across all three key dimensions: knowledge synthesis, retrieval quality and verifiability, with no method exceeding score of 19% across all metrics. DeepScholar-base provides strong baseline for generative research synthesis, consistently improving upon the performance of prior open-source systems and search AIs, as well as achieving competitive performance and up to 6.3 higher verifiability compared to OpenAIs DeepResearch. Our automated evaluation approach is effective, demonstrating high agreement with over 200 human annotations. Experimental Setup. For each benchmarked method, we control the retrieval corpus by allowing each system to access the Web only through the ArXiv API [1]. We additionally avoid possible information leakage during search by filtering out any search results that were published after the query papers publication date. We report results using GPT-4.1-2025-04-14 [9] as the judge for Nugget Coverage, and GPT-4o-2024-08-06 [9] judge for Organization, Relevance Rate, Reference Coverage, Citation Precision and Claim Coverage. We report the Organization score as win rate including ties, we report the strict all score for Nugget Coverage, and we report Claim Coverage with window size of 𝑤 = 1. For all Retrieval Quality metrics, we consider the retrieved set of each given report as the set of any valid ArXiv links found within the report. To measure Document Importance, we use the OpenAlex [10] API to recover citation information. For each metric, we report an average over all reports. DeepScholar-Bench: Live Benchmark and Automated Evaluation for Generative Research Synthesis"
        },
        {
            "title": "5.1 Baselines\nWe briefly overview all of the baseline systems we evaluate, and\nwe provide further details in the Appendix Section 8.2.",
            "content": "5.1.1 Open-source Research Systems. We evaluate three state-ofthe-art open-source systems, DeepResearcher [54], STORM [39] and OpenScholar [19]. For each, we run these systems using the Llama-4-Scout-17B-16E-Instruct model [15], which we serve with 4 A100 GPUs using vLLM [27]. DeepResearcher [54] leverages trained agents to navigate, browse and synthesize information from the web. To train an agent, this work uses end-to-end reinforcement learning and trains Qwen2.57B-Instruct [36]. In our benchmarks, we evaluate DeepResearcher using both the released, trained model from the authors, and using Llama-4-Scout-17B-16E-Instruct model [15] as the core LLM. We report the better performing baseline of these two, which we find in our experiments to be the Llama-4-Scout-17B-16E-Instruct backbone. STORM [39] studies the problem of how to apply LLMs to write grounded, organized long-form articles (e.g., Wikipedia articles) from scratch. The system involves pre-writing stage that discovers diverse research perspectives on topic by stimulating conversations between multiple agents and leveraging web documents. OpenScholar [19] builds specialized retrieval-augmented LLM system for literature synthesis and scientific queries. This method includes trained retriever from the pre-indexed peS2o [40] corpus, consisting of 45 million open-access academic papers up until October 2024, as an initial retrieval source before using web search. In our experiments, we benchmark the system using this pre-indexed corpus and limit web search to the ArXiv API. Search AIs. We evaluate the following models: Llama-45.1.2 Scout-17B-16E-Instruct [15], GPT-4.1-2025-04-14 [9], o3-2025-0416 [9], Claude-opus-4-20250514 [6], and Gemini-2.5-pro [4]. We augment each with search capabilities to ArXiv [1], and use the popular ODS framework [17] to allow the LLM to make tool calls to the search API. 5.1.3 Commercial Systems. We focus our evalution of commercial generative research synthesis systems on OpenAIs o3-deepresearch [9], which provides public API allowing for our evaluation. 5.1.4 DeepScholar-base. Similar to our evaluation of search AIs we evaluate DeepScholar-base with the following models: Llama-4Scout-17B-16E-Instruct [15], GPT-4.1-2025-04-14 [9], o3-2025-0416 [9], Claude-opus-4-20250514 [6], and Gemini-2.5-pro [4]. For each of these baselines, we also use the same or weaker model, either Llama-4 or GPT-4.1, to perform semantic filtering and top-k operators. We limit the method to two round of search, each with at most 2 queries."
        },
        {
            "title": "5.2 Main Results\nTable 2 provides detailed summary of each method’s performance\nscores on all metrics across three key dimensions, knowledge syn-\nthesis, retrieval quality and verifiability. We also provide metadata\nstatistics characterizing the generated reports of each benchmarked\nmethod in Table 5, as well as statistics related to our evaluation",
            "content": "metrics in Table 6. Overall, our evaluation demonstrates two key findings, which we discuss in detail below: first, existing generative research synthesis systems demonstrate significant headway for improvement, and second, DeepScholar-base provides strong baseline for generative research synthesis. 5.2.1 Generative Research Synthesis Systems Demonstrate Large Room for Improvement. From Table 2, we see that no method is able to achieve score greater than .19 across all metrics. Moreover, on several key metrics, including nugget coverage, reference coverage and document importance, each evaluated methods performance remains well below .45. This reflects the inherent difficulty of the generative research task provided by DeepScholar-bench, which requires systems to navigate the live web, reasoning about relevance and importance of documents to perform retrieve sources and then surface key information into coherent report that answers the query. We now analyze each evaluated dimension, comparing performance of the open-source research systems, search AIs and commercial systems to the human-written exemplars. On knowledge synthesis, we see that OpenAI DeepResearch offers the best performance compared to all other prior methods on both Organization, with score of .857, and Nugget Coverage, with score of .392. OpenAI DeepResearch, as well as the o3, Claude and Gemini search AIs achieve relatively high Organization scores compared to humanwritten exemplars. However, on Nugget Coverage all prior methods scores below .40. This demonstrates that while existing systems, especially those using state-of-the-art models, can generate wellorganized and coherent summaries, they still struggle to extract and surface key facts, crucial capability for synthesis tasks. Turning our attention to the retrieval quality performance of prior methods, we once again find significant room for improvement. Once again, OpenAI DeepResearch offers the strongest performance among the other benchmarked prior methods on Relevance Rate, Reference Coverage and Document Importance, but still far from saturates performance. While its Relevance Rate shows strong performance, exceeding that of the human exemplars with score of .629, its Reference Coverage and Document Importance scores remain exceedingly low: .187 and .124 respectively. This demonstrates that while state-of-the-art generative research synthesis systems are capable of retrieving relevant sources, they still struggle to find comprehensive set of notable sources and fall short compared to the ability of human experts. Lastly, we analyze the verifiability performance of prior methods, we see that OpenAI DeepResearch is outperformed on both Citation Precision and Claim Coverage by the search AIs with GPT4.1, o3, Claude and Gemini models. The Claude search AI offers the highest Citation Precision, score of .701 and Claim Coverage, score of .760. Meanwhile, OpenAIs DeepResearch as well as the all other 1The verifiability metrics we use in our evaluation likely significantly under-estimate the actual verifiability of human writing. This is because the metrics we measure, Citation Precision and Claim Coverage, require us to verify entailment relations between claims, within the report, and snippets from the cited reference. For each LLM-based system, we are able to track the precise snippet and context from each cited source, which are directly fed as context to the LLM as it generates the report with citations. On the other hand, for the human-written exemplars, we lack gold labels pointing to the precise snippet of text that each reference refers to. Our measurements for the human-written exemplars instead rely on the title and abstract of each cited source when testing for entailment relations. Table 2: Main Results. Patel et al. Knowledge Synthesis Org. Nug. Cov. Retrieval Quality Verifiability Rel. Rate Ref Cov. Doc Imp. Cite-P Claim Cov (𝑤 = 1) Human-written Exemplars .500 Human-written Exemplars 1.000 .585 1. 1.000 .2781 .2051 DeepResearcher (Llama-4) STORM (Llama-4) OpenScholar (Llama-4) Search AI (Llama-4-Scout) Search AI (GPT-4.1) Search AI (o3) Search AI (Claude) Search AI (Gemini) OpenAI DeepResearch DeepScholar-base (Llama-4) DeepScholar-base (GPT-4.1) DeepScholar-base (GPT-4.1, o3) DeepScholar-base (GPT-4.1, Claude) DeepScholar-base (GPT-4.1, Gemini) Open Source Research Systems .230 .183 .278 .193 .265 .348 .307 .277 .385 .218 .017 Search AIs .445 .490 .610 .583 .583 Commercial Systems .392 .629 DeepScholar Baseline .262 .407 .405 .370 .332 .421 .608 .659 .586 .602 .047 .003 . .060 .050 .165 .131 .061 .008 .006 .013 .009 .009 .026 .008 .010 .312 .238 .010 .316 .498 .425 .701 .415 . .124 .399 .103 .162 .162 .167 .181 .008 .007 .008 .007 .006 .648 .652 .617 .936 .851 .206 .119 . .151 .556 .849 .698 .706 .857 .254 .825 .857 .786 .762 .396 .586 .138 .368 .470 .495 .760 .398 . .826 .636 .614 .817 .865 prior methods are unable to achieve Citation Precision score beyond .5 and Claim Coverage score beyond .6. We also note that the human-written exemplars appear to exhibit rather low Citation Precision and Claim Coverage scores, however these scores are not comparable to the metric measured for the LLM-based systems since our method for measuring verifiability metrics likely underestimate the actual verifiability of human writing1. Overall, we see that prior LLM-based systems exhibit significant headroom for improvement. 5.2.2 DeepScholar-base Provides Strong Baseline for Generative Research Synthesis. We compare the performance of DeepScholarbase to the commercial OpenAI DeepResearch system, search AIs and open-source research systems, finding that DeepScholar-base provides competitive performance against each group of prior methods, offering strong baseline for generative research synthesis. First, we see that DeepScholar-base (GPT-4.1, o3) is competitive with OpenAI DeepResearch, achieving similar or higher Organization, Nugget Coverage, Relevance Rate, Reference Coverage, Citation Precision and Claim Coverage scores. Notably, DeepScholarbase offers significantly higher verifiability than OpenAI DeepResearch, with 1.5 2.3 higher Citation Precision and 4.4 6.3 higher Claim Coverage. However, DeepScholar-bases document importance scores still remain especially low compared to OpenAI DeepResearch, representing significant room for improvement. Next, we compare the performance of DeepScholar-base to the search-AIs, finding that for each evaluated model, DeepScholarbase consistently offers improved performance compared to the corresponding search AI. Specifically, averaged across all 5 baselines with different models for the Search AIs and DeepScholar-base method, DeepScholar-base offers 1.28 higher Organization, 1.29 higher Nugget Coverage, 1.06 higher Relevance Rate, 2.03 higher Reference Coverage 1.64 higher Citation Precision, 1.62 higher Citation Recall. Lastly, we compare DeepScholar-base (Llama-4) to the opensource research systems, all run with the Llama-4 model as well. We see that the prior open-source research systems exhibit tradeoffs among the Knowledge Synthesis, Retrieval Quality and Verifiability dimensions. Specifically, OpenScholar achieves the highest Knowledge Synthesis scores, on both Organization and Nugget Coverage, DeepResearcher achieves the highest Relevance Rate and Reference Coverage on Retrieval Quality, with all systems attaining only very low Document Importance scores, and on Verifiability, DeepResearcher offers the highest Citation-Precision while STORM offers the highest Claim Coverage. In comparison, DeepScholarbase offers strong performance across each dimension. Specifically, Compared to the best-performing prior open-source methods for each metric, DeepScholar-base offers competitive Knowledge Synthesis performance, 1.09 higher Relevance Rates and 2.18 higher Reference Coverage for retrieval, and 2.08 higher Citation Precision and 1.41 higher Claim Coverage scores for verifiability. Overall, the strong relative performance of DeepScholar-base likely reflects the efficiency of the data-processing semantic operators [34] that DeepScholar-base uses to perform LLM-based filtering, ranking and summarization of sources to generate its DeepScholar-Bench: Live Benchmark and Automated Evaluation for Generative Research Synthesis Table 3: Ablation Study Comparing The Effect of Different Retrieval APIs. Knowledge Synthesis Org Nug. Cov. Retrieval Quality Verifiability Rel. Rate Ref Cov. Doc Imp. Cite-P Claim Cov (𝑤 = 1) arxiv.org Retrieval parallel.ai Retrieval taviliy.com Retrieval Oracle Retrieval (arxiv.org) Oracle Retrieval (All) arxiv.org Retrieval parallel.ai Retrieval taviliy.com Retrieval Oracle Retrieval (arxiv.org) Oracle Retrieval (All) .786 .865 .929 .782 . .254 .246 .111 .202 .198 DeepScholar-base (GPT-4.1, Claude) .586 .675 .550 .686 .680 .167 .160 .070 1.000 1.000 .370 .444 .327 .487 .528 DeepScholar-base (Llama-4) .103 .421 .114 .559 .030 .532 1.000 .681 1.000 .693 .262 .265 .229 .316 . .007 .017 .015 1.000 .822 .008 .015 .016 1.000 .822 .936 .846 .711 .955 .941 .648 .223 .442 .658 .796 .817 .781 .578 .899 .828 .826 .543 .676 .868 . Table 4: Human Agreement Evaluation. Evaluation Metric LLM-Classified Labels Human-Agreement Score with LLM Organization Nugget Coverage Retrieval Relevance Score Reference Coverage Pairwise Comparison (Lose / Tie / Win) Nugget Importance (Vital / Non-vital) Graded Relevance (0/1/2) Reference Importance (Not Imp./ Imp.) 78% 72% 70% 82% report. Notably, DeepScholar-base still demonstrates significant room for improvement and far from saturates DeepScholar-Bench, especially on key Knowledge Synthesis and Retrieval Quality metrics, including Nugget Coverage, Reference Coverage, Document Importance, which represent opportunities for future work. Overall,the results shown in Table 3 demonstrate that the performance limitations of existing systems for generative research synthesis lie in both their retrieval capabilities to find and select high-quality references sets, as well as their synthesis abilities to surface key facts and extract insights given the retrieved documents."
        },
        {
            "title": "5.3 Understanding Opportunities for",
            "content": "Improvement. In order to further analyze performance and existing opportunities for improvement, we conduct an ablation study, where we consider different retrievers as well as two oracle retriever settings. Table 3 shows these results for two evaluated DeepScholar-base methods, DeepScholar-base (GPT-4.1, Claude) and DeepScholarbase (Llama-4). The table show the performance of either using three different retrieval APIs, including arxiv.org, the default used in our main results, parallel.ai and tavily.com. In addition the table shows to oracle settings for either DeepScholar-base method: the Oracle Retrieval (arxiv.org) setting, provides the system with the ArXiv references from the human-written exemplars labeled as \"Important\" following our methodology for evaluating Reference Coverage. The Oracle Retrieval (All) setting, provides the system with any from the human-written exemplars labeled as \"Important\" following the same methodology, including both references form ArXiv and once that are found elsewhere. First, we see that given either oracle retrieval setting, the DeepScholarbase (GPT-4.1, Claude) method nearly saturates performance on Retrieval Quality and Verifiability metrics, whereas the same method using the arxiv.org, parallel.ai or taviliy.com retrievers obtain lower scores on each of these metrics. This finding demonstrates significant opportunity to improve performance of generative research systems through improvements to the retrieval method. Specifically, existing systems struggle to find diverse and holistic set of notable sources, reflected by their especially low Reference Coverage and Document Importance scores. Additionally, we also see that oracle retrieval settings for either DeepScholar-base method attain higher Nugget Coverage, improving the score by up to 1.62 compared to the rspective arxiv.org, parallel.ai or tavily.ai retreival settings. However, we note that the oracle retreival methods still far from saturate Nugget Coverage, with the DeepScholar-base (GPT-4.1, Claude) Oracle Retrieval (All) attaining modest score of .528. This demonstrates that even with high retrieval quality, existing LLM systems still struggle to effectively surface important facts and synthesize important insights. Table 5: Report Statistics. Report Length Citations Chars Words Sentences # Unique Refs # Inline Citations Patel et al. Human-written Exemplars DeepResearcher (Llama-4) STORM (Llama-4) OpenScholar (Llama-4) Search AI (Llama-4-Scout) Search AI (GPT-4.1) Search AI (o3) Search AI (Claude) Search AI (Gemini) OpenAI DeepResearch Human-written Exemplars 497 Open Source Research Systems 2573 2766 3513 319 381 483 Search AIs 1968 3168 3844 3977 2810 258 404 501 499 395 Commercial Systems 864 DeepScholar-base (Llama-4) DeepScholar-base (GPT-4.1) DeepScholar-base (GPT-4.1, o3) DeepScholar-base (GPT-4.1, Claude) DeepScholar-base (GPT-4.1, Gemini) DeepScholar Baseline 402 3864 1492 14470 642 5905 1332 12287 663 6118 28 35 31 26 16 16 24 27 19 58 167 69 136 81 23 8 18 9 9 10 11 13 6 17 21 19 16 23 27 7 21 19 5 61 16 8 8 6 19 56 20 35 28 Table 6: Statistics Related to Evaluation Metrics. # Important References from ArXiv.org Median number of citations per reference from ArXiv.org 11.47 647.5 Ref. Cov. Doc. Imp. Avg. value over human-written exemplars Relevant Metric"
        },
        {
            "title": "6 RELATED WORK\nLong-form Synthesis Benchmarks. Several recent benchmarks of-\nfer datasets designed to evaluate long-form research synthesis tasks,\nhowever, their design often involves manually-curated queries,\nwhich are prone to data staleness, contamination and limited scala-\nbility. DeepScholar-bench addresses these challenges by providing",
            "content": "an automated data pipeline and live benchmark with realistic, challenging and recent research synthesis tasks, in contrast to prior works. Specifically, ScholarQABench [19] creates realistic literature review questions with detailed answer rubrics, written by expert PhD annotators from the computer science, biomedicine, physics and neuroscience domains. Similarly, OpenResearcher [55] constructs dataset of around 38 scientific text summarization queries, among other research-style questions, by recruiting experts to write and annotate queries. Likewise, DeepConsult [11] provides suite of expert-curated, open-ended research queries related to business and consulting. Unfortunately, these expert-curated benchmarks, are expensive to construct, and difficult to update, causing them to quickly become outdated, as new information becomes available, These prior benchmark datasets also risk data contamination, as new models are trained on publically available data. Our work instead proposes scalable, automated pipeline to provide live, evergreen dataset that reflects diverse and recent research queries. Similarly to our approach, the FreshWiki dataset [39] is constructed using an automated data pipeline; however, the dataset task focuses on the generation of Wikipedia-like articles, whereas DeepScholar-Bench: Live Benchmark and Automated Evaluation for Generative Research Synthesis this work focuses on significantly different and difficult synthesis task based on cutting-edge research. The FreshWiki framework focuses on evaluating the article pre-writing stage as well as the generated full-length article based on ground truth Wikipedia article and established criteria of good Wikipedia article [16]. This task reflects an interesting area of study; however, in this work, we instead focus on generative research synthesis, and our dataset task focuses on complex research synthesis task derived from high-quality academic papers. Moreover, our automated evaluation approach is tailored to holistically assess the three key capabilities of generative research synthesis: retrieval, knowledge synthesis and verifiability. AcademicEval [53] evaluates long-context generation tasks using an ArXiv-derived dataset, similar to ours, however, its task differs substantially, focusing on summarization without retrieval, which is key component of our task and of generative research synthesis systems more broadly. While the AcademicEval task provides fixed set of references to summarization system, our task requires system to navigate the live web to collect information, and we evaluate this crucial capability of generative research synthesis. Question Answering Benchmarks. Several recent works on generative research synthesis focus their evaluation on question answering (QA) benchmarks, which, unlike DeepScholar-bench, do not evaluate complex long-form research synthesis tasks and instead focus on short-form answers, whcih can be easily evaluated for correctness. These question answering benchmarks include SimpleQA [48], FRAMES [25], GAIA [32], as BrowserComp [49], WebWalkerQA [51] or others traditionally used to evaluate retrievalaugmented generation [2224, 26, 28, 43, 47, 52]. While these benchmarks involve retrieval component, their synthesis task differs substantially from the generative research synthesis task we study. Specifically, each of these QA benchmarks focus on short-form questions with easily verifiable answers and straightforward notions correctness. In contrast, our benchmark provides framework for studying complex, long-form research synthesis tasks, which lack an absolute notion of correctness and admit many possible reasonable answers."
        },
        {
            "title": "7 CONCLUSION\nIn this work, we introduced DeepScholar-bench, a live dataset and\nholistic, automated evaluation framework designed to rigorously\nbenchmark an emerging class of systems designed for generative\nresearch synthesis. By automatically sourcing queries from high-\nquality, recent ArXiv papers, our benchmark mitigates the risks\nof data staleness and training contamination, while offering a real\nresearch synthesis task. Moreover, DeepScholar-bench provides an\nautomated evaluation to holistically measure three critical dimen-\nsions: retrieval quality, knowledge synthesis and verifiability. We\nfurther release DeepScholar-base, a reference pipeline, which we\nfind provides a strong baseline for generative research synthesis.\nOverall our systematic evalaution of prior open-source systems,\nsearch AI’s, OpenAI’s DeepResearch and DeepScholar-base demon-\nstrates significant opportunities for future work, with no system\nexceeding a score of 19% across all metrics. These results demon-\nstrate both the difficulty of DeepScholar-bench and the exciting\nopportunity for further advancement in this space. We hope that",
            "content": "[6] [7] [n.d.]. DeepScholar-bench and DeepScholar-base will support the development of more capable AI systems for generative research synthesis. ACKNOWLEDGMENTS This research was supported in part by affiliate members and other supporters of the Stanford DAWN project, including Meta, Google, and VMware, as well as Cisco, SAP, and Sloan Fellowship. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. REFERENCES [1] [2] [n.d.]. arXiv.org e-Print archive. https://arxiv.org/ [n.d.]. Claude takes research to new places. https://www.anthropic.com/news/ research [3] [n.d.]. Gemini Deep Research your personal research assistant. https: //gemini.google/overview/deep-research/ [4] [n.d.]. Gemini models Gemini API. https://ai.google.dev/gemini-api/docs/ [5] models [n.d.]. Grok 3 Beta The Age of Reasoning Agents xAI. https://x.ai/news/grok3 [n.d.]. Introducing Claude 4. https://www.anthropic.com/news/claude-4 Introducing deep research OpenAI. https://openai.com/index/ introducing-deep-research/ [8] [n.d.]. Introducing Perplexity Deep Research. https://www.perplexity.ai/hub/ blog/introducing-perplexity-deep-research [n.d.]. Model - OpenAI API. https://platform.openai.com [9] [10] [n.d.]. OpenAlex: The open catalog to the global research system OpenAlex. https://openalex.org/ [11] [n.d.]. Su-Sea/ydc-deep-research-evals: you.coms framework for evaluating deep research systems. https://github.com/Su-Sea/ydc-deep-research-evals [n.d.]. TREC CAR TREC CAR. https://trec-car.cs.unh.edu/ [12] [13] 2025. Llama 4 - meta-llama Collection. https://huggingface.co/collections/metallama/llama-4-67f0c30d9fe03840bc9d0164 [14] 2025. lotus-data/lotus. https://github.com/lotus-data/lotus original-date: 202407-16T16:39:06Z. [15] 2025. meta-llama/Llama-4-Scout-17B-16E-Instruct Hugging Face. //huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct https: [16] 2025. Wikipedia:Good article criteria. https://en.wikipedia.org/w/index. php?title=Wikipedia:Good_article_criteria&oldid=1303221295 Page Version ID: 1303221295. [17] Salaheddin Alzubi, Creston Brooks, Purva Chiniya, Edoardo Contente, Chiara von Gerlach, Lucas Irwin, Yihan Jiang, Arda Kaz, Windsor Nguyen, Sewoong Oh, Himanshu Tyagi, and Pramod Viswanath. 2025. Open Deep Search: Democratizing Search with Open-source Reasoning Agents. https://arxiv.org/abs/2503.20201v1 [18] Negar Arabzadeh and Charles L. A. Clarke. 2025. Benchmarking LLM-based Relevance Judgment Methods. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval (Padua, Italy) (SIGIR 25). Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/3726302.3730305 [19] Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike Darcy, David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, Graham Neubig, Dan Weld, Doug Downey, Wen-tau Yih, Pang Wei Koh, and Hannaneh Hajishirzi. 2024. OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs. https://doi.org/10.48550/ arXiv.2411.14199 arXiv:2411.14199 [cs]. [20] Guglielmo Faggioli, Laura Dietz, Charles Clarke, Gianluca Demartini, Matthias Hagen, Claudia Hauff, Noriko Kando, Evangelos Kanoulas, Martin Potthast, Benno Stein, and Henning Wachsmuth. 2023. Perspectives on Large Language Models for Relevance Judgment. In Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval. 3950. https: //doi.org/10.1145/3578337.3605136 arXiv:2304.09161 [cs]. [21] Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling Large Language Models to Generate Text with Citations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 64656488. https://doi.org/10.18653/v1/2023.emnlp-main.398 [22] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps. In Proceedings of the 28th International Conference on Computational Linguistics, Donia Scott, Nuria Bel, and Chengqing Zong (Eds.). International Committee on Computational Linguistics, Barcelona, Spain (Online), 66096625. https://doi.org/10.18653/v1/2020.coling-main.580 [23] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: Dataset for Biomedical Research Question Answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, Hong Kong, China, 25672577. https://doi.org/10.18653/v1/D19-1259 [24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Regina Barzilay and Min-Yen Kan (Eds.). Association for Computational Linguistics, Vancouver, Canada, 16011611. https://doi.org/10.18653/v1/P17-1147 [25] Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. 2025. Fact, Fetch, and Reason: Unified Evaluation of Retrieval-Augmented Generation. https: //doi.org/10.48550/arXiv.2409.12941 arXiv:2409.12941 [cs]. [26] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: Benchmark for Question Answering Research. Transactions of the Association for Computational Linguistics 7 (2019), 452466. https://doi.org/10.1162/tacl_a_00276 Place: Cambridge, MA Publisher: MIT Press. [27] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. https://doi.org/10.48550/arXiv.2309.06180 arXiv:2309.06180 [cs]. [28] Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, HongIn Lee, and Moontae Lee. 2023. QASA: Advanced Question Answering on Scientific Articles. In Proceedings of the 40th International Conference on Machine Learning. PMLR, 1903619052. https://proceedings.mlr.press/v202/lee23n.html [29] Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, Kai Shu, Lu Cheng, and Huan Liu. 2025. From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge. https://doi.org/10.48550/arXiv.2411.16594 arXiv:2411.16594 [cs]. [30] Ruosen Li, Teerth Patel, and Xinya Du. 2024. PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations. https://doi.org/10.48550/ arXiv.2307.02762 arXiv:2307.02762 [cs]. [31] Nelson Liu, Tianyi Zhang, and Percy Liang. 2023. Evaluating Verifiability in Generative Search Engines. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 70017025. https://doi. org/10.18653/v1/2023.findings-emnlp.467 [32] Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. GAIA: benchmark for General AI Assistants. https://doi.org/10.48550/arXiv.2311.12983 arXiv:2311.12983 [cs]. [33] Federico Nanni, Bhaskar Mitra, Matt Magnusson, and Laura Dietz. 2017. Benchmark for Complex Answer Retrieval. https://doi.org/10.48550/arXiv.1705.04803 arXiv:1705.04803 [cs]. [34] Liana Patel, Siddharth Jha, Melissa Pan, Harshit Gupta, Parth Asawa, Carlos Guestrin, and Matei Zaharia. 2025. Semantic Operators: Declarative Model for Rich, AI-based Data Processing. https://doi.org/10.48550/arXiv.2407.11418 arXiv:2407.11418 [cs]. [35] Ronak Pradeep, Nandan Thakur, Shivani Upadhyay, Daniel Campos, Nick Craswell, and Jimmy Lin. 2025. The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models. https://doi.org/ 10.48550/arXiv.2504.15068 arXiv:2504.15068 [cs]. [36] Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 Technical Report. https://doi.org/10.48550/arXiv.2412.15115 arXiv:2412.15115 [cs]. [37] Hossein A. Rahmani, Clemencia Siro, Mohammad Aliannejadi, Nick Craswell, Charles L. A. Clarke, Guglielmo Faggioli, Bhaskar Mitra, Paul Thomas, and Emine Yilmaz. 2024. LLM4Eval: Large Language Model for Evaluation in IR. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, Washington DC USA, 30403043. https://doi.org/10.1145/3626772.3657992 [38] Hossein A. Rahmani, Emine Yilmaz, Nick Craswell, Bhaskar Mitra, Paul Thomas, Charles L. A. Clarke, Mohammad Aliannejadi, Clemencia Siro, and Guglielmo Faggioli. 2024. LLMJudge: LLMs for Relevance Judgments. https://doi.org/10. Patel et al. 48550/arXiv.2408.08896 arXiv:2408.08896 [cs]. [39] Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, and Monica S. Lam. 2024. Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models. https://doi.org/10.48550/arXiv.2402.14207 arXiv:2402.14207 [cs]. [40] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Walsh, Luke Zettlemoyer, Noah Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. 2024. Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 1572515788. https://doi.org/10.18653/v1/2024.acl-long.840 [41] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. https://doi.org/10.48550/arXiv.2104.08663 arXiv:2104.08663 [cs]. [42] Paul Thomas, Seth Spielman, Nick Craswell, and Bhaskar Mitra. 2024. Large language models can accurately predict searcher preferences. https://doi.org/ 10.48550/arXiv.2309.10621 arXiv:2309.10621 [cs]. [43] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop Questions via Single-hop Question Composition. https://doi.org/10.48550/arXiv.2108.00573 arXiv:2108.00573 [cs]. [44] Shivani Upadhyay, Ronak Pradeep, Nandan Thakur, Daniel Campos, Nick Craswell, Ian Soboroff, Hoa Trang Dang, and Jimmy Lin. 2024. Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look. https://doi.org/10.48550/arXiv.2411.08275 arXiv:2411.08275 [cs]. [45] Shivani Upadhyay, Ronak Pradeep, Nandan Thakur, Nick Craswell, and Jimmy Lin. 2024. UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor. https://doi.org/10.48550/arXiv.2406.06519 arXiv:2406.06519 [cs]. [46] Ellen M. Voorhees. 2009. Come Not To Bury Cranfield, but to Praise It. NIST (Oct. 2009). https://www.nist.gov/publications/i-come-not-bury-cranfield-praise-it Last Modified: 2017-02-19T20:02-05:00 Publisher: Ellen M. Voorhees. [48] [47] David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or Fiction: Verifying Scientific Claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 75347550. https://doi.org/10.18653/v1/2020.emnlp-main.609 Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. 2024. Measuring short-form factuality in large language models. https://doi.org/10.48550/arXiv.2411.04368 arXiv:2411.04368 [cs]. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. 2025. BrowseComp: Simple Yet Challenging Benchmark for Browsing Agents. https://doi.org/10.48550/arXiv.2504.12516 arXiv:2504.12516 [cs]. [49] [51] [50] Theodora Worledge, Tatsunori Hashimoto, and Carlos Guestrin. 2024. The Extractive-Abstractive Spectrum: Uncovering Verifiability Trade-offs in LLM Generations. https://doi.org/10.48550/arXiv.2411.17375 arXiv:2411.17375 [cs]. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, and Fei Huang. 2025. WebWalker: Benchmarking LLMs in Web Traversal. https://doi.org/10.48550/arXiv. 2501.07572 arXiv:2501.07572 [cs]. [52] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: Dataset for Diverse, Explainable Multi-hop Question Answering. https://doi.org/10.48550/ arXiv.1809.09600 arXiv:1809.09600 [cs]. [53] Haozhen Zhang, Tao Feng, Pengrui Han, and Jiaxuan You. 2024. AcademicEval: Live Long-Context LLM Benchmark. (Oct. 2024). https://openreview.net/forum? id=iRYExPKnxm [54] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments. https://arxiv.org/abs/2504.03160v4 [55] Yuxiang Zheng, Shichao Sun, Lin Qiu, Dongyu Ru, Cheng Jiayang, Xuefeng Li, Jifan Lin, Binjie Wang, Yun Luo, Renjie Pan, Yang Xu, Qingkai Min, Zizhao Zhang, Yiwen Wang, Wenjie Li, and Pengfei Liu. 2024. OpenResearcher: Unleashing AI for Accelerated Scientific Research. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Delia Irazu Hernandez Farias, Tom Hope, and Manling Li (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 209218. https://doi.org/10. 18653/v1/2024.emnlp-demo.22 DeepScholar-Bench: Live Benchmark and Automated Evaluation for Generative Research Synthesis"
        },
        {
            "title": "8.2 Baselines\nWe provide an extended description of each benchmarked method,\nincluding relevant implementation details, and parameters used in\nour evaluation.",
            "content": "8.2.1 DeepResearcher. The DeepResearcher pipeline follows structured tool-augmented reasoning framework designed for iterative web-based information retrieval. The system mandates explicit reasoning before any tool invocation, with reasoning encapsulated in <think> tags to ensure interpretability and control. After reasoning, the model generates JSON-formatted request specifying the web search tool and its query. These queries are executed via the Lotus Search API, which we replaced with an ArXiv-specific search interface to provide controlled retrieval API for our evaluation. Retrieved results are returned in structured format containing the title, URL, and snippet, and are stored in memory for reference across subsequent reasoning steps. This iterative process continues until the model determines that sufficient evidence has been gathered, after which synthesized final response is produced. For our experiments, we used Llama-4-Scout-17B-16E-Instruct as the base model, replacing the originally proposed DeepResearcher7b, since it demonstrated consistently better retrieval-augmented reasoning performance in our experiments. The prompt was slightly modified to align with LLama-4 prompt style as detailed in Box 1. The retrieval depth was set to 10 sources per query, which is the default in the system and provides balanced trade-off between coverage and efficiency. We restricted each query to single rollout with maximum of 10 steps, following the DeepResearcher defaults; this limit is generous as most rollouts converge in fewer than three steps, but it ensures the system has headroom for more complex queries. The default web search API was replaced with ArXiv search to comply with our benchmark settings. 8.2.2 Openscholar. The OpenScholar pipeline follows four-stage process: initial retrieval, response and feedback generation, iterative refinement, and citation verification. In the first stage, text segments are retrieved from fixed index using contriever model, which encodes texts and retrieves passages based on semantic similarity. These passages are reranked and used to generate an initial draft response, where citations are aligned with the supporting passages. The second stage introduces feedback generation, where the model produces up to three feedback statements highlighting potential improvements in the draft, such as missing content or organization issues; if additional evidence is required, retrieval queries are issued. The third stage iteratively refines the response by conditioning on the previous draft, retrieved passages, and newly added evidence, yielding improved responses at each step until feedback has been fully incorporated. Finally, citation verification ensures that all citation-worthy statements are adequately grounded in the retrieved sources, inserting additional citations where necessary without removing content. For consistency with other baselines, we employ the Llama-4Scout-17B-16E-Instruct model for generation. The retrieval pipeline initially collects 100 text segments from peS2o_v3 using the default pes2o_contriever model. The reranker used is OpenScholar_Reranker, also kept at its default setting. To align parameterization across baselines, we increase the number of sources used in generation (top_n) from 10 to 30. Furthermore, the default search API is replaced with the arXiv API, to provided controlled retreival corpus and API in our experiments. Search AI. Search AIs are implemented using the open-source 8.2.3 OpenDeepSearch (ODS) framework that enables deep web search and retrieval. In particular, the ODS ReAct Agent (instantiated from the smolagents.ToolCallingAgent) is used along with the search agent as an external tool. At each reasoning step, the ReAct agent can either invoke the search agent through web_search action or decide to produce final_answer. The search agent interfaces with the search API to fetch relevant academic articles given query, after which an LLM generates concise summaries of the retrieved content. To maintain consistency with the benchmark setting, the standard search API was replaced with the arXiv API. The regular search agent fails when tasked with full abstract queries; hence the ReAct-based agent was employed, which generates shorter, more effective searchable queries. The agent keeps track of retrieved results across turns, allowing references to past evidence during the reasoning process. After maximum of 5 iterations, the agent is compelled to conclude with final response, ensuring bounded computational steps. For the parameterization of the Search AIs, we set the search agent to retrieve 30 results per query, which is more generous than the default in order to establish fair comparability with other baselines. The reranker parameter was left at infinity, aligning with its default configuration, to avoid prematurely constraining candidate results. The maximum iteration limit was fixed at 5, consistent with the default setup of the ODS framework, providing sufficient exploration without excessive search depth. The ReAct prompt was slightly modified to tailor to the specific use of the ArXiv search API, as presented in Box 2, 3 and 4. STORM. The STORM pipeline follows structured multi8.2.4 stage process to generate comprehensive, Wikipedia-style articles from given topic. First, related Wikipedia articles are retrieved and their TOCs are clustered to identify candidate perspectives, which act as anchors for exploration. This is followed by Simulated Multiturn Conversations where an LLM plays both the question-asking and answering roles, querying retrieval module and synthesizing evidence-based responses. Parallel to this, the model generates draft outline purely from its parametric knowledge in the Draft Outline Generation stage. The outline is then refined by grounding it with retrieved evidence and conversation outputs. In the final step, each section is drafted with explicit inline citations drawing on both parametric knowledge and retrieved references. All the sections are concatenated together to form the final result. For parameter settings, we used STORMs default configurations wherever possible to preserve fidelity to its design: maximum of 3 turns per perspective, 3 perspectives, and up to 3 search queries per turn. For search, we considered the top 15 results for each query, ensuring reasonable breadth without overwhelming the pipeline. Patel et al. Figure 4: DeepScholar-bench dataset schema. To make STORM comparable with other baselines, we raised the number of collected references per section title to 30 (more generous than the default), as this allows for richer evidence integration during drafting. Importantly, we replaced the original search API with arXiv search to control the retrieval API for our benchmark settings. Finally, we use Llama-4-Scout-17B-16E-Instruct as the base model. 8.2.5 OpenAIs DeepResearch. We use OpenAIs DeepResearch system based on the o3-deep-research model with custom MCP to only search ArXiv and return 𝑛 = 30 results per query. To prevent the model from getting search results after the given paper was uploaded, the MCP used custom endpoint to set the latest date that it should retrieve. All other settings were set to default values."
        },
        {
            "title": "Configurations",
            "content": "DeepScholar-base operates through three main stages: retrieval, filtering, and final generation (Figure 3). Retrieval In this stage, an LLM generates 𝑄 search queries conditioned on the input abstract and summaries of prior retrievals. Each query is submitted to the configured search API (ArXiv, tavily, etc.) to obtain up to 𝑠𝑒𝑎𝑟𝑐ℎ_𝐾 relevant papers within the specified date range. The code and prompt used for this step are provided in Figure 5 and Box 5 respectively. This process is repeated 𝑁 times. Filtering Retrieved results are refined using two semantic operators from LOTUS [14, 34]: Sem-Filter and Sem-TopK, which together select the top 𝐾 most relevant papers. The code is given in Figure 6. Final Generation The filtered set of papers is then aggregated via Sem-Agg query to produce the final output. The corresponding code for this step is shown in Figure 7 with prompt in Box 6. Unless otherwise specified, the pipeline parameters are set to 𝑄 = 2, 𝑠𝑒𝑎𝑟𝑐ℎ_𝐾 = 50, 𝑁 = 2, and 𝐾 = 30."
        },
        {
            "title": "8.4 Evaluation Details\n8.4.1 Reference Coverage. Figure 8 illustrates the distribution of im-\nportant citations across the human exemplar reports in DeepScholar-\nBench. For each exemplar, we used the LOTUS program shown\nin Figure 9 to identify which citations are important and there-\nfore essential to include in a high-quality related work section. We\nthen separate these important citations into two groups: those that\nappear on ArXiv (shown in red) and those that do not (shown in\norange). The blue portion of each bar corresponds to non-essential\ncitations, as determined by the same Lotus-based procedure.",
            "content": "The plot highlights two consistent trends. First, many exemplar related work sections contain large number of non-essential citations. While such references may be useful for narrative flow or broader context, they are not indispensable. Non-essential citations can be somewhat subjective, depending on how authors choose to frame the story of their paper. In contrast, the important citations represent the must-have references i.e., the foundational works in the field that are necessary for situating the contribution. Second, we observe that the red segments (important ArXiv citations) are well distributed across exemplars, indicating that ArXiv is reliable and sufficiently broad source for recovering many of the essential references. DeepScholar-Bench: Live Benchmark and Automated Evaluation for Generative Research Synthesis Box 1: Revised DeepResearcher System Prompt optimized to work with Llama-4-Scout-17B-16E-Instruct ## Background information * Today is {strftime(\"%Y-%m-%d\", gmtime())} * You are Deep AI Research Assistant The question give you is complex question that requires *deep research* to answer. will provide you with two tools to help you answer the question: * web search tool to help you perform google search. Tool call format: begin{verbatim} {{\"name\": \"web_search\", \"arguments\": {{\"query\": [\"<query1>\",\"<query2>\",\"<query3>\"]}}}} * webpage browsing tool to help you get new page content. Tool call format: {{\"name\": \"browse_webpage\", \"arguments\": {{\"url_list\": [\"<url1>\",\"<url2>\",\"<url3>\"]}}}} You dont have to answer the question now, but you should first think about the research plan or what to search next. Your output format should be one of the following two formats: <think> YOUR THINKING PROCESS </think> <answer> YOUR ANSWER AFTER GETTING ENOUGH INFORMATION </answer> or <think> YOUR THINKING PROCESS </think> <tool_call> YOUR TOOL CALL WITH CORRECT FORMAT </tool_call> You should always follow the above two formats strictly. You will be heavily penalized if you do not follow the format strictly. Only output the final answer (in words, numbers or phrase) inside the <answer></answer> tag, without any explanations or extra information. If this is yes-or-no question, you should only answer yes or no. 8.4.2 Ablation Study on Verifiability. In the main paper (Section 3.3, we reported verifiability metrics results using sliding window of size 𝑤 = 1 when computing claim coverage. That is, for each claim sentence, we considered citation to be valid if any of the references in the same sentence or within one sentence before or after sufficiently supported the claim. Here, we extend this analysis to study the effect of varying the window size. Specifically, we report the citation coverage achieved by different systems when the window size ranges from 𝑤 = 0 (same-sentence only) up to 𝑤 = 5 (five sentences before or after). As shown in Figure 10, increasing the window size consistently improves citation coverage across all baselines. This is expected: the larger the window, the higher the probability that one of the cited references in the [𝑤, +𝑤] neighborhood of claim provides sufficient support. However, we also note that very large window sizes are less desirable in practice, as they often correspond to references being far from the claims they are intended to support, reducing readability and making it harder for readers to verify the connection between claims and citations. Moreover, from Table 5, we see that real academic writing tends to be densely cited, with at least one citation on average per sentence in the human exemplars. Overall, the results of our ablation study highlight the trade-off between stricter precision (𝑤 = 0) and more lenient recall-oriented settings (𝑤 1). 8.4.3 Document Importance Across Human Exemplars. In this section, we illustrate the distribution of document importance, measured by the number of citations of references in the human-written exemplars in DeepScholar-Bench. Figure 11 reports two histograms: (a) the distribution of citation counts across all references, and (b) the distribution restricted to references that appear on ArXiv. We plot the logarithm of citation counts, with values obtained from the OpenAlex API [10], an open and widely used scholarly database that provides citation-level metadata. While citation counts in OpenAlex may not exactly match those from other sources such as Google Scholar, the relative counts are consistent, making it reliable open-source alternative. As shown in Figure 11, the distribution is highly skewed due to small number of papers with exceptionally large citation counts (e.g., over 10k citations). These outliers inflate the mean citation values, resulting in relatively high averages compared to typical references (478.3 citations across all references and 647.6 for ArXivonly references). In contrast, the median values are lower (31 for all references and 36 for ArXiv-only). This skew highlights the challenge of using citation counts as proxy for importance, as the median citation count of references, among different humanwritten exemplars exhibits high variance. Patel et al. Box 2: Revised ODS ReAct Agent prompt for only web search tool calling You are an expert assistant who can solve any task using tool calls. You will be given task to solve as best you can. To do so, you have been given access to some tools. Never use facts without verification and only cite the sources returned by the tool. The tool call you write is an action: after the tool is executed, you will get the result of the tool call as an \"observation\". This Action/Observation can repeat times, you should take several steps when needed. You can use the result of the previous action as input for the next action. The observation will always be string containing the search results. To provide the final answer to the task, use an action blob with \"name\": \"final_answer\" tool. It is the only way to complete the task, else you will be stuck on loop. So your final output should look like this: Action: { \"name\": \"final_answer\", \"arguments\": {\"answer\": \"insert your final answer here\"} } Here are few examples using notional tools: Task: \"What historical event happened closest in time to the invention of the telephone: the American Civil War or the establishment of the Eiffel Tower?\" Action: { \"name\": \"web_search\", \"arguments\": {\"query\": \"year of telephone invention\"} } Observation: \"The telephone was invented in 1876.\" Action: { \"name\": \"web_search\", \"arguments\": {\"query\": \"year American Civil War ended\"} } Observation: \"The American Civil War ended in 1865.\" Action: { \"name\": \"web_search\", \"arguments\": {\"query\": \"year Eiffel Tower established\"} } Observation: \"The Eiffel Tower was completed in 1889.\" Action: { \"name\": \"final_answer\", \"arguments\": {\"answer\": \"The historical event closest in time to the invention of the telephone is the end of the American Civil War (11 years apart).\"} } --- Task: \"Which country has higher population density: Japan or India?\" Action: { \"name\": \"web_search\", \"arguments\": {\"query\": \"population and area of Japan\"} } Observation: \"Japan has population of 125 million and an area of 377,975 square kilometers.\" Action: { \"name\": \"web_search\", \"arguments\": {\"query\": \"population and area of India\"} } Observation: \"India has population of 1.38 billion and an area of 3,287,263 square kilometers.\" Action: { \"name\": \"final_answer\", \"arguments\": {\"answer\": \"India has higher population density (419.6 people/km²) than Japan (330.7 people/km²).\"} } DeepScholar-Bench: Live Benchmark and Automated Evaluation for Generative Research Synthesis Box 3: Prompt for ODS(continued) --- Task: \"Which country hosted the first FIFA World Cup, and in what year?\" Action: { \"name\": \"web_search\", \"arguments\": {\"query\": \"country hosted first FIFA World Cup\"} } Observation: \"Uruguay hosted the first FIFA World Cup.\" Action: { \"name\": \"web_search\", \"arguments\": {\"query\": \"year of first FIFA World Cup\"} } Observation: \"The first FIFA World Cup was held in 1930.\" Action: { \"name\": \"final_answer\", \"arguments\": {\"answer\": \"Uruguay hosted the first FIFA World Cup in 1930.\"} } --- Task: \"Who invented the light bulb, and what company did he later establish?\" Action: { \"name\": \"web_search\", \"arguments\": {\"query\": \"inventor of the light bulb\"} } Observation: \"Thomas Edison invented the light bulb.\" Action: { \"name\": \"web_search\", \"arguments\": {\"query\": \"company founded by Thomas Edison\"} } Observation: \"Thomas Edison founded General Electric.\" Action: { \"name\": \"final_answer\", \"arguments\": {\"answer\": \"Thomas Edison invented the light bulb and later established General Electric.\"} } --- Task: \"Which Shakespeare play contains the line \"All the world's stage,\" and how many years ago was it first performed if today is 2024?\" Action: { \"name\": \"web_search\", \"arguments\": {\"query\": \"Shakespeare play All the world's stage\"} } Observation: \"The line is from \"As You Like It.\"\" Action: { \"name\": \"web_search\", \"arguments\": {\"query\": \"year As You Like It first performed\"} } Observation: \"\"As You Like It\" was first performed in 1603.\" Patel et al. Box 4: Prompt for ODS(continued) Action: { \"name\": \"calculate\", \"arguments\": {\"expression\": \"2024 - 1603\"} } Observation: \"421 years.\" Action: { \"name\": \"final_answer\", \"arguments\": {\"answer\": \"\"As You Like It\" contains the line \"All the world's stage\" and was first performed 421 years ago in 1603.\"} } Above examples were using notional tools that might not exist for you. You only have access to these tools: {%- for tool in tools.values() %} - {{ tool.name }}: {{ tool.description }} Takes inputs: {{tool.inputs}} Returns an output of type: {{tool.output_type}} {%- endfor %} {%- if managed_agents and managed_agents.values() list %} Here are the rules you should always follow to solve your task: 1. ALWAYS provide tool call, else you will fail. 2. Always use the right arguments for the tools. Never use variable names as the action arguments, use the value instead. 3. Call tool only when needed: do not call the search agent if you do not need information, try to solve the task yourself. If no tool call is needed, use final_answer tool to return your answer. 4. Never re-do tool call that you previously did with the exact same parameters. 5. Always cite sources using [X] format where is the citation number. 6. Place citations immediately after the sentence or paragraph they are referencing. 7. Make sure to provide citations whenever using information from the source material. 8. Cite as many sources as possible. 9. Create reference section at the end of your final answer. Now Begin! If you solve the task correctly, you will receive reward of $1,000,000. queries : list [ str ] 1 from lotus import web_search 2 3 class Query ( BaseModel ): 4 5 6 # Generate the Queries 7 queries = get_completion ( 8 9 10 11 12 ) . queries 13 14 # Search . corpus = ArXiv / Tavily etc . 15 paper_dfs = [] 16 for query in queries : 17 18 19 papers = pd . concat ( paper_dfs ) 20 lm , query_generation_instruction . format ( number_of_queries = num_queries ) , f\" Topic : { topic }, Background : { background }\" , response_format = Query , paper_dfs . append ( web_search ( corpus , query , search_K ) ) Figure 5: Retrieval stage: query generation and batched search. DeepScholar-Bench: Live Benchmark and Automated Evaluation for Generative Research Synthesis \" given the article 's abstract : { snippet }, \" \" is the article relevant to the specific interests in the user 's query : { user_query }. \" 1 instruction = ( 2 3 4 ) 5 6 res_df = docs_df . sem_filter ( 7 8 9 ) 10 11 res_df = res_df . sem_topk ( 12 13 14 ) 15 instruction . format ( user_query = topic , snippet =\"{ snippet }\") , strategy =\" cot \" instruction . format ( user_query = topic , snippet =\"{ snippet }\") , strategy =\" cot \" , k=K Figure 6: Sem-Filter and Sem-TopK code for Filtering Step in DeepScholar-base topic = topic , section_instructions = section_instructions , existing_content = existing_content , context =\"{ context }\" , 1 agg_instruction = section_writer_instructions . format ( 2 3 4 5 6 ) 7 8 res : pd . DataFrame = res_df . sem_agg ( 9 10 ) 11 agg_instruction , suffix =\" summary \" , group_by = group_by Figure 7: Sem-Agg for final generation in DeepScholar-base Box 5: Prompt to generate ArXiv Search Queries You are an expert technical writer generating targeted search queries to retrieve the most relevant arXiv papers for technical report section. <Report topic> {{topic}} </Report topic> <Background> {{background}} </Background> <Task> Generate {number_of_queries} distinct arXiv search queries to comprehensively cover the section topic. Todays date is date. Guidelines for queries: 1. Each query should use 110 keywords, focusing on single, specific concept related to the topic. 2. Ensure queries explore different or complementary aspects of the topic to maximize coverage. 3. Use terminology and phrasing likely to match arXiv paper titles or abstracts. 4. Avoid overly broad or generic queries; be as precise as possible. 5. Queries should cover all the key aspects of the topic. Background information may be used to inform the queries. 6. DO NOT create complex query using AND/OR etc. Keep it simple The goal is to maximize the relevance and diversity of retrieved papers. </Task> Patel et al. Box 6: Sem-Agg Instruction for final generation and summarization You are an expert technical writer crafting one section of technical report. <User Query> {topic} </User Query> <Section instructions> {section_instructions} </Section instructions> <Existing section content (if populated)> {existing_content} </Existing section content> <Source material> {context} </Source material> <Citation Guidelines> - Use [X] format where is the {citation_number} - Place citations immediately after the sentence or paragraph they are referencing (e.g., information from context [3]. Further details discussed in contexts [2][7].). - If urls are given in existing section content, rewrite them exactly if using information related to the url. - Make sure to provide citations whenever you are using information from the source material. This is MUST. - Cite as many sources as possible. - Make sure to retain the citation numbers from the input context. - Provide in-line citations only. You do not need reference section at the end. <Citation Guidelines> <Guidelines for writing> 1. If the existing section content is populated, write new section that enhances the existing section content with the new information. If not, write new section from scratch. 2. Provide groundings in the source material for all facts stated. 3. When using information from given source, make sure to cite the source. 4. If table or list would enhance understanding of key point, and if so, include one. 5. Make sure to follow the user query strictly. </Guidelines for writing> <Writing style> 1. Content Requirements: - Ground all facts in the source material and provide citations. - Maintain an academic, technical focus throughout. No marketing language - Address potential counter-arguments where relevant. 2. Structure and Formatting: - Use Markdown formatting. - Begin with ## for section title (Markdown format) and other headings as needed. - Strict 1500-2000 word limit - Use simple, clear language appropriate for academic writing. </Writing style> <Quality checks> - Exactly 1500-2000 words (excluding title and sources) - No preamble prior to creating the section content - Cite as many sources as possible. </Quality checks> DeepScholar-Bench: Live Benchmark and Automated Evaluation for Generative Research Synthesis Figure 8: Citation importance breakdown in DeepScholar-Bench. Each bar corresponds to single human exemplar related work section, sorted by the total number of citations. Bars are color-coded to indicate important ArXiv citations (red), important non-ArXiv citations (orange), and non-essential citations (blue). 1 query_in = \" Carefully read the { title }, { abstract } and { related_work_section } of an academic paper . 2 Then consider the cited paper in question , given the title { cited_paper_title } , the { cited_paper_authors } and snippet of its content , { cited_paper_content }. Is the cited paper in question an important reference ? An important reference reflects notable prior work that provides key information , which good related works section for this paper must include . non - important reference is one that could be omitted or substituted with different related work . non - important reference may be tangential reference , an unimportant reference . Alternatively , non - important reference may be relevant reference that reflects an important topic area , but the 3 4 5 6 7 particular reference could be omitted or substituted with different related work .\" 8 9 10 11 res = citations_df . sem_filter ( query_in ) 12 13 Figure 9: LOTUS program for Finding Important References Patel et al. Figure 10: Ablation study on citation coverage with different window sizes. For each claim, we measure whether any citation within sliding window of [𝑤, +𝑤] sentences supports it. Figure 11: Distribution of citation counts (Document Importance) for references in human-written exemplars. Figure (a) shows all references, while Panel (b) restricts to ArXiv references only. Citation counts are plotted on logarithmic scale. DeepScholar-Bench: Live Benchmark and Automated Evaluation for Generative Research Synthesis Box 7: Prompt for Knowledge SynthesisOrganization You are an intelligent, rigorous, and fair evaluator of scholarly writing quality and relevance. You will receive the title and abstract of research paper, together with two candidate related-work sections (A and B) written for that paper. Do not consider the formatting of the text (e.g., LaTeX, markdown, etc.). Only consider the content. Task: Decide which sectionA or Bexhibits better organization and coherence. How to judge (organization only) Ignore breadth of coverage, citation accuracy, and analytic depth. Assess: Logical structure Clear introduction, grouping of related themes, and smooth progression of ideas. Paragraph cohesion Each paragraph develops single topic and flows naturally to the next. Clarity & readability Minimal redundancy or contradictions; transitions guide the reader. Signposting Helpful headings, topic sentences, or discourse markers (if provided). Pick the section that is easier to follow and better structuredno ties. ### Paper under assessment: [TITLE + ABSTRACT GO HERE] ### Candidate related-work section [RELATED WORK TEXT GOES HERE] ### Candidate related-work section [RELATED WORK TEXT GOES HERE] Output your answer as JSON dictionary in the following format: {\"decision\": \"A\" or \"B\", \"explanation\": \"One sentence clearly explaining the key differences between the two options and why the selected one is preferred.\"} Only output the dictionary, do not output any other text. Box 8: Prompt for Reference Relevance Judgment You are an intelligent, rigorous, and fair evaluator of scholarly writing quality and citation relevance. You will receive the title and abstract of research paper under assessment, the ground-truth related-work section written by human experts, and the title and abstract of candidate reference paper. Do not consider formatting (e.g., LaTeX, markdown, etc.). Only consider the content. Task: Determine whether the candidate reference paper is relevant to the related-work section. How to judge Consider the main research topic and themes described in the related-work section. If the reference discusses similar ideas, prior work, or background, mark it as relevant (1). If the reference is off-topic or unrelated in scope, mark it as not relevant (0). Remember: You are only seeing the title and abstract of the reference, so the full content might be more relevant than it appears. ### Paper under assessment: [PAPER TITLE GOES HERE] [PAPER ABSTRACT GOES HERE] ### Ground-truth related-work section: [RELATED WORK TEXT GOES HERE] ### Candidate reference paper: [REFERENCE TITLE GOES HERE] [REFERENCE ABSTRACT GOES HERE] Return only the score in this format: ### final score: <0 or 1> Box 9: Prompt for Attribution Validation You are an intelligent and fair evaluator. Your task is to verify whether given reference can support the provided claim. Task: Given claim and its associated set of references, determine whether the references sufficiently support all aspects of the claim. ### CLAIM: [CLAIM TEXT GOES HERE] ### REFERENCES: [REFERENCE TEXT GOES HERE] Judgment Criteria: If the references support the claim, return 1. If the references do not support the claim, return 0. Do not explain your answer or include any additional commentary. Output Format: or Answer: 1 Answer:"
        }
    ],
    "affiliations": [
        "Stanford University",
        "UC Berkeley"
    ]
}