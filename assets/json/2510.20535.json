{
    "paper_title": "ARC-Encoder: learning compressed text representations for large language models",
    "authors": [
        "Hippolyte Pilchen",
        "Edouard Grave",
        "Patrick PÃ©rez"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent techniques such as retrieval-augmented generation or chain-of-thought reasoning have led to longer contexts and increased inference costs. Context compression techniques can reduce these costs, but the most effective approaches require fine-tuning the target model or even modifying its architecture. This can degrade its general abilities when not used for this specific purpose. Here we explore an alternative approach: an encoder that compresses the context into continuous representations which replace token embeddings in decoder LLMs. First, we perform a systematic study of training strategies and architecture choices for the encoder. Our findings led to the design of an Adaptable text Representations Compressor, named ARC-Encoder, which outputs $x$-times fewer continuous representations (typically $x\\!\\in\\!\\{4,8\\}$) than text tokens. We evaluate ARC-Encoder across a variety of LLM usage scenarios, ranging from in-context learning to context window extension, on both instruct and base decoders. Results show that ARC-Encoder achieves state-of-the-art performance on several benchmarks while improving computational efficiency at inference. Finally, we demonstrate that our models can be adapted to multiple decoders simultaneously, allowing a single encoder to generalize across different decoder LLMs. This makes ARC-Encoder a flexible and efficient solution for portable encoders that work seamlessly with multiple LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder , fine-tuning dataset and pretrained models are available at https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 5 3 5 0 2 . 0 1 5 2 : r Preprint ARC-ENCODER: LEARNING COMPRESSED TEXT REPRESENTATIONS FOR LARGE LANGUAGE MODELS Hippolyte Pilchen, Edouard Grave & Patrick Perez Kyutai, Paris, France. {hippolyte.pilchen,egrave,patrick}@kyutai.org"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent techniques such as retrieval-augmented generation or chain-of-thought reasoning have led to longer contexts and increased inference costs. Context compression techniques can reduce these costs, but the most effective approaches require fine-tuning the target model or even modifying its architecture. This can degrade its general abilities when not used for this specific purpose. Here we explore an alternative approach: an encoder that compresses the context into continuous representations which replace token embeddings in decoder LLMs. First, we perform systematic study of training strategies and architecture choices for the encoder. Our findings led to the design of an Adaptable text Representations Compressor, named ARC-Encoder, which outputs x-times fewer continuous representations (typically {4, 8}) than text tokens. We evaluate ARC-Encoder across variety of LLM usage scenarios, ranging from in-context learning to context window extension, on both instruct and base decoders. Results show that ARC-Encoder achieves state-of-the-art performance on several benchmarks while improving computational efficiency at inference. Finally, we demonstrate that our models can be adapted to multiple decoders simultaneously, allowing single encoder to generalize across different decoder LLMs. This makes ARC-Encoder flexible and efficient solution for portable encoders that work seamlessly with multiple LLMs. We release training code1, fine-tuning dataset and pretrained models2."
        },
        {
            "title": "INTRODUCTION",
            "content": "As their use expands, LLMs are required to process increasingly long contexts to incorporate detailed user prompts or external knowledge retrieved from large corpora as in retrieval-augmented generation (RAG) systems (Lewis et al., 2021). However, this implies large computational cost at inference due to the quadratic complexity of Transformer attention mechanisms (Tay et al., 2022). Furthermore, using longer contexts can dilute interesting information leading to poor downstream results or can even reach the context window limit of the LLM, which damages the model capacities. To address this issue, context compression is promising solution. It reduces input length while preserving the semantics necessary for accurate generation (Liu et al., 2025). Techniques fall into two main categories: hard compression, which prunes, summarizes or deletes tokens (Jiang et al., 2023b), offering interpretability and model-agnosticism but limited compression; and soft compression, which encodes context into dense vectors, e.g. memory tokens and gist tokens (Mu et al., 2024; Ge et al., 2024). While achieving higher compression, soft methods often require training specialized encoders and even adapting the decoder itself (Louis et al., 2025a; Tang et al., 2025). Our work aims at leveraging LLMs compression ability in cases where documents should be processed on the fly. Most importantly, we constrain ourselves not to modify the decoder so that the method remains plug-and-play while maintaining degree of flexibility between compression and accuracy on downstream tasks. In this paper, we introduce an Adaptable text Representations Compressor (ARC-Encoder) that produces pooled tokens, optimized to be directly consumable by 1https://github.com/kyutai-labs/ARC-Encoder 2ARC-Encoders & ARC finetuning dataset 1 Preprint decoder in the same way as standard input tokens (injected after the embedding matrix). By preserving the few-shot abilities of base models (Brown et al., 2020), we ensure compatibility with the standard question-answering (QA) evaluation set-up, such as 5-shot evaluation using exact match (EM). We achieve nearly the same accuracy as decoder operating on full text, even at 4 pooling factor while using an encoder adaptable to several decoders by means of small MLP which has less than 1% of the encoder parameters. To the best of our knowledge, our approach achieves stateof-the-art results among models that do not fine-tune the decoder. We further adapt it to context extension applications and show that it also works in zero-shot setting using instruct models as decoders. Here is summary of our main contributions: We introduce ARC-Encoder, method to compute compressed text representations that replace the raw text input in LLMs. Our approach reduces the input sequence length, without requiring any modification to the decoder model. ARC-Encoder preserves strong performance across various benchmarks and scenarios, including in-context learning (ICL). We show that single encoder can be trained to work with multiple decoders, requiring less than 1% of specific parameters per LLM. ARC-Encoder can be further adapted to new decoders with minimal adjustment. ARC-Encoder can be trained to extend decoder context size, by compressing the chunks of large document in parallel, showing competitive results on long-context benchmarks. Finally, we show that both pretraining and fine-tuning are key to the success of our approach. We also find that the compressed representations of Wikipedia require memory on the same order as that needed to store the raw text, allowing to precompute representations."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Encoder-Decoder architectures. Text auto-encoders have long been studied to improve transformers on specific downstream tasks. They process the input into dense embeddings to reduce processing cost while preserving or improving model accuracy. For instance, Atlas (Izacard et al., 2022) retrieves and encodes multiple relevant passages before decoding with focus on knowledge intensive tasks. RAVEN (Huang et al., 2024) uses similar retrieval-augmented encoder-decoder structure and improves in-context learning abilities while using less compute. More recently, Zhang et al. (2025) propose an asymmetric architecture, closer to ours, where smaller encoder aims at improving the decoder generation through cross-attention while using less compute. Context Compression. Context compression reduces the number of tokens processed by model to improve the efficiency of inference. There are two main approaches. Hard compression methods, such as LLMLingua (Jiang et al., 2023b), operate directly in the text space by removing tokens from prompts. It aims at reducing their length while preserving the performance of the model. In contrast, we perform soft compression which involves learning continuous compressed representations. This line of work began with gist tokens (Mu et al., 2024), which summarize task instructions into few tokens by modifying the attention matrix to force generated tokens to only attend to gist tokens. Simarly, in memory tokens (Ge et al., 2024), learnable vectors which come from an encoder are prepended to the input sequence. These vectors serve as condensed representations of the full sequence when passed through the decoder. These methods typically rely on pretraining phase to align the encoders output with the decoders hidden states, followed by fine-tuning of both encoder and decoder to fully leverage the compressed representations (Louis et al., 2025a;b). Recently, more similarly to our pooling method, Tang et al. (2025) explore using merged tokens to replace memory ones. They perform several training stages on the encoder, but also on the decoder in contrast to our work. Other approaches explore the use of pre-computed text embeddings as memory tokens, reaching higher pooling factors (up to 150) with xRAG (Cheng et al., 2024) but performing poorly on certain benchmarks and lacking compression flexibility. All these context compression methods rely on the intrinsic compression capacity of LLMs. Indeed, Kuratov et al. (2025) has proven that an LLM decoder can be used to directly compress text passage of roughly 1568 tokens into just one 4096-dimensional vector. More recently, Eyuboglu et al. (2025) leveraged this property to produce sets of compressed KV-caches for frequently used long documents. 2 Preprint Long Context. Recent work on long-context language modeling combines fine-tuning with extended positional encoding strategies. Together AI (2023) fine-tunes Llama2 7B to handle 32ktoken inputs using position interpolation (Chen et al., 2023). In contrast, Yen et al. (2024) introduce lightweight encoder that processes long inputs in parallel and passes compressed representations to decoder via learned cross-attention, allowing efficient long-sequence handling. Our work follows the idea of Yen et al. (2024) while producing fewer tokens and keeping the decoder untouched."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 ARCHITECTURE The overall architecture that we consider comprises text encoder and an MLP projector, together forming trainable ARC-Encoder, followed by the frozen target decoder. The encoder is based on an LLM transformer, from which we remove the output head and the causal mask. We add pooling mechanism that reduces the number of elements in the sequence from to with the pooling factor (PF). The MLP projector is 2-layer MLP without activation, mapping the encoder output to the embedding dimension of the decoder through dimensional bottleneck. The decoder remains unchanged, as opposed to the encoder and MLP that are trained. The compressed continuous representations from the ARC-Encoder are used instead of the token embeddings in the decoder. 3.2 POOLING METHOD Most context compression works use learned or memory tokens as compressed representations. This makes it harder to compress well sequences of various sizes, as the number of output representations is fixed. Instead, we pool hidden state vectors directly, leading to fixed pooling factor, independent of the input sequence length (Suganthan et al., 2025). We performed an empirical study on how and where to pool tokens to obtain compressed continuous representations. As illustrated in Fig. 1, pooling is performed in the self-attention module. We average consecutive queries to reach the targeted pooling factor, while keys and values remain unchanged. For PF of 2 for example (denoting the encoder as ARC2-Encoder), we group the tokens of the sequence two-by-two. We merge their queries in the last self-attention module, by averaging their continuous hidden states. Then, these pooled queries attend the non-compressed keys and values, mimicking standard self-attention, but with two-times fewer queries, resulting in pooling factor of two. We explored inserting the pooling mechanism earlier in the encoder, but this lead to poorer performance. This follows the intuition that the information should be as processed as possible before pooling (Tang et al., 2025). 3.3 TRAINING Figure 1: Pooling in ARC2Encoder. In the last SA module, queries are merged by pairs of successive tokens. Base pretraining. Memory tokens (Ge et al., 2024) show the importance of the reconstruction task when training the model to create easily decompressible continuous representations. However, pure reconstruction is easier than compressing contexts for downstream tasks. In fact, with proper training, our auto-encoder architecture achieves near-perfect reconstruction at pooling factor on relatively short sequences (up to around 128 tokens). Unfortunately, these compressed representations cannot be well exploited by the decoder on downstream tasks, as the model tends to regurgitate the entire context, instead of extracting pertinent information from it. Thus, we consider second pretraining task, continuation, which is better aligned with inference-time behavior. It consists in replacing subsequences of natural text by their compressed representations, and to teacher-force the continuation immediately following compressed segments. We alternate between these two pretraining tasks as summarized in Fig. 2, and use the standard cross-entropy loss. Additionally, we append 3 Preprint Figure 2: ARC-Encoder pretraining tasks. The encoder, special tokens and the MLP are trained through two alternating tasks: a) Reconstruction: compressed tokens are given to the decoder which is teacher-forced to replicate the full text tokens; b) Continuation: subpart of the tokens in the sequence are compressed and the decoder is teacher-forced to continue starting from the partially compressed sequence. Illustration for ARC2-Encoder. special learned tokens <Cont> and <Rec> after each compressed sequence. The two special tokens are crucial to specify the task during pretraining and enable good downstream results. Fine-Tuning. After pretraining, our ARC-Encoder can be flexibly adapted to range of specific tasks. In particular, it can be used to condition frozen decoder on task-specific inputs, allowing task specialization without modifying the decoder itself. To preserve the decoders few-shot capabilities, we optionally include small number of in-context examples in the continuation objective interleaving compressed documents, full-text queries and answersfollowing structured prompt template described in Appendix C. To compute the loss, we mask all tokens except the ones that continue the last compressed sequence, corresponding to the final answer in few-shot setting. This design encourages generalization and preserves ICL possibilities at inference time with every document being compressed. Furthermore, we demonstrate that zero-shot abilities of instruct decoders can also be preserved; it simply requires to use different pretraining and fine-tuning template. Once pretrained, ARC-Encoder can be fine-tuned to master in-context learning, long context understanding, or other downstream tasks. The pooling factor between the two training stages can be changed. It leads to improved models when pretraining with higher factor than for fine-tuning, see Appendix A.2. We remind that only the encoder is fine-tuned, and that the decoder is unchanged. Multi-Decoder Training. To improve the generality of our method, we design compressor capable of generating token representations that can be used by multiple decoders without any modification. It is nontrivial challenge due to inherent discrepancies between the hidden-state spaces of different decoder architectures. To overcome this, we employ shared encoder but we specialize projector layer (the MLP in Fig. 2) and the special tokens. This set of learned parameters accounts for less than 1% of the encoders weights, enabling decoder-specific adaptation with minimal overhead. We find that training with an alternating objective provides the most stable and effective results. At each training step we sample decoder uniformly and update only its associated projector as well as the shared encoder. This strategy ensures balanced exposure across decoders while maintaining the generalization capacity of the shared encoder."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we first describe the main settings used for our experiments. Second, we evaluate our method on various downstream tasks that use contexts, such as question answering with retrieved documents or reading comprehension. The contexts for these tasks tend to be short, ranging from 30 to 1,500 tokens. Thus, we also evaluate ARC-Encoders for long-context understanding applications. 4 Preprint Finally, we report the results of our ablation study and an analysis of the memory used to encode Wikipedia with continuous representations from ARC-Encoder. 4.1 EXPERIMENTAL SETTING Models. We perform experiments using three different decoders: Llama3.1 8B (Grattafiori et al., 2024), Mistral 7B (Jiang et al., 2023a), both base models, as well as Llama2 7B Chat (Touvron et al., 2023). We design our ARC-Encoder with Llama3.2 3B as the backbone: we remove its last two layers and no causal mask is applied. The MLP consists of two layers: the first projects the 3072-dimensional vectors from the encoder to 2048 dimensions, and the second projects these vectors to 4096-dimensional hidden states, matching the decoder dimension. During inference, we append the special <Cont> token after every sequence of compressed tokens. Training & Datasets. By default, we decide to train all layers of the encoder, including the embedding matrix, using AdamW optimizer (Loshchilov & Hutter, 2019). For pretraining, we use data from Common Crawl that has been filtered and processed using dactory3, keeping samples with quality score higher than 0.2. ARC-Encoder is pretrained on approximately 2.6B tokens. For finetuning, we use two different mixes of synthetic and supervised datasets: one for standard context compression benchmarks using base models as decoders and another for long-context benchmarks using an instruct decoder. See the following sections and Appendix C.3 for more details. 4.2 CONTEXT COMPRESSION Benchmarks. We evaluate our method on question answering, translation, and summarization tasks. For question answering, we use HotpotQA (Yang et al., 2018, HQA) with the distractor setting, Natural Questions (Kwiatkowski et al., 2019, NQ), TriviaQA (Joshi et al., 2017, TQA) and SQuAD (Rajpurkar et al., 2016). Top-5 passages are retrieved using NV-Embed v2 (Lee et al., 2025) from Atlas (Izacard et al., 2022) Wikipedia chunks, simulating RAG setup. We report exact match (EM) as the main metric, where EM = 1 if the normalized predicted and reference answers exactly match. For translation, we evaluate on FLORES (Goyal et al., 2021) averaging BLEU scores across four directions: English to Danish, French, German, and Spanish. Summarization performance is evaluated on CNN-DailyMail using ROUGE-L, which aligns well with human judgments for summarization abilities according to Zhang et al. (2023). All models are evaluated via 5-shot with compressed contexts. The reported pooling factor reflects the per-context ratio of original tokens (using the decoder tokenizer) to compressed tokens. See Appendix D.2 for full evaluation details. Setting & Baselines. Unlike prior works in the context compression literature that often report zero-shot accuracy with instruct decoderspotentially inflating performance when models simply replicate the contextour use of EM aims at better capturing real-world LLM usage by rewarding answers following the few-shot format patterns. We believe it better measures the encoder abilities to produce useful representations, from which the decoder can extract information. Thus, we focus on base model decoders for context compression evaluations. We first set two baselines that reflect the decoder intrinsic abilities. In the first one, denoted closed-book, the decoder relies only on its parametric knowledge and in the second one, called open-book, the decoder has access to uncompressed documents in its context. For meaningful comparisons, we select diverse set of strong baselines that capture different approaches to context compression. These include: i) LLMLingua2 (Pan et al., 2024), which performs hard compression; ii) ICAE (Ge et al., 2024), soft compression approach using memory tokens; iii) xRAG (Cheng et al., 2024), which relies on pre-computed embeddings for retrieval-augmented generation; iv) PISCO (Louis et al., 2025a), which fine-tunes both the encoder and decoder and states that pretraining is not necessary. We re-implemented the last three baselines using our decoder, fine-tuning dataset, and interleaved fine-tuning task format to ensure consistent few-shot evaluation setup while preserving each methods core design. This allows direct comparisons with common evaluation protocol; See Appendix D.1 for implementation details. We fine-tune models on mix of synthetic translation data and supervised datasets (QA, summarization, paraphrasing, and reading comprehension), explicitly excluding the training sets of our 3https://github.com/kyutai-labs/dactory 5 Preprint Table 1: Main comparison of ARC-Encoder and other models. PF (pooling factor): the token reduction factor (e.g., 4) for fixed-ratio methods or the number of compressed tokens used, e.g. 16, when this number is fixed as this yields benchmark-dependent ratios; Param.: number of parameters of the encoder; Avg. length: mean number of tokens per context document. The superscript on ARC-Encoder indicates if the model is specifically trained for one decoder (M for Mistral or for Llama) or both simultaneously (). marks modified re-implementations, see details in Appendix D.1. Best context compression results are in bold, second best are underlined. PF Param. NQ TQA HQA SQuAD FLORES CNN Avg. Methods Avg. length o B 7 t r c 8 1 . 3 l closed-book open-book ICAE-like xRAG-like LLMLingua2 PISCO-like 1 7.2B 32 1 7.1B 1.9 0.6B 7.2B 32 4 ARC4-Encoder 4 ARC4-EncoderM 4 ARC8-EncoderM 8 3.0B Avg. length closed-book open-book ICAE-like xRAG-like LLMLingua2 PISCO-like 1 7.6B 32 1 7.1B 2.0 0.6B 7.6B 32 4 ARC4-Encoder 4 ARC4-EncoderL 4 ARC8-EncoderL 8 3.0B 155 152 1479 29.1 39.9 36.5 30.7 38.8 34.7 36.6 62.4 70.5 66.7 65.2 69.0 68.5 69. 22.8 48.3 24.3 21.5 43.7 24.9 29.4 36.4 69.7 43.6 39.0 68.9 45.1 40.8 67.9 38.4 135 133 1285 21.6 60.6 25.4 47.1 67.1 38.6 20.5 67.3 38.4 21.7 62.1 28.0 45.2 66.3 36.1 35.1 30.6 69.4 37.9 70.5 37. 37.3 45.7 69.9 39.7 70.1 46.9 42.8 69.0 38.9 185 17.1 77.7 58.8 23.9 59.2 38.2 48.1 67.9 71.1 62.0 164 15.3 72.2 61.6 22.3 58.8 40.5 57. 72.6 74.0 66.0 30 956 31.3 28.3 0.9 12.6 33.6 34.5 29.1 31.0 28.3 32.8 31.3 3.4 13.6 35.2 36.5 33.1 33.7 30.6 27.2 15.8 14.6 24.9 19.2 19.3 49.2 38.4 26.1 41.4 36.5 39.5 26.1 45.5 23.8 46.5 43.4 22.9 26.5 17.3 12.7 23.8 19.7 20.7 47.4 39.4 25.0 40.6 38.4 43.3 25.6 47.4 23.7 48.0 45.0 22.8 evaluation benchmarks. This setup better highlights our methods generalization ability (using the train sets of benchmarks strongly improves results, sometimes outperforming the open-book results as shown in Tab. 6 of the Appendix). Since each data sample is drawn from one of the sub-datasets, fine-tuning involves stochasticity; unless otherwise specified, we fix the random seed to 0. For our final ARC-Encoder we follow the best pretraining / fine-tuning pooling factor pairs as shown in Fig. 5. This leads to ARC4-Encoder using the same pretrained encoder as ARC8-Encoder, only with its fine-tuning performed at pooling factor of 4. Results. In Tab. 1, we report our main results on context compression using the best checkpoints of ARC-Encoder. First, we observe that our models outperform the closed-book baseline, showing that frozen decoders can extract useful information for downstream tasks from the compressed tokens. We also note that ARC-Encoder tends to outperform the baselines more on tasks where the decoder cannot rely on its own parametric memory, such as reading comprehension (SQuAD) or summarization (CNN). Furthermore, we observe that both specialized and shared ARC-Encoder perform better when paired with the Llama3.1 8B decoder, likely because it belongs to the same model family as our encoder backbone based on Llama3.2 3B. Finally, ARC4-Encoder nearly matches the open-book baseline without altering the decoder and while achieving 1.8 gains of prefilling FLOPs, as profiled in Appendix B.1. Please note that for certain baselines, our results differ from the ones reported in previous work. This is due to the different setting that we consider in this paper, namely 1) using exact match as the metric, 2) excluding the training sets of the benchmarks from the fine-tuning data and 3) using base models instead of instruct ones. Encoder adaptation to multi-decoder. Through multiple experiments, we found that single encoder can be trained to be used by multiple decoders. More specifically, during both pretraining 6 Preprint and fine-tuning, we use joint learning setup where at each training step, we sample which decoder to use among the two targets. To improve performance, we introduce separate MLP projector for each decoder. This allows lightweight specialization of compressed representations, adding only 15M parameters per decoder. We report results in Tab. 1, showing that in average, the common encoder, ARC-Encoder, looses less than 1.0 point compared to its specialized counterparts. More encoder-decoder pairings are tested in Appendix A.3. Adaptation to new decoders. Once our ARC-Encoder has been trained to work with two decoders, we can adapt it to new decoders with minimal adjustments. Indeed, we solely learn new MLP projector and special tokens to feed third decoder, OLMo-7B (Groeneveld et al., 2024), while keeping the encoder frozen. Fine-tuning leads to better results than the closed-book baseline by only training 15M parameters. However, the score gap with the open-book setting remains relatively larger than when training with the alternating decoder objective as shown in Tab. 1 with Llama and Mistral models. Interestingly, on benchmarks where the decoder was limited by its context window, such as HotpotQA, using ARC4-Encoder outperforms the open-book setup. Figure 3: Extending an ARCEncoder to new decoder. Table 2: Adaptation to new decoder. Due to OLMo 7B 2048-token context window, we truncate documents for open-book baseline to 400 tokens. Per decoder specific parameters are reported. Methods PF Param. NQ TQA HQA SQuAD FLORES CNN Avg. closed-book 7 - O 19.5 open-book 35.3 ARC4-EncoderM 4 15M 31.5 ARC4-Encoder 4 33.1 1 48.3 64.8 62.5 63.1 17.8 22.9 26.5 25.0 11.2 67.9 46.4 44.6 22.2 17.2 17.1 24.2 19.1 18.9 39.6 33.9 33. 4.3 LONG CONTEXT In this section, we adapt our fine-tuning method to handle longer contexts, testing our architecture on long-context understanding tasks. We pretrain and fine-tune an ARC8-Encoder paired with an instruct decoder Llama2 7B Chat (Touvron et al., 2023), and refer the reader to Appendix for technical details. During fine-tuning, we encode fixed-size chunks of long documents in parallel and feed them to the decoder by concatenating the compressed tokens of each chunk. For fine-tuning, we synthesize QA and summarization dataset based on concatenated Wikipedia chunks, PG-19 books (Rae et al., 2019) and ArXiv papers from RedPajama (Weber et al., 2024) using Gemma3-27B (Team et al., 2025). Then, we divide each context in up to 32 chunks of 1024 tokens. Similarly at inference, contexts are truncated to 32k tokens and then ARC8-Encoder processes chunks in parallel. In this setting, we remove the special tokens, as instruction prompts play similar role. Benchmarks & Models. For long-context understanding, we report F1 score on NarrativeQA and QASPER and report Rouge-L on GovReport and QM-Sum validation sets from the ZeroSCROLLS benchmark (Shaham et al., 2023), suite of zero-shot long-context understanding tasks. Specifically, we adopt the task formats and instructions as used in Yen et al. (2024)4. On these benchmarks, we compare our model to Llama2 7B Chat, which is constrained by context window of 4096 tokens, as well as to open-source models specifically designed to extend its limited context window. These include Llama2-32k Instruct (Together AI, 2023), which relies on positional interpolation combined with fine-tuning, and CEPED (Yen et al., 2024), which employs lightweight encoder to process chunks of input in parallel, feeding their representations into decoder through learned cross-attention layers. Results. In Tab. 3, we show that feeding compressed tokens from ARC8-Encoder to Llama2 Chat It allows the model to process up to 8 substantially improves long-context QA performance. more input than its original context window. This also shows that the decoder can interpret the 4https://github.com/princeton-nlp/CEPE Preprint Table 3: Long-context evaluation on long-context benchmarks. The token count includes all tokens fed to the decoder. CEPED uses 2k decoder tokens plus encoder-side tokens. Max. Tokens NQA Qspr GvRp QM-Sum Models Llama2 Chat + CEPED Llama2-32k Instruct ARC8-Encoder + Llama2 Chat 4k (32k//8) 2k + 30k 32k 4k 16.1 20.5 14.2 27.5 17.2 19.7 16.4 28. 15.7 12.7 17.8 14.1 19.8 19.7 17.6 19.1 compressed tokens without any parameter modification. Thus, small models context window can be extended simply by training an external compressing encoder. On some tasks, our approach even outperforms methods that expand Llama2s context window through new learned internal modules or full-model fine-tuning. This advantage may stem partly from our synthesized fine-tuning dataset, which matches the answer-length distributions of evaluation benchmarks, as other models do not fine-tune specifically on these tasks. Crucially, ours is the only approach that leaves the decoder unchanged, ensuring identical behavior across all other tasks while improving the decoder on longcontext understanding. 4.4 COMPRESSION ABLATIONS In this section, we discuss key design choices of our method. We compare context compression results using the same evaluation setting as in Tab. 1, mostly showing the average score on all these benchmarks. Unless stated otherwise, models are pretrained and then fine-tuned using pooling factor of 8 with the Mistral 7B decoder. To reduce the computation costs of these ablations we pretrain on approximately 2B tokens only, roughly 75% of the tokens used for models in Tab. 1. Additional details are provided in Appendix C.1. Training objective. In contrast to Louis et al. (2025a), we show that pretraining is essential for our approach. While fine-tuning is also crucial, it cannot provide competitive results on downstream tasks on its own. Long pretraining is essential for aligning ARC-Encoder outputs with the decoders hidden state space. For example, after 20k pretraining steps, we observe an improvement of approximately +16 points on the average score, while after 80k steps, the improvement reaches +19 points, compared to directly fine-tuning without pretraining. Without fine-tuning the decoder fails to use the compressed context leading to large performance drop in translations, reading comprehension and summarization. Table 4: pretraining struction ratio. Impact of recon- % Rec. Avg. 39.8 0% 20% 41.6 50% 41.5 100% 37. Beyond pretraining length, the choice of pretraining tasks also proves critical: Tab. 4 shows that omitting reconstruction or using too little continuation leads to substantial performance drops. We also tried adding context distillation as in Cheng et al. (2024) during fine-tuning but it did not improve results while adding large computational overhead. Encoder Architecture. This ablation explores design choices for reducing encoder size or boosting performance. Thanks to strong results in multi-decoder and long-context applications, we keep the default setting architecture in Tab. 1 for consistency reasons, though other designs may excel in specific use cases. Tab. 5 shows the trade-off between encoder parameters and performance when truncating layers of the LLM backbone. Alternative pooling schedules also look promising: pooling every two tokens in the last layers performs better than our default at the same pooling factor, suggesting pooling strategies could be adapted to the target pooling factor. In addition, removing causality, which is effective only when training all layers improves encoder capacity. Adding special learned tokens is particularly useful to help encoder generalize to new decoders. Pooling. The pooling operation should merge information from continuous representations while still producing vectors interpretable by the decoder. We experiment with memory tokens, the standard approach in the field. This method performs poorly as sequence length increases (see Tab. 8 in appendix), since the effective compression becomes higher. We also test clustering queries with k-means and averaging those within the same cluster, but this merging of potentially distant tokens 8 Preprint Table 5: Ablations on encoder design. Default setting corresponds to ARC8-EncoderM with only 60k pretraining steps. All results are averaged over 3 fine-tunings with different seeds. Param. NQ TQA HQA SQuAD FLORES CNN Avg. Default setting 3.0B 36.9 67.2 39.9 58.3 27. 20.1 41.7 How to truncate the encoder? Truncate 0 layer Truncate 4 layers Truncate 21 layers 3.2B 38.8 67.7 2.8B 37.6 67.5 1.1B 37.0 67.3 39.4 39.1 32. 61.6 60.1 52.1 How to pool? by 2 every last layers by 2 every two layers 3.0B 38.2 68.4 38.3 68.1 40.0 38. 61.1 61.1 How to modify the encoder? LoRA (rank= 128) causality 38.8 67.0 38.7 67.9 37.0 37. 57.7 57.3 26.9 25.0 23.5 26.7 26.4 27.0 27.0 20.3 20.4 19.2 42.4 41.6 38. 20.1 20.0 42.4 42.1 19.1 19.4 41.1 41.3 proved harmful especially in translation tasks. It assumes in addition non-causal training of the encoder, failing completely without it. The most effective poolings instead use contiguous tokens, either by averaging them as in Fig. 1, or by selecting the last token of each segment. Since averaging proved more robust in the multi-decoder setting, we choose it as our default method. 4.5 MEMORY ANALYSIS When compressing contexts on the fly, ARC4-Encoder already leads to 1.8 speed-up compared to using the natural text. In the case where contexts are potentially used multiple times, such as in RAG systems, even greater speed-ups could be achieved by pre-computing the compressed representations and storing them. This option is only viable if the size of compressed representations is roughly the same as the original text. Here, we explore the following tradeoffs to reduce the size of compressed representations: 1) changing the pooling factor, 2) reducing the dimension of the MLP bottleneck and 3) quantizing the representations using product quantization (Jegou et al., 2011, PQ), by increasing the dimension of the sub-quantizers while keeping the number of centroids fixed. We report results in Fig. 4, showing that by combining these different methods, encoding English Wikipedia with ARC-Encoder requires 80 GiB with minimal impact on performance, or 20 GiB while still improving the closed book baseline significantly. For comparison, the raw text of English Wikipedia requires approximately 24 GiB, thus making ARC-Encoder suitable for pre-computing compressed representations."
        },
        {
            "title": "5 CONCLUSION",
            "content": "Figure 4: Compression results with varying MLP dimensional bottlenecks and number of bits per dimension (B/d). We introduce ARC-Encoder, novel method to compute compressed text representations that can replace the raw text input in large language models. By reducing the context length, our method 4https://github.com/facebookresearch/faiss/wiki 9 Preprint leads to faster prefilling and decoding stages, while leaving the target LLM unchanged. We show that single encoder can be trained to work with multiple decoders, or even extended to new decoders with minimal adaptation. This opens the way towards universal compressed representations. We show that pretraining and fine-tuning are both critical for the success of our approach. In terms of architecture, pooling in the attention mechanism leads to strong results, while allowing constant pooling factor for different sequence sizes, as opposed to memory tokens. Finally, using an MLP between the encoder and decoder allows our approach to compress representations further and to learn single encoder for multiple decoders."
        },
        {
            "title": "REFERENCES",
            "content": "Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. Beat the ai: Investigating adversarial human annotation for reading comprehension. Transactions of the Association for Computational Linguistics, 8:662678, December 2020. ISSN 2307-387X. doi: 10.1162/tacl 00338. URL http://dx.doi.org/10.1162/tacl_a_00338. 20 Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165. 2 Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation, 2023. URL https://arxiv.org/ abs/2306.15595. 3 Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. DialogSum: real-life scenario dialogue summarization dataset. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 5062 5074, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. findings-acl.449. URL https://aclanthology.org/2021.findings-acl.449/. 20 Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, and Dongyan Zhao. xrag: Extreme context compression for retrieval-augmented generation with one token, 2024. URL https://arxiv.org/abs/2405.13792. 2, 5, 8, 22 Nachshon Cohen, Oren Kalinsky, Yftah Ziser, and Alessandro Moschitti. WikiSum: Coherent summarization dataset for efficient human-evaluation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 212219, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.28. URL https://aclanthology.org/2021. acl-short.28/. Qingxiu Dong, Xiaojun Wan, and Yue Cao. ParaSCI: large scientific paraphrase dataset for longer paraphrase generation. In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty (eds.), Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 424434, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.33. URL https://aclanthology.org/2021. eacl-main.33/. 20 Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs, 2019. URL https://arxiv.org/abs/1903.00161. 20 Sabri Eyuboglu, Ryan Ehrlich, Simran Arora, Neel Guha, Dylan Zinsley, Emily Liu, Will Tennien, Atri Rudra, James Zou, Azalia Mirhoseini, and Christopher Re. Cartridges: Lightweight and general-purpose long context representations via self-study, 2025. URL https://arxiv. org/abs/2506.06266. 2 10 Preprint Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in large language model, 2024. URL https://arxiv.org/abs/ 2307.06945. 1, 2, 3, 5, Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. SAMSum corpus: humanannotated dialogue dataset for abstractive summarization. In Lu Wang, Jackie Chi Kit Cheung, Giuseppe Carenini, and Fei Liu (eds.), Proceedings of the 2nd Workshop on New Frontiers in Summarization, pp. 7079, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5409. URL https://aclanthology.org/D19-5409/. 20 Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzman, and Angela Fan. The flores101 evaluation benchmark for low-resource and multilingual machine translation, 2021. URL https://arxiv.org/abs/2106.03193. 5, 24 Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, and Daniel Song et al. The llama 3 herd of models, 2024. URL https: //arxiv.org/abs/2407.21783. 5 Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. Olmo: Accelerating the science of language models. Preprint, 2024. 7 Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan Chang, and Bryan Catanzaro. Raven: In-context learning with retrieval-augmented encoder-decoder language models, 2024. URL https://arxiv.org/abs/2308.07922. 2 Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models, 2022. URL https://arxiv.org/abs/2208. 03299. 2, 5, Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product Quantization for Nearest NeighIEEE Transactions on Pattern Analysis and Machine Intelligence, 33(1):117 bor Search. 128, January 2011. doi: 10.1109/TPAMI.2010.57. URL https://inria.hal.science/ inria-00514462. 9 Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023a. URL https: //arxiv.org/abs/2310.06825. 5 Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models, 2023b. URL https://arxiv. org/abs/2310.05736. 1, 2 Kelvin Jiang, Dekun Wu, and Hui Jiang. FreebaseQA: new factoid QA data set matching triviastyle question-answer pairs with Freebase. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for 11 Preprint Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 318323, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1028. URL https://aclanthology.org/N19-1028/. 20 Matt Gardner Johannes Welbl, Nelson F. Liu. Crowdsourcing multiple choice science questions. 2017. 20 Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16011611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/ P17-1147/. 5, 23 Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, and Mikhail Burtsev. Cramming 1568 tokens into single vector and back again: Exploring the limits of embedding space capacity, 2025. URL https://arxiv.org/abs/2502.13063. 2 Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019. 5, Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models, 2025. URL https://arxiv.org/abs/2405.17428. 5, 20, 23 Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rocktaschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021. URL https: //arxiv.org/abs/2005.11401. 1 Xuyang Liu, Zichen Wen, Shaobo Wang, Junjie Chen, Zhishan Tao, Yubo Wang, Xiangqi Jin, Chang Zou, Yiyu Wang, Chenfei Liao, Xu Zheng, Honggang Chen, Weijia Li, Xuming Hu, Conghui He, and Linfeng Zhang. Shifting ai efficiency from model-centric to data-centric compression, 2025. URL https://arxiv.org/abs/2505.19147. 1 Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. URL https: //arxiv.org/abs/1711.05101. 5 Maxime Louis, Herve Dejean, and Stephane Clinchant. Pisco: Pretty simple compression for retrieval-augmented generation, 2025a. URL https://arxiv.org/abs/2501.16075. 1, 2, 5, 8, Maxime Louis, Thibault Formal, Herve Dejean, and Stephane Clinchant. Oscar: Online soft compression and reranking, 2025b. URL https://arxiv.org/abs/2504.07109. 2 Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens, 2024. URL https://arxiv.org/abs/2304.08467. 1, 2 Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: human generated machine reading comprehension dataset. CoRR, abs/1611.09268, 2016. URL http://arxiv.org/abs/1611.09268. 20 Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Ruhle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Dongmei Zhang. Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression, 2024. URL https://arxiv.org/abs/2403.12968. 5, Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktaschel, and Sebastian Riedel. Kilt: benchmark for knowledge intensive language tasks, 2021. URL https://arxiv.org/abs/2009.02252. 20 12 Preprint Jack Rae, Anna Potapenko, Siddhant Jayakumar, Chloe Hillier, and Timothy Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URL https://arxiv.org/abs/1911.05507. 7, 21 Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text, 2016. URL https://arxiv.org/abs/1606.05250. 5, 23 Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: zero-shot benchmark for long text understanding, 2023. URL https://arxiv.org/abs/2305. 14196. 7, Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. Asqa: Factoid questions meet long-form answers, 2023. URL https://arxiv.org/abs/2204.06092. 20 Paul Suganthan, Fedor Moiseev, Le Yan, Junru Wu, Jianmo Ni, Jay Han, Imed Zitouni, Enrique Alfonseca, Xuanhui Wang, and Zhe Dong. Adapting decoder-based language models for diverse encoder downstream tasks, 2025. URL https://arxiv.org/abs/2503.02656. 3 Jiwei Tang, Zhicheng Zhang, Shunlong Wu, Jingheng Ye, Lichen Bai, Zitai Wang, Tingwei Lu, Jiaqi Chen, Lin Hai, Hai-Tao Zheng, and Hong-Gee Kim. Gmsa: Enhancing context compression via group merging and layer semantic alignment, 2025. URL https://arxiv.org/abs/ 2505.12215. 1, 2, 3 Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: survey, 2022. URL https://arxiv.org/abs/2009.06732. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, and Gael Liu et al. Gemma 3 technical report, 2025. URL https: //arxiv.org/abs/2503.19786. 7, 20 Together AI. Llama-2-7B-32K-Instruct and fine-tuning for Llama-2 models with Together API. https://www.together.ai/blog/llama-2-7b-32k-instruct, August 2023. Accessed: 2025-07-17. 3, 7 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. 5, 7 Maurice Weber, Daniel Y. Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Re, Irina Rish, and Ce Zhang. Redpajama: an open dataset for training large language models. NeurIPS Datasets and Benchmarks Track, 2024. 7, 21 Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering, 2018. URL https://arxiv.org/abs/1809.09600. 5, Howard Yen, Tianyu Gao, and Danqi Chen. Long-context language modeling with parallel context encoding, 2024. URL https://arxiv.org/abs/2402.16617. 3, 7, 24 13 Preprint Biao Zhang, Fedor Moiseev, Joshua Ainslie, Paul Suganthan, Min Ma, Surya Bhupatiraju, Fede Lebron, Orhan Firat, Armand Joulin, and Zhe Dong. Encoder-decoder gemma: Improving the quality-efficiency trade-off via adaptation, 2025. URL https://arxiv.org/abs/2504. 06225. 2 Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto. Benchmarking large language models for news summarization, 2023. URL https: //arxiv.org/abs/2301.13848. 5, 14 Preprint"
        },
        {
            "title": "A FURTHER EXPERIMENTS",
            "content": "A.1 CONTEXT COMPRESSION WITH BENCHMARK TRAIN SETS To demonstrate the generalization ability of ARC-Encoder, we deliberately avoid fine-tuning on the training sets of evaluation benchmarks. However, we believe that this could be an interesting use case for users who wish to specialize ARC-Encoder in given domain. In Tab. 6, we report results from models fine-tuned on our dataset augmented with the HotpotQA and SQuAD training sets. These results show that, even without altering decoders, ARC-Encoders can specialize effectively on specific benchmarks. They outperform the open-book baseline while maintaining efficiency gains and without harming performance on other tasks. Table 6: Performance when adding HotpotQA and SQuAD train sets to the fine-tuning dataset. PF: the token reduction factor (e.g., 4) for fixed-ratio methods or the number of compressed tokens used, e.g. 16, when this number is fixed; Param.: number of parameters of the encoder; The superscript on ARC-Encoder indicates if the model is specifically trained for one decoder (M for Mistral or for Llama) or both simultaneously (). Best context compression results are in bold, second best are underlined. Method PF Param. NQ TQA HQA SQuAD FLORES CNN Avg. 1 closed-book open-book ARC4-Encoder 4 3.0B ARC4-EncoderM 4 ARC8-EncoderM 8 7 t 1 closed-book 8 1 . 3 l open-book ARC4-Encoder 4 3.0B ARC4-EncoderL ARC8-EncoderL 4 8 22.8 62.4 29.1 39.9 48.3 70.5 38.3 68.9 60.5 67.9 60.7 38.4 57.5 39.0 67.0 21.6 60.6 25.4 47.1 67.1 38.6 38.5 69.7 61.9 40.7 68.5 62.1 59.1 67.6 38.6 17.1 77.7 77.0 81.1 74.8 15.3 72.2 78.6 82.3 76. 31.3 29.9 30.9 28.0 32.8 33.2 33.2 30.1 27.2 49.2 26.0 50.1 22.7 50.3 47.9 20.8 47.4 26.5 26.0 51.3 22.2 51.5 48.9 21.7 A.2 POOLING FACTOR GENERALIZATION Fig. 5 demonstrates that ARC-Encoder pretrained at certain pooling factor can still be fine-tuned at another, sometimes even improving results on downstream tasks. This transfer works best as we use smaller pooling factor than the one previously pretrained on. Notably, pretraining at 8 seems to be particularly effective since we can then outperform any other pair on pooling factors of 4 and 8. In contrast, models do not generalize well when fine-tuned to higher pooling factors. This capability greatly benefits the method since it enables to reach better results at various pooling factors while pretraining fewer models. We train our best ARC4-Encoder and ARC8-Encoder from models pretrained using pooling factor of 8. When pooling too much performance degrades sharply as with pooling factor of 32 where the model has an averaged score of 33.1. A.3 ENCODER-DECODER PAIRS Figure 5: Score for various pairs of pooling factor between pretraining and fine-tuning. Following the same recipe as described in Section 3, we can design ARC-Encoder / decoder pairs based on other backbone models. Tab. 7 shows that Llama3.1 8B can also serve as an encoder backbone, displaying higher affinity with Llama decoders, similar to Llama3.2 3B. Adding the MLP projector is crucial to adapt to different decoders in the multi-decoder case as well as in the specific decoder one. Our experiments further reveal that the hidden state spaces of Llama3.1 8B and Mistral 15 Preprint 7B are not fully disentangled: single encoder can feed both decoders with the same compressed representations while outperforming the closed-book baseline. We attribute this compatibility to their similar architectures and training pipelines. Table 7: Different backbones for an ARC4-Encoder paired with different decoders. The score is the average of the metrics in the Tab. 1. For underlined modules, the weights are the same for the two decoders; they are decoder-specific otherwise. Encoder Decoder L8B L3B + MLP L8B L8B + MLP L3B + MLP Llama3.1 8B 46.0 Mistral 7B 44.8 47.2 45. 32.4 32.8 46.2 41.8 45.5 43.2 A.4 POOLING METHODS Table 8: Different pooling methods. Scores are averaged over 3 fine-tunings with different seeds. Pooling method PF NQ TQA HQA SQuAD FLORES CNN Avg. 7 36.9 67.2 Average queries 37.6 68.1 Last queries Kmeans merged queries 37.1 65.8 32 36.8 66.8 Memory tokens t 39.9 39.7 38.4 28. 58.3 59.8 52.3 49.8 27.4 27.3 18.6 31.0 20.1 19.7 19.4 17.0 41.7 42.1 38.6 38.4 A.5 FURTHER BASELINES EVALUATIONS Table 9: Further baselines evaluations. Same setting and specifics as in Tab. Methods PF Param. NQ TQA HQA SQuAD FLORES CNN Avg. o B 7 t M d d 8 1 . 3 l LLMLingua2 7.2B 7.1B closed-book open-book ICAE-like xRAG-like 1 32 1 8 1.9 0.6B 3.6 32 4 ARC4-Encoder 4 ARC4-EncoderM 4 ARC8-EncoderM 8 3.0B 7.2B PISCO-like LLMLingua2 7.6B 7.1B closed-book open-book ICAE-like xRAG-like 1 32 1 8 2.0 0.6B 3.9 32 4 ARC4-Encoder 4 ARC4-EncoderL 4 ARC8-EncoderL 8 3.0B 7.6B PISCO-like 17.1 77.7 58.8 23.9 24.2 59.2 42.0 38.2 48.1 67.9 71.1 62.0 15.3 72.2 61.6 22.3 22.3 58.8 41.9 40.5 57. 72.6 74.0 66.0 29.1 39.9 36.5 30.7 28.7 38.8 35.2 34.7 36.6 62.4 70.5 66.7 65.2 62.4 69.0 66.6 68.5 69.2 22.8 48.3 24.3 21.5 21.8 43.7 36.0 24.9 29.4 36.4 69.7 43.6 39.0 68.9 45.1 40.8 67.9 38.4 21.6 60.6 25.4 47.1 67.1 38.6 20.5 67.3 38.4 21.7 62.1 28.0 21.9 61.1 26.6 45.2 66.3 36.1 37.2 66.1 34.2 35.1 30.6 69.4 37.9 70.5 37. 45.7 69.9 37.3 39.7 70.1 46.9 42.8 69.0 38.9 16 31.3 28.3 0.9 9.0 12.6 4.3 33.6 34.5 29.1 31.0 28.3 32.8 31.3 3.4 6.4 13.6 3.2 35.2 36.5 33.1 33.7 30. 27.2 15.8 14.6 10.7 24.9 22.1 19.2 19.3 49.2 38.4 26.1 26.1 41.4 34.4 36.5 39.5 26.1 45.5 23.8 46.5 43.4 22.9 26.5 17.3 12.7 12.6 23.8 21.3 19.7 20.7 47.4 39.4 25.0 25.2 40.6 34.0 38.4 43.3 25.6 47.4 23.7 48.0 45.0 22. Preprint"
        },
        {
            "title": "B COMPUTATIONAL ANALYSIS",
            "content": "B.1 THEORETICAL GENERATION FLOPS Let us denote the number of tokens of the prompt sequence, the hidden dimension of the model, nlayers its number of layers, the vocabulary size and the overall number of parameters of the decoder. We assume for simplicity that the decoder uses multi-head attention and that the hidden dimension is the same everywhere in the model. The theoretical complexity of prefilling steps, measured in floating point operations (FLOPs) and neglecting norms, is: Multi-head attention: Q, K, projections = 3 d2 Attention scores = s2 = s2 Final projection = d2(depends) Overall = 4s d2 + 2s2 Feedforward network d2 Output projection . In the case where s, as the number of parameters per layer is proportional to d2, we have , which leads to: (cid:113) nlayers Total prefill FLOPs sN if s. (1) Hence, using tokens for prefilling instead of leads to relative FLOPs of 1 . At the opposite, when processing very large prompts, i.e, s, the computational complexity reads: (2) If we use an ARC-Encoder, with pooling factor of x, which has = pN parameters (p < 1) wih the relative size of the encoder vs. decoder, then the number of FLOPs to compress the prompt can be approximated to: Total prefill FLOPs N . (cid:26) sN (p + 1 ) + 1 ( s2 x2 ) if s, if s. (3) For instance, it is approximately 1.5 smaller for ARC4-Encoder and 1.9 smaller for ARC8Encoder in the setting of the main table. B.2 PROFILING We evaluate the computational efficiency of our method using Torch Profiler5 measuring CUDA execution time (s) and tera FLOPs (TFLOPS)for the compression, prefilling and decoding stages with various prompt context lengths and various numbers of tokens to generate. All experiments use Mistral 7B decoder and are run in float32 on one NVIDIA H100 GPU with batch size of 1. We force the decoder to continue its context prompt, compressed or not, and generate {1, 100} tokens. As shown in Fig. 6, generation is less costly in terms of compute when using compressed tokens by ARC-Encoder. The compute cost of compression is amortized during the prefilling phase since the decoder has fewer tokens to process. 5https://docs.pytorch.org/tutorials/recipes/recipes/profiler_recipe. html 17 Preprint (a) Number of TFLOPs (b) CUDA time (s) Figure 6: Measured computational costs. (a) Number of TFLOPs and (b) CUDA time in seconds for the continuation of book from PG19 for various prompt lengths and numbers of tokens to generate on one NVIDIA H100."
        },
        {
            "title": "C TRAINING DETAILS",
            "content": "C.1 DEFAULT SETTING Trainings are performed on 8H100 NVIDIA GPUs using PyTorchs FSDP framework6. The ablations follow the parameters and architectural choices from our best ARC8-Encoder encoders 1, namely: Encoder: Llama3.2 3B truncated of the 2 last layers with every layer trained using noncausal attention mask. When using Llama3.1 8B as encoder we truncate the 8 last layers. Pooling: by averaging queries in the last transformer block of the encoder with pooling factor of 8. MLP projector: 2 learned matrices without activation function, with dimensions sequence 3072 2048 4096 if the encoder is Llama3.2 3B, 4096 2048 4096 otherwise. Training: Special tokens are added depending on the task. 20% reconstruction for pretraining during 60k steps with approximately 2B tokens seen by the encoder and maximum 256 tokens compressed and 256 tokens to continue or reconstruct. During the continuation task, maximum 256 text tokens are prefixed to the compressed sequence to better align with the final few-shot evaluation. Fine-tuning: <Cont> token is appended after each compressed sequence. Compressed representations are used as context and all the samples follow the format below (C.3) with more or less in-context examples as described in Tab. 12. Fine-tuning is performed with the same pooling factor as for the pretraining, unless stated otherwise. Remarks. Inserting text tokens before the compressed sequence in the continuation task introduces significant compute overhead during pretraining. Yet, after fine-tuning with interleaved few-shot samples, it offers no gains in the specific-decoder setting. Standard continuation can substitute for the interleaved one in pretraining, as long as fine-tuning later interleaves compressed 6https://docs.pytorch.org/docs/stable/fsdp.html 18 Preprint and normal tokens. For consistency, we keep interleaved continuation in reported results since it helps ARC-Encoder generalize better in the multi-decoder setting. C.2 HYPERPARAMETERS Table 10: Pretraining hyperparameters. Hyperparameters Pretraining settings optimizer max lr encoder max lr special tokens max lr rate MLP lr scheduler type init lr final lr warmup steps weight decay batch size gradient accumulation steps GPUs max tokens compressed at once prefix text tokens max norm (gradient clipping) mixed precision number of steps AdamW 1 105 1 105 5 105 1 cycle policy 1 1020 1 1010 1000 0.1 16 None 8 256 256 1.0 Yes 60k for ablations 80k for multi-decoder 80k for ARC-Encoder (long context or not) 5 MLP init. Special tokens init. Kaiming Unif. leaky-ReLU slope of Ones Table 11: Fine-tuning hyperparameters. Not listed hyperparameters are identical to pretraining ones. Hyperparameters max lr encoder max lr special tokens max lr rate MLP final lr warmup steps weight decay n. steps batch size gradient accumulation steps max tokens compressed at once interleaved examples contexts chunked to max contexts in parallel Context Compression 2 106 2 106 (3 105 for multi-decoder) 3 105 1 108 50 5 102 4k (8k for multi-decoder) 8 None 2048 # in-context examples (M ) = for Tabs. 1, 2, 6 and 9 No 1 Long Context NA 1000 8k 2 4 1024 NA NA 1024 tokens 31 Multi-Decoder specificities. To add OLMo 7B to the existing multi-decoder ARC4-Encoder, we fine-tune the MLP using 8k steps with 100 warmup steps and maximum learning rate for the MLP of 104. 19 Preprint C.3 CONTEXT COMPRESSION FINE-TUNING To generate synthetic fine-tuning data, we use the vLLM7 library for fast, efficient single-GPU inference. Gemma 3 27B (Team et al., 2025) is then prompted to generate translations of Atlas Wikipedia passages (up to 3 concatenated passages) in various languages that we split into two categories: 1) Spanish, French, German and Danish; 2) Hindi, Russian, Swahili, Arabic, Turkish, Japanese, Finnish and Chinese (simplified). We mix these generated translation datasets with supervised QA, summarization and reading comprehension datasets. For QA datasets, we retrieve the top-5 passages using NV-Embed (Lee et al., 2025), based on Wikipedia sequences8 from KILT framework (Petroni et al., 2021). To improve information retrieval in large compressed contexts, we concatenate retrieved passages when possible and compress them jointly. The number of concatenated passages is randomly sampled between 1 and the maximum number of retrieved passages. This is particularly beneficial for HotpotQA and CNN, with minimal impact on other evaluation benchmarks. For MS MARCO (Nguyen et al., 2016) we removed the samples without answers (no answer). The complete list of the subsets that make up our final fine-tuning dataset is reported in Tab. 12, along with the proportions according to which these subsets are sampled from. # in-context Max. concat. Proportion examples passages Subsets Group 1 translations Group 2 translations Synth. translations 6% 4% QA with retrieved context AdversarialQA (Bartolo et al., 2020) FreebaseQA (Jiang et al., 2019) ASQA (Stelmakh et al., 2023) MS MARCO (Nguyen et al., 2016) SciQ (Johannes Welbl, 2017) 8% 27% 1% 9% 3% Reading comprehension DROP (Dua et al., 2019) ParaSCI (Dong et al., 2021) DialogSum (Chen et al., 2021) SAMSum (Gliwa et al., 2019) WikiSum (Cohen et al., 2021) 20% 12% Summarization 2.5% 2.5% 5% 4 4 4 4 4 5 5 5 5 5 5 5 5 5 3 4 5 Table 12: Fine-tuning dataset for context compression 7https://docs.vllm.ai 8https://huggingface.co/datasets/dmrau/kilt-128 20 Preprint Few-shot compressed fine-tuning template Document: <TOKENS COMPRESSED> Question: <QUESTION> Answer: <ANSWER> Document: <TOKENS COMPRESSED> Question: <QUESTION> Answer: <ANSWER> ... Document: <TOKENS COMPRESSED> Question: <QUESTION> Answer: <ANSWER> C.4 LONG CONTEXT C.4.1 PRETRAINING (the loss is computed only on the last answer) To pretrain ARC-Encoder when paired with an instruct decoder model, we continue to alternate between continuation and reconstruction tasks. We format each of the pretraining samples using the following template C.4.1. Pretraining templates with Llama2 Chat decoder Template: <s> [INST] Prefix+<TOKENS COMPRESSED>+Instruction [/INST] Suffix </s> Reconstruction: Prefix = \"Text:nn\" Instruction = \"n Replicate the input text.\" Suffix = \"Replicated text:n\" Continuation: Prefix = \"Text:nn\" Instruction = \"n Continue the previous text.\" Suffix = \"Text continuation:n...\" C.4.2 FINE-TUNING For this part, we synthesized QA, summarization and paraphrasing examples using the same procedure as in Appendix C.3. QA generation. We split books from PG-19 (Rae et al., 2019) and arXiv papers from RedPajama (Weber et al., 2024) into paragraphs, randomly selecting 5 consecutive paragraphs. We then prompted Gemma3-27B to generate questions and gold answers using instructions such as: As human instructor assessing students comprehension of scientific article, you craft concise question that ideally requires short phrase or sentence to answer. If the article lacks the necessary information, the answer should be unanswerable. For yes/no questions, reply with yes, no, or unanswerable. Then, supply the gold answer. You are given story from book. Your task is to create question that can be answered in short phrase or sentence. Then, provide the gold answer. 21 Preprint The final context in the dataset is the entire book or paper. Additionally, we generate QA examples from Wikipedia by selecting one chunk from the Atlas Wikipedia dataset and appending up to 20 chunks from the same source before prompting the model. Summarization generation. Using the same datasets, we split the texts into 10 groups of passages. Gemma3 27B is prompted to summarize each subsection; then, based on these summaries, it is prompted again to produce higher-level summary, with prompt variations controlling the target length. For Atlas Wikipedia, we directly ask the model to produce short summary from 10 consecutive chunks. In both QA and summarization tasks, we truncate contexts longer than 500k characters and discard those shorter than 1k characters. Paraphrase. To mimic the questions asked in QM-Sum benchmarks, we prompt Gemma3 to reformulate passages of the text. For all tasks, we truncate contexts longer than 500k characters and discard those shorter than 1k characters. Contexts # samples Mean Ctx Median Ctx Mean answer From Atlas From PG-19 books From ArXiv papers From PG-19 books From ArXiv papers From Atlas From PG-19 books From ArXiv papers Summarization 64000 64000 40000 80000 80000 80000 40000 6833 55048 10896 Paraphrase 31854 5625 QA 8065 317179 54526 5593 34915 4875 29272 8325 322942 43386 714 635 834 665 617 43 69 49 Table 13: Fine-tuning dataset statistics for long-context understanding. We report different statistics on the length in characters of the contexts (Ctx) and the answers for each subset of finetuning samples. All samples are inserted in an instruction prompt depending on their task and context dataset using the same template as in C.4.1 with adapted Prefix, Instruction and Suffix."
        },
        {
            "title": "D EVALUATION DETAILS",
            "content": "D.1 BASELINES IMPLEMENTATION LLMLingua2 (Pan et al., 2024): We used the open-source model microsoft/llmlingua-2-xlmroberta-large-meetingbank following the instructions from LLMLingua. xRAG (Cheng et al., 2024): We use the official codebase from xRAG and extend it to support Llama3.1 8B as decoder. Due to architectural similarities between Mistral 7B and Llama3.1 8B, only minor modifications are required. We first pretrain the MLP projector by closely following the instructions and data from the repository and the original paper, adapting it to base models by removing all chat templates. Next, we modify the fine-tuning dataset pre-processing to interleave compressed context in an ICL-style format, aligned with our fine-tuning template (see C.3), which closely matches the evaluation setup. To ensure consistency and avoid variability due to dataset size or quality, we use our own dataset for fine-tuning. We observe in Tabs. 1 and 9 that xRAG performs poorly on translations tasks (FLORES). After further investigations, we believe that compressing the full sequence into one vector leads to loss of information that causes partial-only translations or hallucinations, as illustrated below. 22 Preprint xRAG with Mistral 7B failures to translate English texts To French: Ground-truth: Cette page est accessible facilement `a partir dune seule adresse Web, ce qui la rend facile `a memoriser et `a ecrire pour les etudiants qui ne savent pas utiliser un clavier et qui ont des probl`emes dorthographe. xRAG generation: Avec un seul nom de domaine, il est facile pour les utilisateurs dacceder `a linformation, ce qui est un avantage pour les etudiants. To Spanish: Ground-truth: Son superiores los servidores proxy por varios motivos: redirigen todo el trafico de Internet no unicamente los http. xRAG generation: Estos son mas eficientes que los proxy, ya que no requieren que el usuario realice cambios en sus configuraciones de red. To German: Ground-truth: Vergessen Sie nicht die Extrakosten fur weitere Visa, Abfluggebuhren, Transportmittel an Land etc. fur all die Orte auÃerhalb von Afrika mit einzuberechnen. xRAG generation: Das ist zwar teurer als die Fluge, aber das Geld ist es wert, weil man damit nicht nur das Flugzeug spart, sondern auch die Kosten fur die Ubernachtung, die Verpflegung und die Reiseversicherung. ICAE (Ge et al., 2024): For this re-implementation, we use our own codebase. We follow the hyperparameters and design choices described in the original paper, including the use of special tokens and alternating pretraining tasks. We set the language modeling task ratio to 0.5 and pretrain the encoder on our evaluated decoders (Llama3.1 8B and Mistral 7B) with our crawl dataset for 100k steps (which is half the number of steps reported in the paper, but the training curve had already converged). Additionally, we adapt the fine-tuning template to match our evaluation format (see Appendix C.3). To avoid redundancy with our ablation studies on pooling methods with memory tokens, we retain the fine-tuning dataset from Ge et al. (2024) (PwC), which was specifically synthesized for this purpose. However, due to the poor generalization on our evaluation benchmarks, we present ICAE-like models fine-tuned on our own dataset. PISCO (Louis et al., 2025a): As with ICAE, we re-implemented PISCO using our own codebase, following the hyperparameters and design choices outlined in the original paper (e.g., LoRA applied to both encoder and decoder, encoder architecture, use of memory tokens). Since the official code is not publicly available, we referred to Ge et al. (2024) for implementation details not specified in the paper, such as the use of special tokens. To ensure consistency and avoid variability from dataset quality, we use our custom fine-tuning dataset but increased the number of fine-tuning steps to 8000 to reach near 500k samples which matches the number of training samples used in Louis et al. (2025a). Furthermore, our custom dataset consists in PISCO fine-tuning dataset without the train sets of the evaluation datasets, with extra summarization datasets and synthesized translations data. While sequence-level distillation is key to avoiding reliance on gold labels, early experiments with silver labels showed that using gold labels enables fairer comparison. Additionally, we train variants that process fixed-length input chunks of size 128 tokens, enabling fixed pooling factor and aligning the setup more closely with that of Louis et al. (2025a). D.2 EVALUATION DATASETS Context Compression. We evaluate our pipeline on question answering (QA) and reading comprehension tasks using the following benchmarks: HotpotQA (Yang et al., 2018) (distractor setting on the dev set, 7400 samples) , Natural Questions (Kwiatkowski et al., 2019) (NQ open dev set, 3605 samples), TriviaQA (Joshi et al., 2017) (unfiltered nocontext validation set, 11308 samples), SQuAD (Rajpurkar et al., 2016) (10565 samples). When ground-truth context is not provided, we retrieve the top-5 passages using NV-Embed (Lee et al., 2025), based on Wikipedia sequences from the Atlas framework (Izacard et al., 2022), effectively simulating RAG setup. The number of retrieved Preprint passages used for evaluation is specified for each benchmark. We report Exact Match (EM) as our primary evaluation metric, where answers are normalized and EM = 1 if all characters match exactly. We demonstrate summarization capabilities on the CNN-DailyMail dataset (a subset of 1000 samples of the dev set), evaluating performance with the Rouge-L metric, as Zhang et al. (2023) noted that strong Rouge-L scores in this context are closely aligned with high human approval. For translation tasks, we evaluate on the FLORES benchmark (Goyal et al., 2021) (992 samples), using BLEU scores computed with SacreBLEU9. BLEU scores are averaged over four translation directions: English to Danish, French, German, and Spanish. Models are prompted in 5-shot setting using the following template, using compressed contexts for each example. Examples are sampled from the validation set and are fixed among all models. The reported pooling factor reflects the average per-context compression of tokens, not the ratio over the full prompt (including the textual prompt). It consists in dividing the number of tokens of the full document using the decoder tokenizer by the number of compressed tokens or the number of tokens of the compressed document in the hard compression case. Evaluation QA template examples for n-shot evaluation: Document: <TOKENS COMPRESSED> Question: <QUESTION> Answer: <ANSWER> Document: <TOKENS COMPRESSED> Question: <QUESTION> Answer: <ANSWER> ... the final question Document: <TOKENS COMPRESSED> Question: <QUESTION> Answer: Evaluation translation template examples for n-shot evaluation: Document: <TOKENS COMPRESSED> Question: Translate the previous document into <LANGUAGE>. Answer: <ANSWER> ... the final question Document: <TOKENS COMPRESSED> Question: Translate the previous document into <LANGUAGE>. Answer: Long Context. For long-context understanding, we report results on NarrativeQA (NQA), QASPER (Qspr), GovReport (GvRp), and QM-Sum validation datasets from ZeroSCROLLS (Shaham et al., 2023) benchmark, suite of zero-shot long-context understanding tasks that emphasize instruction-following capabilities. We evaluate on the full validation dataset which consists in respectively 3461, 1726, 973 and 272 samples. Specifically, we adopt the task formats and instructions as used in Yen et al. (2024)10. 9https://github.com/mjpost/sacrebleu 10https://github.com/princeton-nlp/CEPE"
        }
    ],
    "affiliations": [
        "Kyutai, Paris, France"
    ]
}