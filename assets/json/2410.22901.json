{
    "paper_title": "HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models",
    "authors": [
        "Shengkai Zhang",
        "Nianhong Jiao",
        "Tian Li",
        "Chaojie Yang",
        "Chenhui Xue",
        "Boya Niu",
        "Jun Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose an effective method for inserting adapters into text-to-image foundation models, which enables the execution of complex downstream tasks while preserving the generalization ability of the base model. The core idea of this method is to optimize the attention mechanism related to 2D feature maps, which enhances the performance of the adapter. This approach was validated on the task of meme video generation and achieved significant results. We hope this work can provide insights for post-training tasks of large text-to-image models. Additionally, as this method demonstrates good compatibility with SD1.5 derivative models, it holds certain value for the open-source community. Therefore, we will release the related code (\\url{https://songkey.github.io/hellomeme})."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 3 ] . [ 1 1 0 9 2 2 . 0 1 4 2 : r HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models Shengkai Zhang songkey@pku.edu.cn Nianhong Jiao jnhrhythm@tju.edu.cn Tian Li litian215@163.com Chaojie Yang 12131243chaojie@gmail.com Chenhui Xue * great xch@shu.edu.cn Boya Niu * by903033784@163.com Jun Gao gaojun55@gmail.com HelloGroup Inc."
        },
        {
            "title": "Abstract",
            "content": "ments, our solution is as follows: We propose an effective method for inserting adapters into text-to-image foundation models, which enables the execution of complex downstream tasks while preserving the generalization ability of the base model. The core idea of this method is to optimize the attention mechanism related to 2D feature maps, which enhances the performance of the adapter. This approach was validated on the task of meme video generation and achieved significant results. We hope this work can provide insights for posttraining tasks of large text-to-image models. Additionally, as this method demonstrates good compatibility with SD1.5 derivative models, it holds certain value for the open-source community. Therefore, we will release the related code (https://songkey.github.io/hellomeme). 1. Introduction The story begins with our task of generating meme videos, which is similar to video-driven portrait animation methods but comes with several specific requirements. First, the facial expressions and head poses in meme images or videos are often highly exaggerated, adding extra challenges to the task. Second, the technical solution needs to have the potential to extend to half-body or even fullbody compositions. Third, the solution must not compromise the generalization ability of the text-to-image foundation model, allowing us to leverage the rich customization methods of the Stable Diffusion [12] base model to enhance the diversity of content generation. To meet these require- *Intern. First, for tasks involving exaggerated facial expressiondriven, many existing methods perform well. However, these methods usually require that the head pose in the driving video not be too extreme, as large deviations can easily lead to distortion. In our approach, we separately encode the head pose and facial expressions as 2D feature maps and linear features, and then fuse them using spatial knitting attention mechanisms . The fused features serve as the representation of the driving information. This approach improves performance under conditions of exaggerated expressions and poses. Second, for facial reenactment tasks, many works evaluate their performance on body-driven tasks. However, it wasnt until diffusion-based methods [12] emerged that we saw the real promise in addressing both challenges simultaneously. Therefore, we chose to build our solution on the classic SD1.5 model. Based on the results from various Stable Diffusion applications, this technical approach shows ample potential. Third, Stable Diffusion 1.5 [12] has significant firstmover advantage, moderate computational requirements, and strong performance, leading to rich open-source ecosystema vast treasure trove of resources. However, we noticed that most current SD-based facial reenactment methods require optimizing all the parameters of the UNet, which compromises compatibility with SD1.5 [12] derived models. Similar to Animatediff [6] , our approach keeps the SD1.5 UNet weights completely unchanged, optimizing only the inserted adapters parameters. We found that using simple adapter struggled to converge, but the introduction of the SK Attention mechanism effectively solved this issue. Figure 1. Our solution consists of three modules. HMReferenceNet is used to extract Fidelity-Rich features from the reference image, while HMControlNet extracts high-level features such as head pose and facial expression information. HMDenoisingNet receives both sets of features and performs the core denoising function. It can also integrate fine-tuned Animatediff module to generate continuous video frames. 2. Related Work 2.1. Condition When we use portrait photo and set of conditions as input to generate talking video, this set of conditions can either be weak condition, such as an audio clip, or strong condition, such as another talking video. Our task allows the use of the latter, so we focus on discussing the strong condition. If the face bitmap from the talking video is directly used as condition, it may leak identity information during training. Therefore, the common approach is to extract identity-agnostic driving features from it instead. a) The most intuitive strong condition is face landmarks, but they couple facial expressions, head pose, and identity information together, and the expression information they represent is often lossy. b) Methods based on 3D face models can theoretically fully decouple facial expressions, head poses, and identity information. However, due to current methods having limitations in the accuracy of 3D coefficient extraction from wild data, the performance of generative solutions that rely on them as an intermediary is inevitably constrained. c) Directly using the aligned bitmap of facial features as condition is another approach; however, it also contains some identity information, which poses risk of leakage and needs to be mitigated. d) Methods that use motion latent as condition are relatively popular because they do not require explicit extraction, thereby avoiding precision limitations. They also allow for end-to-end training, ensuring optimal performance. However, the drawback is that they need to be used in conjunction with GAN-based generators, which limits fidelity under large poses. Our approach combines the advantages of b) and c), proving to be highly effective in the context of meme video generation. 2.2. Diffussion/Non-diffusion based Methods Currently, the mainstream non-diffusion-based methods are the motion latent-based approaches mentioned earlier, Figure 2. This is the structural diagram of SKCrossAttention, which utilizes the Spatial Knitting Attention mechanism to fuse 2D feature maps with linear features. It performs cross-attention first row by row, then column by column. which are characterized by their speed and high fidelity. This allows them to be applied in real-time scenarios that other methods cannot achieve. However, it is essentially GAN-based method, which lacks sufficient understanding of human body structure. As result, we can observe that its fidelity in scenarios involving large angles or compositions greater than half-body is less than satisfactory. are Diffusion-based methods currently advancing rapidly, and we have already seen wealth of applications that demonstrate the potential of diffusion models to understand the world. Therefore, we need not worry about their insufficient understanding of human structure. The emerging diffusion-based facial reenactment methods have demonstrated high fidelity and generalization ability, with variety of implementation approaches. believe this is just the beginning. Currently, diffusion-based methods require significant computational resources and take considerable amount of time. However, believe this issue will diminish as algorithms and hardware continue to advance. 2.3. T2I Base Model Post Training Text-to-image post-training methods have made significant progress, enabling increasingly complex downstream tasks. Initially, we observed that methods such as Lora, DreamBooth [13], Hypernetworks, and Textual Inversion [3] allowed for easy customization of T2I models in specific scenarios. Approaches like ControlNet [25] and IP-Adapter [24] facilitated controllable image generation, while Animatediff [6] readily extended T2I models to T2V models. Furthermore, benefiting from the preservation of the original UNet capabilities, models trained using these methods can be combined to create various stunning effects. For more complex tasks such as body-driven animation [8] [23] [27], facial reenactment [12] [19] [5] [21], and virtual try-ons [22] [10] [4], it is common to initialize the weights using base SD UNet, followed by updating all weights during subsequent training. This practice inevitably compromises the generalization ability of the underlying text-to-image model and makes it challenging to maintain compatibility with other SD-derived models. Our work attempts to use plugin-based approach for post-training the base T2I model to achieve complex downstream tasks while preserving the generalization ability of the base model. 3. Method As shown in Fig. 1, our solution consists of three modules: HMReferenceNet, HMControlNet, and HMDenoisingNet. HMReferenceNet is used to extract fidelity-rich features from the reference image and is complete SD1.5 UNet, which only needs to be executed once during inference. HMControlNet is responsible for extracting highlevel features include head poses and facial expressions, which are then mapped to three different scales of the latent space in the UNet. HMDenoisingNet is the core denoising model that based on complete SD1.5 UNet, receives features from HMReferenceNet and HMControlNet to generate an image that imparts new head poses and facial expressions to the reference image. HMDenoisingNet can also incorporate with fine-tuned Animatediff [6] module to generate videos. 3.1. Spatial Knitting Attentions It can be observed that when performing self-attention on 2D feature maps or cross-attention between 2D feature maps and linear features, the feature map is typically flattened row by row into linear feature. After the attention operation is completed, it is reshaped back into 2D feature map. Even though 2D positional encoding can be added after flattening the feature map, this operation still partially disrupts the spatial structure information inherent in the 2D layout. We modified the operation of directly flattening the 2D feature map for attention by first performing attention rowwise, followed by attention column-wise. In addressing the meme generation task, we found that using the former approach required updating all parameters to achieve slow convergence. However, with the latter approach, updating only the limited parameters of the inserted module was sufficient to achieve good results. The latter process resembles the interweaving of warp and weft threads during weaving, so we refer to this mechanism as Spatial Knitting Attentions (SK Attentions). We believe the effectiveness of the spatial knitting attentions mechanism lies in its natural preservation of the structural information in the 2D feature map, allowing the neural network to avoid the need to relearn this concept. Fig. 2 and Fig. 3 illustrate the implementation details of the two SKAttention variants we designed, with Appendix discussing their characteristics and applications. 3.2. HMReferenceNet As previously mentioned, HMReferenceNet is complete SD1.5 UNet. Inspired by [23], it leverages the SD1.5 UNets inherent ability to understand visual information while effectively mapping the fidelity-rich features from the reference image to the corresponding hidden layers of HMDenoisingNet. However, in most previous works, the parameters of the ReferenceNet are updated during training, which believe similarly compromises the capability of the base model. In our approach, all weights of HMReferenceNet are kept fixed. In the code implementation, only minor modifications were needed, so merged the implementation of HMReferenceNet and HMDenoisingNet in one class. 3.3. HMControlNet Head movements and facial expressions are global and local features at different levels, so they should be encoded and fused using different mechanisms. We extract the rotation and translation (RT) values of the head in camera space to represent head pose. rectangular box in space is then subjected to RT transformation and perspective transformation, resulting in 2D rasterized image. The four edges of the rectangle are assigned different colors (as shown in Fig. 1), and this 2D image can fully represent the information of head movement. As discussed earlier, the accuracy of the RT values has its limits. However, compared to micro-expressions, small errors in head pose are not as perceptually noticeable, making it acceptable. Although facial expressions are local movements, they Figure 3. This is the structural diagram of SKReferenceAttention, which uses the Spatial Knitting Attention mechanism to fuse two 2D feature maps. Specifically, the two feature maps are first concatenated row by row, followed by performing self-attention along the rows. Afterward, only the first half of each row is retained. similar operation is then performed column by column. convey far more information than head pose, so we use two methods in combination to encode them. First, we trained our own model to extract ARKit facial expression coefficients [2], capturing 51 coefficients. Its advantage is that it is completely decoupled from identity and captures fairly complete range of micro-expressions. However, its limitation is the extraction models accuracy, which has an upper limit, and it tends to make errors under large head poses. To improve accuracy during inference, expression coefficients can also be extracted using ARKit on iOS platforms. Second, to enhance the expressiveness of missing facial details from the expression coefficients, we encode image patches of the eyes and mouth using CLIP image encoding module and fuse them with the expression coefficients. As mentioned earlier, the risk here is that the identity of the reference image could potentially leak during training, so we applied strong random blurring to the patch images during training to mitigate this risk. At this point, the head pose information is encoded into 2D feature map, and the facial expression information is encoded into linear features. We use SKCrossAttention  (Fig. 2)  to fuse them into three scales of feature maps, which are then passed to HMDenoisingNet. 3.4. HMDenosingNet HMDenoisingNet, built on complete SD1.5 UNet, receives features passed from HMReferenceNet and HMControlNet. The features from HMReferenceNet are received using SKReferenceAttention  (Fig. 3)  , which is inspired by [15] and incorporates the Spatial Knitting Attention mechanism. The features passed from HMControlNet are directly added to the corresponding feature maps at three scales in the hidden layers of HMDenoisingNet. Both HMControlNet and SKReferenceAttention use the ZeroConvolution mechanism [25], ensuring stability during the training process. Since the weights of the SD 1.5 UNet remain fixed during training, with only the weights of SKReferenceAttention being updated, the model maintains good compatibility with SD 1.5-derived models during inference. 3.5. Motion The components described above enable single image controllable generation. If the driving condition is continuous video, we can generate frame by frame to achieve controllable video generation. However, using this approach results in significant flickering between video frames. To address the issue of frame discontinuity, we introduced Animatediffs [6] motion module into HMDenoisingNet to improve inter-frame continuity. However, this reduces the fidelity of the generated video, so we fine-tuned the motion module to enhance the quality. Animatediff [6] generates continuous frames using 16frame patch. In our scenario, even with overlapping frames between patches for smoothing, flickering between patches still occurs. Therefore, we divided the video generation process into two stages. In the first stage, video frames are generated frame by frame with fewer sampling steps, and all frames share the same initial noise. The video generated at this stage has poor continuity and fidelity. In the second stage, the frames generated in the first stage are re-noised and used as the initial noise. Combined with the Motion Module, the frames are generated patch by patch, with overlapping frames used for smoothing between patches. This approach not only allows for the generation of longer videos but also improves both frame continuity and fidelity. 3.6. Loss To enhance the representation of exaggerated facial expressions, we applied weighted loss to the eye and mouth regions. The specific method is similar to the FFG loss described in [11], and the loss function is expressed as follows: LLDM = ˆz (1) = mean(LLDM ) + sum(M LLDM ) α β (2) Since the details of the eyes and mouth are mostly generated during the perceptual reconstruction stage, we applied greater weight, denoted as α = (1000 timestep)/1000, in the later stages of sampling. β = 1/(sum(M) + ϵ) is used to normalize the impact caused by varying face sizes in the training data. 4. Experiments 4.1. Implementations Our training data sources include CelebV-HQ [28], VFHQ [20], and other publicly available videos from the internet. After cropping to format similar to VFHQ, we manually selected videos with fixed backgrounds. The total dataset amounts to approximately 180 hours, uniformly preprocessed to format of 15 fps and 512512 resolution. During the training of HMControlNet and SKReferenceAttention, pair of video frames from the same video clip is randomly selected, with one serving as the reference image and the other as the driving image, forming single training sample. The training process utilized 8 NVIDIA A100 GPUs, with batch size of 42. The learning rate was managed using Cosine Scheduler, with maximum and minimum values set at 5e 5 and 1e 7, respectively. The entire training cycle took approximately one week, totaling 200, 000 iterations. For the fine-tuning of Animatediff module, 16 consecutive frames from single video clip were randomly selected as the driving frame sequence, along with one randomly chosen frame as the reference image, forming training sample. The training process utilized 8 NVIDIA A100 GPUs, with an effective batch size of 16 (with gradient accumulation). The learning rate was managed using Cosine Scheduler, with maximum and minimum values set at 3e5 and 1e 7, respectively. The training lasted 6 days, totaling 25, 000 iterations. Another noteworthy point is that during training, all samples share the same text prompt (as shown below). We aimed for this prompt to be capable of generating portrait with the original SD1.5 alone, as the reference image already encapsulates ample redundant information. (best quality), highly detailed, ultra-detailed, headshot, person, well-placed five sense organs, looking at the viewer, centered composition, sharp focus, realistic skin texture 4.2. Quantitative Comparison As shown in Tab. 1, we used two configurations to evaluate the objective metrics for this work. First, we assessed the algorithms self-reenactment performance using 50 video clips from the VFHQ-Test [20] dataset, where the first frame of each video served as the reference image, and the entire sequence of frames was used as driving conditions. Second, we evaluated the algorithms crossreenactment performance by randomly selecting 20 face images from the FFHQ [9] dataset as reference images and usMethod Liveportrait [5] Aniportrait [19] FollowyourEmoji [11] Ours FID 43.84 38.34 39.11 37.69 Self-Reenactment PSNR FVD SSIM LPIPS 262.19 384.98 301.71 231.55 30.66 30.78 30.91 31.08 0.649 0.695 0.695 0.704 0.228 0.147 0.152 0. Cross-Reenactment FID AED APD 313.09 309.52 312.46 304.35 1.02 0.96 0.97 0.81 0.204 0.068 0.071 0.051 Table 1. In comparing our method with the open-source SOTA, its important to note that during FVD evaluation, 25 continuous frames are randomly selected from each sample video to calculate the metrics. This leads to variations in the absolute values of test results each time; however, after multiple validations, we found that their relative rankings remain consistent with the values presented in the table. (a) Ground Truth (b) Liveportrait (c) Aniportrait (d) FollowyourEmoji (e) Ours Figure 4. Examples of self-reenactment performance comparisons, with five frames sampled from each video for illustration. The first row represents the ground truth, with the initial frame serving as the reference image (outlined in red dashed lines). ing the 50 video clips from the VFHQ-Test dataset as driving conditions, resulting in total of 1,000 generated video outcomes in this setup. In the two settings, different evaluation metrics were selected. In the self-reenactment setting, each frame has pixel-level ground truth, so we applied metrics such as FID [7], FVD [16], Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM) [18], and Learned Perceptual Image Patch Similarity (LPIPS) [26] to assess video generation quality. In the cross-reenactment setting, since ground truth is unavailable, we used Average Expression Distance (AED) [14] and Average Pose Distance (APD) [14] to evaluate the consistency of facial expressions and head poses between the driving and generated videos, along with comparative FID between the driving and generated frames as supplement. Based on objective validation metrics, our method consistently outperforms the other three approaches, which aligns with general subjective impressions. It is important to note that the FFHQ dataset has composition with high face-to-frame ratio, which differs significantly from the default settings in most face reenactment tasks. As result, the metrics in the Cross-Reenactment setting are generally somewhat lower than those in the Self-Reenactment setting. 4.3. Qualitative Comparison Fig. 4 presents comparison of the self-reenactment performance of four methods, selecting three sample groups for illustration. Our method shows superior results in scenarios involving occlusions, complex expressions, and large head movements. Analyzing the results alongside objective metrics, it becomes clear that the expression consistency of FollowYourEmoji and Aniportrait with the driving video is comparatively weaker, while LivePortrait encounters challenges with substantial head movements. However, LivePortrait excels in frame-to-frame smoothness, yielding seamless quality that objective metrics do not capture and can only be appreciated when comparing continuous video sequences. 5. Conclusion In summary, our proposed method incorporates lightweight plugins into the foundational text-to-image model to enable customization for complex downstream tasks, demonstrating innovation in network structure design. However, several problems still merit further optimization. Firstly, although we employed two-stage approach, the frame continuity in the generated videos still lags behind GAN-based solutions. While our module is trained on SD1.5, combining it with SD1.5-derived models customized for portrait generation noticeably improves frame continuity, highlighting potential direction for improvement. Additionally, post-training based on video generation foundation models may be the most fundamental solution moving forward. Secondly, the fidelity-rich conditions extracted by HMReferenceNet carry such complete information that when our module is combined with stylized SD1.5-derived models, it significantly diminishes the stylization characteristics. This may be partly due to the fact that our training data primarily features real individuals. Nevertheless, enhancing style expressiveness would make this work even more valuable for applications. Finally, regarding the driving conditions, we selected theoretically optimal approachcombining facial expressions with head posewhich has also performed well in practice. However, to prevent ID information leakage during training, we applied strong random blur to the eye and mouth region bitmaps, which is not very natural solution. Therefore, we believe the current driving conditions are not yet perfect and still have substantial potential for improvement."
        },
        {
            "title": "References",
            "content": "[1] Xiang An, Jiangkang Deng, Jia Guo, Ziyong Feng, Xuhan Zhu, Yang Jing, and Liu Tongliang. Killing two birds with one stone: Efficient and robust training of face recognition cnns by partial fc. In CVPR, 2022. 11 [2] Apple. Arfaceanchor. https://developer.apple. com / documentation / arkit / arfaceanchor / blendshapelocation. 4 [3] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022. 3 [4] Junhong Gou, Siyu Sun, Jianfu Zhang, Jianlou Si, Chen Qian, and Liqing Zhang. Taming the power of diffusion models for high-quality virtual try-on with appearance flow. In Proceedings of the 31st ACM International Conference on Multimedia, MM 23. ACM, Oct. 2023. 3 [5] Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control, 2024. 3, [6] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toimage diffusion models without specific tuning, 2024. 1, 3, 5 [7] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium, 2018. 6 [8] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistent and controllable image-to-video synthesis for character animation, 2024. 3 [9] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks, 2019. 5 [10] Jeongho Kim, Gyojung Gu, Minho Park, Sunghyun Park, and Jaegul Choo. Stableviton: Learning semantic correspondence with latent diffusion model for virtual try-on, 2023. 3 [11] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Follow-your-emoji: Shum, Wei Liu, and Qifeng Chen. Fine-controllable and expressive freestyle portrait animation, 2024. 5, 6 [12] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022. 1, [13] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation, 2023. 3 [14] Aliaksandr Siarohin, Stephane Lathuili`ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation, 2020. 6 [15] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions, 2024. 5 [16] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges, 2019. 6 [17] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. 9 [18] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [19] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animation, 2024. 3, 6 [20] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and Ying Shan. Vfhq: high-quality dataset and benchIn The IEEE Conmark for video face super-resolution. ference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2022. 5 [21] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. Vasa-1: Lifelike audio-driven talking faces generated in real time, 2024. 3 [22] Zhengze Xu, Mengting Chen, Zhao Wang, Linyu Xing, Zhonghua Zhai, Nong Sang, Jinsong Lan, Shuai Xiao, and Changxin Gao. Tunnel try-on: Excavating spatial-temporal tunnels for high-quality virtual try-on in videos, 2024. 3 [23] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model, 2023. 3, 4 [24] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models, 2023. 3, 10 [25] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 3, 5, [26] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric, 2018. 6 [27] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance, 2024. 3 [28] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. CelebVHQ: large-scale video facial attributes dataset. In ECCV, 2022. 5 A. Experiments on Spatial Knitting Attentions To investigate the peculiarity of Spatial Knitting Attentions, we conducted three experiments within the limits of available time and resources. One experiment was designed to compare the differences between SKCrossAttention and the default CrossAttention in SD1.5. The other two experiments involved applications of Spatial Knitting Attentions, demonstrating how SKAttentions can implement functionalities similar to IPAdapterFaceID and ControlNet. The relevant code and models will be released at https://github.com/HelloVision/ ExperimentsOnSKAttentions. SD EXP txt2im SK EXP txt2im SD EXP im2im SK EXP im2im GT Figure 5. SD EXP vs. SK EXP We gathered portrait-related dataset (MomoFaceTrain1 390W), containing 3.9 million images as training data. Each image was captioned with Qwen2-VL2B [17]. Two test sets were used: the first (MomoFaceTest1 1W) contains 10, 000 images with similar data distribution as the training set but no overlap. The second (FFHQTest1 3K) is composed of 3, 500 randomly selected images from the FFHQ dataset. Condition ControlNet txt2im ControlNetSK txt2im ControlNet im2im ControlNetSK im2im GT Figure 6. ControlNet vs. ControlNetSK A.1. Comparison of SKCrossAttention and"
        },
        {
            "title": "CrossAttention",
            "content": "To compare the differences between SKCrossAttention and CrossAttention, we designed two experiments. In the first experiment (SK EXP), we replaced CrossAttention in SD1.5 with SKCrossAttention, keeping other network layers and parameters unchanged. The second experiment, serving as control group (SD EXP), randomly reinitialized and trained the weights of CrossAttention in the original SD1.5 model. Both were trained under identical conditions. Specifically, we used 8 NVIDIA A100 GPUs with batch size of 160 (with gradient accumulation). The learning rate followed Cosine Scheduler, with maximum and minimum values of 5e5 and 1e7, respectively. Training lasted for 42, 000 iterations, with SD EXP taking 62 hours and SK EXP taking 72 hours. Strictly speaking, it is challenging to ensure complete fairness in this comparison experiment because SK EXP requires two CrossAttention operations, resulting in double the number of learnable parameters compared to SD EXP. Therefore, achieving fairness is difficult, whether based on the same number of iterations or training time. Another issue is that, due to limited training resources, neither model has fully converged to an optimal state after 42, 000 iterations. Nevertheless, this experiment provides foundational data for understanding the characteristics of Spatial Knitting Attentions, achieving our intended goal. IPAdapter txt2im IPAdapterSK txt2im Mix txt2im IPAdapter im2im IPAdapterSK im2im Mix im2im Reference Image Figure 7. IPAdapter vs. IPAdapterSK As shown in Tab. 2, SD EXP and SK EXP performed comparably across the two test sets, with SK EXP slightly outperforming in certain metrics. Fig. 5 displays inference examples from three test cases, and the subjective impressions align well with the objective metrics. We believe that, with sufficient training, the differences between the two methods may become more pronounced. Another possibility is that this experiment essentially reset the original parameters of the base model and retrained it, as discussed in previous experiments on face reenactment tasks. This suggests that SK Attentions might be better suited for use as plugin. We will continue to verify these hypotheses in future work, and any relevant findings will be updated on the code repository. A.2. Application of SKAttentions We found that the SKReferenceAttention module, as described earlier, can easily replicate functions similar to those of ControlNet [25], and SKCrossAttention module can similarly emulate the features of IPAdapterFaceID [24]. Consequently, we conducted two additional experiments (ControlNetSK and IPAdapterSKFaceID) to validate the broader applicability of SKAttentions. However, due to differences in data distribution, network structure, and training conditions, fair comparison with the official ControlNet and IPAdapterFaceID models is hard. Thus, these experiments and corresponding comparisons should be considered as rough references only. A.2.1 ControlNetSK Referring to Fig. 1, the implementation of ControlNetSK can directly utilize the HMReferenceNet and HMDenosingNet modules. The conditioning image for ControlNet can be extracted directly using the SD1.5 UNet, with only the weights of SKReferenceAttention updated during training. This approach does not require updating the parameters of the entire downsampling and intermediate modules in the UNet as ControlNet. In the experimental process, we used the Canny edge conditioning from ControlNet to validate feasibility. The training was conducted on 8 NVIDIA A100 GPUs with an effective batch size of 200 (using gradient accumulation). The learning rate followed Cosine Scheduler, with maximum value of 5e5 and minimum of 1e7. The training totaled 42, 000 iterations and took 139 hours. As shown in the results in Tab. 2, ControlNetSK outperforms ControlNet, which is also evident from the visual examples in Fig. 6. This improvement is due to ControlNetSKs training on portrait data, whereas ControlNet is more general model, so its expected that ControlNetSK would perform better in portrait generation scenarios. Moreover, ControlNetSK achieves these results with fewer learnable parameters and training steps, further valiMethod FFHQTest1 3K SSIM PSNR FID 36.27 SD EXP txt2im 35.65 SK EXP txt2im 28.29 SD EXP im2im 28.70 SK EXP im2im ControlNet txt2im 24.25 ControlNetSK txt2im 17.99 19.17 ControlNet im2im ControlNetSK im2im 14.76 68.09 IPAdapter txt2im 42.08 IPAdapterSK txt2im 38.95 Mix txt2im 27.51 IPAdapter im2im 30.75 IPAdapterSK im2im 29.18 Mix im2im 27.90 27.90 28.14 28.15 27.92 27.91 28.42 28.51 27.90 27.89 27.89 28.13 28.13 28.14 0.313 0.315 0.416 0.416 0.398 0.471 0.538 0.587 0.254 0.290 0.291 0.391 0.399 0. sim - - - - - - - - 0.172 0.195 0.372 0.262 0.213 0.399 MomoFaceTest1 1W PSNR FID SSIM 20.66 19.37 13.12 12.41 21.69 12.39 13.71 7.25 62.22 25.13 25.96 20.51 15.10 15.45 27.91 27.91 28.54 28.54 27.94 27.91 28.74 28.98 27.91 27.91 27.92 28.44 28.54 28. 0.356 0.358 0.508 0.508 0.455 0.548 0.628 0.681 0.285 0.346 0.344 0.486 0.511 0.508 sim - - - - - - - - 0.154 0.338 0.440 0.240 0.378 0.479 Table 2. Evaluation results for the SKAttentions-related experiments, where the sim metric represents the similarity between the faces in the reference image and the generated output. dating the effectiveness of this structure. A.3. Conclusion Our experiments validated the characteristics and potential applications of SKAttentions, demonstrating the value of this structure to some extent. However, there are two areas for improvement. Firstly, the training dataset contains approximately one-third low-resolution data, and we plan to continuously enhance both the quantity and quality of training data. Secondly, insufficient training is another limitation; in the future, we will keep iterating on the related models, and improved results will be updated on the code page. We also intend to explore new applications of this structure, such as other versions of ControlNetSK under different conditions. A.2.2 IPAdapterSK FaceID Similar to IPAdapter FaceID, we used face features extracted by InsightFace [1] to represent face ID. These features are decoupled from attributes like lighting, artistic style, and pose, allowing for effective integration with text prompts and offering significant creative freedom. However, unlike IPAdapterFaceID, we directly replicated the face features five times to form linear feature of length five, then incorporated face ID information into the UNet using the SKCrossAttention mechanism. The training process and conditions were similar to those of ControlNetSK. We used 8 NVIDIA A100 GPUs, with an effective batch size of 224 (using gradient accumulation). The learning rate followed Cosine Scheduler, with maximum value of 5e 5 and minimum of 1e 7. total of 42,000 iterations were conducted, taking approximately 102 hours. As seen from the results in Tab. 2 and the examples in Fig. 7, IPAdapter and IPAdapterSK perform at similar, relatively moderate levels. However, when used in combination, they produce substantial improvement in results. This suggests that combining ad adapters for text-to-image base models does not necessarily lead to mutual interference; instead, it can result in mutual enhancement. Furthermore, IPAdapterSK achieving this performance with limited training suggests significant untapped potential for further development."
        }
    ],
    "affiliations": [
        "HelloGroup Inc."
    ]
}