{
    "paper_title": "DoTA-RAG: Dynamic of Thought Aggregation RAG",
    "authors": [
        "Saksorn Ruangtanusak",
        "Natthapath Rungseesiripak",
        "Peerawat Rojratchadakorn",
        "Monthol Charattrakool",
        "Natapong Nitarach"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we introduce DoTA-RAG (Dynamic-of-Thought Aggregation RAG), a retrieval-augmented generation system optimized for high-throughput, large-scale web knowledge indexes. Traditional RAG pipelines often suffer from high latency and limited accuracy over massive, diverse datasets. DoTA-RAG addresses these challenges with a three-stage pipeline: query rewriting, dynamic routing to specialized sub-indexes, and multi-stage retrieval and ranking. We further enhance retrieval by evaluating and selecting a superior embedding model, re-embedding the large FineWeb-10BT corpus. Moreover, we create a diverse Q&A dataset of 500 questions generated via the DataMorgana setup across a broad range of WebOrganizer topics and formats. DoTA-RAG improves the answer correctness score from 0.752 (baseline, using LiveRAG pre-built vector store) to 1.478 while maintaining low latency, and it achieves a 0.929 correctness score on the Live Challenge Day. These results highlight DoTA-RAG's potential for practical deployment in domains requiring fast, reliable access to large and evolving knowledge sources."
        },
        {
            "title": "Start",
            "content": "DoTA-RAG: Dynamic of Thought Aggregation RAG Natthapath Rungseesiripak SCBX Bangkok, Thailand natthapath.r@scbx.com Peerawat Rojratchadakorn SCBX Bangkok, Thailand peerawat.r@scbx.com Saksorn Ruangtanusak SCBX Bangkok, Thailand saksorn.r@scbx.com Monthol Charattrakool SCBX Bangkok, Thailand monthol.c@scbx.com Natapong Nitarach SCB 10X Bangkok, Thailand natapong@scb10x.com 5 2 0 J 4 1 ] . [ 1 1 7 5 2 1 . 6 0 5 2 : r Abstract In this paper, we introduce DoTA-RAG (Dynamic-of-Thought Aggregation RAG), Retrieval-Augmented Generation system optimized for high throughput, large-scale web knowledge indexes. Traditional RAG pipelines often suffer from high latency and limited accuracy over massive, diverse datasets. DoTA-RAG addresses these challenges with three-stage pipeline: query rewriting, dynamic routing to specialized sub-indexes, and multi-stage retrieval and ranking. We further enhance retrieval by evaluating and selecting superior embedding model, re-embedding the large FineWeb10BT corpus. Moreover, we create diverse Q&A dataset of 500 questions generated via the DataMorgana setup across broad range of WebOrganizer topics and formats. DoTA-RAG improves the answer correctness score from 0.752 (baseline, using LiveRAG pre-built vector store) to 1.478 while maintaining low latency and achieves 0.929 correctness score on the Live Challenge Day. These results highlight DoTA-RAGs potential for practical deployment in domains requiring fast, reliable access to large and evolving knowledge sources. Keywords Retrieval-Augmented Generation; Dynamic routing; Hybrid retrieval; RAG benchmark synthesis ACM Reference Format: Saksorn Ruangtanusak, Natthapath Rungseesiripak, Peerawat Rojratchadakorn, Monthol Charattrakool, and Natapong Nitarach. 2025. DoTA-RAG: Dynamic of Thought Aggregation RAG. In Proceedings of SIGIR2025 LiveRAG Challenge. ACM, Padua, Italy, 7 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 INTRODUCTION AND RELATED WORK\nLarge language models (LLMs) have achieved strong results across\nNLP tasks, yet they often struggle with up-to-date or domain-\nspecific knowledge, leading to hallucinations [7]. Retrieval-Augmented",
            "content": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. SIGIR2025 LiveRAG Challenge, TBD 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 9781450312345 https://doi.org/XXXXXXX.XXXXXXX Figure 1: RAG Correctness and Faithfulness in the internal test set for different retrieval-augmented generation approaches. We use the Falcon3-10B-Instruct as the base LLM Generation (RAG) mitigates this by allowing LLMs to access external documents: retriever fetches relevant text, which the LLM then conditions on when generating answers [9]. Since its introduction by Lewis et al., RAG has become central to knowledge-intensive applications. To improve retrieval robustness, query rewriting has emerged as key technique [7], reformulating ambiguous or underspecified queries via prompt-engineered LLMs. For instance, HyDE prompts an LLM to generate hypothetical answer document, enhancing retrieval by focusing on answeranswer similarity [5]. In the LiveRAG competition, we leverage the FineWeb-10BT corpus [12]. To handle its diverse sources, routing techniques are employed to select optimal retrieval paths based on query semantics or metadata [10, 11]. WebOrganizer [16] enhances this by categorizing documents by topic and format, enabling precise sub-corpus selection. To reduce retrieved document count, we use rerankerscrossencoder models that reorder results based on deeper query-document interactions. Inspired by ColBERT [8], production systems like Coheres Rerank 3.5 [2] improve relevance and filter noise. Benchmarking RAG systems remains difficult due to the lack of realistic, diverse test sets. DataMorgana [4] addresses this with configurable tools for generating synthetic Q&A datasets that reflect varied user intents. SIGIR2025 LiveRAG Challenge, February 2025, TBD Ruangtanusak et al. Figure 2: Diagram illustrating the components and workflow of DoTA-RAG. The SIGIR 2025 LiveRAG Challenge mandates fixed 15M-document corpus (FineWeb-10BT) and standardized LLM (Falcon-3-10BInstruct [13]), posing significant challenges for scalable and accurate retrieval. We propose DoTA-RAG (Dynamic of Thought Aggregation RAG) to address these issues. It integrates dynamic routing and hybrid retrieval to enhance retrieval efficiency and accuracy at scale. In summary, our pipeline is built around two key components: Dynamic Routing: framework that optimizes the retrieval path for large-scale corpora, reducing latency and enabling aggregation from diverse sources. Hybrid Retrieval: multi-stage strategy combining dense expansion, sparse filtering, and reranking for high-relevance document selection."
        },
        {
            "title": "2.1 Query Rewriting (Stage 1)",
            "content": "Low-Temp Rewrite Query. Initially, we excluded query rewriting stage, since methods like HyDE [6] and Chain-of-Draft [17], along with crafted prompts, were heavily tested but decreased Correctness and Faithfulness. They frequently did not generalize well or address internal retrieval challenges. On Live Challenge Day, we discovered new failure cases involving user queries with either highly specialized terms or significant misspellings. This led to few or irrelevant results (e.g., \"wut iz rajun cajun crawfsh festivl\", \"wut sum side affects of nicotine gum\"), prompting us to re-evaluate query rewriting to tackle these issues. Appendix details the query rewriting prompt."
        },
        {
            "title": "2.2 Dynamic Namespace Routing (Stage 2)",
            "content": "Ensemble-Based Query Routing. To route queries efficiently across topical domains, we maintain separate Pinecone namespace for each domain (see Appendix C). For every incoming question, we generate four independent classifications using Falcon3-10B-Instruct with self-consistency voting [15]. Each returns one or more namespace predictions, and we tally which namespaces appear most often. We then query the top two namespaces in parallel, shrinking the average search space by 92% and reducing dense-retrieval latency from 100.84 to 19.01 per question. The prompt used for namespace routing can be found in Appendix A."
        },
        {
            "title": "2.3 Hybrid Retrieval (Stage 3)",
            "content": "(1) Dense search. Embed the refined query with Snowflake Arctic-embed-m-v2.0 [18] and fetch k=100 candidates using cosine similarity. (2) BM25 pruning. Compute lexical scores on-the-fly; retain the top 20 passages. (3) Coheres Rerank 3.5 Cross-encode and select the 10 highestscoring passages."
        },
        {
            "title": "2.5 Answer Generation (Stage 5)\nFor answer generation, we use the prompt shown in Appendix A,\nwhich is prepended with the aggregated context and the rewritten\ninput query. The answer is then generated by Falcon3-10B-Instruct\n[13].",
            "content": "Section Summary. DoTA-RAG is RAG pipeline developed for the SIGIR 2025 LiveRAG Challenge, designed to deliver high-quality answers with significantly reduced inference latency over 15Mdocument knowledge base. The system features five key stages: (1) Query Rewriting, which enhances retrieval for noisy or misspelled queries; (2) Dynamic Namespace Routing, using an ensemble classifier to dynamically select relevant sub-indexes and shrink the search space; (3) Hybrid Retrieval, combining dense search, BM25 pruning, and cross-encoder re-ranking; (4) Context Aggregation, which compiles and trims top-ranked passages; and (5) Answer Generation, where Falcon3-10B-Instruct [13] produces answers grounded in the retrieved context."
        },
        {
            "title": "3.1 Internal test set Construction\nTo create our internal test set, we utilized the DataMorgana API\n[4] to generate an initial collection of 1,000 Q&A pairs. To en-\nsure question diversity, we adopted DataMorgana’s categorization\nframework (user types, phrasing styles, premises, and linguistic",
            "content": "DoTA-RAG: Dynamic of Thought Aggregation RAG SIGIR2025 LiveRAG Challenge, February 2025, TBD variations) and enhanced it with our own question-formulation taxonomy (see Table 1). This new categorization introduces carefully defined question types with varying complexity levels, including questions requiring either single or multiple document sources for resolution. For example, Temporal-evolution questions require the retrieval system to distinguish between documents discussing the same entity across different time periods, while challenging the generation module to synthesize information about chronological changes from these temporally distinct sources. While \"Verification\" questions test the systems ability to evaluate mixed assertions containing both facts and falsehoods. This category directly challenges the generation module to distinguish truth from misinformation, correct user misconceptions, and maintain factual integrity even when prompted with partially incorrect premises. Appendix provides descriptions and examples of the newly defined categories. Table 1: The question-formulation categorization. Category Required Document(s) Multi-aspect Comparison Temporal-evolution Problem-solution Procedural Causal Quantitative Verification Two documents Two documents Two documents Two documents Single document Single document Single document Single document Each Q&A pairs document was auto-tagged using WebOrganizers TopicClassifier (24 topics) and FormatClassifier (24 formats) [16]. This tagging process was instrumental in creating MorganaMultiDocQAour 500-question benchmark. The classification ensured diverse document topics and formats throughout the test set, which is critical for robust evaluation of retrieval-augmented models. This diversity prevents benchmark bias toward specific domains or document structures, thereby offering more comprehensive assessment of model generalization capabilities across the heterogeneous content types found on the internet. We then applied stratified sampling to maintain proportional representation across all topic-format combinations, preserving the natural distribution patterns found in broader information ecosystems: (cid:108) 𝑛𝑐 = 𝑁𝑐 (cid:205)𝑐 𝑁𝑐 where 𝑁𝑐 is the number of candidates in category 𝑐 and 𝑛𝑐 the final sample size. This guarantees balanced coverage across the = 24 24 strata. 500(cid:109) , (1)"
        },
        {
            "title": "3.2 Embedding Models (MTEB Leaderboard)\nTo select a suitable dense retriever for our pipeline, we considered\nembedding model performance using the MTEB English retrieval\ntasks leaderboard [3]. Among models with fewer than 1 billion pa-\nrameters, Snowflake’s Arctic v2.0 embedding models demonstrated\nstate-of-the-art results [18], achieving mean scores of 58.56 (large)\nand 58.41 (medium), significantly outperforming our initial model,\nE5-base-v2 [14], which scored 49.67. Based on these findings, we\nre-embedded and re-indexed the FineWeb-10BT corpus [12] using",
            "content": "Arctic-embed-m-v2.0. On our internal test set, this change led to an improvement in retrieval quality, with Recall@10 increasing from 0.469 to 0.518."
        },
        {
            "title": "3.3 Evaluation Metrics\nWe assess system performance using two main metrics: Correctness\nand Faithfulness, each evaluated on a continuous scale.",
            "content": "Correctness (-1 to 2): Assesses the relevance and coverage of the generated answer. score of -1 indicates an incorrect answer, while 2 represents fully correct and relevant response with no extraneous information. Faithfulness (-1 to 1): Measures whether the answer is grounded in the retrieved passages. score of -1 indicates no grounding at all, whereas 1 means the entire answer is fully supported by retrieved content."
        },
        {
            "title": "3.4 LLM-as-a-Judge Evaluation\nWe graded answers for correctness and faithfulness using two auto-\nmatic judges. While Claude 3.5 Sonnet [1] served as a strong judge,\nwe found Falcon3-10B-Instruct [13] to be a faster and cheaper alter-\nnative with comparable evaluation quality. During our experiments\nfor the competition, we frequently used Falcon3-10B-Instruct to\nlower evaluation costs and accelerate the process. On the Morgana-\nMultiDocQA test set, Falcon3-10B-Instruct yielded slightly higher\nscores on both metrics (see Table 2).",
            "content": "Table 2: Judge agreement on the test set. Metric Claude 3.5 Sonnet [1] Falcon3-10B Instruct [13] Correctness [-1:2] Faithfulness [-1:1] 1.382 0. 1.430 0."
        },
        {
            "title": "4.1 Internal test set Result\nOur ablation path incrementally augments the baseline pipeline (E5-\nbase-v2 embeddings [14] + Falcon3-10B-Instruct [13] generation)\nwith various enhancements to isolate their individual and combined\neffects on performance:",
            "content": "Baseline Intfloats e5-base-v2 10 retrieved passages Falcon3-10B-Instruct generation. +Arctic-M Swap in Snowflakes arctic-embed-m-v2.0 [18]. +Routing Dynamic routing for namespace selection. +Pruning Retrieve 100 passages, then reduce to 10 passages using BM25. +Rerank Prune to 20 passages, then use Coheres Rerank 3.5 [2] to select the top 10. +Rewrite Rewrite query to make the RAG pipeline more robust to the live test set this is DoTA-RAG. SIGIR2025 LiveRAG Challenge, February 2025, TBD Ruangtanusak et al. Table 3: Pipelines performance and inference time on internal test set using Claude 3.5 Sonnet [1]. Metrics are shown for full text and truncated inputs. Method Baseline + Arctic-M + Routing + Pruning + Rerank + Rewrite Correctness [-1:2] Faithfulness [-1:1] Sec/Question All words 300-word cap All words 300-word cap 0.752 1.616 1.562 1.562 1.652 1.478 0.761 1.626 1.577 1.566 1.686 1.484 -0.496 -0.216 -0.108 0.428 0.672 0.640 -0.493 -0.225 -0.090 0.404 0.662 0. 100.84 19.01 29.84 35.20 35."
        },
        {
            "title": "6 CONCLUSION\nWe introduce DoTA-RAG, a live Retrieval-Augmented Genera-\ntion pipeline that couples Dynamic-of-Thought Aggregation with\ndynamic routing and hybrid retrieval. By extracting on-the-fly meta-\ndata and steering each query to the most appropriate sub-index,\nDoTA-RAG reconciles the seemingly contradictory requirements\nof web-scale knowledge integration, precision, and low latency that\ncharacterize the SIGIR 2025 LiveRAG Challenge.",
            "content": "Experiments on our 500-question MorganaMultiDocQA benchmark demonstrate that successive upgradesfrom changing the dense retriever to Arctic-embed-m-v2.0 [18] to incorporating query rewritingincrease correctness from 0.752 to 1.478 and faithfulness from -0.496 to 0.640, while maintaining median end-to-end latency of 35.63 seconds per question. On the Live Challenge Day dataset, the same configuration achieves 0.929 correctness, validating the generalization of our RAG beyond in-house data. Our ablation studies highlight two actionable insights for the Gen-IR community: (1) Metadata-guided routing delivers outsized gains. Even lightweight routing cues significantly reduce irrelevant results and cut retrieval latency by more than half compared to static top-𝑘 search. (2) Hybrid retrieval significantly enhances document quality. By combining fan-out dense retriever for broad semantic coverage and sparse retriever for keyword-based filtering, the system improves the faithfulness score from -0.108 to 0.428 substantial gain in factual alignment. Acknowledgments We appreciate the SIGIR 2025 LiveRAG Challenge organizers for supplying the resources and tools needed for our participation and the evaluation set creation. The results in Figure 1 and Table 3 illustrate the incremental improvements from each ablation step on the internal test1 set. Replacing the baseline embeddings with arctic-embed-m-v2.0 [18] (+Arctic-M) yields substantial increase in correctness, though faithfulness remains negative. The addition of dynamic routing (+Routing) and BM25-based pruning (+Pruning) maintains high correctness and leads to notable gain in faithfulness. Incorporating Coheres Rerank 3.5 [2] (+Rerank) further improves both correctness, representing the strongest overall performance. Although adding rewriting decreases performance, we believe including it as part of the DoTA-RAG would better align our model with the test set in the LiveRAG Live Challenge Day, as discussed in Section 2.1."
        },
        {
            "title": "4.2 LiveRAG Live Challenge Day Performance\nOn the official Live Challenge Day leaderboard, our final pipeline\nachieved a correctness score of 0.929, confirming that our re-\ntrieval–generation loop produced high-quality answers. However,\nbecause the evaluation enforced a strict 300-word output cap—a con-\nstraint we overlooked during tuning, our faithfulness score sank to\njust 0.043.",
            "content": "After the score was announced, it was surprising to see our faithfulness score so low. To understand what went wrong, we decided to conduct deeper investigation. We re-ran the evaluation using Claude 3.5 Sonnet as the judge [1], with prompt tailored to the faithfulness criteria. We reassessed the 500 answers, both uncut and capped at 300 words. The faithfulness score dropped significantly from 0.702 to 0.336."
        },
        {
            "title": "5 LIMITATIONS AND FUTURE WORK\nIn future research endeavors, our goal is to explore strategies for\nmulti-source routing that leverage graph-based knowledge bases,\ndelving into the complexities of these systems. Furthermore, we\nare planning to investigate self-improvement methods applied after\ngenerating responses, aiming to refine this process. Another avenue\nof our research involves developing techniques for compacting\ncontext within windows larger than 8,000 tokens, as well as honing\nthe procedures involved in reasoning retrieval, thereby enhancing\noverall performance.",
            "content": "1Note that inference time (Sec/Question) is omitted for the Baseline, as it uses pre-built index that is not directly comparable to our FineWeb-based Pinecone index. DoTA-RAG: Dynamic of Thought Aggregation RAG SIGIR2025 LiveRAG Challenge, February 2025, TBD [18] Puxuan Yu, Luke Merrick, Gaurav Nuti, and Daniel Campos. 2024. Arctic-Embed 2.0: Multilingual Retrieval Without Compromise. arXiv:2412.04506 [cs.CL] https: //arxiv.org/abs/2412.04506 References [1] Anthropic. 2024. Claude 3.5 Sonnet Model Card Addendum. paperswithcode.com/paper/claude-3-5-sonnet-model-card-addendum. cessed: 2025-05-18. https:// Ac- [2] Cohere. 2024. Introducing Rerank 3.5: More Relevant Results with Less Compute. https://cohere.com/blog/rerank-3pt5. Accessed: 2025-05-18. [3] Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, Márton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzemiński, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan Rystrøm, Roman Solomatin, Ömer Çağatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafał Poświata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Björn Plüster, Jan Philipp Harries, Loïc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek Šuppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael Günther, Mengzhou Xia, Weijia Shi, Xing Han Lù, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, and Niklas Muennighoff. 2025. MMTEB: Massive Multilingual Text Embedding Benchmark. arXiv preprint arXiv:2502.13595 (2025). doi:10.48550/arXiv.2502.13595 [4] Simone Filice, Guy Horowitz, David Carmel, Zohar Karnin, Liane Lewin-Eytan, and Yoelle Maarek. 2025. Generating Diverse Q&A Benchmarks for RAG Evaluation with DataMorgana. arXiv:2501.12789 [cs.CL] https://arxiv.org/abs/2501. [5] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise ZeroShot Dense Retrieval without Relevance Labels. arXiv:2212.10496 [cs.IR] https: //arxiv.org/abs/2212.10496 [6] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2023. Precise Zero-Shot Dense Retrieval without Relevance Labels. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 17621777. doi:10.18653/v1/2023. acl-long.99 [7] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented Generation for Large Language Models: Survey. arXiv:2312.10997 [cs.CL] https://arxiv.org/abs/2312.10997 [8] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. arXiv:2004.12832 [cs.IR] https://arxiv.org/abs/2004.12832 [9] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. arXiv:2005.11401 [cs.CL] https://arxiv.org/abs/ 2005.11401 [10] Xiaoqian Li, Ercong Nie, and Sheng Liang. 2023. From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL. arXiv:2311.06595 [cs.CL] https://arxiv.org/abs/2311.06595 [11] Darshil Modi. 2024. AutoMeta RAG: Enhancing Data Retrieval with Dynamic Metadata-Driven RAG Framework. Accessed: 2025-05-18. [12] Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. 2024. The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. https://openreview.net/forum?id=n6SCkn2QaG [13] TII Team. 2024. The Falcon 3 family of Open Models. [14] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text Embeddings by Weakly-Supervised Contrastive Pre-training. arXiv preprint arXiv:2212.03533 (2022). [15] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. arXiv:2203.11171 [cs.CL] https://arxiv.org/abs/2203. [16] Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, and Luca Soldaini. 2025. Organize the Web: Constructing Domains Enhances PreTraining Data Curation. arXiv:2502.10341 [cs.CL] https://arxiv.org/abs/2502. 10341 [17] Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. 2025. Chain of Draft: Thinking Faster by Writing Less. arXiv:2502.18600 [cs.CL] https://arxiv.org/abs/ 2502.18600 SIGIR2025 LiveRAG Challenge, February 2025, TBD Ruangtanusak et al. Prompt Effective prompt engineering is critical for leveraging large language models (LLMs) in variety of tasks, from rewriting text to classifying queries and generating informative responses. In this section, we outline carefully designed prompt templates that guide the behavior of LLM-powered assistants. These prompts are constructed to ensure clarity, precision, and relevance in model outputs, enhancing both usability and accuracy across diverse scenarios, as illustrated by the examples in Figure 3 (Query Rewriting), Figure 4 (Routing Namespace Classify), and Figure 5 (Generation Prompt). Query Rewriting You are helpful assistant. Your task is to rewrite sentences by correcting typos and improving are written in clear, natural English. If typo is intentional or acceptable as-is, leave it unchanged. wording ensure they the to Figure 3: Query Rewriting Routing Namespace Classify Q: {question} Available namespaces: {choices_str} Step1: Identify what the question is about. Step2: Choose only the most relevant namespaces. Step3: Return final result in //boxed{}. Figure 4: Routing Namespace Classify Generation prompt You are helpful assistant. Context: <passages> Question: <query> Figure 5: Generation Prompt"
        },
        {
            "title": "Formulations with Examples",
            "content": "Multi-aspect. question about two different aspects of the same entity/concept. For example: What are the advantages of AI-powered diagnostics, and what are the associated risks of bias in medical decision-making?, How do cryptocurrencies enable financial inclusion, and what are the security risks associated with them?. The information required to answer the question needs to come from two documents; specifically, the first document must provide information about the first aspect, while the second must provide information about the second aspect. Comparison. comparison question that requires comparing two related concepts or entities. The comparison must be natural and reasonable, i.e., comparing two entities by common attribute that is meaningful and relevant to both entities. For example: Who is older, Glenn Hughes or Ross Lynch?, Are Pizhou and Jiujiang in the same province?, Pyotr Ilyich Tchaikovsky and Giuseppe Verdi have this profession in common. The information required to answer the question needs to come from two documents; specifically, the first document must provide information about the first entity/concept, while the second must provide information about the second entity/concept. Temporal-evolution. question that explores how something has changed, progressed, or developed over time. The first document covers the earlier historical period or initial state, while the second document covers the later historical period or final state. For example: How has smartphone technology evolved over the past two decades?, What changes have occurred in climate policy since the Paris Agreement?, How has the portrayal of women in film changed from the 1950s to today?. The answer should describe trends, shifts, or stages of development across timeline. Problem-solution. Questions that ask about both problem and potential solutions. The first document details the problem and its implications, while the second document explores possible solutions or mitigation strategies. For example: What are the main challenges facing global food security, and what innovative agricultural technologies offer the most promising solutions?, What are the causes and consequences of urban air pollution, and what policies have proven effective in reducing it?, What factors contribute to the rise in mental health issues among teenagers, and what interventions can schools implement to support student wellbeing?, Why is plastic waste growing environmental concern, and what strategies can be used to reduce its impact?, What are the limitations of current cybersecurity measures, and how can emerging technologies address them?. Procedural. question that asks how to do something or requests step-by-step instructions. For example: How do you train neural network using TensorFlow?, What are the steps to apply for student visa to the United States?, How can set up secure home Wi-Fi network?. The answer should provide clear, ordered list of steps or detailed process to follow. Causal. question that seeks to understand why something happens or explores the relationship between cause and effect. For example: Why does increasing carbon dioxide in the atmosphere lead to global warming?, What causes inflation to rise during economic booms?, Why do some people develop allergies while others do not?. The answer should explain the underlying reasons or mechanisms behind the phenomenon. Quantitative. question that seeks numerical data, statistics, or measurements. For example: What is the average life expectancy in Japan?, How many people use public transportation in New DoTA-RAG: Dynamic of Thought Aggregation RAG SIGIR2025 LiveRAG Challenge, February 2025, TBD Figure 6: Distribution of topics (left) and document formats (right) in Fineweb-10BT, based on WebOrganizer classifiers. The area of each block reflects the number of documents per domain in the corpus. York City each day?, What was the global GDP growth rate in 2023?. The answer should include specific numbers, percentages, or quantitative comparisons supported by data. Verification. question that asks to confirm or deny the truth of particular claim or statement. These questions involve evaluating one or more statements, where at least one is true and at least one is false. For example: Is it true that vitamin cures the common cold and that antibiotics are effective against viruses?, Did Einstein win the Nobel Prize for his theory of relativity and was he born in Austria?. The answer should clearly indicate which statements are correct and which are incorrect, with justification. Fineweb-10BT Characteristic Fineweb-10BT is 15-million-document web corpus that we embed with Snowflake Arctic-embed-m-v2.0 in the Pinecone vector database to use in our RAG pipeline. Using WebOrganizers [16] 24-label TopicClassifier and 24-label FormatClassifier, we tagged every document along two orthogonal axestopic and document format. As the treemap in Figure 6 shows, the largest topical slices are Finance & Business (9.1%), Sports & Fitness (7.6%), and Entertainment (7.6%), while the dominant formats are News Article (17.4%), Personal Blog (15.6%), and Product Page (14.9%). Crucially, long-tail domains (Software Dev., History, Adult, FAQ, Legal) remain present, giving the corpus the heterogeneity that retrieval-augmented models need for robust generalization. Internal benchmark. We constructed diverse 500-question benchmark by sampling across the full 2424 topicformat grid, ensuring wide coverage of content types and styles. Query routing. To improve retrieval speed, we map each topic to dedicated Pinecone namespace. For each query, we use Falcon310B-Instruct with self-consistency voting to predict the relevant topics, querying only the top two namespaces."
        }
    ],
    "affiliations": [
        "SCB 10X Bangkok, Thailand",
        "SCBX Bangkok, Thailand"
    ]
}