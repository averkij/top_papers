{
    "paper_title": "MIGE: A Unified Framework for Multimodal Instruction-Based Image Generation and Editing",
    "authors": [
        "Xueyun Tian",
        "Wei Li",
        "Bingbing Xu",
        "Yige Yuan",
        "Yuanzhuo Wang",
        "Huawei Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite significant progress in diffusion-based image generation, subject-driven generation and instruction-based editing remain challenging. Existing methods typically treat them separately, struggling with limited high-quality data and poor generalization. However, both tasks require capturing complex visual variations while maintaining consistency between inputs and outputs. Therefore, we propose MIGE, a unified framework that standardizes task representations using multimodal instructions. It treats subject-driven generation as creation on a blank canvas and instruction-based editing as modification of an existing image, establishing a shared input-output formulation. MIGE introduces a novel multimodal encoder that maps free-form multimodal instructions into a unified vision-language space, integrating visual and semantic features through a feature fusion mechanism.This unification enables joint training of both tasks, providing two key advantages: (1) Cross-Task Enhancement: By leveraging shared visual and semantic representations, joint training improves instruction adherence and visual consistency in both subject-driven generation and instruction-based editing. (2) Generalization: Learning in a unified format facilitates cross-task knowledge transfer, enabling MIGE to generalize to novel compositional tasks, including instruction-based subject-driven editing. Experiments show that MIGE excels in both subject-driven generation and instruction-based editing while setting a state-of-the-art in the new task of instruction-based subject-driven editing. Code and model have been publicly available at https://github.com/Eureka-Maggie/MIGE."
        },
        {
            "title": "Start",
            "content": "MIGE: Unified Framework for Multimodal Instruction-Based Image Generation and Editing Xueyun Tian, Wei Li, Bingbing Xu, Yige Yuan, Yuanzhuo Wang, Huawei Shen CAS Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China University of Chinese Academy of Sciences, Beijing, China {tianxueyun23z, xubingbing, yuanyige20z, wangyuanzhuo, shenhuawei}@ict.ac.cn weili.ucas.ict@gmail.com 5 2 0 2 8 ] . [ 1 1 9 2 1 2 . 2 0 5 2 : r Figure 1: Demonstrating the comprehensive capabilities of MIGE. As unified framework, MIGE excels in subject-driven generation and instruction-based editing while performing well in the new compositional task of instruction-based subject-driven editing."
        },
        {
            "title": "Abstract",
            "content": "Despite significant progress in diffusion-based image generation, subject-driven generation and instruction-based editing remain challenging. Existing methods typically treat them separately, struggling with limited high-quality data and poor generalization. However, both tasks require capturing complex visual variations while maintaining consistency between inputs and outputs. Therefore, we propose MIGE, unified framework that standardizes task representations using multimodal instructions. It treats subject-driven generation as creation on blank canvas and instruction-based editing as modification of an existing image, establishing shared input-output formulation. MIGE introduces novel multimodal encoder that maps free-form multimodal instructions into unified vision-language space, integrating visual and semantic features through feature fusion mechanism. This unification enables joint training of both tasks, providing two key advantages: (1) Cross-Task Enhancement: By leveraging shared visual and semantic representations, joint training improves instruction adherence and visual consistency in both subjectdriven generation and instruction-based editing. (2) Generalization: Learning in unified format facilitates cross-task knowledge transfer, enabling MIGE to generalize to novel compositional tasks, including instruction-based subject-driven editing. Experiments show that MIGE excels in both subject-driven generation and instruction-based editing while setting state-of-the-art in the new task of instructionbased subject-driven editing. Code and model have been publicly available at this link."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in diffusion models (Rombach et al., 2022; Peebles and Xie, 2023; Labs, 2023; Esser et al., 2024) have greatly advanced customized image generation. Central to this field are subject-driven image generation, which focuses on preserving given subject, and instruction-based image editing, which emphasizes flexible modifications based on textual instructions. Existing methods address these tasks separately, each tailored to specific input-output format, limiting generalization and hindering unified instruction-following model. Subject-driven generation aims to preserve subject identity while adhering to textual instructions. Fine-tuning methods, like DreamBooth (Ruiz et al., 2023), require separate training for each subject during testing, which limits generalization. In contrast, zero-shot methods like KOSMOS-G (Pan et al., 2023) rely on multimodal alignment but struggle with instruction following, mainly due to the scarcity of highquality multimodal training data, especially for multi-subject inputs. Instruction-based editing, exemplified by InstructPix2Pix (Brooks et al., 2023) and EmuEdit (Sheynin et al., 2024), modifies existing images based on instructions. These methods require large, high-quality editing datasets to handle diverse instructions but struggle with maintaining local-to-global consistency. Both tasks face significant challenges due to limited data, which hinders their ability to adapt to diverse subjects or editing requirements. Furthermore, they are typically trained independently, lacking cross-task consistency, which prevents unified understanding of the tasks. Despite these challenges, the two tasks share fundamental principle: preserving visual consistency between inputs and outputs while capturing visual variations based on instructions. Additionally, they also follow similar input-output paradigm, with complementary data focuses. Subject-driven image generation trains models to produce subject-accurate images, while instruction-based editing focuses on manipulating images without altering irrelevant identities. The above limitations as well as the complementary nature underscore the need for unified framework that combines the strengths of both tasks to achieve improved performance. Inspired by the above analysis, we propose unified framework, MIGE, which leverages Multimodal Instructions to unify subject-driven Generation and instruction-based Editing. This vision-language representation enables substituting both entities and entire images in text prompts with their visual counterparts, allowing flexible task and instruction combinations. Structurally, we model subject-driven generation as creating an image on blank canvas and instruction-based editing as modifying an existing image. This coherent input-output mapping simplifies the process while enabling both tasks to reinforce each other through joint training. Moreover, this integrated approach fosters emergent compositional capabilities beyond what either task can achieve alone. Building on our unified framework, we address two critical challenges. First, for multimodal instruction encoding, existing methods (Li et al., 2023a; Pan et al., 2023; Li et al., 2024b) primarily extract semantic features from images by CLIP vision encoder (Radford et al., 2021), which is insufficient for preserving fine-grained subject details. To overcome this limitation, we introduce multimodal encoder equipped with feature fusion mechanism. This innovation integrates VAE (Kingma, 2013) visual features into semantic tokens, effectively capturing both detailed visual information and semantics. Secondly, we enhance compositionality through joint training, facilitating instruction-based subject-driven editing. To further optimize performance in this complex task, we develop novel data construction pipeline based on Multimodal Large Language Model (MLLM). This pipeline autonomously generates diverse multimodal instructions and corresponding output images. Moreover, to address the absence of an evaluation benchmark for this new composition task, we introduce MIGEBench. This specialized benchmark evaluates compositionality in terms of subject preservation and instruction adherence, providing comprehensive assessment of the new task. Our experiments demonstrate that joint training under the MIGE framework enables mutual reinforcement between subject-driven generation and instruction-based editing by leveraging complementary data, leading to significant performance gains over training on individual tasks. Specifically, we observe improved instruction adherence on DreamBench (Ruiz et al., 2023) and enhanced detail preservation on EmuEdit and MagicBrush (Zhang et al., 2024) test sets. Furthermore, MIGE achieves state-of-the-art results on MIGEBench, showcasing its fine-grained controllability in the emerging compositional tasks. Extensive case studies (as shown in Figure 5 and Figure 6) further illustrate the ability of MIGE to generate precise and consistent outputs, highlighting its effectiveness in diverse scenarios. In summary, our contributions include: Unified Framework: We propose MIGE, unified framework that integrates subject-driven generation and instruction-based editing, facilitating joint training and mutual enhancement of both tasks. Compositional Capability: We observed that joint training unlocks the compositional capability of instruction-based subject-driven editing. We propose novel data construction pipeline and introduce the MIGEBench benchmark for comprehensive evaluation. Strong Performance: The extensive experimental results show that MIGE achieves competitive results in both subject-driven generation and instruction-based editing while establishing new state-of-the-art in instructionbased subject-driven image editing."
        },
        {
            "title": "2 Related Work",
            "content": "In this section, we focus on discussing the most relevant studies in this section. more comprehensive discussion, including additional related work, can be found in the Appendix D. Universal generative model Unifying tasks in image generation is challenging, and several approaches have been proposed. Pixwizard (Lin et al., 2024) leverages task vectors to integrate image generation and editing but struggles with visionlanguage instructions due to architectural limitaInstruct-Imagen (Hu et al., 2024) unifies tions. tasks through multimodal instructions but remains ineffective for editing even after fine-tuning. ACE (Han et al., 2024) encodes various inputs as condition units within transformer but is limited to editing. OmniGen (Xiao et al., 2024b) concatenates conditions before the noise and uses autoregressive transformers, but fails to distinguish between background and reference images, resulting in poor background preservation. UniReal (Chen et al., 2024) makes finer distinctions but requires complex prompts for task differentiation. In contrast, MIGE ensures input-output consistency with conditional input and multimodal instructions, simplifying task distinction and offering better scalability without the need for complex hierarchical prompts. Subject-Driven Image Editing Subject-driven image editing allows image modifications based on user-specified subject, typically through addition or replacement. Methods like DreamEdit (Li et al., 2023b) and DreamMix (Yang et al., 2024) require multiple input images for fine-tuning, associating Figure 2: Demonstration of MIGE as unified framework for processing multimodal instructions and conditional inputs across diverse tasks and scenarios. new concepts with special tokens, followed by iterative inpainting. PBE (Yang et al., 2023) conditions diffusion model on compressed reference image representations, while TIGIC (Li et al., 2025) integrates reference images by positioning them in predefined backgrounds. MADD (He et al., 2024) offers more diverse control by providing masks, boxes, or target coordinates. Overall, these methods rely on multiple reference images or precise cues like bounding boxes and masks, and cannot follow raw textual instructions for accurate and controllable generation, making them inflexible."
        },
        {
            "title": "3.1 Task Unification",
            "content": "Current methods treat subject-driven generation and instruction-based image editing separately, hindering performance due to data limitations and poor generalization. Since both aim to preserve visual consistency while incorporating instructed changes, unifying them allows mutual improvement. Joint training on diverse data enhances subject preservation and instruction following beyond the capabilities of individual task-specific models. Therefore, we propose MIGE to unify the two tasks for joint training. By using multimodal instructions as unified task representation, it supports flexible combinations and provides multimodal guidance. Additionally, we use conditional input to structurally unify the tasks, enhancing visual consistency. Their combination not only provides rich visual and instructional information but also naturally represents different tasks. We demonstrate the input-output format of MIGE in Figure 2. Unified Multimodal Instruction To enable joint training across multiple tasks, unified task repFigure 3: Overall framework of MIGE. MIGE consists of two components: multimodal encoder for processing multimodal instructions and transformer-based diffusion model for modeling input-output relationships. The encoder incorporates feature fusion mechanism to integrate visual and semantic features from reference image. resentation is essential. We introduce multimodal instructions composed of interleaved images and text, which provide both visual references and textual guidance for various controllable generation tasks. As shown in Figure 2, <imagehere> serves as placeholder, sequentially replaced by input images, which can be the reference subject or the entire scene, complementing the semantics to form interleaved expressions. This unified approach effectively accommodates subject-driven generation and instruction-based editing, while remaining extensible to the more complex combination task. Unified Conditional Input We adopt conditional input design to unify tasks structurally, ensuring clear task distinction while enabling shared capabilities. By concatenating different conditions, we differentiate initial generation states and capture task-specific nuances, enhancing execution accuracy. As shown in Figure 2, for instruction-based editing, we concatenate the VAE-encoded source image with the noise tensor, guiding generation based on the given image. For subject-driven generation, we use an all-zero tensor akin to blank canvas, prompting the model to generate freely while preserving specified visual features. This design effectively captures the differences between the two tasks while ensuring input-output consistency in editing. Moreover, structuring tasks within unified framework allows the model to leverage shared capabilities and easily extend to new tasks."
        },
        {
            "title": "3.2 Architecture",
            "content": "The architecture of MIGE, as shown in Figure 3, consists of two main components: multimodal encoder for processing multimodal instructions and transformer-based diffusion model (Peebles and Xie, 2023) for modeling input-output relationships. The diffusion model takes concatenated latent noise and conditional input along the channel dimension as input, performing controllable generation under the control of multimodal conditions. To further enhance the integration of visual and semantic information from the reference image, we introduce novel feature fusion mechanism in the encoder. Multimodal Encoder To map the multimodal instructions into unified vision-language semantic space, we design multimodal encoder comprising Large Language Model (LLM) and an image feature encoding component, which includes pretrained VAE encoder (Kingma, 2013) for visual feature extraction, pretrained ViT from EVA-CLIP (Fang et al., 2023) for semantic feature extraction, Q-Former (Li et al., 2023a), and linear projection layer. Each image is represented by 32 tokens, which are then encoded by the LLM alongside text tokens to form unified multimodal condition. (a) Subject Addition Dataset Construction Pipeline (b) Subject Replacement Dataset Construction Pipeline Figure 4: Data construction pipelines for instruction-based subject-driven image editing. Unlike prior works (Li et al., 2024a; Pan et al., 2023; Li et al., 2024b) that mainly extract semantic features of the reference images, which lack the fine-grained details necessary for preserving subject-specific features. To address this, we propose feature fusion mechanism that combines the strengths of different visual encoders. ViT serves as semantic feature extractor, while VAE encoder acts as visual feature extractor, leveraging its ability to compress and reconstruct images. As shown in Figure 3, we use the ViT semantic features compressed by the Q-Former, denoted as fs, as guidance to adaptively incorporate the visual features fv extracted by the VAE. The fusion mechanism is expressed as: fimg = fs + MLP(Attn(fs, fv)) where Attn(fs, fv) = Q(fs)K(fv) (fv). Here denotes the output dimension of the features, is linear layer, and both and are two-layer networks, where K() = () = Linear(GELU(Linear())). Through this fusion mechanism, we can effectively capture both visual and semantic information of reference images simultaneously without introducing additional image tokens. 3."
        },
        {
            "title": "Joint Training",
            "content": "Multimodal instructions and conditional input unify task representation and input-output formats, enabling joint training. We fine-tune MIGE on data from all tasks to enhance cross-task synergy. Except for the two image encoders, all parameters are jointly trained to align the conditional space of the diffusion model with the multimodal encoder, as shown in Figure 3. This approach improves task coordination and consistency across modalities. Data Construction Joint training enables multitask learning, balancing subject preservation and instruction, followed by modeling task relationships. We create multi-task dataset for joint multimodal instruction tuning, covering subject-driven image generation, instruction-based image editing, and instruction-based subject-driven image generation. For subject-driven image generation, we follow the data construction methods of KOSMOS-G (Pan et al., 2023) and UNIMO-G (Li et al., 2024b), using an LLM to extract entities from captions, which are then fed into Grounded SAM (Ren et al., 2024) for segmentation. Subjects200k dataset from OmniControl (Tan et al., 2024) is also incorporated for better object preservation. For instruction-based editing, we filter existing datasets and use rulebased strategy to construct multimodal instructions. Instruction-based subject-driven image generation is an emerging task that involves two subtasks: instruction-based subject addition and subject replacement. This allows users to add or replace specified subject in an image using multimodal instructions, as shown in Figure 1. However, there is no sufficient dataset available for this task. For instruction-based subject addition, we propose pipeline inspired by SAM-FB (He et al., 2024), as shown in Figure 4a. Starting with the SA-1B (Kirillov et al., 2023) dataset, we construct the source image and multimodal instruction for input-output pairs. We use SAM (Kirillov et al., 2023) to segment annotated entities, filter and retain the main subject using MLLM, inpaint the remaining parts to create the background, and then combine the subject name with the target image to generate the multimodal instruction via GPT-4o (Hurst et al., 2024). Due to resource limitations, Figure 5: Qualitative comparison for subject-driven image generation (top rows) and instruction-based image editing (bottom rows). We compare the universal model and task-specific models on the two tasks, respectively. The prompts listed in the figure are used for MIGE and vary according to the usage of each model. we only process part of the SA-1B dataset and obtain about 200k samples, but the pipeline can be scaled to generate more. For instruction-based subject replacement, we filter from existing editing data, using Grounded SAM to obtain subject segmentation and construct the multimodal instruction to form input-output pairs, as shown in Figure 4b. We also introduce virtual try-on data constructed using IDM-VTON (Choi et al., 2024), resulting in approximately 110k data samples. More details about training data construction are in Appendix B."
        },
        {
            "title": "4 Experiments",
            "content": "4."
        },
        {
            "title": "Implementation Details",
            "content": "MIGE includes conditional diffusion model and multimodal encoder. Our design allows flexible selection of various diffusion models, and we initialize with PIXART-α (Chen et al., 2023) pretrained at 512512 resolution. Parameters introduced for handling conditional inputs are initialized to zero, while the original weights of the model remain unchanged. The multimodal encoder consists of pretrained Flan-T5-XXL (Chung et al., 2024) as LLM for initialization and an image encoding component. This includes query tokens, Q-Former, and projector, all initialized with BLIP-2 (Li et al., 2023a) checkpoint (pretrain_flant5xxl). The frozen VAE encoder, used as the visual feature extractor, is the same as the one in the diffusion model. Additionally, zero-initialized MLP layer is introduced in the feature fusing mechanism for progressive visual feature integration. MIGE is trained on our multi-task dataset using the AdamW optimizer (Loshchilov, 2017) with weight decay of 0.03 and learning rate of 1e-5 for 18 epochs on 48 H20 GPUs, totaling six days of training with batch size of 960 (20 per GPU). 1:1 sampling strategy is applied for subject addition and replacement tasks, and during training, there is 5% chance of dropping either the conditional input or multimodal condition, with an additional 5% chance of dropping both, enabling classifierfree guidance during inference."
        },
        {
            "title": "4.2 Evaluation Results",
            "content": "As unified model, MIGE excels in various image generation and editing tasks, outperforming existing task-specific models. In this section, we highlight its strong performance in subject-driven image generation and instruction-based editing, showcasing its emerging capability in instruction-based subject-driven image generation on our new benchmark. For detailed evaluation results, refer to Appendix C. Additional results on these tasks can be found in Figure 12."
        },
        {
            "title": "Methods",
            "content": "DINO CLIP-I CLIP-T Task-specific Models KOSMOS-G (Pan et al., 2023) UNIMO-G (Li et al., 2024b) 0.694 0.668 0.847 0."
        },
        {
            "title": "Universal Models",
            "content": "Omnigen (our test) (Xiao et al., 2024b) Unireal (Chen et al., 2024) MIGE (ours) /only_subject data /wo_VAE feature 0.711 0.702 0.744 0.726 0.741 0.800 0.806 0.830 0.823 0.828 0.287 0.329 0.312 0.326 0.293 0.289 0. Table 1: Quantitative results for subject-driven image generation on DreamBench. MIGE outperforms universal models in subject preservation and remains competitive with task-specific models."
        },
        {
            "title": "4.2.1 Subject-driven Image Generation",
            "content": "Generating images that satisfy both image and text constraints from multimodal prompt is challenging task. We compare MIGE with two taskspecific methods that also use MLLM to encode multimodal conditions, as well as two universal models, as shown in Table 1. DINO and CLIP-I were used to assess subject fidelity, while CLIPT evaluated adherence to multimodal instructions. Our results on DreamBench demonstrate that MIGE achieves competitive text fidelity while better preserving subject features, particularly outperforming the DINO metric. The qualitative comparison in Figure 5 further demonstrates that MIGE not only performs better in single-subject generation tasks but also preserves the distinct characteristics of each subject in multi-subject generation, whereas other models either fail to retain all subjects or lose their individual features. This advantage arises from the ability of MIGE to flexibly combine multiple reference entities in multimodal instructions and integrate additional visual features through its feature fusion mechanism. 4.2.2 Instruction-based Image Editing Instruction-based image editing enables users to modify source image based on free-form multimodal instructions, including adding, removing, modifying object properties, or altering the overall style. Table 2 presents quantitative analysis of the Emu Edit and MagicBrush test sets. DINO and CLIP-I evaluate similarity to the source image, while CLIP-T measures alignment with the target description. CLIPdir quantifies the alignment between CLIP text and image embedding changes, whereas L1 and L2 capture pixel-level differences. As shown in Table 2, MIGE achieves the highest CLIP-T score and surpasses all task-specific models in CLIPdir, demonstrating its superior ability to effectively follow multimodal instructions. As universal model, MIGE outperforms all other universal models across all metrics on the MagicBrush test set, achieving the lowest L1 and L2 scores and the highest CLIP-I, DINO, and CLIP-T scores, highlighting its strong instruction fidelity and fine-detail preservation. This capability is further illustrated in Figure 5, where MIGE is the only model that accurately follows the instruction to add Daffy Duck image to the red suitcase without altering other unrelated areas. 4.2. Instruction-based Subject-driven Image Editing Benchmark Construction Instruction-based subject-driven image editing is novel task. Existing methods rely on masks or positional coordinates for editing (Li et al., 2023b; Yang et al., 2024) but lack support for instruction-based editing. Current benchmarks for subject addition and replacement separately evaluate foreground and background similarities without providing complete edited image as ground truth, making them unsuitable for this task. To address these issues, we construct 500 for benchmark of 1,000 test samples: instruction-based subject addition and 500 for subject replacement. Data is sourced from SEED-Data-Edit (Ge et al., 2024), with subjects extracted using Grounded SAM (Ren et al., 2024). Captions for target images are generated by GPT-4o and refined through manual review. For compatibility with prior methods, our benchmark also includes masks. Details of the construction process are provided in Appendix A. Evaluation Results Our evaluation focuses on editing ability and subject preservation. Editing ability is assessed using DINO, CLIP-I, and CLIPT: DINO and CLIP-I measure similarity to the ground truth image, while CLIP-T evaluates alignment with the target caption. Subject preservation is evaluated by extracting the edited subject via Grounded SAM and comparing it to the input subject image using DINO and CLIP-I. This separates the evaluation of image-level editing from subjectlevel feature preservation. Methods that do not support instruction-based editing are marked with cross in the table, and masks used during testing. Quantitative comparisons with other methods are Methods DINO CLIP-I CLIP-T CLIPdir L1 L2 DINO CLIP-I CLIP-T CLIPdir L1 L2 Emu Edit Test set MagicBrush Test set InstructPix2Pix (Brooks et al., 2023) MagicBrush (Zhang et al., 2024) UltraEdit (Zhao et al., 2024) 0.759 0.800 0.862 PixWizard (Lin et al., 2024) ACE (Han et al., 2024) OmniGen (Xiao et al., 2024b) MIGE (ours) /only_edit data /wo_VAE feature /wo_multimodal instruction 0.824 0.847 0.766 0.832 0.785 0.799 0.592 0.831 0.857 0.875 0.862 0.877 0.819 0.865 0.841 0.846 0. 0.288 0.295 0.301 0.288 0.299 0.299 0.306 0.302 0.300 0.280 0.086 0.102 0.100 0.066 0.104 0.123 0.114 0.104 0.098 0.113 Task-specific Models 0.036 0.027 0. 0.763 0.847 0.879 Universal Models 0.038 0.028 0.065 0.027 0.046 0.035 0.074 0.865 0.856 0.821 0.889 0.796 0.811 0.548 0.122 0.085 0.050 0.105 0.079 0.160 0.088 0.117 0.103 0. 0.843 0.888 0.897 0.897 0.899 0.863 0.905 0.862 0.868 0.714 0.289 0.304 0.305 0.289 0.304 0.289 0.306 0.300 0.299 0.277 0.105 0.127 0.119 0.054 0.127 0.087 0.119 0.111 0.105 0. 0.097 0.062 0.042 0.073 0.065 0.116 0.055 0.094 0.098 0.170 0.028 0.018 0.006 0.023 0.023 0.045 0.013 0.036 0.036 0.066 Table 2: Quantitative results for instruction-based image editing on Emu Edit test set and MagicBrush test set. MIGE achieves the best overall performance on MagicBrush test set among universal models, with strong instruction fidelity and detail preservation. Methods Subject Preserving Instruction DINO CLIP-I CLIP-T DINO CLIP-I Editing Methods Subject Preserving Instruction DINO CLIP-I CLIP-T DINO CLIP-I Editing source-target 0.668 0.842 0.271 source-target 0.783 0. 0.295 (cid:37) PBE (Yang et al., 2023) (cid:37) TIGIC (Li et al., 2025) (cid:37) MADD (He et al., 2024) OmniGen (Xiao et al., 2024b) (cid:34) (cid:34) MIGE(ours) /subject data + edit data /only_compositional data 0.810 0.789 0.736 0.802 0.863 0.789 0.780 0.885 0.874 0.852 0.868 0.909 0.873 0.860 0.304 0.313 0.284 0.296 0.307 0.299 0. 0.521 0.453 0.446 0.580 0.652 0.503 0.585 0.792 0.744 0.742 0.792 0.834 0.764 0.808 (cid:37) PBE (Yang et al., 2023) (cid:37) TIGIC (Li et al., 2025) (cid:37) MADD (He et al., 2024) OmniGen (Xiao et al., 2024b) (cid:34) (cid:34) MIGE(ours) /subject data + edit data /only_compositional data 0.843 0.840 0.885 0.791 0.909 0.807 0.879 0.908 0.901 0.930 0.870 0.940 0.895 0. 0.321 0.325 0.316 0.312 0.322 0.304 0.324 0.495 0.455 0.519 0.605 0.638 0.309 0.577 0.794 0.753 0.785 0.814 0.838 0.683 0.820 (a) Results on instruction-based subject replacement. (b) Results on instruction-based subject addition. Table 3: Quantitative results on instruction-based subject-driven editing. Methods marked with cross in the instruction column use masks, while the others generate images using multimodal instructions. Overall, MIGE significantly outperforms others in both tasks, showcasing superior editing and subject preservation abilities. shown in Tables 3a and 3b. We computed DINO and CLIP-I metrics between the source and target images, as well as the CLIP-T metric between the source image and target caption, shown in the first row of the table (denoted as source-target) as baseline. In terms of editing ability, MIGE achieves the highest overall improvement across all metrics, demonstrating its effectiveness in performing edits guided by multimodal instructions. As shown in the qualitative comparison in Figure 6, MIGE understands the meaning of replace in instructions, rather than simply pasting the entity onto the image. For subject preservation, the results show that MIGE achieves the best performance in both tasks, as demonstrated in Figure 6."
        },
        {
            "title": "4.3.1 Effectiveness of Joint Training",
            "content": "To assess the effectiveness of joint training, we train models separately on individual datasets (denoted as only_subject data, only_edit data, and only_compositional data) and compare their performance with the jointly trained model. The results in Table 1 and Table 2 show that joint training leads to consistent improvements across all metrics, demonstrating that subject-driven generation and instruction-based editing reinforce each other. As shown in Table 3, joint training also enhances the performance of the compositional new task, further highlighting its overall benefits. These findings emphasize both the effectiveness and necessity of joint training. In conclusion, joint training of subjectdriven generation and instruction-based editing within our unified framework not only boosts compositional capability but also improves the performance of each individual task."
        },
        {
            "title": "4.3.2 Effectiveness of Feature Fusing",
            "content": "MIGE employs feature fusion mechanism in the multimodal encoder to integrate semantic features from ViT and visual features from VAE. As shown in Table 1 and Table 2, compared to the model without VAE features (denoted as wo_VAE feature), incorporating VAE features significantly improves detail preservation in reference images, benefiting both subject-driven image generation and instruction-based image editing. This is particularly evident in the improved CLIP-I and DINO scores and the significant reduction in L1 and L2 metrics, demonstrating that the inclusion of addiFigure 6: Qualitative results on the benchmark for the subject addition and subject replacement. The upper section compares subject addition results, while the lower section compares subject replacement. During testing, the <imagehere> placeholder in the multimodal instruction is replaced according to the image sequence. MIGE demonstrates flexibility in editing and excels in subject preservation ability and input-output consistency. tional visual features helps maintain consistency between the input and output."
        },
        {
            "title": "4.3.3 Effectiveness of Instruction-based",
            "content": "Subject-driven Image Editing Dataset Joint training in subject-driven image generation and instruction-based image editing enables generalization to instruction-based subject-driven image editing (denoted as subject data + edit data). To enhance the capability of MIGE in this new task, particularly in understanding spatial terms and size descriptions, we constructed task-specific dataset for joint training. As shown in Table 3a and Table 3b, the task-specific data significantly improved the models overall abilities. This demonstrates the effectiveness of our constructed dataset, and the proposed data generation pipeline serves as valuable reference for future dataset construction."
        },
        {
            "title": "4.3.4 Effectiveness of Multimodal Instruction\nExisting instruction-based editing works (Brooks\net al., 2023; Sheynin et al., 2024; Zhang et al.,\n2024) typically use text instructions as the con-\nditional input, while we extend this to multimodal\ninstructions. To measure the benefit of multimodal\ninstructions, we trained the model with text-only\nediting instructions for comparison. As shown in\nTable 2, using multimodal instructions consistently\nimproves performance over text-only instructions",
            "content": "(denoted as wo_multimodal instruction). This enhances input-output consistency and instructionfollowing ability in multi-task training. The significant improvement in the L1 and L2 metrics indicates finer control over images and more accurate edits. While text-only instructions provide the necessary changes, the high CLIPdir score and lower values in other metrics show that multimodal instructions add visual context, enabling more precise and faithful modifications."
        },
        {
            "title": "5 Conclusion",
            "content": "We present MIGE, unified framework that combines subject-driven generation and instructionbased editing. By leveraging multimodal instructions and conditional input, MIGE enables joint training that reinforces both tasks. This approach improves task synergy and addresses data scarcity. Joint training also unlocks new capabilities like instruction-based subject-driven image editing. We introduce pipelines for this new task to construct training data and MIGEBench for evaluation. Our experiments show that joint training leads to significant improvements in subject fidelity and instruction adherence, demonstrating the effectiveness of unifying these tasks. This integration enhances controllability and offers promising directions for future multimodal image generation and editing."
        },
        {
            "title": "6 Limitations",
            "content": "Our model faces common challenges in context accuracy, complex composition, and visual faithfulness, especially in multi-entity tasks. Ethical concerns, like deepfakes, also arise. Nevertheless, our framework shows promise in enabling unified controlled image generation. Instruction-based subjectdriven generation remains challenging, with difficulties in handling spatial relationships and adjusting subject sizes to fit the background. Additionally, the technology carries the risk of generating misleading images, but our work is intended solely for research, emphasizing its exploratory nature rather than real-world deployment."
        },
        {
            "title": "References",
            "content": "Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou. 2019. Gradio: Hassle-free sharing and testing of ml models in the wild. arXiv preprint arXiv:1906.02569. Tim Brooks, Aleksander Holynski, and Alexei Efros. 2023. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402. Alper Canberk, Maksym Bondarenko, Ege Ozguroglu, Ruoshi Liu, and Carl Vondrick. 2025. Erasedraw: Learning to insert objects by erasing them from images. In European Conference on Computer Vision, pages 144160. Springer. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. 2023. Pixartα: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426. Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. 2024. Unireal: Universal image generation and editing via learning real-world dynamics. arXiv preprint arXiv:2412.07774. Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, and Jinwoo Shin. 2024. Improving diffusion models for authentic virtual try-on in the wild. arXiv preprint arXiv:2403.05139. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. 2024. Scaling rectified flow transformers for highIn Forty-first Internaresolution image synthesis. tional Conference on Machine Learning. Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. 2023. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19358 19369. Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. 2024. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007. Zhen Han, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang, Chaojie Mao, Chenwei Xie, Yu Liu, and Jingren Zhou. 2024. Ace: All-round creator and editor following instructions via diffusion transformer. arXiv preprint arXiv:2410.00086. Jixuan He, Wanhua Li, Ye Liu, Junsik Kim, Donglai Wei, and Hanspeter Pfister. 2024. Affordance-aware object insertion via mask-aware dual diffusion. arXiv preprint arXiv:2412.14462. Hexiang Hu, Kelvin CK Chan, Yu-Chuan Su, Wenhu Chen, Yandong Li, Kihyuk Sohn, Yang Zhao, Xue Ben, Boqing Gong, William Cohen, et al. 2024. Instruct-imagen: Image generation with multi-modal instruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47544763. Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. 2024. Hq-edit: high-quality dataset for arXiv preprint instruction-based image editing. arXiv:2404.09990. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Diederik Kingma. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. 2023. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026. Black Forest Labs. 2023. Flux. https://github.com/ black-forest-labs/flux. Dongxu Li, Junnan Li, and Steven Hoi. 2024a. Blipdiffusion: Pre-trained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023a. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR. Pengzhi Li, Qiang Nie, Ying Chen, Xi Jiang, Kai Wu, Yuhuan Lin, Yong Liu, Jinlong Peng, Chengjie Wang, and Feng Zheng. 2025. Tuning-free image cusIn Eutomization with image and text guidance. ropean Conference on Computer Vision, pages 233 250. Springer. Tianle Li, Max Ku, Cong Wei, and Wenhu Chen. 2023b. Dreamedit: Subject-driven image editing. arXiv preprint arXiv:2306.12624. Wei Li, Xue Xu, Jiachen Liu, and Xinyan Xiao. 2024b. UNIMO-G: Unified image generation through multimodal conditional diffusion. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 61736188, Bangkok, Thailand. Association for Computational Linguistics. Weifeng Lin, Xinyu Wei, Renrui Zhang, Le Zhuo, Shitian Zhao, Siyuan Huang, Junlin Xie, Yu Qiao, Peng Gao, and Hongsheng Li. 2024. Pixwizard: Versatile image-to-image visual assistant with open-language instructions. arXiv preprint arXiv:2409.15278. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. 2025. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer. Loshchilov. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101. Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. 2023. Kosmos-g: Generating images in context with multimodal large language models. arXiv preprint arXiv:2310.02992. William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. 2024. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. Highresolution image synthesis with latent diffusion modIn Proceedings of the IEEE/CVF conference els. on computer vision and pattern recognition, pages 1068410695. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2023. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510. Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. 2024. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88718879. Jing Shi, Ning Xu, Trung Bui, Franck Dernoncourt, Zheng Wen, and Chenliang Xu. 2020. benchmark and baseline for language-driven image editing. In Proceedings of the Asian Conference on Computer Vision. Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. 2022. Resolution-robust large mask inpainting with fourier In Proceedings of the IEEE/CVF convolutions. winter conference on applications of computer vision, pages 21492159. Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. 2024. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 3. MosaicML NLP Team et al. 2023. Introducing mpt7b: new standard for open-source, commercially usable llms. Qwen Team. 2024. Qwen2.5: party of foundation models. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024. Qwen2vl: Enhancing vision-language models perception arXiv preprint of the world at any resolution. arXiv:2409.12191. Guangxuan Xiao, Tianwei Yin, William Freeman, Frédo Durand, and Song Han. 2024a. Fastcomposer: Tuning-free multi-subject image generation with localized attention. International Journal of Computer Vision, pages 120. Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. 2024b. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340. Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael Ryoo, et al. 2024. xgen-mm (blip-3): family of open large multimodal models. arXiv preprint arXiv:2408.08872. Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. 2023. Paint by example: Exemplar-based image editIn Proceedings of the ing with diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1838118391. Yicheng Yang, Pengxiang Li, Lu Zhang, Liqian Ma, Ping Hu, Siyu Du, Yunzhi Zhuge, Xu Jia, and Huchuan Lu. 2024. Dreammix: Decoupling object attributes for enhanced editability in customized image inpainting. arXiv preprint arXiv:2411.17223. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. 2023. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721. Ahmet Burak Yildirim, Vedat Baday, Erkut Erdem, Aykut Erdem, and Aysegul Dundar. 2023. Instinpaint: Instructing to remove objects with diffusion models. arXiv preprint arXiv:2304.03246. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. 2024. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36. Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. 2024. Ultraedit: Instructionbased fine-grained image editing at scale. arXiv preprint arXiv:2407.05282."
        },
        {
            "title": "A Benchmark Construction",
            "content": "Instruction-based subject-driven image editing is novel task lacking evaluation benchmarks. Previous works (Li et al., 2023b; Yang et al., 2024) did not support instructions and lacked ground truth target images. To address this, we designed benchmark construction pipeline with multiple manual inspections to ensure image and caption quality. As shown in Figure 7, we first filter valid editing pairs using Gradio (Abid et al., 2019) interface, as shown in Figure 8. Then, Qwen2.514B-Instruct extracts entity names and constructs multimodal instructions. After manually reviewing object names, we use Grounding DINO (Liu et al., 2025) for bounding boxes and SAM for segmentation. Cropped entities are saved with black and white backgrounds for diverse test scenarios. Prompts are detailed in Appendix E. For evaluation, Qwen2-VL-7B generates captions that compare the generated images with their ground truth counterparts. These captions are then manually reviewed to identify and remove any unrecognized entities. This review process ensures that the generated results are accurate and reliable. The final benchmark consists of 500 samples for both subject addition and replacement tasks. By carefully avoiding artifacts from inpainting and incorporating multiple rounds of manual review, we guarantee the high quality of the data. Figures 9 and 10 display some example pairs from the benchmark to illustrate the evaluation process. Figure 7: The pipeline of benchmark construction."
        },
        {
            "title": "B Dataset Construction",
            "content": "To enable joint training of MIGE, we designed series of pipelines for data construction and processing. Our dataset includes three tasks: subjectdriven image generation, instruction-based image editing, and instruction-based subject-driven image editing. The proportions of the training data are shown in Figure 11. In this section, we will detail the pipelines for processing each data component and other related details. Figure 8: The annotation interface for benchmarks is built with Gradio. Figure 9: Subject addition examples in our benchmark. Figure 10: Subject replacement examples in our benchmark. Figure 11: Composition of training data. Subject-driven Image Generation Data Construction We construct multimodal instructionimage pairs by replacing entities in image captions with corresponding images. The dataset is sourced from BLIP3-GROUNDING-50M (Xue et al., 2024) and internal data. For the former, we discard low-quality pairs with CLIP-T scores below 0.255. Entities are filtered based on annotated bounding boxes, retaining those with sizes between 0.05 and 0.8. After segmenting entities using SAM (Kirillov et al., 2023), we further compute the CLIP-T score between photo of [entity] and the segmented image to mitigate incomplete pairs. For internal data, we follow the KOSMOS-G and UNIMO-G processing pipelines, using MPT-7B-Instruct (Team et al., 2023) to extract entities. The original image and entity text are input into Grounded SAM to obtain entity images, which are filtered by CLIP-T score above 0.255 and size ratio between 0.1 and 0.85. To enhance subject preservation, we incorporate 112,846 samples from subject200k, as in OminiControl (Tan et al., 2024). Instruction-based Image Editing Data Construction Previous works on instruction-based editing relied solely on text, covering object-level additions, deletions, modifications, and global changes in background or style. We integrated data from InstructPix2Pix (Brooks et al., 2023), UltraEdit (Zhao et al., 2024), MagicBrush (Zhang et al., 2024), SEED-Data-Edit (Ge et al., 2024), GIER (Shi et al., 2020), and HQ-Edit (Hui et al., 2024), filtering out instances with target image aesthetic scores above 5.5. To enhance multimodal instruction alignment, we reformulated text-based instructions based on task type. Specifically, for instructions containing phrases like in the image or to the image, we replaced them with in<imagehere> or to<imagehere>. For others, we appended of<imagehere> for global edits or in<imagehere> for localized modifications. Instruction-based Subject-driven Image Editing Data Construction To enhance instructionbased subject-driven image editing, we propose dedicated data construction pipelines. For subject addition, we design pipeline inspired by SAMFB (He et al., 2024) to construct background images, foreground entities, and multimodal instructions, as shown in Figure 4a. We start with filtering SA-1B (Kirillov et al., 2023) images with an aesthetic score above 5 and select entities where the foreground-to-image ratio is between 0.1 and 0.5. Qwen2-VL-7B (Wang et al., 2024) is used to verify entity completeness. The selected entities serve as foregrounds, while the backgrounds are inpainted using LAMA (Suvorov et al., 2022). We then extract foreground-related text using Qwen2-VL-7B and generate multimodal instructions with GPT-4o (Hurst et al., 2024). Due to resource constraints, we process the first 500,000 SA-1B samples, yielding 193,247 multimodal instruction-foregroundbackground pairs. For subject replacement, as shown in Figure 4b, we aim at constructing foreground entities and multimodal instructions. We filter SEED-Data-Edits part1-unsplash based on predefined rules, retaining only replace-task data where the match score in the annotations exceeds 0.3 and the target images aesthetic score is above 5.5. Qwen2.5-14B-Instruct (Team, 2024) identifies whether the main entity is being replaced. The target entity is replaced with <imagehere> to construct multimodal prompt. Using the annotated edited subject information, we apply Grounded SAM for segmentation, obtaining 79,693 multimodal instruction-replace entity image-source image-target image pairs. In addition, we construct 34,947 virtual try-on samples using IDM-VTON (Choi et al., 2024)."
        },
        {
            "title": "C Evaluation Details",
            "content": "We evaluated the subject-driven generation capability of MIGE on DreamBench, which contains 750 prompts covering 30 subjects. For each prompt, four images were generated using seeds 0, 1, 2, and 3, resulting in total of 3,000 images. Following KOSMOS-G, we selected one image per prompt. Then we extracted the subject using Grounded SAM and used it as input, aligning with the training process. Subject fidelity was assessed using DINO and CLIP-I, while CLIP-T measured adherence to multimodal instructions. For instruction-based editing evaluation, the Emu Edit benchmark contains known issues, including incorrect image-caption pairs and duplicate source-target captions. Prior works have handled these inconsistencies differently, leading to incomparable results. Therefore, we reimplemented all currently available open-source methods on both test sets using fixed random seed (seed=0) for consistency. Consequently, CLIP-I, L1, and DINO were computed on 3,589 Emu Edit test samples,"
        },
        {
            "title": "E Prompts",
            "content": "LLMs and MLLMs are integral to our training data and benchmark construction pipeline, and the prompts used are presented in this section. while CLIPdir and CLIP-T were evaluated on 2,899 samples after removing problematic data."
        },
        {
            "title": "D Related Work",
            "content": "Subject-Driven Image Generation Recent advances in generative models have significantly improved image customization, enabling finer and more flexible control over generated content. Subject-driven image generation, which preserves the features of given images while creating new content, has garnered increasing attention. DreamBooth (Ruiz et al., 2023) fine-tunes the entire U-Net using multiple images of the same subject, making it resource-intensive. IP-Adapter (Ye et al., 2023) introduces test-time tuning-free approach by decoupling image and text, training only additional attention layers, but struggles to balance prompt adherence and image fidelity. FastComposer (Xiao et al., 2024a) and Blip-Diffusion (Li et al., 2024a) leverage embeddings to condition images on text but face challenges in subject preservation and efficiency with generalized vision-language inputs. KOSMOS-G (Pan et al., 2023) and UNIMO-G (Li et al., 2024b) use Multimodal Large Language Models (MLLMs) as multimodal encoders for greater input flexibility but require large datasets to train task-specific encoders and alignment networks. However, these methods lack strong instruction-following capabilities, limiting their scalability to new tasks and applications. Instruction-based Image Editing Instructionbased image editing modifies images at the object level (e.g., adding, removing, or altering objects) or the image level (e.g., changing style or environment) based on user instructions. This approach offers greater flexibility than caption-based methods but demands stronger instruction-following capabilities. Inst-inpaint (Yildirim et al., 2023) focuses on object removal, while EraseDraw (Canberk et al., 2025) fine-tunes model for object addition using data derived from removal tasks. InstructPix2Pix (Brooks et al., 2023), trained on large synthetic dataset, supports four types of edits, and MagicBrush (Zhang et al., 2024) refines this capability by fine-tuning on real, manually annotated data. EmuEdit (Sheynin et al., 2024) enhances diffusion-based editing with task vector to differentiate tasks. However, these methods often struggle with maintaining input-output consistency. Figure 12: Qualitative results of subject-driven image generation (top) , instruction-based image editing (middle), and instruction-based subject-driven image editing (bottom). Prompt for Qwen2-VL-7B to determine the completeness of subject in the image Prompt: Determine if the subject in the image is complete. If it is complete and not an abstract object such as background, grass, sky, tree, stone, or part of another item, please return True. Otherwise, return False. Table 4: Prompt design for subject completeness evaluation in an image. The model should identify whether the subject is complete, not abstract elements. Prompt for GPT-4o to generate multimodal instruction Prompt: The object is {object_name}. It is located in bounding box with coordinates ({x}, {y}, {w}, {h}) on an image of size {width}x{height}. Describe its size, relative position, and relation to surrounding objects. Avoid describing the overall scene or unrelated elements. Your response should start with Add <imagehere> to the [position] of <imagehere>. (The first <imagehere> indicates the object and the second indicates the image.) Keep the <imagehere> symbol in the first sentence in your reply. Answer briefly in two sentences: Table 5: Prompt design for GPT-4o to generate multimodal instruction for subject addition training data. Prompt for Qwen2.5-14B-Instruct to determine whether an edit instruction pertains to the main subject of an image Prompt: You are an assistant that determines whether an edit instruction pertains to the main subject of an image. The main subject refers to humans or animals only. If the edit instruction is related to the main subject, respond with yes. If it pertains to background, large areas of vegetation, or environmental information, respond with no. Here are some examples: 1. replace the grass with sand no 2. replace the trees with palm trees no 3. replace the dirt road with cobblestone path no 4. replace the bird with parrot yes Now, analyze the following instruction and respond accordingly: {instruction} Table 6: Prompt design for determining if an edit instruction pertains to the main subject of the image. The model evaluates whether the instruction relates to humans, animals, or other environmental features."
        },
        {
            "title": "Prompt for extracting the main subject and modifying the edit instruction",
            "content": "Prompt 1: Replace Examples You are an assistant that determines the main subject of an edit instruction and outputs the extracted main subject along with modified prompt. Given an edit instruction like replace with Y: - Extract (the replacement subject). - Output the extracted subject. - Modify the instruction by replacing with <imagehere> in the original prompt and output the new instruction. Examples: 1. replace one woman with man Output: man, replace one woman with <imagehere> 2. replace the castle with another castle Output: castle, replace the castle with <imagehere> 3. replace the desk with white one in the bottom middle Output: desk, replace the desk with <imagehere> in the bottom middle Prompt 2: Add Examples You are an assistant that extracts the added subject in the edit instruction. Given an edit instruction like add to the image: - Extract (the added subject). - Output the extracted subject. - Modify the instruction by replacing with <imagehere> in the original prompt and output the new instruction. Here are some examples: 1. add human over the stone in the bottom left Output: human, add <imagehere> over the stone in the bottom left 2. add car on the road at the bottom Output: car, add <imagehere> on the road at the bottom 3. Add an owl on the left shoulder Output: an owl, Add <imagehere> on the left shoulder Table 7: Prompt designs for extracting and modifying edit instructions based on either replacement or addition of subjects."
        }
    ],
    "affiliations": [
        "Huawei Shen CAS Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China",
        "University of Chinese Academy of Sciences, Beijing, China"
    ]
}