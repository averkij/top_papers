{
    "paper_title": "MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers",
    "authors": [
        "Ajay Jaiswal",
        "Lauren Hannah",
        "Han-Byul Kim",
        "Duc Hoang",
        "Arnav Kundu",
        "Mehrdad Farajtabar",
        "Minsik Cho"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding how transformer components operate in LLMs is important, as it is at the core of recent technological advances in artificial intelligence. In this work, we revisit the challenges associated with interpretability of feed-forward modules (FFNs) and propose MemoryLLM, which aims to decouple FFNs from self-attention and enables us to study the decoupled FFNs as context-free token-wise neural retrieval memory. In detail, we investigate how input tokens access memory locations within FFN parameters and the importance of FFN memory across different downstream tasks. MemoryLLM achieves context-free FFNs by training them in isolation from self-attention directly using the token embeddings. This approach allows FFNs to be pre-computed as token-wise lookups (ToLs), enabling on-demand transfer between VRAM and storage, additionally enhancing inference efficiency. We also introduce Flex-MemoryLLM, positioning it between a conventional transformer design and MemoryLLM. This architecture bridges the performance gap caused by training FFNs with context-free token-wise embeddings."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 3 ] . [ 1 8 9 3 0 0 . 2 0 6 2 : r MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers Ajay Jaiswal, Lauren Hannah, Han-Byul Kim, Duc Hoang, Arnav Kundu, Mehrdad Farajtabar, Minsik Cho Apple Understanding how transformer components operate in LLMs is important, as it is at the core of recent technological advances in artificial intelligence. In this work, we revisit the challenges associated with interpretability of feed-forward modules (FFNs) and propose MemoryLLM, which aims to decouple FFNs from self-attention and enables us to study the decoupled FFNs as context-free token-wise neural retrieval memory. In detail, we investigate how input tokens access memory locations within FFN parameters and the importance of FFN memory across different downstream tasks. MemoryLLM achieves context-free FFNs by training them in isolation from self-attention directly using the token embeddings. This approach allows FFNs to be pre-computed as token-wise lookups (ToLs), enabling on-demand transfer between VRAM and storage, additionally enhancing inference efficiency. We also introduce Flex-MemoryLLM, positioning it between conventional transformer design and MemoryLLM. This architecture bridges the performance gap caused by training FFNs with context-free token-wise embeddings. Correspondence: Ajay Jaiswal: ajaiswal23@apple.com Date: February 3,"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) are omnipresent, demonstrating rapidly evolving capabilities across critical domains such as healthcare, finance, education, and mobility. Modern LLMs (Grattafiori et al., 2024; Liu et al., 2025, 2024; Jiang et al., 2024; Adler et al., 2024; Gunter et al., 2024) are sequential stacks of transformer blocks in association with embedding layers that project textual input to latent representation for processing, followed by projection back to tokens. Each transformer block is constituted by two key computationally expensive components: self-attention (Attn) and feed-forward (FFN) modules. Self-attention modules within LLMs are considered the key component in transformer blocks, constructing representations of the current input by aggregating relevant information from the context (Sukhbaatar et al., 2019). Numerous works have studied the role and optimization (Vig, 2019; Xiao et al., 2023, 2024; Clark et al., 2019) of self-attention by investigating token-level attention patterns generated during input text processing. In contrast, FFNs, while holding approximately two-thirds of the LLMs parameters, have been relatively underexplored, and their roles in information processing flow deserve in-depth studies. One potential reason for limited investigation can be attributed to the existence of tightly intertwined relationship between FFNs and self-attention in modern LLMs: FFNs consume non-interpretable additive mixture of self-attention output and residual stream (Figure 1a) that makes the attempt to study the contributions of FFNs nontrivial. Some notable prior works (Geva et al., 2021, 2022; Dar et al., 2023) have attempted to establish, within the limited scale of GPT-2, that FFNs in pretrained LLMs serve as neural key-value memory over textual input patterns such as n-grams or semantic topics. However, such interpretations suffer from two critical shortcomings: (1) dependency on multiple forward and backward passes with calibration dataset fed through the model followed by careful mining of relevant input phrases with manual annotation; (2) inability to discretely define interpretable queries for key-value memory access within FFNs. Motivated by the interpretability challenges associated with FFNs in LLMs, we pose critical question: How can we disentangle FFNs from self-attention to encode deterministic memory mapped to finite human1 Figure 1 Architecture comparison of Conventional Transformer (base) v/s MemoryLLM with Residual Stream Perspective: (a) FFN input in conventional transformers is sequential and non-interpretable latent snapshot of residual stream, including prior self-attention module output; (b) MemoryLLM decouples FFNs across all transformer blocks completely from self-attention modules and trains them in isolation of the residual stream, directly on token-indexed input embeddings. interpretable vocabulary? Unlike previous attempts which focus on investigating de facto FFNs of pretrained LLMs, we adopt an alternative approach: designing and training from scratch novel transformer architecture. Our main goal is to enforce clear separation between self-attention and FFNs, in order to clarify the FFNs role as context-free retrieval memory over an interpretable, finite set of tokens. In this work, we propose MemoryLLM, which makes dramatic simplification of conventional transformer architecture that entails training self-attention and FFNs modules in independent of and parallel to each other. MemoryLLM features interpretability-focused architecture (Figure 1b) where self-attention heads are trained in conventional fashion using the incoming residual stream, while FFNs are trained in isolation directly on context-free and token-indexed embedding vectors. This isolation allows us to build discrete tokenlevel retrieval memory in FFNs. MemoryLLM enables two main advantages: FFN Interpretability: With fixed and discrete token-wise query space, FFNs interpretation as neural key-value retrieval memories can be well-studied for each token in models vocabulary. LLM Efficiency: With static token embedding-based training directly from embedding layer, FFN modules in MemoryLLM can be pre-computed and offloaded to storage devices. We train-from-scratch MemoryLLM at different parameters scale (250M, 750M, and 1B) to study its capabilities. Our contributions are as follows: We revisit the challenge in investigating the role of FFNs through novel lens of token-indexed neural retrieval memory. We present TKV (token-key-value) framework to investigate how FFNs construct persistent context-free memory over the models vocabulary. Building on top of prior tools (Geva et al., 2021, 2022), we explore the spatial perspective of tokenindexed memory where lexically and semantically similar query tokens tend to access similar memory location within FFNs for retrieval. We find that FFNs in MemoryLLM acts as reservoirs of token-level parametric knowledge learned directly from training data. They play dominant role in retrieval-based tasks in comparison to inferential or logical thinking tasks. 2 MemoryLLM also addresses the memory and computational bottleneck of LLMs at inference by facilitating pre-computation of FFNs as token-wise lookups (ToLs) and plug-n-play (PnP) memory transfer from storage devices under resource constraints. To closely match performance of conventional LLMs, we introduce Flex-MemoryLLM. This architecture is positioned between standard transformer blocks and MemoryLLM, effectively bridging the gap by splitting FFNs parameters among context-aware and context-free FFN modules."
        },
        {
            "title": "2.1 Motivation",
            "content": "Modern LLMs are composed of sequential stacking of self-attention and feed-forward modules. Figure 1(a) illustrates residual information flow perspective (Elhage et al., 2021) in conventional LLM, where at transformer layer L, the self-attention module takes the snapshot XL of residual stream and transforms it with contextual information before adding it back to the stream. Next, the FFN module reads the residual stream, which now carries the contextual information, processes it and adds it back to residual stream, resulting in XL+1 for next layer. In LLMs, self-attention is often regarded as the key driver of success, attracting significant research into its functionality and optimization. In contrast, FFNs, which constitute approximately two-thirds of total parameters, have been underexplored, particularly regarding how they process the residual stream and store information during training. In this two-step mechanism within conventional transformer block, FFNs consume non-interpretable latent input, which is an additive mixture of self-attention output and residual stream. This representation dynamically evolves between layers, and is the primary bottleneck in understanding FFN function during token processing. Earlier works (Geva et al., 2021, 2022) argue that feed-forward layers emulate neural key-value memories. They support this claim by mapping keys within FFNs to manually annotated textual patterns in the training data. However, mapping keys of an intermediate transformer layer within an FFN back to initial input tokens is neither straight-forward nor optimal. The residual stream changes significantly between X0 XL with complex amalgamation of contextual information from previous layers. Consequently, the query space to investigate FFNs emulated key-value memory remains non-interpretable latent input, with contextdependent properties. To address this, we decouple FFNs from the residual stream, enabling deterministic investigation of FFNs as reservoir of key-value memory accessible with finite query space. In the next subsection, we present MemoryLLM. This architecture allows us to study FFNs in isolation of self-attention, where each token id from an input prompt can be mapped to static indices within context-free token-level memory."
        },
        {
            "title": "2.2 Architecture Design",
            "content": "2.2.1 Preliminaries: Conventional LLMs An input text = {t1, t2, ..., tM } of tokens is transformed to embedding vectors d with an embedding matrix RV over vocabulary space . self-attention module at transformer layer is parameterized with WQ, WK, and WV weight matrices. The contextualized token representation is computed by: 0 Attn(XL) = softmax XLW . (2.1) (cid:32) XLW ) (XLW dk (cid:33) Next, SwiGLU (Shazeer, 2020) based FFNs in modern LLMs are parameterized with three matrices with as intermediate expansion dimension: WU p, WDown, and WGate. In conventional transformer layer, FFN output is an additive mixture of self-attention and residual stream ( XL): XL = XL + Attn(XL), FFN( XL) = WDown (cid:16)(cid:16) XLW Up (cid:17) SiLU (cid:16) XLW Gate (cid:17)(cid:17) . (2.2) (2.3) 2.2.2 MemoryLLM In this section, we describe the architecture design of MemoryLLM, which disentangles the sequential dependence of FFN modules from self-attention output and the residual stream (Equation 2.2,2.3) in transformer layer. MoLE (Jie et al., 2025) illustrates that in mixture-of-experts (MoE), the majority of experts can be trained directly with token-level input embeddings. However, MoLEs expert computation remains conditional on contextual information from router trained with self-attention output. Here, we explore the potential of dramatic simplification in conventional transformer layer by parallelizing the context-aware selfattention module with context-free FFN computation, independent of the residual stream and self-attention. More specifically, we propose to train all feed-forward modules {FFNL0, FFNL1, ..., FFNLN 1 } across an layer LLM, directly with token-indexed context-free embedding vectors X0 RM generated out of tokens IDs from tokenizer for given input text. Figure 1(b) presents the architecture design of MemoryLLM where the self-attention module is trained in conventional fashion (Equation 2.1) with incoming residual stream, while the FFN module from layer is computed as follows: ˆX0 = LayerNormL(X0), (cid:16)(cid:16) ˆX0W SiLU (cid:17) Up FFN( ˆX0) = WDown (cid:16) ˆX0W Gate (cid:17)(cid:17) . (2.4) (2.5) Despite all FFNs across each transformer block receiving the same input ( ˆX0) during training, we found that having an independent layer norm (LNL) for each FFNL significantly helps in convergence. Overall, for transformer layer with incoming residual stream XL, the computation for outgoing residual stream XL+1 can be given by: MemoryLLM XL+1 = XL + Attn(XL) + FFN(X0) (2.6) Since the embedding layer output (X0) is exclusively determined by the tokenizers unique token IDs, the inputs to all the FFNs in MemoryLLM are static and drawn from set limited to the size of vocabulary (V ) of the model, regardless of phase (training or inference). This unique design not only permits context-free token-level information storage within FFNs but also facilitates plug-n-play flexibility for dynamic size LLM, where FFN from transformer layer can be removed depending on its importance and VRAM constraints without disrupting the residual information flow."
        },
        {
            "title": "2.3 Key Benefits: Interpretability and Efficiency",
            "content": "2.3.1 Interpretability: TKV Framework In this section, we address the three key limitations of existing works (Sukhbaatar et al., 2019; Geva et al., 2021, 2022; Nichani et al., 2024): (1) the relationship between FFN memory locations and non-interpretable contextualized latents of input prefix tokens, which can significantly shift based on context; (2) the laborious reverse-engineering process required to manually mine input prefixes from calibration training data based on FFN key activations; and (3) the underexplored influence of how key-value FFN memory on downstream task performance. In modern LLMs (e.g., LLaMa and GPT variants), SwiGLU based feed-forward modules are composed of three parameter matrices: up-projection (WU p), gate-projection (WGate) and down-projection (WDown) matrices. To overcome these interpretability limitations, we develop TKV (token-key-value) framework for FFNs that propose interpreting the up-projection (key) and down-projection (value) matrices as neural retrieval memory (Geva et al., 2021; Meng et al., 2022), containing key-value pairs (memory cells). In this framework, the gate-projection acts as learned reweighting function for keys during pretraining. Specifically, the gateprojection determines how strongly each memory cell of the FFN memory is amplified or suppressed. Figure 2 presents detailed overview of our TKV framework. text sequence is transformed into token-level query vectors (xi) accessing FFN neural memory (represented with WU and WDown matrices) in context-free 4 Figure 2 TKV Framework: Input text is tokenized into discrete token IDs as context-free query vectors for FFN memory cells. WU and WDown projection matrices emulate the behavior of Keys and Values while the WGate matrix can be interpreted as reweighting function of token memory coefficients. fashion, generating retrieved output (o). This is added to the residual stream (Figure 1) in parallel to selfattention. For given query vector corresponding to token t, the neural memory retrieval process within FFN is two step process: Step I: Estimate the memory cell coefficient (cki in WU RKd matrix with dot product between and importance score of key ki for query vector q. In SwiGLU based FFNs, cki with gki to amplify or suppress some specific keys: ) corresponding to all keys ki represented as column vectors Intuitively, cki can be perceived as is further element-wise reweighted . cki = (q1d p[:,ki] ) gki. (2.7) Step II: For query q, the retrieved memory output is ckiweighted linear combination of value vectors vi, which are represented as row vectors in down-projection matrix (WDown): FFN(X 0 ) = o1d = (cid:88) cki vki . (2.8) 5 The TKV framework of MemoryLLM addresses the challenge of undefined input query prefixes in prior works by defining finite set of human-interpretable query vectors from the vocabulary token IDs. This eliminates the need for reverse manual annotation of training data to study the relationship between input prefixes and corresponding memory cells within FFNs. In Section 3, we will investigate the token-level spatial properties of memory cells and their importance in downstream tasks. 2.3.2 Efficiency: FFNs as Pre-computed Lookups In modern LLMs, FFNs account for approximately two-thirds of the model parameter budget, creating significant computational and VRAM overheads that limit deployment in resource-constrained settings. In contrast to conventional designs, FFNs in MemoryLLM perform inference with static, context-free embedding outputs. This allows for pre-computation of FFN modules over all vocabulary and off-loading into storage for asynchronous communication. Hence, MemoryLLM addresses both memory and computational constraints while improving interpretability. Figure 3 FFNs as Pre-computed Token-wise Lookups: Outputs corresponding to each vocabulary tokens for all FFN modules across transformer blocks can be pre-computed offline and stored as static token-indexed lookups (ToLs) in storage devices. Figure 3 illustrates how to pre-compute FFNs one-time and build static lookup table for each vocabulary token ID across all transformer layers, which can be stacked horizontally and offloaded to storage devices for asynchronous prefetch during inference. Mathematically, for each token embedding x1d associated with token ti for -layer LLM, we generate ToLs as: ti ToL1(N d) xti = ConcatN 1 k=0 FFNLk (xti), dim=1 (cid:110) (cid:111) (2.9) On-demand Plug-n-Play ToLs: Our ToL formulation depends on token embeddings without any connection to intermediate features and access to contextual information. In addition to reduced computational cost per token with pre-computed ToLs, MemoryLLM presents two fold on-demand plug-n-play design: Token-distribution follows Zipf law: The token distribution of modern LLM-generated content adheres to Zipfs law irrespective of tokenizer (He et al., 2025; Zhemchuzhina et al., 2022). Therefore, Figure 4 Percentage increase in perplexity when FFN computation for layer is dropped in Base and MemoryLLM. 6 Figure 5 Semantically Similar Tokens Build Memory Outputs with Similar Keys: t-SNE plot with K-Means clustering of ck vectors, which represent each keys contribution to memory outputs, yields clusters of tokens with semantically similar properties. VRAM requirements for MemoryLLM can be significantly reduced by caching fraction of ToLs for frequent tokens, and loading less frequently used ToLs from storage. Non-uniform importance of ToLs across layers: Prior works (Yin et al., 2023; Jaiswal et al., 2024a,b) have established that not all layers contributed equally to the model performance. Figure 4 shows that FFN contribution to MemoryLLM performance drops notably after first few layers, while FFNs across conventional base LLM shows non-uniform U-shaped behavior due to disruption in residual flow. This suggests that ToLs of later FFN layers can be offloaded permanently from VRAM with minimal disruption in residual flow. During inference, the output of layer for {t1, t2, ..., tM } tokens in MemoryLLM can be written as: XL+1 = XL + Attn(XL) + ToL[{t1,t2,...,tM },L,:] (2.10) Note that based on the aforementioned on-demand plug-n-play policy, ToL of token id ti can be loaded into VRAM if there is cache miss with suitable caching policy."
        },
        {
            "title": "3.1 Spatial Distribution of Neural Memory in FFNs",
            "content": "In section 2.3.1, we developed the TKV framework that emulate FFNs as neural token-wise retrieval memory with key-value pairs. To build the output of memory (oq) for query token vector q, each element cki within the ck vector can be interpreted as proxy indicator of how much ki will contribute to oq. In an ideal setting, ck remains one-hot vector where single key among keys build the output oq while in worst-case scenario, ck is uniform vector where oq is average of the values corresponding to all keys. Here, we build on top of the prior works (Geva et al., 2021, 2022) for large-scale LLaMa-3.1 tokenizer (Grattafiori et al., 2024), and ask: Is there relationship between semantically similar tokens and the specific keys they activate to generate memory outputs? To answer this question, we perform K-means clustering on ck vectors corresponding to all vocabulary tokens to identify if semantically similar tokens have similar importance score (cki ) for certain keys and can be clustered together. Figure 5 shows K-means clustering results of ck vectors from layer L0, corresponding to all vocabulary tokens in MemoryLLM-1B. Surprisingly, we found well-formed clusters that capture multiple human-interpretable cues such as punctuation marks, personal names, geographical locations, linguistic properties. This finding positively supports the conjecture that semantically similar tokens tend to build final 7 memory outputs with similar keys. This property provides opportunities for researchers to explore knowledge editing and injection, or toxicity suppression with targeted alteration of certain keys in interpretable fashion. Next, we study how this clustering behavior translates across other layers of our MemoryLLM-1B model and empirically quantify it with the clustering coefficient (CC). In Figure 6(a), we find high CC value across all layers in the model checkpoint, despite slight decrease across some middle layer. Since, outlier coefficients dominate in building memory output, we study the average number of outlier coefficients cross ck vectors of 128k vocabulary tokens. Figure 6 (b) shows that terminal layers tend to have higher number of outlier keys, which predominantly contribute towards output formation of FFNs. This hints at superior token-level information convergence within limited keys. Figure 6 (a) Clustering coefficient for ck vectors from FFN memory across 24 layers of MemoryLLM-1B. (b) Average outliers count in ck vectors corresponding to each vocabulary token."
        },
        {
            "title": "3.2 Probing FFN Memory Across Downstream Tasks",
            "content": "MemoryLLM design focuses on decoupling FFNs from the residual flow and allows us to study the impact of FFN memory in isolation on model performance. First, we start with establishing that MemoryLLM is well able to disentangle the tight coupling of FFNs within the residual flow. Following the formulation of MemoryLLM in Equation 2.6, we designed an experiment where we control the contribution of FFNs with an interpolation scaler as αFFN(X0) for MemoryLLM and conventional base model checkpoints. Figure 7 illustrates how decreasing contribution ratio of FFN memory has relatively lesser impact on MemoryLLM performance degradation in comparison to conventional LLM designs. This behavior can be explained with the unique design choice of MemoryLLM, which doesnt disrupt the residual flow as significantly as base model and provides favorable setting for posttraining model compression techniques. Table 1 Controlled investigation of FFNs across tasks: Reducing the contribution of FFNs with gradually decreasing (α) hurts tasks that heavily rely on recall or retrieval of known information relatively more than reasoning or logical thinking tasks. Alpha Recall/Retrieval Dominated Tasks Logical/Reasoning Dominated Tasks Wikitext-2 () LAMBDA() SiQA() ARC-Easy() HellaSwag() Winogrande() BoolQ() PIQA() 1.0 0.9 0.8 0.7 0.6 0.5 24.348 24.6518+1.24% 25.5173+4.80% 27.4568+12.76% 32.6135+33.94% 53.7072+120.57% 0.3613 0.3643+0.82% 0.35461.87% 0.300416.86% 0.200244.59% 0.108669.93% 0.7830 0.77800.64% 0.72607.28% 0.670014.43% 0.662015.45% 0.538031.29% 0.5156 0.50432.19% 0.441414.40% 0.416819.16% 0.394923.40% 0.317038.52% 0.3724 0.3749+0.68% 0.3750+0.70% 0.36940.79% 0.36142.96% 0.330011.38% 0.5233 0.51930.76% 0.51781.06% 0.49884.68% 0.49016.34% 0.50124.23% 0.6205 0.61770.45% 0.61590.74% 0.61650.64% 0.61351.14% 0.61311.18% 0.7018 0.7035+0.24% 0.7062+0.63% 0.7057+0.55% 0.69800.54% 0.68013.09% Second, we ask: Does the influence of FFNs, viewed as token-indexed key-value memories, remain consistent across different tasks? To investigate, we consider two broad categories of tasks: (a) tasks which heavily rely on recall or retrieval of known information things that are explicitly stored in FFN memory from the training data (e.g., wikitext-2, LAMBDA, SiQA, ARC-Easy); (b) tasks that require logical, causal, or inferential thinking where answer isnt directly stored in FFN memory and it must be derived (e.g., HellaSwag, Winogrande, BoolQ, PIQA). To probe the contribution of FFN memory, we decrease α similarly to our prior formulation. Table 1 presents the results of this study of FFNs in MemoryLLM-1B checkpoint 8 Figure 7 Model performance comparison with regulated contribution of FFNs in MemoryLLM and base checkpoints. Figure 8 Architecture comparison of MemoryLLM & Flex-MemoryLLM with exactly same total number of parameters. across two task categories for α 1.0, ..., 0.5, where 1.0 indicate FFNs contribute as wholly to residual flow. We observe novel finding: gradually reducing FFNs contribution hurts tasks that heavily rely on recall or retrieval of known information relatively more than reasoning or logical thinking tasks."
        },
        {
            "title": "4.1 MemoryLLM Comparison with Conventional LLM",
            "content": "In section 2.3.2, we discussed how FFNs in MemoryLLM after training can be precomputed and offloaded as ToLs to reduce VRAM requirements as well as computational cost. With two-thirds of total model parameters typically occupied by feed-forward modules, this lookup strategy reduces effective active parameters in VRAM to be one-third of total model parameters. Although MemoryLLM is designed to improve FFN interpretability, we investigate how MemoryLLM architecture compares to conventional base LLMs on performance standards. To minimize any hyperparameter influence on performance, we used the same training settings for MemoryLLM and base architecture for fixed number of training tokens (25-150 billion). For additional implementation details, please refer Appendix A. Table 2 outlines two findings: (1) in reference to total number of parameters, MemoryLLM performance fall short of conventional base LLM counterpart; (2) however in reference to effective active number of parameters (ToLs are not counted as active), MemoryLLM notably outperforms its dense counterpart. These results motivate us to explore methods to bridge the performance gap between MemoryLLM and the base model without compromising the architectures ability to treat FFNs as context-free, plug-and-play neural memory. 9 Table 2 PPL comparison of conventional LLMs (base-250M & 750M) wrt. MemoryLLM with 50B training tokens. base-750 MemoryLLM MemoryLLM base-250 Active Params Total Params C4 Wikitext-2 737M 737M 19.730 25.491 402M 1208M 20.933 27.258 245M 737M 22.079 29.976 265M 265M 23.190 32."
        },
        {
            "title": "4.2 Flex-MemoryLLM: Bridging Conventional LLM and MemoryLLM",
            "content": "In contrast to conventional dense LLM designs where all FFNs parameters are involved in computation over residual flow, MemoryLLM makes drastic simplification of using entire FFN parameters as static token-level lookup memory over vocabulary. This abrupt simplification limits the computational capability of MemoryLLM leading to lower performance with same training budget in comparison to its dense counterpart of same total parameter count. To address the performance gap, we propose Flex-MemoryLLM, which provide smooth bridge between performance and interpretability. Figure 8 presents the architecture comparison of MemoryLLM & Flex-MemoryLLM with exactly same total number of parameters. As shown in the diagram, Flex-MemoryLLM splits total FFN parameters in MemoryLLM between two parts: FFN Compute (FFN-C): linear dense module which operates on residual flow and increases the computational capability of MemoryLLM; and FFN Memory (FFN-M): context-free neural memory similar to FFNs in MemoryLLM trained with token embeddings with no connection to residual flow. In our experiments, we use the architecture design recipes of LLaMa family models, where feed-forward modules approximately hold 8h2 parameters with intermediate expansion factor of 2.67, where represents hidden dimension of the model. To create Flex-MemoryLLM block, we move βh2 parameters from FFN Memory module in MemoryLLM to FFN compute with pursuit to increase the total active parameters of the Flex-MemoryLLM (More details in Appendix A). We experimented with β {1, 2, 3} and we found that β = 3 can closely match the performance and training trajectory of conventional base counterpart at the same time enable offloading 5h2 parameters as FFN memory from VRAM to storage devices during inference. Figure 9 Performance comparison of conventional LLM base with MemoryLLM and Flex-MemoryLLM models with 1B total parameter count and varying active parameter counts. Training is performed with same recipes & equal token (150B) counts for fairness. Figure 9 presents the performance comparison of conventional base-1B LLM model having 1208M active parameters with respect to three Flex-MemoryLLM-βh2 versions with β {1, 2, 3} and MemoryLLM having 704M, 604M, 503M, and 402M active parameter counts respectively. All model checkpoints are trained with exactly same training recipes on 150B tokens for fair evaluation. We observe that, while there exists notable performance gap between base-1B model and MemoryLLM, the performance gap significantly diminishes as we move from MemoryLLM to Flex-MemoryLLM versions. The flexible design Flex-MemoryLLM that split FFN parameters across FFN-C and FFN-M to gradually improve model capacity provides an interesting balance between efficiency and performance, having close to base performance with approximately 5h2 reduction in active parameters. These results also encourages future studies to closely investigate the over-parameterization ratio of FFNs in modern LLMs and its relationship with training dynamics of LLMs. 10 Figure 10 Performance comparison of conventional dense LLMs with MemoryLLM and variants of Flex-MemoryLLM at different scale of parameter training tokens and model parameters with same training recipes for fair comparison. In Figure 10, we studied the performance of models with 250M, 750M, and 1B total parameter sizes. The rightmost data point across all three model scale curves represent the conventional base design while leftmost indicate the MemoryLLM. Data points in middle represent three different version of Flex-MemoryLLM-βh2 with β {1, 2, 3}. We find that: (1) with scaling training from 25B tokens to 150B tokens, the performance gap between MemoryLLM and Flex-MemoryLLM models wrt. base with same total parameter count notably diminishes; (2) ppl difference of Flex-MemoryLLM-3h2 with 704M active parameters closely matches its dense counterpart with 1.2B active parameters; and (3) interestingly, Flex-MemoryLLM-3h2 1B model with 704M active parameters outperforms the base-737M model indicating Flex-MemoryLLM can be alternative strategy to train superior models with fixed active parameter count."
        },
        {
            "title": "4.3 MemoryLLM & Flex-MemoryLLM: Alternative Approach wrt. Conventional LLM Pruning",
            "content": "Figure 11 PPL comparison of MemoryLLM, Flex-MemoryLLM, and LLM pruning methods wrt. active parameters. In this section, we investigate how MemoryLLM and Flex-MemoryLLM compare wrt. conventional LLM pruning training techniques, which aim to reduce total active parameter of the model. Figure 11 presents our comparison of MemoryLLM, and Flex-MemoryLLM-βh2 built upon 1B and 750M total parameter count wrt. three pruning techniques (Magnitude, SparseGPT, Wanda). The red dotted line indicate the performance of base model with 1B and 750M total and active parameters count. Clearly, we can observe that both MemoryLLM & Flex-MemoryLLM-βh2 models are significantly superior compared to training base model and pruning it to match same active parameter count. These findings illustrate our proposed architecture designs as an alternative to developing novel pruning techniques."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose MemoryLLM, modified transformer architecture that explicitly decouples feed-forward networks (FFNs) from the residual stream and self-attention. In our work, FFNs are trained in isolation using context-free token embeddings, enabling their interpretation as neural key-value memory over finite, human-interpretable query space (the vocabulary). We found that knowledge associated with lexically and 11 semantically similar tokens are indexed across similar memory locations within FFNs. This knowledge is crucial for the performance of retrieval-based tasks. In addition to improved interpretability, this design allows FFNs to be pre-computed as token-wise lookups (ToLs) enabling reduced memory footprint & compute cost."
        },
        {
            "title": "References",
            "content": "B. Adler, N. Agarwal, A. Aithal, D. H. Anh, P. Bhattacharya, A. Brundyn, J. Casper, B. Catanzaro, S. Clay, J. Cohen, et al. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704, 2024. S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den Driessche, J.-B. Lespiau, In International Improving language models by retrieving from trillions of tokens. B. Damoc, A. Clark, et al. conference on machine learning, pages 22062240. PMLR, 2022. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. D. Chen and W.-t. Yih. Open-domain question answering. In A. Savary and Y. Zhang, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 3437, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-tutorials.8. URL https://aclanthology.org/ 2020.acl-tutorials.8/. D. Chen, A. Fisch, J. Weston, and A. Bordes. Reading wikipedia to answer open-domain questions. arXiv preprint arXiv:1704.00051, 2017. K. Clark, U. Khandelwal, O. Levy, and C. D. Manning. What does bert look at? an analysis of berts attention. arXiv preprint arXiv:1906.04341, 2019. G. Dar, M. Geva, A. Gupta, and J. Berant. Analyzing transformers in embedding space. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1612416170, 2023. N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html. M. Geva, R. Schuster, J. Berant, and O. Levy. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 54845495, 2021. M. Geva, A. Caciularu, K. Wang, and Y. Goldberg. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Proceedings of the 2022 conference on empirical methods in natural language processing, pages 3045, 2022. A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. A. Graves, G. Wayne, and I. Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014. A. Graves, G. Wayne, M. Reynolds, T. Harley, I. Danihelka, A. Grabska-Barwinska, S. G. Colmenarejo, E. Grefenstette, T. Ramalho, J. P. Agapiou, A. P. Badia, K. M. Hermann, Y. Zwols, G. Ostrovski, A. Cain, H. King, C. Summerfield, P. Blunsom, K. Kavukcuoglu, and D. Hassabis. Hybrid computing using neural network with dynamic external memory. Nature, 538:471476, 2016. URL https://api.semanticscholar.org/CorpusID:205251479. T. Gunter, Z. Wang, C. Wang, R. Pang, A. Narayanan, A. Zhang, B. Zhang, C. Chen, C.-C. Chiu, D. Qiu, et al. Apple intelligence foundation language models. arXiv preprint arXiv:2407.21075, 2024. K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pages 39293938. PMLR, 2020. X. O. He. Mixture of million experts. arXiv preprint arXiv:2407.04153, 2024. Y. He, Q. Zeng, and M. Jiang. Pre-trained models perform the best when token distributions follow zipfs law. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2799728009, 2025. 12 A. K. Jaiswal, B. Hu, L. Yin, Y. Ro, T. Chen, S. Liu, and A. Akella. Ffn-skipllm: hidden gem for autoregressive decoding with adaptive feed forward skipping. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1694316956, 2024a. A. K. Jaiswal, L. Yin, Z. A. Zhang, S. Liu, J. Zhao, Y. Tian, and Z. Wang. From low rank gradient subspace stabilization to low-rank weights: Observations, theories, and applications. In International Conference on Machine Learning, 2024b. URL https://api.semanticscholar.org/CorpusID:271218569. A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. S. Jie, Y. Tang, K. Han, Y. Li, D. Tang, Z.-H. Deng, and Y. Wang. Mixture of lookup experts. arXiv preprint arXiv:2503.15798, 2025. V. Karpukhin, B. Oguz, S. Min, P. S. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage retrieval for open-domain question answering. In EMNLP (1), pages 67696781, 2020. A. Kaushal, T. Vaidhya, and I. Rish. Lord: Low rank decomposition of monolingual code llms for one-shot compression. arXiv preprint arXiv:2309.14021, 2023. U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172, 2019. G. Lample, A. Sablayrolles, M. Ranzato, L. Denoyer, and H. Jégou. Large memory layers with product keys. Advances in Neural Information Processing Systems, 32, 2019. P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. Y. Li, Y. Yu, Q. Zhang, C. Liang, P. He, W. Chen, and T. Zhao. Losparse: Structured compression of large language In International Conference on Machine Learning, pages models based on low-rank and sparse approximation. 2033620350. PMLR, 2023. A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr, C. Ruan, D. Dai, D. Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. J. Liu, J. Wu, X. Yu, Y. Su, P. Mishra, G. Ramesh, S. Ranjan, C. Manem, X. Sun, Z. Wang, et al. Instella: Fully open language models with stellar performance. arXiv preprint arXiv:2511.10628, 2025. K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in gpt. Advances in neural information processing systems, 35:1735917372, 2022. R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. E. Nichani, J. D. Lee, and A. Bietti. Understanding factual recall in transformers via associative memories. arXiv preprint arXiv:2412.06538, 2024. F. Petroni, A. Piktus, A. Fan, P. Lewis, M. Yazdani, N. De Cao, J. Thorne, Y. Jernite, V. Karpukhin, J. Maillard, et al. Kilt: benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 25232544, 2021. A. Roberts, C. Raffel, and N. Shazeer. How much knowledge can you pack into the parameters of language model? arXiv preprint arXiv:2002.08910, 2020. T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, E. Hambro, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. N. M. Shazeer. Glu variants improve transformer. ArXiv, abs/2002.05202, 2020. URL https://api.semanticscholar.org/ CorpusID:211096588. S. Sukhbaatar, J. Weston, R. Fergus, et al. End-to-end memory networks. Advances in neural information processing systems, 28, 2015. S. Sukhbaatar, E. Grave, G. Lample, H. Jegou, and A. Joulin. Augmenting self-attention with persistent memory. arXiv preprint arXiv:1907.01470, 2019. 13 J. Vig. multiscale visualization of attention in the transformer model. arXiv preprint arXiv:1906.05714, 2019. B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y. Dong, O. Kuchaiev, B. Li, C. Xiao, et al. Shall In Proceedings of the 2023 we pretrain autoregressive language models with retrieval? comprehensive study. conference on empirical methods in natural language processing, pages 77637786, 2023a. H. Wang, S. Agarwal, Y. Tanaka, E. Xing, D. Papailiopoulos, et al. Cuttlefish: Low-rank model training without all the tuning. Proceedings of Machine Learning and Systems, 5:578605, 2023b. J. Weston, S. Chopra, and A. Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014. G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. G. Xiao, J. Tang, J. Zuo, J. Guo, S. Yang, H. Tang, Y. Fu, and S. Han. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024. L. Yin, Y. Wu, Z. Zhang, C.-Y. Hsieh, Y. Wang, Y. Jia, G. Li, A. Jaiswal, M. Pechenizkiy, Y. Liang, et al. Outlier weighed layerwise sparsity (owl): missing secret sauce for pruning llms to high sparsity. arXiv preprint arXiv:2310.05175, 2023. E. Zhemchuzhina, N. Filippov, and I. P. Yamshchikov. Pragmatic constraint on distributional semantics. arXiv preprint arXiv:2211.11041, 2022."
        },
        {
            "title": "A Implementation Details",
            "content": "Table 3 Model training configurations for our Base, MemoryLLM, Flex-MemoryLLM Models. All model checkpoints are trained within the paper adopt exactly same configuration for fair comparison. Category Common Loss Optimizer Key Value Tokens Count Vocabulary size Tokenizer Dataset Sequence Length Hidden Activation 25-150 Billion 128,256 meta-llama/Llama-3.1-8B C4 2048 SiLU Name Z-loss Name Weight Decay Beta1 Beta2 Schedular Warmup Iterations Type Max LR Min LR Cross Entropy 1.0e-6 Adam 0.1 0.9 0.95 5000 Cosine 1.0e-04 1.0e-05 Table 4 FFN parameter division for usage as context-dependent FFN and context-free Memory FFN. Architecture FFN-C FFN-M Base MemoryLLM Flex-MemoryLLM-h2 Flex-MemoryLLM-2h2 Flex-MemoryLLM-3h2 33.554M 0M 0M 33.554M 4.194M 29.360M 8.388M 25.165M 12.582M 20.971M"
        },
        {
            "title": "B Understanding Decoding Cost with ToLs",
            "content": "In this section, we aim to investigate how does our proposed architectures MemoryLLM and Flex-MemoryLLM empirically perform wrt. decoding speed (ms/token) which accounts for loading ToLs from storage devices to VRAM. We also enlist the empirically observed inference memory requirements while running our experiments. All experiments are reported using 1A100 GPU with ToLs stored in BF16 and 2048 sequence length."
        },
        {
            "title": "C Background Work",
            "content": "C.1 Memory Augmented Architectures Memory-augmented models are designed to expand models effective parameter space without incurring large computational overhead. Early work on memory networks was introduced by (Weston et al., 2014), and later extended with fully end-to-end trainable variants with (Sukhbaatar et al., 2015). Neural Turing Machines (Graves et al., 2014, 2016) incorporate an external, trainable memory that works alongside other neural components to simulate differentiable, trainable computing system. Product-key networks (Lample et al., 2019) improve the efficiency and scalability of memory retrieval and propose key-value memory layer that can scale to very large sizes while keeping exact search on the key space. More recently, PEER (He, 15 Table 5 Architecture design of Base, MemoryLLM, Flex-MemoryLLM Models models with different scale in our experiments. Total Params Configuration Active Params # Layers Hidden Dim Intermediate Dim #Attn Heads 250M 750M 1B Base Flex-MemoryLLM-h2 Flex-MemoryLLM-2h2 Flex-MemoryLLM-3h2 MemoryLLM Base Flex-MemoryLLM-h2 Flex-MemoryLLM-2h2 Flex-MemoryLLM-3h2 MemoryLLM Base Flex-MemoryLLM-h2 Flex-MemoryLLM-2h2 Flex-MemoryLLM-3h2 MemoryLLM 265M 154M 132M 110M 88M 737M 430M 368M 307M 245M 1208M 704M 604M 503M 402M 24 24 24 24 24 24 24 24 24 24 24 24 24 24 960 960 960 960 960 1600 1600 1600 1600 1600 2048 2048 2048 2048 2048 2560 2240 1920 1600 4272 3738 3200 2668 4272 5464 3418 4096 4778 5464 16 16 16 16 16 16 16 16 16 16 32 32 32 32 32 Table 6 Empirical Memory Requirement and Token Decoding estimated for MemoryLLM and Flex-MemoryLLM variants in comparison to Base transformer model at 1B total parameter scale. Inference Memory (GB) Decoding Speed (ms/token) Base 9.541 21.50 Flex-MemoryLLM-h2 Flex-MemoryLLM-2h2 Flex-MemoryLLM-3h2 MemoryLLM 7.025 18.75 7.409 20. 7.825 21.47 6.041 14.42 2024) has advanced these ideas by replacing traditional vector-based memory values with rank-one matrices, linking memory-augmented architectures with mixture-of-experts models. Accurate factual generation remains critical objective for generative models, often evaluated using opendomain question answering benchmarks (Chen et al., 2017; Chen and Yih, 2020) and other tasks requiring substantial knowledge (Petroni et al., 2021). Models that can effectively encode factual knowledge from training data are better equipped to provide correct responses to knowledge-intensive queries. While larger models generally demonstrate improved factual accuracy (Roberts et al., 2020; Brown et al., 2020), hallucination remains persistent challenge. One effective approach for mitigating this issue is retrieval-augmented generation, which leverages external knowledge sources to improve factual consistency (Lewis et al., 2020; Karpukhin et al., 2020; Khandelwal et al., 2019). Several language models have incorporated text retrieval from the pretraining stage. REALM (Guu et al., 2020) augments BERT model with one retrieval step to solve QA tasks. Retro (Borgeaud et al., 2022) enhances auto-regressive decoding with multiple rounds of retrieval, once per 64 tokens. The retrieved texts are injected through two-layer encoder and then several cross-attention layers in the decoder. Retro++ (Wang et al., 2023a) explores the scalability of Retro by reproducing Retro up to 9.5B parameters. Meanwhile, several models are adapted to retrieval in the finetuning stage. WebGPT (Nakano et al., 2021) learns to use search engine through imitation learning in text-based web-browsing environment. Toolformer (Schick et al., 2023) performs decoding with multiple tools including search engine, and the finetuning data is labeled by the language model itself. C.2 Understanding Feed-Forward Networks in Transformers. Several studies have investigated the role of feed-forward networks (FFNs) in transformers, particularly their contribution to storing and retrieving knowledge learned during pretraining. (Geva et al., 2021) demonstrated that FFNs can be interpreted as keyvalue memories that activate on specific lexical or semantic patterns, while follow-up work showed that FFNs promote vocabulary-level concepts during prediction (Geva et al., 2022). Additional related analyses in embedding space further explored how FFN activations correspond to linguistic features and factual recall (Dar et al., 2023; Nichani et al., 2024). Within their framework, the first layer acts as pattern detector (keys\") while the second layer projects specific information into the residual stream (values\"). This modularity is evidenced by the identification of specific \"knowledge neurons\" responsible for storing distinct facts. More broadly, the interpretation of neural networks as associative or 16 persistent memory systems connects this line of work to earlier memory-augmented architectures (Sukhbaatar et al., 2019). However, these analyses rely on contextualized residual activations and require extensive posthoc mining of calibration data, making the inferred query space indirect and difficult to interpret. recent work, MoLE (Jie et al., 2025), illustrates that in mixture-of-experts (MoEs), majority of experts can be trained directly with token-level input embeddings. However, MoLEs experts computation remains conditional on contextual information from routers trained with attention output. In addition, MoLEs success is strongly tied to shared experts trained in conventional fashion with intermediate hidden features as input. It remains unclear if FFN computation within dense LLMs can be disentangled from any intermediate activations without significantly hurting model trainability. In contrast, our work eliminates contextual ambiguity by training dense transformer FFNs directly on context-free token embeddings, enabling deterministic and tokenlevel interpretability."
        },
        {
            "title": "D Understanding Storage Challenges of ToLs",
            "content": "Our proposed MemoryLLM and Flex-MemoryLLM architectures provide an opportunity to pre-compute computationally expensive FFN modules as token-wise lookup tables (ToLs), which can be offload to storage devices in resource-constrained settings. This leads to the question: How does the trade-off between VRAM and storage devices look like and what can be done to minimize ToLs storage cost? We first estimate the total storage cost of LUT as follows: Storage Size = vocab_size num_layers hidden_dim bits_per_param (D.1) For our MemoryLLM-1B model with 24 layers and 2048 hidden dimension trained with LLaMa-3.1 tokenizer with 128,256 vocabulary size, 12.6 GB of storage space is required for ToLs with F16 precision. To address our question, we performed preliminary investigation1 of storage challenges of ToLs from three different perspectives: D1. Quantization of token-wise ToLs, D2. Low Rank compression of token-wise ToLs, and D3. Layer-wise ToLs compression. D.1 Quantization of Token-wise ToLs Table 7 Performance comparison of MemoryLLM-1B with various low-precision token-wise lookup table. Precision Size (GB) Wikitext-2 () LAMBDA() SiQA() ARC-Easy() HellaSwag() Winogrande() BoolQ() PIQA() 16-bit 8-bit 4-bit 12.6 GB 6.3 GB 3.15 GB 24.348 24.347 24.439 0.3613 0.3615 0.3610 0.7830 0.7831 0.7822 0.5156 0.5156 0.5157 0.3724 0.3722 0. 0.5233 0.5433 0.5429 0.6205 0.6206 0.6214 0.7018 0.7021 0.7011 D.2 Low Rank Compression of Token-wise ToLs Table 8 Performance comparison of MemoryLLM-1B with uniform low-rank SVD compression of ToLs across 24 layers. Rank Reduction Hidden Dim #Params #Params Total ToL #Params Total ToL Size Storage Reduction % C4-PPL 0% 10% 20% 30% 40% 50% 2048 1843 1638 1433 1228 1024 - 5673.01M 5041.99M 4410.98M 3779.96M 3152.01M - 90.58M 80.51M 70.43M 60.35M 50.33M 6304.03M 5763.60M 5122.51M 4481.41M 3840.31M 3202.35M 12.60 GB 11.52 GB 10.24 GB 8.96 GB 7.68 GB 6.40 GB 0% 8.57% 18.74% 28.91% 39.08% 49.20% 18.919 18.923 18.958 19.015 19.126 19.586 Several recent works (Li et al., 2023; Wang et al., 2023b; Kaushal et al., 2023) have explored the lowrank characteristics associated with weights and gradients to address storage demands and computational 1An effective novel compression technique for ToLs compression is out of the scope of this work. Our preliminary investigation reveals high redundancy within the ToLs and leaves sophisticated studies to capitalize these redundancies as future work. complexity linked to the large matrices of LLMs. For given transformer layer L, the corresponding ToLs with have dimension of vocab_size hidden_dim represented as ToLL RV d. simple SVD decomposition with rank of ToLL will produce two matrices RV and Rrd and instead of storing ToLL, we can store its low-rank representation (U, V) if is sufficiently low. We estimate the rank r, below which storage of U, will save space as follows: (V r) + (r d) V (V + d) (D.2) Solving for MemoryLLM-1B model with 24 layers and 2048 hidden dimension trained with LLaMa-3.1 tokenizer with 128,256 vocabulary size, gives 2015. It implies that if we can save storage space if we can perform 2% rank reduction within ToLL. We first start with investigating the low-rank properties within the ToLs of corresponding to different layers. Figure 13 presents the 2048 normalized singular values corresponding to different layers across 24 transformer blocks of MemoryLLM-1B. We observe that the majority of ToLs elicit heavy tail behavior, indicating better low-rank expressivity. Heavy tails indicate that only small fraction of singular values carries maximum information and the corresponding matrix can be well approximated using fraction of basis vectors from SVD with small reconstruction error. In addition, another observation indicate that ToLs of terminal layers tends to have better low-rank properties and friendly to compression in comparison to the middle layers. For simplicity, we perform uniform SVD on the ToLs across all layers, and report our findings in Table 8. We observe that simple SVD-based low rank compression can significantly reduce ToL storage cost ( 2) with marginal change in performance of the model. Provided the existence of non-uniform low-rank properties across different layers, we strongly believe that ToLs can be further compressed using non-uniform rank reduction techniques with relatively superior performance compared to uniform SVD. D.3 Layer-wise ToLs Compression Figure 12 Performance change when layer ToL is dropped in MemoryLLM-1B. Each subplot is task; within each subplot, the y-axis is the task performance and the x-axis the is the layer of the dropped. As discussed in Section 2.3.2, the unique design of MemoryLLM allows plug-n-play design for ToLs usage without any disruption in residual flow. To address the storage challenge of ToLs, we investigate the layerwise importance of ToLs to validate if ToLs of some layers can be dropped without significant impact on model capabilities. Figure 12 presents performance across 8 tasks when the ToL corresponding to certain layer is dropped. Across all the tasks, the major performance degradation comes from dropping ToLs of first few layers. This strongly suggest that majority of ToLs prominently across the middle layers are highly redundant and have marginal impact on performance. Under limited storage availability, dropping middle layer ToLs is promising compression direction. 18 Figure 13 Normalized and sorted 2048 singular values of the ToLs corresponding to different 24 layers of MemoryLLM and Flex-MemoryLLM models with 1B scale."
        }
    ],
    "affiliations": [
        "Apple"
    ]
}