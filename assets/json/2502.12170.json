{
    "paper_title": "MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections",
    "authors": [
        "Da Xiao",
        "Qingye Meng",
        "Shengping Li",
        "Xingyuan Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/Caiyun-AI/MUDDFormer ."
        },
        {
            "title": "Start",
            "content": "MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections Da Xiao 1 Qingye Meng 2 Shengping Li 2 Xingyuan Yuan 2 5 2 0 2 3 1 ] . [ 1 0 7 1 2 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We propose MUltiway Dynamic Dense (MUDD) connections, simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.82.4 compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/ Caiyun-AI/MUDDFormer. 1. Introduction Residual connections (He et al., 2016), which help mitigate the vanish gradient problem, have become indispensable to training deep learning architectures from CNNs to Transformers, the latter becoming the de facto backbone for foundation models. Though being very simple and effective, residual connections still have limitations to be solved, especially with deep Transformers with dozens of layers made common by prevailing Transformer-based LLMs. On one hand, although theoretical (Merrill et al., 2022) and 1Beijing University of Posts and Telecommunications, Beijing, China 2ColorfulClouds Technology Co., Ltd., Beijing, China. Correspondence to: Da Xiao <xiaoda99@bupt.edu.cn>. Figure 1. Downstream average accuracy of Pythia and MUDDPythia with different sizes. experimental (Tay et al., 2021b) work have suggested that adding layers increases the expressive capacity and generalization performance of Transformers, it is observed that increasing depth beyond certain point yields diminishing returns (Petty et al., 2023). The common practice of using Pre-Norm to stabilize training leads to the issue of representation collapse (Liu et al., 2020b), where hidden features in deeper layers become highly similar, and for popular families of open-weight LLMs, large fraction of the layers can be removed with minimal degradation of performance (Gromov et al., 2024). On the other hand, mechanistic interpretability studies reveal that Transformers do in-context learning tasks by composing model components (attention heads and MLPs) across different layers to form circuits, (Elhage et al., 2020; Wang et al., 2023; Merullo et al., 2024; Ni et al., 2025), where layers communicate with each other by writing to and reading from different subspaces of the residual stream. The residual stream as the shared communication channel may be overloaded and become the bottleneck for very deep models, hindering the formation of sophisticated circuits spanning distant layers necessary for complex tasks. Dense connections (Huang et al., 2017) was proposed as promising solution to the above issues, by allowing subsequent layers to directly access outputs of all preceding layers (Figure 2 (a)). It has shown effectiveness for CNNs with DenseNet (Huang et al., 2017), and recently for Transformers with DenseFormer (Pagliardini et al., 2024). However, 1 Multiway Dynamic Dense Connections these dense connection approaches use static (either fixed (Huang et al., 2017) or learnable (Pagliardini et al., 2024)) dense connection weights that are shared across sequence positions and different input streams of Transformer block. As will be shown, this rigidity severely limits their expressive capacity in Transformers. In this work, we propose MUltiway Dynamic Dense (MUDD) connections, simple yet effective approach to address the shortcomings of residual connections. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input (the query, key, value or residual) of Transformer block. These weights are used by depth-wise aggregate modules to combine outputs from all preceding layers, creating multiple input streams for the current layer. MUDD connections can be seen as depth-wise multi-head attention (Vaswani et al., 2017) and the crosslayer communication bandwidth is expanded far beyond the restriction of the residual stream. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer models. We conduct extensive experiments focusing on language model pretraining to evaluate MUDDFormers effectiveness, efficiency and scalability. MUDDFormer significantly outperforms Transformer across various model architectures and scales (from 405M model on 7B tokens to 2.8B models on 300B tokens), achieving performance of Transformers trained with 1.82.4 compute. Notably, MUDDPythia2.8B matches Pythia-6.9B in pretraining perplexity and downstream tasks and even rivals Pythia-12B in five-shot in-context learning settings (Figure 1), while adding only 0.23% parameters and 0.4% computation. We also evaluate MUDD connections on vision Transformers and analyze the trained models to elucidate why MUDD connections work. 2. Method We begin with standard Transformer decoder with layers on input sequence = {x0, ..., xT }: X0 = Embedding(X) Xi = Bi(Xi1), [1, L] Transformer(X) = XL (1) where Bi() is the ith Transformer block1 composed of multi-head attention (MHA) module followed by fully connected feed-forward network (FFN), both wrapped with Pre-LayerNorm (LN) residual connections: XA = MHA(LN(X), LN(X), LN(X)) + B(X) = FFN(LN(XA)) + XA 1In this paper we use layer and block interchangeably. (2) In this architecture, the output Xi RT (D is model dim) of layer is used as input to layer + 1. With dense connections, the input to layer + 1 is an aggregation RT of outputs of all + 1 preceding layers, from the embedding till layer i: X:i := {X0, ..., Xi} (Figure 2 (a)). In this way, the cross-layer communication bandwidth is significantly increased compared to residual connections. To obtain Transformer with dense connections, we just add Depth-wise Aggregate (DA) module after each layer to provide input for the next layer (cf. Eq. 1): 0 = X0 = Embedding(X) Xi = Bi(X i1); = DAi(X:i), [1, L] (3) DenseTransformer(X) = In the following subsections, we progressively derive Transformer with MUDD Connections (MUDDFormer), focusing on the computation inside the DA module after layer i. 2.1. Static Dense Connections In the simplest case, the DA module aggregates previous layers outputs by taking weighted sum of them (Figure 2 (b), also equivalent to DenseFormer (Pagliardini et al., 2024)): = DAstatic (X:i; θs ) = wsum( (cid:88) 1 aij := + 1 ai , (i+1)T X:i ) Xj (4) j=0 where wsum() is the weighted sum function taking the sequences of weights and values as inputs. Scalar aij is the jth value of the dense connection weight vector ai Ri+1 which is trainable parameters, i.e., θs = {ai}. 2.2. Dynamic Dense Connections In Transformer-like sequence models, each layer processes information from multiple sequence positions and may benefit from differentiated and input-dependent dense connectivity for each position. Dynamic dense connections expand the connection weight for Xj from static scalar aij to vector Aij RT , allowing Xj to contribute differentially to each position [1, ] of based on the hidden state Xi[t] RD at that position. The + 1 weight vectors stack into matrix Ai RT (i+1) which is generated dynamically by function Ai() depending on Xi (Figure 2 (c), cf. Eq. (4)): = DAdynamic (X:i; θd ) = wsum( (i+1) Ai = Ai( D Xi ), (i+1)T X:i ) (5) 1 Aij Xj (with broadcasting) := (cid:88) j=0 Multiway Dynamic Dense Connections Figure 2. Architecture of Multiway Dynamic Dense Connections. where Aij is the jth column of Ai (with slight abuse of notation). We instantiate Ai : RD Ri+1 with an MLP parameterized by W1 and W2 which computes connection weights position-wise: 0 = X0 = Embedding(X) 0 = , .., V 0 = i1, 0 = Xi = i(X = DAQ i1, (X:i), .., DAR i1, i1); (8) (X:i), [1, L] Ai(Xi) = GELU(RMSNorm(Xi)W1)W2 + ai (6) We apply RMSNorm to Xi before MLP to stabilize training. We also add static weight vector ai acting as learnable prior for dense connectivity. The trainable parameters are = {W1 RD(i+1), W2 R(i+1)(i+1), ai Ri+1}. θd Overall, dynamic dense connections can be viewed as form of depth-wise single-headed self-attention (Vaswani et al., 2017) with query Xi and keys X:i, and the attention weights are computed from the query side. 2.3. Multiway Dynamic Dense Connections In Transformer block, single input is reused simultaneously as the query, key, value and residual of the MHA module (Figure 2 (e) top). These input streams play divergent roles and we hypothesize that they benefit from differentiated dense connectivity. To enable this, we first turn normal Transformer block B(X) into multi-input one B(X Q, K, , R) by decoupling its input into four streams for query, key, value and residual, respectively (Eq. (7), Figure 2 (e) bottom), and then instantiate four DA modules, each specializing in one streams dense connectivity (Eq. (8), Figure 2 (d))2: = MHA(LN(X Q), LN(X K), LN(X )) + B(X Q, K, , R) = FFN(LN(X A)) + (7) 2This is logical view for clarity. In practice, these DSs can be combined for efficiency. See pseudocode in Appendix A. MUDDFormer(X) = R By making the dense connections multiway, the cross-layer communication bandwidth is further increased significantly. Multiway dynamic dense connections can be seen as depthwise multi(4)-head attention. This vertical cross-layer attention can be composed with the horizontal cross-token attention in Transformer to form pathways adaptively, enhancing information flow across the whole model when performing in-context learning tasks.3 At this point, we finally obtain MUDDFormer by integrating static, dynamic and multiway dense connections. Complete pseudo-code for MUDDFormer is given in Appendix A. 2.4. Parameter Re-allocation Due to the dense connections, MUDDFormers upper layers have the opportunity to process more information than lower layers and thus may need more parameters. We re-allocate the parameters of standard Transformers to make the size of FFN sub-layers grows with depth. Specifically, let Df be the hidden dim of the original FFN, we compute (i), the hidden dim of FFN at layer for MUDDFormer using linear interpolation: (i) = 0.5(L i) + 1.5(i 1) 1 Df (9) i.e., the FFN hidden dim grows linearly from 0.5Df to 1.5Df . The total number of parameters remains unchanged. 3For an illustrative example of this, see Figure 6. Multiway Dynamic Dense Connections 2.5. Optional Normalization To stabilize training models with large depth/width ratios, we propose variant of MUDDFormer by applying RMSNorm before and after DA module, and adding residual connection to DA module after the post-RMSNorm: X:i = {Norm(X0), ..., Norm(Xi)} (PreDANorm) = Norm(DAi(X:i))+Xi (PostDANorm) (10) It is similar to the hybrid-norm strategy used by recent models such as Gemma 2 (Team et al., 2024) and Grok-1 (xai org, 2024), though we apply it to DA modules instead of MHA/MLP modules. We use this PrePostDANorm variant to train the DeepNarrow models in scaling law experiments in Section 3.1 and the MUDDViT model in Appendix D. 2.6. Complexity Analysis Table 1 shows the ratios of extra parameters and computation introduced by MUDD connections with both analytical results and typical concrete values. The derivations are in Appendix B. The ratio of extra parameters, i.e. parameter count of W1 and W2 of DA modules divided by that of the whole model, is proportional to the rectified depth/width ratio η = L+3 . The ratio of extra computation, i.e. FLOPs of generating MUDD connection weights and cross-layer aggregation divided by FLOPs of the whole forward pass, besides proportional to η, decreases with ρ = . Both ratios are negligible for commonly used settings. Table 1. Ratios of extra parameters and computation introduced by MUDD connections: (last row) analytical results and (upper rows) concrete values for typical model architectures and hyperparameters. = number of layers, = sequence length. Model Size 1.4B 1.34B 2.8B 6.9B Formula 0.22% 0.49% 0.23% 0.14% η 6 Rparams RFLOPs η = L+3 ρ = 0.38% 24 2048 4096 0.0132 0.8% 0.4% 0.26% 32 4096 4096 0.0085 42 1536 4096 0.0293 2.67 32 2560 4096 0.0137 1.6 1 η 3 + ρ/4 3. Experiments Implementation Details We implement MUDDFormer model and training in JAX. We initialize the MUDD connection weight generating parameters W1 and W2 with N(0, 1 ) and 0 respectively, and initialize the static weight vector ai with 1 at aii and 0 elsewhere. This reduces MUDDFormer to Transformer at the beginning of training, which is found to be critical for good performance. If PrePostDANorm is used, we initialize the scale parameters of Pre-DA and Post-DA RMSNorms with 1 and 1e-3, respectively, and initialize ai to 0 because Xi is added as the residual after DA modules (Equation (10)). For the other parameters outside DA modules, we use Xavier normal initializer. Organization Our evaluation focuses on language modeling with decoder-only Transformer architecture, analyzing both pretraining scaling laws (Section 3.1) and downstream task performance (Section 3.2) with large scale training on the Pile dataset (Gao et al., 2020). Section 3.3 elucidates why MUDDFormer works through analyzing trained models, followed by efficiency analysis (Section 3.4) and ablations (Section 3.5). Extended vision experiments are provided in Appendix D. 3.1. Scaling Laws Settings Table 2 (top half) specifies the model sizes and hyperparameters for scaling experiments, which are mostly taken from GPT-3 specifications (Brown et al., 2020). We untie input and output embedding matrices. We train with context length 2048 and set the number of training tokens to roughly match Chinchilla scaling laws (Hoffmann et al., 2022). The other hyperparameters are in Appendix C.1. In another set of scaling experiments we trade off some width for depth to train DeepNarrow models (Table 2 (bottom half)) to see if MUDD connections offer any benefits for this style of scaling. Table 2. Model sizes and hyperparameters for scaling experiments. params nlayers dmodel nheads learning rate batch size (in tokens) tokens 405M 834M 1.4B 24 24 24 Scaling by depth 797M 1.34B 34 42 1024 1536 2048 1280 1536 16 24 32 20 24 3e-4 2.5e-4 2e2.5e-4 2e-4 0.5M 0.5M 0.5M 0.5M 0.5M 7B 15B 26B 15B 26B Baselines We compare MUDD with two recently proposed approaches to enhancing residual connections in Transformers: DenseFormer (Pagliardini et al., 2024) (same as the static dense connections described in Section 2.2) and Hyper-Connections (Zhu et al., 2025). We also compare to Transformer with dynamic dense connections (DDFormer) as described in Section 2.2. All these approaches are applied to an improved and now widely adopted Transformer architecture (Touvron et al., 2023) with rotary positional encoding (RoPE) (Su et al., 2024), SwiGLU MLP (Shazeer, 2020), etc. (often called Transformer++). We also include the plot for the original Transformer architecture used by GPT-3 as comparison. The details for these baselines are in Appendix C.2. Results Figure 3 plots Pile validation loss scaling curves of the models. While DenseFormer and Hyper-Connections Multiway Dynamic Dense Connections 3.2. Large Scaling Training and Downstream Evaluations Settings We compare MUDDFormer with the open source Pythia model suit (Biderman et al., 2023) at large scale training on 300B tokens of Pile. Specifically, we train two models, MUDDPythia-1.4B and MUDDPythia-2.8B, and compare them with Pythia models ranging from 1.4B to 12B. For fair comparison and clear quantification of the gain brought by MUDD, except adding MUDD connections as described in Section 2, MUDDPythia uses exactly the same architecture choices (e.g. parallel attention and MLP, rotary embedding with 1/4 head dim) and training hyperparameters (e.g. optimizer settings, learning rate schedule, batch size, context length, initialization methods) as Pythia (refer Biderman et al. (2023) Appendix for details). To evaluate if MUDD connections also work well with more advanced Transformer++ architecture and training recipe at this large scale, we also train MUDDFormer-2.8B based on Transformer++ instead of Pythia architecture with larger learning rate of 3.2e-4 cosine decayed to 3.2e-6. Except these two changes, the other architectural and training hyperparameters are kept the same as MUDDPythia-2.8B. Evaluation Datasets Besides the datasets used by Pythia for downstream evaluation (LAMBADA (Paperno et al., 2016), PIQA (Bisk et al., 2020), WinoGrande (Sakaguchi et al., 2021), ARC (Clark et al., 2018), SciQ (Welbl et al., 2017), LogiQA (Liu et al., 2020a)), we also include BoolQ (Clark et al., 2019) and HellaSwag (Zellers et al., 2019) for commonsense reasoning, RACE (Lai et al., 2017) for reading comprehension, all of which are widely used benchmarks. We evaluate zero-shot and five-shot results using LM evaluation harness (Gao et al., 2023). Results As shown in Table 3 and Figure 1, besides lower Pile validation ppl, MUDDPythia also significantly outperforms Pythia at 1.4B and 2.8B scales on downstream task accuracies. Notably, MUDDPythia-2.8B matches Pythia6.9B (2.46 compute) on both pretraining ppl and downstream evaluation. Augmented with better Transformer++ architecture and training recipe, MUDDFormer-2.8B even outperforms Pythia-12B. Table 3 also reports target span perplexities on randomly sampled subset of the FLAN Collection dataset (Longpre et al., 2023), which features data of instructing following, chain-of-thought, in-context few-shot learning, etc. The advantage of MUDDPythia on FLAN is even larger than on Pile with MUDDPythia-2.8B significantly outperforming Pythia-6.9B, showing that MUDD connections have more advantage in improving these valued emergent abilities of LLMs (Wei et al., 2022). The enhanced in-context learning ability is also manifested by the larger accuracies of 5shot results compared to 0-shot (e.g. 2.9% vs. 1.9%), where MUDDPythia-2.8B is on par with Pythia-12B (4.29 comFigure 3. Scaling curves of MUDDFormer and baseline models. Figure 4. Depth scaling of MUDDFormer and Transformer++. show clear advantage over Transformer++, DDFormer outperforms these two baselines by adding dynamicity to dense connections. MUDDFormer further improves upon DDFormer by making the dense connections multiway, significantly outperforming all baselines on models ranging from 405M to 1.4B. It can be estimated that MUDDFormer834M matches the loss of Transformer++ trained with 1.89 compute. Figure 3 also shows that as an architectural improvement, MUDDFormers gain over Transformer++ (LMUDD) remains stable while scaling, exceeding Transformer++s own gain over Transformer (L++) beyond 834M parameters. This shows the favorable scalability of MUDDFormer, particularly considering that Transformer++ has incorporated major architectural improvements (RoPE, SwiGLU MLP, etc.) over original Transformer since its invention. Figure 4 demonstrates MUDDFormers enhanced depth utilization: while Transformer++ shows diminishing returns beyond 24 layers (almost coincident scaling curves), MUDDFormer DeepNarrow maintains gains up to 42 layers. This validates that MUDD connections alleviate depth-induced bottlenecks by enhancing cross-layer information flow. 5 Multiway Dynamic Dense Connections Table 3. Zero-shot and five-shot downstream evaluations results. Model Pile ppl FLAN ppl LAM BADA PIQA Wino Grande ARC -E ARC -C SciQ Logi QA BoolQ Hella Swag RACE -M RACE -H Avg acc /acc Pythia-1.4B 7.29 MUDDPythia-1.4B 6.92 Pythia-2.8B 6.63 MUDDPythia-2.8B 6.29 Pythia-6.9B Pythia-12B 6.29 6.01 9.30 8. 8.16 7.50 7.85 7.26 MUDDFM-2.8B 6.01 7.08 - Pythia-1.4B MUDDPythia-1.4B - Pythia-2.8B - MUDDPythia-2.8B - Pythia-6.9B Pythia-12B MUDDFM-2.8B - - - - - - - - - - 0-shot 61.6 63.9 64.7 68.5 67.3 70. 70.7 5-shot 54.5 58.2 60.5 63.6 63.8 67.3 65.6 71.0 71. 73.9 74.6 75.2 76.0 75.7 71.0 73.0 73.6 75.5 75.5 76. 76.4 57.2 57.4 59.4 61.4 60.9 63.9 63.4 57.5 59. 60.6 63.6 63.7 64.2 66.8 60.5 61.6 64.4 66.5 67.3 70. 26.1 86.6 21.4 63.3 26.2 87.2 23.0 62.0 29.5 88.2 21.2 64.5 31.9 90.4 21.5 68.1 31.3 89.7 25.3 63.7 31.8 90.2 22.4 67.4 40.5 42.6 45.4 46.8 48.0 50. 70.4 34.2 91.8 24.0 67.4 49.5 63.1 64.1 67.3 70.3 70.2 71. 28.9 92.2 22.9 63.0 28.2 94.0 23.8 61.5 32.3 94.3 21.7 65.6 34.0 95.5 28.1 67.5 35.6 95.1 27.0 65.7 36.5 95.3 21.8 68.0 40.5 42.6 45.1 47.1 48.1 50. 73.0 39.2 95.6 25.2 70.9 49.8 37.3 38.7 38.1 39.0 40.6 40. 40.6 35.4 37.9 38.4 44.5 39.0 40.1 41.4 33.9 34. 34.9 36.7 37.0 38.3 50.8 51.7/+0.9 53.1 55.0/+1.9 55.1 56.5 38. 56.9 34.6 35.2 35.6 37.3 36.5 38.8 51.2 52.5/+1.3 54.1 57.0/+2. 56.4 57.2 38.0 58.4 pute). The multiway dense connections applied to the query, key, and value streams of MHA modules are likely to enhance the functionality of attention heads, which are crucial for in-context learning (also see analysis in Section 3.3). MUDDFormer in Figure 6 to highlight the benefit of MUDD connections and input stream decoupling, which results in more direct and cleaner information pathways. MUDD has similar effect on Q/K-composition circuits. Finally, the gains of MUDD connections with the 2.8B model are larger than those with the 1.4B model in both zero-shot and five-shot evaluations, further demonstrating the scalability of MUDD connections. 3.3. Analyzing and Understanding We analyze and compare Pythia-2.8B and MUDDPythia2.8B trained in Section 3.2 to elucidate why MUDD connections work. The analysis is done on 1024 randomly sampled sequences of length 2048 from Pile validation set. Representation Collapse Figure 5 quantifies representation collapse through cosine similarity between the inputs of adjacent layers. While Pythia exhibits progressive collapse with >0.97 similarity in later layers, MDDDPythia maintains more distinct input representations, particularly in the value stream. Thanks to input stream decoupling and stream-specific aggregation, DA modules can freely aggregate distinct value input streams for MHA modules of each layer at each sequence position. These values will then be moved to other positions by MHA at this layer without polluting the residual stream of the current position. The relative importance of dense connections for the value stream is also evidenced by ablation studies (Section 3.5). We compare illustrative V-composition circuits4 in Transformer and 4common and important in many tasks, e.g. Wang et al. (2023); Ni et al. (2025). For introduction, refer to Elhage et al. (2020). Figure 5. Cosine similarity between the inputs of the current layer and the preceding layer. Figure 6. Illustrative V-composition circuits in Transformer vs. in MUDDFormer. Colored circles are MHAs inputs of the query (yellow), key (red), value (green) and residual (black) streams and output (blue). LayerNorms and MLPs are omitted. 6 Multiway Dynamic Dense Connections Attention Head Activation Transformer models often exhibit null attention (Vig & Belinkov, 2019) where attention heads focus on the initial tokens or some special tokens as default attention sinks (Xiao et al., 2024) when no relevant tokens are found. We define head to be active at position if its maximum attention weight does not fall on the first two positions of the sequence or on tokens <bos>, . and n. We compute the activation ratio of attention heads for each layer by averaging over all heads of that layer and all sequence positions and plot the results in Figure 7. In Pythia, most heads remain inactive beyond the first few layers, limiting their contribution. MUDDPythia demonstrates 2.4 higher activation ratio across layers, particularly in deeper layers.5 This vitalization of attention heads stems from the multiway dense connections on the Q/K/V streams of MHA modules, ultimately improving in-context learning. Figure 7. Attention head activation ratio by layers. 3.4. Training and Inference Efficiency Besides theoretical complexity analysis in Section 2.6, we assess training and inference efficiency of MUDDFormer compared with Transformer in real-world settings. Settings Though we use Pythias architecture in large scale training, the evaluation in this section is done on untrained models with Transformer++ (Llama) architecture, which is of more practical relevance due to better performance. We measure on three model sizes: 1.3B, 2.8B and 6.9B, which are Llama-ed version of Pythia 1.4B, 2.8B and 6.9B respectively. We train on Google Cloud TPU v5p-128 pods with context length of 2048, batch size of 2M tokens and measure training throughput. We do inference on NVIDIA A100 80G GPU with prompt length of 4096, batch size of 1 and measure the speed to generate 128 tokens. We repeat the measurements 3 times and take the average. We implement training and inference in pure JAX and PyTorch respectively without writing any Pallas/Triton-based custom kernels. We use torch.compile (PyTorch 2.5.1) to accelerate both Transformer++ and MUDDFormer. Results As shown in Table 4, the training and inference 5Visualization of sampled attention patterns for heads in Pythia and MUDDPythia are shown in Appendix E. overheads, while larger than the theoretical estimates in Table 1 and not negligible, are entirely acceptable considering the significant performance gain. The overheads primarily stem from the series of small operations and additional I/O introduced by DA modules. We believe that kernel fusion techniques offer potential for further acceleration and leave it for future work. Table 4. Training throughput and inference speed comparison between Transformer++ and MUDDFormer. Model Training (K tokens/s) Inference (tokens/s) Size TFM++ MUDDFM TFM++ MUDDFM 1.3B 2.8B 6.9B 1147 684 332 1030 89.8% 325 181 575 84.0% 318 95.6% 95. 286 88.1% 163 90.0% 89.7 94.0% 3.5. Ablations and Variants We conduct ablation studies with the 405M Transformer++/MUDDFormer models in scaling law experiments for language modeling in Section 3.1. Ablation settings We do two groups of experiments and report the perplexity results in Table 5. In the first group, we progressively add the four components, i.e. static (Section 2.1), dynamic (Section 2.2), multiway (Section 2.3) dense connections and parameter re-allocation (2.4)) to Transformer++ to finally obtain MUDDFormer to compare the contribution of each component. In the second group, we focus on the multiway aspect and study the effect of dense connections for the four decoupled inputs streams by replacing each of them with normal residual connection respectively. Results All three ingredients of dense connections, i.e. static, dynamic and multiway, make contributions. While parameter re-allocation is effective on MUDDFormer, it deteriorates Transformer++. Removing dense connections for each of the four streams hurts performance and the value stream benefits most from dense connections. Table 5. Ablations of MUDDFormers components. Config ppl Config ppl Transformer++ +Static Dense +Dynamic Dense +Multiway Static Dense +Multiway Dynamic Dense +Mul. Dyn. Dense+Re-alloc 10.77 +Re-alloc 11.93 11.68 MUDDFormer 10.77 11.44 dense 10.89 11.09 dense 10.90 11.27 dense 11.05 10.83 dense 11.14 Variants with Sparse Connectivity We design MUDDFormer variants by approximating its dense connections with two sparse connectivity patterns: 1) dilation and periodicity (MUDDFormer-kp, also used in (Pagliardini et al., Multiway Dynamic Dense Connections closely related to DenseFormer and HC but differs in critical ways. First, unlike DenseFormer, our MUDD connections dynamically compute per-position weights conditioned on the hidden states. Second, although HC uses combination of static and dynamic weights to expand the hidden states, it does not employ explicit all-to-all cross-layer dense connectivity. Moreover, none of existing approaches consider decoupling the four input streams of Transformer block by multiway design, which is shown to bring significant performance gain in MUDDFormer. Mechanistic Interpretability Research in this field employs various attribution methods (Conmy et al., 2023; Hanna et al., 2024) to uncover the circuits within Transformers that underlie specific capabilities (Elhage et al., 2020; Wang et al., 2024; Ni et al., 2025). These studies reveal the critical role of cross-layer interactions between attention heads and MLPs in enabling complex reasoning - key insight motivating MUDD connections design, which explicitly facilitates such interactions. Cross-Layer KV Cache Optimization Brandon et al. (2024) proposes Cross-Layer Attention (CLA) to reduce Transformer KV cache size by sharing keys and values between adjacent layers, trading expressiveness for efficiency. Our MUDD connections enable cross-layer information flow between KV caches via dense connections on key and value streams. This enhances KV cache expressiveness and utility, improving in-context learning as evidenced by experiments. OmniNet (Tay et al., 2021a) achieves fully global KV Cache receptive field by allowing each token to attend to all tokens in all layers. MUDDFormer attains similar effect in much more efficient way via composition of cross-layer dense connections and within-layer attention. Intra-Layer Architectural Innovations Many other studies attempt to enhance the performance or efficiency of foundational sequence models with individual layers, including attention mechanisms (Ye et al., 2024; Leviathan et al., 2024; Liu et al., 2024), sub-quadratic linear attention or RNN/SSM architectures (Gu & Dao, 2024; Dao & Gu, 2024; Peng et al., 2024; Yang et al., 2024) and sparse Mixture-of-Experts (MoE) (Fedus et al., 2022; Dai et al., 2024). By contrast, MUDD connections focus on crosslayer communication, making it orthogonal and complementary to these approaches. We leave the exploration of combining MUDD connections with these within-layer optimizations for future work. 5. Conclusion We introduced Multiway Dynamic Dense connections to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Experimental results showed that MUDDFormer is effective, efficient and Figure 8. PPL vs. relative training and inference speed of MUDDFormer variants. 2024)): each DA module aggregates the outputs of every k-th block, and the DA modules are inserted after every blocks. 2) sliding window (MUDDFormer-SWn): each DA module accesses to the outputs from only previous blocks plus the embeddings. Figure 8 shows the Pile validation perplexities and relative training and inference speed (compared to Transformer++) of these MUDDFormer variants. We measure perplexities with 405M models to align with ablation results in Table 5, while training and inference speeds are measured using 1.3B untrained models as in Table 4, the size of which is of more practical value. While fully dense (11) connectivity achieves the best performance, these sparse variants provide spectrum of performance-efficiency trade-offs. For example, switching from MUDDFormer-11 to MUDDFormer-22 increases relative training/inference speed from 89.8%/88.1% to 97.8%/93.4% with only 0.18 increase in ppl. In comparison, MUDDFormeer-SW8 is inferior with lower speed / ppl ratio, highlighting the indispensability of long-range (>8) cross-layer interactions. 4. Related Work Enhancing Residual Connections Despite the pervasive use of residual connections (He et al., 2016) in modern deep architectures, various approaches have been proposed to address their issues such as representational collapse and diminishing return for deeper models by strengthening cross-layer communication. Huang et al. (2017) introduced DenseNet for CNNs. Inspired by it, Pagliardini et al. (2024) proposed DenseFormer for Transformers, which uses Depth Weighted Averaging modules to aggregate outputs from all preceding layers with static, learnable weights. Most recently, Zhu et al. (2025) proposed Hyper-Connections (HC), an alternative to residual connections that uses both static and dynamic weights to adjust inter-layer dependencies. Other research has explored different forms of cross-layer attention (ElNokrashy et al., 2022; Fang et al., 2023; Wang et al., 2024) which retrieve or update representations across different layers in more flexible manner. MUDDFormer is 8 Multiway Dynamic Dense Connections scalable. It significantly outperforms Transformer baselines in language and vision tasks and improves emergent abilities such as in-context learning with minimal overhead. MUDD connections have the potential to become an indispensable component of next-generation foundation models."
        },
        {
            "title": "Acknowledgements",
            "content": "We are grateful to Google Cloud for providing the compute for model training, and to Shun Wang and Tingting Zhang for their technical support and help in troubleshooting TPU resource allocation and training. ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024. Dao, T. and Gu, A. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. In Proceedings of the Forty-First International Conference on Machine Learning (ICML), 2024. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020."
        },
        {
            "title": "References",
            "content": "Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., OBrien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning (ICML), pp. 23972430. PMLR, 2023. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Brandon, W., Mishra, M., Nrusimha, A., Panda, R., and Kelly, J. R. Reducing transformer key-value cache size with cross-layer attention. arXiv preprint arXiv:2405.12981, 2024. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Conmy, A., Mavor-Parker, A., Lynch, A., Heimersheim, S., and Garriga-Alonso, A. Towards automated circuit discovery for mechanistic interpretability. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pp. 1631816352, 2023. Elhage, N., Neel, N., Olsson, C., et al. mathcircuits. for URL ematical framework Transformer Circuits Thread, https://transformer-circuits.pub/ 2021/framework/index.html. transformer 2020. ElNokrashy, M., AlKhamissi, B., and Diab, M. Depth-wise attention (dwatt): layer fusion method for data-efficient classification. arXiv preprint arXiv:2209.15168, 2022. Fang, Y., Cai, Y., Chen, J., Zhao, J., Tian, G., and Li, G. Cross-layer retrospective retrieving via layer attention. arXiv preprint arXiv:2302.03985, 2023. Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):52325270, 2022. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noach, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/ 10256836. Gromov, A., Tirumala, K., Shapourian, H., Glorioso, P., and Roberts, D. A. The unreasonable ineffectiveness of the deeper layers. arXiv preprint arXiv:2403.17887, 2024. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. In Proceedings of the First Conference on Language Modeling, 2024. Dai, D., Deng, C., Zhao, C., Xu, R., Gao, H., Chen, D., Li, J., Zeng, W., Yu, X., Wu, Y., et al. Deepseekmoe: Towards Hanna, M., Pezzelle, S., and Belinkov, Y. Have faith in faithfulness: Going beyond circuit overlap when finding 9 Multiway Dynamic Dense Connections model mechanisms. In Proceedings of Conference on Language Modeling (COLM), 2024. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pp. 770778, 2016. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., et al. An empirical analysis of compute-optimal large language model training. Advances in Neural Information Processing Systems, 35: 3001630030, 2022. Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pp. 47004708, 2017. Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017. Leviathan, Y., Kalman, M., and Matias, Y. tive attention improves transformer. arXiv:2410.02703, 2024. SelecarXiv preprint Liu, A., Feng, B., Wang, B., Wang, B., Liu, B., Zhao, C., Dengr, C., Ruan, C., Dai, D., Guo, D., et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. Liu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang, Y. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint arXiv:2007.08124, 2020a. Liu, L., Liu, X., Gao, J., Chen, W., and Han, J. Understanding the difficulty of training transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020b. Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J., et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023. Merrill, W., Sabharwal, A., and Smith, N. A. Saturated transformers are constant-depth threshold circuits. Transactions of the Association for Computational Linguistics, 10:843856, 2022. Merullo, J., Eickhoff, C., and Pavlick, E. Talking heads: Understanding inter-layer communication in transformer language models. arXiv preprint arXiv:2406.09519, 2024. 10 Ni, R., Xiao, D., Meng, Q., Li, X., Zheng, S., and Liang, H. Benchmarking and understanding compositional relational reasoning of llms. In Proceedings of the ThirtyNinth AAAI Conference on Artificial Intelligence (AAAI), 2025. Pagliardini, M., Mohtashami, A., Fleuret, F., and Jaggi, M. Denseformer: Enhancing information flow in transformers via depth weighted averaging. In Proceedings of the Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS), 2024. Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernandez, R. The lambada dataset: Word prediction arXiv preprint requiring broad discourse context. arXiv:1606.06031, 2016. Peng, B., Goldstein, D., Anthony, Q., Albalak, A., Alcaide, E., Biderman, S., Cheah, E., Du, X., Ferdinan, T., Hou, H., et al. Eagle and finch: Rwkv with matrixvalued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 2024. Petty, J., van Steenkiste, S., Dasgupta, I., Sha, F., Garrette, D., and Linzen, T. The impact of depth and width on transformer language model generalization. arXiv preprint arXiv:2310.19956, 2023. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Tay, Y., Dehghani, M., Aribandi, V., Gupta, J., Pham, P. M., Qin, Z., Bahri, D., Juan, D.-C., and Metzler, D. Omninet: Omnidirectional representations from transformers. In International Conference on Machine Learning (ICML), pp. 1019310202. PMLR, 2021a. Tay, Y., Dehghani, M., Rao, J., Fedus, W., Abnar, S., Chung, H. W., Narang, S., Yogatama, D., Vaswani, A., and Metzler, D. Scale efficiently: Insights from pretraining and fine-tuning transformers. arXiv preprint arXiv:2109.10686, 2021b. Team, G., Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Rame, A., et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Multiway Dynamic Dense Connections Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Vig, J. and Belinkov, Y. Analyzing the structure of attention in transformer language model. arXiv preprint arXiv:1906.04284, 2019. Wang, K., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J. Interpretability in the wild: circuit for indirect object identification in gpt-2 small. In Proceedings of the Eleventh International Conference on Learning Representations (ICLR), 2023. Wang, K., Xia, X., Liu, J., Yi, Z., and He, T. Strengthening layer interaction via dynamic layer attention. arXiv preprint arXiv:2406.13392, 2024. Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. Welbl, J., Liu, N. F., and Gardner, M. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209, 2017. xai org. Grok-1. 2024. URL https://github.com/ xai-org/grok-1. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations (ICLR), 2024. Yang, S., Wang, B., Zhang, Y., Shen, Y., and Kim, Y. Parallelizing linear transformers with the delta rule over In Proceedings of the Thirty-Eighth sequence length. Annual Conference on Neural Information Processing Systems, 2024. Ye, T., Dong, L., Xia, Y., Sun, Y., Zhu, Y., Huang, G., and Wei, F. Differential transformer. 2024. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Zhu, D., Huang, H., Huang, Z., Zeng, Y., Mao, Y., Wu, B., Min, Q., and Zhou, X. Hyper-connections. In Proceedings of the Thirteenth International Conference on Learning Representations (ICLR), 2025. A. PyTorch Style Pseudo-code for MUDDFormer Multiway Dynamic Dense Connections 1 2 3 5 6 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27 # = batch_size; # = layer_index; = num_ways = 4; = seq_len; = model_dim = DA_hidden_dim = C*(L+1) def generate_dw(x, mudd_theta): # x: BxTxD w1, w2, = mudd_theta # w1: DxK, w2: Kx(C*(l+1)), a: Cx(l+1) dw = GELU(RMSNorm(x) @ w1) @ w2 + dw = rearrange(dw, 'B (C L)-> L', C=4) return dw def DA(Xs, mudd_theta): # Xs: List (l+1)x[BxTxD] dw = generate_dw(Xs[-1], mudd_theta) xs = [] for c, way in enumerate(['Q', 'K', 'V', 'R']): = sum([dw[c, :, :, j:j+1] * Xs[j] # BT1,BTD->BTD for in range(len(Xs))]) xs.append(x) return xs def muddformer(x, model): = model.embedding(x) Xs = [x] xq, xk, xv, xr = x, x, x, for block in model.blocks: attn_out = block.attn(LN(xq), LN(xk), LN(xv)) + xr = block.ffn(LN(attn_out)) + attn_out Xs.append(x) xq, xk, xv, xr = DA(Xs, block.mudd_theta) return xr B. Details of Complexity Analysis Compared to Transformer++, extra compute and parameters in MUDDFormer are introduced by DA modules, increasing from bottom layer to top layer, due to varied hidden dim Ki of DA at layer i. We approximately calculates Rparams and RF LOP s, the ratios of extra parameters and computation by considering an average layer in the middle of MUDDFormer. In this layer, = 4(L + 1) = 4( L+1 2 + 1) = 2(L + 3). We assume in typical Transformer architecture. We denote ρ = . To simplify, we ignore RMSNorm because it is negible in terms of parameters and computation. and η = L+3 Ratio of extra parameters Rparams = Ratio of extra FLOPs. (cid:80)L i=1( W1 (cid:122) (cid:125)(cid:124) (cid:123) DKi + 12LD2 W2 (cid:122)(cid:125)(cid:124)(cid:123) 2 ) 2 DK + 12D2 generate dense weight Aij Code Line 6 (cid:123) (cid:125)(cid:124) (cid:122) DKi + 2 + (cid:80)L i=1( Depthwise Aggregate Code Line 13-15 (cid:122) (cid:125)(cid:124) (cid:123) DKi ) RF LOP = 2T DK + DT (12D + ) LDT (12D + ) 2 2 = 2DK + D(12D + ) DK 12D2 (assume K) = + 3 6D = η 6 (11) (12) (assume 2D K) = 4(L + 3) 12D + = η 3 + ρ/4 2K 12D + 12 Multiway Dynamic Dense Connections C. Hyperparameters and Baselines for Scaling Law Experiments C.1. Hyperparameters We use the AdamW optimizer with β1 = 0.9, β2 = 0.95, gradient clip value of 1.0, weight decay of 0.1, 1% learning rate warmup steps followed by cosine decay to 10% of its maximal value, and no dropout. These hyperparameters are mostly taken from the GPT-3 paper (Brown et al., 2020) and are also used by all the baseline models listed below. C.2. Baseline Models Transformer: The standard Transformer based on GPT-3. Transformer++: An improved Transformer architecture adopted by Llama (Touvron et al., 2023) etc. with rotary positional encoding (RoPE) (Su et al., 2024), SwiGLU MLP (Shazeer, 2020), RMSNorm instead of LayerNorm and no linear bias. DenseFormer: The DenseFormer model from Pagliardini et al. (2024) without dilation, which has the best performance according to the paper. We implemented the model in JAX based on the PyTorch code released by the authors6. Hyper-Connections: The dynamic hyper-connections with expansion rate = 4 (DHC4) from Zhu et al. (2025), which achieves superior results on language model pre-training and is the recommended configuration in the paper. We implemented the model in JAX based on the PyTorch Implementation given in Appendix of the paper. DDFormer: Transformer with dynamic dense connections but without multiway splitting as described in Section 2.2. D. Image Classification with ViT Besides decoder-only transformer for language modeling, we apply MUDD connections to Vision Transformer (ViT, an encoder-only Transformer) (Dosovitskiy et al., 2020) for image classification on the ImageNet-1k dataset (ILSVRC-2012). Implementation and experimental settings (e.g. the use of RandAugmention+MixUp, fixed 2D sincos position embedding and global average pooling) are based on the Big Vision codebase7. We use AdamW with β1 = 0.9, β2 = 0.999, gradient clip value of 1.0, weight decay of 0.3, learning rate of 0.003 with 10000 warmup steps followed by cosine decay to 0, batch size of 4096, RandAugment of 1evel 10, Mixup of 0.2 and dropout rate of 0.1. We use ViT-S/16 as the baseline model and equip it with MUDD connections to obtain MUDDViT-S/16. We also compare with 1.72 larger model ViT-M/16  (Table 6)  . We report validation loss and top-1 accuracy results on 90 and 300 epochs in Table 7. As can be seen, the gain from MUDD connections decreases bit during the training progress, probably because many epochs of repeated passes over the same dataset diminish the additional expressive capacity brought by MUDD connections. Despite this, MUDDViT-S/16 still outperforms ViT-S/16 by 2% on epoch 300, also surpassing ViT-M/16. Table 6. ViT Model architectures for ImageNet-1k classification. Model nlayers dmodel dmlp nheads params (MUDD)ViT-S/16 ViT-M/ 12 12 384 512 1536 2048 6 8 22M 39M Table 7. ViT for ImageNet-1k classification results. Model val. loss acc@e90 acc@e300 Rel. size ViT-S/16 MUDDViT-S/16 ViT-M/16 0.993 0.877 0.890 53.4 56.0 55. 76.0 78.0 77.9 1 1.007 1.72 6https://github.com/epfml/DenseFormer 7https://github.com/google-research/big vision 13 E. Visualization Multiway Dynamic Dense Connections Head activation from attention patterns In Section 3.3, we show that the ratio of head activations in MUDDPythia-2.8B is larger than that in Pythia-2.8B. Here we draw the actual attention patterns on randomly sampled sequence of length 32 from Pile validation set for the 32 heads in layer 25 of these two models in Figure 9 and 10. It is clear that attentions in Pythia mainly concentrate on the sink token (inactive) while attentions in MUDDPythia disperse on various tokens (active). Cross-layer dynamic weights To better understand MUDDPythia, we visualize dynamic dense connection weights. Due to the high variance of the norm of hidden states Xi, we scale those weights by the average norm of each layer, rectifying the importance of weights. The rectified mean and standard deviation of dynamic connection weights in MUDDPythia-2.8B are shown in Figure 11 and 12, respectively. It is evident that the patterns of the four streams (query, key, value, residual) differ from each other, validating the necessity of separating them from the standard residual stream. It is noteworthy that in the value-stream connections, most layers have salient and more dynamic weight on output of the first layer, thus forming long-range channel to transport bottom information for attention heads in upper layers. Figure 9. Attention patterns for the 32 heads in the 25th layer of Pythia-2.8B. Figure 10. Attention patterns for the 32 heads in the 25th layer of MUDDPythia-2.8B. 14 Multiway Dynamic Dense Connections Figure 11. Mean of dynamic dense connections of MUDDPythia-2.8B. Figure 12. Standard deviation of dynamic dense connections of MUDDPythia-2.8B."
        }
    ],
    "affiliations": [
        "Beijing University of Posts and Telecommunications, Beijing, China",
        "ColorfulClouds Technology Co., Ltd., Beijing, China"
    ]
}