{
    "paper_title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs",
    "authors": [
        "Qijun Luo",
        "Mengqi Li",
        "Lei Zhao",
        "Xiao Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training language models on long sequence data is a demanding requirement for enhancing the model's capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose a memory-efficient and exact BP method called StreamBP, which performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger, while using comparable or even less BP time. Note that StreamBP's sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop a communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at https://github.com/Ledzy/StreamBP."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 7 7 0 3 0 . 6 0 5 2 : r StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs Qijun Luo1 Mengqi Li1 Lei Zhao2 Xiao Li1 1The Chinese University of Hong Kong, Shenzhen 2Shanghai Jiao Tong University {qijunluo,mengqili1}@link.cuhk.edu.cn, l.zhao@sjtu.edu.cn, lixiao@cuhk.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Training language models on long sequence data is demanding requirement for enhancing the models capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose memory-efficient and exact BP method called StreamBP, which performs linear decomposition of the chain rule along the sequence dimension in layerwise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by 2.8 5.5 larger, while using comparable or even less BP time. Note that StreamBPs sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at https: //github.com/Ledzy/StreamBP."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) [1, 6, 31] have been regarded as powerful approach toward general artificial intelligence [3]. Recently, there has been growing interest in training LLMs for reasoning tasks, leading to significant improvements on math and code benchmarks as shown by OpenAI-o1 and DeepSeek-R1 [12, 8]. However, such models often require extremely long input sequences due to the inclusion of detailed reasoning traces [8, 34, 23, 32, 33], which results in substantial GPU memory consumption during backpropagation (BP) when training reasoning models using either reinforcement learning (RL) or supervised finetuing (SFT). This considerable memory usage mainly stems from storing intermediate activations during the BP process. This work aims to provide an efficient solution to such memory issue. Background and related works. Due to rapid growth of research in this field, we provide non-exhaustive background overview on reasoning models and memory-efficient training methods. Training LLM on long reasoning traces. To incentivize long chain of thought (CoT) [28] reasoning capability, RL is one of most popular approaches [12, 8]. We also refer to, e.g., [7, 24, 21, 34, 18, Corresponding Author Preprint. Under review. 36, 10, 27], for replicating the \"aha moment\" of reasoning described in DeepSeek-R1. The core step of RL for incentivizing reasoning ability is to use rule-based reward, e.g., the correct answer of the math question, to provide training signal towards fitting the correctly generated answer by the model itself with reasoning traces. It has been observed that as the RL training progresses, the sequence length will be increased [8]. Apart from RL techniques, it has been shown that the simple SFT training pipeline with reasoning data distilled from strong reasoning models such as R1 can also incentivize reasoning ability of LLMs; see, e.g., [23, 32, 15, 29]. The length of SFT reasoning traces varies widely, ranging from 8k to 32k, and in some cases exceeding 100k. Training models on these long traces requires significant memory cost in storing activation values. The essential roles of RL and SFT for incentivizing reasoning ability are still under active discussion [35]. Memory-efficient training methods. Existing memory-efficient solution for training LLMs mainly focus on designing parameter-efficient or low-dimensional version of Adam [13, 19], significantly reducing the memory usage of storing the gradient and optimizer states. Typical algorithms include LoRA [9] and its variants, e.g., [5, 17], GaLore [37], BAdam [22], etc. However, in the context of training reasoning models with long reasoning sequences, the dominant source of memory consumption arises from the BP process for storing intermediate activations. Gradient checkpointing technique [4] reduces the memory cost by recomputing the activation during the backward process. However, it still stores the full activation of the reforwarded layer and the full logits of the output layer; see Figure 1 for an illustration. MsT [20] reduces the memory cost of SFT logits by iteratively processing mini-sequence. However, it still requires the full storage of layer activations during reforward and does not apply to RL objectives. Sequence parallel approaches increase the maximum sequence length by partitioning sequence across GPUs [16, 11, 14], which requires access of multiple GPUs. Main contributions. Motivated by the above observations, this work aims to address the memory issue occurred during the BP process. Our main contributions are summarized below. (C.1) We propose memory-efficient and exact backpropagation algorithm called StreamBP, which significantly reduces memory usage of storing the intermediate activation values when training LLMs on ultra-long sequences, e.g., training reasoning models. StreamBP is based on linear decomposition of the chain rule and involves nontrivial and intricate developments for the language modeling head and transformer layers. As result, StreamBP is compatible with common training objectives, including SFT, GRPO, and DPO. Moreover, by leveraging the causal structure of LLM, StreamBP is able to save computational FLOPs compared to the standard BP, achieving faster BP speed than the gradient checkpointing baseline. To support multi-GPU training, we also develop distributed implementation of StreamBP with special attention to gradient and parameter communication, significantly improving training efficiency and broadening its applicability. (C.2) Empirically, we measure the maximum sequence length and time cost of StreamBP under both single GPU and distribuetd training settings. Specifically, we measure the memory and time cost of BP in Section 4.1. StreamBP substantially scales up the maximum sequence length by 2.8 5.5 compared to the gradient checkpointing baseline across different model scales while achieving comparable or even less BP time. It is worth mentioning that StreamBPs sequence length scaling ability can be directly transferred to batch size scaling for accelerating the training speed, as its memory cost can be linearly related to the sequence length. In Section 4.2, we verify the effectiveness of StreamBP under various training objectives, showing that it significantly increases maximum sequence length under the objective of SFT, GRPO, and DPO. In Section 4.3, we verify the effectiveness of StreamBP under Deepspeed ZeRO-2 distributed training scheme, where StreamBP achieves 5 5.6 larger sequence length than gradient checkpointing."
        },
        {
            "title": "2 Preliminary: Peak Memory Cost during Backpropagation",
            "content": "As concrete example, we utilize PyTorch CUDA memory snapshot tool to record detailed GPU memory usage across 2 forward and backward processes of Qwen 3-4B model under the sequence length of 8192, where the gradient checkpointing technique is applied. The result is shown in Figure 1. We exclude the memory cost of optimizer states and focus solely on the BP process. Peak memory cost. The peak memory cost occurs at the end of the 2nd forward pass. Apart from the 16GB allocated for BF16 parameters and gradients, approximately 14GB is used for storing 2 Figure 1: Memory profile during backpropagation of Qwen 3-4B with gradient checkpointing, visualized using PyTorchs memory profiler. Sequence length is set to 8192. Optimizer states are excluded as we focus on the BP process. intermediate computations. This includes BF16 checkpointed layer inputs, BF16 logits, FP32 logits, and the gradient of FP32 logits. Since the logits is of dimension sequence length (T ) vocabulary size (C), their memory cost is independent of the model size and is subject to of the model class. Second peak memory cost. The 2nd memory peak happens at the start of the 2nd backward, where the model reforward the checkpointed inputs of the last transformer layer to compute layer activations. The activation values will be temporarily stored for computing the gradient of the layers parameter and inputs. We remark that as the model size grows up, the 2nd peak memory will increase and become closer to the peak memory. We provide more detailed explanation of the memory profile in Appendix C."
        },
        {
            "title": "3 Memory-Efficient Exact Stream Backpropagation",
            "content": "In this section, we introduce the design of the proposed algorithm, which computes exact gradient with reduced memory cost and floating point operations (FLOPs) during the BP process. 3.1 Main Idea Consider transformation happened during the models forward pass: fW (Zin) = Zout, where is the weight associated with the transformation, fW () can be any mapping inside the model, e.g., transformer layer or the language modeling head. By chain rule, the gradient of weight (cid:16) vec(Zout) vec(W ) (cid:17) L vec(W ) = vec(Zout) , where is the loss, vec() is the vectorized operator, are given by and vec(Zout)/vec(W ) denotes the Jacobian matrix. During BP, when the gradient L/vec(Zout) is ready, gradient checkpointing method will reforward Zin through fW (), and then calculate and store all the intermediate activations that are required for computing vec(Zout)/vec(W ) and vec(Zout)/vec(Zin), where vec(Zout)/vec(Zin) will be used for calculating L/vec(Zout) for the preceding layer. As shown in Figure 1, the memory cost of storing these intermediate activation values can be huge. To reduce the memory cost of these intermediate values, we introduce the stream backpropagation (StreamBP). Let vec(Zout) = [vec(Z (1) out )] be any partition of vec(Zout). StreamBP is based on the following linear decomposition: out ), . . . , vec(Z (D) out ), vec(Z (2) vec(W ) = (cid:18) vec(Zout) vec(W ) (cid:19) vec(Zout) = (cid:33) (cid:32) (cid:88) i=1 vec(Z (i) out ) vec(W ) vec(Z (i) out ) . (1) 3 By strategically partitioning vec(Zout), storing the intermediate activations required for computing vec(Z (i) out )/vec(W ) can be significantly cheaper than storing those required for calculating vec(Zout)/vec(W ), and is often proportional to the partitioned chunk size. Motivated by this fact, StreamBP sequentially computes the decomposed components in (1) across all the partitions, and accumulates them in running sum, yielding the exact gradient. We refer to Appendix A.1 for quick grasp of how StreamBP can significantly reduce memory cost using linear transformation example. Next, we elaborate on applying StreamBP to concrete transformer LLM, which necessitates nontrivial and intricate developments. 3.2 StreamBP for Transformer LLMs In this section, we apply StreamBP to significantly reduce the memory cost consumed by language modeling head and transformer layer during BP. Additionally, we will discuss that StreamBP requires even less computational FLOPs compared to the standard BP with gradient checkpointing, enabling potential time acceleration over standard approaches. 3.2.1 StreamBP for Language Modeling Head: SFT, GRPO, and DPO The language modeling head performs the following linear transformation: HWlm_head = logits. Here, Wlm_head RdC is the weight of language modeling head, RT is the hidden states output of the last transformer layer. The logits RT will be used to compute the objective function. C, d, are the vocabulary size, hidden dimension, and sequence length, respectively. As shown in Figure 1, the logits and its gradient give rise to huge memory consumption, due to the large vocabulary size and sequence length. In the following, we analyze one-by-one how the memory of logits can be significantly reduced using StreamBP in the regime of SFT, GRPO, and DPO. Supervised finetuning (SFT). The (un-normalized) objective of SFT is given by LSFT(logits, ) := (cid:88)T 1 t=1 log softmax(logitst,:)Yt, (2) where RT 1 is the label vector. Importantly, each positions logits contribute to the objective independently. To perform StreamBP, the logits and label are evenly partitioned into chunks across the sequence dimension, i.e., {(logits(i), (i))i = 1, . . . , D} with logits(i) R((T 1)/D)C. Then, we sequentially accumulate the gradient across all partitions = 1, . . . , as glm_head += LSFT(logits(i), (i)) Wlm_head , gH += LSFT(logits(i), (i)) , (3) where glm_head and gH are initialized from zero. The operator += denotes the in-place summation. logits(i) and its gradient will be cleaned from memory once they have been used in (3). After the accumulation across all partitions, glm_head and gH will be the exact gradient of the Wlm_head and H, respectively. During this computation, StreamBP only stores logits(i) and its gradient sequentially for all i, which only costs 1/D memory compared to the original approach. Group relative policy optimization (GRPO). The objective of GRPO is given by LGRPO(logits) := (cid:34) j=1πold(q)] ˆAj,t, clip (cid:18) πθ(j, t) πold(j, t) , 1 ϵ, 1 + ϵ β log (cid:19) (cid:27) ˆAj,t 1 (cid:88) To(cid:88) (cid:18) 1 To j=1 t=1 [qDq,{oj }G (cid:26) πθ(j, t) πold(j, t) min (4) (cid:35) . (cid:19) πθ(j, t) πref(j, t) (j,t) For compact presentation, we omit the compensation term in GRPOs KL divergence without loss of generality. The detailed definition of notation is in Appendix A.2. Here, logits {logitsπθ } contains logits generated by target policy, old policy, and reference policy, respectively, with logitsπ RGToC. The policys output is determined by logits, i.e., , logitsπold , logitsπref π(j, t) := π(oj,tq, oj,<t) = softmax(logitsπ,j,t,:)oj,t. 4 Note that each (j, t) in (4) contributes to the objective independently and only depends on logitsπ,j,t,:, which enables us to perform StreamBP along the sequence dimension. Specifically, let us partition the logits along the sequence dimension as {logits(i) := {logits(i) }i = 1, . . . , D} πθ with logits(i) π RG(To/D)C. Define the objective of the sequence partition as , logits(i) πold , logits(i) πref L(i) GRPO(logits(i)) := [qDq,{oj }G j=1πold(q)] (cid:20) 1 (cid:88)G j=1 (cid:88)"
        },
        {
            "title": "1\nTo",
            "content": "tTi (cid:21) (j, t) , (5) where Ti := {(i 1) To LGRPO(logits) = (cid:80)D following accumulation for = 1, . . . , D: < To i=1 L(i) Z} denotes the sequence range of the partition. We have GRPO(logits(i)). Then, similar to SFT, StreamBP sequentially performs the glm_head += L(i) GRPO(logits(i)) Wlm_head , gH += L(i) GRPO(logits(i)) . (6) Direct preference optimization (DPO). The objective of DPO is given by (cid:20) LDPO(logits) := (cid:18) log σ β (cid:88)T t=1 (cid:18) log πθ(yw,tx, yw,<t) πref(yw,tx, yw,<t) log πθ(yl,tx, yl,<t) πref(yl,tx, yl,<t) (cid:19) (cid:19)(cid:21) . (7) ℓ(t) } with logitsπ RT C, σ() is the sigmoid function. is the Here, logits := {logitsπθ sequence length of response y. To ease expression, we assume yw and yl have the same length without loss of generality. The policys output is determined by logits: , logitsπref π(ytx, y<t) = softmax(logitst,:)yt. Unlike SFT and GRPO, DPOs objective cannot be divided into summation of losses on individual partitions, due to the existence of the non-linear log-sigmoid transformation. Fortunately, its gradient retains separable structure: LDPO = E(x,yw,yl)D (cid:20)(cid:16) 1 σ (cid:16) β (cid:88)T t=1 (cid:17)(cid:17) β ℓ(t) (cid:88)T t=1 (cid:21) . ℓ(t) (8) StreamBP partitions logits across the sequence dimension {logits(i) := {logits(i) πθ 1, . . . , D} with logits(i) tions for = 1, . . . , D: }i = π R(T /D)C. Based on the partition, it performs the following accumula- , logits(i) πref ℓ += (cid:88) tTi ℓ(t), glm_head += β (cid:88) ℓ(t) Wlm_head , tTi gH += β (cid:88) ℓ(t) , tTi (9) where Ti := {(i 1) performs the following in-place correction to compute the exact gradient: Z}. After finishing the above accumulation, StreamBP < glm_head (σ(βℓ) 1)glm_head, gH (σ(βℓ) 1)gH . (10) 3.2.2 StreamBP for Transformer Layers: Attention and MLP We now apply StreamBP to the transformer layer. To ease presentation, we disregard components such as normalization layer, multi-head mechanism, and residual connection without loss of generality. transformer layer consists of two consecutive transformations of attention and MLP: = fattn(Hin), Hout = fMLP(O), (11) where Hin RT and Hout RT are the input and output of the transformer layer, respectively. The two transformations are given by Attention: = HinWq, = HinWk, = HinWv, = QK RT , = softmax(S, ) RT , = RT d. Q, K, RT MLP: Hup = OWup RT dup , Hgate = OWgate RT dup Hout = (σ(Hgate) Hup) Wdown RT d. (12) 5 Figure 2: StreamBP for transformer layer (best view in color). The stored activations of StreamBP is highlighted in orange. Unlike the gradient checkpointing approach that reforward Hin to compute all the activatioins required for the backward of Hout, StreamBP computes the activations only for one partition of (i) out at time, which reduces the memory cost by large margin. Computing Hout/Hin requires storing Q, K, V, M, O, Hup, Hgate, Hout, where RT is the attention mask, which is required when using mini-batch training or techniques such as sliding window attention. The storage of and can be avoided using flash attentions tiling approach. Define the partition of quantity RT along the sequence dimension as {H (i)i = 1, . . . , D} with (i) R(T /D)d. Let (:i) R(iT /D)d be the concatenation of {H (j)}i j=1 along the sequence dimension. StreamBP for transformer layer is built upon the following observation: Property 3.1 The computation of (i) out /W only depends on O(i), Q(i), (:i), and (:i). To justify the above property, StreamBP sequentially performs the following partitioned attention and MLP for each chunk i: Partitioned Attention: Q(i) = (i) (:i) = (:i) in Wq, (:i) = (:i) in Wk, in Wv S(i) = Q(i)K (:i) , (i) = softmax(S(i), (i)), O(i) = (i)V (:i) Partitioned MLP: (i) up = O(i)Wup, (i) σ(H (i) gate) (i) up out = (cid:16) (i) gate = O(i)Wgate (cid:17) Wdown R(T /D)d. (13) When the partitioned reforward finished, the activation for computing vec(H (i) in memory. Then, we accumulate the gradient for = 1, . . . , as out )/vec(W ) is stored vec(gW ) += vec(H (i) out ) vec(W ) vec(H (i) out ) , vec(gHin ) += vec(H (i) out ) vec(Hin) vec(H (i) out ) . (14) When the accumulation is finished, gW and gHin become the exact gradient of weight matrices := {Wq, Wk, Wv, Wup, Wgate, Wdown} and Hin, respectively. Note that (:i) and (:i) are needed for each partitioned forward. Hence, we compute and at once and cache it during StreamBP of the current layer. We remark that the partitioned attention is compatible with the flash attention. We illustrate the StreamBP for transformer layer in Figure 2. Memory efficiency of StreamBP. StreamBP only needs to store the following activation values: Q(i), K, , (i), O(i), (i) out . Note that when grouped query attention [2] is used with group size G, and only costs 1/G memory of Q. Consequently, StreamBP costs approximately 1/D memory for activation value compared to the standard BP. gate, and (i) up , (i) Computational efficiency of StreamBP. For long sequence training, the most computational expensive operation involved in transformer layer is the calculation of the pre-attention score S. StreamBP reduces the FLOPs of the operation by approximately half. Specifically, the standard implementation = QK costs FLOPs of 2T 2d2, while StreamBP only costs FLOPs of (1+D)T 2d2 for computing 6 (a) Qwen 3-8B (b) Qwen 3-14B (c) Qwen 3-32B (LoRA Grad.) Figure 3: Peak BP memory cost measurement of Qwen 3-8B, 14B, and 32B models under different sequence lengths. Under the 80GB memory limit, StreamBP scales the maximum sequence length by 2.8 5.5 larger compared to gradient checkpointing. Sequence length Baseline w/o ckpt Baseline w/ ckpt MsT StreamBP Acceleration over ckpt 6k 2.0 2.5 2.6 2.6 24k 24.3 25.2 21.2 4.4% 0.4% 6.0% 7.4% 10.5% 10.9% 12.9% 15k 10.8 11.2 10. 21k 19.3 20.3 17.2 18k 14.7 15.2 13.2 12k 7.4 7.6 7.0 9k 4.7 4.9 4.7 27k 31.8 25.8 Table 1: BP Time cost (in seconds) under different sequence lengths. The result is based on Qwen 3-4B model and is averaged over 50 independent trials. The acceleration of StreamBP becomes more apparent as the sequence length grows up, corroborating our analysis in Section 3.2.2. S(i) = Q(i)K (:i) across partitions. The FLOPs reduction is since that StreamBP utilizes the causal structure of language models, which uses (:i) rather than in the computation of S(i). For all the other operations, StreamBP across all partitions shares the same FLOPs as the standard BP with checkpointing. Note that and are only computed once and get cached. HBM overhead of StreamBP. Each partitioned gradient calculation in (1) requires loading model weight from high bandwidth memory (HBM) to register for computation, which induces additional overhead compared to the standard BP. Meanwhile, StreamBP reduces the HBM throughput of attention mask by approaximately half. The overhead directly depends on the number of partitions D. In Section 4.4, we empirically study how affects the BP time and memory cost. 3.3 Distributed StreamBP Although StreamBP is naturally compatible with modern distributed training techniques such as distributed data parallel (DDP) and Deepspeed ZeRO [25], directly applying these techniques to StreamBP is inefficient, due to redundant communications of gradients and parameters. To this end, we also develop distributed StreamBP. The major designs contain gradient communication and parameter communication. We put the detailed communication designs and efficiency analysis of distributed StreamBP is in Appendix A.3. and empirically study its efficiency in Section 4.3."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we evaluate StreamBP based on the Qwen 3 model series. The result applies for any causal language models such as Llama, Mistral, and Gemma, since StreamBP is not restricted to certain model class. Our evaluation mainly consists of 3 parts, including 1) backpropogation cost; 2) training cost; and 3) distributed training. All the experiments are conducted using A800-80GB GPUs. The detailed setup is presented in Appendix D. 7 4B 8B 14B 32B Objective Method"
        },
        {
            "title": "DPO",
            "content": "Baseline w/o ckpt Baseline w/ ckpt StreamBP Baseline w/o ckpt Baseline w/ ckpt StreamBP Baseline w/o ckpt Baseline w/ ckpt StreamBP 7.0 28.5 200.0 0.9 8.5 18.2 3.1 18.7 57.2 7.1 36.5 247.0 1.0 12.1 27.5 3.5 32.2 100."
        },
        {
            "title": "Full",
            "content": "3.4 15.7 72."
        },
        {
            "title": "LoRA",
            "content": "5.1 30.6 142.4 0.7 9.5 16.5 2.5 25.5 60.9 2.9 23.0 84.6 0.4 6.9 10.1 1.4 18.5 36.9 0.4 5.9 16.3 0.2 4.4 7.8 Table 3: Maximum sequence length (in thousands) on single A800-80GB GPU. 4.1 Backpropagation Cost Measure Memory cost. We report the memory cost of StreamBP and baseline (i.e., standard BP) with/without gradient checkpointing in Figure 3. Given the 80GB memory limit, StreamBP is able to increase the maximum sequence length by 2.8-5.5 and 23.4-36.3 compared to baseline with/without gradient checkpointing, respectively. Importantly, the memory cost of StreamBP is linearly related to the sequence length, as StreamBP does not store the full attention mask and is compatible with flash attention. Thus, StreamBPs sequence length scaling can be directly transferred to batch size scaling for accelerating the training speed. Note that the ratio differs across different model sizes, which attributes to distinct configurations of hidden dimensions. Time cost. We compare the BP time cost of StreamBP with baseline approaches, and present the result in Table 1. The result shows that StreamBP consistently exhibits faster BP time compared to the gradient checkpointing baseline across wide range of sequence lengths, and beats the long-sequence training baseline MsT [20] more significantly. As the sequence length increases, the acceleration becomes more significant. The result aligns with our analysis in Section 3.2.2, where StreamBP reduces approximately half of computation in calculating attention scores, whose computational FLOPs is quadratically related to the sequence length. In Table 2, we show that StreamBPs memoryefficiency enables the usage of much larger batch size, which helps further accelerate the BP speed. Batch Size Baseline w/ ckpt MsT StreamBP 1 7.42 7.64 7.03 2 7.12 7.41 6.82 4 7.04 7.28 6.63 16 7.06 6. Table 2: Per-sample BP time cost of Qwen 3-4B model under different batch sizes. The sequence length is 9000. StreamBP achieves further acceleration by utilizing substantially larger batch size. 4.2 Training Cost Measure Figure 4: Memory cost comparison with MsT. Sequence length scaling. We present the maximum sequence length of StreamBP and baseline methods given single A800-80GB GPU in Table 3. For all the objectives, StreamBP significantly increases the maximum sequence length over baseline approaches, which justifies the efficiency of StreamBP in reducing the memory cost of transformer layer activations and logits. For SFT, even 32B model can achieve sequence length up to 16k. Note that the sequence length scaling ratio can be equally transferred to batch size scaling ratio for acceleration. For example, for the SFT of 8B model, StreamBP enables 72 15.7 4.5 larger batch size than the baseline with gradient checkpointing. Comparison with long-sequence training baseline. We compare StreamBP with the long-sequence SFT baseline MsT, and plot the peak memory of training Qwen 3-8B LoRA model in Figure 4. The result shows that StreamBP achieves approximately 1.7 larger sequence length than MsT. In terms of time-efficiency, StreamBP requires significantly less BP time compared to MsT, as shown in Table 1. The acceleration becomes more significant as the sequence length scales up. 8 3 # of GPUs Baseline w/ checkpoint 18.6 Distributed StreamBP 92.4 4 20.5 112.0 5 22.3 121.5 6 23.1 128.7 7 23.5 131. 8 23.7 131.8 Table 4: Maximum sequence length (in thousands) of Qwen 3-8B under ZeRO-2 training scheme. Sequence length Baseline w/ checkpoint Distributed StreamBP 6k 8.4 6.4 9k 9.6 8.3 12k 12.2 10. 15k 18.5 14.8 18k 22.9 17.2 21k 26.3 20.7 Table 5: Per-sample BP time cost (in seconds) of Qwen 3-8B under ZeRO-2 training scheme. 4.3 Efficiency under Distributed Training We measure the maximum sequence scaling and BP time under the Deepspeed ZeRO-2 SFT scheme, where the gradient and optimizer states are partitioned across different GPUs and each GPU processes its local data batch. The results are shown in Table 4 and Table 5. Compared to baseline with gradient checkpointing, distributed StreamBP scales to maximum sequence length approximately 5-5.6 larger and achieves noticeably faster BP speed. 4.4 Ablation Study and Additional Experiments Effect of partition size. We fix the partition size of transformer layer to be 1k, 2k, 5k, and /3, examining the memory and time cost across wide range of sequence lengths. The result is shown in Figure 5. Note that partition will not be performed if it is larger than the sequence length. When the sequence length is relatively small, e.g., less than 10k, the BP times under different partition sizes are close. However, as the sequence length scales up, using partition size that is too small introduces substantial overhead. The overhead attributes to the additional HBM throughput on loading model weight from HBM to register for computation and repetitive kernel launches. Fortunately, as shown in Figure 5b, large partition size only introduces marginal additional memory cost. Hence, for long sequence training, one can use relatively large partition size to maximize training efficiency. Additional experiments. We provide more experiments in Appendix and Appendix C.2. Here are the summarized results: 1) We design experiments to verify the correctness of StreamBP in terms of gradient calculation, which justifies the strict mathematical equivalence between StreamBP and the standard BP. 2) We present the sequence scaling using single RTX3090-24GB GPU. The result shows that StreamBP is able to scale up the maximum sequence length to 15k, which is about 4.4 larger compared to gradient checkpointing. 3) We present the memory profile of StreamBP in Figure 8. The profile demonstrates substantial memory reduction in logits and activation values during layer reforwarding, which is consistent with our analysis in Section 3. (a) Time v.s. sequence length. (b) Memory v.s. sequence length. Figure 5: Time and memory costs of StreamBP on Qwen 3-8B with varying partition sizes (ps)."
        },
        {
            "title": "5 Conclusion and Discussions on Limitations",
            "content": "In this work, we developed memory-efficient and exact BP method called StreamBP. Compared to gradient checkpointing baseline, StreamBP requires significantly less memory cost and enjoys faster BP time by leveraging the causal structure of LLMs. StreamBP can be used for long sequence training of any transformer LLMs, which finds wide applications such as training reasoning models. We also developed communication-efficient distributed StreamBP to support multi-GPU training. Limitations. Currently, StreamBP does not support MoE or multimodal models. Nonetheless, these can be addressed with simple implementation extensions, as the underlying principle remains unchanged. Additionally, StreamBPs partition size can have clear impact on BP time. This overhead can be mitigated by employing fused backward operator to reduce the HBM throughput. We leave these directions for future work."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 48954901, 2023. [3] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023. [4] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. [5] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs. Advances in Neural Information Processing Systems, 36, 2023. [6] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [7] Hugging Face. Open R1: fully open reproduction of deepseek-r1, January 2025. [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [9] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [10] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. [11] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509, 2023. [12] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [13] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 10 [14] Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems, 5:341353, 2023. [15] Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir Patil, Matei Zaharia, et al. Llms can easily learn to reason from demonstrations structure, not content, is what matters! arXiv preprint arXiv:2502.07374, 2025. [16] Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. Sequence parallelism: Long sequence training from system perspective. arXiv preprint arXiv:2105.13120, 2021. [17] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. In Forty-first International Conference on Machine Learning, 2024. [18] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [19] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations. [20] Cheng Luo, Jiawei Zhao, Zhuoming Chen, Beidi Chen, and Anima Anandkumar. Mini-sequence transformer: Optimizing intermediate memory for long sequences training. arXiv preprint arXiv:2407.15892, 2024. [21] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. DeepScaleR: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. [22] Qijun Luo, Hengxu Yu, and Xiao Li. BAdam: memory efficient full parameter optimization method for large language models. Advances in Neural Information Processing Systems, 37:2492624958, 2024. [23] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [24] Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero. https://github.com/Jiayi-Pan/TinyZero, 2025. Accessed: 2025-01-24. [25] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. Zero-offload: Democratizing billion-scale model training. arXiv preprint arXiv:2101.06840, 2021. [26] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020. [27] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025. [28] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [29] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-R1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. 11 [30] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-theart natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online, October 2020. Association for Computational Linguistics. [31] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [32] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. [33] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. [34] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [35] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. [36] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. [37] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection. arXiv preprint arXiv:2403.03507, 2024."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Preliminary: Peak Memory Cost during Backpropagation 3 Memory-Efficient Exact Stream Backpropagation"
        },
        {
            "title": "3.1 Main Idea .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.2 StreamBP for Transformer LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "3.2.1 StreamBP for Language Modeling Head: SFT, GRPO, and DPO . . . . . . 3.2.2 StreamBP for Transformer Layers: Attention and MLP . . . . . . . . . . ."
        },
        {
            "title": "3.3 Distributed StreamBP .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Experiments 4.1 Backpropagation Cost Measure . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Training Cost Measure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Efficiency under Distributed Training . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Ablation Study and Additional Experiments . . . . . . . . . . . . . . . . . . . . . 5 Conclusion and Discussions on Limitations Additional Details of Stream Backpropagation A.1 Linear Example of StreamBP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Notation of GRPO . . A.3 Distributed StreamBP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Experiments B.1 Correctness Verification of StreamBP . . . . . . . . . . . . . . . . . . . . . . . . B.2 Sequence Scaling on Single RTX3090-24GB . . . . . . . . . . . . . . . . . . . Analysis of Memory Profile C.1 Memory Profile Explanation of Gradient Checkpointing . . . . . . . . . . . . . . . C.2 Memory Profile of StreamBP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Experiment Setup 1 2 3 3 4 5 7 7 8 9 9 10 14 14 15 16 16 16 17 17"
        },
        {
            "title": "A Additional Details of Stream Backpropagation",
            "content": "A.1 Linear Example of StreamBP Given the input = [X1, X2, . . . , XD] RDm and weight matrices W1 Rmn, W2 Rnk. Consider the following two linear transformations: = [Y1, . . . , YD] := XW1 RDn = [Z1, . . . , ZD] := W2 RDk. For the ease of expression, define dM := objective L. The gradient of and weight matrices are given by as the gradient of quantity with respect to the dW1 = (dY ), The standard approach of calculating dW1 and dW2 requires storing the intermediate value and dY . Based on (1), the above expression can be written as dY = (dZ)W 2 , dW2 = (dZ). dYi = (dZi)W 2 , dW1 = (cid:88) i= (dYi), dW2 = (cid:88) i=1 (dZi). Hence, one can sequentially compute Yi and dYi, accumulate dZi, then drop the Yi and dYi. Compared to the standard gradient computation, this approach effectively reduces the memory cost of intermediate values to 1/D without introducing additional computational cost. In practice, to better utilize the parallel computation and reduce the HBM load, one can process in chunk-wise rather than sample-wise manner. Below, we demonstrate the memory efficiency of StreamBP with numerical experiment on the linear example. dYi and Experiment on linear example. We empirically measure the memory cost of StreamBP and compare it with standard BP, based on concrete linear example. Specifically, let R10616384, W1, W2 R1638416384, = (cid:80) j,k Zj,k. StreamBP partitions as {X (i)i = 1, , D} with (i) R(106/D)16384. We present the memory cost and time cost of standard BP and StreamBP in Table 6. Standard BP StreamBP = 20 = 50 = 100 = 200 = Peak memory (GB) Intermediate memory (GB) Time cost (seconds) 36.01 25.15 14.48 13.25 2.39 14.50 12.47 1.61 14.70 12.20 1.34 14.79 12.07 1.21 15. 11.99 1.13 18.42 Table 6: Memory and time cost of standard BP and StreamBP under different partition number. Compared to standard BP, StreamBP with = 20 reduces memory cost by 63.2% with almost no time overhead. As increases, the intermediate memory cost is further reduced. A.2 Notation of GRPO We restate the GRPOs objective below: LGRPO(logits) := (cid:34) j=1πold(q)] [qDq,{oj }G (cid:26) πθ(j, t) πold(j, t) min 1 (cid:88) To(cid:88) (cid:18) 1 To j=1 t=1 ˆAj,t, clip (cid:18) πθ(j, t) πold(j, t) , 1 ϵ, 1 + ϵ (cid:19) (cid:27) ˆAj,t β log (cid:35) (cid:19) . πθ(j, t) πref(j, t) (j,t) To ease presentation, in the above equation we omit the compensation term in GRPOs KL divergence term without loss of generality. StreamBP directly applies to GRPOs KL divergence term. Here, is the prompt sequence, Dq is the dataset of prompt, and oj is j-th response sequence with respect to q. and To are the group size and the length of response. For the ease of expression, we assume all the responses are of the same length without loss of generality. ˆAj,t is the estimated advantage. πθ, πold, and πref are the target policy, old policy and reference policy, respectively. 14 Figure 6: Design of distributed StreamBP. When backward through transformer layer, its parameter is gathered beforehand. During the StreamBP of the layer, no gradient or parameter communication is fired. When BP of the layer is finished, the gradient will be reduced across layers and the paramater will be sharded across GPUs. A.3 Distributed StreamBP StreamBP is naturally compatible with distributed training techniques such as Deepspeed ZeRO. However, to ensure the efficiency, the gradient communication and parameter communication needs to be customized. The design of distributed StreamBP is shown in Figure 6. Gradient communication design. Gradient communication happens in almost all distributed training. During the backward, when the local gradient buffer of all processes are ready, gradient averaging operation (i.e. all-reduce or reduce-scatter) is performed across all the processes. As shown in (1), parameters gradient in StreamBP is ready until the accumulation operation is finished. To avoid redundant gradient communication, the gradient averaging of distributed StreamBP is performed after the accumulation. In this sense, distributed StreamBP will have the same gradient communication cost as the standard BP. Parameter communication design. Parameter communication is required when using modelparallel. For instance, ZeRO-3 partitions each parameter evenly across different GPUs, and gather the parameter when the operators associated with it is called. By (1), StreamBP requires the access of unpartitioned parameter for times during the backward. Hence, naively implementing modelparallel for StreamBP costs (D 1) all-gather operations. To reduce the communication cost, distributed StreamBP caches the weight locally until the accumulation of its gradient is finished, avoiding inducing additional parameter communication cost compared to the standard BP. This design will introduce an additional memory cost of storing single transformer layer in each GPU. Efficiency of distributed StreamBP. The choice of batch size significantly affects the communication cost. For example, when using Deepspeed ZeRO-2, using batch size 1 with gradient accumulation step leads to approximately times heavier backward communication cost than using batch size with gradient accumulation step 1. StreamBP enables the usage of much larger batch size compared to the standard BP, which significantly reduces the communication cost. We remark ZeRO-2 reduces the above overhead by overlapping the communication and computation."
        },
        {
            "title": "B Additional Experiments",
            "content": "B.1 Correctness Verification of StreamBP We empirically measure the gradient difference of StreamBP and baseline methods to justify the correctness of our implementation. Importantly, the float point operations has the following associativity issue due to numerical precision: (a b) = (b c) (15) where denotes floating-point addition. Therefore, it is impossible for the gradient difference to be zero given the different computation order. To verify the correctness, we use the gradient computed by the pure FP32 BP of the standard BP as ground truth, denoted as base32. Then, we calculate its difference with the gradient obtained by pure BF16 BP of StreamBP and standard BP, respectively. Additionally, we include StreamBP run under FP32 precision, denoted as stream32, to verify its numerical equivalence to base32. Specifically, we define the absolute error and relative error as Erabs(g) = (cid:88) 1 gbase32 gi, Errel(g) = 1 (cid:88) gbase32 gbase32 gi + 1010 . (16) Table 7 reveals two key findings: 1) stream32s micro-scale deviations (Erabs 109) validate mathematical equivalence to base32, and 2) stream16 demonstrates 0.04% relative deviation from base16, indicating that StreamBP exhibits no precision loss compared to standard bfloat16 computation. Module lm_head Transformer layers base 2.56e-7 2.46e-5 Erabs stream16 stream32 base16 Errel stream16 2.64e-7 2.47e6.66e-9 1.75e-7 1.43% 6.63% 1.47% 6.59% stream32 0.04% 0.04% Table 7: Gradient precision analysis. Bolded values confirm StreamBPs numerical correctness in float32. Near-identical base16/stream16 pairs demonstrate StreamBPs precision preservation under bfloat16. Transformer layers aggregate self-attention, feedforward, and normalization gradients. B.2 Sequence Scaling on Single RTX3090-24GB We present the memory cost of training Qwen 3-8B LoRA model in Figure 7. Under the 24GB memory budget, StreamBP allows sequence scaling up to 15k, which is 4.4 larger than the gradient checkpointing baseline. Figure 7: Peak BP memory cost measurement of Qwen 3-8B with LoRA (rank=32) on single RTX3090-24GB GPU"
        },
        {
            "title": "C Analysis of Memory Profile",
            "content": "C.1 Memory Profile Explanation of Gradient Checkpointing We provide detailed explanation of Figure 1. First, the \"Model parameters\" part stores the BF16 model parameters and constantly occupy nearly 8GB memory. Then, during the 1st forward process, gradient checkpointing gradually stores the checkpointed layer inputs, and hence it gradually consumes more memory. At the end of the 1st forward and the beginning of the 1st backward, the FP32 logits and its gradient are computed, which suddenly consumes huge amount of memory due to the very large vocabulary size of Qwen 3. This huge memory occupation is released after calculating the gradient of lm_head (tied with embedding) as shown in the brown rectangle above the \"BF16 logits copy\". During the 1st backward, BP calculates and stores the gradients of all the parameters and the memory consumption continues to increase. One interesting phenomenon is that the memory usage of the checkpointed layer activations enclosed in the yellow triangle goes to decrease during the 1st backward process. This is because that the stored corresponding activations will be deleted once the current weights gradients are computed, as they are no longer needed. Another interesting observation is that there are some triangle-shaped bumps during the 1st backward, for which we draw subfigure to interpret during the 2nd backward process. We explicitly show the reforward process of gradient checkpointing, and show the reforwarded layer activations that will be deleted once the corresponding gradients are computed, yielding triangle-shaped bump. An additional remark on Figure 1 is in order. As shown in Figure 1, the current implementation of the \"Transformers\" package stores an additional \"BF16 logits copy\" and an additional tied embedding and \"lm_head\" gradient in the 2nd backward process, which may be optimized as they are not used. C.2 Memory Profile of StreamBP The memory profile of StreamBP is shown in Figure 8. Compared to gradient checkpointings profile in Figure 1, StreamBP reduces the peak memory greatly by partitioning the logits across the sequence dimension. The second peak memory in reforwarded activation values is also greatly reduced, which is now only marginally higher than the storage of the key and value states. Figure 8: Memory profile with StreamBP under the same settings as Figure 1. The partition size of logits and transformer layer are set to 100 and 500, respectively."
        },
        {
            "title": "D Experiment Setup",
            "content": "We implement our algorithm using Hugging Face Transformers library [30]. The detailed experiment setup is presented below. Backpropogation cost. We decouple the optimization, focusing purely on the time and memory cost during the BP. This result serves as the minimum requirement on training language model under given sequence length. In particular, under batch size 1, we measure the BP memory cost of Qwen 3-8B, 14B and 32B models using single A800-80GB GPU. For 32B model, we inject rank-32 LoRA adapters and only calculate the gradient of the adapters, as single A800 GPU cannot store the model and the full gradient simultaneously. We adopt the BF16 data type for storing model weight and gradient. The partition size of language modeling head is set to 100 for all the experiments. The partition size for transformer layer is set to 500 for maximum sequence length measurement, and is set to /3 for time measurement. Training cost. We measure the maximum sequence length of training 4B, 8B, 14B, and 32B models using the objective of SFT, GRPO, and DPO, respectively. Our implementation is built upon Hugging Face TRL library [26]. We adopt pure BF16 data type in full training, and mixed-precision scheme in LoRA model training. The LoRA rank is set to 32. Importantly, when using LoRA mode, there is no need to store the reference model in DPO and GRPO, as one can disable the adapter in training model to recover the reference model. The batch size is set to 1 except for GRPO, where the group size is set to 8. We also compare the memory cost of StreamBP with long-sequence training baseline method MsT to demonstrate the effectiveness of StreamBP. The partition size of language modeling head is set to 100 for all the experiments. The partition size for transformer layer is set to 500 for maximum sequence length measurement, and is set to /3 for time measurement. All the experiments are conducted in single A800-80GB GPU. Distributed training. Under the communication-efficient design in Appendix A.3, we measure the maximum sequence length and time cost of distributed StreamBP under Deepspeed ZeRO-2 training paradigm, where the gradient and optimizer states are partitioned across GPUs. Our evaluation is conducted using single-node server with 8 A800-80GB GPUs connected by NvLink."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong, Shenzhen"
    ]
}