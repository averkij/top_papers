{
    "paper_title": "Hardware and Software Platform Inference",
    "authors": [
        "Cheng Zhang",
        "Hanna Foerster",
        "Robert D. Mullins",
        "Yiren Zhao",
        "Ilia Shumailov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "It is now a common business practice to buy access to large language model (LLM) inference rather than self-host, because of significant upfront hardware infrastructure and energy costs. However, as a buyer, there is no mechanism to verify the authenticity of the advertised service including the serving hardware platform, e.g. that it is actually being served using an NVIDIA H100. Furthermore, there are reports suggesting that model providers may deliver models that differ slightly from the advertised ones, often to make them run on less expensive hardware. That way, a client pays premium for a capable model access on more expensive hardware, yet ends up being served by a (potentially less capable) cheaper model on cheaper hardware. In this paper we introduce \\textit{\\textbf{hardware and software platform inference (HSPI)}} -- a method for identifying the underlying \\GPU{} architecture and software stack of a (black-box) machine learning model solely based on its input-output behavior. Our method leverages the inherent differences of various \\GPU{} architectures and compilers to distinguish between different \\GPU{} types and software stacks. By analyzing the numerical patterns in the model's outputs, we propose a classification framework capable of accurately identifying the \\GPU{} used for model inference as well as the underlying software configuration. Our findings demonstrate the feasibility of inferring \\GPU{} type from black-box models. We evaluate HSPI against models served on different real hardware and find that in a white-box setting we can distinguish between different \\GPU{}s with between $83.9\\%$ and $100\\%$ accuracy. Even in a black-box setting we are able to achieve results that are up to three times higher than random guess accuracy."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 7 9 1 5 0 . 1 1 4 2 : r a"
        },
        {
            "title": "A PREPRINT",
            "content": "Cheng Zhang, Yiren Zhao Imperial College London Hanna Foerster, Robert D. Mullins University of Cambridge Ilia Shumailov Google DeepMind"
        },
        {
            "title": "ABSTRACT",
            "content": "It is now common business practice to buy access to large language model (LLM) inference rather than self-host, because of significant upfront hardware infrastructure and energy costs. However, as buyer, there is no mechanism to verify the authenticity of the advertised service including the serving hardware platform, e.g. that it is actually being served using an NVIDIA H100. Furthermore, there are reports suggesting that model providers may deliver models that differ slightly from the advertised ones, often to make them run on less expensive hardware. That way, client pays premium for capable model access on more expensive hardware, yet ends up being served by (potentially less capable) cheaper model on cheaper hardware. In this paper we introduce hardware and software platform inference (HSPI) method for identifying the underlying GPU architecture and software stack of (black-box) machine learning model solely based on its input-output behavior. Our method leverages the inherent differences of various GPU architectures and compilers to distinguish between different GPU types and software stacks. By analyzing the numerical patterns in the models outputs, we propose classification framework capable of accurately identifying the GPU used for model inference as well as the underlying software configuration. Our findings demonstrate the feasibility of inferring GPU type from black-box models. We evaluate HSPI against models served on different real hardware and find that in white-box setting we can distinguish between different GPUs with between 83.9% and 100% accuracy. Even in black-box setting we are able to achieve results that are up to three times higher than random guess accuracy."
        },
        {
            "title": "Introduction",
            "content": "The widespread adoption of large language models (LLMs) has transformed the technological landscape, integrating machine learning models across various sectors. However, deploying these powerful models often entails substantial upfront investments in specialized hardware infrastructure and energy, leading many businesses to opt for third-party LLM providers. This practice raises concerns about transparency and accountability, as buyers currently lack the means to verify the actual hardware used to serve the models they purchase. Moreover, reports have emerged suggesting that some providers may deploy models that deviate subtly from their advertised counterparts, potentially optimized for less expensive hardware to reduce costs2. difference in hardware could not only introduce performance differences in terms of run time and model accuracy but may also indicate other potential issues. malicious provider might employ poorer security measures, for example by running GPU without TEE present or deploying the GPUs at restricted geographical location that is different from the agreed one. There might also be cases of man-in-the-middle that is conning both the service provider and the client by using the service providers service for oneself and serving counterfeit GPU to client. Moreover, malicious provider could be after the clients prompts or data, leading to privacy concerns. Therefore, being able to identify serving hardware or software platform can serve as useful signal for variety of reasons. This paper introduces hardware and software platform inference (HSPI) for machine learning, novel problem formulation for identifying the underlying GPU architecture and potentially the software stack of (black-box) machine learning model solely by examining its input-output behavior. HSPI works by exploiting subtle differences in how Shared first authorship 2For example, here is provider discussing strategies of reducing costs in model serving including changing models appropriately for smaller hardware."
        },
        {
            "title": "Hardware and Software Platform Inference",
            "content": "A PREPRINT Figure 1: Overview of hardware and software platform inference (HSPI). HSPI aims to identify the underlying hardware and software platform of deep learning models. Engineered requests are sent to service provider and responses are collected. With only the responses, HSPI predicts information on the hardware and software supply chains of the service provider. different GPUs and software environments perform calculations, which result in unique subtle patterns in the models output. By analyzing these numerical patterns, our proposed classification framework can accurately discern the specific device employed for model inference. HSPI has significant implications for ensuring transparency and accountability. By enabling buyers to independently verify the hardware used by their providers, HSPI can help establish trust and prevent potential cost-saving measures that might compromise model performance. To this end, we introduce two methods: HSPI with Border Inputs (HSPI-BI) and HSPI with Logits Distributions (HSPI-LD). We demonstrate the efficacy of these techniques in HSPI under both white-box and black-box setups, extending across both vision and language tasks. Our work demonstrates the practicality of HSPI and examines its performance for various models running over diverse device families, showcasing the influence of system optimizations and variations within software and hardware supply chains. Overall, we make the following contributions: We define the Hardware and Software Platform Inference (HSPI), detailing the underlying assumptions. We introduce two methods: HSPI with Border Inputs (HSPI-BI) and HSPI with Logit Distributions (HSPI-LD), and show their near-perfect success rates in white-box settings distinguishing between quantization levels and high accuracy rates of between 83.9% and 100% for distinguishing between real hardware platforms. Our empirical findings also indicate success rates in certain black-box setups that are up to three times higher than random guess accuracy. We experiment with both emulated quantization and real GPU hardware. We describe the limitations of HSPI in whiteand black-box setups and provide discussion on their potential usage across software and hardware supply chains for transparency and ML governance."
        },
        {
            "title": "2 What makes HSPI possible?",
            "content": "In this section we discuss why it is possible to infer the hardware and software configuration of machine learning serving platform just from input-output pairs. While familiar to those in high-performance computing, we explain how varying software and hardware configurations can shift model into different Equivalence Classes, and how arithmetic ordering and optimizations can contribute to computational discrepancies. Equivalence Classes: Different hardware and software configurations give us various computational results. When the computational results stay the same and do not deviate between settings, we talk about them being in the same equivalence class (EQC). EQCs are used to group similar computational behaviors that yield consistent results under specific settings, such as quantization levels, GPU architectures, CUDA versions, and batch sizes. For instance, quantization can alter numerical precision and thereby shift model into different EQC, resulting in subtle variations in outputs. GPU architecture and batch size also affect precision consistency, as different hardware or parallelizing data processing methods introduce minor deviations in results. Schlögl et al. provide more granular analysis, examining EQCs layer by layer across models, identifying how architectural choices impact computational stability and precision deviations [Schlögl et al., 2023]. Factors Influencing Computational Deviations: Schlögl et al. describe several possible reasons for precision deviations which include arithmetic units using faster approximations, intermediate values being rounded to fit in"
        },
        {
            "title": "Hardware and Software Platform Inference",
            "content": "A PREPRINT registers of different sizes, transformations made during execution planning, and various tricks that optimize performance in ML toolboxes such as loop unrolling, constant folding, and arithmetic simplifications [Schlögl et al., 2021]. In follow-up paper they show that the order of aggregation and choice of convolution algorithm can largely affect computation results [Schlögl et al., 2023]. simple example for aggregation errors leading to varying results when we are rounding all decimals to integers is given by following equations: (100.4 + 0.4) + 0.5 = 101 + 0.5 = 102 100.4 + (0.4 + 0.5) = 100.4 + 1 = 101. (1) (2) Similar examples can be found in any quantization due to finite numerical precision and stochastic pipelining. Furthermore, various methods exist to optimize the calculation of convolutional layers. Convolution calculation can for example be generalized to matrix multiplications. The space of calculations can also be transformed to, e.g. the Fourier domain, which then allows convolutions to be replaced by multiplication. Varying implementations for convolutions include the direct loop, GEMM [NVIDIA, 2024], Winograd [Winograd, 1980] and Fourier transform [Smith, 1997]. Schlögl et al. find that the extent of deviations in computations is influenced by several factors within neural network architectures. Layers involving high multiplication counts per convolution tend to amplify deviations, as they accumulate rounding errors and precision loss. Additionally, data-preserving layer types, which retain the full spectrum of input data, contribute to increased deviations, while layers that aggregate data (e.g., pooling layers) help mitigate these errors. Precision choice also plays critical role: single-precision operations show higher likelihood of deviation than both double and half precision due to their reduced bit representation [Schlögl et al., 2023]."
        },
        {
            "title": "3 Related Work",
            "content": "This section reviews relevant prior literature and positions our work within it. Hardware Fingerprinting in ML: Schlögl et al. first identify unique hardware fingerprints, discovering that different hardware platforms are in different EQCs [Schlögl et al., 2021, Schlögl et al., 2021]. This lays the groundwork to our hardware fingerprinting methods for GPUs. They discuss boundary samples, which are inputs to model that output results at the decision boundary between two classes. In classification for example an input can lie at the boundary of being classified as cat or dog, depending on the hardware. They use an adaptation of the search algorithm iterative fast-gradient-sign-method (FGSM) also known as Projected Gradient Descent (PGD) for generating adversarial samples and differentiate between 4 CPU models. They develop their method to be half remote, as they argue that it is more realistic to assume that only one CPU micro-architecture can be accessed locally and instantly, while more latency would be involved in communicating with others. This is why boundary sample generation is split into local and remote phase, where the local phase only involves finding an input very close to boundary and the remote phase checks whether this sample flips the classification label only on one microarchitecture. However, this algorithm is inefficient due to the two phase setup and the remote phase has success rate of creating border sample of only 28.25% on CIFAR10. Furthermore, GPUs and black-box setting are not considered, i.e., it is assumed that the ML model that is being run is known. Additionally, the analyses contain only models in the visual domain. Model Inference Variability: Further investigations into convolutional neural networks (CNNs) by Schlögl et al. reveal that numerical deviations in calculations arise from variations in SIMD usage and convolution algorithms [Schlögl et al., 2023]. Their analysis serves as foundation for our research, providing insight into how these deviations can be exploited in model inference time. Model Training Variability: Additionally, related research also shows big variances in model training in terms of accuracy and training time. Even after excluding variations introduced through random seeds, training configurations, deep learning libraries, and hardware factors, an accuracy difference of up to 2.9% can be introduced through software implementation differences of deep learning [Pham et al., 2020]. Others formulate this problem as repeatability problem and underline the non-determinism in the whole training stack greatly impacting model performance and safety [Zhuang et al., 2022, Alahmari et al., 2020]. Exploitation of Floating Point Inaccuracies: Some other work has also emerged exploiting floating point inaccuracies, for example, against robustness verifiers of neural networks [Jia and Rinard, 2021, Zombori et al., 2021]. They point out that errors in floating point can be used against verifiers that do not consider deviations in floating point computations. While Zombori et al. suggest adding small perturbations to the weights as an adhoc mitigation, they warn against other attacks using these deviations. As such, Clifford et al. for instance show that deviations in how specific operations are calculated on different hardware platforms can enable locking models to certain hardware [Clifford et al., 2024]."
        },
        {
            "title": "Hardware and Software Platform Inference",
            "content": "A PREPRINT Black-box LLM Model Identification: Other works have focused on identifying the LLM model family from black-box setting with only access to output text through semantic analysis [Iourovitski et al., 2024, Pasquini et al., 2024, McGovern et al., 2024], variant where logit outputs are assumed [Yang and Wu, 2024] or more white-box setting where outputs from each layer can be accessed [Zeng et al., 2023, Zhang et al., 2024]. Accuracies for distinguishing between model families in the black-box setting have been 72% and 95% with as little as 8 interactions [Iourovitski et al., 2024, Pasquini et al., 2024] which shows that we can create more white-box setting to start our LLM hardware platform inference with little probing."
        },
        {
            "title": "4 Methodology",
            "content": "This section begins by outlining HSPI formally and listing the underlying assumptions. Subsequently, we introduce the concept of border inputs, which are inputs that are specifically designed to elicit divergent behavior across varying hardware and software configurations. Finally, we describe method for model differentiation that is achievable even with not-specifically-crafted data, by identifying deviations in floating-point distribution of model-returned logits. 4.1 Problem Formulation and Assumptions The implementation of model serving in practical settings differs in accessibility. Service providers like Google, OpenAI, or Anthropic keep the underlying model architecture undisclosed, and users simply send queries and receive responses through API calls. Most services, including OpenAI APIs, are capable of providing users with the model output probabilities (logits). In other scenarios, we have cloud providers, such as Azure and AWS, offering deployment of open-source models, like LLaMA [Dubey et al., 2024], where both the model weights and architectures are known. We thus evaluate the following two representative scenarios: White-box access to the model the deployed model is known and can be accessed locally. For example, the deployed model is available publicly e.g. LLaMA [Dubey et al., 2024] or Gemma [Team et al., 2024]. It is thus possible in an unrestricted way to send inputs and receive responses from the model locally; Black-box access only the deployed model can only be accessed via the input interface, where it returns the output probabilities. This is similar to the current serving practices of Google, OpenAI, or Anthropic. We also assume that it is possible to access different hardware platforms (H = {H0, H1..., HN 1}), where HSPI then tries to identify the hardware Ht that the model is deployed on. To the best of our knowledge, currently, all known hardware platforms suitable for model serving can be rented, making our approach feasible and realistic. As newer hardware devices get released, such as recent Groq [Gwennap, 2020] and Cerebras [Lie, 2022], they quickly become privately rentable through either cloud providers or directly from respective hardware vendors. We expect the list of platforms to grow and newer devices to eventually become accessible on-demand. It is important to note that slight performance and numerical variations may arise from the underlying software stack, even though the algorithm and hardware remain constant. For example, in context of machine learning GPU kernel fusion strategies and runtime scheduling can influence the EQC of the model. We also explicitly include these variations in our H. Consequently, HSPI can also be used to determine both the hardware platform and the software configuration, thereby identifying the combined hardware-software supply chain. 4.2 HSPI-BI: HSPI with Border Inputs We reintroduce the concept of boundary samples with the name border inputs. As explained by Schlögl et al. these are specially crafted inputs that are at the decision boundary between two or more output classes of model [Schlögl et al., 2023]. Similarly to Schlögl et al. we find that the idea of border inputs is similar to the idea of adversarial examples and we also modify PGD to create border inputs but formulate our loss function differently. Specifically, consider model which runs in two different hardware environments. These environments could for example differ in hardware device, quantization level, or batch size. We recognize that deploying models on alternative hardware environments or might lead to subtle divergences in logit outputs FH due to discrepancies in quantization level, the sequence of GPU arithmetic operations [Schlögl et al., 2023], or disparities in the core library implementations (eg. SDPA vs. FlashAttention). To find point of maximum logit discrepancy we can start changing an input such that the two logit outputs FH (X) and FH (X) become as different as possible, with the goal that ultimately the predicted class labels and differ. We can construct such loss by defining it as the sum of two cross-entropy loss terms, each comparing the output logits of one model with the target labels from the other model. Specifically, we define the loss Lpgd as the cross entropy loss between the output logits of FH and the predicted class"
        },
        {
            "title": "Hardware and Software Platform Inference",
            "content": "A PREPRINT label of FH , and the cross entropy loss between the output logits of FH and predicted class label of FH , i.e., Lpgd = L(FH (X), y) + L(FH (X), y). (3) The loss should grow as the difference between FH (X) and FH (X) increases. To encourage the models to predict different class labels, and y, we aim to maximize this loss. For this, we adjust the input by moving it slightly in the direction that maximizes the loss, as determined by the gradient. At each step, we also clamp the modified input within valid range to keep it close to the original input. Another approach to increasing the distance between FH (X) and FH (X) is to push FH closer to specific target class, while pushing FH further away from that same target class. We can achieve this by defining the loss as the difference between the cross entropy loss for FH (X) with respect to the target label yt, and the cross-entropy loss for FH (X) with the same target label. By adjusting the input in the direction of the negative gradient, the loss is minimized: L1-vs-1, targeted = L(FH (X), yt) L(FH (X), yt). (4) To generate inputs that are classified differently across various hardware environments, we can extend Equation (4). For all models on different hardware setups, we sum the cross-entropy loss between the target label and each models logits. Then, for the one model where we want distinct class, we subtract the loss with the target label: L1-vs-rest, targeted = (cid:88) kN {i} (L(FHi(X), yt) L(FH (X), yt)). (5) As the number of hardware environments increases, optimizing between models becomes more challenging. In such cases, we can apply Equation (3) iteratively across pairs of models to estimate the most likely hardware environment probabilistically. Furthermore, when we consider testing various border inputs in black box setting we do not necessarily need access to logits anymore. In order to determine the hardware environment probabilistically with set of border inputs only the output class is needed. 4.3 HSPI-LD: HSPI with Logits Distributions An alternative approach to executing the HSPI task involves developing classification model by using the distribution of output logits. The idea is that the logits of set of inputs should reveal characteristics of the hardware environment. The logit distribution is especially revealing if inputs are used that are very diverse in distribution of classes and closeness to class boundaries. To further emphasize these differences, we can convert the logits into binary representations, or more cost-efficiently, split the floating-point components (Figure 2). The theory is that models running on different hardware configurations may produce distinct bit patterns in their logits, with certain bits used more or less frequently depending on the hardware. Given access to all hardware configurations and input samples Xi , we can create classifier that learns to identify the environment (e.g., whether sample was processed by FH or FH ). Mathematically, this classifier can be defined as: (cid:26)1, 0, if = FH if = FH G(F (Xi)) = (6) . In practice, we use an SVM as the classifier and train this classifier on small calibration set of logits."
        },
        {
            "title": "5 Evaluation",
            "content": "In this section, we first describe the experiment setup in Section 5.1, and then present the results of white-box and black-box attacks in Section 5.2 and Section 5.3 respectively. 5.1 Experiment Setup We consider the white-box and black-box attack setups described in Section 4.1. We build the set of hardware under two distinct configurations: (1) Initially, we examine models in low-precision formats (quantization), popular technique integrated into widely-used platforms like HuggingFace [Wolf et al., 2020], TensorRT [NVIDIA, 2024], and TorchAO [Torchao, 2024]. This is because hardware devices typically feature specific arithmetic operators (e.g., INT4 for A100 and FP8 for H100), and we regard differentiating different low-precision formats as preliminary step before differentiating actual GPUs. (2) We then extend the experiments to setup close to real deployment scenarios, comparing actual GPUs to operate with various arithmetic configurations, different kernel implementations, and across varying device families."
        },
        {
            "title": "Hardware and Software Platform Inference",
            "content": "A PREPRINT Figure 2: Splitting an FP32 logit into three INT32 numbers. In case that rounding noise pollutes the bit distribution in FP32 logits, before training SVMs, for each logit, we extract the sign, exponent, and fraction, zero pad each component and view each as an integer. Vision Models We mainly use the image classification models from torchvision and fine-tune them on CIFAR10, including ResNet18, ResNet50 [He et al., 2016], VGG16 [Simonyan and Zisserman, 2014], EfficientNet-B0 [Tan and Le, 2019], DenseNet-121 [Fu et al., 2021], MobileNet-v2 [Sandler et al., 2018], MobileNet-v3-small, and MobileNetV3-large [Howard et al., 2019]. Detailed fine-tuning parameters and the fine-tuned accuracy on the test split of CIFAR10 can be found in Appendix A.1. We perform the experiments on standard floating-point formats (FP32, FP16, BF16) and low-precision formats first, then extend to actual GPUs. The low-precision formats include MXINT8 [Darvish Rouhani et al., 2020], FP8-E3M4 (noted as FP8-E3) [Shen et al., 2024], FP8-E4M3 (noted as FP8-E4), and dynamic INT8 (noted as INT8) [Kim et al., 2021]. We show in Appendix A.2 that all low-precision formats have negligible accuracy loss compared to the original model checkpoints. For actual GPUs, we include NVIDIA H100, A100, GeForce RTX2080 Ti, and Quadro RTX8000. Not all GPUs where used for all experiments due to different server locations and the difficulty of connecting all of them for the attack generation phase. For HSPI-BI method, we randomly sample images from CIFAR10 as initial border images, and apply the PGD method described in Section 4.2 to update the border images. The projection step refers to the operation of scaling and rounding pixel values to integers between 0 and 255. For HSPI-LD method, we use images of size 224x224 that we create through randomly sampling floating point values between 0 and 1 and then converting them into the valid pixel range of integers between 0 and 255. These random images perform better than images from datasets such as CIFAR10 as they trigger more diverse areas of the feature space and hence produce set of more distinct logits. To distinguish between logits with an SVM, we always use set of 10 or 25 images logits and since in our access model we test with exactly the same images, we report training accuracy for all results. Language Models We use the open-sourced instruction-tuned LLMs from HuggingFace, including DistillGPT2 [Sanh et al., 2019], LLaMA-3.1-8B [Dubey et al., 2024], QWen-2.5-1.5B/3B/7B [Yang et al., 2024], Phi-3.5-mini [Abdin et al., 2024], Gemma-2-2B/9B [Team et al., 2024], and Mistral-7B-v0.3 [Jiang et al., 2023]. Using the official chatting templates, these instruction-tuned models can be directly deployed to chatbot applications. Similar to vision models, we first run experiments of differentiating low-precision formats, then extend to actual GPUs. Since LLM quantization is more challenging than vision models, besides FP16 and BF16, we adopt two quantization methods that has been proven effective and integrated into HuggingFace: fine-grained dynamic INT8 quantization [Torchao, 2024], noted as INT8-FD, and half-quadratic quantization [Badri and Shaji, 2023], noted as HQQ-4bit. For actual GPUs, we consider NVIDIA A100, L40S, RTX A6000, and GeForce RTX3090. For HSPI-BI method, we use random texts as the initial border requests, and apply the PGD method to update the border requests. border request can be represented as list of vocabulary indexes (input IDs). Since input IDs are one-hot encoded, we need to ensure that the updated border requests are still valid input IDs at the start of each PGD step. To achieve this, we first update the one-hot encoded input IDs, then use argmax to find the index of the maximum value in the updated vector for each token. For HSPI-LD method, we send artificial requests to guide the model to generate random words. Figure 3 shows an example of the border requests and the generated responses. We append several random words to each request. The logits of generated tokens (response) plus the quantization level/GPU form pair of input and label to train an SVM machine. Since the vocabulary size varies across model families, we sample the generated tokens and logits."
        },
        {
            "title": "Hardware and Software Platform Inference",
            "content": "A PREPRINT Figure 3: Generating HSPI-LD samples using LLMs. We guide LLMs to generate random words. The logits are flattened to form an input vector for training hardware platform classifiers. FP16: FROG INT8: PLANE MXINT8: DEER FP16: PLANE FP16: PLANE FP8-E4: CAT FP16: FROG FP8-E3: CAT BF16: CAT INT8: PLANE MXINT8: BIRD BF16: DOG BF16: HORSE FP8-E4: CAT BF16: DEER FP8-E3: CAT Figure 4: Example border images of MobileNet-v3-Small generated by HSPI-BI. The predicted label changes when fed to the same model quantized to different number formats. The subcaption follows the format of model format : predicted label. GPUs FP32 BF16 FP16 MXINT8 FP8-E3 FP8-E4 RTX8000 vs H100 RTX8000 vs 2080Ti INT8 Table 1: Table showing success in creating border images for different GPUs in white-box with an inference time batch size of 1. 5.2 White-box Attacks In the white-box setup, as explained in Section 4.1, we have access to the target model and its internal states. We can generate model-specific border inputs (HSPI-BI) or train classifier on the output logits of the target model (HSPI-LD) to distinguish between different quantization levels or GPUs. Vision Models The HSPI-BI method always works to distinguish between all quantization levels in the white-box settings. Examples for border images can be found in Figure 4. The difference in logit values between quantization levels is visualized with the kernel density estimate in Figure 5. Extending to GPUs, the difference in logit bit distribution between RTX8000 and A100 can be seen in Figure 6. While for some GPUs the logit bit difference is quite significant, for others our tests show identical or too similar logit outputs, making it impossible to create border images. As can be seen in Table 1, for example, border images can be created between RTX8000 and H100, however not between RTX8000 and RTX2080Ti. Our tests also show that creating border images is simpler for larger batch sizes, but for consistency in Table 1, we focus on batch size 1. For instance, we found that RTX8000 vs H100 failed at batch size 1 but succeeded at 32 for FP8-E3 and FP8-E4. Section 6.1 discusses that further."
        },
        {
            "title": "Hardware and Software Platform Inference",
            "content": "A PREPRINT Figure 5: Kernel density estimate of logit distributions of different quantization classes on the classification of the same 5000 images for CIFAR10 with ResNet18, i.e., 50000 logits. Figure 6: histogram showing the difference in logit bit distribution for the classification of the same 5000 images for CIFAR10 with ResNet18, i.e., 50000 logits, between Nvidia Quadro RTX 8000 and NVIDIA A100. With HSPI-LD, we are able to distinguish between all quantization levels for the models ResNet18, ResNet50, VGG16, EfficientNet, DenseNet, and MobileNet-V2 with 100% accuracy when using an SVM with set of 10 images logits per sample. Depending on the model, slightly different random creations of images performed better than others to most effectively capture the distribution. When comparing across GPUs, we are able to distinguish the A100 well from the RTX8000 and RTX2080Ti with an accuracy of 100% for precisions FP32, BF16 and FP16. The difference in logits is also shown in Figure 6. However, we are not able to distinguish between the RTX8000 and RTX2080Ti, as there was no numerical difference between these two across all logits tested. In this way, we see the limitations to both methods in the case of two GPUs with similar architectures, where calculations are in the same EQC. Figure 7: Example border request of DistillGPT2 generated by HPI-BI. When the border request is sent to the same model checkpoint deployed in FP16 and BF16, we observe different responses."
        },
        {
            "title": "Hardware and Software Platform Inference",
            "content": "A PREPRINT Training Model FP16 BF16 INT8-FD HQQ-4bit Avg. Acc. LLaMA-3.1-8B QWen-2.5-3B Phi-3.5-mini 0.996 1 0.996 1 1 0.996 1 1 1 1 1 0.988 0.999 1 0.995 Table 2: By-class accuracy of white-box HSPI-LD on quantization levels for LLMs. HPI-LD is able to differentiate LLMs deployed in FP16, BF16, INT8-FD, and HQQ-4bit. The SVM classifier is trained using responses of 256 random word requests. Group GPUs Ariths. Kernels 1 2 3 A100, A6000 A100 A100 FP16 FP16, BF16 FP Plain Plain Plain, FlashAttn2 Acc. 1.00 1.00 1.00 Table 3: Initial white-box experiments on actual GPUs. With fixed CUDA version, we perform HSPI-LD on LLaMA3.1-8B. The trained SVM distinguishes GPUs, arithmetic modes, and even kernel implementations in Group 1, 2, and 3 respectively. Language Models We find that HSPI-BI does not work well with LLMs. With enough iterations, HSPI-BI can generate border requests that lead to different responses across quantization levels. Figure 7 illustrates an example of border requests for LLMs. However, we find that the PGD process is unstable due to the discontinuity in its projection. Moreover, limited GPU memory constrains our ability to scale experiments to larger LLMs and bigger request batch sizes. This is because the forward pass of the target model is part of the computation graph for gradient descent, requiring caching the intermediate activations of LLMs in memory for backward propagation. Figure 8: The difference of bit distribution between RTXA6000 and A100 (white-box HSPI-LD). We send the same 256 requests to QWen-2.5-3B deployed on RTXA6000 and A100 and compare the bit distribution of FP32 log probabilities generated by the model. Tokens and logits are sampled in the plot but the difference is still obvious. tilj denotes the log probability of i-th tokens j-th logit. On the contrary, HSPI-LD achieves remarkable success in both quantization experiments and actual GPU experiments. Table 2 shows the results of quantized LLaMA-3.1-8B, Phi-3.5-mini, and QWen-2.5-3B deployed in FP16, BF16, INT8-FD, and HQQ-4bit, with an accuracy over 99.5% for each model. When extending to actual GPUs, we first perform three initial experiments in Table 3 to verify that HSPI-LD can distinguish between different GPUs, arithmetic modes, and kernel implementations. We then conduct comprehensive experiment in Table 4, mixing all these factors. We treat this experiment as classification task with 18 unique labels. Remarkably, HSPI-LD achieves an overall accuracy of 83.9% (random guess accuracy = 5.6%). Note that HSPI-LD is suitable for language models given the size of LLMs. HSPI-LD only runs inference on the model, consuming much less memory. Besides, the collection of responses (logits) for each GPU is independent, without the need of across-node communication."
        },
        {
            "title": "Hardware and Software Platform Inference",
            "content": "A PREPRINT GPUs Ariths. Kernels Class Idx Acc. F1. Plain SDPA FlashAttn2 Plain SDPA FlashAttn2 Plain SDPA FlashAttn2 Plain SDPA FlashAttn2 Plain SDPA FlashAttn2 Plain SDPA FlashAttn FP16 BF16 FP16 BF16 FP16 BF A100 A6000 L40S Average 1 2 3 4 5 7 8 9 10 11 12 13 14 15 16 17 18 0.742 0.645 0.695 0.745 0.683 0. 1 1 1 1 1 1 0.680 0.656 0.688 0.705 0.656 0.674 1 1 1 1 1 0.688 0.644 0.664 0.689 0.622 0.636 1 1 1 1 1 1 0.839 0. Table 4: White-box experiments on actual GPUs. We perform white-box HSPI-LD on setup mixing GPUs, arithmetic modes, and kernel implementations. We treat this as classification task with 18 unique labels. By-class accuracy and F1 score are in the last two columns. HSPI-LD achieves an overall accuracy of 83.9% using only 256 requests (random guess accuracy = 5.6%). Method Training Model Test Model FP32 BF FP16 MXINT8 FP8-E3 FP8-E4 INT8 Avg. F1. HSPI-BI HSPI-LD VGG16 ResNet18 MobileNet-v2 VGG16 ResNet18 ResNet50 MobileNet-v2 EfficientNet DenseNet-121 Other models Other models Other models Other models Other models Other models Other models Other models Other models 0 0 0.235 0.394 0.332 1 0.026 0 0.102 0 0 0.345 1 1 1 0.8 1 0.167 0.25 0.218 1 0.986 1 0.8 1 0.996 0.234 0.293 0.167 0 0.318 0.056 0.342 0 0. 0.159 0.286 0.286 0.95 0.972 0.602 0.768 0.2 0.926 0.253 0.167 0.444 0.65 0.682 0.634 0.69 0.612 0. 0.218 0.286 0.444 0.2 0.446 0.642 0.498 0.592 0.638 0.147 0.206 0.345 0.599 0.677 0.562 0.561 0.486 0. Table 5: Transferability of HSPI-BI and HSPI-LD on quantized vision models. We consider set of models including VGG-16, ResNet-18, ResNet-50, MobileNet-V2, EfficientNet-B0, and DenseNet-121. Each row trains the SVM classifier on one model and evaluate the transferability in terms of F1-Score on the rest models. Random guess F1-score is 0.144. For HSPI-BI, all the experiments were run on RTXA6000 with 400 iterations, and HSPI-LD experiments were run on Nvidia Quadro RTX 8000. 5.3 Black-box Attacks In the black-box setup, as explained in Section 4.1, we do not have access to or the knowledge of the underlying model. For both HSPI-BI and HSPI-LD, we presuppose the availability of support set of models Fs, with the understanding that the target model Ft is excluded from this collection. Hence, we construct the border inputs (for HSPI-BI) or the classifier (for HSPI-LD) using Fs, expecting that their functionality will transfer to the unseen model Ft. Vision Models For HSPI-BI, we estimate the transferability of set of border images by training the images on one model and testing them on other unseen models. Table 5 contains three experiments following this setup. The border images trained on MobileNet-v2 achieve an average F1-score of 0.345 across 7 unseen models (random guess F1-score = 0.144). We also have the following observations:"
        },
        {
            "title": "Hardware and Software Platform Inference",
            "content": "A PREPRINT (a) Transferability vs batch size (b) Transferability vs num iters Figure 9: Transferability of border images trained on MobileNet-v3-Small. (a) The transferability is improved with larger batch sizes. (b) Given batch size (256 in the plot), overfitting happens if the border images are trained with too many iterations. If border images are trained in batches, larger batch size will improve the transferability across models. Figure 9a sweeps the batch size of border images trained on MobileNet-v3-Small and plots the resultant transferability. With the increasing batch sizes, the transferability is enhanced. However, we cannot further increase the batch size after 256 due to limited GPU memory. Overfitting happens if the border images are trained with too many iterations. Figure 9b presents the transferability of the same group of border images trained with various number of iterations. The decreasing transferability at 800 and 1200 iterations indicates that the border images overfit the training model. The border images trained on more complex model tend to have better transferability. For example, in Table 5, the border images trained on MobileNet-v2 has higher F1-Score than those trained on ResNet18 and VGG16. However, again, more complex models require more GPU memory during training, which leads to limited scalability of HSPI-BI. For HSPI-LD, we compare how well an SVM trained on one model performs on all other models found in Table 5. We can see that the transferability in detecting BF16 and FP16 is particularly high, while for other quantization levels such as FP32 and MXINT8 the transferability is very low. Furthermore, ResNet18, followed by DenseNet perform best as models with high transferability to other models. Language Models We only perform HSPI-LD in the black-box setup for language models as the HSPI-BI method is unstable. We train an SVM on set of training models Fs and test it on another set of unseen models Ft. The results of quantized models are shown in Table 6. The transferability measured with average accuracy is between 22.7% and 60.3% (random guess accuracy is 25%). We find that overfitting also occurs if the SVM is trained with too many iterations, and the transferability heavily depends on the choice of models in the training set. Ft f3 f5 f7 f5, f7 FP BF16 INT8-FD HQQ-4bit Avg. Acc. Avg. F1. 0.0 1.0 0.369 0.853 0.0 0.074 0.969 0.274 0.865 0.293 0.980 0.450 0.041 0.783 0.093 0. 0.227 0.538 0.603 0.594 0.108 0.487 0.545 0.582 Table 6: Transferability of black-box HPI-LD on quantized LLMs. We split set of LLMs, = {Gemma-2-2B, Gemma-2-9B, LLaMA-3.1-8B, Phi-3.5-mini, Mistral-7B-v0.3, QWen-2.5-1.5B, QWen-2.5-3B, QWen-2.5-7B}, into supportive (training) set Fs and test set Ft := Fs. The SVM classifier is trained using responses from Fs, and tested on Ft. Random guess accuracy is 25%. We use fi (i = 0 . . . , 7) to denote i-th model in F."
        },
        {
            "title": "6 Discussion",
            "content": "6.1 Limitations and Black-box Failure Cases We notice that not all software configurations have an effect on the final model performance, leaving computations in the same EQC. For example, we tested the HSPI-BI method on different CUDA versions, while keeping the hardware and"
        },
        {
            "title": "Hardware and Software Platform Inference",
            "content": "A PREPRINT the rest of the software stack fixed. ResNet18 model trained on the CIFAR10 and run on an RTX8000 GPU, resulted in no measurable differences, thereby HSPI-BI completely failed to distinguish between the CUDA versions. Similarly, not all hardware is distinguishable. For example, HSPI-BI failed to differentiate the RTX8000 and the RTX2080Ti (both have NVIDIA Compute Capability = 7.5 and similar number of Tensor Cores around 550). Furthermore, we see big differences in calculated logits between different inference time batch sizes. We find that the same batch size needs to be used to distinguish between hardware. This makes our methods more complicated. For example for HSPI-BI, some border images work across few batch sizes, but do not work for all. This means that we would need to compute border inputs for all most popular batch sizes between set of hardware platforms or at least enough to make probable guess on it. Similarly, we would need to collect different sets of logits for various batch sizes for HSPI-LD, considering that Anthropic recently released new APIs for batched request. This situation prompts inquiry into the nuances of software-hardware supply chains. For instance, the extent of floating-point deviations in logits varies significantly depending on the targeted supply chain. Identifying different quantization levels may be straightforward, but discerning subtle distinctions, such as those between FlashAttention V2 and V3, can present challenge. Minor differences or distinctions in the supply chain may pose challenges to both HSPI-BI and HSPI-LD techniques. Nonetheless, there is another argument that an empirically-guided design of the loss function in HSPI-BI, or enhanced feature engineering for HSPI-LD could improve the distinction of these subtle variations. This aspect warrants further investigation in future research. 6.2 Software Supply Chains The variations in the software supply chain can significantly influence the reproducibility of results, even with the same model and hardware. The particular software supply chain can be succinctly categorized into various runtimes, compilers, or implementations of high-level frameworks, such as PyTorch or Tensorflow. This highlights the need for careful version control and standardized compilation practices to mitigate potential discrepancies. 6.3 Hardware Supply Chains wide range of hardware is now available through various cloud platform providers, including GPUs from different vendors (NVIDIA, AMD, Intel) and specialized AI accelerators (Google TPUs, AWS Inferentia, Groq and Cerebras). It is also now popular for application developers to directly purchase model API access. For instance, emerging service providers like OpenRouter 3 operate as marketplaces for LLM API. They offer platform that aggregates LLM open model inference services from various service providers and even provide query blending, whereby queries are processed using mix of cost-effective and expensive models. Typically, it is not fully disclosed to the users how these models are served. Limited information on the hardware used by such service providers, make it difficult for users to trust the results. Various hardware can introduce slight performance differences and without hardware platform accountability, API calls could be served from provider implementing less secure or incompliant practices. The HSPI methods we have introduced solution to check the potential hardware supply chain used by these service providers. 6.4 ML Governance In the perfect HSPI scenario, our methods can help identify the exact components in models software and hardware supply chains as mentioned in Section 6.2 and Section 6.3. This would allow users to produce and validate the software and hardware bill of materials, as suggested by recent research [Arora et al., 2022]. By doing so, HSPI could become key tool in enabling ML governance framework of standards and principles to ensure that an ML system operates responsibly upon deployment. The proposed HSPI problem formulation, including Schlögl et al.s insights on EQCs, will help in shaping standards in different stages of an ML models life cycle from testing to deployment [Chandrasekaran et al., 2021]. Traceability and Accountability: In complex ML workflows, traceability of the exact software and hardware configurations used in model training and inference is essential for building trust and accountability. This would help in identifying the origin of specific model behavior differences, making reproduction of behavior easier. This traceability is not only crucial for debugging but also for audits, security, and mitigating risks from potential tampering or errors. Establishing Industry Standards: HSPI can drive the adoption of industry-wide standards for documenting and verifying hardware and software configurations in ML development. Standardized reporting of hardware architectures, library versions, and compilers would enable researchers and practitioners to replicate results reliably. This would also help in establishing ML quality control norms. 3https://openrouter.ai/"
        },
        {
            "title": "Hardware and Software Platform Inference",
            "content": "A PREPRINT Model Sharing and Deployment Reliability: Replicability through HSPI and newly established industry standards can further model sharing practices. Detailed information about the training and inference environment can reduce deployment issues related to hardware and software mismatches and can help users understand potential limitations or dependencies on specific hardware or software components. This will ultimately create an environment fostering confidence in using models developed by other people. 6.5 Robustness of HSPI Several mitigation strategies can be employed to make the inference of hardware and software platforms significantly more difficult. These strategies focus on disrupting the patterns and subtle variations exploited by HSPI. For logits, introducing random bit flips into the decision-making process can help mask the unique quantization patterns associated with specific hardware architectures. This technique adds layer of noise that obscures the underlying hardware fingerprints, while incurring minimal overheads associated with sampling random numbers. Similarly, for border inputs, adding random noise to the input data can disrupt the precise calculations that lead to divergent behavior across different hardware and software configurations. This noise injection makes it harder for attackers to identify the specific conditions that trigger variations in model outputs, yet can potentially come with utility degradation."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we introduce Hardware and Software Platform Inference (HSPI) method for identifying the underlying hardware and software stack solely based on the input-output behavior of machine learning models. Our approach leverages inherent quantization limitations and the arithmetic operation orders of different GPU architectures to distinguish between various GPU types and software stacks. By analyzing quantization patterns in the models outputs, our classification framework can accurately identify the specific GPU and software configuration used for model inference. Our findings demonstrate the feasibility of inferring this information from black-box models, underscoring its implications for ensuring transparency and accountability in the LLM market."
        },
        {
            "title": "References",
            "content": "Alex Schlögl, Nora Hofer, and Rainer Böhme. Causes and effects of unanticipated numerical deviations in neural network inference frameworks. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 5609556107. Curran Associates, Inc., 2023. Alexander Schlögl, Tobias Kupek, and Rainer Böhme. Forensicability of deep neural network inference pipelines. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 25152519, 2021. doi:10.1109/ICASSP39728.2021.9414301. NVIDIA. Deep learning performance: Matrix multiplication, 2024. URL https://docs.nvidia.com/ deeplearning/performance/dl-performance-matrix-multiplication/index.html. Accessed: 202410-30. S. Winograd. Arithmetic Complexity of Computations. SIAM Publications, Philadelphia, PA, 1980. Steven W. Smith. The scientist and engineers guide to digital signal processing. California Technical Publishing, USA, 1997. ISBN 0966017633. Alexander Schlögl, Tobias Kupek, and Rainer Böhme. iNNformant: Boundary samples as tell-tale watermarks. In Workshop on Information Hiding and Multimedia Security (IH&MMSec), pages 8186. ACM, 2021. Hung Viet Pham, Shangshu Qian, Jiannan Wang, Thibaud Lutellier, Jonathan Rosenthal, Lin Tan, Yaoliang Yu, and Nachiappan Nagappan. Problems and Opportunities in training deep learning software systems: An analysis of variance. In 2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE), pages 771783, 2020. Donglin Zhuang, Xingyao Zhang, Shuaiwen Leon Song, and Sara Hooker. Randomness in neural network training: Characterizing the impact of tooling. In Proceedings of Machine Learning and Systems, volume 4, pages 316336, Santa Clara, CA, USA, 2022. MLSys. Saeed Alahmari, Dmitry Goldgof, Peter Mouton, and Lawrence Hall. Challenges for the repeatability of deep learning models. IEEE Access, 8:211860211868, 2020. Kai Jia and Martin Rinard. Exploiting verified neural networks via floating point numerical error. In Static Analysis: 28th International Symposium, SAS 2021, Chicago, IL, USA, October 1719, 2021, Proceedings 28, pages 191205. Springer, 2021."
        },
        {
            "title": "Hardware and Software Platform Inference",
            "content": "A PREPRINT Dániel Zombori, Balázs Bánhelyi, Tibor Csendes, and Márk Jelasity. Fooling complete neural network verifier. In International Conference on Learning Representations (ICLR), 2021. Eleanor Clifford, Adhithya Saravanan, Harry Langford, Cheng Zhang, Yiren Zhao, Robert Mullins, Ilia Shumailov, and Jamie Hayes. Locking machine learning models into hardware. arXiv preprint arXiv:2405.20990, 2024. Dmitri Iourovitski, Sanat Sharma, and Rakshak Talwar. Hide and seek: Fingerprinting large language models with evolutionary learning. arXiv preprint arXiv:2408.02871, 2024. Dario Pasquini, Evgenios Kornaropoulos, and Giuseppe Ateniese. Llmmap: Fingerprinting for large language models. arXiv preprint arXiv:2407.15847, 2024. Hope McGovern, Rickard Stureborg, Yoshi Suhara, and Dimitris Alikaniotis. Your large language models are leaving fingerprints. arXiv preprint arXiv:2405.14057, 2024. Zhiguang Yang and Hanzhou Wu. fingerprint for large language models. arXiv preprint arXiv:2407.01235, 2024. Boyi Zeng, Chenghu Zhou, Xinbing Wang, and Zhouhan Lin. Huref: Human-readable fingerprint for large language models. arXiv preprint arXiv:2312.04828, 2023. Jie Zhang, Dongrui Liu, Chen Qian, Linfeng Zhang, Yong Liu, Yu Qiao, and Jing Shao. Reef: Representation encoding fingerprints for large language models. arXiv preprint arXiv:2410.14273, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Linley Gwennap. Groq rocks neural networks. Microprocessor Report, Tech. Rep., jan, 2020. Sean Lie. Cerebras architecture deep dive: First look inside the hw/sw co-design for deep learning: Cerebras systems. In 2022 IEEE Hot Chips 34 Symposium (HCS), pages 134. IEEE Computer Society, 2022. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-Art Natural Language Processing. pages 3845. Association for Computational Linguistics, October 2020. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6. NVIDIA. TensorRT Open Source Software, January 2024. URL https://github.com/NVIDIA/TensorRT. Torchao. torchao: PyTorch native quantization and sparsity for training and inference, October 2024. URL https/ /github.com/pytorch/torchao. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 61056114. PMLR, 2019. Yihui Fu, Jian Wu, Yanxin Hu, Mengtao Xing, and Lei Xie. Desnet: multi-channel network for simultaneous speech dereverberation, enhancement and separation. In 2021 IEEE Spoken Language Technology Workshop (SLT), pages 857864. IEEE, 2021. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 45104520, 2018. Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF international conference on computer vision, pages 13141324, 2019. Bita Darvish Rouhani, Daniel Lo, Ritchie Zhao, Ming Liu, Jeremy Fowers, Kalin Ovtcharov, Anna Vinogradsky, Sarah Massengill, Lita Yang, Ray Bittner, et al. Pushing the limits of narrow precision inferencing at cloud scale with microsoft floating point. Advances in neural information processing systems, 33:1027110281, 2020. Haihao Shen, Naveen Mellempudi, Xin He, Qun Gao, Chang Wang, and Mengni Wang. Efficient post-training quantization with fp8 formats. Proceedings of Machine Learning and Systems, 6:483498, 2024."
        },
        {
            "title": "Hardware and Software Platform Inference",
            "content": "A PREPRINT Sehoon Kim, Amir Gholami, Zhewei Yao, Michael Mahoney, and Kurt Keutzer. I-bert: Integer-only bert quantization. In International conference on machine learning, pages 55065518. PMLR, 2021. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. In NeurIPS EMC2 Workshop, 2019. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. AQ Jiang, Sablayrolles, Mensch, Bamford, DS Chaplot, de las Casas, Bressand, Lengyel, Lample, Saulnier, et al. Mistral 7b (2023). arXiv preprint arXiv:2310.06825, 2023. Hicham Badri and Appu Shaji. Half-quadratic quantization of large machine learning models, November 2023. URL https://mobiusml.github.io/hqq_blog/. Arushi Arora, Virginia Wright, and Christina Garman. Strengthening the security of operational technology: Understanding contemporary bill of materials. Journal of Critical Infrastructure Policy, 3(1):111135, 2022. Varun Chandrasekaran, Hengrui Jia, Anvith Thudi, Adelin Travers, Mohammad Yaghini, and Nicolas Papernot. Sok: Machine learning governance. arXiv preprint arXiv:2109.10870, 2021."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Fine-tuning Hyper-parameters of Vision Models The checkpoints of vision models in Section 5 are downloaded from torchvision 4. We fine-tune these models using the following hyper-parameters: Optimizer Learning Rate LR Scheduler Batch size Num Epochs SGD 1eLinear 128 3 A.2 Quantization and Test Accuracy of Vision Models We perform sanity check before the HSPI experiments in case the quantization breaks the models. Model FP32 BF16 FP16 MXINT8 FP8-E3 FP8-E4 INT VGG16 ResNet18 ResNet50 MobileNet-v2 MobileNet-v3-small MobileNet-v3-large EfficientNet-B0 DenseNet-121 0.882 0.937 0.965 0.936 0.914 0.946 0.957 0.962 0.882 0.937 0.965 0.936 0.914 0.946 0.954 0.961 0.881 0.937 0.965 0.936 0.914 0.946 0.956 0.962 0.879 0.936 0.963 0.929 0.909 0.943 0.954 0.954 0.876 0.931 0.962 0.928 0.907 0.937 0.945 0. 0.875 0.929 0.959 0.926 0.903 0.934 0.953 0.949 0.875 0.934 0.951 0.928 0.907 0.945 0.954 0.952 4https://pytorch.org/vision/stable/models.html#classification"
        }
    ],
    "affiliations": [
        "Imperial College London",
        "University of Cambridge",
        "Google DeepMind"
    ]
}