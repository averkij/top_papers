{
    "paper_title": "MMaDA: Multimodal Large Diffusion Language Models",
    "authors": [
        "Ling Yang",
        "Ye Tian",
        "Bowen Li",
        "Xinchen Zhang",
        "Ke Shen",
        "Yunhai Tong",
        "Mengdi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce MMaDA, a novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts a unified diffusion architecture with a shared probabilistic formulation and a modality-agnostic design, eliminating the need for modality-specific components. This architecture ensures seamless integration and processing across different data types. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning strategy that curates a unified CoT format across modalities. By aligning reasoning processes between textual and visual domains, this strategy facilitates cold-start training for the final reinforcement learning (RL) stage, thereby enhancing the model's ability to handle complex tasks from the outset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm specifically tailored for diffusion foundation models. Utilizing diversified reward modeling, UniGRPO unifies post-training across both reasoning and generation tasks, ensuring consistent performance improvements. Experimental results demonstrate that MMaDA-8B exhibits strong generalization capabilities as a unified multimodal foundation model. It surpasses powerful models like LLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in multimodal understanding, and excels over SDXL and Janus in text-to-image generation. These achievements highlight MMaDA's effectiveness in bridging the gap between pretraining and post-training within unified diffusion architectures, providing a comprehensive framework for future research and development. We open-source our code and trained models at: https://github.com/Gen-Verse/MMaDA"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 9 0 8 5 1 . 5 0 5 2 : r MMaDA: Multimodal Large Diffusion Language Models Ling Yang1,4, Ye Tian2, Bowen Li2, Xinchen Zhang3, Ke Shen4, Yunhai Tong2, Mengdi Wang1 1Princeton University 2Peking University 3Tsinghua University 4ByteDance Seed Equal Contribution"
        },
        {
            "title": "Abstract",
            "content": "We introduce MMaDA, novel class of multimodal diffusion foundation models designed to achieve superior performance across diverse domains such as textual reasoning, multimodal understanding, and text-to-image generation. The approach is distinguished by three key innovations: (i) MMaDA adopts unified diffusion architecture with shared probabilistic formulation and modalityagnostic design, eliminating the need for modality-specific components. This architecture ensures seamless integration and processing across different data types. (ii) We implement mixed long chain-of-thought (CoT) fine-tuning strategy that curates unified CoT format across modalities. By aligning reasoning processes between textual and visual domains, this strategy facilitates coldstart training for the final reinforcement learning (RL) stage, thereby enhancing the models ability to handle complex tasks from the outset. (iii) We propose UniGRPO, unified policy-gradientbased RL algorithm specifically tailored for diffusion foundation models. Utilizing diversified reward modeling, UniGRPO unifies post-training across both reasoning and generation tasks, ensuring consistent performance improvements. Experimental results demonstrate that MMaDA-8B exhibits strong generalization capabilities as unified multimodal foundation model. It surpasses powerful models like LLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in multimodal understanding, and excels over SDXL and Janus in text-to-image generation. These achievements highlight MMaDA effectiveness in bridging the gap between pretraining and post-training within unified diffusion architectures, providing comprehensive framework for future research and development. We open-source our code and trained models at: https://github.com/Gen-Verse/MMaDA Date: May 22, 2025 Correspondence: Ling Yang at yangling0818@163.com"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have revolutionized natural language processing (NLP) by achieving state-ofthe-art performance in diverse tasks, from text generation (e.g., ChatGPT [13]) to complex reasoning (e.g., DeepSeek-R1 [4]). Inspired by their success, the research community has extended LLMs to the multimodal domain, giving rise to multimodal large language models (MLLMs) or vision-language models (VLMs) [514], such as GPT-4 [15] and Gemini [12]. These models aim to provide unified framework for both understanding and generating across heterogeneous modalitiestext, images, and beyond. Early multimodal approaches combined language models with diffusion models [1619] to handle discrete (e.g., text) and continuous (e.g., image) modalities separately. Subsequent autoregressive (AR) methods simplified 1 Table 1 Specific design choices employed by different unified multimodal foundation model families, including their core loss functions. The next-token prediction loss is defined as LNTP = Exi [ log Pθ(xi x<i)], representing the standard negative log-likelihood of generating the next token xi conditioned on its preceding context x<i. The (cid:2)ϵ ϵθ(xt, t, c)2(cid:3), training objective for continuous diffusion models is given by LDiff-cont = Et,x0q(x0),ϵN (0,I),c where xt denotes the noised version of the original data x0 at timestep t, and is an optional conditioning signal. The model learns to predict the noise ϵ added to the data, thereby enabling the reconstruction of x0 through an iterative denoising process defined as xt1 = Fθ(xt, t). The discrete diffusion (masked token prediction) loss is given z(M ) by LDiff-disc = Ez , which measures the average negative log-likelihood of correctly iM log pθ(z predicting the original discrete tokens at positions masked by the set , given the visible context provided by the masked sequence z(M ) (cid:105) masked) 1 (cid:80) (cid:104) . masked Network Architecture Language Vision Pre-training Loss for Language Loss for Vision Sampling Language Vision Language Scheduler Vision Scheduler Post-Training Language CoT Language RL Vision CoT Vision RL Tasks Und. (with Reasoning) Image Gen. (with Reasoning) Text Gen. (with Reasoning) Representative Models AR (One Model) AR + Diffusion (Two Models) AR + Diffusion (One Model) Ours MMaDA (One Model) AR AR LNTP LNTP AR Diffusion LNTP LDiff-cont AR Diffusion LNTP LDiff-cont or LDiff-disc Diffusion Diffusion LDiff-disc LDiff-disc xi Pθ(xi x<i) xi Pθ(xi x<i) - - xi Pθ(xi x<i) xt1 = Fθ(xt, t) - DDPM [23] xi Pθ(xi x<i) xt1 = Fθ(xt, t) - DDPM [23]/Cosine [24] xt1 = Fθ(xt, t) xt1 = Fθ(xt, t) Semi-AR Remask [22] Cosine Remask [24] - - - DPO [25] () () () - - - - () () () - - - - () () () Mixed Long-CoT UniGRPO with Diversified RM Mixed Long-CoT UniGRPO with Diversified RM () () () Emu3 [25], Janus [13, 14] DreamLLM [16] Transfusion [21], Show-o [20] MMaDA (Ours) architectures by training single transformer with next-token prediction, unifying discrete and continuous generation in single model [814]. Another line of work leverages modality-specific training objectives within shared architecture: for example, Show-o [20] and Transfusion [21] combine autoregressive and diffusion modeling for modeling textual and visual semantics, respectively. Although recent advancements have explored diffusion-based architectures for global context modeling and parallel generation [20, 21], existing unified multimodal foundation models predominantly focus on model architecture design and pretraining strategies, leaving critical gap in the exploration of post-training methodologies, particularly in non-autoregressive settings. To address this gap, we systematically investigate the design space for unified multimodal diffusion foundation models, introducing novel framework that advances both architectural and training paradigms (a comprehensive comparison in table 1). This work bridges the gap between pretraining and post-training in unified multimodal diffusion models, offering holistic framework for future research in this emerging field. Our contributions can be summarized as: Unified Diffusion Foundation Architecture: We propose MMaDA, class of diffusion-based models that extend traditional generators into generalist task solvers via shared probabilistic formulation and modality-agnostic architecture. This design eliminates modality-specific components while maintaining strong performance across tasks. Mixed Long-CoT Post-Training: We introduce mixed long chain-of-thought (CoT) finetuning to enable cold-start training. By curating unified CoT format across tasks, we align reasoning processes between modalities (e.g., textual and visual), fostering cross-modal synergy and learning intermediate reasoning before final output generation. Unified Reinforcement Learning (UniGRPO): We develop unified diffusion-centric reinforcement learning 2 Figure 1 Qualitative comparison across three tasks (more results in appendix and appendix D.). algorithm (UniGRPO) tailored for multimodal generation. This approach leverages diversified reward modeling to enhance the models ability to perform complex reasoning and maintain factual consistency in generation. State-of-the-Art Performance: MMaDA achieves superior and balanced performance across three critical tasks: textual reasoning, multimodal understanding, and text-to-image generation. Notably, it outperforms both autoregressive and diffusion-based baselines in terms of accuracy, efficiency, and task adaptability."
        },
        {
            "title": "2.1 Pretraining with Unified Diffusion Architecture and Objective\nData Tokenization To establish a unified modeling framework capable of processing both textual and\nvisual data, we adopt a consistent discrete tokenization strategy across both modalities. This design enables\nthe model to operate under a single modeling objective, i.e., the prediction of discrete masked tokens. For text\ntokenization, we utilize the tokenizer from LLaDA [22], which serves as the backbone for our MMaDA model.\nFor image tokenization, we leverage the pretrained image quantizer adopted from Show-o [20], which is based\non the MAGVIT-v2 [24] architecture and converts raw image pixels into sequences of discrete semantic tokens.\nGiven an input image with dimensions H × W , the encoder generates a token map with dimensions H\n,\nf × W\nf\nwhere f represents the downsampling factor. In this implementation, we employ a downsampling factor of\nf = 16 and a codebook size of 8192. This configuration transforms a 512 × 512 pixel image into a sequence of\n32 × 32 = 1024 discrete tokens. The transformed discrete image tokens are used in both understanding and",
            "content": "3 Figure 2 An overview of MMaDA pipeline. generation modeling tasks. Unified Probabilistic Formulation for Pretraining Recent unified multimodal frameworks aim to integrate multiple modeling objectivessuch as autoregressive generation and diffusion-based denoisinginto single architecture for joint understanding and generation tasks [20, 21] (see preliminaries in appendix A.1). However, these approaches often introduce complex hybrid mechanisms that hinder model efficiency and In contrast, we propose streamlined framework that not only simplifies the architectural coherence. complexity but also introduces unified diffusion objective to model both visual and textual modalities under shared probabilistic formulation. By aligning the noise corruption and semantic recovery processes across modalities, we enable more effective cross-modal interactions during pretraining, facilitating seamless integration of heterogeneous data sources. Specifically, we formulate MMaDA as mask token predictor (for both image and text tokens), parametric model pθ(xt) that takes xt as input and predicts all masked tokens simultaneously. The model is trained with unified cross-entropy loss computed only on the masked image/text tokens: Lunify(θ) = Et,x0,xt (cid:34) 1 (cid:88) i=1 I[xi = [MASK]] log pθ(xi (cid:35) 0xt) , (1) where x0 is ground truth, the timestep is sampled uniformly from [0, 1], and xt is obtained by applying the forward diffusion process to x0. I[] denotes the indicator function to ensure that the loss is computed only over the masked tokens. Specific pretraining tasks are detailed in section 4."
        },
        {
            "title": "2.2 Post-Training with Mixed Long-CoT Finetuning\nCold Start Long-CoT Data Curation We investigate how CoT mechanisms [26] can enhance post-training\nfor our unified multimodal diffusion framework and observe their effectiveness in promoting cross-modal\nsynergies. To this end, we curate a compact dataset of long CoT trajectories across three core tasks: textual\nreasoning, multimodal reasoning, and text-to-image generation. This dataset enables stable post-training of\nour pretrained MMaDA model through the following principles:",
            "content": "Unified CoT Format: critical challenge in vision-language foundation models is the heterogeneity of output formats across tasks (e.g., text vs. image generation). We propose task-agnostic CoT format: <special_token> < reasoning_process > <special_token> < result > . The <reasoning_process> encodes step-by-step reasoning trajectories preceding the final output. This unified structure bridges modality-specific outputs and facilitates knowledge transfer between tasks. For instance, enhanced textual reasoning capabilities directly improve the realism of generated images by aligning semantic logic with visual synthesis. Diversity, Complexity, and Accuracy: We leverage open-source large language and vision-language models (LLM/VLMs) to generate diverse reasoning trajectories across tasks (in figure 2). To ensure quality, we employ state-of-the-art models as verifiers to filter out inaccurate or shallow reasoning, selecting only high-quality, long-form CoT samples. Unlike prior unified models that focus on generic understanding and generation, our MMaDA is explicitly designed for: 1. Reasoning-intensive tasks (e.g., mathematical problem-solving), and 2. World-knowledge-aware text-to-image generation, where factual consistency is critical. Mixed Long-CoT Finetuning Leveraging our unified diffusion architecture and probabilistic formulation, we develop mixed-task long-CoT finetuning strategy to jointly optimize the model across heterogeneous tasks. This approach not only enhances task-specific capabilities but also creates strong initialization for subsequent reinforcement learning (RL) stages. The training process follows these steps: 1. Prompt Preservation and Token Masking: We retain the original prompt p0 and independently mask tokens in the result (x0), denoted as rt. 2. Joint Input and Loss Computation: The concatenated input [p0, rt] is fed into our pre-trained mask predictor to compute the loss. This enables the model to reconstruct masked regions (r0) using contextual information from both the prompt and corrupted result. The objective function is defined as: LMixed-SFT = Et,p0,r0,rt 1 (cid:88) i=1 I[ri = [MASK]] log pθ(ri 0p0, rt) , (2) where denotes the sequence length. Here, [p0, r0] and [p0, rt] correspond to the clean data x0 and its noisy counterpart xt, respectively. This formulation ensures the model learns to recover masked tokens while maintaining alignment with the original prompt and task-specific reasoning logic."
        },
        {
            "title": "2.3.1 Unified GRPO for Diffusion Foundation Models",
            "content": "With our mixed long-CoT fine-tuning, MMaDA demonstrates the ability to generate unified and coherent reasoning chains prior to final outputs. To further enhance its performance on knowledge-intensive tasks and complex reasoning/generation scenarios, we propose UniGRPO, novel policy-gradient-based reinforcement learning algorithm tailored for diffusion foundation models. This approach enables diffusion-centric RL training framework that unifies task-specific objectives across diverse modalities and reasoning paradigms. The method is structured into two core components: (1) unified mathematical formulation for diffusion-based RL, and (2) diversified reward modeling to align policy gradients with task-specific rewards. 5 Challenges in Adapting Autoregressive GRPO to Diffusion Models The original GRPO [27] relies on computing token-level log-likelihoods πθ(oi,tq, oi,<t) and sequence-level probabilities πθ and πref (preliminary in appendix A.2). In autoregressive (AR) LLMs, these metrics are efficiently derived via the chain rule of generation. However, diffusion models introduce three critical challenges: 1. (1) Local Masking Dependency: Token-level log-likelihoods log πθ(oi,tq, oi,<t) are only valid within masked regions during the diffusion process, unlike AR models where all tokens are valid. 2. (2) Mask Ratio Sensitivity: uniform mask ratio must be sampled for the response segment to approximate the policy distribution πθ, as diffusion dynamics depend on masking patterns. 3. (3) Non-Autoregressive Sequence-Level Likelihoods: The sequence-level log-likelihood cannot be directly accumulated from token-level probabilities due to the absence of an autoregressive chain rule in diffusion models. Prior approaches address these issues with suboptimal strategies. LLaDA [22] employs Monte Carlo sampling over numerous mask ratios (e.g., 128 samples), incurring high computational costs for on-policy RL. d1 [28] fixes the mask ratio and randomizes question masking, which reduces noise diversity and ignores the multi-step denoising nature of diffusion. Unified Formulation for Diffusion GRPO To overcome these limitations, we introduce UniGRPO, computationally efficient approximation algorithm designed for diffusion architectures. Given batch of responses {oi}G for query q, each response is fixed during gradient updates to ensure stable policy evaluation. We highlight three critical points of our UniGRPO as: i=1 1. Structured Noising Strategy: For each oi, we sample masking ratio pi [0, 1] uniformly and construct perturbed version oi,p by replacing tokens with [MASK]. The random seed for pi varies across gradient steps. This strategy aims to preserve stochasticity while ensuring the model is exposed to various stages of the diffusion denoising process, from nearly fully masked to nearly fully denoised answers. By doing so, UniGRPO learns from multi-step denoising information, which is consistent with conventional training methodologies for diffusion models and allows for the full utilization of their multi-step generative power. 2. Efficient Log-Likelihood Approximation: We define the expected per-token log-likelihood under the perturbed distribution as: θ(oi,t q, o, pi) = Epi[0,1] [I[oi,t,p = [MASK]] log pθ(oi,t,pq)] π The sequence-level log-likelihood is then approximated by averaging over masked tokens: π θ ="
        },
        {
            "title": "1\nM",
            "content": "(cid:88) oi,tM log pθ(oi,tq), where denotes the number of masked tokens. (3) (4) 3. Policy Gradient Objective: The per-token reward is computed as the ratio between current and old θ(oi,tq,o,pi) old(oi,tq,o,pi) . The final UniGRPO objective integrates clipped surrogate i,t(θ) = π policy likelihoods: π rewards and KL regularization: JUniGRPO(θ) = (q,a)D,{oi}G i=1πθold (q),{pi[0,1]}G i=1 (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi (cid:32) oi (cid:88) t=1 (cid:33)(cid:35) (cid:16) min i,t(θ) ˆAi,t, (5) (cid:16) clip i,t(θ), 1 ε, 1 + ε (cid:17) ˆAi,t (cid:17) βDKL(π θπ ref) , where ˆAi,t denotes the advantage estimate, ε controls the clipping range, and β balances the KL divergence penalty. Through this design, UniGRPO captures the essential multi-step denoising dynamics of diffusion models. By allowing the model to predict answers under diverse masking conditions while preserving the natural 6 7: 8: 9: 10: 11: 12: 13: structure of the input, it avoids the pitfalls of both computational inefficiency (as in LLaDA) and oversimplified prediction (as in d1). The training procedure of UniGRPO is outlined in algorithm 1. For further details on the UniGRPO algorithm, please refer to appendix and section 5.2. Algorithm 1 UniGRPO Policy Gradient Optimization Require: Reference model πref, prompt distribution D, number of completions per prompt G, number of inner updates µ, diffusion steps 1: Initialize policy πθ πref 2: while not converged do 3: 4: 5: 6: πold πθ Sample prompt Sample completions oi πold( q), for [G] For each oi, compute reward ri and advantage Ak Sample starting timestep t0 U(0, 1) Generate µ 1 uniformly spaced timesteps t1, . . . , tµ1 from [t0, ] for gradient update iterations = 1, . . . , µ do (πold) using equation (15) if = 1 then Sample starting mask ratio r1 U(0, 1) and compute initial timestep t1 = r1 else Uniformly divide remaining timesteps: tn = (n1) (µ1) (T t1) + t1 Construct input (q, masked oi) using timestep tn (with always unmasked) For πθ, πold, πref, estimate log-probabilities of masked tokens in oi at tn Compute UniGRPO objective equation (5) and update πθ via gradient descent 14: 15: 16: 17: return πθ"
        },
        {
            "title": "2.3.2 Diversified Reward Modeling",
            "content": "We further simplify the optimization objective of UniGRPO (equation (5)) as follows: Remark 1. General optimization objective of UniGRPO: JUniGRPO(θ) = Eoπθ (q)[F(RUni(o)) βP (o)], (6) where RUni(o) denotes the reward obtained from the model-generated response o, () is the penalty term, which denotes the KL divergence as specified in equation (5). This is unified rule-based reward system, where RUni() can be instantiated with diverse rewards for different tasks. To address the varied requirements of different tasks, we have defined range of rewards under the unified formulation equation (6), providing tailored RL optimization directions for each task branch. We mainly adopt three types of rewards: Textual Reasoning Rewards: We apply UniGRPO on the training split of the GSM8K [29] dataset and define composite reward. This includes Correctness Reward of 2.0 for correct answer, and Format Reward of 0.5 if the response adheres to our predefined format: <think>...</think>. Multimodal Reasoning Rewards: For mathematical tasks such as GeoQA [30] and CLEVR [31], we adopt the same Correctness and Format Rewards as in textual reasoning. In addition, for caption-based tasks, we further introduce CLIP Reward of 0.1 CLIP(image, text), where the original CLIP score measuring text-image alignment is scaled by 0.1 to balance its influence. Text-to-Image Generation Rewards: For image generation tasks, we incorporate the same CLIP Reward to assess text-image semantic alignment, alongside an Image Reward that reflects human preference scores. Both rewards are scaled by factor of 0.1 to ensure balanced contribution during optimization."
        },
        {
            "title": "3 Flexible Sampling Strategies at Inference Time",
            "content": "Semi-Autoregressive Sampling for Text Generation For text generation, we adopt the semi-autoregressive denoising strategy introduced in LLaDA [22], which integrates autoregressive decoding with diffusion-based denoising. Specifically, the output sequence is partitioned into multiple blocks and generated from left to right. Within each block, logits are computed for all masked positions, and subset of tokens is selectedeither randomly or based on confidence scoresfor denoising. The masking schedule follows linear schedule, consistent with LLaDA. The denoising process is repeated for given steps. 2 = 512 denoising steps. The In our evaluation, we set the total sequence length to = 1024 and perform sequence is divided into blocks of 64 tokens. At each step, we unmask the 2 tokens with the lowest confidence within the current block, irrespective of their positions. Once all tokens in block are denoised, the process proceeds to the next block. qualitative comparison is provided below. As shown, semi-autoregressive denoising strategy tends to generate more intricate and detailed descriptions, whereas non-autoregressive fixed-length generation often produces very short responses. This observation is consistent with findings reported for LLaDA. For instruction-tuned models, given that the training process incorporates substantial number of EOS tokens, directly applying the lowest-confidence remasking strategy without dividing into blocks leads to an unnaturally high frequency of EOS tokens in the generated sentences. Qualitative Comparison of Different Sampling Strategy Prompt: Please describe this image in detail. Fix-length Generation ( Length = 1024, Steps = 512, Block Size = 1024) The image is detailed oil painting depicting still life scene. Semi-Autoregressive ( Length = 1024, Steps = 512, Block Size = 64) The painting depicts serene still life scene on tablecloth with textured, multicolored pattern. In the center, there is rustic, cream-colored ceramic pitcher with brown accents, holding vibrant bouquet of sunflowers. The sunflowers, with their bright yellow petals and deep brown centers, are arranged in way that they seem to burst forth from the pitcher, creating sense of abundance and vitality. To the right of the pitcher, there is small, ornate glass vase with patina finish, adding an element of contrast and visual interest to the composition. In the foreground, sliced loaf of bread is placed on the tablecloth, with piece of bread and knife nearby, suggesting meal or snack. The background is softly blurred, focusing attention on the main elements of the painting. The overall composition is balanced and harmonious, with the warm tones of the sunflowers and ceramic pitcher complementing the cooler bronze of the vase and the soft pastels of the bread. Parallel Non-Autoregressive Sampling for Image Generation For image generation, we adopt low-confidence remasking strategy and follow cosine noise schedule, consistent with the setup in MAGVIT-v2 [24]. In contrast to text generation, we do not employ semi-autoregressive approach; instead, the entire output sequence is treated as single generation block. During evaluation, we generate sequences of length 1024, corresponding to 512512 resolution images. The denoising process consists of 50 timesteps, and we apply classifier-free guidance with guidance scale set to 3.5. 8 Table 2 Evaluation on Multimodal Understanding Benchmarks. Model POPE MME Flickr30k VQAv2(test) GQA MMMU MMB SEED Understanding-Only LLaVA-v1.5 [32] InstructBLIP [33] Qwen-VL-Chat [34] mPLUG-Owl2 [35] LLaVA-Phi [36] 85.9 78.9 - 85.8 85.0 1510.7 1212.8 1487.5 1450.2 1335.1 Unified Understanding & Generation DreamLLM [16] SEED-X [19] Chameleon [8] LWM [9] Emu [11] Show-o [20] Gemini-Nano-1 [12] - 1435.7 - 948.4 - 1097.2 - 1410. - 84.2 - 75.2 - 80.0 - 86.1 MMaDA (Ours)"
        },
        {
            "title": "4 Experiments",
            "content": "- - - - - - 52.3 74.7 - 77.4 62.5 - 67.6 78.5 - 78.2 79.4 71.4 72.9 - 66.0 55.8 57.2 69.4 62.7 76.7 62.0 49.5 57.5 56.1 - - 47.9 - 44.8 - 58.0 - 61. 35.4 - - - - - 35.6 - - - 26.7 26.3 30.2 64.3 - 60.6 - 59.8 - - - - - - - 68.5 58.6 - 58.2 - - - - - - - - - 64."
        },
        {
            "title": "4.1 Experimental Setup\nDatasets To train MMaDA, we utilized a diverse range of datasets tailored for corresponding training stages\nas follows: (1) Foundational Language and Multimodal Data: For basic text generation capabilities, we\nadopt the RefinedWeb [37] dataset. For multimodal understanding and generation tasks, we incorporate\nwidely-used open-sourced image-text datasets [38–42]. (2) Instruction Tuning Data: To enhance instruction-\nfollowing capabilities, we use Alpaca [43] for textual instructions and LLaVA-1.5 [32] for visual instruction\ntuning. (3) Reasoning Data: For Mixed Long-CoT finetuning, we curated a diverse set of reasoning datasets.\nFor textual mathematical and logical reasoning, we employed datasets from ReasonFlux [44], LIMO [45],\ns1k [46], OpenThoughts [47], and AceMath-Instruct [48]. For multimodal reasoning, we used the LMM-\nR1 [49] model to generate responses on GeoQA [30] and CLEVR [31], and retained correctly answered\ninstances. Additionally, for world knowledge-aware image generation, we used GPT-4.1 to synthesize factual\nitem-description pairs spanning science, culture, and landmarks, formatted into unified CoT-style traces.(4)\nReinforcement Learning Data: For UniGRPO training, we adopt the original mathematical and logical\ndatasets used in Reasoning [30, 31, 50].",
            "content": "Evaluation and Baselines We evaluate our MMaDA on three distinct tasks using task-specific metrics and baselines:(1) Multimodal Understanding: Following LLaVA [32], we evaluate on POPE, MME, Flickr30k, VQAv2, GQA, and MMMU, and compare against understanding-only models [3236], as well as unified models [8, 9, 11, 12, 16, 19, 20]. (2) Image Generation: We assess generation quality using 50K prompts from our test set to compute CLIP Score [51] and ImageReward [52] to evaluate textual alignment and human preference alignment. We adopt GenEval [53] for general evaluation and WISE [54] for evaluating world knowledge-based generation, comparing against generation-specific models [5558] and unified baselines [8, 9, 11 13, 16, 19, 20, 59]. (3) Text Generation: we evaluate instruction-following and reasoning performance on MMLU, GSM8K, and related benchmarks, comparing with LLaMA2-7B, Qwen2-7B, and LLaDA-8B. Implementation Details We initialize MMaDA with LLaDA-8B-Instructs pretrained weights [22] and an image tokenizer with Show-os pretrained ones. We perform joint training across three stages: Stage1: The initial model is trained for 200K steps using foundational language and multimodal data, including RefinedWeb for text generation, ImageNet-1k for class-conditional image generation, and additional image-text datasets for 9 Table 3 Evaluation on Image Generation Benchmarks. Model Wise (Cultural) Image Reward CLIP Score Single Obj. Two Obj. Counting Colors Position Color Attr. Overall GenEval Generation-Only LlamaGen [58] SDv1.5 [55] SDv2.1 [55] DALL-E 2 [56] SDXL [57] - 0.34 0.30 - 0.43 Unified Understanding & Generation DreamLLM [16] SEED-X [19] Chameleon [8] LWM [9] Emu [11] Show-o [20] Janus [13] Gemini-Nano-1 [12] VAR-GPT [59] - - - - - 0.28 0.16 - - 0.67 MMaDA (Ours) 0.79 0.84 0.95 0.83 1.13 0.76 0.77 0.83 0.78 0.81 0.92 1.03 0.89 0.94 1.15 13.43 23.54 27.41 25.20 32.12 18.33 23.15 20.32 26.21 22.29 28.94 29.45 24.58 28.85 32. 0.71 0.97 0.98 0.94 0.98 - 0.97 - 0.93 - 0.95 0.97 - 0.96 0.99 0.34 0.38 0.51 0.66 0.74 - 0.58 - 0.41 - 0.52 0.68 - 0.53 0.76 0.21 0.35 0.44 0.49 0.39 - 0.26 - 0.46 - 0.49 0.30 - 0.48 0. 0.58 0.76 0.85 0.77 0.85 - 0.80 - 0.79 - 0.82 0.84 - 0.83 0.84 0.07 0.04 0.07 0.10 0.15 - 0.19 - 0.09 - 0.11 0.46 - 0.13 0.20 0.04 0.06 0.17 0.19 0.23 - 0.14 - 0.15 - 0.28 0.42 - 0.21 0. 0.32 0.43 0.50 0.52 0.55 - 0.49 0.39 0.47 0.53 0.61 - 0.53 0.63 captioning. This is followed by another 400K steps where ImageNet is replaced with more diverse image-text pairs. Stage2: The model is then jointly trained for 50,000 steps using Instruction Tuning Data and Reasoning Data. Stage3: This final stage consists of UniGRPO training with Reinforcement Learning Data for 50,000 steps. Training is performed on 64 A100 (80GB) GPUs using global batch size of 1,280. The AdamW optimizer is employed with an initial learning rate of 5e-5 and cosine learning rate scheduler."
        },
        {
            "title": "4.2 Multimodal Understanding",
            "content": "Table 2 reports the multimodal understanding performance of our method on standard benchmarks, including POPE, MME, Flickr30k, VQAv2, GQA, and MMMU. For outputs from MMaDA that contain reasoning traces, we use the final answer as the prediction. Compared with dedicated understanding-only models such as LLaVA-v1.5, InstructBLIP, and Qwen-VL-Chat, our model achieves comparable or superior results across most benchmarks, despite being trained under unified objective. When compared to other unified models (e.g., SEED-X, DreamLLM, Janus, Emu3, and Show-o), our method consistently outperforms them across several benchmarks, particularly benefiting from the proposed Mixed Long-CoT Finetuning and UniGRPO Reinforcement Learning stages. Notably, this is the first demonstration of diffusion-based MLLM exhibiting strong understanding capabilities, highlighting the potential of our unified architecture in bridging generation and understanding tasks. Qualitative results are in appendix and appendix D."
        },
        {
            "title": "4.3 Text-to-Image Generation",
            "content": "Table 3 presents the evaluation results on text-to-image generation benchmarks. Our model achieves the highest performance in both CLIP Score and ImageReward across generation-only and unified models, attributed to the UniGRPO training stage with rewards explicitly aligned to these metrics. Furthermore, our method demonstrates superior compositionality and object counting capabilities on GenEval, benefiting from the reasoning-intensive training of the understanding branch. Notably, on WISE [54] Cultural benchmark, which is designed to evaluate world knowledge-aware generation, our model significantly outperforms prior approaches, owing to its joint training on text-based reasoning, which is typically absent in existing unified models. Qualitative results are in appendix and appendix D. 10 Qualitative Comparison of Multimodal Reasoning Question: In the diagram, triangle ABC is shown. Points and are located on segments AB and AC, respectively. Lines BE and CD intersect at point F. Given that angle BAC is 60, angle DCA is 40, and angle EBA is 30, what is the measure of angle CFE? Show-o: The angle measure of angle CFE is 60. Emu3: To solve for the degree measure of angle CFE, we can utilize the properties of triangles and angles. Since angle ABE measures 30, angle ACD measures 40, and angle ABC measures 60, we can find angle ACD by subtracting angle and angle ABE from 180: 180 - 40 - 30 = 110. Since triangle ABC is an isosceles triangle, angle BAC measures 40. Therefore, angle CFE can be found by subtracting angle BAC from angle ABC: 40 - 110 = -30. However, angles cannot have negative measures, so we take the positive value, giving us 60. Therefore, the degree measure of angle CFE is 60. Janus Pro 7B: To find the measure of angle CFE, we can use the properties of triangles and the given angles. First, we know that angle ABE is 30. Since angle ABE and angle ACD are adjacent angles in the same triangle, they are supplementary. Next, we know that angle is 60. Since angle and angle ACD are adjacent angles in the same triangle, they are supplementary. Now, we can find angle CFE. Angle CFE is the supplement of angle C, so angle CFE = 180 - 120 = 60. Therefore, the degree measure of angle CFE is 60. Therefore, angle ACD is 180 - 30 = 150. Therefore, angle is 180 - 60 = 120. MMaDA: <think> To determine the measure of angle CFE, we need to analyze the given information and use the properties of triangles and angles. 1. **Identify the given angles:** - Angle = 60 - Angle ACD = 40 - Angle ABE = 30 2. **Use the fact that the sum of the angles in triangle is 180:** - In triangle, the sum of the angles is 180. Therefore, we have: 60 + ADC + 40 = 180 60 + AEB + 30 = 180 3. **Determine the measure of angle CFE:** - Since angle AEB is an exterior angle to triangle CEF, it is equal to the sum of the two non-adjacent interior angles. Therefore, angle AEB = angle ACD + angle CFE = 40 + angle CFE = 90. - Therefore, angle CFE = 90 - 40 = 50. Thus, the measure of angle CFE is 50 . </think> The measure of angle CFE is 50 . 11 Qualitative Comparison of World Knowledge-Aware Text-to-Image Generation Prompt: famous statue gifted to the United States by France. Other models: Show-o Emu Janus Pro 7B MMaDA: <think> The description refers to well-known landmark that was given from France to the United States. This is known as the Statue of Liberty. </think> low-angle shot of the Statue of Liberty against blue sky with few wispy clouds."
        },
        {
            "title": "4.4 Textual Reasoning",
            "content": "Table 4 Evaluation on LLM Benchmarks. Model Arch MMLU ARC-C TruthfulQA GSM8K MATH GPQA LLaMA-2-7B LLaMA-3-8B Qwen2-7B AR AR AR LLaDA-8B Diffusion MMaDA-8B(Ours) Diffusion 45.9 64.5 70.3 65.9 68.4 46.3 53.1 60.6 47.9 57.4 39.0 44.0 54. 46.4 43.1 14.3 53.1 80.2 70.7 73.4 3.2 15.1 43.5 27.3 36.0 25.7 25.9 30. 26.1 28.4 table 4 details the language modeling performance of MMaDA across range of benchmarks, encompassing general tasks such as MMLU, ARC-C, and TruthfulQA, as well as mathematical tasks including GSM8K, MATH, and GPQA. Despite being trained on limited task-specific tokens and solely open-source text data, MMaDA achieves comparable performance compared to strong baselines such as Qwen2-7B and LLaMA3-8B on MMLU, ARC-C, and consistently outperforms LLaDA-8B on math benchmarks. Notably, MMaDA (Ours) pioneers the joint training of unified diffusion model for text generation, multimodal reasoning, and image generationa multi-task configuration rarely explored in prior unified architectures. These results underscore the viability of diffusion-based models as general-purpose LLMs and indicate potential for stronger future performance through enhanced text data and scaling. Qualitative results are in appendix C. 12 Qualitative Comparison of Textual Reasoning Question: What is the result of (2016)(12+8) ? Show-o: < Show-o does not support this task > Emu3: < Emu3 does not support this task > Janus Pro 7B: < Janus Pro 7B does not support this task > . 20 - 16 = 4. 4 times 20 is 80. Let me look at the So the result is 20. (2016)(12+8) 4 (2016)(12+8) 4 Let me check that division. . Dividing that number by 4. . Hmm, lets break this down step by (20 - 16) and (12 + 8). Both of those are inside parentheses, so should compute each part MMaDA: <think> Okay, let me try to figure out the result of this expression: step. First, need to make sure apply the order of operations correctly. Right, order of operations: parentheses first, then multiplication and division from left to right, then addition and subtraction from left to right. Let me (2016)(12+8) write the expression again to visualize it better: 4 So, the parentheses are within the numerator, which is part of the fraction, and the denominator is 4. According to PEMDAS, should handle the parentheses first before doing the multiplication and division. parentheses part: separately. Starting with the first parentheses: 20 - 16. Thats straightforward. 20 minus 16 equals 4. Okay, so that part simplifies to 4. Next, the second part inside the parentheses: 12 + 8. Thats addition, so 12 plus 8 is 20. So now, the numerator becomes 4 20. So now, the numerator is 4 20. Let me do that multiplication. Now the entire expression is 80 4 (since 4 20 is 80), so 80 4 = 20. Wait second, let me verify that again to make sure didnt make mistake. Original expression: First parentheses: Second parentheses: 12 + 8 = 20. Multiply those two: 4 20 = 80. Divide by 4: But wait, maybe theres another way to approach this. Sometimes with order of operations, parentheses are easier to catch. Lets see. The original expression is (2016)(12+8) multiplication, and the denominator is 4. 4 is 20. the result is still 20. So thats good way. Either way, multiplication and division are done from left to right. Alternatively, lets check if theres any possibility that the entire numerator is being divided. entire expression is 420 reassuring. So that seems consistent. Another way to check: look at it by using properties of operations. But in this case, since the expression is written with multiplication and division, the order of operations tells us to do all calculations in the numerator first, then divide by the denominator. Another check: operations. Hmm, think thats solid. The answer should be 20. Let me just make sure theres no alternative interpretation. Lets write the original expression again: . Original expression: The numerator is 4 20 = 80, denominator is 4. So 80 divided by 4 is 20. Therefore, Im confident that the answer is 20. </think> Lets see. The , so 4 20 is 80, then 80 divided by 4 is 20. So thats another way to see it, which is . Simplifying, 4 20 = 80, then 80 divided by Alternatively, you could factor out the 4 in the numerator and cancel with the denominator, but in this case, So the numerator simplifies to 80. 4 goes into 80 twenty times Then 80 divided by 4 is 20. So no errors in the let me verify the multiplication. 4 20 = 80. 4 You could write it as 420 . The parentheses in the numerator give us Lets go through each step again. Yep, exactly, thats correct. Yep, that seems right. (2016)(12+8) 4 All steps lead to 20. Check. Check. . Check. 80 4 = 20. 4 4 4 The result of (2016)(12+8) is calculated as follows: 1. **Simplify the parentheses inside the numerator:** - 20 16 = 4 - 12 + 8 = 20 2. **Multiply the simplified results:** - 4 20 = 80 3. **Divide by 4:** - 80 4 = **Final Answer:**"
        },
        {
            "title": "5.1 Ablation Study",
            "content": "We present quantitative ablation results of our MMaDA across different training stages: Mixed Long-CoT fine-tuning and UniGRPO. All results generated follow the sampling process in section 3. As shown in the table 5, after Stage 1, our model still lags behind most baselines. In Stage 2, Mixed Long-CoT fine-tuning substantially enhances the models reasoning capabilities, particularly in mathematical and geometric domains. In Stage 3, UniGRPO further improves performance, allowing the model to achieve results comparable to state-of-the-art methods across various tasks, including mathematical reasoning, geometric problem-solving, and image generation benchmarks such as CLIP Score and ImageReward. These results demonstrate that UniGRPO effectively boosts both the models understanding/reasoning and generative capabilities."
        },
        {
            "title": "5.2 Design Choices of UniGRPO\nEffect of General Masking Strategy To evaluate the\nimpact of our proposed masking strategy, we first conduct a\ncomparative analysis with d1 [28] within our reinforcement\nlearning framework. Given the substantial computational\ncost associated with large-scale ablation studies, we perform\nthese experiments on the GSM8K dataset, utilizing 8 A100\nGPUs. Both the original d1 methodology and our Uni-\nGRPO approach are applied to this dataset, starting from\nthe same pre-trained checkpoint of our MMaDA. We present\nthe reward trends during training in figure 3. As shown in\nthe figure, our method consistently achieves higher reward\nvalues during training, aligning well with our theoretical\nanalysis. In contrast to d1, UniGRPO removes masking\nfrom the question and applies partial masking to the an-\nswer rather than masking it entirely. This results in input\nsequences that retain partial noise, encouraging the model\nto learn across multiple denoising timesteps. Consequently, this better leverages the intrinsic characteristics\nof diffusion models and improves the overall learning capacity.",
            "content": "Figure 3 Comparison of different masking strategies on GSM8K reward trends during training. Effect of Uniformly Random masking In place of fully random masking across iterations, we adopt uniformly random masking strategy for the answer portion. Specifically, we first sample random starting timestep, and then uniformly generate the remaining denoising timesteps across the full diffusion timesteps (set to 1000 in our experiments). For instance, given randomly selected starting timestep of 100 and total of 5 training iterations, the remaining timesteps are uniformly spaced and set to 300, 500, 700, and 900. This design ensures more consistent coverage of the diffusion process while retaining randomness. We illustrate the training reward trends resulting from this structured masking strategy in figure 4. As shown, the baseline approach with fully random timestep selection tends to introduce instability during training, leading to more frequent reward fluctuations and requiring greater number of steps to converge. In contrast, our uniformly spaced sampling strategy effectively approximates the behavior of Monte Carlo averaging in log-likelihood estimation, resulting in improved stability and faster convergence. Figure 4 Comparison of different random masking strategies on GSM8K reward trends during training. 14 Figure 5 Qualitative Illustration of Synergy Across Modalities. Table 5 Ablations on Mixed Long-CoT fine-tuning and UniGRPO"
        },
        {
            "title": "Model",
            "content": "GSM8K MATH500 GeoQA CLEVR CLIP Score"
        },
        {
            "title": "ImageReward",
            "content": "MMaDA After Stage 1 + Mixed Long-CoT Finetuning + UniGRPO (MMaDA) 17.4 65.2 73.4 4.2 26.5 36.0 8.3 15.9 21.0 10.3 27.5 34.5 23.1 29.4 32. 0.69 0.84 1."
        },
        {
            "title": "5.3 Synergy Across Various Tasks",
            "content": "Throughout the joint training process, we observe clear synergy across the three task categoriestext generation, multimodal understanding, and image generation. As shown in figure 6, all key performance metrics exhibit consistent improvements during Stage 2 (training steps 120K200K), reflecting the mutually beneficial nature of our unified training framework. This synergy is also evident qualitatively: as illustrated in figure 5, the models responsesboth textual and visualbecome increasingly complex and coherent. Specifically, textual outputs grow more informative and logically structured, while visual understanding yields more precise and grounded descriptions. Consequently, for the same prompt, the generated images become more accurate, detailed, and better aligned with the given instructions, demonstrating the effectiveness of joint optimization in enhancing cross-modal alignment and compositionality. Figure 6 Key Performance Metrics Across Three Tasks."
        },
        {
            "title": "5.4 Sampling Efficiency\nWe identify sampling efficiency as a key advantage of diffusion models over autoregressive (AR) approaches.\nUnlike AR models, which generate tokens sequentially, diffusion models enable parallel token generation within\neach denoising step, substantially reducing the number of forward passes required. To quantify this advantage,\nwe evaluate the performance of MMaDA under varying numbers of denoising steps. In our setup, generation\nbegins from 1024 [MASK] tokens, allowing up to 1024 denoising steps—corresponding to a 512 × 512 resolution\nimage. As shown in table 6, image generation maintains strong performance even with as few as 15 or 50",
            "content": "15 Table 6 Generation performance of MMaDA under different denoising steps. * Metrics: CLIP Score for image generation and multimodal understanding, MMLU accuracy for text generation. Task Denoising Steps Metrics* Image Generation Multimodal Understanding Text Generation 1024 50 15 1024 512 256 1024 512 256 32.8 32.0 31.7 35.5 36.1 35. 66.9 66.3 65.7 steps. For text and multimodal tasks, coherent outputs can be achieved with just quarter or half of the full steps. These results underscore the efficiency potential of diffusion-based language models and suggest that future advances in sampling techniques or higher-order solvers could further enhance their speed and quality."
        },
        {
            "title": "5.5 Task Extension",
            "content": "A notable advantage of diffusion-based models is their natural ability to perform inpainting and extrapolation without requiring additional fine-tuning. This stems from the fact that these tasks can be formulated as masked token prediction problems, which are inherently integrated into the training objective of diffusion models. While prior work such as Show-o demonstrates this property only in the context of image generation, MMaDA extends it further to multimodal understanding and text generation. As illustrated in Figure 7, our model supports inpainting across three modalities: (i) predicting missing spans in text sequences, (ii) completing answers in visual question answering given an image and partial input, and (iii) performing image inpainting conditioned on incomplete visual prompts. These examples showcase the flexibility and generalization capabilities of our unified diffusion architecture across diverse generation and reasoning tasks."
        },
        {
            "title": "6 Related Work",
            "content": "Figure 7 Inpainting Task Extension. Multimodal Large Language Models for Multimodal Understanding Recent developments in large language models (LLMs) such as Gemini-2.0 [60], o1-preview [61], and DeepSeek-R1 [62] have improved the evolution of multimodal large language models (MLLMs) [6365]. Early efforts in this domain, including LLaVA [66], MiniGPT-4 [67], and InstructBLIP [33], showcased impressive capabilities in multimodal understanding. These studies advanced the integration of LLMs into multimodal contexts by projecting features from pretrained modality-specific encoders, such as CLIP [51], into the input space of LLMs, thereby facilitating multimodal understanding and reasoning within unified transformer. Many efforts have been made for MLLMs regarding vision encoders, alignment adapters, and curated datasets [34, 6870], and most of them follow an autoregressive generation paradigm that has been proven effective for text generation in LLMs. However, they are usually not capable of performing textual and multimodal reasoning concurrently. In this work, MMaDA develops diffusion foundation models to fill this gap. 16 Diffusion Models and Autoregressive Models for Visual Generation large number of diffusion models [5557, 7185] have demonstrated notable success in visual generation. In addition to the typical denoising diffusion process on the continuous space, series of frameworks, such as D3PM [86] and VQDiffusion [87], adopt discrete diffusion modeling [86, 88] for visual generation. Specifically, the image is denoted as sequence of discrete tokens using pretrained image tokenizers [24, 8991]. In tthe raining stage, the model is optimized to recover the original values of portion of these tokens that are randomly masked. Transformer series [1, 2, 9294] has demonstrated significant capabilities in autoregressive modeling for NLP tasks. Many approaches [9599] try to apply the autoregressive modeling to perform visual generation by modeling semantic dependency within visual details. For example, LlamaGen [58] employs Llama architectures [94] and refines codebook design to enhance the performance of discrete tokenizers in class-conditional image generation. VAR [100] replaces the next-token prediction paradigm with next-scale prediction by designing multi-scale image tokenizer. However, existing autoregressive methods still lag behind diffusion methods in terms of visual generation capabilities. In this work, MMaDA train diffusion models to model textual and visual contents, and infer efficiently with AR or Semi-AR sampling. Unified Vision-Language Foundation Models Recently, numerous studies [16, 19, 101104] have focused on developing unified multimodal foundation models that excel in both understanding and generation. Approaches such as SEED-X [19] and DreamLLM [16], along with others [514], represent all modalities as series of tokens and employ unified transformer architecture to train the entire system end-to-end. For example, Emu3 [25] trains single transformer from scratch using mixture of multimodal tokenized sequences, optimized solely with next-token prediction. While these unified autoregressive models show promise, they can struggle with visual generation tasks. Transfusion [21] and Show-o [20] employ autoregressive modeling for text generation and diffusion modeling for visual generation. Nevertheless, they mainly focus on pretraining strategies. Exploring effective post-training designs is still lacking for existing unified multimodal foundation models."
        },
        {
            "title": "7 Conclusion",
            "content": "This work introduces unified diffusion foundation model, namely MMaDA, that integrates textual reasoning, multimodal understanding, and generation within single probabilistic framework. To the best of our knowledge, MMaDA is the first to systematically explore the design space of diffusion-based foundation models, proposing novel post-training strategies. Extensive experiments across diverse vision-language tasks demonstrate that MMaDA is comparable to or even better than specialized models, highlighting the potential of diffusion models as next-generation foundation paradigm for multimodal intelligence. However, MMaDA also has limitations due to its current model size (8B parameters), and we will use larger model size for better performance."
        },
        {
            "title": "References",
            "content": "[1] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, 2020. [3] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [4] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 17 [5] Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, and Ying Shan. VL-GPT: generative pre-trained transformer for vision and language understanding and generation. CoRR, abs/2312.09251, 2023. [6] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. CoRR, abs/2307.05222, 2023. [7] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. CoRR, abs/2312.13286, 2023. [8] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [9] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint, 2024. [10] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. [11] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Emu: Generative pretraining in multimodality. In ICLR, 2023. [12] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 1, 2023. [13] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. [14] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [15] OpenAI. Gpt-4 technical report. 2023. [16] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. DreamLLM: Synergistic multimodal comprehension and creation. In ICLR, 2024. [17] Jing Yu Koh, Daniel Fried, and Russ Salakhutdinov. Generating images with multimodal language models. Advances in Neural Information Processing Systems, 36:2148721506, 2023. [18] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. In European Conference on Computer Vision, pages 126142. Springer, 2024. [19] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. [20] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [21] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [22] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. [23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, pages 68406851, 2020. 18 [24] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. [25] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [26] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [27] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [28] Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion large language models via reinforcement learning, 2025. [29] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [30] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric P. Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning, 2022. [31] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning, 2016. [32] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, pages 2629626306, 2024. [33] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023. [34] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. CoRR, abs/2308.12966, 2023. [35] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In CVPR, pages 1304013051, 2024. [36] Yichen Zhu, Minjie Zhu, Ning Liu, Zhiyuan Xu, and Yaxin Peng. Llava-phi: Efficient multi-modal assistant with small language model. In Proceedings of the 1st International Workshop on Efficient Multimedia Computing under Limited, pages 1822, 2024. [37] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon LLM: outperforming curated corpora with web data only. In NeurIPS, 2023. [38] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, pages 248255, 2009. [39] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, pages 35583568, 2021. [40] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, pages 40154026, 2023. [41] David McClure. laion-aesthetics-12m-umap. https://huggingface.co/datasets/dclure/ laion-aesthetics-12m-umap, 2024. 19 [42] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, Limin Wang, and Hongsheng Li. Journeydb: benchmark for generative image understanding. In NeurIPS, 2023. [43] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. GitHub, 2023. [44] Ling Yang, Zhaochen Yu, Bin Cui, and Mengdi Wang. Reasonflux: Hierarchical llm reasoning via scaling thought templates. arXiv preprint arXiv:2502.06772, 2025. [45] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning, 2025. [46] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. [47] OpenThoughts Team. Open Thoughts. https://open-thoughts.ai, January 2025. [48] Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acemath: Advancing frontier math reasoning with post-training and reward modeling. arXiv preprint, 2024. [49] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl, 2025. [50] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. [52] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. [53] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. In NeurIPS, 2023. [54] Yuwei Niu, Munan Ning, Mengren Zheng, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Bin Zhu, and Li Yuan. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. [55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 1068410695, 2022. [56] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. CoRR, abs/2204.06125, 2022. [57] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [58] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [59] Xianwei Zhuang, Yuxin Xie, Yufan Deng, Liming Liang, Jinghan Ru, Yuguo Yin, and Yuexian Zou. Vargpt: Unified understanding and generation in visual autoregressive multimodal large language model. arXiv preprint arXiv:2501.12327, 2025. [60] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [61] OpenAI. Openai o1 system card. preprint, 2024. 20 [62] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [63] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, et al. Multimodal foundation models: From specialists to general-purpose assistants. Foundations and Trends in Computer Graphics and Vision, 16(1-2):1214, 2024. [64] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. [65] Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. Hallucination of multimodal large language models: survey. arXiv preprint arXiv:2404.18930, 2024. [66] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36, 2024. [67] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. CoRR, abs/2304.10592, 2023. [68] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [69] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. [70] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [71] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [72] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Zhongdao Wang, James T. Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR. OpenReview.net, 2024. [73] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [74] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. Raphael: Text-toimage generation via large mixture of diffusion paths. NeurIPS, 36, 2024. [75] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning, 2024. [76] Xinchen Zhang, Ling Yang, Guohao Li, Yaqi Cai, Jiake Xie, Yong Tang, Yujiu Yang, Mengdi Wang, and Bin Cui. Itercomp: Iterative composition-aware feedback learning from model gallery for text-to-image generation. In ICLR, 2025. [77] Minshuo Chen, Song Mei, Jianqing Fan, and Mengdi Wang. An overview of diffusion models: Applications, guided generation, statistical rates and optimization. arXiv preprint arXiv:2404.07771, 2024. [78] Ye Tian, Ling Yang, Xinchen Zhang, Yunhai Tong, Mengdi Wang, and Bin Cui. Diffusion-sharpening: Fine-tuning diffusion models with denoising trajectory sharpening. arXiv preprint arXiv:2502.12146, 2025. [79] Hui Yuan, Kaixuan Huang, Chengzhuo Ni, Minshuo Chen, and Mengdi Wang. Reward-directed conditional diffusion: Provable distribution estimation and reward improvement. Advances in Neural Information Processing Systems, 36:6059960635, 2023. [80] Yingqing Guo, Hui Yuan, Yukang Yang, Minshuo Chen, and Mengdi Wang. Gradient guidance for diffusion models: An optimization perspective. arXiv preprint arXiv:2404.14743, 2024. 21 [81] Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data. In International Conference on Machine Learning, pages 46724712. PMLR, 2023. [82] Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, and Hongsheng Li. Rectified diffusion: Straightness is not your need in rectified flow. arXiv preprint arXiv:2410.07303, 2024. [83] Ling Yang, Jingwei Liu, Shenda Hong, Zhilong Zhang, Zhilin Huang, Zheming Cai, Wentao Zhang, and Bin Cui. Improving diffusion-based image synthesis with context prediction. Advances in Neural Information Processing Systems, 36:3763637656, 2023. [84] Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, and Bin Cui. Structure-guided adversarial training of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 72567266, 2024. [85] Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, Di ZHANG, et al. Videotetris: Towards compositional text-to-video generation. Advances in Neural Information Processing Systems, 37:2948929513, 2024. [86] Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. NeurIPS, pages 1798117993, 2021. [87] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1069610706, 2022. [88] Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. Autoregressive diffusion models. In ICLR. OpenReview.net, 2022. [89] Kevin P. Murphy. Probabilistic Machine Learning: Advanced Topics. MIT Press, 2023. [90] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pages 1287312883, 2021. [91] Yuchao Gu, Xintao Wang, Yixiao Ge, Ying Shan, and Mike Zheng Shou. Rethinking the objectives of vectorquantized tokenizers for image synthesis. In CVPR, pages 76317640, 2024. [92] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 30, 2017. [93] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [94] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [95] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In ICML, pages 40554064, 2018. [96] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pages 1287312883, 2021. [97] Suman Ravuri and Oriol Vinyals. Classification accuracy score for conditional generative models. NeurIPS, 32, 2019. [98] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, pages 16911703, 2020. [99] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. [100] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. 22 [101] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. [102] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. NeurIPS, 36, 2024. [103] Hanrong Ye, De-An Huang, Yao Lu, Zhiding Yu, Wei Ping, Andrew Tao, Jan Kautz, Song Han, Dan Xu, Pavlo Molchanov, et al. X-vila: Cross-modality alignment for large language model. arXiv preprint arXiv:2405.19335, 2024. [104] Emanuele Aiello, LILI YU, Yixin Nie, Armen Aghajanyan, and Barlas Oguz. Jointly training large autoregressive multimodal models. In ICLR, 2024. [105] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In CVPR, pages 1069610706, 2022. [106] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In CVPR, pages 1131511325, 2022. [107] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. [108] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in neural information processing systems, 34:1245412465, 2021. [109] Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. [110] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [111] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015."
        },
        {
            "title": "Appendix",
            "content": "A Preliminaries of Discrete Diffusion, PPO and GRPO A.1 Discrete Diffusion and Mask Token Prediction Discrete denoising diffusion models have emerged as powerful paradigm for modeling discrete data representations, particularly in visual and textual domains. Recent works such as VQ-Diffusion [105], MaskGIT [106], and Muse [107] demonstrate their effectiveness in generating high-quality discrete tokens. Building on these advancements, Show-o [20] unifies discrete diffusion and mask token prediction into single framework, enabling joint optimization of noise corruption and semantic recovery. Below, we formalize the key components of this discrete diffusion process. A.1.1 Forward and Reverse Diffusion Processes The forward diffusion process progressively corrupts the initial data x0 (e.g., sequence of visual/textual tokens) into noisy latent variables x1, ..., xT via fixed Markov chain q(xtxt1). At each step, this process stochastically perturbs tokenssuch as replacing them with uniform noise or introducing special [MASK] token. The reverse process, parameterized by learned model pθ, reconstructs x0 from xT by sequentially sampling q(xt1xt, x0). 0 {1, 2, ..., K} corresponds to an To formalize this, consider single token xi 0 entry in codebook. For simplicity, we omit the index in subsequent derivations. The transition probabilities between consecutive tokens are defined by matrix Qt RKK, with [Qt]mn = q(xt = mxt1 = n). The forward Markov process for the entire token sequence is then expressed as: at position in x0, where xi q(xtxt1) = v(xt)Qtv(xt1), (7) where v(x) is one-hot vector of length (with 1 at position x). The categorical distribution over xt is derived from Qtv(xt1). critical property of the Markov chain allows us to marginalize intermediate steps and compute the direct transition from x0 to xt: q(xtx0) = v(xt)Qtv(x0), with Qt = QT QT 1 Q1. Furthermore, the posterior distribution conditioned on x0 is analytically tractable: q(xt1xt, x0) = q(xtxt1, x0)q(xt1x0) q(xtx0) = (cid:0)v(xt)Qtv(xt1)(cid:1) (cid:0)v(xt1)Qt1v(x0)(cid:1) v(xt)Qtv(x0) . This property is essential for deriving the variational lower bound of the diffusion process. (8) (9) A.1.2 Transition Matrix Design and Masking Strategy The design of the transition matrix Qt is pivotal to the success of discrete diffusion models. Early works [108, 109] propose injecting uniform noise into the categorical distribution, leading to: Qt = αt + βt βt ... βt βt αt + βt ... βt βt βt ... . . . αt + βt , (10) where αt [0, 1] controls the retention probability, and βt = (1 αt)/K ensures uniform diffusion across all categories. However, this approach often introduces abrupt semantic changes due to aggressive replacement of tokens with unrelated categories. 24 To address this limitation, Show-o adopts mask-and-replace strategy inspired by masked language modeling. special [MASK] token is introduced, expanding the token space to + 1 states. The transition matrix is redefined as: Qt = αt + βt βt ... βt γt βt αt + βt ... βt γt βt βt ... . . . αt + βt γt 0 0 ... 0 , (11) where: - Ordinary tokens have probability αt to remain unchanged, βt to be uniformly diffused, and γt to be replaced by [MASK]. - The [MASK] token retains its state with probability 1. This design explicitly signals corrupted positions during the forward process, enabling the reverse network to focus on reconstructing masked regions. The parameters satisfy αt + Kβt + γt = 1, ensuring proper normalization. A.1.3 Variational Objective and Loss Simplification Following the D3PM framework [109], the variational lower bound for the discrete diffusion process is derived as: Eq(x0)[log pθ(x0)] LELBO(x0, θ) (cid:88) t=1 Eq(x0,xt)[log pθ(x0xt)] + C. (12) To simplify training, Show-o leverages the mask token prediction objective. Specifically, the model learns neural network pθ to reconstruct masked tokens in x0 from the noised xt, following the methodology of MaskGIT [106]. This approach avoids explicit modeling of the full posterior and instead focuses on recovering only the corrupted regions, significantly reducing computational complexity. A.2 Proximal Policy Optimization and Group Relative Policy Optimization Proximal Policy Optimization (PPO) [110] is widely adopted reinforcement learning algorithm that balances policy updates with stability through clipped surrogate objective. The core idea of PPO lies in constraining policy changes to proximal region around the previous policy, thereby preventing large, destabilizing updates. This is achieved by introducing clipping mechanism that bounds the importance sampling ratio during optimization. Specifically, PPO maximizes the following objective: JPPO(θ) = E(q,a)D,otπθold (q) min (cid:34) (cid:32) πθ(ot q, o<t) πθold (ot q, o<t) ˆAt, clip (cid:32) πθ(ot q, o<t) πθold(ot q, o<t) (cid:33) (cid:33)(cid:35) , 1 ε, 1 + ε ˆAt , (13) where (q, a) denotes question-answer pair sampled from the dataset D, ε is the clipping threshold, and ˆAt is the estimated advantage at time step t. The advantage ˆAt is typically computed using Generalized Advantage Estimation (GAE) [111], which combines multiple temporal differences with discounting and mixing parameters (γ, λ): ˆAGAE(γ,λ) = (cid:88) l=0 (γλ)lδt+l, with δl = Rl + γV (sl+1) (sl). (14) This formulation ensures robustness against high-variance estimates while maintaining compatibility with off-policy data. In contrast, Group Relative Policy Optimization (GRPO) [27] introduces two key innovations to address limitations in PPOs value function dependency and global advantage estimation. First, GRPO eliminates the explicit modeling of the value function (s) and instead computes advantages in group-relative manner. For , and the advantage of each response each (q, a) pair, the behavior policy πθold is normalized relative to its group: generates responses {oi}G i=1 ˆAi,t = ri mean({Ri}G std({Ri}G i=1) i=1) , 25 (15) where ri represents the raw reward for response oi. This design enables GRPO to focus on relative performance within local context, reducing sensitivity to absolute reward scales. Second, GRPO extends PPOs clipped objective by incorporating an explicit KL divergence penalty term between the current policy πθ and reference policy πref. The final objective is defined as: JGRPO(θ) = (q,a)D,{oi}G i=1πθold (q) (cid:32) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi oi (cid:88) t=1 (cid:16) min ri,t(θ) ˆAi,t, clip (cid:16) ri,t(θ), 1 ε, 1 + ε (cid:17) (cid:17) ˆAi,t βDKL(πθπref) (cid:33)(cid:35) , (16) where ri,t(θ) = πθ(oi,tq,oi,<t) is the importance sampling ratio at time t. Notably, GRPO computes the loss πθold (oi,tq,oi,<t) at the sample level: for each sequence, the per-token loss is averaged first, followed by averaging across all sequences in the group. This hierarchical aggregation enhances stability in multi-response settings."
        },
        {
            "title": "B Details of UniGRPO",
            "content": "In this section, we detail our UniGRPO training process and illustrate the differences compared to LLaDA and d1 (diff-GRPO) [28]. Notably, LLaDA does not incorporate reinforcement learning procedure, yet it includes an algorithm for computing log probabilities. Diff-GRPO introduced the first GRPO-style RL algorithm for diffusion language models and proposed an alternative method to compute log probabilities for given question-answer pair. We first outline the methodologies of these two prior works: (1) LLaDA While LLaDA does not employ an RL process, it offers method to estimate the log probability of given pair (q, a) using Monte Carlo simulation. For given answer, mask ratios are randomly sampled from the interval (0, 1) (where = 128 in the original work). Subsequently, batch of inputs is constructed. Each input instance consists of the original question tokens concatenated with the answer, where portion of the answer tokens (corresponding to the sampled mask ratio) is replaced by [MASK] tokens. model forward pass is performed on this batch, and the logits corresponding to the masked regions are collected and averaged to obtain the log probability log p(q, a). The principal drawback of this method is its significant computational overhead, rendering it impractical for on-policy RL algorithms. (2) d1 (diff-GRPO) The d1 framework [28] introduced the first GRPO-style RL algorithm for diffusion language models. To overcome the efficiency issues of LLaDA, d1 employs novel masking strategy. In each iteration, only single forward pass is performed, eschewing Monte Carlo simulation. The input is constructed by applying random mask to the question tokens and completely masking all answer tokens. The stochasticity is intended to be achieved by selecting different tokens for masking across different iterations, even with the same mask ratio. While this approach in d1 enables GRPO training, we identify potential limitations: Question Masking: The random masking of question tokens is of unclear practical significance. In typical question-answering scenarios, the question is always fully observed during both training and inference. We argue that such random masking of questions serves primarily to introduce stochasticity without direct relevance to the tasks practical application. Answer Masking Strategy: By consistently masking the entire answer, the model is effectively trained only on the initial denoising step (i.e., predicting the full sequence from fully masked state). We contend that this approach provides insufficient learning depth, potentially causing the model to behave more like single-step predictive AR model rather than leveraging the multi-step denoising capabilities inherent to diffusion models. This underutilizes the diffusion models unique strengths. As introduced in section 2.3.1, our UniGRPO addresses the masking issues with the following key modifications: 1. Unmasked Questions: In UniGRPO, all question tokens remain unmasked during training. This aligns the training process with the inference conditions and practical use-cases where the question is always fully provided. 26 2. Iteratively Varied Answer Masking: We apply random mask ratio to the answer tokens. Crucially, this mask ratio, denoted ρa, varies uniformly with the training iteration µ (e.g., ρa (0, 1) sampled anew or cycled through discrete steps each iteration). This strategy maintains stochasticity while exposing the model to diverse denoising stagesfrom nearly fully masked to almost fully denoised answers. This enables UniGRPO to learn from multi-step denoising, aligning with standard diffusion model training and leveraging their full generative potential across multiple steps. At each iteration, we sample mask ratio rµ uniformly from predefined range, apply this ratio to the answer tokens, and perform single forward pass using the unmasked question and masked answer. We then compute token-level log-likelihoods in the masked regions, apply the GRPO objective, and update the policy accordingly. 27 Qualitative Comparisons (with Reasoning CoT) Qualitative Comparison of World Knowledge-Aware Text-to-Image Generation (1) Prompt: The largest terrestrial carnivore from the Arctic. Other models: Show-o Emu3 Janus Pro 7B MMaDA: <think> largest terrestrial carnivore from the Arctic is the Polar Bear. </think> majestic polar bear stands on snowy landscape, its fur glowing against the icy backdrop. Qualitative Comparison of World Knowledge-Aware Text-to-Image Generation (2) Prompt: famous landmark building in Europe. Other models: Show-o Emu Janus Pro 7B MMaDA: <think> famous landmark building in Europe is likely the Eiffel Tower. </think> The Eiffel Tower is hollow, with cloudy and muted sky. 28 Qualitative Comparison of World Knowledge-Aware Text-to-Image Generation (3) Prompt: traditional strong spirit from Russia. Other models: Show-o Emu3 Janus Pro 7B MMaDA: <think> The user is describing traditional strong spirit from Russia. vodka. </think> An silver bottle of vodka sits next to decanter and halved lime. well-known spirit that fits this description is Qualitative Comparison of Textual Reasoning (1) Question: How many plums will balance one pear, given that 3 apples and one pear weigh as much as 10 plums, and one apple and 6 plums balance one pear? Assume that fruits of the same kind have the same weight. Show-o: < Show-o does not support this task > Emu3: < Emu3 does not support this task > Janus Pro 7B: < Janus Pro 7B does not support this task > Since all fruits are equal in weight, probably the same. need to figure out how many plums are needed to balance the weight of single pear. MMaDA: <think> Okay, lets see. gives me some information. Let me try to parse it step by step. First, the problem says: 3 apples together with pear have the same weight as 10 plums. Then, the combination of one apple and 6 plums weighs the same as pear. need to find out how many plums are needed to equal the weight of single pear. Let me start by writing this down. weight of pear, and as the weight of plum. So, based on the first statement given: 3A + = 10M Right, because 3 apples plus pear equals 10 plums. Second, saying one apple and 6 plums weigh the same as pear. Thats because all apples and plums are equal. So, = - 6M So, now we have two equations: 1) 3A + = 10M 2) + 6M = Our goal is to find out how many plums are needed to balance one pear. That is, we need to find the value of in = nM . From equation (2): + 6M = = = 6M . Lets assign variables. Lets let represent the weight of an apple, as the So, + 6M = The problem 29 Now substitute this value of into equation (1): 3A + = 10M 3(R - 6M) + = 10M 3R - 18M + = 10M 4R - 18M = 10M 4R = 28M = 7M So one pear weighs as much as 7 plums. Lets check with equation (2): + 6M = = - 6M But if = 7M , then = 7M 6M = . So an apple weighs as much as 1 plum, which works when we check the first equation: 7M = 10M So both equations are satisfied. Therefore, the answer is 7 plums. </think> 3A + = 10M 3(M) + 7M = 10M 3M + Solve for in terms of and ** To determine how many plums are to balance single pear, we set up the equations based on the given conditions: 1. Let represent the weight of an apple, represent the weight of pear, and the weight of plum. 2. The equations are: - Equation 1: 3A + = 10M - Equation 2: + 6M = (1 apple + 6 plums equals pear) **Step 1: From Equation 2: = 6M **Step 2: 3(R 6M ) + = 10M 3R 18M + = 10M 4R 18M = 10M 4R = 28M = 7M **Final Answer:** 7 plums are needed to balance the weight of single pear. Substitute into Equation 1** 30 Qualitative Comparison of Textual Reasoning (2) Question: How many positive divisors does the integer possess, where = 73 112 134? Show-o: < Show-o does not support this task > Emu3: < Emu3 does not support this task > Janus Pro 7B: < Janus Pro 7B does not support this task > , 1 pb 2 . . . pc Hmm, let me think. So the formula seems correct. Let me try to recall the general formula here. if is 21 31, then the positive divisors are 1, 2, 3, 6. Using the formula, like 22 31. Then the divisors are 1, 2, 4, 3, 6, 12. Another example: 22 52. Then MMaDA: <think> Okay, so need to figure out how many positive divisors the integer has, where is given as 7 cubed times 11 squared times 13 to the fourth power. remember that the number of divisors of number is related to its prime factorization exponents. First, right that if number is expressed in product of primes, like number in the form = pa then the number of positive divisors is (a + 1)(b + 1)...(c + 1). Right? Because for each exponent in the prime factorization, you add 1, then multiply them all together for the total number of divisors. Let me verify that. Take an example: (1+1)*(1+1) = 2*2 = 4, which checks out. Another example: That makes 6 divisors. The formula gives (2+1)*(1+1) = 3*2 = 6, so thats correct. divisors count is (2+1)*(2+1) = 3*3=9. So, applying this formula to in the given problem: is 73 112 134. Here, the exponents are 3, 2, and 4. So, applying the number of divisors formula should be (3 + 1)(2 + 1)(4 + 1). Lets compute that step by step. First, calculate each exponent plus one: together: 4 * 3 * 5. Let me check that. 4 times 3 is 12, then 12 times 5 is 60. So the number of positive divisors is 60. Wait, but let me make sure didnt make mistake in the multiplication. Let me verify the steps again. 12*5 is 60. But just to be thorough, let me write the original number as 73 112 134. The prime factors are 7, 11, 13. Each raised to the exponents 3, 2, and 4, respectively. Since they are all prime, the formula for divisors uses the exponents as factors plus 1. Then multiplying the results: 4*3*5=60. Is there any possible way this could be wrong? Well, maybe if the exponents were different, but the original expression is already in its exponential form. Let me check if misread. For example, if one of the exponents was not what is stated, but the problem writes 73 times 112 times 134, so the exponents are 3, 2, 4, thats correct. Alternatively, could there be mistake in the multiplication? Let me check 4*3*5 again. Correct. So 60 is the correct answer. Wait, just to be thorough, lets list the combinations: any divisor is 7a 11b 13c, where 0 3, 0 2, and 0 4. The number of choices for is 4, for is 3, for is 5, so 4*3*5 = 60 possible positive divisors. Perfect. dont see any mistakes here. The answer is 60. </think> 3 + 1 is 4. 2 + 1 is 3. 4 + 1 is 5. Then, multiplying those results Yes, that seems correct. 4*3 is 12, 12*5 is 60. 4*3 is 12, 2 pc **Solution:** The integer is given as = 73 112 134. To determine the number of positive divisors of n, we use the formula for the number of divisors: if number is in the form pa 1 pb , then it has (a + 1)(b + 1) . . . (c + 1) divisors. **Step-by-Step Solution:** 1. **Identify the Exponent:** - 73: Exponent = 3 - 112: Exponent = 2 - 134: Exponent = 4 2. **Apply the Divisor Formula:** - For each prime exponent, (a + 1), and then multiply: - For exponent 3: 3 + 1 = 4 - For exponent 2: 2 + 1 = 3 - For exponent 4: 4 + 1 = 5 - Multiply the results: 4 3 5 = 60 **Answer:** The integer has 60 positive divisors. 31 Qualitative Comparison of Textual Reasoning (3) Question: \"Hope Number\" is natural number with an odd number of positive divisors. Find the largest \"Hope Number\" within 1000. Show-o: < Show-o does not support this task > Emu3: < Emu3 does not support this task > Janus Pro 7B: < Janus Pro 7B does not support this task > ak 1 pa2 2 . . . Hmm, that for example, take 9. Its prime factorization is 32. Its divisors are 1, 3, 9. MMaDA: <think> Okay, so need to find the largest \"Hope Number\" which is natural number with an odd number of positive divisors. Let me start by recalling what Hope Number is. The question says number has an odd number of divisors. means number has an odd number of different divisors. First, remember that numbers have divisors related to their prime factorization. If number has the prime factorization = pa1 , the number of divisors is (a1 + 1)(a2 + 1) (ak + 1). This product is odd if and only if every ai + 1 is odd, i.e., if every exponent ai is even. That means the number itself is perfect square (since each exponent is even). To check: which is odd. Another example, 16. divisors, which is odd. How about 15? are 1, 3, 5, 15thats four divisors, which is even. Therefore, only perfect squares have an odd number of divisors. Therefore, all Hope Numbers less than 1000 are perfect squares. Therefore, the problem reduces to finding the largest perfect square less than or equal to 1000. Lets compute that. The square root of 1000 is approximately 31.62. So we need to check the largest integer such that n2 Lets check 31. 312 = 961. 322 = 1024, which is greater than 1000. So 312 = 961 is the largest perfect square within 1000. Therefore, 961 is the largest Hope Number within 1000. But wait, let me confirm that 961 is indeed Hope Number. and 31 is prime. The exponents are all even, so the number of divisors is 2 + 1 = 3: 1, 31, 961, which is odd. checks out. And to be thorough, is there perfect square between 961 and 1000? 322 = 1024 > 1000, so there are none. Therefore, the largest Hope Number within 1000 is 961. </think> Prime factorization is 24. Its divisors are 1, 2, 4, 8, 16. Thats five Its prime factorization is 3 5, which is not perfect square. Its prime factorization is 312, That Lets count its divisors. Thats three divisors, Its divisors 1000. The largest \"Hope Number\" within 1000 is **961**. **Step-by-Step Explanation:** 1. **Understanding Hope Property:** Hope Number has an odd number of positive divisors. of divisors **if and only if it is perfect square** (because in the divisor formula, all exponents must be even so all ai + 1 are odd, and their product is odd). 2. **Largest Hope Number:** Compute perfect squares 1000. 312 = 961 and 322 = 1024. Since 322 > 1000, the largest one is 312 = 961. 3. **Verification:** Any non-square number below 1000 has an even number of divisors; only perfect squares have an odd number of divisors. **Answer:** number has an odd number 32 Qualitative Comparison of Multimodal Reasoning (1) Question: In the given figure, square ABCD is inscribed in circle with center O. Point is located on side CD. What is the value of angle APB? Show-o: The angle APB is 90 degrees. Emu3: The question seems to be mix of letters and numbers, not coherent question or statement. If you meant \"is\" or \"are\", the correct statement would be \"The measure of angle APB is\" or \"The measure of angle APB is\". If you meant \"is\" or \"are\", the correct statement would be \"The measure of angle APB is\" or \"The measure of angle APB is\". Janus Pro 7B: To find the measure of angle APB, we can use the properties of the square and the circle. 1. **Square ABCD**: The square is circumscribed around the circle, meaning all its sides are tangent to the circle. This implies that the diagonal of the square is equal to the diameter of the circle. 2. **Circle O**: The center of the circle is also the center of the square. 3. **Diagonals of Square**: The diagonals of square bisect each other and are equal in length. 4. **Angle APB**: Since ABCD is square, angle APB is right angle (90 degrees). Therefore, the measure of angle APB is 90 degrees. MMaDA: <think> To determine the measure of angle APB, we start by noting that square ABCD is circumscribed within circle O. This means that the circle is the circumcircle of the square, and the center of the circle is the same as the center of the square. Lets denote the center of the circle as O. Since ABCD is square, the diagonals AC and BD are equal in length and bisect each other at right angles. that angle AOB is 90 degrees because the diagonals of square are equal and bisect each other at right angles. Now, consider triangle APB. Since lies on side CD, we can use the property of the circle that the angle subtended by chord at the center is twice the angle subtended at any point on the circumference on the same side. In this case, AP and BP are chords of the circle meeting at point on CD (which is opposite AB). Points and are at adjacent vertices, and is on CD. The key point is that when moves along CD, angle APB is fixed, as AB is fixed chord and moves along the arc opposite AB. For square inscribed in circle, the arc AB subtends right angle at any point on the circle opposite AB (i.e., on arc CD). Therefore, for any point on CD, angle APB is always 45. This follows because AP is half the degree of the arc AB, which is 90, so: This means AP = 1 90 = 45 Thus, the measure of angle APB is 45 . </think> The measure of angle APB is 45 . 33 Qualitative Comparison of Multimodal Reasoning (2) Question: What number of large things are the same material as the purple cylinder? Show-o: There are two large things that are the same material as the purple cylinder. Emu3: 2 Janus Pro 7B: There is one large object that shares the same material as the purple cylinder, which is the large green cylinder. Answer: 1. MMaDA: <think> To determine the number of items in the image, lets analyze the objects present: 1. The large green cylinder is made of material that appears to be rubber. 2. The large yellow cube is made of material that appears to be metal. 3. The small gray cube is made of material that appears to be rubber. 4. The small cyan sphere is made of material that appears to be metal. 5. The small cyan cylinder is made of material that appears to be metal. 6. The small purple cylinder is made of material that appears to be rubber. The green cylinder is the only large object made of material that is the same as the small purple cylinder. Therefore, there is 1 large object made of the same material as the purple cylinder. </think> The number of large things that are the same material as the purple cylinder is 1. Qualitative Comparison of Multimodal Reasoning (4) Question: Divide 24 apples equally among the people in the picture, how many apples can each person get? Show-o: In the image, there are four people, each with different number of apples. The person with the most apples has the most apples. Emu3: 24. Janus Pro 7B: To divide 24 apples equally among the four people in the picture, you would divide 24 by 4. 34 24 / 4 = 6 Each person can get 6 apples. MMaDA: <think> To determine how many apples each person can get, we need to divide the total number of apples by the number of people. 24 4 = 6 So, each person can get 6 apples. </think> Each person can get 6 apples. Qualitative Examples (without Reasoning CoT) MMaDA can also efficiently handle general and simple tasks without generating CoT. Qualitative Example of Text-to-Image Generation 35 Qualitative Example of Multimodal Understanding (1) MMaDA: The image captures serene and picturesque scene of group of dolphins swimming in the ocean during sunset. The sky is painted with hues of orange, yellow, and pink, transitioning into deep blue as it meets the horizon. The sun is setting on the horizon, casting warm glow that reflects off the waters surface, creating shimmering effect. The dolphins are positioned in the foreground, with their bodies partially submerged in the water, their heads and upper bodies visible above the surface. The water appears calm, with gentle ripples around the dolphins, adding to the tranquil atmosphere. The overall composition of the image conveys sense of peace and natural beauty. Prompt: Please describe this image in detail. Qualitative Example of Multimodal Understanding (2) MMaDA: The image is vibrant and colorful landscape painting that captures the essence of sunset over field of yellow flowers. The sky is stunning blend of warm, soft hues, transitioning from deep purples and blues at the top to warm oranges and yellows near the horizon. The sun is positioned low on the horizon, casting golden glow that illuminates the sky and the field below. The field of yellow flowers, likely sunflowers, stretches across the foreground, creating sea of color that contrasts beautifully with the cooler tones of the sky. The painting is executed with broad, expressive brushstrokes, giving it dynamic and lively feel. The overall composition is balanced and harmonious, capturing the tranquility and beauty of the natural world. Prompt: Please describe this image in detail. 36 Qualitative Example of Multimodal Understanding (3) MMaDA: The image depicts modern dining area with minimalist aesthetic. The centerpiece is long dining table with sleek, dark finish, supported by four sturdy legs. The table is set with few items, including black vase, few books, and small decorative object, adding touch of elegance to the space. The table is surrounded by four blue velvet chairs with wooden legs, providing striking contrast to the dark tones of the table. The room has clean, open feel with light-colored walls and large windows that allow natural light to flood in, creating bright and airy atmosphere. The left is round mirror with simple frame, enhancing the modern look of the room. Prompt: Please describe this image in detail. Qualitative Example of Multimodal Understanding (4) MMaDA: The image depicts charming, narrow street in European city, likely in, given the architectural style and the presence of red tram car. The street is lined with tall, narrow buildings painted in soft pastel colors, including shades of pink, green, and beige. The buildings have ornate balconies and shutters, adding to the quaint charm of the scene. The street is paved with cobblestones, and there tram tracks run down the center, indicating that this is tram lane. red tram car is parked on the right. The tram is stationary, and the overall atmosphere is serene and picturesque. Prompt: Please describe this image in detail."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Peking University",
        "Princeton University",
        "Tsinghua University"
    ]
}