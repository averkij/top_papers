{
    "paper_title": "INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge",
    "authors": [
        "Angelika Romanou",
        "Negar Foroutan",
        "Anna Sotnikova",
        "Zeming Chen",
        "Sree Harsha Nelaturu",
        "Shivalika Singh",
        "Rishabh Maheshwary",
        "Micol Altomare",
        "Mohamed A. Haggag",
        "Snegha A",
        "Alfonso Amayuelas",
        "Azril Hafizi Amirudin",
        "Viraat Aryabumi",
        "Danylo Boiko",
        "Michael Chang",
        "Jenny Chim",
        "Gal Cohen",
        "Aditya Kumar Dalmia",
        "Abraham Diress",
        "Sharad Duwal",
        "Daniil Dzenhaliou",
        "Daniel Fernando Erazo Florez",
        "Fabian Farestam",
        "Joseph Marvin Imperial",
        "Shayekh Bin Islam",
        "Perttu Isotalo",
        "Maral Jabbarishiviari",
        "Börje F. Karlsson",
        "Eldar Khalilov",
        "Christopher Klamm",
        "Fajri Koto",
        "Dominik Krzemiński",
        "Gabriel Adriano de Melo",
        "Syrielle Montariol",
        "Yiyang Nan",
        "Joel Niklaus",
        "Jekaterina Novikova",
        "Johan Samir Obando Ceron",
        "Debjit Paul",
        "Esther Ploeger",
        "Jebish Purbey",
        "Swati Rajwal",
        "Selvan Sunitha Ravi",
        "Sara Rydell",
        "Roshan Santhosh",
        "Drishti Sharma",
        "Marjana Prifti Skenduli",
        "Arshia Soltani Moakhar",
        "Bardia Soltani Moakhar",
        "Ran Tamir",
        "Ayush Kumar Tarun",
        "Azmine Toushik Wasi",
        "Thenuka Ovin Weerasinghe",
        "Serhan Yilmaz",
        "Mike Zhang",
        "Imanol Schlag",
        "Marzieh Fadaee",
        "Sara Hooker",
        "Antoine Bosselut"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (\\ie, multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts. Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 2 ] . [ 1 9 9 7 9 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "INCLUDE: EVALUATING MULTILINGUAL LANGUAGE UNDERSTANDING WITH REGIONAL KNOWLEDGE MAIN CONTRIBUTORS: Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed A. Haggag, Imanol Schlag ADVISORS: Marzieh Fadaee, Sara Hooker, Antoine Bosselut AFFILIATIONS: EPFL, Cohere For AI, Cohere For AI Community, ETH Zurich, Swiss AI Initiative DATA CONTRIBUTORS: Snegha A, Alfonso Amayuelas, Azril Hafizi Amirudin, Viraat Aryabumi, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen, Aditya Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando Erazo Florez, Fabian Farestam, Joseph Marvin Imperial, Shayekh Bin Islam, Perttu Isotalo, Maral Jabbarishiviari, Borje F. Karlsson, Eldar Khalilov, Christopher Klamm, Fajri Koto, Dominik Krzeminski, Gabriel Adriano de Melo, Syrielle Montariol, Yiyang Nan, Joel Niklaus, Jekaterina Novikova, Johan Samir Obando Ceron, Debjit Paul, Esther Ploeger, Jebish Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell, Roshan Santhosh, Drishti Sharma, Marjana Prifti Skenduli, Arshia Soltani Moakhar, Bardia Soltani Moakhar, Ran Tamir, Ayush Kumar Tarun, Azmine Toushik Wasi, Thenuka Ovin Weerasinghe, Serhan Yilmaz, Mike Zhang CORRESPONDING AUTHORS: angelika.romanou@epfl.ch, antoine.bosselut@epfl.ch"
        },
        {
            "title": "ABSTRACT",
            "content": "The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (i.e., multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in variety of regional contexts. Our novel resource, INCLUDE, is comprehensive knowledgeand reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. We release INCLUDE for public use.1 1https://huggingface.co/datasets/CohereForAI/include-base-"
        },
        {
            "title": "Preprint",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "The rapid advancement of AI technologies underscores the importance of developing LLMs that are proficient across diverse linguistic and cultural contexts, ensuring fair and equitable performance for stakeholders from various language groups. However, the lack of high-quality evaluation benchmarks in many languages discourages practitioners from training multilingual LLMs to meet this challenge. This evaluation gap limits the effective deployment of LLMs for many regions, exacerbates digital divides, and inhibits the economic and societal value of AI tools in many underserved communities. The source of this gap is the multitude of challenges in evaluating LLMs for multilingual contexts. First, at meta-level, the majority of benchmarks for LLMs are only in English (Hendrycks et al., 2020, inter alia). While non-English benchmarks exist for some tasks (Singh et al., 2024; Aakanksha et al., 2024; Pozzobon et al., 2024), they usually focus on single languages (Li et al., 2023; Koto et al., 2024), specific regions (Adelani et al., 2024; Canete et al., 2020; Guevara-Rukoz et al., 2020; Cahyawijaya et al., 2022; Etxaniz et al., 2024b), or particular domain (Wang et al., 2024a), ignoring the importance of joint evaluation to trace and unlock the benefits that multilingual capabilities could bring to low-resource languages (Pfeiffer et al., 2022; Ustun et al., 2024; Aryabumi et al., 2024). Technical challenges also abound due to the manner in which multilingual datasets are often collected. Certain datasets are constructed using manually applied templates, resulting in low prompt and completion diversity (Muennighoff et al., 2022). Many more are composed of translations from high-resource languages (e.g., English; Holtermann et al., 2024; Myung et al., 2024; Lai et al., 2023; Foroutan et al., 2023). These datasets often contain errors (Ponti et al., 2020; Plaza et al., 2024) and create translationese artifacts (Vanmassenhove et al., 2021; Hartung et al., 2023; Savoldi et al., 2021; Ji et al., 2023). Most importantly, they do not accurately reflect the regional and cultural contexts captured by different languages (Aakanksha et al., 2024; Awad et al., 2020; Ramezani & Xu, 2023; Singh et al., 2024). As seen in Figure 1 (a) (Regional Knowledge), legal question posed in English, Russian, or Greek would likely reflect user located in different environment, where different laws may apply to respond correctly. Similarly, also seen in Figure 1 (a) (Cultural Knowledge), historical or cultural perspectives on the same topic may differ among the populaces of different regions. To resolve this gap, we design pipeline to collect large multilingual language understanding benchmark (i.e., INCLUDE) by collecting regional resources (e.g., educational, professional, and practical tests) that are specific to countries and originally created by native speakers of each countrys official languages. This collection avoids translationese (Bizzoni et al., 2020) and also captures cultural nuances associated with each language, enabling rigorous evaluation of how state-of-the-art models serve diverse language users around the world. In our experiments, we sample INCLUDE into two subsets for different evaluation budgets and assess an array of closed and open models on these partitions. Our results demonstrate that current models achieve high variance in performance between different languages in INCLUDE, and that models often struggle with questions requiring regional knowledge. Further analysis reveals that models score particularly low on languages on which they are not intentionally trained (i.e., limiting regional knowledge acquisition), and that the possibility of transferring global (i.e., English-aligned) perspectives improves performance for less regional topics across languages."
        },
        {
            "title": "2 PRELIMINARIES: LANGUAGE & KNOWLEDGE",
            "content": "Language availability. Languages are typically characterized as high, medium, or low resource depending on reported language availability (Joshi et al., 2020), i.e., the amount of available data in language that is available online. Interestingly, the language availability of documents used for training models (Penedo et al., 2024; Xue, 2020; Conneau et al., 2020; Computer, 2023; Ustun et al., 2024; Singh et al., 2024) differs drastically from the language distribution of non-English LLM benchmarks, with the latter being more scarce. Inspired by this discrepancy, we include 44 languages in our INCLUDE benchmark. In Figure 1, we characterize the availability of the included languages based on their reported availability in the mC4 corpus (Xue, 2020), and in Table 8 show further detailed metadata for each language. Language represents regional knowledge. For LLM-based systems to practically useful, they must enable interaction in the preferred languages of their users and be knowledgeable of the environments"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overview of INCLUDE. (a) Motivation: Multilingual benchmarks must reflect the (b) cultural and regional knowledge of the language environments in which they would used. INCLUDE is multilingual benchmark compiled from academic, professional, and occupational license examinations reflecting regional and cultural knowledge in 44 languages. of those users. We define regional knowledge as the specific information, culture, and practices related to local environment that is relevant for users context. However, LLMs such as GPT-4 tend to exhibit Western bias (Tao et al., 2024) due to the overrepresentation of Western text in training data (AlKhamissi et al., 2024). In INCLUDE, we specifically include questions encompassing the regional and cultural knowledge of diverse set of high, medium, and low-resource languages."
        },
        {
            "title": "3 THE INCLUDE BENCHMARK",
            "content": "INCLUDE is dataset of 197,243 MCQA pairs from 1,926 examinations across 44 languages and 15 scripts. These examinations are collected from local sources in 52 countries, representing rich array of cultural and regional knowledge. All questions in the dataset are presented in their native languages and scripts. In this section, we describe the data collection procedure for INCLUDE, as well as additional categorical labels we assign to each question in the dataset for later analysis."
        },
        {
            "title": "3.1 DATA COLLECTION",
            "content": "To construct INCLUDE, we collect sources of multiple-choice exams in collaboration with native speakers and regional associations. We primarily focused on three types of exams: Academic Exams: Exams from variety of subjects (e.g., Humanities, STEM, etc.) at different levels (e.g., middle & high school, university), including country-specific national entrance exams. Professional Certifications & Licenses: Exams issued by industry-specific regulatory bodies for specialized fields, e.g., licensing exams for areas such as legal and medical practice. Regional Licenses: Exams administered by regional authorities that assess specific qualifications, such as driving and marine licenses. We design INCLUDE to assess multilingual capabilities that span beyond academic knowledge to cultural and region-specific understanding. Our data collection focuses on license and certification exams that capture regional knowledge of specific countries (in their official languages), and nontranslated academic content from the humanities and social sciences to capture cultural knowledge."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of the collected data grouped by script. We depict the languages associated with each script, the total samples in each script, and the percentage of the samples that were collected from new sources that have not been published by the community yet. From the collected sources, we extract the multiple-choice questions with their corresponding options and correct answers. More specifically, as this data came in different formats (e.g., PDFs, Javascript HTML forms), we use multiple pipelines to extract QA samples from these sources and curate them in machine-readable manner. The goal of this stage was to automate data extraction and then rely on human evaluation for verification and feature annotation. Quality Control with Native Speakers. After automatic extraction, we provide native speakers (co-authors in this work) with parsed multiple-choice questions to ensure they were extracted correctly from source documents. In cases of extraction mistakes, annotators performed manual correction of parsed questions and answer options using the original document as guide. In addition to performing corrections, annotators also filtered out samples that referred to images or tables, and verified that samples that rely on additional context (e.g., reading comprehension) include the reference text in the question field. Finally, annotators also labeled each question with additional exam metadata, such as the language of the MCQ, its topic in both English and the original language, the academic level (if relevant), and the country of origin. In total, we parsed and verified 118,606 samples across 1,926 exam sources, amounting to 60.2% of the total data in INCLUDE. Rounding out the Benchmark. To round out our benchmark, we also consolidate existing datasets with extensive domain coverage in single non-English languages: ArabicMMLU (Koto et al., 2024), ChineseMMLU (Li et al., 2023), TurkishMMLU (Yuksel et al., 2024), PersianMMLU (Ghahroodi et al., 2024) and VNHSGE (Dao et al., 2023), as well as multilingual benchmark with limited domain coverage across multiple European languages: EXAMS (Hardalov et al., 2020). We repurpose 78,637 samples from these published benchmarks, amounting to 39.8% of the data in INCLUDE. In sum, INCLUDE is the largest collection of multilingual exam data to-date. It is composed of 197,243 QA pairs from both novel and existing sources, and covers examinations from more than 1,926 exams in 44 different languages, 15 scripts, and 58 knowledge domains. Figure 1 and Appendix Table 8 summarize the language, domain, and knowledge diversity of the INCLUDE benchmark."
        },
        {
            "title": "3.2 CATEGORIZING KNOWLEDGE",
            "content": "The language breadth of INCLUDE provides the opportunity to investigate what factors drive multilingual performance. Consequently, we annotate INCLUDE samples with category labels corresponding to factors such as the topic of question and its region-specificity. Given the prohibitive cost of performing sample-level annotation, we only perform coarse annotation by labeling the exam sources of QA pairs, rather than individual samples. We describe our categorization schemes below."
        },
        {
            "title": "Preprint",
            "content": "Academic Domain. We manually categorized 1,926 unique exams, following the methodology in Hendrycks et al. (2020). Our categorization follows two-level taxonomy: high-level academic area (e.g., Humanities), and particular academic field within this area (e.g., History, Philosophy, Literature).2 Each exam is categorized based on its title, which indicates its topic (e.g., Greek History) and associated level (e.g., high school, undergraduate, professional certification). Figure 7 provides breakdown of the number of exam samples per language, organized by this taxonomy. Regionality. To account for regional knowledge, we categorize exam questions into two major groups: region-agnostic and region-specific knowledge. Region-agnostic questions do not require knowledge of particular regions (e.g., mathematics, physics), and their answers should remain common regardless of the language in which question is posed. In total, 34.4% of all questions collected were classified as region-agnostic. In contrast, region-specific questions require knowledge that may depend on particular cultural or geographical context. This category is further divided into three sub-categories: Explicitly Regional: question is classified as region-explicit when it pertains to legal, regulatory, or procedural knowledge of regions. Examples include questions about local laws, certifications, clinical guidelines, or licensing requirements (see Figure 1(a)). 18.8% of all questions were region-explicit. Cultural: Language often serves as an implicit marker of culture. For instance, in Figure 1(a), the answer to question about historical figures in Greek exam may reflect different perspective than similar question posed for Persian exam. We categorize questions as cultural when they pertain to regions cultural or historical context. This category includes questions for subjects inherently tied to regions language, history, or social norms. 16.4% of questions were classified as cultural. Implicitly Regional: Finally, the region-implicit category is catch-all for other questions whose answers may depend on certain degree of regional knowledge understanding. These questions are not explicitly regional or culture-related, but may require regional context to answer correctly. For example, business practices may be different depending on region, even if the underlying theory is common in many places. In total, 30.4% of all questions collected were classified as region-implicit. Detailed annotation procedures for these categories are described in Appendix A.4, and general statistics about regional labels per academic area and academic field are provided in Figure 7."
        },
        {
            "title": "4 EXPERIMENTAL SETUP",
            "content": "In this section, we describe our experimental settings for evaluating models on INCLUDE."
        },
        {
            "title": "4.1 DATA SELECTION",
            "content": "The breadth of INCLUDE (197,243 QA pairs in 44 languages) makes it amenable to many evaluation use cases, including monolingual evaluation in 44 languages. However, for multilingual evaluation, this same scale is prohibitively expensive for many researchers.3 Consequently, we curate two subsets of INCLUDE for benchmarking multilingual LLMs in different resource settings. INCLUDE-BASE: This subset uniformly samples 22,635 QA pairs (12% of INCLUDE) across languages, knowledge tasks, and academic levels. The goal of this subset is to develop multilingual benchmark with broad language and task coverage. Each language has maximum of 550 samples, with 500 drawn from domains that correspond to regional knowledge and 50 from STEM subjects. INCLUDE-LITE: lightweight subset, uniformly drawn from INCLUDE-BASE, designed for rapid assessment of multilingual LLMs with total of 10,770 samples (6% of INCLUDE). The upper limit per language is 250 samples and only includes region-specific domains. For standardization (and alignment with prior benchmarks; Hendrycks et al., 2020), INCLUDE-BASE and INCLUDE-LITE contain only multiple-choice questions with four answer options. Questions from INCLUDE with fewer than four options were omitted during sampling, and questions with more than four options were pruned of options until only four remained. In the following sections, we benchmark models (5.1) and perform analysis (5.2-5.3) on INCLUDE-BASE and INCLUDE-LITE.4 2This taxonomy is adapted from the Outline of Academic Disciplines found on Wikipedia. 3The cost of evaluating INCLUDE using GPT-4o with 5-shot demonstrations exceeded $1000. 4We will publicly release both INCLUDE-BASE and INCLUDE-LITE."
        },
        {
            "title": "4.2 MODELS",
            "content": "We assess INCLUDE on GPT-4o (Achiam et al., 2023) as state-of-the-art multilingual and generalpurpose model. We also investigate the role of scaling by benchmarking models that self-report parameters, comparing the larger Llama-3.1-70B-Instruct (Dubey et al., 2024), the Aya-expanse-32B (Aryabumi et al., 2024), and the Qwen2.5-14B (Yang et al., 2024) 14-billion parameter model with Llama-3.1-Instruct-8B (Dubey et al., 2024), Aya-expanse-8B (Aryabumi et al., 2024) and Qwen2.57B. Additionally, we benchmark Mistral-7B (Jiang et al., 2023) and Gemma-7B (Team et al., 2024) along with their Instruct variants. We note that some of the models evaluated neither explicitly claim to support multiple languages nor disclose the languages they were pretrained on. However, in practice, they are heavily adopted in multilingual use cases relative to explicitly multilingual models. Furthermore, even reportedly multilingual models (e.g., Aya-Expanse, which supports 23 languages) do not support all 44 languages included in our benchmark. Prompting. Following Hendrycks et al. (2020), we report both 5-shot and zero-shot scores. For the zero-shot setting, we employ Chain-of-Thought (CoT; Wei et al., 2022) approach by appending the translation of lets think step by step to the prompt (Kojima et al., 2022). We evaluate models using both (1) In-Language (IL) Prompts, which present the prompt instructions in the same language as the sample, and (2) English (Eng.) Prompts, which provide the prompt instructions in English. For both prompting language settings, we also test setting where we add Regional (Reg.) prefix to the (either in-language or English) prompt to explicitly contextualize the region and language of the sample, asking the models to consider the cultural and regional characteristics. The maximum generation lengths for the 5-shot and zero-shot CoT configurations are set to 512 and 1024 tokens. Finally, we additionally evaluate INCLUDE using the Harness-Eval framework (Gao et al., 2024) using both In-Language Prompts and English Prompts."
        },
        {
            "title": "5.1 GENERAL PERFORMANCE",
            "content": "Table 1 shows the performance of all models evaluated across the 44 languages in INCLUDE-BASE. For larger models, e.g., GPT-4o, Llama-3.1-70B-Instruct, Aya-expanse-32B and Qwen2.5-14B, we provide results for both 5-shot and zero-shot CoT, and report only 5-shot accuracy for other models. Among all models, GPT-4o achieves the highest performance, reaching an accuracy of 77.1% across all domains and examples. We observe that CoT prompting moderately enhances GPT4os performance, particularly in Professional and STEM-related exams  (Table 2)  , where the most substantial improvements were seen. In contrast, the smallest gains were observed in exams related to Licenses and the Humanities. Drawing on prior studies that compare CoT and non-CoT prompting strategies across different domains (Sprague et al., 2024), we hypothesize that this observation is due to reasoning skills required in professional examinations (e.g., medicine, law), and computation-heavy subjects in STEM. In contrast, we observe significant performance drops by Llama-3.1-70B-Instruct and Qwen2.5-14B when using CoT prompting on INCLUDE-BASE, with the largest drops on subjects involving more mathematical reasoning  (Table 2)  . For smaller models (8B parameters), Gemma-7B delivers the best overall performance, with Qwen2.5-7B and Qwen2.5-7B-Instruct closely behind. While Gemma-7B excels in the Humanities and Licenses categories, the Qwen models surpass others in STEM, Applied Sciences, and Professional domains  (Table 2)  . When comparing models from the same family across different scales, we observe that the Aya-expanse-32B model outperforms the 8B model by 12% on average, while the Qwen2.5-14B model shows an average 7% improvement over its 7B counterpart on INCLUDE-BASE across prompting settings. As the pretraining data remained consistent across the different sizes within both the Aya-expanse and Qwen2.5 model families, we conclude that, with similar training data, increasing model size significantly enhances multilingual capabilities. Interestingly, we see little benefit to instruction-tuning for improving performance on INCLUDE-BASE. Most instruction-tuned models perform slightly worse or on par with their base counterpart, with an outlier performance drop of 15% across prompting settings for Gemma-7B-Instruct. possible explanation for this gap is that instruction-tuned models may have been post-trained predominantly on English data, potentially diminishing multilingual capabilities acquired during pretraining."
        },
        {
            "title": "Model",
            "content": "GPT-4o - 5-shot - Zero-shot CoT Llama-3.1-70B-Inst. - 5-shot - Zero-shot CoT Aya-expanse-32B - 5-shot - Zero-shot CoT Qwen2.5-14B - 5-shot - Zero-shot CoT Aya-expanse-8B Mistral-7B (v0.3) Mistral-7B-Inst. (v0.3) Gemma-7B Gemma-7B-Inst. Qwen2.5-7B Qwen2.5-7B-Inst. Llama-3.1-8B Llama-3.1-8B-Inst. INCLUDE-LITE INCLUDE-BASE # Langs"
        },
        {
            "title": "IL\nPrompt",
            "content": "Eng. Prompt Reg. + IL Prompt Reg. + Eng. Prompt"
        },
        {
            "title": "IL\nPrompt",
            "content": "Eng. Prompt Reg. + IL Prompt Reg. + Eng. Prompt - - 22 23 - - - - 22 22 - - 77.1 78.2 70.5 60.6 52.6 50.6 60.9 46. 37.6 44.0 43.5 54.4 39.2 53.4 53.4 50.9 53.4 76.2 78.4 70.4 55.3 57.2 57.1 61.3 50.7 46.3 45.0 44.6 54.9 40.2 54.8 54.2 52.3 54. 76.3 77.7 70.6 60.2 49.0 52.5 60.9 46.5 38.1 44.0 44.2 54.3 38.7 53.3 52.8 50.9 52.7 76.3 77. 70.6 55.4 60.0 58.0 60.8 51.4 48.0 45.2 44.7 54.9 39.7 54.2 53.7 51.9 53.4 77.3 79.0 70.6 60. 52.4 51.4 61.4 47.3 37.2 43.3 43.6 54.5 38.7 54.1 53.8 51.0 53.4 76.3 78.9 70.7 56.0 56.6 57. 61.7 51.0 46.0 44.9 44.5 54.9 39.7 55.2 54.6 51.8 54.6 76.2 77.6 70.6 60.6 49.7 52.9 61.1 47. 37.9 43.8 44.2 54.2 38.1 54.0 53.2 51.0 53.0 76.2 78.5 70.6 55.6 60.0 57.8 61.0 51.6 47.8 45.0 44.7 54.7 39.2 54.5 53.9 51.6 54. Table 1: Results on INCLUDE-LITE and INCLUDE-BASE. In-language Prompt (IL) reports model accuracy when the prompt instructions are presented in the same language as the sample. English Prompt (Eng.) reports model accuracy when the prompt instructions are provided in English. Inlanguage Regional Prompt (Reg. + IL) reports model accuracy when regional prefix is added to the In-language Prompt. English Regional Prompt (Reg. + Eng.) reports model accuracy when regional prefix is added to the English Prompt. # Langs reports the number of languages from INCLUDE publicly reported to be intentionally included in the pretraining data of each model. Overall, we observe minimal performance differences across the various prompting settings, except in the CoT setting where Qwen2.5-14B and Aya-expanse benefit significantly from English prompts (over the in-language variants) and Llama-3.1-70B-Instruct performs better with in-language prompts. Outside of these exceptions, English prompts offer modest performance improvements for most models, but this improvement remains within 12% of in-language prompt performance. Apart from Aya-expanse-8B, specifying the region of the question in the prompt prefix does not generally improve performance, likely because the language of the question serves as an implicit proxy of the relevant region of the question for most languages in our benchmark. Finally, we observe that performance on INCLUDE-LITE is nearly equivalent to INCLUDE-BASE across all models, with differences within 1%, demonstrating its suitability for resource-constrained evaluation settings. Likewise, when using the Harness evaluation framework, model performance remains consistent, with results also within 1%  (Table 4)  ."
        },
        {
            "title": "5.2 LANGUAGE ANALYSIS",
            "content": "To better understand how LLMs perform on questions in languages seen and unseen during pretraining, we take deeper look into three open models, i.e., Aya-expanse-8B, Aya-expanse-32B, and Qwen2.57B, for which we have details surrounding pretraining data (and its associated language distribution). In this analysis, we specifically test three language exposure scenarios: performance on languages the model has been intentionally5 trained on, performance on languages the model was not reported to be trained on but for which the corresponding script was reported to be trained on, and performance on completely unseen languages and scripts during pretraining. Figure 3 presents the language-stratified performance of these models on INCLUDE-BASE. As expected, the models demonstrate better performance on languages that were reported as part of their pretraining data (Trained on Language). All models also demonstrate some degree of knowledge transfer to languages they were not trained on but which share the same script as languages in their pretraining data (Trained on Script). In this scenario, Aya-expanse-32B achieves 44.1% accuracy, 5We denote intentionally in this context to mean that the authors specifically reported this language as being covered in the pretraining corpus of the model."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Performance of models stratified by language using in-language prompting. Results are grouped by whether the language was explicitly included in the pretraining dataset of the model (Trained on Language), whether similar language with the same script was in the pretraining corpus (Trained on Script), or whether there was no linguistically similar language in the pretraining corpus (Neither). Color dotted lines represent average performance for each category for particular model. Black dotted lines represent average performance across all script-aligned languages. while Qwen2.5-7B reaches 51.7% accuracy, aligning with previous research that suggests shared scripts enable cross-lingual transfer between languages (Muller et al., 2021; Xhelili et al., 2024). Other factors may also contribute to each models performance on unseen languages, though, such as cross-lingual transfer across topologically-similar languages. For example, the presence of Turkish data may enhance the models performance on Azerbaijani (Senel et al., 2024). Pretraining data contamination, where languages that were not intended to be in the pretraining data may still be unintentionally included (Blevins & Zettlemoyer, 2022), may also contribute to these transfer results. Lastly, we observe that all three models perform poorly on languages whose scripts were not represented in the pretraining corpus (Neither in Figure 3) with the exception of the Georgian language for the Aya-expanse-32B model. Data contamination is also less of confounding factor in these cases as language identification is more robust for unique scripts (Kargaran et al., 2023). In these cases, the model often performs worse than random, likely due to not being able to produce responses in the correct format (further details in 5.4)."
        },
        {
            "title": "5.3 REGIONAL & ACADEMIC DOMAIN KNOWLEDGE PERFORMANCE",
            "content": "Using the category labels outlined in Section 3.2, we conduct stratified analysis of five-shot GPT4os ability to answer different types of regional questions in INCLUDE-BASE. We first note that overall performance differs strongly between languages. In some languages, the model consistently performs well across all academic domains and regional knowledge types, while in others, it struggles across the board (see Appendix Tables 9, 10, and 11). However, lower performance in certain languages is often linked to questions requiring regional knowledge (e.g., historical knowledge, professional certifications, medical licenses), suggesting that the models knowledge about regions varies significantly and that its performance across languages reflects this differential. Among regional categories, in Appendix Figure 6, we see the model performs worst on cultural questions, followed by region-explicit questions. Professional certification exams"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: GPT-4o performance (In-language Prompt) on regional history exams (cultural) and global history exams from that region (region-implicit) based on total of 11,148 questions from INCLUDE. In each language (except Telugu), models perform better on the global history exam than the regional history exam. in different regions are particular challenge for GPT-4o (average 68.6%). In Persian, the models accuracy on certification exams is notably low (43.2%), whereas it performs better on subjects such as Geography and Sociology (over 66%). Similarly, in Greek, GPT-4o achieves an average accuracy of 71.3%, but only 54.1% on medical license questions. Further language performance comparisons across various academic disciplines are shown in Appendix Figure 5. Despite these findings, we note that performance across regional question types (e.g., region-agnostic, cultural) is not deconfounded from other features such as topical difficulty and academic level. Indeed, we observe that region-agnostic questions are among the most challenging as models struggle with the mathematical nature of many STEM topics (Frieder et al., 2023; Borges et al., 2024). On average, subjects such as Mathematics and Chemistry show the lowest average accuracies (Appendix Figure 6). Unfortunately, as the region label of any question depends on its subject, it is naturally confounded with the models ability in that subject, regardless of whether the subject is regional or not. History is one of the few fields where we can achieve more controlled study of regional difference as exams can be divided into two categories: those testing region-specific historical knowledge (e.g., Armenian history, which we label as cultural) and general history taught in particular region (e.g., World History; region-implicit6). In Figure 4, we observe that for all languages that have History exams with both cultural and region-implicit labels (with the exception of Telugu), the model performs better on the general history exams, indicating lack of cultural knowledge necessary to answer questions for more region-specific topics. Overall, the variance in performance among different regional categories in our results suggests that model performance on INCLUDE may not be rooted in across-the-board language comprehension issues, but instead in grasping specialized regional knowledge for different languages."
        },
        {
            "title": "5.4 CHALLENGES IN MULTILINGUAL EVALUATION",
            "content": "Multilingual LLMs do not generate outputs the same way in all languages. Throughout our experiments, we observed that models did not always follow the exact format primed by the 5-shot examples or zero-shot instructions (Format Errors in Table 3), which required generating longer output length to rectify. To empirically measure the impact of this seemingly minute evaluation design choice, we assess the five-shot performance of GPT-4o on INCLUDE-BASE across various decoded output lengths, focusing specifically on its ability to generate correct response within the first follow-up tokens (k = 50, 100, 200, and 512). As in our main results, we use the 5-shot prompt template from Hendrycks et al. (2020), without explicitly instructing the model how to generate correct answers. Instead, the model must induce the format from the provided demonstrations. Table 12 presents the performance of GPT-4o across the 44 languages of the benchmark, evaluated under four different generation window settings. On average, the model shows 3.1% performance 6We note World History as region-implicit because the manner in which the subject is taught and evaluated may vary between regions, even if the subject material seems like it should be universal."
        },
        {
            "title": "Preprint",
            "content": "improvement when increasing the generation length window from 50 to 512 tokens. However, this effect is not uniform; some languages experience significant improvements, such as Uzbek (+17.2%), Armenian (+13.1%), and Malayalam (+12.9%). Many others remain largely unaffected. manual review and analysis of the generated outputs in languages with the largest gains reveal that the model often generates verbose responses, explaining the context before providing the final answer (i.e., ignoring the formatting in the demonstrations, but reaching the correct response). One possible explanation for these discrepancies is the models limited ability to leverage in-context learning effectively in certain languages, potentially due to imbalances in language resources during the alignment phase (Zhang et al., 2024c). When models are prompted with instructions in English, we observe modest performance improvement of 1.5% across all models. Interestingly, in specific cases (e.g., experiments with the Aya-expanse family models, Qwen2.5-14B, and Llama-3.1-Instruct-70B using CoT prompting), we observe significant changes in the frequency of format errors  (Table 3)  . However, this change in frequency does not appear to impact the performance of the models on regional knowledge understanding (Answer Acc., Table 3), suggesting that the choice of instruction language may not significantly enhance or impair the models ability to reflect regional knowledge, but rather primarily influence the format and consistency of the outputs in certain models. Overall, these results demonstrate that standardizing evaluation is challenge in tasks that may lead to different output patterns (Nayab et al., 2024), which is compounded in multilingual evaluations. In particular, given the incentive to lower generation lengths at test time (to lower inference or API costs), reliable multilingual assessment requires anticipating how models will produce outputs in different languages and scripts, and how evaluation settings might inadvertently affect measures of performance. Specifically, practitioners should be reflective about penalizing models for format errors when assessing capabilities and intentionally probe for format errors, given they may not speak or read the languages being evaluated."
        },
        {
            "title": "6 RELATED WORK",
            "content": "In recent years, the creation of benchmarks has substantially improved the evaluation of LLMs. Pioneering efforts such as GLUE and SuperGLUE (Wang et al., 2018; 2019) played an important role in advancing tasks related to language understanding. Recent benchmarks, such as MMLU (Hendrycks et al., 2020), HellaSwag (Zellers et al., 2019), ARC (Clark et al., 2018), GSM8K (Cobbe et al., 2021), and BigBench (Srivastava et al., 2022), focus on evaluating models for more complex knowledge comprehension and reasoning. In addition to being used as final evaluations, they are often used to monitor and compare LLM performance during pretraining, rather than more traditional measures such as perplexity (Penedo et al., 2024). However, these benchmarks evaluate models only using English data, limiting their use in the development of multilingual LLMs. Evaluating multilingual models requires benchmarks that assess models for these same complex abilities across diverse languages. However, initial multilingual benchmarks focus on more basic linguistic abilities (Conneau et al., 2018; Ponti et al., 2020), and collections of such tasks (Liang et al., 2020; Hu et al., 2020; Ruder et al., 2021; Asai et al., 2023; Ahuja et al., 2023a;b). Furthermore, these benchmarks generally include only few high-resource languages or are based on translations from high-resource languages, limiting the assessment of regional knowledge comprehension and reasoning capabilities. Finally, similar to English evaluations, multilingual benchmarks have trended toward saturation (Zhang et al., 2024a; Wang et al., 2024b). Although there have been efforts to create language-specific MMLU-like datasets, coverage remains limited to few languages (Li et al., 2023; Koto et al., 2024; Ghahroodi et al., 2024). Most similar to our proposed effort, the Exams dataset (Hardalov et al., 2020) encompasses questions covering 16 languages across 24 topics collected from elementary and high school science curricula. The Aya dataset (Singh et al., 2024) also includes sustantial release, covering 513 million data points across 101 languages, including in-language evaluation sets developed by native speakers assessing general performance and safety. However, the Aya dataset is not focused on collecting in-language exams. Our work develops multilingual benchmark encompassing 44 languages, integrating questions from academic and professional examinations and broadening the evaluation spectrum of multilingual LLMs to include region-specific knowledge."
        },
        {
            "title": "Preprint",
            "content": "Finally, rich body of work has developed benchmarks to assess LLMs for cultural understanding. Arora et al. (2024) evaluate various aspects of culture and language using questions from community forums on 15 topics. Aakanksha et al. (2024) curate safety dataset that encompasses local nuances. Myung et al. (2024) compile questions about food, sports, holidays, education, and family translated into multiple languages. Synthetic benchmarks, such as NormAd (Rao et al., 2024), generate culturally-rooted stories to measure how well models grasp societal norms. Tools such as CultureBank source cultural descriptions from online platforms such as TikTok (Shi et al., 2024), offering alternative ways to ground cultural benchmarks in dynamic, real-world knowledge. Etxaniz et al. (2024a) analyze the LLM understanding of regional and non-regional knowledge using parallel evaluation dataset in English and Basque. Beyond benchmarking, Chiu et al. (2024) proposed tool that facilitates human-machine collaboration for co-creation of complex datasets, challenging the multicultural understanding and adaptability of LLMs. In contrast to this line of work, our study goes beyond culture as dimension of regional knowledge, and also assesses LLMs on questions that reflect region-related factual knowledge (e.g., professional standards, law, clinical guidelines)."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We release INCLUDE, comprehensive multilingual evaluation suite designed to assess performance of large language models (LLMs) across wide range of subjects and languages for rich array of cultural and regional knowledge. INCLUDE contains 197,243 MCQA pairs from 1,926 examinations across 44 languages and 15 scripts collected from 52 countries. Overall, our results from evaluating 15 models on INCLUDE indicate there remains considerable room for model improvement in multilingual regional knowledge understanding and that regional knowledge understanding varies significantly across languages. INCLUDE offers researchers and developers novel and valuable benchmark for evaluating and improving the regional understanding abilities of future multilingual models in the language environments where they would be used."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "The primary goal of our benchmark is to reduce disparities in regional knowledge understanding across languages, addressing the inequities in access to technology and its benefits that often result from these gaps. We have designed the benchmark to reflect diverse range of linguistic, cultural, and regional contexts, sourcing data from local and region-specific exam materials. Throughout the data collection process, we ensured that no private or sensitive information was included. We only collected data from exams for which there were no license issues. Our benchmark aims to capture and integrate essential cultural knowledge across many languages. We emphasize the importance of local engagement and encourage developers using this benchmark for the evaluation of monolingual models to actively consult with local stakeholders. To promote equitable access to technology and the development of multilingual large language models, we release our benchmark to the community. Limitations In line with responsible research practices, we acknowledge several limitations in our work. First, INCLUDE spans 44 languages with varying levels of resource availability, leading to different distributions of questions from various academic disciplines across languages. This disparity complicates direct comparisons between performance in disciplines across languages. Additionally, the difficulty of exams may vary not only between languages but also within the same language if exams originate from different sources. However, this limitation is also reflection of one of the strengths of our benchmark. Questions are sourced from local examinations that reflect the regional and cultural nuances of the environments in which those exams are implemented, which was our motivation for new evaluation benchmark. Naturally, this precludes exact correspondence between questions across languages. Another practical limitation is that our regional knowledge labels were annotated at the exam topic level, rather than at the individual question level, so questions are classified based on their overarching topic, rather than individual content."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We plan to release two subsets, INCLUDE-BASE and INCLUDE-LITE, alongside the associated documentation and code for data processing and evaluation. These resources will be made publicly"
        },
        {
            "title": "Preprint",
            "content": "available upon acceptance. To mitigate the risk of data contamination during fine-tuning, INCLUDE will be released in incremental batches over period of four months. Further details regarding experimental settings, including resource utilization, hyperparameters, and baseline configurations, can be found in Section 4 and Appendix A.5, which provide comprehensive overview of our methodology."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We thank Deniz Bayazit, Badr Alkhamissi, Mete Ismayilzada and Reza Banaei for reading and providing comments on drafts of this paper. We also gratefully acknowledge the support of the Swiss National Science Foundation (No. 215390), Innosuisse (PFFS-21-29), the EPFL Center for Imaging, Sony Group Corporation, and the Allen Institute for AI. This work was supported as part of the Swiss AI Initiative by grant from the Swiss National Supercomputing Centre (CSCS) under project ID a06 on Alps."
        },
        {
            "title": "REFERENCES",
            "content": "Aakanksha, Arash Ahmadian, Beyza Ermis, Seraphina Goldfarb-Tarrant, Julia Kreutzer, Marzieh Fadaee, and Sara Hooker. The multilingual alignment prism: Aligning global and local preferences to reduce harm, 2024. URL https://arxiv.org/abs/2406.18682. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. David Ifeoluwa Adelani, Jessica Ojo, Israel Abebe Azime, Jian Yun Zhuang, Jesujoba O. Alabi, Xuanli He, Millicent Ochieng, Sara Hooker, Andiswa Bukula, En-Shiun Annie Lee, Chiamaka Chukwuneke, Happy Buzaaba, Blessing Sibanda, Godson Kalipe, Jonathan Mukiibi, Salomon Kabongo, Foutse Yuehgoh, Mmasibidi Setaka, Lolwethu Ndolela, Nkiruka Odu, Rooweither Mabuya, Shamsuddeen Hassan Muhammad, Salomey Osei, Sokhar Samb, Tadesse Kebede Guge, and Pontus Stenetorp. Irokobench: new benchmark for african languages in the age of large language models, 2024. URL https://arxiv.org/abs/2406.03368. Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng, Krithika Ramesh, Prachi Jain, Akshay Nambi, Tanuja Ganu, Sameer Segal, Maxamed Axmed, et al. Mega: Multilingual evaluation of generative ai. arXiv preprint arXiv:2303.12528, 2023a. Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Maxamed Axmed, Kalika Bali, et al. Megaverse: Benchmarking large language models across languages, modalities, models and tasks. arXiv preprint arXiv:2311.07463, 2023b. Badr AlKhamissi, Muhammad ElNokrashy, Mai AlKhamissi, and Mona Diab. Investigating cultural alignment of large language models. arXiv preprint arXiv:2402.13231, 2024. Shane Arora, Marzena Karpinska, Hung-Ting Chen, Ipsita Bhattacharjee, Mohit Iyyer, and Eunsol Choi. Calmqa: Exploring culturally specific long-form question answering across 23 languages, 2024. URL https://arxiv.org/abs/2406.17761. Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, Kelly Marchisio, Max Bartolo, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee, Ahmet Ustun, and Sara Hooker. Aya 23: Open weight releases to further multilingual progress. 2024. URL https://arxiv.org/abs/2405.15032. Akari Asai, Sneha Kudugunta, Xinyan Velocity Yu, Terra Blevins, Hila Gonen, Machel Reid, Yulia Tsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi. Buffet: Benchmarking large language models for few-shot cross-lingual transfer. arXiv preprint arXiv:2305.14857, 2023."
        },
        {
            "title": "Preprint",
            "content": "Edmond Awad, Sohan Dsouza, Azim Shariff, Iyad Rahwan, and Jean-Francois Bonnefon. Universals and variations in moral decisions made in 42 countries by 70,000 participants. Proceedings of the National Academy of Sciences, 117(5):23322337, 2020. Yuri Bizzoni, Tom Juzek, Cristina Espana-Bonet, Koel Dutta Chowdhury, Josef van Genabith, and Elke Teich. How human is machine translationese? comparing human and machine translations of text and speech. In Marcello Federico, Alex Waibel, Kevin Knight, Satoshi Nakamura, Hermann Ney, Jan Niehues, Sebastian Stuker, Dekai Wu, Joseph Mariani, and Francois Yvon (eds.), Proceedings of the 17th International Conference on Spoken Language Translation, pp. 280290, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.iwslt-1.34. URL https://aclanthology.org/2020.iwslt-1.34. Terra Blevins and Luke Zettlemoyer. Language contamination helps explains the cross-lingual capabilities of English pretrained models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 35633574, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.233. URL https://aclanthology.org/ 2022.emnlp-main.233. Beatriz Borges, Negar Foroutan, Deniz Bayazit, Anna Sotnikova, Syrielle Montariol, Tanya Nazaretzky, Mohammadreza Banaei, Alireza Sakhaeirad, Philippe Servant, Seyed Parsa Neshaei, Jibril Frej, Angelika Romanou, Gail Weiss, Sepideh Mamooler, Zeming Chen, Simin Fan, Silin Gao, Mete Ismayilzada, Debjit Paul, Philippe Schwaller, Sacha Friedli, Patrick Jermann, Tanja Kaser, Antoine Bosselut, EPFL Grader Consortium, and EPFL Data Consortium. Could chatgpt get an engineering degree? evaluating higher education vulnerability to ai assistants. Proceedings of the National Academy of Sciences, 121(49):e2414955121, 2024. doi: 10.1073/pnas.2414955121. URL https://www.pnas.org/doi/abs/10.1073/pnas.2414955121. Samuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji, Genta Indra Winata, Bryan Wilie, Rahmad Mahendra, Christian Wibisono, Ade Romadhony, Karissa Vincentio, Fajri Koto, et al. Nusacrowd: Open source initiative for indonesian nlp resources. arXiv preprint arXiv:2212.09648, 2022. Jose Canete, Gabriel Chaperon, Rodrigo Fuentes, Jou-Hui Ho, Hojin Kang, and Jorge Perez. Spanish pre-trained bert model and evaluation data. In PML4DC at ICLR 2020, 2020. Yu Ying Chiu, Liwei Jiang, Maria Antoniak, Chan Young Park, Shuyue Stella Li, Mehar Bhatia, Sahithya Ravi, Yulia Tsvetkov, Vered Shwartz, and Yejin Choi. Culturalteaming: Ai-assisted interactive red-teaming for challenging llms (lack of) multicultural knowledge, 2024. URL https://arxiv.org/abs/2404.06664. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168, 2021. Together Computer. Redpajama: an open dataset for training large language models, October 2023. URL https://github.com/togethercomputer/RedPajama-Data. Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. arXiv preprint arXiv:1809.05053, 2018. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. UnIn Proceedings of the 58th Ansupervised cross-lingual representation learning at scale. nual Meeting of the Association for Computational Linguistics, pp. 84408451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https://www.aclweb.org/anthology/2020.acl-main.747."
        },
        {
            "title": "Preprint",
            "content": "Xuan-Quy Dao, Ngoc-Bich Le, The-Duy Vo, Xuan-Dung Phan, Bac-Bien Ngo, Van-Tien Nguyen, Thi-My-Thanh Nguyen, and Hong-Phuoc Nguyen. Vnhsge: Vietnamese high school graduation examination dataset for large language models. arXiv preprint arXiv:2305.12199, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Julen Etxaniz, Gorka Azkune, Aitor Soroa, Oier Lopez de Lacalle, and Mikel Artetxe. Bertaqa: How much do language models know about local culture? arXiv preprint arXiv:2406.07302, 2024a. Julen Etxaniz, Oscar Sainz, Naiara Perez, Itziar Aldabe, German Rigau, Eneko Agirre, Aitor Ormazabal, Mikel Artetxe, and Aitor Soroa. Latxa: An open language model and evaluation suite for basque. arXiv preprint arXiv:2403.20266, 2024b. Negar Foroutan, Mohammadreza Banaei, Karl Aberer, and Antoine Bosselut. Breaking the language barrier: Improving cross-lingual reasoning with structured self-attention. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 94229442, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.632. URL https://aclanthology.org/2023. findings-emnlp.632. Simon Frieder, Luca Pinchetti, Alexis Chevalier, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, and Julius Berner. Mathematical capabilities of chatgpt, 2023. URL https://arxiv.org/abs/2301.13867. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/12608602. Omid Ghahroodi, Marzia Nouri, Mohammad Vali Sanian, Alireza Sahebi, Doratossadat Dastgheib, Ehsaneddin Asgari, Mahdieh Soleymani Baghshah, and Mohammad Hossein Rohban. Khayyam challenge (persianmmlu): Is your llm truly wise to the persian language? arXiv preprint arXiv:2404.06644, 2024. Adriana Guevara-Rukoz, Isin Demirsahin, Fei He, Shan-Hui Cathy Chu, Supheakmungkol Sarin, Knot Pipatsrisawat, Alexander Gutkin, Alena Butryna, and Oddur Kjartansson. Crowdsourcing latin american spanish for low-resource text-to-speech. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 65046513, 2020. Momchil Hardalov, Todor Mihaylov, Dimitrina Zlatkova, Yoan Dinkov, Ivan Koychev, and Preslav Nakov. Exams: multi-subject high school examinations dataset for cross-lingual and multilingual question answering. arXiv preprint arXiv:2011.03080, 2020. Kai Hartung, Aaricia Herygers, Shubham Vijay Kurlekar, Khabbab Zakaria, Taylan Volkan, Soren Grottrup, and Munir Georges. Measuring sentiment bias in machine translation. In International Conference on Text, Speech, and Dialogue, pp. 8293. Springer, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Carolin Holtermann, Paul Rottger, Timm Dill, and Anne Lauscher. Evaluating the elementary multilingual capabilities of large language models with multiq. arXiv preprint arXiv:2403.03814, 2024. Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. Xtreme: massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In International Conference on Machine Learning, pp. 44114421. PMLR, 2020. Meng Ji, Pierrette Bouillon, and Mark Seligman. Translation Technology in Accessible Health Communication. Cambridge University Press, 2023."
        },
        {
            "title": "Preprint",
            "content": "Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state and fate of linguistic diversity and inclusion in the nlp world. arXiv preprint arXiv:2004.09095, 2020. Amir Hossein Kargaran, Ayyoob Imani, Francois Yvon, and Hinrich Schuetze. GlotLID: Language identification for low-resource languages. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 61556218, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. findings-emnlp.410. URL https://aclanthology.org/2023.findings-emnlp. 410. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Neural Information Processing Systems, 2022. URL https://api.semanticscholar.org/CorpusID:249017743. Fajri Koto, Haonan Li, Sara Shatnawi, Jad Doughman, Abdelrahman Boda Sadallah, Aisha Alraeesi, Khalid Almubarak, Zaid Alyafeai, Neha Sengupta, Shady Shehata, et al. Arabicmmlu: Assessing massive multitask language understanding in arabic. arXiv preprint arXiv:2402.12840, 2024. Viet Lai, Chien Nguyen, Nghia Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan Rossi, and Thien Nguyen. Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback. In Yansong Feng and Els Lefever (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 318327, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.emnlp-demo.28. URL https://aclanthology.org/2023.emnlp-demo.28. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212, 2023. Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, et al. Xglue: new benchmark dataset for cross-lingual pre-training, understanding and generation. arXiv preprint arXiv:2004.01401, 2020. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022. Benjamin Muller, Antonios Anastasopoulos, Benoˆıt Sagot, and Djame Seddah. When being unseen from mBERT is just the beginning: Handling new languages with multilingual language models. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 448462, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.38. URL https://aclanthology.org/ 2021.naacl-main.38. Junho Myung, Nayeon Lee, Yi Zhou, Jiho Jin, Rifki Afina Putri, Dimosthenis Antypas, Hsuvas Borkakoty, Eunsu Kim, Carla Perez-Almendros, Abinew Ali Ayele, et al. Blend: benchmark for llms on everyday knowledge in diverse cultures and languages. arXiv preprint arXiv:2406.09948, 2024. Sania Nayab, Giulio Rossolini, Giorgio Buttazzo, Nicolamaria Manes, and Fabrizio Giacomelli. Concise thoughts: Impact of output length on llm reasoning and cost. arXiv preprint arXiv:2407.19825, 2024. Guilherme Penedo, Hynek Kydlıˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557, 2024."
        },
        {
            "title": "Preprint",
            "content": "Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe. Lifting the curse of multilinguality by pre-training modular transformers. In Marine Carpuat, MarieCatherine de Marneffe, and Ivan Vladimir Meza Ruiz (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 34793495, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.255. URL https://aclanthology.org/ 2022.naacl-main.255. Irene Plaza, Nina Melero, Cristina del Pozo, Javier Conde, Pedro Reviriego, Marina Mayor-Rocher, and Marıa Grandury. Spanish and llm benchmarks: is mmlu lost in translation? arXiv preprint arXiv:2406.17789, 2024. Edoardo Maria Ponti, Goran Glavaˇs, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen. Xcopa: multilingual dataset for causal commonsense reasoning. arXiv preprint arXiv:2005.00333, 2020. Luiza Pozzobon, Patrick Lewis, Sara Hooker, and Beyza Ermis. From one to many: Expanding the scope of toxicity mitigation in language models, 2024. URL https://arxiv.org/abs/ 2403.03893. Aida Ramezani and Yang Xu. Knowledge of cultural moral norms in large language models. arXiv preprint arXiv:2306.01857, 2023. Abhinav Rao, Akhila Yerukola, Vishwa Shah, Katharina Reinecke, and Maarten Sap. Normad: benchmark for measuring the cultural adaptability of large language models, 2024. URL https://arxiv.org/abs/2404.12464. Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, et al. Xtreme-r: Towards more challenging and nuanced multilingual evaluation. arXiv preprint arXiv:2104.07412, 2021. Beatrice Savoldi, Marco Gaido, Luisa Bentivogli, Matteo Negri, and Marco Turchi. Gender bias in machine translation. Transactions of the Association for Computational Linguistics, 9:845874, 2021. Lutfi Kerem Senel, Benedikt Ebing, Konul Baghirova, Hinrich Schuetze, and Goran Glavaˇs. KardesNLU: Transfer to low-resource languages with the help of high-resource cousin benchmark and evaluation for Turkic languages. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16721688, St. Julians, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.eacl-long.100. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models. arXiv preprint arXiv:2310.16789, 2023. Weiyan Shi, Ryan Li, Yutong Zhang, Caleb Ziems, Chunhua yu, Raya Horesh, Rogerio Abreu de Paula, and Diyi Yang. Culturebank: An online community-driven knowledge base towards culturally aware language technologies, 2024. URL https://arxiv.org/abs/2404.15238. Shivalika Singh, Freddie Vargus, Daniel Dsouza, Borje Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Chien, Sebastian Ruder, Surya Guthikonda, Emad Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet Ustun, Marzieh Fadaee, and Sara Hooker. Aya dataset: An open-access collection for multilingual instruction tuning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1152111567, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.620. URL https://aclanthology.org/2024.acl-long.620."
        },
        {
            "title": "Preprint",
            "content": "Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. To cot or not to cot? chainof-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183, 2024. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. Yan Tao, Olga Viberg, Ryan Baker, and Rene Kizilcec. Cultural bias and cultural alignment of large language models. PNAS nexus, 3(9):pgae346, 2024. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Silma Team. Silma. 2024. URL https://www.silma.ai. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Ahmet Ustun, Viraat Aryabumi, Zheng Yong, Wei-Yin Ko, Daniel Dsouza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. Aya model: An instruction finetuned open-access multilingual language model. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1589415939, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.845. URL https://aclanthology.org/2024.acl-long.845. Eva Vanmassenhove, Dimitar Shterionov, and Matthew Gwilliam. Machine translationese: Effects of algorithmic bias on linguistic complexity in machine translation. arXiv preprint arXiv:2102.00287, 2021. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019. Xidong Wang, Nuo Chen, Junying Chen, Yan Hu, Yidong Wang, Xiangbo Wu, Anningzhe Gao, Xiang Wan, Haizhou Li, and Benyou Wang. Apollo: An lightweight multilingual medical llm towards democratizing medical ai to 6b people. ArXiv, abs/2403.03640, 2024a. URL https: //api.semanticscholar.org/CorpusID:268253217. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Neural Information Processing Systems, 2022. URL https://api.semanticscholar. org/CorpusID:246411621. Orgest Xhelili, Yihong Liu, and Hinrich Schutze. Breaking the script barrier in multilingual pre-trained language models with transliteration-based post-training alignment. arXiv preprint arXiv:2406.19759, 2024."
        },
        {
            "title": "Preprint",
            "content": "L Xue. mt5: massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934, 2020. Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Arda Yuksel, Abdullatif Koksal, Lutfi Kerem enel, Anna Korhonen, and Hinrich Schutze. Turkishmmlu: Measuring massive multitask language understanding in turkish. arXiv preprint arXiv:2407.12402, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Hugh Zhang, Jeff Da, Dean Lee, Vaughn Robinson, Catherine Wu, Will Song, Tiffany Zhao, Pranav Raja, Dylan Slack, Qin Lyu, et al. careful examination of large language model performance on grade school arithmetic. arXiv preprint arXiv:2405.00332, 2024a. Jingyang Zhang, Jingwei Sun, Eric Yeats, Yang Ouyang, Martin Kuo, Jianyi Zhang, Hao Frank Yang, and Hai Li. Min-k%++: Improved baseline for detecting pre-training data from large language models. arXiv preprint arXiv:2404.02936, 2024b. Miaoran Zhang, Vagrant Gautam, Mingyang Wang, Jesujoba Alabi, Xiaoyu Shen, Dietrich Klakow, and Marius Mosbach. The impact of demonstrations on multilingual in-context learning: multidimensional analysis. arXiv preprint arXiv:2402.12976, 2024c."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: GPT-4o performance across academic disciplines for Korean, Persian, Armenian, Hindi, Greek, and Russian. Each bar is annotated with the number of questions with correct answers."
        },
        {
            "title": "A APPENDIX",
            "content": "Figure 6: GPT-4o model performance on INCLUDE-BASE. (a) Performance across regional labels. While models typically perform better across region-explicit and regional-implicit questions, it is difficult to disentangle the difficult of questions due to regionality from the subject matter itself (i.e., region-agnostic questions may contain more STEM subjects that are traditionally harder for LLMs). (b) Performance across academic disciplines within STEM area. We observe models perform particularly poorly on Math and Chemistry questions. A.1 COLLECTED LANGUAGES Table 8 provides information about the languages in INCLUDE."
        },
        {
            "title": "Model",
            "content": "# samples"
        },
        {
            "title": "Humanities",
            "content": "STEM Domain-Specific"
        },
        {
            "title": "Professional Licenses",
            "content": "13294 2478 1964 3165 1736 Accuracy () GPT-4o - 5-shot - Zero-shot CoT Llama-3.1-70B-Instruct - 5-shot - Zero-shot CoT Aya-expanse-32B - 5-shot - Zero-shot CoT Qwen2.5-14B - 5-shot - Zero-shot CoT Aya-expanse-8B Mistral-7B (v0.3) Mistral-7B-Instruct (v0.3) Gemma-7B Gemma-7B-Instruct Qwen2.5-7B Qwen2.5-7B-Instruct Llama-3-8B Llama-3-8B-Instruct 79.0 79.9 71.2 61.9 49.6 52.9 61.4 48. 37.8 44.2 44.5 55.1 38.6 53.4 53.5 51.7 50.7 74.2 78.6 69.9 57.5 43.0 47.8 60.9 44.4 32.3 43.4 42.7 53.6 37.7 54.2 53.3 49.8 46. 76.8 80.4 74.2 63.5 49.1 55.4 66.0 51.6 37.3 43.9 43.2 55.5 42.0 59.1 58.1 52.1 52.9 70.1 73. 64.4 56.7 34.7 44.3 57.1 41.6 40.2 38.6 40.1 47.7 34.5 51.3 49.5 43.4 44.3 82.1 81.1 73.7 58. 49.5 52.9 65.1 46.9 29.7 44.3 43.7 62.2 44.9 57.8 58.6 51.3 54.4 Table 2: Accuracy performance of GPT-4o (In-language prompting) on INCLUDE-BASE grouped by high-level topics. Where Humanities include Social Science, Humanities, and General knowledge. STEM includes Applied Science and STEM. Domain-specific covers Business & Commerce and Health oriented education. Professional includes professional certifications. Licenses cover Marine, Fishing, and Driving licenses."
        },
        {
            "title": "Model",
            "content": "GPT-4o - 5-shot - Zero-shot CoT Llama-3.1-70B-Instruct - 5-shot - Zero-shot CoT Aya-expanse-32B - 5-shot - Zero-shot CoT Qwen2.5-14B - 5-shot - Zero-shot CoT Aya-expanse-8B Mistral-7B (v0.3) Mistral-7B-Instruct (v0.3) Gemma-7B Gemma-7B-Instruct Qwen2.5-7B Qwen2.5-7B-Instruct Llama-3.1-8B Llama-3.1-8B-Instruct In-language Prompt"
        },
        {
            "title": "English Prompt",
            "content": "Total Acc. Answer Acc. Format Errors (%) Total Acc. Answer Acc. Format Errors (%) 77.3 79.0 70.6 60.6 52.4 51.4 61.4 47.3 37.2 43.3 43.6 54.5 38.7 54.1 53.8 51.0 53.4 79.0 79. 70.6 67.9 56.2 57.2 62.4 53.1 43.8 43.3 43.8 54.5 38.7 55.1 54.0 51.0 53.4 2.5 0.2 0.0 10. 16.9 10.2 1.5 10.9 18.0 0.0 0.4 0.0 0.0 1.9 0.5 0.0 0.0 76.3 78.9 70.7 56.3 56.6 57. 61.7 51.0 46.0 44.9 44.5 54.9 39.7 55.2 54.6 51.8 54.6 78.0 79.1 70.7 67.8 62.7 58.4 61.7 52. 50.7 44.9 44.5 54.9 39.7 55.2 54.6 51.8 54.6 2.2 0.2 0.0 17.0 9.7 1.1 0.0 1.9 9.2 0.0 0.1 0.0 0.1 0.0 0.0 0.0 0. Table 3: Results on INCLUDE-BASE for In-language and English prompting strategies. Total Accuracy represents the raw accuracy of the model for answering INCLUDE questions in each respective subset. Answer Accuracy represents the accuracy of the model when only considering samples where an answer is extracted from the models output in the correct response format. Formatting Errors (%) describes the percentage of model responses that are not formatted correctly and so do not output any answer option. We mark these incorrect by default in Total Accuracy and do not include them when computing Answer Accuracy."
        },
        {
            "title": "Model",
            "content": "In-Language Prompt English Prompt In-Language Prompt English Prompt INCLUDE-LITE INCLUDE-BASE Llama3.1-70B-Instruct Aya-expanse-32B Qwen2.5-14B Aya-expanse-8B Mistral-7B Mistral-7B-Instruct Gemma-7B Gemma-7B-Instruct Qwen2.5-7B Qwen2.5-7B-Instruct Llama-3.1-8B Llama-3.1-8B-Instruct 70.3 58.9 61.8 47.3 44.5 43.8 53.6 39.1 54.4 54.5 51.2 53. 70.6 59.5 61.9 48.0 44.7 43.9 53.1 39.7 54.9 54.6 52.1 54.4 70.6 47.2 62.3 47.2 44.1 44.2 53.5 38.6 55.0 54.8 51.2 53.5 70.9 47.8 62.6 47.8 44.6 44.3 53.2 39.3 55.5 54.8 51.9 54.4 Table 4: Harness evaluation results on INCLUDE-BASE."
        },
        {
            "title": "STEM",
            "content": "Academic field Logic Law Language Visual Arts, History, Philosophy, Religious studies, Performing arts, Culturology, Literature"
        },
        {
            "title": "Label\nAgnostic\nRegion Explicit\nCulture",
            "content": "Region implicit/ Culture Sociology, Political sciences, Anthropology Region implicit/Culture Economics Psychology Geography Region implicit/Agnostic/Region explicit Region implicit/Region explicit Region implicit/Agnostic Math, Physics, CS, Biology, Earth science, Chemistry, Engineering Qualimetry"
        },
        {
            "title": "Other",
            "content": "Accounting Management, Marketing, Industrial and labor relations, International trade, Risk management and insurance, Business administration, Business ethics, Business, Finance Agriculture, Library and museum studies, Transportation Military Sciences, Public Administration, Public Policy Architecture and Design, Family and consumer science, Environmental studies and forestry, Education Journalism, media studies, and communication, Social Work, Human physical performance and recreation Driving license, Marine license, Fishing license, Medical license, Public administration, Professional certification Agnostic/Region implicit/Region explicit Region implicit/Region explicit"
        },
        {
            "title": "Region explicit",
            "content": "Region implicit/Region explicit/Agnostic Region implicit/Agnostic Region implicit/Region explicit"
        },
        {
            "title": "Multiple exams",
            "content": "Region implicit/Culture Table 5: Annotation schema for high-level Academic area and fine-grained Academic field. The Label column lists the most likely regionality label for these exams in our dataset (e.g., region- {agnostic, implicit, explicit} or cultural), though all exams from which we collect data are individually labeled with regionality category. The first label is the most frequent one."
        },
        {
            "title": "Academic\nSTEM studies",
            "content": "Academic Domain-specific studies Professional License Avg (%) Albanian Arabic Armenian Azerbaijani Basque Belarusian Bengali Bulgarian Chinese Croatian Dutch; Flemish Estonian Finnish French Georgian German Greek Hebrew Hindi Hungarian Indonesian Italian Japanese Kazakh Korean Lithuanian Malay Malayalam Nepali Macedonian Persian Polish Portuguese Serbian Spanish Tagalog Tamil Telugu Turkish Ukrainian Urdu Uzbek Vietnamese Russian 95.0 77.8 52.7 71.3 - 51.8 71.1 93.8 71.5 89.0 86.6 90.7 67.0 83.8 87.6 62.6 84.7 62.0 77.7 66.3 84.0 87.7 - 80.4 91.6 92.0 84.5 69.6 - 96.0 66.0 100.0 84.7 92.2 83.6 86.8 70.6 66.9 62.0 85.8 61.7 63.6 84.4 77.5 88.0 82.0 32.0 73.6 - 42.0 90.0 60.0 66.7 82.0 87.5 98.0 87.0 50.0 - 64.0 84.0 - 71.9 80.6 69.1 87.2 - - - 97.1 - 66.0 - 86.0 25.0 64.6 63.3 86.0 88.0 - 54.0 70.7 52.0 84.0 65.3 84.0 86.0 83.4 83.5 80.5 - 71.4 - - - - 58.2 - 80.0 100.0 77.8 81.2 - - 89.2 - 91.5 - - 91.7 - - - 82.5 80.3 55.0 - 89.3 - - 67.9 - 96.0 - - - 75.9 - 100.0 - - 70. - - - - 64.8 - 84.3 - 52.1 - - - - - - - 58.6 - 71.8 - 84.8 95.5 78.1 - 46.4 81.2 - - 61.6 - 49.6 80.0 - - - - - - - - - 73.3 - - - 76.2 72.2 - - - - - 84.5 - - - - 68.1 - 87.0 - 88.6 57.7 - - - 96.0 - - - - 80.9 83.2 - 81.6 - - - - 90.7 - - - - - - - 63.9 89.50 78.30 53.60 71.90 64.80 50.90 76.80 90.70 66.10 88.40 86.40 92.40 69.90 80.70 87.60 66.90 71.50 86.20 75.10 75.80 79.50 90.00 81.60 80.40 69.00 90.60 83.00 70.80 72.40 92.40 64.60 78.80 76.40 91.60 84.40 87.40 69.10 68.20 65.30 85.60 62.50 69.70 84.50 75.00 Table 6: Accuracy performance of GPT-4o (5-shot) on INCLUDE-BASE for each language. Humanities include Social Science, Humanities, and General knowledge. STEM includes Applied Science and STEM. Domain-specific covers Business & Commerce and Health oriented education. Professional includes professional certifications. Licenses cover Marine, Fishing, and Driving licenses. Aya-expanse-8B XGLM-7B Qwen-2.5-7B LLaMA-3.1-8B"
        },
        {
            "title": "Full Benchmark\nNewly collected",
            "content": "0.02 0.01 0.17 0.14 0.13 0.11 0.29 0.25 Table 7: Data contamination rates per model on INCLUDE-BASE."
        },
        {
            "title": "Availability Count",
            "content": "Albanian Amharic Arabic Armenian Assamese Azerbaijani Basque Belarusian Bengali Bulgarian Chinese Croatian Czech Danish Dutch; Flemish Estonian Finnish French Georgian German Greek Hebrew Hindi Hungarian Indonesian Italian Japanese Kannada Kazakh Korean Lithuanian Malay Malayalam Marathi Nepali Macedonian Oriya Panjabi; Punjabi Persian Polish Portuguese Russian Serbian Sinhala; Sinhalese Slovak Spanish Swedish Tagalog Tamil Telugu Turkish Ukrainian Urdu Uzbek Vietnamese latin geez perso-arabic armenian bengali-assamese latin latin cyrillic bengali-assamese cyrillic chinese latin latin latin latin latin latin latin mkherduli latin greek hebrew devanagari latin latin latin kanji kannada cyrillic hangul latin latin vatteluttu devanagari devanagari cyrillic odia gurmukhi perso-arabic latin latin cyrillic cyrillic sinhala latin latin latin latin tamil telugu latin cyrillic perso-arabic latin latin Indo-European Afro-Asiatic Afro-Asiatic Indo-European Indo-European Turkic Isolate Indo-European Indo-European Indo-European Sino-Tibetan Indo-European Indo-European Indo-European Indo-European Uralic Uralic Indo-European Kartvelian Indo-European Indo-European Afro-Asiatic Indo-European Uralic Austronesian Indo-European Japonic Dravidian Turkic Koreanic Indo-European Austronesian Dravidian Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Austronesian Dravidian Dravidian Turkic Indo-European Indo-European Turkic Austro-Asiatic Albanian Semitic Semitic Armenian Indo-Iranian Azerbaijani North Slavic East Indo-Iranian Slavic South Eastern Chinese Slavic South Western Slavic West Germanic Germanic Finnic Finnic Italic Georgian Germanic Greek Semitic Indo-Iranian Hungarian Malayo-Polynesian Italic Japanese Southern Western Korean Eastern Baltic Malayo-Polynesian Southern Indo-Iranian Indo-Iranian Slavic South Eastern Indo-Iranian Indo-Iranian Indo-Iranian Slavic West Italic Slavic East Slavic South Indo-Iranian Slavic West Italic Germanic Malayo-Polynesian Southern South-Central Southern Slavic East Indo-Iranian Eastern Mon-Khmer"
        },
        {
            "title": "Mid\nLow\nHigh\nLow\nLow\nMid\nLow\nLow\nMid\nMid\nHigh\nMid\nHigh\nMid\nHigh\nMid\nMid\nHigh\nLow\nHigh\nMid\nMid\nMid\nMid\nHigh\nHigh\nHigh\nLow\nLow\nMid\nMid\nMid\nLow\nMid\nMid\nLow\nLow\nLow\nHigh\nHigh\nHigh\nHigh\nMid\nLow\nMid\nHigh\nMid\nLow\nMid\nLow\nHigh\nMid\nLow\nLow\nHigh",
            "content": "2365 131 15137 1669 323 6937 719 687 15259 2937 12977 2879 50 732 2222 952 1574 2457 599 1590 6570 2457 5167 2267 12013 3038 2699 335 5736 1781 1397 1021 275 313 1470 2075 241 453 23990 2023 1407 10169 1636 325 131 2559 5102 530 945 11568 2710 1482 122 2878 8901 Table 8: Languages in INCLUDE with their associated metadata and the total count of the samples per language."
        },
        {
            "title": "Preprint",
            "content": "A.2 DETAILS ON EXAM SOURCES PARSING Figure 8 presents the questionnaire we distributed to the community to gather diverse set of multiple-choice exams. It was distributed among university student organizations and researchers at our institution.7 Participation was voluntary and not incentivized. A.3 PERFORMANCE ACROSS ACADEMIC AREAS AND FIELDS Distribution of academic areas and academic fields with the respective number of questions is presented in Figure 7. GPT-4o performance across languages and academic areas is in Table 9. GPT-4o performance across languages, academic fields, and related regional features is in Tables 10 and 11. A.4 REGIONAL LABELS: ANNOTATION First, we categorized the exams into one of eight broad academic areas, e.g., Humanities or Social Sciences, and then further classified each exam into specific academic fields, e.g., History or Geography. This categorization was done manually, taking into account both the exams learning level and the exams original topic. Building on these categories, we applied one of four labelsagnostic, culture-related, region-explicit, or region-implicitbased on the degree of dependence on localized knowledge required to answer the exam questions. The labels reflect the extent to which specific cultural or regional knowledge is necessary. Table 5 provides examples illustrating how different exams were typically classified under each label, to show the relationship between categories and labels. The region implicit label was applied when we suspected that exam content might vary across regions but could not reliably detect specific regional differences. For example, historical events, literary works, and religious interpretations may differ significantly depending on the region. Similarly, fields like marketing, management, social work, and insurancethough rooted in shared theoretical foundationscan be practiced differently across regions. When we encountered such uncertainty, we labeled the subject as region implicit. Within the Humanities, fields such as Visual Arts, History, Philosophy, Religious Studies, Performing Arts, Culturology, and Literature were labeled region implicit when the content was not explicitly tied to particular region. However, if the exam was region-specific (e.g., Greek literature), we categorized it as culture-related. In the Social Sciences, Psychology was classified as region explicit if the exam focused on regional clinical practices; otherwise, it was region implicit when dealing with broader psychological theories that may vary across regions. Geography was labeled region implicit if the exam involved political geography and agnostic if it focused on general geographic knowledge. Similarly, disciplines like Sociology, Political Science, and Anthropology were classified as either region implicit or culture-related, depending on whether regional specificity was required, much like History. For Economics, exams were labeled agnostic when covering general economic theories, region explicit when addressing regional regulations, and region implicit when regional applications were uncertain. In STEM fields, most disciplines were categorized as agnostic, with the exception of Qualimetry, which was labeled region explicit due to its specific application in post-Soviet countries for quantitative and qualitative assessment according to regional standards.8 Exams related to theoretical medical subjects, such as Anatomy, were classified as agnostic. In contrast, exams covering clinical practices and guidelines specific to region were labeled as region explicit, while others were marked as region implicit if regional dependence was unclear. Accounting is generally tied to region-specific practices, so it was consistently classified as region explicit. Other disciplines within the Business and Commerce category were treated similarly to Economics and labeled as mostly as region implicit. In some cases, there were agnostic and region explicit exams. 7We will provide institutional details upon paper publication. 8https://en.wikipedia.org/wiki/Qualimetry"
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Academic domain and academic fields with the number of examples across all languages. Exams in Applied Science disciplines were typically categorized as region implicit due to the potential involvement of regional variations. Similarly, exams in Military Sciences, Public Administration, and Public Policy were marked as region explicit when tied to specific regions (e.g., Basics of National Security of the Republic of Azerbaijan) and region implicit when regional specifics were less pronounced. For exams focused on theoretical aspects, we used the agnostic label (e.g., Theoretical Foundations of Food Engineering in Agriculture). Finally, exams covering multiple topics were classified as region implicit unless they explicitly focused on cultural aspects of particular region, in which case they were labeled as culture-related. A."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "Each model was evaluated using single A100 GPU (80GB memory), with evaluation times averaging approximately 4 hours for INCLUDE-BASE. For all models, we set the decoding temperature to 0, prioritizing deterministic outputs. We configured response context windows based on model size and task requirements. For models such as Aya-Expanse-8B, Mistral-7B (v0.3), Mistral-7B-Instruct (v0.3), Gemma-7B, Gemma-7B-Instruct, Qwen2.5-7B, Qwen2.5-7B-Instruct, Llama-3-8B, Llama-3-8B-Instruct, XGLM-7.5B, BLOOM-7.1B,"
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Exam source collection form sent to the academic community. and BLOOMZ-7.1B, we set window size of 40 tokens. Larger models, including Aya-Expanse-32B and GPT-4, were evaluated using 512-token context window for 5-shot tasks and 1024 tokens for zero-shot chain-of-thought (CoT) reasoning. A.6 EXPERIMENTS ON MONOLINGUAL MODELS To assess the performance of monolingual models on our benchmark, we evaluate seven open-source monolingual models on the relevant language-specific subsets of INCLUDE, (i.e., the languages these"
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Accuracy of different models on languages where both existing benchmark data and newly collected data are available. Each point represents the accuracy score of model for specific language. (a) Points of the same color represent the accuracy scores of single model across different languages. (b) Points of the same color represent the accuracy scores for single language across different models. models were pre-trained on). We test Baichuan-7B (Yang et al., 2023), SILMA-9B-Instruct (Team, 2024), Calm2-7B-chat (Touvron et al., 2023), Korean-Mistral, ruGPT3.5-13B and SauerkrautLM-v214b-DPO. We compare their performance with the results of the most performant large-, medium-, and small-scale models on the specific language subsets. The results of this evaluation are presented in the Table 13. The table reveals that most monolingual models underperform the state-of-the-art small multilingual model Qwen-2.5 (7B), with the exception of the German monolingual model, SauerkrautLM-v2-14B-DPO, which performs on par with Qwen in the German language. A.7 LIMITATIONS OF THE SCOPE OF EXISTING BENCHMARKS As discussed in the motivation and related work sections, there is currently no multilingual benchmark that offers both high language coverage and incorporates regional knowledge. Existing benchmarks typically fall into one of two categories: either they focus extensively on single language across various regional dimensions, or they cover certain number of languages with predominantly agnostic knowledge. In Table 15, we provide more details on the list of existing benchmarks that were mentioned in the paper, which feature original content (not machine-translated), highlighting their language and knowledge coverage. In relation to these monolingual existing resources, the INCLUDE benchmark makes three significant contributions: (1) introduces original datasets from languages that are either not covered or only partially covered by existing benchmarks, (2) leverages publicly available, knowledge-intensive multiple-choice question benchmarks in various languages, (3) organizes both the existing and newly introduced data under unified taxonomy of knowledge, differentiating between regional knowledge and region-agnostic knowledge. In this context, INCLUDE incorporates existing datasets, which account for 39.8% of the total collected data and 31.7% of the INCLUDE-BASE benchmark. To understand the correlations between the newly collected data and the existing benchmarks integrated into INCLUDE, we analyzed model performance in languages where both existing benchmark data and newly collected data are available. We compared performance across these datasets and examined the correlation between them. Results were stratified by language and by model type. We visualized the performances using plots (Figure 9) and calculated the R2 scores to quantify the correlations  (Table 14)  . The analysis reveals two key conclusions: First, for given language with multiple models having published performance data, models performance on INCLUDE can generally be predicted. However, for given model with published performance across different languages, its performance on"
        },
        {
            "title": "Preprint",
            "content": "INCLUDE cannot reliably be predicted when applied to newly published language benchmark. This indicates that while INCLUDE is less impactful for languages with existing published benchmarks, it is particularly valuable for assessing performance in languages with no prior resources. A.8 ANALYSIS OF OUTPUT ERRORS ON INCLUDE As outlined in Section 5.3 of the paper, model performanceand consequently, the errorsare heavily influenced by the models proficiency in the specific task and language. We conducted more detailed error analysis by manually investigating sample of INCLUDE generations. We focused on six languages spanning high-resource (Chinese, Turkish), medium-resource (Bengali, Greek, Korean), and low-resource (Armenian) categories, selecting subject areas with the largest performance gaps compared to other subjects within the same language. We manually examined 150 examples, covering at least two subjects per language with 10 examples per subject, analyzing the questions and answers generated by GPT-4o. We observed four main types of errors related to computational mistakes, factual mistakes, lack of regional knowledge, and model hallucinations  (Table 16)  . Each error type highlights distinct limitations in the models capabilities, from arithmetic and factual knowledge to regional understanding and prompt adherence. Our analysis revealed that the models errors were distributed as follows: 38.6% were due to lack of regional knowledge, 32% resulted from hallucinations, 26.7% were factual mistakes, and 2.7% were computational errors. A.9 DATA CONTAMINATION PREVENTION As described in Section 3.1, INCLUDE is made up of few previously-published benchmarks incorporated into INCLUDE, but also newly-collected exam materials from sources contributed by our multilingual community of native speakers. significant portion of the newly-collected data was derived from PDFs and textbooks, which are less likely to have been included in models trained primarily on web-based data. In this section, we analyze the degree of contamination within the models using the mink%++ (Zhang et al., 2024b) method for training data detection in LLMs. This method determines whether an input next token forms mode or has relatively high probability under the conditional categorical distribution. Using this scoring mechanism, one can predict if an input sequence is part of the models training data based on decision threshold. This method achieves SOTA on the WikiMIA (Shi et al., 2023) benchmark for training data detection. We use the decision threshold that achieves the best performance on WikiMIA as the decision threshold for our analysis. Using this method, we computed the contamination rate for each language on four main-stream multilingual models: Aya-8B, XGLM-7B, LLaMA-3.1-8B, and Qwen-2.5-7B. We show the contamination rate results in Table 7. To further mitigate the risk of benchmark saturation as result of data leakage when new models are trained, we have held back the complete dataset, comprising 197,243 entries. Instead, we will release these further questions and answers incrementally over the next year. We have also reserved held-out dataset covering wide range of the collected languages to be used for future experimental studies specifically aimed at analyzing data leakage over time."
        },
        {
            "title": "Preprint",
            "content": "Language Academic Area Accuracy Count Albanian Arabic Armenian Azerbaijani Basque Belarusian Bengali Bulgarian Chinese Croatian Dutch; Flemish Estonian Finnish French Georgian German Greek"
        },
        {
            "title": "Italian",
            "content": "Humanities Business & Commerce Social Science Humanities Business & Commerce General Knowledge Other STEM Social Science Humanities Other STEM Social Science Applied Science Humanities Business & Commerce Health-Oriented Education Social Science Other Humanities STEM Humanities General Knowledge Other STEM Humanities STEM Social Science Applied Science Humanities Business & Commerce Health-Oriented Education Other Social Science Humanities STEM Social Science Humanities Social Science Humanities STEM Humanities Health-Oriented Education Social Science Humanities Other Social Science Humanities Social Science Humanities Business & Commerce Other Social Science"
        },
        {
            "title": "Humanities\nOther",
            "content": "Applied Science Humanities General Knowledge Health-Oriented Education Other Social Science"
        },
        {
            "title": "Applied Science\nHumanities\nOther\nSocial Science",
            "content": "95.1 85.7 94.5 79.0 79.3 86.7 76.2 82.0 67.6 34.7 72.2 28.0 50.5 75.9 74.1 62.5 80.2 67.6 64.8 50.8 42. 62.0 80.1 84.3 88.0 96.4 60.0 91.2 73.2 67.8 53.5 60.9 68.3 76.1 86.8 82.0 90.8 86.0 86.8 90.1 97. 69.5 75.6 64.6 86.5 68.1 74.3 87.6 62.6 83.8 89.1 57.5 84.2 60.0 88. 83.1 72.9 83.1 91.5 64.1 74.6 79.8 66.3 71.2 82.4 83.2 60.0 84.8 85.7 85.0 95.5 89.8 223 223 55 105 82 105 105 50 225 79 50 196 108 108 96 96 108 500 490 50 166 166 166 50 250 50 71 87 71 87 142 71 250 50 250 243 243 161 36 226 45 226 266 47 500 91 37 64 266 133 50 500 71 96 71 71 142 71 341 125 125 125 50 125 35 167 155 167 Language Japanese Kazakh Korean Lithuanian Malay Malayalam Academic Area Other Humanities Other Social Science Humanities Business & Commerce Other STEM Social Science Humanities Business & Commerce Social Science Humanities General Knowledge Health-Oriented Education Other STEM Nepali Other Macedonian Persian Polish Portuguese Russian Serbian Spanish Tagalog Tamil"
        },
        {
            "title": "Vietnamese",
            "content": "Humanities Business & Commerce STEM Social Science Humanities Other Social Science Other STEM Applied Science Humanities Business & Commerce Health-Oriented Education Other Applied Science Humanities Business & Commerce Health oriented education Other STEM Social Science Humanities STEM Social Science Humanities Health oriented education STEM Social Science Humanities Other General knowledge STEM Applied Science Humanities Social Science Humanities Business & Commerce STEM Social Science"
        },
        {
            "title": "Humanities\nSTEM\nSocial Science",
            "content": "Accuracy Count 80.2 80.4 46.0 91.6 91.6 77.5 81.2 97.1 93.5 84.3 79.8 84. 64.3 73.1 55.0 80.9 66.0 72.4 96.9 89.3 86.0 92.5 55.3 62.4 74.5 80.0 62.5 58.3 81.8 56.9 67.1 67. 87.0 76.8 66.7 74.1 63.9 80.9 76.8 90.4 84.0 95.2 77.2 96.0 88.0 89.6 86.8 90.7 70.6 54.0 73.5 66.0 66. 62.0 75.9 52.0 62.0 92.4 84.0 79.2 61.7 63.3 62.9 73.3 84.0 71.4 88.0 86.0 80.8 500 250 250 335 40 48 34 77 178 178 145 56 78 100 194 47 224 224 50 53 141 250 141 496 48 84 154 84 67 169 69 69 69 85 97 94 69 313 50 250 25 25 250 425 75 500 50 166 191 166 166 166 50 166 250 50 300 49 240 240 50 21 250 50 250 Table 9: GPT-4o (5-shot, In-language prompting) performance on INCLUDE-BASE per language and academic area. Areas with less than 30 examples were excluded from the analysis."
        },
        {
            "title": "Preprint",
            "content": "Language Academic Field Regional Feature Accuracy Count Albanian Arabic Armenian Azerbaijani History Philosophy Visual Arts Business Sociology History Language Accounting Multiple exams Driving License Geography Sociology History History Literature Driving License Chemistry Geography Agriculture Law Management Health Economics Basque Professional certification Belarusian Bengali Bulgarian Chinese Croatian Dutch; Flemish Estonian Finnish French Georgian German Greek Hebrew Hindi Language Literature Math Language Literature Multiple exams Professional certification Biology History Philosophy Geography Medicine Driving License Professional certification Political sciences History Philosophy Religious Studies Psychology Sociology History Literature Economics Geography Sociology Language Law Economics Political Sciences Sociology Culturology Language Driving License Geography History Language Law Geography Visual Arts Management Medical License Professional Certification Economics Logic Driving License Education History Literature Multiple Exams Medicine Driving License Professional Certification Geography Implicit Implicit Implicit Implicit Implicit Implicit Culture Explicit Implicit Explicit Implicit Implicit Culture Implicit Culture Explicit Agnostic Implicit Implicit Explicit Implicit Implicit Implicit Explicit Culture Culture Agnostic Culture Culture Implicit Explicit Agnostic Implicit Implicit Implicit Explicit Explicit Explicit Implicit Implicit Implicit Implicit Implicit Implicit Culture Culture Implicit Implicit Implicit Culture Explicit Implicit Implicit Implicit Culture Culture Explicit Implicit Implicit Culture Explicit Implicit Implicit Implicit Explicit Explicit Implicit Agnostic Explicit Implicit Implicit Culture Implicit Explicit Explicit Explicit Implicit 93.1 97.6 94.0 85.7 94.5 73.3 80.0 89.5 86.7 76.2 65.3 66.7 26.3 41.1 40.0 72.2 20.0 50.5 85.3 76.2 66.7 80.2 70.7 64.8 47.9 67.4 40. 62.5 61.9 80.1 84.3 89.5 93.9 98.5 91.2 57.1 84.5 52.1 84.8 88.2 83.5 90.2 95.7 94.8 89.4 81.4 81.7 93.9 90.1 89. 69.3 73.7 61.5 48.6 94.8 79.0 68.1 68.1 93.8 85.7 83.6 50.0 90.6 89.1 54.1 60.9 85.8 60.0 88. 84.3 86.7 73.2 83.1 91.5 57.7 70.4 75.0 58 82 83 223 55 30 40 57 105 105 49 33 95 95 35 79 30 196 34 42 36 96 58 426 43 49 40 126 166 166 38 115 135 250 35 71 71 33 119 79 51 93 135 141 102 109 33 147 215 95 96 35 77 124 47 47 161 168 171 54 32 64 133 133 50 500 70 30 41 71 71 71 71 48 Table 10: GPT-4o (5-shot, In-language prompting) performance on INCLUDE-BASE per language, academic field, and regional label. Fields with less than 30 examples were excluded from the analysis (Part 1)"
        },
        {
            "title": "Preprint",
            "content": "Language Academic Field Regional Feature Accuracy Count Hungarian Indonesian Italian Japanese Kazakh Korean Lithuanian Malay Malayalam Nepali North Macedonian Persian Polish Portuguese Russian Serbian Spanish Tagalog Tamil Telugu Turkish Ukrainian Urdu Uzbek Vietnamese Agriculture Architecture and Design Environmental Studies and Forestry Economics Geography Human Physical Performance and Recreation Language Professional Certification Economics Geography Sociology Agriculture History Professional Certification Psychology Sociology Driving License Medical License Professional Certification History History Literature Professional Certification Economics History Finance Professional Certification Earth Science Economics History Accounting Geography History Multiple Exams Health Marine License Driving License Professional Certification History Philosophy Visual Arts Business Sociology Literature Driving License Professional Certification Geography Sociology Professional Certification Math Agriculture Philosophy Management Health Economics Education Law Management Medicine Marine License Qualimetry Economics History Philosophy Psychology Sociology Language Law Literature Philosophy Economics Geography Culturology History Language Driving License Multiple Exams Education History History Economics Geography Political Sciences History Philosophy Business Geography Sociology Law Physics Psychology Culturology History Law Medical License History Geography Implicit Explicit Implicit Implicit Implicit Implicit Culture Region explicit Region explicit Implicit Implicit Implicit Implicit Region explicit Implicit Implicit Region explicit Region explicit Region explicit Culture Implicit Culture Region explicit Implicit Implicit Implicit Region explicit Agnostic Implicit Implicit Region explicit Implicit Implicit Culture Implicit Explicit Explicit Explicit Implicit Implicit Implicit Implicit Implicit Culture Explicit Explicit Implicit Implicit Explicit Agnostic Implicit Implicit Implicit Implicit Implicit Implicit Explicit Implicit Explicit Explicit Explicit Implicit Implicit Implicit Implicit Implicit Culture Explicit Implicit Implicit Explicit Implicit Culture Culture Culture Explicit Implicit Implicit Culture Implicit Explicit Implicit Implicit Implicit Implicit Implicit Implicit Implicit Explicit Agnostic Implicit Culture Implicit Explicit Explicit Implicit Implicit 82.4 85.7 74.4 80.8 48.1 71.2 79.5 83.2 77.8 87.5 87.7 85.7 90.4 95.5 95.0 87.7 96.0 86.1 66.7 78.4 94.9 76.7 46.0 91. 91.6 77.5 81.2 97.1 93.5 84.3 79.8 85.3 61.5 72.7 55.0 80.9 83.2 61.6 95.8 97.3 97.1 89.3 92.5 51.6 81.6 43.2 66.0 74. 80.0 61.7 70.0 83.3 57.9 70.3 89.7 87.0 72.2 66.2 73.3 56.5 79.7 63.9 91.5 87.5 99.2 91.1 69.6 67.0 93.8 90.3 95.6 86.2 91.6 85.3 79.2 90. 70.6 73.0 64.7 63.9 60.0 73.2 63.3 71.2 74.6 75.9 53.8 91.7 92.4 84.0 79.2 61.7 66.1 60.6 73. 88.3 80.8 170 42 129 78 81 125 78 125 36 32 57 35 94 155 60 65 99 201 201 241 79 250 250 335 40 48 34 77 178 178 129 52 77 100 194 250 250 48 74 102 224 31 125 125 47 63 496 47 40 84 57 37 126 69 36 65 60 69 69 36 235 56 125 45 46 109 64 31 91 203 116 106 75 500 100 119 36 45 82 30 73 63 166 130 36 250 50 250 124 109 240 239 250 Table 11: GPT-4o (5-shot, In-language prompting) performance on INCLUDE-BASE per language, academic field, and regional label. Fields with less than 30 examples were excluded from the analysis (Part 2)"
        },
        {
            "title": "Language",
            "content": "Acc (k:50) Acc (k:100) Acc (k:200) Acc (k:512) Total gain Uzbek Armenian Malayalam Urdu Greek Korean Chinese Finnish Basque Polish Azerbaijani Dutch; Flemish Telugu Hindi German Malay Tamil Arabic russian Italian Spanish Japanese Georgian Vietnamese Turkish Kazakh Portuguese Bengali Persian Belarusian French Indonesian Albanian Lithuanian Estonian Croatian Hungarian Nepali Bulgarian Hebrew Macedonian Serbian Tagalog Ukrainian 51.4 28.0 57.0 53.7 58.0 60.4 57.2 63.3 60.0 74.1 67.7 81.9 63.9 72.0 64.0 80.6 67.3 76.3 72.6 88.0 82.4 78.6 86.2 82.4 63.5 79.2 72.8 75.2 60.9 49.5 80.0 77.8 88.9 89.7 92.0 87.8 75.3 71.8 90.7 86.0 92.4 91.5 87.4 85.5 60.6 30.7 57.4 56.8 58.2 61.0 61.8 64.4 60.8 75.2 69.2 82.9 63.9 72.4 65.5 81.8 67.3 76.8 73.6 88.5 83.1 78.6 86.4 82.5 64.1 79.6 73.5 75.4 61.1 50.0 80.2 78.2 89.3 89.7 92.0 88.0 75.3 72.0 90.7 86.0 92.4 91.5 87.4 85.5 66.6 36.0 61.0 58.8 63.8 62.4 63.5 67.0 63.8 75.4 70.4 83.8 64.8 73.7 65.5 82.4 67.8 77.9 74.1 89.2 83.3 79.4 87.0 84.9 64.4 80.4 73.5 76.1 61.3 50.0 80.4 78.4 89.3 90.1 92.4 88.2 75.5 71.6 90.7 86.0 92.4 91.5 87.4 85.5 68.6 41.1 69.9 62.2 66.4 68.8 65.5 69.1 64.8 78.1 71.5 85.3 66.6 74.4 66.2 82.8 69.5 78.4 74.6 89.6 84.0 80.0 87.6 83.8 64.8 80.4 74.0 76.3 61.9 50.2 80.7 78.5 89.5 90.3 92.4 88.0 75.5 72.0 90.7 86.0 92.4 91.5 87.4 85. 17.2 13.1 12.9 8.5 8.4 8.4 8.3 5.8 4.8 4.0 3.8 3.4 2.7 2.4 2.2 2.2 2.2 2.1 2.0 1.6 1.6 1.4 1.4 1.4 1.3 1.2 1.2 1.1 1.0 0.7 0.7 0.7 0.6 0.6 0.4 0.2 0.2 0.2 0.0 0.0 0.0 0.0 0.0 0.0 Table 12: GPT-4o performance for different values of for in-language prompting (the output generation length) per language on INCLUDE-BASE and total performance gain from = 50 to 512."
        },
        {
            "title": "SoTA Monolingual",
            "content": "Monolingual Acc GPT-4o Qwen2.5-14B Qwen2.5-7B"
        },
        {
            "title": "Chinese\nArabic\nJapanese\nKorean\nRussian\nGerman",
            "content": "Baichuan-7B SILMA-9B-Instruct calm2-7b-chat Korean-Mistral-Nemo-sft-dpo-12B ruGPT-3.5-13B SauerkrautLM-v2-14b-DPO 38.7 56.9 25.0 35.3 53.8 56.8 68.1 78.1 75.0 75.0 69.0 66.2 82.2 70.5 69.2 83.2 68.2 58.3 78.3 61.6 64.7 76.8 59.6 56.1 Table 13: Accuracy of the multilingual and monolingual models for answering INCLUDE-BASE questions for specific target languages."
        },
        {
            "title": "Model",
            "content": "R2 R2 0.646 GPT-4o 0.077 0.985 Qwen2.5-14B 0.546 0.770 Aya-expanse-32B 0.290 0.495 Aya-expanse-8B 0.333 0.953 Qwen2.5-7B 0.412 0.945 Mistral-7B 0.231 0.833 Gemma-7B 0.001 0.020 0.831 0.001 0.930 Llama 3.1-70B Llama 3.1-8B Table 14: R2 scores between the performance different models for newly-collected data and existing benchmarks stratified by language and model."
        },
        {
            "title": "Knowledge Coverage",
            "content": "Region agnostic (%) Region related (%)"
        },
        {
            "title": "VNHSGE\nEXAMS",
            "content": "Vietnamese 16 languages Academic knowledge (elementary school, high school, university), Driving License Academic knowledge (elementary school, high school, university) Academic knowledge (elementary school, high school, university) Academic knowledge (elementary school, high school, university) High school examinations High school examinations INCLUDE (ours) 44 languages Academic knowledge (elementary school, high school, university), Professional examinations (Medical exam, Bar exam, Teaching exam), Occupational Licenses (Driving license, Marine license and more) 24.8% 75.2% 25.6% 74.4% 63.1% 36.9% 34.8% 40.4% 43.7% 65.2% 59.6% 56.3% 7.8% 92.2% Table 15: Existing published benchmarks descriptives and the comparison with INCLUDE-BASE."
        },
        {
            "title": "Model Hallucinations",
            "content": "Errors that occur when the model fails to perform arithmetic required to answer question correctly. Errors that involve the model lacking knowledge of facts unrelated to specific region or language. For example, in the question, Which of the following has value between 1 and 1000? Choices: [Gini coefficient, base interest rate, personal credit score, corporate economic survey index], the model may choose incorrectly due to lack of factual knowledge. Errors that arise when the model lacks knowledge specific to particular region, even though it demonstrates proficiency in the language itself. Errors that occur when the model fails to follow the prompt format or does not provide an answer, indicating challenges with language understanding or instruction processing. Percentage of errors (%) 2.7% 26.7% 38.6% 32.0% Table 16: Breakdown of error types."
        }
    ],
    "affiliations": [
        "Cohere For AI",
        "Cohere For AI Community",
        "EPFL",
        "ETH Zurich",
        "Swiss AI Initiative"
    ]
}