{
    "paper_title": "Revisiting Diffusion Model Predictions Through Dimensionality",
    "authors": [
        "Qing Jin",
        "Chaoyang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in diffusion and flow matching models have highlighted a shift in the preferred prediction target -- moving from noise ($\\varepsilon$) and velocity (v) to direct data (x) prediction -- particularly in high-dimensional settings. However, a formal explanation of why the optimal target depends on the specific properties of the data remains elusive. In this work, we provide a theoretical framework based on a generalized prediction formulation that accommodates arbitrary output targets, of which $\\varepsilon$-, v-, and x-prediction are special cases. We derive the analytical relationship between data's geometry and the optimal prediction target, offering a rigorous justification for why x-prediction becomes superior when the ambient dimension significantly exceeds the data's intrinsic dimension. Furthermore, while our theory identifies dimensionality as the governing factor for the optimal prediction target, the intrinsic dimension of manifold-bound data is typically intractable to estimate in practice. To bridge this gap, we propose k-Diff, a framework that employs a data-driven approach to learn the optimal prediction parameter k directly from data, bypassing the need for explicit dimension estimation. Extensive experiments in both latent-space and pixel-space image generation demonstrate that k-Diff consistently outperforms fixed-target baselines across varying architectures and data scales, providing a principled and automated approach to enhancing generative performance."
        },
        {
            "title": "Start",
            "content": "Qing Jin 1 Chaoyang Wang"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in diffusion and flow matching models have highlighted shift in the preferred prediction targetmoving from noise (ε) and velocity (v) to direct data (x) prediction particularly in high-dimensional settings. However, formal explanation of why the optimal target depends on the specific properties of the data remains elusive. In this work, we provide theoretical framework based on generalized prediction formulation that accommodates arbitrary output targets, of which ε-, v-, and x-prediction are special cases. We derive the analytical relationship between datas geometry and the optimal prediction target, offering rigorous justification for why x-prediction becomes superior when the ambient dimension significantly exceeds the datas intrinsic dimension. Furthermore, while our theory identifies dimensionality as the governing factor for the optimal prediction target, the intrinsic dimension of manifold-bound data is typically intractable to estimate in practice. To bridge this gap, we propose k-Diff, framework that employs data-driven approach to learn the optimal prediction parameter directly from data, bypassing the need for explicit dimension estimation. Extensive experiments in both latent-space and pixel-space image generation demonstrate that k-Diff consistently outperforms fixed-target baselines across varying architectures and data scales, providing principled and automated approach to enhancing generative performance. 6 2 0 2 9 2 ] . [ 1 9 1 4 1 2 . 1 0 6 2 : r 1. Introduction One of the key design choices of diffusion models (SohlDickstein et al., 2015) is the network output prediction. Early works on low-dimensional data primarily utilize noise prediction (ε-prediction) (Ho et al., 2020; Song et al., 2020), paradigm that was later largely superseded by velocity *Equal contribution Project lead 1Independent Researcher. Correspondence to: Qing Jin <jinqingking@gmail.com>. Preprint. January 30, 2026. 1 prediction (v-prediction) (Salimans & Ho, 2022; Karras et al., 2022), especially following the rise in popularity of flow matching models (Lipman et al., 2022). Most recently, research into high-resolution generationparticularly for models operating directly in pixel spacehas revealed that predicting the clean data (x-prediction) often yields superior results and enhanced stability compared to other options (Hoogeboom et al., 2024; Li & He, 2025; Hafner et al., 2025). This empirical shift suggests that as we scale models to higher resolutions and more complex data distributions, the optimal choice of prediction target is not static but evolves with the task. Despite these empirical advancements, the theoretical landscape remains fragmented, leaving several fundamental questions unanswered. First, it is unclear whether the optimal prediction target is strictly confined to the conventional triad of ε, v, and x, or if superior alternatives exist within more expansive, generalized objective space. For instance, in low-dimensional settings, it remains unproven whether vprediction represents universal optimum; conversely, for high-dimensional data, we lack definitive answer as to whether x-prediction is an indispensable requirement. Second, the field lacks rigorous theoretical explanation for how data dimensionality fundamentally dictates the efficiency and stability of these targets. While prior literature has invoked the manifold hypothesis to qualitatively justify x-prediction for sparse, high-dimensional data (Li & He, 2025), such conceptual frameworks are insufficient for rigorous analysis and cannot substitute for formal quantitative description. This absence of mathematical precision leads to further ambiguity: whether direct, quantifiable relationship exists between the datas ambient or intrinsic dimension and its optimal prediction target, and if so, whether the latter can be analytically derived from the former. Indeed, the precise mechanism by which the optimal target shifts from v-prediction toward x-prediction as dimensionality increases remains unknown. Finally, from practical standpoint, there remains no principled methodology to determine the ideal target for given dataset. Currently, researchers and practitioners are forced to rely on computationally expensive heuristic-driven searches or trial-anderror, which hampers the efficiency of model development for novel high-dimensional domains. In this work, we systematically address these gaps through Revisiting Diffusion Model Predictions Through Dimensionality novel theoretical and algorithmic framework. To this end, we first introduce generic formalism that accommodates prediction targets of arbitrary functional forms, treating standard targets as specific instances of unified objective. To gain fundamental insights into the training process, we analyze much simplified diffusion model, which consists of single linear layer tasked to learn across the entire spectrum of Signal-to-Noise Ratios (SNR) of the diffusion process. Surprisingly, such an SNR-invariant linear model exhibits remarkably rich learning dynamics, offering tractable yet powerful lens through which to examine the interplay between data dimensionality and prediction targets. Specifically, the optimization dynamics decouple into two distinct modes: parallel mode aligned with the data manifold and perpendicular mode orthogonal to it. Notably, we demonstrate that these two modes exhibit fundamentally different learning behaviors; while the parallel mode is governed by data recovery, the perpendicular mode is dominated by noise elimination. Since the relative contribution of these two modes is governed by the datasets dimensionality, fixed prediction targetchosen without regard for these geometric propertiesis inherently suboptimal. To further investigate the quantitative relationship between dimensionality and the optimal prediction objective, we introduce generalized target formulation: = kx(1k)n with 0 1. While mathematically concise, this expression defines continuous objective space that naturally encompasses conventional targets as discrete points along the spectrum; specifically, = 0, = 0.5, and = 1 recover ε-, v-, and x-prediction, respectively (up to constant scaling factor). By optimizing the loss with respect to the parameter k, we derive the optimal target configuration as function of both the intrinsic and ambient dimension of the data. This derivation establishes formal, quantitative bridge between the geometric properties of the data manifold and the optimal prediction objective. Finally, our parameterization of = kx (1 k)n also allows us to move beyond the selection of isolated candidates and instead treat the search for an optimal target as continuous optimization problem over the scalar k. Indeed, since determining the intrinsic dimension of dataset is often intractable, we propose k-Diffa framework that requires the introduction of only single extra learnable parameter to automatically identify the optimal prediction target via standard backpropagation. Extensive experiments in both latentspace and pixel-space image generation demonstrate that k-Diff consistently achieves comparable or superior results to fixed-target baselines. By adding negligible parameter overhead, k-Diff provides principled and automated approach to optimizing generative performance across varying data scales and representation spaces. 2. Related Work Low-Dimensional Diffusion Models The foundations of modern diffusion models (Sohl-Dickstein et al., 2015) were established through ε-prediction objectives (Ho et al., 2020; Song et al., 2020), which focused primarily on capturing noise patterns in relatively low-resolution or lowdimensional data regimes. Frameworks such as Denoising Diffusion Probabilistic Models (DDPM) (Ho et al., 2020) demonstrated the initial efficacy of this approach for image synthesis. However, as the field transitioned toward deterministic sampling and straighter probability paths, velocity prediction (v-prediction) emerged as dominant alternative within the Flow Matching (Lipman et al., 2022) and Progressive Distillation paradigms (Salimans & Ho, 2022). Notably, through elaborated parameterization, the EDM framework (Karras et al., 2022) leverages v-prediction to ensure the neural network maintains constant output variance across the entire SNR spectrum, thereby significantly easing training convergence. While these models improve stability and enable high-quality generation with fewer sampling steps in moderately high-dimensional spaces, the choice of target remains largely decoupled from the geometric properties of the data. Pixel-Space High-Resolution Diffusion As generative modeling scaled to high-resolution pixel spaces, the limitations of standard noise and velocity targets became more apparent. Recent large-scale models, particularly those operating directly in pixel space, have increasingly adopted data prediction (x-prediction) or heavily weighted SNR-based loss functions to maintain training stability (Hoogeboom et al., 2024; Li & He, 2025; Hafner et al., 2025). While empirical evidence suggests that x-prediction is superior for handling the sparse structures and high-frequency details of high-resolution images, this shift from to has remained largely heuristic. Our work provides the first quantitative bridge explaining why this transition is natural consequence of the increasing dimensionality of the data manifold. Learning and Generalization Dynamics of Diffusion Models growing body of literature seeks to understand the mathematical underpinnings of diffusion models beyond their generative performance, focusing specifically on their ability to generalize versus their tendency to memorize the training set. By investigating the inductive biases inherent in convolutional architectures, (Kamb & Ganguli, 2024) establishes formal link between models creativityits capacity to generate novel samples rather than merely memorizing training dataand the properties of locality and equivariance. They provide both theoretical and empirical evidence that these architectural constraints govern how diffusion models generalize from discrete training data to the 2 Revisiting Diffusion Model Predictions Through Dimensionality broader manifold. Regarding the mechanics of the sampling process, (Biroli et al., 2024) identifies three distinct dynamical regimes within the reverse-time generation process and characterizes two phase transitions between the regimes. The first is the speciation transition, where the evolution moves from pure noise toward the emergence of coarse structures corresponding to specific class, in manner analogous to symmetry-breaking phase transition. This is followed by the collapse transition into third regime where the diffusion model displays memorization of individual training samples, phenomenon similar to condensation in glass phase. In parallel, to understand the acquisition of generation and memorization during training, (Bonnaire et al., 2025) leverages an analysis of diffusion models using random feature networks and elucidates transition from effective generation at early training stages to rote memorization in later phases. This training evolution is characterized analytically as two-timescale process, identifying specific training windowwhich is proportional to the dataset sizewhere the model generates effectively before the onset of memorization. Despite these foundational studies, existing literature primarily addresses the global properties of diffusion training and sampling; the specific impact of the choice of prediction target on these dynamics remains largely unexplored. Learning and Generalization Dynamics of Neural Networks The study of learning and generalization in highdimensional neural networks is deeply rooted in the analysis of their training dynamics and the geometric properties of their loss landscapes. Seminal work on weight initialization (He et al., 2016; Schoenholz et al., 2016) demonstrated that preserving signal variance across layers is essential for the stability of deep architectures. Furthermore, the analysis of exact solutions for deep linear networks (Saxe et al., 2013) has provided fundamental understanding of how the spectral properties of the data determine the speed (Advani et al., 2020) charand order of feature learning. acterize the high-dimensional dynamics of generalization error by leveraging random matrix theory, revealing how the ratio of parameters to data and the signal-to-noise ratio govern the emergence of over-fitting and the trade-off between bias and variance. Our work builds upon this tradition by extending these analytical frameworks to the generative setting, utilizing simplified linear diffusion model to map the relationship between data dimensionality and the optimal prediction objective. 3. Theoretical Analysis of the Optimal"
        },
        {
            "title": "Prediction",
            "content": "u to be modeled by neural networks, which are defined as = αx + σn = φx + ψn (1a) (1b) Here, RD represents the clean data, and RD is Gaussian noise. α, σ, φ, ψ are process parameters that characterize the diffusion process. Typically, α and σ are pre-defined, and φ and ψ are left to be designed based on the needs. For example, for the widely-used flow matching model with v-prediction, we have α = and σ = 1 t, where denotes the forward and backward diffusion time, and φ = 1, ψ = 1. On the other hand, for x-prediction, we would have φ = 1 and ψ = 0. Without loss of generality, we choose α as the independent variable, and the other three parameters are functions of α. Meanwhile, for simplicity, we omit the subscription α in or although they are functions of α. Note that and are constants that do not depend on α. 3.1. Problem Definition To derive the optimal prediction target, we begin by investigating the learning dynamics of single-layer linear diffusion model trained on intrinsically low-dimensional data embedded in high-dimensional space. Following prior work (Li & He, 2025), we consider simplified setting where the data RD is projected from an intrinsic latent (cid:101)x Rd via an orthogonal mapping = (cid:101)x. Here, RDd is an arbitrary sub-matrix of DD orthogonal matrix, such that = Idd. In this formulation, and denote the intrinsic and ambient dimensions of the data, respectively, while Idd represents the identity matrix. Consider diffusion model consisting of single linear layer RDD. The model estimates the target from the noisy input via: ˆu = (2) In practice, rather than directly minimizing the Mean Squared Error (MSE) between and ˆu, training often involves an alternative variable wsuch as the velocity which is itself linear combination of data and noise n. As detailed in Appendix A, the errors associated with these two variables are related by scaling factor κ (Kingma & Gao, 2023): ˆw = κ ( ˆu u) (3) Consequently, when training the model using the MSE loss of w, the objective function becomes: = 1 2 (cid:90) (cid:101)DαE (cid:2)W u2(cid:3) (4) The general form of the diffusion process involves the noisy input from the diffusion process and the output or target where denotes expectation over and/or n, and is the integration interval (typically [0, 1]). Here, we define the Revisiting Diffusion Model Predictions Through Dimensionality effective integration measure (cid:101)Dα as (cid:101)Dα := Dα κ2 (5) where Dα is the probability measure of the sampling distribution for α (e.g., U[0, 1] or logit-normal (Esser et al., 2024)). The term κ2 thus acts as loss weighting function dependent on α. For brevity, we hereafter omit the integral interval unless explicitly required. To facilitate tractable analysis, we assume the intrinsic data (cid:101)x is whitened: Σ (cid:101)x(cid:101)x = (cid:2) (cid:101)x(cid:101)xT (cid:3) = Idd (6) This is widely-adopted simplification in the study of neural network learning dynamics (Saxe et al., 2013; Tian et al., 2021). Similarly, for Gaussian white noise n, we have: Σnn = (cid:2)nnT (cid:3) = (7) where denotes the identity matrix. Under these assumptions, we now analyze the learning dynamics of the linear diffusion model. 3.2. Learning Dynamics As derived in Appendix B, given the diffusion process and the linear layer configuration defined above, the network weights evolve during training according to: (cid:90) dW d(cid:101)t τ = (cid:101)Dα (cid:0)α2W + σ2W φαP ψσI(cid:1) (8) where τ represents the inverse learning rate and (cid:101)t denotes the training time. Notably, since is projection operator, the dynamics of can be decoupled into two orthogonal modes: one parallel to the data manifold and one perpendicular to it. (cid:90) (cid:90) = = τ dW d(cid:101)t τ dW d(cid:101)t (cid:101)Dα (cid:16) (α2 + σ2)W (φα + ψσ)P (cid:17) (9a) (cid:16) (cid:17) σ2W ψσ(I ) (cid:101)Dα (9b) Specifically, we define: = T = (I ) (10a) (10b) which represent the parallel and perpendicular components of , respectively. Remarkably, the two modes exhibit significantly distinct dynamics. These two modes exhibit remarkably distinct learning dynamics. While the parallel mode depends on data-related parameters such as α and φ, these factors are largely irrelevant to the perpendicular mode. Physically, the parallel mode is driven by the underlying data distribution, resembling process of data recovery. In contrast, the perpendicular mode is not data-driven; its dynamics are dominated by noise elimination within the ambient space. Furthermore, these diverging dynamics impose different requirements on the parameters φ and ψ. An optimal design must therefore account for the competing impacts of both modes. Based on these dynamics, we arrive at the following theorem regarding the equilibrium weights and the resulting loss. Theorem 3.1. Under the loss and learning dynamics defined in Eq. (4) and Eq. (9), the optimal weight is achieved at the equilibrium of both modes, given by: = + (cid:101)Dα(φα + ψσ) (cid:101)Dα(α2 + σ2) (cid:82) (cid:82) = (11a) + (cid:82) (cid:82) (cid:101)Dαψσ (cid:101)Dασ (I ) (11b) The corresponding optimal loss is obtained by substituting into Eq. (4): = (cid:16) (cid:90) 1 2 (cid:124) (cid:101)Dα(φ2 + ψ2) (cid:0) (cid:82) (cid:101)Dα(φα + ψσ)(cid:1)2 (cid:82) (cid:101)Dα(α2 + σ2) (cid:17) (cid:125) (cid:123)(cid:122) Parallel Contribution + 1 2 (cid:124) (cid:16) (cid:90) (D d) (cid:101)Dαψ2 (cid:0) (cid:82) (cid:82) (cid:101)Dαψσ(cid:1)2 (cid:101)Dασ2 (cid:123)(cid:122) Perpendicular Contribution (cid:17) (cid:125) (12) Remark. Based on the optimal loss derived in Eq. (12), we can make several key observations. First, the optimal loss is composed of two distinct contributions: the intramanifold loss, arising from the data manifold, and the residual loss, originating from the perpendicular ambient space. The intra-manifold loss is proportional to the intrinsic dimension of the manifold on which the data reside. Conversely, the residual loss from the perpendicular ambient space is proportional to the codimension (D d). Second, consistent with the learning dynamics and the optimal weights, the intra-manifold loss depends on the datadependent parameters α and φ. However, the contribution from the perpendicular ambient space is entirely independent of these factors. Third, in low-dimensional regimes where d, the intra-manifold contribution dominates the total loss. In contrast, for high-dimensional data where d, the ambient contribution becomes the primary contributor. Fourth, by setting ψ = 0which corresponds to x-predictionthe perpendicular contribution of the loss vanishes, leaving only the intra-manifold contribution to determine the total loss. Revisiting Diffusion Model Predictions Through Dimensionality Up to this point, our analysis has addressed the general case where the parameters (α, σ, φ, ψ) may take arbitrary values. To further elucidate the impact of the dimensions and D, one could theoretically minimize the optimal loss in Eq. (12) with respect to φ and ψ using variational methods (assuming α and σ are predefined). However, such procedure is highly complex, as the results depend not only on the dimensions and but also on several auxiliary factors. These include the choice of the optimization objective (which determines the scaling factor κ), the sampling distribution of α used during training, and the specific integration interval I. Therefore, in the following section, we introduce further simplifications to gain deeper insights into how the optimal prediction targets are fundamentally determined by the relationship between the dimensions and D. Algorithm 1 Training step # net(z, t): Diffusion Model # w_k: trainable parameter for # x: training batch = sample t() = randn like(x) = sigmoid(w_k) = * + (1 - t) * = * - (1 - k) * = ((1 - 2 * k) * + u) / (k * (1 - t) + (1 - k) * t) u_pred = net(z, t) v_pred = ((1 - 2 * k) * + u_pred) / (k * (1 - t) + (1 - k) * t) loss = l2 loss(v - v_pred) Algorithm 2 Sampling step (Euler) 3.3. k-Parameterized Diffusion Prediction Target # z: current samples at To further elucidate the influence of the intrinsic and ambient dimensions on the optimal prediction target, we introduce simplification where φ = and ψ = (1 k). Here, 0 1 is constant parameter independent of α. Under this parameterization, the network prediction target takes the form: = kx (1 k)n (13) We further specialize our analysis to the flow matching framework, setting α = and σ = 1 t. Notably, up to constant scaling factor, the conventional ε-, v-, xpredictions are recovered as special cases where = 0, = 0.5, and = 1, respectively. This leads to the following central result: Theorem 3.2. Consider diffusion process defined by α = and σ = 1 t. Let the prediction target be parameterized by φ = and ψ = (1 k), where [0, 1] is independent of t. For the objective defined in Eq. (4), assuming U[0, 1] and loss weighting κ = 1 (i.e., = u), the optimal loss in Eq. (12) is minimized when: = + (14) Theorem 3.2 provides an explicit analytical relationship between the optimal prediction target and the dimensions and D. Specifically, Eq. (14) reveals three distinct regimes: Low-dimensional / Dense data (D d): Here, 0.5, confirming that v-prediction is optimal when the data fills the ambient space. High-dimensional / Sparse data (D d): In this regime, where data resides on low-dimensional manifold, 1, establishing x-prediction as the superior choice. = sigmoid(w_k) u_pred = net(z, t) v_pred = ((1 - 2 * k) * + u_pred) / (k * (1 - t) + (1 - k) * t) z_next = + (t_next - t) * v_pred Intermediate regimes: For general data distributions, neither conventional target is optimal; instead, the ideal lies between 0.5 and 1. Our analysis provides, to our knowledge, the first quantitative derivation of the optimal relative to data geometry. Furthermore, it demonstrates that for the vast majority of real-world datasets, the optimal prediction target lies in continuous space between the discrete points of ε-, v-, and x-prediction. 4. Learnable Diffusion Prediction Target While Eq. (14) provides the theoretical optimal based on the dimensions and D, estimating the intrinsic dimension of high-dimensional, non-linear manifolds is often intractable in practice. To address this, we propose an adaptive approach that treats as learnable parameter. Specifically, we define trainable scalar wk such that = sigmoid(wk). The parameter wk is optimized alongside the network weights θ via standard backpropagation using the following objective: L(θ, wk) = (cid:90) 1 2 (cid:101)DαE (cid:2)uθ(z) u2(cid:3) (15) Note that here depends on wk via as in Eq. 13. This end-to-end, data-driven formulation enables the model to automatically discover the optimal balance between noise 5 Revisiting Diffusion Model Predictions Through Dimensionality (a) Latent space diffusion. (b) Pixel space diffusion. Figure 1. FID-50k convergence dynamics of k-Diff in latent and pixel spaces. (a) Performance comparison in latent space using LightningDiT-XL/1 architecture on ImageNet-256 over 64 epochs. k-Diff maintains convergence profile nearly identical to the original Flow Matching (v-prediction) framework. (b) Convergence comparison in pixel space using JiT-B/16 architecture over 200 epochs. While x-prediction exhibits slightly faster initial descent, k-Diff recovers this gap as the learnable target stabilizes, indicating that the dynamic optimization of does not compromise training efficiency or final generative performance across disparate architectures. elimination and data recovery. Beyond the simplified case of constant (timeindependent), we also explore more flexible timedependent formulation, k(t). For this purpose, we employ piecewise linear approximation: the interval [0, 1] is partitioned into bins (e.g., = 128), where each bin endpoint ti is associated with learnable parameter wk(ti). For any [ti, ti+1], the value of k(t) is obtained via linear interpolation of the sigmoid-transformed endpoints (see Appendix for more details). We term our method k-Diff and evaluate its performance in both latent-space and pixel-space diffusion regimes. By adaptively determining the optimal prediction target, kDiff eliminates the need for manual hyper-parameter tuning across different data domains. Furthermore, our approach is orthogonal to existing diffusion enhancementssuch as feature alignment, advanced sampling schemes, or improved autoencodersand can be seamlessly integrated into existing pipelines. The pseudo-code for the training and sampling procedures of k-Diff is provided in Alg. 1 and Alg. 2. For numerical stability, and following the practice in (Li & He, 2025), we clamp the denominator in the velocity calculation, k(1t)+ (1 k)t, to minimum value of 0.05 to prevent division-byzero errors when necessary (see Table 4 for more details). 5. Experiments In this section, we provide empirical results to verify the effectiveness of our proposed method of k-Diff to learn the optimal prediction. Our experiments focus on ImageNet of resolutions 256256 and 512512. For ImageNet 256256, we verify with LightningDiT-XL/1 from (Yao et al., 2025) for latent space generation and JiT-B/16 from (Li & He, 2025) for pixel space generation. For ImageNet 512512, we only verify with JiT-B/32 from (Li & He, 2025) for pixel space generation. 5.1. Latent Space Diffusion We first verify our method within the latent space diffusion regime, primarily adopting the architecture and training protocol established by (Yao et al., 2025). Specifically, we train the model for 64 epochs and perform sampling using Classifier-Free Guidance (CFG) (Ho & Salimans, 2021) scale of 1.5 with the Heun solver (Heun, 1900) for 50 sampling steps (NFE=99). While these hyperparameters deviate slightly from the original report (Yao et al., 2025)which utilized CFG=10.0 with an Euler solver and 250 stepswe justify this configuration by noting that the baseline Flow Matching model (v-prediction) yields an FID of 2.08 under our settings, which is highly consistent with the reported value of 2.11. In comparison, our model achieves superior FID of 2.05 after the same 64 epochs of training. This is further illustrated in Fig. 1a, where the k-Diff method exhibits convergence profile highly similar to the original v-prediction framework, as measured by the FID-50k metric across the training process. To assess the long-term convergence of our method, we extended the training process to 800 epochs, ultimately reaching final FID of 1.34. As illustrated in the convergence plot of FID-50k in Fig. 2, we observe noticeable overfitting trend in the latter stages of training; while the final model remains highly performant, the peak generative quality is reached around epoch 384 with minimum FID of 1.22. Despite this intermediate peak, we maintain consistent reporting standard: all results tabulated in 1 and 2 and the visual samples presented in Fig. 4 are produced by the 6 Revisiting Diffusion Model Predictions Through Dimensionality Figure 2. Long-term FID-50k dynamics for k-Diff on ImageNet256. Training was conducted in latent space using LightningDiTXL/1 architecture. The model reaches optimal generative performance at epoch 384 with peak FID of 1.22. As training extends to 800 epochs, marginal degradation occurs, with the final FID converging to 1.34. This suggests subtle shift in the models generalization-memorization balance during the latter stages of extended training. final 800-epoch checkpoint to provide clear view of the models stability over the entire training run. Note that even with this slight degradation, the final FID of 1.34 remains comparable to that from the original work (Yao et al., 2025), demonstrating the robustness of the k-Diff framework even when trained far beyond the point of initial convergence. It is also instructive to examine the optimization trajectory of the learnable parameter and its final steady-state value to evaluate how the empirical optimum compares with the standard v-prediction objective (k = 0.5). As shown in Fig. 3, the value monotonically increases from its initial value of 0.5 to final converged value of 0.66. This shift indicates that while the latent space is lower-dimensional than the pixel space, it still possesses significant ambient dimensions. The dynamics within this orthogonal subspace pull the optimal toward higher value, suggesting that target closer to x-prediction is more effective at resolving the data manifold in this specific latent representation. We further investigate the impact of the time-dependence of on generative quality. As detailed in the ablation study in Appendix E, we observe that constant is sufficient for achieving optimal performance, whereas time-dependent parameterization using 128 bins fails to yield further improvements. This empirical finding aligns with our theoretical analysis, suggesting that the benefits of stable, consistent prediction objective outweigh the potential flexibility of time-varying target. In Fig. 4, we present selection of generated samples produced by the model trained with the k-Diff objective within the latent space. These visual results demonstrate that k-Diff effectively captures the complex, multi-modal distribution of the ImageNet-256 dataset. Notably, the samples exhibit high levels of structural integrity and fine-grained textures, Figure 3. Evolution of in Latent vs. Pixel Space. We plot the trajectory of against normalized training progress (total epochs: 800 for latent, 600 for pixel). In pixel space (JiT-B/16), exhibits sharp ascent, converging to = 1.0 (x-prediction) within the first 5% of training. Conversely, in latent space (LightningDiTXL/1), climbs gradually to steady-state value of 0.66. This stark difference in optimization dynamics validates that k-Diff autonomously adapts to the representation space: the high ambient dimensionality of pixels strongly favors x-prediction, while the compressed latent manifold identifies an optimal target between vand x-prediction. Figure 4. Qualitative Results. Selected examples on ImageNet 256256 from LightningDiT-XL/1 trained with k-Diff. ranging from the intricate patterns in biological species to the sharp geometric features of architectural structures. The quality of these images suggests that the learned target = 0.66 strikes an optimal balance between denoising stability and the preservation of manifold-specific details. 5.2. Pixel Space Diffusion In addition to the latent space experiments, we evaluate the efficacy of our method within the pixel space regime. As shown in Fig. 1b, we compare the convergence trajectory of k-Diff against the x-prediction baseline utilizing the JiT framework (Li & He, 2025) for ImageNet-256 with the model architecture JiT-B/16. Our empirical results indicate that our method achieves final convergence pro7 Revisiting Diffusion Model Predictions Through Dimensionality params Gflops FID IS 119 675+49M 119 675+49M 119 675+49M 119 675+49M 675+49M 119 839+415M 146 119 675+49M 2.27 2.06 1.42 1.35 1.26 1.13 1.34 278.2 277.5 305.7 295.3 310.6 262.6 301.7 ImgNet 256256 Latent-space Diffusion DiT-XL/2 (Peebles & Xie, 2023) SiT-XL/2 (Ma et al., 2024) REPA (Yu et al., 2024), SiT-XL/2 LightningDiT-XL/1 (Yao et al., 2025) DDT-XL/2 (Wang et al., 2025b) RAE (Zheng et al., 2025), DiTDH-XL/2 k-Diff (LightningDiT-XL/1) Pixel-space Diffusion ADM-G (Dhariwal & Nichol, 2021) RIN (Jabri et al., 2022) SiD (Hoogeboom et al., 2023) , UViT/2 VDM++ (Kingma & Gao, 2023), UViT/2 SiD2 (Hoogeboom et al., 2024), UViT/2 PixelFlow (Chen et al., 2025), XL/4 PixNerd (Wang et al., 2025a), XL/16 JiT-B/16 (Li & He, 2025) k-Diff (JiT-B/16) 4.59 3.42 2.44 2.12 1.73 1.98 2.15 3.66 3.64 1120 334 555 555 137 2909 134 25 554M 410M 2B 2B N/A 677M 700M 131M 131M 186.7 182.0 256.3 267.7 - 282.1 297 275.1 274.9 Table 1. Comparison results on ImageNet 256256. We evaluate our k-Diff objective across both latent space (using LightningDiTXL/1) and pixel space (using JiT-B/16) configurations. FID (Heusel et al., 2017) and IS (Salimans et al., 2016) are computed using 50K samples. Following (Li & He, 2025), reported parameters include the generator and tokenizer decoder required at inference time, while excluding auxiliary pre-trained modules. GFLOPs are calculated for single forward passexcluding the tokenizerserving as proxy for the computational cost per iteration during both training and inference. For multi-scale methods (Chen et al., 2025), we measure the finest resolution level. file comparable to x-prediction (FID=4.70 for both cases after training 200 epochs), demonstrating that k-Diff can adaptively recover the performance of specialized targets in high-dimensional pixel environments. This adaptive behavior is further elucidated in Fig. 3, which tracks the evolution of the learnable parameter during the pixel-space training. Unlike the latent space case, we observe that rapidly ascends from its initialization at 0.5 and converges to value near 1.0 (corresponding to x-prediction) in fewer than 30 epochs. This accelerated convergence suggests that the high dimensionality of the pixel space creates strong signal for the model to prioritize data recovery over noise estimation. The fact that k-Diff autonomously identifies x-prediction as the optimal objective confirms previous hypothesis (Hoogeboom et al., 2024; Li & He, 2025) and the results derived in Theorem 3.2: in spaces where the ambient dimension is much larger than the manifold dimension d, the optimal target naturally shifts toward x. By reaching this state so early in the training process, k-Diff eliminates the need for manual objective selection while ensuring the model is optimized for the correct geometric regime from the outset. We evaluate the performance of k-Diff against established baselines across multiple resolutions and generation spaces. The results for ImageNet-256, spanning both latent and pixel-space diffusion regimes, are summarized in Table 1. Furthermore, we extend our comparative analysis to pixel8 params Gflops FID IS 525 675+49M 525 675+49M 525 675+49M 675+49M 525 839+415M 642 3.04 2.62 2.08 1.28 1. 240.8 252.2 274.6 305.1 259.6 ImgNet 512512 Latent-space Diffusion DiT-XL/2 (Peebles & Xie, 2023) SiT-XL/2 (Ma et al., 2024) REPA (Yu et al., 2024), SiT-XL/2 DDT-XL/2 (Wang et al., 2025b) RAE (Zheng et al., 2025), DiTDH-XL/2 Pixel-space Diffusion ADM-G (Dhariwal & Nichol, 2021) RIN (Jabri et al., 2022) SiD (Hoogeboom et al., 2023) , UViT/4 VDM++ (Kingma & Gao, 2023), UViT/4 SiD2 (Hoogeboom et al., 2024), UViT/4 PixNerd (Wang et al., 2025a), XL/16 JiT-B/32 (Li & He, 2025) k-Diff (JiT-B/32) 559M 320M 2B 2B N/A 700 133M 133M 172.7 216.0 248.7 278.1 - 245.6 271.0 277.1 Table 2. Comparison results on ImageNet 512512. This table summarizes the performance of our k-Diff method in the highresolution pixel space regime using the JiT-B/32 architecture. Metric notations and evaluation protocols are identical to those described in Table 1. 1983 415 555 555 137 583 26 26 7.72 3.95 3.02 2.65 2.19 2.84 4.02 4. space diffusion at the higher resolution of ImageNet-512, as detailed in Table 2. Across these benchmarks, our method consistently matches or outperforms existing fixed-target approaches, validating its effectiveness as general-purpose objective for high-fidelity generative modeling. 6. Conclusion In this work, we have addressed fundamental yet often overlooked question in generative modeling: how to determine the optimal prediction target for diffusion and flow matching processes. By analyzing the learning dynamics of linear diffusion model, we provided rigorous theoretical framework that identifies the intrinsic and ambient dimensions of data as the governing factors of training efficiency. Our derivation of the analytical relationship, = D/(D + d), offers principled justification for the empirical shift toward x-prediction in high-dimensional settings while revealing that conventional targetssuch as noise and velocityare often suboptimal for the complex geometries of real-world manifolds. To bridge the gap between theory and practice, we introduced k-Diff, data-driven framework that adaptively learns the optimal prediction target through standard backpropagation. By parameterizing the target spectrum and treating it as learnable component, k-Diff bypasses the intractable task of explicit dimension estimation by introducing only single additional learnable scalar. Our experiments across both latent and pixel spaces demonstrate that k-Diff not only matches or exceeds the performance of manually tuned baselines but also provides more robust and automated path for scaling generative models to novel domains. As diffusion models continue to grow in complexity, k-Diff offers simple, orthogonal, and theoretically grounded approach to optimizing the core objective of the generative process with negligible computational and parameter overhead. Revisiting Diffusion Model Predictions Through Dimensionality"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents theoretical and algorithmic framework intended to advance the field of Machine Learning by improving the training efficiency and fidelity of generative diffusion models. By introducing principled method to learn the optimal prediction target, our work reduces the need for manual hyperparameter tuning and lowers the computational barrier for high-quality image synthesis. While we recognize that improvements in generative modeling can be dual-usepotentially enhancing both creative tools and the production of synthetic mediaour research is focused on the fundamental mathematical dynamics of these systems. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here beyond the established ethical considerations associated with generative AI development."
        },
        {
            "title": "Acknowledgement",
            "content": "QJ gratefully acknowledges GPUHub for the generous gift and the partial support of computing resources used in this study."
        },
        {
            "title": "References",
            "content": "Advani, M. S., Saxe, A. M., and Sompolinsky, H. Highdimensional dynamics of generalization error in neural networks. Neural Networks, 132:428446, 2020. Biroli, G., Bonnaire, T., De Bortoli, V., and Mezard, M. Dynamical regimes of diffusion models. Nature Communications, 15(1):9957, 2024. Bonnaire, T., Urfin, R., Biroli, G., and Mezard, M. Why diffusion models dont memorize: The role of implicit dynamical regularization in training. arXiv preprint arXiv:2505.17638, 2025. Chen, S., Ge, C., Zhang, S., Sun, P., and Luo, P. Pixelflow: Pixel-space generative models with flow. arXiv preprint arXiv:2504.07963, 2025. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Goyal, P., Dollar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K. Accurate, large minibatch SGD: Training ImageNet in 1 hour. arXiv:1706.02677, 2017. Hafner, D., Yan, W., and Lillicrap, T. Training agents arXiv preprint inside of scalable world models. arXiv:2509.24527, 2025. He, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630645. Springer, 2016. Heun, K. Neue methoden zur approximativen integration der differentialgleichungen einer unabhangigen veranderlichen. Z. Math. Phys, 45:2338, 1900. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Ho, J. and Salimans, T. Classifier-free diffusion guidance. In NeurIPS Workshops, 2021. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Hoogeboom, E., Heek, J., and Salimans, T. simple diffusion: End-to-end diffusion for high resolution images. In International Conference on Machine Learning, pp. 1321313232. PMLR, 2023. Hoogeboom, E., Mensink, T., Heek, J., Lamerigts, K., Gao, R., and Salimans, T. Simpler diffusion (sid2): 1.5 fid on imagenet512 with pixel-space diffusion. arXiv preprint arXiv:2410.19324, 2024. Jabri, A., Fleet, D., and Chen, T. Scalable adaptive computation for iterative generation. arXiv preprint arXiv:2212.11972, 2022. Kamb, M. and Ganguli, S. An analytic theory of creativity in convolutional diffusion models. arXiv preprint arXiv:2412.20292, 2024. Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35: 2656526577, 2022. Kingma, D. and Gao, R. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36:65484 65516, 2023. Kingma, D. P. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kynkaanniemi, T., Aittala, M., Karras, T., Laine, S., Aila, T., and Lehtinen, J. Applying guidance in limited interval improves sample and distribution quality in diffusion models. Advances in Neural Information Processing Systems, 37:122458122483, 2024. 9 Revisiting Diffusion Model Predictions Through Dimensionality Li, T. and He, K. Back to basics: Let denoising generative models denoise. arXiv preprint arXiv:2511.13720, 2025. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., VandenEijnden, E., and Xie, S. Sit: Exploring flow and diffusionbased generative models with scalable interpolant transformers. In European Conference on Computer Vision, pp. 2340. Springer, 2024. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Yao, J., Yang, B., and Wang, X. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1570315712, 2025. Yu, S., Kwak, S., Jang, H., Jeong, J., Huang, J., Shin, J., and Xie, S. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. Zheng, B., Ma, N., Tong, S., and Xie, S. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. Salimans, T. and Ho, J. fast sampling of diffusion models. arXiv:2202.00512, 2022."
        },
        {
            "title": "Progressive distillation for\narXiv preprint",
            "content": "Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Improved techniques for Radford, A., and Chen, X. training gans. Advances in neural information processing systems, 29, 2016. Saxe, A. M., McClelland, J. L., and Ganguli, S. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013. Schoenholz, S. S., Gilmer, J., Ganguli, S., and SohlDickstein, J. Deep information propagation. arXiv preprint arXiv:1611.01232, 2016. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. pmlr, 2015. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Tian, Y., Chen, X., and Ganguli, S. Understanding selfsupervised learning dynamics without contrastive pairs. In International Conference on Machine Learning, pp. 1026810278. PMLR, 2021. Wang, S., Gao, Z., Zhu, C., Huang, W., and Wang, L. Pixnerd: Pixel neural field diffusion. arXiv preprint arXiv:2507.23268, 2025a. Wang, S., Tian, Z., Huang, W., and Wang, L. Ddt: arXiv preprint Decoupled diffusion transformer. arXiv:2504.05741, 2025b. 10 Revisiting Diffusion Model Predictions Through Dimensionality A. Definition of the Scaling Factor κ Here we provide more detailed description of the scaling factor κ defined in Eq. 3. Our analysis is aligned with the derivations presented in (Kingma & Gao, 2023). Generally, for the diffusion process we have and defined in Eq. 1, and the estimated value from the neural network ˆu. Now, we could solve for the estimated ˆx and ˆn from the requirement = α ˆx + σ ˆn ˆu = φ ˆx + ψ ˆn (16a) (16b) which is similar in form to the equation of the original diffusion process given by Eq. 1. Indeed, from Eq. 1 and Eq. 16, we have = = ψz + σu φσ ψα φz αu φσ ψα ˆx = ˆn = ψz + σ ˆu φσ ψα φz α ˆu φσ ψα and respectively. Now, for any target variable for MSE minimization, which is linear combination of and and defined as where ξ and η are some parameters and can be functions of α, we have its estimated value defined as = ξx + ηn So the loss defined for and that for is related by ˆw = ξ ( ˆx x) + η ( ˆn n) ˆw = ξ ˆx + η ˆn ψz + σu φσ ψα (cid:19) + η (cid:18) φz α ˆu φσ ψα φz αu φσ ψα (cid:19) ( ˆu u) + η α φσ ψα ( ˆu u) = ξ = ξ (cid:18) ψz + σ ˆu φσ ψα σ φσ ψα ξσ ηα φσ ψα = κ ( ˆu u) = ( ˆu u) which is Eq. 3. Here, we have defined κ := ξσ ηα φσ ψα (17a) (17b) (18a) (18b) (19) (20) (21a) (21b) (21c) (21d) (21e) (22) Now we give some specific examples to illustrate the usage of this relationship. For example, for the original u-loss where = u, we have For x-loss, we have = x, ξ = 1, η = 0, and For ε-loss, we have = n, ξ = 0, η = 1, and κ = κ = σ φσ ψα κ = α φσ ψα For v-loss, we have = n, ξ = 1, η = 1, and κ = α + σ φσ ψα 11 (23) (24) (25) (26) Revisiting Diffusion Model Predictions Through Dimensionality B. Derivation of the Learning Dynamics in Eq. 9 Here we derive the learning dynamics given in Eq. 9. The learning dynamics for the weight is generally given by the gradient descent τ dW d(cid:101)t = L (27) Here, as before, τ represents the inverse of the learning rate, and (cid:101)t denotes the training progress of the network. From the definition of the MSE loss in Eq. 4, we have τ dW d(cid:101)t (cid:90) = = (cid:20) 1 2 (cid:101)DαE (cid:2)W u2(cid:3) (cid:101)DαE (cid:2)W u2(cid:3) = (cid:90) (cid:21) = = (cid:90) (cid:101)DαE (cid:2)2 (W u) zT (cid:3) (cid:18) (cid:2)zzT (cid:3) (cid:2)uzT (cid:3) (cid:19) (cid:101)Dα 1 2 1 2 (cid:90) where for simplicity we have omitted the integral interval without ambiguity. Now from the definition in Eq. 1, we have (cid:2)zzT (cid:3) = (cid:2)(αx + σn)(αx + σn)T (cid:3) = α2E (cid:2)xxT (cid:3) + ασE (cid:2)xnT (cid:3) + ασE (cid:2)nxT (cid:3) + σ2E (cid:2)nnT (cid:3) (cid:2)uzT (cid:3) = (cid:2)(φx + ψn)(αx + σn)T (cid:3) = φαE (cid:2)xxT (cid:3) + φσE (cid:2)xnT (cid:3) + ψαE (cid:2)nxT (cid:3) + ψσE (cid:2)nnT (cid:3) Since and are independent random variables, and [n] = 0, we have (cid:2)xnT (cid:3) = [x] [n]T = 0 (cid:2)nxT (cid:3) = [n] [x]T = 0 From the definition = (cid:101)x and the assumptions in Eq. 6, we have (cid:2)xxT (cid:3) = (cid:104) (cid:101)x (P (cid:101)x)T (cid:105) (cid:101)x(cid:101)xT (cid:3) = (cid:2) = Now using the white noise assumption in Eq. 7, we have (cid:2)zzT (cid:3) = α2P + σ2I (cid:2)uzT (cid:3) = φαP + ψσI The learning dynamics is thus given by τ dW d(cid:101)t (cid:90) (cid:90) (cid:90) = = = (cid:18) (cid:18) (cid:101)Dα (cid:101)Dα (cid:2)zzT (cid:3) (cid:2)uzT (cid:3) (cid:19) (cid:0)α2P + σ2I(cid:1) (cid:0)φαP + ψσI(cid:1) (cid:19) (cid:101)Dα (cid:0)α2W + σ2W φαP ψσI(cid:1) 12 (28a) (28b) (28c) (28d) (28e) (29a) (29b) (29c) (29d) (30a) (30b) (31a) (31b) (31c) (32a) (32b) (33a) (33b) (33c) Revisiting Diffusion Model Predictions Through Dimensionality which is exactly Eq. 8. Now we multiply both sides of the equation on the right by and , respectively. Notice that is projection operation, and (34a) (34b) (34c) (35a) (35b) (35c) (35d) (35e) (36a) (36b) (P )(P ) = (I )(I ) = P (I ) = (I )P = 0 we have τ d(W ) d(cid:101)t = τ"
        },
        {
            "title": "P P T",
            "content": "dW d(cid:101)t (cid:90) = (cid:101)Dα (cid:0)α2W + σ2W φαP ψσI(cid:1) (cid:90) (cid:90) (cid:90) = = = (cid:101)Dα (cid:0)α2W P + σ2W φαP P ψσP (cid:1) (cid:101)Dα (cid:0)α2W + σ2W φαP ψσP (cid:1) (cid:101)Dα (cid:0)(α2 + σ2)W (φα + ψσ)P (cid:1) (I ) and τ d(W (I )) d(cid:101)t = τ dW d(cid:101)t (cid:90) = (cid:101)Dα (cid:0)α2W + σ2W φαP ψσI(cid:1) (I ) (cid:90) (cid:90) = = (cid:101)Dα (cid:0)α2W (I ) + σ2W (I ) φαP (I ) ψσ(I )(cid:1) (36c) (cid:101)Dα (cid:0)σ2W (I ) ψσ(I )(cid:1) (36d) which are exactly Eq. 9 by the definition in Eq. 10. C. Derivation of the Optimal Loss in Theorem 3.1 Now the optimal value of the two modes and are given by the condition that the time derivatives in Eq. 9a and Eq. 9b vanish, respectively. Notice that is not dependent on α and so are and W, so the equilibrium conditions given by (cid:90) (cid:16) (cid:101)Dα (α2 + σ2)W (φα + ψσ)P (cid:17) (cid:17) ψσ(I ) = = 0 (cid:16) σ2W are equivalent to (cid:90) (cid:101)Dα (cid:90) (cid:101)Dα(α2 + σ2) (cid:90) (cid:101)Dα(φα + ψσ) = 0 (cid:90) (cid:101)Dασ2 (I ) (cid:90) (cid:101)Dαψσ = 0 which immediately gives Eq. 11. (37a) (37b) (38a) (38b) Revisiting Diffusion Model Predictions Through Dimensionality The optimal loss is given by substituting Eq. 11 into Eq. 4, which is = = = = = (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) 1 2 1 2 1 2 1 2 1 2 Now as in Eq. 32, we have (cid:101)DαE (cid:2)W u2(cid:3) (cid:18) (cid:20) (cid:101)DαE Tr (W u)(W u)T (cid:19)(cid:21) (cid:18) (cid:2)(W u)(W u)T (cid:3) (cid:19) (cid:18) (cid:2)W zzT uzT zuT + uuT (cid:3) (cid:19) (cid:18) (cid:2)zzT (cid:3) (cid:2)uzT (cid:3) E (cid:2)zuT (cid:3) + (cid:2)uuT (cid:3) (cid:19) (cid:101)Dα Tr (cid:101)Dα Tr (cid:101)Dα Tr (cid:2)zzT (cid:3) = α2P + σ2I (cid:2)uzT (cid:3) = φαP + ψσI (cid:2)zuT (cid:3) = (cid:2)uzT (cid:3)T = φαP + ψσI (cid:2)uuT (cid:3) = (cid:2)(φx + ψn)(φx + ψn)T (cid:3) = φ2E (cid:2)xxT (cid:3) + φψE (cid:2)xnT (cid:3) + φψE (cid:2)nxT (cid:3) + ψ2E (cid:2)nnT (cid:3) = φ2P + ψ2I we have = (cid:90) 1 2 (cid:101)Dα Tr (cid:18) (α2P +σ2I)W (φαP +ψσI)W W (φαP +ψσI)+(φ2P +ψ2I) From Eq. 11, and notice that and are directly proportional to and , respectively, we have = = W = (I ) = (I )W (I ) = (I )W = 0 T = W = 0 and thus Tr (cid:0)W T (cid:1) = Tr = Tr = Tr = Tr (cid:16) (cid:16) (W + )P (W (cid:17) (cid:17) + ) W (cid:17) (cid:16) (cid:32) (cid:82) (cid:82) (cid:82) (cid:82) T (cid:101)Dα(φα + ψσ) (cid:101)Dα(α2 + σ2) (cid:17)2 (cid:17)2 Tr(P ) (cid:101)Dα(φα + ψσ) (cid:101)Dα(α2 + σ2) (cid:16)(cid:82) (cid:16)(cid:82) = 14 (cid:101)Dα(φα + ψσ) (cid:101)Dα(α2 + σ2) (cid:33) (39a) (39b) (39c) (39d) (39e) (40a) (40b) (40c) (40d) (40e) (40f) (40g) (cid:19) (41) (42a) (42b) (42c) (42d) (43a) (43b) (43c) (43d) (43e) Revisiting Diffusion Model Predictions Through Dimensionality Tr (cid:0)W (I )W (cid:1) = Tr (cid:17) + ) (cid:16) (W = Tr (cid:0)W = Tr (cid:0)W (cid:32) (cid:82) (cid:82) = Tr (cid:16)(cid:82) (cid:16)(cid:82) = (cid:101)Dαψσ (cid:101)Dασ2 )(I )(W (cid:1) + (I )W (cid:1) T (cid:101)Dαψσ (cid:101)Dασ2 (cid:17)2 (cid:17)2 Tr(I ) (I ) (cid:82) (cid:82) (cid:33) (I ) (cid:101)Dαψσ (cid:101)Dασ Tr (cid:0)W T (cid:1) = Tr (cid:0)P T (cid:1) = Tr (cid:17) (cid:16) (cid:82) (cid:82) = (cid:101)Dα(φα + ψσ) (cid:101)Dα(α2 + σ2) Tr (cid:0)P (cid:1) Tr (cid:0)W (I )(cid:1) = Tr (cid:0)(I )W (cid:1) (cid:82) = Tr (W ) (cid:101)Dαψσ (cid:101)Dασ2 = (cid:82) Tr (cid:0)I (cid:1) and With these results, we have (44a) (44b) (44c) (44d) (44e) (45a) (45b) (45c) (46a) (46b) (46c) (α2P + σ2I)W (φαP + ψσI)W (φαP + ψσI) + (φ2P + ψ2I) (cid:19) (cid:18) (cid:18) (cid:101)Dα Tr (cid:101)Dα Tr = = = (cid:90) (cid:90) (cid:90) 1 1 2 1 2 (α2P + σ2I)W 2W (φαP + ψσI) + (φ2P + ψ2I) (cid:19) (cid:32) (cid:101)Dα Tr (cid:18) (α2 + σ2)P + σ2(I ) (cid:19) (cid:18) 2W (φα + ψσ)P + ψσ(I ) (cid:19) (cid:18) + (φ2 + ψ2)P + ψ2(I ) (cid:19)(cid:33) (cid:90) = 1 2 (cid:32) (cid:101)Dα (α2 + σ2) Tr (cid:18) T (cid:19) (cid:18) + σ2 Tr (I )W (cid:19) 2(φα + ψσ) Tr (cid:18) T (cid:19) 2ψσ Tr (cid:18) (I ) (cid:19) + (φ2 + ψ2) Tr(P ) + ψ2 Tr(I ) (cid:90) = 1 2 (cid:32) (cid:101)Dα (α2 + σ2) (cid:16)(cid:82) (cid:16)(cid:82) (cid:101)D α( φα + ψσ) (cid:101)D α(α2 + σ2) (cid:17)2 (cid:17)2 Tr(P ) + σ2 (cid:16)(cid:82) (cid:16)(cid:82) (cid:17)2 (cid:17)2 Tr(I ) (cid:101)D α ψσ (cid:101)D ασ2 2(φα + ψσ) (cid:82) (cid:82) (cid:101)D α( φα + ψσ) (cid:101)D α(α2 + σ2) Tr (cid:0)P (cid:1) 2ψσ Tr (cid:0)I (cid:1) (cid:82) (cid:82) (cid:101)D α ψσ (cid:101)D ασ 15 (47a) (47b) (47c) (cid:33) (47d) (47e) (47f) (47g) (48a) (48b) (48c) (49a) (49b) Revisiting Diffusion Model Predictions Through Dimensionality + (φ2 + ψ2) Tr(P ) + ψ2 Tr(I ) (cid:33) (cid:16)(cid:82) (cid:101)Dα(φα + ψσ) (cid:82) (cid:101)Dα(α2 + σ2) (cid:17)2 Tr(P ) + (cid:16)(cid:82) (cid:17) (cid:101)Dαψσ (cid:82) (cid:101)Dασ2 1 2 Tr(I ) = 1 2 (cid:90) 1 2 + 1 2 (cid:90) = 1 2 (cid:17) (cid:16)(cid:82) 2 (cid:82) (cid:101)Dα(φα + ψσ) (cid:101)Dα(α2 + σ2) Tr(P ) (cid:16)(cid:82) 2 (cid:17)2 (cid:101)Dαψσ (cid:82) (cid:101)Dασ 1 2 Tr(I ) (cid:101)Dα(φ2 + ψ2) Tr(P ) + (cid:101)Dαψ2 Tr(I ) (cid:90) 1 2 (cid:101)Dα(φ2 + ψ2) (cid:16)(cid:82) (cid:101)Dα(φα + ψσ) (cid:82) (cid:101)Dα(α2 + σ2) (cid:17) Tr(P ) + 1 2 (cid:90) (cid:101)Dαψ2 (cid:16)(cid:82) (cid:17)2 (cid:101)Dαψσ (cid:82) (cid:101)Dασ2 Tr(I ) where we have introduced dummy variables α, σ, φ, ψ to avoid symbolic ambiguity. Tr(P ) = Tr(P ) = Tr(Idd) = Tr(I ) = Tr(I) Tr(P ) = Notice that and we have = 1 2 which is Eq. 12. (cid:90) (cid:101)Dα(φ2 + ψ2) (cid:17)2 (cid:16)(cid:82) (cid:101)Dα(φα + ψσ) (cid:82) (cid:101)Dα(α2 + σ2) + 1 2 (D d) (cid:90) (cid:101)Dαψ2 (cid:16)(cid:82) (cid:101)Dαψσ (cid:17)2 (cid:82) (cid:101)Dασ2 (50) D. Derivation of the Optimal Value of in Theorem 3.2 Now for the simplified case where α = t, σ = 1 t, φ = k, ψ = (1 k) with 0 1 independent of t, we have the optimal loss in Eq. 12 simplified as 1 2 = (cid:0)k2 + (1 k)2(cid:1) (cid:90) (cid:101)Dα (cid:16) (cid:82) (cid:101)Dαα (1 k) (cid:82) (cid:17) (cid:101)Dα(1 α) (cid:82) (cid:101)Dα (α2 + (1 α)2) + 1 2 (D d) (1 k)2 (cid:90) (cid:101)Dα where for uniform sampling of α on the interval [0, 1] (1 k)2 (cid:16)(cid:82) (cid:82) (cid:17)2 (cid:101)Dα(1 α) (cid:101)Dα(1 α)2 dα κ2 (cid:90) (cid:101)Dα = (cid:90) 1 0 16 (51a) (52) Revisiting Diffusion Model Predictions Through Dimensionality If we use the original MSE loss of u, we have κ = 1, and (cid:90) (cid:101)Dα = (cid:90) (cid:101)Dαα = (cid:90) (cid:90) (cid:101)Dα(1 α) = (cid:90) (cid:101)Dαα2 = (cid:101)Dα(1 α)2 = (cid:90) 1 0 (cid:90) 1 0 (cid:90) 1 0 (cid:90) 0 (cid:90) 1 0 dα = 1 dαα = 1 2 dα(1 α) = 1 2 dαα2 = 1 3 dα(1 α)2 = 1 3 so we have = 1 2 (cid:32) (cid:0)k2 + (1 k)2(cid:1) (cid:0) 1 2 2 (1 k)(cid:1)2 (cid:33) 2 3 (cid:32) (D d) (1 k)2 + 1 2 (1 k)2 (cid:0) 1 2 (cid:1)2 (cid:33) 1 3 (cid:18) 1 2 k2 1 2 + (cid:19) 5 8 + 1 8 (D d)(1 k)2 (cid:18) (cid:19) 5 4 k2 + 1 8 (cid:2)2(D + d)k2 4Dk + (2D + 3d)(cid:3) (D d)(1 k)2 + = = = 1 2 1 4 1 16 which is minimized by This is the condition given in Eq. 14. E. Ablation Study = + (53a) (53b) (53c) (53d) (53e) (54a) (54b) (54c) (54d) (55) In this section, we provide ablation study to justify our design choices regarding the parameterization of and the impact of the time-/α-dependence of k. Number of independent parameters. We first investigate whether decoupling the coefficients for and ε improves performance by introducing more degrees of freedom. In this setup, the target is defined as k0x k1ε, where k0 and k1 are two independent trainable parameters both in the range of [0, 1]. As shown in Table 3, this two-parameter approach achieves an FID of 2.04, which is nearly identical to the 2.05 achieved by our single-parameter k-Diff (kx (1 k)ε). This suggests that the relationship between the data and noise components is captured sufficiently by single interpolation constant, and the additional complexity of second independent parameter yields no significant generative gain. Constant vs. Time-Dependent k. We also examine whether should remain constant across the diffusion process or vary with respect to the timestep (or equivalently, α). We implement time-dependent version of k-Diff using 128 bins, where k(t) is determined via linear interpolation across the range of t. This configuration resulted in degraded FID of 2.17. We hypothesize that constant provides more stable optimization target, whereas time-dependent forces the model to adapt to shifting prediction objective at different noise levels. This lack of target consistency may increase the difficulty of the learning task and reduce the models robustness to estimation errors, whereas fixed-target objectives (such as x, ε, or vin flow matching) benefit from consistent output distribution across all timesteps. In Fig. 5a, we plot the trained parameters φ = k(t) and ψ = 1 k(t) for the time-dependent configuration, observing that the converged values deviate significantly from their initial state of 0.5. Excluding the boundary regions near = 0 and = 1, we find that k(t) monotonically decreases as increases. Specifically, k(t) originates at 0.56 for 0 and exhibits gradual decline in the small-t regime, followed by accelerated decreasing toward value near 0.3 as approaches 1. 17 Revisiting Diffusion Model Predictions Through Dimensionality"
        },
        {
            "title": "Target Type\nConstant\nConstant",
            "content": "# of 1 (k-Diff) 2 (k0, k1) 1 (k-Diff) Time-dependent Learn value(s) are from Epoch 64. # of Time Bins Learned Value(s) FID 2.05 2.04 2.17 0.522 (0.556, 0.509) See Fig. 5a 1 1 128 Table 3. Ablation study on the parameterization and time-dependency of k. All models were trained for 64 epochs using the LightningDiT-XL/1 architecture on ImageNet 256256 in latent space. We compare the performance of single vs. dual independent values and the effect of using constant versus time-dependent k(t) interpolated across 128 bins. (a) (b) Figure 5. (a) The learned time-dependent k(t) for 128 time bins. (b) Probability distribution of the logit normal distribution with µ = 0.0, σ = 1.0. Crucially, k(t) remains essentially stagnant at its initial value of 0.5 in the immediate vicinity of the boundaries (t {0, 1}). We attribute this lack of optimization at the end-points to the density of the time-sampling distribution adopted during training. As illustrated by the probability density function of the logit-normal distribution (µ = 0, σ = 1) in Fig. 5b, the sampling density vanishes at the boundaries. Consequently, the gradient signal at these points is insufficient to drive the parameters away from their initialization. F. Implementation Details For latent diffusion, we adopt the training configuration of LightningDiT (Yao et al., 2025), with the exception of the sampling procedure. We utilize the Heun solver with 50 sampling steps (NFE=99) and Classifier-Free Guidance (CFG) (Ho & Salimans, 2021) scale of 1.5. Notably, we observe that our method, when applied to the original flow matching setting of LightningDiT, achieves comparable or superior performance for 64 training epochs. For pixel-space diffusion, we strictly follow the experimental protocol from JiT (Li & He, 2025) for both training and inference, employing the Heun solver with 50 steps and CFG scale of 2.9. comprehensive summary of all architectural and sampling hyperparameters is provided in Table 4. 18 Revisiting Diffusion Model Predictions Through Dimensionality architecture tokenizer depth hidden dim heads image size patch size bottleneck dropout in-context class tokens in-context start block training loss epochs warmup epochs (Goyal et al., 2017) optimizer batch size learning rate learning rate schedule weight decay ema decay time sampler noise scale clip of (1 t) in division class token drop (for CFG) sampling ODE solver ODE steps time steps CFG scale (Ho & Salimans, 2021) CFG interval (Kynkaanniemi et al., 2024) Latent-space Diffusion LightningDiT-XL Pixel-space Diffusion JiT-B VA-VAE (Yao et al., 2025) 28 1152 16 256 1 - 0 - - - 12 768 12 256 (other settings: 512) image size / 16 128 0 32 v-loss 64 (ablation), 800 0 200 (ablation), 600 5 Adam (Kingma, 2014), β1, β2 = 0.9, 0.95 1024 2e-4 constant 0 {0.9996, 0.9998, 0.9999} logit(t)N (µ, σ2) µ = 0.0, σ = 1.0 1.0 0.05 for x-prediction and 0 for others µ = 0.8, σ = 0.8 1.0 image size / 0.05 0.1 Heun (Heun, 1900) 50 linear in [0.0, 1.0] 1.5 2.9 [0.1, 1] Table 4. Configurations of experiments."
        }
    ],
    "affiliations": []
}