{
    "paper_title": "QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models",
    "authors": [
        "Hyesung Jeon",
        "Seojune Lee",
        "Beomseok Kang",
        "Yulhwa Kim",
        "Jae-Joon Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The demand for efficient deployment of large language models (LLMs) has driven interest in quantization, which reduces inference cost, and parameter-efficient fine-tuning (PEFT), which lowers training overhead. This motivated the development of quantization-aware PEFT to produce accurate yet efficient quantized models. In this setting, reducing quantization error prior to fine-tuning is crucial for achieving high model accuracy. However, existing methods that rely on low-rank adaptation suffer from limited representational capacity. Recent Fourier-related transform (FT)-based adapters offer greater representational power than low-rank adapters, but their direct integration into quantized models often results in ineffective error reduction and increased computational overhead. To overcome these limitations, we propose QWHA, a method that integrates FT-based adapters into quantized models by employing the Walsh-Hadamard Transform (WHT) as the transform kernel, together with a novel adapter initialization scheme incorporating adaptive parameter selection and value refinement. We demonstrate that QWHA effectively mitigates quantization errors while facilitating fine-tuning, and that its design substantially reduces computational cost. Experimental results show that QWHA consistently outperforms baselines in low-bit quantization accuracy and achieves significant training speedups over existing FT-based adapters. The code is available at https://github.com/vantaa89/qwha."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 8 2 4 7 1 . 9 0 5 2 : r QWHA: QUANTIZATION-AWARE WALSH-HADAMARD ADAPTATION FOR PARAMETER-EFFICIENT FINETUNING ON LARGE LANGUAGE MODELS Hyesung Jeon1 Seojune Lee1 Beomseok Kang1 Yulhwa Kim2 Jae-Joon Kim1 1Seoul National University, 2Sungkyunkwan University {hjeon2k, leeseojune, beomseok, kimjaejoon}@snu.ac.kr yulhwakim@skku.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "The demand for efficient deployment of large language models (LLMs) has driven interest in quantization, which reduces inference cost, and parameter-efficient fine-tuning (PEFT), which lowers training overhead. This motivated the development of quantization-aware PEFT to produce accurate yet efficient quantized models. In this setting, reducing quantization error prior to fine-tuning is crucial for achieving high model accuracy. However, existing methods that rely on low-rank adaptation suffer from limited representational capacity. Recent Fourier-related transform (FT)-based adapters offer greater representational power than low-rank adapters, but their direct integration into quantized models often results in ineffective error reduction and increased computational overhead. To overcome these limitations, we propose QWHA, method that integrates FT-based adapters into quantized models by employing the Walsh-Hadamard Transform (WHT) as the transform kernel, together with novel adapter initialization scheme incorporating adaptive parameter selection and value refinement. We demonstrate that QWHA effectively mitigates quantization errors while facilitating fine-tuning, and that its design substantially reduces computational cost. Experimental results show that QWHA consistently outperforms baselines in low-bit quantization accuracy and achieves significant training speedups over existing FT-based adapters. The code is available at https://github.com/vantaa89/qwha."
        },
        {
            "title": "INTRODUCTION",
            "content": "Fine-tuning enables large language models (LLMs) to generalize beyond their pre-training, allowing adaptation to various domains (Wei et al., 2022; Liu et al., 2023; Qin et al., 2024; DeepSeek-AI et al., 2025). While full fine-tuning yields superior accuracy, it often incurs significant overhead due to the extensive computations required to update all the trainable model parameters (Loshchilov & Hutter, 2017; Zhu et al., 2025). Parameter-efficient fine-tuning (PEFT) addresses this issue by optimizing only small subset of the parameters while leaving most of them frozen (Li & Liang, 2021; Liu et al., 2022; Hu et al., 2022; Liu et al., 2024; Kopiczko et al., 2024). Beyond reducing training overhead, recent studies have shown that combining PEFT with model compression techniques can enhance inference efficiency at the same time (Dettmers et al., 2023). Among these techniques, quantization, which lowers the bit precision of model parameters, has gained particular attention due to its robustness against accuracy degradation under high compression ratios (Frantar et al., 2023; Lin et al., 2024; Dettmers et al., 2024; Kim et al., 2024b; Shao et al., 2024; Ashkboos et al., 2024; Zhang et al., 2024; Liu et al., 2025). Consequently, quantization-aware PEFT (QA-PEFT) has been widely explored as promising approach for efficient adaptation and inference in LLMs. Prior works on QA-PEFT typically relied on low-rank adaptation (LoRA) (Li et al., 2024; Guo et al., 2024; Kim et al., 2024a; Liao et al., 2024; Deng et al., 2025). In contrast, for standard PEFT, several alternatives to LoRA have recently been proposed to address the representational limitations of lowEqual contribution Corresponding authors 1 Figure 1: Overview of Quantization-aware Walsh-Hadamard Adaptation (QWHA). The weight update from QWHA is formulated as = 1, where is predefined Walsh-Hadamard transform (WHT) matrix and is trainable sparse coefficient matrix consisting of values and their indices E. The multiplication 1 indicates the expansion of learned coefficients (i.e., c), over the transform basis (i.e., columns of 1). Note that, the coefficients are the only trainable parameters, and remains constant. Our key contributions are in the adoption of WHT into the adapter (WHA) and their initialization, particularly (AdaAlloc) and (Refinement). Table 1: Comparison of adapter types and parameter selection strategies. Adapter types include low-rank adapters (LoRA), recent FT-based adapters (DCA and DHA), and our proposed adapter (WHA). Strategies to determine parameter location in include magnitude-based selection, random uniform selection, training via reparameterization, and our proposed method (AdaAlloc). Ability Factors LoRA DCA DHA WHA Magnitude Random Trainable AdaAlloc Adapter Type Parameter Selection Strategy Fine-tuning Quantization Error Reduction rank structures. In particular, Fourier-related transform (FT)-based adapters have emerged as strong alternatives. They train sparse set of coefficients to represent weight updates in the transform domain, offering superior representational capacity (Gao et al., 2024b; Du et al., 2025; Shen et al., 2025). However, our observations show that directly applying FT-based adapters to quantized models often yields worse performance than LoRA-based methods specifically designed for QA-PEFT. This highlights the importance of explicit consideration for quantization effects when fine-tuning quantized models. LoRA-based methods adopt quantization-aware initialization strategies that compensate for the errors between fulland low-precision weights using low-rank approximation with the adapters prior to fine-tuning. However, applying such initialization in FT-based adapters is nontrivial, as identifying the optimal sparse set of parameters and their values to approximate given matrix is an NP-hard problem (Natarajan, 1995). Moreover, the choice of transform type becomes an additional design consideration. This raises research question: how to effectively exploit FTbased adapters in QA-PEFT. To the best of our knowledge, neither FT-based adapters nor their initialization techniques have been explored in the context of QA-PEFT. In this paper, we present QWHA, novel QA-PEFT method that introduces FT-based adapter together with quantization-aware initialization scheme, as illustrated in Figure 1 and Table 1. We adopt WHT in our adapter design (WHA), inspired by its high-fidelity reconstruction ability in the spectral domain, to effectively compensate for quantization errors (Hedayat, 1978). In addition, the WHT kernel consists solely of 1 elements, enabling efficient computations using only additions and subtractions, thereby eliminating matrix multiplications (Dao-AILab, 2024). We further reduce computation by applying single transform in the adapter, unlike conventional FT-based adapters that apply two transforms. For quantization-aware adapter initialization, we develop tractable solution that first selects parameter locations and then assigns their values c. We introduce channel-wise parameter allocation scheme that guarantees lower bound on the number of parameters per channel to facilitate fine-tuning while allocating more parameters to channels with larger quantization errors, and then select the highest-magnitude coefficients within each channel to effectively reduce quantization error (AdaAlloc). Finally, we refine the selected parameter values, thereby enabling substantial reduction of quantization error (Refinement). We theoretically analyze the superior representation capacity of our proposed adapter and empirically validate the benefits of our adapter design and initialization method across diverse datasets and models."
        },
        {
            "title": "2.1 LLM QUANTIZATION",
            "content": "LLM quantization is key technique for improving inference efficiency by reducing the memory bottleneck caused by model weights through lowering their bit precision (Frantar et al., 2023), typically expressed by the following equation: (cid:18) W0 WQ = ( WQ + z) WQ = clamp z, 0, 2n 1 round (1) (cid:18) (cid:19) (cid:19) Here, W0 denotes the pre-trained weight matrix, while WQ and WQ represent the quantized integer weights and the corresponding dequantized weights, respectively. and are quantization scales and integer zero-points. Clamping is applied to the rounded and shifted value within the range 0 to 2n 1, where is the bit-width. LLMs generally contain outliers, small fraction of weights that are exceptionally large compared to the main distribution, and LLM quantization is highly sensitive to these outliers (Dettmers et al., 2024; Kim et al., 2024b; Tseng et al., 2024; An et al., 2025). These outliers induce corresponding outliers in the quantization error. Most quantization errors WQ = W0 WQ are bounded within small range (e.g., [ 2 )), since most weights within the clamping range are mapped to the nearest quantization level. In contrast, for outliers, the quantization error is defined as the difference between the original large weight and the clamping boundary values, resulting in extremely large errors that lead to significant accuracy degradation. Thus, reducing outlier-induced error is critical, and recent post-training quantization techniques for LLMs focus on mitigating these errors to preserve model accuracy (Dettmers et al., 2024; Kim et al., 2024b; Shao et al., 2024; Tseng et al., 2024; Zhang et al., 2024). Details on the distribution of quantization errors are presented in Appendix A. 2 , 2.2 QUANTIZATION-AWARE PEFT typical quantization-aware PEFT (QA-PEFT) adopts LoRA (Hu et al., 2022), which injects pair of low-rank matrices into linear layers to approximate the weight updates as follows: = (WQ + )X s.t. = BA (2) Here, Rrdin and Rdoutr are low-rank adapters, fine-tuned instead of frozen quantized weight WQ Rdoutdin , where Rdin(bs) is the activation matrix with batch size and sequence length s. Since there is no prior information about the weight updates before fine-tuning, LoRA typically initializes as random matrix and as zero matrix. In QA-PEFT, however, initializing the adapters to minimize quantization error prior to fine-tuning plays crucial role in accuracy. Early approaches addressed this by reconstructing quantization errors via singular value decomposition (SVD) to initialize low-rank adapters (Li et al., 2024; Guo et al., 2024). More recent works, such as RA-LoRA (Kim et al., 2024a) and CLoQ (Deng et al., 2025), adopt advanced decomposition strategies and improved calibration to further mitigate this limitation. However, existing QA-PEFT methods remain restricted to LoRA, and no prior studies have explored the use of other advanced adapters for QA-PEFT, which will be discussed in the next section. 2.3 FOURIER TRANSFORM-BASED ADAPTERS Sparse adapters have recently emerged as strong alternative to various low-rank adapters (Bhardwaj et al., 2024; Gao et al., 2024b; Shen et al., 2025; Du et al., 2025). SHiRA (Bhardwaj et al., 2024) proposes directly updating sparse subset of the weight matrix, enabling multi-adapter fusion. More recent methods adopt Fourier-related Transforms (FT) to represent the weight update in the spectral domain by applying transforms along both the rows and columns of the matrix as follows: = = = 1F 1 (3) Here, Rdindin and Rdoutdout are the orthonormal transform kernels. Prior works on these FT-based adapters have primarily focused on identifying suitable transform kernels. FourierFT (Gao et al., 2024b) employs the discrete Fourier transform (DFT), while LoCA (Du et al., 2025) replaces the DFT with the discrete cosine transform (DCT) to avoid discarding imaginary 3 components. SSH (Shen et al., 2025) instead leverages the discrete Hartley transform (DHT) for the same purpose. As these kernels are composed of sinusoidal functions, corresponds to the coefficients of the frequency components, which collectively represent . We denote DCT and DHT-based adapters as DCA and DHA throughout the paper. Since the transform kernels are fixed matrices, is the only learnable parameter during fine-tuning. To reduce the number of trainable parameters, is treated as sparse matrix. Specifically, = Scatter(c, E) is constructed from value vector Rp and an index list Np2, where Scatter assigns F(El,1,El,2) = cl for 0 1, with all other entries fixed to zero throughout training and inference. At the initialization stage, since there is no information on , previous works generally select the locations randomly and the values of the spectral coefficients are initialized to zero (Gao et al., 2024b; Du et al., 2025). SSH (Shen et al., 2025) proposes an advanced parameter selection strategy under the assumption that the frequency patterns of pre-trained and fine-tuned weights are similar. It first transforms the pre-trained weights and selects half of the positions with the largest spectral coefficients, while the remaining half are chosen randomly. Overall, previous works demonstrate that sparse adapters achieve superior accuracy improvements in full-precision fine-tuning compared to low-rank adapters. However, their advantages over lowIn adrank adapters have only been empirically demonstrated, without theoretical justification. dition, transforms within FT-based adapters incur heavy computational overhead (H and in Equation 3). Moreover, their application to QA-PEFT, particularly with initialization strategies that reconstruct quantization error, has not yet been explored."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "In this section, we present our proposed method, QWHA (Quantization-Aware Walsh-Hadamard Adaptation). First, we present the formulation of our proposed WHT-based adapter. Next, we analyze the key component that enables FT-based adapters to achieve greater representational capacity than low-rank adapters, and demonstrate why WHA, in particular, excels at mitigating quantization error during adapter initialization. Finally, we introduce parameter initialization strategy that reduces quantization error and enhances fine-tuning capability. Note that the experiments in this section use the 4-bit quantized LLaMA-3.2-3B model, with the total number of trainable parameters (r) = (cid:80) (dl,in + dl,out) fixed by setting = 64 across all adapters. llayers 3.1 QA-PEFT ADAPTER DESIGN WHT-based Adapter (WHA) We design our proposed adapter by constructing the weight update as the transformation of sparse matrix through an orthogonal transform 1. Specifically, we adopt the WHT (Hedayat, 1978; Kunz, 1979), particular instance of the FT whose kernel consists only of 1 entries, for the transform (details on WHT and other FT kernels are provided in Appendix B.1). Accordingly, our adapter is formulated as follows: = (WQ + )X s.t. = 1. (4) The advantages of our adapter design are discussed in the following paragraphs. Full-Rank Adapter. FT-based adapters exhibit greater representational capability than LoRA variants because they offer higher rank capacity given the same number of parameters. The representational power of low-rank adapters is strictly bounded by their inner dimension (Equation 2). In contrast, since the transform kernels in FT-based adapters are orthogonal and therefore full-rank, the rank of the adapter depends solely on the sparse matrix (Equation 3 and 4). Given that nonzero parameters are selected uniformly at random, if both rows and columns receive more than two parameters on average, then achieves full rank rmax = min(din, dout) with high probability (Coja-Oghlan et al., 2020). Since our adapter initialization in Section 3.2 assigns at least few elements to each channel and selects parameters independently per channel, the full-rank conditions are satisfied. Details of this condition are provided in Appendix B.2. Figure 2(a) presents the empirical analysis of the rank of adapter weights, normalized by the maximum achievable rank rmax and averaged across layers. While LoRA achieves less than 6.3% of the normalized rank, FT-based adapters are nearly full-rank. Hence, our proposed WHA exhibits high representational capacity. 4 Figure 2: (a) Comparison of rank in weight updates between low-rank and FT-based adapters across linear layers. (b) Cumulative distribution of ℓ2 norm of singular values and transform coefficients with Pareto hill index η for the quantization error WQ in the 14th-layer Value projection. The vertical blue line indicates point where the adapters have the same number of parameters. Figure 3: (a) Average coverage of outlier components within the selected parameters. (b) ℓ2 norm of the layer output error after initialization on the 14th-layer Key projection. The vertical blue lines indicate points where the adapters have the same number of parameters. Single transform. Conventional FT-based adapters apply transforms to both the input and output dimensions of the sparse matrix as denoted in Equation 3. However, we find no clear advantage of this approach over single transform in the context of quantization. Since quantization errors are defined group-wise within each output channel, the channels can be treated as independent, and multiple transforms do not improve the representational capacity (i.e., rank) of the adapter. Therefore, to avoid unnecessary operations, we design WHA to perform single transform as described in Equation 4. Benefits of WHT over other transforms. As discussed in Section 2.1, quantization errors exhibit heavy-tailed outliers. For QA-PEFT, where mitigating such errors is crucial, the adapter must capture the outlier structure with small number of parameters, as in the case of sparse adapters using the sparse matrix . We strategically adopt the WHT for our adapter design to effectively capture such outliers (Hedayat, 1978; Kunz, 1979). The WHT kernel consists only of 1 entries, and its basis functions are square-wave patterns with sharp transitions. In contrast, prior FT-based adapters adopt DCT or DHT, whose sinusoidal bases exhibit smooth transitions. This structural difference makes the WHT better aligned with abrupt changes such as outlier values. Therefore, WHT inherently provides more compact coefficient representation of quantization errors compared to DCT or DHT. We empirically demonstrate this by analyzing the cumulative energy in adapter parameters (Figure 2(b)), defined as the ℓ2 norm of coefficients from the transform of WQ in FT-based adapters, and the ℓ2 norm of singular values of WQ in low-rank adapters. Both coefficients and singular values follow Pareto-like distribution (see Appendix B.3), which can be characterized by the Pareto hill index η, where smaller η indicates sharper distribution (Arnold, 1983). Since the total cumulative energy equals WQ2 , the fastest convergence curve of WHT, with the smallest η, demonstrates that it concentrates the largest portion of energy within small number of coefficients, enabling accurate reconstruction with limited number of parameters. As result, WHA effectively compensates for quantization errors, particularly large-magnitude ones from salient weight channels, as shown empirically in Figure 3. For fair comparison, we use the same parameter initialization method described in Section 3.2. We define outlier coverage as the ratio of the ℓ1 sum of coefficients captured by the selected parameter locations to that of all coefficients corresponding to the top 10% magnitude outliers of WQ. 5 Algorithm 1 QWHA Initialization Algorithm Require: Weight quantization error WQ Rdoutdin , Activation Rdin(bs), WHT matrix Require: Budget p, Accumulated budget p, channel-wise budget (p0, . . . , pdout) Ndout Require: Parameter value vector Rp, index list Np2 Initialize p, c, 0 Set Σ1/2 ΣU := SVD(XX ) Set (p0, . . . , pdout ) AdaAlloc(p, WQ), 1R for = 0 to dout 1 do Set (WQ)i,:R Set p,..., p+pi1 TopKIndex Set B(i1,...,ipi ),: Set p,..., p+pi1 vB(BB)1 Accumulate p + pi (vB1) pi end for Update Rdoutdin c, 3.2 QUANTIZATION-AWARE ADAPTER INITIALIZATION Parameter budget allocation Channel-wise parameter selection Value refinement Objective Function. Our goal in initializing WHA is to minimize the layer output error (WQX) caused by weight quantization, using coefficient matrix with non-zero elements. Formally, the objective is given by: arg min c,E WQX 1X2 (5) where denotes Frobenius norm. Following the reduction procedure used in Frantar et al. (2023) and Deng et al. (2025), this reduces to: arg min c,E WQR 1R2 (6) Here, = Σ1/2 is the invertible square root of the Hessian matrix attained by SVD as XX = ΣU . detailed derivation on this reduction is provided in Appendix C.1. As we aim to find sparse (c, E) that minimizes Equation 6, it constitutes an NP-hard sparse approximation problem (SAP) (Natarajan, 1995). To make this problem more tractable, we decompose it into two subproblems: first, parameter selection to determine the locations of the nonzero elements to fine-tune (E); and second, value refinement to optimize the values of the selected positions (c). Parameter Selection with AdaAlloc. Given number of parameter (budget) for layer, naive selection method to reduce quantization error is to choose the largest-magnitude elements from the dense solution WQH of Equation 6. However, since large-magnitude coefficients are often clustered in few channels containing outliers, parameters become overly concentrated in small number of channels. As result, magnitude-based selection yields low-rank , degrading fine-tuning capability. Conventional methods prevent this rank reduction by incorporating random selection. For example, LoCA initializes parameter locations randomly and then optimizes these locations during fine-tuning. Thus, from the perspective of initialization, LoCA is equivalent to random selection at this stage. Additionally, SSH allocates half of the parameters randomly, while it selects the other half based on magnitude. However, these randomness-based approaches result in high layer output error because they fail to capture the parameters critical for reducing the error. To construct sparse that is high-rank and minimizes initialization error, we first allocate the parameter budget adaptively across output channels in proportion to their error magnitudes: (cid:36) pi (WQ)i,:t j=1(WQ)j,:t (cid:80)dout (cid:37) , (7) where is temperature hyperparameter controlling allocation sharpness. Because the parameter budget must be an integer, we apply the floor operation, which may leave fewer than dout parameters unassigned. These remainders are distributed to the output channels with the smallest allocations to ensure (cid:80)dout i=1 pi = p. Since all output channels receive parameter budgets proportional to their 6 Table 2: Layer output error (ℓ2 norm, scaled by 1 103) after initialization. None denotes the error before initialization. Method None Random SSH Magnitude AdaAlloc Query Key Value Out Gate Up Down Average 13.84 0.54 28.08 4.66 1.88 25.76 21.36 7.21 10.55 0.43 22.98 3.70 1.57 23.05 19.21 5.96 6.99 0.30 17.38 2.70 1.25 19.85 16.96 4.57 5.95 0.25 15.10 2.24 1.04 16.52 14.00 3. 5.11 0.27 14.92 2.01 1.13 17.97 15.25 3.86 Figure 4: Rank of adapter weights. errors, maintains full rank, while allocating more parameters to important channels with higher quantization error. Next, within the budget of each output channel, we select parameters based on magnitude to effectively reduce the error. We compare the rank and layer output error of previous selection methods and AdaAlloc, as shown in Figure 4 and Table 2. For fair comparison, all selection methods use the same value assignment method discussed in the next paragraph. AdaAlloc is the only parameter selection method that simultaneously achieves nearly full-rank and maintains low layer output error. Examples of the selected parameters are provided in Appendix C.2. Value Refinement. To assign each parameter value that effectively reduces layer output error, we solve the channel-wise SAP derived from Equation 6 for each ith output channel with given parameter budget pi: min xB 2, where = (WQ)i,:R, = 1R. (8) Here, is the ith row of , constrained to have pi non-zero elements. We first select the pi largestmagnitude entries from the channel-wise dense solution x0 = vB1 = (WQH)i,:. Next, rather than directly reusing the values from dense solution, we refine them to minimize layer output error. Specifically, we re-project onto the rows of corresponding to the selected indices, denoted as Rpidin , which serve as the most relevant basis vectors: = vB(BB)1. (9) This allows the selected basis vectors to account for the impact of unselected vectors, yielding more accurate approximation. Without this step, interactions among basis vectors are ignored, leading to suboptimal error reduction. Note that the refinement is applicable regardless of the parameter selection strategy. Figure 5 shows that refinement is crucial for reducing layer output error, presenting the averaged layer output error of model with parameters selected by AdaAlloc. Further details on the error analysis are provided in Appendix C.3. Finally, and for channel are initialized to the selected indices and their refined values x. Algorithm 1 summarizes the initialization process, with details provided in Appendix C.4."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Figure 5: Layer output error. We evaluate the effectiveness of QWHA in terms of model accuracy and training efficiency. We first compare QWHA with state-of-the-art QA-PEFT baseline and sparse high-rank adapters including FT-based adapters. Then, we provide detailed analysis of the impact of using WHA and AdaAlloc. Finally, we demonstrate the efficiency of QWHA regarding WHA. Models and Datasets. We evaluate QWHA on the Mistral-7B-v0.3 (Mistral AI, 2024) and LLaMA (Grattafiori et al., 2024) model families, including LLaMA-3.1-8B and LLaMA-3.2-3B. For instruction fine-tuning, we use the Stanford-Alpaca dataset (Taori et al., 2023)1 with 52k samples. 1https://huggingface.co/datasets/yahma/alpaca-cleaned 7 Table 3: Accuracy (%) evaluation results on CSQA and GSM8k benchmarks. QA Init. denotes the existence of quantization-aware initialization. Bits Method Adapter QA Coefficient LLaMA-3.1-8B LLaMA-3.2-3B Mistral-7B-v0.3 Type Init. Selection CSQA GSM8k CSQA GSM8k CSQA GSM8k 16 Pre-trained Fine-tuned - - 4 3 2 GPTQMagR CLoQ SHiRA LoCA SSH QWHA GPTQMagR CLoQ SHiRA LoCA SSH QWHA GPTQMagR CLoQ SHiRA LoCA SSH QWHA - LoRA Sparse DCA DHA WHA - LoRA Sparse DCA DHA WHA - LoRA Sparse DCA DHA WHA - - - - - - - - - Random LoCA SSH AdaAlloc - - Random LoCA SSH AdaAlloc - - Random LoCA SSH AdaAlloc 70.78 71. 69.11 69.58 71.07 71.45 70.75 71.50 67.76 68.71 69.68 70.21 69.86 70.50 41.00 56.49 51.84 56.71 56.06 60.98 6.22 59.74 2.58 53.83 54.36 54.36 53.98 56.10 2.65 53.75 45.49 53.15 50.34 55. 0.45 33.89 27.74 33.97 30.55 37.83 64.99 66.43 64.43 65.48 63.10 65.59 65.83 66.11 61.49 64.35 62.90 63.30 63.57 64.80 42.90 54.89 52.91 53.87 54.01 57.03 3.18 44. 3.34 39.27 40.71 40.33 39.80 41.47 2.43 39.20 35.33 36.69 38.13 39.58 0.08 26.53 22.59 23.88 25.77 29.11 70.49 71.87 69.54 71.32 70.88 71.55 71.57 71.70 67.57 69.91 69.36 69.64 69.65 70. 45.91 61.80 59.08 62.03 62.31 63.84 13.72 54.51 10.39 52.01 51.02 47.99 47.99 53.68 1.29 46.25 46.70 46.10 47.15 47.84 0.00 33.36 33.57 33.89 32.06 35.33 We evaluate on zero-shot commonsense question answering (CSQA)(Gao et al., 2024a), covering seven multiple-choice benchmarks(Clark et al., 2018; 2019; Zellers et al., 2019; Talmor et al., 2019; Bisk et al., 2020; Sakaguchi et al., 2021). For arithmetic reasoning, we adopt the GSM8k (Cobbe et al., 2021) dataset and evaluate zero-shot accuracy following Cobbe et al. (2021). Baselines. We include full fine-tuned model (Fine-tuned) and quantized model, which use GPTQ (Frantar et al., 2023) with MagR (Zhang et al., 2024) (GPTQMagR) as baselines. We note that our method is also compatible with any other quantization schemes. We also include CLoQ, recent QA-PEFT method that shares our goal of layer output error reduction during initialization for low-rank adapters. Other LoRA-based methods (Kim et al., 2024a; Liao et al., 2024) involving layer-wise calibration or layer-wise parameter allocation are orthogonal to our approach and can be integrated in future work. We evaluate sparse adapters, including SSH and LoCA (FT-based) and SHiRA (non FT-based). We note that LoCA further fine-tunes the randomly selected parameter indices via reparameterization with cost of additional training overhead. We also build advanced hybrid baselines that integrate transforms or parameter selection strategies from prior works into our schemes by applying DCA and DHA with our AdaAlloc, or applying various parameter selection strategies to our WHA. Implementation Details. Following prior work, adapters are applied to linear layers with parameter budget of (r = 64), and quantization is performed with group size of 64. Note that we apply constant scaling factor α to all adapters, while the equations in the preceding sections omitted it by α = 1 for simplicity. We set the AdaAlloc temperature to = 1, which suffices to meet the full-rank condition. We use WikiText-2 (Merity et al., 2016) as calibration dataset for adapter initialization, following Deng et al. (2025), to ensure generality. All experiments are conducted on NVIDIA A100 80GB GPUs, and training hyperparameters are provided in Appendix D.1. 4.1 FINE-TUNED MODEL ACCURACY Main evaluation. Table 3 shows that QWHA outperforms both low-rank adapters with quantization-aware initialization and conventional sparse adapters. In particular, the effectiveness of QWHA is evident in the 2-bit setting, where it achieves scores at least 2-3% higher than the baselines. Without quantization-aware initialization, sparse adapters, including FT-based adapters, perform worse than low-rank adapters in several cases. This underscores the need for quantization8 Table 4: Accuracy (%) evaluation results on CSQA and GSM8k benchmarks with variants of adapter types and parameter selection strategies in LLaMA-3.2-3B. QA Init. denotes the existence of quantization-aware initialization, and Refine. denotes the value refinement during initialization. Adapter QA Coefficient Refine. 4-bit 3-bit 2-bit Type WHA WHA WHA WHA WHA WHA DCA DHA Sparse Init. Selection Random Random Magnitude LoCA SSH AdaAlloc AdaAlloc AdaAlloc AdaAlloc CSQA GSM8k CSQA GSM8k CSQA GSM8k 66.00 65.91 66.07 65.75 65.96 66.11 65.54 65.92 65.60 40.94 40.71 41.01 40.94 40.78 41.47 39.72 40.84 40.94 63.53 63.91 64.52 63.73 62.92 64.80 64.77 64.35 63.43 37.60 37.30 36.69 36.92 36.92 39.58 37.30 38.89 37.53 54.03 54.48 56.49 53.93 54.20 57.03 55.95 56.05 55.97 24.41 24.48 28.12 21.15 27.14 29.11 27.29 27.52 26. Table 5: Training time (hours) on Alpaca dataset. Batch Size 1 2 4 8 16 CLoQ SHiRA QWHA SSH LoCA 12.5 7.1 5.0 4.1 3.6 15.5 8.2 5.5 4.3 3.7 18.2 9.7 6.0 4.6 3.9 63.3 45.8 26.1 13.3 8.3 92.3 53.4 30.1 16.5 9.8 Figure 6: Accuracy of CLoQ and QWHA. aware initialization, especially in sub-4-bit settings where fine-tuning alone cannot fully restore performance. We note that task-specific results of the CSQA benchmark are presented in Appendix D.2. Effect of WHA and AdaAlloc. We further examine the effectiveness of WHA and AdaAlloc, with QWHA consistently outperforming both low-rank adapters and advanced variants of sparse adapters. Figure 6 for 4-bit quantized LLaMA-3.2-3B shows that increasing the number of parameters in CLoQ cannot close the accuracy gap with QWHA, as QWHA with (r > 32) already surpasses CLoQs maximum achievable score. This highlights the advantage of WHA, which provides superior representational capacity than low-rank adapters. Table 4 further demonstrates that WHA and AdaAlloc achieve the best results in each respective category of adapter type and parameter selection method. We note that LoCAs post-hoc location selection undermines the effectiveness of quantization-aware initialization based on the initially chosen parameters, unlike in PEFT. Additional ablations on quantization group size are provided in Appendix D.3. 4.2 COMPUTATIONAL EFFICIENCY Training time for QWHA on the Alpaca dataset with LLaMA-3.1-8B is reported in Table 5, where QWHA achieves substantial speedup over previous FT-based adapters by leveraging WHA. WHA employs single transform instead of the double transform used in conventional FT-based adapters, while achieving higher accuracy. Moreover, the fast recursive WHT kernel replaces matrix multiplications with smaller number of additions and subtractions. In contrast, the recursive kernels of DCT and DHT, which require the DFT, are slower than direct matrix multiplication due to duplicated computations for imaginary parts. As result, WHT achieves similar training time to low-rank adapters or to simple sparse adapter. In contrast, LoCA incurs additional latency even compared to SSH, due to the training of location parameters. Memory usage remains almost identical across all adapters with the same number of parameters. Detailed results on the training time of each transform kernel and the memory usage of each adapter are provided in Appendix D.4 and Appendix D.5."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we introduce QWHA, novel QA-PEFT framework featuring Walsh-Hadamard transform-based adapter and its quantization-aware parameter initialization scheme. WHA offers strong fine-tuning capability and excels in quantization error reduction. The proposed AdaAlloc scheme facilitates both fine-tuning and quantization error reduction during parameter selection, while parameter refinement enables substantial quantization error reduction. We validate QWHA across diverse models and datasets, where it consistently outperforms existing baselines in accuracy and demonstrates its effectiveness. We also show that using WHA with single transform provides computational benefits, enabling more efficient training than conventional FT-based adapters."
        },
        {
            "title": "REFERENCES",
            "content": "Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang. Systematic outliers in large language models, 2025. Barry C. Arnold. Pareto Distributions. International Co-operative Publishing House, 1983. ISBN 9780429169410. doi: https://doi.org/10.1201/b18141. Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian Croci, Bo Li, Pashmina Cameron, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. Proceedings of the 37th International Conference on Neural Information Processing Systems (NeurIPS), 37:100213100240, 2024. Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Viswanath Ganapathy, Shreya Kadambi, Rafael Esteves, Shubhankar Borse, Paul Whatmough, Risheek Garrepalli, Mart Van Baalen, Harris Teague, and Markus Nagel. Sparse high rank adapters. In Proceedings of the 38th International Conference on Neural Information Processing Systems, NeurIPS 24, 2024. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168. Amin Coja-Oghlan, Alperen A. Ergur, Pu Gao, Samuel Hetterich, and Maurice Rolvien. The rank of sparse random matrices. The Proceedings of the 31th ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 579591, 2020. Dao-AILab. fast-hadamard-transform. https://github.com/Dao-AILab/ fast-hadamard-transform, 2024. Accessed: 2025-05-17. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng 11 Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Yanxia Deng, Aozhong Zhang, Naigang Wang, Selcuk Gurses, Zi Yang, and Penghang Yin. Cloq: Enhancing fine-tuning of quantized llms via calibrated lora initialization. Transactions on Machine Learning Research (TMLR), 2025. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Proceedings of the 36th International Conference on Neural Information Processing Systems (NeurIPS), 36:1008810115, 2023. Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: sparse-quantized representation for near-lossless llm weight compression, 2024. Zhekai Du, Yinjie Min, Jingjing Li, Ke Lu, Changliang Zou, Liuhua Peng, Tingjin Chu, and Mingming Gong. Loca: Location-aware cosine adaptation for parameter-efficient fine-tuning. 13th International Conference on Learning Representations (ICLR), 2025. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan-Adrian Alistarh. Gptq: Accurate posttraining quantization for generative pre-trained transformers. In 11th International Conference on Learning Representations (ICLR), 2023. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024a. URL https://zenodo.org/records/12608602. Ziqi Gao, Qichao Wang, Aochuan Chen, Zijing Liu, Bingzhe Wu, Liang Chen, and Jia Li. Parameter-efficient fine-tuning with discrete fourier transform. Proceedings of the 41st International Conference on Machine Learning (ICML), 2024b. Diakoumis P. Gerakoulis and Saeed S. Ghassemzadeh. System and method for generating orthogonal codes, Mar 2004. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Han Guo, Philip Greengard, Eric P. Xing, and Yoon Kim. Lq-lora: Low-rank plus quantized matrix decomposition for efficient language model finetuning, 2024. A. Hedayat. Hadamard matrices and their applications. The Annals of Statistics, 6, 11 1978. doi: 10.1214/aos/1176344370. Ashok S. Hedayat, Neil J. A. Sloane, and John Stufken. Orthogonal Arrays: Theory and Applications. Springer Series in Statistics. Springer, 1999. ISBN 978-0-387-98766-8. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. 10th International Conference on Learning Representations (ICLR), 2022. Minsoo Kim, Sihwa Lee, Wonyong Sung, and Jungwook Choi. Ra-lora: Rank-adaptive parameterefficient fine-tuning for accurate 2-bit quantized large language models. In Findings of the Association for Computational Linguistics 2024 (ACL), pp. 1577315786, 2024a. 12 Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization, 2024b. Dawid J. Kopiczko, Tijmen Blankevoort, and Yuki M. Asano. Vera: Vector-based random matrix adaptation, 2024. Henry O. Kunz. On the equivalence between one-dimensional discrete walsh-hadamard and multidimensional discrete fourier transforms. IEEE Transactions on Computers, C-28(3):267268, 1979. doi: 10.1109/TC.1979.1675334. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021. Yixiao Li, Yifan Yu, Chen Liang, Nikos Karampatziakis, Pengcheng He, Weizhu Chen, and Tuo Zhao. Loftq: Lora-fine-tuning-aware quantization for large language models. In 12th International Conference on Learning Representations (ICLR), 2024. Baohao Liao, Christian Herold, Shahram Khadivi, and Christof Monz. Apiq: Finetuning of 2-bit quantized large language model. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2099621020, 2024. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration, 2024. Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A. Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In The 35th Annual Conference on Neural Information Processing Systems (NeurIPS), 2022. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, KwangTing Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation, 2024. Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. Spinquant: Llm quantization with learned rotations, 2025. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. CoRR, abs/1609.07843, 2016. URL http://arxiv.org/abs/1609.07843. Mistral AI. Mistral 7b v0.3. https://huggingface.co/mistralai/Mistral-7B-v0. 3, 2024. Model card, Apache 2.0 license, released 2024/11/30. B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing, 24(2):227234, 1995. doi: 10.1137/S0097539792240406. URL https://doi.org/10. 1137/S0097539792240406. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2024. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Jennifer R. Seberry and Mieko Yamada. Hadamard matrices, sequences, and block designs. In Jeffrey H. Dinitz and Douglas R. Stinson (eds.), Contemporary Design Theory: Collection of Surveys, pp. 431560. Wiley, 1992. 13 Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models, 2024. Yixian Shen, Qi Bi, Jia-Hong Huang, Hongyi Zhu, Andy Pimentel, and Anuj Pathania. Ssh: Sparse spectrum adaptation via discrete hartley transformation. Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL), 2025. N. J. A. Sloane. library of hadamard matrices. http://neilsloane.com/hadamard/, 2004. Accessed: 2025-05-16. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. In North American Chapter of the Association for Computational Linguistics (NAACL), 2019. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks, 2024. Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2022. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Aozhong Zhang, Naigang Wang, Yanxia Deng, Xin Li, Zi Yang, and Penghang Yin. Magr: Weight magnitude reduction for enhancing post-training quantization, 2024. Hanqing Zhu, Zhenyu Zhang, Wenyan Cong, Xi Liu, Sem Park, Vikas Chandra, Bo Long, David Z. Pan, Zhangyang Wang, and Jinwon Lee. Apollo: Sgd-like memory, adamw-level performance, 2025."
        },
        {
            "title": "A QUANTIZATION ERROR DISTRIBUTION",
            "content": "We present the distribution of quantization errors and their relationship to outliers in the pre-trained weights, as discussed in Section 2.1, in Figure 7. Figure 7(a) shows the overall error distribution, while Figure 7(b) highlights the channel-wise similarity between quantization errors and pre-trained weights in the 14th layer of LLaMA-3.2-3B. During quantization, values are divided by the quantization scale, typically defined per group within each output channel, and then rounded to an integer and clamped within range determined by the bit-width. Most quantization errors remain within this rounding range, but large-magnitude outliers are often clamped, leading to large errors. Because model accuracy is highly sensitive to outlier weights, their quantization errors can significantly degrade performance. In QA-PEFT, it is therefore crucial to mitigate such outlier-induced errors during initialization by adapting the weights, particularly for large-magnitude values originating from salient outliers. Figure 7: (a) Weight quantization error distribution and (b) its channel-wise similarity to the pretrained weights in 14th layer Key projection of 4-bit quantized LLaMA-3.2-3B. In Figure (b), each pixel represents the ℓ2 norm of weight quantization errors (left) and that of pre-trained weights (right) for each output channel ordered by channel index from top-left to bottom-right. 15 WHT-BASED ADAPTER (WHA) B.1 FT-BASED ADAPTER KERNELS We describe class of Fourier-related transform (FT) kernels employed in our adapters and prior studies in this section (Gao et al., 2024b; Du et al., 2025; Shen et al., 2025). Walsh-Hadamard Transform (WHT). The Walsh-Hadamard Transform (WHT) matrix introduced in Equation 4 is constructed following conventions in prior works (Tseng et al., 2024; Ashkboos et al., 2024). For dimension = 2n, WHT matrix RN is defined recursively as the Kronecker product of smaller matrices: H2 = (cid:21) (cid:20)1 1 1 1 1 2 , HN = H2 H2n1 , (10) where denotes the Kronecker product. For non-power-of-two dimensions, Hadamard matrices exist for certain values (Seberry & Yamada, 1992; Hedayat et al., 1999; Gerakoulis & Ghassemzadeh, 2004), which can be retrieved from Sloane (2004). More generally, for = 2n m, where Hm is known Hadamard matrix, the transform is defined as: The rows of HN form an orthogonal basis, known as Walsh-Hadamard bases, satisfying: HN = H2n Hm. HN = HN = IN . (11) (12) The matrix H2n can be computed in O(n log n) time (Kunz, 1979). In practice, HN can be precomputed once and cached for reuse across layers of the same size, incurring negligible cost in both computation and memory. To further accelerate computation, we employ the Fast Hadamard multiplication kernel from Dao-AILab (2024), which avoids explicit matrix construction by using fused kernel of only additions and subtractions. Discrete Fourier Transform (DFT). FourierFT (Gao et al., 2024b) was the first study of FTbased adapters and used the discrete Fourier transform (DFT). The transform kernel CN is defined as: Hjk = 1 ei 2πjk = 1 (cid:26) cos (cid:19) (cid:18) 2πjk sin (cid:19)(cid:27) (cid:18) 2πjk , 0 j, < N. (13) Although effective, later works adopted real-valued FT variants to avoid the complex-domain nature of the DFT, since deep learning frameworks typically discard the imaginary components and compute only with the real values. Discrete Hartley Transform (DHT). SSH (Shen et al., 2025) employs the discrete Hartley transform (DHT), real-valued variant of the FT with kernel: Hjk = ℜ(cid:0) 1 ei 2πjk (cid:1) ℑ(cid:0) 1 ei 2πjk (cid:1) = 1 cas (cid:19) (cid:18) 2πjk , 0 j, < N, (14) where cas(x) = cos + sin x. Discrete Cosine Transform (DCT). LoCA (Du et al., 2025) employs another real-valued FT, the discrete cosine transform (DCT), whose kernel is: Hjk = 1 (cid:113) 2 cos (cid:16) π(2k+1)j 2N (cid:17) = 0 0 < < , 0 < N. (15) 16 B.2 RANK OF WHA This section provides detailed explanation of the full-rank property of WHA and its conditions, as discussed in Section 3.1 and illustrated in Figure 2(a). To preserve the expressiveness of finetuned model under limited parameter budget, it is critical to ensure high rank capacity in the weight update. Unlike low-rank adapters, which inherently restrict the parameter subspace, WHA is sparsely structured yet can retain high representational capacity by maintaining full rank. This also holds in typical sparse adapters, including FT-based adapters. We build on theoretical insights from prior work on sparse random matrices Coja-Oghlan et al. (2020), which provides conditions under which such matrices are full rank. Specifically, consider random sparse matrix Rdoutdin , where each input and output channel has and non-zero entries on average. Then, is full rank when k, 2 as din, dout , and thus full rank with high probability. Following the notations in Coja-Oghlan et al. (2020), we derive the corresponding condition for our setting to guarantee full-rank behavior in WHA. Condition Function. We define the probability generating functions for the distributions of random non-zero entries per column and per channel. Given that these distributions are degenerate, the generating functions and their derivatives are: D(z) = zk, D(z) = kzk1, D(1) = k, K(z) = zl, (z) = lzl1, (1) = l, Then, the condition function Φ(z) that determines the full rank condition is given by: (cid:18) Φ(z) = 1 (cid:19) (z) [1 K(z) (1 z)K (z)] . To ensure the full rank of the matrix A, the inequality must hold as: Φ(z) < Φ(0), 0 < 1, (16) (17) (18) (19) Substituting the explicit forms for D(z), K(z), D(z), (z) into Equation B.2 yields the right hand side as: Φ(z) = (1 zl1)k + kzl1 k(l 1) zl. As Φ(z = 0) = 1 , the condition in Equation B.2 finally simplifies to: k(l 1) (1 zl1)k + kzl1 zl 1 < 0, 0 < 1. (20) (21) Practical Considerations. The inequality in Equation B.2 shows that the condition generally holds for integers k, 2. For the total number of parameters = r(din + dout) with 2, we have = p/din > and = p/dout > under random selection, thus satisfying the full-rank condition when din, dout are sufficiently large. Importantly, since AdaAlloc selects indices independently within each channel, the parameters across input rows can be treated as uniformly random selections. Moreover, AdaAllocs per-channel allocation with remainder assignment and temperature control guarantees at least two elements in every channel (i.e., 2). Empirically, parameter budgets corresponding to (r 4) ensure at least two elements per row (i.e., 2), even for linear layers with large output dimensions, which might otherwise receive few parameters per input row. Hence, the full-rank conditions hold, and the matrix in QWHA is nearly full rank. 17 B.3 ENERGY CONCENTRATION OF WHT In this section, we quantify the energy concentration property of WHT discussed in Section 3.1, using Figure 2(b) and Figure 3(a). Distribution of Singular Values and Coefficients. Figure 8 presents the distributions of singular values from SVD and of transform coefficients sorted by their squared magnitudes. Here, the area under each plot is equal to WQ2 (details in the following paragraph). The distributions follow Pareto-like behavior, where sharpness can be quantified using the hill index η. The Pareto hill index is value which implies the heaviness of tail. In fact, this is the reciprocal of the Pareto tail index, defined as the mean of the log ratios of consecutive order statistics from the top-k largest magnitudes. smaller hill index η implies faster convergence of the cumulative distribution (Arnold, 1983). Hence, WHT exhibits the sharpest distribution, making it feasible to retain more information with fewer parameters (r) when implemented in sparse adapter, as shown in Figure 2. Figure 8: Singular value and coefficient magnitude distributions with the Pareto hill index η in the 14th-layer Key projection of LLaMA-3.2-3B. The vertical line indicates the point where the adapters have the same number of parameters. Energy of Singular Values and Coefficients. Throughout this work, we use the term energy to denote the squared ℓ2 norm of the singular values in decomposition or the spectral coefficients in transform. We show that the total energy of both SVD and orthonormal transforms reduces to the Frobenius norm of the transformed matrix. (For example, in Figure 8, the area under each curve corresponds to WQ2 .) Proposition 1. Let Rmn be matrix, and let its singular value decomposition (SVD) be = ΣV , where Rmm and Rnn are orthonormal matrices, and Σ Rmn is diagonal matrix with entries σi on the diagonal for = 1, . . . , min(m, n). Define = for an orthonormal matrix Rnn. Then, we have: 2 = min(m,n) (cid:88) i=1 = 2 σ2 . Proof. (i) Identity σ2 . Given = ΣV , due to the orthonormality of , reduces to: = (cid:80)min(m,n) i=1 = (U ΣV )(U ΣV ) = ΣU ΣV = ΣΣV . Therefore, by the cyclic property of trace and the orthonormality of , 2 reduces to: = tr(M ) = tr(V ΣΣV ) = tr (cid:0)ΣΣ(V )(cid:1) = tr(ΣΣ) = Σ2 . 2 Since ΣΣ is diagonal with entries σ2 for = 1, . . . , min(m, n): 2 = Σ2 = min(m,n) (cid:88) σ2 . i=1 18 (22) (23) (24) (ii) Identity 2 = 2 . Given = H, due to the orthonormality of H: = (M H)(M H) = M H. (25) By the cyclic property of trace, 2 reduces to: 2 = tr (cid:0)(M )(H H)(cid:1) = tr(M ) = 2 . We note that this equivalence also applies to the coefficients defined with = H, with Rmm, such that 2 = 2 . = 2 (26) Outlier Reconstruction Ability of WHT. Due to its energy concentration ability discussed above, WHT can reconstruct quantization errors with large outliers during initialization, which is critical for final model performance. Table 6 presents the numerical values corresponding to Figure 3(a), showing the proportion of outlier coefficients captured by each adapter. Table 6: Percentage of outlier coefficients captured by each adapter under parameter budget of (r = 64) in the 14th-layer Key projection of LLaMA-3.2-3B. Higher is better. Adapter Type Query Key DCA DHA WHA 6.62 18.82 20. 12.50 32.29 33.60 Value 11.68 21.98 23.30 Out Gate Up Down Average 6.31 13.19 14.00 4.62 14.14 15.20 4.34 8.00 8.72 4.53 11.01 11.53 7.23 17.06 18. 19 QUANTIZATION-AWARE INITIALIZATION OF WHA C.1 OBJECTIVE FUNCTION We reduce the original optimization objective in Equation 5 to the form in Equation 6, following the approach of Frantar et al. (2023); Deng et al. (2025). Using the notations from Section 3.2, the reduction proceeds as follows: WQX 1X2 =(WQ 1)X2 = tr(cid:0)(WQ 1)XX (WQ 1)(cid:1) = tr(cid:0)(WQ 1)RR(WQ 1)(cid:1) =(WQ 1)R2 =WQR 1R2 , (27) (28) (29) (30) (31) (32) where = Σ1/2 Rdindin is an invertible square root of the Hessian Gram matrix XX , such that XX = ΣU . Following Deng et al. (2025), we add small regularization term λ = 0.0001 tr(XX )/din to the diagonal if is not originally invertible. This reduction allows us to replace with R, enabling efficient and effective calibration using multiple input data points. Rather than solving the optimization problem separately for each sample X, we can accumulate the contribution of activations via and solve single reduced problem. C.2 PARAMETER SELECTION STRATEGIES We present the parameter selection patterns of each method discussed in Section 3.2. As shown in Figure 9, magnitude-based selection allocates parameters to limited number of channels, while conventional methods such as SSH and LoCA incorporate random selection to avoid rank reduction. However, these approaches fail to reduce quantization error during initialization because the selected parameters are not optimal for error reconstruction. In contrast, AdaAlloc identifies the most important locations within each channel while preventing rank reduction through per-channel budgets, thereby providing the most effective initialization and fine-tuning. Figure 9: Parameter selection patterns and two example zoomed-in results of each method in the 14th-layer Query projection of LLaMA-3.2-3B. 20 C.3 VALUE REFINEMENT We present the layer output error after WHA initialization with and without value refinement in Table 7, as discussed in Figure 5 in Section 3.2. Without refinement of the selected coefficients in the initial dense solution matrix WQH, correlations among the columns are ignored, and the impact of sparsifying other columns cannot be considered, leading to suboptimal error reconstruction. Table 7: Layer output error (ℓ2 norm, scaled by 103) after initialization with and without value refinement in 4-bit quantized LLaMA-3.2-3B. Parameters are selected by AdaAlloc. None denotes the error before initialization. Method Query Key Value None W/o Refinement Refined 13.84 11.39 5.11 0.54 0.62 0. 28.08 26.21 14.92 Out 4.66 4.39 2.01 Gate 1.88 2.11 1.13 Up Down Average 25.76 24.33 17.97 21.36 20.97 15.25 7.21 7.06 3.86 C.4 CHANNEL-WISE PARAMETER SELECTION AND INITIALIZATION We provide detailed description of the formulation and solution of the sparse approximation problem underlying Algorithm 1, based on the notations in Section 3.2. Sparse Approximation Problem. With channel-wise breakdown of the objective in Equation 6, the goal is to initialize the i-th channel of the parameter matrix , denoted Fi,:, given the perchannel parameter budget pi. The objective is for Fi,:H 1R to closely approximate the projected quantization error (WQ)i,:R in the ℓ2 sense. As we constrain Fi,: to have exactly pi non-zero elements, the term Fi,:H 1R becomes sparse linear combination of standard basis vectors: Fi,:H 1R = pi (cid:88) k=0 Fi,jk e(jk)H 1R, (33) where e(jk) is the jk-th standard basis vector. Since e(jk)H 1R corresponds to the jk-th channel of 1R, the problem reduces to selecting pi rows from 1R that best approximate (WQ)i,:R. Greedy Algorithm for Sparse Approximation. The problem generalizes to standard sparse approximation problem: given full set of basis vectors β = {u1, u2, . . . , un} with each ui Rd, we aim to select vectors whose linear combination best approximates target vector Rd. We represent the sparse coefficient vector as = [x1, x2, . . . , xk] Rk, corresponding to the selected basis vectors. Formally, we solve: min xB2 2 subject to x0 = k, (34) where Rkd is submatrix formed from selected rows of the original basis. In our setting, = 1R, = (WQ)i,:R, and = pi. Since this problem is NP-hard, we adopt greedy approximation. We first compute = vB1 = WQH, which is in fact the non-sparse solution to the objective in Equation 6, and select the entries of with the largest magnitudes. Let the corresponding indices be i1, i2, . . . , ik, and define the selected basis = [ui1; . . . ; uik ]. We then solve least-squares problem over the selected support: = argmin(xi1 , ,xik ) [xi1 xi2 xik ] v2 2 = vB(BB)1. (35) While this solution is numerically optimal when is orthogonal, we empirically demonstrate its effectiveness under general conditions. Combined with our AdaAlloc-based parameter allocation strategy, this initialization consistently yields high quantization error reconstruction ability while maintaining full rank capacity."
        },
        {
            "title": "D EXPERIMENTAL DETAILS AND ABLATIVE STUDY",
            "content": "D.1 FINE-TUNING HYPERPARAMETERS We follow the hyperparameter settings adapted from Gao et al. (2024b) and Deng et al. (2025). Training is performed using the AdamW optimizer (Loshchilov & Hutter, 2017). Table 8 reports the key settings, including minibatch size, weight decay, dropout ratio, learning rate scheduler, maximum sequence length, number of training epochs, warmup ratio, and the adapter scaling factor α, which can be applied to adapters in Equation 2 through Equation 4. The scaling factors for low-rank and FT-based adapters are defined so that the ℓ2 norm of the gradients applied to their respective parameters is aligned. The learning rates for each combination of model, task, method, and bit-width are summarized in Table 9. We note that 128 sequences of length 2048, randomly sampled from the WikiText-2 (Merity et al., 2016) dataset, are used as calibration set for quantization and adapter initialization, as these processes are robust to the choice of dataset (Frantar et al., 2023; Zhang et al., 2024; Deng et al., 2025). Table 8: Hyperparameter settings for Alpaca and GSM8K training Dataset Method Alpaca GSM8K CLoQ QWHA CLoQ QWHA Optimizer Batch Size LR Scheduler Max Sequence Length Epochs Warmup Ratio Weight Decay Dropout Scale AdamW 64 cosine 512 3 0.1 1 6 0.03 0. 0.1 1 0 4000 0.1 1 0 4000 Table 9: Learning rate for each model and bit widths on Alpaca and GSM8K training. Model Bits Alpaca GSM8K CLoQ QWHA CLoQ QWHA Llama-3.1-8B Llama-3.2-3B Mistral-7B-v0.3 4 3 2 4 3 2 4 3 2 1e-5 1e-5 1e-5 1e-4 1e-4 2e3e-5 2e-5 2e-5 3e-5 3e-5 2e-5 3e-5 3e-5 5e-5 5e-6 5e-6 7e-6 1e-4 1e-4 7e-5 1e-4 1e-4 1e3e-5 3e-5 3e-5 5e-5 7e-5 5e-5 7e-5 1e-4 2e-4 2e-5 3e-5 3e-5 22 D.2 COMMONSENSE QUESTION-ANSWERING BENCHMARK We present the detailed accuracy scores on the commonsense question answering (CSQA) benchmark in this section, while the averaged scores are reported in Section 4. The results for each of the seven individual tasks, are provided in Table 10 and Table 11, corresponding to Table 3 and Table 4, respectively. Table 10: Accuracy (%) evaluation results of CSQA benchmarks. Model LLaMA-3.1-8B LLaMA-3.2-3B Mistral-7B-v0.3 Bits Method 4 3 2 16 4 2 16 4 3 2 Pre-trained Fine-tuned GPTQMagR CLoQ SHiRA LoCA SSH QWHA GPTQMagR CLoQ SHiRA LoCA SSH QWHA GPTQMagR CLoQ SHiRA LoCA SSH QWHA Pre-trained Fine-tuned GPTQMagR CLoQ SHiRA LoCA SSH QWHA GPTQMagR CLoQ SHiRA LoCA SSH QWHA GPTQMagR CLoQ SHiRA LoCA SSH QWHA Pre-trained Fine-tuned GPTQMagR CLoQ SHiRA LoCA SSH QWHA GPTQMagR CLoQ SHiRA LoCA SSH QWHA GPTQMagR CLoQ SHiRA LoCA SSH QWHA Arc-c Arc-e BoolQ Hella. Obqa 44.80 82.11 53.41 46.40 81.99 56.40 45.00 81.77 48.98 45.40 83.12 50.34 46.60 82.81 53.24 47.60 83.06 55.12 44.60 83.18 54.69 47.00 83.64 55.20 43.80 81.59 48.81 44.80 82.42 49.91 44.80 80.98 53.41 45.20 81.13 54.61 43.00 82.81 53.24 45.20 81.68 54.69 25.00 53.15 24.23 35.40 67.74 37.80 31.40 64.31 33.28 35.60 68.13 37.63 35.00 67.58 39.08 37.40 74.62 41.72 43.00 73.39 45.99 43.80 74.95 48.29 42.60 74.95 44.97 42.40 74.95 47.70 41.60 71.80 43.17 42.20 74.34 47.78 42.80 75.35 47.95 41.60 75.78 48.98 40.00 70.86 42.92 41.40 72.97 46.33 40.40 72.26 44.54 41.20 70.83 44.71 41.80 71.62 44.88 41.80 72.51 47.18 29.00 54.28 26.62 37.40 66.02 35.24 34.80 63.64 33.28 34.60 64.46 34.64 36.00 64.65 34.81 37.20 65.26 37.29 44.20 82.14 52.30 45.80 84.19 55.03 44.00 80.55 51.37 44.00 83.91 54.52 44.60 82.69 53.50 45.40 83.88 53.84 45.60 84.71 53.75 45.40 84.74 54.69 42.20 77.06 49.23 43.40 80.55 52.99 44.20 79.57 51.37 43.40 80.95 51.96 43.20 81.99 51.54 44.20 82.32 52.56 29.80 54.37 26.37 38.00 74.25 43.94 36.60 71.38 39.33 37.80 75.99 43.77 36.00 76.79 44.03 37.60 78.53 45.39 81.10 81.57 78.96 79.50 81.14 80.98 78.62 80.26 76.52 77.48 79.17 79.88 75.97 80.05 38.51 56.19 51.31 59.22 57.79 64.94 71.63 73.15 70.83 72.35 68.01 73.40 73.32 73.15 66.08 71.25 68.43 69.74 70.37 72.64 38.89 56.27 56.40 58.42 58.33 61.99 78.24 80.05 76.52 78.58 78.41 78.28 78.45 78.79 75.00 77.44 77.31 77.31 77.23 76.94 41.46 66.46 64.27 67.63 68.77 69.78 78.91 79.85 75.87 75.63 78.76 79.56 79.44 79.62 73.74 74.30 77.63 78.25 80.27 78.70 36.73 64.14 56.76 63.79 62.00 68.11 73.61 76.71 71.34 74.25 73.33 74.33 74.38 74.44 68.29 72.41 71.31 72.17 72.17 72.72 39.32 59.77 55.11 56.26 56.21 61.76 80.42 81.09 79.71 81.09 80.46 81.12 81.18 80.93 78.22 80.37 79.52 80.17 79.93 80.31 48.88 70.95 66.58 69.78 70.15 71.97 Piqa Wino. Average 81.23 81.99 79.87 79.92 81.56 81.56 81.56 81.99 78.24 79.05 80.30 80.25 81.56 80.79 57.67 72.31 68.72 70.95 71.38 74.54 77.48 77.91 77.15 77.86 76.99 78.13 78.67 79.00 76.12 78.35 77.75 78.13 78.18 79.71 59.36 70.35 69.86 71.44 70.51 73.88 82.26 82.43 81.72 82.37 82.43 82.86 82.64 82.70 80.63 81.39 81.45 81.39 81.39 82.21 62.62 75.41 73.88 74.54 75.35 76.44 73.88 74.66 73.32 73.16 73.40 72.30 73.16 72.77 71.59 73.01 71.51 72.14 72.14 72.38 51.70 61.88 57.14 61.64 59.59 65.51 69.85 70.17 69.14 68.82 66.85 68.98 68.35 69.85 66.14 67.72 65.67 66.30 65.98 67.01 52.80 59.19 57.30 57.30 57.54 61.80 73.88 74.51 72.93 74.74 74.11 75.45 74.66 74.66 70.64 73.24 72.14 72.30 72.30 73.01 57.85 63.61 61.56 64.72 65.11 67.17 70.78 71.84 69.11 69.58 71.07 71.45 70.75 71.50 67.76 68.71 69.68 70.21 69.86 70.50 41.00 56.49 51.84 56.71 56.06 60.98 64.99 66.43 64.43 65.48 63.10 65.59 65.83 66.11 61.49 64.35 62.90 63.30 63.57 64.80 42.90 54.89 52.91 53.87 54.01 57.03 70.49 71.87 69.54 71.32 70.88 71.55 71.57 71.70 67.57 69.91 69.36 69.64 69.65 70.22 45.91 61.80 59.08 62.03 62.31 63. 23 Table 11: Accuracy (%) evaluation results of CSQA benchmarks on LLaMA-3.2-3B. Bits 4 3 Adapter QA Init. Type Coeff. Selection Refine. Arc-c Arc-e BoolQ Hella. OBQA PiQA Wino. Average WHA WHA WHA WHA WHA WHA DCA DHA Sparse WHA WHA WHA WHA WHA WHA DCA DHA Sparse WHA WHA WHA WHA WHA WHA DCA DHA Sparse Random Random Magnitude LoCA SSH AdaAlloc AdaAlloc AdaAlloc AdaAlloc Random Random Magnitude LoCA SSH AdaAlloc AdaAlloc AdaAlloc AdaAlloc Random Random Magnitude LoCA SSH AdaAlloc AdaAlloc AdaAlloc AdaAlloc 48.55 48.04 48.81 47.61 47.61 48.98 47.10 47.70 47.61 44.88 44.97 47.35 46.25 44.45 47.18 46.08 45.31 45.65 34.56 34.90 37.71 33.96 34.39 37.29 35.92 36.09 35.67 73.70 73.36 72.01 73.06 73.40 73.15 72.47 73.06 72. 70.54 71.38 72.31 69.49 67.38 72.64 72.05 72.18 70.75 59.05 58.54 60.65 58.08 55.77 61.99 57.15 58.59 56.40 74.53 74.22 75.26 74.16 75.26 75.78 73.24 75.60 73.49 71.53 71.38 72.69 71.38 71.83 72.51 73.55 72.32 68.35 65.17 64.31 64.83 64.71 64.50 65.26 66.18 66.67 67.16 74.43 74.50 74.39 74.18 74.45 74.44 75.14 74.61 75. 71.71 72.38 72.40 71.93 71.38 72.72 73.11 72.91 72.77 55.68 58.13 60.88 56.27 58.80 61.76 61.94 61.12 63.06 43.20 42.00 42.80 43.20 43.00 41.60 43.00 41.60 43.60 41.40 41.80 41.00 41.80 40.20 41.80 42.20 41.40 41.00 34.80 35.40 37.40 35.80 36.00 37.20 37.00 36.00 36.40 78.67 78.78 79.43 78.56 78.07 79.00 79.11 78.67 78. 77.86 78.62 79.38 78.51 78.35 79.71 78.18 78.94 77.58 70.67 70.84 72.58 70.67 70.73 73.88 72.58 72.52 73.23 68.90 70.48 69.77 69.46 69.93 69.85 68.75 70.17 68.82 66.77 66.85 66.54 66.77 66.85 67.01 68.19 67.40 67.96 58.25 59.27 61.40 58.01 59.19 61.80 60.85 61.33 59.91 66.00 65.91 66.07 65.75 65.96 66.11 65.54 65.92 65. 63.53 63.91 64.52 63.73 62.92 64.80 64.77 64.35 63.43 54.03 54.48 56.49 53.93 54.20 57.03 55.95 56.05 55.97 D.3 ABLATION ON QUANTIZATION GROUP SIZE We conduct an ablation study on the effect of quantization group size in the LLaMA-3.2-3B model using 2-bit quantization, where the impact of group size on model accuracy is most clearly observed, as shown in Table 12. Smaller group sizes provide finer granularity, leading to higher model accuracy. However, they also incur greater computational overhead during the quantization and dequantization process due to the increased number of quantization parameters. Considering this trade-off, we adopt group size of 64 for our experiments, consistent with prior works on quantization-aware PEFT. Table 12: GSM8k accuracy (%) of QWHA on LLaMA-3.2-3B with 2-bit quantization using various quantization group sizes. We adopt group size of 64 for the main experiments. Group Size 32 64 128 256 Score 29. 29.11 24.48 22.51 24 D.4 TRAINING TIME OF ADAPTERS We compare the training time of adapters using single-transform and two-transform designs on both WHT and conventional transform kernels, such as the DCT used in LoCA and the DHT used in SSH, in Table 13. While employing WHA reduces training time, applying it with single transform further decreases computation. The impact of single transform is especially evident in DCT and DHT, where training time is substantially reduced since their computational overhead due to the transform is larger than that of WHT. Note that DCT and DHT have identical training times, as their computational cost is the same and differs only in the element values within the transform kernel. Our proposed WHA employs 1D WHT in the context of quantization, whereas conventional FTbased PEFT methods such as LoCA and SSH use 2D DCT and 2D DHT, respectively. Table 13: Training time (hours) of FT-based adapters with different transform kernels on LLaMA3.1-8B with the Alpaca dataset. Batch Size 1 2 4 8 16 WHT DCT / DHT 1D 2D 1D 2D 18.2 9.7 6.0 4.6 3.9 25.3 13.1 8.0 5.5 4.3 46.2 32.1 17.4 9.0 6. 63.3 45.8 26.1 13.3 8.3 D.5 TRAINING MEMORY USAGE OF ADAPTERS We report the memory usage of each method under the same experimental setting as in Section 4, using NVIDIA A100 80GB GPU. As shown in Table 14, QWHA shows memory usage comparable to LoRA. SSH also exhibits similar memory usage as QWHA, since the only difference between them is the computation with pre-defined transform kernel matrix. Since this matrix is shared across layers, the memory overhead is negligible. In contrast, LoCA incurs additional memory consumption due to the training of location parameters, resulting in few gigabytes of overhead depending on the batch size. Table 14: GPU memory usage (GB) during fine-tuning on LLaMA-3.1-8B with 4-bit quantization using the Alpaca dataset. All adapters use the same number of trainable parameters with (r = 64). Batch Size CLoQ QWHA LoCA 1 2 4 8 22.1 26.6 32.6 44.7 68.8 22.1 27.2 33.2 45.3 69.4 23.3 28.4 34.4 46.4 70."
        }
    ],
    "affiliations": [
        "Seoul National University",
        "Sungkyunkwan University"
    ]
}