{
    "paper_title": "All is Not Lost: LLM Recovery without Checkpoints",
    "authors": [
        "Nikolay Blagoev",
        "OÄŸuzhan Ersoy",
        "Lydia Yiyu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training LLMs on decentralized and wimpy computation nodes, e.g., multiple on-spot instances, lowers the training cost and enables model democratization. The inevitable challenge here is the churn of nodes due to failures and the operator's scheduling policies, leading to losing a stage - a part of the model. The conventional approaches to recover from failures are to either use checkpointing, where periodically a copy of the entire model is sent to an additional storage, or redundant computation. These approaches yield significant communication and/or computation overhead even in non-failure cases and scale poorly in settings with large models. In this paper, we propose, CheckFree, an efficient recovery method where a failing stage is substituted by a weighted average of the closest neighboring stages. In contrast to the state of the art, CheckFree requires no additional computation or storage. However, because of the nature of averaging neighbouring stages, it can only recover failures of intermediate stages. We further extend our method to CheckFree+ with out-of-order pipeline execution to tolerate crashes of the first and last stages. Thanks to out-of-order pipelining, behaviour of those stages is mimicked by their neighboring ones, which allows CheckFree+ to recover them by simply copying the weights from the immediate neighbour. To be able to recover the (de)embedding layers, CheckFree+ copies those layers to the neighboring stages, which requires relatively small storage overhead. We extensively evaluate our method on LLaMa models of model sizes from 124M to 1.5B with varying failure frequencies. In the case of low and medium failure rates (5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant computation in terms of convergence in wall-clock time by over 12%. Both of our proposals can be run via our code available at: https://github.com/gensyn-ai/CheckFree."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 1 6 4 5 1 . 6 0 5 2 : r All is Not Lost: LLM Recovery without Checkpoints Nikolay Blagoev1, OÄŸuzhan Ersoy1 and Lydia Yiyu Chen2,3 1Gensyn, 2University of Neuchatel, 3TU Delft Training LLMs on decentralized and wimpy computation nodes, e.g., multiple on-spot instances, lowers the training cost and enables model democratization. The inevitable challenge here is the churn of nodes due to failures and the operators scheduling policies, leading to losing stage - part of the model. The conventional approaches to recover from failures is to either use checkpointing, where periodically copy of the entire model is sent to an additional storage, or redundant computation. These approaches yield significant communication and/or computation overhead even in non-failure cases and scale poorly in settings with large models. In this paper, we propose, CheckFree, an efficient recovery method where failing stage is substituted by weighted average of the closest neighboring stages. In contrast to the state of the art, CheckFree requires no additional computation or storage. However, because of the nature of averaging neighbouring stages, it can only recover failures of intermediate stages. We further extend our method to CheckFree+ with out-of-order pipeline execution to tolerate crashes of the first and last stages. Thanks to out-of-order pipelining, behaviour of those stages is mimicked by their neighboring ones, which allows CheckFree+ to recover them by simply copying the weights from the immediate neighbour. To be able to recover the (de)embedding layers, CheckFree+ copies those layers to the neighboring stages, which requires relatively small storage overhead. We extensively evaluate our method on LLaMa models of model sizes from 124M to 1.5B with varying failure frequencies. In the case of low and medium failure rates (5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant computation in terms of convergence in wall-clock time by over 12%. Both of our proposals can be run via our code available at: https://github.com/gensyn-ai/CheckFree. 1. Introduction Large Language Models (LLMs) have demonstrated an unprecedented ability to perform on variety of language tasks. State of the art LLM training utilizes multiple Graphic Processing Units (GPUs) to parallelize and shard the training. The model is split across several GPUs (constituting stages), which communicate with the node servicing the previous layers and node servicing the next layers to execute the model in pipeline parallel (PP) fashion 1. Typically, pipelining is combined with data parallelism (DP) where several of these pipelines are trained in parallel on different data, synchronizing their gradients at the end of an iteration. Training such large models can take several months even on specialized high performance clusters Dubey et al. (2024); Yuan et al. (2022). Recent works have proposed training models on cheaper GPUs, such as decentralized or cloud spot instances (preemptible GPUs) Jang et al. (2023); Thorpe et al. (2023); Ryabinin et al. (2023). Due to the high training time, (hardware) failures during the training are inevitable Zhang et al. (2022). Moreover, the number of failures is expected to be higher for training on such wimpy spot instances since the failures can also occur due to spot nodes becoming 1Together all nodes of distributed model is termed pipeline. Corresponding author(s): nikolay@gensyn.ai 2025 Gensyn. All rights reserved. All is Not Lost: LLM Recovery without Checkpoints Table 1 Comparison of failure recovery strategies regarding the additional costs required even in the non-failure cases. is the model, ğ‘† is the parameter size per stage, is the embedding layer, and ğ‘ are the number of nodes. Here, we exclude Oobleck Jang et al. (2023) since it does not recover full stage failure. Additional Memory Additional Comm. Additional Comp. Non-faulty storage Recovery of stages Checkpoint (Wang et al. (2023)) ğ‘‚(F ) ğ‘‚(F ) 0 Yes All stages Redundant Comp. (Thorpe et al. (2023)) ğ‘‚(F ) ğ‘‚(F ) Forward pass No Non-consecutive stages CheckFree CheckFree+ 0 0 0 No Non-consecutive intermediate stages ğ‘‚(E ) ğ‘‚(E ) 0 No Non-consecutive stages unavailable Thorpe et al. (2023). In pipeline, stage can have partial nodes or all nodes failure. We omit the former scenario, since failing node can be trivially recovered by using the alive nodes of the same stage as the checkpoints. For example, when new nodes join, the most recent weights can be downloaded from peer in the same stage, as per Jang et al. (2023). In this work, we aim to design recovery methods for stage failures. This could happen when either (i) there is no DP and each stage is only run by single node or (ii) all the nodes of the same stage fails simultaneously. The latter case is plausible for real deployment via spot instances since such nodes of the same stage would be rented from the same geographic location (to minimize the DP communication overhead) Yuan et al. (2022), and in the case of high demand on that location, all spot instances becomes unavailable simultaneously Thorpe et al. (2023). To deal with stage failures during training, conventional approaches make use of checkpointing - periodically saving the model state to non-volatile storage, which should remain reachable throughout the training Wang et al. (2023). When failure occurs, the state of the model is reverted to the previously stored one, thus losing several GPU hours of training time. careful tradeoff is chosen between checkpointing frequency and training throughput, as checkpointing too frequently to remote storage can slow down the training time Wang et al. (2023). For example, LLaMa 70B requires roughly 520GBs when serialized and even on high bandwidth networks2, would require upwards of 20 minutes per checkpoint, thus impacting training at high checkpointing frequency. Alternatively to checkpointing, Bamboo Thorpe et al. (2023) uses redundant computation across the pipeline to recover failures where each node stores and does redundant computation for the following stage (in addition to their assigned stage). Both recovery methods require significant overhead either in storage, communication or computation. In this work, we propose an efficient recovery method CheckFree. Our novel approach can recover, even when an entire stage has failed, without the need for checkpointing and without redundant computations, thus significantly outperforming the state of the art. To achieve this, we exploit layer stacking Agarwal et al. (2024) and LLMs; natural resilience to layer omission Elhoushi et al. (2024). When stage has crashed, we reinitialize it with weighted average of its two neighbouring stages, on new node. However, this strategy fails to recover the first and last stages without suffering from significant convergence drop. For those case, we utilize the idea of out of order pipeline parallelism Blagoev et al. (2025), such that the second and second to last stage learn the behaviour of the first and last stage respectively. We refer to the extension that also recovers the first and last stages as CheckFree+. visualization of the different approaches is presented in Fig. 1, and the comparison of their costs is given in Table 1. The rest of the paper is structured as follows. We first present relevant background knowledge on 2Assuming high-bandwidth CPU-to-GPU bus and 500Mb/s connection. All is Not Lost: LLM Recovery without Checkpoints (a) Checkpointing with external storage. Redundant computation (and (b) storage). (c) Our solution(s). Figure 1 Three recovery strategies from stage failure. a) In typical checkpointing strategies, an external non-faulty storage is used, which periodically stores checkpoints. Upon failure, new node receives stale weights from the storage and continues training. b)In decentralized training, redundant computation can be used to recover from failure of an entire stage Thorpe et al. (2023). Each stage performs also the computations of the subsequent stage. When new node joins, it can download the stage 1 weights from the previous stage. c) Our solutions CheckFree and CheckFree+ do not require checkpointing or redundant computation. Instead, we use the weights of the two neighbouring stages to initialize the failed stage. CheckFree+ also recovers the first and last stages by copying the corresponding neighbouring stage which is trained to simulate the same behaviour. fault tolerance and checkpointing. We then present our main method, with theoretical justification of its stability. We further extend our work with novel use of out of order pipelining for redundant knowledge encoding. Lastly, we extensively evaluate our solution across various models and settings with failures and demonstrate superior performance of our method, which improves training time by over 12% in low stage failure frequencies relative to redundant computation. 2. Related Work Failure Recovery. As the models are growing and training process becoming ever more expensive, fault-tolerant distributed LLM training has been receiving greater recognition Ryabinin et al. (2023); Thorpe et al. (2023); Jang et al. (2023); Liang et al. (2025). Works such as SWARM Ryabinin et al. (2023) and DMoE Ryabinin and Gusev (2020) studied efficient recovery in pipeline-parallel training in the presence of failures. Recovery of weights and optimizer states has typically been performed through checkpointing Wang et al. (2023), where periodically the values of the weights and optimizer state are stored in some non-faulty remote storage. On node failure, nodes return to the previous checkpoint they have stored (locally) and new coming nodes download the weights from the remote storage. The training proceeds from the previous checkpoint and the models convergence is unaffected. In decentralized training via spot instances, churn rates are even higher and there may not be an available non-faulty storage to save the checkpoints to. Thus, checkpointing strategies are inefficient if not impossible for this setting. Recent works have attempted to address these shortcomings. Bamboo Thorpe et al. (2023) makes use of redundant computation, where each node computes the forward pass for itself and redundantly for the next node in the pipeline. In the case of failure of node (or stage), the previous node(s) can immediately continue the training with the redundant weights. However, such mechanism can prove too costly for large models, as each node must now perform double the training Jang et al. (2023). To address this, Oobleck Jang et al. (2023) makes use of the data parallel replication to recover from failures, where new nodes can get the weights from any of the nodes servicing the same weights as it. While efficient, such recovery mechanism fails when an entire stage is gone. Layer Stacking. Layer (or model) stacking has emerged in recent years as an efficient method for training LLMs by starting from small model and periodically initializing new weights Agarwal et al. (2024); Du et al. (2024); Saunshi et al. (2024). Across commonly used strategies such as copying layer to increase the depth and random initialization, it is shown that copying the last layer provides the All is Not Lost: LLM Recovery without Checkpoints biggest speed up and has the best over all performance, even outperforming training the full model Du et al. (2024). In Agarwal et al. (2024), authors further demonstrated that such method of initialization approximates accelerated Nesterov gradient descent. Finally, in Saunshi et al. (2024), authors showed the resemblance of models trained with stacking to looped LLMs Lan et al. (2020) and further found that stacking new layers in the middle greatly improves the models reasoning for downstream tasks. Layer Omission. As the yang to layer stacking, layer skipping is about removal (pruning) of layers during training or afterwards. LayerSkip demonstrated that during training layers can be skipped to improve early exit inference and to speed up training Elhoushi et al. (2024). SkipPipe further demonstrated arbitrary layer skipping and out of order execution as viable method to speed up distributed training Blagoev et al. (2025). Both of these are reminiscent of earlier work on Stochastic Depth, which has been shown to work as gradient noise regularization Veit et al. (2016). The work Ma et al. (2022); Zhou et al. (2023) demonstrated that models can converge even with layers removed/skipped during training. This implies certain redundancy between neighbouring layers of Large Language Models. 3. Problem Statement Notation. We assume model , composed of an embedding layer E, several layers with residual connections ğ‘“1... ğ‘“ğ¿, and deembedding layer 1 : = 1 (ğ¼ + ğ‘“ğ¿) ... (ğ¼ + ğ‘“2) (ğ¼ + ğ‘“1) E, where is composing operation which is distributive around summation and ğ¼ is the identity matrix. The weights of each layer ğ‘– are given by ğ‘Šğ‘–. The goal of the model (F : Y) is to minimize some loss function (ğ”¼(ğ‘¥,ğ‘¦)ğ·L) over some dataset (ğ· with distribution Y) via some optimization algorithm. Distributed Training. The model is split over stages ğ‘†0...ğ‘†ğ‘ , each holding non overlapping and consecutive partition of the layers. The weights of stage ğ‘– are given by ğ‘Šğ‘ ,ğ‘–. Nodes serve single stage and communicate with their peers to train the model. Communication occurs over the network and is associated with delays dependent on the bandwidth and latency between pairs of nodes. No central non-faulty storage exists to which model weights can be offloaded. Failure pattern. Nodes can fail at arbitrary points. failing node is disconnected from the training and its local progress is lost. This can occur due to hardware errors or preemptible instances temporarily disconnecting. Upon failure of ğ‘˜ nodes ğ‘˜ new nodes can be made available within negligible amount of time (e.g. starting new on spot node in different location to join the training). We distinguish between failures where at least one node remains per stage and failures where some stage has no nodes in it. Dealing with the former type of failures is trivial as new nodes can download the parameters for given stage from node alive serving that stage Ryabinin et al. (2023); Jang et al. (2023). Such recovery has no impact on the convergence of the model (as all replicas have the same weights). We henceforth focus only on recovering from failures where stage is entirely disconnected. When stage fails, its weights are essentially replaced with all zeroes (ğ‘Šğ‘ ,ğ‘– = 0). We assume that no two consecutive stages can fail, which is an assumption also used in redundant computation Thorpe et al. (2023). We additionally assume that new nodes can be made available to replace (at least some of) the nodes that have crashed in the missing stage. This can be done by spawning new nodes in different region in on-spot instances or keeping some participants on standby in decentralized training. 4 All is Not Lost: LLM Recovery without Checkpoints 4. Our Recovery Method 4.1. Motivation Our recovery method draws inspiration from several key (known and experimented) observations regarding training of LLMs in the presence of layer omissions and stacking. LLMs are resilient to layer omission. Recent works have demonstrated that in both training and inference time, LLMs are resilient to layer omission Blagoev et al. (2025); Elhoushi et al. (2024); Zhang et al. (2024). This is partially attributed to the residual connections Veit et al. (2016) and suggests degree of redundancy encoded in the layers (neighbouring layers can perform each others functionality). Layer stacking can improve LLMs performance. It has shown that initializing new layers through neighbouring one improves training efficiency as well as the models reasoning Saunshi et al. (2024); Du et al. (2024); Agarwal et al. (2024). Figure 2 Varying reinitialization strategies for failed stages. Based on these observations, we consider recovering failed stage by copying neighboring stage. However, in our initial experiments, we observe that simple copy of one stage is an insufficient initialization method, as such recovery greatly hampers models convergence. We present this in Fig. 2 where we train the same model and reinitialize models on failure using different strategies. We compare three different strategies - random (randomly initializing the new stage), copy (copying the previous stage), and weighted (or gradient) averaging. In the later sections, we extend the recovery method by reinitializing failed stages as weighted average of their two neighbours. Considering each stage as an independent few-shot learner, as in Reddi et al. (2023), averaging of stages can be seen similar to model merging Wortsman et al. (2022); Frankle et al. (2020) where several models are averaged together to achieve an improved ensemble model. From Fig. 2, it can be seen that though copying neighbouring stage is significantly better than random reinitialization (of the failed stage), gradient averaging significantly outperforms the other two methods for reinitializing. In the following sections, we explain how we choose these weights (for CheckFree) as well as how we can recover the first and last stages since they have only single neighbour (via CheckFree+). 4.2. CheckFree: Memoryless Recovery Algorithm 1 Recovery algorithm for stage ğ‘– Require: new node assigned to stage ğ‘–, ğœ† learning rate 1: Receive ğ‘Šğ‘ ,ğ‘–1 and ğœ”ğ‘–1 of stage ğ‘– 1 where ğœ”ğ‘–1 = ğ‘Šğ‘ ,ğ‘–12 2: Receive ğ‘Šğ‘ ,ğ‘–+1 and ğœ”ğ‘–+1 of stage ğ‘– + 1 where ğœ”ğ‘–+1 = ğ‘Šğ‘ ,ğ‘–+12 3: Initialize the weights of the failed stage ğ‘Šğ‘ ,ğ‘– 4: Update learning rate ğœ† 1.1ğœ† 5: Continue training from the current batch ğœ”ğ‘–1ğ‘Šğ‘ ,ğ‘–1+ğœ”ğ‘–1ğ‘Šğ‘ ,ğ‘–+1 ğœ”ğ‘–1+ğœ”ğ‘–+1 In the case of no checkpointing or redundant computation, if stage failure happens, then by default, the models weights for the failed stages are lost. Our method does not directly recover the exact weights of the failed stage, but recovers (maintains) the models performance by replacing the failed stages with 5 All is Not Lost: LLM Recovery without Checkpoints combination of the remaining ones. Specifically, by taking into account the observations given in the previous section, we replace the failed stage ğ‘Šğ‘ ,ğ‘– = 0 with ğ‘Šğ‘ ,ğ‘– = , i.e. the weighted average of the weights of its neighbouring stages. ğœ”ğ‘–1ğ‘Šğ‘ ,ğ‘–1+ğœ”ğ‘–+1ğ‘Šğ‘ ,ğ‘–+1 ğœ”ğ‘–1+ğœ”ğ‘–+1 Now, we discuss how to select the weights ğœ”ğ‘–1 and ğœ”ğ‘–+1. known approach is to copy the previous stage when initializing new weights (ğœ”ğ‘–+1 = 0). However, as we see in Figure 2, this proves inferior to aggregated averaging. naive way of averaging would be uniform average of the two stages, i.e. (ğœ”ğ‘–1 = ğœ”ğ‘–+1). Such averaging, though, does not distinguish the importance and convergence of the stages, and thereby leads to slower convergence of the overall model. For that reason, we use the weights of the last gradient norm of the given stage, i.e. ğœ”ğ‘–1 = ğ‘Šğ‘ ,ğ‘–12 and ğœ”ğ‘–+1 = ğ‘Šğ‘ ,ğ‘–+12. Conceptually, this gives more weight to stages which have not converged as much yet, thus partially offloading their functionality to the new stage. To achieve weighted gradient averaging, for each stage ğ‘†ğ‘–, the nodes need to store the gradient norm ğ‘Šğ‘ ,ğ‘–2 and send it to the new-coming nodes. The communication and storage overhead of this is negligible, as ğ‘Šğ‘ ,ğ‘–2 is single scalar, which is awfully smaller than the size of the weights of the stage ((ğ‘Šğ‘ ,ğ‘–2) ğ‘Šğ‘ ,ğ‘–). Once the weights are reinitialized, to further assist the new-formed stages in diverging from their (possibly) inferior state, we scale up the learning rate (by 1.1). Our solution is presented in Algorithm 1. The method presented so far cannot recover the first and last stage, as there is no second neighbour to average from. Additionally, as identified in several other works, the first and last stage perform different functionality than the other stages (the first stage especially) and are critical to the performance of the model Saunshi et al. (2024); Bhojanapalli et al. (2021). simple copy of one neighbour results in significant drop in performance. naive approach is to assume that these stages either do not fail (hosted on on-demand instances) or have significantly smaller chances of failure (hosted on more trust-worthy devices). With such an assumption, CheckFree can recover during training. 4.3. CheckFree+: First and Last Stage Recovery Here we present CheckFree+ which extends CheckFree by also recovering the first and last stage failures. We take inspiration from out-of-order pipeline parallelism presented in Blagoev et al. (2025) to mimic redundant computation Thorpe et al. (2023) without actually duplicating computation. For half the microbatches we run the stages in standard order (ğ‘†0, ğ‘†1, ğ‘†2...ğ‘†ğ¿1, ğ‘†ğ¿, ğ‘†0)3 and for the other half we swap the order of the first two and last two stages excluding the (de)embedding layers (i.e. ğ‘†0, ğ‘†2, ğ‘†1...ğ‘†ğ¿, ğ‘†ğ¿1, ğ‘†0). In this way, the neighboring stages of the first and last stages, ğ‘†2 and ğ‘†ğ¿1, redundantly learn the behaviour of ğ‘†1 and ğ‘†ğ¿ respectively, without any additional computation, as half the time they take their places in the pipeline. For the same reason, this would result in the two layers having very similar weights (within some noise range of each other), meaning they can easily be recovered from one another. Out-of-order training, however, comes with small degradation to convergence, as noted in Blagoev et al. (2025). As such, if the failure chance is almost negligible, this solution would prove slower than no checkpointing, due to the longer time needed to converge. CheckFree+ cannot recover the embedding and deembedding (LM head) layers. While some works explore sharing the weights between the two Hillier et al. (2024), thus suggesting that in case of failure of either or 1 the weights can be reinitialized by copying the other ones, in this work we simply send their weights to the previous and following stages. Thus in case of failure the weights can be recovered exactly without loss of data. While this may resemble some form of decentralized checkpointing, we note that the sizes of these layers are significantly smaller compared to the other stages, thus presenting minimal communication and storage overhead. 3Note, here ğ‘†0 holds only embedding and deembedding layers. 6 All is Not Lost: LLM Recovery without Checkpoints 4.4. Convergence Analysis of CheckFree We base our proof on Ma et al. (2022). The model is comprised of an embedding layer E, deembedding layer 1, and series of residual functions = 1 (ğ¼ + ğ‘“ğ‘™) ... (ğ¼ + ğ‘“1) E. Post failure of layer ğ‘˜, the model is modified as = (ğ¼ + ğ‘“ğ‘™) ... (ğ¼ + ğ‘“ğ‘˜+1) (ğ¼ + ğœ”1 ğ‘“ğ‘˜+1 + ğœ”2 ğ‘“ğ‘˜1) (ğ¼ + ğ‘“ğ‘˜1)... (ğ¼ + ğ‘“1) E. Assumption 1. The loss function (F ) is L-smooth and convex: (F1) (F2) ğ¿ F1 F2 F1, F2. Assumption 2. For some ğ›¿ [0, 1) at any timestep ğ‘¡ the model reduction error is bounded by: where ğ‘š is 0-1 vector selecting given layers of the model. Fğ‘¡ ğ‘š Fğ‘¡ 2 ğ›¿2Fğ‘¡ 2 This is common assumption in literature Zhou et al. (2023); Ma et al. (2022). Given these assumptions, the convergence of model past failure with our recovery method is given by: ğ‘¡ ğ”¼F ğ‘¡1 F02 ( 1 ğ‘¡ ) + ğ‘¡ 2ğ”¼ğœ”1 ğ‘“ğ‘˜+1 + ğœ”2 ğ‘“ğ‘˜1 ğ‘“ğ‘˜2 This, logically, implies that every failure slows down convergence with the error incurred by the initialization. The details of the proof are provided in Appendix B. 5. Evaluation In this section we extensively evaluate CheckFree and CheckFree+ on models ranging from 124M to 1.5B (details of each model can be found in Appendix A.1). We demonstrate that our solutions outperform the state of the art in terms of failure recovery and does not suffer from significant decrease in convergence, despite incurring lower storage and computation cost compared to existing failure recovery strategies. Setup. We perform our tests on private cluster of H100 GPUs. Communication delays between nodes is simulated based on realistic bandwidth and latency measurements between 5 geo-distributed locations from Google Cloud. We use the rates of 5%, 10%, or 16% as the probability of stage failure within an hour. These values are inspired from Bamboo Thorpe et al. (2023) where they use double of these failure rates per node, while for us they represent stage failures. 4 This is very challenging churn rate to deal with in real systems. Baselines. We compare against two baselines - checkpointing and redundant computation. For checkpointing we create checkpoints for the small models every 50 iterations and for the medium models every 100 iterations (roughly corresponding to every 3 hours, as per Scao et al. (2022)). We investigate the effects of checkpointing frequency on convergence in Fig 4b. For redundant computation, each stage computes redundantly the next stage in the forward pass as per Thorpe et al. (2023). To account for the higher memory requirement, we use half the microbatch size, but double the microbatch count, thus keeping the same batch size. Details of all experiments can be found in Appendix A. 4These probabilities could be significantly lower. Under the assumption that there are ğ‘˜ devices each with an independent probability of disconnecting ğ‘, the chance of an entire stage failing is ğ‘ğ‘˜. However, in practice one would use datacenter responsible per stage in distributed training since then the data parallel communication latency is significantly lower. Thus, failure of stage is plausible if the datacenter becomes fully occupied and spot instances simultaneously become unavailable. 7 All is Not Lost: LLM Recovery without Checkpoints (a) Small model at 10% failure rate. (b) Medium model at 10% failure rate. Figure 3 Convergence of small and medium model sizes under 10% failure rate. Table 2 Performance of three different recovery strategies for three different failure rates. Train time corresponds to the wall-clock necessary to reach validation loss of under 2.85. Failure rate Checkpointing 10% 16% 5% Redundant Comp. 10% 16% 5% Iteration time (s) Train time (h) 91.4 558.2 91.4 621. 92.1 634.4 151.0 419.6 151.0 419.6 151.0 419.6 5.1. Performance with failures CheckFree 10% 91.3 405.9 5% 91.3 367.8 16% 92.1 563.0 CheckFree+ 10% 16% 5% 91.3 355.1 91.3 367.8 92.1 460.6 Here we evaluate the training throughput of different recovery strategies - checkpointing, redundant computation, CheckFree and CheckFree+. In these experiments we focus on the 500M models, splitting them across 7 stages - 1 hosting the embedding and deembedding layers and stages each holding the same number of transformer blocks (per experiment these are detailed in Appendix A.1). All nodes, except for those in the first stage (holding and 1) can fail. We perform the throughput tests for 500 iterations, simulating the failures of different stages across iterations, so that the failure patterns between tests are the same. In Table 2, we report iteration and train time (how long until given validation loss is reached). The reported train time is the wall clock time needed to reach the \"converged\" iterations reported in Fig. 3. We observe similar iteration time as checkpointing as the frequency of checkpointing is high enough that it does not impact iteration time. Combining the number of iterations needed to reach the convergence Fig. 3, checkpointing experiences significantly higher train time due to the need to restart training. CheckFree+ requires higher number than redundant computing and lower number than checkpointing. Due to lower iteration time for CheckFree+ and CheckFree, the resulting tain time of redundant computing is higher. Thus, our solution demonstrates clear improvement in performance, being 12% faster at 5% failure rate than redundant computation and significantly faster than checkpointing. Additionally, our method has better scalability than redundant computation in low-bandwidth networks and higher models, due to the minimal overhead. Also, in case of stage failure, the recovery time of that stage is around 30 seconds. 5.2. Ablation Study Here we investigate the effects of several components individually on models convergence with various hyperparameters and failure rates. Varying model sizes. We empirically demonstrate the convergence of our recovery method on three 8 All is Not Lost: LLM Recovery without Checkpoints (a) Varying failure frequencies. (b) Varying checkpointing frequencies. Figure 4 Experimental results on varying failure and checkpointing frequencies for CheckFree+ on medium model. The convergence is presented over the number of iteration, not wall-clock time. (a) Convergence of large models at 16% failure rate. (b) Comparison of convergence of CheckFree+ to method without swaps in no failure setting. Figure 5 Convergency on large model and the overhead study of swapping in CheckFree+. models and dataset pairs - (small) 120M LLaMa with TinyStories Eldan and Li (2023), (medium) 500M LLaMa with the OpenWebText dataset Gokaslan et al. (2019), and (large) 1.5B LLaMa with the RedPyjamas dataset Weber et al. (2024). For the small and medium experiments we fix the crash rate at 10%. For the large ones we use crash rate of 16% to demonstrate that even at highly challenging churn rates our method can produce viable results. We evaluate 4 recovery strategies - checkpointing, redundant computation, CheckFree and CheckFree+. We present the results of the small and medium models in Fig. 3. These figures demonstrate that for different model sizes, our solution is superior to checkpointing convergence-wise and can still converge. Note that the figures plot over iteration count, not wall-clock time. As such, despite redundant computation converging faster in terms of iterations, they incur high overhead, which decreases their throughputs. The results of the large model are presented in Fig. 5a, as they werent trained to convergence for CheckFree. While our solution does converge slower iteration-wise than redundant computation, it significantly outperforms it wall-clock-wise. Varying failure frequency. One important question to consider is how our method scales with different failure frequencies. Here we investigate this by repeating the tests on the medium (500M) model in 3 settings - 5%, 10%, and 16% stage failure chance. We summarize the results in Fig. 4a. As expected, CheckFree+ performs the best in low-failure settings. Yet, it can be seen that the performance (validation loss) slightly degrades even when the failure rate is tripled, which demonstrates the robustness of our recovery method. 9 All is Not Lost: LLM Recovery without Checkpoints Table 3 Evaluation perplexity on 1.5B models trained for 38K iterations. Perplexity OpenWebText Common Crawl Stack Exchange Arxiv Redundant Computation 15.223 11.13 19.38 10.84 CheckFree 15.114 12.25 19.41 11.67 Checkpointing frequency. Our initial comparison against checkpointing assumes checkpoint roughly every 100 iterations (corresponding to around every 3 hours Scao et al. (2022)). higher checkpointing rate is expected to yield better convergence results (due to the significantly smaller loss of training) but incurs higher communication overhead to send the model weights to the external non-faulty storage. Here, we investigate the empirical tradeoff of checkpointing strategies, against the proposed CheckFree+. We repeat the experiments of the medium model, comparing against 3 checkpointing frequencies - every 10, every 50, and every 100 iterations, at 10% failure chance. The results are plotted in Fig. 4b. We observe that our solution outperforms even high frequency checkpointing (every 10 iterations) case, due to the need to rollback the model after every failure with checkpointing. Effects of swapping on convergence. While CheckFree+ performs well in settings with failures, it incurs non-negligible affect to its convergence in 0% failure chance, due to the swapping. Here we empirically quantify this effect on medium model. We compare the convergence of model trained with and without swapping. The results are plotted in Fig. 5b. We observe significant slowdown in convergence when swapping is used. 5.3. Model evaluation We evaluate the large model trained via our recovery method against model trained by traditional training without failures (equivalent to redundant computation in convergence), both trained to the same iteration count. We evaluate the perplexity of the foundational models across four different datasets - OpenWebText Gokaslan et al. (2019) and Common Crawl, Stack Exchange, and Arxiv as present in Computer (2023). We present the results in Table 3. We see similar performance between model trained with no faults and with our recovery strategy, despite the drastically different resultant weights. The difference in performance could be improved with further training, as redundant computation requires much higher wall-clock time to reach the same iteration count. 6. Conclusion Failure recovery is critical for enabling training of LLMs in distributed setting on wimpy computing nodes with relatively high failure rates. Prior art recovers stage failures within pipeline through redundant computation or checkpointing. We propose CheckFreea recovery algorithm which recovers stage failures by reinitializing the failed (or lost) weights with the weighted average of the weights of the neighboring stages. We further extend the protocol with CheckFree+ to tolerate first and last stage failures, via out of order pipelining. CheckFree exploits the inherent resilience of LLMs to layer omissions and efficient layer initialization techniques to recover from challenging stage loss faults, while requiring no non-faulty storage and no redundant computation. We extensively demonstrate the convergence of our method via empirical tests. We evaluate the effectiveness of CheckFree on LLaMa models of different sizes, showing that such lightweight recovery reduces the training time compared to the state of the art in 5% stage churn rate by over 12%. CheckFree+ also exhibits robust and steady convergence results for varying failure frequencies. 10 All is Not Lost: LLM Recovery without Checkpoints While we assume that stage failures occur uniformly across stages with pre-specified failure rate, the proposed solutions CheckFree and CheckFree+ unfortunately cannot recover from cases of consecutive stages failing together, as there is no neighboring stages for the reinitialization strategy. We believe that extending our methods with lightweight checkpointing scheme will address this limitation and will explore this extension in our future work. Moreover, we will work on improving the convergence of our methods (especially CheckFree+) in non-faulty case to reduce the number of iterations required for training."
        },
        {
            "title": "References",
            "content": "Naman Agarwal, Pranjal Awasthi, Satyen Kale, and Eric Zhao. Stacking as accelerated gradient descent. arXiv preprint arXiv:2403.04978, 2024. Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, and Andreas Veit. Understanding robustness of transformers for image classification. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 1021110221. IEEE, 2021. doi: 10.1109/ICCV48922.2021.01007. URL https://doi.org/10.1109/ICCV48922.2021.01007. Nikolay Blagoev, Lydia Yiyu Chen, and Oguzhan Ersoy. Skippipe: Partial and reordered pipelining framework for training llms in heterogeneous networks. CoRR, abs/2502.19913, 2025. doi: 10.48550/ARXIV.2502.19913. URL https://doi.org/10.48550/arXiv.2502.19913. Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data. Wenyu Du, Tongxu Luo, Zihan Qiu, Zeyu Huang, Yikang Shen, Reynold Cheng, Yike Guo, and Jie Fu. Stacking your transformers: closer look at model growth for efficient LLM pre-training. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/ paper/2024/hash/143ea4a156ef64f32d4d905206cf32e1-Abstract-Conference.html. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, AurÃ©lien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste RoziÃ¨re, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, GrÃ©goire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https://doi.org/10.48550/arXiv.2407.21783. Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english? CoRR, abs/2305.07759, 2023. doi: 10.48550/ARXIV.2305.07759. URL https://doi.org/10.48550/ arXiv.2305.07759. Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed Aly, Beidi Chen, and Carole-Jean Wu. Layerskip: Enabling early exit inference and self-speculative decoding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long 11 All is Not Lost: LLM Recovery without Checkpoints Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1262212642. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.681. URL https://doi.org/10.18653/v1/2024. acl-long.681. Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 32593269. PMLR, 2020. URL http://proceedings.mlr.press/v119/frankle20a.html. Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http://Skylion007. github.io/OpenWebTextCorpus, 2019. Dylan Hillier, Leon Guertler, Cheston Tan, Palaash Agrawal, Ruirui Chen, and Bobby Cheng. Super tiny language models. CoRR, abs/2405.14159, 2024. doi: 10.48550/ARXIV.2405.14159. URL https://doi.org/10. 48550/arXiv.2405.14159. Insu Jang, Zhenning Yang, Zhen Zhang, Xin Jin, and Mosharaf Chowdhury. Oobleck: Resilient distributed training of large models using pipeline templates. In Jason Flinn, Margo I. Seltzer, Peter Druschel, Antoine Kaufmann, and Jonathan Mace, editors, Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023, pages 382395. ACM, 2023. doi: 10.1145/3600006.3613152. URL https://doi.org/10.1145/3600006.3613152. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: In 8th International Conference on lite BERT for self-supervised learning of language representations. Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=H1eA7AEtvS. Yuhang Liang, Xinyi Li, Jie Ren, Ang Li, Bo Fang, and Jieyang Chen. Attnchecker: Highly-optimized fault tolerant attention for large language model training. In Proceedings of the 30th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming, PPoPP 2025, Las Vegas, NV, USA, March 1-5, 2025, pages 252266. ACM, 2025. doi: 10.1145/3710848.3710870. URL https://doi.org/10.1145/3710848.3710870. Xiaolong Ma, Minghai Qin, Fei Sun, Zejiang Hou, Kun Yuan, Yi Xu, Yanzhi Wang, Yen-Kuang Chen, Rong Jin, and Yuan Xie. Effective model sparsification by scheduled grow-and-prune methods. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=xa6otUDdP2W. Sashank J. Reddi, Sobhan Miryoosefi, Stefani Karp, Shankar Krishnan, Satyen Kale, Seungyeon Kim, and Sanjiv Kumar. Efficient training of language models using few-shot learning. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. Max Ryabinin and Anton Gusev. Towards crowdsourced training of large neural networks using decentralized mixture-of-experts. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and HsuanTien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings. neurips.cc/paper/2020/hash/25ddc0f8c9d3e22e03d3076f98d83cb2-Abstract.html. Max Ryabinin, Tim Dettmers, Michael Diskin, and Alexander Borzunov. SWARM parallelism: Training large models can be surprisingly communication-efficient. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 2941629440. PMLR, 2023. URL https://proceedings.mlr.press/v202/ryabinin23a.html. Nikunj Saunshi, Stefani Karp, Shankar Krishnan, Sobhan Miryoosefi, Sashank Jakkam Reddi, and Sanjiv Kumar. On the inductive bias of stacking towards improving reasoning. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/ paper/2024/hash/837bc5db12f3d394d220815a7687340c-Abstract-Conference.html. 12 All is Not Lost: LLM Recovery without Checkpoints Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman CastagnÃ©, Alexandra Sasha Luccioni, FranÃ§ois Yvon, Matthias GallÃ©, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, BenoÃ®t Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo LaurenÃ§on, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. BLOOM: 176b-parameter openaccess multilingual language model. CoRR, abs/2211.05100, 2022. doi: 10.48550/ARXIV.2211.05100. URL https://doi.org/10.48550/arXiv.2211.05100. John Thorpe, Pengzhan Zhao, Jonathan Eyolfson, Yifan Qiao, Zhihao Jia, Minjia Zhang, Ravi Netravali, and Guoqing Harry Xu. Bamboo: Making preemptible instances resilient for affordable training of large dnns. In Mahesh Balakrishnan and Manya Ghobadi, editors, 20th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2023, Boston, MA, April 17-19, 2023, pages 497513. USENIX Association, 2023. URL https://www.usenix.org/conference/nsdi23/presentation/thorpe. Andreas Veit, Michael J. Wilber, and Serge J. Belongie. Residual networks behave like ensembles of relatively shallow networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 550558, 2016. URL https://proceedings. neurips.cc/paper/2016/hash/37bc2f75bf1bcfe8450a1a41c200364c-Abstract.html. Zhuang Wang, Zhen Jia, Shuai Zheng, Zhen Zhang, Xinwei Fu, T. S. Eugene Ng, and Yida Wang. GEMINI: fast failure recovery in distributed training with in-memory checkpoints. In Jason Flinn, Margo I. Seltzer, Peter Druschel, Antoine Kaufmann, and Jonathan Mace, editors, Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023, pages 364381. ACM, 2023. doi: 10.1145/3600006.3613145. URL https://doi.org/10.1145/3600006.3613145. Maurice Weber, Daniel Y. Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher RÃ©, Irina Rish, and Ce Zhang. Redpajama: an open dataset for training large language models. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/ hash/d34497330b1fd6530f7afd86d0df9f76-Abstract-Datasets_and_Benchmarks_Track.html. Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba SzepesvÃ¡ri, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 2396523998. PMLR, 2022. URL https://proceedings. mlr.press/v162/wortsman22a.html. Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri Dao, Beidi Chen, Percy Liang, Christopher RÃ©, and Ce Zhang. Decentralized training of foundation models in heterogeneous environments. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/ paper/2022/hash/a37d615b61f999a5fa276adb14643476-Abstract-Conference.html. Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft& verify: Lossless large language model acceleration via self-speculative decoding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1126311282. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.607. URL https: //doi.org/10.18653/v1/2024.acl-long.607. All is Not Lost: LLM Recovery without Checkpoints Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: open pre-trained transformer language models. CoRR, abs/2205.01068, 2022. Hanhan Zhou, Tian Lan, Guru Venkataramani, and Wenbo Ding. Every parameter matters: Ensuring the convergence of federated learning with dynamic heterogeneous models reduction. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/ paper/2023/hash/526356453b7301c9b29aa0533f62bdef-Abstract-Conference.html. 14 All is Not Lost: LLM Recovery without Checkpoints A. Reproducibility information This section describes relevant information for reproducing our results. A.1. Models We train three different model sizes, all of the LLaMa family. We provide details of their hyperparameters in Table 4. Table 4 Model hyperparameters. Size Parameters Dim Heads Layers Stages Context Learning rate Small Medium 124M 512 8 12 4 512 6 10 500M 1024 16 24 6 1024 3 104 Large 1.5B Elhoushi et al. (2024) 2048 16 24 6 4096 3 104 A.2. Optimizers All tests were performed with the Adam optimizer with no weight decay and betas (0.9,0.999) A.3. Datasets TinyStories Eldan and Li (2023) - Available with license CDLA-sharing-1.0. 100 texts were reserved for validation. Used for the small models. OpenWebText Gokaslan et al. (2019) - Available with license Creative Commons Zero. 100 texts were reserved for validation. Used for the medium models. RedPajamas v2 Weber et al. (2024) - Available with license Apache-2.0 license. 20 texts were reserved for validation. Used for the large models. A.4. Hardware For convergence tests we run on 2, 4, and 8 H100s for the small, medium, and large models respectively. We simulate the faults on recoveries across nodes without actual communication (other than data-parralel aggregation) to speed up iteration time. This has no difference from test performed on geo-distributed nodes, except that runtime will be longer wall-clock wise due to the communication overhead. Small models converged in roughly 8 hours, medium - in 2 days, and large models - in 2 weeks. For throughput results we test on 8 H100s spawning 20 separate nodes. Communication between all 20 nodes is simulated based on realistic latency and bandwidth taken from profiling Google Cloud nodes in 5 different locations. Thus our results on throughput accurately reflect real deployment situation on geo-distributed nodes. A.5. Different initialization strategies For these experiments we trained the medium models on the OpenWebText dataset, allowing for any stage (but those holding embedding and de-embedding) to crash with probability of 16%. We test three All is Not Lost: LLM Recovery without Checkpoints different strategies - random (randomly initializing the new strategy), copying (copying the previous layer), and weighted gradient averaging. B. Extended Proof From the assumptions given in the main body, it follows: Fğ‘¡ ğ‘¡ 2 = Fğ‘¡ ğ‘š Fğ‘¡ + ğ‘š Fğ‘¡ ğ‘¡ 2 Where ğ‘š this time selects all layers that are the same (non-failed ones). Fğ‘¡ ğ‘š Fğ‘¡ + ğ‘š Fğ‘¡ ğ‘¡ 2 ğ‘¡ 2) ğ›¿(ğœ”1 ğ‘“ğ‘˜+1 + ğœ”2 ğ‘“ğ‘˜12 ğ‘“ğ‘˜2) 1Fğ‘¡ 2 ğ›¿2 ğ‘¡ 2 ğ›¿ 2F ğ›¿(Fğ‘¡ 2 We can divide the training past random failure of model F0 into two parts - standard optimization with the modified model and post-recovery reduction error: ğ‘¡ ğ”¼F ğ‘¡1 F02 = ğ‘¡ ğ”¼F ğ‘¡1 0 + 0 F02 ğ‘¡ ğ”¼F ğ‘¡1 0 2 + ğ‘¡ ğ”¼F 0 F02 The left hand side is standard optimization problem. This depends on the optimizer, but here we will asusme it convergences inversely proportional to ğ‘¡ - ( 1 ğ‘¡ ). The right hand is the error due to replacement of stages. What this tells us is that if the error is small, the convergence is not as affected (which intuitively makes sense). The right side can be rewritten as: ğœ”1 ğ‘“ğ‘˜+1 + ğœ”2 ğ‘“ğ‘˜1 ğ‘“ğ‘˜2, thus: ğ‘¡ ğ”¼F ğ‘¡1 F02 ( 1 ğ‘¡ ) + ğ‘¡ 2ğ”¼ğœ”1 ğ‘“ğ‘˜+1 + ğœ”2 ğ‘“ğ‘˜1 ğ‘“ğ‘˜2 QED."
        }
    ],
    "affiliations": [
        "Gensyn",
        "TU Delft",
        "University of Neuchatel"
    ]
}