{
    "paper_title": "Temporal Pair Consistency for Variance-Reduced Flow Matching",
    "authors": [
        "Chika Maduabuchi",
        "Jindong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Continuous-time generative models, such as diffusion models, flow matching, and rectified flow, learn time-dependent vector fields but are typically trained with objectives that treat timesteps independently, leading to high estimator variance and inefficient sampling. Prior approaches mitigate this via explicit smoothness penalties, trajectory regularization, or modified probability paths and solvers. We introduce Temporal Pair Consistency (TPC), a lightweight variance-reduction principle that couples velocity predictions at paired timesteps along the same probability path, operating entirely at the estimator level without modifying the model architecture, probability path, or solver. We provide a theoretical analysis showing that TPC induces a quadratic, trajectory-coupled regularization that provably reduces gradient variance while preserving the underlying flow-matching objective. Instantiated within flow matching, TPC improves sample quality and efficiency across CIFAR-10 and ImageNet at multiple resolutions, achieving lower FID at identical or lower computational cost than prior methods, and extends seamlessly to modern SOTA-style pipelines with noise-augmented training, score-based denoising, and rectified flow."
        },
        {
            "title": "Start",
            "content": "Temporal Pair Consistency for Variance-Reduced Flow Matching Chika Maduabuchi 1 Jindong Wang 1 6 2 0 2 4 ] . [ 1 8 0 9 4 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Continuous-time generative modelssuch as diffusion models, flow matching, and rectified flowlearn time-dependent vector fields but are typically trained with objectives that treat timesteps independently, leading to high estimator variance and inefficient sampling. Prior approaches mitigate this via explicit smoothness penalties, trajectory regularization, or modified probability paths and solvers. We introduce Temporal Pair Consistency (TPC), lightweight variance-reduction principle that instead couples velocity predictions at paired timesteps along the same probability path, operating entirely at the estimator level without modifying the model architecture, probability path, or solver. We provide theoretical analysis showing that TPC induces quadratic, trajectory-coupled regularization that provably reduces gradient variance while preserving the underlying flow-matching objecInstantiated within flow matching, TPC tive. improves sample quality and efficiency across CIFAR-10 and ImageNet at multiple resolutions, achieving lower FID at identical or lower computational cost than prior methods, and extends seamlessly to modern SOTA-style pipelines with noise-augmented training, score-based denoising, and rectified flow. 1. Introduction Continuous-time generative models have become popular framework for high-fidelity image synthesis, encompassing both diffusion models (DMs) and deterministic continuous normalizing flows (NFs). DMs have achieved state-of-theart performance across wide range of benchmarks, benefiting from stable training objectives and well-understood stochastic dynamics (Ho et al., 2020; Rombach et al., 2022; Yang et al., 2023), but are often at the cost of long sampling 1William & Mary. <jdw@wm.edu>. Correspondence to: Jindong Wang Preprint. February 6, 2026. 1 Figure 1. Sample quality vs. sampling efficiency. Frechet Inception Distance (FID ) versus number of function evaluations (NFE, log scale) on CIFAR-10. Temporal Pair Consistency (TPC) consistently shifts the qualityefficiency frontier by suppressing temporal oscillations in the learned vector field, achieving lower FID at identical or lower computational cost without modifying the underlying model or solver. trajectories and substantial computational overhead. In parallel, recent work has revisited deterministic formulations based on ordinary differential equations, demonstrating that carefully designed probability pathssuch as those used in flow matching (FM) (Lipman et al., 2023) and rectified flow (Liu et al., 2023)can offer greater flexibility and improved sampling efficiency while retaining competitive generation quality (Li et al., 2023). Despite recent progress, existing flow-matchingbased generative models still exhibit important limitations in how temporal dynamics are learned. Standard flow matching trains the velocity field as function of state and time, but does so independently at each time step, without explicitly constraining predictions across nearby times (Lipman et al., 2023; Liu et al., 2023). Intuitively, this independence wastes temporal correlation already present along each probability path: gradients at different timesteps share randomness but are treated as independent noise, inflating estimator variance (Boffi et al., 2025; Albergo & Vanden-Eijnden, 2023). Prior work has observed that this lack of temporal coherence induces curved trajectories in the marginal flow, which in turn lead to elevated gradient variance during training, as we also empirically observe (Figure 3), and Submission and Formatting Instructions for ICML 2026 increased numerical error when coarse time discretizations are used at inference (Geng et al., 2025; Lee et al., 2025; Reu et al., 2025). In practice, this manifests as reduced sample efficiency: achieving high-quality samples requires finer discretization or larger number of function evaluations, even when using identical solvers and probability paths (Esser et al., 2024). Figure 1 provides direct manifestation of this effect: when the vector field is learned independently across time (FM, RF), identical sampling budgets yield substantially worse sample quality, whereas temporal pair consistency achieves markedly lower FID at the same number of function evaluations. In this work, we introduce Temporal Pair Consistency Flow Matching (TPC-FM), variance-reduced formulation of flow matching that explicitly enforces temporal coherence in the learned velocity field. Prior work on continuoustime generative models has explored temporal regularization through path-length penalties, Jacobian constraints, straighttrajectory objectives, or solverand architecture-level design choices to improve stability and efficiency (Grathwohl et al., 2019a; Kidger, 2022; Liu et al., 2023; Ma et al., 2025; Geng et al., 2025). While effective, these approaches typically modify the function class, probability path, or inference procedure. TPC instead exploits structural property of standard flow matching (Lipman et al., 2023): during training, velocity predictions at different timesteps along the same probability path are learned independently, despite being strongly correlated by construction. Directly enforcing temporal smoothness is nontrivial in this setting, since the target velocity field is defined only implicitly through stochastic path samples rather than an explicit time-continuous objective (Albergo & Vanden-Eijnden, 2023; Boffi et al., 2025). We show that this independence can be addressed by pairing timesteps sampled along the same path and encouraging consistency between their corresponding velocity predictions, yielding lightweight, self-supervised regularization that operates entirely within the existing flowmatching objective. By coupling stochastic evaluations across timewithout altering the probability path, neural architecture, or solverTPC stabilizes optimization, substantially reduces training-time variance, and improves sample efficiency in practice, complementing recent advances in probability-path design and high-resolution flow-based generation (Esser et al., 2024; Ma et al., 2025; Zhai et al., 2025). TPC-FM supports both simple and adaptive mechanisms for constructing temporal pairs. We show in Figure 3 that fixed antithetic pairing strategy, which symmetrically couples early and late timesteps along the probability path, already yields substantial variance reduction, analogous to classical antithetic sampling in Monte Carlo estimation (Ren et al., 2019). To further adapt temporal coupling to the data and model, we additionally introduce learnable monotone pairing function that discovers effective temporal correspondences while preserving the ordered structure of the path. Temporal consistency is applied stochastically during training, ensuring that TPC-FM functions as variancereduction mechanism rather than hard constraint. Because the formulation relies only on paired evaluations of the same probability path, it applies seamlessly to existing FM frameworks and remains compatible with recent deterministic path constructions such as rectified flow (Liu et al., 2023). We evaluate TPC-FM across spectrum of widely adopted image generation benchmarks that reflect both controlled analysis settings and modern high-resolution evaluation protocols. Specifically, we consider unconditional generation on CIFAR-10 (Krizhevsky & Hinton, 2009) and ImageNet (Krizhevsky et al., 2012) at resolutions up to 128 128, which remain the standard benchmarks for assessing optimization behavior, sample efficiency, and likelihoodquality trade-offs in continuous-time generative models, including recent diffusion and flow-matching frameworks (Karras et al., 2022; Yang et al., 2025; Ifriqi et al., 2025; Zhai et al., 2025). To assess practical relevance at modern resolutions, we further evaluate TPC-FM under noise-augmented training with score-based denoising on conditional ImageNet-64 and ImageNet-128, following the evaluation protocols used by recent state-of-the-art diffusion (Karras et al., 2022) and flow-based models (Zhai et al., 2025). Across diffusion-based, flow-matching, and rectifiedflow formulations, TPC-FM consistently improves sample quality and sampling efficiency without introducing additional architectural complexity or inference cost. In the rectified-flow setting, temporal pair consistency complements trajectory straightening, yielding improved performance under both one-step and full-simulation regimes. Ablation studies further demonstrate that these gains are robust across pairing strategies and hyperparameter choices, confirming temporal coherence as key factor in stabilizing and accelerating flow-based generative modeling. Contributions. Our contributions are threefold: (i) We introduce Temporal Pair Consistency (TPC), general variance-reduction principle for flow matching that enforces temporal coherence in the learned velocity field by coupling stochastic evaluations across time along the same probability path. TPC operates entirely within the standard flow-matching objective, without modifying probability paths, solvers, or model architectures. We provide theoretical analysis that formalizes TPC as quadratic, trajectory-coupled regularizer and establishes contraction and variancereduction guarantees. (ii) We present practical instantiations of TPC using both fixed and learnable timestep pairing mechanisms, and show that temporal coupling can be incorporated without altering the underlying training loss or sampling 2 Submission and Formatting Instructions for ICML 2026 procedure. We further empirically demonstrate that TPC substantially reduces training-time variance in the learned vector field, by coupling temporally correlated gradients. (iii) We demonstrate that TPC consistently improves sample quality and sampling efficiency across multiple continuous-time generative frameworks, including flow matching and rectified flow, on widely adopted image generation benchmarks. In particular, TPC yields gains under both standard probability-flow sampling and modern SOTA-style pipelines with noiseaugmented training and score-based denoising. 2. Related Work Temporal and trajectory regularization in continuoustime models. TPC is related at high level to prior work on temporal smoothness, consistency regularization, and trajectory regularization in continuous-time generative models. Several approaches explicitly regularize the dynamics of learned ODEs by penalizing path length, kinetic energy, or Jacobians of the velocity field, often to improve numerical stability or invertibility in continuous normalizing flows (Chen et al., 2018; Grathwohl et al., 2019b; Kidger, 2022; Yang et al., 2025). Related ideas appear in diffusion models, where smoothness of the score or drift over time is implicitly encouraged through architectural design or discretization choices (Song et al., 2021b; Karras et al., 2022), as well as in recent work that explores straighttrajectory learning through latent-augmented or variational formulations of flow matching (Ma et al., 2025). However, there are several important distinctions. First, TPC does not impose an explicit smoothness constraint on the velocity field with respect to time, nor does it regularize Jacobians or higher-order derivatives of the learned dynamics, unlike pathor Jacobian-based regularization methods (Grathwohl et al., 2019b; Kidger, 2022). Instead, it operates directly on paired stochastic evaluations of the flow-matching objective (Lipman et al., 2023), coupling velocity predictions at two timesteps sampled along the same probability path. As result, TPC primarily targets the variance of the stochastic gradient estimator, rather than imposing deterministic regularizer on the learned dynamics or function class. tion (Lu & Song, 2025; Issenhuth et al., 2024). In contrast, TPC does not enforce invariance of model outputs or define fixed-point mapping across time; instead, it couples stochastic evaluations of the existing flow-matching objective to reduce estimator variance during training, while preserving the underlying continuous-time dynamics. Relation to flow matching and rectified flow. Finally, TPC is complementary to recent advances in designing efficient probability paths for continuous-time generative modeling, including flow matching (Lipman et al., 2023), rectified flow (Liu et al., 2023), and temporally structured or expansive flow constructions (Ifriqi et al., 2025). While these methods focus on the choice of probability paths and the geometry of trajectories, TPC addresses different aspect: the temporal structure of the training objective itself. Because TPC relies only on paired evaluations of the existing path sampler, it applies seamlessly to different probability paths and formulations, including both flow matching and rectified flow, without modifying the underlying generative model, solver, or sampling procedure. These distinctions position TPC as lightweight and general mechanism for improving optimization stability and sample efficiency, rather than as an alternative probability path or solver design. 3. Method Flow matching learns time-indexed vector field that transports simple reference distribution into the target data distribution by integrating an ordinary differential equation. In this section, we introduce Temporal Pair Consistency Flow Matching (TPC-FM), variance-reduced formulation of flow matching that enforces temporal coherence between velocity predictions at paired timesteps. Importantly, TPC does not impose an explicit smoothness or Jacobian penalty on the velocity field; instead, it reduces stochastic gradient variance by coupling paired evaluations of the existing flow-matching objective. The key idea is to couple training examples across time in order to stabilize gradient estimates, improve sample efficiency, and reduce the mismatch between adjacent predictions of the velocity field. We begin by revisiting the flow-matching formulation 3.1, then introduce temporal pairs 3.2, describe fixed and learnable pairing mechanisms 3.3, offer theoretical insights 3.5, and conclude with the full training objective 3.6. Consistency regularization. TPC is also superficially related to consistency regularization techniques widely used in supervised and self-supervised learning, where model predictions are encouraged to be invariant under perturbations or augmentations (Laine & Aila, 2017; Sohn et al., 2020). Recent work has extended this idea to generative modeling through consistency models, which enforce output-level agreement across noise levels to enable one-step genera3.1. Conditional Flow Matching Let x0 p0 denote base distribution (e.g., (0, I)) and x1 p1 the data distribution. probability path {pt}t[0,1] is induced by conditional map xt = Φt(x0, x1), with associated velocity ut(xt) = tΦt(x0, x1). Flow matching parameterizes velocity field vθ(x, t) and minimizes LFM(θ) = Etρ, (x0,x1) (cid:13)vθ(xt, t) ut(xt)(cid:13) (cid:13) 2 2, (cid:13) (1) 3 Submission and Formatting Instructions for ICML 2026 where ρ is typically uniform on [0, 1]. Eq. (1) performs independent L2(pt) regressions across t, with no coupling between vθ(, t) and vθ(, t). 3.2. Temporal Pairing for Variance Reduction Temporal Pair Consistency (TPC) reduces variance in flow matching by coupling stochastic evaluations at two timesteps sampled along the same probability path. Intuition. In standard flow matching (Lipman et al., 2023), velocity predictions at different timesteps are trained independently, even when they lie on the same probability path and share endpoint randomness. As result, the corresponding stochastic gradients are strongly correlated but treated as independent noise, leading to unnecessarily high estimator variance (Figure 3). TPC exploits this shared randomness by pairing timesteps and enforcing consistency between their velocity predictions, effectively yielding control-variate estimator that cancels temporal noise while preserving the original objective. Formally, for ρ, let = ψ(t) and draw common endpoint pair (x0, x1). The induced flow-matching states (xt, ut) = P(x0, x1, t) and (xt, ut) = P(x0, x1, t) yield velocity predictions vt = vθ(xt, t) and vt = vθ(xt, t). Standard flow matching minimizes Evt ut2 2 independently across t. TPC instead augments this objective with paired estimator ℓTPC(t, t) = vt ut2 2 + vt ut2 2 + λvt vt2 2, which explicitly couples evaluations across time while preserving the underlying probability path and solver. By increasing correlation between paired gradient estimates through shared endpoint randomness (x0, x1), this construction yields strict variance reduction via control-variate effect and induces early collapse of training variance. Algorithm 1 describes practical implementations, including fixed and learned pairing rules and stochastic gating. 3.3. Pairing Mechanisms Temporal Pair Consistency is defined by pairing operator ψ : [0, 1] [0, 1] mapping primary timestep ρ to an auxiliary timestep = ψ(t). Given shared endpoints (x0, x1), pairing induces coupled flowmatching states (xt, ut) = P(x0, x1, t) and (xt, ut) = P(x0, x1, t), whose joint evaluation increases correlation between stochastic gradients and enables variance reduction. Fixed antithetic pairing. canonical choice is the antithetic map ψfix(t) = 1 t, which pairs early and late times along the same probability path. For commonly used symmetric interpolants Φt (e.g., linear or affine paths), the joint law of (xt, x1t) exhibits time-reversal symmetry (Lipman et al., 2023), making (t, 1 t) an antithetic pair in the sense of classical Monte Carlo variance reduction (Ren et al., 2019). As result, the paired gradients g(t, ξ) and g(1t, ξ) tend to be negatively correlated, yielding reduced estimator variance without introducing additional parameters. Learned monotone pairing. To allow adaptive pairings beyond fixed symmetry, we introduce learnable map ϕ : [0, 1] [0, 1] with = ϕ(t) and impose the structural constraint ϕ(t) 0 to preserve temporal order. We pai=1 ai σ(t + bi) + c(cid:1), where rameterize ϕ as ϕ(t) = σ(cid:0) (cid:80)H σ denotes the sigmoid, ai = softplus(ai) > 0 ensures nonnegative slope contributions, bi control transition locations, and sets global offset. single hidden layer suffices to approximate any monotone function on [0, 1] while keeping ϕ low-capacity, consistent with its role as structural estimator component rather than predictive model. Monotonicity is weakly enforced during optimization by penalizing order violations on grid {gk = k/K}K k=1, via r(ϕ) = (cid:80)K1 k=1 1{ϕ(gk+1) < ϕ(gk)}, where 1{} denotes the indicator function and is small constant (K = 32). This regularizer biases ϕ toward monotone solutions while retaining sufficient flexibility for data-dependent pairing. Both ψfix and ϕ are implemented within the same paired estimator and training loop (Algorithm 1). 3.4. Temporal Consistency Objective Temporal Pair Consistency augments the conditional flowmatching risk by introducing quadratic coupling between velocity evaluations at paired times along shared probability path. Let ρ, = ψ(t), and (xt, ut), (xt, ut) = P(x0, x1, t), (x0, x1, t) denote path-sampled states with predictions vt = vθ(xt, t) and vt = vθ(xt, t). The temporal consistency penalty is defined pointwise as ℓTPC(t, t) := vt vt2 2, which enforces coherence of vθ across paired evaluations sharing the same endpoint randomness (x0, x1). For single sample, the resulting objective takes the form L(θ) = vt ut2 2 + λtpcvt vt2 2 + λmono r(ϕ), At where λtpc, λmono 0. the population level, this corresponds to minimizing Tikhonov-regularized 2 + λtpcEt,tvθ(xt, t) risk Et,(x0,x1)vθ(xt, t) ut2 vθ(xt, t)2 2, which selects, among near-minimizers of the unregularized flow-matching objective, velocity fields with reduced temporal oscillation along path-coupled states. From an optimization perspective, the paired term induces correlation between stochastic gradients g(t, ξ) = θvt ut2 2 under shared randomness ξ = (x0, x1), yielding control-variate effect and strict variance reduction. This coupling operates entirely at the 2 and g(t, ξ) = θvt ut2 4 Submission and Formatting Instructions for ICML estimator level and preserves the underlying conditional flow-matching formulation, as formalized in Section 3.5. at fixed step size (or reducing the required NFE for target error). Stochastic gating. To avoid over-regularization and ensure that temporal coupling functions as variancereduction mechanism rather than dominant bias, we apply TPC through gated estimator. Let Bernoulli(ptpc) and define the gated loss ℓTPC(t, t) = ℓTPC(t, t). The resulting objective satisfies E[ℓTPC] = ptpcℓTPC while preserving stochastic exposure to the unregularized flowmatching gradient. This randomized coupling stabilizes optimization, induces early variance collapse, and maintains expressive flexibility of the learned velocity field. Algorithm 1 summarizes the complete training procedure. 3.5. Theoretical Analysis The following analysis (Appendix A) formalizes the intuition (Section 3.2), showing that temporal pairing induces control-variate estimator with strictly reduced gradient variance. TPC as regularized population objective. Let xt = Φt(x0, x1) be the path-sampled state and ut(x0, x1) the conditional FM target. Standard FM minimizes R(v) = Ev(xt, t) ut2 2, whose population minimizer is the conditional expectation (an L2 projection). TPC augments TPC = Ev(xt, t) FM with the coupled penalty v2 v(xt, t)2 2 for paired = ψ(t), yielding Rλ(v) = R(v) + λv2 TPC. This is Tikhonov-regularized risk that selects, among near-minimal FM solutions, predictors with reduced temporal oscillation along path-coupled states and satisfies TPC R(v)/λ the deterministic contraction bound vλ2 for any vλ arg min Rλ. Optimization: strict variance reduction via coupling. For the per-sample FM gradient g(t, ξ) = θvθ(xt, t) ut2 2 (with shared randomness ξ generating (x0, x1)), temporal pairing induces control-variate estimator g(t, ξ) αg(t, ξ). Under mild regularity that implies nontrivial positive correlation between paired gradients, the optimal α yields the standard strict reduction Var(g αg) = Var(g)(1 ρ2), where ρ = Corr(g, g). TPC increases this correlation by explicitly enforcing coherence of vθ across paired times, thereby reducing gradient noise. Sampling: improved probabilityflow numerics. Generation integrates the probabilityflow ODE zt = vθ(zt, t). Classical discretization bounds depend not only on Lipschitzness in but also on temporal variation of the vector field. Because vθTPC penalizes temporal roughness along states visited by the path sampler (and locally approximates time-derivative seminorm), TPC reduces effective time-variation on-trajectory, improving numerical stability These results certify TPC as principled mechanismsimultaneously estimator-theoretic (variance reduction), functional-analytic (temporal regularization), and numerical (ODE stability)that explains the empirical improvements observed for flow matching. 3.6. Full Training Objective The complete training procedure combines conditional flow matching, stochastic temporal pairing, and monotonicity regularization (Algorithm 1). For ρ, = ψ(t), and Bernoulli(ptpc), the TPCFM estimator minimizes the joint expectation (cid:104)vθ(xt, t) ut 2 + λtpc vθ(xt, t) vθ(xt, t)2 2 + λmono r(ϕ) (cid:105) . (2) where (xt, ut), (xt, ut) = P(x0, x1, t), (x0, x1, t) are path-sampled states. The first term recovers standard flow matching, while the remaining terms introduce randomized quadratic coupling across paired timesteps and enforce monotonic structure on the pairing map ϕ. Gradients w.r.t. θ arise from both flow-matching and temporal-consistency terms, while ϕ is optimized via the paired consistency loss and monotonicity regularization. Because the objective depends only on paired evaluations of the path sampler P, it applies unchanged to deterministic or stochastic, continuous or discretized probability paths. 4. Experiments 4.1. Setup We evaluate on standard unconditional image generation benchmarks following established protocols in diffusion and continuous-time generative modeling (Ho et al., 2020; Song et al., 2021b; Lipman et al., 2023; Liu et al., 2023), comparing against state-of-the-art diffusionand flow-based models reported on these benchmarks (Dhariwal & Nichol, 2021; Karras et al., 2022). Experiments are conducted on CIFAR-10 and ImageNet at resolutions 3232, 6464, and 128128, enabling direct comparison across diffusion, score-based, flow-matching, and rectified-flow methods. Unless otherwise stated, all models are unconditional and share the same U-Net backbone, architectural capacity, and training budget, ensuring that observed differences arise from the learning objective and probability path rather than model size or optimization artifacts. We report negative log-likelihood (NLL) where applicable, Frechet Inception Distance (FID) for sample quality, and the number of function evaluations (NFE) as measure of sampling efficiency. For flow-based models, samples are generated by solving the 5 Submission and Formatting Instructions for ICML 2026 Algorithm 1 Flow Matching with Temporal Pair Consistency (TPC) Require: Model fθ, path sampler P, pairing function ϕ, TPC probability ptpc, weights λtpc, λmono. 1: Monotone pairing function: 2: ϕ(t) = σ (cid:16)(cid:80)H i=1 ai σ(t + bi) + (cid:17) , where ai = learned = ϕ(t) fixed else = fixed = 1 if fixed pairing then Sample x0 and U(0, 1) (xt, ut) = P(x0, x1, t) vt = fθ(xt, t) LF = vt ut2 Sample Bernoulli(ptpc) if = 1 then softplus( ai) > 0. 3: Fixed pairing: 4: Learned pairing: 5: for each minibatch (x1) do 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: end for 28: return fθ, ϕ end if (xt, ut) = P(x0, x1, t) vt = fθ(xt, t) LT = vt vt2 LT = 0 = learned else end if Construct grid g1, . . . , gK r(ϕ) = (cid:80)K1 = LF + λtpcLT + λmonor(ϕ) Update θ, ϕ by SGD i=1 1[ϕ(gi+1) < ϕ(gi)] learned generative ODE from noise to data: Flow Matching models use adaptive ODE solvers with absolute and relative tolerances set to 105 (Lipman et al., 2023), while Rectified Flow models use adaptive RungeKutta (RK45) solvers for full simulation and fixed-step Euler solvers for few-step, one-step, and distillation settings (Liu et al., 2023). All FID, IS, and recall metrics are computed using standard evaluation protocols over 50k samples. To isolate the effect of temporal structure, we ablate the temporal prior probability ptpc, transition strength λtpc, and monotonicity regularization weight λmono, considering both fixed and learned pairing mechanisms; hyperparameters are selected on held-out validation sets, fixed across datasets, and evaluated across ODE, SDE, and rectified-flow formulations under both one-step and full-simulation regimes. For ImageNet experiments targeting modern SOTA regimes, we additionally evaluate noise-augmented flow matching with score-based denoising at sampling time, following high6 resolution generative pipelines used in prior work (Dhariwal & Nichol, 2021; Song et al., 2021b; Karras et al., 2022), using identical noise configurations for baseline FM and TPC-FM unless otherwise stated to ensure controlled comparisons. 4.2. Unconditional Image Generation Flow Matching Table 1 reports unconditional image generation results on CIFAR-10 and ImageNet at resolutions 3232, 6464, and 128128. Across all datasets and resolutions, TPC-FM consistently improves upon prior flowmatching objectives, achieving lower FID at identical or lower NFE while maintaining competitive likelihoods. On CIFAR-10, TPC-FM reduces FID from 6.35 (FM w/ OT) to 3.19 at the same NFE, matching the best reported NLL among flow-based methods. Similar gains are observed on ImageNet 3232 and 6464, where TPC-FM attains the lowest FID among all compared diffusion and flowmatching models without increased sampling cost. At higher resolution, TPC-FM continues to scale favorably, improving FID on ImageNet 128128 from 20.9 to 18.6 while matching likelihood performance. Notably, these improvements are achieved without additional sampling steps or architectural changes, indicating that temporal pair consistency improves the learned probability paths rather than relying on increased numerical resolution. Rectified Flow Table 2 reports results for rectified flow (RF), probability-flow ODEs, and SDE baselines under one-step and full-simulation regimes. Consistent with prior work (Liu et al., 2023), rectified flows outperform probability-flow ODEs in the one-step setting and achieve competitive qualityefficiency trade-offs under full simulation. Across all rectification depths, applying temporal pair consistency (TPC-RF) yields consistent improvements in FID and recall at identical NFE, both for single-step generation and adaptive RungeKutta simulation. For example, TPC-2RF improves one-step FID from 4.85 to 4.55 while increasing recall, and under full simulation reduces FID from 2.58 to 2.15 without additional function evaluations. These gains indicate that temporal pair consistency improves the quality of the learned vector field itself, complementing rectification without relying on increased solver depth or numerical resolution. 4.3. Extending TPC to Modern SOTA-Style Flow While the preceding sections demonstrate that TPC improves standard flow matching under probabilityflow sampling, this regime alone does not reflect how state-of-the-art generative models are trained and evaluated at modern resolutions. In practice, competitive ImageNet performance relies on noise-augmented training with score-based denoising at sampling time, design shared by modern diffusion Submission and Formatting Instructions for ICML Table 1. Diffusion and Flow-Matching Results on CIFAR-10, ImageNet 3232, ImageNet 6464, and ImageNet 128128. Blank entries indicate methods that do not report unconditional ImageNet results and are evaluated primarily under guided or SOTA-style training; see Tables 3 and 4 for matched comparisons. CIFAR-10 ImageNet 3232 ImageNet 6464 Model NLL FID NFE NLL FID NFE NLL FID NFE DDPM (Ho et al., 2020) Score Matching (Lipman et al., 2023) i-DODE (Zheng et al., 2023b) ScoreFlow (Lipman et al., 2023) FM w/ Diffusion (Lipman et al., 2023) FM w/ OT (Lipman et al., 2023) I-CFM (Tong et al., 2024) OT-CFM (Tong et al., 2024) Discrete FM (Gat et al., 2024) V-RFM (Guo & Schwing, 2025) TPC-FM (Ours) 3.12 3.16 2.56 3.09 3.10 2.99 2.99 7.48 19.94 11.20 20.78 8.06 6.35 3.66 3.58 3.63 3.58 274 242 162 428 183 142 146 134 1024 3.19 142 3.54 3.56 3.69 3.55 3.54 3.53 3.53 6.99 5.68 10.31 14.14 6.37 5.02 4. 262 178 138 195 193 122 122 3.32 3.40 3.36 3.33 3.31 17.36 19.74 24.95 16.88 14.45 3.31 13. 264 441 601 187 138 138 Model (IN 128 128) NLL FID MGAN (Hoang et al., 2018) PacGAN2 (Lin et al., 2018) Logo-GAN-AE (Sage et al., 2018) Self-cond. GAN (Luˇcic et al., 2019) Uncond. BigGAN (Luˇcic et al., 2019) PGMGAN (Armandpour et al., 2021) FM w/ OT (Lipman et al., 2023) 2. 58.9 57.5 50.9 41.7 25.3 21.7 20.9 TPC-FM (Ours) 2.90 18.6 Table 2. Benchmarking TPC on RF, SDE, ODE Methods. Method NFE () IS () FID () Recall () Table 3. Conditional ImageNet 128 128 generation performance (FID ). All methods are evaluated under noise-augmented training with score-based denoising at sampling time. One-Step Generation + Distil (Euler solver, N=1) 1-Rectified Flow 2-Rectified Flow 3-Rectified Flow VP ODE sub-VP ODE 1 1 1 1 9.08 9.01 8.79 8.73 8.80 6.18 4.85 5.21 16.23 14.32 0.45 0.50 0.51 0.29 0.35 Full Simulation (RungeKutta RK45), Adaptive 1-Rectified Flow 2-Rectified Flow 3-Rectified Flow VP ODE sub-VP ODE 127 110 104 140 9.60 9.24 9.01 9.37 9.46 2.58 3.36 3.96 3.93 3.16 Full Simulation (Euler solver, N=2000) VP SDE sub-VP SDE 2000 2000 9.58 9. 2.55 2.61 0.57 0.54 0.53 0.51 0.55 0.58 0.58 One-Step Generation + Distil (Euler solver, N=1) TPC-1RF (ours) TPC-2RF (ours) TPC-3RF (ours) 1 1 9.21 9.14 8.92 5.86 4.55 4.83 0.47 0.53 0.53 Full Simulation (RungeKutta RK45), Adaptive TPC-1RF (ours) TPC-2RF (ours) TPC-3RF (ours) 127 110 9.78 9.42 9.18 2.15 2.95 3.45 0.6 0.58 0.55 Model Model Class FID ADM-G (Dhariwal & Nichol, 2021) CDM (Ho et al., 2022) Simple Diffusion (Hoogeboom et al., 2023) RIN (Jabri et al., 2023) Diff/FM Diff/FM Diff/FM Diff/FM BigGAN (Brock et al., 2019) BigGAN-deep (Brock et al., 2019) TARFLOW (σ = 0.05) (Zhai et al., 2025) TARFLOW (σ = 0.15) (Zhai et al., 2025) FM + noise + denoising (baseline) TPC-FM + noise + denoising (ours) GAN GAN NF NF FM FM 2.97 3.52 1.94 2.75 8.70 5.70 5.29 5.03 6.8 4.9 heuristics. We evaluate this setting on conditional ImageNet generation at 64 64 and 128 128, comparing TPC-FM against diffusion (Dhariwal & Nichol, 2021; Nichol & Dhariwal, 2021), GAN (Brock et al., 2019), normalizing-flow (Zhai et al., 2025), and flow-matching baselines under identical noise-augmented and denoised protocols (Tables 4 and 3). and flow-based models (Dhariwal & Nichol, 2021; Song et al., 2021b; Zhai et al., 2025). As result, gains under clean probabilityflow sampling do not fully characterize practical performance at scale. We therefore evaluate TPC as an orthogonal training principle under noise-augmented flow matching with scorebased denoising, preserving the underlying flow-matching objective while matching modern SOTA evaluation protocols (Zhai et al., 2025). As in prior work, denoising is performed using the model-implied score of the noiseaugmented probability flow, ensuring that improvements arise from the learned flow field rather than post-processing Noise specification. In all SOTA-style evaluations, we adopt additive Gaussian noise during training to match modern high-resolution flow-based pipelines (Song et al., 2021b; Zhai et al., 2025). Specifically, states along the probability path are perturbed as xt xt + ε, with ε (0, σ2I). The noise scale σ is selected to balance denoising strength and flow fidelity and is chosen exclusively to optimize FID under the baseline flow-matching setup. Unless otherwise stated, we use σ = 0.05 for ImageNet-64 and σ = 0.15 for ImageNet-128. These values are then held fixed when training TPC-FM, with no additional tuning, ensuring that all improvements are attributable to temporal pair consistency rather than noise hyperparameter selection. 7 Submission and Formatting Instructions for ICML Table 4. Conditional ImageNet 64 64 generation performance (FID ). All methods are evaluated under noise-augmented training with score-based denoising at sampling time. Table 6. Learned TPC-FM FID results across temporal prior probability ptpc and transition strength λtpc. Model Model Class FID EDM (Karras et al., 2022) iDDPM (Nichol & Dhariwal, 2021) ADM (dropout) (Dhariwal & Nichol, 2021) IC-GAN (Casanova et al., 2021) BigGAN (Brock et al., 2019) CD (LPIPS) (Song et al., 2023) iCT-deep (Song & Dhariwal, 2024) TARFLOW [4-1024-8-8-N (0, 0.052)] (Zhai et al., 2025) TARFLOW [2-768-8-8-N (0, 0.052)] (Zhai et al., 2025) TARFLOW [2-1024-8-8-N (0, 0.052)] (Zhai et al., 2025) FM + noise + denoising (baseline) TPC-FM + noise + denoising (ours) Diff/FM Diff/FM Diff/FM GAN GAN CM CM NF NF NF FM FM 1.55 2.92 2.09 6.70 4.06 4.70 3.25 3.99 2.90 2.66 3.6 2. Table 5. Fixed TPC-FM FID results across temporal prior probability ptpc and transition strength λtpc. ptpc 0.25 0.50 0.75 1.00 λtpc Best 0. 0.25 0.5 1.0 3.895 5.101 4.089 4.309 3.833 4.794 4.101 3.594 4.104 4.756 4.075 4. 4.314 4.216 3.931 6.545 3.833 4.216 3.931 3.594 Baseline 6.35 4.4. Ablation Studies Tables 57 analyze the effects of the temporal prior probability ptpc, transition strength λtpc, and monotonicity regularization λmono on CIFAR-10. Across all settings, TPC-FM consistently improves over the FM w/ OT baseline (FID 6.35), demonstrating robustness to hyperparameter choice. Moderate temporal coupling yields the best performance, while overly large λtpc degrades quality, indicating that excessive temporal constraints can restrict the learned probability path. Learning ptpc further improves performance over fixed priors, achieving the best overall result at ptpc = 0.75, λtpc = 0.10 (FID 3.19). Introducing weak monotonicity regularizer consistently improves sample quality, with small λmono values yielding the lowest FID, while stronger regularization degrades performance. These results indicate that temporal pair consistency is effective under mild coupling and regularization, with learned pairing providing additional gains; representative qualitative samples are shown in Figure 2. 5. Conclusion ptpc 0.25 0.50 0.75 1.00 λtpc Best 0. 0.25 0.5 1.0 3.922 3.895 3.193 4.402 4.249 3.885 3.620 4.382 4.767 4.266 4.016 4. 4.513 4.723 3.898 6.223 3.922 3.885 3.193 4.382 Baseline 6.35 Table 7. Effect of λmono on FID for learned and fixed TPC. λmono Learned FID Fixed FID 0 0.001 0.005 0. 0.020 0.050 3.563 3.193 4.124 3. 3.406 3.304 4.012 3.594 4.671 4. 3.882 3.741 Figure 2. ImageNet Qualitative Samples. tently improves sample quality and efficiency across onestep and full-simulation regimes and extends seamlessly to modern SOTA-style pipelines, demonstrating that simple temporal coupling can replace more complex path or solver designs. We introduced Temporal Pair Consistency (TPC), lightweight variance-reduction principle with theoretical guarantees that enforces coherence between velocity predictions at paired timesteps along the same probability path. Applied to flow matching and rectified flow, TPC consisLimitations This work focuses on unconditional image generation at resolutions up to 128 128. While we show that TPC remains effective under modern SOTA-style training pipelines, extending it to conditional settings, higher resolutions, and other modalities is left to future work. 8 Submission and Formatting Instructions for ICML"
        },
        {
            "title": "Acknowledgements",
            "content": "This work was partially supported by the Commonwealth Cyber Initiative (CCI) program (H-2Q25-020), the William & Mary Faculty Research Award, the Modal Academic Compute Award, and computational resources provided by the NSF ACCESS program under allocations CIS250827 and CIS251183, with support from the NCSA Delta and DeltaAI systems."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Albergo, M. S. and Vanden-Eijnden, E. Building normalizing flows with stochastic interpolants. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=li7qeBbCR1t. Partition-Guided GANs . Armandpour, M., Sadeghian, A., Li, C., and Zhou, In 2021 IEEE/CVF M. Conference on Computer Vision and Pattern Recognition (CVPR), pp. 50955105, Los Alamitos, CA, USA, IEEE Computer Society. doi: 10.1109/CVPR46437.2021.00506. URL https://doi.ieeecomputersociety.org/ 10.1109/CVPR46437.2021.00506. June 2021. Boffi, N. M., Albergo, M. S., and Vanden-Eijnden, E. Flow map matching with stochastic interpolants: mathematical framework for consistency models. Transactions on Machine Learning Research, 2025. ISSN 28358856. URL https://openreview.net/forum? id=cqDH0e6ak2. Brock, A., Donahue, J., and Simonyan, K. Large scale GAN training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=B1xsqj09Fm. Casanova, A., Careil, M., Verbeek, J., Drozdzal, M., and Romero, A. Instance-conditioned GAN. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum? id=aUuTEEcyY_. of the 32nd International Conference on Neural Information Processing Systems, NIPS18, pp. 65726583, Red Hook, NY, USA, 2018. Curran Associates Inc. Dhariwal, P. and Nichol, A. Q. Diffusion models beat GANs on image synthesis. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https: //openreview.net/forum?id=AAWuCvzaVt. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., Podell, D., Dockhorn, T., English, Z., and Rombach, R. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Gat, I., Remez, T., Shaul, N., Kreuk, F., Chen, R. T. Q., Synnaeve, G., Adi, Y., and Lipman, Y. Discrete flow matching. In Globerson, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J., and Zhang, C. (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 133345133385. Curran Associates, Inc., 2024. doi: 10.52202/079017-4239. https://proceedings.neurips. URL cc/paper_files/paper/2024/file/ f0d629a734b56a642701bba7bc8bb3ed-Paper-Conference. pdf. Geng, Z., Deng, M., Bai, X., Kolter, J. Z., and He, K. Mean flows for one-step generative modeling. In The Thirtyninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview. net/forum?id=uWj4s7rMnR. Grathwohl, W., Chen, R. T. Q., Bettencourt, J., and Duvenaud, D. Scalable reversible generative models with free-form continuous dynamics. In International Conference on Learning Representations, 2019a. URL https: //openreview.net/forum?id=rJxgknCcK7. Grathwohl, W., Chen, R. T. Q., Bettencourt, J., Sutskever, I., and Duvenaud, D. FFJORD: free-form continuous dynamics for scalable reversible generative models. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019b. URL https://openreview. net/forum?id=rJxgknCcK7. Guo, P. and Schwing, A. Variational rectified flow matching. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/ forum?id=Rk18ZikrFI. Chen, R. T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. Neural ordinary differential equations. In Proceedings Ho, J., Jain, A., and Abbeel, P. Denoising diffusion In Larochelle, H., Ranzato, probabilistic models. 9 Submission and Formatting Instructions for ICML 2026 M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 68406851. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/ 4c5bcfec8584af0d967f1ab10179ca4b-Paper. pdf. Ho, J., Saharia, C., Chan, W., Fleet, D. J., Norouzi, M., and Salimans, T. Cascaded diffusion models for high fidelity image generation. J. Mach. Learn. Res., 23(1), January 2022. ISSN 1532-4435. Hoang, Q., Nguyen, T. D., Le, T., and Phung, D. MGAN: Training generative adversarial nets with multiple generators. In International Conference on Learning Representations, 2018. URL https://openreview.net/ forum?id=rkmu5b0a-. Hoogeboom, E., Heek, J., and Salimans, T. Simple diffusion: end-to-end diffusion for high resolution images. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. Ifriqi, T. B., Nguyen, J., Alahari, K., Verbeek, J., and Chen, R. T. Q. Flowception: Temporally expansive flow matching for video generation, 2025. URL https: //arxiv.org/abs/2512.11438. Issenhuth, T., Santos, L. D., Franceschi, J.-Y., and Rakotomamonjy, A. Improving consistency models with generator-induced coupling. In ICML 2024 Workshop on Structured Probabilistic Inference & Generative Modeling, 2024. URL https://openreview.net/ forum?id=wt5ymd3h8h. Jabri, A., Fleet, D. J., and Chen, T. Scalable adaptive computation for iterative generation. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. Jiang, Y., Chang, S., and Wang, Z. Transgan: Two pure transformers can make one strong gan, and that can scale up. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 1474514758. Curran Associates, Inc., 2021. URL https://proceedings.neurips. cc/paper_files/paper/2021/file/ 7c220a2091c26a7f5e9f1cfb099511e3-Paper. pdf. Karras, T., Aittala, M., Hellsten, J., Laine, S., Lehtinen, J., and Aila, T. Training generative adversarial networks with limited data. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1210412114. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/ 8d30aa96e72440759f74bd2306c1fa3d-Paper. pdf. Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2656526577. Curran Associates, Inc., 2022. URL https://proceedings.neurips. cc/paper_files/paper/2022/file/ a98846e9d9cc01cfb87eb694d946ce6b-Paper-Conference. pdf. Kidger, P. On neural differential equations, 2022. URL https://arxiv.org/abs/2202.02435. Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, 2009. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural In Pereira, F., Burges, C., Bottou, L., and networks. Weinberger, K. (eds.), Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012. URL https://proceedings.neurips. cc/paper_files/paper/2012/file/ c399862d3b9d6b76c8436e924a68c45b-Paper. pdf. Laine, S. and Aila, T. Temporal ensembling for semiIn International Conference on supervised learning. Learning Representations, 2017. URL https:// openreview.net/forum?id=BJ6oOfqge. Lee, J., Moradijamei, B., and Shakeri, H. Multimarginal stochastic flow matching for high-dimensional In Fortysnapshot data at second International Conference on Machine Learning, 2025. URL https://openreview.net/forum? id=ZLyb8DwXXE. irregular time points. Li, L., Hurault, S., and Solomon, J. Self-consistent velocity matching of probability flows. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=C6fvJ2RfsL. Lin, Z., Khetan, A., Fanti, G., and Oh, S. Pacgan: The power of two samples in generative adversarial networks. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, Submission and Formatting Instructions for ICML 2026 R. (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips. cc/paper_files/paper/2018/file/ 288cc0ff022877bd3df94bc9360b9c5d-Paper. pdf. Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=PqvMRDCJT9t. Liu, X., Gong, C., and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=XVjTT1nw5z. Lu, C. and Song, Y. Simplifying, stabilizing and scaling continuous-time consistency models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=LyJi5ugyJx. Luˇcic, M., Tschannen, M., Ritter, M., Zhai, X., Bachem, O., and Gelly, S. High-fidelity image generation with fewer labels. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 41834192. PMLR, 0915 Jun 2019. URL https://proceedings.mlr.press/ v97/lucic19a.html. Luhman, E. and Luhman, T. Knowledge distillation in iterative generative models for improved sampling speed, 2021. URL https://arxiv.org/abs/ 2101.02388. Ma, C., Xiao, X., Wang, T., Wang, X., and Shen, Y. Learning straight flows: Variational flow matching for efficient generation, 2025. URL https://arxiv.org/abs/ 2511.17583. Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=B1QRgziT-. Nichol, A. Q. and Dhariwal, P. Improved denoising diffusion probabilistic models, 2021. URL https:// openreview.net/forum?id=-NEXDKk8gZ. of Proceedings of Machine Learning Research, pp. 54205428. PMLR, 0915 Jun 2019. URL https:// proceedings.mlr.press/v97/ren19b.html. Reu, T., Dromigny, S., Bronstein, M. M., and Vargas, F. Gradient variance reveals failure modes in flowIn The Thirty-ninth Annual based generative models. Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum? id=pVaqdFlUAO. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1068410695, June 2022. Sage, A., Timofte, R., Agustsson, E., and Gool, L. V. Logo synthesis and manipulation with clustered generative adversarial networks. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5879 5888, 2018. doi: 10.1109/CVPR.2018.00616. Salimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=TIdIXIpzhoI. Sauer, A., Schwarz, K., and Geiger, A. Stylegan-xl: ScalIn ACM SIGing stylegan to large diverse datasets. GRAPH 2022 Conference Proceedings, SIGGRAPH 22, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450393379. doi: 10. 1145/3528233.3530738. URL https://doi.org/ 10.1145/3528233.3530738. Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C. A., Cubuk, E. D., Kurakin, A., and Li, C.-L. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 596608. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/ 06964dce9addb1c5cb5d6e3d9838f733-Paper. pdf. Song, J., Meng, C., and Ermon, S. Denoising diffuIn International Conference on sion implicit models. Learning Representations, 2021a. URL https:// openreview.net/forum?id=St1giarCHLP. Ren, H., Zhao, S., and Ermon, S. Adaptive antithetic sampling for variance reduction. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 Song, Y. and Dhariwal, P. Improved techniques for training consistency models. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=WNzy9bRDvG. 11 Submission and Formatting Instructions for ICML 2026 In The Eleventh International Conferauto-encoders. ence on Learning Representations, 2023a. URL https: //openreview.net/forum?id=HDxgaKk956l. Zheng, K., Lu, C., Chen, J., and Zhu, J. Improved techniques for maximum likelihood estimation for diffusion odes. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023b. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modIn Ineling through stochastic differential equations. ternational Conference on Learning Representations, 2021b. URL https://openreview.net/forum? id=PxTIG12RRHS. Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. Tong, A., FATRAS, K., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Wolf, G., and Bengio, Y. Improving and generalizing flow-based generative models with minibatch optimal transport. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https:// openreview.net/forum?id=CD9Snc73AW. Expert Certification. Xiao, Z., Kreis, K., and Vahdat, A. Tackling the generative learning trilemma with denoising diffusion gans, 2022. URL https://arxiv.org/abs/2112.07804. Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., Zhang, W., Cui, B., and Yang, M.-H. Diffusion models: comprehensive survey of methods and applications. ACM Comput. Surv., 56(4), November 2023. ISSN 0360-0300. doi: 10.1145/3626235. URL https://doi.org/10. 1145/3626235. Yang, L., Zhang, Z., Zhang, Z., Liu, X., Liu, J., Xu, M., Meng, C., Ermon, S., Zhang, W., and CUI, B. Consistency flow matching: Defining straight flows with velocity consistency, 2025. URL https://openreview. net/forum?id=bS76qaGbel. Zhai, S., ZHANG, R., Nakkiran, P., Berthelot, D., Gu, J., Zheng, H., Chen, T., Bautista, M. A., Jaitly, N., and Susskind, J. M. Normalizing flows are capable generative models. In Forty-second International Conference on Machine Learning, 2025. URL https: //openreview.net/forum?id=2uheUFcFsM. Zhao, S., Liu, Z., Lin, J., Zhu, J.-Y., and Han, S. Differentiable augmentation for data-efficient gan training. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volInc., ume 33, pp. 75597570. Curran Associates, 2020. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/ 55479c55ebd1efd3ff125f1337100388-Paper. pdf. Zheng, H., He, P., Chen, W., and Zhou, M. Truncated diffusion probabilistic models and diffusion-based adversarial 12 Submission and Formatting Instructions for ICML A. Theoretical Analysis A.1. Preliminaries Let p0, p1 be distributions on Rd. probability path {pt}t[0,1] is admissible if there exists measurable vector field : Rd [0, 1] Rd such that (pt, ) solves the continuity equation tpt(x) + (pt(x)u (x)) = 0, pt=0 = p0, pt=1 = p1, (3) in the weak sense: for all φ (Rd), (cid:90) dt φ(x)pt(x)dx = (cid:90) φ(x), (x)pt(x)dx. Fix path sampler inducing joint law on (x0, x1, t) with ρ on [0, 1] and (x0, x1) π, together with measurable map Φ : [0, 1] Rd Rd Rd such that Let ut(x0, x1) Rd denote conditional target velocity (depending on the sampler). For measurable predictor : Rd [0, 1] Rd define the FM population risk xt = Φt(x0, x1), (x0, x1, t) π ρ. R(v) := v(xt, t) ut(x0, x1)2 2. Lemma A.1 (L2 projection / regression form). Let Gt := σ(xt, t) and assume Eut2 2 < . Then v(x, t) := E(cid:2)ut(x0, x1) xt = x, t(cid:3) is the (a.s.) unique minimizer of R(v) over all Gt-measurable v, and R(v) R(v) = v(xt, t) v(xt, t)2 2. Proof. Expand take conditional expectation given Gt and use E[u Gt] = to kill the cross term; then average. u2 = v2 + u2 + 2v v, u, (4) (5) Lemma A.1 shows that conditional flow matching is pointwise-in-time regression problem: for each t, the predictor v(, t) is fit independently as an L2 projection. Crucially, the objective (4) imposes no coupling across time, so the learned vector field may exhibit arbitrary temporal oscillations even when achieving minimal population risk. The remainder of this appendix studies how introducing temporal coupling alters this geometry. A.2. TPC-FM as quadratic regularization in trajectory-coupled Hilbert space Rather than penalizing abstract smoothness in t, we regularize discrepancies of predictions evaluated along the same sampled trajectory. This enforces temporal coherence only on states that are jointly realizable under the path sampler, which is the relevant geometry for both optimization and sampling. Fix pairing rule ψ : [0, 1] [0, 1] (possibly randomized; treat the randomness as absorbed into ρ). Let = ψ(t) and define paired states (xt, xt) = (Φt(x0, x1), Φt(x0, x1)). Define the quadratic form v2 TPC := v(xt, t) v(xt, t)2 2. Define the TPC-regularized population objective Rλ(v) := R(v) + λv2 TPC, λ > 0. Let µ denote the path marginal on (x, t): dµ(x, t) = pt(x)ρ(t) dx dt and define the Hilbert space := L2(µ; Rd), f, gH = (cid:90) (cid:90) 1 0 13 (x, t), g(x, t) pt(x)ρ(t) dx dt. (6) (7) Submission and Formatting Instructions for ICML 2026 Define an operator Aψ on functions by (Aψv)(x0, x1, t) := v(Φt(x0, x1), t) v(Φψ(t)(x0, x1), ψ(t))."
        },
        {
            "title": "Then",
            "content": "v2 TPC = Aψv2 2, i.e., TPC is the L2(π ρ)-seminorm of Aψv. Lemma A.2 (Tikhonov selection inequality). Let arg min R(v) and vλ arg min Rλ(v). Then λvλ2 TPC R(v) R(vλ) R(v), vλ2 TPC R(v) λ . Moreover, if R(vλ) R(v) + ε, then vλ TPC inf v: R(v)R(v)+ε v2 TPC + ε λ . (8) (9) (10) Proof. Optimality gives R(vλ) + λvλ2 against any with R(v) R(v) + ε. TPC R(v) + λv2 TPC. Drop λv TPC 0 to get (9). For (10), compare Lemma A.2 formalizes TPC as selection principle: among all predictors achieving near-minimal FM risk, the regularized objective prefers those with smaller temporal variation along coupled trajectories. This is the precise sense in which TPC reduces temporal oscillation at the population level. A.3. Anchor theorem: population regularization correlated gradients strict variance reduction Let vθ be parametric model. Define the per-sample FM loss and gradient ℓ(θ; t, ξ) := vθ(xt, t) ut(ξ)2 2, g(t, ξ) := θℓ(θ; t, ξ), (11) where ξ denotes the shared randomness producing (x0, x1) and any sampler noise, so that xt = Φt(ξ) and ut = ut(ξ). Define the paired time = ψ(t) and h(t, ξ) := g(t, ξ). We now connect the regularized population view to stochastic optimization. Although TPC modifies the objective, its most immediate algorithmic effect is to correlate gradient evaluations across time, enabling classical variance-reduction mechanisms. Lemma A.3 (Optimal scalar control variate). Let G, be square-integrable random vectors in Rm and consider (cid:98)Gα := 2 and covariance Cov(G, H) := EGEG, EH. GαH with α R. Define scalar variance Var(Z) := EZ EZ2 Then the minimizer is and the minimum variance equals α = Cov(G, H) Var(H) , Var(G αH) = Var(G) (1 ρ2), ρ := Cov(G, H) (cid:112)Var(G)Var(H) . Proof. Expand optimize the quadratic in α to get (12), then substitute and simplify to (13). Var(G αH) = Var(G) + α2Var(H) 2α Cov(G, H), We now derive sufficient condition for Cov(g(t, ξ), g(t, ξ)) > 0 from smoothness + path coupling. Assume: LΦ < : Ext xs2 Lg < : Eg(t, ξ) g(s, ξ)2 2 Φt s2, 2 L2 s, [0, 1], gt s2, s, [0, 1], where (15) is implied by local Lipschitzness of vθ and bounded Jacobians on the path support. (12) (13) (14) (15) Submission and Formatting Instructions for ICML 2026 Lemma A.4 (Correlation lower bound from Lipschitz continuity). Let := g(t, ξ) and := g(t, ξ) with = ψ(t). Assume EG = EH (or work with centered versions) and Var(G) > 0. Then Corr(G, H) 1 EG H2 2 2 Var(G) . (16) Consequently, if EG H2 2 2(1 ρ0)Var(G) for some ρ0 (0, 1), then Corr(G, H) ρ0. Proof. For centered G, H, EG H2 2 = EG2 2 + EH2 2 2EG, = 2Var(G) 2Cov(G, H), hence Cov(G, H) = Var(G) 1 EG H2 2 and dividing by Var(G) yields (16). The following result consolidates the preceding components into single statement: population-level regularization, gradient correlation, and strict variance reduction are shown to be consequences of the same temporal coupling mechanism. Theorem A.5. Let vλ arg min Rλ(v). The key requirement for variance reduction is that gradients at paired times are positively correlated. We now show that this is not an ad-hoc assumption, but follows from smooth dependence on time together with path coupling induced by sharing the same endpoints (x0, x1). Assume (14)(15) and that the pairing ψ satisfies ψ(t) a.s. for some (0, 1). Then: (i) (Population contraction in the TPC metric) vλ2 TPC R(v) λ . (ii) (Quantitative correlation) For = g(t, ξ), = g(ψ(t), ξ), EG H2 2 L2 Et ψ(t)2 L2 g2, hence by Lemma A.4, whenever the RHS is positive. Corr(G, H) 1 L2 g2 2 Var(G) =: ρ0, (17) (iii) (Strict variance reduction) With (cid:98)gα := αH and α from Lemma A.3, Var((cid:98)gα ) = Var(G)(1 ρ2) Var(G)(1 ρ2 0) < Var(G), for any ρ0 (0, 1) satisfying (17). Proof. (i) is Lemma A.2. (ii) uses (15) and ψ(t) . (iii) is Lemma A.3 with the lower bound (17). The final step is to connect temporal regularity of the learned vector field to numerical behavior during probabilityflow sampling. Since generation integrates the learned ODE only along states visited by the sampler, trajectory-wise temporal coherence is the relevant notion. Submission and Formatting Instructions for ICML 2026 A.4. ODE link: TPC as control of temporal roughness and solver error Let zt solve the probabilityflow ODE zt = vθ(zt, t), z0 p0, (18) and let (cid:98)zt be the explicit Euler discretization with step size = 1/N : (cid:98)zk+1 = (cid:98)zk + vθ((cid:98)zk, tk), tk = kh. Assume vθ is Lipschitz in with constant Lx and differentiable in with tvθ(x, t)2 Lt on the region visited by (zt, (cid:98)zt). Lemma A.6 (Global error bound with explicit time-variation term). There exists constant (depending on bounds on vθ and xvθ on the trajectory tube) such that z1 (cid:98)z12 eLx (1 + Lt). (19) Proof sketch. Write the local truncation error ztk+1 ztk hvθ(ztk , tk) and use Taylor expansion in time: ztk+1 = ztk + (cid:90) tk+1 tk vθ(zs, s) ds = ztk + hvθ(ztk , tk) + Bound the integrand by Lipschitz in and t: (cid:90) tk+1 tk (cid:2)vθ(zs, s) vθ(ztk , tk)(cid:3)ds. and use Gronwall to accumulate. vθ(zs, s) vθ(ztk , tk) Lxzs ztk + Lts tk, Lemma A.6 isolates temporal variation of the vector field as an explicit contributor to global discretization error. Thus, any mechanism that suppresses temporal roughness along sampled trajectories directly improves numerical stability at fixed step size. For local pairings ψ(t) = + with small (random or deterministic), v(xt+, + ) v(xt, t) = tv(xt, t) + (xv(xt, t)) xt + O(2), hence (formally) 1 2 Ev(xt+, + ) v(xt, t)2 2 Etv(xt, t) + (xv(xt, t)) xt2 2. (20) Thus small vTPC (for local ψ) yields small trajectory-averaged temporal derivative, which is exactly the quantity entering time-variation terms in discretization bounds such as (19). For global pairings (e.g. antithetic ψ(t) = 1 t), vTPC still controls low-frequency temporal oscillations in spectral sense (via the quadratic form induced by Aψ), which again reduces effective temporal roughness on the sampler support. A.5. Uniform convergence under TPC constraint Let be class of predictors bounded by v(x, t)2 a.s. Define the constrained class Vτ := {v : v2 For i.i.d. samples {(xi i=1, define the empirical risk 1, ti)}n 0, xi TPC τ }. (cid:98)R(v) := 1 (cid:88) i= v(xi ti , ti) uti (xi 0, xi 1)2 2. Proposition A.7 (Rademacher bound for constrained class). With probability 1 δ, (cid:12) (cid:98)R(v) R(v)(cid:12) (cid:12) (cid:12) c1 Rn(Vτ ) + c2 B2 sup vVτ (cid:114) log(1/δ) , (21) for universal constants c1, c2 > 0 and vector-valued Rademacher complexity Rn(). In typical parameterizations, Rn(Vτ ) is non-increasing in τ . 16 Submission and Formatting Instructions for ICML 2026 (a) CIFAR-10 (b) ImageNet-32 (c) Training variance Figure 3. Qualitative samples and training variance behavior. CIFAR-10 and ImageNet-32 qualitative generations (left, middle), and training variance advantage of TPC-FM (right), where TPC-FM exhibits early variance collapse and sustained stability throughout training. A.6. Limitations No statement above directly implies monotone improvement in FID/IS: FID = FID(cid:0)model bias, optimization error, solver error, estimation noise(cid:1), and our results certify only the mechanism-level reductions: TPC vTPC (cid:40) Corr(g(t, ξ), g(t, ξ)) Var(SGD gradients), temporal roughness on sampler support ODE discretization error. A.7. Takeaway Under the stated assumptions, vλ arg min Rλ vλ2 TPC R(v) λ , and for paired-time gradients = g(t, ξ), = g(ψ(t), ξ), Var(G αH) = Var(G)(1 ρ2) < Var(G) whenever ρ > 0, with ρ quantitatively lower bounded via Lemma A.4 under path-coupled Lipschitz regularity. Moreover, for sampling zt = vθ(zt, t), Euler-type global error contains explicit dependence on temporal variation (Lemma A.6), which is controlled in trajectory-averaged sense by local TPC penalties through (20). B. Generated Samples 17 Submission and Formatting Instructions for ICML 2026 Table 8. Comparison of GAN, ODE, and SDE generative models on CIFAR-10 under one-step and full-simulation settings. Method GAN NFE () IS () FID () Recall () SNGAN (Miyato et al., 2018) StyleGAN2 (Karras et al., 2020) StyleGAN-XL (Sauer et al., 2022) StyleGAN2 + ADA (Karras et al., 2020) StyleGAN2 + DiffAug (Zhao et al., 2020) TransGAN + DiffAug (Jiang et al., 2021) GAN with U-Net TDPM (T=1) (Zheng et al., 2023a) Denoise. Diff. GAN (T=1) (Xiao et al., 2022) ODE One-Step Generation (Euler solver, N=1) DDIM Distillation (Luhman & Luhman, 2021) NCSN++ (VE ODE; Distill) (Song et al., 2021b) Progressive (Salimans & Ho, 2022) DDIM (Song et al., 2021a) 1 1 1 1 1 1 1 1 1 1 1 1 8.22 9.18 - 9.40 9.40 9.02 8.65 8.93 8.36 2.57 - - 21.7 8.32 1.85 2.92 5.79 9.26 8.91 14.6 9.36 2.54 9.12 > 20 0.44 0.41 0.47 0.49 0.42 0.41 0.46 0.19 0.51 0.0 - - ODE Full Simulation (RungeKutta RK45), Adaptive NCSN++ (VE ODE) (Song et al., 2021b) 176 9.35 5.38 0. SDE Full Simulation (Euler solver) DDPM (Ho et al., 2020) NCSN++ (VE SDE) (Song et al., 2021b) 1000 2000 9.46 9.83 3.21 2.38 0.57 0. ODE Full Simulation (Euler solver) DDIM (Song et al., 2021a) DDIM (Song et al., 2021a) 10 100 - - 13.36 4.16 - -"
        }
    ],
    "affiliations": [
        "William & Mary"
    ]
}