{
    "paper_title": "Novel Object 6D Pose Estimation with a Single Reference View",
    "authors": [
        "Jian Liu",
        "Wei Sun",
        "Kai Zeng",
        "Jin Zheng",
        "Hui Yang",
        "Lin Wang",
        "Hossein Rahmani",
        "Ajmal Mian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing novel object 6D pose estimation methods typically rely on CAD models or dense reference views, which are both difficult to acquire. Using only a single reference view is more scalable, but challenging due to large pose discrepancies and limited geometric and spatial information. To address these issues, we propose a Single-Reference-based novel object 6D (SinRef-6D) pose estimation method. Our key idea is to iteratively establish point-wise alignment in the camera coordinate system based on state space models (SSMs). Specifically, iterative camera-space point-wise alignment can effectively handle large pose discrepancies, while our proposed RGB and Points SSMs can capture long-range dependencies and spatial information from a single view, offering linear complexity and superior spatial modeling capability. Once pre-trained on synthetic data, SinRef-6D can estimate the 6D pose of a novel object using only a single reference view, without requiring retraining or a CAD model. Extensive experiments on six popular datasets and real-world robotic scenes demonstrate that we achieve on-par performance with CAD-based and dense reference view-based methods, despite operating in the more challenging single reference setting. Code will be released at https://github.com/CNJianLiu/SinRef-6D."
        },
        {
            "title": "Start",
            "content": "Novel Object 6D Pose Estimation with Single Reference View Jian Liu1,2, Wei Sun1, Kai Zeng1, Jin Zheng3, Hui Yang1, Lin Wang2, Hossein Rahmani4, Ajmal Mian5 1 Hunan University, 2 Nanyang Technological University 3 Central South University, 4 Lancaster University, 5 The University of Western Australia {jianliu, wei sun}@hnu.edu.cn, linwang@ntu.edu.sg, h.rahmani@lancaster.ac.uk, ajmal.mian@uwa.edu.au 5 2 0 2 7 ] . [ 1 8 7 5 5 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Existing novel object 6D pose estimation methods typically rely on CAD models or dense reference views, which are both difficult to acquire. Using only single reference view is more scalable, but challenging due to large pose discrepancies and limited geometric and spatial information. To address these issues, we propose Single-Reference-based novel object 6D (SinRef-6D) pose estimation method. Our key idea is to iteratively establish point-wise alignment in the camera coordinate system based on state space models (SSMs). Specifically, iterative camera-space point-wise alignment can effectively handle large pose discrepancies, while our proposed RGB and Points SSMs can capture longrange dependencies and spatial information from single view, offering linear complexity and superior spatial modeling capability. Once pre-trained on synthetic data, SinRef6D can estimate the 6D pose of novel object using only single reference view, without requiring retraining or CAD model. Extensive experiments on six popular datasets and real-world robotic scenes demonstrate that we achieve on-par performance with CAD-based and dense reference view-based methods, despite operating in the more challenging single reference setting. Code will be released at https://github.com/CNJianLiu/SinRef-6D. 1. Introduction Object 6D pose estimation aims to predict the 3D rotation and 3D translation of an object coordinate system relative to the camera coordinate system [3, 36, 40, 69, 76, 81, 86], constituting foundational task in augmented reality and robotic manipulation [7, 18, 43, 44, 66, 80]. Currently, instance-level methods [10, 11, 26, 58, 67, 68, 71, 78] have attained high precision but are limited to objects encountered during training. In contrast, category-level methods [8, 12, 34, 39, 42, 45, 74, 85] can generalize to objects within the same category but still necessitate retraining for objects of novel categories. Recently, many novel object Corresponding author Figure 1. Comparison of manual reference view-based novel object 6D pose estimation methods. (a) Dense reference views-based methods typically rely on 1: 3D object reconstruction or 2: template matching, which is timeand storage-consuming. (b) The proposed SinRef-6D estimates novel object pose using only single reference view, providing enhanced efficiency and scalability. pose estimation methods [17, 20, 23, 49, 56, 62, 82] have been proposed that do not require retraining for previously unseen objects, thereby exhibiting enhanced generalization. Novel object pose estimation methods can be mainly divided into two categories: CAD model-based [53, 59, 60, 65], where textured CAD model of the novel object is required during training and inference; manual reference view-based [9, 51, 72, 75], where set of manually labeled reference views of the novel object is required. It is well known that obtaining accurate textured CAD models requires specialized equipment and expert knowledge, which hinders the scalability of mobile devices. Since manual reference views are relatively easy to acquire, methods based on them offer greater scalability. Specifically, manual reference view-based methods typically solve pose through 3D object reconstruction or directly obtain coarse pose via template matching, as shown in Fig. 1 (a), where the switch indicates whether 3D reconstruction is required based on 1 dense reference views. These methods rely on dense reference views, which are timeand storage-consuming. Furthermore, template matching-based methods require the use of novel template generation techniques or an extra pose refinement process to obtain accurate object pose, which further increases the computational complexity. To address the above challenges, we aim to explore CAD model-free, sparse reference view-based novel object 6D pose estimation method, eliminating template matchingbased retrieval process. Specifically, we formulate the task as the extreme case of sparse reference, where only single reference is utilized. However, using single reference introduces several unique challenges, including large pose discrepancies and limited geometric and spatial information. The comparison of dense reference views-based and single reference view-based methods is shown in Fig. 1. To achieve the aforementioned objective, we propose SinRef-6D, generalizable model designed to establish point-wise alignment between single reference view and the query view. SinRef-6D introduces two key components for novel object 6D pose estimation: 1) Iterative cameraspace point-wise alignment, which leverages geometric and spatial consistency to refine pose estimation, addressing large pose discrepancies; 2) State Space Models (SSMs) efficiently capture long-range dependencies and spatial information from sparse single-view data while offering linear complexity and superior spatial modeling capability, making them well-suited for single-reference pose estimation. Specifically, we propose to align the reference and query point clouds within common camera coordinate system. Given the importance of spatial information for point-wise alignment and the need for lightweight model for mobile deployment, we introduce RGB and Points SSMs to effectively encode reference and query views, generating pointwise features for both views, which are then used to establish point-wise alignment and estimate the object pose. To handle the potential large pose discrepancies between the reference and query views, we propose to iteratively refine the alignment process in the camera coordinate system, leading to more accurate and robust novel object pose estimation. Our main contributions are summarized as follows: We introduce SinRef-6D, an efficient and scalable framework for novel object 6D pose estimation using only single reference view, eliminating the need for computationintensive template matching and multi-view reconstruction, thus enhancing real-world practicality. We propose camera-space point-wise alignment strategy with iterative refinement, enabling direct alignment of query and reference views while effectively handling large pose discrepancies. This enhances geometric consistency and spatial awareness, redefining novel object 6D pose estimation without category-specific retraining. We propose RGB and Points SSMs to capture rich spatial information for establishing point-wise alignment, enabling efficient long-range spatial modeling with linear complexity while reducing computational overhead. 2. Related Work CAD Model-based Methods first require obtaining the CAD model of the novel object, which is then used as prior knowledge for pose estimation. These methods can be further categorized into feature matching-based and template matching-based. Feature matching-based methods [4, 6, 32, 41, 84] learn model to match features between the observed image and CAD model, establishing 2D-3D or 3D-3D correspondences to estimate object pose. Specifically, SAM-6D [41] introduces novel matching score based on semantics, appearance, and geometry to improve segmentation. For pose estimation, it employs two-stage point matching model to establish dense 3D3D correspondences. FreeZe [4] develops method that combines visual and geometric features from various pretrained models to improve pose prediction stability and accuracy. MatchU [32] proposes technique for predicting object pose from RGB-D images by integrating 2D texture with 3D geometric cues. Template matching-based methods [37, 48, 52, 55, 70, 73] render multiple template views of the object with different poses from the CAD model. Then, they retrieve the template that best matches the observed image to obtain coarse pose, followed by refinement process to achieve accurate pose estimation. Specifically, MegaPose [37] proposes render-and-compare-based method and coarse-to-fine pose estimation strategy. GenFlow [48] introduces shape-constrained recurrent flow framework that predicts optical flow between the query and template images while iteratively refining the pose. GigaPose [52] achieves fast and robust pose estimation by striking an effective balance between template matching and patch correspondences. Manual reference view-based methods employ manual reference views as prior knowledge for novel objects, eliminating the need for precise CAD model. These approaches can also be divided into feature matching-based and template matching-based. Feature matching-based methods [5, 25, 27, 38, 46, 64] aim to establish 3D-3D correspondences between the query view and reference views, or 2D3D correspondences between the query view and the 3D object representation reconstructed from reference views. Specifically, FS6D [27] proposes dense prototype matching method to explore geometric and semantic relations between the query view and reference views, estimating the pose of novel objects using only few reference views. OnePose [64] first utilizes Structure from Motion (SfM) to reconstruct the 3D representation of the novel object using all reference views, and then establishes 2D-3D correspondences between the query view and the reconstructed 3D 2 Figure 2. Our proposed SinRef-6D framework. Given normal RGB-D reference view of novel object, we aim to predict its 6D pose from any query view. SinRef-6D comprises four modules: (A) RGB-D images from the reference and query views are segmented, and the segmented depth maps are back-projected into point clouds. (B) The corresponding point clouds from the reference and query views are focalized from the object coordinate system to the camera coordinate system. (C) Leveraging the proposed Points and RGB SSMs (details are shown in Fig. 3 and Fig. 4), features are extracted from the focalized point clouds and RGB images, forming point-wise reference and query features. (D) These features are then used to establish point-wise alignment to solve the object pose. Finally, the computed pose is fed back into module (B) to iteratively improve the accuracy of the point-wise alignment, yielding more precise object pose. representation using graph attention network. Template matching-based methods [2, 16, 19, 47, 57, 73] primarily utilize retrieval and refinement strategy. They directly use labeled reference views as templates to retrieve coarse pose, followed by refinement process to enhance accuracy. Specifically, LatentFusion [57] reconstructs 3D object representation and estimates translation using bounding boxes and depth values. Then, the initial rotation is determined by angle sampling and further refined through gradient updates using render and compare. Gen6D [47] first detects object bounding boxes, then compares the query and reference images via similarity scores to obtain an initial pose. Next, the pose is refined via proposed refiner. FoundationPose [73] increases the quantity and diversity of synthetic data based on diffusion model and achieves performance comparable to instance-level methods. Overall, CAD model-based methods depend on textured CAD models, and manual reference view-based methods require dense reference views, both adding manual effort in real-world applications. Hence, this paper seeks to enable novel object 6D pose estimation with single reference view, reducing manual overhead and enhancing scalability. 3. Methods Single reference view-based novel object 6D pose estimation faces several challenges, including large pose discrepancies between the reference and query views, and limited geometric and spatial information from sparse views. To address these issues, we introduce two key solutions: 1) Iterative camera-space point-wise alignment, which refines the alignment between the reference and query views to handle pose discrepancies. 2) SSMs for efficient feature encoding, capturing long-range dependencies and spatial information from single view with linear complexity. These solutions enable accurate novel object 6D pose estimation without requiring CAD model or dense reference views. Figure 2 shows the overall workflow that comprises four main components: (A) Initialization (Sec. 3.1) segments the (B) novel object in the input reference and query views. Points Focalization (Sec. 3.2) focalizes the novel object in the reference and query views into the camera coordinate system using their corresponding poses. (C) Points & RGB SSMs (Sec. 3.3) employ state space models to extract point- (D) Point-wise Alignwise reference and query features. ment & Pose Solving (Sec. 3.4) derives point-wise alignment relationships using the features extracted in (C) to solve the object pose in the query view. In addition, iterating the process from (B) to (D) allows for further obtaining more accurate point-wise alignment and object pose. The detailed training mode is illustrated in Sec. 3.5. 3.1. Initialization The pipeline of the initialization process is shown in part (A) of Fig. 2. Note that our reference view is not care3 Figure 3. Detailed architecture of the proposed Points SSM. fully selected. From practical real-world application perspective (further details can be found in the supplementary material), reference views are typically acquired manually. Therefore, we randomly select normal view (free of occlusion and with minimal self-occlusion) as the reference view. During training and inference on public datasets, we generate reference views using the same rendering method employed by GigaPose [52]. Specifically, we randomly select one viewpoint from its 50th to 120th viewpoints (oblique views) for rendering, effectively simulating manual reference view collection while introducing perturbations. Since both the reference and query views often contain cluttered backgrounds, we first segment the background. For fair comparison, we employ Mask R-CNN [24] or zero-shot CNOS [50] with FastSAM to segment the input images, and then back-project the segmented depth maps into point clouds. This results in the segmented RGB images and point clouds for both reference (Ir, Pr RNr3) and query (Iq, Pq RNq3) views, where Nr and Nq denote the number of points in the reference and query point clouds, respectively. Notably, CNOS relies on object CAD models for rendering template images, which contrasts with our CAD model-free setup. Based on this, we also use only our single reference view as the template image for similarity matching in CNOS segmentation [50]. 3.2. Points Focalization Figure 4. Detailed architecture of the proposed RGB SSM. For the query point cloud, we apply the same method to transform it into the camera coordinate system as follows: = i (Pq ti) , (2) where ti and Ri represent the translation and rotation of the object solved in the (i 1)-th iteration. represents the query point cloud in the camera coordinate system after the (i 1)-th iteration. Since the object pose in the query view is initially unknown, we do not perform rotation transformation during the first points focalization and instead set the translation t1 to the average coordinate of the object. In subsequent iterations, we use the object pose [Ri+1ti+1] solved in the previous round for coordinate transformation. The overall process is shown in part (B) of Fig. 2. 3.3. Points & RGB SSMs Since point-wise alignment relies on rich spatial features, sequential modeling of point clouds and RGB images enables effective long-range spatial encoding, enhancing feature discrimination and geometric consistency for more precise alignment. S6 models [21] represent class of sequence models that excel in sequence handling. These models extend the earlier S4 model [22], mapping an input sequence (t) (t) via latent state (t) RM according to the ordinary linear differential equations: Since SinRef-6D aims to iteratively align point clouds for precise object pose solving, our first step is to focalize the reference and query point clouds within unified coordinate system. This focalization facilitates point-wise alignment, ensures geometric consistency during iterative refinement, and inherently decouples pose estimation from category priors, enhancing robustness to novel objects. Specifically, as the reference point cloud Pr has pose annotation [Rrtr], we can transform it from the object coordinate system Co to the camera coordinate system Cc as follows: = c (Pr tr) , (1) where tr and Rr denote the annotated translation and rotation, respectively. denotes matrix transpose, denotes the reference point cloud in the camera coordinate system. 4 (3) (t) = Ah (t) + Bx (t) , (t) = Ch (t) + Dx (t) , where RM , RM 1, R1M , and R1 are weighting parameters. Specifically, the continuous dynamical systems are discretized using the following zeroorder hold discretization method in practical computations: ht = Aht1 + Bxt, yt = Ch (t), = exp (A) , = (A)1 (exp (A) I) B, where denotes the discrete step size. Given that both the weighting parameters and discretization method remain constant over time, S4 models can be considered linear time-invariant systems. S6 model [21] further extends the projection matrices of S4 models to enable selective scan of the entire input sequence. Specifically, this paper models (4) the sequences of point clouds and RGB images by designing Points SSM (as shown in Fig. 3) and an RGB SSM (as shown in Fig. 4) as follows: Fr = oints SSM (P = oints SSM (cid:0)P r ) RGB SSM (Ir) , (cid:1) RGB SSM (Iq) , where represents matrix addition. Fr RNrC and RNqC represent the point-wise reference features and the point-wise query features at the i-th iteration, respectively. is the dimension of feature channels. (5) For the Points SSM as shown in Fig. 3, we first perform point-wise scan and use K-Nearest Neighbor (KNN) to sample set of points for each scanned point to form token. Then, we compute all token embeddings and add position embedding to them. Next, the token embeddings are concatenated and passed into the points state space (PSS) blocks to obtain the point-wise feature 2048256. The details of the selective SSM in PSS blocks can be found in the S6 model [21]. For RGB image feature extraction, we propose an RGB SSM based on the cross-scan manner and multi-scale feature fusion, as shown in Fig. 4. The architecture consists of four stages, where each stage employs visual state space (VSS) blocks [79] to extract image features at different scales. These multi-scale features are then fused, reshaped, and chosen by using the image mask to obtain the final image feature representation 2048256. and The complete process is illustrated in part (C) of Fig. 2, where represent the extracted point-wise refq and erence and query features, while denote the extracted features from the reference and query RGB images. 3.4. Point-wise Alignment & Pose Solving Upon acquiring the point-wise reference and query features, our objective is to develop model with the capability to generalize to novel objects for establishing point-wise alignment. This methodology provides enhanced learnability compared to the direct pose regression model for novel objects. Specifically, we input Fr and into the GeoTransformer [61], where they undergo geometric-aware self-attention and cross-attention, yielding the final pointwise reference and query features Fr and q . Then, we obtain the point-wise affinity matrix Ai as follows and select the point pairs with the highest similarity for alignment: Ai = q , (6) where represents matrix multiplication. The point-wise alignment of the reference and query view point clouds in the camera coordinate system is visualized in part (D) of Fig. 2. Once the point-wise alignment relationship is established, we can directly solve the 6D object pose using the weighted singular value decomposition (WSVD) algorithm as follows: [Ri+1ti+1] = SV (cid:0)Ai, , Pq (cid:1) , (7) where rotation Ri+1 and translation ti+1 denote the 6D object pose solved from the i-th iteration. Given the considerable pose discrepancies between the initial query and reference point clouds (especially in terms of rotation), misaligned point pairs may occur during the alignment process, which will result in inaccurate pose estimation. To mitigate this issue, we introduce an iterative alignment strategy that iteratively performs steps (B) to (D) outlined in Fig. 2. Specifically, the estimated object pose from (D) is fed back into (B) to iterate the focalization of the query point cloud. This iterative strategy facilitates gradual convergence of the query and reference point clouds, ultimately yielding more precise object pose estimation. 3.5. Training Mode During practical deployment, we observe that the initial query and reference point clouds have significant pose discrepancies. Subsequently, their pose differences become smaller after each iteration. Using single GeoTransformer with shared weights during the iterative process (D) in Fig. 2 could adversely affect the accuracy of object pose estimation. Therefore, we train two GeoTransformers (i.e., weights are not shared) with identical structures, one for establishing the initial point-wise alignment and the other for subsequent iteration processes. Detailed rationale for this choice is given in Sec. 4.5 and Tab. 6. Since the object pose is solved from the point-wise alignment relationships, we supervise this alignment using the following cross-entropy loss during training: (cid:16) CE(Ai, pq) + CE(Ai Loss = , pr) (cid:88) (8) (cid:17) , i=1,2 where CE (, ) represents the cross-entropy loss function. pq RNq and pr RNr represent the ground truth for and . Each element pq in pq, corresponding to the in point pi , can be simply obtained using the index of the closest point in from the given ground-truth rotation Rgt and translation tgt. Note that if the nearest point distance exceeds specified threshold (set at 15 centimeters in this paper), we discard that point pair. In addition, the elements in pr are obtained in the same manner. to pi 4. Experiments 4.1. Datasets and Evaluation Metrics Datasets: To validate the effectiveness of the proposed method, we conduct extensive experiments on six publicly available datasets (LineMod [28], LM-O [1], TUD-L [31], IC-BIN [14], HB [35], and YCB-V [77]) and real-world scenes (details are shown in our supplementary material). For fair comparison, we follow the BOP setting [31] to train on the synthetic dataset generated by MegaPose [37]. Evaluation Metrics: 1) Recall of the average point distance (ADD) that is less than 10% of the object diameter (ADD-0.1d) [29]; 2) Area under the curve (AUC) of 5 Method Gen6D [47] Gen6D [47] OnePose [64] OnePose++ [25] Input RGB RGB RGB RGB LatentFusion [57] RGB-D FS6D [27] RGB-D SinRef-6D (Ours) RGB-D Ref. view Recon. -free ape benchwise cam can cat driller duck eggbox Object 200 200 200 16 16 1 - - 11.8 31.2 88.0 74.0 85.7 77. 62.1 92.6 97.3 92.4 86.0 99. 66.1 45.6 88.1 88.0 74.4 88. 73.2 - - 77.2 89.8 88. 86.0 98.3 60.7 40.9 47.9 70. 94.5 98.5 93.0 67.4 48.8 74. 92.5 91.7 81.0 98.7 40.5 16. 34.2 42.3 68.1 68.5 66.6 95. - 71.3 99.7 96.3 100.0 98. glue 87.2 - 37.5 48.0 94. 99.5 99.1 holepuncher iron lamp phone - - 54.9 69.7 82.1 97. 74.6 - - 89.2 97.4 74. 92.5 90.9 - - 87.6 97. 94.7 85.0 97.6 - - 60. 76.0 91.5 99.0 97.4 Mean (%) - - 63.6 76.9 87.1 88.9 90. Table 1. Comparison of SinRef-6D with other manual reference view-based methods on the LineMod dataset [28], evaluated using the ADD-0.1d metric. Ref. and Recon. mean Reference and Reconstruction. represents Gen6D [47] without fine-tuning."
        },
        {
            "title": "Method",
            "content": "Reference view 002 master chef can 003 cracker box 004 sugar box 005 tomato soup can 006 mustard bottle 007 tuna fish can 008 pudding box 009 gelatin box 010 potted meat can 011 banana 019 pitcher base 021 bleach cleanser 024 bowl 025 mug 035 power drill 036 wood block 037 scissors 040 large marker 051 large clamp 052 extra large clamp 061 foam brick MEAN PREDATOR [33] 16 17.4 8.3 15.3 44.4 5.0 34.2 24.2 37.5 20.9 9.9 18.1 48.1 17.4 29.5 12.3 10.0 25.0 38.9 34.4 24.1 35.5 24.3 LoFTR [63] 16 50.6 25.5 13.4 52.9 59.0 55.7 68.1 45.2 45.1 1.6 22.3 16.7 1.4 23.6 1.3 1.4 14.6 8.4 11.2 1.8 31.4 26.2 FS6D-DPM [27] 16 36.8 24.5 43.9 54.2 71.1 53.9 79.6 32.1 54.9 69.1 40.4 44.1 0.9 39.2 19.8 27.9 27.7 74.2 34.7 10.1 45.8 42."
        },
        {
            "title": "Ours",
            "content": "1 44.3 34.4 83.9 53.7 79.9 53.8 44.3 94.6 25.5 65.0 88.2 72.9 31.7 77.7 53.7 0.7 51.2 76.2 21.4 0.4 56.3 52.8 Table 2. Comparison of SinRef-6D with other manual reference view-based methods on the complete YCB-V dataset [77], evaluated using the AUC of ADD metric. ADD [77]; 3) BOP metric: Average Recall (AR) of the VSD, MSSD, and MSPD metrics [31]. Specifically, we first perform quantitative comparisons using the ADD-0.1d and AUC of ADD metrics for each instance in the LineMod [28] and YCB-V [77] datasets, respectively, aligning with manual reference view-based methods [25, 27, 33, 47, 57, 63, 64]. Next, we evaluate our results against CAD modelbased methods [6, 37, 41, 52] on five BOP datasets [31], utilizing the BOP metric for comprehensive comparison. 4.2. Implementation Details The initial resolution of the input RGB images is 640480, which are resized to 224 224 after detection and segmentation. Both the reference and query point clouds contain 2048 points (Nr and Nq). The point-wise feature dimension is set to 256. We use the Adam optimizer for model training with batch size of 6, over total of 2.4 million Method FS6D [27] FoundationPose [73] SinRef-6D (Ours) Ref. view LineMod [28] YCB-V [77] 1 1 1 77.5 87.9 90.3 34.7 47.5 52.8 Table 3. Comparison with single reference-based RGB-D method (other settings unchanged) on LineMod and YCB-V datasets, evaluated on the ADD-0.1d and AUC of ADD metrics, respectively. batches. The learning rate is adjusted using the WarmupCosineLR scheduler, starting from 0 and rapidly increasing to 0.001 during the first 1000 batches, then gradually decreasing until the end of training. All experiments are conducted on single GeForce RTX 4090 GPU. 4.3. Quantitative Comparisons with SOTA Methods Comparison with Manual Reference View-based Methods. Table 1 presents detailed performance comparison of SinRef-6D with other manual reference view-based methods [25, 27, 47, 57, 64] on the LineMod dataset [28] using the ADD-0.1d metric. SinRef-6D outperforms OnePose [64] and OnePose++ [25], achieving comparable accuracy to LatentFusion [57] and FS6D [27]. Additionally, we evaluate SinRef-6D on the complete YCB-V [77] dataset using the AUC of the ADD metric, with per-object accuracy results summarized in Tab. 2. These quantitative results further validate the advantages of SinRef-6D. Moreover, we set the number of reference views to 1 for both FS6D [27] and FoundationPose [73] and experiment on LineMod and YCB-V datasets. Notably, FoundationPose utilizes higher-quality synthetic dataset generated with diffusion model and requires time-consuming 3D reconstruction (making it 10 times slower than SinRef-6D). The results, shown in Tab. 3, further reinforce the advantages of SinRef-6D in the single-reference setting. Overall, the above experimental results demonstrate the effectiveness of SinRef-6D, achieving competitive 6D pose estimation accuracy for novel objects using only single reference view. Comparison with CAD Model-based Methods. Table 4 presents performance comparisons of SinRef-6D with CAD model-based methods [6, 37, 41, 52] on five popular datasets using the BOP metric. We experiment using there detection/segmentation methods. Specifically, we first"
        },
        {
            "title": "Method",
            "content": "SinRef-6D (Ours) MegaPose [37] MegaPose* [37] MegaPose* [37] ZeroPose [6] ZeroPose* [6] SAM-6D [41] SinRef-6D (Ours) MegaPose [37] MegaPose* [37] ZeroPose* [6] GigaPose [52] GigaPose* [52] SAM-6D [41] SinRef-6D (Ours)"
        },
        {
            "title": "Input",
            "content": "RGB-D RGB RGB RGB-D RGB-D RGB-D RGB-D RGB-D RGB RGB RGB-D RGB RGB RGB-D RGB-D CAD model -free Detection / Segmentation Single Ref.-based CNOS Mask R-CNN [24] CNOS (FastSAM) [50] LM-O 48.4 18.7 53.7 58.3 26.1 56.2 12.9 61.8 22.9 49.9 53.8 29.6 59.8 10.4 56.5 TUD-L 62.5 20.5 58.4 71.2 61.1 87.2 37.9 88.9 25.8 65.3 83.5 30.0 63.1 30.1 77.4 Dataset IC-BIN 31.6 15.3 43.6 37.1 24.7 41.8 11.2 44.0 15.2 36.7 39.2 22.3 47.3 9.4 35. HB 50.7 18.6 72.9 75.7 38.2 68.2 25.2 63.3 25.1 65.4 65.3 34.1 72.2 29.0 61.0 YCB-V 56.9 13.9 60.4 63.3 29.5 58.4 22.4 65.1 28.1 60.1 65.3 27.8 66.1 21.8 62.2 Mean (%) Time (s) 50.0 17.4 57.8 61.1 35.9 62.4 21.9 64.6 23.4 55.5 61.4 28.8 61.7 20.1 58.6 0.7 25.6 - 93.3 - - 0.3 0.4 16.6 33.9 17.6 0.4 8.5 1.2 1. Table 4. Comparison of SinRef-6D with CAD model-based methods on the LM-O [1], TUD-L [31], IC-BIN [14], HB [35], and YCB-V [77] datasets. We leverage the BOP metric and the mean time across all datasets for evaluation. denotes using the pose refinement method of MegaPose [37]. means that SAM-6D [41] only uses single reference view during testing, with other settings unchanged. Figure 5. The qualitative comparison results on the LineMod dataset [28] are presented, visualizing the outputs of Gen6D [47], our SinRef-6D, and ground truth from top to bottom. show the experimental results using only our single reference view as the template image for CNOS segmentation [50] in the first row. Then, when Mask R-CNN [24] is used to segment the input images, SinRef-6D outperforms MegaPose [37] and ZeroPose [6]. Remarkably, this accuracy even exceeds the performance of these two methods after they leverage the refinement method introduced by MegaPose [37]. Next, when we employ zero-shot CNOS [50] for segmentation, SinRef-6D also achieves competitive accuracy. We also note an increase in inference time. This is primarily because CNOS often segments multiple instances for the same object, resulting in repeated pose estimations. Note that the comparison methods all rely on textured CAD models, requiring specialized equipment for acquisition. Additionally, the refinement process of MegaPose [37] is notably slow and also relies on object CAD models (that is why we do not use it for refinement). In contrast, SinRef-6D is CAD modeland refinement-free, offering enhanced scalability and efficiency. Overall, SinRef-6D Figure 6. The qualitative comparison results on the LM-O [1], TUD-L [31], IC-BIN [14], and YCB-V [77] datasets. We visualize the results of MegaPose [37], ZeroPose [6], our SinRef-6D, and ground truth from top to bottom. demonstrates performance on par with CAD model-based methods, while operating in CAD model-free setup, showcasing its effectiveness and scalability. 4.4. Qualitative Analysis Comparison with Manual Reference View-based Methods. The comparison between SinRef-6D and Gen6D [47] on the LineMod dataset [28] is presented in Fig. 5. These experimental results highlight the superior performance of our method, which can perform novel object pose estimation in cluttered scenes. Specifically, Gen6D [47] relies on dense reference views and template matching to estimate object poses, which requires full coverage of the reference view angles. When the number of reference views is limited or of low quality, pose estimation errors will signifi-"
        },
        {
            "title": "Row Method\nA\nB\nC\nD\nE\nF",
            "content": "w/o RGB w/o Points Focalization RGB SSM DINOv2 [54] RGB SSM ViT [13] Points SSM PT [83] Full Model AR 39.5 0.0 56.9 52.8 60.9 62.2 Param. (M) 138.8 643.6 1238.8 976.7 708.6 691.8 Table 5. Ablation of SinRef-6D components on the YCB-V dataset [77]. Param. means total model parameters."
        },
        {
            "title": "Training iterations",
            "content": "1 2 3 Time (s) Inference iterations, AR (%) 1 37.8 - - 0.70 2 35.0 61.7 - 0.99 3 36.5 62.2 62.2 1.26 4 35.9 62.5 62.6 1. Table 6. Ablation on the number of iterations for point-wise alignment. Note that the time is only on the YCB-V dataset [77]. curacy of point-wise alignment. Next, we eliminate the point cloud focalization process, directly feeding the reference and query point clouds Pr and Pq from part (B) of Fig. 2 into the model. Therefore, the iterative training of the GeoTransformer is also removed. The experimental results (Tab. 5 B) reveal substantial drop in performance, underscoring the significance of the focalization step. We attribute this decline to the larger numerical discrepancies that may arise between the reference and query point clouds without the focalization process. Further, we replace the proposed RGB SSM with DINOv2 [54] (a large vision foundation model) and Vision Transformer [13], and also replace Points SSM with Point Transformer [83]. The corresponding results (Tab. 5 C, D, and E) confirm the effectiveness of SSMs in improving the accuracy of pointwise alignment while reducing the model parameter count. Impact of the Number of Iterations for Point-wise Alignment. Identifying an optimal balance between accuracy and computational efficiency requires careful selection of iteration counts during training and inference. Specifically, the training iterations refer to the number of times the GeoTransformer weights are updated during training, while the inference iterations indicate the number of GeoTransformer iterations during inference. If the inference iterations exceed the training iterations, the difference represents how many times the last GeoTransformer weights from training are repeatedly applied. The results, summarized in Tab. 6, reveal trade-off among accuracy, speed, and model complexity. Based on these findings, we select 2 training and 3 inference iterations as the optimal configuration to achieve the best balance. Impact of Random Reference View Selection. We study the impact of our random reference view selection manner (see Sec. 3.1 for details) on the 6D pose estimation performance. Experimental results in Tab. 7 show that SinRef-6D is highly robust to random reference view. Figure 7. Some failure cases. The top row shows single reference view (randomly selected RGB-D image as described in Sec. 3.1), and the bottom row displays the estimated object pose in query view. As observed, the accuracy of SinRef-6D decreases when the query view is top-down view or the object is reflective metal. cantly increase. In contrast, SinRef-6D abandons template matching and instead leverages iterative alignment between the reference and query views, enabling effective pose estimation of novel objects using only single reference view. Comparison with CAD Model-based Methods. Figure 6 compares SinRef-6D with MegaPose [37] and ZeroPose [6] across four evaluation datasets, all of which use RGB-D input and CNOS [50] for segmentation. These datasets cover diverse range of scenes and novel objects. The experimental results further demonstrate that our method outperforms the comparison methods in terms of robustness, effectively estimating the 6D pose of novel objects even in challenging scenes with occlusions and clutter. Notably, both MegaPose [37] and ZeroPose [6] require textured CAD models of these novel objects, with MegaPose [37] also depending on the time-consuming render-and-compare process. In contrast, SinRef-6D is simple yet effective and CAD-free method, offering greater scalability. Additional and realworld qualitative results are in the supplementary material. Failure Cases Analysis. Since SinRef-6D only uses single reference view captured from an oblique angle, pose estimation accuracy may decrease when the query view does not adequately capture the objects geometric features, such as in top-down views. Furthermore, for objects with incomplete depth information, like reflective metals or transparent materials, establishing accurate point-wise alignments becomes challenging, leading to decrease in pose estimation accuracy. The visualization of some failure cases is shown in Fig. 7. As result, we do not evaluate the T-LESS [30] (includes many top-down views) and ITODD [15] (features top-down views and metallic objects) datasets. Our future work will focus on enhancing the robustness of SinRef-6D in such challenging scenes and objects. 4.5. Ablation Study Effectiveness of Main Components. We conduct thorough ablation study on the main components in SinRef6D to verify their effectiveness. Specifically, we first remove the RGB image component from SinRef-6D, meaning that RGB images are not used during either training or inference. The experimental results (Tab. 5 A) show that RGB images play crucial role in enhancing the acDataset YCB-V [77] LM-O [1] Times Mean 62.10 56.58 20 20 Variance 0.31 0.43 Table 7. Ablation of random reference view selection, we report the mean and variance of 20 times experiments on BOP metric. 5. Conclusion We proposed SinRef-6D, which is simple yet effective framework to tackle the challenges of novel object 6D pose estimation posed by textured CAD models and dense reference views. SinRef-6D iteratively establishes point-wise alignment in the camera coordinate system using our proposed SSMs for reference and query views. By relying solely on single reference image, it achieves CAD modelfree novel object 6D pose estimation, thereby significantly enhancing the scalability for real-world applications. Acknowledgments: This work was supported by the National Natural Science Foundation of China under Grant U22A2059 and Grant 62473141, China Scholarship Council under Grant 202306130074, and Natural Science Foundation of Hunan Province under Grant 2024JJ5098."
        },
        {
            "title": "References",
            "content": "[1] Eric Brachmann, Alexander Krull, Frank Michel, Stefan Gumhold, Jamie Shotton, and Carsten Rother. Learning 6d object pose estimation using 3d object coordinates. In European Conference on Computer Vision, pages 536551, 2014. 5, 7, 9, 2, 3 [2] Dingding Cai, Janne Heikkila, and Esa Rahtu. Gs-pose: Cascaded framework for generalizable segmentation-based 6d object pose estimation. arXiv preprint arXiv:2403.10683, 2024. 3 [3] Tuo Cao, Fei Luo, Yanping Fu, Wenxiao Zhang, Shengjie Zheng, and Chunxia Xiao. Dgecn: depth-guided edge convolutional network for end-to-end 6d pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 37833792, 2022. 1 [4] Andrea Caraffa, Davide Boscaini, Amir Hamza, and Fabio Poiesi. Freeze: Training-free zero-shot 6d pose estimation with geometric and vision foundation models. In European Conference on Computer Vision, 2024. 2 [5] Pedro Castro and Tae-Kyun Kim. Posematcher: One-shot In 6d object pose estimation by deep feature matching. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21482157, 2023. 2 [6] Jianqiu Chen, Mingshan Sun, Tianpeng Bao, Rui Zhao, Liwei Wu, and Zhenyu He. Zeropose: Cad-model-based zero-shot pose estimation. arXiv preprint arXiv:2305.17934, 2023. 2, 6, 7, 8 [7] Kai Chen, Rui Cao, Stephen James, Yichuan Li, Yun-Hui Liu, Pieter Abbeel, and Qi Dou. Sim-to-real 6d object pose estimation via iterative self-training for robotic bin picking. In European Conference on Computer Vision, pages 533 550, 2022. [8] Yamei Chen, Yan Di, Guangyao Zhai, Fabian Manhardt, Chenyangguang Zhang, Ruida Zhang, Federico Tombari, Nassir Navab, and Benjamin Busam. Secondpose: Se (3)- consistent dual-stream feature fusion for category-level pose In Proceedings of the IEEE/CVF Conference estimation. on Computer Vision and Pattern Recognition, pages 9959 9969, 2024. 1 [9] Jaime Corsetti, Davide Boscaini, Changjae Oh, Andrea Cavallaro, and Fabio Poiesi. Open-vocabulary object 6d pose In Proceedings of the IEEE/CVF Conference estimation. on Computer Vision and Pattern Recognition, pages 18071 18080, 2024. 1 [10] Zheng Dang, Lizhou Wang, Yu Guo, and Mathieu Salzmann. Match normalization: Learning-based point cloud registration for 6d object pose estimation in the real world. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(06):44894503, 2024. 1 [11] Yan Di, Fabian Manhardt, Gu Wang, Xiangyang Ji, Nassir Navab, and Federico Tombari. So-pose: Exploiting selfocclusion for direct 6d pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1239612405, 2021. 1 [12] Yan Di, Ruida Zhang, Zhiqiang Lou, Fabian Manhardt, Xiangyang Ji, Nassir Navab, and Federico Tombari. Gpv-pose: Category-level object pose estimation via geometry-guided In Proceedings of the IEEE/CVF Conpoint-wise voting. ference on Computer Vision and Pattern Recognition, pages 67816791, 2022. 1 [13] Alexey Dosovitskiy et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [14] Andreas Doumanoglou, Rigas Kouskouridas, Sotiris Malassiotis, and Tae-Kyun Kim. Recovering 6d object pose and In Proceedings of predicting next-best-view in the crowd. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 35833592, 2016. 5, 7, 2, 4 [15] Bertram Drost, Markus Ulrich, Paul Bergmann, Philipp Hartinger, and Carsten Steger. Introducing mvtec itodd-a dataset for 3d object recognition in industry. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pages 22002208. 8 [16] Yuming Du, Yang Xiao, Michael Ramamonjisoa, Vincent Lepetit, et al. Pizza: powerful image-only zero-shot zerocad approach to 6 dof tracking. In International Conference on 3D Vision, pages 515525, 2022. 3 [17] Zhiwen Fan, Panwang Pan, Peihao Wang, Yifan Jiang, Dejia Xu, and Zhangyang Wang. Pope: 6-dof promptable pose estimation of any object in any scene with one reference. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 77717781, 2024. 1 [18] Bowen Fu, Sek Kun Leong, Xiaocong Lian, and Xiangyang Ji. 6d robotic assembly based on rgb-only object pose estimation. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 47364742, 2022. 1 [19] Ning Gao, Vien Anh Ngo, Hanna Ziesche, and Gerhard Neumann. Sa6d: Self-adaptive few-shot 6d pose estimator for In 7th Annual Conference on novel and occluded objects. Robot Learning, 2023. 3 [20] Minghao Gou, Haolin Pan, Hao-Shu Fang, Ziyuan Liu, Cewu Lu, and Ping Tan. Unseen object 6d pose esarXiv preprint timation: benchmark and baselines. arXiv:2206.11808, 2022. 1 [21] Albert Gu and Tri Dao. Mamba: Linear-time sequence arXiv preprint modeling with selective state spaces. arXiv:2312.00752, 2023. 4, 5 [22] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. 4 [23] Frederik Hagelskjær and Rasmus Laurvig Haugaard. Keymatchnet: Zero-shot pose estimation in 3d point clouds arXiv preprint by generalized keypoint matching. arXiv:2303.16102, 2023. 1 [24] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 29612969, 2017. 4, 7, [25] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. Onepose++: Keypoint-free oneshot object pose estimation without cad models. In Advances in Neural Information Processing Systems, pages 35103 35115, 2022. 2, 6 [26] Yisheng He, Haibin Huang, Haoqiang Fan, Qifeng Chen, and Jian Sun. Ffb6d: full flow bidirectional fusion network for 6d pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 30033013, 2021. 1 [27] Yisheng He, Yao Wang, Haoqiang Fan, Jian Sun, and Qifeng Chen. Fs6d: Few-shot 6d pose estimation of novel objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68146824, 2022. 2, 6 [28] Stefan Hinterstoisser, Stefan Holzer, Cedric Cagniart, Slobodan Ilic, Kurt Konolige, Nassir Navab, and Vincent Lepetit. Multimodal templates for real-time detection of texture-less In Proceedings of the objects in heavily cluttered scenes. IEEE/CVF International Conference on Computer Vision, pages 858865, 2011. 5, 6, 7, 2 [29] Stefan Hinterstoisser, Vincent Lepetit, Slobodan Ilic, Stefan Holzer, Gary Bradski, Kurt Konolige, and Nassir Navab. Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes. In Asian Conference on Computer Vision, pages 548562, 2013. 5 [30] Tomaˇs Hodan, Pavel Haluza, ˇStepan Obdrˇzalek, Jiri Matas, Manolis Lourakis, and Xenophon Zabulis. T-less: An rgbd dataset for 6d pose estimation of texture-less objects. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 880888, 2017. 8 [31] Tomas Hodan, Martin Sundermeyer, Yann Labbe, Van Nguyen Nguyen, Gu Wang, Eric Brachmann, Bertram Drost, Vincent Lepetit, Carsten Rother, and Jiri Matas. Bop challenge 2023 on detection segmentation and pose estimation of seen and unseen rigid objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 56105619, 2024. 5, 6, 7, 2, 3 [32] Junwen Huang, Hao Yu, Kuan-Ting Yu, Nassir Navab, Slobodan Ilic, and Benjamin Busam. Matchu: Matching unseen objects for 6d pose estimation from rgb-d images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1009510105, 2024. 2 [33] Shengyu Huang, Zan Gojcic, Mikhail Usvyatsov, Andreas Wieser, and Konrad Schindler. Predator: Registration of In Proceedings of the 3d point clouds with low overlap. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42674276, 2021. 6 [34] HyunJun Jung, Shun-Cheng Wu, Patrick Ruhkamp, et al. Housecat6da large-scale multi-modal category level 6d object pose dataset with household objects in realistic scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2249822508, 2024. 1 [35] Roman Kaskman, Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic. Homebreweddb: Rgb-d dataset for 6d pose estimation of 3d objects. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 2019. 5, 7, 2 [36] Akshay Krishnan, Abhijit Kundu, Kevis-Kokitsi Maninis, James Hays, and Matthew Brown. Ominnocs: unified nocs dataset and model for 3d lifting of 2d objects. In European Conference on Computer Vision, pages 127145, 2024. 1 [37] Yann Labbe, Lucas Manuelli, Arsalan Mousavian, Stephen Tyree, Stan Birchfield, Jonathan Tremblay, Justin Carpentier, Mathieu Aubry, Dieter Fox, and Josef Sivic. Megapose: 6d pose estimation of novel objects via render & compare. In Proceedings of the 6th Conference on Robot Learning, 2022. 2, 5, 6, 7, 8 [38] JongMin Lee, Yohann Cabon, Romain Bregier, Sungjoo Yoo, and Jerome Revaud. Mfos: Model-free & one-shot object pose estimation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 29112919, 2024. 2 [39] Taeyeop Lee, Jonathan Tremblay, Valts Blukis, Bowen Wen, Byeong-Uk Lee, Inkyu Shin, Stan Birchfield, In So Kweon, Tta-cope: Test-time adaptation for and Kuk-Jin Yoon. In Proceedings of category-level object pose estimation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2128521295, 2023. [40] Hongyang Li, Jiehong Lin, and Kui Jia. Dcl-net: Deep correspondence learning network for 6d pose estimation. In European Conference on Computer Vision, pages 369385, 2022. 1 [41] Jiehong Lin, Lihua Liu, Dekun Lu, and Kui Jia. Sam-6d: Segment anything model meets zero-shot 6d object pose esIn Proceedings of the IEEE/CVF Conference on timation. Computer Vision and Pattern Recognition, pages 27906 27916, 2024. 2, 6, 7 [42] Jianhui Liu, Yukang Chen, Xiaoqing Ye, and Xiaojuan Qi. Ist-net: Prior-free category-level pose estimation with implicit space transformation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13978 13988, 2023. 1 [43] Jian Liu, Wei Sun, Hui Yang, Zhiwen Zeng, Chongpei Liu, Jin Zheng, Xingyu Liu, Hossein Rahmani, Nicu Sebe, and Ajmal Mian. Deep learning-based object pose estimation: comprehensive survey. arXiv preprint arXiv:2405.07801, 2024. 1 10 [44] Jian Liu, Wei Sun, Hui Yang, Pengchao Deng, Chongpei Liu, Nicu Sebe, Hossein Rahmani, and Ajmal Mian. Diff9d: Diffusion-based domain-generalized category-level 9-dof object pose estimation. arXiv preprint arXiv:2502.02525, 2025. [45] Xingyu Liu, Gu Wang, Yi Li, and Xiangyang Ji. Catre: Iterative point clouds alignment for category-level object pose In European Conference on Computer Vision, refinement. pages 499516, 2022. 1 [46] Xingyu Liu, Gu Wang, Ruida Zhang, Chenyangguang Zhang, Federico Tombari, and Xiangyang Ji. Unopose: Unseen object pose estimation with an unposed rgb-d reference image. arXiv preprint arXiv:2411.16106, 2024. 2 [47] Yuan Liu, Yilin Wen, Sida Peng, Cheng Lin, Xiaoxiao Long, Taku Komura, and Wenping Wang. Gen6d: Generalizable model-free 6-dof object pose estimation from rgb images. In European Conference on Computer Vision, pages 298315, 2022. 3, 6, 7, 2 [48] Sungphill Moon, Hyeontae Son, Dongcheol Hur, and Sangwook Kim. Genflow: Generalizable recurrent flow for In Proceedings of 6d pose refinement of novel objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1003910049, 2024. 2 [49] Van Nguyen Nguyen, Yinlin Hu, Yang Xiao, Mathieu Salzmann, and Vincent Lepetit. Templates for 3d object pose estimation revisited: Generalization to new objects and robustness to occlusions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67716780, 2022. 1 [50] Van Nguyen Nguyen, Thibault Groueix, Georgy Ponimatkin, Vincent Lepetit, and Tomas Hodan. Cnos: strong baseline for cad-based novel object segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21342140, 2023. 4, 7, [51] Van Nguyen Nguyen, Thibault Groueix, Georgy Ponimatkin, Yinlin Hu, Renaud Marlet, Mathieu Salzmann, and Vincent Lepetit. Nope: Novel object pose estimation from sinIn Proceedings of the IEEE/CVF Conference gle image. on Computer Vision and Pattern Recognition, pages 17923 17932, 2024. 1 [52] Van Nguyen Nguyen, Thibault Groueix, Mathieu Salzmann, and Vincent Lepetit. Gigapose: Fast and robust novel object pose estimation via one correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99039913, 2024. 2, 4, 6, 7 [53] Brian Okorn, Qiao Gu, Martial Hebert, and David Held. In IEEE InZephyr: Zero-shot pose hypothesis rating. ternational Conference on Robotics and Automation, pages 1414114148, 2021. 1 [54] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 8 [55] Evin Pınar Ornek, Yann Labbe, Bugra Tekin, Lingni Ma, Cem Keskin, Christian Forster, and Tomas Hodan. Foundpose: Unseen object pose estimation with foundation features. In European Conference on Computer Vision, pages 163182, 2024. [56] Panwang Pan, Zhiwen Fan, Brandon Feng, Peihao Wang, Chenxin Li, and Zhangyang Wang. Learning to estimate 6dof pose from limited data: few-shot, generalizable approach using rgb images. In International Conference on 3D Vision, pages 10591071, 2024. 1 [57] Keunhong Park, Arsalan Mousavian, Yu Xiang, and Dieter Fox. Latentfusion: End-to-end differentiable reconstruction and rendering for unseen object pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1071010719, 2020. 3, 6 [58] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. Pvnet: Pixel-wise voting network for 6dof pose In Proceedings of the IEEE/CVF Conference estimation. on Computer Vision and Pattern Recognition, pages 4561 4570, 2019. 1 [59] Giorgia Pitteri, Slobodan Ilic, and Vincent Lepetit. Cornet: Generic 3d corners for 6d pose estimation of new objects without retraining. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 2019. 1 [60] Giorgia Pitteri, Aurelie Bugeau, Slobodan Ilic, and Vincent Lepetit. 3d object detection and pose estimation of unseen objects in color images with local surface embeddings. In Proceedings of the Asian Conference on Computer Vision, 2020. 1 [61] Zheng Qin, Hao Yu, Changjian Wang, Yulan Guo, Yuxing Peng, and Kai Xu. Geometric transformer for fast and robust In Proceedings of the IEEE/CVF point cloud registration. Conference on Computer Vision and Pattern Recognition, pages 1114311152, 2022. 5 [62] Ivan Shugurov, Fu Li, Benjamin Busam, and Slobodan Ilic. Osop: multi-stage one shot object pose estimation framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68356844, 2022. [63] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching In Proceedings of the IEEE/CVF Conwith transformers. ference on Computer Vision and Pattern Recognition, pages 89228931, 2021. 6 [64] Jiaming Sun, Zihao Wang, Siyu Zhang, Xingyi He, Hongcheng Zhao, Guofeng Zhang, and Xiaowei Zhou. Onepose: One-shot object pose estimation without cad models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68256834, 2022. 2, 6 [65] Martin Sundermeyer, Maximilian Durner, En Yen Puang, Zoltan-Csaba Marton, Narunas Vaskevicius, Kai Arras, and Rudolph Triebel. Multi-path learning for object pose estimation across domains. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1391613925, 2020. 1 [66] Stephen Tyree, Jonathan Tremblay, Thang To, Jia Cheng, Terry Mosier, Jeffrey Smith, and Stan Birchfield. 6-dof pose estimation of household objects for robotic manipulation: 11 [78] Li Xu, Haoxuan Qu, Yujun Cai, and Jun Liu. 6d-diff: keypoint diffusion framework for 6d object pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 96769686, 2024. [79] Yuzhong Zhao Hongtian Yu Lingxi Xie Yaowei Wang Qixiang Ye Yue Liu, Yunjie Tian and Yunfan Liu. Vmamba: Visual state space model. In Advances in Neural Information Processing Systems, 2024. 5 [80] Jiyao Zhang, Weiyao Huang, Bo Peng, Mingdong Wu, Fei Hu, Zijian Chen, Bo Zhao, and Hao Dong. Omni6dpose: benchmark and model for universal 6d object pose estimation and tracking. In European Conference on Computer Vision, 2024. 1 [81] Mengchen Zhang, Tong Wu, Tai Wang, Tengfei Wang, Ziwei Liu, and Dahua Lin. Omni6d: Large-vocabulary 3d object dataset for category-level 6d object pose estimation. In European Conference on Computer Vision, pages 216232, 2024. 1 [82] Chen Zhao, Yinlin Hu, and Mathieu Salzmann. Locposenet: Robust location prior for unseen object pose estimation. In International Conference on 3D Vision, pages 10721081, 2024. 1 [83] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and In Proceedings of the Vladlen Koltun. Point transformer. IEEE/CVF International Conference on Computer Vision, pages 1625916268, 2021. 8 [84] Heng Zhao, Shenxing Wei, Dahu Shi, Wenming Tan, Zheyang Li, Ye Ren, Xing Wei, Yi Yang, and Shiliang Pu. Learning symmetry-aware geometry correspondences for 6d object pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14045 14054, 2023. [85] Linfang Zheng, Tze Ho Elden Tse, Chen Wang, Yinghan Sun, Hua Chen, Ales Leonardis, Wei Zhang, and Hyung Jin Chang. Georef: Geometric alignment across shape variation for category-level object pose refinement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1069310703, 2024. 1 [86] Jun Zhou, Kai Chen, Linlin Xu, Qi Dou, and Jing Qin. Deep fusion transformer network with weighted vector-wise keypoints voting for robust 6d object pose estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1396713977, 2023. 1 An accessible dataset and benchmark. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 1308113088, 2022. 1 [67] Gu Wang, Fabian Manhardt, Jianzhun Shao, Xiangyang Ji, Nassir Navab, and Federico Tombari. Self6d: SelfIn Eurosupervised monocular 6d object pose estimation. pean Conference on Computer Vision, pages 108125, 2020. 1 [68] Gu Wang, Fabian Manhardt, Federico Tombari, and Xiangyang Ji. Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1661116621, 2021. 1 [69] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas Guibas. Normalized object coordinate space for category-level 6d object pose and size In Proceedings of the IEEE/CVF Conference estimation. on Computer Vision and Pattern Recognition, pages 2642 2651, 2019. [70] Tianfu Wang, Guosheng Hu, and Hongguang Wang. Object pose estimation via the aggregation of diffusion features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1023810247, 2024. 2 [71] Bowen Wen, Chaitanya Mitash, Baozhang Ren, and Kostas Bekris. Se(3)-tracknet: Data-driven 6d pose tracking by calibrating image residuals in synthetic domains. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 1036710373, 2020. 1 [72] Bowen Wen, Jonathan Tremblay, Valts Blukis, Stephen Tyree, Thomas Muller, Alex Evans, Dieter Fox, Jan Kautz, and Stan Birchfield. Bundlesdf: Neural 6-dof tracking and In Proceedings of 3d reconstruction of unknown objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 606617, 2023. 1 [73] Bowen Wen, Wei Yang, Jan Kautz, and Stan Birchfield. Foundationpose: Unified 6d pose estimation and tracking of novel objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17868 17879, 2024. 2, 3, 6 [74] Yijia Weng, He Wang, Qiang Zhou, Yuzhe Qin, Yueqi Duan, Qingnan Fan, Baoquan Chen, Hao Su, and Leonidas Guibas. Captra: Category-level pose tracking for rigid and articulated objects from point clouds. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1320913218, 2021. 1 [75] Jun Wu, Yue Wang, and Rong Xiong. Unseen object pose In IEEE International Conferestimation via registration. ence on Real-time Computing and Robotics, pages 974979, 2021. 1 [76] Yangzheng Wu, Mohsen Zand, Ali Etemad, and Michael Greenspan. Vote from the center: 6 dof pose estimation in rgb-d images by radial keypoint voting. In European Conference on Computer Vision, pages 335352, 2022. 1 [77] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. Posecnn: convolutional neural network for 6d object pose estimation in cluttered scenes. arXiv preprint arXiv:1711.00199, 2017. 5, 6, 7, 8, 9, 2, 12 Novel Object 6D Pose Estimation with Single Reference View"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 8. Qualitative results of SinRef-6D deployment in real-world robotic manipulation scenarios. The left column displays the reference view of novel object, while the other columns correspond to query views. Notably, the reference view is not carefully selected. From practical application perspective, reference views are typically acquired manually. As such, we randomly select normal view (free of occlusion, with minimal self-occlusion, and resembling typical robotics manipulation view) using an Intel RealSense L515 RGBD camera as the reference view. To the best of our knowledge, we are the first to present method for novel object 6D absolute pose estimation using only single reference view in real-world robotic manipulation scenarios. This approach simultaneously eliminates the need for object CAD models, dense reference views, and model retraining, offering enhanced efficiency and scalability while demonstrating strong generalization to potential real-world robotic applications. 1 Figure 9. Qualitative comparison results on the HB dataset [35]. Note that we cannot obtain the ground-truth poses for the HB dataset, as its evaluation is conducted on the official BOP Challenge [31]. This document aims to complement the main paper. Specifically, we first include real-world deployment experiments to demonstrate its superior scalability, where single reference view of novel object is manually captured and directly used for 6D pose estimation (without retraining or CAD model). We then add more qualitative comparisons across the HB [35], LM-O [1], TUD-L [31], IC-BIN [14], YCB-V [77], and LineMod [28] datasets to further validate the effectiveness of SinRef-6D. A. Real-world Deployment To demonstrate the high scalability of SinRef-6D in realworld scenes, we conduct experiments with randomly selected novel objects (unseen during training). Specifically, we provide only single reference view for each novel object and estimate its 6D poses across multiple viewing angles. Both the reference and query views are segmented using Mask R-CNN [24]. Some qualitative results are presented in Fig. 8, where the first column represents the reference view, and the other four columns represent the query views under different perspectives. These results demonstrate that SinRef-6D can seamlessly extend to novel objects in real-world scenes without retraining the model or relying on CAD models. Furthermore, we include demo in the supplementary material to provide more intuitive demonstration of real-world deployment. B. More Qualitative Comparison Due to space limitations in the main text, we provide additional qualitative comparisons here to further demonstrate the effectiveness of SinRef-6D. Specifically, Fig. 9, Fig. 10, and Fig. 11 illustrate comparisons with CAD model-based MegaPose [37] and ZeroPose [6] across the HB [35], LM-O [1], TUD-L [31], IC-BIN [14], and YCB-V [77] datasets. Fig. 12 presents comparison with the manual reference view-based Gen6D [47] on the LineMod dataset [28]. These extensive qualitative comparisons further validate the robustness of SinRef-6D, showcasing its ability to reliably estimate 6D poses of novel objects in diverse instances and scenes. 2 Figure 10. Qualitative comparison results on the LM-O [1] and TUD-L [31] datasets. Figure 11. Qualitative comparison results on the IC-BIN [14] and YCB-V [77] datasets. 4 Figure 12. Qualitative comparison results on the LineMod dataset [28]."
        }
    ],
    "affiliations": [
        "Central South University",
        "Hunan University",
        "Lancaster University",
        "Nanyang Technological University",
        "The University of Western Australia"
    ]
}