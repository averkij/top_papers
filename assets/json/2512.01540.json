{
    "paper_title": "FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention",
    "authors": [
        "Zipeng Wang",
        "Dan Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 0 4 5 1 0 . 2 1 5 2 : r FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention Zipeng Wang zwang253@cse.ust.hk Dan Xu danxu@cse.ust.hk"
        },
        {
            "title": "The Hong Kong University of Science and Technology",
            "content": "Figure 1. FlashVGGT achieves significant speedup and scales to larger inputs. Our method enables both fast single-forward inference on long sequences (left) and memory-efficient online inference (center) while maintaining competitive accuracy versus VGGT [54] (right)."
        },
        {
            "title": "Abstract",
            "content": "3D reconstruction from multi-view images is core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott. github.io/flashvggt_page/. 1. Introduction Reconstructing 3D geometry from multi-view images is fundamental problem in computer vision [19]. Traditional pipelines such as Structure-from-Motion (SfM) [24, 39, 43, 48] and Multi-View Stereo (MVS) [35, 44, 45] have dominated this task for decades. These methods rely on perscene, iterative optimization pipelines that include feature detection, matching, triangulation, and bundle adjustment. While often accurate, these pipelines are computationally intensive and fragile, requiring extensive processing and 1 soning while scaling to long input sequences. Motivated by these insights, we propose FlashVGGT, an efficient architecture that overcomes the computation bottleneck of VGGT through compressed descriptor attentions. Our core innovation reformulates the global attention block by generating compact set of descriptor tokens via spatial resampling, which encapsulate key information from each frame. Global attention is then approximated via cross-attention from image tokens to descriptors (i.e., using image tokens as queries and descriptors as keys/values). This design reduces the computational complexity of global attention from O(N 2) to O(N 2/r2), where is the spatial compression ratio. Empirically, FlashVGGT achieves over 90% inference speedup on 1,000-image sequences, with accuracy comparable to VGGT. Furthermore, the compactness of our descriptor tokens enables scalable inference for very long sequences (e.g., 3,000 images) through an chunk-recursive scheme. When processing sequences that exceed memory limits, we divide the input into sequential chunks. By caching and reusing descriptor tokens from previous chunks, later chunks incorporate historical context while maintaining global receptive field across the entire sequence. Crucially, unlike StreamVGGT [64], which caches full-resolution tokens from all transformer layers, our method only stores the compressed descriptors. This achieves an r2 reduction in peak memory usage, enabling scalable reconstruction for substantially larger inputs and resource-constrained scenarios. The main contributions of this work are summarized as follows: (i) We propose FlashVGGT, an efficient framework that alleviates the quadratic complexity of global attention in VGGT. (ii) We design chunk-recursive inference mechanism that enables online reconstruction of long sequences using cached descriptors. (iii) Extensive experiments demonstrate that FlashVGGT achieves competitive accuracy while reducing inference time by over 90% on 1,000-image inputs and scales to over 3,000 views. 2. Related Work Feed-forward 3D Reconstruction. An emerging paradigm in 3D reconstruction is to directly predict 3D structures from images using deep-learning models trained on large datasets. This paradigm, often referred to as feed-forward 3D reconstruction, replaces per-scene iterative optimization with one or several forward passes of neural network, offering greater efficiency and robustness than traditional methods. Early efforts in this line of research [25, 56, 61] predict pairwise 3D point maps and employ per-scene global alignment to reconstruct from multiple views. Subsequent works [22, 57, 59] sought to directly predict 3D structures from multiple images in single forward pass. Notably, VGGT [54] introduced transformer-based architecture with alternating frame-wise and global attention (a) Computational Breakdown of VGGT (b) Histogram of Attention Scores Distributions Figure 2. (a) The global attention block is the primary computational bottleneck in VGGT, dominating total inference time. (b) Global attention is highly sparse, with most scores concentrated near zero, suggesting that full self-attention is highly inefficient. In contrast, frame attention exhibits more uniform distribution. careful tuning for each scene, especially under challenging conditions [33, 42]. Recent advances have shifted toward learning-based approaches that bypass the complexities of traditional pipelines by directly predicting 3D structure using neural networks [25, 53, 56, 59]. These models are trained end-toend on large-scale datasets [3, 12, 20, 37, 41, 60], enabling direct 3D prediction from multi-view input. This feedforward paradigm eliminates the need for sequential postprocessing and per-scene optimization. Moreover, they exhibit improved robustness by learning strong priors from diverse, large-scale data. recent milestone in this direction is the Visual Geometry Grounding Transformer (VGGT) [54], which performs high-fidelity 3D reconstruction from hundreds of views in single forward pass. VGGTs success stems from an alternating attention backbone that combines frame-wise and global attention blocks [16, 50], enabling effective aggregation of frame and global context. However, this architecture has key limitation: the global attention block requires selfattention over all image tokens. As shown in Fig. 2a, this leads to quadratic complexity, creating severe bottleneck and limiting scalability of VGGT to long sequences. This work is driven by central question: Is full selfattention truly necessary for global reasoning in VGGT? We base our approach on two key insights. First, classical methods show that accurate inter-frame associations can be inferred from sparse keypoints and descriptors [4, 32], suggesting that dense token-to-token attention may be unnecessary. Second, we observe that VGGTs global attention maps are inherently sparse, with most scores near zero (Fig. 2b), implying that much of the computation is spent on irrelevant token pairs. These observations motivate our quest for more efficient alternative that retains global reablocks. This approach enables reconstruction from hundreds of images in single forward pass with high accuracy. However, VGGTs global attention module requires full self-attention over all image tokens, causing computational cost to scale nearly quadratically with the number of input images. This results in significant overhead for largescale inputs. Another line of research [11, 15, 55] performs 3D reconstruction in an online manner. While these methods offer improved memory efficiency, their reconstruction accuracy often lags behind that of offline counterparts.. Efficient Vision Transformers. Vision Transformers [1, 16, 29] have become pivotal component in modern computer vision, powered by the self-attention mechanism. However, major limitation of Vision Transformers is the quadratic computational complexity of self-attention with respect to input size, which leads to prohibitive overhead for large-scale inputs. Many efforts [13, 27, 28, 49] have been made to mitigate the computational demands of selfattention. One line of research introduces sparsity [10, 58] or low-rank approximations [18, 21] into the attention computation. Another direction focuses on reducing the number of tokens involved in the attention operation by clustering [34], merging[5, 6] or selection [17, 52]. Despite these advancements, most existing efficient Transformer methods are designed for 2D tasks like image classification. Consequently, it remains an open challenge to design efficient Vision Transformers for 3D reconstruction, as it demands maintaining long-range, multi-view geometric consistency. Concurrent Work. Concurrently with our work, several other approaches have been proposed to address the efficiency bottleneck in VGGT. FastVGGT [46] employs token merging to reduce the number of tokens fed into global attention. However, the process of identifying and merging similar tokens across the entire sequence introduces considerable computational overhead. FasterVGGT [51] introduces block sparsity into the attention matrix to reduce computation. While effective for moderate sparsity levels, this approach suffers from significant performance degradation when sparsity increases. StreamVGGT [64] enables incremental reconstruction from streaming input by caching intermediate tokens from all global attention blocks. This design, however, leads to substantial memory overhead, limiting its scalability to longer sequences beyond tens of images. In contrast, FlashVGGT provides more principled approach to global context compression, enabling superior balance among inference speed, memory consumption and accuracy compared to these concurrent approaches. 3. Method 3.1. Visual Geometry Grounding Transformers We build our approach upon the Visual Geometry GroundFor completeness, we ing Transformer (VGGT) [54]. briefly recap its architecture, which consists of three main stages: image encoding, feature aggregation via alternating attention, and 3D reconstruction heads. Image Encoder. Given sequence of input images {Ii}S i=1, DINO [9] encoder extracts set of feature tokens for each image. This results in set of token sequences = {F1, F2, . . . , FS}, where each Fi RN represents the tokens from the i-th image. Each sequence includes special learnable camera token that stores camera information and several register tokens [14]. Alternating Attention. The core of VGGT is transformer aggregator composed of identical layers, each containing two attention blocks designed to capture both intraand inter-frame relationships. For simplicity, we omit the layer index in the following formulations. Frame Attention processes tokens within each frame independently. For each frame i, self-attention is computed over its tokens to refine local features: i = SelfAttn(Fi), Fi RN (1) Global Attention models interactions across all frames. The tokens are concatenated into single global sequence RKC, where = . Standard self-attention is then computed over all tokens: = SelfAttn(G) (2) The output RKC is then reshaped back into frame sequences for subsequent processing. Reconstruction Heads. The final aggregated token features are used by two separate heads to predict 3D properties for each view: (i) camera head predicts camera extrinsics and intrinsics, and (ii) DPT head [40] predicts depth map and an aleatoric uncertainty map [23]. While this architecture is powerful, its scalability is limited by the quadratic complexity O(K 2) = O(S2N 2) of the dense global self-attention, as identified in Fig. 2a. 3.2. Descriptor-Based Global Attention We introduce an efficient alternative to the dense global attention block that preserves global reasoning while reducing its complexity. Our approach replaces standard self-attention in global attention blocks with descriptorbased cross-attention mechanism, while keeping the encoder, frame attention, and reconstruction heads unchanged due to their minimal computational overhead. Fig. 3 provides an overview of our framework. Compressed Descriptor Tokens. Given the input to i=1) RKC, we global block, = Reshape({F first restore its spatial structure by reshaping it to RSHW C, where and are the height and width of the 2D patch token grid for each frame. We then generate compact set of descriptor tokens by applying spatial compression. Compared to pooling-based methods, interpolation better preserves local spatial information in the i}S 3 Figure 3. Architecture Overview. Our framework encodes input images into tokens using DINO [9] and processes them through alternating frame and global attention blocks. Unlike VGGTs dense global attention over all tokens, FlashVGGT generates compact set of descriptor tokens via spatial compression and computes efficient cross-attention from image tokens to these descriptors. The final aggregated tokens are fed into reconstruction heads to predict camera parameters and depth maps. original features (see discussion in Sec. 4.4). We employ bilinear interpolation to resample each frames spatial dimensions (H, ) to lower resolution (H/r, W/r), where is the compression factor: = Reshape (Interp (G, (H/r, W/r))) (3) The resulting descriptor tokens have size of RKdC, where Kd = H/r W/r. Auxiliary Descriptor Tokens. To maintain geometric consistency, we augment the compressed descriptors with three types of auxiliary tokens: (i) camera and register tokens from all frames; (ii) all tokens from the first image (which defines the world coordinate system) and (iii) all tokens from key-frames selected via k-means clustering [30] on average frame tokens. The key-frame selection is highly efficient, converging in under 2 seconds for 1,000 images on single NVIDIA H800 GPU as it operates on per-frame averages rather than individual tokens. These auxiliary tokens act as geometric anchors, preserving high-fidelity information from camera parameters, the world coordinate frame, and representative views. This prevents the loss of critical details during descriptor compression, ensuring robust geometric reasoning across the entire sequence. Descriptor Attention. We reformulate the global attention operation from Eq. (2) as cross-attention layer. The original, full-resolution tokens are used as queries, while the descriptor tokens are used as the shared keys and values. This allows the full-resolution tokens to be updated by compact set of descriptors representing the global context. = CrossAttn(Q = G, KV = D). (4) Crucially, the operation maintains global receptive field, preserving the models ability to capture long-range dependencies across all input images. Figure 4. Chunk Recursive Inference. For long input sequences, we process them in chunked manner while retaining global reception by caching descriptor tokens. Complexity. Our design significantly reduces the computational complexity of the global block. The standard selfattention in Eq. (2) requires O(K 2) = O(S2N 2) operations. Our descriptor-based cross-attention reduces this to: O(K Kd) = O(S2N 2/r2), With = 4 as in our experiments, the complexity reduction is about 16 . 3.3. Chunk-Recursive Inference To scale reconstruction to sequences that exceed GPU memory constraints, we propose chunk-recursive inference scheme. This method processes long sequences sequentially while maintaining global context across all previously seen chunks, as shown in Fig. 4. Problem Formulation. Let the input sequence of images be divided into consecutive chunks, {C1, C2, . . . , CT }. For the t-th chunk (1 ), let Dt denote the descriptor tokens generated from chunk Ct using the method described in Section 3.2. We maintain set of memory tokens Mt that 4 accumulates global information from all chunks processed up to step t, initialized as M0 = . Descriptor Attention with Memory. For chunk t, the global attention computation incorporates historical context through memory mechanism that maintains information from all previously processed chunks. The queries remain the full-resolution image tokens Gt from the current chunk Ct. The keys and values are formed by concatenating the current chunks descriptors Dt with the memory tokens from previous chunks Mt1: Ht = CrossAttn (Q = Gt, KV = [Mt1, Dt]) , (5) where [, ] denotes the concatenation operation along the sequence dimension. This design enables each token in the current chunk to attend to both the locally compressed context (Dt) and the globally accumulated history (Mt1), effectively maintaining global receptive field across the entire sequence while operating on individual chunks. Memory Update. After processing chunk t, the memory is updated by appending the current chunks descriptors. To constrain memory growth during long sequences, we implement dropping mechanism that retains only the descriptor tokens from every p-th frame. Let Dretain = Dt[:: p] denote the subset of descriptors from every p-th frame within the current chunk. The memory is then updated as: Mt = [Mt1, Dretain ] (6) This selective update rule ensures the memory Mt compactly represents the entire sequence history while limiting its size to grow sublinearly with the number of frames. Complexity. Our chunk-recursive scheme achieves substantial memory efficiency gains over the naive KV-caching in StreamVGGT [64]. While StreamVGGTs memory usage is O(KL), where is the number of global attention blocks, our approach reduces this to O(KL/(pr2)) through descriptor compression (r) and memory dropping (p). 3.4. Model Training Training Strategy. Our training follows two-stage curriculum. The first stage trains the model on 2-24 randomly shuffled views following VGGTs procedure. The second stage fine-tunes the model on ordered sequences to enable chunk-recursive inference, applying causal mask to the global attention block that restricts each image to attend only to previous frames in the sequence. Unless otherwise specified, we use spatial compression ratio of = 4, memory drop ratio of = 5, and select key frame every 200 images across all experiments. Training Data. We train our model on seven datasets, which is subset of VGGTs training data, covering diverse scenarios including synthetic/real-world data, scenelevel/object-centric configurations, and indoor/outdoor enSpecifically, we use BlendedMVS [60], vironments. CO3Dv2 [41], ScanNet itscenes [3], MVSSynth [20], and VirtualKitti [8]. [12], Mapillary [37], Ark4. Experiment We evaluate FlashVGGT against state-of-the-art methods across three core tasks: (i) monocular and sparse-view reconstruction (Sec. 4.1), long-sequence dense 3D reconstruction (Sec. 4.2), and (iii) online dense 3D reconstruction (Sec. 4.3). We subsequently analyze the effectiveness of our key design choices in Sec. 4.4. All evaluations are conducted on single NVIDIA H800 GPU. 4.1. Monocular and Sparse Reconstruction Camera Pose Estimation. Following [54], we evaluate our method on the CO3Dv2 [41] and RealEstate10K [63] datasets for camera pose estimation on short sequences. For each scene, we randomly select 10 images and report standard metrics: RRA (Relative Rotation Accuracy) and RTA (Relative Translation Accuracy), and AUC, the area under the accuracy-threshold curve for the minimum of RRA and RTA. As shown in Tab. 1, FlashVGGT achieves highly competitive performance. On the outof-distribution RealEstate10K dataset, our method closely matching VGGTs metrics, while being better than the concurrent work FastVGGT [46]. On the CO3Dv2 dataset, FlashVGGT also significantly outperforms all other efficient methods like FastVGGT [46] and CUT3R [55], and remains within narrow margin of the original VGGT. Monocular Depth Estimation. We evaluate single-image depth prediction on the Sintel [7], Bonn [38], and NYUv2 [36] datasets, reporting standard metrics: Absolute Relative Error (Abs Rel) and accuracy under threshold τ < 1.25. As shown in Tab. 2, FlashVGGT demonstrates strong performance across all benchmarks. On Sintel, our method achieves competitive results that are comparable with VGGT and significantly outperform other efficient methods. For the Bonn and NYU-v2 datasets, FlashVGGT consistently ranks as the second-best method, closely following VGGT within small gap while substantially outperforming FastVGGT and other concurrent approaches. These results demonstrate that our efficient descriptorbased attention preserves the strong geometric reasoning capabilities of VGGT, while our subsequent experiments will show it does so at fraction of the computational cost. 4.2. Long-sequence Dense 3D Reconstruction This section presents our main results on long-sequence dense 3D reconstruction. We conduct thorough evaluation of state-of-the-art offline methods capable of processing long sequences, comparing against Fast3R [59], VGGT [54], and FastVGGT [46]. For VGGT, we adopt the memory-efficient implementation from [46], which maintains the original accuracy while enabling inference on 5 Table 1. Camera Pose Estimation on RealEstate10K [63] and Co3Dv2 [41]. Table 2. Monocular Depth Estimation on Sintel [7], Bonn [38] and NYU-v2 [36]. Method Fast3R [59] CUT3R [55] FLARE [62] VGGT [54] FastVGGT [46] FlashVGGT RealEstate10K (unseen) Co3Dv2 Method Sintel Bonn NYU-v2 RRA@30 RTA@30 AUC@30 RRA@30 RTA@30 AUC@30 Abs Rel τ < 1.25 Abs Rel τ < 1.25 Abs Rel τ < 1.25 99.05 99.82 99.69 99.97 99.92 99.92 81.86 95.10 95.23 96.22 94.76 95.61 61.68 81.47 80.01 85.32 84. 85.30 97.49 96.19 96.38 98.96 97.51 98.23 91.11 92.69 93.76 97.13 96.01 96.75 73.43 75.82 73.99 88.59 86. 86.88 Fast3R [59] CUT3R [55] FLARE [62] VGGT [54] FastVGGT [46] FlashVGGT 0.544 0.418 0.606 0.335 0.337 0.346 0.509 0.520 0.402 0.599 0. 0.586 0.169 0.058 0.130 0.053 0.056 0.054 0.796 0.967 0.836 0.970 0.952 0.957 0.093 0.081 0.089 0.056 0. 0.058 0.898 0.914 0.898 0.951 0.943 0.947 Table 3. Large-Scale Dense 3D Reconstruction. Evaluation across 100, 500, and 1,000 image sequences, with results averaged over N-RGBD [2], 7-Scenes [47], and ScanNet [12]. Point cloud and camera pose metrics are multiplied by 100 for better readability. Frames Method Depth Point Camera Resource Abs Rel τ < 1.25 Acc Comp CD NC APE ARE RPE-Trans RPE-Rot Time (s) Mem. (GB) 100 500 1000 Fast3R [59] VGGT [54] FastVGGT [46] FlashVGGT Fast3R [59] VGGT [54] FastVGGT [46] FlashVGGT Fast3R [59] VGGT [54] FastVGGT [46] FlashVGGT 0.038 0.029 0. 0.028 0.045 0.035 0.034 0.034 0.122 0.048 0.034 0.032 0.951 0.983 0. 0.990 0.962 0.967 0.967 0.969 0.855 0.951 0.986 0.991 1.164 0.962 0. 0.897 1.432 1.484 1.388 1.314 3.076 2.039 1.322 1.160 1.900 1.162 1. 1.142 1.590 1.209 1.241 1.283 1.457 1.004 1.089 1.096 1.532 1.062 1. 62.10 72.48 68.34 2.654 1.537 1.663 3.123 2.935 3.011 1.019 70.14 1. 2.834 1.511 1.347 1.314 58.8 71.15 66.70 6.784 4.414 4.561 8.570 6.855 7.064 1. 70.18 4.298 6.950 2.267 1.521 1.206 52.5 68.65 66.05 12.67 6.519 5. 22.36 15.80 8.400 1.128 69.63 5.237 8.242 0.494 0.353 0. 0.447 2.343 1.453 1.722 1.474 9.530 2.222 2.553 2.067 0.756 0.493 0. 0.621 2.120 1.558 1.952 1.576 11.34 7.029 2.898 2.802 4.40 4.93 2. 1.54 62.40 90.97 29.04 12.54 224.10 372.80 78.22 35.32 13.94 12.26 12. 12.07 33.30 37.22 39.33 33.39 61.95 68.40 72.60 60.74 sequences of over 1,000 images. Following [46], we benchmark performance across totally 107 scenes from NRGBD [2], 7-Scenes [47], and subset of ScanNet [12] on 100, 500 and 1,000 images respectively. We evaluate each method comprehensively across 12 metrics covering depth estimation, point cloud reconstruction and camera pose estimation. For depth estimation, we report Absolute Relative Error (Abs Rel) and τ < 1.25 ratio. For point cloud reconstruction, we report Accuracy (Acc), Completeness (Comp), Chamfer Distance (CD), and Normal Consistency (NC). For camera estimation, we report Absolute Translation Error (APE), Absolute Rotation Error (ARE), Relative Translation Error (RPE-Trans), and Relative Rotation Error (RPE-Rot). Additionally, we report inference time and maximum GPU memory usage to provide complete picture of each methods practical utility. The comprehensive results in Tab. 3 demonstrate FlashVGGTs superior scalability and efficiency while maintaining competitive reconstruction quality across varying sequence lengths. For 100-image sequences, FlashVGGT achieves performance comparable to VGGT while being over 3 faster. At 500 images, it remains highly competitive with VGGT while achieving an over 8 speedup. For 1,000-image sequences, VGGT suffers from noticeable performance degradation due to attention dilution across excessive tokens, whereas FlashVGGT maintains high accuracy with over 10 faster inference. Across all sequence lengths, FlashVGGT consistently outperforms other efficient methods like FastVGGT [46] and Fast3R [59], establishing superior balance between efficiency and reconstruction fidelity. Fig. 5 presents qualitative comparison of FlashVGGT against other methods. The results demonstrate that FlashVGGT produces more complete and robust reconstructions from long input sequences while achieving significantly faster inference. Notably, VGGT exhibits substantial performance degradation as sequence length increases, as seen in the room reconstruction from 1,000 images. We attribute this failure to noisy and redundant interactions over extremely long input (over 1M tokens for 1,000 images). In contrast, FlashVGGT avoids this pitfall by learning compact, stable set of descriptor tokens that distill essential information, maintaining consistent performance across long sequences. 4.3. Online Dense 3D Reconstruction We evaluate FlashVGGT in an online inference setting with chunk size of 10 images against three recent online reconstruction methods: CUT3R [55], TTT3R [11], and StreamVGGT [64]. Experiments are conducted on NRGBD [2] using 500-image sequences from each scene. As shown in Tab. 4, our method significantly outperforms previous approaches across all metrics. FlashVGGT achieves the best reconstruction quality while being over 3.3 faster than the fastest competitor CUT3R. Notably, we achieve 6 Figure 5. Qualitative comparison of long-sequence dense 3D reconstruction. FlashVGGT produces more robust reconstructions over long input sequences while being significantly faster. Only points with top 90% confidence are shown for better visualization. Table 4. Online Dense 3D Reconstruction on N-RGBD [2]. Method Abs Rel Acc Comp APE Time (s) Mem (GB) CUR3R [55] TTT3R [11] StreamVGGT [64] FlashVGGT 0.375 0.134 0.086 0.047 4.890 3.567 2.456 3.426 1.954 1.235 23.456 16.434 6.543 34.19 35.67 209. 1.912 0.625 4.792 12.52 6.16 6.16 70.70 13. this while using less than quarter of the memory required by StreamVGGT. Qualitative results in Fig. 6 further demonstrate our approachs superiority. FlashVGGT successfully reconstructs complete room geometry and fine details (e.g., tiny objects on the table), while CUT3R and TTT3R suffer from accumulated errors and fail to recover meaningful structures. Although StreamVGGT produces reasonable geometry, it requires over 20 more time and still fails to capture fine details effectively. 4.4. Model Analysis and Discussion Spatial Compression Methods. We evaluated five strategies for producing descriptor tokens: average pooling, topkkk selection based on token norm, nearest-neighbor interpolation, bilinear interpolation, and lightweight learnable compressor consisting of depth-wise convolution followed by point-wise linear layer  (Table 5)  . Our analysis reveals that interpolation-based methods consistently outperform other approaches. We argue that their edge stems from locality preservation: as the DINO encoder outputs tokens corresponding to 1414 pixel patches, aggressive aggregation methods such as pooling merge information from Figure 6. Qualitative comparison of online 3D reconstruction. All methods are evaluated on sequence of 500 images. many distant patches and wash out fine-grained cues. Interpolation, instead, blends only handful of spatially adjacent tokens with distance-aware weights, retaining highfrequency detail that downstream tasks find useful. While top-k selection preserves original token values, it relies on the assumption that larger token norms correlate with informative local descriptors, which may not always hold. Finally, the learnable compressor neither improves quality nor stability; its limited capacity appears insufficient to capture the rich spatial patterns of the descriptors. Spatial Compression Ratio. Fig. 7 illustrates the trade-off Table 5. Comparison of different spatial compression techniques. Evaluated on N-RGBD [2] with 100 input images. Abs Rel Acc Comp NC APE ARE Pooling Top-k Learned Nearest"
        },
        {
            "title": "Bilinear",
            "content": "0.019 0.019 0.023 0.014 0.014 0.560 0.569 0.643 0.441 0.301 0.331 0.675 0.273 75.68 75.13 68.33 76.96 2.256 2.234 2.658 1. 4.008 4.516 5.183 3.456 0.436 0.272 77.75 1.890 3. between reconstruction accuracy (Chamfer Distance, left axis) and inference speed (right axis) across different compression ratios r. While larger values yield faster inference, they also lead to progressive decline in accuracy as fine-grained spatial information is lost. The ratio = 4 provides an optimal balance, offering significant speedup with minimal loss in reconstruction quality. Beyond this point, the rate of performance degradation increases substantially for diminishing gains in speed. Figure 7. The impact of different compression ratios. The ratio of 4 provides balanced choice between quality and speed. Auxiliary Descriptor Tokens. We identify that including the auxiliary descriptor tokens as described in Sec. 3.2 is crucial for maintaining reconstruction quality, as they augment the fine-grained information losses in compression. As shown in Fig. 8, omitting these auxiliary tokens degrades global geometric consistency. This effect is particularly pronounced in long sequences with low inter-frame overlap, e.g., autonomous driving scenarios. Figure 8. The impact of auxiliary descriptor tokens. Including auxiliary tokens improves geometric quality. Confidence Maps. Fig. 9 shows comparison of the confidence maps predicted by VGGT and our method. VGGT Figure 9. Analysis of confidence maps. Ours method produces spatially coherent confidence scores that better preserve planar structures (e.g., the computer screen) while filtering noise. tends to produce over-confident predictions, assigning disproportionately low confidence scores to homogeneous regions such as walls and computer screens. This often results in gaps and holes in the final reconstruction after filtering out low-confidence points. In contrast, our method generates more calibrated and spatially coherent confidence map. This allows for the effective preservation of structural details while robustly filtering out noise, leading to more complete and reliable 3D reconstructions. 5. Conclusion In this work, we introduced FlashVGGT, novel framework that overcomes the scalability bottleneck of global attention in feed-forward 3D reconstruction. We identified that the full global self-attention in models like VGGT is computationally prohibitive and largely unnecessary. Our solution centers on compressed descriptor attention paradigm that replaces dense attention with efficient cross-attention from all image tokens to compact set of learned descriptor tokens. Furthermore, the compact nature of these descriptors enables chunk-recursive inference scheme, allowing FlashVGGT to process very long sequence with manageable memory footprint. Through extensive experiments, we demonstrated that FlashVGGT achieves superior balance between efficiency and accuracy. It matches the reconstruction quality of VGGT while achieving over 90% inference speedup for 1,000-image sequences and scaling effectively to over 3,000 images. By making high-fidelity, largescale 3D reconstruction both fast and practical, FlashVGGT opens the door for more demanding real-world applications. Limitations: While highly efficient for long sequences, our method exhibits slight performance degradation on shorter sequences. Furthermore, the design space for descriptor attention remains largely unexplored. Please refer to the supplementary material for detailed discussion."
        },
        {
            "title": "References",
            "content": "[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇcic, and Cordelia Schmid. Vivit: video vision transformer. In ICCV, 2021. 3 [2] Dejan Azinovic, Ricardo Martin-Brualla, Dan Goldman, Matthias Nießner, and Justus Thies. Neural rgb-d surface reconstruction. In CVPR, 2022. 6, 7, 8 [3] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. arXiv, 2021. 2, 5 [4] Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc Van Gool. Speeded-up robust features (surf). Computer vision and image understanding, 2008. 2 [5] Daniel Bolya and Judy Hoffman. Token merging for fast stable diffusion. In CVPR, 2023. [6] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv, 2022. 3 [7] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. naturalistic open source movie for optical flow evaluation. In ECCV, 2012. 5, 6 [8] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv, 2020. 5 [9] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. EmergIn ing properties in self-supervised vision transformers. ICCV, 2021. 3, 4 [10] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, and Zhangyang Wang. Chasing sparsity in vision transformers: An end-to-end exploration. NeurIPS, 2021. [11] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and Anpei Chen. Ttt3r: 3d reconstruction as test-time training. arXiv, 2025. 3, 6, 7 [12] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 2, 5, 6 [13] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. NeurIPS, 2022. 3 [14] Timothe Darcet, Maxime Oquab, Julien Mairal, and Piotr arXiv, Bojanowski. Vision transformers need registers. 2023. 3 [15] Kai Deng, Zexin Ti, Jiawei Xu, Jian Yang, and Jin Xie. Vggtlong: Chunk it, loop it, align itpushing vggts limits on kilometer-scale long rgb sequences. arXiv, 2025. [16] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv, 2020. 2, 3 [17] Mohsen Soroush Abbasi Koohpayegani, Fayyaz, Farnoush Rezaei Sunando Sengupta, Hamid Reza Vaezi Joze, Eric Sommerlade, Hamed Pirsiavash, and Jurgen Gall. Adaptive token sampling for efficient vision transformers. In ECCV, 2022. 3 Jafari, [18] Dongchen Han, Tianzhu Ye, Yizeng Han, Zhuofan Xia, Siyuan Pan, Pengfei Wan, Shiji Song, and Gao Huang. Agent attention: On the integration of softmax and linear attention. In ECCV, 2024. 3 [19] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. 1 [20] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. Deepmvs: Learning multi-view stereopsis. In CVPR, 2018. 2, 5 [21] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In ICML, 2021. 3, 2 [22] Nikhil Keetha, Norman Muller, Johannes Schonberger, Lorenzo Porzi, Yuchen Zhang, Tobias Fischer, Arno Knapitsch, Duncan Zauss, Ethan Weber, Nelson Antunes, et al. Mapanything: Universal feed-forward metric 3d reconstruction. arXiv, 2025. 2, 3 [23] Alex Kendall and Roberto Cipolla. Modelling uncertainty in deep learning for camera relocalization. In ICRA, 2016. [24] Jan Koenderink and Andrea Van Doorn. Affine structure from motion. Journal of the Optical Society of America A, 8 (2):377385, 1991. 1 [25] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding image matching in 3d with mast3r. In ECCV, 2024. 2 [26] Hao Li, Zhengyu Zou, Fangfu Liu, Xuanyang Zhang, Fangzhou Hong, Yukang Cao, Yushi Lan, Manyuan Zhang, Gang Yu, Dingwen Zhang, et al. Iggt: Instance-grounded geometry transformer for semantic 3d reconstruction. arXiv, 2025. 3 [27] Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren. Efficientformer: Vision transformers at mobilenet speed. NeurIPS, 2022. 3 [28] Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, and Yixuan Yuan. Efficientvit: Memory efficient vision transformer with cascaded group attention. In CVPR, 2023. [29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. 3 [30] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129137, 1982. 4 [31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv, 2017. 1 [32] David Lowe. Object recognition from local scale-invariant features. In ICCV, 1999. 2 [33] David Lowe. Distinctive image features from scaleinvariant keypoints. IJCV, 60(2):91110, 2004. 2 [34] Dmitrii Marin, Jen-Hao Rick Chang, Anurag Ranjan, Anish Prabhu, Mohammad Rastegari, and Oncel Tuzel. Token pooling in vision transformers. arXiv, 2021. 3 [35] Pierre Moulon, Pascal Monasse, Romuald Perrot, and Renaud Marlet. OpenMVG: Open multiple view geometry. In International Workshop on Reproducible Research in Pattern Recognition, 2016. 1 [36] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Indoor segmentation and support inference from Fergus. rgbd images. In ECCV, 2012. 5, 6 [55] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In CVPR, 2025. 3, 5, 6, [56] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In CVPR, 2024. 2 [57] Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, and Tong He. Permutation-equivariant visual geometry learning. arXiv, 2025. 2, 3 [58] Cong Wei, Brendan Duke, Ruowei Jiang, Parham Aarabi, Graham Taylor, and Florian Shkurti. Sparsifiner: Learning sparse instance-dependent attention for efficient vision transformers. In CVPR, 2023. 3 [59] Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In CVPR, 2025. 2, 5, 6 [60] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: largescale dataset for generalized multi-view stereo networks. In CVPR, 2020. 2, 5 [61] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and MingHsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. arXiv, 2024. [62] Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gordon Wetzstein. Flare: Feed-forward geometry, appearance and camera estimation from uncalibrated sparse views. In CVPR, 2025. 6 [63] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv, 2018. 5, 6 [64] Dong Zhuo, Wenzhao Zheng, Jiahe Guo, Yuqi Wu, Jie Zhou, and Jiwen Lu. Streaming 4d visual geometry transformer. arXiv, 2025. 2, 3, 5, 6, 7 [37] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In ICCV, 2017. 2, 5 [38] E. Palazzolo, J. Behley, P. Lottes, P. Gigu`ere, and C. Stachniss. ReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting Residuals. In IROS, 2019. 5, 6 [39] Linfei Pan, Daniel Barath, Marc Pollefeys, and Johannes Schonberger. Global structure-from-motion revisited. In ECCV, 2024. [40] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In ICCV, 2021. 3 [41] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In ICCV, 2021. 2, 5, 6 [42] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, Superglue: Learning feature and Andrew Rabinovich. matching with graph neural networks. In CVPR, 2020. 2 [43] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016. 1 [44] Johannes Lutz Schonberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In ECCV, 2016. 1 [45] Steven Seitz, Brian Curless, James Diebel, Daniel Scharstein, and Richard Szeliski. comparison and evaluation of multi-view stereo reconstruction algorithms. In CVPR, 2006. 1 [46] You Shen, Zhipeng Zhang, Yansong Qu, and Liujuan Cao. Fastvggt: Training-free acceleration of visual geometry transformer. arXiv, 2025. 3, 5, 6, [47] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene coordinate regression forests for camera relocalization in rgb-d images. In CVPR, 2013. 6, 1 [48] Shimon Ullman. The interpretation of structure from motion. Proceedings of the Royal Society of London. Series B. Biological Sciences, 203(1153):405426, 1979. 1 [49] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. Fastvit: fast hybrid vision In ICCV, transformer using structural reparameterization. 2023. 3 [50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017. 2 [51] Chung-Shien Brian Wang, Christian Schmidt, Jens Piekenbrinck, and Bastian Leibe. Faster vggt with block-sparse global attention. arXiv, 2025. 3 [52] Junke Wang, Xitong Yang, Hengduo Li, Li Liu, Zuxuan Wu, and Yu-Gang Jiang. Efficient video transformers with spatial-temporal token selection. In ECCV, 2022. 3 [53] Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and David Novotny. Vggsfm: Visual geometry grounded deep structure from motion. In CVPR, 2024. [54] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In CVPR, 2025. 1, 2, 3, 5, 6 10 FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention"
        },
        {
            "title": "Supplementary Material",
            "content": "Overview. The supplementary material is organized as follows: Sec. elaborates on training specifics and hyperparameters. Sec. presents extended experimental analysis and ablation studies. Finally, Sec. discusses the limitations of our current work and promising directions for future research. ble computational overhead. This suggests that distributing full-resolution information across the sequence helps maintain local detail preservation. Notably, all auxiliary components contribute to performance with minimal impact on efficiency, confirming our design provides an effective accuracy-efficiency trade-off. A. Implementation Details Model and Optimization. We initialize FlashVGGT from pre-trained VGGT [54] checkpoint. During training, we freeze the image encoder and reconstruction heads, optimizing only the alternating attention aggregator which comprises approximately 50% of the total parameters while employing the original VGGT loss functions. We use the Adam-W optimizer [31] with an initial learning rate of 4 106, linear warmup, and cosine decay. Both training stages run for 10,000 iterations on 4 H800 GPUs, completing in approximately 16 hours. Training Protocol. We adopt VGGTs dynamic batching scheme, randomly sampling 2 to 24 frames per iteration. Input images are pre-processed by resizing the longer side to 518 pixels while randomizing the aspect ratio between 0.33 and 1.0. We apply standard data augmentation including color jittering and random grayscale conversion. For training stability, we employ gradient norm clipping with threshold of 1.0, and leverage both bfloat16 precision and gradient checkpointing as in the original VGGT. B. Additional Analysis Ablation of auxiliary descriptor tokens. As shown in Tab. 6, we analyze the contribution of each auxiliary token component using 500-image sequences from 7Scenes [47]. The full model achieves the best overall performance, validating the importance of all auxiliary tokens. The most critical component is the reference frame tokens, whose removal causes the most severe degradation, particularly in pose estimation (APE increases by 96% and ARE by 68%). This confirms that preserving the full coordinate frame is essential for global geometric consistency. Camera tokens also prove vital, as their absence leads to noticeable deterioration in reconstruction quality while providing minimal memory savings. This demonstrates that explicit camera parameter representation significantly aids the networks geometric reasoning. While key frame tokens offer the most modest improvements, they still enhance both reconstruction (CD) and camera pose (APE) metrics with negligiTable 6. Detailed ablations of auxiliary descriptor tokens. Evaluated on 7Scenes [47] with 500 input images. Abs Rel CD NC APE ARE Time (s) Mem (GB) w/o Cam tokens w/o First frame w/o Key frames Full model 0.066 0.067 0.067 0.066 2.849 2.866 2.859 64.01 58.91 63.68 3.908 7.660 4. 8.115 13.608 8.123 2.748 64.12 3.904 8.115 12.99 12.67 12. 12.99 33.40 33.39 33.33 33.40 Key-frame selection methods. As shown in Tab. 7, we compare different strategies for selecting key frames. The clustering-based approach consistently outperforms both random and fixed-stride selection across all accuracy metrics. Our proposed clustering method achieves the best performance, demonstrating its ability to select representative frames that better capture the scenes geometric diversity. This comes at minimal computational cost, adding only 0.29 seconds compared to the fastest method. While fixedstride selection offers slightly better efficiency, it shows clear performance limitations, particularly in pose estimation. This suggests that uniformly distributed frames may miss critical viewpoints needed for optimal geometric reconstruction. Random selection performs similarly to fixedstride but with more variability across metrics, confirming that naive approaches cannot reliably identify the most informative frames for 3D reconstruction. The results validate that our clustering-based key frame selection effectively identifies geometrically representative views, providing better reconstruction quality with negligible overhead compared to simpler alternatives. Table 7. Comparison of different key-frame selection methods. Abs Rel CD NC APE ARE Time (s) Mem (GB) Random Fix stride Cluster 0.067 0.067 0. 2.789 2.784 63.92 64.02 4.108 4.096 8.123 8.189 12.74 12.70 2. 64.12 3.904 8.115 12.99 33.39 33.39 33. Memory retain rate p. We analyze the trade-off between efficiency and accuracy by varying the memory retain rate p, which controls how many historical descriptor tokens are preserved during chunk-recursive inference. As shown in Fig. 10, lower values of (more aggressive mem1 ory dropping) yield better efficiency but gradually degrade reconstruction quality. Notably, the performance drop from = 1 to = 5 is minimal compared to the substantial efficiency gains. This suggests that carefully selected memory dropping can eliminate redundant historical information with negligible impact on reconstruction quality. Table 9. Comparison of resources consumption across different input images. - denotes model running out memory. Methods 400 600 800 1000 1200 137.84 32. 245.47 52.01 386.07 79.31 - - Time (s) PFLOPs VGGT FastVGGT FlashVGGT VGGT FastVGGT FlashVGGT 17.01 6.45 4.05 4.24 0. 0.29 61.82 16.63 9.84 16.92 2.23 17.25 38.04 5. 1.10 2.43 Mem (GB) VGGT FastVGGT 18.50 19.34 30.98 32. 43.45 45.97 FlashVGGT 16.97 27.92 38.83 26.44 67.61 9. 4.30 55.93 59.29 49.76 38.1 51.25 105.61 14. - - 6.70 9.62 68.40 72.60 - - 60. 71.61 Figure 10. The impact of different memory retain rate p. The rate of 5 provides balanced choice between quality and speed. Chunk size. We analyze the impact of chunk size on online reconstruction performance in Tab. 8. The results demonstrate that chunk size has minimal effect on reconstruction quality across all metrics. However, chunk size significantly impacts computational efficiency: increasing from 1 to 100 frames per chunk provides 2.3 speedup at the cost of 49% higher memory usage. This reveals flexible tradeoff between speed and memory constraints. Applications prioritizing throughput can use larger chunks (e.g., 100) for faster processing, while memory-constrained environments can employ smaller chunks (e.g., 10-50). The stability of reconstruction metrics across chunk sizes confirms the robustness of our chunk-recursive scheme, making it adaptable to diverse deployment scenarios. Table 8. The impact of different chunk size in online setting. Chunk Abs Rel CD NC APE ARE Time (s) Mem (GB) 1 10 50 100 0.086 0.087 0.087 0.087 2.342 2.391 2.362 2.330 59.16 59.74 60.70 60.97 5.165 5.042 5.224 5. 1.031 0.980 1.003 1.034 25.50 14.58 12.58 10.90 11.42 13.33 14.95 16.98 Resources consumption. As shown in Tab. 9, FlashVGGT demonstrates substantial efficiency gains across all resource metrics while scaling to longer sequences than compet- (i) Computational Efficiency: FlashVGGT ing methods. achieves remarkable reductions in both time and FLOPs. For 1,000 images, our method is 10.1 faster than VGGT [54] and requires 15.8 fewer FLOPs. Even compared to FastVGGT [46], FlashVGGT provides 2.1 speedup and 2.1 FLOP reduction, demonstrating the superiority of our descriptor-based approach over token merg- (ii) Memory Efficiency: FlashVGGT maintains the ing. lowest memory footprint across all sequence lengths. At 2 1,000 images, it uses 11% less memory than VGGT and 16% less than FastVGGT. This memory advantage enables FlashVGGT to successfully process 1,200-image sequences where both baselines fail due to out-of-memory errors. Note that FastVGGT takes more memory as it needs to compute (iii) Scalability: The token similarity across all tokens. computational advantages become more pronounced with longer sequences. While VGGT and FastVGGT cannot process beyond 1,000 images, FlashVGGT maintains efficient operation at 1,200 images with only 51.25 seconds and 71.61GB memory, demonstrating robust scalability for large-scale reconstruction tasks. These results confirm that our descriptor-based attention mechanism provides fundamental improvements in computational efficiency without compromising on reconstruction quality, enabling practical processing of very long image sequences. Comparison with latent cross-attention. While our method shares the high-level intuition of reducing computation via asymmetric attention with latent cross-attention methods like Perceiver [21], our approach differs fundamentally. Unlike Perceiver-style methods that use randomly initialized learnable tokens as queries to aggregate information from the input, we use the original input tokens as queries and spatially compressed version as keys and values. This design preserves the original input resolution, making it more suitable for dense prediction tasks like 3D reconstruction. Furthermore, our compressed descriptors carry strong data-dependent priors through spatial resampling, maintaining the inputs structural distribution rather than learning generic latent representation. To validate our approach, we compare against Perceiver-style alternative that uses additional learnable latent tokens per frame. During frame attention, these latent tokens interact with tokens in the same frame, while in global attention, we compute cross-attention from frame tokens to these latent tokens. As shown in Tab. 10, our method significantly outperforms the Perceiver-style approach across all metrics. These results demonstrate that leveraging data-dependent compression is crucial for high-quality 3D reconstruction. Table 10. Comparsion between our compressed descriptor attention and Perceiver-style latent cross-attention. Abs Rel CD NC APE ARE Time (s) Mem (GB) Perceiver-style Ours 0.097 0. 5.645 34.02 14.573 12.564 13.56 2. 64.12 3.904 8.115 12.99 34.56 33. C. Discussions Limitations. While FlashVGGT achieves substantial efficiency improvements over VGGT [54], several limitations merit discussion. First, our method exhibits slight performance gap on short sequences, as evidenced in Tab. 1. However, this gap diminishes with longer sequences where our architectural advantages become more pronounced. Second, similar to the original VGGT, our models performance can degrade under challenging conditions involving large deformations or extreme lighting variations. This limitation, however, could potentially be mitigated through finetuning on domain-specific data without requiring architectural changes. These aspects represent promising directions for future work in enhancing robustness and applicability. Design Space. While our framework achieves substantial efficiency improvements, its design space offers rich opportunities for future exploration. For instance, although our experiments in Tab. 5 demonstrate that simple convolutional compressor underperforms interpolation, more sophisticated learnable architectures for token compression and selection remain unexplored. Investigating adaptive mechanisms that can dynamically adjust compression strategies based on input characteristics could potentially yield further performance gains. Integration with other architectures. While our primary evaluation is based on VGGT [54], our descriptor-based attention mechanism is general module that can be readily integrated into other architectures employing the alternating attention backbone [22, 26, 57]. This portability paves the way for developing more efficient variants of state-ofthe-art models across various tasks, including metric reconstruction [22] and semantic 3D understanding [26]."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology"
    ]
}