{
    "paper_title": "NaviTrace: Evaluating Embodied Navigation of Vision-Language Models",
    "authors": [
        "Tim Windecker",
        "Manthan Patel",
        "Moritz Reuss",
        "Richard Schwarzkopf",
        "Cesar Cadena",
        "Rudolf Lioutikov",
        "Marco Hutter",
        "Jonas Frey"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language models demonstrate unprecedented performance and generalization across a wide range of tasks and scenarios. Integrating these foundation models into robotic navigation systems opens pathways toward building general-purpose robots. Yet, evaluating these models' navigation capabilities remains constrained by costly real-world trials, overly simplified simulations, and limited benchmarks. We introduce NaviTrace, a high-quality Visual Question Answering benchmark where a model receives an instruction and embodiment type (human, legged robot, wheeled robot, bicycle) and must output a 2D navigation trace in image space. Across 1000 scenarios and more than 3000 expert traces, we systematically evaluate eight state-of-the-art VLMs using a newly introduced semantic-aware trace score. This metric combines Dynamic Time Warping distance, goal endpoint error, and embodiment-conditioned penalties derived from per-pixel semantics and correlates with human preferences. Our evaluation reveals consistent gap to human performance caused by poor spatial grounding and goal localization. NaviTrace establishes a scalable and reproducible benchmark for real-world robotic navigation. The benchmark and leaderboard can be found at https://leggedrobotics.github.io/navitrace_webpage/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 2 9 0 9 6 2 . 0 1 5 2 : r NaviTrace: Evaluating Embodied Navigation of Vision-Language Models Tim Windecker1,2, Manthan Patel1, Moritz Reuss2, Richard Schwarzkopf3, Cesar Cadena1, Rudolf Lioutikov2,4, Marco Hutter1 and Jonas Frey1 Fig. 1: We introduce NaviTrace, novel VQA benchmark for VLMs that evaluates models on their embodiment-specific understanding of navigation across challenging real-world scenarios. Abstract Visionlanguage models demonstrate unprecedented performance and generalization across wide range of tasks and scenarios. Integrating these foundation models into robotic navigation systems opens pathways toward building general-purpose robots. Yet, evaluating these models navigation capabilities remains constrained by costly real-world trials, overly simplified simulations, and limited benchmarks. We introduce NaviTrace, high-quality Visual Question Answering benchmark where model receives an instruction and embodiment type (human, legged robot, wheeled robot, bicycle) and must output 2D navigation trace in image space. Across 1000 scenarios and more than 3000 expert traces, we systematically evaluate eight state-of-the-art VLMs using newly introduced semantic-aware trace score. This metric combines Dynamic Time Warping distance, goal endpoint error, and embodimentconditioned penalties derived from per-pixel semantics and correlates with human preferences. Our evaluation reveals consistent gap to human performance caused by poor spatial grounding and goal localization. NaviTrace establishes scalable and reproducible benchmark for real-world robotic navigation. The benchmark and leaderboard can be found at https:// leggedrobotics.github.io/navitrace_webpage/. I. INTRODUCTION The rise of foundation models with general-purpose capabilities has sparked push to develop robots that are equally general-purposecapable of flexible, wide-ranging behavior in real-world environments. Given their significant potential, it is crucial to rigorously assess how these models perform in real-world robotic applications. Such evaluations 1Robotic Systems Lab, ETH Zurich, Zurich, Switzerland 2Intuitive Robots Lab, KIT, Karlsruhe, Germany 3FZI Research Center for Information Technology, Karlsruhe, Germany 4Robotics Institute Germany inform model development, reveal limitations, and establish benchmarks for comparing approaches. However, assessing the navigation capabilities of these models remains challenging. Navigation is critical for numerous robotic applications, including last-mile delivery, industrial inspection, search and rescue, and assistive tools for visually impaired people. Yet only limited evaluation methods currently exist for these capabilities. Applying existing Vision Language Models (VLMs) to the navigation task itself can be straightforward: text instruction and an image observation can be provided as input, prompting the language model to generate text description that the robots control system can translate into motor commands [1], [2]. While various flavors of such systems exist, it remains unclear which VLM system performs best. there exist To evaluate this, three main directions in the literature: The first is to perform real-world closedloop rollouts on set of navigation tasks and measure the success rate. However, such experiments are expensive, timeconsuming, and inherently do not scale well for evaluating performance across diverse operating environments, while also lacking reproducibility. The second is to run the same closedloop experiments in simulation. This approach improves reproducibility but still faces significant limitations. The diversity of tasks is constrained by the scenarios created in simulation, which are inherently simplified in terms of dynamics, mostly feature static scenes, and have limited semantics. Important factors, such as varying terrain properties or social norms, that should influence an agents navigation behavior, are difficult to encode. Lastly, Visual Question TABLE I: An overview of vision language navigation and VLM datasets. Task Data Source Annotations Name R2R [12] REVERIE [14] RxR [15] Type VLN VLN VLN OctoNav-Bench [16] Embodied Navigation EgoWalk [17] VLN CityWalker [18] SocialNav-SUB [3] Social-LLaVA [4] NaviTrace (ours) Point Goal Nav. Social Navigation VQA Social Navigation VQA Nav. Traces for VLMs VLM Nav. Description MP3D [13] MP3D [13] MP3D [13] Habitat (MP3D, HM3D, Gibson, ProcTHOR) gation recordings 50 of egocentric navi2000 of city walking videos from the internet SCAND [19] SCAND [19] 1000 diverse real images Sim Real Description Automatic Manual Scoring Embodiments 21,567 language navigation instructions 21,702 language instructions that require navigating and identifying an object 126k multilingual time-aligned language instructions, 126k demonstration paths 45k+ annotated instructions with trajectories that combine the task types: object goal, point goal, image goal, instance-image goal, and VLN, 10k+ instruction-think-action pairs Automatic traversability region and language goals with extracted odometry trajectories With visual odometry extracted trajectory poses 4968 unique questions, 24840 human responses 40k questions fully annotated by humans 1k language instructions, 3k+ traces that describe 2D paths / / / / Success Rate, Navigation Error Success Rate, SPL Success Rate, SPL, Navigation Error, Normalized Dynamic Time Warping Success Rate, SPL MSE, Absolute Displacement Error, and Final Displacement Error Average Orientation Error Probability of Agreement, ConsensusWeighted Probability of Agreement Human judges Human-preference aligned nav. metric , , , , , Answering (VQA) benchmarks can overcome some of these limitations by making use of high-quality human annotations, which can incorporate semantics, social preferences, and geometric cues. While several navigation-focused VQA datasets exist, they typically (i) constrain outputs to text answers rather than trace-level plans, and (ii) evaluate only legged or wheeled robot embodiments [3], [4]. To fill this gap, we introduce NaviTrace, VQA benchmark specially designed to evaluate embodiment-specific navigation performance across 1,000 diverse scenarios and four different embodiments. Each task within NaviTrace consists of single real-world image paired with highquality language instruction, enabling efficient data collection while capturing challenging navigation tasks. Following the most intuitive approach to answering navigation questions, we provide solutions per embodiment as 2D paths in image space, which we refer to as traces. This carefully chosen formulation is more expressive than low-level commands such as Forward [1] and can also support longer-horizon planning. It can be seen as an extension of pointinga common task that is evaluated and optimized in current foundation models [5], [6] and widely used to assess the visual grounding of VLMs [7]. Furthermore, traces have proven beneficial for addressing manipulation tasks [8][11]. NaviTrace tests VLMs for instruction following, spatial understanding, and physical understanding of varying embodiments (human, legged robot, wheeled robot, and bicycle), and categorizes scenarios based on the type of navigation challenges. We develop semantic-aware score to measure how well the predicted navigation trace aligns with human preferences. To achieve this, we combine the Dynamic Time Warping distance to ground-truth trace, goal endpoint error, and pixelwise embodiment-conditioned penalties derived from semantic segmentation model. We show that our metric, while inexpensive to compute and annotate, is competitive with more expensive human-derived metrics in aligning with human preferences. Specifically, our main contributions are: embodiments navigate in 1000 diverse and challenging real-world scenarios. 2) Semantic-aware Score: new metric to measure the accuracy of 2D traces for real-world images. We test the score for alignment with human preferences by showcasing its correlation to human expert judgments. 3) Evaluation of VLMs: Comprehensive assessment of current state-of-the-art VLMs on our benchmark. II. RELATED WORK Table provides an overview of benchmarks evaluating the navigation performance of vision-based agents and VLMs. Vision Language Navigation Benchmarks. Several relevant works focus on the evaluation of vision-language navigation (VLN) tasks, where agents follow natural language instructions using visual input. Well-established benchmarks include Room-to-Room (R2R) [12], REVERIE [14], and Room-Across-Room (RxR) [15]. R2R and RxR feature finegrained instructions, while REVERIE uses coarser descriptions that also require object identification (e.g., Bring me the bottom picture next to the top of the stairs on level one [14]). All three benchmarks rely on Matterport3D (MP3D) scenes [13] in simulation, which provides realistic indoor environments but restricts navigation to discrete viewpoint transitions without realistic physics simulation. OctoNav-Bench [16] extends VLN benchmarks by combining multiple task types into free-form instructions. It leverages the Habitat simulator [20] that supports continuous action spaces. While simulators enable the training of reinforcement learning policies, they remain constrained to the underlying indoor training environments and often fail to accurately model the physical interactions corresponding to visual observations, which is one of the main causes of the visual sim-to-real gap. Other benchmarks address this limitation by directly collecting real-world data. EgoWalk [17] records egocentric navigation with annotated language goals and extracted trajectories. CityWalker [18] uses internet videos and visual odometry to extract trajectories, and therefore does not have language-conditioned tasks. 1) NaviTrace: novel high-quality benchmark for evaluating the ability of VLMs to predict how different Existing VLN benchmarks present several limitations for VLM evaluation. Most require trajectory predictions in specialized action spaces that VLMs cannot natively predict. They focus exclusively on human navigation, overlooking cross-embodiment challenges. To address these gaps, NaviTrace uses manually collected real-world images and VLMaccessible 2D trace prediction across multiple embodiment types. Vision-Language Model Benchmarks. There exists variety of VLM benchmarks, which adapt standard computer vision tasks into VLM-compatible formats [21][23], test embodied skills such as pointing [7], spatial understandto ing [24], or embodied reasoning [5]. Most relevant NaviTrace are benchmarks in social navigation. SocialNavSub [3] evaluates models through VQA on videos of robothuman interactions, asking about spatial relations, robot and pedestrian motion, as well as interaction dynamics. Similarly, the VQA dataset SNEI [4] contains social scenarios in crowded spaces, asking models to describe perceptions, predict future movements, reason about robot actions, and give general explanation of what is happening. However, to the best of our knowledge, no existing benchmark directly evaluates VLMs on navigation tasks or their understanding of differences between embodiments when navigating. III. NAVITRACE BENCHMARK We introduce NaviTrace, benchmark for evaluating the ability of VLMs to predict navigation strategies for different embodiments in real-world scenarios (see Figure 1). To ensure relevance, diversity, and high-quality annotation, we manually collect real-world images and perform all labeling by hand. The dataset contains 1,000 scenarios with more than 3,000 traces, divided evenly into validation and test splits. The test set annotations remain secret and are used to evaluate the public leaderboard. During evaluation, VLM receives structured prompt with an image, task description, and an embodiment type. The model must predict path that solves the task, and its output is measured using our novel task-specific score function. A. Data Collection Each scenario in NaviTrace combines images, instructions, traces, and embodiment types to capture realistic navigation challenges (see Figure 1 for examples). Image. Each scenario includes distinct first-person image of real-world environment. Most images are crowd-sourced and captured with consumer devices such as phones or GoPros, complemented by 164 curated samples from the publicly available GrandTour dataset [25]. To preserve privacy, we anonymize all personal data using EgoBlur [26] to blur faces and license plates. Task Instruction. Each image is paired with manually written instruction solvable purely from the visual information. These instructions emphasize cases where different embodiments behave differently, while still reflecting everyday scenarios. They are formulated either as goals (e.g., \"Go to the red car\") or as directional instructions (e.g., \"Go forward, then turn left at the traffic light.\"). Task Categories. To classify capabilities of models according to navigation-relevant attributes, we tagged each scenario with one or more categories, describing the main challenges of the navigation task: Geometric Terrain Property Assessment: Decisions based on the shape, structure, or 3D geometry of permanent terrain features (e.g., stairs, cliff, or closed doors). Semantic Terrain Property Assessment: Decisions requiring semantic understanding of properties (e.g., sidewalk, or road), or physical qualities (e.g., terrain stiffness, or friction). Accessibility: Barrier-free access for embodiments such as wheelchairs or delivery robots (e.g., wheelchair ramps, or automatically opening doors). Visibility: Scenarios with occlusions, poor lighting, or ambiguous information (e.g., blocked lines-of-sight, or unclear signage). Social Norms: Normative constraints from rules or signage (e.g., crosswalks, walking on pedestrian walkway, or following sign to not step on grass). Dynamic Obstacle Avoidance: Reacting to and planning around moving obstacles (e.g., humans, or vehicles). Stationary Obstacle Avoidance: Navigation around fixed obstacles not part of the general terrain structure (e.g., debris, or road closures). Ground-Truth Trace. We define trace as sequence of 2D points given as image coordinates that describes navigation path. This representation is detached from robotspecific controls, ensuring compatibility with diverse model architectures. We draw one trace per suitable embodiment, and multiple traces if equally valid alternatives exist (e.g., avoiding an obstacle from the left or right). Embodiments. We model four embodiment types to capture various real-world navigation behaviors: Human: regular pedestrian unable to climb tall obstacles. Legged Robot: quadruped (e.g., ANYmal [27]) with behavior similar to humans but shorter in stature. Wheeled Robot: small, wheelchair-like delivery robot that favors walkways and ramps. Bicycle: cyclist following traffic rules, preferring bike lanes or streets, and avoiding stairs. We deliberately exclude cars, since their viewpoint differs fundamentally from the embodiments above. B. Data Quality To ensure scenario diversity, we analyze the dataset along five factors: (i) geographical location, (ii) urban vs. rural setting, (iii) natural vs. structured environment, (iv) lighting conditions, and (v) weather. The geographic distribution is shown on the left in Figure 2. While the dataset is geographically concentrated in Switzerland, it also includes samples from several other countries to provide broader international representation. The right side of Figure 2 summarizes the distribution of scenarios across the remaining factors. The scenarios are Fig. 2: Left: Geographic distribution of image sources, with the inner circle denoting countries and the outer circle specifying cities or regions. Images originating from the GrandTour Dataset [25] are explicitly marked in the outer circle. Right: Distribution of scenarios by setting (urban vs. rural), environment type (natural vs. structured), lighting, and weather. balanced between an urban and rural setting. Structured environments appear more frequently than natural ones, because urban scenes rarely contain natural elements. Most images were captured in daylight under clear or cloudy weather, resulting in high visual quality. This shows tendency toward favorable conditions for vision, however the benchmark primarily targets navigation challenges rather than visual perception under difficult conditions. C. Score To fairly evaluate VLM-generated navigation traces, we design score function that balances three factors: (i) how closely the path follows the ground truth, (ii) whether it reaches the intended goal, and (iii) whether it avoids unsafe or irrelevant regions. Later, we describe how we make the score range easier to interpret and show that our score formulation aligns well with human preferences. Formally, trace is sequence of points = [(x1, y1), . . . , (xn, yn)] in image space. We compare it against ground-truth traces across 1), . . . , (x modalities = [(x m)] and select the trace with the lowest error: m, 1, Score(T, G) = min DTW(T, ) + FDE(T, ) (1) + Penalty(T ) Trace Similarity: We utilize Dynamic Time Warping (DTW) [28] with the Euclidean distance as the error metric, to measure trace similarity. DTW aligns sequences by stretching or compressing the time axis and can be computed using dynamic programming: DT (T, ) = D(n, m) D(0, 0) = 0 D(i, 0) = D(0, j) = (i, > 0) D(i, j) = d((xi, yi), (x j, + min{D(i 1, j), D(i, 1), j)) (2) (3) (4) (5) D(i 1, 1)} Goal Reaching: To reward reaching the correct target, we add the Final Displacement Error (FDE), which measures the Euclidean endpoint distance: FDE(T, ) = d((xn, yn), (x m, m)) (6) Semantic Penalty: Finally, we introduce embodimentspecific semantic costs that penalize traces crossing undesired regions. Using Mask2Former model [29] trained on Mapillary Vistas [30], we infer semantic masks and map each class to manually tuned penalty values me(Si) depending on embodiment e. Classes representing more dangerous areas or obstacles are assigned higher penalty values. To allow for small deviations, we exclude tolerance band around the ground-truth. The penalty is averaged pixel-wise along the predicted trace: Penalty(T ) = 1 Pixels(T ) (cid:88) iPixels(T ) me(Si) (7) Scaling: In order to make the score values easier to interpret, we scale them to range where the worst score is at 0 and the best score at 100. We achieve this by setting the ground-truth performance 0 as the lower bound. For the upper bound, we select the performance of just drawing vertical line through the image center, which corresponds to the Straight Forward baseline performance of 3234.75. This Fig. 3: Left: Comparison between penalty cost masks based on Mask2Former and manual segmentation. These masks are used to punish traces crossing unsafe or irrelevant areas. Right: We show that the score function aligns with human preference by calculating the correlation between the score ranking and pairwise ranking created by human. results in the scaled score function: (cid:91)Score(T, G) = 3234.75 Score(T, G) 3234. 100 (8) Note that negative values are possible and do occur in our later experiments as some models perform worse than the Straight Forward baseline. Evaluation: We acknowledge that, for each term, within our proposed score function, different choices may be made. Complexities can arise from defining the exact start and end points, accounting for whether the path intersects hazardous terrain, and recognizing that distances measured in 2D image space do not directly translate to 3D navigation behavior. For example, at greater distances, higher accuracy is required to follow path, although in practice, receding-horizon control approach may be applied. Given these complexities and challenges, it is essential to evaluate whether the proposed score function aligns with human judgments. To do so, we compute Spearman correlations [31] between human pairwise ranking of predictions and several score variants. To cover the full quality range of predictions, we compile an equal mix of human, model, baseline, and intentionally flawed predictions. Annotators perform pairwise comparisons to produce human ranking, which we correlate with each score variant using Spearmans rank correlation. This correlation ranges from perfect agreement (1) to no relation (0), ignoring linear relationships in favor of rank order. The procedure is illustrated in Figure 3. We begin by comparing plain DTW similarity with alternative measures such as root mean square error (RMSE) and discrete Fréchet distance (see Table II), and observe that DTW consistently achieves the highest performance across all three trace-similarity metrics. We then evaluate the additional contribution of the FDE term for goal-reaching and find further, consistent improvement in performance. Extending the score with our Mask2Former-based semantic penalty leads to another clear performance gain. To assess how well these automatically derived semantic cost terms TABLE II: Spearman correlation between variants of the score function and human ranking. Score Variant Spearman Correlation [] RMSE Fréchet DTW DTW + FDE DTW + FDE + Manual Penalty DTW + FDE + Mask2Former (ours) 0.8167 0.8310 0.8417 0.8656 0.8723 0.8707 align with expert annotations, we asked human annotators to semantically segment images into task-relevant, irrelevant (but safe), and hazardous regions (see Figure 3). While dense manual semantic labeling is resource-intensive, it offers only limited performance gains over our Mask2Former-based strategy. Taken together, these findings validate our decision to combine all three terms. IV. EXPERIMENTS Our experiments aim to address three key questions: 1) How well do current VLMs predict navigation traces? 2) Does performance vary with embodiment or task category? 3) Which aspects of the tasks pose the greatest challenges? To answer these questions, we first establish five baselines that give insight into the core difficulties of predicting navigation traces. Next, we outline our deployment of state-of-the-art VLMs, before presenting and analyzing the benchmark results for the test split. A. Baselines We compare VLM performance against five baselines: Human: Multiple participants collectively solve all test split scenarios, providing an upper bound for model performance. Straight Forward: Places vertical line through the image center. Fig. 4: Left: Ranking of VLMs, the uninformed baseline Straight Forward, and human expert performance split into each embodiment. Note that higher score is better. Right: Performance per task category for the same models. Oracle-Goal Straight Line: Connects the given start and goal points with direct line. In contrast to VLMs, this method has direct access to the goal and start point. Only predict goal point: To isolate the difficulty of identifying the goal, we use Gemini 2.5 Pro to predict only the goal location and connect it to the given start via straight line. Only predict path: Conversely, given both start and goal, Gemini 2.5 Pro predicts only the navigation path. Together, these baselines capture informed strategies, an upper bound with human performance, and provide context for assessing VLM performance. B. Models We evaluate all VLMs by querying each model through API calls. After preliminary testing, we select eight representative models: Gemini 2.5 Pro [32], GPT-5 [33], o3 [34], Claude Sonnet 4 [35], Qwen 2.5 VL 72B [36], Qwen 3 VL 235B A22B Thinking [36], Mistral Medium 3.1 [37], and Gemma 3 27B [38]. Among these models, Gemini 2.5 Pro, GPT-5, o3, and Qwen 3 VL automatically generate reasoning steps. Each model receives carefully crafted prompt specifying the task, output format, and expected embodiment behavior. Models are instructed to return navigation traces as lists of 2D points in JSON format, which we parse to compute performance scores. C. Performance We first analyze performance across embodiment types for both VLMs and human experts (see Figure 4). As naive uninformed reference, we include the Straight Forward baseline. Human experts clearly outperform all VLMs, highlighting the gap between model capabilities and task difficulty. Among the models, Gemini 2.5 Pro ranks best, followed by GPT5, Qwen 3 VL, and o3 with the Straight Forward baseline ranking unexpectedly close behind o3. Example predictions of the top four models are shown in Figure 5. Generally, we do not observe significant differences between embodiment types for all the models. TABLE III: Comparison between informed baselines, human and Gemini 2.5 Pro. Note that higher score is better. Model Score [] Only goal point with Gemini 2.5 Pro Gemini 2.5 Pro Oracle-Goal Straight Line Only path with Gemini 2.5 Pro Human Expert 29.65 34.38 51.89 56.55 75.40 Turning to task categories in Figure 4 on the right, we again observe only minor variation. This uniformity should not be mistaken for balanced competence. Rather, the overall weakness of the models masks whether category and embodiment-specific differences exist. The competitiveness of the naive Straight Forward baseline highlights this deficit. Our experiments demonstrate that Gemini 2.5 Pro achieves the best overall performance in general navigation capabilities. To gain clearer understanding of the challenges involved in navigation, we decompose the task into goal-point prediction and path-shape prediction. Therefore, we compare Gemini 2.5 Pro with baseline models that have access to privileged information as well as with human experts (see Table III). Using Gemini 2.5 Pro to only predict the goal point and then connecting it with straight line yields only slightly worse results than having Gemini 2.5 Pro predict the full trace. While baselines with explicit access to the goal point perform significantly better, suggesting that locating the goal area is already major challenge. In particular, predicting only path shape with Gemini 2.5 Pro performs better than Oracle-Goal Straight Line, showing that the model possesses basic understanding of the scenarios. However, even when providing the goal point, Gemini 2.5 Pro falls short of human expert performance without this advantage. Overall, Gemini 2.5 Pro struggles especially in recognizing goal areas but also underperforms in shaping meaningful paths, highlighting the dual difficulty of the task. Finally, we provide qualitative insights into the reasoning Fig. 5: Example predictions by the models Gemini 2.5 Pro, GPT-5, Qwen 3 VL, and o3. process of models such as Gemini 2.5 Pro and o3. Figure 6 contains an example reasoning output of o3 where the task is to \"go to the red car\". While the models textual reasoning correctly distinguishes between the available path options and identifies the correct solution, its predicted trace fails to align with this reasoning. This is common pattern we observe when qualitatively analyzing o3s reasoning and suggests gap between linguistic reasoning and spatial grounding, particularly in localizing traversable structures within the image. Fig. 6: Example of o3s reasoning with the prediction in pink on the left and the steps on the right. The model reasons correctly but is unable to predict corresponding trace. D. Summary of Key Findings Our evaluation reveals four critical insights about current VLM navigation capabilities and areas of future work: (1) Large human performance gap. Across all four embodiments and task categories, VLM scores are substantially worse than both human and oracle-like baselines, highlighting significant room for improvement (see Figure 4 and Table III). (2) Goal localization is the dominant failure mode. When models predict only the goal location and we connect it with straight line, scores are similar to full-trace predictions. Yet even with the correct goal, path shaping lags behind human performance (see Table III). (3) Embodiment robustness. Aggregate performance differences across Human, Legged Robot, Wheeled Robot, and Bicycle embodiments are small, suggesting general limitations in spatial grounding rather than embodiment-specific blind spots (see Figure 4). (4) Score function alignment with human preference. Our semanticaware trace score, that builds on the DTW distance [28] with endpoint error and embodiment-conditioned penalties using automated semantics [29], [30], correlates more strongly with human preference than DTW alone. Using manual segmentation yields an additional but modest gain. V. CONCLUSION We presented NaviTrace, novel benchmark for evaluating VLM navigation capabilities across different embodiments, along with novel semantic-aware scoring function for fair evaluation of 2D navigation traces. NaviTrace provides the first systematic evaluation framework for embodied navigation in real-world scenarios, featuring 1, 000 diverse images from urban and rural environments and four embodiment types. Our benchmark extends pointing tasks to sequential navigation prediction, creating natural bridge between high-level VLM reasoning and low-level robotic control. To encourage future progress, we will make NaviTrace publicly available with test tasks, leaderboard, and validation split for potential fine-tuning applications. NaviTrace establishes an essential testbed for developing and evaluating navigation-capable VLMs, enabling advances in embodied AI towards truly capable robotic navigation systems. VI. LIMITATIONS NaviTrace has several key limitations. The dataset is geographically concentrated in Switzerland, which may limit generalizability to other regions with different infrastructure and navigation norms. The benchmark is restricted to singleimage scenarios, preventing evaluation of temporal reasoning and multi-step planning required in dynamic environments. The current embodiment selection is limited to ground vehicles and excludes aerial drones. Additionally, our semantic scoring function relies on automated segmentation models that may introduce systematic evaluation biases. While annotating traces has proven to be easy and efficient, the proposed scoring functionalthough shown to align effectively with human preferences and sufficient for evaluating VLM navigation capabilitiesmay still fail to capture more nuanced aspects of human preferences. For instance, while multiple ground-truth traces can capture ambiguities, they constrain the score function to recognize only specific points as goals rather than broader targets such as an entire doorway. ACKNOWLEDGMENT This work was supported by the German Research Foundation (DFG) 448648559, Luxembourg National Research Fund (Ref. 18990533), and the Swiss National Science Foundation (SNSF) as part of the projects No.200021E_229503 and No.227617. We thank Kaiqi Qu, Omkar Jarande, and Qicai Tan for joining us in the labeling effort. We also thank all the people helping us collect images and participating in the evaluation of human performance. REFERENCES [1] A.-C. Cheng, Y. Ji, Z. Yang, Z. Gongye, X. Zou, J. Kautz, E. Bıyık, H. Yin, S. Liu, and X. Wang, Navila: Legged robot visionlanguage-action model for navigation, 2025. [Online]. Available: https://arxiv.org/abs/2412.04453 [2] J. Zhang, K. Wang, S. Wang, M. Li, H. Liu, S. Wei, Z. Wang, Z. Zhang, and H. Wang, Uni-navid: video-based vision-language-action model for unifying embodied navigation tasks, 2025. [Online]. Available: https://arxiv.org/abs/2412.06224 [3] M. J. Munje, C. Tang, S. Liu, Z. Hu, Y. Zhu, J. Cui, G. Warnell, J. Biswas, and P. Stone, Socialnav-SUB: Benchmarking VLMs for scene understanding in social robot navigation, in ICRA 2025 Workshop: Human-Centered Robot Learning in the Era of Big Data and Large Models, 2025. [Online]. Available: https://openreview.net/forum?id=cCuylmKVXq [4] A. Payandeh, D. Song, M. Nazeri, J. Liang, P. Mukherjee, A. H. Raj, Y. Kong, D. Manocha, and X. Xiao, Social-llava: Enhancing robot navigation through human-language reasoning in social spaces, 2024. [Online]. Available: https://arxiv.org/abs/2501.09024 [5] G. R. Team et al., Gemini robotics: Bringing ai into the physical world, 2025. [Online]. Available: https://arxiv.org/abs/2503.20020 [6] M. Deitke et al., Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models, 2024. [Online]. Available: https://arxiv.org/abs/2409.17146 [7] L. Cheng, J. Duan, Y. R. Wang, H. Fang, B. Li, Y. Huang, E. Wang, A. Eftekhar, J. Lee, W. Yuan, R. Hendrix, N. A. Smith, F. Xia, D. Fox, and R. Krishna, Pointarena: Probing multimodal grounding through language-guided pointing, 2025. [Online]. Available: https://arxiv.org/abs/2505.09990 [8] J. Yang, R. Tan, Q. Wu, R. Zheng, B. Peng, Y. Liang, Y. Gu, M. Cai, S. Ye, J. Jang, Y. Deng, L. Liden, and J. Gao, Magma: foundation model for multimodal ai agents, 2025. [Online]. Available: https://arxiv.org/abs/2502. [9] D. Niu, Y. Sharma, G. Biamby, J. Quenum, Y. Bai, B. Shi, T. Darrell, and R. Herzig, Llarva: Vision-action instruction tuning enhances robot learning, 2024. [Online]. Available: https://arxiv.org/abs/2406.11815 [10] V. de Bakker, J. Hejna, T. G. W. Lum, O. Celik, A. Taranovic, D. Blessing, G. Neumann, J. Bohg, and D. Sadigh, Scaffolding dexterous manipulation with vision-language models, 2025. [Online]. Available: https://arxiv.org/abs/2506.19212 [11] R. Zheng, Y. Liang, S. Huang, J. Gao, H. D. III, A. Kolobov, F. Huang, and J. Yang, Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies, 2025. [Online]. Available: https://arxiv.org/abs/2412.10345 [12] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid, S. Gould, and A. van den Hengel, Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. [13] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Nießner, M. Savva, S. Song, A. Zeng, and Y. Zhang, Matterport3d: Learning from rgb-d data in indoor environments, 2017. [Online]. Available: https://arxiv.org/abs/1709.06158 [14] Y. Qi, Q. Wu, P. Anderson, X. Wang, W. Y. Wang, C. Shen, and A. v. d. Hengel, Reverie: Remote embodied visual referring expression in real indoor environments, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. [15] A. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge, Roomacross-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding, 2020. [Online]. Available: https: //arxiv.org/abs/2010.07954 [16] C. Gao, L. Jin, X. Peng, J. Zhang, Y. Deng, A. Li, H. Wang, and S. Liu, Octonav: Towards generalist embodied navigation, 2025. [Online]. Available: https://arxiv.org/abs/2506. [17] T. Akhtyamov, M. A. Mdfaa, J. A. Ramirez, S. Bakulin, G. Devchich, D. Fatykhov, A. Mazurov, K. Zipa, M. Mohrat, P. Kolesnik, I. Sosin, and G. Ferrer, Egowalk: multimodal dataset for robot navigation in the wild, 2025. [Online]. Available: https://arxiv.org/abs/2505.21282 [18] X. Liu, J. Li, Y. Jiang, N. Sujay, Z. Yang, J. Zhang, J. Abanes, J. Zhang, and C. Feng, Citywalker: Learning embodied urban navigation from web-scale videos, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2025, pp. 68756885. [19] H. Karnan, A. Nair, X. Xiao, G. Warnell, S. Pirk, A. Toshev, J. Hart, J. Biswas, and P. Stone, Socially compliant navigation dataset (scand): large-scale dataset of demonstrations for social navigation, IEEE Robotics and Automation Letters, vol. 7, no. 4, pp. 11 80711 814, 2022. [20] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, D. Parikh, and D. Batra, Habitat: platform for embodied ai research, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019. [21] X. Fu, Y. Hu, B. Li, Y. Feng, H. Wang, X. Lin, D. Roth, N. A. Smith, W.-C. Ma, and R. Krishna, BLINK: Multimodal Large Language Models Can See but Not Perceive. Springer Nature Switzerland, October 2024, p. 148166. [Online]. Available: http://dx.doi.org/10.1007/978-3-031-73337-6_9 [22] S. Tong, E. Brown, P. Wu, S. Woo, M. Middepogu, S. C. Akula, J. Yang, S. Yang, A. Iyer, X. Pan, A. Wang, R. Fergus, Y. LeCun, and S. Xie, Cambrian-1: fully open, vision-centric exploration of multimodal llms, in Advances in Neural Information Processing Systems, A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, Eds., vol. 37. [Online]. Curran Associates, Available: https://proceedings.neurips.cc/paper_files/paper/2024/file/ 9ee3a664ccfeabc0da16ac6f1f1cfe59-Paper-Conference.pdf Inc., 2024, pp. 87 31087 356. [23] R. Ramachandran, A. Garjani, R. Bachmann, A. Atanov, O. F. Kar, and A. Zamir, How well does gpt-4o understand vision? evaluating multimodal foundation models on standard computer vision tasks, 2025. [Online]. Available: https://arxiv.org/abs/2507. [24] M. Du, B. Wu, Z. Li, X. Huang, and Z. Wei, Embspatialbench: Benchmarking spatial understanding for embodied tasks with large vision-language models, 2024. [Online]. Available: https://arxiv.org/abs/2406.05756 [25] J. Frey, T. Tuna, L. F. T. Fu, C. Weibel, K. Patterson, B. Krummenacher, M. Müller, J. Nubert, M. Fallon, C. Cadena, and M. Hutter, Boxi: Design Decisions in the Context of Algorithmic Performance for Robotics, in Proceedings of Robotics: Science and Systems, Los Angeles, United States, June 2025. [26] N. Raina et al., Egoblur: Responsible innovation in aria, 2023. [27] M. Hutter, C. Gehring, D. Jud, A. Lauber, C. D. Bellicoso, V. Tsounis, J. Hwangbo, K. Bodie, P. Fankhauser, M. Bloesch, R. Diethelm, S. Bachmann, A. Melzer, and M. Hoepflinger, Anymal - highly mobile and dynamic quadrupedal robot, in 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, October 2016, p. 3844. [Online]. Available: http://dx.doi.org/10.1109/IROS.2016.7758092 [28] P. Senin, Dynamic time warping algorithm review, Information and Computer Science Department University of Hawaii at Manoa Honolulu, USA, vol. 855, no. 1-23, p. 40, 2008. [29] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar, Masked-attention mask transformer for universal image segmentation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022, pp. 12901299. [30] G. Neuhold, T. Ollmann, S. Rota Bulò, and P. Kontschieder, The mapillary vistas dataset for semantic understanding of street scenes, in International Conference on Computer Vision (ICCV), 2017. [Online]. Available: https://www.mapillary.com/dataset/vistas [31] J. Hauke and T. Kossowski, Comparison of values of pearsons and spearmans correlation coefficients on the same sets of data, Quaestiones geographicae, vol. 30, no. 2, pp. 8793, 2011. [32] G. Comanici et al., Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. [Online]. Available: https://arxiv.org/abs/2507. 06261 [33] OpenAI, Gpt-5 system card, OpenAI, Tech. Rep., August 2025, version updated August 13, 2025; PDF available at https://cdn.openai. com/gpt-5-system-card.pdf. and Openai 2025, o3 Tech. Rep., April openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/ o3-and-o4-mini-system-card.pdf. system card, OpenAI, https://cdn. available o4-mini pDF [34] , at [35] Anthropic, Claude opus 4 & claude sonnet 4 system card, [Online]. Available: https: Anthropic, Tech. Rep., May 2025. //www.anthropic.com/claude-4-system-card technical [36] S. Bai et al., Qwen2.5-vl arXiv:2502.13923, 2025. report, arXiv preprint [37] M. AI, Mistral medium 3.1, August 2025. [Online]. Available: https://mistral.ai/models [38] G. Team et al., Gemma 3 technical report, 2025. [Online]. Available: https://arxiv.org/abs/2503."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "Robotic Systems Lab",
        "University of Stuttgart"
    ]
}