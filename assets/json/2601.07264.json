{
    "paper_title": "The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents",
    "authors": [
        "Weihao Xuan",
        "Qingcheng Zeng",
        "Heli Qi",
        "Yunze Xiao",
        "Junjue Wang",
        "Naoto Yokoya"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 1 4 6 2 7 0 . 1 0 6 2 : r The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents Weihao Xuan1,2*, Qingcheng Zeng3*, Heli Qi2,4, Yunze Xiao5, Junjue Wang1, Naoto Yokoya1,2 1The University of Tokyo, 2RIKEN AIP, 3Northwestern University 4Waseda University, 5Carnegie Mellon University"
        },
        {
            "title": "Abstract",
            "content": "Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains critical challenge. fundamental pillar of this trustworthiness is calibration, which refers to an agents ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain under-explored. In this work, we systematically investigate verbalized calibration in tooluse agents, revealing fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tooluse agents. More broadly, this work establishes foundation for building self-aware agents that can reliably communicate uncertainty in highstakes, real-world deployments."
        },
        {
            "title": "Introduction",
            "content": "Autonomous agents based on LLMs represent transformative leap in artificial intelligence, evolving beyond static text processing to engage with *Equal contribution. Corresponding author. dynamic, real-world environments actively. By leveraging external tools and iterative reasoning, these systems are demonstrating rapid increases in proficiency in executing complex, long-horizon tasks that were previously intractable. The research community has recently witnessed remarkable strides across critical domains, including sophisticated code agents (Jimenez et al., 2024; Yang et al., 2024, 2025; Trae Research Team et al., 2025), autonomous web agents (He et al., 2024; Wei et al., 2025b), and advanced deep search agents (Wu et al., 2025; Li et al., 2025b,a; Team, Tongyi DeepResearch et al., 2025). The implications of this progress extend far beyond technical novelty. Indeed, these agents are poised to exert substantial economic impact and fundamentally reshape the future of work (Shao et al., 2025). While the trustworthiness and safety of LLMs have been examined extensively, research specifically addressing trustworthiness in multi-turn agents remains sparse (Hua et al., 2024; Yu et al., 2025; Shi et al., 2025). fundamental pillar of this trustworthiness is calibration, which refers to an agents ability to report confidence scores that reliably reflect its actual performance. Within the limited existing literature, search agent-oriented benchmarks (Wei et al., 2025a; Zhou et al., 2025) consistently report that tool-use agents exhibit higher calibration errors than standalone LLMs, suggesting that external tools often exacerbate overconfidence. However, these studies focus narrowly on search scenarios, leaving open critical question: is miscalibration universal consequence of tool use, or does it depend on the nature of the tool itself? In this work, we present systematic pilot study that answers this question by comparing representative tool-use agents (Jin et al., 2025; Xue et al., 2025) against their standard instruction-following counterparts. Our analysis reveals critical confidence dichotomy: not all tools affect calibration equally. Specifically, we identify two distinct tool categories with opposing effects. Evidence tools (e.g., web search), which retrieve external information laden with noise and uncertainty, systematically induce severe overconfidence. In contrast, verification tools (e.g., code interpreters), which provide deterministic execution feedback, can ground reasoning and mitigate miscalibration. This dichotomy persists across both promptingbased strategies and standard tool-use-oriented RL, indicating fundamental challenge that cannot be resolved through prompt engineering alone. To improve calibration across tool-use scenarios, we propose the Calibration Agentic RL (CAR) framework, novel RL-based fine-tuning approach that jointly optimizes task accuracy and the reliability of expressed confidence. Through holistic evaluation of diverse reward structures, we demonstrate that our trained agents maintain competitive accuracy while achieving significantly better calibration than baselines. Furthermore, we validate the robustness of our approach: agents trained in controlled local retriever environments generalize effectively to more noisy, API-based web search scenarios, and the framework proves effective in tool-integrated mathematical reasoning. Our primary contributions are summarized as follows: We conduct systematic pilot study that reveals critical confidence dichotomy in tooluse agents: while verification tools provide grounding, evidence tools inherently predispose agents to severe overconfidence. We propose CAR, an RL-based framework for optimizing agent calibration, supported by holistic benchmark of reward structures, including our novel Margin-Separated Calibration Reward (MSCR), that provides key insights for future reward design. We validate the effectiveness and robustness of our methodology across distinct task domains and demonstrate successful crossenvironment generalization from local to noisy web settings."
        },
        {
            "title": "2 Related Work",
            "content": "Calibration in LLMs The calibration of LLMs has emerged as central theme in recent literature. Verbalized confidence (Lin et al., 2022; Tian et al., 2023) has gained prominence due to its inherent interpretability and simplicity, with subsequent evaluations across instruct LLMs (Xiong et al., 2024), reasoning LLMs (Zeng et al., 2025; Yoon et al., 2025), and vision-language models (Xuan et al., 2025; Liu et al., 2025) consistently characterizing these models as exhibiting moderate overconfidence. In terms of training, Damani et al. (2025) proposed an RL-based framework to encourage calibration in single-turn LLMs. Despite extensive investigation into static LLMs, calibration in autonomous agents remains notably sparse. Recent work proposes post-hoc framework that trains an external predictor to estimate trajectory success (Anonymous, 2025). However, this approach neither analyzes how different tool types systematically affect confidence dynamics nor improves the agents intrinsic verbalized calibration. To address these gaps, we first conduct systematic pilot study examining how different tool types affect agent calibration, then propose an RL-based framework to improve intrinsic verbalized confidence. Tool-use Agents The paradigm of LLMs is shifting from static text generation to autonomous agents capable of interacting with external environments. Equipping agents with tool-use capabilities enables them to overcome inherent limitations, such as hallucinations and calculation errors. Recent literature has explored diverse tool integration strategies, which can be broadly categorized by their function. On one hand, evidence tools such as web search enable agents to retrieve external information to augment their knowledge; for instance, Search-R1 (Jin et al., 2025) leverages RL to intrinsically motivate agents to seek information when their internal knowledge is insufficient. On the other hand, verification tools such as code interpreters provide deterministic feedback to validate reasoning steps. Xue et al. (2025) integrates external Python interpreters to enhance agents mathematical reasoning capabilities through robust code execution. However, despite these advancements making agents significantly more capable, emerging evidence suggests critical side effect: toolinduced overconfidence. Preliminary results indicate that agents often place blind trust in tool outputs or overestimate their ability to solve tasks simply because tools are available (Wei et al., 2025a; Zhou et al., 2025). In this paper, we systematically analyze this overconfidence across different tool types and propose mechanisms to mitigate it. Figure 1: The confidence dichotomy and the proposed RL framework. (a) Our pilot study reveals tooldependent divergence in calibration: evidence tools (e.g., web search), which operate in noisy retrieval environments, In contrast, verification tools (e.g., code interpreters), which provide systematically induce overconfidence. deterministic execution feedback, exhibit better alignment between confidence and accuracy. (b) To address this miscalibration, we fine-tune agents with joint RL objective that combines task accuracy and calibration rewards, producing robust agents with reliable uncertainty expression."
        },
        {
            "title": "3.1 Overall Motivation",
            "content": "The primary objective of this pilot study is to anis swer the question raised in our introduction: miscalibration universal consequence of tool use, or does it depend on the nature of the tool itself? To this end, we systematically isolate and quantify the impact of different tool types on LLM calibration. We specifically analyze the confidence shifts that occur when transitioning from standard text generation to tool-augmented agentic workflows. By contrasting distinct operational modes, including standard prompting, prompting-based tool use, and RL-optimized tool use, we aim to elucidate how invoking external tools fundamentally modulates the reliability of model confidence."
        },
        {
            "title": "3.2 Experimental Setup",
            "content": "To ensure rigorous comparison, we establish three distinct experimental configurations: 1. Direct Prompting: The LLM is instructed to address queries utilizing only its internal parametric knowledge. 2. Prompting-based Tool-Use: The LLM is prompted to function as an autonomous agent with the capacity to invoke external tools, without updates to its model weights. 3. RL-based Tool-Use: This setting adopts similar agentic prompts as the second configuration. However, the model is fine-tuned explicitly via RL to enhance its multi-turn tool interaction capabilities. We focus on Web Search and Code Interpreter as they represent two fundamental paradigms of agentic tool use. Evidence tools, exemplified by Web Search, are characterized by open-ended, stochastic outputs containing noisy, unstructured information. These properties are shared by other informationretrieval tools such as RAG and API queries. Conversely, verification tools, exemplified by Code Interpreter, provide deterministic, structured feedback that facilitates logical grounding. These properties are shared by other execution-based tools such as calculators, SQL, and symbolic solvers. Most other tools used by agents fall within the spectrum defined by these two paradigms. We evaluate three configurations across two representative tool categories using Qwen2.5-3B-Instruct (Qwen Team et al., 2025) as the backbone model: Evidence Tools (Web Search): We evaluate opendomain question answering performance using the NQ (Kwiatkowski et al., 2019) and HotpotQA (Yang et al., 2018) datasets. These tasks require agents to retrieve external information from noisy retrieval environment. For the RL-based variant, we adopt the training configurations of Search-R1 (Jin et al., 2025) using the VeRL framework (Sheng et al., 2024). Verification Tools (Code Interpreter): We assess mathematical reasoning capabilities using the"
        },
        {
            "title": "Setting",
            "content": "Acc. MCIP NQ"
        },
        {
            "title": "HotpotQA",
            "content": "AIME"
        },
        {
            "title": "Code",
            "content": "AIME2025 MATH-500 Direct prompting Prompting-based tool-use RL-based tool-use Direct prompting Prompting-based tool-use RL-based tool-use Direct prompting Prompting-based tool-use RL-based tool-use Direct prompting Prompting-based tool-use RL-based tool-use Direct prompting Prompting-based tool-use RL-based tool-use 15.5 34.3 43.1 18.7 22.4 27.4 12.2 4.4 18.2 15.7 5.8 19.8 63.4 48.6 77. 0.879 0.901 0.948 0.859 0.911 0.967 0.968 0.915 0.868 0.968 0.904 0.879 0.971 0.913 0.890 Table 1: Pilot study results across tool-use configurations and domains using Qwen2.5-3B-Instruct as the backbone. Accuracy (Acc.) is reported for all tasks. Lower MCIP indicates fewer overconfidence issues. AIME2024/2025 (MAA Committees) and MATH500 (Lightman et al., 2023) benchmarks. These tasks allow agents to execute Python code and receive deterministic feedback. The RL-based model is implemented following the SimpleTIR (Xue et al., 2025) methodology for tool-integrated reasoning. (cid:80) Dwrong We measure the Mean Confidence on Incorrect Predictions (MCIP) on the intersectional wrong questions across the three settings to understand how additional tool use affects confidence, defined as MCIP = 1 ˆpi, where Dwrong = iDwrong = yi denotes the set of incorrectly answered questions and ˆpi is the models predicted confidence for its chosen answer on example i. Specifically, we use this metric to check whether agents show significantly different confidence patterns across various tool-use configurations. To ensure the power of our analysis, we additionally conducted t-tests (STUDENT, 1908) between each configuration."
        },
        {
            "title": "3.3 Results",
            "content": "The empirical results of our pilot study, detailed in Table 1, reveal critical confidence dichotomy in agent behavior. First, consistent with observations by Wei et al. (2025a), integrating evidence tools (i.e., web search) leads to marked deterioration in calibration. Regardless of whether the agent employs prompting-based strategies or RL-enhanced capabilities, the presence of search tools systematically exacerbates overconfidence. And our t-tests suggest that the differences are statistically significant from direct prompting to RL-based tool-use. This suggests that the inherent noise and ambiguity of retrieved information predispose agents to inflated certainty. Conversely, verification tools (i.e., Code interpreters) yield sharply contrasting dynamic. In this domain, tool usage mitigates overconfidence, with RL-based agents achieving the lowest MCIP (statistically significant in the reverse order). These divergent outcomes challenge the assumption that tool use exerts uniform influence on agent confidence. Instead, we identify clear heterogeneity driven by tool type: evidence tools, which introduce external information with inherent noise, interfere with confidence estimation, whereas verification tools, which provide deterministic feedback, ground the reasoning process and temper unwarranted certainty. This dichotomy motivates our development of calibration-aware training strategies for evidence tool scenarios."
        },
        {
            "title": "4 Calibration Agentic RL (CAR)",
            "content": "As demonstrated in our pilot study, agents operating with evidence tools are systematically prone to severe overconfidence due to the inherent noise in retrieved information. To mitigate this challenge in multi-turn agentic tasks, we introduce the Calibration Agentic RL (CAR) framework. This design is specifically engineered to enhance the agents ability to provide reliable confidence estimates alongside its tool-use actions."
        },
        {
            "title": "4.1.1 Training Details",
            "content": "We first focus on evidence tool scenarios where tool use exacerbates miscalibration, leveraging the Search-R1 framework (Jin et al., 2025) as our primary testbed. For the retrieval component, we directly adopt the established local search engine configuration utilized within Search-R1. This setup employs the 2018 Wikipedia dump (Karpukhin et al., 2020) as the knowledge source, coupled with E5 (Wang et al., 2024) as the dense retriever. Furthermore, to optimize the policy of our tool-use agents, we employ Group Relative Policy Optimization (GRPO) (Shao et al., 2024) as the foundational RL algorithm, and the mixture of NQ (Kwiatkowski et al., 2019) and HotpotQA (Yang et al., 2018) as our training data. We employ suite of instruction-tuned LLMs as our policy networks: Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and Qwen3-4B-Instruct2507. The selection of instruction-tuned variants is driven by two factors. First, these models exhibit superior adherence to complex instructions, facilitating more reliable verbalized confidence reporting. Second, empirical evidence from the original Search-R1 study indicates negligible performance disparities between base and instruction-tuned architectures in this context."
        },
        {
            "title": "4.1.2 Reward Design",
            "content": "Baseline Methods We evaluate CAR against three baseline methodologies: (1) Vanilla SearchR1: the reward architecture proposed by Jin et al. (2025), which utilizes an exact match (EM) outcome reward alongside structural reward to enforce adherence to the reasoning-actionobservation chain; (2) Temperature Scaling: verified post-hoc calibration method (Guo et al., 2017) applied to the vanilla Search-R1 baseline with temperature fixed at 1.5. Superior performance over this baseline would indicate that the model has internalized genuine calibration capabilities rather than merely adjusting surface-level probabilities; and (3) MASH: Gul et al. (2025) introduced penalty mechanism for excessive search tool usage that fosters robust abstention behavior. Given the correlation between selective abstention and improved calibration (Kirichenko et al., 2025; Song et al., 2025), we include this as comparative baseline. Proposed Reward Architecture Following Damani et al. (2025), we augment the system prompt to require agents to output numerical confidence score (ranging from 0 to 100) within <confidence> XML tags. Our reward design comprises two components: (1) Extended Format Reward To ensure structural integrity, we extend the standard Search-R1 format constraints. While the original formulation validates the logical ordering of reasoning, action, and observation, our design additionally mandates the presence of the confidence tag. Consequently, the boolean reward function ff ormat(y) returns value of rue only when all structural requirements, including the confidence encapsulation, are strictly satisfied. (2) Calibration-motivated Outcome Reward We reward both the accuracy of the final answer and the expressed confidence. Given the gold answer y, predicted answer y, and verbalized confidence q, we experiment with two formulations: Weighted Brier Score Reward. Following Damani et al. (2025), we use the Brier score (Brier et al., 1950) to form combined reward: R(y, q, y) = 1yy λ(q 1yy)2. (1) When λ = 1, this reduces to the RLCR formulation. However, in this setting, the lowest reward for correct attempt equals the highest reward for an incorrect one, which may make learning sensitive to the training data distribution. To restore positive incentive margin for correctness, we experiment with weighting coefficient λ = 1/3 on the Brier term. Margin-Separated Reward (MSCR). To address the optimization instability caused by incentive overlap in Brier scores, we propose MSCR. This formulation decouples calibration terms for correct and incorrect predictions to guarantee strict reward margin:"
        },
        {
            "title": "Calibration",
            "content": "RMSCR(y, q, y) = 1yy (cid:2)1 + β1(1 (1 q)2)(cid:3) 1yy (cid:2)β2q2(cid:3) , (2) where β1, β2 > 0 control the calibration magnitude. This formulation enforces strict separation: correct answers receive base reward of at least 1 (even with = 0), while incorrect answers receive at most 0 (at = 0) and incur penalties for false confidence. This eliminates the safe failure loophole, ensuring that the least confident correct answer strictly outperforms the most honest incorrect answer. Unified Reward Function The total reward combines format constraints with calibration-aware scoring. Denoting the model output as τ , the chosen calibration function as Rcal(y, q, y) and assigning penalty λf for format violations: (cid:40) rϕ(x, τ ) = Rcal(y, q, y) Rcal(y, q, y) λf if fformat(τ ) = True, otherwise. (3) In the otherwise case, if cannot be extracted, we fall back to minimal calibration reward (treating as default value) to maintain the correctness gradient while penalizing format errors via λf ."
        },
        {
            "title": "4.1.3 Evaluation Details",
            "content": "We evaluate our trained agents on the following benchmarks: (1) the validation sets of NQ and HotpotQA, serving as in-distribution (ID) datasets; and (2) SimpleQA-verified (Haas et al., 2025), curated subset of SimpleQA (Wei et al., 2024) comprising 1,000 rigorously filtered questions for outof-distribution (OOD) assessment. The retrieval corpus remains the 2018 Wikipedia dump for all local-retriever evaluations. We employ four metrics for comprehensive analysis: (1) Accuracy (Acc.), to verify that calibration improvements do not come at the cost of task performance; (2) Expected Calibration Error (ECE), the canonical metric for confidence-accuracy alignment, calculated using 10-bin scheme; (3) Brier Score, which captures both calibration and refinement as the squared difference between confidence and correctness; and (4) AUROC, to assess whether agents can reliably distinguish correct from incorrect predictions via confidence ranking. This finding suggests that the calibration mechanisms learned in search scenarios are not artifacts of the training set but instead transferable skills that generalize reliably to less familiar queries. comparative analysis of the three CAR configurations reveals the critical role of reward gap magnitude. The weighted Brier score with λ=1 (i.e., vanilla RLCR) yields the lowest ECE but suffers from significant accuracy degradation, indicating reward hacking behavior. In contrast, MSCR achieves superior accuracy-calibration trade-off: across most settings, it attains higher accuracy than the weighted Brier variant with λ=1/3 while simultaneously improving calibration. These results suggest that strict reward separation is essential for robust calibration training, point we discuss further in Section 6."
        },
        {
            "title": "5.1 General Results on Search Agents",
            "content": "A comprehensive summary of our experimental findings is presented in Table 2. The results demonstrate the robust effectiveness of CAR: across all three backbone models with different sizes, we consistently observe substantial ECE reductions compared to baseline methods. This improvement is consistent across both in-distribution (ID) and out-of-distribution (OOD) settings, with ECE relative reductions of up to 68% through explicit calibration-aware RL training. Crucially, under our optimal configuration (MSCR), agents maintain accuracy levels competitive with reward structures that strictly incentivize correctness, confirming that our design successfully balances calibration and task performance. Furthermore, our analyses of AUROC and temperature scaling suggest that CAR achieves these gains not through mere rescaling of confidence scores, but by inducing more nuanced confidence reasoning. This is evidenced by AUROC relative improvements of up to 17% in our best setting, confirming that the model has genuinely improved its ability to distinguish correct from incorrect outputs. This improved reasoning capability translates into robust generalization, as evidenced by the comparison between ID and OOD settings. Our results indicate that CAR engenders fundamental understanding of confidence rather than superficial alignment with in-distribution data. Specifically, on the SimpleQA-verified dataset, agents trained via CAR exhibit marked calibration improvements. Our primary experiments employed simulated retriever environment with static Wikipedia dump. However, real-world deployment poses additional challenges: commercial API-based retrievers often exhibit stochastic behavior and return noisy or extraneous information. In this section, we investigate whether the calibration capabilities learned in controlled settings transfer to these more challenging, API-driven environments. Setup We use the Serper API as our retrieval backbone and evaluate both vanilla Search-R1 and CAR (MSCR) on the SimpleQA-verified benchmark. As shown in Table 3, our trained agents achieve superior calibration compared to the vanilla baseline while maintaining competitive accuracy. These results confirm that the calibration capabilities acquired in simulated environments are not brittle but transfer robustly to the stochastic and noisy conditions of real-world API interactions."
        },
        {
            "title": "5.3 Tool-integrated Reasoning",
            "content": "Building on our pilot study, we extend evaluation to Tool-integrated Reasoning (TIR), where agents leverage code interpreters to solve mathematical problems. This extension allows us to examine how CAR performs with verification tools, which our pilot study identified as inducing calibration dynamics different from those of evidence tools. Setup We utilize the SimpleTIR (Xue et al., 2025) framework with Qwen2.5-3B-Instruct as Model Strategy HotpotQA (ID) Acc ECE Brier AUROC Acc ECE Brier AUROC Acc ECE Brier AUROC SimpleQA-verified (OOD) NQ (ID) Qwen2.5-3B Qwen2.5-7B Qwen3-4B 43.1 Vanilla Search-R1 43.1 Temperature Scaling 43.4 MASH CAR (Weighted Brier, λ=1) 22.3 CAR (Weighted Brier, λ=1/3) 44.5 45.5 CAR (MSCR) 61.1 Vanilla Search-R1 61.1 Temperature Scaling 67.0 MASH CAR (Weighted Brier, λ=1) 65.2 CAR (Weighted Brier, λ=1/3) 67.7 69.3 CAR (MSCR) 45.7 Vanilla Search-R1 45.7 Temperature Scaling 36.3 MASH CAR (Weighted Brier, λ=1) 28.5 CAR (Weighted Brier, λ=1/3) 44.2 45.5 CAR (MSCR) 0.528 0.500 0.479 0.091 0.307 0.303 0.363 0.311 0.309 0.221 0.281 0.238 0.452 0.380 0.543 0.115 0.274 0.272 0.519 0.489 0.470 0.091 0.313 0.303 0.367 0.330 0.315 0.242 0.293 0. 0.438 0.377 0.514 0.120 0.274 0.272 0.600 0.600 0.612 0.941 0.677 0.699 0.563 0.563 0.543 0.693 0.629 0.637 0.634 0.634 0.645 0.918 0.718 0.724 27.4 27.4 28.9 24.3 28.5 29.2 54.8 54.8 55.6 48.7 52.8 56. 46.6 46.6 35.5 36.6 45.0 45.9 0.699 0.674 0.656 0.148 0.329 0.286 0.424 0.371 0.421 0.330 0.368 0.326 0.408 0.343 0.498 0.184 0.281 0.269 0.686 0.651 0.631 0.148 0.329 0.289 0.423 0.380 0.422 0.358 0.379 0. 0.391 0.340 0.457 0.186 0.281 0.270 0.599 0.599 0.598 0.902 0.651 0.688 0.551 0.551 0.521 0.583 0.600 0.641 0.671 0.671 0.695 0.856 0.727 0.740 34.8 34.8 35.1 21.2 35.8 36.6 40.7 40.7 41.5 24.4 40.5 40. 42.2 42.2 37.4 33.1 40.1 41.8 0.610 0.583 0.589 0.027 0.203 0.192 0.441 0.385 0.465 0.050 0.177 0.150 0.287 0.254 0.299 0.036 0.127 0.106 0.587 0.549 0.560 0.027 0.203 0.191 0.398 0.347 0.420 0.053 0.176 0. 0.210 0.198 0.213 0.033 0.127 0.106 0.702 0.702 0.696 0.983 0.740 0.763 0.776 0.776 0.775 0.940 0.798 0.837 0.874 0.874 0.859 0.973 0.912 0.929 Table 2: Main results organized by backbone model. Dashed lines separate baselines from CAR variants."
        },
        {
            "title": "Method",
            "content": "SimpleQA-verified (Serper API) Acc ECE Brier AUROC Qwen2.5-3B Qwen2.5-7B Qwen3-4B Vanilla S-R1 CAR (MSCR) Vanilla S-R1 CAR (MSCR) Vanilla S-R1 CAR (MSCR) 76.18 76.18 70.28 71.01 85.27 84.97 0.213 0.175 0.204 0. 0.140 0.034 0.219 0.175 0.204 0.180 0.140 0.073 0.659 0.823 0.831 0. 0.825 0.765 Table 3: Tool generalization under noisy API-driven retriever (Serper API). We evaluate Vanilla Search-R1 and CAR (MSCR) on SimpleQA-verified. the backbone model. We compare two configurations: vanilla SimpleTIR as the baseline and our MSCR design. For evaluation, we adopt the AIME2024/2025 (MAA Committees) and MATH500 (Lightman et al., 2023) benchmarks, utilizing E2B1 as the code execution sandbox. Results Table 4 presents the quantitative results. Consistent with our findings in the search domain, CAR yields robust calibration improvements for TIR agents, with significant reductions in ECE and Brier scores alongside increases in AUROC. These gains persist across all evaluated benchmarks, confirming the generalizability of our reward formulation. However, examining absolute performance reveals an important nuance. Despite these improvements, ECE metrics for TIR agents remain elevated compared to both pure reasoning models (Zeng et al., 2025) and our search-based agents. Furthermore, calibration efficacy correlates with task complexity: agents exhibit substantially lower ECE on MATH-500 than on the more challenging AIME benchmarks. These observations align with our 1https://e2b.dev/ pilot study hypothesis that verification tools, while providing grounding through deterministic feedback, still exhibit calibration dynamics that depend on task difficulty. We conclude that while explicit calibration rewards provide necessary corrective signal, ultimate calibration performance in TIR settings remains bounded by the models intrinsic reasoning capabilities."
        },
        {
            "title": "6 Discussion",
            "content": "Our findings reveal that tool use introduces systematic yet heterogeneous effects on agent calibration, challenging the implicit assumption that tool augmentation uniformly improves or degrades reliability. In this section, we discuss the broader implications of these findings and examine the mechanisms underlying the observed confidence dichotomy. From Static Elicitation to Tool-Modulated Dynamics. The transition from static LLMs to autonomous agents necessitates fundamental reevaluation of verbalized calibration paradigms. While extensive literature establishes that models can accurately express uncertainty in singleturn QA (Tian et al., 2023) or standard Chainof-Thought reasoning (Xiong et al., 2024; Zeng et al., 2025), our analysis reveals that tool integration introduces non-trivial heterogeneity that disrupts this alignment. Specifically, we contextualize the severe miscalibration observed in recent webbrowsing agents (Wei et al., 2025a; Zhou et al., 2025) not merely as general capability failure, but as symptom of the evidence tool dynamic, where stochastic retrieval artificially inflates internal certainty. This stands in sharp contrast to verification scenarios, where deterministic feedback"
        },
        {
            "title": "Method",
            "content": "AIME2025 Acc ECE Brier AUROC Acc ECE Brier AUROC Acc ECE Brier AUROC MATH-500 AIME2024 Qwen2.5-3B Vanilla SimpleTIR 18.2 20.8 CAR (MSCR) 0.692 0. 0.630 0.485 0.489 0.548 19.8 21.1 0.687 0.519 0.632 0.410 0.498 0. 77.0 76.9 0.151 0.057 0.193 0.168 0.622 0.636 Table 4: Results on mathematical reasoning benchmarks of tool-integrated reasoning agents. provides the grounding often assumed but absent in open-ended search. These findings suggest that calibration research in agentic settings must account for tool-type-specific dynamics rather than treating tool use as monolithic phenomenon. Why Do Evidence Tools Induce Overconfidence? Our pilot study indicates that evidence tools systematically amplify overconfidence, but the underlying mechanism warrants further examination. We attribute this phenomenon to fundamental asymmetry in feedback signals. Verification tools such as code interpreters provide explicit execution feedback: syntax errors, runtime exceptions, and type mismatches offer clear signals that something is wrong. While successful execution does not guarantee correctness, as logical errors may still produce plausible but incorrect outputs, these tools nonetheless provide partial grounding via observable failure modes. In contrast, evidence tools offer little answer-level correctness feedback. web search typically returns results, regardless of whether they are relevant, accurate, or sufficient to answer the query. The absence of negative feedback leads agents to conflate the presence of retrieved information with the correctness of their answer. This effect is compounded by form of false certainty induced by the retrieval action itself. Having performed an explicit information-seeking step, the agent treats it as due diligence, even when the retrieved content is noisy or misleading. Retrieved passages often contain surface-level lexical overlap with the query, which the agent mistakes for genuine evidential support. Extending RLCR to Tool-Use Agents via MSCR. Our approach builds on calibration-motivated RL, particularly RLCR (Damani et al., 2025), which shows that optimizing binary correctness rewards can degrade calibration by encouraging guessing and proposes incorporating calibration terms into the training objective. We extend this principle to the agentic setting. Tool-use agents face additional exogenous noise and expanded action spaces, thereby enlarging the space of degenerate solutions in which confidence becomes uninformative or strategically manipulated. Concretely, we find that reward overlap between correct and incorrect trajectories makes learning sensitive to data difficulty and can incentivize confidence collapse. Our proposed MSCR addresses this by enforcing strict separation between reward landscapes for correct and incorrect trajectories, preserving correctness margin while still shaping confidence within each region. Empirically, this design improves calibration and failure discrimination beyond what simple rescaling would achieve, and generalizes to noisy retrieval environments, suggesting that calibrationmotivated RL remains effective in multi-step tool use provided that incentive separation is maintained."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we systematically investigated the calibration dynamics of tool-use agents. Our pilot study revealed fundamental confidence dichotomy: while verification tools provide deterministic feedback that grounds reasoning, evidence tools introduce stochastic noise that systematically induces overconfidence. To address the miscalibration in tool-use agents, we proposed the Calibration Agentic RL (CAR) framework, incorporating novel Margin-Separated Calibration Reward (MSCR) that strictly separates incentives for correct and incorrect predictions. Extensive experiments demonstrate that CAR significantly reduces calibration error while maintaining competitive task performance, with robust generalization from local simulation to noisy, real-world API environments. Our findings underscore the necessity of tool-specific calibration strategies and establish foundation for building self-aware agents capable of reliably communicating uncertainty in highstakes deployments."
        },
        {
            "title": "Limitations",
            "content": "In this work, we studied the confidence dichotomy between evidence and verification tools in controlled agentic settings and proposed the Calibration Agentic RL framework to address miscalibration in evidence-tool scenarios. One key limitation is that our experiments focus on models with 3B to 7B parameters due to computational constraints. While we observe consistent patterns across three backbone architectures, it remains unclear how this phenomenon evolves with scale. Furthermore, our evaluation primarily focuses on short-answer question answering and mathematical reasoning, where correctness is well-defined. Calibration behavior in more open-ended generation scenarios, such as long-form report writing or multi-step autonomous planning, may involve underspecified correctness signals or more delayed feedback loops that our current framework does not address. We leave these explorations to future work."
        },
        {
            "title": "References",
            "content": "Anonymous. 2025. Agentic confidence calibration. Under review at ICLR 2026. Glenn W. Brier and 1 others. 1950. Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1):13. Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, and Jacob Andreas. 2025. Beyond binary rewards: Training lms to reason about their uncertainty. Preprint, arXiv:2507.16806. Mustafa Omer Gul, Claire Cardie, and Tanya Goyal. 2025. Pay-per-search models are abstention models. Preprint, arXiv:2510.01152. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML17, page 13211330. JMLR.org. Lukas Haas, Gal Yona, Giovanni DAntonio, Sasha Goldshtein, and Dipanjan Das. 2025. Simpleqa verified: reliable factuality benchmark to measure parametric knowledge. Preprint, arXiv:2509.07968. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. 2024. Webvoyager: Building an end-toend web agent with large multimodal models. arXiv preprint arXiv:2401.13919. Wenyue Hua, Xianjun Yang, Mingyu Jin, Zelong Li, Wei Cheng, Ruixiang Tang, and Yongfeng Zhang. 2024. TrustAgent: Towards safe and trustworthy LLM-based agents. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1000010016, Miami, Florida, USA. Association for Computational Linguistics. Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2024. Swe-bench: Can language modPreprint, els resolve real-world github issues? arXiv:2310.06770. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. Preprint, arXiv:2503.09516. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In EMNLP (1), pages 67696781. Polina Kirichenko, Mark Ibrahim, Kamalika Chaudhuri, and Samuel J. Bell. 2025. Abstentionbench: Reasoning llms fail on unanswerable questions. Preprint, arXiv:2506.09038. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466. Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, Xinyu Wang, Zile Qiao, Zhen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025a. Websailor-v2: Bridging the chasm to proprietary agents via synthetic data and scalable reinforcement learning. Preprint, arXiv:2509.13305. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025b. Websailor: Navigating super-human reasoning for web agent. Preprint, arXiv:2507.02592. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Preprint, 2023. arXiv:2305.20050. Lets verify step by step. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Teaching models to express their uncertainty in words. Preprint, arXiv:2205.14334. Jiarui Liu, Weihao Xuan, Zhijing Jin, and Mona Diab. 2025. Taming object hallucinations with verified atomic confidence estimation. arXiv preprint arXiv:2511.09228. MAA Committees. Aime problems and solutions. https://artofproblemsolving.com/wiki/ index.php/AIME_Problems_and_Solutions. Qwen Team, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, and 24 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Yijia Shao, Humishka Zope, Yucheng Jiang, Jiaxin Pei, David Nguyen, Erik Brynjolfsson, and Diyi Yang. 2025. Future of work with ai agents: Auditing automation and augmentation potential across the u.s. workforce. Preprint, arXiv:2506.06576. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256. Yucheng Shi, Wenhao Yu, Wenlin Yao, Wenhu Chen, and Ninghao Liu. 2025. Towards trustworthy gui agents: survey. Preprint, arXiv:2503.23434. Linxin Song, Taiwei Shi, and Jieyu Zhao. 2025. The In hallucination tax of reinforcement finetuning. Findings of the Association for Computational Linguistics: EMNLP 2025, pages 21052120, Suzhou, China. Association for Computational Linguistics. STUDENT. 1908. The probable error of mean. Biometrika, 6(1):125. Team, Tongyi DeepResearch, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, and 1 others. 2025. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning. 2023. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 54335442, Singapore. Association for Computational Linguistics. Trae Research Team, Pengfei Gao, Zhao Tian, Xiangxin Meng, Xinchen Wang, Ruida Hu, Yuanan Xiao, Yizhou Liu, Zhao Zhang, Junjie Chen, Cuiyun Gao, Yun Lin, Yingfei Xiong, Chao Peng, and Xia Liu. 2025. Trae agent: An llm-based agent for software engineering with test-time scaling. Preprint, arXiv:2507.23370. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2024. Text embeddings by weakly-supervised contrastive pre-training. Preprint, arXiv:2212.03533. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. 2024. Measuring short-form factuality in large language models. Preprint, arXiv:2411.04368. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. 2025a. Browsecomp: simple yet challenging benchmark for browsing agents. Preprint, arXiv:2504.12516. Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, Hyokun Yun, and Lihong Li. 2025b. Webagent-r1: Training web agents via endto-end multi-turn reinforcement learning. Preprint, arXiv:2505.16421. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. Webdancer: Towards autonomous information seeking agency. Preprint, arXiv:2505.22648. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2024. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. Preprint, arXiv:2306.13063. Weihao Xuan, Qingcheng Zeng, Heli Qi, Junjue Wang, and Naoto Yokoya. 2025. Seeing is believing, but how much? comprehensive analysis of verbalized calibration in vision-language models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 14081450, Suzhou, China. Association for Computational Linguistics. Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, and Bo An. 2025. Simpletir: End-to-end reinforcement learning for Preprint, multi-turn tool-integrated reasoning. arXiv:2509.02479. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. John Yang, Kilian Lieret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. 2025. Swe-smith: Scaling data for software engineering agents. Preprint, arXiv:2504.21798. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, Brussels, Belgium. Association for Computational Linguistics. Dongkeun Yoon, Seungone Kim, Sohee Yang, Sunkyoung Kim, Soyeon Kim, Yongil Kim, Eunbi Choi, Yireun Kim, and Minjoon Seo. 2025. Reasoning models better express their confidence. Preprint, arXiv:2505.14489. Miao Yu, Fanci Meng, Xinyun Zhou, Shilong Wang, Junyuan Mao, Linsey Pang, Tianlong Chen, Kun Wang, Xinfeng Li, Yongfeng Zhang, Bo An, and Qingsong Wen. 2025. survey on trustworthy llm agents: Threats and countermeasures. Preprint, arXiv:2503.09648. Qingcheng Zeng, Weihao Xuan, Leyang Cui, and Rob Voigt. 2025. Thinking out loud: Do reasoning modIn Proceedings of els know when theyre right? the 2025 Conference on Empirical Methods in Natural Language Processing, pages 13941407, Suzhou, China. Association for Computational Linguistics. Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, Yuxin Gu, Sixin Hong, Jing Ren, Jian Chen, Chao Liu, and Yining Hua. 2025. Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese. Preprint, arXiv:2504.19314."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Northwestern University",
        "RIKEN AIP",
        "The University of Tokyo",
        "Waseda University"
    ]
}