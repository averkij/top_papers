{
    "paper_title": "LONGCODEU: Benchmarking Long-Context Language Models on Long Code Understanding",
    "authors": [
        "Jia Li",
        "Xuyuan Guo",
        "Lei Li",
        "Kechi Zhang",
        "Ge Li",
        "Jia Li",
        "Zhengwei Tao",
        "Fang Liu",
        "Chongyang Tao",
        "Yuqi Zhu",
        "Zhi Jin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current advanced long-context language models offer great potential for real-world software engineering applications. However, progress in this critical domain remains hampered by a fundamental limitation: the absence of a rigorous evaluation framework for long code understanding. To gap this obstacle, we propose a long code understanding benchmark LONGCODEU from four aspects (8 tasks) to evaluate LCLMs' long code understanding ability required for practical applications, including code unit perception, intra-code unit understanding, inter-code unit relation understanding, and long code documentation understanding. We evaluate 9 popular LCLMs on LONGCODEU (i.e., 6 general models and 3 code models). Our experimental results reveal key limitations in current LCLMs' capabilities for long code understanding. Particularly, the performance of LCLMs drops dramatically when the long code length is greater than 32K, falling far short of their claimed 128K-1M context windows. In the four aspects, inter-code unit relation understanding is the most challenging for LCLMs. Our study provides valuable insights for optimizing LCLMs and driving advancements in software engineering."
        },
        {
            "title": "Start",
            "content": "LONGCODEU: Benchmarking Long-Context Language Models on"
        },
        {
            "title": "Long Code Understanding",
            "content": "Jia Li, Xuyuan Guo, Lei Li, Kechi Zhang, Ge Li, Jia Li Zhengwei Tao, Fang Liu, Chongyang Tao, Yuqi Zhu, Zhi Jin Key Lab of High Confidence Software Technology (Peking University), MoE, School of Computer Science, Peking University, China {lijiaa,zhangkechi}@pku.edu.cn, lige@pku.edu.cn, zhijin@pku.edu.cn , 5 2 0 2 6 ] . [ 1 9 5 3 4 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Current advanced long-context language models offer great potential for real-world software engineering applications. However, progress in this critical domain remains hampered by fundamental limitation: the absence of rigorous evaluation framework for long code understanding. To gap this obstacle, we propose long code understanding benchmark LONGCODEU from four aspects (8 tasks) to evaluate LCLMs long code understanding ability required for practical applications, including code unit perception, intra-code unit understanding, intercode unit relation understanding, and long code documentation understanding. We evaluate 9 popular LCLMs on LONGCODEU (i.e., 6 general models and 3 code models). Our experimental results reveal key limitations in current LCLMs capabilities for long code understanding. Particularly, the performance of LCLMs drops dramatically when the long code length is greater than 32K, falling far short of their claimed 128K1M context windows. In the four aspects, inter-code unit relation understanding is the most challenging for LCLMs. Our study provides valuable insights for optimizing LCLMs and driving advancements in software engineering."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in long-context language models (LCLMs) like Gemini-1.5 (Team et al., 2024) and GPT-4o (GPT, 2024), which support context windows exceeding hundreds of thousands of tokens, offer unprecedented potential for real-world software engineering applications. These models promise transformative improvements in related downstream tasks, such as repository-level code generation (Zhang et al., 2024b; Bi et al., 2024), real-world GitHub issues resolution(Jimenez et al., 2023), and long code summarization (Dhulshette et al., 2025). However, progress in this critical area Corresponding authors. Figure 1: Examples of synthetic long code with independent functions and real-world long code with nonstandalone functions. Dependencies are highlighted. remains hampered by fundamental limitation: the lack of rigorous evaluation frameworks for long code understandinga capability essential for real-world software development tools that require accurate code unit perception, intra-code unit understanding, inter-code unit relation understanding, and long code documentation understanding in code repositories. Current benchmarks generally fall into two categories, which face five fundamental limitations that hinder comprehensive evaluation of LCLMs long code understanding capabilities. The first category includes studies such as RepoQA (Liu et al., 2024), whose task design has insufficient diversity like only focusing on needle function search. While these evaluations are useful, ❶ they do not capture the real-world full range of code understanding capabilities needed for practical coding scenarios. Additionally, benchmarks like L-Eval An et al. (2023) further compound these limitations by using synthetic long code through simple joining of independent code snippets. ❷ This approach overlooks the natural dependencies between code segments, as shown in Figure 1. ❸ These studies also face issues with data contamination. They neither enforce temporal constraints on code release dates nor address potential model pre-exposure through evaluating on previously published datasets. ❹ Their maximum supported context length of 36.5K tokens also falls far short of stress-testing the claimed 128K-1M context windows of modern LCLMs. The second category includes benchmarks like Long Code Arena (Bogomolov et al., 2024), LongBench (Bai et al., 2023), SWE-bench (Jimenez et al., 2023), and DevEval (Li et al., 2024b), which evaluate long-context understanding based on performance in downstream tasks. ❺ However, these benchmarks entangle code understanding with other task-specific challenges like code generation and bug fixing. This makes it hard to determine whether performance limitations are due to lack of code understanding or other factors. To address these limitations, we propose LONGCODEU, benchmark designed to isolate and comprehensively evaluate LCLMs capacity to understand and reason about real-world, dependencyrich, long code contexts. Our benchmark offers the following key features: Comprehensive Tasks stem from Practical Applications. We evaluate LCLMs from four aspects (8 tasks) to evaluate the LCLMs long code understanding capability required for practical applications, including code unit perception, intra-code unit understanding, intercode unit relation understanding, and long documentation understanding. Extra-long Code Context. Each task contains around 500 gathered long codes. The lengths of examples change in 08K, 816K, 1632K, 3264K, and 64128K following the normal distribution, which far exceeds the maximum length of 36.5K supported by existing benchmarks (An et al., 2023). Real-world Repository. The benchmark is collected from real-world code repositories. Long code consists of one or more real code file contents, instead of being composed of multiple independent code units like current benchmarks (Liu et al., 2024). Reducing Data Contamination. We collect up-to-date code repositories that are created after 2024-06 from GitHub1, which are later than most prevailing LCLMs cut-off dates thus reducing the risk of data contamination. We evaluate 9 popular LCLMs, which contain 6 general models (i.e., GPT-4o (GPT, 2024), Claude-3.5-Sonnet (cla, 2024), Gemini-1.5-Flash (Team et al., 2024), DeepSeek-V2.5 (Bai et al., 2023), Mistral-v0.3 (Jiang et al., 2023), and Phi3.5 (Abdin et al., 2024) ) and 3 code models (i.e., DeepSeek-Coder-V2 (Bai et al., 2023), Qwen2.5Coder (Hui et al., 2024), CodeLlama (Roziere et al., 2023)) on LONGCODEU. The experimental results reveal key limitations in current LCLMs capabilities for long code understanding. Especially, LCLMs performance drops dramatically when the long code length is greater than 32K, falling far short of their claimed context windows such as 128K-1M tokens. In the four aspects, intercode unit relation understanding is the most challenging for LCLMs. Our findings provide valuable insights for optimizing LCLMs and driving advancements in software engineering."
        },
        {
            "title": "2 Related Works",
            "content": "Benchmarks on Long Code Understanding. Existing studies can be mainly categorized into two types. The first category predominantly explores the long code understanding ability required in downstream applications like the needle-in-ahaystack task, but they face significant limitations. RepoQA (Liu et al., 2024) is pioneer introduced needle function retrieval task. However, the single task is insufficient to evaluate the complex long code understanding ability. L-Eval (An et al., 2023), widely-used benchmark, has limitations as well. It constructs artificial \"long code\" by concatenating independent code snippets, ignoring context dependency in real-world source code. These limitations highlight the need for more comprehensive and reliable benchmarking approach for evaluating LCLMs long code understanding ability. The second category includes research like LCArea (Bogomolov et al., 2024). They verify LCLMs long code understanding ability by measuring their performance on downstream tasks such as LongBench (Bai et al., 2023), SWE-bench (Jimenez et al., 2023), and EvoCodeBench (Li et al., 2024a). Although they provide an intuitive way to assess LCLMs, it suffers from significant drawback. The performance of downstream tasks is confounded by 1https://github.com/ Table 1: The comparison between existing benchmarks and LONGCODEU. #Num is the abbreviation of number. #Div Tasks refers to diverse tasks. #High Disp represents high dispersion. #Max-L and #Avg-L mean the maximum length and the average length of long code. #Trunk-L means whether each example has the length label. #Doc refers to documentation related to repositories. #Task represents the number of tasks (i.e., examples). Benchmark Comprehensive Code Tasks #Num #Div Tasks #High Disp Extra-long Data #Max-L #Avg-L #Length-L The second category benchmarks (Only some benchmarks are listed) Real-world Repository Code #Doc #Num Reduce Data Leaking Data Scale Data Time #Task LongBench [6] LC-Arena [8] LONGPROC [30] DevEval [21] 2 6 1 1 The first category benchmarks RepoQA [22] L-Eval [5] LONGCODEU 1 1 8 0.4K 2K 0.3K 16K 36.5K 128K 31.5K 54.8K Function File Function File Function Function File 62 0 164 50 0 2023.022023.08 2023.012024.05 No Limit 2023.11-2024.02 No Limit No Limit 2024.062024.11 1,000 200 1,825 500 90 3,"
        },
        {
            "title": "3.1 Tasks",
            "content": "In real-world software development, long code understanding is usually oriented towards code repositories. These repositories take functions as basic code units, establish relations among units to achieve complex applications, and introduce documentation to describe code-related information. LCLMs with good long code understanding abilities should be able to perceive and understand basic code units, relations among units, and associated code documentation, which is essentially required for dealing with downstream tasks such as repository-level code generation, issue resolving, and long code summarization. In this paper, we propose LONGCODEU to comprehensively evaluate LCLMs long code understanding ability from four aspects: code unit perception, intra-code unit understanding, inter-code unit relation understanding, and long documentation understanding. Four aspects of LONGCODEU are shown in Figure 2 These long code understanding tasks share the following procedure: given an instruction, long code, and anchor input, LCLMs output the desired answer. The instruction briefly describes the request of each task. The anchor input is the detailed demand such as code unit. In LONGCODEU, instruction and anchor input are generally short, while long code is long containing 0128K tokens."
        },
        {
            "title": "3.1.1 Code Unit Perception",
            "content": "Understanding long code, particularly in code repositories, requires identifying its numerous functions, as they form the foundational code unit for comprehending long codes overall functionality. In this paper, we treat function as the code unit and introduce the code unit perception task to evaluate LCLMs code unit identifying ability in long code. Concretely, this task requires LCLMs to identify all defined functions in long code and return Figure 2: Four understanding aspects in LONGCODEU. multiple factors, and these works do not decouple the long code understanding ability independently, which is orthogonal to our objective-evaluating LCLMs capacity to understand and reason about real-world, dependency-rich, long code. Long Context Language Models. Recent studies have explored diverse ways to extend large language models context window size. The direct way is to fine-tune models on long sequences (Wu et al., 2021), but they are often effort-costing. Some approaches involve additional fine-tuning for longer context (Xiong et al., 2023; Chen et al., 2023a,b,c; Peng et al., 2023). They down-scale the input position indices to match the original context window size of models with several training steps. There are also some training-free studies, which use window attention to clip the long sequences (Han et al., 2023; Ding et al., 2023; Xiao et al., 2023). Concurrently, series of approaches modify the relative distance to extend the extrapolation length (Zhang et al., 2024a; Jin et al., 2024). In this paper, we construct comprehensive benchmark to evaluate LCLMs long code understanding ability."
        },
        {
            "title": "3 LONGCODEU Benchmark",
            "content": "In this section, we first introduce long code understanding tasks (3.1). Then, we describe the construction process of LONGCODEU (3.2). Finally, we present the evaluation metrics (3.3). their corresponding function names, where long code is composed of one or more code file contents collected from real-world repositories. This ability is the cornerstone for downstream tasks."
        },
        {
            "title": "3.1.2\nIntra-Code Unit Understanding\nBased on code unit perception, we further evaluate\nLCLMs’ ability to understand the internal logic and\nsemantics of code units.",
            "content": "Code Unit Data Flow Analysis. In this section, we propose the code unit data flow analysis task that verifies whether LCLMs can understand the internal logic of code units by tracking data flow to certain extent. Given code unit in long code and variable name in the code unit, LCLMs are required to figure out lines where the value of the given variable changes by tracing data flow. For example, after executing the line upper_string += 2\", the value of variable upper_string\" changes. The given code unit is randomly selected from long code and its position in long code is also random. Evaluating code unit understanding ability is significant for LCLMs to discover potential vulnerabilities, repair vulnerabilities, and optimize code. Code Unit Semantic Analysis. In addition to analyzing the data flow of code units, we propose the code unit semantic analysis task to further verify LCLMs intra-code unit understanding ability. The task asks LCLMs to return code unit from the long code that satisfies the given description. The long code contains real-world code files, where all descriptions of code units are removed to prevent LCLMs from acquiring clues in them. This task requires LCLMs understand the semantics of code units and then return the desired code unit."
        },
        {
            "title": "3.1.3 Inter-Code Unit Relation Understanding\nIn real-world long code, especially in code repos-\nitories, code units are non-standalone. Grasping\ncode unit relations is essential for LCLMs to un-\nderstand the complex functionality of long code,\nwhere code unit relations mainly containing de-\npendency relations and semantic relations. The\ndependency relation indicates the calls among code\nunits. The semantic relation focuses on the func-\ntional similarities of code units.",
            "content": "Dependency Relation Analysis. (T1) Given code unit, this task requires LCLMs to find code units that are invoked by the given unit from long code, where the long code covers one or multiple code files and is collected from the same repository with the given code unit. The dependency relation analysis ability can assist LCLMs correctly identifying other code units related to vulnerable units and determining vulnerability scopes in real-world applications. (T2) Considering that in real-world applications, apart from long code as LCLMs input, developers usually use natural language requirements to interact with LCLMs. Thus, LCLMs also need to understand dependency relation between long code and requirements. Given natural language description, this task requires LCLMs to find code units from the long code that are invoked for generating the desired code that satisfies the given description. The ability can ensure that LCLMs sucessfully invoke existing code units in repository-level code generation and correctly integrate generated code into the current repository. Semantic Relation Extraction. Even if two code units have no dependency relationship, they might be semantically similar such as having similar implementation or logic. This section analyzes semantic relations of code units in long code. (T1) Given code unit, this task asks LCLMs to extract semantically similar code units with the given unit from long code. Extracting semantic relations of units can effectively help LCLMs to improve software development efficiency by reusing similar code units when programming, and enhance software maintainability such as finding potentially similar vulnerabilities among semantically similar units. (T2) As described in dependency relation analysis (T2), understanding the semantical relations of code units and natural language requirements is more in line with practical development scenarios. For instance, in repository-level code generation, developers input requirement to LCLMs. LCLMs can find semantically similar units with the given requirement from the current repository and then refer to these units to generate desired codes. In this task, LCLMs need to extract semantically similar code units to given requirement, which challenges LCLMs to understand the semantics of units in long code and reason their semantic relations."
        },
        {
            "title": "3.1.4 Long Documentation Understanding",
            "content": "In real-world software engineering, code documentation also plays crucial role. It encompasses diverse range of code-related information including descriptions of code units, usage patterns, architecture designs, and more. Consequently, it is essential not merely to verify the long code underTable 2: Statistics of LONGCODEU. #Num means the number of examples in each task. #C-File represents whether the output can be obtained by aggregating crossfile content. #Avg-L is the average length of the output. #Gran means the granularity of the output."
        },
        {
            "title": "Task",
            "content": "CU_P CU_SA CU_DFA DRA_T1 DRA_T2 SRE_T1 SRE_T"
        },
        {
            "title": "Output",
            "content": "#Num"
        },
        {
            "title": "Format",
            "content": "#C-File #Avg-L #Gran 487 500 500 500 500"
        },
        {
            "title": "Document",
            "content": "0.4K 0.2K"
        },
        {
            "title": "Name\nFunction",
            "content": "0.03K"
        },
        {
            "title": "Line",
            "content": "0.3K 0.3K 1.0K 1.1K 0.7K"
        },
        {
            "title": "Function\nFunction\nFunction\nFunction",
            "content": "NL standing ability, but also to analyze the long code documentation understanding ability of LCLMs. We introduce the long documentation understanding task. Given long documentation and code unit name such as function name contained in the documentation, this task requires LCLMs to extract the related information to the unit name. We ensure that the long documentation contains the related knowledge of the given unit name, aiming to effectively evaluate LCLMs. Analyzing long documentation is critical to verify the performance of LCLMs in real-world software development."
        },
        {
            "title": "3.2 Benchmark Construction",
            "content": "The pipeline for constructing LONGCODEU encompasses 6 stages as follows. Stage ❶: Repository Selection. Referring to the TIOBE index (Tio, 2024) for programming language popularity, the most popular language is Python. Thus, we conduct experiments on Python and will expand to other languages in the future. In Python source platforms, PyPI is rich Python package index tool. We identify the top 10 popular programming topics in PyPI and obtain the top 50 packages with the most stars in each topic. We then select repositories following four criteria: open-source repositories, created after 202406, non-fork and non-malicious, and more than 50 stars. Finally, we crawl these repositories from GitHub and obtain 116 real-world repositories. Stage ❷: Code Parse. We use tree-sitter2 to design code parser. This parser identifies code units defined in repositories, traces the definition of code units, and analyzes unit relations: First, it performs static analysis of each file in repository and extracts unit names defined in it. Then, it executes 2https://tree-sitter.github.io/tree-sitter/ unit symbol navigation, finding the definitions of all code units in the repository. Finally, it extracts unit names invoked in code unit to grasp their dependency relations. Combining the three steps, the parser can traverse predefined code units within repository and obtain intricate dependencies. To obtain semantic relations among units, we apply an advanced embedding model to encode code units, and use the cosine similarities of their representations to measure semantic relations. Stage ❸: Requirement Collection. We extract requirements contained in the signature of code units with tree-sitter and invite two developers to check requirements. Not that it is enough to construct examples for T2-type tasks even if some code units do not contain requirements in repositories. Stage ❹: Documentation Annotation. We collect documentation from collected code repositories and select the documentation with standard: being easy to distinguish the related information of code units. Then we invite two developers to manually label 500 examples. Stage ❺: Deduplication. For tasks whose input is code unit, we exclude trivial units (e.g., empty or initialization functions). To ensure the quality of LONGCODEU, we randomly select files from repositories as long codes, and ensure long codes of all examples in each task are non-repetitive. Stage ❻: Benchmark Construction. Based on the above stages, we construct around 500 examples for each task supporting the maximum length of 128K tokens. We make LONGCODEU satisfy following goals: comprehensive tasks from practical applications, extra-long code context, real-world repositories, and reducing data contamination."
        },
        {
            "title": "3.3 Automatic Evaluation",
            "content": "We focus on evaluating the long code understanding ability required for LCLMs to complete downstream tasks. Tasks in LONGCODEU commonly require LCLMs to retrieve dispersive snippets from long code and execute reasoning. Thus, we mainly adopt metrics used in retrieval tasks to measure LCLMs, including recall and precision. For proper evaluation, we refine existing metrics according to the output granularity of tasks: ❶ Output with code lines refers to the code unit data flow analysis task, which extracts several lines from long code. We first use exact match (EM) to measure each line in outputs. Then, we calculate recall and precision based on EM, acquiring EM-R and EM-P metrics. ❷ Output with code unit names is related to the code unit perception task. We first find the correct names in output and then calculate the longest common subsequence (LCS) of unit names between output and ground truth, since it not only reflects whether the unit name is correct but also indicates LCLMs ability to perceive the position of units in long code. Then, we calculate recall and precision based on LCS, named LCSR and LCS-P. ❸ Output with code units refers to code unit semantic analysis and inter-code unit relation understanding tasks. Code unit semantic analysis only returns one unit. We use CodeBLEU as its metric, where it is popular metric to indicate the consistency of two code sequences. For intercode unit relation understanding, it returns multiple code units. We first determine whether the name of each generated unit is in ground truth. Then, we calculate CodeBLEU of each unit if it exists in the ground truth. For the code unit whose name is not in the ground truth, its CodeBLEU value is set to 0. Based on CodeBLEU value of each unit, we finally calculate CodeBLEU-based recall and precision, dubbed CB-R and CB-P. ❹ Output with descriptions is related to the long documentation understanding task. Considering that documentation contains many text descriptions, we employ BLEU for evaluation, which is commonly used to measure the consistency between two sequences in the natural language process field. These automatic evaluations can effectively measure LCLMs long code understanding ability since the outputs of each task are deterministic and task features are incorporated into these metrics. Figure 5 shows the automatic evaluation correlates well with human annotation, which further demonstrates the reliability of our automatic metrics."
        },
        {
            "title": "4 Experiment Setups",
            "content": "In this section, we aim to answer the following research questions through series of experiments. RQ1. How is the long code understanding ability of LCLMs? We evaluate LCLMs long code understanding ability on LONGCODEU in 5.1. RQ2. What is performance of LCLMs across long code lengths? We explore the performance of LCLMs on long code with different lengths. 5.2 demonstrates LCLMs performance comparison across long code lengths. RQ3. How do developers select models in realworld application scenarios? Based on the experimental results, we summarize the empirical lessons we learned, aiming to help developers select suitable LCLMs (5.3)."
        },
        {
            "title": "4.1 Base LCLMs",
            "content": "We evaluate 9 advanced LCLMs on LONGCODEU, which contain 6 general models (i.e., GPT-4o (GPT, 2024), Claude-3.5-Sonnet (cla, 2024), Gemini-1.5Flash (Team et al., 2024), DeepSeek-V2.5 (Bai et al., 2023), Mistral-v0.3 (Jiang et al., 2023), and Phi-3.5 (Abdin et al., 2024) ) and 3 code models (i.e., DeepSeek-Coder-V2 (Bai et al., 2023), Qwen2.5-Coder (Hui et al., 2024), CodeLlama (Roziere et al., 2023)). DeepSeek-R1 and GPTo3-mini are newly released LCLMs, but their availability through invoking API is unstable or limited to usage frequency. We can not present their performance now and will evaluate them in the future."
        },
        {
            "title": "4.2 Experimental Setup",
            "content": "We use greedy search for all experiments. We evaluate LCLMs within their maximum context window length. In the semantic relation extraction task, we use an advanced embedding model stella_en_400M_v5 to encode code units and natural language requirements with the 1,024 dimension version, and select the top five similar code units by calculating the cosine similarities of their embeddings. Constrained by computing resources, we evaluate DeepSeek-V2.5 by invoking API3 provided by DeepSeek. Although the context length of DeepSeek-V2.5 achieves 128K tokens, the API provided by DeepSeek only supports up to 60K tokens. Thus, we analyze DeepSeek-V2.5 on long codes with 064K tokens."
        },
        {
            "title": "5.1 Long Code Understanding Capability",
            "content": "Table 3 presents the performance of LCLMs on LONGCODEU. For LCLMs with context length of less than 128K tokens, we only evaluate them within their supporting maximum lengths. Comparison across LCLMs. We observe significant performance gaps among LCLMs. For similar-scale models, the performance of code LCLMs is better than the counterpart of general models on most tasks. For example, Qwen2.5Coder outperforms Phi-3.5 24.31% on intracode unit understanding (the second-aspect task) in terms of CodeBLEU, and DeepSeek-CoderV2 achieves 11.75% average improvements on 3https://api-docs.deepseek.com/api/deepseek-api Table 3: The performance of LCLMs on LONGCODEU. We only report recall-based results (EM-R, LCS-R, and CB-R) due to page limitation. The precision-based results (EM-P, LCS-P, and CB-P) can be found in Appendix A. #Param Context Size Task CU_P CU_SA CU_DFA DRA_T1 DRA_T2 SRE_T1 SRE_T2 LDU #Avg Code Models Qwen2.5-Coder DeepSeek-Coder-V2 CodeLlama General Models Phi-3.5 Mistral-v0.3 DeepSeek-V2.5 Claude-3.5-Sonnet Gemini-1.5-Flash GPT-4o 7.6B 15.7B 33.7B 3.8B 7.3B 236B 128K 128K 16K 128K 32K 128K 200K 1000K 128K 43.47 38.67 68.57 39.92 57.42 70.58 43.82 58.45 56. 71.06 65.21 62.41 46.75 63.90 82.11 40.60 83.46 86.76 Open-Source LCLMs 30.38 47.26 68.82 74.01 48.42 79.87 Open-Source LCLMs 30.76 46.66 72. 49.52 58.00 77.47 9.59 22.92 34.94 9.66 18.92 56."
        },
        {
            "title": "Proprietary Source LCLMs",
            "content": "45.65 80.37 87.87 29.37 72.51 71.58 28.70 46.42 48.88 24.34 24.61 44.48 18.99 33.91 49.08 26.55 39.84 44. 21.81 26.21 36.34 14.48 32.50 47.42 27.77 38.69 43.14 21.83 50.69 46.92 37.06 40.49 55.29 34.14 58.64 85. 30.53 46.24 67.70 41.81 81.43 87.54 35.53 61.39 65.83 Figure 3: Performance comparison across tasks and long code lengths on LONGCODEU (grey blocks indicate unavailable configurations). The rate of performance degradation exhibits task-specific and model-specific patterns. inter-code unit relation understanding (the thirdaspect task) in CB-R. Among open-source models, DeepSeek-V2.5 performs the best, which is related to its large number of parameters. In proprietary source models, GPT-4o achieves the best performance, while Claude-3.5-Sonnet is not satisfactory. Comparison across Tasks. In the four aspects, LCLMs perform the best in code unit perception and long documentation understanding, and achieve moderate results in intra-unit code understanding. Inter-code unit relation understanding is the most challenging. Their performances are reasonable. Because its fundamental ability for LCLMs to understand code documentation and perceive code units. Based on this, LCLMs understand intra-unit code and then analyze inter-code unit relations, thereby comprehending long code. When observing the third-aspect task (i.e., inter-code unit relation understanding) closely, it can be seen that the performance of dependency relation analysis is lower than that of semantic relation extraction. We also find that no LCLM outperforms others on all tasks. For instance, in terms of scale-similar code models and general models, the former outperforms the latter in code unit perception, while code models perform worse than general models in long documentation understanding."
        },
        {
            "title": "5.2 Performance across Long Code Lengths",
            "content": "To address RQ2, we classify examples into five buckets according to the long code length, including 08K, 816K, 1632K, 3264K, and 64128K on each task. We conduct experiments on these classes to investigate the true context ability supported by LCLMs in long code understanding. Figure 3 and Appendix shows the results of LCLMs across long code lengths. Our experimental results reveal key limitations in current LCLMs capabilities for long code understanding. We can also observe negative correlation between the long code length and the performance of LCLMs. LCLMs performance drops dramatically when the long code length is greater than 32K tokens, falling well short of their claimed 128K1M context windows. Besides, when the long code contains 64128K tokens, the performance of LCLMs is near to 10 or even close to 0 on some tasks such as dependency relation analysis and semantic relation extraction. In addition, the degradation slopes of performances vary by task. For example, there is large slope in long documentation understanding, which means that LCLMs are suitable for processing relatively short documentation. The slope on code unit understanding including code unit data flow analysis and code unit semantic analysis tasks is relatively small. LCLMs fail to model code context effectively in their claimed context windows."
        },
        {
            "title": "5.3 Empirical Lessons.",
            "content": "Based on the above experiments, we summarize the empirical lessons we learned as: ❶ The smallsize LCLMs such as Qwen2.5-Coder can satisfy developers need for code unit perception and understanding if long code length is less than 16K. Otherwise, we suggest choosing larger-size models. ❷ For understanding long code documentation with more than 32K tokens, it is recommended to use GPT-4o and Gemini-1.5-Flash. Otherwise, Mistral-v0.3 and DeepSeek-Coder-V2 can achieve satisfying performances. ❸ For tasks with high requirements for understanding inter-code unit relations, we suggest selecting the most powerful LCLMs that developers can access. Because when long code length exceeds 64K which is common in repository-level tasks, strong LCLMs also achieve weak perfrmances. These poor performances explain why powerful LCLMs perform unsatisfactorily in repository-level downstream tasks. For example, GPT-4o only achieves 4.00% Success@1 on repository-level code translation benchmarkRepoTransBench (Wang et al., 2024)."
        },
        {
            "title": "6.1 Case Study",
            "content": "We analyze the outputs of LCLMs, particularly GPT-4o, on the inter-code unit relation understanding task. Appendix presents two notable examples. GPT-4o often extracts code units that are structurally similar or share overlapping tokens but have distinct or even opposite functionalities. This highlights the need to enhance models ability to distinguish confusing code units."
        },
        {
            "title": "6.2 Code Understanding or Memorization?",
            "content": "We collect small number of early-released code repositories from GitHub that LCLMs have potentially encountered during LCLMs training, aiming to analyze the dependency degree of LCLMs on memorization and long code understanding ability. Concretely, we enter only an instruction and anchor input to models, withholding the long code context, and assess their performance. We select tasks where models can work normally even without long code context, such as code unit semantic analysis and dependency relation analysis (T2). As shown in Figure 4, the memorization performance (w/o context) is much lower than the results with long code context, even though models might have met these contexts when training. The δ score (the results with context minus the performance without context in yellow) relieves the memory phenomenon (Yu et al., 2023) and also reveals the significant importance for measuring LCLMs long code understanding ability."
        },
        {
            "title": "6.3 Reliable Evaluation Metrics?",
            "content": "Reliable evaluation metrics are essential for assessing long code understanding. We measure the consistency between our metrics and human evaluation by selecting 20 GPT-4o outputs on each task and inviting two advanced developers to manually evaluate them. Using Kendall-Tau τ (Kendall, 1938), we found that the average τ value is at least 0.75 across all tasks, with the minimum value exceeding 0.7 (Figure 5). This demonstrates strong correlation between our metrics and human evaluation, confirming the reliability of our metrics."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we propose comprehensive long code understanding benchmark LONGCODEU. It introduces four aspects (8 tasks) to evaluate LCLMs long code understanding ability required for practical applications, including code unit perception, intra-code unit understanding, inter-code unit relation understanding, and long code documentation understanding. Our experimental results reveal key limitations in current LCLMs capabilities for long code understanding. When the long code length contains more than 32K tokens, the performance of LCLMs drops dramatically, falling far short of their claimed 128K1M context windows. We hope our findings can provide valuable insights for optimizing LCLMs and driving advancements in software engineering."
        },
        {
            "title": "Limitations",
            "content": "This paper proposes benchmark - LONGCODEU to evaluate the long code understanding ability of long context language models from four aspects which are essential capabilities required for LCLMs to complete real-world downstream tasks. Based on LONGCODEU, we evaluate 9 popular LCLMs and analyze their strengths and shortcomings. We think that DevEval has three limitations. ❶ LONGCODEU is monolingual benchmark (i.e., requirements in English and code in Python) and ignores other languages. In practice, LLMs require understanding requirements in different natural languages (e.g., Chinese, Spanish) and generating programs in various programming languages (e.g., Java, C). Thus, we plan to build multilingual LONGCODEU in future work. ❷ Most recently, there are few newly released LCLMs such as DeepSeek-R1 and OpenAI o3mini-high. Constrained by the availability or stabilization of API, we do not provide the performance of these newly released models. In the future, we will evaluate their long code understanding abilities on LONGCODEU. ❸ In our experiments, we only consider the long code with 0128K tokens, although some LCLMs have supported longer context windows, e.g., Claude-3-5-Sonnet with 200K context size and even Gemini-1.5-Flash with 1000K context size. We will continue to update and evolve our benchmark, in order to support LONGCODEU to measure LCLMs on longer codes."
        },
        {
            "title": "References",
            "content": "2024."
        },
        {
            "title": "Claude",
            "content": "3.5 haiku. https://www.anthropic.com/claude/haiku. 2024. Gpt-4o. https://openai.com/index/hello-gpt-4o/. 2024. Tiobe-index. https://www.tiobe.com/tiobeindex/. Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. 2023. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508. Zhangqian Bi, Yao Wan, Zheng Wang, Hongyu Zhang, Batu Guan, Fangxin Lu, Zili Zhang, Yulei Sui, Hai Jin, and Xuanhua Shi. 2024. Iterative refinement of project-level code context for precise code genarXiv preprint eration with compiler feedback. arXiv:2403.16792. Egor Bogomolov, Aleksandra Eliseeva, Timur Galimzyanov, Evgeniy Glukhov, Anton Shapkin, Maria Tigina, Yaroslav Golubev, Alexander Kovrigin, Arie van Deursen, Maliheh Izadi, et al. 2024. Long code arena: set of benchmarks for long-context code models. arXiv preprint arXiv:2406.11612. Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Lidong Bing. 2023a. Clex: Continuous length extrapolation for large language models. arXiv preprint arXiv:2310.16450. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023b. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595. Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang, Yongbin Li, and Rui Yan. 2023c. Fortify the shortest stave in attention: Enhancing context awareness of large language models for effective tool use. arXiv preprint arXiv:2312.04455. Nilesh Dhulshette, Sapan Shah, and Vinay Kulkarni. 2025. Hierarchical repository-level code summarization for business applications using local llms. arXiv preprint arXiv:2501.07857. Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. 2023. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486. Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. 2023. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770. Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu. 2024. Llm maybe longlm: Self-extend Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. 2023. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039. Xi Ye, Fangcong Yin, Yinghui He, Joie Zhang, Howard Yen, Tianyu Gao, Greg Durrett, and Danqi Chen. 2025. Longproc: Benchmarking long-context language models on long procedural generation. arXiv preprint arXiv:2501.05414. Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, et al. 2023. Kola: Carefully benchmarking world knowledge of large language models. arXiv preprint arXiv:2306.09296. Kechi Zhang, Ge Li, Huangzhao Zhang, and Zhi Jin. 2024a. Hirope: Length extrapolation for code models using hierarchical position. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1361513627. Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. 2024b. Codeagent: Enhancing code generation with tool-integrated agent systems for realworld repo-level coding challenges. arXiv preprint arXiv:2401.07339. llm context window without tuning. arXiv preprint arXiv:2401.01325. Maurice Kendall. 1938. new measure of rank correlation. Biometrika, 30(1-2):8193. Jia Li, Ge Li, Xuanming Zhang, Yunfei Zhao, Yihong Dong, Zhi Jin, Binhua Li, Fei Huang, and Yongbin Li. 2024a. Evocodebench: An evolving code generation benchmark with domain-specific evaluations. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Jia Li, Ge Li, Yunfei Zhao, Yongmin Li, Huanyu Liu, Hao Zhu, Lecheng Wang, Kaibo Liu, Zheng Fang, Lanshen Wang, Jiazheng Ding, Xuanming Zhang, Yuqi Zhu, Yihong Dong, Zhi Jin, Binhua Li, Fei Huang, Yongbin Li, Bin Gu, and Mengfei Yang. 2024b. Deveval: manually-annotated code generation benchmark aligned with real-world code repositories. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 3603 3614. Association for Computational Linguistics. Jiawei Liu, Jia Le Tian, Vijay Daita, Yuxiang Wei, Yifeng Ding, Yuhan Katherine Wang, Jun Yang, and Lingming Zhang. 2024. Repoqa: Evaluating long context code understanding. arXiv preprint arXiv:2406.06025. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Yanlin Wang, Yanlin Wang, Suiquan Wang, Daya Guo, Jiachi Chen, John C. Grundy, Xilin Liu, Yuchi Ma, Mingzhi Mao, Hongyu Zhang, and Zibin Zheng. 2024. Repotransbench: real-world benchmark for repository-level code translation. CoRR, abs/2412.17744. Jeff Wu, Long Ouyang, Daniel Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. 2021. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453. ing these instructions. These instructions standardize the output format of the models (even though some models may not output strictly in accordance with these specifications), facilitating the parsing of the streaming output of the models using regular expressions during the evaluation process. Figure 4: Assessing long code understanding vs. memorization on CU_SA (left) and DRA_T2 (right) tasks. Figure 5: The value of Kendall-Tau τ between our automatic metrics and human evaluation. Rrecision-based results in RQ We present the performance of LCLMs on several tasks which can be measured by precision-based metrics in Figure 6."
        },
        {
            "title": "B Case Study",
            "content": "Figure 7 shows representative error case in the dependency relation analysis task. We can find that the output of GPT-4o extracts an error code unit stream_async\" that is confusing to correct invoked function stream\" since the two code snippets have similar structures. Figure 8 demonstrates generated result of GPT4o in the semantic relation extraction task. We can observe that the output contains an error delete\" function which has opposite functionalities to the anchor input, i.e., the given natural language description."
        },
        {
            "title": "C Instructions in Our Benchmark",
            "content": "We present the instructions employed in diverse tasks. Each instruction has undergone iterative refinement to ensure that different models can not only achieve relatively high metrics but also produce outputs that appear satisfactory and are applicable to real-world development scenarios when usFigure 6: Performance comparison across long code lengths on tasks which can be measured by precision-based metrics. (grey blocks indicate unavailable configurations). The rate of performance degradation exhibits task-specific and model-specific patterns. Figure 7: For the dependency relation analysis task, the output of GPT-4o extracts error code unit stream_async\" that is confusing to correct invoked function stream\". Figure 8: For the semantic relation extraction task, the output contains an error delete\" function which has opposite functionalities to the anchor input, i.e., the given natural language description. (a) Code Unit Perception Youre an expert Python programmer. Identify all defined Python functions and extract their function names from the long Python source codes. Only extract the Python function names that are defined in the long Python source codes. Do not extract the Python function names that are mentioned but not defined in the long Python source codes. Do not extract the Python function names that are defined in the classes of the long Python source codes. All extracted Python function names are stored in list Function_names according to the order these function names appear in the long Python source codes. Make sure the output is strictly in this format: Function_names = [Function_name_1, Function_name_2, ..., Function_name_k] Do not include any explanations or additional text. (b) Code Unit Data Flow Analysis You are an expert Python programmer. Identify the {Target_function_name} function from the long Python source code and extract the lines where the value of the {Target_variable_name} variable changes within the {Target_function_name} function. Only extract lines where the value of the {Target_variable_name} variable changes. Do not extract lines where the {Target_variable_name} variable is referenced but its value does not change. Return the extracted lines according to the order they appear in the {Target_function_name} function. Make sure the output is in the following format: ###### The extracted code line ###### {extracted_code_line} ###### Line number ###### {line_number} ###### The extracted code line ###### {extracted_code_line} ###### Line number ###### {line_number} ... ###### The extracted code line ###### {extracted_code_line} ###### Line number ###### {line_number} Do not include any explanations or additional text in the output. (c) Code Unit Semantic Analysis Youre an expert Python programmer. Understand the long Python source codes. Identify and extract the function that satisfies the given programming requirement. Extract the entire source code of the function that satisfies the given programming requirement. Do not only extract the name of the function that satisfies the given programming requirement. Extracted the entire source code of the function: def function_name(parameters): # function body Do not include any explanations or additional text. (d) Dependency Relation Analysis (T1) Youre an expert Python programmer. Extract all functions and methods that the {Target_function_name} function calls from the long Python source codes. Only extract functions and methods from the given long Python context. Do not extract functions and methods from the Python standard library or third-party libraries. Only extract functions and methods that the {Target_function_name} function directly calls. Do not extract functions and methods that the {Target_function_name} function indirectly calls. Extract the entire source code of functions and methods that the {Target_function_name} calls. Do not only extract the name of functions and methods that the {Target_function_name} function calls. Ensure indentation is preserved. Please follow the format to complete the skeleton below: ###### function_1 ###### def function_name_1(parameters): # function body ###### function_2 ###### def function_name_2(parameters): # function body ###### method_1 ###### def method_name_1(self, parameters): # method body ###### method_2 ###### def method_name_2(self, parameters): # method body Do not include any explanations or additional text. (e) Dependency Relation Analysis (T2) Youre an expert Python programmer. Understand the Python programming requirement of the {Target_function_name} function. Extract all functions and methods that the {Target_function_name} function calls from the long Python source codes. Only extract functions and methods from the given long Python context. Do not extract functions and methods from the Python standard library or third-party libraries. Only extract functions and methods that the {Target_function_name} function directly calls. Do not extract functions and methods that the {Target_function_name} function indirectly calls. Ensure indentation is preserved. Do not include any explanations or additional text. (f) Semantic Relation Extraction (T1) Youre an expert Python programmer. Identify and extract the functions or classes that are most semantically similar to the {Target_function_name} function from the long Python source codes. Extract the entire source code of functions and classes that are the top-5 semantically similar to the {Target_function_name} function. Do not only extract the name of functions and classes that are the top-5 semantically similar to the {Target_function_name} function. The order of extracted five functions or classes is in order of decreasing similarity to the {Target_function_name} function. Ensure indentation is preserved. **Do not extract the target function {Target_function_name} itself.** Please follow the format to complete the skeleton below: ###### Top-1 ###### def name_1(parameters): # function body ###### Top-2 ###### def name_2(parameters): # body ###### Top-3 ###### def name_3(parameters): # body ###### Top-4 ###### def name_4(parameters): # body ###### Top-5 ###### def name_5(parameters): # body Do not include any explanations or additional text. (g) Semantic Relation Extraction (T2) Youre an expert Python programmer. Understand the Python programming requirement of the {Target_function_name} function. Identify and extract the functions or classes that are most semantically similar to the {Target_function_name} function from the long Python source codes. Extract the entire source code of functions and classes that are the top-5 semantically similar to the {Target_function_name} function. Do not only extract the name of functions and classes that are the top-5 semantically similar to the {Target_function_name} function. The order of extracted five functions or classes is in order of decreasing similarity to the {Target_function_name} function. Ensure indentation is preserved. **Do not extract the target function {Target_function_name} itself.** Please follow the format to complete the skeleton below: ###### Top-1 ###### def name_1(parameters): # function body ###### Top-2 ###### def name_2(parameters): # body ###### Top-3 ###### def name_3(parameters): # body ###### Top-4 ###### def name_4(parameters): # body ###### Top-5 ###### def name_5(parameters): # body Do not include any explanations or additional text. (h) Long Documentation Understanding You are an expert Python programmer. Understand the multiple natural language documentations and identify the one that describes the {Target_API_name} API. Each documentation is labeled with sequence number and enclosed between special markers: <BEGIN >indicates the start of documentation, and <END >indicates its conclusion. Extract and return only the complete documentation corresponding to the {Target_API_name} API, exactly as it appears. Please follow the format to complete the skeleton below: # The sequence number {sequence number} # The complete documentation {complete documentation} Do not include any explanations or additional text."
        }
    ],
    "affiliations": [
        "Key Lab of High Confidence Software Technology (Peking University), MoE, School of Computer Science, Peking University, China"
    ]
}