{
    "paper_title": "IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering",
    "authors": [
        "Parker Liu",
        "Chenxin Li",
        "Zhengxin Li",
        "Yipeng Wu",
        "Wuyang Li",
        "Zhiqin Yang",
        "Zhenyuan Zhang",
        "Yunlong Lin",
        "Sirui Han",
        "Brandon Y. Feng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) excel at descriptive tasks, but whether they truly understand scenes from visual observations remains uncertain. We introduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding through active creation rather than passive recognition. Grounded in the analysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs) with actively using programming and rendering tools to recreate the underlying 3D structure of an input image, achieving agentic inverse rendering through tool use. This \"understanding-by-creating\" approach probes the tool-using generative capacity of VLAs, moving beyond the descriptive or conversational capacity measured by traditional scene understanding benchmarks. We provide a comprehensive suite of metrics to evaluate geometric accuracy, spatial relations, appearance attributes, and overall plausibility. Initial experiments on agentic inverse rendering powered by various state-of-the-art VLMs highlight current limitations, particularly in visual precision rather than basic tool usage. IR3D-Bench, including data and evaluation protocols, is released to facilitate systematic study and development of tool-using VLAs towards genuine scene understanding by creating."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 9 2 3 3 2 . 6 0 5 2 : r IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering Parker Liu1,, Chenxin Li1,, Zhengxin Li2, Yipeng Wu2, Wuyang Li3, Zhiqin Yang1, Zhenyuan Zhang4, Yunlong Lin5, Sirui Han4,, Brandon Y. Feng6, 1CUHK, 2TJU, 3EPFL, 4HKUST, 5XMU, 6MIT https://ir3d-bench.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) excel at descriptive tasks, but whether they truly understand scenes from visual observations remains uncertain. We introduce IR3DBench, benchmark challenging VLMs to demonstrate understanding through active creation rather than passive recognition. Grounded in the analysis-bysynthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs) with actively using programming and rendering tools to recreate the underlying 3D structure of an input image, achieving agentic inverse rendering through tool use. This understanding-by-creating approach probes the tool-using generative capacity of VLAs, moving beyond the descriptive or conversational capacity measured by traditional scene understanding benchmarks. We provide comprehensive suite of metrics to evaluate geometric accuracy, spatial relations, appearance attributes, and overall plausibility. Initial experiments on agentic inverse rendering powered by various state-of-the-art VLMs highlight current limitations, particularly in visual precision rather than basic tool usage. IR3D-Bench, including data and evaluation protocols, is released to facilitate systematic study and development of tool-using VLAs towards genuine scene understanding by creating."
        },
        {
            "title": "Introduction",
            "content": "What cannot create, do not understand. Richard Feynman Vision-language models (VLMs) have made striking progress on tasks that resemble surface-level scene comprehension: answering questions, describing layout, grounding text in pixels [1, 2, 3, 4, 5, 6]. However, understanding, in deeper sense, remains questionable. When VLM generates text tokens that convey the meaning of three red cylinders in scene, does its internal world model actually know what this means? Can it prove true understanding by reconstructing the world that image came from? This paper proposes different test of visual intelligence, one grounded not in passive recognition but in active creation through tool use. Inspired by the analysis-by-synthesis paradigm in human perception [7, 8, 9, 10], we frame this challenge as agentic inverse rendering: vision-language agent (VLA) an agent powered by VLM reconstructing the 3D scene behind single image by writing an explicit, executable program that recreates the scene from scratch. Agentic inverse rendering embodies the cyclical process of analysis-by-synthesis: analyzing the input, synthesizing hypothesis, comparing it to the original, and refining based on the comparison. Similar to recent progress in tool-augmented VLMs [11, 12, 13, 14], our programmatic approach recasts the traditional goal of inverse rendering as multimodal programming task where the VLA must Equal Contribution, Corresponding Author. Preprint. Under review. Figure 1: Understanding by Creating. (A) Humans demonstrate understanding by constructing internal world models with mental representations of object layouts, spatial relations, and physical attributes, which they can use to recreate observed scenes. In contrast, visionlanguage agents (VLAs) are typically evaluated on recognition tasks like captioning or VQA, which only tap into surface-level visual comprehension. (B) IR3D-Bench shifts the focus to generative reconstruction with inverse rendering via tool use. While VLAs show sparks of scene understanding, their ability to build coherent, executable 3D programs reveals how incomplete their internal world models remain. actively use tools to demonstrate understanding: to recreate given image, the agent must compose valid Python script, communicated to Blender [15] via Model Context Protocol (MCP) [16, 17], that expresses the underlying 3D structure when executed by Blender. This tool-using programmatic representation lifts the VLAs internal world model beyond vectorized features to an explicit 3D physical space. However, despite the early promise shown by VLA using tools like BlenderMCP [17] to generate 3D content based on 2D image prompts, the generated outputs remain unreliable. More importantly, the limitations in scene understanding of current VLM are not yet well understood. To systematically evaluate VLM scene understanding through the lens of understanding-by-creating, we introduce the IR3D-Bench benchmark, where we prompt the VLA with an image and task it to perform agentic inverse rendering by producing Blender script that, when executed, reconstructs the original scene. This explicitly tests the agents ability to use programming tools to externalize its understanding. We evaluate VLA performance using suite of metrics that assess geometry, spatial relations, appearance attributes, and overall plausibility. Unlike existing multimodal benchmarks that primarily focus on descriptive or conversational tasks such as 3D captioning, 3D Visual Question Answering (VQA), or 3D visual grounding, IR3D-Bench directly tests the agentic generative capacity: the ability to use tools to synthesize the latent 3D world state from 2D view. Our experiments suggest that while scripting errors occur, the dominant bottleneck is not tool usage itself, but lack of visual precision in the agents perception. When agents cannot reliably distinguish fine-grained differences between their rendered output and the target image, they quickly plateau in self-correction, even with iterative prompting. This indicates that future progress may hinge less on instruction tuning or syntax scaffolding and more on enhancing the fidelity of visual representations within multimodal models. We release IR3D-Bench to facilitate the systematic study of VLM scene understanding and the development of agents that can observe, reason about, and truly understand scenes by demonstrating the ability to recreate them in an actionable, structured format. Just as human children show early signs of understanding by attempting to recreate what they see [18, 19, 20], our benchmark embraces Feynmans insight that creation is the test of understanding and challenges AI systems to prove their comprehension by generating, through tool use, the world they perceive [21, 22]."
        },
        {
            "title": "2 Agentic Inverse Rendering with VLMs",
            "content": "This section formally defines our task of agentic inverse rendering. We discuss connections to the broader field of inverse rendering, while highlighting the fundamental shift enabled by VLAs. Traditional inverse rendering seeks to infer the underlying scene properties (e.g., geometry, appearance, illumination) that best explain the observation. This is often framed as an optimization problem 2 Figure 2: Overview of the IR3D-Bench Pipeline. The benchmark consists of two stages: Stage 1: Inverse Rendering. Given raw image and camera parameters, the agent is prompted to infer structured scene representation in JSON format. The predicted objects are rendered in Blender and matched to GT annotations using geometric alignment and per-object mask comparisons. Stage 2: Benchmark Evaluation. Reconstruction quality is evaluated along three axes: Localization (object count, spatial alignment, and relation consistency), Visual Appearance (shape and material accuracy via maskand attribute-level scores), and Language-Aligned Semantics (layout fidelity and object plausibility assessed via GPT-4o). Together, these metrics provide comprehensive view of the VLAs internal world model and generative precision. over differentiable graphics pipeline: Given rendering model and camera parameters, the goal is to find 3D scene parameters that minimize the difference between the rendered image and the input. Our task differs substantially from conventional approaches to 3D inverse rendering through focus on agentic tool use. Rather than performing low-level predictions like geometric primitives, depth maps, or volumetric densities, the VLA actively uses programming tools to synthesize an explicit, executable program that regenerates the 3D scene depicted in single input image I. This program P, typically Python script for Blender, acts as structured, symbolic hypothesis about the scene composition. When executed within deterministic rendering environment R, the script should produce an image R(P) that matches the original as closely as possible under set of predefined metrics. This directly embodies the analysis-by-synthesis cycle: the VLA analyzes and synthesizes as its understanding of the scenes generative process. This shift from continuous parameter regression (classic inverse rendering) to structured program synthesis introduces distinct challenges. The VLA must use programming tools effectively, producing outputs that are not only semantically meaningful but also syntactically precise. syntactically invalid script is unexecutable; subtle errors in coordinate systems can lead to geometrically plausible but incorrect reconstructions; and inconsistencies in naming conventions can cause API errors. These strict requirements of tool use limit the tolerance for hallucinations, imprecision, or vagueness often seen in VLM outputs for free-form tasks like captioning or dialogue, and demand higher level of precision in the agents understanding of both the visual scene and the programming tools. To manage the complexity of unconstrained 3D scenes and to enable rigorous evaluation, our study constrains the task domain using scenes from the CLEVR dataset [23]. CLEVR provides ground-truth scene graphs, 6D object poses, material properties, and other metadata for scenes composed of simple 3 primitive objects under controlled lighting. This controlled environment allows us to focus evaluation on the agents ability to use programming tools effectively rather than on scene complexity itself. The interface with the 3D environment (e.g., Blender MCP) is crucial for agentic inverse rendering. It acts as the bridge, interpreting the VLAs generated text as executable code. This programmatic formulation allows us to assess model performance at multiple levels of abstraction: Does it correctly place objects in 3D space (geometric accuracy)? Does it preserve spatial relationships between objects (relational reasoning)? Does it match visual features like color, material, and shape (appearance fidelity)? Is the overall reconstructed scene plausible and similar to the original (holistic quality)? These evaluations probe not only the VLAs ability to act as an agent using Blender tools but its capacity to internalize and reconstruct latent 3D world state from 2D image. IR3D-Bench thus positions agentic inverse rendering not merely as reconstruction task, but as foundational step towards agents that possess practical, generative understanding of the 3D world, enabling them to not just perceive it, but also to reconstruct and manipulate it."
        },
        {
            "title": "3 Related Work",
            "content": "Our work intersects with several established and emerging research areas: classic inverse rendering, broader efforts in 3D scene understanding using VLMs, and programmatic scene modeling. Classic Inverse Rendering Traditional inverse rendering has long sought to recover the underlying physical and geometric properties of scene, such as shape, material, and illumination, from one or more 2D images [24]. This is typically formulated as an optimization problem where the parameters of (often differentiable) graphics pipeline are adjusted to minimize the discrepancy between rendered image and the observed input [25, 26, 27]. This paradigm has led to substantial progress in physically grounded scene reconstruction, with notable advances in volumetric or implicit neural representations [28, 29, 30], point-based rendering [31, 32] and differentiable path tracing [33]. These methods can achieve high-fidelity reconstructions and are often grounded in physical priors. Our approach, while also form of inverse rendering, shifts the target representation from continuous weights or geometric primitives to discrete, executable programs, prioritizing interpretability and editability, and explicit demonstration of understanding through creation. VLMs for 3D Scene Understanding The capabilities of Vision-Language Models (VLMs) have recently been extended to the 3D domain [34, 35]., leading to benchmarks and methods for tasks such as 3D Visual Question Answering [36, 37], 3D visual grounding [38, 39], 3D captioning [40, 41], and embodied AI navigation or interaction based on language instructions [42, 43]. These works primarily focus on the VLMs ability to interpret or describe existing 3D information, whether presented as point clouds, meshes, or within simulated environments [44, 45, 46]. Some recent efforts also explore 3D-aware LLMs or multimodal instruction tuning for broader 3D reasoning [47, 48, 49, 50, 51, 52]. While these approaches advance 3D scene comprehension, they generally do not position the model as an agent using programming tools to generate the underlying 3D scene structure from single 2D image in an explicit, programmatic form. Our work uniquely focuses on this agentic, tool-using generative synthesis capability as test of deeper understanding demonstrated through creation. Programmatic Scene Generation and Analysis-by-Synthesis Representing scenes as programs or through generative grammars has rich history in computer graphics and vision [53, 54, 55, 56]. More recently, with the advent of powerful tool-augmented language models, there is renewed interest in having models generate code that produces complex outputs [11], including visual content [57, 58, 59, 60, 61, 62, 63]. This echoes with the analysis-by-synthesis paradigm [8], where perception involves generating internal hypotheses (programs) to explain visual sensory input. Some works generate 2D images or simple 3D assets programmatically from text or sketches [58, 64, 65]. Others focus on programs that control simulation environments [66] or robotic actions [67]. However, the specific task of VLA reconstructing detailed 3D scene from single RGB image by using programming and rendering tools, and systematically benchmarking this agentic understanding-by-creating ability, remains less explored, and IR3D-Bench directly addresses this gap. 4 IR3D-Bench Suite In this section, we introduce the core components of IR3D-Bench, our proposed benchmark for evaluating VLMs scene understanding via agentic inverse rendering shown in Fig. 2. Sec. 4.1 describes how we integrate the CLEVR dataset for our benchmark. Sec. 4.2 presents the inverse rendering pipeline, which involves VLAs using tools to reconstruct 3D scene representations from visual inputs. Sec. 4.3 defines set of evaluation metrics to assess VLA performance."
        },
        {
            "title": "4.1 CLEVR Dataset Integration",
            "content": "CLEVR dataset [23] has been widely adopted in 3D vision tasks [68, 69, 70, 71, 72]. To facilitate controlled evaluation of agentic inverse rendering and 3D scene understanding, we adopt the validation split of CLEVR, which contains 15,000 synthetic images rendered from 3D scene graphs. Each image depicts structured scene composed of 3 to 10 objects, with precise annotations of object-level geometry and semantics, including 3D coordinates P3D R3 , pixel-space projections P2D R3, shapes ts, colors tc, sizes tz, materials tm, and inter-object spatial relationships defined as: for each spatial relation {right, left, front, behind}, R(r) denotes the set of objects that are in relation with object and be the number of objects, indexed by = 0, 1, . . . , 1. The images are rendered at resolution of 480 320 pixels using Blender [15]. These rich annotations make CLEVR particularly suitable for benchmarking object-centric 3D reconstruction and spatial reasoning in controlled setting. {0, 1, . . . , 1} {i}, where R(r) In IR3D-Bench, we leverage CLEVR as structured testbed for evaluating VLAs as tool-using agents. For each scene, we use the rendered RGB image from CLEVR along with well-designed textual prompt that specifies the agentic inverse rendering task, and provide both as input to the VLA. The prompt is constructed under fixed camera intrinsic K, and extrinsic across all samples, and encodes the reconstruction objective and relevant assumptions as illustrated in Fig. 7. The VLA is tasked with predicting set of object-level parameters structured according to predefined JSON schema. 4."
        },
        {
            "title": "Inverse Rendering Pipeline",
            "content": "We evaluate how effectively the agent has reconstructed the full 3D scene using Blender [15]. Here, we define the attributes from four dimensions that the agent must correctly specify: 3D position ˆP3D, shape ˆts, color ˆtc, size ˆtz, and material ˆtx. Rendering is performed under fixed camera intrinsic matrix and extrinsic parameters E, determining the rendering viewpoint. This controlled setup ensures consistent projection geometry across all evaluations. Reprojection and Alignment The coordinate systems of the generated and the ground-truth (GT) scenes are misaligned in CLEVR, potentially leading to unfair or failed evaluation. To address this issue, we adopt fixed camera model with known intrinsics and extrinsics to project the predicted 3D object centers into the 2D image plane. The projection is defined as: (cid:35) (cid:34) ˆp2D = π(K, E, P3D) = [R t] P3D 1 , where ˆP3D R3 is the predicted object center in world coordinates, and ˆp2D denotes its corresponding location in the image space. Object Matching To resolve object correspondences between agent-generated reconstructions (Fig. 2 Stage 1) and GT, we first augment each agent-generated object with textual description name (e.g., red large metal sphere) that encodes its semantic attributes: ˆtc + ˆtz + ˆtm + ˆts. We then extract structured attribute vectors from both agent-generated and GT objects, denoted as ˆti and tj respectively, where ˆti represents the attribute set (color, size, shape, material) of the i-th predicted object, and tj that of the j-th GT object. For each attribute dimension in color, size, material, and shape, we compute the semantic similarity using CLIP [73] text encoder: s(ˆti, tj) = 1 denote the k-th attribute (as text string) 4 of the i-th predicted and j-th GT object. This yields similarity matrix RN between the predicted and the GT objects. We convert it into cost matrix = 1 S, and solve the assignment using the Hungarian algorithm: ϕ = arg minϕ i=1 Si,ϕ(i), i=1 Ci,ϕ(i) = arg maxϕ , where ˆt(k) k=1 CLIP and t(k) (cid:16)ˆt(k) , t(k) (cid:80)N (cid:80)N (cid:80)4 (cid:17) i 5 Figure 3: Visual Results with Selected VLMs. Gemini-2.5-pro demonstrates strong understanding of object spatial positions and relative layouts. Grok-3 excels at modeling fine-grained details such as material and color. Qwen2.5-VL-72B struggles in more complex scenarios. where ϕ is bijective mapping from the set of predicted object indices {1, . . . , } to GT object indices {1, . . . , }. To avoid spurious matches, we apply similarity threshold τ and define the final matching function as Match(i) = ϕ(i) if Si,ϕ(i) τ, and Match(i) = 1 if otherwise. Per-Object Mask Evaluation To quantitatively assess the fidelity of each reconstructed object, we employ the Segment Anything Model (SAM) [74] to extract instance-level segmentation masks in zero-shot manner. Specifically, we project the 3D center of each predicted object onto the image plane to obtain 2D point ˆp2D, which serves as the prompt to SAM. The model then returns binary segmentation mask ˆMi corresponding to the predicted object i. Similarly, we apply the same procedure to the GT 3D object centers to extract ground-truth masks Mj via the same SAM-based pipeline, ensuring consistent and unbiased mask generation."
        },
        {
            "title": "4.3 Evaluation Metrics",
            "content": "To comprehensively evaluate the performance of VLMs on agentic inverse rendering and 3D scene understanding, we propose suite of evaluation metrics grouped into four major categories: Localization, Visual Appearance, and Language-Aligned Semantics. This multi-faceted evaluation protocol allows us to assess both geometric accuracy and semantic fidelity of the reconstructed scenes."
        },
        {
            "title": "4.3.1 Localization",
            "content": "We evaluate object-level localization from four perspectives: geometric accuracy, object count consistency, bounding box similarity and spatial relations. These metrics jointly assess how accurately the model reconstructs spatial layouts in the scene. Pixel Distance We compute the average ℓ2 distance between the 2D projected centers of generated objects ˆpi and their corresponding GT centers pj, after optimal bipartite matching. This quantifies the geometric proximity between generated and GT object positions in image space. Count Accuracy We evaluate the consistency of object enumeration by comparing the generated object count with the GT number of objects. BBox Edge Score To quantify the structural similarity between generated and GT bounding boxes, we propose center-to-edge distance-based metric. Given predicted box ˆbi = (x1, y1, x2, y2) and the GT box bj, we first compute the distances from the box center to each of its four edges. These 6 directional distances capture both the size and aspect ratio of the box. BBox Edge Score is then formulated as sbbox(ˆbi, bj) = 1 distances for the generated and GT boxes along each direction {left, right, top, bottom}, and ϵ is added to prevent division by zero. This metric yields score in the range [0, 1], where higher values indicate greater similarity in both size and alignment. represent the center-to-edge and d(j) d(j) +ϵ , where d(i) (cid:12) (cid:12)d(i) (cid:12) d(j) (cid:80) (cid:80) (cid:12) (cid:12) (cid:12) Spatial Relations We measure how well the VLA recovers pairwise object relations like left of, right of, in front of, and behind. Given the predicted 3D positions ˆP3D, we derive relational labels based on predefined geometric rules and compare them with GT {R(r) }. Relation Accuracy is reported as the proportion of correctly predicted relations across all annotated object pairs."
        },
        {
            "title": "4.3.2 Visual Appearance",
            "content": "Mask-level We evaluate the quality of generated segmentation masks ˆMi obtained in Per-Object Mask Evaluation by comparing them with GT masks Mj using Intersection-over-Union (IoU) and DICE Score. These metrics capture spatial overlap and foreground prediction accuracy, respectively. Semantic-level To evaluate the consistency of object-level semantic attributes, we convert both predicted and GT properties (color, size, material, and shape) into textual descriptions. These are embedded using the CLIP model, and cosine similarity is computed between the predicted and reference embeddings. Attribute-wise scores are reported along with an Overall Appearance Score obtained by averaging across all annotated attributes."
        },
        {
            "title": "4.3.3 Language-Aligned Semantics",
            "content": "We use GPT-4o to assess perceptual quality and semantic coherence as LLM score. Both predicted and GT scenes are JSON-serialized and presented to GPT-4o, which provides ratings from 0 to 5 in three dimensions: 1) Object Appearance, correctness of color, shape, and material; 2) Scene Layout, consistency of the spatial object arrangement with GT; 3) Overall Visual Quality, holistic realism and semantic alignment of the entire scene. The evaluation prompt used is shown in Figure 7 (A)."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we evaluate the agentic inverse rendering capabilities of VLMs as they interact with programming and rendering tools to demonstrate their understanding by creating."
        },
        {
            "title": "5.1 Benchmark Setup",
            "content": "Models We evaluate more than 20 state-of-theart VLMs, with diverse model sizes (from 2B to 70B), architecture, and training paradigms. Closed-source models include GPT-4o [2], GPT4.1 [75], GPT-4V [75], Gemini-2.0-Flash [76], Gemini-2.5-Pro [76], Claude-3.5-Sonnet [3], Claude-3.7-Sonnet [3], and Grok-3 [77]. Open source models include DeepSeek-VL2 [78], LLaVA-NeXT [79], LLaMA-3.2 [80], H2OVL [81], Phi-3.5-Vision [82], Pixtral [83], Aria [84], Idefics [85], InternVL2.5 [86], InternVL3 [87], and Qwen2.5-VL [88]. Figure 4: Holistic comparison over 14 metrics. Prompt Design We design single-turn prompt that instructs VLM to extract geometric information from given image. The VLM predicts the attributes of each object, such as shape, size, position, and material, based on predefined output format, returning structured JSON file. The intrinsic and extrinsic parameters of the camera are fixed and cannot be inferred, ensuring consistent reconstruction of the scene (see Sec. 5.2.3 for details). The full prompt is provided in the appendix. 7 Table 1: Evaluation of VLMs on IR3D-Bench. We report performance across key aspects of 3D scene reconstruction from single image. For each column, only the top three performances are highlighted from dark (highest) to light (lowest). Model Release Layout & Localization Relation Instance Seg. CLIP Score LLM Score Pix. Dist. Count Acc Bbox Rel. Acc IoU Dice Color Size Material Shape Overall Obj App. Layout Overall Gemini-2.5-pro 2025-03 0.3791 Gemini-2.0-flash 2025-02 0. Claude3.5-Sonnet 2024-10 0.5402 Claude-3.7-Sonnet 2025-02 0. GPT-4.1 GPT-4o grok-3 2025-04 0.4366 20240.5528 2024-12 0.4378 DeepSeek-VL2 2024-12 Llama-3.2-11B-Vision 2024H2OVL-Mississippi-2B 2024-10 LLaVA-NeXT Mistral3 Phi-3.5-Vision phi4_mm Pixtral-12B Aria Idefics3-8B 2025-01 0.6835 2025-01 0. 2024-07 0.6027 2025-02 0.6192 2024-11 0. 2024-11 0.5932 2024-08 0.9100 InternVL2.5-8B 20240.9511 InternVL2.5-38B 2024-11 0.5233 InternVL3-8B 20250.5549 InternVL3-38B 2025-04 0.4560 Qwen2.5-VL-7B 20250.6537 Qwen2.5-VL-72B 2025-01 0.4082 Latest Proprietary Models 0. 0.46 0.28 0.38 0.42 0.30 0. 0.11 0.08 0.09 0.09 0.08 0. 0.08 0.18 0.13 0.14 0.14 0. 0.11 0.13 96.12 97.00 99.50 96.59 97.67 99. 93.19 96.77 97.39 97.71 98.34 99.42 97.55 97.34 98. 96.70 98.36 98.66 98.04 99.15 99.87 99.75 99. 98.60 99.09 99.87 99.88 99.89 93. 94.97 91.39 96.36 94.59 94.22 97. Open-source Models Failed 0.12 0.44 0.13 0. 0.42 0.17 0.18 0.28 0.38 0. 0.42 0.30 0.39 0.03 0.06 0. 0.03 0.07 0.05 0.03 0.03 0. 0.05 0.07 0.04 0.08 0.04 0. 0.03 0.05 0.11 0.08 0.06 0. 0.11 0.08 0.13 0.06 0.13 92.11 96. 96.31 99.56 99.79 99.85 91.44 96.35 93.08 94.82 93. 96.02 99.28 99.90 99.03 95.96 99.22 92.22 98.35 99. 95.35 99.85 99.92 99.85 96.85 99.90 96. 99.58 99.83 99.80 99.98 99.98 89. 97.95 87.06 92.63 98.93 92.09 96. 99.80 99.79 99.98 100.00 100.00 99.86 99.20 99.49 98. 99.96 98.82 99.15 99.98 100.00 100.00 99.47 98.21 99. 99.71 99.86 99.98 99.99 99.86 99.98 96. 99.80 0.45 0.37 0.50 0.53 0. 0.29 0.33 0.38 0.26 0.45 0. 0.23 0.25 0.11 0.22 0.23 0. 0.18 0.40 0.21 1.00 0.99 0. 0.93 1.00 0.94 0.98 0.69 0. 0.80 0.92 0.98 0.87 0.97 1. 1.00 1.00 1.00 0.96 1.00 2. 2.99 2.67 3.05 2.68 2.90 3. 2.03 3.17 2.10 2.59 3.22 2. 3.14 3.02 3.26 3.00 3.25 3. 3.24 2.05 2.08 1.85 2.10 1. 1.94 2.06 0.96 2.16 1.01 1. 2.15 1.91 1.79 1.86 2.17 1. 2.22 1.95 2.20 2.62 2.72 2. 2.82 2.34 2.52 2.71 1.47 2. 1.53 2.04 2.78 2.44 2.48 2. 2.83 2.49 2.89 2.55 3.02 Figure 5: Qualitative results illustrating the effect of increasing refinement iterations on performance. Starting from GPT-4o outputs, iterative refinements (#1, #5, #10) progressively improve alignment with the GT. Gemini-2.5-pro results are also shown for comparison."
        },
        {
            "title": "5.2 Experimental Results",
            "content": "General Trends As shown in Table 1, several models (DeepSeek-VL2 variants, LLaMA-3.2-11BVision, H2OVL-Mississippi-2B) failed to produce valid outputs, indicating either insufficient 3D understanding or incompatibility with the task format. Among those completing the benchmark, most VLMs demonstrate strong recognition of object-level attributes (color, material, shape, size) as indicated by high CLIP scores (GPT-4o: 0.98, Claude-3.7: 0.95); Pixel Distance is low (GPT4o: 0.0004, Gemini-2.5-pro: 0.0003), showing most models can well estimate the position of the object center. However, their spatial understanding between objects is notably weaker. Size-related metrics such as IoU and DICE are low (IoU: GPT-4o: 0.43, Claude-3.7: 0.40), indicating difficulty in estimating object scale and boundaries; Relational Accuracy, which captures reasoning over inter-object spatial relations, is below 0.3 for most models (GPT-4o: 0.28, Gemini: 0.26), showing persistent errors in understanding relative positions, proximity, and containment. Together, these results suggest that while VLMs can describe what is in the scene and how it looks, they still struggle to understand where things are and how they relate in structured 3D space. 8 Figure 7: Prompt Design and Error Analysis. (A) Inverse rendering prompt includes structured format, fixed camera, and attribute guidelines. (B) Ablation of prompt components reveals distinct failure modes, highlighting the importance of precise prompt engineering. Model-specific Analysis We conduct an in-depth analysis on two VLMs, Gemini-2.5-pro and QwenVL-2.5, which are representative of two paradigms (high-end proprietary model v.s. open-source model). Gemini-2.5-pro is consistently strong across all evaluated dimensions. With CLIP score of 0.97, it shows precise recognition of object-level appearance (color, shape, and material). In terms of spatial reasoning, it achieves an IoU of 0.41 and DICE score of 0.55, showing reliable estimation of object size and extent. Its Pixel Distance is low at 0.29, reflecting accurate object localization, while Relational Accuracy reaches 0.26, among the highest in our benchmark. These results suggest that Gemini-2.5-pro not only recognizes what is present in scene, but also exhibits measurable capacity to infer where objects are and how they are spatially organized. In contrast, Qwen-VL-2.5, as leading open-source alternative, performs reasonably well in appearance-related tasks with CLIP score of 0.89. However, its spatial understanding remains limited. The model records Pixel Distance of 0.42 and low IoU and DICE scores of 0.28 and 0.36, which indicate difficulties in precise object localization and shape reconstruction. Its Relational Accuracy is 0.18, suggesting substantial challenges in modeling inter-object spatial relationships. Still, it shows consistent recognition of object categories and general scene layout, which points to solid foundation for future improvements. Iterative Refinements We examine if agentic inverse rendering can improve with iterative reasoning and self-correction, using GPT-4o as the backbone VLM (prompt design detailed in the appendix). At each refinement step, the VLA sees the GT image, the previous rendering, and the JSON scene description. We apply ten refinement iterations, and some renderings are shown in Figure 5. As the number of refinement steps increases, we observe consistent improvements in object spatial alignment, color fidelity, and scale accuracy. After ten iterations, the output quality becomes comparable to that of Gemini-2.5-pro. Figure 6 confirms this trend in quantitative metrics: pixel distance decreases while bounding box scores increase as we scale refinement iterations, suggesting that the models scene understanding improves after refinement. Figure 6: Understanding scales with more refinements. Impact of Prompt Design We conduct ablation studies on four key components of our prompt: (1) task decomposition and clarification, (2) fixed camera parameters, (3) structured output format, and (4) detailed attribute guidelines. Figure 7 (B) shows that removing any component leads to failure cases. Task decomposition helps the model understand complex instructions and accurately determine object count. Fixed camera ensures consistent object orientation and spatial alignment. structured output format guides the model to produce valid and complete JSON results. Clear attribute guidelines improve the accuracy of size, shape, and material predictions."
        },
        {
            "title": "6 Conclusion",
            "content": "IR3D-Bench redefines VLM scene understanding through agentic inverse rendering, challenging VLAs to reconstruct 3D scenes from 2D images via automatic tool-use. Our experiments find that 9 current VLMs grasp high-level object attributes and tool-use abilities, but struggle with precise spatial control. We found that iterative refinement and careful prompt design can improve reconstruction quality, providing guidance for future VLM research. With IR3D-Bench, we provide the community with systematic framework to measure progress of VLM scene understanding, moving beyond passive observation to agentic understanding-by-creating."
        },
        {
            "title": "References",
            "content": "[1] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [2] OpenAI et al. GPT-4o System Card. arXiv:2410.21276, 2024. [3] Anthropic. The Claude 3 Model Family: Opus, Sonnet, Haiku. https://www.anthropic. com/news/claude-3-family. [4] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [6] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. [7] Richard Langton Gregory. Perceptions as hypotheses. Philosophical Transactions of the Royal Society of London. B, Biological Sciences, 290(1038):181197, 1980. [8] Alan L. Yuille and Daniel Kersten. Vision as bayesian inference: analysis by synthesis? Trends in Cognitive Sciences, 10:301308, 2006. [9] Ilker Yildirim, Tejas Kulkarni, Winrich Freiwald, and Joshua Tenenbaum. Efficient and robust analysis-by-synthesis in vision: computational framework, behavioral tests, and modeling neuronal representations. In Annual Conference of the Cognitive Science Society, 2015. [10] Thomas G. Bever and David Poeppel. Analysis by synthesis: (re-)emerging program of research for language and vision. Biolinguistics, 2010. [11] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:68539 68551, 2023. [12] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1495314962, 2022. [13] Didac Suris, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1185411864, 2023. [14] Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems, 37:126544126565, 2024. [15] Blender Online Community. Blender - 3D modelling and rendering package, 2016. [16] Model Context Protocol. https://github.com/modelcontextprotocol, 2024. 10 [17] Siddharth Ahuja. Blendermcp. https://github.com/ahujasid/blender-mcp, 2025. [18] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 110, 2018. [19] Licheng Yu, Xinlei Chen, Georgia Gkioxari, Mohit Bansal, Tamara Berg, and Dhruv Batra. Multi-target embodied question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 63096318, 2019. [20] Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. 3d concept learning and reasoning from multi-view images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 92029212, 2023. [21] Chan Hee Song, Jihyung Kil, Tai-Yu Pan, Brian Sadler, Wei-Lun Chao, and Yu Su. One step at time: Long-horizon vision-and-language navigation with milestones. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1548215491, 2022. [22] Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, and Gaurav Sukhatme. Embodied bert: transformer model for embodied, language-guided visual task completion. arXiv preprint arXiv:2108.04927, 2021. [23] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary In Proceedings of the IEEE conference on computer vision and pattern visual reasoning. recognition, pages 29012910, 2017. [24] Harry G. Barrow and Jay M. Tenenbaum. Recovering intrinsic scene characteristics from images. In Computer Vision Systems, 1978. [25] Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft rasterizer: differentiable renderer for image-based 3d reasoning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 77087717, 2019. [26] Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan A. Carr, Jonathan Ragan-Kelley, and Frédo Durand. Difftaichi: Differentiable programming for physical simulation. International Conference on Learning Representations (ICLR), 2020. [27] Tzu-Mao Li, Michal Lukáˇc, Michaël Gharbi, and Jonathan Ragan-Kelley. Differentiable vector graphics rasterization for editing and learning. ACM Transactions on Graphics (TOG), 39(6):115, 2020. [28] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. ECCV, 2020. [29] Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In ICCV, 2021. [30] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. In NeurIPS, 2021. [31] Olivia Wiles, Georgia Gkioxari, and Noah Snavely. Synsin: End-to-end view synthesis from single image. In CVPR, 2020. [32] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (TOG), 42(4), 2023. [33] Cheng Zhang, Yihang Guo, Zexiang Dong, Ravi Ramamoorthi, and Manmohan Chandraker. Path-space differentiable rendering. In SIGGRAPH Asia, 2020. [34] Hanxun Yu, Wentong Li, Song Wang, Junbo Chen, and Jianke Zhu. Inst3d-lmm: Instance-aware 3d scene understanding with multi-modal instruction tuning, 2025. 11 [35] Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu. Llava-3d: simple yet effective pathway to empowering lmms with 3d-awareness. arXiv preprint arXiv:2409.18125, 2024. [36] Daich Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1910719117, 2021. [37] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 3d-vista: Pretrained transformer for 3d vision and text alignment. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 28992909, 2023. [38] Dave Zhenyu Chen, Angel X. Chang, and Matthias Nießner. Scanrefer: 3d object localization in rgb-d scans using natural language. ArXiv, abs/1912.08830, 2019. [39] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas J. Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In European Conference on Computer Vision, 2020. [40] Dave Zhenyu Chen, Ali Gholami, Matthias Nießner, and Angel X. Chang. Scan2cap: Contextaware dense captioning in rgb-d scans. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 31923202, 2020. [41] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. Advances in Neural Information Processing Systems, 36:7530775337, 2023. [42] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, Karmesh Yadav, Qiyang Li, Ben Newman, Mohit Sharma, Vincent-Pierre Berges, Shiqi Zhang, Pulkit Agrawal, Yonatan Bisk, Dhruv Batra, Mrinal Kalakrishnan, Franziska Meier, Chris Paxton, Alexander Sax, and Aravind Rajeswaran. Openeqa: Embodied question answering in the era of foundation models. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1648816498, 2024. [43] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: benchmark for interpreting grounded instructions for everyday tasks. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1073710746, 2019. [44] Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, and Siyuan Huang. Sceneverse: Scaling 3d vision-language learning for grounded scene understanding. In European Conference on Computer Vision, pages 289310. Springer, 2024. [45] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. Advances in Neural Information Processing Systems, 37:135062135093, 2025. [46] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Alaa Maalouf, Shuang Li, Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, et al. Conceptfusion: Open-set multimodal 3d mapping. arXiv preprint arXiv:2302.07241, 2023. [47] Haonan Chang, Kowndinya Boyalakuntla, Shiyang Lu, Siwei Cai, Eric Jing, Shreesh Keskar, Shijie Geng, Adeeb Abbas, Lifeng Zhou, Kostas Bekris, et al. Context-aware entity grounding with open-vocabulary 3d scene graphs. In CoRL, 2023. [48] Qiao Gu, Ali Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, Chuang Gan, Celso Miguel de Melo, Joshua B. Tenenbaum, Antonio Torralba, Florian Shkurti, and Liam Paull. Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning. In ICRA, 2024. [49] Yujie Hong, Huajun Zhen, Peixi Chen, et al. 3d-llm: Injecting the 3d world into large language models. arXiv:2307.12981, 2023. 12 [50] Sha Zhang, Di Huang, Jiajun Deng, Shixiang Tang, Wanli Ouyang, Tong He, and Yanyong Zhang. Agent3d-zero: An agent for zero-shot 3d understanding. arXiv:2403.11835, 2024. [51] Zeju Li, Chao Zhang, Xiaoyan Wang, et al. 3dmit: 3d multi-modal instruction tuning for scene understanding. arXiv:2401.03201, 2024. [52] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa: Embodied question answering in the era of foundation models. In CVPR, 2024. [53] Tejas D. Kulkarni, Pushmeet Kohli, Joshua B. Tenenbaum, and Vikash K. Mansinghka. Picture: probabilistic programming language for scene perception. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 43904399, 2015. [54] Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Josh Tenenbaum. Learning to infer graphics programs from hand-drawn images. Advances in neural information processing systems, 31, 2018. [55] Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis, William Freeman, Joshua Tenenbaum, and Jiajun Wu. Learning to infer and execute 3d shape programs. In 7th International Conference on Learning Representations, ICLR 2019, 2019. [56] Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy Mitra, and Daniel Ritchie. Shapeassembly: Learning to generate programs for 3d shape structure synthesis. ACM Transactions on Graphics (TOG), 39(6):120, 2020. [57] Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David Ross, Cordelia Schmid, and Alireza Fathi. Scenecraft: An llm agent for synthesizing 3d scenes as blender code. In Forty-first International Conference on Machine Learning, 2024. [58] Yunzhi Zhang, Zizhang Li, Matt Zhou, Shangzhe Wu, and Jiajun Wu. The scene language: Representing scenes with programs, words, and embeddings. ArXiv, abs/2410.16770, 2024. [59] Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, and Stephen Gould. 3d-gpt: Procedural 3d modeling with large language models. arXiv preprint arXiv:2310.12945, 2023. [60] Hou In Ivan Tam, Hou In Derek Pun, Austin Wang, Angel Chang, and Manolis Savva. Scenemotifcoder: Example-driven visual program learning for generating 3d object arrangements. arXiv preprint arXiv:2408.02211, 2024. [61] Yutaro Yamada, Khyathi Chandu, Yuchen Lin, Jack Hessel, Ilker Yildirim, and Yejin Choi. L3go: Language agents with chain-of-3d-thoughts for generating unconventional objects. arXiv preprint arXiv:2402.09052, 2024. [62] Chi Zhang, Penglin Cai, Yuhui Fu, Haoqi Yuan, and Zongqing Lu. Creative agents: Empowering agents with imagination for creative tasks. arXiv preprint arXiv:2312.02519, 2023. [63] Mengqi Zhou, Jun Hou, Chuanchen Luo, Yuxi Wang, Zhaoxiang Zhang, and Junran Peng. Scenex: Procedural controllable large-scale scene generation via large-language models. arXiv e-prints, pages arXiv2403, 2024. [64] Jiayuan Mao, Xiuming Zhang, Yikai Li, William Freeman, Joshua Tenenbaum, and Jiajun Wu. Program-guided image manipulators. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40304039, 2019. [65] Fan-Yun Sun, Weiyu Liu, Siyi Gu, Dylan Lim, Goutam Bhat, Federico Tombari, Manling Li, Nick Haber, and Jiajun Wu. Layoutvlm: Differentiable optimization of 3d layout via vision-language models. ArXiv, abs/2412.02193, 2024. [66] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 84948502, 2018. 13 [67] Jacky Liang, Wenlong Huang, F. Xia, Peng Xu, Karol Hausman, Brian Ichter, Peter R. Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 94939500, 2022. [68] Christopher Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv:1901.11390, 2019. [69] Martin Engelcke, Adam Kosiorek, Oiwi Parker Jones, and Ingmar Posner. Genesis: Generative scene inference and sampling with object-centric latent representations. arXiv:1907.13052, 2019. [70] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1145311464, 2021. [71] Adam Santoro, David Raposo, David Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. simple neural network module for relational reasoning. Advances in neural information processing systems, 30, 2017. [72] Drew Hudson and Christopher Manning. Compositional attention networks for machine reasoning. arXiv:1803.03067, 2018. [73] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [74] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [75] OpenAI et al. GPT-4 Technical Report. arXiv:2303.08774, 2023. [76] Anthropic. The Gemini 2 Model Family: Google Deepmind. https://gemini.google.com/. [77] Anthropic. The Grok Model Family: xAI. https://grok.com/. [78] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts visionlanguage models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. [79] Yuanhan Zhang, Bo Li, haotian Liu, Yong Jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, 2024. Accessed on 2024-05-06. [80] Meta AI. The Llama 3 Herd of Models. arXiv:2407.21783, 2024. [81] Shaikat M. Galib, Shanshan Wang, Guanshuo Xu, Pascal Pfeiffer, Ryan Chesler, Mark Landry, and SriSatish Ambati. H2ovl-mississippi vision language models technical report. ArXiv, abs/2410.13611, 2024. [82] Phi-4 Research Team. Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs. arXiv:2503.01743, 2025. [83] Pravesh Agrawal et al. Pixtral 12B. arXiv:2410.07073, 2024. [84] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-of-experts model. arXiv:2410.05993, 2024. 14 [85] Hugo Laurenccon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. Obelisc: An open web-scale filtered dataset of interleaved image-text documents. arXiv:2306.16527, 2023. [86] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of opensource multimodal models with model, data, and test-time scaling. arXiv:2412.05271, 2024. [87] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv:2504.10479, 2025. [88] Qwen An Yang, Baosong Yang, and Beichen Zhang et al. Qwen2.5 technical report. arXiv:2412.15115, 2024. [89] OpenAI. gpt4o, 2024. [90] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023."
        },
        {
            "title": "A Experiment Setup Details",
            "content": "A."
        },
        {
            "title": "Implement Details",
            "content": "To ensure consistency across models with reasoning capabilities, such as GPT-4o [89] and Grok-3 [77] etc, which often generate intermediate \"thought\" steps or chain-of-thought reasoning, we discard all non-structured outputs during inference. We extract only the final JSON-formatted response that conforms to our predefined schema. In designing the evaluation metrics, we intentionally ignore lighting-related attributes (shading and brightness) since these aspects are not explicitly modeled by the agents and often introduce high variability. Moreover, accurately recovering lighting conditions from single image is inherently ambiguous, even for human annotators, making it an unrealistic expectation for current VLMs. Thus, we focus evaluation on geometry and semantics, rather than photometric fidelity. Additionally, we observe that most models output zero object rotation by default; hence, we omit rotation from evaluation to simplify the task and focus on more meaningful aspects of spatial understanding. In this paper, we evaluate all models on representative subset of the CLEVR datasets validation split, containing 1,500 imagescene pairs with GT annotations. A.2 Task prompt Design of Inverse Rendering Prompt The complete prompt used for inverse rendering is presented in Table 2. Given an input image, the vision-language model is instructed to extract scene-level geometry, material properties, and lighting parameters, and to output the results in strict JSON format. This structured output is then parsed and used to reconstruct the 3D scene within Blender [15]. The prompt is carefully designed to ensure consistency, reproducibility, and compatibility with downstream scene synthesis workflows. Design of LLM Score Prompt The complete prompt used to elicit LLM-based evaluations of reconstructed 3D scenes is detailed in Table 3. This prompt instructs the LLM to compare predicted scene description against ground-truth JSON, and to assign scores across multiple dimensions of fidelity and accuracy. The evaluation process is fully self-contained, requiring the model to analyze only the provided JSON content without recourse to external assumptions. Scores are returned in structured JSON format, with each score accompanied by concise justification referencing specific attributes or spatial discrepancies. Design of Refinement Prompt The complete prompt used for refining the scene description based on prior outputs is presented in Table 4. This prompt instructs the LLM to revise the predicted JSON scene by leveraging both the ground-truth image and the rendered image from the current JSON, under fixed camera setup. The objective is to produce refined scene JSON that is visually and spatially aligned with the ground-truth reference, in terms of object layout, attributes, and relationships. LLM is required to output valid JSON object conforming to strict schema (the same as the inverse rendering prompt), with no additional text, ensuring consistency and interpretability for downstream evaluation."
        },
        {
            "title": "B More Experimental Results",
            "content": "B.1 More Visual Results We present additional qualitative comparisons in Figure 8. Among all models, Gemini-2.5 Pro [76] achieves the most faithful reconstructions in terms of geometry, spatial layout, and material appearance. Grok-3 [77] shows competitive performance in recovering fine material details, though with minor spatial inconsistencies. In contrast, models such as Mistral-3 [90], InternVL-Chat-3B [87], Qwen2-5LVL-72B [88], and Claude-3-7 [3] often exhibit noticeable errors in object positioning and material prediction, leading to spatial misalignments, incorrect relative depths, and inconsistencies in color or reflectance. These observations further highlight the strengths of Gemini-2.5 Pro in holistic scene understanding. 16 Table 2: Prompt for Inverse Rendering 3D Inverse Rendering Prompt Specification #1 Task Description Please carefully analyze the provided image, identifying all major geometric objects, their properties, and the scenes lighting setup. Your task is to extract object and lighting information and return the result strictly following the JSON format specified below. The Camera parameters are fixed and should be used as provided in the JSON structure. This JSON will be used by Python script to reconstruct the scene in Blender. #2 Output Format Requirements Please output **only** valid JSON object, without any additional explanations, comments, or code block markers (like json ...). The JSON object must adhere to the following structure: \"camera\": { // --- CRITICAL: Use these FIXED values for the camera --- \"name\": \"MainCamera\", \"location\": [0.0, -10.0, 5.0], \"lens\": 50.0, \"rotation_euler\": [65.0, 0.0, 0.0], // Provided in degrees \"sensor_width\": 36.0, \"sensor_height\": 36.0, \"clip_start\": 0.1, \"clip_end\": 100.0 // --- Do NOT estimate camera parameters --- }, \"lighting\": { \"sun_energy\": float, // Estimated sun light intensity (e.g., between 2.0 and 5.0) \"sun_rotation_euler_degrees\": [float, float, float], // Rotation angles [X, Y, Z] \"environment_color\": [float, float, float, float], // RGBA, e.g., [0.8, 0.8, 0.8, 1.0] \"environment_strength\": float // Light strength, e.g., between 1.0 and 1.5 }, \"objects\": [ { \"name\": \"descriptive_name_string\", // e.g., \"green large metal cylinder\" \"location\": [float, float, float], // [X, Y, Z] \"rotation_euler\": [float, float, float], // Usually [0, 0, 0] \"size_params\": { // One of the following based on shape: // \"size\": float // \"radius\": float, \"depth\": float // If cylinder // \"radius\": float // If sphere // If cube }, \"material\": { \"name\": \"MaterialNameString\", // e.g., \"GreenMetal\" \"base_color\": [float, float, float, float], // [R, G, B, A] \"metallic\": float, // Between 0.0 and 1.0 \"roughness\": float // Between 0.0 and 1.0 } } // ... Potentially multiple object entries ... ] #3 Object Analysis Guidelines: Identification: Find all clearly visible, primary geometric objects in the image. Name (name): Use the format \"color size material shape\", e.g., \"blue large rubber cube\". Location (location): Estimate the objects center position as [X, Y, Z]. Assume the ground is at = 0, and is usually half the objects height. Estimate and based on the objects left-right and front-back position in the image. Rotation (rotation_euler): For CLEVR-style images, objects are usually upright. Use [0.0, 0.0, 0.0] unless there is visual evidence to the contrary. Size Parameters (size_params): Based on the object shape, include: cube: \"size\": cylinder: \"radius\": float, \"depth\": sphere: \"radius\": float (estimated edge length) float (estimated radius) float (estimated base radius and height) Estimate all size values visually relative to other objects in the image. Material (material): name: Generate concise material name, e.g., \"GreenMetal\". Reuse the same name for identical materials across objects. base_color: RGBA color in format [R, G, B, A], where values are between 0.0 and 1.0. metallic: Use 1.0 for metallic surfaces, 0.0 for non-metal surfaces. roughness: Estimate based on surface appearance smooth/mirror-like surfaces have low roughness (near 0.0), matte/dull surfaces have high roughness (near 1.0). 17 Table 3: Evaluation Prompt for Scoring 3D Sene Json 3D Scene JSON Description Evaluator Prompt #1 Role and Task You are an AI evaluator specializing in 3D scene descriptions. Your task is to compare Predicted JSON scene description with Ground Truth (GT) JSON and evaluate the accuracy and consistency of the predicted scene. Instructions The Predicted JSON will be provided first. The GT JSON will follow. Focus only on the data in the JSONs no external visual interpretation or assumptions. Acknowledge and account for structural differences across JSONs. Your output must be single valid JSON object following the format below no other text or explanations. Scoring Scale (Per Dimension) 5: Excellent Highly accurate and consistent 4: Good Mostly accurate, minor discrepancies 3: Fair Captures core ideas but with noticeable issues 2: Poor Significant inaccuracies 1: Very Poor Major incorrect aspects 0: Completely Incorrect or Missing Evaluation Dimensions GPT4.1-JSON Object Appearance Fidelity Focus: Can plausible object matches be found? How accurate are the predicted attributes vs GT? Match predicted name (color/size/material/shape) with GT attributes. Compare predicted material fields (e.g., metallic) with GT material category. Compare predicted size_params to size descriptors like \"small\"/\"large\". Score reflects object match quality and attribute-level accuracy. GPT4.1-JSON Scene Layout Accuracy Focus: For matched objects, how close are predicted location values to GT 3d_coords? Score reflects 3D spatial alignment accuracy. GPT4.1-JSON Overall Visual Quality & Similarity Focus: Holistic assessment of how well the predicted JSON matches the GT. Consider object count, attributes, locations. Identify any major inconsistencies within the predicted data. Score reflects overall scene description quality. Expected Output Format (After receiving both JSONs) #2 # #4 1 2 3 #5 json { \"GPT4_1_JSON_Object_Appearance_Fidelity\": { \"score\": <integer 0-5>, \"justification\": \"<string explanation of matching success and attribute accuracy, noting structural differences and specific examples>\" }, \"GPT4_1_JSON_Scene_Layout_Accuracy\": { \"score\": <integer 0-5>, \"justification\": \"<string qualitative assessment of the similarity between predicted locations and GT 3d_coords for matched objects>\" }, \"GPT4_1_JSON_Overall_Visual_Quality_and_Similarity\": { \"score\": <integer 0-5>, \"justification\": \"<string explanation for the overall data accuracy score>\" } } 18 Table 4: Structured refinement prompt for 3D scene JSON correction based on GT and predicted image comparison. Refinement Prompt Based on GT and Predicted Images #1 Inputs GT Image: Ground truth image of the scene (accurate reference). Pred Image: Rendered image from the current JSON scene. Current JSON: Scene description generated from the predicted image. #2 Refinement Goals Objective: Refine the parameters of all objects in 3D scene JSON to closely match provided ground truth (GT) image, under fixed camera setup. NOTICE: The refined attributes of all objects in the refined json file should not be all the same as the input json file. Goal: Achieve refined scene JSON whose rendered image (with the fixed camera) is visually and spatially consistent with the GT image, in terms of object count, placement, size, shape, material, and inter-object relationships. #3 Constraints All changes must be grounded in visual evidence from GT vs Pred and metric feedback. The final output must be valid JSON object that strictly follows the original schema. Output only the JSON object no extra explanation, comments, or formatting. #4 Output Format Requirements Please output only valid JSON object, without any additional explanations, comments, or code block markers (like json ... ). The JSON object must adhere to the following structure: Camera format: \"camera\": { // --- CRITICAL: Use these FIXED values for the camera --- \"name\": \"MainCamera\", \"location\": [0.0, -10.0, 5.0], \"lens\": 50.0, \"rotation_euler\": [65.0, 0.0, 0.0], // Provided in degrees \"sensor_width\": 36.0, \"sensor_height\": 36.0, \"clip_start\": 0.1, \"clip_end\": 100.0 // --- Do NOT estimate camera parameters --- }, Lighting format: \"lighting\": { \"sun_energy\": float, // Estimated sun light intensity (e.g., between 2.0 and 5.0) \"sun_rotation_euler_degrees\": [float, float, float], // Rotation angles [X, Y, Z] \"environment_color\": [float, float, float, float], // RGBA, e.g., [0.8, 0.8, 0.8, 1.0] \"environment_strength\": float // Light strength, e.g., between 1.0 and 1.5 }, Objects format: \"objects\": [ // --- CRITICAL: Refine the parameters of each object --- { \"name\": \"descriptive_name_string\", // e.g., \"green large metal cylinder\" \"location\": [float, float, float], // [X, Y, Z] \"rotation_euler\": [float, float, float], // Usually [0, 0, 0] \"size_params\": { // One of the following based on shape: // \"size\": float // \"radius\": float, \"depth\": float // If cylinder // \"radius\": float // If sphere // If cube }, \"material\": { \"name\": \"MaterialNameString\", // e.g., \"GreenMetal\" \"base_color\": [float, float, float, float], // [R, G, B, A] \"metallic\": float, // Between 0.0 and 1.0 \"roughness\": float // Between 0.0 and 1.0 } } // ... Potentially multiple object entries ... ] 19 Figure 8: More Visual Results with Selected VLMs on IR3D-Bench B.2 Impact of Prompt Design To quantitatively assess the contribution of different prompt components, we perform ablation studies on four key elements: structured output format, fixed camera configuration, task decomposition, and attribute specification. We use GPT-4o [89] as the VLA for conducting these experiments. As shown in Table 5, the full prompt achieves the best overall performance across almost all evaluation metrics, with total Pixel Distance of 0.5528 and DICE score 0.11. Removing the structured format results in outputs lacking essential attributes required for rendering, thereby making it fail to render and compute downstream reconstruction metrics. This highlights the critical role of structured prompts in constraining the output space and ensuring syntactic and semantic completeness. Eliminating the fixed camera setup results in worsened spatial consistency, particularly evident in reduced shape accuracy (99.14 vs. 99.88) and lower layout score (1.91 vs. 1.94). Without task decomposition and clarification, the model struggles with compositional reasoning, leading to drop in object count accuracy (0.91 vs. 0.94) and Pixel Distance (0.7156 vs. 0.5528). Finally, omitting attribute guidelines significantly impacts fine-grained predictions, notably reducing material accuracy (97.59 vs. 98.66) and object-level score (2.73 vs. 2.90). These results empirically validate that each prompt component plays critical role in guiding the VLM toward faithful and consistent scene reconstruction. B.3 Analysis of Failure Cases Among the evaluated models, LLaMA-3.2-11B-Vision [80], DeepSeek-VL2 [78] (tiny and small version), and H2OVL-Mississippi-2B [81] represent clear failure cases on IR3D-Bench. Despite the general success of many models, these models consistently fail to produce valid outputs suitable for inverse rendering. As illustrated in Figure 9, LLaMA-3.2-11B-Vision repeatedly emits static 20 Table 5: Quantitative results of models with various prompt designs on IR3D-Bench. We report performance across key aspects of 3D scene reconstruction from single image."
        },
        {
            "title": "Models",
            "content": "Layout & Localization Relation Instance Seg."
        },
        {
            "title": "LLM Score",
            "content": "Pix. Dist. Count ACC Bbox Rel. ACC IOU DICE Color Size Material Shape Overall Obj Layout Overall w.o. Given Format - w.o. Given Camera w.o. Task Analysis 0. 0.6718 w.o. Attribute Guides 0.5891 GPT-4o 0.5528 - 0.94 0.91 0.94 0.94 - 0. 0.27 0.26 0.29 - 0.24 0. 0.27 0.3 - 0.02 0.06 0. 0.07 - 0.03 0.09 0.1 0. - - 96.57 97.58 97.01 97.53 97.15 98.15 96. 98.36 - 98.02 97.79 97.59 98. - 99.59 99.14 99.59 99.88 - 93.46 93.65 94.03 94.22 - 2. 2.84 2.73 2.9 - 1.91 2. 1.85 1.94 - 2.38 2.31 2. 2.52 Figure 9: Failure output of selected models on IR3D-Bench template across all test cases, with identical object attributes and all object locations fixed at (0, 0, 0), rendering the outputs semantically meaningless and non-renderable. DeepSeek-VL2-tiny directly copies the JSON template, including placeholders such as [float, float, float], without generating any instance-specific values. Similarly, DeepSeek-VL2-small fails to populate essential fields, instead outputting large arrays of zeros without assigning any object-level attributes. H2OVLMississippi-2B generates multiple objects, but with identical and repetitive attributes across all instances, suggesting template replication without true scene interpretation. In all these cases, the lack of structurally valid and semantically grounded output prevents rendering and quantitative evaluation, highlighting the importance of model understanding and prompt grounding in 3D scene tasks."
        },
        {
            "title": "C Further Analysis",
            "content": "C.1 Limitation While IR3D-Bench offers novel lens to evaluate vision-language models (VLMs) through agentic inverse rendering, several limitations remain. First, the benchmark is constructed on the CLEVR dataset, which contains synthetic scenes with clean geometry and controlled semantics. While this design enables precise evaluation, it lacks the visual richness and noise inherent in real-world data. We intentionally refrain from using real-world datasets at this stage because most models still struggle to perform reliably even under the simplified CLEVR setting. Second, our current evaluation focuses on single-view, static scene reconstruction, without considering temporal consistency or multi-view fusion, both of which are essential for reasoning in dynamic and embodied environments. Lastly, we fix the camera intrinsics and extrinsics and omit illumination modeling. This decision is made not only to reduce task ambiguity, but also because current models already exhibit substantial limitations in reconstructing geometry and semantics under these simplified conditions. Introducing additional complexity at this stage may obscure rather than clarify the core challenges in agentic visual understanding. 21 C.2 Future Extension IR3D-Bench provides structured setting that can support the construction of high-quality instructionoutput pairs for supervised fine-tuning (SFT) or chain-of-thought (CoT) training. By leveraging successful inverse rendering examples, we can curate targeted datasets to improve VLMs compositional reasoning and program generation capabilities. Building on this, future extensions of IR3D-Bench could extend to multi-view and dynamic scenes, and incorporate camera parameter estimation and illumination modeling. Ultimately, extending the benchmark to real-world datasets with diverse appearance, clutter, and geometry will enable comprehensive evaluation and training of VLAs in open-world, visually complex environments."
        }
    ],
    "affiliations": [
        "CUHK",
        "EPFL",
        "HKUST",
        "MIT",
        "TJU",
        "XMU"
    ]
}