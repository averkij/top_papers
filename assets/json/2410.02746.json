{
    "paper_title": "Contrastive Localized Language-Image Pre-Training",
    "authors": [
        "Hong-You Chen",
        "Zhengfeng Lai",
        "Haotian Zhang",
        "Xinze Wang",
        "Marcin Eichner",
        "Keen You",
        "Meng Cao",
        "Bowen Zhang",
        "Yinfei Yang",
        "Zhe Gan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect image inputs for language interactions. The success of CLIP as a vision-language foundation model relies on aligning web-crawled noisy text annotations at image levels. Nevertheless, such criteria may become insufficient for downstream tasks in need of fine-grained vision representations, especially when region-level understanding is demanding for MLLMs. In this paper, we improve the localization capability of CLIP with several advances. We propose a pre-training method called Contrastive Localized Language-Image Pre-training (CLOC) by complementing CLIP with region-text contrastive loss and modules. We formulate a new concept, promptable embeddings, of which the encoder produces image embeddings easy to transform into region representations given spatial hints. To support large-scale pre-training, we design a visually-enriched and spatially-localized captioning framework to effectively generate region-text pseudo-labels at scale. By scaling up to billions of annotated images, CLOC enables high-quality regional embeddings for image region recognition and retrieval tasks, and can be a drop-in replacement of CLIP to enhance MLLMs, especially on referring and grounding tasks."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 ] . [ 1 6 4 7 2 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "CONTRASTIVE LOCALIZED LANGUAGE-IMAGE PRE-TRAINING Hong-You Chen, Zhengfeng Lai, Haotian Zhang, Xinze Wang, Marcin Eichner, Keen You, Meng Cao, Bowen Zhang, Yinfei Yang, Zhe Gan Apple"
        },
        {
            "title": "ABSTRACT",
            "content": "Contrastive Language-Image Pre-training (CLIP) has been celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect image inputs for language interactions. The success of CLIP as vision-language foundation model relies on aligning web-crawled noisy text annotations at image levels. Nevertheless, such criteria may become insufficient for downstream tasks in need of fine-grained vision representations, especially when region-level understanding is demanding for MLLMs. In this paper, we improve the localization capability of CLIP with several advances. We propose pre-training method called Contrastive Localized Language-Image Pre-training (CLOC) by complementing CLIP with region-text contrastive loss and modules. We formulate new concept, promptable embeddings, of which the encoder produces image embeddings easy to transform into region representations given spatial hints. To support large-scale pretraining, we design visually-enriched and spatially-localized captioning framework to effectively generate region-text pseudo-labels at scale. By scaling up to billions of annotated images, CLOC enables high-quality regional embeddings for image region recognition and retrieval tasks, and can be drop-in replacement of CLIP to enhance MLLMs, especially on referring and grounding tasks."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large-scale vision-language (VL) pre-training has been an important foundation for the recent tremendous growth of multimodal applications. Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021; Jia et al., 2021) has become great success of VL representation learning that connects images and text by contrastive training on web-crawled image-text pairs. It has been proven strong transferability and generalizability on extensive downstream tasks such as zero-shot image classification and image-text retrieval. Even beyond, CLIP has become arguably the default choice of vision backbone for multimodal large language models (MLLMs) (Liu et al., 2023; McKinzie et al., 2024; Zhang et al., 2024a) due to its superior prior knowledge in aligning vision and language (Tong et al., 2024), facilitating vision inputs to be injected into language models. As VL research gets increasing attention and expedites progress, various more advanced multimodal tasks are demanding stronger vision capabilities. For instance, recent MLLMs (Rasheed et al., 2024; Ren et al., 2024; Lai et al., 2023; Chen et al., 2023; Peng et al., 2023) have been focusing on more fine-grained understanding tasks that require comprehension of the semantic at region levels such as visual question answering (VQA) with referring and grounding instructions. These MLLMs are fine-tuned on referring and grounding data with CLIP as the vision backbone, as seen in works like Kosmos-2 (Peng et al., 2023) and Ferret (You et al., 2023; Zhang et al., 2024b). Due to the need for such region-level understanding, CLIP, which aligns entire images with text captions, seems insufficient, as its regular image-text contrastive loss primarily emphasizes global semantics. To remedy such core localization capability for CLIP, we ask challenging and fundamental question: without sacrificing CLIPs original strong image-level knowledge, can we pre-train stronger image encoder with enhanced localization capability that can be inherently integrated into MLLMs?"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overview of our CLOC pre-training framework. (1) visually-enriched and spatially-localized captioning pipeline generates pseudo-labeled bounding boxes with detailed descriptions for key image regions. (2) lightweight Prompter attached on top of the CLIP image encoder can be prompted to transform the image embedding into the region-focused feature. All parameters are trained end-to-end from scratch with our contrastive localized language-image loss on the annotated region-text datasets. After pre-training, (3a) region features can be generated via the Prompter for region-text tasks like object classification in training-free fashion. (3b) The image encoder, along with the optional Prompter, can also strengthen MLLMs fine-tuning by enhancing their fine-grained image understanding capabilities. To this end, we explore data-driven approach that complements the original CLIP image-text pre-training objective with explicit region-text supervision. Though conceptually simple, several challenges exist. First, it lacks public datasets with region-text annotations at scales large enough for CLIP training, which typically requires hundreds of millions even billions of images. Existing region-text corpus like Visual Genome (Krishna et al., 2017) contains about 108K images, and the largest noisy-labeled grounded dataset GRIT (Peng et al., 2023) features only 20M images. Indeed, such deficiency has probably limited the literature to mainly consider semi-supervised or weaklysupervised approaches as somewhat compromise (Naeem et al., 2023; Yao et al., 2022; 2023). Second, plausible solution is to scale up training data in pursuit of image regions pseudo-labeled with text annotations via some open-vocabulary detectors (Minderer et al., 2024; Zhang et al., 2022). Though it seems feasible, we found it non-trivial to design such pipeline as the annotations are noisy and will greatly affect the final model performance. Third, even if the region-text datasets are given, it is under-explored how to effectively train on them in terms of co-designs of training objectives, model architecture, and more design details. To this end, we propose new pre-training framework illustrated in Figure 1, named Contrastive Localized Language-Image Pre-Training (CLOC), to improve CLIP with better localization capability, especially for MLLMs, by overcoming the above difficulties. Our main contributions are: We propose new learning goal, Promptable Embeddings, that strong vision encoder should produce image embeddings that can be easily transformed into region representations, given some spatial hints (e.g., box referring or text prompts). This formulation not only facilitates the encoder as prior of fine-grained VL alignment, but also enables new possibilities for the interactions between the image encoder and the language decoder. To optimize towards the goal, we design simple and minimal modifications on top of CLIP. We augment the original CLIP loss with region-text contrastive loss, where the region embeddings are extracted from the image embedding by lightweight extractor module conditioned on the spatial hints (i.e., prompts). We design large-scale pseudo-labeling data engine to support CLOC training. We properly combine visual-enriched image captioners and open-vocabulary detectors for an effective recipe that improves previous practice of region annotations (Minderer et al., 2024; Peng et al., 2023). This approach yields two-billion image-text dataset with fine-grained region-text annotations, which serves as the foundation for training our CLOC model. Through extensive experiments across 31 evaluation tasks, including standard image-text tasks, newly constructed region-text tasks, and downstream evaluations with MLLMs, we demonstrate that CLOC significantly and consistently outperforms the CLIP counterpart. We are working on releasing our pre-trained checkpoints and the constructed region-text annotations along with the final version to accelerate future research within the community."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Improving localization of CLIP. Since CLIP was introduced, many follow-up works have been proposed to improve it from various aspects, for different target tasks, and with different approaches. From the aspect relevant to our work, improving the localization capability, most works specifically focus on the downstream dense vision tasks such as open-vocabulary detection (Minderer et al., 2024; Yao et al., 2022; Wu et al., 2023). Another less and arguably more challenging thread is to maintain the generalizability of CLIP on image-level tasks while improving localization. Recent works like SILC (Naeem et al., 2023) and SPARC (Bica et al., 2024) combine localization-enhancing unsupervised objectives with the CLIP loss, but do not attempt with supervision on large-scale explicit pseudo-labeled data like ours. Alpha-CLIP (Sun et al., 2024) shows that the SAM segmentation model (Kirillov et al., 2023) can provide useful conditions for CLIP. Vision encoder pre-training for MLLMs. Building upon the success of large language models (LLMs), popular approach to MLLMs like LLaVA (Liu et al., 2023), typically connects vision encoder (e.g., ViT (Dosovitskiy et al., 2021)) to digest visual inputs and maps them to the LLM decoder input space as token embeddings. Among various types of vision encoders (Oquab et al., 2023; He et al., 2022), CLIP (Radford et al., 2021; Jia et al., 2021) becomes the most popular choice, due to its superior performance on MLLM benchmarks reported by recent studies (Tong et al., 2024). Synthetic annotations for pre-training. Large-scale training data are the fuel of pre-training, especially for CLIP. The literature has been exploring scalable ways to generate high-quality synthetic annotations. For instance, several works demonstrate that visually-enriched image captions improve CLIP (Lai et al., 2024). MOFI (Wu et al., 2024) constructs large alt-text set and augments CLIP with multi-classification task. However, these works only consider image-level annotations but not explicit region-level labels. In the context of dense vision tasks like open-vocabulary detection and segmentation, pseudo-labeling in self-training paradigm has proven an effective approach (Kirillov et al., 2023; Minderer et al., 2024). We are inspired by these efforts and build on them to enhance CLIPs localization capabilities."
        },
        {
            "title": "3 CLOC: CONTRASTIVE LOCALIZED LANGUAGE-IMAGE PRE-TRAINING",
            "content": "3.1 PRELIMINARY: FROM IMAGE-TEXT TO REGION-TEXT ALIGNMENT Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021) trains pair of image and text encoders (denoted as fI and fT , respectively) by contrastively aligning the image and text embeddings. Let mini-batch of image-text pairs {(xi, yi)}N i=1 be sampled from the large-scale training set during each training iteration. The contrastive loss is defined as follows: LCLIP := (LIT + LT )/2. LIT := (cid:88) log 1 exp (cid:0)sim(fI (xi), fT (yi))/τ (cid:1) j=1 exp (cid:0)sim(fI (xi), fT (yj))/τ (cid:1) , (cid:80)N (1) i=1 where sim(, ) is the similarity measurement function and τ is (learnable) logit temperature. The CLIP loss LCLIP averages the symmetrical contrastive loss in which cross-entropy normalized along image-to-text and text-to-image axes, respectively. Conceptually, the CLIP loss aligns images with their associated text, but it overlooks regional information and spatial semantics. We propose augmenting this with region-text alignment on top of LCLIP. Specifically, assume an image-text pair (x, y) can be decomposed into image regions x(1), . . . , x(m), and there exist fine-grained captions y(m) that describe the corresponding image regions x(m). Thus, the original input (x, y) becomes {(x(1), y(1)), . . . , (x(m), y(m))} for regiontext considerations, and (x, y) is special case when the region itself is the whole image. Based on this, we identify several research questions and will answer them in the following sections: 1. Considering the goal is to train an image encoder fI with enhanced localization capability, how should we formulate region-text alignment goal that improves fI ? We propose novel learning task called promptable embeddings in Section 3.2. 2. How to properly extract region embedding from fI (x) as an effective joint design? We propose lightweight promptable region extractor in Section 3.3. 3. How to generate meaningful image regions with high-quality captions? Furthermore, in many cases, the ideal region caption y(m) may not exist in the image-level caption, i.e., y(m) might"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: CLOC promptable embedding architecture. CLOC builds upon the image embedding from CLIP (before pooling and projection) and transforms it into region-aware vision embedding given an encoded prompt; e.g., positional encodings of box coordinates or regional caption encoded by the CLIP text encoder. not be substring of the original y. We design an effective and scalable data engine as visuallyenriched and spatially-localized labeler to generate high-quality region-text pairs in Section 4. 4. With the above considerations, we discuss how to train the model with minimal conflicts towards drop-in replacement of the CLIP model in Section 3.4. 3.2 PROMPTABLE EMBEDDINGS To optimize CLIP with better feature localization and eventually learn an enhanced CLIP vision encoder fI for various VL downstream tasks, we argue that it will require at least two capabilities. (i) First, the encoder should recognize fine-grained small objects (e.g., this image crop is an airplane wheel). (ii) Second, the image embedding produced by the encoder provides holistic understanding such that an MLLM can reason more advanced spatial hierarchy relationships within the scene (e.g., The plane is lowering its front landing gear.). As discussed in Section 2, many previous works improve CLIP toward object detection tasks thus mainly focusing on (i) only; e.g., RegionCLIP (Zhong et al., 2022) that crops out image regions and uses them as additional input images to re-train the CLIP encoders for recognizing objects. However, to support comprehensive VL tasks, (i) is necessary but insufficient without (ii). To achieve this, we introduce new concept, promptable embedding. We consider scenario similar to MLLM use cases, where answers are generated using CLIP image tokens alongside question. We hypothesize that strong encoder for MLLMs should produce an image embedding that can easily be transformed into region representations, given location cues. We re-formulate the CLIP loss based on image-text pairs (x, y) into localized language-image contrastive loss for region-text alignment based on triplets of ({l}, x, y), where is location representation such as bounding box, and possibly there are several boxes as set {l} per image. To make it compatible with CLIP training, we construct promptable embedding transform module, or in short, region prompter = Prompter(l, fI (x)), that extracts the region embedding specified by from the image embedding fI (x). This formulation is inspired by the success of the segmentation model SAM (Kirillov et al., 2023) that predicts the segmentation masks conditioned on location prompt (e.g., box), while CLOC predicts region embedding conditioned on instead. To this end, we decompose the location-image-text triplets as localized region-text pairs. Let z(m) = Prompter(l(m) R4 is the m-th box of image represented as two coordinates (i.e., top-left and bottom-right corners). We then formulate symmetric region-text contrastive loss similar to Equation 1: is the caption of the region specified by l(m) , fI (xi)) and y(m) . l(m) i LRT := 1 (cid:88) (cid:88) i=1 l(m) {li} log (cid:80)N j=1 (cid:16) exp sim(cid:0)z(m) (cid:16) , fT (y(m) sim(cid:0)z(m) (cid:80) {lj } exp l(m) (cid:17) )(cid:1)/τ , fT (y(m) (cid:17) , )(cid:1)/τ (2) where is the number of regions l(m) discuss implementing the Prompter in Section 3.3, and generating l(m) sampled per image. We set = 4 by default. We will in Section 4. with y(m) i"
        },
        {
            "title": "Preprint",
            "content": "LT is the symmetric contrastive loss normalized along text-to-region axis, just like in Equation 1. We define LCLOC = (LRT + LT R)/2. As the Prompter is simple transformer encoder, it allows flexible types of prompts besides bounding boxes we have used, such as points, free-form referring, text, and etc. We further consider the case where the prompt is free-form text, and leave others for future study. We add grounding loss that extracts region feature from the image (e.g., picture of the bedroom) given its regional caption (e.g., large TV), and predicts the bounding box with an MLP regression head, i.e., Lgrounding := 1 4M (cid:88) (cid:88) i= l(m) {li} l(m) BoxHead(cid:0)z(y(m) )(cid:1)2, (3) where z(y) := Prompter(fT (y), fI (x)) is the grounded embedding conditioned on the text (encoded by the CLIP text encoder). The overall loss is := LCLIP + λ(LCLOC + Lgrounding), (4) where λ is weighting scalar. In experiments, we set λ to be the ratio of images in the mini-batch that contain region labels without extra tuning. All the learnable parameters are trained end-to-end. 3.3 CLOC MODEL ARCHITECTURE We implement the promptable embedding introduced in Section 3.2 with minimal extra modules on top of the original CLIP image and text encoders. As illustrated in Figure 2, the original CLIP model remains the same for computing LCLIP. For computing LCLOC/Lgrounding, the image embedding is reused from the CLIP ViT but before the pooled projection and normalization . To extract the region embedding = Prompter(l, (x)) from the image, we consider the location representation as two coordinates (top-left and bottom-right corners of box), each vectorized by positional encoding. The Prompter is simple and lightweight one-layer transformer encoder. It takes the positional encodings prepended with the sequence of image tokens from ViT together as the input, and outputs the region embedding with pooled projection layer. For the grounding loss, we reuse the same CLIP text encoder for encoding the region captions = Prompter(fT (y), (x)) to predict the bounding boxes with two-layer MLP head. Overall, CLOC only adds additional learnable parameters of the lightweight Prompter. Note, that the main overheads in single forward are from encoding the image via the ViT CLOC reuses it for multiple prompts. 3.4 DISCUSSIONS ON DESIGN CHOICES AND EXTENSIONS We provide discussions here on the rationale behind our design choices and some minor extensions. Extracting region embedding with visual prompts. To train our model with LCLOC in Equation 4, it requires extracting region embeddings from the image features given the bounding boxes. perhaps straightforward alternative could be Region-of-Interest (RoI) pooling/alignment (He et al., 2017) from the spatial image feature of ViT before pooling. RoI operations are popular, especially in the object detection literature.1 However, as will be evidenced by worse performance in Section 5, we found it suboptimal for CLOC pre-training here for several reasons. First, unlike object detection datasets that typically contain golden labels, here the pseudo-labels are much noisier on the large-scale web-crawled images. Therefore, the resulting RoI features may be inaccurate due to the imprecise bounding boxes, making model training less effective. Second, unlike dense vision tasks that directly rely on the spatial features, MLLM has transformer decoder that consists of several attention layers such that the constraint of semantics in the spatial feature space becomes somewhat indirect. Our Prompter mimics such inductive bias in pre-training via single-attention-layer encoder that may leverage better global context reasoning compared to RoIs. Avoiding region-text conflicts. While region annotations introduce location information, concern of contrastive learning on the regional captions may be that there are many similar objects within an image (e.g., boats in the harbor) or mini-batch. To mitigate such concerns, we apply two tricks. First, fortunately, we found it sufficient to sample few regions per image for each update, e.g., we 1We observe withdrawn arXiv preprint (https://arxiv.org/abs/2401.06397) proposes to extract RoI features for CLIP regional contrastive learning but only focus on dense vision tasks, not MLLMs."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Overview of the Visually-Enriched and Spatially-Localized (VESL) captioning pipeline. We leverage an existing open-vocabulary detector (e.g., OWLv2) that typically predicts bounding boxes on the images, and assigns the labels from the given text phrase candidates. Previous methods do not tailor how the text phrases are prepared and often use the alt-text attached to the images, which is prone to insufficient region descriptions. We found it crucial for CLOC to train on data from our VESL that re-captions images with the visually-enriched captioner VeCap (Lai et al., 2024) for better visual concept exploitation of the detector. Table 1: Region-text dataset statistics. We summarize the text token length for both images and regions. Partial statistics of the proprietary datasets revealed by their papers. The 20M subset of GRIT is released at: https://huggingface.co/datasets/zzliang/GRIT; we removed the invalid images. Dataset # of images regions per image image caption length region text length Flickr Entities (Plummer et al., 2015) RefCOCO (Yu et al., 2016) RefCOCO+ (Yu et al., 2016) RefCOCOg (Mao et al., 2016) Visual Genome (Krishna et al., 2017) GRIT (proprietary) (Peng et al., 2023) GRIT (released, clean) (Peng et al., 2023) Florence-2 (proprietary) (Xiao et al., 2024) OWLv2 (proprietary) (Minderer et al., 2024) WiT labeled w/ Minderer et al. (2024) VESL WiT (Ours) VESL WiT+DFN (Ours) 32K 20K 20K 27K 108K 91M 17M 126M 2B 300M 300M 2B 8.7 2.5 2.5 2.1 38.0 1.5 1.8 5.4 5.1 11.6 11.5 17.2 70.5 17.1 44.9 35.9 3.6 3.5 8.4 4.7 4.6 2.6 3.9 2.1 2.1 set = 4 in Equation 2 in experiments. Second, we can filter similar texts when computing the negatives in the contrastive loss. More specifically, we ignore the pairs of (cid:0)z(m) )(cid:1) in the )(cid:1) > 0.9, without gradients on fT . denominators of both LRT /T R, if sim(cid:0)fT (y(m) ), fT (y(m) , fT (y(m) j"
        },
        {
            "title": "4 VISUALLY-ENRICHED AND SPATIALLY-LOCALIZED CAPTIONING PIPELINE",
            "content": "As discussed in Section 1 and 3.1, key bottleneck of CLOC training is the region-text annotation datasets in terms of both the data size scales and the label quality, since there are no public datasets with region-text annotations at scales large enough for contrastive pre-training. Inspired by recent works that re-caption images with visually-enriched captions for better CLIP training, we make step further for Visually-Enriched and Spatially-Localized (VESL) labeler which generates more fine-grained captions at the region level. The goal of VESL is, given an"
        },
        {
            "title": "Preprint",
            "content": "image (possibly with the original web-crawled alt-text), annotate it with the grounded bounding boxes each associated with caption in natural language for optimizing Equation 2 in Section 3.2. Concretely, VESL is constructed as pseudo-labeling pipeline with the following steps: 1. Image re-captioning with visual concept exploitation: We follow the VeCap framework (Lai et al., 2024) to generate long, diverse, and detailed image captions. 2. Region phrase candidates extraction: Inspired by Zhang et al. (2022), we apply name entity recognition (NER) to extract leaf entities from the visually-enriched captions as potential candidate phrases describing region inside the image. 3. Open-vocabulary detection with extracted phrases: we generate the final region-text annotations via pre-trained open-vocabulary detector queried with the phrases extracted from Step 2 to match the bounding boxes proposed by the detector. We adopted the OWLv2 detector (Minderer et al., 2024) which contains the CLIP image/text encoder with the detection head. The boxes with detection confidence larger than 0.1 are kept as the region location and the phrases matched with the highest score are considered as their captions. Remarks. We highlight our insights behind the proposed recipe. The most relevant work was proposed in (Minderer et al., 2024) that scales up open-vocabulary (OV) detection via self-training. We are inspired by its success and extend it to CLOC contrastive learning with important modifications. Different from (Minderer et al., 2024) that generates candidate phrases from the n-grams of the web-crawled alt-text of the images for OV detection, we found the alt-text might not have enough details describing the image region content, thus limiting the diversity and quality of the annotations predicted by the OV detector. We thus caption each image augmented with more visual details. However, the long captions make the n-grams candidates verbose and grow exponentially, thus we generate high-quality candidates via name entity recognition instead. We found such pipeline produces training data more suitable for CLOC, as will be validated in Section 5. Our pre-training datasets. Our pre-training data consists of two parts: (i) image-text pairs, and (ii) region-text pairs. For image-text pairs, we reproduce the image re-captioning pipeline from VeCap (Lai et al., 2024), and generate synthetic captions for WiT-300M (Wu et al., 2024) and DFN-5B (Fang et al., 2023) images. For region-text pairs, we pseudo-label WiT-300M and 2Bimage subset of DFN-5B using our VESL pipeline. In VESL, we adopted the official OWLv2 L/14 model (Minderer et al., 2024) as the open-vocabulary detector.2 All images are pseudo-labeled with 448 448 resolution, where maximum number of 20 phrase queries are sampled for moderate computation budget. Table 1 summarizes the statistics of existing region-text datasets and ours. Notably, we also ablate annotating WiT-300M following Minderer et al. (2024) and found it detects less objects with longer region text, likely due to verbose n-grams of alt-text are in lower quality than our approach, as discussed in the remarks. Examples and pseudo codes are in Appendix B."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 SETUP Pre-training. We follow OpenAI-CLIP (Radford et al., 2021) to train both our CLIP baseline model and CLOC model using similar budget of around 14B images seen3. For fair comparison, we use the same hyper-parameters and images for both the CLIP baseline and CLOC. We experimented with the ViT B/16 and L/14 architectures, pre-trained with 224 224 and 336 336 image resolutions, respectively. All parameters are trained end-to-end from scratch. We implement the codebase based on open-source AXLearn framework4 in JAX (Bradbury et al., 2018). We provide hyperparameters and more details in Appendix A. Evaluation tasks. We evaluate our image encoders across wide range of downstream tasks. First, we assess performance on ImageNet image classification (Deng et al., 2009; Shankar et al., 2020) and COCO retrieval (Lin et al., 2014). Second, we construct region-level tasks, including COCO object recognition and region-text retrieval using the GRIT dataset (Peng et al., 2023). Furthermore, we show CLOC is particularly useful for MLLMs, validated by the Ferret model (You et al., 2023) 2OWLv2 CLIP L/14 ST+FT in: https://github.com/google-research/scenic/tree/main/scenic/projects/owl vit 3To avoid confusion, we will refer to CLIP as training with the standard CLIP loss on the same data as CLOC and refer to OpenAI-CLIP as the public pre-trained checkpoint released from Radford et al. (2021). 4https://github.com/apple/axlearn"
        },
        {
            "title": "Preprint",
            "content": "Table 2: Zero-shot evaluation on image-level tasks (accuracy of ImageNet (IN) classification, recall@1 of COCO retrieval) and region-level tasks (mAcc of region object recognition on COCO and LVIS, recall@10 of GRIT region retrieval), using ViT-B/16 as the default encoder backbone. The indentation with different symbols denotes removing () or changing component (). Training data Region tasks Image tasks Models Avg. Image Region COCO (i2t) COCO (t2i) INv1 INv2 GRIT (r2t) GRIT (t2r) COCO Recog. LVIS Recog. Image Region 1 OpenAI-CLIP 2 CLIP proprietary WiT+DFN - - 3 CLOC 4 5 WiT WiT Prompter WiT WiT VESL WiT WiT w/ GLIPv2 WiT WiT 8 CLOC 9 10 11 12 WiT+DFN WiT Prompter WiT+DFN WiT text filtering WiT+DFN WiT Lgrounding WiT+DFN WiT = 2 WiT+DFN WiT 52.4 66.3 68.8 67.0 53.9 68.8 66.1 65.8 65.4 66.0 66. 33.1 68.3 62.3 45.1 76.2 69.6 - - 50.1 66.7 59.7 65.1 49.7 65.6 58.6 44.8 36.3 66.6 59.5 71.5 50.0 65.8 59.2 67.9 46.5 75.5 68.6 65.8 46.5 75.7 68.0 55.5 46.0 75.7 68.4 66.3 46.3 75.7 67.9 66.0 46.2 75.5 67.9 66.5 13 CLOC 14 15 WiT+DFN WiT+DFN 69.2 Prompter WiT+DFN WiT+DFN 70.2 WiT+DFN WiT+DFN 65.3 VESL 49.3 74.9 67.0 63.9 49.7 74.7 67.6 65.7 46.6 75.5 67.7 55.7 16 17 ViT L/14 WiT+DFN WiT+DFN 74.8 ViT H/14 WiT+DFN WiT+DFN 75.7 54.4 80.1 73.2 66.9 55.1 81.3 74.7 67.4 - - 67.2 4.4 63.8 71. 67.4 18.4 66.5 66.8 67.0 65.9 23.0 22.3 68.3 69.4 - - 70.6 55.3 62.2 64.9 70.1 67.1 68.7 70.0 69. 71.1 67.1 66.3 72.9 73.0 - - 26.7 13.2 22.2 23.1 27.2 24.6 24.8 25.8 25.8 28.5 25.4 25. 32.6 35.6 54.0 64.3 61.3 60.2 54.1 61.0 64.2 64.0 63.9 64.0 64.1 65.1 65.6 63.8 70.6 71. - - 57.4 29.4 54.9 56.8 57.6 41.4 56.6 57.2 57.3 57.3 45.3 42.4 60.2 61.3 which requires fine-grained image understanding for referring and grounding tasks. We also evaluate on general multimodal benchmarks using LLaVA-1.5 (Liu et al., 2023) and LLaVA-NeXT (Liu et al., 2024)5, which both use the 7B Vicuna LLM. For all evaluation tasks, we use the same official hyper-parameters, fine-tuning datasets, and codebase for all the image encoders we experimented with, without specific tuning. More details are provided in each subsection and in Appendix A. 5.2 IMAGE AND REGION CLASSIFICATION AND RETRIEVAL TASKS The proposed CLOC training framework enables the encoder to produce not only image embedding but also region embeddings. It can directly be used for region-level tasks without further training, in analogy to the zero-shot capability of CLIP on images. To evaluate such capability and also for fast development and ablation study, we first construct several region-level zero-shot tasks. Besides image-level evaluation like ImageNet classification and COCO image-text retrieval, we additionally construct region-level tasks, including region object recognition and region-text retrieval. More specifically, the region-level tasks leverage the labeled bounding boxes in the evaluation set for CLOC to extract region embedding. For region retrieval, we use validation set of the GRIT dataset (Peng et al., 2023) and encode both the image regions and the region captions. For region classification, the class names are encoded as text embedding (80 / 1203 classes for COCO / LVIS, respectively), and the highest cosine similarity for each region embedding is predicted as its class. We highlight important variables for the performance in Table 2 with the following observations: CLOC performs decently on region-level tasks6 while maintaining strong performance on imagelevel metrics ( 2 vs. 8 13 ). The Prompter is an important ingredient for CLOCs success to go beyond CLIP without compromise ( 3 vs. 4 ; 8 vs. 9 ; 13 vs. 14 ). We replace the Prompter with RoI alignment to 5We use the codebase: https://github.com/xiaoachen98/Open-LLaVA-NeXT. 6For reference, in different data setup, Wu et al. (2023) reports 46.5% mAcc on the same COCO region classification task, trained using 320 320 COCO training images. In contrast, our approach achieves over 70% mAcc, pre-trained on 224 224 large-scale web-crawled dataset with object annotations (thus might not be fair comparisons)."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Results on Ferret-Bench for referring and grounding VQA, based on Ferret (You et al., 2023) equipped with different image encoders. Models are evaluated with OpenAI gpt-4o API instead of the deprecated gpt-4-0314 in the paper. replace Ferret visual sampler with Prompter; see Section 5.3 for details. Avg. ( to CLIP) # of images w/ region labels Referring Description Region Alignment Referring Reasoning Method ViT CLIP CLOC CLOC CLOC CLOC None B/16 B/16 RoI-Align B/16 Prompter B/16 Prompter B/16 Prompter None None OpenAI-CLIP L/14 L/14 CLIP L/14 Prompter CLOC L/14 Prompter CLOC CLOC L/14 Prompter None 300M 300M 2B 2B None None 300M 2B 2B 47.5 48.0 50.2 53.6 54.8 50.8 54.2 51.0 55.9 56.3 50.3 48.4 55.5 53.7 54.9 55.4 54.6 65.7 63.3 67. Grounding in Conversation 45.3 40.0 41.5 42.2 44.7 45.7 43.3 44.9 46.0 47.1 47.7 45.5 49.1 49.8 (+2.1) 51.5 (+3.7) 50.6 50.7 53.9 55.1 (+4.4) 56.9 (+6.2) extract region features and train with LCLOC. We found it performs much worse on region-level tasks than CLOC, possibly due to such strong constraints of RoI features being difficult to learn on the noisy labels with the CLIP loss as discussed in Section 3.4. VESL outperforms Minderer et al. (2024) baseline approach, as the visually-enriched captions improve image retrieval tasks (as expected (Lai et al., 2024)), while also offering versatile visual concepts as text candidates for the OV detector, supporting Section 4 ( 3 vs. 5 ; 13 vs. 15 ). Given the same captions in VESL, OWLv2 slightly outperforms the GLIPv2 detector ( 3 vs. 6 ). Tricks in Section 3.4 offer slight performance gains, but LCLOC is already highly effective on its own ( 10 11 ). Region tasks work well when sampling 2 or 4 boxes per image, making CLOC practical ( 12 ). Scaling up region labels seems saturated at 300M images on region tasks ( 3 8 13 ), while we found it will further improve in MLLM tasks as will be shown in Table 3. Scaling up the ViT model sizes can further improve both image and region tasks ( 13 16 17 ). Overall, CLOC not only achieves strong performance on image-level tasks, but unlocks new capability for zero-shot region-level tasks. We have validated our design choices for architectures, training, and data. Below, the complete setup 13 will be used as default if not specified. 5.3 REFERRING AND GROUNDING WITH FERRET As discussed in Section 1, key motivation is to provide an enhanced image encoder for training MLLMs, particularly for tasks requiring fine-grained image understanding. notable example is Ferret (You et al., 2023), recently proposed MLLM that builds on LLaVA and aims to handle more advanced spatial interactions, such as referring and grounding in VQA tasks. Ferret can take region prompts such as box, point, or free-form location referring to the input image as input, and answer question specific to the region such as Do you know when the object[region] was invented? Ferret thus requires fine-grained image features from the vision encoder for spatial reasoning. We evaluate CLOC by replacing the CLIP ViT encoder with our CLOC ViT as drop-in replacement. We follow the official codebase7 for training the Ferret model. We further consider variant based on Ferret: the Ferret model implements spatial-aware visual sampler that samples image features from the region specified in the question. We replace the sophisticated visual sampler with our simple Prompter introduced in Section 3.3 to extract region embedding with = Prompter(l, (x)) instead, as illustrated in Figure 1(right). In Table 3, we evaluate different pre-trained image encoders on the Ferret-Bench benchmark (You et al., 2023). Ferret-Bench includes challenging multimodal dialogue-style VQA of three tasks constructed with GPT-4. Results show that our Prompter is essential to improve upon the CLIP baseline RoI-Align may even slightly degrade performance. Scaling region labels from 300M to 2B further improves performance. Interestingly, our Prompter (denoted as ) can be replacement 7We use the official Ferret codebase: https://github.com/apple/ml-ferret."
        },
        {
            "title": "Preprint",
            "content": "Table 4: Results on referred object classification (LVIS), referring expression comprehension (0.5 IoU on RefCOCO, RefCOCO+, RefCOCOg), and phrase grounding (0.5 IoU on Flickr30k Entities) with Ferret. Shikra: baseline in Chen et al. (2023). Ferret: replace visual sampler with CLOC prompter. Model Encoder LVIS box point freeform RefCOCO RefCOCO+ RefCOCOg Flickr Avg. val testA testB val testA testB val test val test ( to CLIP) FERRET FERRET FERRET CLIP B/16 72.5 56.9 57.2 80.7 84.2 77.1 71.9 76.1 63.7 75.9 76.2 76.2 78.3 CLOC B/16 74.3 56.7 60.2 84.2 87.0 80.0 74.7 80.0 67.0 78.8 79.5 80.0 81.5 75.7 (+2.9) CLOC B/16 78.9 58.2 61.4 84.4 86.8 78.9 74.0 78.7 65.5 78.0 78.7 80.1 81.4 75.8 (+3.0) 72.8 Shikra OpenAI-CLIP L/14 57.8 67.7 n/a 87.0 90.6 80.2 81.6 87.4 72.1 82.3 82.2 75.8 76.5 FERRET OpenAI-CLIP L/14 79.4 67.9 69.8 87.5 91.4 82.5 80.8 87.4 73.1 83.9 84.8 80.4 82.2 78.7 66.9 70.2 88.0 90.4 83.5 80.1 85.8 73.3 82.8 83.4 79.0 80.1 CLIP L/14 FERRET CLOC L/14 81.6 67.9 69.9 89.0 91.0 84.7 81.4 86.8 74.7 84.0 85.2 82.3 83.3 81.7 (+1.5) FERRET CLOC L/14 79.8 67.9 69.1 88.2 91.1 84.5 80.6 86.7 73.9 84.8 85.1 82.4 83.5 81.4 (+1.2) FERRET - 80.8 80.2 Table 5: Results on multimodal benchmarks using LLaVA-1.5/NeXT with ViT-L/14 and Vicuna-7B. MLLM LLaVAW TextVQA GQA MM-Vet POPE MME-P MME-C Method CLIP CLOC LLaVA-1.5 LLaVA-1.5 CLIP Open-LLaVA-NeXT CLOC Open-LLaVA-NeXT 59.3 64. 67.3 69.5 53.3 54.9 61.4 61.9 62.2 62.7 63.5 64.2 30.0 31. 38.5 40.2 86.7 87.3 87.9 88.3 1451.4 1482.0 1486.1 1451.1 254.3 288. 279.6 312.5 of the FERRET visual sampler in fine-tuning, which is simpler and performs even better up to 6% against both the OpenAI-CLIP and our in-house CLIP. We also evaluate CLOC (2B labeled) on other referring and grounding tasks ranging from referring object classification, referring expression comprehension, and phrase grounding across multiple datasets. As summarized in Table 4, CLOC is also superior evidenced by 1 3% improvements in average of 13 evaluation sets. 5.4 GENERAL VQA WITH LLAVA-1.5 AND LLAVA-NEXT We further show that the CLOC encoder is also competitive against CLIP on general VQA tasks without regression and can even provide performance improvements. We use the Vicuna 7B LLM decoder for two experiments based on LLaVA-1.5 (frozen encoder) and Open-LLaVA-NeXT (unfrozen encoder with AnyRes (Liu et al., 2024) inputs). Since general VQA does not provide spatial referring inputs, we simply replace the ViT in LLaVA. Table 5 summarizes the results. Encouragingly, with our CLOC designs, the improved region-level alignment is also beneficial to some general multimodal benchmarks, as they may also require fine-grained image understanding."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Please see Appendix for more discussions where we comment on the limitations, future directions, computation cost, design rationales, etc. We tackle deficiency of CLIP, to make the semantics aligned in the vision space not only at the image level but also at the region level. We propose new pre-training framework that includes innovations in new learning formulation that strong encoder should be easily transformed in the foresee of downstream use of MLLMs. Our encoder creates new possibility for adapting the features with input prompts of interaction together with MLLMs. To resolve the need for large-scale region-text training data, we carefully design pseudo-labeling pipeline for visually-enriched and spatially-localized captions. Our pre-trained encoder is essentially drop-in replacement of CLIP, with competitive image-text performance, and extra capability demonstrated in region-text tasks and VQA tasks with MLLMs."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "We thank Yanghao Li, Felix Bai, and many others for their invaluable help and feedback."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We made our best efforts to exhaustively state the implementation details. Training hyper-parameters and model architectures are discussed in Section 3.2, 3.3, and 5.1, with summary in Appendix and Table A. For evaluation, as mentioned in Section 5.1, we strictly follow the official setup with the codebase released by the original authors if applicable, with details provided in Section A.2. For our datasets, we provide data processing details in Section 4 and example codes in Appendix B. We are working hard on releasing the annotations with internal approvals."
        },
        {
            "title": "REFERENCES",
            "content": "Ioana Bica, Anastasija Ilic, Matthias Bauer, Goker Erdogan, Matko Boˇsnjak, Christos Kaplanis, Improving Alexey Gritsenko, Matthias Minderer, Charles Blundell, Razvan Pascanu, et al. fine-grained understanding in image-text pre-training. arXiv preprint arXiv:2401.09865, 2024. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao JAX: composable transformations of Python+NumPy programs, 2018. URL http: Zhang. //github.com/jax-ml/jax. Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving clip training with language rewrites. Advances in Neural Information Processing Systems, 36, 2024. Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 29612969, 2017. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022. Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pp. 49044916. PMLR, 2021. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 40154026, 2023."
        },
        {
            "title": "Preprint",
            "content": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123:3273, 2017. Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In CVPR, 2023. Zhengfeng Lai, Haotian Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang, et al. Veclip: Improving clip training via visualenriched captions. In ECCV, 2024. Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, et al. What if we recaption billions of web images with llama-3? arXiv preprint arXiv:2406.08478, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In Computer Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740755. Springer, 2014. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https:// llava-vl.github.io/blog/2024-01-30-llava-next/. Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1120, 2016. Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. In ECCV, 2024. Matthias Minderer, Alexey Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. Advances in Neural Information Processing Systems, 36, 2024. Muhammad Ferjad Naeem, Yongqin Xian, Xiaohua Zhai, Lukas Hoyer, Luc Van Gool, and Federico Tombari. Silc: Improving vision language pretraining with self-distillation. arXiv preprint arXiv:2310.13355, 2023. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. ArXiv, abs/2306.14824, 2023. Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models. In Proceedings of the IEEE international conference on computer vision, pp. 26412649, 2015. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020."
        },
        {
            "title": "Preprint",
            "content": "Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S. Khan. Glamm: Pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1300913018, June 2024. Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel reasoning with large multimodal model. In CVPR, 2024. Vaishaal Shankar, Rebecca Roelofs, Horia Mania, Alex Fang, Benjamin Recht, and Ludwig Schmidt. Evaluating machine accuracy on ImageNet. In Hal Daume III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 86348644. PMLR, 1318 Jul 2020. Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Alpha-clip: clip model focusing on wherever you want. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1301913029, June 2024. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Xiangtai Li, Wentao Liu, and Chen Change Loy. Clipself: Vision transformer distills itself for open-vocabulary dense prediction. arXiv preprint arXiv:2310.01403, 2023. Wentao Wu, Aleksei Timofeev, Chen Chen, Bowen Zhang, Kun Duan, Shuangning Liu, Yantao Zheng, Jonathon Shlens, Xianzhi Du, Zhe Gan, et al. Mofi: Learning image representations from noisy entity annotated images. In ICLR, 2024. Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing unified representation for variety of vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4818 4829, 2024. Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu. Detclip: Dictionary-enriched visual-concept paralleled pre-training for openworld detection. Advances in Neural Information Processing Systems, 35:91259138, 2022. Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, and Hang Xu. Detclipv2: Scalable open-vocabulary object detection pre-training via word-region alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2349723506, 2023. Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. In ICLR, 2023. Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pp. 6985. Springer, 2016. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language In Proceedings of the IEEE/CVF International Conference on Computer image pre-training. Vision, pp. 1197511986, 2023. Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and visionlanguage understanding. Advances in Neural Information Processing Systems, 35:3606736080, 2022."
        },
        {
            "title": "Preprint",
            "content": "Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, Sam Dodge, Keen You, Zhen Yang, Aleksei Timofeev, Mingze Xu, Hong-You Chen, Jean-Philippe Fauconnier, Zhengfeng Lai, Haoxuan You, Zirui Wang, Afshin Dehghan, Peter Grasch, and Yinfei Yang. Mm1.5: Methods, analysis & insights from multimodal llm fine-tuning, 2024a. URL https://arxiv.org/abs/2409.20566. Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, TsuJui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, et al. Ferret-v2: An improved baseline for referring and grounding with large language models. In Conference on Language Modeling, 2024b. Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1679316803, 2022."
        },
        {
            "title": "A EXPERIMENT DETAILS",
            "content": "Below, we provide more details on pre-training hyper-parameters and downstream evaluation tasks. A.1 PRE-TRAINING HYPER-PARAMETERS For pre-training both the in-house CLIP baseline and CLOC, we mainly follow the hyper-parameters in Radford et al. (2021) with our in-house pre-training datasets. The images used for model training are identical for CLIP and CLOC, while CLOC is trained on the extra region-text annotations of the same images via the proposed VESL pipeline (details in Section B). Table summarizes the training hyper-parameters used for all experiments and the setup for components specific to CLOC. In terms of the CLOC architecture, as illustrated in Figure 2, the image and text encoders including the attention pooling and projection layers follow the same as OpenAI-CLIP (Radford et al., 2021). Our Prompter consists of positional encoding matrix for bounding boxes, and single-layer single-head transformer encoder with another set of the global average pooler and projection layer to map the region embeddings into the same dimension as the CLIP text/image embeddings. Table A: Pre-training hyper-parameters and settings for the in-house CLIP baseline and CLOC. General Batch size Image size Image pre-processing Text tokenizer Text maximum length Steps Optimizer Peak learning rate (LR) LR schedule Weight decay Dropout rate 32768 224 224 (ViT B/16) or 336 336 (ViT L/14, H/14) long-side resizing with padding (i.e., tf.image.resize with pad) T5 (Raffel et al., 2020), lowercase 77 tokens 439087 (i.e., 14B examples seen) AdamW (β1 = 0.9, β2 = 0.98) 0.0005 cosine decays with linear warm-up (first 2k steps) 0.2 0.0 CLOC maximum = 4 per image λ = 1.0 #of images contain region text in the mini-batch # of sampled regions CLOC loss weight Encoding box prompts Encoding text prompts Prompter architecture single-layer single-head transformer encoder (same feature dimension as the ViT) BoxHead architecture sinusoidal positional encoding of coordinates (top-left and bottom right of box) encoded by re-using the text encoder (w/ pooling & projection) 2-layer MLP with GELUs activations (Hendrycks & Gimpel, 2016) (in Equation 4) batch size A.2 EVALUATION TASKS We provide more details on the tasks constructed for evaluating the encoders in Section 5. Zero-shot region tasks. Our CLOC training augments new capability for CLIP to generate region-level embeddings. This enables us to perform zero-shot region-text tasks, in analogy to the image-text zero-shot tasks like ImageNet classification and COCO image-text retrieval that CLIP has been evaluated on. In similar rationale of image-level evaluation, we further construct region-level tasks including region object recognition and region-text retrieval. For region object recognition, the class names are encoded by the text encoder into class embedding. We do not add the text prompts (e.g., photo of ...) to object classes used when CLIP (Radford et al., 2021) evaluated on image classification. The CLOC model takes all the labeled bounding boxes in the images to generate region embedding"
        },
        {
            "title": "Preprint",
            "content": "z = Prompter(l, fI (x)). The class embedding with the highest similarity is predicted as the class of the region (i.e., out of 80 / 1203 classes for COCO / LVIS). For region retrieval, similarly, the CLOC model encodes both the image regions and the region captions from the public region-text GRIT dataset that the regions are annotated by the Kosmos-2 pipeline (Peng et al., 2023). We randomly sampled 2K image validation set for fast evaluation. We have verified it is statistically stable compared to the whole set that contains about 20M in total. Unlike image-text retrieval where the image captions are likely unique, the objects in regions of many images might be duplicated. Therefore, we opt to report recall@10 rather than recall@1 for GRIT region retrieval in Table 2. MLLM tasks. To demonstrate our CLOC can benefit MLLM downstream tasks as better image backbone, we consider two sets of MLLM experiments. First, we experiment with Ferret (You et al., 2023) that is capable of taking spatial referring inputs for grounding and referring VQA tasks. Ferret can consume point, bounding box, or free-form referring. It designs complicated visual sampler module that involves point sampling and kNN grouping. We suggest the readers refer to Figure 3 and Section 3.2 in You et al. (2023) for more details. Here, we consider two methods to use CLOC for Ferret model training: (i) we only take the ViT encoder in CLOC to replace the CLIP ViT and still use the original Ferret visual sampler; (ii) we further replace the visual sampler with our simple Prompter (essentially lightweight transformer encoder with box positional encodings) in Section 3.3 as illustrated in Figure 1(3b). More specifically, we simply convert all types of spatial referring as boxes. As evidenced by Table 3 and Table 4, our Prompter can indeed be much simpler alternative and may perform even better as it is more consistent with CLOC pre-training. Second, we evaluate on general VQA tasks that do not consider extra spatial referring inputs. The pre-trained ViT of CLOC is drop-in-replacement of CLIP ViT in two sets of experiments of LLaVA-1.5 (Liu et al., 2023) and LLaVA-NeXT (Liu et al., 2024). The main difference includes different supervised fine-tuning (SFT) datasets. Also, LLaVA-NeXT uses the AnyRes technique that decomposes an image into several subimages that are encoded independently with the ViT and concatenated together as the input for the decoder. LLaVA-1.5 by default freezes the ViT, while LLaVA-NeXT fine-tunes all parameters during SFT. Since the official LLaVA-NeXT is trained on some proprietary datasets that are not reproducible, we use the Open-LLaVA-NeXT repository.8 Our experiments in Table 5 demonstrate CLOC not only improves general VQA tasks besides referring and grounding tasks, but also generalizes well for both LLaVA-1.5 and LLaVA-NeXT settings."
        },
        {
            "title": "B VESL DATA ENGINE",
            "content": "We provide more details about our pseudo-labeling data pipeline proposed in Section 4. B.1 IMPLEMENTATION DETAILS As already mentioned in Section 4, there are three steps for VESL: image re-captioning, region phrase candidates extraction from the captions, and open-vocabulary (OV) detection given the region candidates as queries. For re-captioning, the goal is to replace alt-text with long, diverse, and detailed captions that can be used to generate more visual concepts as the region candidate phrases for the OV detector. Technically, any strong image captioner can be an option. In our paper, we adopt the VeCap pipeline (Lai et al., 2024) and leverage their images with enriched captions. To extract region phrase candidates from the long captions, we adopt name entity recognition (NER) to extract leaf entities from the captions, inspired by Zhang et al. (2022). The code listing below shows the Python example implementation, where stop words and common generic words are filtered, following Minderer et al. (2024). Generating bounding boxes and assigning region captions can be done by querying an OV objection detector. We adopted the OWLv2 detector (Minderer et al., 2024) with their pre-trained L/14 checkpoint to annotate inputs with 448 448 image resolutions. 8https://github.com/xiaoachen98/Open-LLaVA-NeXT"
        },
        {
            "title": "Preprint",
            "content": "\"alibaba\", \"aliexpress\", \"amazon\", \"available\", \"background\", \"blog\", \"buy\", \"co\", \"com\", \"description\", \"diy\", \"download\", \"facebook\", \"free\", \"gif\", \"hd\", \"ideas\", \"illustration\", \"illustrations\", \"image\", \"images\", \"img\", \"instagram\", \"jpg\", \"online\", \"org\", \"original\", \"page\", \"pdf\", \"photo\", \"photography\", \"photos\", \"picclick\", \"picture\", \"pictures\", \"png\", \"porn\", \"premium\", \"resolution\", \"royalty\", \"sale\", \"sex\", \"shutterstock\", \"stock\", \"svg\", \"thumbnail\", \"tumblr\", \"tumgir\", \"twitter\", \"uk\", \"uploaded\", \"vector\", \"vectors\", \"video\", \"videos\", \"wallpaper\", \"wallpapers\", \"wholesale\", \"www\", \"xxx\", \"youtube\" \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"did\", \"do\", \"does\", \"doing\", \"don\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"just\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"now\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"s\", \"same\", \"she\", \"should\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\" 1 from typing import Iterable, List 2 import nltk 3 4 # STOPWORDS_EN and COMMON_GENERIC_WORDS are following: 5 # Section A.2 (Minderer et al., 2024) 6 7 # Stopwords from nltk.corpus.stopwords.words(\"english\"): 8 STOPWORDS_EN = frozenset({ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 }) 24 25 # These words were found by manually going through the most common 1000 words 26 # in sample of alt-texts and selecting generic words without specific meaning: 27 COMMON_GENERIC_WORDS = frozenset({ 28 29 30 31 32 33 34 35 36 37 }) 38 39 40 def _is_all_stopwords(query_words: Iterable[str]) -> bool: return set(query_words).issubset(STOPWORDS_EN) 41 42 43 44 def _get_name_entities(words: List[str]) -> List[str]: 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def find_noun_phrases( 65 66 ) -> List[str]: 67 68 69 70 71 72 73 74 candidate_quries = find_noun_phrases(caption) \"\"\" Returns name entities of image caption as queries, similar to GLIP. \"\"\" pos_tags = nltk.pos_tag(words) grammar = \"NP: {<DT>?<JJ.*>*<NN.*>+}\" cp = nltk.RegexpParser(grammar) result = cp.parse(pos_tags) caption = caption.lower() tokens = nltk.word_tokenize(caption) # Remove common generic words. words = [w for in tokens if not in COMMON_GENERIC_WORDS] queries = _get_name_entities(words)[:max_num_queries] return queries query_words = [t[0] for in subtree.leaves()] # Dont use it if it only consists of stop words. if _is_all_stopwords(query_words): queries = [] for subtree in result.subtrees(): caption: str, max_num_queries: int = 20, queries.append(\" \".join(query_words)) if subtree.label() == \"NP\": return queries continue Listing 1: Python example code for Step 2 of VESL in Section 4 for extracting text candidate queries from caption."
        },
        {
            "title": "Preprint",
            "content": "B.2 MORE VISUALIZATIONS As mentioned in the remarks of Section 4, we found the alt-text sourced from the original webcrawled images might not contain enough details describing the regional content, thus limiting the diversity and quality of the text candidate queries for the OV detector to detect more meaningful objects. In Figure A, we show some cherry-picked examples (since the web-crawled images are quite noisy) to demonstrate the reasons why high-quality captions can help our region-text annotation pipeline. In Minderer et al. (2024), the queries are generated by the n-grams of the alt-text, while ours are by NER as described in Section B.1 on top of the visually-enriched captions. Note that, in both methods we use the same OV detector but with different approaches to generate the queries. As shown in Figure A, for simple images like in the first row, both methods are doing reasonably well to detect message card. However, when the scene becomes complicated (e.g., the second row), our methods can detect more objects since more visual concepts can be extracted from our rich caption as queries for the detector. Similarly, it can be seen that our method captures more items that the alt-text missed, e.g., banana, eggs, butter, etc. in the third row; drawstring in the fourth row; apples and vases in the last row. Also, it is more likely to extract more detailed description of the region rather than class name, such as green-roofed cottage nestles in Figure 3 and decorative metal tree sculptures for the image in the last row of Figure A. We believe such high-quality region labels essentially contribute to better supervision for CLOC pre-training."
        },
        {
            "title": "C MORE DISCUSSIONS",
            "content": "Limitations. One limitation for CLOC is the labeling efforts in preparing the training data. As we discussed in Section 1, there are no public large-scale region-text datasets since it is expensive to infer such labels up to the scales we consider here. Unlike previous work (Zhong et al., 2022) that cropping boxes from images for annotating, our VESL pipeline conducts inference in image-level, thus the cost does not scale with the number of detected regions. With that being said, such inference still requires hundreds of GPUs running in parallel for days to scale up to billions of images. We are working on releasing the annotations to accelerate future research for the community. For CLOC, we focus on the training objective and framework formulation, while making minimal efforts on hyper-parameter tuning, architecture search, dataset cleaning, and etc., thus better performance could be achieved. Besides, although we have included extensive standard evaluation tasks, the fine-grained region knowledge could also be useful on more under-explored tasks. Future directions. We suggest promising future directions. In Section 3.2, our Prompter formulation can take flexible prompts to guide the embeddings for specific tasks. In this work, we consider prompt as single bounding box or text caption, but it has the potential to expand to various types such as points, mask, users free-form referring, or multiple prompts in multiple types together. We believe more versatile Prompter with co-designs for different objectives can have big potential. Similarly, our VESL labeling pipeline limits to the detection box format. Annotators supported for more formats may further boost it. We believe our approach is promising, as more attention has been drawn recently for better image re-captioning (Li et al., 2024; Fan et al., 2024) that VESL relies on. In addition, CLOC provides new capability to extract region features without further training, thus can be used as foundation model for exploring new VL applications. Training cost. We comment on the computation cost of our framework. Our large models (ViT L/14) were trained on 1024 v5p TPUs for about 6 days. To optimize Equation 2, CLOC needs extra computation. The main overheads come from the contrastive matrix but not the lightweight Prompter. Fortunately, we found it feasible since (i) only few boxes in each image need to be sampled per update; (ii) the loss computation becomes smaller proportion when the ViT scales up. Overall, we found the computation acceptable compared to CLIP. More memory-efficient optimization like SigLIP (Zhai et al., 2023) can be implemented with JAX shard map9 ops. Discussions on design rationales. Besides the main discussions we have stressed in the main text, here we provide more thoughts behind our design rationales that reader may be wondering. (1) Why not use other locality-enhanced image encoders? We would like to note that many image encoders with localization capabilities like DINOv2 (Oquab et al., 2023), OWLv2 (Minderer et al., 9https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html"
        },
        {
            "title": "Preprint",
            "content": "2024), CLIP-Self (Wu et al., 2023) etc. are developed specifically for dense vision tasks that cannot perform zero-shot image-level tasks like CLIP and CLOC. Our goal is to develop drop-in replacement for the CLIP encoder that improves localization while maintaining all of CLIPs original strengths, such as zero-shot image-level tasks and its critical role as the image backbone for MLLMs. Furthermore, perhaps well-known within the MLLM community, these encoders have been shown in recent reports that they are not comparable enough to compete with CLIP as the vision backbone for MLLM tasks (Tong et al., 2024) due to CLIPs superiority in vision-language alignment. We thus believe enhancing CLIP itself is more demanding as this paper focuses on. (2) Why not just train CLIP with object detection? One may wonder why we do not just train an encoder with joint optimization of the CLIP contrastive loss with some object detection loss instead of the CLOC design of Equation 4. Although it sounds like plausible approach, we would like to point out that contrastive pre-training and object detection are fundamentally different in their technical rationales. CLIP pre-training is often conducted on large batches of low-resolution and noisy images, while object detection is trained on small batches of high-resolution images. CLIP is by default trained from scratch and object detection is typically initialized from pre-trained encoders and focuses on the detection head. Furthermore, detection requires heavy computation on box proposals to detect all boxes appearing in an image, while our region-text contrastive design allows us to flexibly sample fewer regions per image as motivated in Equation 3. Overall, their data pipeline and distributed training setup are not on the same scale thus such joint training may not be very reasonable. With that being said, some previous works do have attempts that are the exceptions but only for some but not all of the mentioned aspects, and mainly for the purpose of detection. For instance, DetCLIP-v2 (Yao et al., 2023) adds image-text contrastive loss into detection loss to improve openvocabulary capability for detection. OWLv2 pre-trains the detector with rather small resolutions but still with batch size of maximum 256 since each image will need to predict up to 100 boxes during training. Both DetCLIP-v2 and OWLv2 are fine-tuned from pre-trained encoder. On the contrary, we study pre-training the image encoder from scratch, which may be complementary to the previous efforts. CLOC maximizes the similarity in co-design with CLIP, thus making it much easier to develop within the same codebase. (3) Do we really need to train CLOC from scratch? What if we fine-tune from CLIP? As CLIP pre-training is expensive, one may wonder if it is necessary to train from scratch on the proposed region-text datasets, or if we can initialize from standard CLIP trained on image-text pairs only and fine-tunes with CLOC for shorter stage. Our early investigation, even with extensive hyperparameter tuning, suggests it is likely to be suboptimal compared to training from scratch directly. For instance, we initialize from the CLIP model 2 in Table 2 and fine-tunes it for another extra 100K steps with the CLOC training loss Equation 4. The model reaches 64.1%/19.1% mAcc on COCO/LVIS region recognition, which is much worse than 70.1%/27.2% of the model 8 trained from scratch, even with more overall training steps."
        },
        {
            "title": "Preprint",
            "content": "Figure A: Examples comparing our VESL and the labeling approach in Minderer et al. (2024) that directly uses the n-grams of the crawled alt-text. For VESL, each image is annotated with the visual-enriched caption to replace the alt-text, which is used to generate region text candidates that capture the image content better."
        }
    ],
    "affiliations": [
        "Apple"
    ]
}