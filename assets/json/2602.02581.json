{
    "paper_title": "QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals",
    "authors": [
        "Nan Zhang",
        "Eugene Kwek",
        "Yusen Zhang",
        "Muyu Pan",
        "Suhang Wang",
        "Prasenjit Mitra",
        "Rui Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Weight-only quantization is important for compressing Large Language Models (LLMs). Inspired by the spirit of classical magnitude pruning, we study whether the magnitude of weight updates during reasoning-incentivized fine-tuning can provide valuable signals for quantizing Large Reasoning Models (LRMs). We hypothesize that the smallest and largest weight updates during fine-tuning are more important than those of intermediate magnitude, a phenomenon we term \"protecting both ends\". Upon hypothesis validation, we introduce QuantLRM, which stands for weight quantization of LRMs via fine-tuning signals. We fit simple restricted quadratic functions on weight updates to protect both ends. By multiplying the average quadratic values with the count of zero weight updates of channels, we compute channel importance that is more effective than using activation or second-order information. We run QuantLRM to quantize various fine-tuned models (including supervised, direct preference optimization, and reinforcement learning fine-tuning) over four reasoning benchmarks (AIME-120, FOLIO, temporal sequences, and GPQA-Diamond) and empirically find that QuantLRM delivers a consistent improvement for LRMs quantization, with an average improvement of 6.55% on a reinforcement learning fine-tuned model. Also supporting non-fine-tuned LRMs, QuantLRM gathers effective signals via pseudo-fine-tuning, which greatly enhances its applicability."
        },
        {
            "title": "Start",
            "content": "QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals Nan Zhang 1 Eugene Kwek 1 Yusen Zhang 1 Muyu Pan 1 Suhang Wang 1 Prasenjit Mitra 2 Rui Zhang"
        },
        {
            "title": "Abstract",
            "content": "Weight-only quantization is important for compressing Large Language Models (LLMs). Inspired by the spirit of classical magnitude pruning, we study whether the magnitude of weight updates during reasoning-incentivized fine-tuning can provide valuable signals for quantizing Large Reasoning Models (LRMs). We hypothesize that the smallest and largest weight updates during fine-tuning are more important than those of intermediate magnitude, phenomenon we term protecting both ends. Upon hypothesis validation, we introduce QuantLRM, which stands for weight quantization of LRMs via fine-tuning signals. We fit simple restricted quadratic functions on weight updates to protect both ends. By multiplying the average quadratic values with the count of zero weight updates of channels, we compute channel importance that is more effective than using activation or second-order information. We run QuantLRM to quantize various fine-tuned models (including supervised, direct preference optimization, and reinforcement learning fine-tuning) over four reasoning benchmarks (AIME-120, FOLIO, temporal sequences, and GPQA-Diamond) and empirically find that QuantLRM delivers consistent improvement for LRMs quantization, with an average improvement of 6.55% on reinforcement learning fine-tuned model. Also supporting non-fine-tuned LRMs, QuantLRM gathers effective signals via pseudo-fine-tuning, which greatly enhances its applicability. 6 2 0 2 1 3 ] . [ 1 1 8 5 2 0 . 2 0 6 2 : r 1. Introduction Weight-only quantization is important for AI efficiency, as the compression of Large Language Models (LLMs) produces static and efficient models for downstream optimization. According to recent analysis (Zhang et al., 2025; 1The Pennsylvania State University 2Carnegie Mellon University Africa. Correspondence to: Nan Zhang <njz5124@psu.edu>, Rui Zhang <rmz5227@psu.edu>. Figure 1. Overview of channel importance computation. To protect weight updates on both ends of supervised (SFT), direct preference optimization (DPO), and reinforcement learning (RL) fine-tuning pipelines, we fit restricted quadratic functions on LRMs weights and count the number of zero weight updates of each input channel. The input channel importance is the elementwise product of the average quadratic score and the count of zero updates. In practice, we increment the count of zero updates by 1 for each channel, as discussed in Section 3.2. Liu et al., 2025), existing post-training quantization (PTQ) methodologies such as AWQ (Lin et al., 2024) were originally designed for general-purpose LLMs and are less competitive on low-bit quantization of large reasoning models (LRMs) such as W3A16. As popular LRMs have undergone extensive post-training that heavily involves fine-tuning (Olmo et al., 2025; Yang et al., 2025; Guo et al., 2025), their fine-tuning traces are wasted during PTQ. Avoiding quantization-aware training (QAT), current PTQ leverages signals such as activation distribution (Lin et al., 2024) or second-order information (Frantar et al., 2022) to provide an efficient pipeline. However, the rich information encoded by training processes is largely ignored. It is valuable to investigate the utility of fine-tuning traces (e.g., weight updates) for PTQ, as these traces can imply weight importance. For example, the weight that undergoes the largest update can be important for the downstream task, because it encodes the richest fine-tuning traces (Goel et al., 2025). Preprint. February 4, 2026. Inspired by classical magnitude pruning that treats the abso1 QuantLRM lute value of weight as its importance (Han et al., 2015), we study whether the weight updates during reasoningincentivized fine-tuning can provide effective signals for quantizing LRMs. We first find that the magnitude of weight updates alone is not effective, which indicates the insufficiency of purely relying on large magnitude of updates. We then hypothesize that both the smallest and largest updates provide the densest information (protecting both ends), since weights with extremely small updates might be important for general capabilities of LRMs (e.g., linguistic and instruction-following capabilities). We validate this hypothesis by empirical experimention and therefore compute channel importance by multiplying the average quadratic mapping scores by the count of 0 weight updates as shown in Figure 1. This computation protects weight updates on both ends while also emphasizing those that are zero. Following our hypothesis validation, we propose QuantLRM1, which stands for weight quantization of LRMs via fine-tuning signals. We inject computed channel importance scores into the quantization loss function as scaling factors and search for the optimal parameter to minimize the loss. With minimal adaptation on different models, QuantLRM supports mainstream LRMs with state-of-the-art reasoning performance after quantization. In cases where an LRM is not explicitly fine-tuned (e.g., there is no public pre-fine-tuned checkpoint), we show that effective signals can be obtained via simplified fine-tuning pipeline, which greatly enhances the applicability of QuantLRM. We run QuantLRM to quantize various fine-tuned LRMs including supervised, direct preference optimization (DPO), and reinforcement learning (RL) fine-tuning over four reasoning benchmarks: AIME-120 (Mathematical Association of America), FOLIO (Han et al., 2024), temporal sequences of BIG-Bench Hard (Suzgun et al., 2022), and GPQADiamond (Rein et al., 2024). We conduct comprehensive set of experiments over R1 distillation models (Guo et al., 2025), Olmo 3 (Olmo et al., 2025), and Qwen 3 (Yang et al., 2025). We find QuantLRM consistently outperforms the strongest PTQ baselines on W3A16, achieving an average improvement of 6.55% on RL fine-tuned model and at least 1.65% on several supervised fine-tuned (SFT) LRMs. These improvements are particularly promising, since QuantLRM uses one of the smallest calibration datasets. Compatible with vLLM (Kwon et al., 2023) and the AWQ kernel (Lin et al., 2024), QuantLRM offers comparable speedup to state-of-the-art PTQ. Our contributions are: (1) We hypothesize and empirically verify that the smallest and largest weight updates during fine-tuning are more important than those of intermediate 1Our code is at https://github.com/psunlpgroup/ QuantLRM. magnitude, finding that is crucial for quantization; (2) We propose QuantLRM that fit simple restricted quadratic functions to protect weight updates on both ends during quantization; (3) We conduct comprehensive set of experiments to demonstrate the state-of-the-art performance of QuantLRM on LRMs quantization, with significant improvements across wide range of reasoning benchmarks. 2. Related Work Post-Training Quantization (PTQ). Without retraining, PTQ often requires small calibration dataset to reach quantization decisions via forward passes only. As recent representative approaches, AWQ uses activation distribution (Lin et al., 2024); GPTQ uses approximate second-order information (Frantar et al., 2022); GPTAQ designs optimized calibration between the outputs of quantized layers and fullprecision model (Li et al., 2025); and ANY4 (or ANY3 for 3-bit) computes non-uniform 4-bit numeric representation (Elhoushi & Johnson, 2025). drawback is that all these recent methods are originally designed for general-purpose LLMs, not LRMs. Recent benchmarking analyses (Zhang et al., 2025; Liu et al., 2025) establish that although they achieve almost lossless performance on 4-bit LRMs, their scores on 3-bit LRMs are suboptimal. Selecting all of them as baselines, we demonstrate that fine-tuning signals yield significantly better performance on 3-bit quantization; an important result if we want to deploy 3-bit quantization and reap its benefits. Fine-Tuning Traces in LLMs. Researchers mainly study the traces left by fine-tuning for interpretation and safety purposes. For example, understanding weight updates in natural language makes interpreting training processes easier (Goel et al., 2025). Fine-tuning BERT-size model (Devlin et al., 2019) predominantly alters low-dimensional subspace of the models parameters (Zhang et al., 2023), which implies weight importance. However, this method does not provide fine-grained importance scores used for weight-only compression. Moreover, it is computationally more expensive than QuantLRM when applied to LRMs due to its adoption of singular value decomposition. Truthfulness and refusal behaviors are encoded in specific weight updates (Du et al., 2025), which reinforces our motivation of using fine-tuning signals. Regarding safety, works have been done to detect whether fine-tuned model is pursuing unintended objectives by training with hidden behaviors (Marks et al., 2025) and to determine if model has been adversarially fine-tuned by an auditing agent (Egler et al., 2025). Gather fine-tuning signals from these works can be costly. So our methodology is designed based on simple heuristic of protecting both ends. QuantLRM Table 1. Hypothesis validation. We validate our hypothesis by reporting results using mixed-precision quantization. We show the performance of quantizing R1-Distill-Qwen-32B to 3-bit while keeping 5% of weights in 16-bit based on various protection signals. The highest scores are bold. The best protection signal is SFT both ends. Model Variants Accuracy Model Protection Signal AIME-120 FOLIO Temporal GPQA-Diamond Avg R1-Distill-Qwen-32B Activation on AWQ calibration R1-Distill-Qwen-32B Activation on reasoning calibration R1-Distill-Qwen-32B SFT R1-Distill-Qwen-32B SFT mid R1-Distill-Qwen-32B SFT both ends 44.2 40.8 34.2 38.3 49.2 73.9 72.4 70.4 72.4 77.8 99.6 100.0 99.2 99.6 99.6 56.1 54.0 50.5 50.0 54. 68.45 66.80 63.58 65.08 70.28 3. Methodology We validate our hypothesis of protecting weight updates on both ends when computing channel importance. We show how the importance scores can be used for quantization and necessary adaptation needed for different LRMs with diverse sizes. 3.1. Key Idea Hypothesis We hypothesize that signals during reasoningincentivized fine-tuning can indicate weight importance for quantization. This is inspired by the spirit of classical magnitude pruning (Han et al., 2015), where weight magnitude determines importance. Formally, given fine-tuned LRM with weight matrices as linear layers and its pre-finetuned model with weight matrices W, we use Wmℓ to denote the weight updates on each linear module (e.g., self-attention query projection) at layer ℓ: Wmℓ = mℓ Wmℓ (1) Similarly, wmℓ denotes the update of an individual weight. We believe that Wmℓ strongly correlates to weight importance. Hypothesis Validation To validate our hypothesis, we perform proof-of-concepts experiments in Table 1. We evaluate ideas via mix-precision quantization: protect fraction of weights based on channel importance in 16-bit while performing round-to-nearest 3-bit quantization on the rest of weights in linear layers. On channel (input or output channel) of module at layer ℓ, channel importance Ic mℓ is defined as (cid:88) Ic mℓ = 1 ic mℓ (2) where there are individual weights in and ic spond to individual importance score. mℓ correWe first check whether higher wmℓ in means higher ic mℓ (SFT in Table 1). In other words, we test whether the absolute values of weight updates can guide quantization decisions. We present the performance scores in Table 1. 3 As baseline, we use activation for importance computation. Suppose the length of sequence is . The channel importance is defined as Ic mℓ ="
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) (ac mℓ)2 (3) We show the performance of using either the AWQ calibration set (not reasoning-centric) or the reasoning chains of other models as the calibration data (details in Section 4.1). We see that directly using the weight updates yields lower performance than using activation, and that using activation on the AWQ calibration achieves the highest overall scores among the three settings. This indicates the need to design protection signals beyond the direct use of weight update. As an improved version, we then design mapping function to assign high importance scores to both the smallest and largest weight updates (SFT both ends). The reason is that small updates from fully optimized fine-tuning pipelines may indicate high importance with respect to models general capabilities (Zhang et al., 2024). In contrast, we also reflect the mapping function of SFT both ends to protect intermediate magnitudes, protection signal we refer to as SFT mid. As shown in Table 1, our SFT both ends method yields significantly better scores than SFT mid and other activation-based signals, with an average improvement of 1.83% over the strongest signal while protecting only 5% of weights. This improvement validates that strongly correlates to weight (or channel) importance and protecting both ends of weight updates is effective. The suboptimal scores of protecting intermediate magnitudes further indicate that updates on both ends are more important. In Section 5.2, we extend this experiment on more protection signals and other LRMs to demonstrate the effectiveness of our design. 3.2. Our Method Designing the mapping function of SFT both ends, we set ic mℓ), where () maps the weight change wc mℓ = (wc mℓ to its importance score ic mℓ on input channel c. QuantLRM modified (w) as (w) = ymin, ymin + (cid:0)ymax ymin (cid:1) ymin + (cid:0)ymax ymin (cid:1) (cid:18) wmid wmid wmin (cid:18) wmid (cid:19)2 (cid:19)2 wmax wmid = 0, , 0 < wmid, , > wmid. (5) After assigning ymin to zero updates, we further add stronger design to better protect zero updates (since they lie at the far left): counting the number of zero updates within channel. The reason is that channel generally has much more zero updates than ymax, when the default value of ymax is 10. Counting their occurrences in addition to fitting the two restricted quadratic functions enables us to obtain higher Ic mℓ when channel contains many of them. As result, we calculate the number of zero updates within channel as Zc mℓ based on (w) in Equation (5) as mℓ, and we compute Ic Ic mℓ = (cid:32) 1 (cid:88) n= (cid:33) (cid:17) (cid:16) wc,(n) mℓ (Zc mℓ + 1) (6) We add Zc mℓ by 1 to avoid the cases when Zc mℓ = 0. This approach of protecting both ends is outlined in Figure 1, with the best overall scores in Table 1. Injecting Fine-Tuning Signals. After finalizing channel importance in Equation (6), we need to apply it to quantization. We choose to follow AWQ scaling due to its flexibility to incorporate different importance scores. Specifically, scaling factors are searched to minimize quantization loss. For input feature and fine-tuned weights (when is not available, we use W; see Section 5.3 for details), we have the quantization loss: L(s) = (cid:13) (cid:13) (cid:13)Q(cid:0)W diag(s)(cid:1)(cid:16) diag(s)1 (cid:17) WX (cid:13) (cid:13) (cid:13) To inject our fine-tuning signals, we set: = (cid:0)Imℓ (cid:1)α , α = arg min α L(cid:0)(cid:0)Imℓ (cid:1)α(cid:1) Figure 2. Injecting channel importance Imℓ and searching for optimal α to control the strength of scaling. QuantLRM finds the best scaling factors that incorporate fine-tuning signals. Protecting Both Ends of w. quadratic function provides simple and effective mappings that assign weight updates on both ends with high importance scores, which becomes our choice. We also observe that all updates across different modules and layers, (cid:83) m,ℓ Wmℓ, are unevenly distributed, with many updates clustered at low magnitudes. This suggests to treat the left and right ends of the spectrum separately. We then find the median of all the updates wmid and fit two simple restricted quadratic functions. We denote the global minimum update as wmin and the maximum update as wmax. Denoting the output of our mapping function as y, we set ymin = 1 and ymax = 10 as the default choices. Our mapping function consists of two restricted quadratic functions defined as follows: (w) = ymin + (cid:0)ymax ymin (cid:1) (cid:18) wmid wmid wmin (cid:18) wmid , wmid, (cid:19)2 (cid:19)2 (cid:1) ymin + (cid:0)ymax ymin wmax wmid , > wmid (4) where we can derive that (wmid) = ymin and (wmin) = (wmax) = ymax. mℓ = (wc mℓ), we apply Equation (2) to get channel importance for protecting both ends. Setting ic Incorporating Zero Weight Updates. We further observe that wmin = 0 is common for fine-tuned LRMs, as zero weight updates can account for more than 1% of the total. Due to the popularity of zero updates, excluding them from Equation (4) can provide more fine-grained mappings for other updates. Thus, we decide to handle = 0 separately and find wmid on positive updates only. We exclude zero weight updates when fitting the two restricted functions in Equation (4), so the left restricted function has the range 0 < < wmid. Formally, we have Here we drop in Ic mℓ to indicate that Imℓ is vector consisting of all channel importance scores of weight matrix. We search the best α that minimizes the loss function (20 candidates in toal) over the interval of [0, 1]. Figure 2 shows an overview of the injection and search. Since Imℓ is precomputed before the search of α, we go through every target module iteratively to finalize α and save the corresponding s. The actual quantization is then performed by scaling the weights with and applying the standard quantization function. 4 QuantLRM 4. Experiment Setup 4.1. Datasets To demonstrate the effectiveness of QuantLRM, we select four reasoning benchmarks: AIME-120 (Mathematical Association of America) for testing mathematical reasoning on 120 complex questions from 2022 to 2025, FOLIO (Han et al., 2024) for testing logical reasoning, temporal sequences of BIG-Bench Hard (Suzgun et al., 2022) for testing temporal reasoning, and GPQA-Diamond (Rein et al., 2024) for testing scientific reasoning across multiple STEM domains. They cover wide range of reasoning tasks with various difficulty levels. We measure accuracy of LRMs on each benchmark and report the average score (Avg in tables). We use combination of AIME 2021 (Veeraboina, 2023) and 100 randomly sampled instances from the FOLIO training set as validation data. Regarding calibration, we stick to the default calibration set of each baseline. We use the same calibration set for QuantLRM as in AWQ, and the calibration set of QuantLRM and AWQ is smaller than other baselines. We also collect verified reasoning chains (Guha et al., 2025) as the reasoning calibration data. This reasoning set is larger than AWQs default but has the same size as other baselines. Since we find that quantization using activation does not benefit from the larger reasoning calibration (Tables 1 and 3), we pursue the default calibration of each baseline for fair analysis. More details related to the smaller calibration set of QuantLRM and AWQ are in Section A. 4.2. Models and Pseudo-Fine-Tuning To show the wide applicability of QuantLRM over different model families and finetuning pipelines, we run QuantLRM on SFT, DPO, and RL fine-tuned LRMs, and also on pre-fine-tuned LRM via pseudo-finetuning. For SFT models, we select few R1 distillation LRMs (Guo et al., 2025): R1-Distill-Llama-70B, R1-Distill-Qwen-32B, and R1-0528-Qwen3-8B. For the RL model, we select Olmo-3-7B-Think (Olmo et al., 2025). It is fine-tuned via reinforcement learning from verifiable rewards (RLVR). We also select Olmo-3-7B-Think-DPO as the choice of DPO to diversity our selection. Since recent LRMs are typically finetuned through multiple stages, we leverage the weight updates from Olmo-3-7B-Think-DPO to the RL version to quantize the RL fine-tuned model and the updates from Olmo-3-7B-Think-SFT to the DPO version to quantize the DPO model. When we need to quantize an LRM but its pre-fine-tuned checkpoint is not available (or it is not fine-tuned), we perform pseudo-fine-tuning on the target model to collect weight updates. In our experiment, we select Qwen3-1.7B (Yang et al., 2025) as an example. It is worth noting that Qwen3-1.7B has its pre-fine-tuned version available, but its performance on hard reasoning tasks (e.g., AIME-120) is low. In this case, we assume that some fine-tuning is still needed, and we additionally train the model to collect fine-tuning signals for quantizing it. 4.3. Baselines We compare only to PTQ, rather than other types such as QAT, since QuantLRM is weight-only PTQ method with high efficiency (Section 5.4). We select all the PTQ methods discussed in Section 2 as baselines: GPTQ, GPTAQ, AWQ, and ANY3. They form comprehensive list of state-of-theart weight-only quantization methods. We do not report ANY3 on R1-Llama-70B, as its 3-bit quantized model is unable to generate coherent text. For DPO and RL finetuned LRMs, we report the performance of GPTQ and AWQ due to their overall competitive scores on SFT models. 4.4. Implementation Details As we run QuantLRM on many LRMs (various model families with different sizes), model adaptation is needed. Since our pipeline uses only 16-bit arithmetic, large models can produce large Ic mℓ (e.g., large matrices often yield more zero updates), leading to overflow; we mitigate this by slicing of MLP modules into pieces (e.g., 2 or 5 pieces) when counting zero updates (used for R1-Llama-70B and R1-Qwen-32B). We then multiply the average zero-update count by the quadratic mapping scores in Equation (6) to obtain the final channel importance. Another adaptation is to multiply our computed channel importance by the absolute value of activation, which helps on R1-Llama-70B but does not generalize, and we choose all adaptations based on their performance on our validation set. We mainly focus on analyzing 3-bit quantization (W3A16) performance and also show 4-bit performance in Section 5.5, since 4-bit PTQ baselines have almost loseless scores (Zhang et al., 2025) and our priority is on 3-bit. When reporting accuracy scores of all methods, we use vLLM to perform inference. In Section 5.4, we first perform 4-bit quantization and then use the AWQ kernel to measure inference latency, since QuantLRM is compatible with AWQ kernel and the kernel supports only 4-bit LRMs. 5. Results and Analysis Our results aim to answer the following research questions: RQ 1: How does QuantLRM compare against other stateof-the-art PTQ baselines (Sections 5.1 and 5.3)? RQ 2: Is our design of QuantLRM optimal (Section 5.2)? RQ 3: Does QuantLRM support LRMs when their prefine-tuned checkpoints are not available (Section 5.3)? 5 QuantLRM Table 2. Performance of 3-bit quantization on SFT, DPO, and RL fine-tuned LRMs. QuantLRM delivers consistent improvement across various LRMs. We use the default calibration set for each PTQ method. QuantLRM reaches the best performance with one of the smallest calibration datasets. The highest scores of each model are bold excluding 16-bit. Model Variants Accuracy Model Type Precision AIME-120 FOLIO Temporal GPQA-Diamond Avg R1-Llama-70B R1-Llama-70B R1-Llama-70B R1-Llama-70B R1-Llama-70B R1-Qwen-32B R1-Qwen-32B R1-Qwen-32B R1-Qwen-32B R1-Qwen-32B R1-Qwen-32B R1-Qwen3-8B R1-Qwen3-8B R1-Qwen3-8B R1-Qwen3-8B R1-Qwen3-8B R1-Qwen3-8B Olmo-3-7B-Think-DPO Olmo-3-7B-Think-DPO Olmo-3-7B-Think-DPO Olmo-3-7B-Think-DPO SFT SFT SFT SFT SFT SFT SFT SFT SFT SFT SFT SFT SFT SFT SFT SFT SFT DPO DPO DPO DPO 16-bit 3-bit GPTQ 3-bit GPTAQ 3-bit AWQ 3-bit QuantLRM 16-bit 3-bit GPTQ 3-bit GPTAQ 3-bit AWQ 3-bit ANY3 3-bit QuantLRM 16-bit 3-bit GPTQ 3-bit GPTAQ 3-bit AWQ 3-bit ANY3 3-bit QuantLRM 16-bit 3-bit GPTQ 3-bit AWQ 3-bit QuantLRM Olmo-3-7B-Think Olmo-3-7B-Think Olmo-3-7B-Think Olmo-3-7B-Think RLVR 16-bit RLVR 3-bit GPTQ RLVR 3-bit AWQ RLVR 3-bit QuantLRM RQ 4: Is QuantLRM an efficient method (Section 5.4)? RQ 5: Is QuantLRM still competitive on 4-bit quantization (Section 5.5)? 5.1. Overall Results Our overall results of 3-bit quantization are in Table 2. As specified in Sections and 4.1, QuantLRM yields stateof-the-art performance on reasoning tasks with one of the smallest calibration datasets. As the primary focus of this paper is to push the boundary of ultra-low-bit quantization, we discuss the 4-bit quantization performance in Section 5.5. Improvement on SFT. QuantLRM delivers consistent improvements over GPTQ, GPTAQ, AWQ, and ANY3 across all three SFT models, increasing the average score by up to 3.03%. QuantLRM improves the average 3-bit score over the strongest PTQ baseline by 2.12% on R1-Llama-70B. Even on R1-Qwen3-8B, which has far fewer parameters (and thus less room for improvement), QuantLRM improves 6 56.7 37.5 38.3 44.2 49. 56.7 41.7 35.0 50.0 44.2 53.3 59.2 8.3 6.7 34.2 15.0 40.0 57.5 16.7 32.5 35.8 54.2 21.7 28.3 38.3 79.8 77.3 76.8 74.9 78.3 83.7 77.3 72.9 77.3 80.3 80. 79.8 77.3 78.8 77.8 69.0 76.8 70.4 63.5 69.5 65.0 75.4 69.5 76.4 74.4 100.0 99.2 99.2 100.0 99.6 99.2 99.6 99.6 99.6 99.6 99.6 99.6 82.8 84.4 98.4 54.4 99. 98.8 95.6 94.8 96.4 99.6 93.6 84.0 96.8 60.6 60.6 50.0 57.6 58.1 61.1 54.0 60.1 55.1 58.1 60.6 63.6 35.9 30.3 47.5 30.3 48.5 49.0 39.4 35.9 36. 45.5 41.9 35.9 43.4 74.28 68.65 66.08 69.18 71.30 75.18 68.15 66.90 70.50 70.55 73.58 75.55 51.08 50.05 64.48 42.18 66.13 68.93 53.80 58.18 58.40 68.68 56.68 56.15 63. the average score by 1.65% over the best baseline. Since AIME-120 is the hardest task selected (largest performance drop after quantization), it is exciting to see that QuantLRM always leads on this benchmark. The strong performance of QuantLRM demonstrates the advantage of using SFT signals for quantization. Improvement on DPO and RL. QuantLRM also delivers consistent improvements over the most competitive baselines on DPO and RL fine-tuned LRMs, increasing the average score by up to 6.55%. This demonstrates that QuantLRM works on various types of fine-tuning. The fundamental reason is that the traces left by different fine-tuning strategies overlap significantly. The gain from QuantLRM is smallest on DPO, which we attribute to relatively weak fine-tuning signals. Similar to SFT models, QuantLRM leads on AIME-120. QuantLRM Table 3. Ablation study. Performance of quantizing R1-Distill-Qwen-32B, R1-Distill-Llama-8B, and R1-Qwen3-8B to 3-bit while keeping fraction of channels in 16-bit based on various protection signals (the top 5% of weights by channel importance are protected in the 32B model, while the top 30% are protected in the 8B LRMs)."
        },
        {
            "title": "Protection Signal",
            "content": "AIME-120 FOLIO Temporal GPQA-Diamond"
        },
        {
            "title": "Avg",
            "content": "R1-Qwen-32B R1-Qwen-32B R1-Qwen-32B R1-Qwen-32B SFT both ends (no zero handling) SFT both ends w/ 0s maximized SFT both ends w/ 0s minimized SFT both ends R1-Llama-8B Activation on AWQ calibration R1-Llama-8B Activation on reasoning calibration R1-Llama-8B R1-Llama-8B R1-Llama-8B SFT mid SFT both ends (no zero handling) SFT both ends R1-Qwen3-8B Activation on AWQ calibration R1-Qwen3-8B"
        },
        {
            "title": "SFT both ends",
            "content": "34.2 30.8 32.5 49.2 5.0 5.0 4.2 20.0 16.7 0.0 45.0 71.4 76.4 74.9 77.8 52.7 49.3 54.2 63.5 59.1 39.4 72. 98.4 99.2 98.8 99.6 28.0 27.6 28.8 56.8 64.4 24.8 99.6 47.5 50.5 48.5 54.5 19.7 14.1 17.2 36.4 33.3 11.1 45. 62.88 64.23 63.68 70.28 26.35 24.00 26.10 44.18 43.38 18.83 65.75 Table 4. Performance of 4-bit quantization on R1-Llama-70B. We use the default calibration set for each PTQ method. QuantLRM reaches the best performance with one of the smallest calibration datasets."
        },
        {
            "title": "Precision",
            "content": "AIME-120 FOLIO Temporal GPQA-Diamond"
        },
        {
            "title": "Avg",
            "content": "R1-Llama-70B R1-Llama-70B R1-Llama-70B R1-Llama-70B"
        },
        {
            "title": "SFT\nSFT\nSFT\nSFT",
            "content": "16-bit 4-bit GPTQ 4-bit AWQ 4-bit QuantLRM 56.7 55.8 60.8 58.3 79.8 79.3 77.8 77.3 100.0 98.8 99.2 99.6 60.6 64.6 64.6 66.2 74.28 74.63 75.60 75. 5.2. Ablation Study To precisely decode the effect of each component in QuantLRM, we conduct an ablation study using the same mixed-precision quantization setting as in Section 3.1. Testing different protection signals on more LRMs, we report the performance in Table 3. Scores on R1-Qwen-32B highlight the importance of explicitly handling zero updates. When protecting both ends without handling zero updates separately, we refer to Equation (4), which yields the lowest average score. Then, instead of counting zero updates, we choose to directly assign them to ymax or ymin (default 10 or 1). Both choices provide modest gains, especially when zero updates are maximized. Our best design counts zero updates and excludes them when fitting the quadratic functions, resulting in 5.95% improvement in average score. On the two 8B LRMs, SFT both ends yields much better scores than activation-based signals. On R1-Llama-8B, protecting intermediate magnitudes (SFT mid) is not helpful, which echoes our discussion in Section 3.1 and reinforces our motivation for protecting both ends. Notably, on this model, protecting both ends without separately handling zero updates achieves the best average score, slightly outperforming counting zero updates. This contrast highlights the diversity of our LRMs: counting zero updates is the best design on the 32B model, and since it is only marginally worse on R1-Llama-8B, we adopt counting zero updates as our final design. This ablation study demonstrates the benefits of protecting weight updates on both ends and using the zero-update count to amplify channel importance. 5.3. Pseudo-Fine-Tuning practical limitation of using fine-tuning traces is that pre-fine-tuned checkpoint may be unavailable. Luckily, an exciting aspect of QuantLRM is that researchers can gather fine-tuning signals themselves when needed, which greatly enhances the applicability of our method. Because the ultimate goal is to quantize LRMs, fine-tuning can be stopped at any point once quantization performance is satisfactory. To show the results of quantizing model via pseudo-finetuning, we select Qwen3-1.7B and perform SFT on the OpenR1-Math-94k dataset from LlamaFactory (Zheng In Figure 3, we gather weight updates at et al., 2024). 7 QuantLRM ing factors, so we record the search time of QuantLRM and AWQ. We also measure inference latency by reporting the median latency over seven prompts, as specified in Section B. As mentioned in Section 4.4, this experiment is conducted on 4-bit quantized Olmo-3-7B-Think. In Table 5, we observe that QuantLRM incurs small onetime preparation overhead of only 2 minutes and 27 seconds, which is not present in AWQ because AWQ relies purely on activation statistics. Importantly, this extra cost is offline (prior to quantization) and does not affect inference. During quantization, both methods take roughly the same amount of time, with QuantLRM spending few extra seconds loading the precomputed fine-tuning signals. Their inference latency is the same, since we use the same AWQ kernel for inference. Besides the cost of time, QuantLRM consumes similar amount of GPU memory as AWQ as well. In conclusion, paying small amount of extra cost mainly during preparation, QuantLRM is reasonably efficient method that delivers significantly better reasoning accuracy after quantization. 5.5. Performance of W4A16 Quantization We report performance scores of W4A16 in Table 4. The average scores of all three methods surpass the 16-bit model, which echoes recent analyses (Zhang et al., 2025; Liu et al., 2025). Even on AIME-120 and GPQA-Diamond (challenging benchmarks), the 16-bit R1-Llama-70B ranks third or worse. The is the reason why we primarily focus on the performance of W3A16, as 4-bit LRMs perform similarly with little room of improvement. As discussed in Section 5.1, QuantLRM pushes the boundary of 3-bit quantization and delivers state-of-the-art performance. From practical perspective, these findings indicate that QuantLRM serves as safe default across bit-widths: it provides substantial improvements in W3A16 while remaining competitive and stable in W4A16, where current PTQ methods already saturate in performance. 6. Conclusion In this paper, we propose QuantLRM, which stands for quantization of LRMs via fine-tuning signals. QuantLRM gathers weight updates during SFT, DPO, and RL fine-tuning and assigns high importance scores to protect the smallest and largest updates. We empirically demonstrate that QuantLRM delivers consistent improvement for LRMs quantization. Compatible with vLLM and AWQ kernel, QuantLRM offers great inference flexibility with comparable speedup to state-of-the-art quantization methods. More broadly, our findings suggest that fine-tuning traces provide strong and lightweight importance signal for PTQ, offering practical direction to future research. Figure 3. Performance of 3-bit QuantLRM at different training steps. As training progresses, QuantLRMs performance improves. Table 5. Cost of time and inference speed of QuantLRM and AWQ. We report the preparation time prior to quantization, the time used to search for scaling factors, and the inference latency of the 4-bit quantized model. QuantLRM adds little overhead for preparation and search, while achieving the same latency as AWQ."
        },
        {
            "title": "Prep Time",
            "content": "Search Time Tokens/s AWQ QuantLRM 2min27s - 6min28s 6min35s 30.24 30.33 various training steps and measure the corresponding performance. We see that QuantLRMs performance gradually improves as training progresses, and this upward trend is also captured by our least-squares linear regression. The accumulation of weight updates as we have more training steps provides stronger and stronger fine-tuning signals. Since AWQ and GPTQ are applied directly to Qwen3-1.7B without training, their scores are shown as horizontal lines. Always surpassing GPTQ across all training steps, QuantLRM begins to match or surpass AWQ after 1956 training steps. We find that the fine-tuned Qwen3-1.7B has not yet reached convergence, as its 16-bit performance continues to improve after 2800 steps. Since it is not meaningful to quantize model that has not yet converged, we only report the quantized Qwen3-1.7B via weight updates collected during pseudo-fine-tuning. Since QuantLRM benefits from stronger fine-tuning signals for successful quantization, we recommend running pseudo-fine-tuning for few thousand training steps (or few epochs). 5.4. Cost of Time and Inference Speed As discussed in Section 3.2, our method requires computing weight updates, mapping them via quadratic functions, and counting zero updates as preparation prior to quantization. During quantization, the core is to search for optimal scal8 QuantLRM"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. URL https://arxiv. org/abs/1810.04805. Du, H., Li, W., Cai, M., Saraipour, K., Zhang, Z., Lakkaraju, H., Sun, Y., and Zhang, S. How post-training reshapes llms: mechanistic view on knowledge, truthfulness, refusal, and confidence, 2025. URL https://arxiv. org/abs/2504.02904. Egler, S., Schulman, J., and Carlini, N. Detecting adversarial fine-tuning with auditing agents, 2025. URL https: //arxiv.org/abs/2510.16255. Elhoushi, M. and Johnson, J. any4: Learned 4-bit numeric representation for llms, 2025. URL https://arxiv. org/abs/2507.04610. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pretrained transformers. arXiv preprint arXiv:2210.17323, 2022. Goel, A., Kim, Y., Shavit, N., and Wang, T. T. Learning to interpret weight differences in language models, 2025. URL https://arxiv.org/abs/2510.05092. Guha, E., Marten, R., Keh, S., Raoof, N., Smyrnis, G., Bansal, H., Nezhurina, M., Mercat, J., Vu, T., Sprague, Z., Suvarna, A., Feuer, B., Chen, L., Khan, Z., Frankel, E., Grover, S., Choi, C., Muennighoff, N., Su, S., Zhao, W., Yang, J., Pimpalgaonkar, S., Sharma, K., Ji, C. C.-J., Deng, Y., Pratt, S., Ramanujan, V., Saad-Falcon, J., Li, J., Dave, A., Albalak, A., Arora, K., Wulfe, B., Hegde, C., Durrett, G., Oh, S., Bansal, M., Gabriel, S., Grover, A., Chang, K.-W., Shankar, V., Gokaslan, A., Merrill, M. A., Hashimoto, T., Choi, Y., Jitsev, J., Heckel, R., Sathiamoorthy, M., Dimakis, A. G., and Schmidt, L. Openthoughts: Data recipes for reasoning models, 2025. URL https://arxiv.org/abs/2506.04178. Guo, D., Yang, D., Zhang, H., Song, J., Wang, P., Zhu, Q., Xu, R., Zhang, R., Ma, S., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Xu, H., Ding, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Chen, J., Yuan, J., Tu, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., You, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Zhou, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, September 2025. ISSN 1476-4687. doi: 10.1038/s41586-025-09422-z. URL http://dx. doi.org/10.1038/s41586-025-09422-z. Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. Han, S., Schoelkopf, H., Zhao, Y., Qi, Z., Riddell, M., Zhou, W., Coady, J., Peng, D., Qiao, Y., Benson, L., Sun, L., Wardle-Solano, A., Szabo, H., Zubova, E., Burtell, M., Fan, J., Liu, Y., Wong, B., Sailor, M., Ni, A., Nan, L., Kasai, J., Yu, T., Zhang, R., Fabbri, A., Kryscinski, W. M., Yavuz, S., Liu, Y., Lin, X. V., Joty, S., Zhou, Y., Xiong, C., Ying, R., Cohan, A., and Radev, D. FOLIO: Natural language reasoning with first-order logic. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 2201722031, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 1229. URL https://aclanthology.org/2024. emnlp-main.1229/. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving 9 QuantLRM with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. text transformer, 2023. URL https://arxiv.org/ abs/1910.10683. Li, Y., Yin, R., Lee, D., Xiao, S., and Panda, P. GPTAQ: Efficient finetuning-free quantization for asymmetric calibration. In Forty-second International Conference on Machine Learning, 2025. URL https: //openreview.net/forum?id=QdELyl0FST. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=Ti67584b98. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S. Awq: Activation-aware weight quantization for llm compression and acceleration. In MLSys, 2024. Liu, R., Sun, Y., Zhang, M., Bai, H., Yu, X., Yu, T., Yuan, C., and Hou, L. Quantization hurts reasoning? an empirical study on quantized reasoning models. arXiv preprint arXiv:2504.04823, 2025. Marks, S., Treutlein, J., Bricken, T., Lindsey, J., Marcus, J., Mishra-Sharma, S., Ziegler, D., Ameisen, E., Batson, J., Belonax, T., Bowman, S. R., Carter, S., Chen, B., Cunningham, H., Denison, C., Dietz, F., Golechha, S., Khan, A., Kirchner, J., Leike, J., Meek, A., NishimuraGasparian, K., Ong, E., Olah, C., Pearce, A., Roger, F., Salle, J., Shih, A., Tong, M., Thomas, D., Rivoire, K., Jermyn, A., MacDiarmid, M., Henighan, T., and Hubinger, E. Auditing language models for hidden objectives, 2025. URL https://arxiv.org/abs/ 2503.10965. Mathematical Association of America. MAA InvitaURL https://maa.org/ tional Competitions. maa-invitational-competitions/. Accessed: 2025-03-19. Olmo, T., :, Ettinger, A., Bertsch, A., Kuehl, B., Graham, D., Heineman, D., Groeneveld, D., Brahman, F., Timbers, F., Ivison, H., Morrison, J., Poznanski, J., Lo, K., Soldaini, L., Jordan, M., Chen, M., Noukhovitch, M., Lambert, N., Walsh, P., Dasigi, P., Berry, R., Malik, S., Shah, S., Geng, S., Arora, S., Gupta, S., Anderson, T., Xiao, T., Murray, T., Romero, T., Graf, V., Asai, A., Bhagia, A., Wettig, A., Liu, A., Rangapur, A., Anastasiades, C., Huang, C., Schwenk, D., Trivedi, H., Magnusson, I., Lochner, J., Liu, J., Miranda, L. J. V., Sap, M., Morgan, M., Schmitz, M., Guerquin, M., Wilson, M., Huff, R., Bras, R. L., Xin, R., Shao, R., Skjonsberg, S., Shen, S. Z., Li, S. S., Wilde, T., Pyatkin, V., Merrill, W., Chang, Y., Gu, Y., Zeng, Z., Sabharwal, A., Zettlemoyer, L., Koh, P. W., Farhadi, A., Smith, N. A., and Hajishirzi, H. Olmo 3, 2025. URL https://arxiv.org/abs/2512.13961. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-toSuzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., and Wei, J. Challenging big-bench tasks and whether chain-of-thought can solve them, 2022. URL https://arxiv.org/abs/2210.09261. Veeraboina, H."
        },
        {
            "title": "Aime",
            "content": "problem set 1983URL https://www.kaggle. 2023. 2024, com/datasets/hemishveeraboina/ aime-problem-set-1983-2024. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Zhang, N., Liu, Y., Zhao, X., Cheng, W., Bao, R., Zhang, R., Mitra, P., and Chen, H. Pruning as domainspecific LLM extractor. In Duh, K., Gomez, H., and Bethard, S. (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, pp. 14171428, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl. 91. URL https://aclanthology.org/2024. findings-naacl.91/. Zhang, N., Kwek, E., Zhang, Y., Nguyen, N.-H., Mitra, P., and Zhang, R. When reasoning meets compression: Understanding the effects of llms compression on large reasoning models, 2025. URL https: //arxiv.org/abs/2504.02010. Zhang, Z., Liu, B., and Shao, J. Fine-tuning happens in tiny subspaces: Exploring intrinsic task-specific subspaces of pre-trained language models. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 17011713, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. 10 QuantLRM acl-long.95. URL https://aclanthology.org/ 2023.acl-long.95/. Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z., and Ma, Y. Llamafactory: Unified efficient finetuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403. 13372. A. Smaller Calibration Set of QuantLRM and AWQ QuantLRM We use the default calibration size for all quantization methods. For GPTQ, GPTAQ, and ANY3, we use 128 samples of length 2048 tokens from the C4 dataset (Raffel et al., 2023). For AWQ and QuantLRM, we use 128 samples of length 512 instead. Note that QuantLRM still outperforms other baselines despite using one of the smallest calibration sets, demonstrating the efficacy of our method. B. Prompts Used for Speed Analysis We use 7 short prompts to measure the inference speed of QuantLRM and AWQ: Deeply analyze; then decide. Enumerate approaches; pick best. Solve; stress-test; revise. Assume nothing; derive everything. Answer; justify; quantify uncertainty. Decompose; solve parts; recombine. List risks; add safeguards; finalize. The goal is to generate 200 reasoning tokens for each prompt and compute the median latency."
        }
    ],
    "affiliations": [
        "IBM Research",
        "Pennsylvania State University"
    ]
}