{
    "paper_title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
    "authors": [
        "Safal Shrestha",
        "Anubhav Shrestha",
        "Aadim Nepal",
        "Minwu Kim",
        "Keith Ross"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes."
        },
        {
            "title": "Start",
            "content": "Safal Shrestha * 1 Anubhav Shrestha * 1 Aadim Nepal 1 Minwu Kim 1 Keith Ross"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 6 2 0 2 2 ] . [ 1 7 9 9 1 0 . 2 0 6 2 : r Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90% of baseline performance, and yields substantial gains of up to 20 30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes. 1 2 *Equal contribution 1Department of Computer Science, New York University Abu Dhabi, Abu Dhabi, UAE. Correspondence to: Keith Ross <keithwross@nyu.edu>. Preprint. February 3, 2026. 1Code available at https://github.com/safal312/ on-the-limits-of-layer-pruning available and models 2Data are at https:// huggingface.co/collections/safal312/ on-the-limits-of-generative-reasoning-in-llms Large Language Models (LLMs) have achieved remarkable performance across wide range of tasks, success often attributed to their large parameter counts and extensive training data (Hoffmann et al., 2022; Yang et al., 2025a; Grattafiori et al., 2024). However, the scale of modern LLMs raises significant concerns regarding efficiency and costs (LeCun et al., 1989; Wan et al., 2023; Song et al., 2024). These challenges have motivated substantial body of work on model compression techniques aimed at reducing model size while preserving performance, including pruning at multiple granularities ranging from individual neurons to entire layers (Frantar & Alistarh, 2023; Sun et al., 2024; Muralidharan et al., 2024; Sreenivas et al., 2024; Song et al., 2024; Ashkboos et al., 2024; Ma et al., 2023; Ling et al., 2024). Among these approaches, layer pruning has emerged as particularly appealing strategy. By removing entire transformer blocks of contemporary decoder-only models, layer pruning offers simple method for reducing model depth, often requiring minimal or no additional finetuning (Song et al., 2024; Yang et al., 2024; Lu et al., 2024; Men et al., 2025; Kim et al., 2024; Chen et al., 2024). This approach is further motivated by theoretical and empirical works suggesting redundancy across layers in LLMs (Sun et al., 2025; Lad et al., 2024; Men et al., 2025). Furthermore, layer pruning is largely orthogonal to other efficiency techniques such as quantization and sparsification, enabling it to be combined with complementary methods for additional computational savings (Song et al., 2024; Kim et al., 2024). While this approach has achieved notable success in classification benchmarks, it has proven far less effective for reasoning-intensive generative tasks like math and coding, which require the model to generate multi-step chain of thought to arrive at the correct solution (Lu et al., 2024; Chen et al., 2024; Men et al., 2025; Yang et al., 2024; Kim et al., 2024; Nepal et al., 2025). Prior work has largely attributed the failure of layer pruning on generative tasks to the importance of deeper layers for reasoning, without explicitly characterizing how layer removal degrades model behavior (Wang et al., 2025; Song et al., 2025). Moreover, existing methods that partially recover generative performance typically rely on knowledge distillation with large-scale data (in"
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "billions of tokens) and compute, which can be impractical (Muralidharan et al., 2024; Sreenivas et al., 2024). These limitations motivate closer analysis of pruning-induced failure modes and an examination of how much recovery is achievable under realistic post-training constraints. Rather than proposing new pruning algorithm, our goal is to characterize the limits of layer pruning for generative reasoning and to identify practical regimes in which it remains viable. of layers (Yang et al., 2024; Men et al., 2025; Gromov et al., 2024; Song et al., 2024). However, this success has not consistently extended to generative reasoning tasks such as GSM8K, which require multi-step generation to arrive at correct solution (Chen et al., 2024; Men et al., 2025; Gromov et al., 2024; Wang et al., 2025). This discrepancy raises questions about the extent to which layers are redundant across different task contexts. In this paper, we make the following contributions: We demonstrate that generative reasoning tasks are substantially more sensitive to layer pruning than classification benchmarks, with even single-layer removal causing severe degradation, underscoring the depth dependence of such tasks. We provide systematic analysis of pruning-induced failure modes in generative settings. Beyond surfacelevel text degradation, we show that pruning disrupts core algorithmic abilities such as arithmetic computation and valid syntactic generation, which directly impairs multi-step reasoning. Under realistic post-training constraints, we propose supervised finetuning with Self-Generated Responses (SGR) as simple recovery strategy. We show that this SGR finetuning consistently outperforms finetuning on external open-source datasets, achieving strong retention on classification tasks and also on generative benchmarks at moderate pruning ratios. Finally, we show that even the strongest recovery achieved under these constraints fails to fully restore arithmetic and syntactic capabilities. This exposes fundamental limitation of post-pruning recovery for generative reasoning and suggests that, despite apparent layer redundancy, model depth remains critical for algorithmic computation. Overall, our results clarify when and why layer pruning succeeds or fails, and provide practical guidance for its use in settings where preserving generative reasoning ability is priority. 2. Background & Related Work Importance of Layers Recent studies have suggested that layers in large language models, particularly deeper layers, may exhibit substantial redundancy, motivating layer pruning as an effective compression strategy (Gromov et al., 2024; Sun et al., 2025; Men et al., 2025; Yin et al., 2023; Lad et al., 2024). On classification benchmarks, layer pruning has demonstrated notable success, with models retaining over 80% of baseline accuracy even after removing 2025% 2 Limitations of Layer Pruning for Generative Tasks The reasons behind failure of layer-pruned models to perform well on generative tasks has been mainly attributed to deeper layers being more important for reasoning (Song et al., 2025; Wang et al., 2025). Song et al. (2025) observes that difference in evaluation techniques can highlight loss in reasoning abilities of LLMs. Few past studies have also done qualitative analysis of chain of thought to hint at text degeneration with removing layers (Wang et al., 2025), but an in-depth look into various failure modes is missing. In order to recover the lost ability, past studies have applied continued pretraining (Gromov et al., 2024; Sreenivas et al., 2024; Muralidharan et al., 2024; Xia et al., 2023) or lightweight module replacement (Chen et al., 2024) to heal the model and recover original performance. Unless you have access to the pretraining data (in billions of tokens) or compute, continued pretraining/finetuning on open-source datasets has been the common approach to mitigate performance loss with limited success in generative tasks (Sreenivas et al., 2024; Muralidharan et al., 2024; Gromov et al., 2024; Wang et al., 2025). Contrary to such works, we hypothesize that models Self-Generated Responses can prove crucial for regaining models performance. Other Compression Techniques Beyond layer pruning, quantization (Frantar et al., 2022; Dettmers & Zettlemoyer, 2023; Lin et al., 2024) and sparsification (Frantar & Alistarh, 2023; Sun et al., 2024) have also been widely studied for compressing LLMs. While effective in reducing memory footprint, these approaches often fall short of achieving expected throughput gains under realistic batch sizes (Yang et al., 2024; Song et al., 2024; Kim et al., 2024). Moreover, they frequently require specialized hardware or software support, limiting their accessibility in practice (Frantar & Alistarh, 2023; Sun et al., 2024; Song et al., 2024). Layer pruning is largely orthogonal to these techniques and can be combined with them to achieve complementary efficiency gains (Song et al., 2024; Kim et al., 2024). 3. Classification vs. Generative Benchmarks Prior work on layer pruning has primarily evaluated performance using classification benchmarks, where evaluation reduces to scoring fixed set of candidate outputs via logIn this setting, substantial layer likelihood comparison."
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "Table 1. Summary of performance retention. Results are normalized relative to baseline. Results with (*) are sourced from Lu et al. (2024). Classif reports mean performance on classification benchmarks, and Gen reports mean performance on generative benchmarks. Full results are in Table"
        },
        {
            "title": "Model",
            "content": "Gemma2-2B-It LLaMA-3.1-8B-It . Qwen2.5-7B-Instruct"
        },
        {
            "title": "Reverse\nBI",
            "content": "Reverse* BI"
        },
        {
            "title": "Reverse\nBI",
            "content": "0.839 0.828 0.818 0.655 0.706 0.804 0.789 0.801 0.308 0.285 0.321 0. 0.162 0.327 0.147 0.143 Mistralv0.3-7B-Instruct Reverse BI removal has been shown to preserve accuracy with minimal or no additional finetuning. Following standard literature, in this paper, we evaluate classification performance using standard benchmarks, including HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), MMLU (Hendrycks et al., 2020), WinoGrande (Sakaguchi et al., 2021), OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al., 2018), and ARC-C (Clark et al., 2018). All classification results are computed using the lm-evaluation-harness framework (Gao et al., 2021). In contrast, generative benchmarks require the model to produce sequence of tokens to arrive at valid solution, often involving multi-step reasoning or structured generation. These tasks place substantially different demands on the model than classification benchmarks. In this work, we consider diverse set of generative benchmarks spanning multiple domains, including GSM8K for mathematical reasoning (Cobbe et al., 2021), HumanEval+ and MBPP+ for code generation (Liu et al., 2023), and XSUM for summarization (Narayan et al., 2018). Table 1 illustrates the stark discrepancy between classification and generative performance retention under layer pruning for four different models. While classification accuracy remains largely intact, performance on generative tasks degrades substantially. Results are sourced from Lu et al. (2024) when available. Complete results are reported in Table 3. 4. Layer-by-Layer Pruning for Generative"
        },
        {
            "title": "Tasks",
            "content": "To better understand this discrepancy, we analyze the effects of removing individual layers in LLMs. Specifically, we perform single-layer pruning by removing one transformer layer at time and evaluating the resulting model on diverse set of generative benchmarks: GSM8K, HumanEval+, and XSUM. Experiments are conducted on three models 3 from distinct families: Qwen-2.5-7B-Instruct (Yang et al., 2025b), Llama-3.1-8B-Instruct (Grattafiori et al., 2024), and Mistral-7B-Instruct-v0.3 (Jiang et al., 2023). Figure 1 summarizes the results. In few cases, certain layers appear redundant; for example, early layers in Qwen and some deeper layers in Llama can be removed with minimal effect. But largely, across models, even single-layer removal can substantially impact performance. Pruning certain layers can even lead to sharp drops on GSM8K and HumanEval+. While prior work has noted performance degradation in mathematical reasoning under layer pruning (Chen et al., 2024; Wang et al., 2025; Nepal et al., 2025), we additionally observe similar sensitivity in code generation, with the locations of the sharp drops varying across model families and tasks, as seen in Qwen versus Mistral. Taskspecific effects are also pronounced: reasoning-intensive tasks such as mathematics and coding are far more sensitive to depth reduction, whereas summarization tasks like XSUM remain largely stable. These results indicate that layer pruning exhibits strong modeland task-dependent effects, contrasting with its relative robustness on classification benchmarks. Although earlier studies have suggested that pruning affects reasoning in generative tasks (Gromov et al., 2024; Men et al., 2025), detailed characterization of the resulting errors remains limited. In the following sections, we analyze subset of key failure modes that arise in generative reasoning after pruning. 4.1. Text Degeneration Text degeneration (Holtzman et al., 2019) is commonly observed failure mode in pruned language models and can hinder instruction following and coherent generation. We quantify degeneration using two complementary metrics computed with 4-grams: 4-gram repetition and Self-BLEU4, where higher values indicate increased repetition and reduced diversity (Holtzman et al., 2019). We additionally report the average number of generated tokens per prompt in Appendix A.2. As shown in Figure 2, layer pruning often amplifies degenerative behaviors. In agreement with earlier findings (Men et al., 2025), we observe that both early and late layers are particularly important for maintaining stable text generation. In some cases, elevated repetition metrics align with sharp performance drops, such as layer 2 in Qwen. Prior work has primarily attributed pruning-induced performance degradation to looping and repetitive outputs (Wang et al., 2025). However, degeneration alone does not fully account for failures in generative reasoning tasks. Near the points of sharp drops in Qwen and Llama on the math and coding tasks, text generation quality remains largely intact."
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "Figure 1. Effect of removing single layer on model performance across generative benchmarks. Reasoning-intensive tasks such as GSM8K and HumanEval+ exhibit severe performance degradation at specific layers, while XSUM remains comparatively robust except for layers whose removal induces general text degeneration. (We generally skip layer 0 for its poor results.) Figure 2. Text degeneration under single-layer pruning, measured using 4-gram repetition (left) and Self-BLEU4 averaged across responses and normalized relative to the baseline. Conversely, in Mistral, we observe pronounced spike in degeneration metrics at layer 24 without corresponding drop in task performance. Manual inspection reveals that removing this layer causes the model to continue rambling after producing valid response. Overall, these findings indicate that while text degeneration is significant side effect of layer pruning, it is not sufficient explanation for the loss of reasoning ability in generative tasks. 4.2. Degradation of Arithmetic Beyond high-level reasoning behaviors, solving mathematical word problems requires reliable execution of basic arithmetic operations. We find that pruning leads to pronounced degradation of arithmetic ability, even on elementary computations. In particular, removing deeper layers in Qwen and mid-depth layers in Llama results in frequent arithmetic failures during qualitative inspection, where models fail to correctly perform simple calculations (see Appendix A.3). Since standard generative evaluations can obscure true abilities due to auxiliary demands such as multi-token genFigure 3. Effect of single-layer pruning on the arithmetic ability of Llama. eration (Schaeffer et al., 2023; Hu & Frank, 2024; Song et al., 2025), to isolate arithmetic competence, we design controlled evaluation that probes the models ability to produce the first answer token in simple arithmetic prompts. Given inputs of the form Question: What is (7 Answer:, we measure (i) the change in + 5) - 6? logprob of the correct answer token relative to the unpruned baseline, and (ii) top-1 accuracy, i.e., whether the correct token is assigned the highest logprob. Full experimental details are provided in Appendix A.4. Figure 3 reports results over 200 arithmetic problems. We observe substantial drops in both logprob and accuracy after pruning layers near the middle region, despite the absence of any generation requirement in this task. This demonstrates that layer pruning induces structural loss of arithmetic capability, rather than merely degrading Chain of Thought"
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "generation. Moreover, the correspondence between arithmetic failures and performance drops on GSM8K (Figure 1) suggests that significant fraction of mathematical reasoning errors stem directly from impaired arithmetic abilities. Results for Qwen and Mistral are reported in the Appendix A.4. 4.3. Degradation of Parenthesis Tracking Figure 1 shows that layer pruning can cause substantial performance drops on coding benchmarks such as HumanEval+. Similar to arithmetic in mathematical reasoning, correct syntax generation is necessary prerequisite for code reasoning. We observe that removing specific layers significantly degrades the models ability to maintain syntactic consistency, particularly in tracking and closing parentheses. Representative examples are provided in Appendix A.5. Figure 4. Distribution of syntactic error types induced by singlelayer pruning. Unlike math benchmarks, code evaluation allows errors to be categorized based on execution feedback. Leveraging this property, Figure 4 reports the prevalence of common syntactic failures under single-layer pruning for Qwen and Mistral, including unbalanced parentheses, undefined variables, and other invalid syntax. Notably, pruning certain layers (e.g., layer 23 in Qwen) leads to sharp increase in parenthesis-matching errors, indicating severe loss in syntactic skills. In Mistral, we also observe spikes in auxiliary syntax failures, such as malformed markdown code blocks (e.g., python (code)), which can directly disrupt downstream evaluation pipelines. Taken together, these results indicate that layer pruning disrupts not only surface-level text generation but also internal mechanisms responsible for important algorithmic capabilities, such as arithmetic execution and parenthesis tracking. Tasks such as classification or summarization are less dependent on specialized capabilities, which helps explain their relative robustness to depth reduction. In contrast, the pronounced sensitivity observed under single-layer removal highlights why aggressive depth 5 pruning poses fundamental challenge for generative reasoning tasks, making performance retention without continued training almost impossible. 5. Finetuning with Self-Generated Responses Most prior work relies on supervised finetuning with the prompts and the responses in open-source datasets to recover performance after pruning (Chen et al., 2024; Ma et al., 2023; Gromov et al., 2024; Lu et al., 2024; Wang et al., 2025; Xia et al., 2023). While this approach has been effective for classification tasks, its effectiveness for generative reasoning is limited, as evidenced in Table 1. related line of work employs knowledge distillation, where the pruned model is trained to match the logits of the unpruned base model using large-scale data comprising billions of tokens (Muralidharan et al., 2024; Sreenivas et al., 2024), which incurs substantial computational cost. In contrast, we study simpler alternative based on SelfGenerated Responses (SGR), where the unpruned base model itself provides training targets. Rather than loading teacher model during training or relying on reference responses from open-source datasets, we pass only the prompts from the open-source datasets to the base model and use its generated outputs for supervised finetuning of the pruned model. We will show that our approach with SGR can bring substantial gains to classification and also notable boosts to generative tasks. Experimental Setup All fine-tuning experiments use QLoRA and are conducted on two datasets: Alpaca-cleaned (Taori et al., 2023) and Dolci (Ettinger et al., 2025). We primarily evaluate two standard pruning strategies, Block Influence (BI) and Reverse Order. In addition, we include simple iterative pruning procedure that extends the singlelayer ablation analysis in Figure 1 by progressively removing layers with the smallest observed impact on performance for multiple iterations. This procedure is used to examine whether selectively pruning empirically less sensitive layers affects post-pruning recovery behavior, particularly for generative reasoning tasks. Full experimental details are provided in Appendix A.6. Figure 5 compares the perplexity when using the standard approach of doing SFT on prompts and responses to our SGR approach. Here the pruned model has 25% of the layers pruned using reverse pruning, and the finetuning is done with the Dolci dataset for both approaches. We see that supervision derived from the corresponding unpruned model (i.e., our SGR approach) leads to substantially improved recovery, as reflected by consistently lower perplexity on their respective datasets throughout training. In the following sections, we demonstrate that this improved optimization behavior translates into stronger downstream performance"
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "We also observe that the effectiveness of pruning strategies varies by model. While Iterative pruning does not provide consistent gains over standard metrics such as BI and Reverse Order for Llama and Mistral, it performs better for Qwen. This suggests that greedily selecting layers based on empirical redundancy does not generalize uniformly across architectures, potentially reflecting differences in internal layer organization, as previously noted for Qwen models (Gromov et al., 2024). Despite these gains, notable gap remains between classification and generative performance. For instance, while the same Llama model retains nearly 90% classification accuracy, its retention on generative benchmarks is 63.4%  (Table 2)  . Although some divergence is expected given the greater difficulty of generative reasoning tasks, these results suggest that recovering generative performance remains challenging under realistic post-training constraints, particularly without access to large-scale data or compute (Muralidharan et al., 2024; Sreenivas et al., 2024). If preserving generative reasoning is primary objective, aggressive pruning may be impractical. 5.3. Pruning at Moderate Ratios Figure 6. Average accuracy on generative tasks for Qwen and Llama across three pruning strategies at varying pruning ratios. Classification performance at 25% pruning shows the substantial gap between classification and generative task recovery. To investigate the discrepancy between classification and generative task recovery, we evaluate self-generated finetuning at lower pruning ratios on Llama and Qwen. Results are shown in Figure 6 (full results in Table 6). With only two layers removed, both models retain approximately 8590% of their original performance. At moderate pruning ratios ( 10%), retention remains near 80% for BI and Iterative metrics, and around 70% at 15% pruning ratio. Beyond this, performance declines steadily. Notably, while Reverse pruning is competitive with BI on classification tasks, it underperforms on generative tasks, illustrating that optimization for classification does not guarantee generalization to multi-step reasoning. These results also challenge 6 Figure 5. Perplexity curves during training for both standard finetuning and for SGR for the Llama model (BI pruned: 25%) Results for other models in A.7. across both classification and generative benchmarks. 5.1. Results for Classification Tasks We first evaluate performance retention on classification benchmarks at fixed pruning ratio of 25%, as reported in Table 2. Across both Llama-3.1-8B-It and Qwen-2.5-7BInstruct, SGR supervision generally improves classification performance of the pruned models. For example, for Llama using the BI pruning metric, self-generated Alpaca training yields an average retention of 91.4%, representing gain of +25.9 percentage points relative to finetuning on Alpaca alone. Across all ablations, Llama exhibits strong retention on classification benchmarks. For Qwen, performance retention remains robust, with best retention rate of 80.5%. While this is lower than Llama, it is consistent with prior observations that Qwen exhibits less layer redundancy (Gromov et al., 2024). More broadly, though, we observe similar trends for Gemma and Mistral (full results in Appendix A.8), for which SGR consistently achieve retention rates above 80%, with the best result reaching up to 85%. Overall, these results demonstrate that SGR supervision reliably improves classification performance across model families following layer pruning. 5.2. Results on Generative Tasks We next evaluate performance on generative benchmarks, summarized in Table 2. Compared to finetuning on opensource prompts and responses, finetuning with SGR consistently yields substantially higher performance retention. For example, the Llama model pruned using Block Influence (BI) achieves gain of +31.2 percentage points when finetuned on Dolci SGR relative to finetuning on Dolci directly. Across models, SGR supervision provides pronounced improvements. The Qwen model attains retention rate of 52.5%, and similarly strong gains are observed for Gemma and Mistral (full results in Appendix A.8). Overall, finetuning on the more reasoning-intensive Dolci prompts yields stronger recovery than Alpaca across most settings."
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "Table 2. Performance retention (normalized to baseline). Results marked with (*) are sourced from Lu et al. (2024). SGR is our method with the pruning metric in parentheses. Reverse, BI, and Iterative indicate the pruning order (reverse-order, block-interleaved, iterative), while Alpaca and Dolci denote the training data source; S.Alpaca and S.Dolci refer to self-generated variants of the corresponding datasets. / indicate improvement or degradation in mean retention of our SGR approach with respect to the standard approach of doing SFT with the open-source prompts and responses. Full results are provided in Appendix A.8. Model HeSw PIQA MMLU Wino OBQA ARC-E ARC-C Mean GSM8K HumEval+ MBPP+ XSUM Mean Classification Generative LLaMA-3.1-8B-It Open-source responses Reverse + Alpaca* BI + Alpaca Reverse + Dolci BI + Dolci Iterative + Dolci Self-generated responses SGR (Reverse + S.Alpaca) SGR (BI + S.Alpaca) SGR (Reverse + S.Dolci) SGR (BI + S.Dolci) SGR (Iterative + S.Dolci) Qwen2.5-7B-Instruct Open-source responses Reverse + Alpaca BI + Alpaca Reverse + Dolci BI + Dolci Iterative + Dolci Self-generated responses SGR (Reverse + S.Alpaca) SGR (BI + S.Alpaca) SGR (Reverse + S.Dolci) SGR (BI + S.Dolci) SGR (Iterative + S.Dolci) 0.679 0.710 0.787 0.793 0.787 0.682 0.838 0.809 0.826 0. 0.710 0.867 0.681 0.820 0.827 0.740 0.891 0.678 0.851 0.852 0.875 0.897 0.896 0.886 0.896 0.860 0.913 0.920 0.907 0.907 0.832 0.982 0.794 0.958 0.910 0.856 0.982 0.788 0.982 0. 0.934 0.356 0.895 0.943 0.789 0.909 0.967 0.963 0.984 0.835 0.765 0.480 0.703 0.507 0.700 0.848 0.540 0.731 0.545 0.699 0.844 0.729 1.000 1.027 0.926 0.926 1.049 0.979 1.033 1. 0.854 0.789 0.826 0.854 0.859 0.846 0.826 0.732 0.841 0.852 0.870 0.598 0.651 0.710 0.828 0.947 0.769 0.799 0.769 0.947 0.488 0.927 0.488 0.878 0.732 0.488 0.732 0.516 0.854 0. 0.754 0.746 0.863 0.896 0.893 0.807 0.950 0.917 0.924 0.875 0.720 0.874 0.650 0.843 0.828 0.741 0.852 0.652 0.867 0.869 0.769 0.549 0.813 0.887 0.747 0.772 0.912 0.879 0.879 0. 0.570 0.708 0.524 0.626 0.727 0.633 0.748 0.576 0.693 0.775 0.818 0.655 0.844 0.878 0.838 0.843 0.914 0.895 0.903 0.880 0.706 0.804 0.666 0.784 0.798 0.736 0.796 0.668 0.805 0.799 0.453 0.318 0.405 0.469 0.328 0.561 0.724 0.628 0.758 0.647 0.012 0.097 0.059 0.270 0.294 0.025 0.202 0.153 0.329 0.581 0.111 0.344 0.434 0.444 0.301 0.290 0.412 0.556 0.634 0. 0.025 0.167 0.159 0.200 0.333 0.051 0.183 0.142 0.283 0.325 0.084 0.128 0.302 0.308 0.245 0.162 0.481 0.251 0.390 0.312 0.020 0.274 0.120 0.278 0.358 0.024 0.288 0.157 0.398 0. 0.638 0.062 0.077 0.068 0.338 0.179 0.860 0.029 0.754 0.763 0.591 0.768 0.541 0.817 0.812 0.605 0.846 0.600 0.861 0.822 0.321 0.213 0.304 0.322 0.303 0.298 0.619 0.366 0.634 0.536 0.162 0.327 0.220 0.391 0.449 0.176 0.380 0.263 0.468 0.525 prior assumptions that deeper layers of Llama are largely redundant and easily pruned (Men et al., 2025; Sun et al., 2025). Additionally, we find that training with Dolci SGR consistently yields better results when compared to raw Dolci. Especially at lower pruning ratios (like 10 15%), for Qwen, SGR training consistently yields gains of around 15pp (see Figure 14). 6. Post-Recovery Analysis As shown in the previous section, generative reasoning tasks exhibit persistent gap in performance retention relative to classification tasks, even after finetuning, with recovery improving only at more moderate pruning ratios. In this section, we analyze how specific atomic capabilities underlying generative reasoning, namely arithmetic and syntactic correctness, are affected by pruning and to what extent they can be recovered through finetuning. 6.1. Arithmetic Ability Building on the analysis in Section 4.2, we examine how arithmetic ability is affected by pruning and the extent to which it can be recovered through SGR finetuning. Figure 7 reports results for three model families at fixed pruning ratio of 25%. Figure 7. Accuracy on an arithmetic task for baseline models, after pruning, and after pruning followed by finetuning. Results are shown at 25% pruning ratio (Qwen: Iterative, Mistral: BI, Llama: BI). Across all models, pruning leads to substantial drop in arithmetic accuracy, with average performance falling to 34.3%. Finetuning with SGR partially recovers this loss, but performance remains well below the that of the base model. Importantly, this arithmetic task is considerably simpler than full generative mathematical reasoning, as it does not require"
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "multi-step reasoning or long-form generation. The persistent gap between base and fine-tuned pruned models, even on this minimal objective, indicates that pruning irreversibly degrades core computational capabilities. While models such as Llama exhibit relatively smaller absolute drops (e.g., from 60% to 50%), the gap nonetheless reflects lower bound for error introduced by pruning. For more complex reasoning tasks, which compound arithmetic with multi-step generation, this degradation is likely to be further amplified. tion, which are not fully recoverable through training. Although for Qwen in HumanEval+, we see higher rates of syntactically valid code, the fraction of functionally correct solutions remains substantially lower. This disparity highlights that syntactic validity alone does not imply recovery of coding competence. Analogous to arithmetic serving as lower bound for mathematical reasoning, these results suggest that residual syntactic errors provide lower bound on the degradation of coding ability, with deeper semantic reasoning likely affected to an even greater extent. 6.2. Syntax Ability 7. Discussion & Conclusion Across all experiments, we observe consistent disparity between classification and generative task retention under layer pruning. While classification performance is largely preserved, generative reasoning degrades substantially. This gap is not explained solely by token generation: summarization tasks retain performance relatively well, whereas algorithmic tasks such as arithmetic and syntax generation remain highly sensitive to pruning, even under simplified evaluation settings (Sections 6.1, 6.2). These results indicate that pruning primarily disrupts important algorithmic capabilities rather than surface-level text generation. We see that such capabilities are difficult to restore under realistic post-training constraints. Even when arithmetic is reduced to minimal objective that avoids multi-token generation, pruned models fail to recover baseline performance. Prior work suggests that functional circuits in LLMs emerge only after training on billions of tokens (Tigges et al., 2024), which may explain why recovery has previously been observed only with large-scale data and compute (Sreenivas et al., 2024). Under the constrained settings studied here, once these circuits are disrupted by pruning, they appear difficult to reconstruct. This contrasts with classification tasks, which may rely on shallower or more redundant subnetworks, whereas generative reasoning may depend on deeper, non-redundant structures (Petty et al., 2024; Telgarsky, 2016). Although fine-tuning with self-generated responses consistently improves recovery compared to open-source supervision, persistent gap remains between classification and generative performance, indicating an intrinsic limitation of post-pruning recovery. Notably, even under highly favorable conditions, including task-aligned train set and full supervised fine-tuning, generative reasoning performance remains substantially below baseline (see Appendix A.6.2). This suggests that the observed limitations are not merely due to suboptimal training. Taken together, our results indicate that layer pruning should be applied conservatively when generative reasoning is priority. Moderate pruning ratios combined with self-generated Figure 8. Code evaluation outcomes across model families on MBPP+ and HumanEval+. Green and blue denote syntactically valid code, with green passing all tests and blue failing assertions; remaining categories correspond to invalid code due to syntax or execution errors. As discussed in Section 4.3, coding benchmarks are particularly sensitive to layer pruning, as they require strict syntactic correctness in addition to semantic reasoning. Figure 8 summarizes post-recovery code evaluation results across model families. Even after finetuning, pruned models continue to have difficulty generating syntactically valid code. This issue is more pronounced on MBPP+, where code must be generated from natural language descriptions, compared to HumanEval+, which provides function signature as prefix. Overall, finetuning only partially restores syntactic abilities after pruning. Across models, we observe persistent errors such as undefined variables and unbalanced parentheses, consistent with the failure modes identified in Section 4.3. The prevalence of invalid and logically incorrect code indicates that pruning disrupts structural mechanisms required to maintain syntactic and state consistency during genera-"
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "supervision offer practical trade-off under constrained settings, whereas aggressive depth reduction is unlikely to preserve algorithmic reasoning abilities. We hope this work clarifies the limits of layer pruning and informs more principled compression strategies for generative language models."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "Acknowledgements",
            "content": "This work is submitted in part by the NYU Abu Dhabi Center for Artificial Intelligence and Robotics, funded by Tamkeen under the Research Institute Award CG010. The experiments were carried out on the High Performance Computing resources at New York University Abu Dhabi."
        },
        {
            "title": "References",
            "content": "Ashkboos, S., Croci, M. L., Nascimento, M. G. d., Hoefler, T., and Hensman, J. Slicegpt: Compress large language models by deleting rows and columns. arXiv preprint arXiv:2401.15024, 2024. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Chen, X., Hu, Y., Zhang, J., Wang, Y., Li, C., and Chen, H. Streamlining redundant layers to compress large language models. arXiv preprint arXiv:2403.19135, 2024. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dettmers, T. and Zettlemoyer, L. The case for 4-bit precision: k-bit inference scaling laws. In International Conference on Machine Learning, pp. 77507774. PMLR, 2023. Ettinger, A., Bertsch, A., Kuehl, B., Graham, D., Heineman, D., Groeneveld, D., Brahman, F., Timbers, F., Ivison, H., et al. Olmo 3. arXiv preprint arXiv:2512.13961, 2025. Frantar, E. and Alistarh, D. Sparsegpt: Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pretrained transformers. arXiv preprint arXiv:2210.17323, 2022. Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., et al. framework for few-shot language model evaluation. Zenodo, 2021. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Gromov, A., Tirumala, K., Shapourian, H., Glorioso, P., and Roberts, D. A. The unreasonable ineffectiveness of the deeper layers. arXiv preprint arXiv:2403.17887, 2024."
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Lu, Y., Cheng, H., Fang, Y., Wang, Z., Wei, J., Xu, D., Xuan, Q., Yang, X., and Zhu, Z. Reassessing layer pruning in llms: New insights and methods. arXiv preprint arXiv:2411.15558, 2024. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019. Hu, J. and Frank, M. C. Auxiliary task demands mask the capabilities of smaller language models. arXiv preprint arXiv:2404.02418, 2024. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Kim, B.-K., Kim, G., Kim, T.-H., Castells, T., Choi, S., Shin, J., and Song, H.-K. Shortened llama: Depth pruning for large language models with comparison of retraining methods. arXiv preprint arXiv:2402.02834, 2024. Lad, V., Lee, J. H., Gurnee, W., and Tegmark, M. The remarkable robustness of llms: Stages of inference? arXiv preprint arXiv:2406.19384, 2024. LeCun, Y., Denker, J., and Solla, S. Optimal brain damage. Advances in neural information processing systems, 2, 1989. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of machine learning and systems, 6:87100, 2024. Ling, G., Wang, Z., and Liu, Q. Slimgpt: Layer-wise structured pruning for large language models. Advances in Neural Information Processing Systems, 37:107112 107137, 2024. Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:21558 21572, 2023. Ma, X., Fang, G., and Wang, X. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:2170221720, 2023. Men, X., Xu, M., Zhang, Q., Yuan, Q., Wang, B., Lin, H., Lu, Y., Han, X., and Chen, W. Shortgpt: Layers in large language models are more redundant than you expect. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 2019220204, 2025. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can new dataset arXiv preprint suit of armor conduct electricity? for open book question answering. arXiv:1809.02789, 2018. Muralidharan, S., Turuvekere Sreenivas, S., Joshi, R., Chochowski, M., Patwary, M., Shoeybi, M., Catanzaro, B., Kautz, J., and Molchanov, P. Compact language models via pruning and knowledge distillation. Advances in Neural Information Processing Systems, 37:4107641102, 2024. Narayan, S., Cohen, S. B., and Lapata, M. Dont give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint arXiv:1808.08745, 2018. Nepal, A., Shrestha, S., Shrestha, A., Kim, M., Naghiyev, J., Shwartz-Ziv, R., and Ross, K. Layer importance for mathematical reasoning is forged in pre-training and invariant after post-training. arXiv preprint arXiv:2506.22638, 2025. Petty, J., Steenkiste, S., Dasgupta, I., Sha, F., Garrette, D., and Linzen, T. The impact of depth on compositional generalization in transformer language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 72397252, 2024. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Schaeffer, R., Miranda, B., and Koyejo, S. Are emergent abilities of large language models mirage? Advances in neural information processing systems, 36:5556555581, 2023."
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2025b. Yang, Y., Cao, Z., and Zhao, H. Laco: Large language model pruning via layer collapse. arXiv preprint arXiv:2402.11187, 2024. Yin, L., Wu, Y., Zhang, Z., Hsieh, C.-Y., Wang, Y., Jia, Y., Li, G., Jaiswal, A., Pechenizkiy, M., Liang, Y., et al. Outlier weighed layerwise sparsity (owl): missing secret sauce for pruning llms to high sparsity. arXiv preprint arXiv:2310.05175, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Song, J., Oh, K., Kim, T., Kim, H., Kim, Y., and Kim, J.-J. Sleb: Streamlining llms through redundancy verification and elimination of transformer blocks. arXiv preprint arXiv:2402.09025, 2024. Song, X., Wang, K., Li, P., Yin, L., and Liu, S. Demystifying the roles of llm layers in retrieval, knowledge, and reasoning. arXiv preprint arXiv:2510.02091, 2025. Sreenivas, S. T., Muralidharan, S., Joshi, R., Chochowski, M., Mahabaleshwarkar, A. S., Shen, G., Zeng, J., Chen, Z., Suhara, Y., Diao, S., et al. Llm pruning and distillation in practice: The minitron approach. arXiv preprint arXiv:2408.11796, 2024. Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2024. Sun, W., Song, X., Li, P., Yin, L., Zheng, Y., and Liu, S. The curse of depth in large language models. arXiv preprint arXiv:2502.05795, 2025. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpaca: Strong, Replicable Instruction-Following Model, URL https://crfm.stanford. March 2023. edu/2023/03/13/alpaca.html. Stanford Center for Research on Foundation Models (CRFM). Telgarsky, M. Benefits of depth in neural networks. In Conference on learning theory, pp. 15171539. PMLR, 2016. Tigges, C., Hanna, M., Yu, Q., and Biderman, S. Llm circuit analyses are consistent across training and scale. Advances in Neural Information Processing Systems, 37: 4069940731, 2024. Wan, Z., Wang, X., Liu, C., Alam, S., Zheng, Y., Liu, J., Qu, Z., Yan, S., Zhu, Y., Zhang, Q., et al. Efficient large language models: survey. arXiv preprint arXiv:2312.03863, 2023. Wang, K., Lyu, T., Su, G., Geiping, J., Yin, L., Canini, M., and Liu, S. When fewer layers break more chains: Layer pruning harms test-time scaling in llms. arXiv preprint arXiv:2510.22228, 2025. Xia, M., Gao, T., Zeng, Z., and Chen, D. Sheared llama: Accelerating language model pre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a."
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "A. Appendix A.1. Classification and Generative Performance Discrepancy Table 3. Performance retention for various models. Results are normalized relative to baseline. Results with (*) are sourced from Lu et al. (2024). While the Gemma and Llama models are also in Lu et al. (2024), their performance on generative tasks are not reported, thus we train on our own under similar settings with the Alpaca cleaned dataset3 for evaluation."
        },
        {
            "title": "Model",
            "content": "HeSw PIQA MMLU Wino OBQA ARC-E ARC-C Mean GSM8K HumEval+ MBPP+ XSUM Mean"
        },
        {
            "title": "Generative retention",
            "content": "Gemma2-2B-It"
        },
        {
            "title": "Reverse\nBI",
            "content": "0.844 0.796 0.893 0.880 0.925 0.874 0.941 0.957 0.747 0.758 0.785 0. 0.736 0.725 0.839 0.828 0.040 0.042 LLaMA-3.1-8B-It Reverse* BI 0.679 0. 0.875 0.897 0.934 0.356 0.844 0.729 0.870 0.598 0.754 0.746 0.769 0. 0.818 0.655 0.453 0.318 Qwen2.5-7B-Instruct"
        },
        {
            "title": "Reverse\nBI",
            "content": "0.710 0.867 0.832 0.982 0.765 0.480 0.854 0.789 0.488 0.927 0.720 0. 0.570 0.708 0.706 0.804 0.012 0.097 Mistralv0.3-7B-Instruct"
        },
        {
            "title": "Reverse\nBI",
            "content": "0.832 0.827 0.863 0.848 0.769 0.823 0.886 0.896 0.523 0.705 0.849 0. 0.799 0.663 0.789 0.801 0.087 0.056 0.119 0.153 0.111 0.344 0.025 0. 0.158 0.244 0.029 0.099 0.814 0.809 0.251 0.276 0.084 0.128 0.638 0. 0.321 0.213 0.020 0.274 0.591 0.768 0.162 0.327 0.246 0.223 0.096 0. 0.147 0.143 A.2. Tokens with layer pruning Figure 9. Text degeneration results with layer pruning using N-gram repetition (left) and Self-BLEU4 score relative to baseline."
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "A.3. Arithmetic Mistake Example: Weekly Egg Production Question. Gary buys 4 chickens. After two years, he has 8 times as many chickens as he started with. If each chicken lays 6 eggs per day, how many eggs does Gary currently collect every week? Pruned Model Response. Initially, Gary buys 4 chickens. After two years, he has eight times as many chickens: 4 8 = 32 chickens. Each chicken lays 6 eggs per day. The total number of eggs laid per day is computed as: To determine the weekly total, the daily number of eggs is multiplied by 7: 32 6 = 364 eggs. (Arithmetic error) Thus, the solution concludes that Gary collects 2448 eggs per week. 364 7 = 2448 eggs. A.4. Arithmetic Ablation Experiment Details For this experiment, we rely on the EleutherAI/arithmetic. We use the single digit, three operations subset. We restrict the output space to individual digits from 0 to 9. (a) Qwen (b) Mistral Figure 10. Effect of single-layer pruning on the arithmetic ability of various models."
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "A.5. Balanced Parenthesis Error Example: Code Generation with Syntax Error Prompt. Given an array of integers, compute the minimum number of elements that must be changed to make the array palindromic. Generated Response. def smallest_change(arr): = len(arr) changes = 0 for in range(n // 2): if arr[i] != arr[n - 1 - i): changes += 1 else: continue return changes Highlighted Error. arr[n - 1 - i) (parenthesis mismatch) A.6. Finetuning Details A.6.1. QLORA WITH BI AND REVERSE PRUNING STRATEGIES Experimental Results We post-train our models using QLoRA with 4-bit NF4 quantization, learning rate of 2 104, batch size 8, constant learning rate with 50 warmup steps, bf16 training, and sequence length 8192 with gradient checkpointing on single A100 80GB GPU. For experiments with the Alpaca dataset ( 50K), we train for 2 epochs; for Dolci (Ettinger et al., 2025) ( 90K), 1 epoch. We focus on Dolci since our broader objective is to assess whether post-pruning training can preserve generative reasoning performance. We rely on QLoRA because it is comparable to even LoRA trained models in our experiments (see Table 4). We also show in A.6.3 that QLoRA closely matches performance of Full finetuning for recovery in our settings as well. Table 4. Performance retention on classification benchmarks across Gemma and Llama with various pruning strategies. LoRA-trained results from Lu et al. (2024) are marked with an asterisk (*). QLoRA results largely match or outperform LoRA trained models across various pruning strategies showing consistent > 80% performance retention. HeSw = HellaSwag, Wino = Winogrande, OBQA = OpenBookQA."
        },
        {
            "title": "Model",
            "content": "Gemma2-2B-It Reverse-order* PPL* Magnitude-L1* Magnitude-L2* BI* Taylor* QLoRA (Reverse + Dolci) QLoRA (BI + Dolci) LLaMA-3.1-8B-It Reverse-order* PPL* Magnitude-L1* Magnitude-L2* BI* Taylor* QLoRA (Reverse + Dolci) QLoRA (BI + Dolci) HeSw PIQA MMLU Wino OBQA ARC-E ARC-C Mean 0.941 0.837 0.804 0.789 0.957 0.932 0.945 0.961 0.844 0.783 0.660 0.659 0.729 0.965 1.000 0.915 0.747 0.826 0.854 0.669 0.758 0. 0.702 0.927 0.870 0.781 0.402 0.396 0.598 0.811 0.651 0.858 0.785 0.867 0.895 0.812 0.805 0.787 0.816 0.774 0.754 0.891 0.348 0.347 0.746 0. 0.863 0.786 0.736 0.706 0.730 0.594 0.725 0.723 0.708 0.698 0.769 0.735 0.389 0.389 0.549 0.807 0.813 0.780 0.839 0.808 0.793 0.714 0.828 0. 0.813 0.83 0.818 0.782 0.470 0.469 0.655 0.796 0.844 0.831 0.844 0.859 0.844 0.791 0.796 0.846 0.736 0.729 0.679 0.834 0.446 0.446 0.710 0. 0.787 0.689 0.893 0.948 0.951 0.918 0.880 0.890 0.857 0.857 0.875 0.953 0.676 0.676 0.897 0.892 0.896 0.862 0.925 0.616 0.475 0.424 0.874 0. 0.930 0.856 0.934 0.496 0.369 0.368 0.356 0.421 0.895 0."
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "Pruning Strategies In our experiments, we mainly deal with two commonly used layer pruning strategies, Block Influence (BI) and Reverse Order (Men et al., 2025; Lu et al., 2024; Sreenivas et al., 2024). We show that QLoRA with simple pruning metrics like BI and Reverse perform comparably with techniques like LoRA composed with other various pruning metrics (see Table 4). Additionally, we consider an iterative pruning procedure. The iterative procedure extends the single-layer pruning analysis in Figure 1 by greedily removing layers based on observed redundancy. Specifically, we first prune the layer whose removal leads to the smallest performance drop. With this layer removed, we then determine the layer whose removal leads to the smallest drop, and additionally prune that layer. We continue to repeat this process. This pruning strategy allows us to examine whether selectively removing redundant layers affects the recovery behavior observed in generative reasoning tasks. The full procedure is described in Algorithm 1. Algorithm 1 Greedy Iterative Pruning via Benchmark Performance Input: Model with layers, benchmark dataset D, number of layers to prune Output: Pruned layer set for = 1 to do  arg max{1,...,L}P Score(cid:0)M(P{}), D(cid:1) {} end for Return A.6.2. UPPER-BOUND RECOVERY ON GSM8K Figure 11. Comparison between full supervised finetuning (Full-FT) and QLoRA on GSM8K. To estimate an upper bound on recoverable performance under layer pruning, we conduct an experiment under highly favorable conditions. We apply Iterative pruning to the Qwen model until 7 layers using GSM8K exclusively as the calibration dataset, and subsequently finetune the pruned model only on the GSM8K training split. For each training example, we generate eight responses from the unpruned Qwen model to increase output diversity. Finetuning is performed for approximately two epochs, followed by evaluation on the GSM8K test set. Despite these idealized conditions, Figure 11 shows that performance on GSM8K cannot be fully recovered after pruning. This result underscores the difficulty of restoring generative reasoning abilities even when the pruning metric, training data, and evaluation task are perfectly aligned. A.6.3. QLORA VS. FULL FINETUNING We further compare QLoRA against full-parameter supervised finetuning (Full-FT) using self-generated Dolci data. Both methods are applied to the same Iteratively pruned Qwen model with seven layers removed, and evaluated on GSM8K as proxy for generative reasoning quality. At the scale of our experiments, QLoRA achieves recovery comparable to Full-FT, despite operating under significantly reduced memory and compute requirements. While full finetuning may yield additional gains when scaled further, doing"
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "Figure 12. Comparison between full supervised finetuning (Full-FT) and QLoRA on self-generated Dolci data. so incurs substantially higher computational cost and hardware demands. Given our focus on recovery under resourceconstrained post-training settings, we adopt QLoRA throughout the paper as practical and representative finetuning approach for studying the limits of generative reasoning recovery after layer pruning. A.7. Perplexity Curves (a) Qwen (b) Mistral Figure 13. Perplexity curves during training for both standard finetuning and for SGR for the Qwen and Mistral models (Both are BI pruned: 25%). A.8. Full Results with Self-Generated Responses A.9. Pruning at Different Ratios"
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "Figure 14. Differences between finetuning with Self-Generated Responses (SGR) vs on Dolci dataset directly for the Qwen Model. At all pruning ratios, SGR is consistently better than the raw dataset."
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "Table 5. Performance retention (normalized to baseline). Results marked with (*) are sourced from Lu et al. (2024). SGR is our method with the pruning metric in parentheses. Reverse, BI, and Iterative indicate the pruning order (reverse-order, block-interleaved, iterative), while Alpaca and Dolci denote the training data source; S.Alpaca and S.Dolci refer to self-generated variants of the corresponding datasets. / indicate improvement or degradation in mean retention of our SGR approach with respect to the standard approach of doing SFT with the open-source prompts and responses. Model Gemma2-2B-It Open-source data Reverse + Alpaca BI + Alpaca Reverse + Dolci BI + Dolci Self-Generated Responses SGR (Reverse + S.Alpaca) SGR (BI + S.Alpaca) SGR (Reverse + S.Dolci) SGR (BI + S.Dolci) LLaMA-3.1-8B-It Open-source data Reverse + Alpaca* BI + Alpaca Reverse + Dolci BI + Dolci Iterative + Dolci Self-Generated Responses SGR (Reverse + S.Alpaca) SGR (BI + S.Alpaca) SGR (Reverse + S.Dolci) SGR (BI + S.Dolci) SGR (Iterative + S.Dolci) Qwen2.5-7B-Instruct Open-source data Reverse + Alpaca BI + Alpaca Reverse + Dolci BI + Dolci Iterative + Dolci Self-Generated Responses SGR (Reverse + S.Alpaca) SGR (BI + S.Alpaca) SGR (Reverse + S.Dolci) SGR (BI + S.Dolci) SGR (Iterative + S.Dolci) Mistralv0.3-7B-Instruct Open-source data Reverse + Alpaca BI + Alpaca Reverse + Dolci BI + Dolci Iterative + Dolci Self-Generated Responses SGR (Reverse + S.Alpaca) SGR (BI + S.Alpaca) SGR (Reverse + S.Dolci) SGR (BI + S.Dolci) SGR (Iterative + S.Dolci) HeSw PIQA MMLU Wino OBQA ARC-E ARC-C Mean GSM8K HumEval+ MBPP+ XSUM Mean Classification Generative 0.736 0.725 0.708 0.698 0.849 0.849 0.824 0.800 0.769 0.549 0.813 0.887 0.747 0.772 0.912 0.879 0.879 0. 0.570 0.708 0.524 0.626 0.727 0.633 0.748 0.576 0.693 0.775 0.799 0.663 0.601 0.608 0.621 0.812 0.751 0.758 0.772 0.657 0.839 0.828 0.813 0.829 0.854 0.846 0.834 0.859 0.818 0.655 0.844 0.878 0.838 0.843 0.914 0.895 0.903 0.880 0.706 0.804 0.666 0.784 0.798 0.736 0.796 0.668 0.805 0.799 0.789 0.801 0.775 0.785 0.762 0.859 0.818 0.850 0.821 0.780 0.040 0.042 0.129 0.154 0.047 0.067 0.164 0.259 0.453 0.318 0.405 0.469 0.328 0.561 0.724 0.628 0.758 0.647 0.012 0.097 0.059 0.270 0.294 0.025 0.202 0.153 0.329 0. 0.087 0.056 0.375 0.236 0.293 0.182 0.102 0.421 0.395 0.230 0.119 0.153 0.219 0.095 0.103 0.186 0.256 0.389 0.111 0.344 0.434 0.444 0.301 0.290 0.412 0.556 0.634 0. 0.025 0.167 0.159 0.200 0.333 0.051 0.183 0.142 0.283 0.325 0.158 0.244 0.316 0.316 0.244 0.282 0.264 0.526 0.509 0.402 0.029 0.099 0.121 0.113 0.212 0.164 0.121 0. 0.084 0.128 0.302 0.308 0.245 0.162 0.481 0.251 0.390 0.312 0.020 0.274 0.120 0.278 0.358 0.024 0.288 0.157 0.398 0.374 0.246 0.223 0.488 0.285 0.262 0.569 0.569 0.875 0.715 0. 0.814 0.809 0.763 0.778 0.856 0.851 0.768 0.876 0.638 0.062 0.077 0.068 0.338 0.179 0.860 0.029 0.754 0.763 0.591 0.768 0.541 0.817 0.812 0.605 0.846 0.600 0.861 0. 0.096 0.048 0.108 0.072 0.096 0.054 0.084 0.095 0.066 0.042 0.251 0.276 0.308 0.285 0.304 0.317 0.327 0.455 0.321 0.213 0.304 0.322 0.303 0.298 0.619 0.366 0.634 0.536 0.162 0.327 0.220 0.391 0.449 0.176 0.380 0.263 0.468 0.525 0.147 0.143 0.322 0.227 0.224 0.272 0.255 0.479 0.421 0.292 0.844 0.796 0.736 0.729 0.783 0.756 0.732 0. 0.679 0.710 0.787 0.793 0.787 0.682 0.838 0.809 0.826 0.792 0.710 0.867 0.681 0.820 0.827 0.740 0.891 0.678 0.851 0.852 0.832 0.827 0.792 0.813 0.806 0.876 0.860 0.861 0.854 0. 0.893 0.880 0.857 0.857 0.861 0.847 0.805 0.857 0.875 0.897 0.896 0.886 0.896 0.860 0.913 0.920 0.907 0.907 0.832 0.982 0.794 0.958 0.910 0.856 0.982 0.788 0.982 0. 0.863 0.848 0.834 0.848 0.886 0.890 0.873 0.863 0.857 0.871 0.925 0.874 0.930 0.856 0.930 0.874 0.937 0.904 0.934 0.356 0.895 0.943 0.789 0.909 0.967 0.963 0.984 0. 0.765 0.480 0.703 0.507 0.700 0.848 0.540 0.731 0.545 0.699 0.769 0.823 0.844 0.877 0.685 0.891 0.924 0.929 0.908 0.718 0.941 0.957 0.945 0.961 0.984 0.888 0.940 0. 0.844 0.729 1.000 1.027 0.926 0.926 1.049 0.979 1.033 1.022 0.854 0.789 0.826 0.854 0.859 0.846 0.826 0.732 0.841 0.852 0.886 0.896 0.941 0.951 0.851 0.866 0.856 0.906 0.911 0. 0.747 0.758 0.702 0.927 0.702 0.843 0.758 0.955 0.870 0.598 0.651 0.710 0.828 0.947 0.769 0.799 0.769 0.947 0.488 0.927 0.488 0.878 0.732 0.488 0.732 0.516 0.854 0. 0.523 0.705 0.591 0.614 0.682 0.773 0.614 0.750 0.614 0.682 0.785 0.805 0.816 0.774 0.866 0.866 0.845 0.821 0.754 0.746 0.863 0.896 0.893 0.807 0.950 0.917 0.924 0. 0.720 0.874 0.650 0.843 0.828 0.741 0.852 0.652 0.867 0.869 0.849 0.843 0.821 0.786 0.804 0.903 0.851 0.883 0.831 0."
        },
        {
            "title": "On the Limits of Layer Pruning for Generative Reasoning in LLMs",
            "content": "Table 6. Retention on generative benchmarks under increasing pruning levels. We report results on GSM8K, HumanEval, MBPP, and XSUM. Average denotes the mean recovery across benchmarks. Method GSM8K HumanEval MBPP XSUM Average"
        },
        {
            "title": "2 Layers Pruned\nBI\nIterative\nReverse",
            "content": "0.878 0.940 0."
        },
        {
            "title": "3 Layers Pruned\nBI\nIterative\nReverse",
            "content": "0.878 0.896 0."
        },
        {
            "title": "5 Layers Pruned\nBI\nIterative\nReverse",
            "content": "0.538 0.819 0."
        },
        {
            "title": "7 Layers Pruned\nBI\nIterative\nReverse",
            "content": "0.329 0.591 0."
        },
        {
            "title": "2 Layers Pruned\nBI\nIterative\nReverse",
            "content": "0.932 0.938 0."
        },
        {
            "title": "3 Layers Pruned\nBI\nIterative\nReverse",
            "content": "0.926 0.946 0."
        },
        {
            "title": "6 Layers Pruned\nBI\nIterative\nReverse",
            "content": "0.818 0.806 0."
        },
        {
            "title": "8 Layers Pruned\nBI\nIterative\nReverse",
            "content": "0.758 0.671 0.628 0.709 0.736 0.793 0.709 0.421 0.685 0.496 0.458 0.455 0.398 0.358 0.157 0.726 0.631 0. 0.558 0.603 0.502 0.530 0.497 0.341 0.390 0.312 0.251 0.906 0.930 0.925 0.906 0.910 0.891 0.822 0.853 0. 0.861 0.797 0.600 0.977 0.968 0.037 0.963 0.958 0.023 0.788 0.894 0.025 0.754 0.758 0.029 0.796 0.856 0. 0.796 0.715 0.795 0.566 0.697 0.547 0.468 0.524 0.263 0.861 0.848 0.573 0.800 0.827 0.538 0.698 0.691 0. 0.634 0.552 0.366 0.692 0.817 0.950 0.692 0.633 0.817 0.409 0.658 0.476 0.283 0.350 0.142 0.811 0.855 0. 0.755 0.800 0.733 0.655 0.566 0.700 0.634 0.466 0."
        }
    ],
    "affiliations": [
        "Department of Computer Science, New York University Abu Dhabi"
    ]
}