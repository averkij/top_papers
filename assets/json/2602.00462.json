{
    "paper_title": "LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs",
    "authors": [
        "Benno Krojer",
        "Shravan Nayak",
        "Oscar Mañas",
        "Vaibhav Adlakha",
        "Desmond Elliott",
        "Siva Reddy",
        "Marius Mosbach"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations."
        },
        {
            "title": "Start",
            "content": "LATENTLENS: Revealing Highly Interpretable Visual Tokens in LLMs Benno Krojer 1 2 Shravan Nayak 1 3 Oscar Mañas 1 3 Vaibhav Adlakha 1 2 Desmond Elliott * 4 Siva Reddy * 1 2 5 Marius Mosbach * 1 2 Abstract Transforming large language model (LLM) into vision-language model (VLM) can be achieved by mapping the visual tokens from vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LATENTLENS, novel approach for mapping latent representations to descriptions in natural language. LATENTLENS encodes large text corpus and stores contextualized token representations for each token in that corpus. Visual token representations are then compared to these contextualized representations and the top-k nearest neighbor representations serve as descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LATENTLENS instead, the majority of visual tokens are interpretable across all studied models and all layers. More broadly, our findings contribute new evidence on the alignment between vision and language representations and open up new directions for analyzing the latent representations of LLMs. LATENTLENS Demo Code 6 2 0 F 9 ] . [ 2 2 6 4 0 0 . 2 0 6 2 : r 1. Introduction Transforming large language model (LLM) into visionlanguage model (VLM) can be as simple as training linEqual senior contribution. 1Mila Quebec AI Institute, Montreal, Canada 2McGill University, Montreal, Canada 3Université de Montréal, Montreal, Canada 4University of Copenhagen, Copenhagen, Denmark 5Canada CIFAR AI Chair. Correspondence to: Benno Krojer <benno.krojer@mila.quebec>. Preprint. 1 Figure 1. Illustration of our method. LATENTLENS compares latent representations of visual tokens to contextualized text representations obtained from full sentence descriptions. ear transformation or shallow MLP that maps visual representations into the embedding space of frozen LLM (Tsimpoukelli et al., 2021; Merullo et al., 2023; Liu et al., 2023; Beyer et al., 2024; Mañas et al., 2023, inter alia). While VLMs can be further improved via multiple stages of fine-tuning (Bai et al., 2023; Cho et al., 2025), the empirical success of frozen LLMs processing non-language inputs raises an important question: Why is it so easy to adapt an LLM to process data from other modalities? It has been hypothesized that LLMs are universal computation engines that can process arbitrary modalities with minimal adaptation (Lu et al., 2022; Shen et al., 2023; García-de Herreros et al., 2024). Through training on vast amounts of natural language, LLMs may implicitly learn about the physical world, form priors about visual properties (Han et al., 2025), or even learn more sophisticated world model (Patel & Pavlick, 2022). It has also been argued that separately trained vision and language representations may converge to shared structure (Huh et al., 2024), which could facilitate training simple projections between them. However, these hypotheses do not explain how visual representations are integrated inside an LLM and its representation space. Are the visual tokens processed by an LLM interpretable, i.e., do their representations correspond to semantically meaningful language? Existing work suggests that visual tokens are rarely interpretable at the input level LATENTLENS via nearest neighbors in the language model embedding space (Mokady et al., 2021; Neo et al., 2025, EmbeddingLens). Recent work has explored the utility of LogitLens, which uses the LLM unembedding matrix (nostalgebraist, 2020), to interpret and analyze visual token representations (Neo et al., 2025; Jiang et al., 2025b). Finally, trainingbased methods such as sparse autoencoders (Cunningham et al., 2023; Venhoff et al., 2025) or supervised probing (Fu et al., 2025) have also been applied to understand what visual representations encode. However, both embeddingspace and training-based methods are inconclusive about the interpretability of visual tokens in LLMs. In this work, we propose LATENTLENS, novel interpretability method for analyzing latent representations in VLMs. LATENTLENS is training-free and provides finegrained sentence-level descriptions of latent representations. Our key insight is that the most natural comparison for visual token representations are contextual text representations, and not the LLM embedding or unembedding matrix. Thus, LATENTLENS compares visual token representations to their nearest neighbors in large pool of contextualized token representations from intermediate LLM layers. For example, in Figure 1, the visual token of building yields the nearest neighbor stories in the context of ... building with many stories . Empirically, we analyze the interpretability of visual token representations across layers for 10 different VLMs. We find that compared to other training-free approaches (LogitLens and EmbeddingLens), LATENTLENS reveals consistently high interpretability, highlighting that previous methods substantially underestimate the interpretability of visual token representations. Averaged across all models and layers we study, LATENTLENS renders 72% of visual tokens interpretable (according to VLM-judge). When using EmbeddingLens and LogitLens, however, only 30% and 23% of visual token representations are labeled as interpretable, respectively. Through ablation studies, we provide additional insights about the nature of this interpretable alignment, showing, e.g., that even linear projections lead to interpretable visual token representations. We also find evidence for Mid-Layer Leap in the learned projection: the visual token representations at the input and early layers align most strongly to contextualized representations from mid-layers (e.g., layers 816), suggesting that the learned projection targets semantic rather than lexical representations. We also present qualitative analyses showing that LATENTLENS produces rich sentence-level descriptions, unlike LogitLens which might return subwords or next-token predictions. Overall, our findings challenge existing assumptions about the interpretability of visual tokens, and offer new insights about the alignment of vision and language representations. We encourage the reader to explore the interactive demo and will release LATENTLENS as package with easy access to our database of contextual embeddings to facilitate its adoption. 2. Background We first introduce technical background on VLMs and describe prior work on analyzing their latent representations. 2.1. Turning LLMs into VLMs common approach for converting an LLM into VLM is to project representations produced by vision encoder into the embedding space of the LLM via learned connector. More formally, let venc be pre-trained vision encoder, which produces sequence of image embeddings venc(ximg) = [v1, v2, . . . , vTv ] with vi Rdv for given image ximg, let llm be pre-trained language model, and let xtext be textual input. multimodal input is constructed as follows: First, xtext is processed by tokenizer tok and then converted into token embeddings via an embedding matrix Eemb RVd. Second, visual tokens representing ximg are obtained by projecting each image embedding vi into the embedding space of llm using projection function1 proj : Rdv (cid:55) Rd. Finally, the visual and textual representations are concatenated = [p1, . . . , pTv , e1, . . . , eTt], where pi = proj (vi) Rd = 1, . . . , Tv and e1, . . . , eTt = emb(tok(xtext)) RTtd. The resulting multimodal sequence is then processed by the llm and converted into latent representations h(ℓ) Rd, where denotes the position in the sequence and ℓ layer, respectively.2 During the forward pass, textual representations can self-attend to the information encoded in the visual representations h(ℓ) . Training. Given dataset = {(ximg, xtext)i}N i=1 of imagecaption pairs, the llm can be trained by minimizing the cross-entropy loss over the target caption tokens: LLM = (cid:88) t=Tinstr+1 log p(yt y<t, ximg), where: tok(xtext) = [y1, . . . , yTinstr , yTinstr+1, . . . , yT (cid:123)(cid:122) (cid:125) (cid:124) (cid:125) caption (cid:123)(cid:122) instruction unless stated otherwise, only the weights of proj are trained, with the llm and venc weights frozen. ], and, (cid:124) 2.2. Interpreting VLM representations An important open question is what exactly is encoded in visual token representations as they are processed by 1In practice, proj is usually linear layer, an MLP, or an attention-based module (Liu et al., 2023; Wang et al., 2024). 2Color-coding is used for visual token representations (h(ℓ) ). 2 LATENTLENS an LLM, and to what extent are the latent visual token representations (h(ℓ) ) interpretable as language-like tokens? We will briefly introduce popular approaches for studying these questions. We note that other supervised or promptingbased approaches exist such as probing (Belinkov, 2022), SAEs (Cunningham et al., 2023), LatentQA (Pan et al., 2024) or Patchscopes (Ghandeharioun et al., 2024). Instead, we focus on methods which are training-free and directly leverage the LLMs representation space. EmbeddingLens. The simplest approach to interpret visual token representations is to compare them to the elements of the embedding matrix Eemb of llm. This is motivated by the projection layer being trained to output representations that are compatible with the embedding space of llm. For this approach, each pi (or its corresponding latent h(ℓ) at layer ℓ) is compared to the embeddings in llms embedding matrix Eemb via cosine similarity and the top-k most similar embeddings serve as labels. EmbeddingLens has been previously used for interpreting soft prompts (Lester et al., 2021), visual tokens (Mokady et al., 2021; Jiang et al., 2025a) or speech (Ògúnrèmí et al., 2025). LogitLens. slightly different but conceptually very similar approach is the LogitLens (nostalgebraist, 2020). Instead of comparing latent representations directly to embeddings, LogitLens projects the latent representation to the unembedding space of the model by multiplying h(ℓ) with the unembedding matrix Wunemb RdV, obtaining distribution over vocabulary items. Then, the top-k vocabulary items, i.e., those with the largest logits, are retrieved as labels. LogitLens been widely used in previous work analyzing LLMs (Geva et al., 2022; 2023; Fierro et al., 2025), and has recently been adopted for VLMs (Shukor & Cord, 2024; Neo et al., 2025; Jiang et al., 2025b). 3. LATENTLENS In this section, we provide unifying perspective on existing training-free methods for interpreting latent representations and introduce LATENTLENS, novel method for mapping latent representations to natural language descriptions. 3.1. Unifying existing lenses EmbeddingLens and LogitLens share the same goal and setup: To map latent representation h(ℓ) Rd at position and layer ℓ, to natural language description. Here, we provide unified perspective on both methods. Formally, let be set of candidate descriptions, where each description dj is associated with vector rj Rd. h(ℓ) is mapped to description dj in three steps: 1 Compute scores: For each rj, compute score using similarity function : Rd Rd (cid:55) R: , rj), 2 Select top-k vectors: Ii,ℓ = arg top-k({sj}C 3 Return descriptions: {dj : Ii,ℓ}. = 1, . . . , C. sj = (hℓ j=1). The set of possible descriptions for EmbeddingLens and LogitLens is given by the language model vocabulary, i.e., = V. The similarity function is either cosine similarity with the rows of the embedding matrix Wemb (EmbeddingLens) or the inner product with the output embedding matrix Wunemb (LogitLens). Finally, in both cases, the vocabulary items are sorted based on their similarity score and the top-k elements are selected as description of h(ℓ) .3 Limitations of prior lenses. Under our unified framework, two limitations of EmbeddingLens and LogitLens become apparent: 1) The description set is limited to model vocabulary V, containing only (sub-word) tokens. 2) Latent representations h(ℓ) from different layers are always compared to the same reference vectors, which are either the input or output embeddings. However, it is unclear whether the output or input embedding space is the most natural representation space to compare against. For example, LogitLens tends to works best for later layers, closer to the models output embedding space, and reliability can vary significantly across models (Geva et al., 2021; Belrose et al., 2023). 3.2. Method We propose LATENTLENS, novel interpretability method for mapping latent representations to descriptions in natural language. The key idea of LATENTLENS is that the natural point of comparison for latent representations are other contextualized LLM representations to serve as potential nearest neighbors, i.e., token in the context of sentence. Additionally, we believe that limiting the set of descriptions to individual sub-word tokens is unnecessarily restrictive and instead propose to use large corpus of multiple-token sequences, e.g., sentences, which provides semantically richer descriptions for interpretation. Concretely, LATENTLENS works as follows (see also Figure 2): Let be (large) corpus of sentences and be an LLM with layers. We construct set of reference vectors by encoding every sequence dj with and storing the contextualized token representations r(ℓ) at every position of the sequence and every layer ℓ of the model. 3We note that both methods can be seen as unsupervised version of linear probing (Belinkov, 2022), where instead of learning linear transformation Rdk using supervised data, one relies on the pre-trained embeddding or unembedding matrix. 3 LATENTLENS concrete (directly visible), abstract (conceptually related), or global (present elsewhere in image). visual token is interpretable if at least one of the top-5 descriptions is classified as interpretable. We, the authors, validate this judge against human annotations across all three methods (EmbeddingLens, LATENTLENS, and LogitLens), totaling 1,020 instances. We find substantial agreement between the LLM judge and humans with Cohens κ = 0.68. Judge prompt and details on human annotation are in Section C. 4. Experiments 4.1. Experimental setup Models and training. Unless stated otherwise, we train the projection function between different combinations of vision encoders and LLMs using controlled setup. We use three LLMs (OLMo-7B (Groeneveld et al., 2024), Qwen2-7B (Yang et al., 2024), LLaMA3-8B (Grattafiori et al., 2024)), and three vision encoders (CLIP-ViT-L/14336 (Radford et al., 2021), DINOv2-L-336 (Oquab et al., 2023), and SigLIP-so400M-patch14-384 (Zhai et al., 2023)), resulting in total of 9 model combinations. We note that compared to the other vision encoders, DINOv2-L336 was pre-trained without any textual supervision. Models are trained following Molmo using the PixMo-Cap dataset (Deitke et al., 2025), with captions averaging 167 words and 9 sentences each. The trained projector proj is 3-layer MLP and all other weights remain frozen. To reduce confounding factors, we directly use the patchified image as the input to each model. All models are trained for 12K steps with an effective batchsize of 32. Captioning performance. We verify that our models produce reasonable captions using DCScore (Ye et al., 2025), GPT-4o-based judge that rates captions on 110 scale across fine-grained criteria (faithfulness, detail accuracy, hallucinations, completeness). Our models average 6.0/10, with CLIP and SigLIP encoders reaching an average of 6.8 while DINOv2 models score lower at avg. 4.4, likely due to DINOv2s lack of language supervision during pre-training. For reference, off-the-shelf Qwen2-VL-7B-Instruct (Wang et al., 2024) achieves score of 8.5/10. See Section K. LATENTLENS setup. We use 2.99M Visual Genome captions (Krishna et al., 2017) as our corpus of descriptions. All captions are encoded by each individual LLM, and we store all contextual token representations for layers ℓ {1, 2, 4, 8, 16, 24, L-2, L-1} as reference vectors r(ℓ) .5 LLM-judge setup. Although LATENTLENS retrieves full sentence descriptions, we only provide the full words corresponding to the top-5 contextualized token representations 5Due to memory constraints we store at most 20 different contextual representations per token in the models vocabulary. Figure 2. Illustration of LATENTLENS. (1) Contextualized token representations are precomputed in multiple layers of an LLM using large corpus of descriptions. (2) Latent representations of visual tokens are extracted from all layers of the LLM, and (3) compared against the precomputed contextualized token representations. The interpretability of the visual token based on its top-k descriptions can be automatically evaluated by VLM-judge. and every r(ℓ) To analyze the latent representation h(ℓ) of visual token at position and layer ℓ, we compute the cosine similarity between h(ℓ) R, obtain the top-k reference vectors with the highest similarity, and return their corresponding sequences as descriptions. Following the example in Figure 2, visual token representation h(ℓ) may have nearest neighbor r(ℓ) , of the contextualized representation of the token clocks in the sentence: stone tower with gold clocks . Compared to EmbeddingLens and LogitLens, LATENTLENS can naturally be applied at every layer of model and provides full sentence descriptions, allowing for more fine-grained interpretations. 3.3. Evaluating interpretability Determining whether visual token representation is interpretable requires careful consideration of what is semantic match between the description provided by an interpretability method and the underlying image patch. This is complex task, where semantic matches can take many different surface forms. We use GPT-5 (Hurst et al., 2024) as judge to automate this process. Given an image with red bounding box highlighting the visual token region (cf. Figure 1) plus the 8 surrounding visual tokens, as well as the top-5 descriptions returned by either EmbeddingLens, LogitLens, or LATENTLENS, we prompt the judge to determine whether description is interpretable, and to classify such cases as 4While we focus on sentence-level descriptions, our approach can be trivially extended to phrases or even entire paragraphs. LATENTLENS Figure 3. Interpretability of visual tokens across layers using three different lenses. Each curve shows the percentage of interpretable visual tokens per layer across model. (a) EmbeddingLens: large number of visual tokens is interpretable for OLMo variants but less for Llama3 and Qwen2. (b) LogitLens: low interpretability at early layers with stark increase at later layers for most models. (c) LATENTLENS: the majority of visual tokens are interpretable across all models and layers. as descriptions. We make this decision for two reasons: 1) We found that, in contrast to human annotators, the LLMjudge can get distracted by the sentence-level context, giving inconsistent results. 2) This setup provides fair comparison to EmbeddingLens and LogitLens (both only return individual tokens as descriptions) by relying on the same exact judge prompt across all methods. We note that for LATENTLENS, this approach may even underestimate interpretability and we further investigate the benefit of sentencelevel descriptions in Section 5. 4.2. Visual token representations are consistently interpretable across layers The main question we investigate is how interpretable are visual token representations as they are processed by LLMs? To answer this question, we randomly sample 100 image patches from 100 images6 from the PixMo-Cap validation set and compare the fraction of interpretable representations for LATENTLENS, EmbeddingLens, and LogitLens using the LLM judge described in 4.1. Results. Figure 3 shows the fraction of interpretable visual token representations according to LATENTLENS, EmbeddingLens, and LogitLens for all models and layers. For EmbeddingLens (3a), we find that reasonably large number of visual tokens are interpretable for some models, e.g., for all Olmo-based variants, 4060% of visual tokens are interpretable from the input layer onward. For Qwen2-based models, however, only small fraction of visual tokens are interpretable across layers (less than 20%). Llama3-based models fall in between with 2040% of the visual tokens being labelled as interpretable. less For LogitLens (3b), we find that for all models, than 20% of visual token representations are labelled as interpretable at lower layers. For all models except Llama3 + DinoV2, Llama3 + SigLIP, and Qwen2 + SigLIP much more visual token representations are interpretable at the later layers, e.g., 6080% for all OLMo-based models from layer 24 onward. This is consistent with the limitation in particular its of LogitLens discussed in Section 3, applicability only for layers close to the output layer. In contrast, using LATENTLENS (3c) results in consistently higher interpretability scores across all layers. Moreover, there are only minor differences in interpretability scores across model combinations: almost all models have interpretable visual tokens ranges from 6080% across all layers (Qwen2 + CLIP is the only exception with lower fraction of visual tokens labeled as interpretable). We perform series of ablations (see Section for details), confirming that our findings are not tied to our specific training setup. We find no substantial change in LATENTLENS interpretability when reducing the expressivity of the mapping to linear, or when training on much shorter captions. Overall, our results indicate that previous methods underestimate the interpretability of visual tokens and demonstrate the importance of using the right lens for analyzing latent representations. It is particularly interesting that visual tokens from DINOv2, with no explicit linguistic supervision, show consistently high interpretability with all three lenses.7 Returning to our research question of whether visual tokens appear like interpretable words to the LLM, we can now answer: While visual token representations are not necessarily mapped one-to-one to the vocabulary of the LLM, they are often similar to contextual token representations which are semantically related to the image contents. 4.3. Mid-Layer Leap: Visual token representations tend to align to later layer text representations LATENTLENS allows us to compare visual token representations at layer ℓ to contextual token representations from any LLM layer. We now investigate which latent textual repre6To lower API costs, we test 100 patches, resulting in 3 methods 100 patches 9 models 9 layers = 24.3K calls. 7The ability of DINOv2 to predict linguistic attributes of visual concepts has recently been reported (Oneata et al., 2025). LATENTLENS Figure 4. The Mid-Layer Leap: early visual tokens align to later LLM layers. For visual tokens at different stages of LLM processing, we compute their top-5 Nearest Neighbors from all other LLM layers. We find that early visual tokens, even at the input itself, align most to middle layers, e.g., layer 8 or 16. Some model combinations align most to constant layer throughout processing, such as LLaMA3 variants. We analyze the L2 norm distributions and potential outlier effects in Section E. sentations are most similar to visual token representations. Given visual token representation at layer ℓ, we obtain the top-5 contextualized token representations with the highest cosine similarity and report the layer these contextualized representations are obtained from. Results. Figure 4 shows the results of this experiment for all 9 model combinations. Surprisingly, visual token representations from early layers, and even the input layer, tend to have higher cosine similarity not with contextualized token representations from the same layer but instead with those from later layers. For example, with OLMo-7B + SigLIP, for visual token representations at layer 0, the majority of the nearest neighbor contextualized representations are from layer 8 of the LLM. Only once we reach mid-layers such as layer 8, we see diagonal pattern where visual token representations are closest to contextualized representations from the same layer. For other model combinations, results are even more surprising. For Qwen2-7B + SigLIP, the most similar representations for visual tokens from any layer are always from layer 16. Overall, these results suggests that the visual token representations align the most with more contextualized LLM representations rather than lexical representations at the input level. We describe this as the Mid-Layer Leap phenomenon and provide additional analyses of this finding in Sections and F, investigating how much visual token representations change across layers and whether rogue dimensions (Timkey & van Schijndel, 2021) dominate the cosine similarity. We find that visual token representations change very little throughout layers, and no systematic eviFigure 5. Interpretability of visual tokens in off-the-shelf Qwen2-VL-7B-Instruct. We apply LATENTLENS and baselines to an off-the-shelf model that deviates from our controlled setup in many ways (e.g. everything finetuned). We observe the same pattern of LATENTLENS substantially outperforming the baselines. dence for rogue dimensions, respectively. 4.4. Results generalize to off-the-shelf VLMs Finally, we show that LATENTLENS can be applied to any VLM by replicating our main findings on Qwen2-VL-7BInstruct (Wang et al., 2024), strong off-the-shelf model. Results. Figure 5 shows the fraction of interpretable visual token representations across layers for EmbeddingLens, LogitLens, and LATENTLENS. We again find that LATENTLENS leads to high interpretability (6073%) across all layers, demonstrating that our method generalizes to an off-the-shelf model. For EmbeddingLens and LogitLens, only small fraction of visual token representations are labeled as interpretable. We also analyze which contextualized token representations the visual token representations are most similar (results are in Figure 12). We obtain similar results as for the controlled setup, finding that visual token representations at layer 0 align most to contextualized text representations from layer 4, but from layer 1 onward visual token representations align mostly with contextualized text representations from the same layer (forming diagonal pattern). 5. Qualitative results The previous section showed that under the right lens, visual token representations are highly interpretable. Here, we provide qualitative examples for the interpretability provided by LATENTLENS, and compare to LogitLens. LATENTLENS vs. LogitLens descriptions. Figure 6 shows qualitative examples of the highest-scoring descriptions for two image patches extracted using LATENTLENS and LogitLens (see Section and our interactive demo for additional examples). LATENTLENS descriptions are seLATENTLENS Figure 6. Qualitative examples. Highest-scoring descriptions are extracted from different combinations of LLMs and Vision Encoders using both LATENTLENS and LogitLens. Each image is shown in its patchified format, where the green patches are expected to be interpretable according to the automatic judge. The described patch is shown with red outline, and the magnitude of scores are shown in parentheses. For LATENTLENS, the contextualized token used for the similarity calculation is shown in bold. Best viewed in colour. mantically more meaningful than those of LogitLens. For example, even when LogitLens yields some interpretable tokens at late layer on patch of church tower (first row), LATENTLENS provides highly accurate nearest neighbors across all layers such as large tower with clocks . We also note that for LATENTLENS, the magnitude of the cosine similarity typically increases at deeper layers, i.e., the visual token representations become more similar to their textual nearest neighbors. For LogitLens, on the other hand, higher similarity score (logit) does not necessarily translate into more interpretable descriptions. Furthermore, LogitLens descriptions often include unrenderable tokens, subwords, punctuation, and unrelated non-English tokens in Chinese. With LATENTLENS, however, we can simply merge the top matching token into full word with adjacent subwords (since we have access to the entire sentence), if necessary, such as + elf + ry into belfry. Overall, these examples highlight the advantage of LATENTLENS: full-word and even sentence-level descriptions are more interpretable than subword tokens. We quantify this advantage of full-sentence interpretations in Section I. Visually rendered text. Images containing rendered text yield consistently interpretable results when using LATENTLENS. In Figure 7, we show an example for OLMo-7B + CLIP ViT-L/14-336. With LATENTLENS, the highest scoring contextualized representations correspond exactly to the words in the image.8 When using LogitLens, on the other hand, we observe plausible next-token predictions which do not necessarily indicate what the visual token intrinsically encodes. For example, on the visual token where LA8For DINOv2, we would see generic words, e.g., screenshot. Figure 7. LATENTLENS vs. LogitLens results on rendered text for OLMo + CLIP ViT-L/14-336. LogitLens predicts plausible next tokens. We omit the surrounding sentence-level context for simplicity of visualization. TENTLENS predicts couch , LogitLens would predict es or potato instead. In other instances, LogitLens would even apply correct next-token-prediction on the rendered text and predict Tomato . 6. Related Work We situate our work in the literature on understanding the mapping between vision and language models, the interpretability of VLMs, and analysing LLM representations. Connecting (frozen) vision and language models. Several works have shown that LLMs can easily be adapted to process multimodal inputs (Tsimpoukelli et al., 2021; Mañas et al., 2023; Merullo et al., 2023; Lu et al., 2022), e.g., via small MLP or attention-based modules. Current state-ofthe art VLMs (Yang et al., 2025; Deitke et al., 2025; Li et al., 2025) follow the same mapping paradigm but include 7 LATENTLENS more sophisticated image token pre-processing, adaptation training stages or connector designs. Some works, in spirit related to us, have also explored representing visual tokens explicitly as weighted sum of the LLM vocabulary (Liao et al., 2025; Masry et al., 2025), instead of vanilla MLP projector. Various works study why an LLM can so easily adapt to non-linguistic inputs: Lu et al. (2022) frame LLMs as universal computation engines that can process any data sequence with minimal weight updates. Patel & Pavlick (2022) argue how LLMs, only trained on language, nonetheless learn implicit world models of the physical world, e.g., of color RGB spaces. Han et al. (2025) disentangle the visual priors learned during LLMs text-only pre-training into perception and reasoning priors. Our setup of understanding the frozen alignment between vision and language models is most similar to (Merullo et al., 2023), (Lu et al., 2022) and (Shukor & Cord, 2024), all of which explicitly keep the LLM frozen to characterize how vision integration is nonetheless possible. Interpreting VLMs. Various interpretability methods, often initially developed for unimodal text or vision models (nostalgebraist, 2020; Cunningham et al., 2023; Belinkov, 2022), have been applied to VLMs (Neo et al., 2025), e.g., to characterize cross-modal concepts (Papadimitriou et al., 2025), cross-modality circuits (Nikankin et al., 2025), and the role of attention mechanisms in extracting information from visual tokens (Neo et al., 2025; Zhang et al., 2025). Most closely related to us are works that ask what given visual token encodes in VLMs. Aside from training-based approaches like probes on visual tokens (Fu et al., 2025), many approaches leverage the inherent embedding space of the LLM to interpret visual tokens: E.g., Mokady et al. (2021) and Jiang et al. (2025a) study whether the LLM embedding matrix can interpret visual tokens, but only show qualitative examples on 1-2 models. LogitLens has been more widely used to interpret visual tokens at later LLM layers and applied to reduce hallucinations (Neo et al., 2025; Jiang et al., 2025b; Shukor & Cord, 2024; Park & Li, 2025; Wu et al., 2025). However, these works often involve either only small studies to quantify the interpretability via LogitLens, or only few models and with closed set of object classes as labels. Overall, neither of these works compare the interpretability via the embedding and unembedding matrix (Logitlens). To the best of our knowledge, no prior work has considered contextual embeddings for this purpose. Notably, Phukan et al. (2025) leverage the average intermediate contextual embedding of the generated answer to mitigate hallucination on VQA. They do not investigate the interpretability of visual tokens, and only rely on contextual embeddings from single generated (not large collection). Another fundamental question is how vision and language embedding spaces relate to each other, such as characterizing fundamental modality gap (Liang et al., 2022; Jiang et al., 2024) or narrow-cone effects (Shukor & Cord, 2024) in models. Our findings can also broadly be seen as further evidence for high structural similarity of vision and language representation spaces, coined as the Platonic Representation Hypothesis (Huh et al., 2024). Representations in LLMs. Prior work has investigated how other non-discrete tokens (i.e., soft prompts) or intermediate states are represented in LLMs. Soft prompts have been found to sometimes showcase interpretable neighbors, albeit generic concepts only broadly related to the task (Lester et al., 2021). Bailey et al. (2023) challenge this view of soft prompts as word-like units. Hidden representations and contextual word embeddings in language models have been extensively studied along axes such as layer evolution (Voita et al., 2019; Aken et al., 2020), embedding geometry (Ethayarajh, 2019) and semantics such as word sense disambiguation (Peters et al., 2018; Eyal et al., 2022; Wiedemann et al., 2019; Chang & Chen, 2019). Notably, Eyal et al. (2022) also leverage large index of contextual word embeddings linked to the sentence they occurred in. 7. Discussion and Conclusion In this work, we established that visual tokens are consistently interpretable across LLM layers, even at the input to the LLM. The ability to find interpretable visual tokens relies on using contextualized textual token representations, instead of the input or output embedding layers of the underlying model. Our findings challenge the inconclusive results of previous work in systematic study across interpretability methods, models, and layers. These results also offer an explanation for what is happening under the hood when connecting separately trained vision encoders and LLMs, and why this requires minimal updates of weights and architecture. For example, one piece in the puzzle for explaining this straightforward mapping is our Mid-Layer Leap observation: visual tokens at the input are already aligned with semantic intermediate language representations (e.g., layers 816), rather than word-level embeddings. Broadly, it is long-standing goal to understand the relation between the physical world and abstract symbolic processing, both in human cognition (Harnad, 1990) as well as AI systems (Huh et al., 2024; Bisk et al., 2020). Our empirical study contributes toward understanding how visual and linguistic representations interface in neural systems, and to what extent we can find isomorphisms. We foresee many exciting avenues for future work. fundamental question is to what extent vision and language representations share deeper structural similarities beyond interpretability. LATENTLENS may extend beyond VLMs to other non-linguistic tokens such as soft prompts (Lester 8 LATENTLENS et al., 2021), latent thinking (Hao et al., 2025), or speech (Ògúnrèmí et al., 2025). It could also be applied to natively multimodal models (Team, 2024; Zhou et al., 2025; Team et al., 2023) or refined as tool with dynamic corpus9. In terms of downstream implications, we see promise in mitigating hallucination (Jiang et al., 2025b), as well as causal ablations investigating how interpretable vs. noninterpretable visual tokens affect task performance."
        },
        {
            "title": "Limitations",
            "content": "We acknowledge several limitations of our approach. First, LATENTLENS requires pre-computing and storing large corpus of contextual embeddings, which incurs storage overhead compared to methods like LogitLens that rely solely on model weights. Second, our interpretability judgments may be influenced by the Visual Genome corpus of sentences used for contextual embeddings. Third, the dominance of nouns in our nearest-neighbor results may partly reflect corpus biases rather than fundamental properties of visual token representations. Finally, while we study 10 VLM configurations, our findings may not generalize to architectures that differ substantially from the transformer-based models we analyze."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank our many colleagues from McGill NLP and the wider Mila community for their valuable feedback and brainstorming. BK is supported by the Vanier Canada Graduate Scholarships. DE was funded by IVADO Thematic Semester on Autonomous Agents and by research grant (VIL53122) from VILLUM FONDEN. MM is supported by the Mila P2v5 grant and the Mila-Samsung grant. SR is supported by Canada CIFAR AI Chairs program and IVADO R3AI. Finally, we thank Sonia Joseph and Elinor Poole-Dayan for final feedback on the draft."
        },
        {
            "title": "References",
            "content": "a feature. In ICML 2023 Workshop on DeployableGenerativeAI, 2023. Belinkov, Y. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207219, 2022. Belrose, N., Furman, Z., Smith, L., Halawi, D., Ostrovsky, I., McKinney, L., Biderman, S., and Steinhardt, J. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023. Beyer, L., Steiner, A., Pinto, A. S., Kolesnikov, A., Wang, X., Salz, D., Neumann, M., Alabdulmohsin, I., Tschannen, M., Bugliarello, E., et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. Bisk, Y., Holtzman, A., Thomason, J., Andreas, J., Bengio, Y., Chai, J., Lapata, M., Lazaridou, A., May, J., Nisnevich, A., Pinto, N., and Turian, J. Experience grounds language. In Webber, B., Cohn, T., He, Y., and Liu, Y. (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8718 8735, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main. 703. URL https://aclanthology.org/2020. emnlp-main.703/. Chang, T.-Y. and Chen, Y.-N. What does this word mean? explaining contextualized embeddings with natural language definition. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 60646070, 2019. Cho, J. H., Madotto, A., Mavroudi, E., Afouras, T., Nagarajan, T., Maaz, M., Song, Y., Ma, T., Hu, S., Jain, S., et al. Perceptionlm: Open-access data and models for detailed visual understanding. arXiv preprint arXiv:2504.13180, 2025. Aken, B. v., Winter, B., Löser, A., and Gers, F. A. Visbert: Hidden-state visualizations for transformers. In Companion Proceedings of the Web Conference 2020, pp. 207211, 2020. Cunningham, H., Ewart, A., Riggs, L., Huben, R., and Sharkey, L. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen Technical Report. arXiv preprint arXiv:2309.16609, 2023. URL https://arxiv.org/abs/2309.16609. Deitke, M., Clark, C., Lee, S., Tripathi, R., Yang, Y., Park, J. S., Salehi, M., Muennighoff, N., Lo, K., Soldaini, L., et al. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 91104, 2025. Bailey, L., Ahdritz, G., Kleiman, A., Swaroop, S., DoshiVelez, F., and Pan, W. Soft prompting might be bug, not 9We present an initial exploration of this in Section Ethayarajh, K. How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings. In Inui, K., Jiang, J., Ng, V., and 9 LATENTLENS Wan, X. (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5565, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1006. URL https://aclanthology.org/D19-1006/. Eyal, M., Sadde, S., Taub-Tabib, H., and Goldberg, Y. Large scale substitution-based word sense induction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 47384752, 2022. Fierro, C., Foroutan, N., Elliott, D., and Søgaard, A. How do multilingual language models remember facts? In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 1605216106, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl. 827. URL https://aclanthology.org/2025. findings-acl.827/. Fu, S., tyler bonnen, Guillory, D., and Darrell, T. Hidden in plain sight: VLMs overlook their visual representaIn Second Conference on Language Modeling, tions. 2025. URL https://openreview.net/forum? id=qQb1JLrwol. García-de Herreros, P., Gautam, V., Slusallek, P., Klakow, D., and Mosbach, M. What explains the success of crossmodal fine-tuning with ORCA? In Tafreshi, S., Akula, A., Sedoc, J., Drozd, A., Rogers, A., and Rumshisky, A. (eds.), Proceedings of the Fifth Workshop on Insights from Negative Results in NLP, pp. 816, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.insights-1.2. URL https:// aclanthology.org/2024.insights-1.2/. Geva, M., Schuster, R., Berant, J., and Levy, O. Transformer feed-forward layers are key-value memories. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 54845495, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 446. URL https://aclanthology.org/2021. emnlp-main.446/. Geva, M., Caciularu, A., Wang, K., and Goldberg, Y. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 3045, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main. URL https://aclanthology.org/2022. 3. emnlp-main.3/. Geva, M., Bastings, J., Filippova, K., and Globerson, A. Dissecting recall of factual associations in auto-regressive language models. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1221612235, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.751. URL https://aclanthology. org/2023.emnlp-main.751/. Ghandeharioun, A., Caciularu, A., Pearce, A., Dixon, L., and Geva, M. Patchscopes: unifying framework for inspecting hidden representations of language models. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Groeneveld, D., Beltagy, I., Walsh, E., Bhagia, A., Kinney, R., Tafjord, O., Jha, A., Ivison, H., Magnusson, I., Wang, Y., Arora, S., Atkinson, D., Authur, R., Chandu, K., Cohan, A., Dumas, J., Elazar, Y., Gu, Y., Hessel, J., Khot, T., Merrill, W., Morrison, J., Muennighoff, N., Naik, A., Nam, C., Peters, M., Pyatkin, V., Ravichander, A., Schwenk, D., Shah, S., Smith, W., Strubell, E., Subramani, N., Wortsman, M., Dasigi, P., Lambert, N., Richardson, K., Zettlemoyer, L., Dodge, J., Lo, K., Soldaini, L., Smith, N., and Hajishirzi, H. OLMo: Accelerating the science of language models. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1578915809, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long. 841. URL https://aclanthology.org/2024. acl-long.841/. Han, J., Tong, S., Fan, D., Ren, Y., Sinha, K., Torr, P., and Kokkinos, F. Learning to see before seeing: Demystifying llm visual priors from language pre-training. arXiv preprint arXiv:2509.26625, 2025. Hao, S., Sukhbaatar, S., Su, D., Li, X., Hu, Z., Weston, J. E., and Tian, Y. Training large language models to In Second Conreason in continuous latent space. ference on Language Modeling, 2025. URL https: //openreview.net/forum?id=Itxz7S4Ip3. 10 LATENTLENS Harnad, S. The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(1-3):335346, 1990. Honnibal, M., Montani, I., Van Landeghem, S., and Boyd, A. spaCy: Industrial-strength Natural Language Processing in Python. 2020. doi: 10.5281/zenodo.1212303. Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Zhang, P., Li, Y., Liu, Z., and Li, C. LLaVA-onevision: Easy visual task transfer. Transactions on Machine Learning Research, 2025. ISSN 28358856. URL https://openreview.net/forum? id=zKv8qULV6n. Huh, M., Cheung, B., Wang, T., and Isola, P. Position: The platonic representation hypothesis. In Fortyfirst International Conference on Machine Learning, 2024. URL https://openreview.net/forum? id=BH8TYy0r6u. Liang, V. W., Zhang, Y., Kwon, Y., Yeung, S., and Zou, J. Y. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. Advances in Neural Information Processing Systems, 35: 1761217625, 2022. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Jiang, C., Xu, H., Dong, M., Chen, J., Ye, W., Yan, M., Ye, Q., Zhang, J., Huang, F., and Zhang, S. Hallucination augmented contrastive learning for multimodal large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2703627046, June 2024. Liao, J., Niu, Y., Meng, F., Li, H., Tian, C., Du, Y., Xiong, Y., Li, D., Zhu, X., Yuan, L., Dai, J., and Cheng, Y. Langbridge: Interpreting image as combination of lanIn Proceedings of the IEEE/CVF guage embeddings. International Conference on Computer Vision (ICCV), pp. 2375223762, October 2025. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instrucIn Thirty-seventh Conference on Neural tion tuning. Information Processing Systems, 2023. URL https: //openreview.net/forum?id=w0H2xGHlkw. Jiang, J., Zhou, J., Peng, B., Ning, X., and Zhu, Z. Analyzing fine-grained alignment and enhancing vision understanding in multimodal language models. In The Thirtyninth Annual Conference on Neural Information Processing Systems, 2025a. URL https://openreview. net/forum?id=PLBVtJt4td. Jiang, N., Kachinthaya, A., Petryk, S., and Gandelsman, Interpreting and editing vision-language represenY. In The Thirteenth tations to mitigate hallucinations. International Conference on Learning Representations, 2025b. URL https://openreview.net/forum? id=94kQgWXojH. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma, D. A., et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):3273, 2017. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 30453059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 243. URL https://aclanthology.org/2021. emnlp-main.243/. Lu, K., Grover, A., Abbeel, P., and Mordatch, I. Frozen pretrained transformers as universal computation engines. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pp. 76287636, 2022. Mañas, O., Lopez, P. R., Ahmadi, S., Nematzadeh, A., Goyal, Y., and Agrawal, A. Mapl: Parameter-efficient adaptation of unimodal pre-trained models for visionlanguage few-shot prompting. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 25232548, 2023. Masry, A., Rodriguez, J. A., Zhang, T., Wang, S., Wang, C., Feizi, A., Suresh, A. K., Puri, A., Jian, X., Noel, P.-A., Madhusudhan, S. T., Pedersoli, M., Liu, B., Chapados, N., Bengio, Y., Hoque, E., Pal, C., Laradji, I. H., Vazquez, D., Taslakian, P., Gella, S., and Rajeswar, S. Alignvlm: Bridging vision and language latent spaces for multimodal document understanding. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/ forum?id=vAxGuGmshO. Merullo, J., Castricato, L., Eickhoff, C., and Pavlick, E. Linearly mapping from image to text space. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=8tYRqb05pVn. Mokady, R., Hertz, A., and Bermano, A. H. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021. 11 LATENTLENS Neo, C., Ong, L., Torr, P., Geva, M., Krueger, D., and Barez, F. Towards interpreting visual information processing in vision-language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=chanJGoa7f. Nikankin, Y., Arad, D., Gandelsman, Y., and Belinkov, Y. Same task, different circuits: Disentangling modality-specific mechanisms in vlms. arXiv preprint arXiv:2506.09047, 2025. nostalgebraist. interpreting gpt: lens, https://www.lesswrong. the logit"
        },
        {
            "title": "URL",
            "content": "2020. com/posts/AcKRB8wDpdaN6v6ru/ interpreting-gpt-the-logit-lens. Ògúnrèmí, T., Manning, C. D., Jurafsky, D., and Livescu, K. Transcribe, translate, or transliterate: An investigation of intermediate representations in spoken language models. arXiv preprint arXiv:2510.02569, 2025. Oneata, D., Elliott, D., and Frank, S. Seeing what tastes good: Revisiting multimodal distributional semantics in the billion parameter era. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 2417424191, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 9798-89176-256-5. doi: 10.18653/v1/2025.findings-acl. 1240. URL https://aclanthology.org/2025. findings-acl.1240/. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., ElNouby, A., et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Pan, A., Chen, L., and Steinhardt, J. Latentqa: Teaching llms to decode activations into natural language. arXiv preprint arXiv:2412.08686, 2024. Papadimitriou, I., Su, H., Fel, T., Kakade, S. M., and Gil, S. Interpreting the linear structure of vision-language model embedding spaces. In Second Conference on Language Modeling, 2025. URL https://openreview.net/ forum?id=qPsmGjpq1j. Park, S. and Li, S. GLSim: Detecting object hallucinations in LVLMs via global-local similarity. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/ forum?id=ZO8LyCizx9. Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep contextualized word representations. In Walker, M., Ji, H., and Stent, A. (eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 22272237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL https://aclanthology.org/N18-1202/. Phukan, A., Divyansh, D., Morj, H. K., Vaishnavi, V., Saxena, A., and Goswami, K. Beyond logit lens: Contextual embeddings for robust hallucination detection & grounding in vlms. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 96619675, 2025. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language suIn Meila, M. and Zhang, T. (eds.), Propervision. ceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 87488763. PMLR, 1824 Jul 2021. URL https://proceedings.mlr.press/ v139/radford21a.html. Shen, J., Li, L., Dery, L. M., Staten, C., Khodak, M., Neubig, G., and Talwalkar, A. Cross-modal fine-tuning: Align then refine. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 3103031056. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/ v202/shen23e.html. Shukor, M. and Cord, M. Implicit multimodal alignment: On the generalization of frozen LLMs to multimodal inputs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=9622QfVSAb. Team, C. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Team, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Patel, R. and Pavlick, E. Mapping language models to grounded conceptual spaces. In International conference on learning representations, 2022. Timkey, W. and van Schijndel, M. All bark and no bite: Rogue dimensions in transformer language models obscure representational quality. In Moens, M.-F., Huang, LATENTLENS P., Peng, R., Men, R., Gao, R., Lin, R., Wang, S., Bai, S., Tan, S., Zhu, T., Li, T., Liu, T., Ge, W., Deng, X., Zhou, X., Ren, X., Zhang, X., Wei, X., Ren, X., Fan, Y., Yao, Y., Zhang, Y., Wan, Y., Chu, Y., Cui, Z., Zhang, Z., and Fan, Z.-W. Qwen2 technical report. ArXiv, abs/2407.10671, 2024. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Ye, Q., Zeng, X., Li, F., Li, C., and Fan, H. Painting with words: Elevating detailed image captioning with benchmark and alignment learning. arXiv preprint arXiv:2503.07906, 2025. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1197511986, 2023. Zhang, Z., Yadav, S., Han, F., and Shutova, E. Cross-modal information flow in multimodal large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1978119791, 2025. Zhou, C., YU, L., Babu, A., Tirumala, K., Yasunaga, M., Shamis, L., Kahn, J., Ma, X., Zettlemoyer, L., and Levy, O. Transfusion: Predict the next token and diffuse imIn The Thirteenth ages with one multi-modal model. International Conference on Learning Representations, 2025. X., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 45274546, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emnlp-main.372. URL https://aclanthology. org/2021.emnlp-main.372/. Tsimpoukelli, M., Menick, J. L., Cabi, S., Eslami, S., Vinyals, O., and Hill, F. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200212, 2021. Venhoff, C., Khakzar, A., Joseph, S., Torr, P., and Nanda, N. How visual representations map to language feature space in multimodal llms. arXiv preprint arXiv:2506.11976, 2025. Voita, E., Sennrich, R., and Titov, I. The bottom-up evolution of representations in the transformer: study with machine translation and language modeling objectives. In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 43964406, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1448. URL https://aclanthology.org/D19-1448/. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. Wiedemann, G., Remus, S., Chawla, A., and Biemann, C. Does BERT make any sense? interpretable word sense disambiguation with contextualized embeddings. In Proceedings of the 15th Conference on Natural Language Processing, KONVENS 2019, Erlangen, Germany, October 9-11, 2019, 2019. URL https: //corpora.linguistik.uni-erlangen. de/data/konvens/proceedings/papers/ KONVENS2019_paper_43.pdf. Wu, Z., Yu, X. V., Yogatama, D., Lu, J., and Kim, Y. The semantic hub hypothesis: Language models share semantic representations across languages and modalities. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=FrFQpAgnGE. Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., Dong, G., Wei, H., Lin, H., Tang, J., Wang, J., Yang, J., Tu, J., Zhang, J., Ma, J., Xu, J., Zhou, J., Bai, J., He, J., Lin, J., Dang, K., Lu, K., Chen, K.-Y., Yang, K., Li, M., Xue, M., Ni, N., Zhang, P., Wang, 13 A. Limitations We acknowledge several limitations of our approach: LATENTLENS Storage requirements: LATENTLENS requires pre-computing and storing large corpus of contextual embeddings, which incurs storage overhead compared to methods like LogitLens that rely solely on model weights. Our current corpus uses float8 compression to reduce storage to approximately 25% of float32 size, but the storage cost scales with both corpus size and the number of LLM layers analyzed. Corpus constraints: Our interpretability judgments are constrained by the Visual Genome corpus used for contextual embeddings. Domains not well-represented in this corpus (e.g., specialized scientific imagery, non-Western cultural contexts) may yield less meaningful interpretations. Future work could explore domain-specific corpora or dynamic corpus generation. Noun bias: The dominance of nouns in our nearest-neighbor results (approximately 4550% as shown in Section H.2) may partly reflect corpus biases in Visual Genome rather than fundamental properties of visual token representations. Visual Genomes region descriptions naturally emphasize objects and entities over actions or relations. Model scope: While we study 10 VLM configurations (9 controlled setups plus Qwen2-VL), our findings may not generalize to architectures that differ substantially from the transformer-based models we analyze. In particular, we focus on frozen LLMs with MLP connectors; models with different connector architectures (e.g., Q-Former, Perceiver) or training paradigms may exhibit different interpretability patterns. Evaluation subjectivity: Despite validating our LLM judge against human annotations (κ = 0.68), interpretability judgments retain inherent subjectivity. What constitutes meaningful interpretation depends on the task and context. Our binary interpretable/non-interpretable classification may miss nuances in interpretation quality."
        },
        {
            "title": "Appendix Overview",
            "content": "A E G K M Limitations Storage, corpus constraints, noun bias, model scope, evaluation subjectivity LATENTLENS Design How the corpus of contextual embeddings was built and stored LLM Judge LLM judge prompt, human annotation protocol, and validation Ablations Robustness to seed, connector depth, caption detail, and task type L2 Norm Analysis Visual tokens have higher L2 norms and behave differently Layer Alignment Details Token drift through LLM processing Off-the-Shelf Model Analysis Qwen2-VL-7B-Instruct results Fine-grained Interpretation Analysis POS tags, visual attributes, interpretation types Phrase-Level Interpretations How helpful phrase-level interpretations are compared to single words Dynamic Corpus Generation Evolutionary search to generate better descriptions with maximal cosine similarity Captioning Quality Ensuring our models produce good captions with DCScore Qualitative Examples 20 non-cherry-picked for LATENTLENS, LogitLens and EmbeddingLens Behind The Scenes Beyond what the reader sees in the final product (this paper) we show stories, lessons learned, and what was discarded B. LATENTLENS Design B.1. Contextual Embedding Corpus For LATENTLENS, we extract embeddings from Visual Genome (Krishna et al., 2017) phrase-region annotations. We process 2.99M phrases (2,991,848 unique after deduplication) through each LLM, storing up to 20 contextual embeddings per vocabulary token at layers [1, 2, 4, 8, 16, 24, N-2, N-1] where is the number of layers. This results in approximately 2.5M embeddings across 26,862 unique tokens per layer. To reduce storage requirements, embeddings are stored in float8 format (25% of fp32 size). We provide embeddings for OLMo-7B, LLaMA3-8B, Qwen2-7B, and Qwen2-VL-7B-Instruct. LATENTLENS The corpus was chosen because Visual Genome contains the right level of detailed descriptions for visual scenes and objects (e.g., the door of pickup truck, multiple cows standing in field), providing contextual embeddings that are semantically relevant to our task of interpreting visual tokens. Each phrase is processed through the LLM, and we use reservoir sampling to limit storage to 20 embeddings per token per layer. C. Human Annotations and LLM Judge Design We developed the LLM judge through iterative prompt refinement, particularly regarding: showing only the full image with red box around the region of interest vs. additionally showing that cropped region as separate image; how to explain the exact task in prompt format. We designed both word-level and sentence-level judge, and find that the word-level is more reliable. Providing full sentences instead leads to more frequent overor under-interpretations with LLM reasoning that finds associations where none might exist. We then validate the LLM judge via correlation with humans: Do humans and the LLM judge mostly agree whether certain top-5 nearest neighbors are interpretable given region in the image and the whole image context? C.1. LLM Judge Prompt We use GPT-5 with the following prompt: You are visual interpretation expert specializing in connecting textual concepts to specific image regions. Your task is to analyze list of candidate words and determine how strongly each one relates to the content of the highlighted region. Inputs 1. Full Image: An image containing red bounding box highlighting the region of interest. 2. Cropped Region: close-up view of the exact region highlighted by the red bounding box. Only rely on this if it is too small in the full image (e.g. text is too small to read), otherwise rely on the full image. 3. Candidate Words: list of words to evaluate. Evaluation Guidelines There are three types of relationships to consider between the candidate words and the highlighted region: Concrete: word is concretely related if it names something that is literally visible in the cropped region. This includes: objects or parts of objects clearly present; colors, textures, or materials visible; text, numbers, or symbols shown; shapes, patterns, or visual features. Abstract: word is abstractly related if it describes broader concepts, emotions, or activities related to whats shown in the cropped region, but isnt literally present. This includes: emotions or feelings (beautiful, scary, peaceful); activities or functions (driving, cooking, reading); cultural or conceptual associations (luxury, tradition, modern); qualities or characteristics (elegant, rustic, professional); anything deemed semantically related to the region or the whole image context. Global: word is globally related if it describes something that exists elsewhere in the full image (outside the highlighted region), but not in the cropped region itself. This includes: objects visible in other parts of the image; colors present in other parts; text or elements in different regions. Important Note: For regions with text, the connection can be concrete (characters/words shown) or abstract (concepts implied by the text). Output Format Return JSON object with: abstract_words (list), global_words (list). reasoning (string), interpretable (true/false), concrete_words (list), C.2. Human Annotation and Validation To validate our automated LLM judge, the authors manually annotated visual tokens across all three interpretability methods: EmbeddingLens (360 instances), LATENTLENS (300 instances), and LogitLens (360 instances), totaling 1,020 annotations. For each instance, we showed annotators the image with the highlighted region and the top-5 candidate descriptions from the respective method (Figure 8). Annotators indicated which candidates (if any) were related to the regioneither concretely (directly visible), abstractly (conceptually related), or globally (present elsewhere in image). Each instance was annotated by at least one author, with an average of 1.8 annotators per instance. visual token is labeled interpretable if majority of annotators selected at least one candidate as related. Comparing human majority vote against the LLM judge, we find substantial agreement: Cohens κ = 0.68 and accuracy = 84.4% across all 1,020 instances. 15 LATENTLENS Figure 8. Screenshot of the human annotation interface. Annotators see the image with highlighted region (red box) and the top-5 candidate descriptions. For each candidate, they select whether it is related to the region concretely, abstractly, globally, or not at all. Table 1. Ablation results showing LATENTLENS interpretability change and nearest-neighbor overlap with baseline, averaged across all layers. Interp.: Change in % interpretable (baseline = 71.3%). Top-5 NN Overlap: Average number of matching neighbors out of 5. Token = same subword; Phrase = same full caption. Captioning ablations maintain interpretability with partial overlap (2/5), suggesting multiple valid mappings exist. Top-5 NN Overlap Model Variant Interp. Token Phrase Baseline (OLMo + ViT-L) Different seed Linear connector First-sentence captions Unfrozen LLM Spatial Task (frozen) Spatial Task (unfrozen) 71.3% +1.3% +0.8% 1.6% +6.4% 33.2% 29.2% 2.5 2.1 1.8 1.9 0.0 0.0 2.2 2.0 1.5 1.5 0.0 0.0 D. Ablations In this section we ablate several components of our training setup to determine to what extent our findings depend on our particular setup. In other words, will given image patch be mapped to the same nearest-neighbour words, regardless of training data, task, or mapping function? Specifically, if we do find similar LATENTLENS interpretations of the same input across ablations, it could imply that visual tokens represent something akin to raw task-independent input and all the task-specific extraction and computation is conducted by the LLM. Experimental setup. We focus on OLMo + CLIP-ViT and ablate the following 5 aspects: 1. The random seed used for initializing the connector weights and controlling the training data sampling. 2. Using linear mapping instead of 3-layer MLP to map visual tokens into the LLM prefix space. 3. Varying the level of detail in the captioning dataset by training on single-sentence captions instead of the detailed multi-sentence Pixmo-Cap dataset. 4. Unfreezing the LLM during training. When allowing the LLM weights to adapt to the task of processing and describing visual tokens, two outcomes are plausible: the proportion of interpretable tokens either drops or it rises. LATENTLENS For the former, we can conceive that some of the LLM weights specialize to be visual processing model (and not language model), with less training pressure to represent tokens as words frozen LLM can process. On the other hand, we can also imagine that now the connector module simply became more expressive, being able to align any visual token to LLM embeddings, even if the initial structure of both embedding space was sometimes non-trivially different. 5. Changing the task from captioning to spatial prediction task. The model is trained to answer the question Where is [OBJECT]? with either top or bottom. The hypothesis we test is whether the language-based task of captioning leads to the observed alignment of visual tokens to the LLM embedding space. For each ablation we measure two metrics, averaged across all layers: (1) LATENTLENS Interpretability: The percentage of visual tokens judged interpretable by GPT-5, reported as change from baseline (71.3%). (2) LATENTLENS Overlap: Average number of matching top-5 LATENTLENS nearest neighbors with the baseline model (out of 5). We report both token-level overlap (same subword in both top-5 sets) and phrase-level overlap (same full contextual phrase). Interpretability is robust to training variations but requires language-based objectives. Results are summarized in Table 1. Simply changing the seed results in only around half of the top-5 LATENTLENS results to overlap with the original run. All of them are still highly similar concepts but it is some indication that even the seed can have some influence, albeit it is unclear how semantically meaningful it is. On the other hand, we find encouraging results when replacing the 3-layer MLP connector with single matrix (linear layer) and when replacing highly detailed captions as training data with single-sentence captions: The level of interpretability is almost the same, and we still see significant amount of overlap between the original top-5 nearest neighbors (2.1 and 1.8 out of 5, respectively). Thus, the relationship between vision encoder and LLM embeddings spaces can be linearly aligned in an interpretable manner and moreover, short captioning data is enough to do so10. Next, we observe large increase in the amount of interpretable visual tokens (+6.4%) when unfreezing the LLM during training. Thus, with this more expressive connector in the form of some LLM weights, the model can learn more interpretable alignment. Finally, we observe that the language-based task of captioning is necessary for interpretable visual tokens. When instead training the model to generate single token (top or bottom) given question about the location of an object, interpretability drops by around 30%, with zero overlap to the original LATENTLENS top-5 NNs. The tokens that are still marked as interpretable for this spatial task are the same few generic words such as upper, background, or outside. E. Vision vs. Text Token L2 Norms Figure 9 shows the L2 norm distributions for all 9 model combinations (3 LLMs 3 vision encoders). For each model, we show separate histograms for visual tokens (left) and text tokens (right), colored by layer (yellow=early, red=late). Key observations: Vision tokens have larger L2 norms than text tokens across all models, often by 12 orders of magnitude. OLMo-7B maintains relatively small L2 norms (max 1000) for both modalities. LLaMA3-8B and Qwen2-7B exhibit much larger L2 norms for visual tokens, with max values exceeding 100,000 in some cases. DINOv2 consistently produces the largest L2 norms across all LLMs. The 99th percentile (p99, black dotted line) and maximum (red dashed line) markers show substantial outliers in visual token distributions. 10Follow-up work could explore what the limit of simplicity of the training data is for this interpretable alignment to appear. 17 LATENTLENS Figure 9. L2 norm distributions of vision vs. text tokens across layers. Each row corresponds to an LLM (OLMo-7B, LLaMA3-8B, Qwen2-7B), and each column shows vision encoder (CLIP, DINOv2, SigLIP). Within each cell, Vision tokens (left) and Text tokens (right) are shown. Colors indicate layer depth (yellow=early, red=late). The x-axis uses log scale. Black dotted lines mark the 99th percentile; red dashed lines mark the maximum value. Are High L2 Norms from Sparse Outliers or Uniform Scaling? To understand whether high L2 norms are driven by few large embedding dimensions (sparse outliers) or uniformly larger values across all dimensions, we extract the full embedding vector of the visual token with maximum L2 norm for each model and analyze its distribution. Figure 10 shows histograms of individual embedding dimension values for these max-L2-norm tokens. The key finding is striking: All distributions are approximately Gaussian, not sparse. High L2 norms come from uniformly larger values across all 35844096 dimensions, not from few extreme outliers. OLMo-7B has 100 smaller embedding scale than LLaMA3-8B and Qwen2-7B. Standard deviations are 1020 for OLMo vs. 170011000 for LLaMA3/Qwen2. LLaMA3-8B max tokens occur at layer 0 (input), while OLMo and Qwen2 max tokens occur at layer 24 (late layers). This suggests that the MLP connector (or already the vision encoder itself) learns fundamentally different embedding scales depending on the target LLM architecture, with LLaMA3 and Qwen2 resulting in much larger magnitude projections than OLMo. 18 LATENTLENS Figure 10. Distribution of embedding dimension values for max L2 norm visual tokens. Each row corresponds to an LLM (OLMo-7B, LLaMA3-8B, Qwen2-7B), and each column shows vision encoder. We extract the visual token with the largest L2 norm and plot histogram of its individual embedding dimension values. All distributions are Gaussian-like, indicating that high L2 norms result from uniform scaling across all dimensions rather than sparse outliers. F. Layer Alignment Details To understand why visual tokens at the input layer align most strongly with contextual text representations from later layers (the Mid-Layer Leap, Section 4.3), we investigate how much visual token representations change as they pass through the LLM. Figure 11 shows the cosine similarity between each token at layer ℓ and its own representation at layer 0, averaged across all tokens. For text tokens, similarity to the input embedding drops rapidly within the first few layers (often below 0.4 by layer 4), reflecting the contextualization process where tokens incorporate information from surrounding context. In contrast, visual tokens maintain much higher similarity to their input representationsoften above 0.8 through mid-layersindicating they undergo substantially less transformation during LLM processing. This minimal drift of visual tokens explains the Mid-Layer Leap: since visual tokens are already pre-contextualized by the vision encoder and connector, they arrive at the LLM in representational state more similar to how the LLM represents text after several layers of processing. The frozen LLM then processes these tokens with relatively little modification, as evidenced by the high layer-to-layer similarity. 19 LATENTLENS Figure 11. Vision tokens follow only minor drift through LLM processing: We compare the same token (e.g. same position in the image or text passage) to its input-layer embedding across layers. We find that text tokens early on have little similarity with their initial embeddings, perhaps following some process of contextualization, abstraction, or simply preparing for next-token prediction. On the other hand, visual tokens display much higher cosine similarity to their input embeddings, especially until middle layers. G. Results for an off-the-shelf model This section provides more details for Section 4.4, where we show our findings generalize to an off-the-shelf VLM such as Qwen2-VL-7B-Instruct (Wang et al., 2024). Here, we provide more empirical evidence that we still find similar Mid-Layer Leap phenomenon on Qwen2-VL, albeit less pronounced (presumably since the LLM weights were finetuned). Setup Qwen2-VL differs from our controlled training along several axes: it was trained in multiple stages on 1.4 trillion tokens; crucially the LLM and vision encoder are unfrozen at some stages; it uses various different datasets at different stages, e.g. instruction tuning data; and finally it relies on elaborate image token preprocessing. While this leads to stronger model worth investigating with LATENTLENS, it also makes it harder to ablate and fully understand which components contribute to certain insights (which we conduct in Section with our controlled setup instead). We apply the same evaluation methodology as before: 100 random visual token patches evaluated by our LLM judge across layers ℓ {0, 1, 2, 4, 8, 16, 24, 26, 27}. Since Qwen2-VL uses finetuned version of Qwen2s LLM (not the frozen base model), we extract contextual embeddings from Qwen2-VLs own LLM backbone to ensure proper alignment. Mid-Layer Leap Figure 12 shows the layer alignment and token drift for Qwen2-VL. We observe the expected diagonal pattern for most layers, where visual tokens at layer have most nearest neighbors from LLM layer n. However, for visual tokens at layer 0, we again observe the leap where most nearest neighbors come from LLM layer 4. Likewise, visual tokens undergo less change throughout LLM layers compared to text tokens, similar to our controlled setup (Figure 11), 20 LATENTLENS (a) Layer alignment (b) Token drift Figure 12. Qwen2-VL layer analysis. (a) Which LLM layers contextual embeddings are most similar to visual tokens at each processing stage. (b) Cosine similarity of tokens at each layer to their input-layer representation. Visual tokens undergo substantial transformation (0.960.10), while text tokens start low ( 0.15) and stay lowunlike frozen LLMs where visual tokens retain higher similarity. though they do change more in Qwen2-VL than in our frozen LLM models. H. Fine-grained Interpretation Analysis Beyond measuring whether visual tokens are interpretable (Section 4), we analyze what kinds of interpretations LATENTLENS produces. We examine three dimensions: (1) interpretation typeswhether nearest neighbors describe something concrete, abstract, or global (based on LLM judge outputs); (2) parts-of-speechthe grammatical categories of matched words (using spacy from Honnibal et al. (2020)); and (3) visual attributeshow often color, shape, and texture words appear. Key findings: concrete interpretations dominate (6575%), nouns are most common (4550%), and color words decline from early to late layers while other ratios remain stablesuggesting the frozen LLM does not progressively abstract away from concrete visual content. H.1. Interpretation Types Figure 13. Breakdown of interpretation types. Top row: categories (Concrete 65%, Abstract 19%, Global 16%). Second row: top most frequent nearest-neighbor words per category. Lower rows: example Visual Genome phrases showing context, with target word in bold. Data aggregated across all 10 models (9 controlled setups plus Qwen2-VL). Figure 13 provides visual breakdown of interpretation types aggregated across all models. Figure 14 shows the breakdown for all 9 model combinations individually. We categorize LATENTLENS interpretations as: 21 Concrete: The nearest neighbor word directly names something visible in the image region (objects, colors, textures). LATENTLENS Abstract: The word describes concept related to but not literally visible (emotions, activities, functions). Global: The word describes something present elsewhere in the image but not in the highlighted region. Across all models and layers, concrete interpretations dominate (7075% on average), with abstract and global each contributing 1115%. The ratios remain remarkably stable across layers, suggesting that the frozen LLM does not progressively abstract away from concrete visual descriptions. Figure 15 shows the same analysis for Qwen2-VL-7B-Instruct, the off-the-shelf model from Section G. The pattern is similar: concrete interpretations dominate (6278%), though we observe slight decrease in later layers (2627: 62%) with corresponding increase in abstract interpretations. This may reflect Qwen2-VLs unfrozen LLM having learned some layer-wise abstraction. Figure 14. Interpretation types for all 9 model combinations. Each bar shows the breakdown of interpretable tokens into concrete (directly visible), abstract (conceptually related), and global (present elsewhere in image). Concrete interpretations dominate across all models and layers (7090%). SigLIP models show notably higher global interpretations (2030%), consistent with less localized nearest neighbors. LATENTLENS Figure 15. Interpretation types for Qwen2-VL-7B-Instruct. Similar to the frozen LLM models, concrete interpretations dominate. slight decrease in concrete (and increase in abstract) is observed in later layers (2627), possibly reflecting the unfrozen LLMs learned abstraction. H.2. Parts-of-Speech and Visual Attributes Figure 16 shows the distribution of parts-of-speech among all nearest neighbors for each of the 9 trained model combinations. Nouns dominate at approximately 4550%, which aligns with the visual nature of the interpretationsvisual tokens primarily encode objects and entities. Proper nouns account for 1020%, verbs 1015%, and adjectives around 5%. The distribution is relatively stable across layers, with some variation between models (e.g., OLMo-7B + ViT-L shows more variability). Figure 17 shows the same analysis for Qwen2-VL-7B-Instruct, which exhibits similar patterns. Figure 18 shows the frequency of visual attribute words (colors, shapes, textures) for each trained model. Color words are the most common at around 56% in early layers, declining to around 3% in later layers. This suggests that raw color information is more prominent in early visual token representations. Shape and texture words are rare throughout (<1%). Figure 19 shows the same for Qwen2-VL. LATENTLENS Figure 16. Parts-of-speech distribution across layers for 9 trained models. Nouns dominate (4550%), followed by proper nouns and verbs. The distribution is relatively stable across layers with some per-model variation. Figure 17. Parts-of-speech distribution for Qwen2-VL-7B-Instruct. Similar pattern to trained models: nouns dominate, followed by proper nouns and verbs. 24 LATENTLENS Figure 18. Visual attribute word frequency for 9 trained models. Color words are common (56% early, declining to 3% late), while shape and texture words are rare (<1%). 25 LATENTLENS Figure 19. Visual attribute word frequency for Qwen2-VL-7B-Instruct. Similar pattern: color words dominate visual attributes, shape and texture are rare. I. Phrase-Level Interpretation Examples Quantifying the value of context. To measure how much the preceding context contributes to interpretation quality, we randomly sample 50 examples where the LLM judge deemed the top LATENTLENS result interpretable. For each example, we manually label whether the preceding context helps interpretation compared to the word alone, using three categories: yes (context clearly aids understanding), neutral (context neither helps nor hurts), or no (context is misleading or irrelevant). For instance, comparing large stone tower with gold clocks to just clocks the former provides richer spatial and material context that better matches the visual content. We find that in 64% of cases, the preceding context provides better interpretation than the word alone. In 28% of cases the context was neutral (the word alone was sufficient), and in only 8% was the context misleading. This validates that LATENTLENSs phrase-level interpretations offer meaningful advantages over token-level approaches. Qualitative examples. We show 12 randomly selected (non-cherry-picked) examples from our phrase annotation study. Each panel shows the visual tokens patch (red box) with preprocessing matching the respective vision encoder (CLIP preserves aspect ratio with padding, SigLIP and DINOv2 squash to square). Below each image we show: the LATENTLENS phrase versus random Visual Genome phrase containing the same token. The highlighted word shows the matched token, demonstrating how contextual information can sometimes aid interpretation. 26 LATENTLENS LatentLens: pals : the pair were spotted posing in matching person - branded aprons at the event , with large blue car parked behind them Random phrase, same token: the trees are behind the giraffe LLaMA3+DINOv2, L4 LatentLens: black eye with white specks around it Random phrase, same token: rocks beside zebra LLaMA3+SigLIP, L30 LatentLens: woman wearing black sweatpants and grey long-sleeve t-shirt LatentLens: the blue shirt the bald man has on. Random phrase, same token: smiling bald man Random phrase, same token: photo was taken by jenny lee silver OLMo+DINOv2, L4 LLaMA3+CLIP, L16 Figure 20. Phrase annotation examples (14). Each panel shows vision tokens patch (red box) preprocessed as the vision encoder sees it. LatentLens: Top contextual nearest neighbor phrase from LATENTLENS. Random: random VG phrase containing the same token. LATENTLENS LatentLens: multiple types of cherry tomatoes in multiple colours , all in bluegreen boxes Random phrase, same token: something black w/ wild colours deflated before the tent; hot air balloon, maybe? OLMo+SigLIP, L4 LatentLens: five stars on the side of the bus Random phrase, same token: the stars below the plane. OLMo+CLIP, L30 LatentLens: the cow is mostly black LatentLens: man wearing white short sleeve tunic Random phrase, same token: the non black car on the roadway. Random phrase, same token: two unicorns dancing together Qwen2+DINOv2, L8 Qwen2+SigLIP, L0 Figure 21. Phrase annotation examples (58). 28 LATENTLENS LatentLens: green leaves are in the background . Random phrase, same token: the green vegetation in the background Qwen2+CLIP, L24 LatentLens: bowl is white and has two sections Random phrase, same token: dried splinted wood sections LLaMA3+DINOv2, L16 LatentLens: barbecue sauce on the side of styrofoam cup Random phrase, same token: metal rooster on pole LLaMA3+SigLIP, L1 LatentLens: pale coloured wooden model of an old house with pitched roof and gable windows stands on table . Random phrase, same token: row of three windows LLaMA3+CLIP, L16 Figure 22. Phrase annotation examples (912). J. Dynamic Corpus Generation As noted in Section 5, our fixed corpus contains at most 20 contextual embeddings per vocabulary token. We investigate whether dynamically generating phrase contextsrather than relying on fixed corpuscan yield better LATENTLENS interpretations. Method. We use an evolutionary search approach: given visual token and its top-5 LATENTLENS nearest neighbors from the fixed corpus, we iteratively generate variations using GPT-5 and keep those with highest cosine similarity to the visual embedding. Crucially, since LLMs are autoregressive, we only modify words before the target token (the token whose contextual embedding we extract), keeping the target token at the end of each phrase. Specifically, we run 6 rounds of evolution with 20 variations per round, keeping the top-5 phrases. We evaluate on 20 visual tokens that our LLM judge marked as interpretable (OLMo-7B + CLIP ViT-L/14, layer 16). Results. Dynamic generation improves cosine similarity in 85% of cases (17/20), with an average improvement of +0.017. Table 2 shows representative examples. The evolved phrases tend to be more concise and visually specificfor instance, white and black peaked building with peaked roof (similarity 0.415) evolves to grand arched beige building (similarity 0.463). Interestingly, in 35% of cases (7/20), tokens that were not the top-1 match in the original corpus rose to become the best match after evolution. For example, when the original top-5 contained multiple candidate tokens (building, facade, mansion), the evolutionary search sometimes found that different token with better surrounding context outperformed the original top-1. This suggests that the fixed corpuss limitation of 20 sentences per token may cause some tokens to be underrepresented due to suboptimal context, even when they would be semantically fitting. Here, we illustrate three levels of descriptions and their richness obtained from: 1) dynamically generated context, 2) sentences from fixed corpus, and 3) same token but with lowest scoring context: 29 LATENTLENS Original (VG corpus) Evolved (dynamic) shiny red apples with bright green leaves white and black peaked building with peaked roof the sign reads dried beef king forest of lush trees nice clear blue sky colorful stamen against green leaves grand arched beige building the placard shows assorted meats bright canopy of treetops glistening blue sky sim +0.052 +0.047 +0.039 +0.031 +0.023 Table 2. Examples of dynamic phrase generation improving LATENTLENS interpretations. The target token is shown in bold. Evolved phrases achieve higher cosine similarity to the visual embedding by finding more appropriate contextual framing. Note that evolution can also discover better target tokens (e.g., beef meats). Dynamic: colorful fish with white stripes (cosine similarity: 0.36) Fixed Corpus: man wearing white striped [...] (cosine similarity: 0.34) Lowest: white stripe above two blue stripes (cosine similarity: 0.26) K. Captioning Quality Evaluation We evaluate caption quality using DCScore (Ye et al., 2025), GPT-4o-based LLM judge that rates generated captions on 110 scale across 300 validation images from PixMo-Cap. Evaluation rubric. DCScore evaluates each caption on four fine-grained criteria, each scored 110: Faithfulness: How accurately the caption reflects the actual image content Detail accuracy: Coverage and correctness of salient visual details Hallucinations: Absence of non-existent content (higher = fewer hallucinations) Completeness: Whether the caption captures all key aspects of the image The overall score is holistic quality rating that considers all criteria. We provide the judge with both the full image and the generated caption, asking it to return structured JSON response with all sub-scores. Full results. Table 3 shows the complete caption quality scores for all 9 trained model combinations plus the off-the-shelf Qwen2-VL-7B-Instruct as an upper bound reference. Table 3. Caption quality scores (DCScore, 110 scale) on 300 PixMo-Cap validation images. Higher is better. Our trained models average 6.0, while the fully instruction-tuned Qwen2-VL achieves 8.5. LLM Vision Encoder Score OLMo-7B OLMo-7B OLMo-7B CLIP ViT-L/14 SigLIP DINOv2-Large LLaMA3-8B CLIP ViT-L/14 LLaMA3-8B SigLIP LLaMA3-8B DINOv2-Large Qwen2-7B Qwen2-7B Qwen2-7B CLIP ViT-L/14 SigLIP DINOv2-Large Upper bound: Qwen2-VL-7B-Instruct 6.60 6.75 4.30 6.79 6.95 4.46 7.08 6.77 4.54 8.50 LATENTLENS Key observations. (1) DINOv2 models consistently underperform on captioning (4.34.5) compared to CLIP and SigLIP models (6.67.1), likely due to DINOv2s lack of language supervision during pretraining. (2) Qwen2-7B achieves the highest average score (6.13) across vision encoders among our trained models. (3) The gap between our models (avg 6.0) and Qwen2-VL (8.5) reflects that our models were trained only on captioning with frozen LLM, while Qwen2-VL underwent full multimodal instruction tuning. (4) Notably, captioning performance does not correlate with visual token interpretabilityDINOv2 models show comparable interpretability (Figure 3) despite lower captioning scores. Sample captions. Figure 23 shows example captions from three representative models on the same validation image, illustrating how different model combinations produce captions of varying quality and detail. OLMo + CLIP-ViT (8/10) The image depicts bustling square, likely in Italy, characterized by mix of old and new architecture. Dominating the scene is grand, ancient building with distinctive yellow facade, adorned with clock tower and prominent cross. The building features series of arches and grand entrance, flanked by two large columns. The square is bustling with people, some seated on benches, others strolling around. Qwen2 + SigLIP (8/10) The image depicts vibrant and bustling square in Italy, characterized by mix of historical and modern architecture. The central focus is large, ornate fountain, which is the focal point of the square. To the left of the fountain, there is building with distinctive clock tower adorned with statues. The sky above is clear blue, enhancing the overall beauty of the scene. LLaMA3 + DINOv2 (7/10) The image captures bustling square in European city, likely in Italy or Spain, with vibrant mix of architectural styles. Dominating the scene is large, ornate building with golden facade and prominent clock tower. Adjacent to the clock tower is smaller building with similar golden hue. In the foreground, large fountain draws crowd of people. Figure 23. Sample captions for the same validation image. Per-image DCScores shown in parentheses. L. Qualitative examples To demonstrate that our findings are not cherry-picked, we present 20 randomly sampled examples across 5 layers (0, 8, 16, 24, and final layer). For each example, we randomly select one of 10 models (9 trained models plus Qwen2-VL) and show top-3 predictions from each method: EmbeddingLens (nearest neighbors in input embedding matrix), LogitLens (LM head predictions), and LATENTLENS (ours, contextual nearest neighbors with phrase context). For LATENTLENS, we show the top-3 Visual Genome phrases containing the matched token (highlighted in yellow ), demonstrating the semantic richness of phrase-level interpretations. For baselines, we show the top-3 tokens with EmbeddingLens and LogitLens colored backgrounds. Across all randomly sampled examples, LATENTLENS consistently provides more interpretable resultsthe contextual phrases typically describe the visual content more accurately than isolated tokens from the baselines. 31 LATENTLENS OLMo+SigLIP, L0 LLaMA3+CLIP, L0 LatentLens: (1) bears paws covered in swirling grass (2) blue tail of plane with white swirls (3) the cats tail is frizzy . EmbeddingLens: hairy Skinny flashy LogitLens: cir faucet rawn LatentLens: (1) vase sitting on square table (2) water glass on white tablecloth (3) cup and saucer on wooden table EmbeddingLens: planted opies avou LogitLens: [?] uzey tôi OLMo+SigLIP, OLMo+SigLIP, L0 LatentLens: (1) train with the letters s. w.n . in white (2) sign reading \"carrer den falconer \" (3) train with the letters s.w .n. in white EmbeddingLens: Vanessa [?] Wolf LogitLens: hausen ens experiment.. LatentLens: (1) the water had the sun gleaming down (2) ...ncluding dissassembled bicycle and... (3) bathing suit for swimming and suntanning . EmbeddingLens: [HI] 218 Education LogitLens: erne rone crest Figure 24. Layer 0 (input): Four randomly sampled visual tokens at layer 0. 32 LATENTLENS Qwen2+DINOv2, L8 Qwen2-VL, L8 LatentLens: (1) ...s, probably 3 [ though the one beneat... (2) second adult bows low in to inspect t... (3) ...e bear+person [ though that might not... EmbeddingLens: -c [?] (translate.. LogitLens: tô Según ==========.. LatentLens: (1) controls are on the center console (2) dials on motorcycle dashboard glow red (3) controls are on the center console EmbeddingLens: [TH] .urlencoded .DropDownL.. LogitLens: [?] <nav Jihad Qwen2+CLIP, L8 LLaMA3+CLIP, L8 LatentLens: (1) wikipedia screen is displayed on the ... (2) few paragraphs written on the screen (3) ... real [check wikipedia ] EmbeddingLens: \" .DrawString incorrect LogitLens: stantiate chia IsValid LatentLens: (1) sky with two large willowy clouds (2) baby blue sky with wispy cirrus clouds (3) ...red with thin branches with green le... EmbeddingLens: Schultz 95 Vert LogitLens: strstr Kron [?] Figure 25. Layer 8 (early-mid): Four randomly sampled visual tokens at layer 8. 33 LATENTLENS Qwen2+CLIP, L16 Qwen2+SigLIP, L16 LatentLens: (1) stone chimney attached to house (2) pole and trees in front of building (3) low, grey roof of residence . EmbeddingLens: (translate.. ([... .isRequired LogitLens: chantment [?] onUpdate LatentLens: (1) framed pencil sketch on wall (2) pencil sketched artwork of fruit basket (3) framed drawing to left of the bed EmbeddingLens: There div -cut LogitLens: [?] [?] fdc Qwen2+DINOv2, L16 OLMo+CLIP, LatentLens: (1) ...ith black polka dots (2) ...er with blue stipe (3) man wearing white shirt with blue stipe EmbeddingLens: .COMP Specifies rà LogitLens: CascadeType =nil oppon LatentLens: (1) man with balding head standing near cart (2) ...diners, four of whom are toasting, a... (3) older man sitting on wooden bench EmbeddingLens: him men Boys LogitLens: minority mostly oya Figure 26. Layer 16 (middle): Four randomly sampled visual tokens at layer 16. LATENTLENS LLaMA3+CLIP, L24 Qwen2+SigLIP, L24 LatentLens: (1) ...leaves against muted background (2) ...hink black and white stripes (3) ...st wall with muted white lights . EmbeddingLens: white White allied LogitLens: .scalablyt.. UnitOfWork [?] LatentLens: (1) man wearing white baseball cap. (2) lady in off-white sweater (3) back of woman wearing white turtleneck EmbeddingLens: .leading machining GetX LogitLens: _unregister [?] (\"/ LLaMA3+DINOv2, L24 LLaMA3+CLIP, L24 LatentLens: (1) woman in tan blazer (2) man wearing grey coat (3) woman wears an off-white ski jacket EmbeddingLens: Drawable LogitLens: classpath [?] [?] LatentLens: (1) ... lime green , apricot orange , turqu... (2) blue, bergundy and gray coat. (3) tan/ yellow dog laying across young girls lap EmbeddingLens: Gold gold white LogitLens: GOLD ConverterF.. HEST Figure 27. Layer 24 (late): Four randomly sampled visual tokens at layer 24. 35 LATENTLENS LLaMA3+SigLIP, L31 OLMo+CLIP, L31 LatentLens: (1) ...in degree of filth, or at least gr... (2) beneath it fahrradstrae, capital f,... (3) ...ot passenger plane, but plane fo... EmbeddingLens: [?] Yard LogitLens: [?] _defaults mia LatentLens: (1) wall-mounted , white box and toilet pape... (2) wall-mounted mirror, with decorative, b... (3) wall-mounted flat screen television EmbeddingLens: quil Cl %; LogitLens: speakers speaker . Qwen2+CLIP, L27 Qwen2+SigLIP, L27 LatentLens: (1) the black tilted wheel (2) tilted black pole (3) tilted cookie sheet EmbeddingLens: fileName [?] elapsed LogitLens: -left left right LatentLens: (1) window on the side of the conference room (2) the app for watching youtube videos (3) the app for watching vimeo videos EmbeddingLens: [TH] [?] , LogitLens: [?] <?= [?] Figure 28. Final layer: Four randomly sampled visual tokens at the final layer (31 for OLMo/LLaMA, 27 for Qwen2). 36 M. Behind The Scenes LATENTLENS The goal of this section is to make science more transparent and engaging, showing not just the polished paper at the end but also all the detours and lessons learned. Note: Due to anonymity certain details might be omitted, and released at full length for the camera-ready version. M.1. From start to finish The pivot. This project started last winter (December 2024) when the first author, originally working on vision-andlanguage models in general, was determined to pivot towards understanding models and fundamental science. So interpretability was the natural direction to look into, from afar it had been intriguing to follow the field the past few years. But just doing interpretability for the sake of interpretability did not feel like the right approach. So what was an actual fundamental question that many people would care about, ourselves included, where interpretability could naturally help? Brainstorming. The first author tried to present too many potential ideas at once in their lab meeting. There was not enough time to present them all, and it would not have been fun presentation to follow with too many disjoint pitches. So lab colleague simply asked: Which one of these questions are you genuinely excited about? And this is how we ended up with this paper and the research question we ask: How can frozen LLMs possibly make sense of visual tokens? Do they look like language tokens to the model? We started with this simple question but the first author could not have possibly imagined all these new insights we would stumble upon, such is the process of science. It was really situation of unknown unknowns: the kind of experiments and ideas we would eventually end up with were inconceivable at the time, and how with each new experiment, three new options opened up. As result writing the paper in 8 pages was very challenging and three sections had to be cut even from the appendix. Dont trust assumptions. If the first author had to take away just one lesson from the project, it would be to never trust long-held assumptions (either by oneself or the field). We had assumed that visual tokens were not really interpretable with nearest neighbors from the embedding matrix, and several papers hinted in that direction. In retrospect, we should have simply tested this empirically across several models from day one. Instead, the first author read papers about anisotropy and other interpretability literature, prematurely concluding that embeddings from different modalities live in entirely different narrow cones.11 So, then the question became: If these tokens are not interpretable and live in different subspaces, maybe we have to perform more linear algebra and embedding space tricks to show how they relate to text? Rotate them around, learn some simple mappings, specific subspaces, ...? The Mosaic Dataset. Eventually, the first author hypothesized: Maybe visual tokens are not directly mapped to interpretable words (measured by NNs from the embedding matrix) because real data is messy? Visual objects in scene span many tokens, or single token might represent several objects and different attributes. So it would be no surprise that they dont just cleanly map to single concept. But what if we could simplify this situation? What if we could make the data we train on simpler and simpler until we see very predictable nearest neighbors. This is where the project went off-track for while and we started experimenting with toy datasets we coined Mosaic, where the model gets as input an image like this: The LLM would then be trained to predict the sequence of color words one after the other (red blue red pink ...), essentially 11We even ran lots of experiments on anisotropy and concluded that effects reported in other papers seemed overly simplified. At this point wouldnt feel confident saying that different modalities live in different narrow cones inside e.g. an LLM. 37 LATENTLENS copying each visual tokens corresponding color to the text space. We observed some interesting quirks, but surprisingly not many interpretable nearest neighbors (i.e. color words from the embedding matrix). In fact, fewer than on the natural images we report in this paper. We even ran various causal patching experiments, studied high-norm outliers, etc! They all seemed interesting at the time, but again, simply testing our assumptions early would have saved us these detours. At this point we had already adopted the Molmo codebase for training models, since we cared about how frozen LLMs make sense of visual tokens. But most VLMs you can take off-the-shelf unfreeze the LLM weights at some stage of training, which is why we opted for our controlled training setup that the final paper still builds upon. Hope. The first author does not remember when exactly we finally questioned our assumptions, and empirically tested the interpretability on natural images. But sometime around July, we started exploring interactive demos of natural images with the top NN from the embedding matrix. Since we happened to explore OLMo+CLIP-ViT, we were surprised to see so many meaningful words. The cosine similarity was low (between e.g. 0.07 and 0.15), much lower than what LATENTLENS eventually yields. Nonetheless this was encouraging, but it soon became clear that automating the judgment of whether an NN is interpretable is not trivial.12 first glimpse of the final story. With these exciting findings (40% to 60% of tokens in OLMo+CLIP-ViT are interpretable), the initial story of the paper was roughly: Visual tokens at layer 0 are more interpretable than some would assume! In the background we kept working on directions that did not end up in the main paper or even appendix: 1) We conducted ablations under which conditions this interpretability would increase or decrease, now in Section D, though without any trace of these initial EmbeddingLens results. 2) We always wondered what these strange non-interpretable tokens encode... They often had the same EmbeddingLens nearest neighbors, they were often in non-salient background regions. Were they something akin to task vectors or register tokens? At the time this seemed like an interesting enough story to publish: Past work assumes visual tokens are rarely interpretable at the input via EmbeddingLens but here we show for some models that is not the case. We show ablations, we show patching experiments. Some qualitative results. It is good we kept exploring further: Some co-authors kept wondering, especially SR and MM: What happens to visual tokens at LLM layers after the input? And thats where the final paper we now have started taking shape. We adopted EmbeddingLens and LogitLens not just at the beginning or end of the model but throughout. And eventually we wondered: What if we use contextual embeddings instead of static embedding matrices? (Thank you MM and Elinor Poole-Dayan for the nudge!) M.2. Lessons and Reflections Automation. This project co-occurred with the rise of Cursor and Claude Code. Especially interactive demos for quick exploration are now much easier to build, crucial for projects like this one. Due to the speed of experimentation, lot of ideas on the side did not make it into the paper but can now be easily continued as follow-up work. Lessons on science. As mentioned above, testing the simplest assumptions early on is something the first author will keep as guiding principle for future research. The field knows less than one would expect. Models change constantly, experiments might look different with slightly different setups. Re-run what others have seemingly done before. Interpretability is fun. The process of open-ended discovery is very enjoyable and naturally leads to interesting brainstorming sessions with colleagues and friends. The first author highly recommends working at least on one interpretability project in ones research journey. The community is quite unique, reflective and open to good ideas (ideally) regardless of whether they are immediately useful downstream. The first author recently wrote blog post, which goes into detail about how the pivot to interpretability last year went: Better late than never: Getting into interpretability in 2025. Personal. This project carries lot of meaning. It will be the final chapter of the first authors PhD and was in many ways perfect ending to this five-year long journey. have rarely felt so proud of work, and again not just because of the final product, but the whole process: Every single collaborator on here was fantastic, either as my closest friends throughout the PhD or as recent cherished connections and mentors. special shout out goes to MM and DE who both put lot of care into this paper. This project also perfectly encapsulates the first authors strengthened conviction to stay in academia for the long haul. In way it represented what academia ideally could be and what we should strive for it to be. Looking back at other projects, never before did have so many enjoyable interactions when sharing what we are working on. Even before the work is now getting out, gave four talks about it (three in-person), and many more Mila coffee table chats. It made me 12It would take another 3 months to develop the full LLM judge we eventually relied upon. 38 realize how science is so much more than papers, and papers are just one way to let others know what you found. One can give talks, write blog posts, make videos, tell others about your work, write it into the snow, showcase it as demo. LATENTLENS"
        }
    ],
    "affiliations": [
        "Canada CIFAR AI Chair",
        "McGill University",
        "Mila Quebec AI Institute",
        "University of Copenhagen",
        "Université de Montréal"
    ]
}