{
    "paper_title": "GRACE: Generative Representation Learning via Contrastive Policy Optimization",
    "authors": [
        "Jiashuo Sun",
        "Shixuan Liu",
        "Zhaochen Su",
        "Xianrui Zhong",
        "Pengcheng Jiang",
        "Bowen Jin",
        "Peiran Li",
        "Weijia Shi",
        "Jiawei Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE."
        },
        {
            "title": "Start",
            "content": "GRACE: GENERATIVE REPRESENTATION LEARNING VIA CONTRASTIVE POLICY OPTIMIZATION Jiashuo Sun1 Shixuan Liu2 Zhaochen Su3 Xianrui Zhong1 Pengcheng Jiang1 Bowen Jin1 Peiran Li4 Weijia Shi5 Jiawei Han1 1University of Illinois UrbanaChampaign 2Australian National University 3Hong Kong University of Science and Technology 4University of WisconsinMadison 5University of Washington"
        },
        {
            "title": "ABSTRACT",
            "content": "Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as black-box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide generative policy. In GRACE, the LLM acts as policy πθ that produces explicit, human-interpretable rationalesstructured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with multi-component reward function that maximizes similarity between querypositive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross-category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE. 5 2 0 2 6 ] . [ 1 6 0 5 4 0 . 0 1 5 2 : r Figure 1: Joint comparison of generative and embedding performance across existing baselines and our GRACE models. GRACE models shift instruction-tuned bases upward, simultaneously improving embedding performance while retaining generative competence. 1 Figure 2: Comparison of standard contrastive learning (top) and our RL-based method (bottom). Given query with positive (D+) and negative (D) documents, our policy model generates rationales for q, D+, and D, concatenates them to obtain the final representation, and is optimized with rewards that increase similarity between the and D+ while decreasing similarity between the and D."
        },
        {
            "title": "INTRODUCTION",
            "content": "The advent of Large Language Models (LLMs) has marked paradigm shift in the field of Natural Language Processing (NLP) OpenAI (2023); Yang et al. (2024; 2025); Dubey et al. (2024). Owing to their vast parameter scale and pre-training on massive text corpora, these models have demonstrated remarkable capabilities in language understanding, reasoning, and generation, leading to breakthroughs in nearly all NLP tasks. One of the most critical application areas is the use of LLMs as universal text encoders to provide high-quality semantic representationsor text embeddingsfor downstream tasks such as semantic retrieval, text clustering, and recommendation systems Wang et al. (2024); BehnamGhader et al. (2024); Lee et al. (2025); Springer et al. (2025). From BERT Devlin et al. (2019) to contemporary instruction-tuned LLMs Ouyang et al. (2022), the research community has persistently explored methods to more effectively leverage the knowledge within these models to construct more accurate and robust representation spaces. However, the prevailing paradigm for training LLMs as representation models harbors profound inherent contradiction. Current approaches predominantly rely on contrastive learning frameworks that optimize discriminative objectives such as InfoNCE loss van den Oord et al. (2018), treating the LLM merely as parameterized encoder function fθ : Rd. This paradigm forces these inherently generative models to produce static embedding vectors through simple pooling mechanisms, fundamentally suppressing their capacity for structured reasoning and natural language generation. The model learns to minimize distances between positive pairs while maximizing distances from negatives, but this process occurs entirely within an opaque latent space. Consequently, we lose the interpretability that makes LLMs valuablethe ability to understand and articulate their reasoning process. When an embedding model determines that two texts are similar, we cannot inspect why it made that judgment or which semantic features it prioritized. This fundamental limitation motivates us to reconceptualize the role of contrastive signals in representation learning. Rather than treating contrastive objectives as loss functions to be minimized through gradient descent, we view them as reward signals that guide generative policy. This perspective naturally leads to reinforcement learning framework in which the LLM acts as policy πθ that generates interpretable understandings of input texts. These understandings serve dual purpose: they provide human-readable explanations of the models semantic reasoning and are simultaneously encoded into high-quality representations of the inputs. By formulating representation learning as sequential decision-making problem, we leverage the full generative capacity of LLMs, yielding interpretable reasoning and effective text representations. To realize this vision, we present GRACE (Generative Representation Learning via Contrastive Policy Optimization), framework that turns LLMs into interpretable representation learners using policy-gradient optimization. The model first produces explicit rationales that analysizes and 2 reasoning the input. From we derive the final embedding via mean pooling over hidden states. We recast contrastive learning signals as rewards that increase querypositive similarity and decrease querynegative similarity. Optimizing this reward with standard policy-gradient methods teaches the model to generate faithful rationales while simultaneously learning effective text representations. The main contributions of this work can be summarized as follows. First, we present the first empirical evidence that rewards derived from contrastive learning can be leveraged to train policy models, resulting in improved representational capabilities. Second, we propose novel methodology that enables the transformation of existing LLMs into powerful representation models while preserving their general-purpose capabilities without performance degradation, as shown in Figure 1. Third, this work represents substantial advancement in text representation interpretability, as the models reasoning can be directly inspected through its textual outputs. Fourth, our method yields significant performance gain of avg 11.5 % over baseline models when evaluated on the MTEB benchmark. Finally, to facilitate reproducibility and advance future research in this domain, we will make all models, datasets, and code publicly available."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "2.1 CONTRASTIVE LEARNING FOR TEXT EMBEDDINGS Traditional contrastive learning for text representation follows discriminative paradigm. Given dataset = {(qi, d+ denotes relevant document, and j=1 denotes irrelevant documents, an encoder fθ : Rd is trained to minimize the = {d i,j}m InfoNCE loss: i=1 where qi denotes query, d+ , )}N LInfoNCE = log exp(sim(fθ(qi), fθ(d+ exp(sim(fθ(qi), fθ(d+ ))/τ ) + (cid:80) dD ))/τ ) exp(sim(fθ(qi), fθ(d))/τ ) (1) where sim(, ) denotes cosine similarity and τ is temperature parameter. In practice, in-batch negatives Karpukhin et al. (2020) are commonly employed for computational efficiency: Lbatch = 1 (cid:88) i=1 log exp(sim(qi, d+ )/τ ) j=1 exp(sim(qi, d+ )/τ ) (cid:80)B (2) where is the batch size. The effectiveness of this approach critically depends on hard negative miningidentifying the most challenging negative samples that lie close to the decision boundary. 2.2 POLICY GRADIENT OPTIMIZATION Policy gradient methods optimize parametrized policy πθ by directly maximizing the expected reward over generated trajectories. Given prompt and policy πθ, the probability of generating sequence is πθ(yx) = πθ(yt x, y<t). (3) (cid:89) t=1 With reward function r(x, y) that evaluates the quality of response to prompt x, the optimization objective is the expected reward: J(πθ) = Eyπθ(x) (cid:2)r(x, y)(cid:3). (4) By applying the policy gradient theorem, the gradient of the objective can be expressed as θJ(πθ) = Eyπθ(x) [r(x, y) θ log πθ(yx)] , (5) which provides an unbiased estimator of the true gradient Williams (1992); Sutton et al. (1999). To reduce the variance of policy gradient estimates, it is common to subtract baseline b(x) that does not depend on the sampled action. The gradient then becomes: θJ(πθ) = Eyπθ(x) (cid:2)( r(x, y) b(x) ) θ log πθ(yx)(cid:3) (6)"
        },
        {
            "title": "2.3 GROUP RELATIVE POLICY OPTIMIZATION",
            "content": "Group Relative Policy Optimization (GRPO) treats model training as reinforcement learning problem Shao et al. (2024). Given policy πθ, GRPO samples group of responses {y1, ..., yG} πθold(x) from the old policy θold for each input x, and computes the advantage of each response relative to the group: ˆAi = (cid:80)G r(x, yi) 1 std({r(x, yj)}G j=1 r(x, yj) j=1) (7) where r(x, yi) is the reward of yi, and the token-level advantage ˆAi,t = ˆAi. The importance ratio is defined as ri,t(θ) = πθ(yi,tx,yi,<t) πθold (yi,tx,yi,<t) . The objective without KL divergence is: JGRPO(θ) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 yi yi (cid:88) t=1 (cid:16) min ri,t(θ) ˆAi,t, clip(ri,t(θ), 1 ε, 1 + ε) ˆAi,t (cid:17) (8)"
        },
        {
            "title": "OPTIMIZATION",
            "content": "We propose to fundamentally reimagine the role of contrastive signals in representation learning. Rather than treating them as loss functions to be minimized, we reconceptualize them as reward signals that guide generative policy. Our framework transforms the LLM from passive encoder that outputs static embeddings into an active agent that generates interpretable rationale of the input text. 3.1 RATIONALE-GENERATING POLICY To enable the LLM to perform structured reasoning for similarity judgments, we adopt policy that generates an explicit reasoning trace (rationale) for each input. Let P() denote the prompting function that prepends the representation instruction to the input text. For any input {q, d+, d}, the policy πθ produces structured rationale: πθ( P(x)). (9) The rationale identifies salient semantic features, key concepts, and potential relations grounded in domain knowledge, providing transparent trace that supports downstream similarity assessment. 3.2 FROM RATIONALE TO REPRESENTATION Given the generated rationale for input x, we obtain contextualized hidden states by conditioning on both the instruction-augmented input and the rationale: = πθ(P(x) r) RLd, (10) where is the sequence length, is the hidden dimension, and denotes concatenation. To focus on semantic content while excluding instructional artifacts, we apply masked mean pooling over the last-layer hidden states: = 1 (cid:88) tM Et, = { : Lsys < L, maskt = 1 }, (11) where Rd is the final representation and Lsys denotes the length of system prompt tokens to be excluded. Anchoring representation extraction to an explicit rationale yields semantically rich, interpretable embeddings and enables more reliable similarity judgments."
        },
        {
            "title": "3.3 CONTRASTIVE REWARDS AS POLICY GUIDANCE",
            "content": "Given the recent advances of outcome-reward and policy optimization methods in LLMs, and the practical difficulties and inefficiencies of training value models, our algorithm is correspondingly designed around policy-based optimization approach. For exposition, we adopt Group Relative Policy Optimization (GRPO) Shao et al. (2024) to walk through the procedure, though our framework is agnostic to the specific policy-gradient algorithm."
        },
        {
            "title": "3.3.1 ROLLOUT STRATEGY",
            "content": "To balance exploration diversity with computational efficiency, we adopt an asymmetric rollout strategy. For each training instance (qi, d+ ), we employ different generation strategies based on the text type: , yqi πθ(P(qi)), yd πθ(P(d )), {y(k) d+ k=1 πθ(P(d+ }K )) (12) For positive documents d+ , we perform stochastic rollouts to generate diverse rationales, enabling the model to explore different interpretational perspectives of the same content. In contrast, queries qi and negative documents undergo single-sample generation to produce reference representations, which serves as fixed anchors for reward computation. The computed similarity-based contrastive rewards facilitate advantage estimation within the GRPO framework, driving policy updates that enhance both the quality and diversity of text understanding across all input types. 3.3.2 REWARD DESIGN We design composite reward function that translates contrastive learning objectives into actionable policy guidance. For each positive document rollout y(k) , we compute four synergistic reward d+ components that collectively shape the learning dynamics. i,m}Mi = {d Let m=1 be the set of negatives for query qi.The foundation is the contrastive learning reward RCL, which encourages semantic alignment between queries and relevant documents while penalizing spurious correlations with irrelevant ones: R(i,k) CL = sim(cid:0)hqi, h(k) d+ (cid:1) Mi(cid:88) m=1 sim(cid:0)hqi , hd i,m (cid:1). (13) To ensure semantic coherence across multiple interpretations of the same document, we introduce the consistency reward, which encourages similar representations among concurrent rollouts: R(i,k) consist = 1 (cid:88) j=k sim(h(k) d+ , h(j) d+ ) (14) Most critically, we incorporate hard negative mining inspired by in-batch negative sampling strategies. For each query, we identify the most challenging distractors (positives from other training instances that exhibit spuriously high similarity). Let denote the batch size and {1, . . . , K} index rollouts. We select, for each other instance j, the maximum similarity across its rollouts to obtain the hardest distractor for the current query: R(i) hard = 1 1 (cid:88) j=1 j=i max 1lK sim(cid:0)hqi, h(l) d+ (cid:1). (15) The composite reward integrates these complementary objectives: CL + λ1R(i,k) where λ1 and λ2 are hyperparameters that balance the relative contributions of consistency preservation and hard negative discrimination. consist + λ2R(i) total = R(i,k) R(i,k) (16) hard To sharpen the reward distribution and stabilize training dynamics, we apply temperature scaling to the composite rewards: ˆR(i,k) total = R(i,k) total τ (17) where τ is the reward temperature parameter that controls the sharpness of the advantage distribution."
        },
        {
            "title": "3.4 POLICY OPTIMIZATION OBJECTIVE",
            "content": "Following the GRPO framework, we compute advantages relative to the group baseline but remove standard deviation: A(i,k) = R(i,k) final"
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) l=1 R(i,l) final The policy is then optimized to maximize the advantage-weighted likelihood: Ltotal = E(q,d+,d)D (cid:34) (cid:88) (cid:88) i=1 k=1 A(i,k) log πθ(y(k) d+ P(d+ )) (cid:35) Since our optimization is on-policy, we omit importance sampling here. (18) (19) 3.5 UNSUPERVISED LEARNING EXTENSION Inspired by SimCSEs Gao et al. (2021) unsupervised paradigm, we extend our framework to settings where only raw text is available without explicit query-document pairs. The key insight is that different interpretations of the same text should maintain semantic coherence while being distinguishable from interpretations of different texts. Given batch of texts = {xi}B i=1, we perform asymmetric rollouts for each text: yanchor xi πθ(P(xi)), {y(k) xi }K k=1 πθ(P(xi)) (20) Here the anchor serves as the positive counterpart for its own rollouts, directly analogous to SimCSEs same sentence positive constructed via independent noise, while the rollouts probe diverse yet semantically consistent interpretations of xi. We instantiate the unsupervised reward with the self-alignment term between the anchor interpretation and each rollout of the same text. Given the anchor representation hanchor xi , the reward is and rollout h(k) xi R(i,k) (cid:16) self = sim hanchor xi , h(k) xi (cid:17) . (21) The remaining terms, within-text consistency across rollouts of the same xi and in-batch hard-negative mining against other texts in the batchare identical to their definitions in Sec. 3.3.2. For brevity, we omit their formulas here. When enabled, the overall unsupervised objective is the same weighted combination as in Sec. 3.3.2."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4.1 DATASETS AND EVALUATION METRICS We conduct comprehensive evaluations using the Massive Text Embedding Benchmark (MTEB) Muennighoff et al. (2023), standardized framework that covers 7 task categories and 56 datasets, spanning retrieval (Retr.), reranking (Rerank.), clustering (Clust.), pair classification (PairClass.), classification (Class.), semantic textual similarity (STS), and summarization (Summ.). For representation aggregation, we adopt mean pooling Reimers & Gurevych (2019), explicitly excluding instruction tokens to avoid instruction-specific artifacts. 6 Categories # of datasets 15 Retr. Rerank. Clust. 4 PairClass. Class. 3 12 STS 10 Summ. 1 Avg. Qwen2.5-1.5B-Instruct 29.32 32.45 43.88 46.95 25.44 27.88 36.21 39.55 36.18 39.20 52.02 54.84 35.77 39.42 53.87 55.36 LLaMA-3.2-3B-Instruct 38.16 40.44 47.35 49.12 32.05 34.28 39.92 41.30 48.12 51.27 58.66 60.44 Qwen2.5-3B-Instruct 44.16 46.55 52.87 54.85 36.85 38.82 43.26 44. 53.72 56.61 74.08 79.64 47.36 50.89 58.15 60.72 53.36 57.23 65.94 68.25 Qwen3-4B-Instruct-2507 48.16 49.72 53.38 55.85 38.55 40.76 43.02 45. 55.33 57.20 78.81 82.94 54.87 55.41 69.94 71.02 44.11 47.26 56.39 59.42 59.25 61.78 65.55 64.02 66.15 68.22 70.05 74.65 66.02 68.35 74.12 77. Table 1: Supervised results on MTEB. 26.32 26.78 28.43 30.41 27.78 28.14 28.63 29.10 26.26 28.55 29.68 30.10 29.44 29.62 29.91 30.46 30.33 32.92 43.21 45. 39.34 41.54 47.39 48.49 44.12 46.59 52.10 54.74 45.49 46.87 54.34 56.64 Base - w/ reasoning - w/ CL training GRACE Base - w/ reasoning - w/ CL training GRACE Base - w/ reasoning - w/ CL training GRACE Base - w/ reasoning - w/ CL training GRACE 22.15 24.83 38.95 40.44 31.28 33.21 42.42 44.01 37.38 39.42 45.90 49.42 37.42 38.91 48.66 52.11 4.2 BASELINES Supervised Baselines We compare four variants in the supervised setting: (1) Base (No Training) uses the pre-trained instruction-tuned model without any representation-specific optimization and serves as the starting point; (2) Base w/ Reasoning applies our reasoning prompt to the same model but performs no further training, isolating the effect of reasoning-style outputs on embeddings; (3) Contrastive Learning (CL) fine-tunes the base model with standard InfoNCE objective using in-batch negatives Chen et al. (2020), representing the predominant contrastive approach; and (4) GRACE introduces reward-guided policy optimization that explicitly aligns generative reasoning with representation quality. Unsupervised Baselines In the unsupervised setting, we report: (1) Other Open Models, including representative encoder baselines (e.g., BERT Devlin et al. (2019), RoBERTa Liu et al. (2019)) and recent LLM-embedding methods (e.g., LLM2Vec BehnamGhader et al. (2024), Echo Springer et al. (2025)); (2) Base (No Training), the same instruction-tuned model used zero-shot without representation-specific training; (3) SimCSE, which fine-tunes with the unsupervised SimCSE objective based on dropout-induced positives Gao et al. (2021); and (4) GRACE, which applies the same reward-guided policy optimization to align generative reasoning with embedding quality in an unsupervised regime. Implementation Details We conduct experiments with four decoder-only language models: Qwen2.5-1.5B/3B-Instruct Yang et al. (2024), Qwen3-4B Yang et al. (2025), and LLaMA-3.23B-Instruct Dubey et al. (2024). For supervised training, we use replication of the public portion of the E5 dataset Wang et al. (2024), which contains 1.5M samples following BehnamGhader et al. (2024). For unsupervised training, we follow the SimCSE Gao et al. (2021) setup without collecting any additional unlabeled corpus. All experiments are run on single node with 4 NVIDIA H100 GPUs (94GB each). Training is performed for 2 epochs with batch size of 64. We set the maximum prompt length to 1024 tokens and the maximum response length to 2048 tokens, applying right-side truncation when sequences exceed these limits. 7 Categories # of datasets Retr. Rerank. Clust. 4 11 PairClass. Class. 3 12 STS 10 Summ. Avg. 56 BERT RoBERTa LLM2VecLLaMA-3-8B EchoMistral-7B Base - w/ SimCSE GRACE Base - w/ SimCSE GRACE Base - w/ SimCSE GRACE Base - w/ SimCSE GRACE 10.59 62.63 24.75 71.63 22.15 31.28 34.57 31.28 35.42 36.55 37.38 42.25 43.15 37.42 42.18 43.67 Other Open Models 43.44 29.05 49.20 33.51 30.12 56.95 39.74 72.31 56.33 41.92 65.91 47.43 61.66 8.62 69.00 22.85 54.36 55.24 67.85 73.64 29.82 28.64 25.59 31. 38.33 37.86 48.84 49.02 Qwen2.5-1.5B-Instruct 29.32 38.94 41.86 25.44 33.12 35.49 36.18 50.67 51.12 35.77 47.53 50. 44.11 58.21 56.44 26.32 28.34 29.07 30.33 39.65 41.45 LLaMA-3.2-3B-Instruct 38.16 41.27 43.15 32.05 34.96 36. 48.12 52.88 55.27 47.36 50.73 53.62 59.25 65.44 63.18 27.78 29.24 29.52 39.34 43.00 44.04 Qwen2.5-3B-Instruct 44.16 48.33 49.72 36.85 39.87 41.58 53.72 68.22 70.44 53.36 60.15 62.91 66.15 70.84 69.38 26.26 29.56 29. 44.12 49.17 50.15 Qwen3-4B-Instruct-2507 48.16 50.72 52.34 38.55 41.63 42.87 55.33 69.14 70.05 54.87 61.25 62. 66.02 72.48 71.66 29.44 29.62 30.16 45.49 50.11 51.03 Table 2: UnSupervised results on MTEB. 4.3 MAIN RESULTS Supervised Results Table 1 reports supervised MTEB results across seven task families for four decoder-only backbones and four training settings. Averaged across the four backbones, our method improves the MTEB average by 11.52% over the Base model, indicating consistent improvements across model sizes and families. The gains are broad based, with especially strong uplift on retrieval and pair classification, while classification, clustering, STS, and summarization also improve. The intermediate variants also contribute: explicit reasoning provides modest but reliable increase, and contrastive training further enlarges margins; the final method delivers the most balanced cross-task performance. Unsupervised Results Table 2 summarizes unsupervised MTEB performance and shows consistent stepwise improvement from Base, SimCSE and GRACE across all four backbones. Averaged across backbones, our unsupervised method improves the MTEB average by 6.85% over the Base model. Relative to widely used open baselines, our best unsupervised results (e.g., Qwen3-4B and Qwen2.5-3B) past LLM2Vec and Echo, while comfortably exceeding encoder-only BERT/RoBERTa averages. Improvements are broad based, with retrieval and pair classification benefiting most, and STS remaining strong without sacrificing performance on classification or clustering. These trends suggest that even without supervised signals, injecting reasoning-aware contrastive objectives yields robust, transferable enhancements over both naive LLM pooling and standard SimCSE-style tuning. 4.4 ABLATION STUDIES The MTEB benchmark covers wide range of embedding tasks across diverse types, domains, and difficulty levels. For computational efficiency and comparability, following BehnamGhader et al. (2024), we evaluate representative 16-task subset for ablations and analysis (Table 4.5)."
        },
        {
            "title": "4.5 SUBSET OF MTEB BENCHMARK",
            "content": "To enable compute-efficient ablations while preserving coverage across the MTEB taxonomy, we evaluate on compact, representative subset of 16 datasets spanning seven task families (Table 4.5): Retrieval (3), Reranking (2), Clustering (3), Pair Classification (1), Classification (3), STS (3), and Summarization (1). We run all ablation studies on this 16-task subset in order to systematically isolate component contributions, probe hyperparameter sensitivity, and compare training strategies under fixed compute budget, while maintaining fidelity to the full benchmark. The selection balances domain diversity (biomedical, news, open-domain, intent), supervision formats (point-wise, pair-wise, list-wise), and difficulty, yielding stable signals for both representation quality and downstream utility. For each task we report the official metric defined by MTEB (e.g., nDCG@10 for retrieval, MAP for reranking, V-measure for clustering, accuracy/AP for classification and pair classification, and Spearman correlation for STS; summarization uses the SummEval correlation provided by the harness). Task Retrieval (3) Reranking (2) Clustering (3) Dataset"
        },
        {
            "title": "SciFact\nArguAna\nNFCorpus",
            "content": "StackOverflowDupQuestions SciDocsRR BiorxivClusteringS2S MedrxivClusteringS2S TwentyNewsgroupsClustering Pair Classification (1) SprintDuplicateQuestions Classification (3) STS (3) SummEval (1) Overall Banking77Classification EmotionClassification MassiveIntentClassification STS17 SICK-R STSBenchmark SummEval 16 datasets Table 3: Subset of MTEB tasks used for our ablations and analysis. 4.5.1 ABLATION ANALYSIS OF REWARD FUNCTION DESIGN We conduct comprehensive ablation study on the two key hyperparameters in our reward function: consistency (λ1) and hard negative mining (λ2), evaluating 5 5 grid of configurations with five discrete values for each parameter (0.0, 0.3, 0.5, 0.7, 1.0). Figure 3 presents the performance landscape for GRACE-3B under both supervised and unsupervised paradigms, revealing several critical insights. The results demonstrate that removing all reward constraints (λ1 = 0, λ2 = 0) yields poor performance for supervised and unsupervised training respectively, confirming the necessity of these structured reward signals. When only one reward component is active, pure hard negative mining (λ1 = 0, λ2 = 1) reaches 50.2 % and 45.2 %, while pure consistency weighting (λ1 = 1, λ2 = 0) achieves 49.3 % and 45.0 %, indicating the former provides better baseline. Notably, the model exhibits significantly higher sensitivity to the hard negative mining weight (λ2) compared to the consistency weight (λ1), suggesting that hard negative discrimination plays more critical role in determining overall performance. 4.5.2 COMPARISON WITH ALTERNATIVE RL ALGORITHMS To verify that our approach is not tied to specific optimization scheme, we also applied it to different RL algorithms. As shown in Table 4.5.2, our method consistently improves performance across Figure 3: Reward function ablation study for GRACE-3B showing performance across different combinations of the consistency weight (λ1) and the hard negative mining weight (λ2). Left: supervised training; Right: unsupervised training. The heat intensity indicates performance levels, with darker red representing higher scores. all three algorithms, demonstrating both its portability and generalizability. Among them, GRPO remains the most effective in our setting, while REINFORCE++, ReMax and DAPO bring smaller gains, likely because their design focuses on issues (e.g., reward hacking, weak credit assignment and long-CoT instability) that are less critical in our tasks. These findings confirm that our framework can be readily integrated with diverse RL optimizers, ensuring wide applicability. Algorithm # of datasets GRACE-3B Retr. Rerank. Clust. 2 3 PairClass. Class. 1 3 STS 3 Summ. Avg. 16 w/ ReMax Li et al. (2024) w/ REINFORCE++ Hu et al. (2025) w/ DAPO Yu et al. (2025) w/ GRPO Shao et al. (2024) 42.63 43.10 44.58 44.72 61.24 64.9 65.62 65.71 35.61 37.67 39.24 38.99 68.3 68.0 70.67 70. 62.3 63.18 63.58 64.35 70.8 71.60 72.8 72.53 29.04 29.91 30.04 30.09 53.36 54.64 55.78 55.89 Table 4: Comparison of our supervised method with different RL algorithms on subsets of MTEB. 4.5.3 GENERALIZATION TO GENERAL DOMAIN TASKS Table 5 evaluates whether embedding-oriented fine-tuning affects broad capabilities across mathematics (GSM8K Cobbe et al. (2021)), knowledge and reasoning (MMLU Hendrycks et al. (2021), BBH Suzgun et al. (2023), TriviaQA Joshi et al. (2017), FEVER Thorne et al. (2018)), and code generation (HumanEval Chen et al. (2021)). Across four backbones, our method preserves general performance: both supervised and unsupervised settings yield near-zero average shifts relative to the instructiontuned base. In sharp contrast, the CL fine-tuning baseline suffers severe deterioration, indicating that naive contrastive objectives can substantially erode general-domain competence. We attribute the stability of our approach to the way the contrastive signal is integrated into learning. Rather than minimizing token-agnostic loss, we optimize contrastive reward within an RL framework, which (1) aligns updates with the generative policy, preserving instruction following and problem solving while shaping the embedding geometry; (2) uses relative, advantage-weighted updates that resist representation collapse and the rescaling/drift common in direct InfoNCE-style training; and (3) keeps reasoning generation intact so the policy continues to practice skills needed for general tasks. As result, representations improve for retrieval objectives without sacrificing the general capabilities conferred by pretraining and instruction tuning. 10 Table 5: Performance on General Domain Tasks Dataset Metric GSM8K MMLU TriviaQA FEVER BBH HumanEval EM EM EM Acc EM Pass@1 Avg. Qwen-2.5-1.5B-Instruct 54.94 0.0 55.12 54.81 18.35 0.0 18.10 18.29 66.91 50.28 67.43 66.75 LLaMA-3.2-3B-Instruct 14.74 0.0 15.01 14.69 29.21 0.0 29.05 29.18 64.52 50.01 65.10 64.40 Qwen-2.5-3B-Instruct 62.60 0.0 61.20 61.50 28.80 0.0 27.60 28. 71.50 50.13 72.80 71.20 Qwen-3-4B-Instruct-2507 69.45 0.0 69.71 69.38 31.55 0.0 31.40 31.52 83.53 51.07 84.02 83.40 25.25 0.0 25.01 25. 9.72 0.0 9.61 9.80 35.00 0.0 34.30 34.80 35.01 0.0 34.88 35.09 46.95 0.0 47.30 47.05 38.41 0.0 38.90 38.55 52.80 0.0 53.10 52. 68.90 0.0 69.35 69.01 40.74 8.38 41.08 40.88 28.89 8.33 29.27 28.91 51.40 8.35 51.50 51.10 60.73 8.51 61.13 60.74 -32.36 +0.34 +0. -20.56 +0.38 +0.02 -43.05 +0.10 -0.30 -52.22 +0.40 +0.01 Base Bsse w/ CL training GRACE (Supervised) GRACE (Unsupervised) Base Bsse w/ CL training GRACE (Supervised) GRACE (Unsupervised) Base Bsse w/ CL training GRACE (Supervised) GRACE (Unsupervised) Base Bsse w/ CL training GRACE (Supervised) GRACE (Unsupervised) 32.06 0.0 32.54 32.21 16.75 0.0 17.02 16.81 57.90 0.0 59.10 58.00 75.96 0.0 76.42 76."
        },
        {
            "title": "5 ANALYSIS",
            "content": "5.1 EFFICIENCY ANALYSIS We decompose end-to-end latency into encoding (Tencode), generation (Tgen), and matching (Tmatch). Figure 6(a) shows that for generative pipelines (Base and GRACE), Tgen overwhelmingly dominates the budget, whereas Tencode and Tmatch are comparatively small. As result, encoder-style approaches (BERT and Direct forward method) achieve much lower latency since they avoid generation. Increasing the decoding budget from G=256 to G=512 further amplifies Tgen with diminishing returns, indicating practical knee around G256. Figure 6(b) summarizes the qualitylatency trade-off. BERT and Direct forward method occupy the ultralow-latency region but underperform on accuracy, while generative methods deliver higher quality at greater cost. At matched G, GRACE consistently shifts the Pareto frontier upward relative to Base, yielding better accuracy without extra latency and making GRACE at G=256 strong default for balanced deployments. Pragmatically, because Tgen dominates end-to-end latency, the most effective lever is to accelerate generation. Deploying on newer accelerators and using optimized inference stacks with fused attention kernels, paged/continuous batching, and CUDA Graphs can materially reduce Tgen at fixed G. Additional gains come from KV-cache optimizations, speculative/assisted decoding, and lowprecision execution (FP8/INT8/INT4) that preserve accuracy under our evaluation budgets. In short, improving generation throughput shifts the entire qualitylatency curve downward without changing the training procedure, making GRACE at G=256 even more attractive for balanced deployments. 5.2 JOINT ANALYSIS OF GENERATIVE AND EMBEDDING PERFORMANCE Figure 1 illustrates the positioning of our proposed GRACE models in the joint landscape of generative and embedding performance. Existing encoder-only methods such as E5 Wang et al. (2024), LLM2Vec BehnamGhader et al. (2024), and text-embedding-3 1 achieve strong embedding quality but exhibit minimal generative competence, while instruction-tuned decoders (e.g., GPT OpenAI (2023), Claude Anthropic (2024), Gemini Gemini Team (2025), Grok xAI (2025), Deepseek DeepSeek-AI 1https://openai.com/index/new-embedding-models-and-api-updates/ 11 (a) Latency breakdown across different approaches. (b) Performancelatency trade-off. Figure 4: Efficiency comparison of different embedding approaches. et al. (2025)) demonstrate strong generative capabilities but poor embedding performance. This highlights the long-standing trade-off between the two dimensions. By contrast, the GRACE family consistently shifts base models upward in embedding quality while largely preserving their generative competence. For example, GRACE-1.5B and GRACE-3B improve the embedding strength of Qwen2.5-1.5B and LLaMA-3.2-3B by more than 15 % on average without sacrificing generative ability. Similarly, GRACE-4B substantially enhances Qwen-3-4B, pushing it into previously unattained regime of balanced performance. These results underscore the effectiveness of our generative-contrastive optimization framework in bridging the gap between high-quality embeddings and generative reasoning. 5.3 TRAINING PROGRESSION ANALYSIS As shown in Figure 5, both task performance and response length increase steadily with more training steps. The accuracy on subtasks follows consistent upward trajectory, reflecting that extended training directly strengthens the models ability for representation. At the same time, the generated responses become progressively longer. Importantly, this lengthening indicates that the models are producing answers with richer information density and more explicit reasoning chains. In earlier stages, responses tend to be short and often incomplete, whereas later stages exhibit structured explanations that combine factual correctness with reasoning depth. (a) Accuracy progression across training steps. (b) Response length progression across training steps. Figure 5: Training progression of GRACE models. Left: accuracy on subtasks steadily improves with more training steps. Right: response length also increases, reflecting enhanced information density and richer reasoning chains. 12 (a) Performance of various representation approaches in supervised fine-tuning. (b) Performance of various representation approaches in unsupervised training. Figure 6: Comparison of different token representation methods across GRACE model variants. Mean pooling from both last layer (LL) and penultimate layer (PL) consistently outperform EOS token and max pooling approaches in both supervised and unsupervised settings. 5.4 EFFECTS OF VARIOUS REPRESENTATION APPROACHES We investigate the impact of different token representation methods on model performance across both supervised and unsupervised settings. Figure 6 presents comparative analysis of four representation approaches: EOS token, Max Pooling, Mean Pooling from the last layer (LL), and Mean Pooling from the penultimate layer (PL). The results demonstrate that mean pooling approaches consistently outperform both EOS token and max pooling methods across all model variants in both settings. Mean pooling from the last layer and penultimate layer achieve remarkably similar performance levels and slightly higher in unsupervised models, while EOS token and max pooling show substantially lower performance. The minimal performance difference between LL and PL mean pooling suggests that both layers capture equally effective representations, which aligns with Skean et al. (2025), indicating that comprehensive token aggregation through mean pooling is crucial for optimal performance. (a) Sensitivity w.r.t. batch size (bs). (b) Sensitivity w.r.t. rollout number (n). Figure 7: Hyperparameter sensitivity for GRACE models. Both curves show monotonic performance improvements. Left: performance increases monotonically with larger bs. Right: higher yields steady gains with reduced variance at larger values. The best setting is bs = 64 and = 8, with further improvements possible given more GPU resources. 5.4.1 HYPERPARAMETER SENSITIVITY ANALYSIS We analyze the sensitivity of model performance to two training-time hyperparameters: the batch size bs and the number of rollouts per prompt n. Overall, all models exhibit monotonic improvement 13 Original Doc: BBC - History - British History in depth: Prime Ministers and Politics Timeline On This Day Prime Ministers and Politics Timeline Do you know which prime minister brought fallen women to 10 Downing Street? Or which one fought duel? Or who was known as the Goat? Take political journey through nearly 300 years of high ideals and low cunning, from Gordon Brown to the first man to hold prime minister... Base Model (480 tokens) Training Step 200 (562 tokens) Training Step 600 (824 tokens) Training Step 1000 (1051 tokens) The text is comprised of an interactive timeline on the history of British Prime Ministers, concise biography of several past Prime Ministers, and broader overview of the office. ... It begins with timed anecdotes highlighting key figures and events from 1681 to the present day. Provides an overview of several key British prime ministers and the history of the office. Starts with Margaret ... Thatcher, Britains first female PM, and also mentions Robert Walpole and Henry CampbellBannerman. ... Walpole is seen as the first PM, Campbell-Bannerman the first formally called so. Informative passage about key British Prime Ministers, such as Margaret Thatcher, James Callaghan, and Harold Wilson. ... Traces the evolution of the role from 18th century cabinet system, with Walpole as early figure. ... Highlights crises like Harold Wilsons industrial disputes and economic instability. ... Explains parliamentary accountability and collective responsibility. Provides comprehensive understanding of the office of the Prime Minister and its historical context. ... Roots traced back to Robert Walpole, later evolution shaped by constitutional changes. ... Mentions modern figures like Thatcher and Tony Blair, situating them in UKs political-economic. ... Discusses powers and constraints of PM, from cabinet appointments to parliamentary limits. ... References anecdotes (e.g., fallen women) as outdated, while contextualizing gender role shifts. Table 6: Case study for output evolution. As training progresses, outputs grow from brief topic recognition to structured lists, then detailed coverage, and finally full contextualization. Doc Case (Belt and Road Initiative) Query Case (Machu Picchu) Original Document: policy coordination is an important guarantee for implementing the Initiative... We should carry forward the spirit of friendly cooperation of the Silk Road by promoting cultural and academic ... Model Response: The Belt and Road Initiative is an extensive cooperation project proposed by China... Policy Coordination involves inter-governmental cooperation, expansion of shared interests, and enhancing mutual trust... People-to-People Bonds promote cultural exchange, volunteer services, and media cooperation... The initiative faces challenges such as debt sustainability and geopolitical risks... Original Query: In which country would you find the ruins of the ancient Inca city known as Machu Picchu? Model Response: Machu Picchu is located in Peru... situated in the Andes Mountains, Cusco Region, at about 2,430m elevation... rediscovered in 1911 by Hiram Bingham... UNESCO World Heritage Site and major tourist destination... Table 7: Representative cases of model responses: left column shows document understanding (doc case), right column shows query expansion (query case). with increasing batch size. Figure 7 (right) varies {2, 4, 8} and shows consistent, monotonic gains across models, with diminishing variance at higher n. In practice, we find bs 32 and 4 to be good Pareto choice under typical compute budgets, while larger settings such as bs = 64 and = 8 deliver the best observed performance. Further improvements are likely possible given more GPU resources. 5.5 CASE STUDY 5.5.1 OUTPUT EVOLUTION DURING TRAINING Table 6 illustrates how outputs evolve with training. The base model provides only brief topical summary, while step 200 introduces structured listing with concrete figures. By step 600, the output grows substantially longer and begins to integrate historical context, crises, and institutional concepts. At step 1000, the response is the most comprehensive: it connects anecdotes with broader political and constitutional developments, offering coherent narrative. This progression shows that training not only increases output length but also enhances contextualization and information density, reflecting richer internal representations. 14 5.5. IN-DEPTH ANALYSIS OF MODEL RESPONSE PATTERNS We highlight two representative cases that demonstrate the models response patterns across different input types, shown in Table 7. For long document on the Belt and Road Initiative, the model goes beyond paraphrase to produce compact outline (policy coordination, infrastructure and finance, people-to-people links, geopolitical constraints), turning an abstract preface into structured analysis. For minimal factual query (e.g., Where is Machu Picchu?), it answers precisely while adding concise, high-signal context (Peru; Andean setting; Inca and UNESCO notes; basic access considerations), avoiding unnecessary narrative. This adaptive behavior yields information-dense, well-factored texts whose embeddings align with latent topics and relations, improving separability for retrieval, clustering, and pair classification."
        },
        {
            "title": "6.1 LLM AS EMBEDDING",
            "content": "In recent years, the rise of large language models (LLMs) has sparked growing interest in using them directly as text embedding models Zhang et al. (2025); Yan et al. (2025); Ji et al. (2025). Research in this area has generally followed two paths: tuning-free approaches Li & Zhou (2025); Springer et al. (2025), which study the impact of instructions on embedding quality, and tuning-based approaches Muennighoff et al. (2025); BehnamGhader et al. (2024), which adapt models for improved performance. While tuning-free methods offer simplicity, state-of-the-art results increasingly rely on tuning-based strategies. Notable examples include the BGE Xiao et al. (2024) and GTE Li et al. (2023) series, which strengthen semantic representations through contrastive learning on large-scale text pairs. Building on this line of work, we reinterpret contrastive learning as reward signal guiding generative policy, turning LLMs into interpretable representation models. 6.2 REINFORCEMENT LEARNING IN REASONING Reinforcement learning has been central to advancing reasoning language models, exemplified by DeepSeek-R1 DeepSeek-AI et al. (2025), which achieved major breakthroughs with Reinforcement Learning with Verifiable Rewards (RLVR). RL formulates reasoning as policy optimization, improving capabilities by maximizing expected rewards. Algorithms such as PPO Schulman et al. (2017) and GRPO Shao et al. (2024) dominate, with GRPO boosting efficiency by removing the critic model and adopting group-wise reward normalization. Extensions including DAPO Yu et al. (2025), ReMax Li et al. (2024), and REINFORCE++ Hu et al. (2025) further refine this paradigm. Inspired by these advances, we reinterpret contrastive learning as reward signal that guides generative policy, thereby turning LLMs into representation models."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We propose GRACE (Generative Representation Learning via Contrastive Policy Optimization), novel framework that reframes contrastive signals as rewards for generative policy, turning LLMs from opaque encoders into interpretable representation learners. Optimized with standard policy-gradient updates, GRACE directly shapes the reasoning that gives rise to embeddings, yielding representations whose semantics are inspectable through the rationales. Empirically, GRACE delivers consistent, cross-category gains on MTEB across multiple backbones in both supervised and unsupervised settings, while preserving general capabilities on non-embedding tasks."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Claude 3.5 sonnet model card addendum, 6 2024. URL https://www-cdn. anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_ Card_Claude_3_Addendum.pdf. Addendum to the Claude 3 model card. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. Llm2vec: Large language models are secretly powerful text encoders. CoRR, 15 abs/2404.05961, 2024. doi: 10.48550/ARXIV.2404.05961. URL https://doi.org/10. 48550/arXiv.2404.05961. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv. org/abs/2107.03374. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 15971607. PMLR, 2020. URL http://proceedings. mlr.press/v119/chen20j.html. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. URL https://doi.org/10.48550/arXiv.2501.12948. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171 4186. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1423. URL https://doi.org/10.18653/v1/n19-1423. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https: //doi.org/10.48550/arXiv.2407.21783. Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 6894 6910. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.EMNLP-MAIN. 552. URL https://doi.org/10.18653/v1/2021.emnlp-main.552. Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning and long-context multimodality, 2025. URL https://arxiv.org/abs/2507.06261. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ. Jian Hu, Jason Klein Liu, Haotian Xu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. Yifan Ji, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan Li, Zhiyuan Liu, Yu Gu, Ge Yu, and Maosong Sun. Learning more effective representations for dense retrieval through deliberate thinking before search. CoRR, abs/2502.12974, 2025. doi: 10.48550/ARXIV.2502.12974. URL https://doi.org/10.48550/arXiv.2502.12974. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pp. 16011611. Association for Computational Linguistics, 2017. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 67696781. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.EMNLP-MAIN.550. URL https://doi.org/10.18653/v1/2020. emnlp-main.550. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id= lgsyLSsDRe. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. CoRR, abs/2308.03281, 2023. doi: 10.48550/ARXIV.2308.03281. URL https://doi.org/10.48550/arXiv.2308. 03281. Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. Remax: simple, effective, and efficient reinforcement learning method for aligning large language models. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, 17 July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= Stn8hXkpe6. Ziyue Li and Tianyi Zhou. Your mixture-of-experts LLM is secretly an embedding model for free. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id= eFGQ97z5Cd. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7. Niklas Muennighoff, Nouamane Tazi, Loıc Magne, and Nils Reimers. MTEB: massive text In Andreas Vlachos and Isabelle Augenstein (eds.), Proceedings embedding benchmark. of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pp. 20062029. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EACL-MAIN.148. URL https: //doi.org/10.18653/v1/2023.eacl-main.148. Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=BC4lIvfSzv. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 39803990. Association for Computational Linguistics, 2019. doi: 10.18653/V1/D19-1410. URL https://doi.org/10.18653/v1/D19-1410. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/ 1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL https://doi.org/10.48550/arXiv.2402.03300. Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Patel, Jalal Naghiyev, Yann LeCun, and Ravid Shwartz-Ziv. Layer by layer: Uncovering hidden representations in language models. CoRR, abs/2502.02013, 2025. 18 Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan. Repetition improves language model embeddings. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=Ahlrf2HGJR. Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging bigbench tasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 1300313051. Association for Computational Linguistics, 2023. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: largescale dataset for fact extraction and verification. In Marilyn A. Walker, Heng Ji, and Amanda Stent (eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pp. 809819. Association for Computational Linguistics, 2018. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018. URL http://arxiv.org/abs/1807.03748. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 1189711916. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.642. URL https://doi.org/10.18653/v1/2024.acl-long.642. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229256, 1992. xAI. Grok 4 model card, 8 2025. URL https://data.x.ai/ 2025-08-20-grok-4-model-card.pdf. Model card. Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. C-pack: Packed resources for general chinese embeddings. In Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang (eds.), Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, pp. 641649. ACM, 2024. Ruiran Yan, Zheng Liu, and Defu Lian. O1 embedder: Let retrievers think before action. CoRR, abs/2502.07555, 2025. doi: 10.48550/ARXIV.2502.07555. URL https://doi.org/10. 48550/arXiv.2502.07555. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412. 15115. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, 19 Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025. doi: 10.48550/ARXIV.2505.09388. URL https://doi.org/10.48550/arXiv.2505.09388. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. DAPO: an open-source LLM reinforcement learning system at scale. CoRR, abs/2503.14476, 2025. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models. CoRR, abs/2506.05176, 2025. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch FSDP: experiences on scaling fully sharded data parallel. Proc. VLDB Endow., 16(12):38483860, 2023."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 TRAINING ALGORITHM )}N , i=1, Initial policy πθ Algorithm 1 GRACE: Generative Representation Learning via Contrastive Policy Optimization Require: Training data = {(qi, d+ Require: Hyperparameters: rollouts K, batch size B, coefficients λ1, λ2 Ensure: Fine-tuned policy πθ with enhanced representation capabilities 1: for epoch = 1 to Nepochs do 2: 3: 4: 5: 6: )}B // Phase 1: Generate interpretable rationale via policy for = 1 to in parallel do for batch = {(qi, d+ Generate query rationale: yqi =r πθ(P(qi)) πθ(P(d Generate negative rationale: yd Sample positive rationale: {y(k) d+ )) k=1 πθ(P(d+ }K i=1 do , )) 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: end for // Phase 2: Extract semantic representations via masked pooling Encode all rationale: = πθ(P(x) y) for all (x, y) pairs Apply masked mean pooling (Eq. 11): }K k=1 MaskedPool(E) // Phase 3: Compute multi-dimensional contrastive rewards for = 1 to do , {h(k) d+ hqi, hd Identify hard negatives: Hi = {j = : maxl sim(hqi, h(l) d+ for = 1 to do )} (cid:80) K1 R(i,k) R(i,k) R(i) CL = sim(cid:0)hqi, h(k) (cid:1) (cid:80)Mi d+ j=k sim(h(k) consist = 1 d+ hard = 1 total = R(i,k) total = R(i,k) m=1 sim(cid:0)hqi, hd , h(j) ) d+ max 1lK sim(cid:0)hqi, h(l) d+ consist + λ2R(i,k) CL + λ1R(i,k) total /τ j=1 j=i (cid:80)B B1 hard (cid:1) i,m (cid:1) R(i,k) ˆR(i,k) end for end for // Phase 4: Optimize policy via GRPO (cid:80)K Compute group baseline: bi = 1 Compute advantages: A(i,k) = R(i,k) Update policy: θ θ + αθLGRPO(θ) where k=1 R(i,k) total bi total LGRPO = (cid:80) i,k A(i,k) log πθ(y(k) d+ P(d+ )) end for 29: 30: end for 31: return Optimized policy πθ A.2 THEORETICAL PERSPECTIVE ANALYSIS Our framework establishes principled connection between contrastive learning and reinforcement learning, leveraging their shared ability to learn from feedback rather than absolute ground truth labels. This fundamental similarity enables natural integration of their objectives. A.2.1 UNIFIED LEARNING WITHOUT GROUND TRUTH Both contrastive learning and reinforcement learning operate on comparative signals: contrastive learning distinguishes positive from negative samples, while reinforcement learning optimizes based on rewards signals. This parallel allows us to reformulate the contrastive objective as reward signal: LCL = log exp(sim(q, d+)) dD exp(sim(q, d)) (cid:80) = sim(q, d+) (cid:88) dD sim(q, d) (22) This transformation enables policy gradient optimization without requiring explicit labels, only relative preferences encoded in the contrastive structure. A.2.2 CONNECTION TO INFONCE Our framework can be understood as implicitly optimizing generative variant of the InfoNCE objective. Consider the expected reward under our multi-faceted design: Eπθ [R(i,k) total ] = (cid:104) R(i,k) CL + λ1R(i,k) consist + λ2R(i) hard (cid:105) Expanding the contrastive learning component: R(i,k) CL = log p(y(k) d+ p(yd qi, πθ) qi, πθ) With hard negative mining across the batch: R(i) hard = log 1 1 (cid:88) j=i max p(y(l) d+ qi, πθ) (23) (24) (25) Combining these terms, the expected total reward approximates: E[Rtotal] log p(d+q, πθ) p(dq, πθ) (cid:81) j=i maxl p(y(l) d+ qi, πθ) + λ1E[consistency] (26) A.2.3 CONVERGENCE ANALYSIS The optimization landscape of our framework benefits from the variance reduction properties of both contrastive learning and policy gradient optimization. The consistency reward acts as regularizer, bounding the policy updates: θJ(θ) [RCL + λ1Rconsist + λ2Rhard] (27) where is constant dependent on the policy parameterization. The consistency term λ1Rconsist ensures that y(k) ϵ for small ϵ, preventing divergent interpretations and ensuring stable d+ convergence. y(j) d+ This theoretical foundation reveals that our approach maintains the discriminative power of contrastive learning while unleashing the generative capabilities of LLMs, enabling the policy to actively interpret and understand text rather than merely discriminate between samples. A.2.4 ADDITIONAL TRAINING DETAILS We incorporate length regularization to prevent degenerate solutions where the model generates excessively long responses without meaningful content: R(i,k) final = (cid:40) ˆR(i,k) γ total < Lmax or y(k) d+ if y(k) d+ otherwise [1] = EOS (28) where Lmax is the maximum allowed sequence length and γ is penalty coefficient for over-length generations. 22 A.2.5 ADDITIONAL IMPLEMENTATION DETAILS Training uses the AdamW Loshchilov & Hutter (2019) optimizer with fixed learning rate of 1106 and no warmup schedule. The maximum prompt length is set to 1024 tokens and the maximum response length to 2048 tokens, with over-length sequences truncated on the right side. length penalty γ = 1.0 is applied when responses reach the maximum length without emitting an EOS token. For GRPO, we generate = 8 rollouts per positive document using temperature sampling with = 1.0, and compute advantages relative to mean baseline without normalization. The consistency weight λ1 = 0.2, and the hard negative weight λ2 = 0.2. The scaling τ for our training is set to 10. Multi-GPU training is managed with Fully Sharded Data Parallel (FSDP) Zhao et al. (2023), with parameter and optimizer state sharding to alleviate memory constraints. Training-time generation leverages vLLM 2 with tensor-parallel size 2 and 50% GPU memory utilization in eager mode for stability. Instruction tokens are automatically detected and excluded from pooling operations to prevent contamination in representation extraction. At inference time, we adopt the same generation and pooling configurations as training to ensure consistency. 2https://github.com/vllm-project/vllm"
        }
    ],
    "affiliations": [
        "Australian National University",
        "Hong Kong University of Science and Technology",
        "University of Illinois Urbana-Champaign",
        "University of Washington",
        "University of Wisconsin-Madison"
    ]
}