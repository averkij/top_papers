{
    "paper_title": "Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation",
    "authors": [
        "Chongcong Jiang",
        "Tianxingjian Ding",
        "Chuhan Song",
        "Jiachen Tu",
        "Ziyang Yan",
        "Yihua Shao",
        "Zhenyi Wang",
        "Yuzhang Shang",
        "Tianyu Han",
        "Yu Tian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3's model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 1 ] . [ 1 0 8 8 0 1 . 1 0 6 2 : r Medical SAM3: Foundation Model for Universal Prompt-Driven Medical Image Segmentation Chongcong Jiang1*, Tianxingjian Ding1*, Chuhan Song2*, Jiachen Tu3, Ziyang Yan4, Yihua Shao5, Zhenyi Wang1, Yuzhang Shang1, Tianyu Han6, and Yu Tian1(cid:66) 1 University of Central Florida, Orlando, USA 2 University College London, London, UK 3 University of Illinois Urbana-Champaign, Champaign, USA 4 University of Trento, Trento, Italy 5 The Hong Kong Polytechnic University, China 6 University of Pennsylvania, Philadelphia, USA yu.tian2@ucf.edu Abstract. Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3s model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/ Medical-SAM3. Keywords: Medical Image Segmentation Foundation Models FineTuning SAM3 * Co-first authors. Corresponding to Z. Yan and Y. Tian. 2 Jiang et al. Fig. 1. Universal medical image segmentation via text prompting with Medical SAM3. Our proposed model unifies diverse medical imaging modalitiesranging from radiology (CT, MRI, X-Ray) to optical imaging (Fundus, Dermoscopy, Endoscopy) and pathologyinto single framework."
        },
        {
            "title": "Introduction",
            "content": "Medical image segmentation aims to delineate clinically relevant structures and abnormalities in medical images at the pixel or voxel level. By enabling objective quantification of disease extent and anatomical changes, segmentation supports lesion assessment, surgical or radiotherapy planning, and longitudinal Medical SAM3 for Medical Image Segmentation 3 follow-up [35,12,62]. Despite remarkable progress in deep learning, many models remain optimized for specific tasks and data distributions, making adaptation to new modalities, anatomies, pathologies, or clinical sites challenging [16,37,57]. This reliance on expert dense annotation and dataset-specific optimization limits scalability and hinders deployment in long-tail rare conditions and heterogeneous real-world settings, especially under distribution shift. Methodologically, the field has been dominated by fully supervised specialist models trained with dense annotations. Convolutional neural networks (CNNs) and vision transformers (ViTs) have achieved strong performance for medical segmentation [54,46,8,18], and automated pipelines further reduce manual tuning [24]. However, these advances largely remain within dataset-centric paradigm and do not readily generalize across modalities and clinical sites, motivating promptable foundation models that provide more unified and scalable interface for segmentation. Segmentation foundation models offer promising alternative, aiming to generalize across tasks through prompt-based interaction while reducing taskspecific retraining [56]. The Segment Anything Model (SAM) [29] demonstrated remarkable zero-shot generalization in natural images via visual prompts, and subsequent models such as SAM3 [7] extend this paradigm with concept-based prompting. However, critical gap remains. Medical images differ substantially from natural scenes in acquisition protocols and semantic structure, often leading to unstable performance under zero-shot or lightly adapted settings [20,45,23]. More crucially, many previous foundation models achieve competitive results only by relying on ground-truth-derived bounding boxes, essentially utilizing oracle localization cues [41,79,75,58]. While effective for interactive refinement, such privileged geometric priors largely remove the localization challenge and reduce the problem to boundary refinement, which may confound comparisons when geometric priors are not available at deployment. In real deployments, boxes must be provided by clinician or an upstream detector. Without such cues, text-only prompts often degrade sharply under severe domain misalignment[89,40,59], motivating holistic model adaptation for robust, prompt-driven medical segmentation. To address these challenges, we present Medical SAM3, universal promptdriven foundation model for medical image segmentation obtained by holistically adapting SAM3 on large-scale, heterogeneous 2D and 3D medical datasets with paired segmentation masks and text prompts. By moving beyond lightweight adapters and reducing reliance on pre-defined geometric cues (e.g., bounding boxes), Medical SAM3 learns robust domain-specific representations while preserving promptable flexibility under severe domain shift. We further conduct systematic diagnostic study of vanilla SAM3 in medical settings and evaluate Medical SAM3 across both internal validation tasks and external validation tasks spanning diverse organs, modalities, and dimensionalities. Across this suite, Medical SAM3 achieves state-of-the-art performance and supports spatial-promptfree, semantic-driven paradigm for medical image segmentation. In summary, our contributions are threefold: (i) we introduce Medical SAM3 by holistically 4 Jiang et al. adapting SAM3 for universal, text-guided medical segmentation without privileged spatial prompts; (ii) we provide diagnostic study that characterizes the failure modes of vanilla SAM3 under severe domain shift and its reliance on geometric cues; and (iii) we curate large-scale textimagemask aligned medical segmentation corpus and establish strong results through extensive internal and external evaluations across diverse organs, modalities, and 2D/3D settings."
        },
        {
            "title": "2 Related Works",
            "content": "Specialist Medical Image Segmentation. Fully supervised specialist models remain the dominant paradigm in medical image segmentation. Early encoder decoder CNNs and their variants, represented by FCN and U-Net, establish strong inductive biases for dense prediction and are widely extended with attention and redesigned skip connections [73,39,54,49,91,69,70,71]. For volumetric imaging, 3D architectures directly model spatial context in CT and MRI, including 3D U-Net and V-Net [9,46]. Beyond architecture design, automated training pipelines such as nnU-Net substantially reduce manual engineering and provide strong baselines across datasets [24]. Large scale multi-organ segmentation systems further demonstrate that broad anatomical coverage can be achieved when sufficient annotations and standardized pipelines are available [78,60]. More recently, Transformer based designs improve global context modeling for medical segmentation, including hybrid and fully Transformer architectures [8,19,18,83,90,72]. In parallel, selective state space models (SSMs), exemplified by Mamba, have been explored to capture long range dependencies with improved efficiency, inspiring Mamba-based medical segmentation architectures such as U-Mamba, SegMamba, VM-UNet, and Swin-UMamba [15,42,82,55,36]. Text Guided and Open Vocabulary Segmentation. Text guided segmentation in general vision is commonly approached by aligning dense visual features with language representations to enable open vocabulary mask prediction [40,52,84,11,33]. Referring expression segmentation further studies phrase grounded masks through explicit cross modal fusion [61,77,85]. These lines of work provide complementary perspectives on semantic conditioning and prompt design that are relevant to text based target specification in medical segmentation. Promptable Segmentation Foundation Models. Interactive medical segmentation predates recent foundation models and commonly improves an automatic prediction with lightweight user inputs, such as clicks or scribbles, as exemplified by DeepIGeoS [74]. SAM introduces promptable interface via prompt encoder and mask decoder, enabling segmentation conditioned on spatial prompts [29], and SAM 2 extends this design with memory for streaming image and video settings [53]. Medical adaptations of SAM style models have been studied through supervised domain adaptation and parameter efficient customization, including MedSAM and Medical SAM Adapter [41,79]. Extending Medical SAM3 for Medical Image Segmentation 5 Fig. 2. Overview of Medical SAM3. Medical SAM3 takes text prompt and medical images (2D or slice-based 3D) as input. detector segments target instances in the current frame, while an optional tracker propagates masks across frames via memory bank. The final prediction is produced by merging detected and propagated masks, supporting semantic-driven segmentation without privileged spatial prompts. promptable segmentation to volumetric data has been explored from different perspectives, including learning native 3D promptable models such as SAMMed3D and using memory mechanisms for 3D image/video settings such as MedSAM2 [75,43]. In addition, universal medical segmentation has been investigated via prompt driven multi task learning and minimal interaction paradigms, including UniSeg, MedUniSeg, and One-Prompt Segmentation [87,86,80]. Most recently, concept based prompting has been introduced in SAM3 to broaden conditioning beyond purely geometric cues [7], and large vocabulary medical segmentation driven by text prompts has also been explored [89]."
        },
        {
            "title": "3 Method",
            "content": "In this paper, we propose full fine-tuning strategy to adapt SAM3 [7], largescale promptable segmentation foundation model, to medical imaging under severe domain shift. Unlike parameter-efficient or partial fine-tuning approaches, we update all model parameters to enable comprehensive domain adaptation. Crucially, we introduce no architectural modifications to SAM3. Figure 2 illustrates our training pipeline for 2D and 3D modalities, which follows SAM3s detectortracker design for sequential inputs. At frame t, Medical SAM3 combines mask detected by the detector with mask propagated from slice t1 by the tracker, and updates the memory bank for subsequent propagation."
        },
        {
            "title": "3.1 Unified Input Formulation",
            "content": "Medical imaging spans wide spectrum of departments, encompassing natively planar modalities such as Histopathology, Fundus photography, Dermatology, 6 Jiang et al. and Projection Radiography (X-ray). To harmonize these heterogeneous data sources into generalist foundation model, we unify these modalities within common 2D feature space. By treating each medical scan as high-fidelity 2D image, we maximize the models applicability across diverse clinical workflows without being constrained by inconsistent 3D acquisition geometries. This strategy not only simplifies the integration of diverse clinical workflows but also enables the perception backbone to prioritize high-resolution spatial features (at 1008 1008 pixels), which are often compromised in computationally heavy volumetric frameworks. To leverage the 33 diverse datasets during joint training, we structure each sample into text-driven triplet (I, M, t), where is the image, is the corresponding mask, and is the text prompt derived directly from the datasets clinical labels. Unlike traditional segmentation models that require fixed, closed-set label space, our approach exploits the semantic flexibility of the pre-trained text encoder. By associating masks with their native clinical nomenclature, the model learns to associate varied terminology with their corresponding visual features. This strategy avoids the need for complex label re-mapping while allowing the model to internalize vast range of anatomical and pathological descriptors across disparate medical domains."
        },
        {
            "title": "3.2 Stratified Tuning",
            "content": "Medical images contain critical diagnostic details that necessitate high spatial resolution. We maintain training resolution of 1008 1008 pixels to align with the high-frequency spatial priors inherited from the original large-scale pre-training. This ensures that the positional embeddings remain synchronized with the perception backbone. To mitigate the significant domain gap between natural and medical textures without catastrophic forgetting, we employ Layer-wise Learning Rate Decay (LLRD). For base learning rate ηbase, the learning rate ηl for the l-th layer of the vision backbone is defined as: ηl = ηbase γLl (1) where = 12 is the total number of layers and γ = 0.85 is the decay factor. This stratified strategy allows shallow layers to retain general-purpose visual primitives, such as edges and textures, while forcing deeper layers to specialize in complex medical semantics."
        },
        {
            "title": "3.3 Text-Driven Semantic Alignment",
            "content": "In practical clinical environments, the requirement for manual bounding boxes as spatial priors often creates bottleneck, as it assumes the clinician has already identified the targets precise location. To maximize the utility of Medical SAM3 as an autonomous assistant, we transition from prompt-dependent paradigm to strictly text-driven semantic alignment strategy. By utilizing clinical concepts Medical SAM3 for Medical Image Segmentation 7 as the sole input during training, we force the model to develop an intrinsic spatial awareness that bridges abstract medical nomenclature with pixel-level morphological features. This alignment process is formulated as semantic-to-spatial distillation task. Without the crutch of bounding box, the transformer decoder must learn to treat the text embedding ztxt = Etxt(c) not merely as class label, but as discriminative spatial query. Through this pure text-driven supervision, the model is compelled to identify long-range correlations between high-level clinical descriptors (e.g., irregular mass, calcified node) and specialized pathological textures within the vision backbones feature maps. This global-to-local reasoning path ensures that the linguistic manifold and the visual manifold are explicitly aligned. Consequently, at inference time, the model can interpret conceptual keywords and autonomously perform zero-shot localization, effectively simulating clinicians cognitive process of translating diagnostic term into visual search."
        },
        {
            "title": "3.4 Set-Prediction Objective",
            "content": "We optimize the model using multi-task objective that jointly supervises instance discovery and semantic segmentation. Given predicted queries {ˆyi}N i=1 and ground-truth instances {yj}M , we establish one-to-one assignment π j=1 via bipartite Hungarian matching. To address potential sparse supervision in medical scenes, an auxiliary one-to-many (O2M) matcher πo2m is employed to enhance training stability. The total objective is: Ltotal = Lfind(π) + λo2mLfind(πo2m) + Lseg, (2) where all terms are normalized by the batch-wise matched instance count. Finding Loss. For matched queries, Lfind supervises classification, presence, and localization: Lfind(π) = λceLce + λprLpres + 1{j=}(λℓ1Lℓ1 + λgLgiou), (3) where Lce is focal-style classification loss and Lpres supervises query presence. The box regression terms (ℓ1 and GIoU) are computed only for positive assignments (j = ). Segmentation Loss. To ensure precise mask boundariescritical for clinical quantificationthe segmentation loss Lseg combines pixel-wise and structural terms: Lseg = λfLfocal seg + λdLdice + λspLseg-pres, (4) where Ldice improves boundary adherence and Lseg-pres provides semantic presence supervision. 8 Jiang et al."
        },
        {
            "title": "4.1 Datasets",
            "content": "To develop prompt-driven foundation model with strong generalization to medical segmentation tasks, we fine-tune Medical SAM3 on diverse multi-domain collection assembled from publicly accessible datasets, where each sample is paired with segmentation mask and text prompt that is manually curated or derived from dataset labels. As shown in Table 1, the collected corpus encompasses 33 datasets across 10 imaging modalitiesincluding radiography (CXR and X-ray/angiography), ultrasound, endoscopy, pathology, fundus, dermoscopy, microscopy, virtual microscopy, electron microscopy, and othersamounting to total of 76,956 images and 263,705 mask annotations. Radiography is the largest contributor with 40,160 images, dominated by large-scale CXR collections. Ultrasound and endoscopy/fetoscopy form two mid-sized groups with 12,179 and 12,887 images, respectively, while the remaining modalities provide long-tail diversity that improves coverage of appearance, acquisition, and annotation styles. The median annotation area varies by several orders of magnitude, ranging from 58 px in electron microscopy nuclei to over one million pixels in chest radiographs, highlighting substantial scale variation in segmentation targets. For consistency, we standardize all datasets to an 85/15 split for training and validation with fixed seed of 42, yielding approximately 65.4k training images and 11.5k validation images. Our evaluation protocol is designed to rigorously test robustness under domain shift, we evaluate Medical SAM3 on 10 internal validation tasks derived from held-out splits of the fine-tuning corpus. Complementing this, we conduct external validation on 7 segmentation tasks that were entirely excluded from the model development pipeline. Unifying these diverse benchmarks within prompt-driven framework requires structuring each sample as an (image, mask, text) triplet. While our architecture natively supports prompts of varying granularity, spanning from broad categories to detailed descriptive attributes, we prioritize atomic clinical concepts in this study to establish consistent baseline. We define dataset-specific label taxonomy and map it to unified vocabulary of canonical concept names. Single-class datasets are assigned single global concept; Multi-class datasets use one to one mapping from label indices to anatomy or pathology terms defined by the dataset specification. This label-to-text dictionary ensures that each segmentation mask is paired with consistent and standardized prompt across datasets."
        },
        {
            "title": "4.2 Experimental Settings",
            "content": "For both training and evaluation, we use text-only prompts for all internal and external tasks. Prompts are instantiated by applying the label-to-text mapping protocol in Sec. 4.1, resulting in single canonical concept term per class for both Medical SAM3 for Medical Image Segmentation 9 training and evaluation. We compare the results of the original SAM3 [7] and our Medical SAM3. The original SAM3 is evaluated using the official checkpoint without any additional training on medical data. Medical SAM3 is initialized from the same SAM3 checkpoint and obtained by full parameter fine tuning on the training splits described in Sec. 4.1. All training and testing are implemented in PyTorch with distributed data parallelism using the NCCL backend. Experiments are conducted on one node with four NVIDIA H100 GPUs with 80GB memory. We train for up to 10 epochs and select the final checkpoint based on internal validation performance. We optimize with AdamW using β1 = 0.9 and β2 = 0.999. We use group-wise learning rates of 3 104 for the decoder, segmentation head, and dot-product scoring, 5 105 for the vision backbone, 5 105 for the language backbone, and 1 104 for the geometry prompt encoder. The learning rate schedule uses linear warmup followed by an inverse-square-root decay. Training uses only text prompts paired with ground-truth segmentation masks, without any spatial or interactive prompts such as points or bounding boxes. Model selection is based on performance on the internal validation set. We follow the set-prediction objective in Sec. 3.4 for instance discovery and mask prediction. We use focal Hungarian matching with an auxiliary one-to-many branch to improve assignment stability under sparse supervision and severe foregroundbackground imbalance. All matching and loss hyperparameters are summarized in Table 2. During evaluation, we select the highest-confidence mask generated from the text prompt. For multi-class scenarios, we query each class independently and resolve overlaps via pixel-wise maximal confidence, yielding single nonoverlapping semantic map. This strategy ensures consistent predictions suited for text-only deployment. We report Dice coefficient and Intersection-over-Union (IoU) as primary metrics. Table 2. Matching and loss hyperparameters used in the set-prediction objective."
        },
        {
            "title": "Param",
            "content": "Val. Param O2O matcher BinaryHungarianMatcherV2 O2M matcher BinaryOneToManyMatcher"
        },
        {
            "title": "Lseg",
            "content": "wcls wgiou γmatch top-k αo2m λce αcls pos. weight λℓ1 αseg λf λsp 2.0 2.0 2 4 0. 20.0 0.25 10 5.0 0.6 20.0 1.0 wbox αmatch stable threshold λo2m λpr γcls padded Nq λg γseg λd Val. 5.0 0.25 false 0.4 2.0 20.0 2 200 2.0 2.0 30.0 Jiang et al."
        },
        {
            "title": "5 Results",
            "content": "Fig. 3. Radar chart overview of segmentation performance. Results are split by internal validation (top) and external generalization (bottom), reporting Dice (left) and IoU (right) scores. The red area (Medical SAM3) significantly covers the blue area (SAM3) in all scenarios, aligning with the metrics in Table 3. Internal validation on held-out splits. Table 3 (top) reports results on 10 internal held-out splits. Medical SAM3 improves over the original SAM3 on all tasks, increasing the average Dice from 54.0% to 77.0% and the average IoU from 43.3% to 67.3%. These gains highlight that full-parameter fine-tuning strengthens medical domain visual priors and improves text-to-mask alignment, enabling reliable localization even when only class name is provided. The improvements are most pronounced for small, thin, or low-contrast targets where text-only prompting is particularly challenging. For retinal vessel segmentation, performance increases substantially on DRIVE from 24.8% to 55.8% Dice and on COph100 from 34.1% to 63.1% Dice, indicating better boundary adherence for fine vascular structures. We also observe strong gains on modality-specific targets with large appearance shifts, including fetal head segmentation on PS-FHAOP23 from 65.7% to 91.6 Dice and placental vessel segmentation on FetoPlac from 56.6% to 77.0% Dice. Overall, the consistent gains across all internal heldout splits indicate that Medical SAM3 achieves strong in-domain adaptation under text-only prompting setting. 11 Table 3. Quantitative comparison on internal (10) and external (7) testing datasets. We report Dice and IoU (%). Medical SAM3 for Medical Image Segmentation Dataset Dice (%) IoU (%) SAM3 Ours SAM3 Ours Internal datasets (10) 65.7 PS-FH-AOP23 24.8 DRIVE 34.1 COph100 16.3 Breast Cancer 62.0 Intraretinal Fluid 67.7 M2CAI 56.6 FetoPlac 68.9 GlaS15 57.3 SegThy 86.2 PAPILA 91.6 +25.9 55.8 +31.0 63.1 +29.0 43.8 +27.5 85.0 +23.1 88.1 +20.4 77.0 +20.5 88.2 +19.4 78.5 +21.2 99.4 +13.1 50.3 14.2 22.1 11.6 50.4 54.5 42.9 59.8 48.4 78.7 84.8 +34.5 39.2 +25.0 46.6 +24.6 35.7 +24.0 75.2 +24.8 81.5 +27.0 64.3 +21.4 80.7 +21.0 66.2 +17.8 98.7 +20.1 Avg. (Internal) 54.0 77.0 +23. 43.3 67.3 +24.0 External datasets (7) 4.2 TN3K 23.9 HC18 0.0 CVC 0.0 ETIS 18.4 PH2 17.9 CHASE 18.6 STARE 40.8 +36.6 92.6 +68.7 87.9 +87.9 86.1 +86.1 92.7 +74.3 62.6 +44.7 54.4 +35.8 3.4 17.3 0.0 0.0 14.9 9.8 10.3 32.7 +29.3 86.9 +69.6 81.2 +81.2 79.3 +79.3 87.5 +72.6 45.7 +35.9 37.8 +27. Avg. (External) 11.9 73.9 +62.0 8.0 64.4 +56.4 In digital pathology, Medical SAM3 markedly improves breast cancer tissue segmentation from 16.3% to 43.8% Dice and gland segmentation on GlaS15 from 68.9% to 88.2% Dice, showing robust adaptation to stain and texture variations under the same protocol. On high-contrast targets where the baseline is already strong, Medical SAM3 maintains or further boosts accuracy, with PAPILA reaching 99.4% Dice and 98.7% IoU. External validation under domain shift. To assess zero-shot generalization, we evaluate Medical SAM3 on seven external datasets that are excluded from training, spanning ultrasound, endoscopy, and fundus photography: TN3K, HC18, CHASE_DB1, STARE, CVC-Clinic, ETIS-Larib, and PH2. Table 3 (bottom) shows consistent improvements over the original SAM3 across all tasks, with average Dice increasing from 11.9% to 73.9% and average IoU rising from 8.0% to 64.4%. The most striking recovery occurs in endoscopic polyp segmentation (CVC and ETIS), where the baseline SAM3 suffers catastrophic failure due to weak text-visual alignment; in contrast, Medical SAM3 successfully grounds the target, achieving 87.9% and 86.1% Dice, respectively. Similarly, in ultrasound (HC18) and dermatology (PH2) tasks, the model overcomes domain gaps 12 Jiang et al. to boost performance by over 68%, proving its capability to reliably localize anatomical structures in unseen domains without additional adaptation. Fig. 4. Visualization of the segmentation performance of SAM3 and Medical SAM3 Qualitative Results Figure 4 provides representative visual comparisons between SAM3 and Medical SAM3 under the same text-only prompting protocol. Across diverse modalities, the original SAM3 frequently fails to localize the target anatomy, producing either near-empty masks or severe over-segmentation that collapses to large foreground regions. This behavior is particularly evident for thin and low-contrast structures such as retinal vessels, where SAM3 outputs noisy masks with widespread false positives, while Medical SAM3 recovers fine Medical SAM3 for Medical Image Segmentation 13 vascular branches with substantially cleaner boundaries. Similar improvements are observed on endoscopic targets, where SAM3 tends to miss small regions or yields fragmented predictions, whereas Medical SAM3 produces coherent masks that better match the ground truth. On dermoscopy, Medical SAM3 also delineates lesion extent more accurately and avoids the spurious background activations seen in SAM3."
        },
        {
            "title": "6 Discussion and Conclusion",
            "content": "Our study reveals that the main bottleneck for universal medical segmentation with promptable foundation models is not the availability of prompt interface, but the reliability of semantic grounding under domain shift. While strong geometric cues in medical imaging can often simplify segmentation into boundary refinement task, relying solely on text prompts exposes the more critical challenge: mapping clinical concepts to spatially precise masks across heterogeneous appearances. The consistent performance gains of Medical SAM3 indicate that robust text grounding is attainable when adaptation is treated as holistic representation problem rather than merely prompt-engineering problem. In particular, the improvements observed across diverse modalities point to shared latent structure that can be learned when the model is forced to align high-level language concepts with localization-relevant visual features. This perspective also helps explain why failures are most visible on small, thin, or low-contrast targets: such cases demand stronger coupling between semantics and spatial evidence, and are less forgiving to misalignment. These findings have substantial implications for both evaluation and deployment. Benchmarking should explicitly distinguish interactive settings (where users or upstream detectors provide spatial hints) from deployment-consistent semantic-only settings; otherwise, comparisons may be confounded by privileged localization priors. From systems standpoint, text-driven interface is attractive precisely because it offers unified way to query segmentation targets across departments and modalities. However, realizing this promise requires standardized prompt protocols and careful handling of terminology and label granularity, since clinical language is inherently variable. Despite these advancements, several limitations persist. First, full adaptation at high resolution can be computationally demanding, motivating future work on parameter-efficient strategies and distillation without sacrificing robustness. Second, while planar representation improves universality across inconsistent acquisition geometries, it may underutilize native volumetric continuity; native 3D prompting and explicit inter-slice consistency constraints are promising directions. Third, our current evaluation prioritizes atomic concept prompts; extending to synonym-robust, attribute-rich, and compositional prompts will be important for real clinical usage. Finally, broader multi-center validation and reliability analyses, such as uncertainty estimation, are necessary to quantify deployment readiness. 14 Jiang et al. Overall, Medical SAM3 supports semantic-driven paradigm for universal medical segmentation and highlights that robust promptability in medicine is primarily an alignment and adaptation challenge. Future progress will likely come from combining scalable multi-domain training, richer clinical language handling, and efficiency-oriented adaptation to enable practical and trustworthy deployment."
        },
        {
            "title": "References",
            "content": "1. ACOUSLIC Consortium: ACOUSLIC-AI: Abdominal circumference ultrasound image dataset. Grand Challenge (2024) 2. Al-Dhabyani, W., Gomaa, M., Khaled, H., Fahmy, A.: Dataset of breast ultrasound images. Data in Brief 28, 104863 (2020) 3. Ali, S., et al.: PolypGen: multi-center polyp detection and segmentation dataset for generalisability assessment. Scientific Data 10(1), 75 (2023) 4. Araújo, T., Aresta, G., Castro, E., Rouco, J., Aguiar, P., Eloy, C., Polónia, A., Campilho, A.: Classification of breast cancer histology images using convolutional neural networks. PloS one 12(6), e0177544 (2017) 5. Bano, S., Casella, A., Vasconcelos, F., Qayyum, A., et al.: FetReg: Fetoscopic placental vessel segmentation and registration. Medical Image Analysis 76, 102330 (2022) 6. Candemir, S., Jaeger, S., Palaniappan, K., Musco, J.P., Singh, R.K., Xue, Z., Karargyris, A., Antani, S., Thoma, G., McDonald, C.J.: Lung segmentation in chest radiographs using anatomical atlases with nonrigid registration. IEEE Transactions on Medical Imaging 33(2), 577590 (2014) 7. Carion, N., et al.: Sam 3: Segment anything with concepts. arXiv preprint arXiv:2511.16719 (2025) 8. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou, Y.: Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306 (2021) 9. Çiçek, Ö., Abdulkadir, A., Lienkamp, S., Brox, T., Ronneberger, O.: 3d u-net: learning dense volumetric segmentation from sparse annotation. In: MICCAI. pp. 424432. Springer (2016) 10. Codella, N., Rotemberg, V., Tschandl, P., Celebi, M.E., Dusza, S., Gutman, D., Helba, B., Kalloo, A., Liopyris, K., Marchetti, M., et al.: Skin lesion analysis toward melanoma detection 2018: challenge hosted by the International Skin Imaging Collaboration (ISIC). arXiv preprint arXiv:1902.03368 (2019) 11. Ding, Z., Wang, J., Tu, Z.: Maskclip: Mask transformer for open-vocabulary universal image segmentation. In: CVPR (2023) 12. Esteva, A., Kuprel, B., Novoa, R.A., Ko, J., Swetter, S.M., Blau, H.M., Thrun, S.: Dermatologist-level classification of skin cancer with deep neural networks. Nature 542(7639), 115118 (2017) 13. Gamper, J., Koohbanani, N.A., Benet, K., Khuram, A., Rajpoot, N.: PanNuke: An open pan-cancer histology dataset for nuclei instance segmentation and classification. European Congress on Digital Pathology pp. 1119 (2019) 14. Gómez-Flores, W., Cervantes-Sánchez, F., Escalante-Ramírez, B.: BUS-UCLM: breast ultrasound dataset for lesion detection and classification. Pattern Recognition Letters 155, 3340 (2022) Medical SAM3 for Medical Image Segmentation 15 15. Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023) 16. Guan, H., Liu, M.: Domain adaptation for medical image analysis: survey. IEEE Transactions on Biomedical Engineering 69(3), 11731185 (2021) 17. Hatamizadeh, A., Hosseini, H., Patel, N., Choi, J., Pole, C.C., Hoeferlin, C.M., Schwartz, S.D., Terzopoulos, D.: RAVIR: dataset and methodology for the semantic segmentation of retinal arteries and veins in infrared reflectance imaging. IEEE Journal of Biomedical and Health Informatics 26(7), 32723283 (2022) 18. Hatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H.R., Xu, D.: Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images. In: MICCAI. Springer (2022) 19. Hatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A., Landman, B., Roth, H.R., Xu, D.: Unetr: Transformers for 3d medical image segmentation. In: WACV. pp. 574584. IEEE (2022) 20. He, S., Bao, R., Grant, P.E., Ou, Y.: Accuracy of segment-anything model (sam) in medical image segmentation: comprehensive evaluation. International Journal of Computer Assisted Radiology and Surgery 19(1), 3146 (2024) 21. van den Heuvel, T.L.A., de Bruijn, D., de Korte, C.L., van Ginneken, B.: Automated measurement of fetal head circumference using 2d ultrasound images. PloS One 13(8), e0200412 (2018) 22. Hong, W.c., et al.: CholecSeg8k: semantic segmentation dataset for laparoscopic cholecystectomy based on CholecT50. In: arXiv preprint arXiv:2012.12453 (2020) 23. Huang, Y., Yang, X., Liu, L., Zhou, H., Chang, A., Zhou, X., Chen, R., Yu, J., Chen, J., Chen, C., et al.: Segment anything model for medical images? Medical Image Analysis 92, 103061 (2024) 24. Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: nnu-net: self-configuring method for deep learning-based biomedical image segmentation. Nature Methods 18(2), 203211 (2021) 25. Jha, D., Smedsrud, P.H., Riegler, M.A., Halvorsen, P., de Lange, T., Johansen, D., Johansen, H.D.: Kvasir-SEG: segmented polyp dataset. In: International Conference on Multimedia Modeling. pp. 451462. Springer (2020) 26. Kaggle: Ultrasound nerve segmentation challenge. https://www.kaggle.com/c/ ultrasound-nerve-segmentation (2016), brachial plexus segmentation in ultrasound images 27. Kainz, P., Urschler, M., Schulter, S., Wohlhart, P., Lepetit, V.: You should use regression to detect cells. Medical Image Computing and Computer-Assisted Intervention (MICCAI) pp. 276283 (2015) 28. Kashani, A.H., et al.: Automated intraretinal cystoid fluid segmentation using optical coherence tomography images and deep learning. Translational Vision Science & Technology 10(14), 23 (2021) 29. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.: Segment anything. In: ICCV. pp. 40154026 (2023) 30. Kovalyk, O., Morales-Sánchez, J., Verdú-Monedero, R., Sellés-Navarro, I., PalazónCabanes, A., Sancho-Gómez, J.L.: PAPILA: Dataset with fundus images and clinical data of both eyes of the same patient for glaucoma assessment. Scientific Data 9(1), 291 (2022) 31. Kumar, N., Verma, R., Sharma, S., Bhargava, S., Vahadane, A., Sethi, A.: dataset and technique for generalized nuclear segmentation for computational pathology. IEEE Transactions on Medical Imaging 36(7), 15501560 (2017) 16 Jiang et al. 32. Levinshtein, A., et al.: UroCell: dataset for electron microscopy segmentation of urinary bladder cells. Data in Brief 30, 105522 (2020) 33. Liang, F., Wu, B., Dai, X., Li, K., Zhao, Y., Zhang, H., Zhang, P., Vajda, P., Marculescu, D.: Open-vocabulary semantic segmentation with mask-adapted clip. In: CVPR (2023) 34. Lin, Z., Wei, D., Liao, J., Xu, X., Bhagat, S., et al.: NucMM-Z: dataset for nuclear segmentation in zebrafish brain using electron microscopy. MICCAI (2021) 35. Litjens, G., Kooi, T., Bejnordi, B.E., Setio, A.A.A., Ciompi, F., Ghafoorian, M., Van Der Laak, J.A., Van Ginneken, B., Sánchez, C.I.: survey on deep learning in medical image analysis. Medical Image Analysis 42, 6088 (2017) 36. Liu, J., Yang, H., Caverly, H.Y., et al.: Swin-umamba: Mamba-based unet with imagenet-based pretraining. In: MICCAI (2024) 37. Liu, Q., Chen, C., Qin, J., Dou, Q., Heng, P.A.: Feddg: Federated domain generalization on medical image segmentation via episodic learning in continuous frequency space. In: CVPR. pp. 10131023 (2021) 38. Liu, Y., Tian, Y., Wang, C., Chen, Y., Liu, F., Belagiannis, V., Carneiro, G.: Translation consistent semi-supervised segmentation for 3d medical images. IEEE Transactions on Medical Imaging (2024) 39. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: CVPR. pp. 34313440 (2015) 40. Lüddecke, T., Ecker, A.: Image segmentation using text and image prompts. In: CVPR. pp. 70867096 (2022) 41. Ma, J., He, Y., Li, F., Han, L., You, C., Wang, B.: Medsam: Segment anything in medical images. Nature Communications 15, 654 (2024) 42. Ma, J., Li, F., Wang, B.: U-mamba: Enhancing long-range dependency for biomedical image segmentation. In: MICCAI (2024) 43. Ma, J., Zhu, Y., Wang, B.: Medical sam 2: Segment medical images as video via segment anything model 2. arXiv preprint arXiv:2408.00874 (2025), updated version of MedSAM-2 44. Maier-Hein, L., et al.: Can masses of non-experts train highly accurate image classifiers? crowdsourcing approach to instrument segmentation in laparoscopic images. Medical Image Computing and Computer-Assisted Intervention (MICCAI) (2014) 45. Mazurowski, M.A., Dong, H., Gu, H., Yang, J., Konz, N., Zhang, Y.: Segment anything model for medical image analysis: an experimental study. Medical Image Analysis 89, 102918 (2023) 46. Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks for volumetric medical image segmentation. In: 3DV. pp. 565571. IEEE (2016) 47. Moreira, I.C., Amaral, I., Domingues, I., Cardoso, A., Cardoso, M.J., Cardoso, J.S.: INbreast: Toward full-field digital mammographic database. Academic Radiology 19(2), 236248 (2012) 48. Ngoc Lan, P., et al.: BKAI-IGH NeoPolyp: colonoscopy dataset for colorectal polyp detection and segmentation. IEEE Access 9, 163026163039 (2021) 49. Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich, M., Misawa, K., Mori, K., McDonagh, S., Hammerla, N.Y., Kainz, B., et al.: Attention u-net: Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999 (2018) 50. PCMMD Consortium: PCMMD: Plasma cell multiple myeloma dataset for cell detection and segmentation. Scientific Data (2025), microscopy dataset for plasma cell segmentation Medical SAM3 for Medical Image Segmentation 17 51. Popescu, D., Diaconu, A.M., Deac, A., Stanciu, O., Dogaru, R., Bacila, C.: ARCADE: Automatic region-based coronary artery disease diagnostics using x-ray angiography images. Medical Image Analysis 83, 102636 (2023) 52. Rao, Y., Zhao, W., Liu, G., Lu, J., Zhou, J.: Denseclip: Language-guided dense prediction with context-aware prompting. In: CVPR. pp. 1808218091 (2022) 53. Ravi, N., Gabeur, V., Hu, Y.T., Hu, R., Ryali, C., Ma, T., Khedr, H., Rädle, R., Rolland, C., Gustafson, L., et al.: Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 (2024) 54. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: MICCAI. pp. 234241. Springer (2015) 55. Ruan, J., Xiang, S.: Vm-unet: Vision mamba unet for medical image segmentation. arXiv preprint arXiv:2402.02491 (2024) 56. Shao, Y., He, H., Li, S., Chen, S., Long, X., Zeng, F., Fan, Y., Zhang, M., Yan, Z., Ma, A., et al.: Eventvad: Training-free event-aware video anomaly detection. In: Proceedings of the 33rd ACM International Conference on Multimedia. pp. 25862595 (2025) 57. Shao, Y., Liang, S., Ling, Z., Yan, M., Liu, H., Chen, S., Yan, Z., Zhang, C., Qin, H., Magno, M., et al.: Gwq: Gradient-aware weight quantization for large language models. arXiv preprint arXiv:2411.00850 (2024) 58. Shao, Y., Lin, D., Zeng, F., Yan, M., Zhang, M., Chen, S., Fan, Y., Yan, Z., Wang, H., Guo, J., et al.: Tr-dq: Time-rotation diffusion quantization. arXiv preprint arXiv:2503.06564 (2025) 59. Shao, Y., Lin, X., Long, X., Chen, S., Yan, M., Liu, Y., Yan, Z., Ma, A., Tang, H., Guo, J.: Icm-fusion: In-context meta-optimized lora fusion for multi-task adaptation. arXiv preprint arXiv:2508.04153 (2025) 60. Shao, Y., Xu, Y., Long, X., Chen, S., Yan, Z., Liu, H., Wang, Y., Tang, H., Yang, Y.: Accidentblip: Agent of accident warning based on ma-former. In: 2025 IEEE Intelligent Vehicles Symposium (IV). pp. 21562161. IEEE (2025) 61. Shao, Y., Yan, M., Liu, Y., Chen, S., Chen, W., Long, X., Yan, Z., Li, L., Zhang, C., Sebe, N., et al.: In-context meta lora generation. arXiv preprint arXiv:2501.17635 (2025) 62. Shen, D., Wu, G., Suk, H.I.: Deep learning in medical image analysis. Annual Review of Biomedical Engineering 19, 221248 (2017) 63. Sirinukunwattana, K., Pluim, J.P., Chen, H., Qi, X., Heng, P.A., Guo, Y.B., Wang, L.Y., Matuszewski, B.J., Brber, E., et al.: Gland segmentation in colon histology images: The GlaS challenge contest. Medical Image Analysis 35, 489502 (2017) 64. Sirinukunwattana, K., Raza, S.E.A., Tsang, Y.W., Snead, D.R., Cree, I.A., Rajpoot, N.M.: Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images. IEEE Transactions on Medical Imaging 35(5), 11961206 (2016) 65. Society for Imaging Informatics in Medicine: SIIM-ACR pneumothorax segmentation challenge. In: Kaggle Competition (2019) 66. Song, S., et al.: CT2US: Cross-modal supervision for abdominal organ segmentation. Medical Image Analysis 82, 102603 (2022) 67. Staal, J., Abramoff, M.D., Niemeijer, M., Viergever, M.A., Van Ginneken, B.: Ridge-based vessel segmentation in color images of the retina. IEEE Transactions on Medical Imaging 23(4), 501509 (2004) 68. Tahir, A.M., Chowdhury, M.E.H., Khandakar, A., Rahman, T., Qiblawey, Y., Khurshid, U., Kiranyaz, S., Ibtehaz, N., Rahman, M.S., Al-Maadeed, S., et al.: COVID-QU-Ex dataset: large-scale collection of covid-19, non-covid, and lung opacity chest x-ray images. Informatics in Medicine Unlocked 30, 100893 (2022) Jiang et al. 69. Tian, Y., Liu, F., Pang, G., Chen, Y., Liu, Y., Verjans, J.W., Singh, R., Carneiro, G.: Self-supervised pseudo multi-class pre-training for unsupervised anomaly detection and segmentation in medical images. Medical image analysis 90, 102930 (2023) 70. Tian, Y., Pang, G., Liu, F., Chen, Y., Shin, S.H., Verjans, J.W., Singh, R., Carneiro, G.: Constrained contrastive distribution learning for unsupervised anomaly detection and localisation in medical images. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 128140. Springer (2021) 71. Tian, Y., Pang, G., Liu, Y., Wang, C., Chen, Y., Liu, F., Singh, R., Verjans, J.W., Wang, M., Carneiro, G.: Unsupervised anomaly detection in medical images with memory-augmented multi-level cross-attentional masked autoencoder. In: International workshop on machine learning in medical imaging. pp. 1121. Springer (2023) 72. Tian, Y., Shi, M., Luo, Y., Kouhana, A., Elze, T., Wang, M.: Fairseg: large-scale medical image segmentation dataset for fairness learning using segment anything model with fair error-bound scaling. arXiv preprint arXiv:2311.02189 (2023) 73. Tian, Y., Wen, C., Shi, M., Afzal, M.M., Huang, H., Khan, M.O., Luo, Y., Fang, Y., Wang, M.: Fairdomain: Achieving fairness in cross-domain medical image segmentation and classification. In: European Conference on Computer Vision. pp. 251271. Springer (2024) 74. Wang, G., Zuluaga, M.A., Li, W., Pratt, R., Patel, P.A., Aertsen, M., Doel, T., David, A.L., Deprest, J., Ourselin, S., Vercauteren, T.: Deepigeos: deep interactive geodesic framework for medical image segmentation. IEEE TPAMI 41(7), 15591572 (2018) 75. Wang, H., Guo, S., Ye, J., Deng, Z., Ren, Y., Li, Y., Wan, X.: Sam-med3d. arXiv preprint arXiv:2310.15161 (2023) 76. Wang, X., et al.: BTXRD: Bone tumor x-ray dataset for detection and segmentation. Scientific Data (2025), bone tumor X-ray dataset 77. Wang, Z., Lu, Y., Li, Q., Tao, X., Guo, Y., Gong, M., Liu, T.: Cris: Clip-driven referring image segmentation. In: CVPR. pp. 1168611695 (2022) 78. Wasserthal, J., Breit, H.C., Meyer, M.T., Pradella, M., Hinck, D., Sauter, A.W., Heye, T., Boll, D.T., Cyriac, J., Yang, S., et al.: Totalsegmentator: Robust segmentation of 104 anatomic structures in ct images. Radiology: Artificial Intelligence 5(5) (2023) 79. Wu, J., Fu, R., Fang, H., Liu, Y., Wang, Z., Xu, Y., Jin, Y., Arbel, T.: Medical sam adapter: Adapting segment anything model for medical image segmentation. Medical Image Analysis 102, 103547 (2025) 80. Wu, J., Min, X.: One-prompt to segment all medical images. In: CVPR (2024) 81. Wunderling, T., Golla, B., Poudel, P., Taber, C., Gockel, M., Modersitzki, J.: SegThy: novel dataset for thyroid segmentation in ultrasound images. International Journal of Computer Assisted Radiology and Surgery 12(8), 14051414 (2017) 82. Xing, Z., Ye, T., Yang, Y., Liu, G., Zhu, L.: Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation. In: MICCAI (2024) 83. Yan, Z., Dong, W., Shao, Y., Lu, Y., Liu, H., Liu, J., Wang, H., Wang, Z., Wang, Y., Remondino, F., et al.: Renderworld: World model with self-supervised 3d label. In: 2025 IEEE International Conference on Robotics and Automation (ICRA). pp. 60636070. IEEE (2025) Medical SAM3 for Medical Image Segmentation 19 84. Yan, Z., Li, L., Shao, Y., Chen, S., Wu, Z., Hwang, J.N., Zhao, H., Remondino, F.: 3dsceneeditor: Controllable 3d scene editing with gaussian splatting. arXiv preprint arXiv:2412.01583 (2024) 85. Yang, Z., Wang, J., Tang, Y., Chen, K., Zhao, H., Torr, P.H.: Lavt: Language-aware vision transformer for referring image segmentation. In: CVPR. pp. 1815518165 (2022) 86. Ye, Y., Chen, Z., Zhang, J., Xie, Y., Xia, Y.: Meduniseg: 2d and 3d medimage segmentation via prompt-driven universal model. arXiv preprint ical arXiv:2410.05905 (2024) 87. Ye, Y., Xie, Y., Zhang, J., Chen, Z., Xia, Y.: Uniseg: prompt-driven universal segmentation model as well as strong representation learner. In: MICCAI. pp. 508518. Springer (2023) 88. Zhang, X., et al.: COph100: comprehensive ophthalmic fundus image dataset for deep learning. Scientific Data (2023), fundus image dataset for retinal vessel segmentation 89. Zhao, Z., et al.: One model to rule them all: Towards universal segmentation for medical images with text prompts. arXiv preprint arXiv:2312.17183 (2025), also referred to as SAT 90. Zhou, H.Y., Guo, J., Zhang, Y., Yu, L., Wang, L., Yu, Y.: nnformer: Interleaved transformer for volumetric segmentation. arXiv preprint arXiv:2109.03201 (2021) 91. Zhou, Z., Rahman Siddiquee, M.M., Tajbakhsh, N., Liang, J.: Unet++: nested u-net architecture for medical image segmentation. In: MICCAI. pp. 311. Springer (2018) 20 Jiang et al. Table 1. Summary of datasets used for training. The table reports the number of images and annotations across distinct medical modalities, and the median annotation area represents the typical scale of the segmentation targets. Dataset Images Anns Median Area (px) CXR (3 datasets) COVID-QU-Ex Chest Xray Masks and Labels Chest X-Ray Pneumothorax 33,920 704 67,839 1,415 370 7,542 1,022,508 7,183 X-ray (2 datasets) BTXRD ARCADE Ultrasound (7 datasets) BUSI BUS-UCLM US-Nerve ACOUSLIC ps-fh-aop-2023 CT2USforKidneySeg SegThy Endoscopy (6 datasets) CholecSeg8k m2caiSeg PolypGen BKAI-IGH NeoPolyp Kvasir-SEG Pathology (4 datasets) PanNuke MonuSeg Breast Cancer Segmentation GlaS@MICCAI 3,746 1,500 647 264 2,323 300 4,000 4,586 59 8,080 614 1,710 1,000 1,000 2,540 82 151 165 Virtual Microscope (2 datasets) MUCIC Colon Tissue MUCIC HL60 Granulocytes 60 Electron Microscopy (2 datasets) NucMM-Z UroCell 62 5 2,273 2,316 647 281 2,323 300 3,999 4,601 130 112,521 804 2,003 1,117 1,060 21,978 2,887 3,495 1, 12,396 3,987 581 163 3,517 3,519 Microscopy (1 dataset) PCMMD Fundus/OCT (5 datasets) Intraretinal Cystoid Fluid PAPILA COph100 RAVIR Dataset DRIVE 1,459 488 324 23 20 Dermoscopy & Others (2 datasets) ISIC_2018 FetoPlac 2,594 483 4,601 488 324 141 20 2,594 994 Total 76,956 263,705 14,746 1,472 17,348 24,102 6,954 58,330 8,048 11,807 2,118 749 102,493 70,698 39,138 34,489 811 172 15,524 10, 1,688 4,857 58 241 26,227 202 172,486 7,168 3,638 28,176 429,033 6,"
        }
    ],
    "affiliations": [
        "The Hong Kong Polytechnic University",
        "University College London",
        "University of Central Florida",
        "University of Illinois Urbana-Champaign",
        "University of Pennsylvania",
        "University of Trento"
    ]
}