{
    "paper_title": "SimpleFold: Folding Proteins is Simpler than You Think",
    "authors": [
        "Yuyang Wang",
        "Jiarui Lu",
        "Navdeep Jaitly",
        "Josh Susskind",
        "Miguel Angel Bautista"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Protein folding models have achieved groundbreaking results typically via a combination of integrating domain knowledge into the architectural blocks and training pipelines. Nonetheless, given the success of generative models across different but related problems, it is natural to question whether these architectural designs are a necessary condition to build performant models. In this paper, we introduce SimpleFold, the first flow-matching based protein folding model that solely uses general purpose transformer blocks. Protein folding models typically employ computationally expensive modules involving triangular updates, explicit pair representations or multiple training objectives curated for this specific domain. Instead, SimpleFold employs standard transformer blocks with adaptive layers and is trained via a generative flow-matching objective with an additional structural term. We scale SimpleFold to 3B parameters and train it on approximately 9M distilled protein structures together with experimental PDB data. On standard folding benchmarks, SimpleFold-3B achieves competitive performance compared to state-of-the-art baselines, in addition SimpleFold demonstrates strong performance in ensemble prediction which is typically difficult for models trained via deterministic reconstruction objectives. Due to its general-purpose architecture, SimpleFold shows efficiency in deployment and inference on consumer-level hardware. SimpleFold challenges the reliance on complex domain-specific architectures designs in protein folding, opening up an alternative design space for future progress."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 0 8 4 8 1 . 9 0 5 2 : r SimpleFold: Folding Proteins is Simpler than You Think Yuyang Wang, Jiarui Lu, Navdeep Jaitly, Josh Susskind, Miguel Angel Bautista Apple Protein folding models have achieved groundbreaking results typically via combination of integrating domain knowledge into the architectural blocks and training pipelines. Nonetheless, given the success of generative models across different but related problems, it is natural to question whether these architectural designs are necessary condition to build performant models. In this paper, we introduce SimpleFold, the first flow-matching based protein folding model that solely uses general purpose transformer blocks. Protein folding models typically employ computationally expensive modules involving triangular updates, explicit pair representations or multiple training objectives curated for this specific domain. Instead, SimpleFold employs standard transformer blocks with adaptive layers and is trained via generative flow-matching objective with an additional structural term. We scale SimpleFold to 3B parameters and train it on approximately 9M distilled protein structures together with experimental PDB data. On standard folding benchmarks, SimpleFold-3B achieves competitive performance compared to state-of-the-art baselines, in addition SimpleFold demonstrates strong performance in ensemble prediction which is typically difficult for models trained via deterministic reconstruction objectives. Due to its general-purpose architecture, SimpleFold shows efficiency in deployment and inference on consumer-level hardware. SimpleFold challenges the reliance on complex domain-specific architectures designs in protein folding, opening up an alternative design space for future progress. Code: https://github.com/apple/ml-simplefold Correspondence: Yuyang Wang: yuyangw@apple.com; Miguel Angel Bautista: mbautistamartin@apple.com Date: September 24,"
        },
        {
            "title": "Introduction",
            "content": "Protein folding, the task of predicting proteins three-dimensional atomic structure from its amino acid (AA) sequence, is longstanding challenge in computational biology with far-reaching implications in drug discovery (Jumper et al., 2021; Baek et al., 2021). In this paper, we approach the protein folding problem purely from generative modeling perspective without making strong assumptions about the natural generation process of protein structures. We draw parallels between protein folding and vision generative models (i.e., text-to-image or text-to-3D generation (Poole et al., 2022; Lin et al., 2023a; Hong et al., 2024a,b)), where the input AA sequence plays the role of text prompt to generative model which outputs the all-atom 3D coordinates. Inspired by the recent success of generative models in the vision domain we build generalpurpose yet powerful architecture based solely on standard transformer blocks with adaptive layers (Vaswani et al., 2017; Peebles and Xie, 2023) which we trained at larger scale than previous protein folding models, both in terms of model size and training data. Established protein folding models like AlphaFold2 (Jumper et al., 2021) and RoseTTAFold (Baek et al., 2021) have achieved groundbreaking accuracy by relying on carefully engineered architectures that integrate computationally heavy domain-specific designs for protein folding tasks such as multiple sequence alignments (MSAs) of AA sequences, pair representations, and triangle updates (Jumper et al., 2021; Baek et al., 2021). These design choices (MSA, pair representations, triangular updates, etc.) are an attempt to hard-code our current understanding of the underlying structure generation process into these models, instead of opting to Work was completed while J.L. was an intern with Apple. 1 Figure 1 Example predictions of SimpleFold on targets (a) chain of 7QSW (RubisCO large subunit) and (b) chain of 8DAY (Dimethylallyltryptophan synthase 1), with ground truth shown in light aqua and prediction in deep teal. (c) Generated ensembles of target chain of 6NDW (Flagellar hook protein FlgE) with SimpleFold finetuned on MD ensemble data. (d) Performance of SimpleFold on CASP14 with increasing model sizes from 100M to 3B. (e) Inference time of different sizes of SimpleFold on consumer level hardware, i.e., M2 Max 64GB Macbook Pro. let models to learn this directly from data, which could be beneficial for variety of reasons. For example, (Lin et al., 2023b) showed that for orphan proteins (those with few or no close homologs) approaches based on protein language models (PLM) tend to outperform approaches that rely on MSA like AlphaFold2. In this paper, we propose strong departure from domain-specific designs towards much more general architectural design which has been demonstrated to be effective in generative modeling problems and can ultimately leverage data and compute as effectively as possible. Although folding models initially treated protein structure prediction as deterministic problem via reconstruction objectives (Jumper et al., 2021; Baek et al., 2021; Lin et al., 2023b), recent works have explored building generative modeling for folding (Jing et al., 2024a). Generative approaches provide way to model how native protein structures appear in nature, i.e., as non-deterministic minimizer of the the Gibbs free energy of the atomic system. Generative models naturally capture this uncertainty and make it straightforward to generate ensembles of viable conformations instead of single deterministic output. Following this insight, recent work has explored diffusionand flow-based generative models (Ho et al., 2020; Song et al., 2021; Lipman et al., 2023) for protein folding (Jing et al., 2024a, 2023; Abramson et al., 2024; Wohlwend et al., 2024), as well as de novo protein structure generation (Bose et al., 2023; Watson et al., 2023b; Yim et al., 2023b,a; Geffner et al., 2025; Lin et al., 2024). However, these approaches still employ the expensive architectural components from AlphaFold2 like pair representations and triangle updates. In this work, we propose SimpleFold, flow-matching based folding model that directly maps protein sequence to its full 3D atomic structure without relying on MSA, pairwise interaction maps, triangular updates or any other equivariant geometric modules. Our architecture is inspired by recent transformer-based text-to-image and text-to-3D flow matching models (Peebles and Xie, 2023; Ma et al., 2024), with strong emphasis on 2 departing from current architecture designs using general-purpose transformer backbone trained end-to-end with flow-matching training objective. Crucially, we demonstrate that strong folding performance (see Fig. 1 can be achieved without explicit pairwise representations, triangle updates, or MSA, which significantly reduces architectural complexity and challenges preconceived notions around the necessity of these designs (Lin et al., 2023b). SimpleFold represents strong departure from previous of protein folding models, and we summarize our contributions as follows: We revisit protein folding as conditional generative task and introduce SimpleFold, flow-based transformer folding model that eliminates MSA, pairwise representations, and triangle modules. We scale SimpleFold to 3B parameters and train it on approximately 9M distilled structures together with PDB experimental data. Our most powerful SimpleFold-3B model shows strong results in folding compared to folding baselines with hard-coded heuristic designs and also achieves competitive performance on protein folding benchmarks. We release family of models ranging from an efficient 100M model to large 3B model for the best performance (see Fig. 1(d)). SimpleFold-100M recovers 90% performance of our best model on major folding benchmarks while being very efficient in inference even on consumer-level devices."
        },
        {
            "title": "2.1 Flow-Matching Preliminary",
            "content": "Flow-matching generative models (Lipman et al., 2023; Albergo and Vanden-Eijnden, 2023) approach generation as time-dependent process that moves noise to data through integrating an ordinary differential equation (ODE) over time. For time [0, 1], flow matching defines path of probability distributions pt(xt) that continuously transforms tractable distribution p0 (e.g., Gaussian) into an arbitrarily complex data distribution pD. This transformation is described by flow ψt such that pt = [ψt] p0, where denotes the pushforward operator. In practice, the transformation is parameterized by learnable time-dependent velocity field vθ(xt, t), and the generative process is defined by integrating the ODE, dxt = vθ(xt, t) dt, from noise to data. To train the model, we implement linear interpolant path Albergo and Vanden-Eijnden (2023) (also referred to as rectified flow (Liu et al., 2023; Esser et al., 2024)) between samples from the empirical data distribution pD and noise samples ϵ (0, I), such that xt = tx + (1 t)ϵ, (2.1) where the target velocity is defined as vt = ϵ. In flow matching, we train network vθ to match the target across time and data via ℓ2 regression objective E[vθ(xt, t) vt2]. This yields consistent gradients with respect to the true (intractable) marginal score. As shown in prior work (Albergo et al., 2023; Kingma and Gao, 2023), under Gaussian marginals, diffusion and flow matching become equivalent up to change of hyper-parameters."
        },
        {
            "title": "2.2 Folding with Flow-Matching",
            "content": "SimpleFold casts protein folding as flow-matching generative model which generates protein structures from noise, conditioned on given amino acid sequence. This amino acid sequence-to-protein structure generative model is conceptually very similar to text-to-image or text-to-3D generative models in computer vision. In particular, given protein with Na heavy atoms, we build linear interpolant between noise ϵ and all-atom positions x, where ϵ, RNa3, conditioned on the amino acid sequence RNr , where Nr is number of residues or amino acids in the protein. Unlike earlier work that modeled only the Cαbackbone with flow-matching models (Lin and AlQuraishi, 2023; Lin et al., 2024; Geffner et al., 2025), we generate full-atom conformations including both backbones and side chains. 3 Training objective. The network vθ takes the amino acid sequence as conditioning input vθ(xt, s, t) to model the target velocity field. In particular, the flow-matching objective is defined as follows: ℓFM = Ex,s,ϵ,t (cid:20) 1 Na vθ(xt, s, t) (x ϵ) (cid:21) , (2.2) where xt is noisy structure sampled during training as given in Eq. 2.1. We also include an additional local distance difference test (LDDT) loss similar to (Abramson et al., 2024). This loss measures the atomic pairwise distances error between the generated structure ˆx(xt) at timestep and ground truth structures x. During training, ˆx(xt) is estimated through one step Euler, i.e., ˆx(xt) = xt + (1 t) vθ(xt, s, t). The LDDT loss is formulated as follows: ℓLDDT = Ex,s,ϵ,t (cid:34) (cid:80) i=j 1(δij < C)σ(δij ˆδt (cid:80) 1(δij < C) ij) (cid:35) , (2.3) where δij = xi xj and ˆδt ij = ˆx(xt)i ˆx(xt)j denote the distances between atom i, in ground truth and predicted structures, respectively. The term σ() is nonlinear function on pair distance errors and is cutoff distance which controls neighboring atoms to be included in the loss. The model is trained with weighted combination of flow-matching and LDDT terms: where α(t) is weighting term related to timestep in flow process and is also dependent to different training phases (see Sect. 4.1). ℓ = ℓFM + α(t)ℓLDDT, (2.4) Timestep resampling. To improve training efficiency and force generating structures with fine details (Esser et al., 2024; Geffner et al., 2025), the timestep is sampled from the distribution: p(t) = 0.98LN(0.8, 1.7) + 0.02U(0, 1), where LN is logistic-normal distribution (Atchison and Shen, 1980) and is uniform distribution. Unlike popular timestep resampling in image generation (Esser et al., 2024), where timesteps are more densely sampled in the middle of the flow process (i.e., around = 0.5), we shift the sample weight towards timesteps that are closer to clean data (i.e., = 1), similar to findings in (Geffner et al., 2025) in the context of unconditional generation. This improves quality of generated samples especially in modeling refined structures of side chain atoms. We attribute this to the fact that protein structures contain strong coarse-to-fine hierarchy secondary structure - Cαbackbone - side chain, thus oversampling close to the data manifold drives the model to better learn the refined atomic positions. Additional details regarding the LDDT loss and timestep resampling can be found in Appendix C.1."
        },
        {
            "title": "2.3 Architecture",
            "content": "Architectural components like triangle updates and explicit modeling of interactions between single representations and pair representations have been adopted as standard in protein folding models since AlphaFold2 (Jumper et al., 2021) was introduced. It remains an open question whether these architectural design decisions are necessary condition to build performant models. In strong departure from previous approaches, SimpleFold uses an architecture solely based on general-purpose transformer modules (see comparison in Fig. 5). In Fig. 2 we show an architecture diagram of SimpleFold, which contains three major modules: light-weighted atom encoder and decoder which are symmetric (i.e., same number of blocks and hidden size) and heavy residue trunk. All modules are implemented with standard transformer blocks with adaptive layers conditioned on the timestep (see bottom left of Fig. 2). The atom encoder takes in noisy atomic coordinates xt together with their corresponding atomic features (e.g., atomic type and charge, see Appendix for details) and outputs atom tokens RNada , where xt is encoded through Fourier positional embeddings. In the atom encoder we use local attention mask that constraints atom latents to only attend to local neighborhood around their residue (i.e., atom tokens only attend to atom tokens of nearby residues in the sequence). The grouping operation takes the output of the 4 atom encoder and conducts average pooling to atom tokens within the same residue to obtain residue tokens RNrda (see an illustration of grouping and ungrouping operations in Fig. 6). Similar to text-to-image and text-to-3D generative models, we use frozen pretrained protein language model (PLM) to embed the AA sequence into an informative latent representation. We leverage ESM2-3B (Lin et al., 2023b) in all our models to encode the AA sequence into per-residue conditioning embeddings RNrde. Sequence embeddings are then concatenated with the residue tokens along the channel dimension and fed into the residue trunk. The residue trunk contains most of the parameters of the model and is where most of the compute is spent on. The ungrouping operation projects residue tokens to corresponding atom tokens. Specifically, we broadcast the same residue token to the number of atoms particular residue contains, which is defined by AA types. skip connection from the output of atom encoder is also added to distinguish between different atoms within the same residue. Finally, the atom decoder updates the atom tokens and outputs the predicted velocity field ˆvt. Local attention masks are also applied in the atom decoder as the encoder. We adopt modern implementation stack for all the transformer blocks including QK-normalization (Esser et al., 2024) and SwiGLU (Shazeer, 2020) in place of standard FFN for better performance and training stability. The overall architecture of SimpleFold incorporates the hierarchical structure in proteins implementing fine - coarse - fine\" scheme to balance the performance and efficiency. To encode the positional information of atoms and residues, we employ rotary position embedding (RoPE) (Su et al., 2024). Particularly in each attention block within the residue trunk, they query and key vectors of the n-th residue in amino acid sequence are rotated by eiθn. In both the atom encoder and decoder, we extend the positional embedding to 4D axial RoPE. The first three axes are 3D atomic coordinates from reference conformers (see Appendix A), which are local structures predicted at the amino acid level by rule-based cheminformatic method. The last axis is the 1D indexing to the corresponding residue token. Each axis in 4D axial RoPE controls rotation of quarter of the hidden dimension in both query and key. Figure 2 Overview of SimpleFolds architecture built on general-purpose standard Transformer block with adaptive layers. Atom encoder, residue trunk, and atom decoder all share the same general-purposed building block. Our model circumvents the need for pair representations or triangular updates. SimpleFold strongly departs from the design choices in previous work (Chakravarty and Porter, 2022; Lin et al., 2023b; Abramson et al., 2024). Unlike AlphaFold2 (Chakravarty and Porter, 2022) or ESMFold (Lin et al., 2023b) which explicitly keep pair representation initialized by embeddings from expensive MSA search or attention score from the pretrained PLM, SimpleFold only keeps single sequence representation which does not require triangle update and is thus far more efficient. In contraposition to previous works Lin et al. (2024); Chakravarty and Porter (2022); Lin et al. (2023b) which rely on equivariant architectures to generate physically meaningful results, SimpleFold is built on standard non-equivariant Transformer blocks. To handle the rotational symmetries of protein structures, we apply SO(3) data augmentation during training, which randomly rotates structure targets, and rely on the capacity of the model to directly learn such symmetries during training."
        },
        {
            "title": "2.4 Sampling\nTo fold a protein with a given amino acid sequence s in inference, we initialize atomic coordinates as Gaussian\nnoise x0 ∼ N (0, I) and integrate the learned vector field from t = 0 to t = 1, which generates a full-atom\nstructure corresponding to the input sequence. We perform stochastic generation using a Langevin-style\nSDE formulation of the flow process, leveraging the equivalence between the learned velocity field vθ and a\nscore function sθ, namely sθ(xt, s, t) = (tvθ(xt, s, t) − xt)/(1 − t) (Albergo et al., 2023; Song et al., 2021). In\nparticular, we apply the Euler–Maruyama integrator (Ma et al., 2024):",
            "content": "dxt = vθ(xt, s, t) dt + 1 2 w(t)sθ(xt, t, c) dt + (cid:112)τ w(t) Wt, (2.5) where w(t) > 0 is time-dependent diffusion coefficient, Wt is reverse-time Wiener process, and τ controls the scale of stochasticity. We find w(t) = 2(1t) , which defines stochasticity scheduler following SNR of flow t+η process and η is small constant for numerical stability, gives the best sampling quality. We stick to this setting in all our experiments unless mentioned otherwise. Similar to previous flow-matching based protein generative models (Geffner et al., 2025), we find that τ balances the generation of accurate refined structures and modeling the ensemble of conformations."
        },
        {
            "title": "2.5 Confidence Module",
            "content": "Providing confidence estimation for generated protein structures can greatly help understand the quality of generation (Chakravarty and Porter, 2022; Lin et al., 2023b). To this end, we develop an additional predicted LDDT (pLDDT) module which predicts per-residue LDDT value (ranging from 0 to 100) as confidence score. After the folding model is fully trained, we train the pLDDT module in separate training stage while freezing all the parameters in the folding model (see Fig. 8). During training the pLDDT module, we sample protein structures ˆx on the fly, and feed ˆx into the folding model with timestep = 1 for adaptive layers to acquire the final residue tokens r. The pLDDT module is composed of 4 layers of standard transformer blocks without adaptive layers, which takes in and outputs pLDDT. Following (Chakravarty and Porter, 2022), the target LDDT is discretized into 50 bins and the pLDDT module is trained through cross-entropy objective."
        },
        {
            "title": "2.6 Training on Distilled Data",
            "content": "The number of computationally predicted protein structures is ever growing, two particular cases are AlphaFold Protein Structure Database (AFDB) (Varadi et al., 2022) and ESM Metagenomic Atlas (Lin et al., 2023b) containing over hundreds of millions computationally predicted protein structures. AFESM (Yeo et al., 2025) further combines these two resources and categorizes all distilled structures from both datasets into 5.12M non-singleton structural clusters. Though many folding models are trained on distilled data typically via self-distillation (Chakravarty and Porter, 2022; Abramson et al., 2024), the distilled data used is in relatively small scale compared to the complete set of publicly available computationally predicted structures. Making use of the vast amounts of distilled data currently available to train powerful folding models is thus an understudied problem. We train SimpleFold with data mix of 3 different sources. First, we include around 160K structures from PDB with cutoff of May 2020 following ESMFold (Lin et al., 2023b). Additionally, we use the SwissProt set from AFDB. Within SwissProt distilled structures, we select samples with average pLDDT greater than 85 and standard deviation of pLDDT smaller than 15, which yields approximately 270K distilled samples. Moreover, we use representative protein structures for each cluster in AFESM (Yeo et al., 2025). We filter these structures with pLDDT larger than 0.8 resulting in more than 1.9M distilled strictures. All SimpleFold models except the largest 3B model are trained on the combination of three datasets listed above, adding up to approximately 2M structures. To train our biggest model SimpleFold-3B, we explore an extended version AFESM (which we call AFESM-E) by also including structures beyond the cluster representatives. In particular, for each cluster, we randomly pick maximum of 10 proteins structures with average pLDDT larger than 80, which resulting in total of 8.6M distilled structures. Since larger models with larger capacity benefit from larger training sets, we train our largest SimpleFold-3B on the distilled AFESM-E data together with PDB and SwissProt."
        },
        {
            "title": "3 Related Work",
            "content": "Protein Folding Since the development of AlphaFold2 (Chakravarty and Porter, 2022) and RoseTTAFold (Baek et al., 2021) which achieved groundbreaking performance in protein folding with learning-based methods, many works have continued to investigate this problem (Ahdritz et al., 2024; Baek et al., 2023; Li et al., 2022). AlphaFold2 introduced domain specific network modules like triangle attention and design decisions like explicitly modeling interactions between single and pair representations. It also relied on MSA to extract evolutionary information of protein sequences in the hopes to nudge the model towards biological experts understanding of the underlying data generation process. OmegaFold (Wu et al., 2022) and ESMFold (Lin et al., 2023b) replaced MSA with learned embeddings from pretrained protein language model, which are efficient in inference and especially beneficial for orphan proteins. Some works also aimed at accelerating the models through efficient implementations of AlphaFold2 modules, like FastFold (Cheng et al., 2022) and MiniFold (Wohlwend et al.). These folding models are built on regression objectives of local frame instead of direct modeling of all-atom positions. Therefore structural predictions of these models lack diversity for ensemble generation. Flow-Matching for Proteins Generative models, especially diffusion and flow-matching based methods, have been introduced to protein folding given its superior performance in generating high-quality plausible samples. AlphaFlow/ESMFlow (Jing et al., 2024a) proposed to tune AlphaFold2/ESMFold with flow-matching objectives and demonstrated advantages in ensemble generation. However, (Jing et al., 2024a) were not build from the ground up as generative models and instead rely on powerful pretrained AlphaFold2 and ESMFold models which were trained with deterministic regression objective. AlphaFold3 (Abramson et al., 2024) and its architectural reproductions (e.g., Boltz-1 (Wohlwend et al., 2024), Protenix (Team et al., 2025), Chai-1 (Boitreaud et al., 2024)) also used diffusion to build generative models for protein complexes of biomolecular interactions. In addition, several works have investigated diffusion or flow-matching models for de novo protein structure generation, like RFDiffusion (Watson et al., 2023a), Genie-2 (Lin et al., 2024), P(all-atom) (Qu et al., 2024). Though these works have employed diffusion or flow-matching generative models for proteins, they still heavily rely on heuristic architectural designs from AlphaFold series like expensive triangle attention and explicit modeling of pair representations. Some are also built on crafted equivariant diffusion process (Jing et al., 2023). Proteina (Geffner et al., 2025) attempts to build simplified architecture but still explicitly applies pair representation, and it only models Cα generation. Previously, MCF (Wang et al., 2023) investigated conformation generation of small molecular systems with general-purpose transformer backbone. In strong departure from previous protein folding models, SimpleFold aims at tackling the folding problem with general purpose transformer backbone and learning symmetries in the underlying data generation process directly from training data."
        },
        {
            "title": "4.1 Experimental settings",
            "content": "We train family of SimpleFold models at different sizes (i.e., 100M, 360M, 700M, 1.1B, 1.6B, and 3B) to investigate the scaling ability of proposed framework in folding. When scaling up model sizes, we increase the depth and hidden size of atom encoder and decoder as well as residue trunk altogether (see detailed configurations in Tab. 5). During training we copy one protein Bc times per GPU with different flow timestep sampled and accumulate gradients from Bp different proteins on different GPUs, following AlphaFold2 (Chakravarty and Porter, 2022; Abramson et al., 2024). Therefore, the effective batch size is Bc Bp (see Tab. 6 for batch settings). We empirically find that this strategy leads to more stable gradient and better performance than naively building batch with randomly selected proteins. Pre-training. The overall training of SimpleFold consistent of two training stages pre-training and finetuning, which only differ on the data used to train the model. During the pre-training stage of SimpleFold we use large dataset containing as much available data as possible. Finetuning, on the other hand, is performed on high-quality data to increase the fidelity of generated structures. In pre-training, SimpleFold is trained on 7 approximately 2M (8.7M for the 3B model) data structures including all three data sources, namely PDB, SwissProt from AFDB, and AFESM. We set the maximal amino acid sequence length to 256, where we keep shorter sequence without padding while crop longer sequences to 256 residues. We set α(t) = 1 in Eq. 2.4 which uses LDDT supervision through the whole flow process. All models are trained with effective batch size 512 except for 1.6B and 3B models which are trained with batch size 1024 and 3072, respectively. We use the AdamW optimizer (Loshchilov and Hutter, 2019) with learning rate 0.0001 and linear warmup for the first 5000 steps. In finetuning, SimpleFold is trained on PDB and SwissProt subsets only which contain higher Finetuning. quality data. We set maximal sequence length to 512 which allows access to larger protein structures in this training phase. We accordingly half Bc in each batch to fit in GPU memory. We set α(t) = 1 + 8ReLU(t 0.5) in Eq. 2.4 which gradually increases weight of LDDT loss to maximum value of 5 when approaching clean data (t = 1). We keep AdamW as an optimizer with the same learning rate 0.0001 in finetuning. In both pre-training and finetuning, we apply an exponential moving average (EMA) of all model weights with decay of 0.999 following common practice in flow-matching generative models. pLDDT training. After SimpleFold is pretrained and finetuned, we train the pLDDT module with all other components frozen. The pLDDT module is trained on combination of PDB and SwissProt data, which contains experimental and high-quality distilled data. During pLDDT training, we set α(t) = 1, and SimpleFold generates structure samples on the fly with 200 steps and τ = 0.3. As in finetuning, We set maximal sequence length to 512 and apply AdamW optimizer with the learning rate 0.0001."
        },
        {
            "title": "4.2 Protein Folding",
            "content": "We evaluate SimpleFold on two widely adopted protein structure prediction benchmarks: CAMEO22 and CASP14, which are rigorous tests for generalization, robustness, and atomic-level accuracy in folding models. CAMEO22 (Haas et al., 2018) follows the setting in (Jing et al., 2023) which contains 183 targets structures of between 100 and 750 residues (see Fig. 1(a)(b) for example samples). In addition, CASP14 (Pereira et al., 2021) is more challenging benchmark containing selective targets for biennial blind prediction challenge. We evaluate on subset of targets from CASP14 (Pereira et al., 2021), comprising 70 single-chain proteins of varying length from 50 to 1000 amino acids. We set τ = 0.01 for SimpleFold in inference which empirically shows best general performance in folding. We report standard structure prediction metrics: TM-score and GDT-TS assess global structural similarity and are sensitive to topological correctness; LDDT and LDDT-Cα measure local atomic accuracy across all atoms and Cα atoms, respectively; root mean square deviation (RMSD) measures the averaged distance of atomic positions between two superimposed structures, which is the lower the better. For each metric, we report both the mean and the median score over all the test samples (separated by slashes). We report all the metrics for all-atom models and only report TM-score and GDT-TS for backbone-only models (see details in Appendix D). Table 1 summarizes results on CASP14 and CAMEO22. We group approaches based on strategies to extract protein sequence information, namely through MSA search or protein language model (PLM). RoseTTAFold, RoseTTAFold2, and AlphaFold2 are MSA-based models while ESMFold and OmegaFold leverage embeddings from pretrained PLM in place of MSA search. We also color baselines based on whether they are trained with generative objectives, i.e., diffusion / flow-matching or autoregression instead of direct regression to ground truth structures. For example, AlphaFlow and ESMFlow are flow-matching models finetuned from AlphaFold2 and ESMFold, respectively. Interestingly, both finetuned models fall behind their original regression starting points in all metrics. We attribute this to the fact that protein folding benchmarks, including CAMEO22 and CASP14, usually contain only one ground truth\" target, which favors regression models that make deterministic point-wise predictions. Despite its simplicity, SimpleFold achieves competitive performance compared with these baselines. In both benchmarks, SimpleFold shows consistently better performance than ESMFlow which is also flow-matching model built with ESM embeddings. On CAMEO22, SimpleFold demonstrates comparable results to the best folding models (e.g., ESMFold, RoseTTAFold2, and AlphaFold2). In particular, SimpleFold achieves over 8 Table 1 Performance of protein folding on the CAMEO22 (top) and CASP14 (bottom) benchmarks. For each metric, we report the average / median over all samples. Here, orange denotes baselines trained with regression objectives, green denotes baselines trained with generative objectives (i.e., diffusion/flow-matching or autoregression), and blue denotes our SimpleFold, which is trained with generative objective but without MSA. Type Model TM-score GDT-TS LDDT LDDT-Cα RMSD MSAbased PLMbased Ours MSAbased PLMbased Ours RoseTTAFold (Baek et al., 2021) AlphaFlow (Jing et al., 2024a) AlphaFold2 (Jumper et al., 2021) RoseTTAFold2 (Baek et al., 2023) ESM3 (Hayes et al., 2025) ESMDiff (Lu et al., 2024a) EigenFold (Jing et al., 2023) OmegaFold (Wu et al., 2022) ESMFlow (Jing et al., 2024a) ESMFold (Lin et al., 2023b) SimpleFold-100M SimpleFold-360M SimpleFold-700M SimpleFold-1.1B SimpleFold-1.6B SimpleFold-3B RoseTTAFold (Baek et al., 2021) AlphaFlow (Jing et al., 2024a) RoseTTAFold2 (Baek et al., 2023) AlphaFold2 (Jumper et al., 2021) ESMDiff (Lu et al., 2024a) ESM3 (Hayes et al., 2025) EigenFold (Jing et al., 2023) ESMFlow (Jing et al., 2024a) OmegaFold (Wu et al., 2022) ESMFold (Lin et al., 2023b) SimpleFold-100M SimpleFold-360M SimpleFold-700M SimpleFold-1.1B SimpleFold-1.6B SimpleFold-3B CAMEO22 0.780 / 0.860 0.840 / 0.927 0.863 / 0.942 0.864 / 0.947 0.746 / 0.840 0.754 / 0.847 0.750 / 0.840 0.805 / 0.899 0.818 / 0.893 0.853 / 0.933 0.803 / 0.878 0.826 / 0.905 0.829 / 0.915 0.833 / 0.924 0.835 / 0.916 0.837 / 0.916 0.715 / 0.775 0.808 / 0.853 0.844 / 0.903 0.845 / 0.904 0.694 / 0.758 0.701 / 0.760 0.710 / 0.790 0.767 / 0.844 0.774 / 0.832 0.826 / 0. 0.746 / 0.787 0.782 / 0.841 0.788 / 0.845 0.793 / 0.851 0.799 / 0.864 0.802 / 0.867 CASP14 0.654 / 0.678 0.740 / 0.812 0.802 / 0.881 0.845 / 0.907 0.521 / 0.499 0.534 / 0.567 0.590 / 0.637 0.627 / 0.679 0.693 / 0.773 0.701 / 0.792 0.611 / 0.628 0.674 / 0.758 0.680 / 0.767 0.697 / 0.796 0.712 / 0.801 0.720 / 0.792 0.562 / 0.572 0.661 / 0.711 0.740 / 0.824 0.783 / 0. 0.447 / 0.430 0.459 / 0.488 0.539 / 0.575 0.539 / 0.544 0.625 / 0.723 0.622 / 0.711 0.513 / 0.544 0.585 / 0.654 0.591 / 0.668 0.607 / 0.668 0.630 / 0.709 0.639 / 0.703 0.575 / 0.605 0.741 / 0.798 0.816 / 0.856 0.727 / 0.767 0.746 / 0.815 0.696 / 0.745 0.792 / 0.834 0.721 / 0.752 0.773 / 0.803 0.775 / 0.809 0.776 / 0.807 0.782 / 0.816 0.773 / 0.802 0.464 / 0.456 0.632 / 0.662 0.638 / 0.669 0.778 / 0. 0.525 / 0.539 0.627 / 0.726 0.637 / 0.705 0.537 / 0.549 0.617 / 0.657 0.630 / 0.674 0.640 / 0.676 0.660 / 0.699 0.666 / 0.709 0.798 / 0.827 0.855 / 0.893 0.893 / 0.923 0.893 / 0.926 0.829 / 0.892 0.827 / 0.867 0.871 / 0.906 0.822 / 0.852 0.844 / 0.878 0.850 / 0.886 0.850 / 0.883 0.853 / 0.889 0.852 / 0.884 0.705 / 0.723 0.767 / 0.799 0.824 / 0.869 0.856 / 0. 0.669 / 0.730 0.715 / 0.824 0.725 / 0.802 0.659 / 0.685 0.703 / 0.762 0.714 / 0.763 0.723 / 0.758 0.741 / 0.798 0.747 / 0.829 5.721 / 2.864 3.846 / 2.122 3.578 / 1.857 3.571 / 1.707 5.294 / 2.622 4.528 / 2.693 3.973 / 2.019 4.897 / 2.855 4.775 / 2.681 4.557 / 2.423 4.350 / 2.334 4.397 / 2.187 4.225 / 2.175 9.676 / 6.420 7.091 / 3.949 6.744 / 3.292 5.027 / 3. 10.503 / 6.974 9.845 / 4.042 8.679 / 4.016 11.157 / 8.976 9.382 / 4.828 9.289 / 4.431 9.249 / 4.462 8.424 / 4.722 7.732 / 3.923 95% performance of RoseTTAFold2/AlphaFold2 on most metrics without applying expensive and heuristic triangle attention and MSA. On the more challenging CASP14 benchmark, SimpleFold achieves even better performance than ESMFold. In particular, SimpleFold-3B obtains TM-score of 0.720 / 0.792 and GDT-TS of 0.639 / 0.703 in comparison to 0.701 / 0.792 and 0.622 / 0.711 of ESMFold. SimpleFold also shows competitive or even better performance to baselines that applies MSA like RoseTTAFold and AlphaFlow. It is also notable that all models except AlphaFold2 show significant performance drop on CASP14 compared to CAMEO22, even AlphaFlow which is finetuned flow-matching model using pre-trained AlphaFold2 model as initialization. We attribute this to the fact that AlphaFold2 leverages templates from MSA and uses regression training objective. We note that the performance drop of SimpleFold on CASP14 w.r.t. CAMEO22 is much smaller compared to many baselines model like ESMFold. Given that neither ESMFold or SimpleFold rely on MSA, this demonstrates that SimpleFold is very robust in predicting valid structures on challenging tasks. For completeness, we report results of SimpleFold using different model sizes. The smallest model SimpleFold100M shows competitive performance given its advantage of efficiency in both training and inference. In particular, SimpleFold achieves more than 90% of the performance ESMFold on CAMEO22, which demonstrates 9 Table 2 Evaluation on MD ensembles. Results of baseline models are taken from (Jing et al., 2024a; Lu et al., 2024a), to which the evaluation pipeline for our SimpleFold (SF) and SimpleFold-MD (SF-MD) adheres. No Tuning Tuned AF2 MSA-sub. SimpleFold ESMDiff ESMFlow-MD AlphaFlow-MD SimpleFold-MD Pairwise RMSD Global RMSF Per target RMSF RMWD RMWD trans contri RMWD var contri MD PCA W2 Joint PCA W2 % PC sim > 0.5 Weak contacts Transient contacts Exposed residue Exposed MI matrix ρ 0.10 0.21 0.52 3.58 2.86 2.27 1.99 2.86 23 0.27 0.28 0.32 0. 0.22 0.29 0.51 4.28 3.33 2.24 2.23 3.57 21 0.37 0.27 0.37 0.10 0.44 0.45 0.60 4.22 3.74 1.74 1.62 2.59 37 0.36 0.27 0.39 0.14 0.18 0.49 0.68 7.48 5.18 3.37 2.29 6.32 23 0.52 0.26 - - 0.19 0.31 0.76 3.60 3.13 1.74 1.51 3.19 26 0.55 0.34 0.49 0.20 0.48 0.60 0.85 2.61 2.28 1.30 1.52 2.18 44 0.62 0.41 0.50 0.25 0.45 0.48 0.67 4.17 3.40 1.88 1.34 2.85 38 0.56 0.34 0.60 0. the effectiveness of building folding model using general purpose architectural blocks. Moreover, scaling up the model sizes of SimpleFold models results in better performance across the board, which indicates the benefit of designing general purpose approach that benefits from scale. It is notable that scaling up model sizes improves performance substantially more in CASP14, i.e the more challenging benchmark, than in CAMEO22. This is clear empirical evidence that models with larger capacity are more capable of solving complex folding tasks."
        },
        {
            "title": "4.3 Confidence Measure with pLDDT",
            "content": "Fig. 3(a) shows an example of predicted structure with pLDDT where red and orange denotes low pLDDT and blue denotes high pLDDT. As illustrated, SimpleFold is confident about most predictions of secondary structures while being uncertain about flexible loops. Fig. 3(b) and (c) depict comparison of pLDDT and actual LDDT-Cα. We include targets from CAMEO22 and 1000 random selected protein chains from PDB after Jan 2023. pLDDT achieves the Pearsons corelation of 0.77 w.r.t LDDT-Cα, which indicates that pLDDT module of SimpleFold correctly models the overall quality of predicted structures. It is also noted that our pLDDT module does not adhere to the generative flow process to output pLDDT. Therefore, it can be applied to measure the quality of predictions from other models seamlessly, which we leave for future investigation. Figure 3 (a) An example prediction of SimpleFold with pLDDT (color red to dark blue denote pLDDT low to high following visualization from Chakravarty and Porter (2022)). (b) & (c) Comparison of pLDDT and LDDT-Cα. 10 Table 3 Two-state conformation results. For the last two metrics, both mean and median are reported over the targets. Results are taken from the ESMDiff paper (Lu et al., 2024a), to which the evaluation pipeline for the rest models adhere. Type Model Res. flex. (global) Res. flex. (per-target) TM-ens Res. flex. (global) Res. flex. (per-target) TM-ens Seqbased FoldFlow2 (Huguet et al., 2024) MultiFlow (Campbell et al., 2024a) Str2Str (Lu et al., 2024b) Eigenfold (Jing et al., 2023) ESMDiff (Lu et al., 2024a) ESMFlow (Jing et al., 2024a) MSAbased MSA-Subs. (Jumper et al., 2021) AlphaFlow (Jing et al., 2024a) Ours SimpleFold-100M SimpleFold-360M SimpleFold-700M SimpleFold-1.1B SimpleFold-1.6B SimpleFold-3B 0.027 0.113 0.174 0.126 0.420 0. 0.398 0.455 0.492 0.537 0.552 0.557 0.501 0.639 Apo/holo 0.057 / 0.055 0.211 / 0.194 0.326 / 0.307 0.407 / 0.401 0.489 / 0.515 0.496 / 0.522 0.404 / 0.371 0.527 / 0.527 0.216 / 0.208 0.360 / 0.342 0.731 / 0.728 0.830 / 0.870 0.838 / 0.877 0.856 / 0. 0.856 / 0.894 0.864 / 0.893 0.500 / 0.532 0.520 / 0.528 0.524 / 0.538 0.526 / 0.537 0.522 / 0.508 0.550 / 0.552 0.852 / 0.887 0.864 / 0.898 0.870 / 0.899 0.870 / 0.900 0.877 / 0.912 0.893 / 0.916 0.051 0.092 0.161 0.225 0.402 0.269 0.350 0.385 0.391 0.359 0.307 0.337 0.240 0. Fold-switch 0.009 / 0.005 0.068 / 0.061 0.246 / 0.233 0.279 / 0.255 0.341 / 0.288 0.345 / 0.329 0.320 / 0.303 0.384 / 0.376 0.291 / 0.241 0.310 / 0.314 0.328 / 0.310 0.346 / 0.344 0.339 / 0.318 0.288 / 0.263 0.199 / 0.191 0.269 / 0.250 0.615 / 0.644 0.614 / 0.653 0.626 / 0.685 0.700 / 0.755 0.714 / 0.765 0.730 / 0.788 0.656 / 0.677 0.689 / 0.746 0.693 / 0.713 0.698 / 0.755 0.721 / 0.770 0.734 / 0."
        },
        {
            "title": "4.4 Ensemble Generation",
            "content": "4.4.1 Molecular dynamic ensemble SimpleFold trivially models the distribution of protein structures, due its generative training objective. Namely, SimpleFold does not only generate one deterministic structure for an input AA sequence but is also capable of generating the ensemble of different conformations. To demonstrate this ability of SimpleFold, we benchmark the performance on the ATLAS dataset (Vander Meersche et al., 2024), which assess generation of molecular dynamic (MD) ensemble structures. ATLAS contains contains all-atom MD simulations of 1390 proteins. We follow AlphaFlow (Jing et al., 2024a) for training, validation, and test split of ATLAS and evaluate generated 250 conformations for each protein in test set. Tab. 2 compares SimpleFold with baseline models on ATLAS (see Tab. 9 for SimpleFold of different sizes). Reported metrics comprehensively measure the quality of generated ensembles from predicting flexibility (e.g., RMSD and RMSF r), distributional accuracy (e.g., RMWD), and ensemble observables (e.g., exposed residue and exposed MI matrix). Firstly, we directly evaluate our largest SimpleFold-3B without additional tuning on MD simulation data in ATLAS. We set τ = 0.6 (Eq. 2.5) in inference to add more stochasticity than folding tasks. We compare our approach to baseline models, AlphaFold2 (Chakravarty and Porter, 2022) and MSA subsampling (Del Alamo et al., 2022). MSA subsampling introduces more stochasticity to AlphaFold2 by subsampling the aligned AA sequences from MSA search. Note the ESMFold is trained via deterministic regression objective, thus cannot be applied to ensemble generation without additional tuning. Compared to baselines, SimpleFold achieves superior performance on generating ensembles that match the distribution from MD simulations. We also report the results of SimpleFold-MD, finetuned model on the training data split of ATLAS, comparing to baselines that are also additionally tuned (i.e., ESMDiff (Lu et al., 2024a), ESMFlow-MD (Jing et al., 2024a), and AlphaFlow-MD (Jing et al., 2024a)). In particular, fully trained SimpleFold is tuned for additional 20K iterations, where we keep α(t) = 1 (Eq. 2.4). As shown in Tab. 2, SimpleFold consistently achieves better performance than ESMFlow-MD where both rely on the ESM embedding without MSA. SimpleFold also shows better performance than AlphaFlow-MD on metrics related to ensemble observables (e.g., exposed residue and MI matrix), which are key feature in the identification of cryptic pockets in drug discovery. 4.4.2 Multi-state structure prediction We also evaluate the capacity of SimpleFold to generate structures for proteins showing more than one natural conformation. We adopt the benchmarking set of apo-holo conformational change (Saldaño et al., 2022) (Apo/holo) and fold-switchers (Chakravarty and Porter, 2022) (Fold-switch) following EigenFold (Jing et al., 11 2023). The target in each dataset is represented by (1) an amino acid sequence and (2) two distinct ground truth structures. The model is required to produce diverse yet accurate set of samples covering both conformational states and reflecting correct local flexibility. We compare SimpleFold with collection of existing approaches including both (1) sequence-based approaches: FoldFlow2 (Huguet et al., 2024), MultiFlow (Campbell et al., 2024b), Str2Str (Lu et al., 2024b), EigenFold (Jing et al., 2023), ESMDiff (Lu et al., 2024a) and ESMFlow (Jing et al., 2024a); (2) MSA-based methods, including MSA subsampling (Del Alamo et al., 2022) and AlphaFlow (Jing et al., 2024a). For each dataset, we report the global and per-target residue flexibility (res. flex.), as well as the ensemble TM score (TM-ens) (Jing et al., 2023). The evaluation protocol follows previous works (Jing et al., 2023; Lu et al., 2024a), where five samples are generated to compute the metrics with respect to the two ground truth conformations for each target. In inference, we empirically set τ = 0.8 for SimpleFold which generates structures that align with both native conformations and correctly model residue flexibility. As shown in Tab. 3, SimpleFold obtains state-of-the-art performance on Apo/holo, where SimpleFold outperforms strong MSA-based approaches like AlphaFlow significantly. On Fold-switch, SimpleFold shows comparable or even better performance than ESMFlow which is also applies flow-matching objective and is built on ESM embeddings. The results validate the capability of our SimpleFold in predicting the structures of high quality (i.e., ensemble TM-score) as well as correctly modeling the flexibility in structures (i.e., residue flexibility). Also, the overall performance of SimpleFold increases with the model size growing, which further showcase potential of our proposed framework in generating protein ensembles. Experiments on both MD ensemble and multi-state structure benchmarks demonstrate the capability of SimpleFold in modeling the ensemble of protein structures, which can be beneficial for applications that requires flexibility modeling of protein structures (e.g., molecular docking)."
        },
        {
            "title": "4.5 Effects of Scaling in Protein Folding",
            "content": "Figure 4 Scaling behavior of SimpleFold. Training Gflops vs. folding performance on GDT-TS and (b) TM-score. Training steps vs. folding performance on (c) GDT-TS and (d) TM-score. How data scale affects the performance (e) GDT-TS and (f) TM-score. All models are benchmarked on CAMEO22. 12 SimpleFold benefits from increasing model sizes as proven by recent success of generative models in other domains, like vision and language generation. We note that the effects of scaling both training data and model sizes have note yet been rigorously investigated in protein folding. In this section, we empirically show the scaling behavior of SimpleFold from both model and data perspectives, highlighting important considerations for building powerful biological generative models. To assess the benefit of scaling up the model size in SimpleFold, we train models with different sizes from the smallest with 100M parameters to the largest with 3B parameters. All models are trained with full pre-training data containing PDB, SwissProt from AFDB, and filtered AFESM. Fig. 4(a)-(d) illustrate how model sizes affect the performance of folding (also see Fig. 1(d)). Larger models trained with larger training budget (i.e., training Gflops and training iterations), are preferred to achieve better performance. We believe these results highlight the positive scaling behavior of SimpleFold and highlight an direction of progress to obtain more powerful generative models in biology. We also show the benefits of scaling up training data in SimpleFold. We train SimpleFold-700M with different sources of training data: (1) PDB only (160K structures), (2) combination of PDB and SwissProt (SP, 270K structures) from AFDB, (3) filtered representative proteins from AFESM (1.9M structures) in addition to PDB and SwissProt, and (4) the extended AFESM set (AFESM-E) which contains additional proteins besides the representative protein in each cluster (a total of 8.6M structures). As shown in Fig. 4(e) and (f), SimpleFold as we increase the total number of unique structures in the data mix, the final performance of SimpleFold tends to improve after 400k training iterations. These experimental results support our core contribution to build simplified and scalable folding model that benefits from the growing total of protein data available either experimentally or distilled from different models."
        },
        {
            "title": "5 Conclusions and Future Work",
            "content": "We have introduced SimpleFold, flow-matching based generative model for protein folding that represent strong departure from the architectural designs in previous approaches. SimpleFold is solely built with general-purpose transformer blocks with adaptive layers, dispensing away with heuristic designs like expensive pair representations and triangular updates introduced by AlphaFold2. SimpleFold is trained with simple flow-matching training objective and an additional LDDT loss instead of combination of multiple protein specific loss terms. This simplified framework allows us to train SimpleFold at scale both in terms of model size and training data. Our largest (and most powerful) model, SimpleFold-3B, demonstrates competitive performance on standard folding tasks. Due to its generative training objective SimpleFold demonstrates very strong or even state-of-the-art results on multiple ensemble generation tasks. To the best of our knowledge, SimpleFold is the first work that rigorously demonstrates good scaling behavior in protein folding. SimpleFold highlights the potential of significantly simplifying protein structure prediction architectures, reducing reliance on computationally complex network blocks. We believe SimpleFold represents disruptive approach for protein folding that relies on scaling up general purpose architecture blocks to learn the symmetries of the underlying data generation process directly from training data. With the codebase and checkpoints publicly available, we believe SimpleFold can be applied and extended to various protein related applications. Due to its nature of being simplified architecture built on standard transformer blocks, SimpleFold is extendable with common finetuning techniques like adapater (Houlsby et al., 2019) and LoRA (Hu et al., 2022) on specific protein structure data and tasks beyond folding. SimpleFold can also directly benefit from distillation for faster inference and more efficient deployment of the largest SimpleFold-3B models. Besides large models, we also released more efficient versions SimpleFold-100M, which is light-weighted and much faster in deployment and can be suitable for inference time is bottleneck. We hope SimpleFold serves as an initiative for the community to build efficient and powerful protein generative models."
        },
        {
            "title": "Acknowledgments",
            "content": "The authors thank Tianrong Chen, Jiatao Gu and Shuangfei Zhai for helpful discussions. The authors want to acknowledge the Boltz-1 (Wohlwend et al., 2024) team for opensourcing their codebase."
        },
        {
            "title": "References",
            "content": "Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J. Ballard, Joshua Bambrick, Sebastian W. Bodenstein, David A. Evans, Chia-Chun Hung, Michael ONeill, David Reiman, Kathryn Tunyasuvunakool, Zachary Wu, Akvile Zemgulyte, Eirini Arvaniti, Charles Beattie, Ottavia Bertolli, Alex Bridgland, Alexey Cherepanov, Miles Congreve, Alexander I. Cowen-Rivers, Andrew Cowie, Michael Figurnov, Fabian B. Fuchs, Hannah Gladman, Rishub Jain, Yousuf A. Khan, Caroline M. R. Low, Kuba Perlin, Anna Potapenko, Pascal Savy, Sukhdeep Singh, Adrian Stecula, Ashok Thillaisundaram, Catherine Tong, Sergei Yakneen, Ellen D. Zhong, Michal Zielinski, Augustin Zidek, Victor Bapst, Pushmeet Kohli, Max Jaderberg, Demis Hassabis, and John M. Jumper. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, 630:493500, 2024. Gustaf Ahdritz, Nazim Bouatta, Christina Floristean, Sachin Kadyan, Qinghui Xia, William Gerecke, Timothy ODonnell, Daniel Berenberg, Ian Fisk, Niccolò Zanichelli, et al. Openfold: Retraining alphafold2 yields new insights into its learning mechanisms and capacity for generalization. Nature methods, 21(8):15141524, 2024. Michael S. Albergo, Nicholas M. Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh International Conference on Learning Representations (ICLR), 2023. Jhon Atchison and Sheng Shen. Logistic-normal distributions: Some properties and uses. Biometrika, 67(2):261272, 1980. Minkyung Baek, Frank DiMaio, Ivan Anishchenko, Justas Dauparas, Sergey Ovchinnikov, Gyu Rie Lee, Jue Wang, Qian Cong, Lisa N. Kinch, R. Dustin Schaeffer, et al. Accurate prediction of protein structures and interactions using three-track neural network. Science, 373(6557):871876, 2021. Minkyung Baek, Ivan Anishchenko, Ian Humphreys, Qian Cong, David Baker, and Frank DiMaio. Efficient and accurate prediction of protein structure using rosettafold2. BioRxiv, pages 202305, 2023. Marco Biasini, Tobias Schmidt, Stefan Bienert, Valerio Mariani, Gabriel Studer, Jürgen Haas, Niklaus Johner, Andreas Daniel Schenk, Ansgar Philippsen, and Torsten Schwede. Openstructure: an integrated software framework for computational structural biology. Biological crystallography, 69(5):701709, 2013. Jacques Boitreaud, Jack Dent, Matthew McPartlon, Joshua Meier, Vinicius Reis, Alex Rogozhnikov, and Kevin Wu. Chai-1: Decoding the molecular interactions of life. BioRxiv, 2024. Avishek Joey Bose, Tara Akhound-Sadegh, Guillaume Huguet, Kilian Fatras, Jarrid Rector-Brooks, Cheng-Hao Liu, Andrei Cristian Nica, Maksym Korablyov, Michael Bronstein, and Alexander Tong. Se (3)-stochastic flow matching for protein backbone generation. arXiv preprint arXiv:2310.02391, 2023. Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997, 2024a. Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997, 2024b. Devlina Chakravarty and Lauren Porter. Alphafold2 fails to predict protein fold switching. Protein Science, 31(6): e4353, 2022. Shenggan Cheng, Xuanlei Zhao, Guangyang Lu, Jiarui Fang, Zhongming Yu, Tian Zheng, Ruidong Wu, Xiwen Zhang, Jian Peng, and Yang You. Fastfold: Reducing alphafold training time from 11 days to 67 hours. arXiv preprint arXiv:2203.00854, 2022. Diego Del Alamo, Davide Sala, Hassane Mchaourab, and Jens Meiler. Sampling alternative conformational states of transporters and receptors with alphafold2. Elife, 11:e75751, 2022. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. Tomas Geffner, Kieran Didi, Zuobai Zhang, Danny Reidenbach, Zhonglin Cao, Jason Yim, Mario Geiger, Christian Dallago, Emine Kucukbenli, Arash Vahdat, et al. Proteina: Scaling flow-based protein structure generative models. arXiv preprint arXiv:2503.00710, 2025. Jan Haas, Stefan Roth, Andreas Arnold, Torsten Kiefer, Lukas Schmidt, Laura Bordoli, and Torsten Schwede. Continuous automated model evaluation (cameo) complementing the critical assessment of structure prediction in casp12. Proteins: Structure, Function, and Bioinformatics, 86(S1):387398, 2018. doi: 10.1002/prot.25431. URL https://onlinelibrary.wiley.com/doi/full/10.1002/prot.25431. Thomas Hayes, Roshan Rao, Halil Akin, Nicholas Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Tran, Jonathan Deaton, Marius Wiggert, et al. Simulating 500 million years of evolution with language model. Science, page eads0018, 2025. J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. Fangzhou Hong, Jiaxiang Tang, Ziang Cao, Min Shi, Tong Wu, Zhaoxi Chen, Shuai Yang, Tengfei Wang, Liang Pan, Dahua Lin, and Ziwei Liu. 3dtopia: Large text-to-3d generation model with hybrid diffusion priors. arXiv preprint arXiv:2403.02234, 2024a. URL https://arxiv.org/abs/2403.02234. Fangzhou Hong, Jiaxiang Tang, Ziang Cao, Min Shi, Tong Wu, Zhaoxi Chen, Shuai Yang, Tengfei Wang, Liang Pan, Dahua Lin, and Ziwei Liu. 3dtopia: Large text-to-3d generation model with hybrid diffusion priors. arXiv preprint arXiv:2403.02234, 2024b. URL https://arxiv.org/abs/2403.02234. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 27902799. PMLR, 2019. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Guillaume Huguet, James Vuckovic, Kilian Fatras, Eric Thibodeau-Laufer, Pablo Lemos, Riashat Islam, Cheng-Hao Liu, Jarrid Rector-Brooks, Tara Akhound-Sadegh, Michael Bronstein, et al. Sequence-augmented se (3)-flow matching for conditional protein backbone generation. arXiv preprint arXiv:2405.20313, 2024. Bowen Jing, Ezra Erives, Peter Pao-Huang, Gabriele Corso, Bonnie Berger, and Tommi Jaakkola. Eigenfold: Generative protein structure prediction with diffusion models. arXiv preprint arXiv:2304.02198, 2023. Bowen Jing, Bonnie Berger, and Tommi Jaakkola. Alphafold meets flow matching for generating protein ensembles. arXiv preprint arXiv:2402.04845, 2024a. Bowen Jing, Hannes Stärk, Tommi Jaakkola, and Bonnie Berger. Generative modeling of molecular dynamics trajectories. Advances in Neural Information Processing Systems, 37:4053440564, 2024b. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583589, 2021. Diederik Kingma and Ruiqi Gao. Understanding the diffusion objective as weighted integral of elbos. arXiv preprint arXiv:2303.00848, 2023. Georgii Krivov, Maxim Shapovalov, and Roland Dunbrack Jr. Improved prediction of protein side-chain conformations with scwrl4. Proteins: Structure, Function, and Bioinformatics, 77(4):778795, 2009. Ziyao Li, Xuyang Liu, Weijie Chen, Fan Shen, Hangrui Bi, Guolin Ke, and Linfeng Zhang. Uni-fold: an open-source platform for developing protein folding models beyond alphafold. bioRxiv, pages 202208, 2022. Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023a. URL https://arxiv.org/abs/2211. 10440. Yeqing Lin and Mohammed AlQuraishi. Generating novel, designable, and diverse protein structures by equivariantly diffusing oriented residue clouds. arXiv preprint arXiv:2301.12485, 2023. Yeqing Lin, Minji Lee, Zhao Zhang, and Mohammed AlQuraishi. Out of many, one: Designing and scaffolding proteins at the scale of the structural universe with genie 2. arXiv preprint arXiv:2405.15489, 2024. 15 Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with language model. Science, 379(6637):11231130, 2023b. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In ICLR, 2023. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. Jiarui Lu, Xiaoyin Chen, Stephen Zhewen Lu, Chence Shi, Hongyu Guo, Yoshua Bengio, and Jian Tang. Structure language models for protein conformation generation. arXiv preprint arXiv:2410.18403, 2024a. Jiarui Lu, Bozitao Zhong, Zuobai Zhang, and Jian Tang. Str2str: score-based framework for zero-shot protein conformation sampling. In The Twelfth International Conference on Learning Representations, 2024b. Jiarui Lu, Xiaoyin Chen, Stephen Zhewen Lu, Aurelie Lozano, Vijil Chenthamarakshan, Payel Das, and Jian Tang. Aligning protein conformation ensemble generation with physical feedback. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=Asr955jcuZ. Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. Milot Mirdita, Konstantin Schütze, Yoshitaka Moriwaki, Lim Heo, Sergey Ovchinnikov, and Martin Steinegger. Colabfold: making protein folding accessible to all. Nature methods, 19(6):679682, 2022. William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. Joana Pereira, Adam Simpkin, Marcus Hartmann, Daniel Rigden, Ronan Keegan, and Andrei Lupas. High-accuracy protein structure prediction in casp14. Proteins: Structure, Function, and Bioinformatics, 89(12): 16871699, 2021. Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. URL https://arxiv.org/abs/2209.14988. Zaixiang Qu, Ruizhe Chen, Dongyu Xue, Xiangxin Zhou, Xiangxiang Zeng, and Quanquan Gu. P(all-atom) is unlocking new path for protein design. bioRxiv, 2024. doi: 10.1101/2024.08.16.608235. URL https://www.biorxiv.org/content/10.1101/ 2024.08.16.608235v1. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 35053506, 2020. Tadeo Saldaño, Nahuel Escobedo, Julia Marchetti, Diego Javier Zea, Juan Mac Donagh, Ana Julia Velez Rueda, Eduardo Gonik, Agustina García Melani, Julieta Novomisky Nechcoff, Martín Salas, et al. Impact of protein conformational diversity on alphafold predictions. Bioinformatics, 38(10):27422748, 2022. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. Martin Steinegger and Johannes Söding. Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. Nature biotechnology, 35(11):10261028, 2017. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. ByteDance AML AI4Science Team, Xinshi Chen, Yuxuan Zhang, Chan Lu, Wenzhi Ma, Jiaqi Guan, Chengyue Gong, Jincai Yang, Hanyu Zhang, Ke Zhang, et al. Protenix-advancing structure prediction through comprehensive alphafold3 reproduction. BioRxiv, pages 202501, 2025. Yann Vander Meersche, Gabriel Cretin, Aria Gheeraert, Jean-Christophe Gelly, and Tatiana Galochkina. Atlas: protein flexibility description from atomistic molecular dynamics simulations. Nucleic acids research, 52(D1):D384D392, 2024. 16 Mihaly Varadi, Stephen Anyango, Mandar Deshpande, Sreenath Nair, Cindy Natassia, Galabina Yordanova, David Yuan, Oana Stroe, Gemma Wood, Agata Laydon, et al. Alphafold protein structure database: massively expanding the structural coverage of protein-sequence space with high-accuracy models. Nucleic acids research, 50(D1): D439D444, 2022. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Yuyang Wang, Ahmed Elhag, Navdeep Jaitly, Joshua Susskind, and Miguel Angel Bautista. Swallowing the bitter pill: Simplified scalable conformer generation. arXiv preprint arXiv:2311.17932, 2023. Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M. Wicky, Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Sheffler, Jue Wang, Preetham Venkatesh, Isaac Sappington, Susana Vázquez Torres, Anna Lauko, Valentin De Bortoli, Emile Mathieu, Sergey Ovchinnikov, Regina Barzilay, Tommi S. Jaakkola, Frank DiMaio, Minkyung Baek, and David Baker. De novo design of protein structure and function with rfdiffusion. Nature, 620 (7976):10891100, 2023a. doi: 10.1038/s41586-023-06415-8. URL https://www.nature.com/articles/s41586-023-06415-8. Joseph Watson, David Juergens, Nathaniel Bennett, Brian Trippe, Jason Yim, Helen Eisenach, Woody Ahern, Andrew Borst, Robert Ragotte, Lukas Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):10891100, 2023b. Jeremy Wohlwend, Mateo Reveiz, Matt McPartlon, Axel Feldmann, Wengong Jin, and Regina Barzilay. Minifold: Simple, fast, and accurate protein structure prediction. Transactions on Machine Learning Research. Jeremy Wohlwend, Gabriele Corso, Saro Passaro, Mateo Reveiz, Ken Leidal, Wojtek Swiderski, Tally Portnoi, Itamar Chinn, Jacob Silterra, Tommi Jaakkola, et al. Boltz-1: Democratizing biomolecular interaction modeling. bioRxiv, pages 202411, 2024. Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su, Zuofan Wu, Qi Xie, Bonnie Berger, et al. High-resolution de novo structure prediction from primary sequence. BioRxiv, pages 202207, 2022. Jingi Yeo, Yewon Han, Nicola Bordin, Andy Lau, Shaun Kandathil, Hyunbin Kim, Eli Levy Karin, Milot Mirdita, David Jones, Christine Orengo, et al. Metagenomic-scale analysis of the predicted protein structure universe. bioRxiv, pages 202504, 2025. Jason Yim, Andrew Campbell, Andrew YK Foong, Michael Gastegger, José Jiménez-Luna, Sarah Lewis, Victor Garcia Satorras, Bastiaan Veeling, Regina Barzilay, Tommi Jaakkola, and Frank Noé. Fast protein backbone generation with se(3) flow matching. arXiv preprint arXiv:2310.05297, 2023a. URL https://arxiv.org/abs/2310.05297. Jason Yim, Brian Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. Se (3) diffusion model with application to protein backbone generation. arXiv preprint arXiv:2302.02277, 2023b. Yang Zhang and Jeffrey Skolnick. Scoring function for automated assessment of protein structure template quality. Proteins: Structure, Function, and Bioinformatics, 57(4):702710, 2004."
        },
        {
            "title": "A Data Pipeline",
            "content": "We largely adopt the data pipeline implemented in Boltz-11 (Wohlwend et al., 2024), which is an open-source replication of AlphaFold3 (Abramson et al., 2024). Tab. 4 lists the input features for SimpleFold. It is noted that since SimpleFold does not apply MSA or template search, input features are also simplified compared to AlphaFold. In cropping larger proteins, we follow cropping algorithm that combines both spatial and contiguous cropping strategies introduced in previous work (Chakravarty and Porter, 2022; Abramson et al., 2024; Wohlwend et al., 2024). Following this setting, we set the neighborhood size in cropping uniformly between zero and 40 tokens to balance spatial and contiguous cropping. Table 4 Input features to SimpleFold. Feature residue_index token_index restype esm_embed noised_pos ref_pos ref_mask ref_element ref_charge ref_atom_name_chars ref_space_uid Shape Description Residue number in the tokens original input chain. Token number. Increases monotonically. One-hot encoding of the sequence: 20 amino acids + unknown. [Nr] [Nr] [Nr] [Nr, 37, 2560] Protein sequence embedding from all layers in ESM2-3B. Noised atom positions, xt in Å (random rotation applied). [Na, 3] Atom positions in the reference conformer in Å (no rotation applied). [Na, 3] Mask indicating atoms used in the reference conformer. [Na] One-hot encoding of the element number for each atom. [Na, 128] Charge for each atom in the reference conformer. [Na] One-hot encoding of atom names in the reference conformer. [Na, 4, 64] Encoding of the residue index associated with reference conformer. [Na] time length [1] [1] Timestep in flow process. Number of residues, Nr. During training, atomic positions of protein are mean centered and augmented with random rotation. After centering, we scale the position by global factor of 1/16 to make the atomic positions live in the [1, 1] interval. Similarly, we also scale ref_pos by 1/5 to standardize the positions in reference conformers."
        },
        {
            "title": "B Model Architecture",
            "content": "B.1 Architecture Comparison to AlphaFold2 Fig. 5 depicts the comparison of major compute blocks in AlphaFold2 and SimpleFold (Fig. 5(a) borrowed from original AlphaFold2 paper (Chakravarty and Porter, 2022)). As shown in the figure, SimpleFold does not rely on either explicit pair representations or MSA. Instead, we only keep sequence-level representation and leverage embeddings extracted from pretrained PLM (i.e., ESM2 (Lin et al., 2023b)). Compared AlphaFolds Evoformer block which includes expensive triangle attention to interact between pair and sequence representations, SimpleFold follows simple DiT architecture (Peebles and Xie, 2023) which is more computationally efficient. B.2 Model Configurations Table 5 lists the configurations of different SimpleFold models from the smallest 94M to largest 2.86B. In implementation, we apply the same architecture for the atom encoder and atom decoder. Though AlphaFold2 is similar to our smallest SimpleFold-100M in terms of number of parameters (both are around 95M), its forward Gflops are much higher than our largest SimpleFold-3B ( 30Tflops vs. 1.4Tflops). This is 1https://github.com/jwohlwend/boltz Figure 5 Major neural network blocks of (a) Evoformer in AlphaFold2, and (b) Transformer with adaptive layer in SimpleFold. because AlphaFold2 relies on expensive triangle update as well as explicit modeling pair representations from MSA. SimpleFold, on the other hand, is built on general-purposed transformer blocks which are much more computationally efficient. Table 5 Configurations of different variants of SimpleFold with comparison to AlphaFold2 and ESMFold in number of parameters and forward Gflops. Model AlphaFold2 ESMFold SimpleFold-100M SimpleFold-360M SimpleFold-700M SimpleFold-1.1B SimpleFold-1.6B SimpleFold-3B # Params Gflops Dim. # Heads # Blocks Dim. # Heads # Blocks Atom Enc. / Dec. Residue Trunk 95M 30935.0 710M 3399.7 94M 360M 687M 1.11B 1.58B 2.86B 66.5 189.9 310.4 496.0 750.0 1382.4 - - 256 256 256 384 512 640 - - 4 4 4 6 8 10 - - 1 2 2 2 3 4 - - 768 1024 1152 1280 1536 2048 - - 12 16 16 20 24 32 - - 8 18 28 36 36 36 B.3 Grouping and Ungrouping Fig. 6 illustrates how grouping and ungrouping operations are conducted in SimpleFold. In grouping, we conduct average pooling over atoms tokens from one residue to obtain residue token. While in ungrouping, we replicate the same updated residue tokens to all atoms within the residue."
        },
        {
            "title": "C Training and Inference",
            "content": "C.1 Additional Training Details Timestep Resampling. logit-normal distribution LN is given: In training, we resample timestep with p(t) = 0.02 U(0, 1) + 0.98 LN(0.8, 1.7), and 19 Figure 6 Illustration of (a) grouping and (b) ungrouping operations in SimpleFold. LN(t; m, s) = 1 t(1 t)s exp 2π (logit(t)) m)2 2s . (C.1) We set = 0.8, = 1.7 to sample timestep more densely around = 1 so the model better learns to capture the refined details as shown in Fig 7. Figure 7 Distribution of resampled timestep compared to uniform distribution. Rigid alignment. Following Abramson et al. (2024); Wohlwend et al. (2024), we apply rigid alignment between one-step denoising atomic coordinates and true coordinates before computing the flow-matching MSE loss (Eq. 2.2) in training to reduce the loss variance. In particular, ˆx(xt) is estimated through one step Euler, i.e., ˆx(xt) = xt + (1 t) vθ(xt, s, t), and the true coordinates is aligned with the denoised coordinates Figure 8 Illustration of pLDDT module training. through Kabsch algorithm (Wohlwend et al., 2024) to obtain x. The velocity target is re-calculated by interpolating the aligned and noise ϵ. Though such rigid alignment strategy helps in faster convergence, it does not make significant difference in final performance as also mentioned in Wohlwend et al. (2024). LDDT Loss. Following AlphaFold3 (Abramson et al., 2024), the nonlinear function σ in Eq. 2.3 is given as: σ(x) = 1 4 (sigmoid(0.5 x) + sigmoid(1 x) + sigmoid(2 x) + sigmoid(4 x)), (C.2) which mimics the how LDDT is computed for evaluation. We set the cutoff distance = 15Åin Eq. 2.3, which is the typical setting for the LDDT metric. Batching. Tab. 6 lists the detailed setting of training batch for different model sizes. Table 6 Settings of pre-training and finetuning batches for different SimpleFold models. Model # Copies Bc # Prot. Bp Eff. Bsz. # Copies Bc # Prot. Bp Eff. Bsz. Pre-training Finetuning SimpleFold-100M SimpleFold-360M SimpleFold-700M SimpleFold-1.1B SimpleFold-1.6B SimpleFold-3B 16 16 16 16 8 24 32 32 32 32 128 128 512 512 512 512 1024 8 8 8 8 4 12 32 32 32 32 128 128 256 256 256 256 512 1536 pLDDT Training. Fig. 8 shows the training pipeline for pLDDT module. In particular, we use fully trained SimpleFold-1.6B to extract residue tokens. 21 C.2 Additional Inference Details During inference, we use the EulerMaruyama integrator shown in Eq. 2.5 starting from tϵ = 0.0001 and the number of time steps is set to be 500 without additional statement. In practice, we set η = 0.01 in w(t) = 2(1t) for numerical stability. And following (Geffner et al., 2025), we set w(t) = 0 for 0.99 and t+η discretize the time interval logarithmically from = tϵ to = 1. After each sampler step, we rescale the center of all atomic positions to origin to align with the training setting. At the end of the flow trajectory, we rescale the coordinates by multiplying 16 to map the protein structure back to Å scale. C.3 Inference Time Tab. 7 lists inference time of SimpleFold in comparison to baseline models, AlphaFold2, ESMFold, AlphaFlow, and ESMFlow. SimpleFold shows advantage in inference efficiency especially when sequence is longer (e.g., 1024). Also, ESM2 adds little overhead in inference. Table 7 Inference time (in seconds) of different models. All models are benchmarked on single H100 with batch size 1. Steps/Recycles 64 Sequence length 256 512 AlphaFold2 ESMFold AlphaFlow ESMFlow ESM2 SimpleFold-100M SimpleFold-360M SimpleFold-700M SimpleFold-1.1B SimpleFold-1.6B SimpleFold-3B SimpleFold-100M SimpleFold-360M SimpleFold-700M SimpleFold-1.1B SimpleFold-1.6B SimpleFold-3B 3 3 10 10 1 200 200 200 200 200 200 500 500 500 500 500 500 3.7 3.0 1.1 1.100 12.3 10.0 3.7 3.7 <0.1 <0. 8.0 1.745 26.6 5.8 0.1 3.6 7.2 9.8 12.6 13.0 14.0 9.0 18.0 24.5 31.5 32.5 35.4 3.6 7.4 10.2 12.8 13.0 14.0 9.0 18.5 25.5 32.0 32.5 35.4 3.8 7.6 10.4 12.8 13.8 15.6 9.5 19 26 32 34.5 37.2 25.5 7.4 85.2 24.6 0.2 4.2 8 11.4 15 18.2 27.8 10.5 20 28.5 37.5 45.5 72. 1024 111.5 43.6 371.7 145.5 0.4 5.6 11.6 16.6 22.2 29.4 44.6 14 29 41.5 55.5 73.5 111."
        },
        {
            "title": "D Evaluation",
            "content": "D.1 Folding Baselines AlphaFold2. The AlphaFold2 (AF2) baseline was established using the official implementation wrapped using ColabFold (Mirdita et al., 2022). We utilized the standard released weights with three model recycles. We adopt the MMSeqs2 engine (Steinegger and Söding, 2017) to search for multiple sequence alignments (MSAs) as model input. No template or Amber relax is applied to the predictions. RoseTTAFold. We utilized the release models of RoseTTAFold (Baek et al., 2021) via ColabFold (Mirdita et al., 2022), employing its publicly available pre-trained model weights. We keep the default configurations of both models for inference and use MMseqs2(Steinegger and Söding, 2017) for MSA search. In specific, we use the proposed pipeline in the ColabFold Notebook including the end-to-end 3-track model forward, TRFold refinement and side-chain packing using SCRWL4 (Krivov et al., 2009). RoseTTAFold2. Experiments of RoseTTAFold2 (Baek et al., 2023) are similarly conducted via ColabFold (Mirdita et al., 2022) with the pre-trained model weights. We follow the default inference configuration as 22 described in the RoseTTAFold2 (Baek et al., 2023) official repository by setting -n_recycles=3, -nseqs=256 and -subcrop=-1. ESMFold. Our experiments employed the ESMFold implementation from ColabFold (Mirdita et al., 2022) and model checkpoints from (Lin et al., 2023b). We used the pretrained esmfold_v1 model for inference as recommended by the authors, the performance of which is better than the esmfold_v0 model which was used for experiments in ESM2 paper (Lin et al., 2023b). We set the number of recycles to be 3, aligned with the AF2 setting. OmegaFold. The implementation of OmegaFold used was based on the original repository (Wu et al., 2022). We relied on the default pre-trained model shipped with the release. The inference pipeline is strictly adhering to the default setting. EigenFold. The EigenFold implementation as provided (Jing et al., 2023) was leveraged for our baseline runs. We utilized the standard pre-trained weights for decoder and make node/edge embeddings from OmegaFold as instructed by the authors. Defaults settings applied during inference included -alpha 1 -beta 3 -elbo_step 0.2 . AlphaFlow/ESMFlow. We utilized the codebase for AlphaFlow and ESMFlow released by the authors of (Jing et al., 2024a), employing its pre-trained model checkpoints on PDB data (with suffix pdb_base_202402.pt). The setup largely mirrored the default configurations for both models described in the repository, specifically by setting tmax to be 1.0 and the flow steps to be 10. ESM3/ESMDiff. The implementation of ESMDiff and ESM3 used was based on the ESMDiff original repository (Lu et al., 2024a). No additional training was performed; the provided pre-trained model was used directly (both pretrained ESM3 and finetuned ESMDiff) to predict the structures for each target. We used standard hyperparameters as listed by the authors, including num_steps=25, T=1.4, top_p=0.9. D.2 PDB Cutoff Date Tab. 8 lists the PDB cutoff date of most baselines in training. SimpleFold uses May 1, 2020 as the cutoff date following most baselines. Table 8 Cutoff date of PDB for training. Model PDB cutoff date AlphaFold2 Chakravarty and Porter (2022) RoseTTAFold2 (Baek et al., 2023) ESMFold (Lin et al., 2023b) EigenFold (Jing et al., 2023) AlphaFlow (Jing et al., 2024a) ESMFlow (Jing et al., 2024a) ESM3 (Hayes et al., 2025) ESMDiff (Lu et al., 2024a) SimpleFold May 1, 2018 April 30, 2020 May 1, 2020 April 30, 2020 May 1, 2018 May 1, 2020 May 1, 2020 May 1, May 1, 2020 D.3 Estimation of Gflops We leverage DeepSpeed (Rasley et al., 2020) library to estimate forward flops for SimpleFold as well as baseline models. In particular, we use deepspeed.profiling.flops_profiler.get_model_profile 2 function to get the compute profile for the models. In estimating the flops, we set the number of residues to be 256 and number of atoms to be 2304, namely, 9 atoms per residue. 2https://www.deepspeed.ai/tutorials/flops-profiler/ 23 D.4 Targets in Folding Tasks List of 183 targets in CAMEO22 (Haas et al., 2018): 7dz2-C, 7eoz-A, 7fac-A, 7fgb-A, 7fgp-A, 7fh0-B, 7lt7-A, 7lx4-A, 7m1z-B, 7mj3-A, 7n3y-A, 7n6h-A, 7n99-A, 7oj1-A, 7oj2-A, 7oju-A, 7pc1-A, 7pce-A, 7pcv-A, 7pk5-A, 7pkw-A, 7pl4-A, 7pl7-A, 7pqi-A, 7pqw-A, 7pup-A, 7pwe-A, 7q6d-A, 7q6g-A, 7q83-D, 7q9e-C, 7qau-A, 7qpe-A, 7qsw-B, 7qsw-C, 7qsx-A, 7qys-L, 7r08-E, 7r0o-B, 7r3w-D, 7r49-B, 7rlk-D, 7rmy-A, 7roa-A, 7rpn-A, 7rt7-D, 7rup-A, 7ruq-A, 7s03-A, 7s8k-B, 7sao-A, 7sbd-H, 7sfn-B, 7skh-B, 7skj-A, 7snc-A, 7snj-A, 7soo-A, 7spn-A, 7sz2-B, 7t12-B, 7t1j-B, 7t5w-B, 7te2-A, 7tgi-B, 7th2-C, 7tif-A, 7tol-A, 7tvw-A, 7u04-H, 7u0e-H, 7uav-A, 7ug9-A, 7upm-A, 7upv-A, 7uqv-D, 7uwg-C, 7ux0-A, 7uxt-A, 7v2s-B, 7v5f-A, 7v8t-A, 7vbo-A, 7vd7-B, 7vf3-B, 7vfc-A, 7vfq-C, 7vi8-B, 7vil-A, 7vma-A, 7vmf-A, 7vmh-C, 7vp3-C, 7vp6-D, 7vpu-A, 7vqk-A, 7vr2-A, 7vrf-A, 7vt4-A, 7vt5-A, 7vyu-A, 7w06-A, 7w16-A, 7w42-B, 7w52-B, 7w6x-A, 7w7h-E, 7w89-A, 7w8u-A, 7wa9-A, 7wbn-A, 7wf6-A, 7wf8-B, 7wf9-A, 7wfx-A, 7whf-G, 7wj0-A, 7wjt-B, 7wq5-A, 7wua-A, 7x0g-A, 7x0q-A, 7x0r-B, 7x15-A, 7x1k-A, 7x7w-A, 7x8c-B, 7xce-A, 7xjt-B, 7xtm-B, 7y0i-A, 7y39-B, 7y3k-A, 7y3w-A, 7y4n-A, 7y78-B, 7y79-B, 7y8u-E, 7y9b-A, 7ycv-A, 7ymo-A, 7yrt-C, 7yta-B, 7yvt-B, 7yvz-A, 7ywq-A, 7z06-A, 7zc8-A, 7zgi-B, 7zgm-A, 7zk1-A, 7zty-A, 7zva-A, 7zw9-A, 8a28-A, 8a4a-A, 8ag9-A, 8ajp-A, 8b26-A, 8b55-A, 8b5t-A, 8b5v-A, 8b73-A, 8cwp-A, 8cxl-A, 8d03-A, 8d08-D, 8d7f-A, 8day-A, 8dgg-A, 8di0-C, 8di1-A, 8dkr-B, 8doa-A, 8ds5-A, 8dt0-A, 8dt6-C, 8dte-A, 8dys-A, 8e8t-B, 8e8u-C, 8gxf-B, 8qcw-A List of 70 targets in CASP14 (Pereira et al., 2021): T1024, T1025, T1026, T1027, T1028, T1029, T1030, T1031, T1032, T1033, T1034, T1035, T1036s1, T1037, T1038, T1039, T1040, T1041, T1042, T1043, T1045s1, T1045s2, T1046s1, T1046s2, T1047s1, T1047s2, T1048, T1049, T1050, T1052, T1053, T1054, T1055, T1056, T1057, T1058, T1060s2, T1060s3, T1061, T1062, T1064, T1065s1, T1065s2, T1067, T1068, T1070, T1072s1, T1073, T1074, T1076, T1078, T1079, T1080, T1082, T1083, T1084, T1087, T1088, T1089, T1090, T1091, T1092, T1093, T1094, T1095, T1096, T1098, T1099, T1100, T1101 D.5 Evaluation Pipeline In evaluation for folding tasks (Tab. 1), all metrics for all-atom models are computed using Folding. OpenStructure (Biasini et al., 2013) unless mentioned otherwise. In particular, we deploy the official docker image of OpenStructure 2.9.13 and use the following command to evaluate the structures. ost compare-structures -m {MODEL_FILE} -r {REFERENCE_FILE} -o {OUTPUT_FILE} --fault-tolerant --min-pep-length 4 --lddt --bb-lddt --rigid-scores --tm-score Notably, for protein folding / generation models that cannot output all-atom structures, we instead adopt the TM-score (Zhang and Skolnick, 2004) for evaluation because the OpenStructure pipeline fails in those cases. We compile the TMscore.cpp c++ source code and compare two structures as follows: TMscore -seq {MODEL_FILE} {REFERENCE_FILE} MD ensemble generation. For ATLAS MD ensemble generation (Tab. 2), we base our evaluation pipeline on the dataset split and benchmarking metrics used in previous studies (Jing et al., 2024a,b; Lu et al., 2025), which cover from predicting flexibility to ensemble observables. To obtain the predicted ensemble, = 250 (Jing et al., 2024a) conformations are sampled from baselines and SimpleFold for each of the 82 test targets, where the median across all targets is reported for each metric. In specific, we report the Pearsons correlation for pairwise RMSD, global and per-target RMSF; the root mean of 2-Wasserstein distance (W2 distance) and its translation and variance contribution, W2 distance between predicted and true ensembles regarding the 3https://git.scicore.unibas.ch/schwede/openstructure/ first two principal components from PCA by either MD or joint (MD and predicted), and the percentage of samples with cosine similarity > 0.5 between the top principal components of predicted and true ensemble; for the observables, we evaluate the Jaccard similarity (J) of the weak contacts, transient contacts, and exposed residue as well as the Spearman correlation ρ of the exposed mutual information (MI) matrix. We refer the readers to Jing et al. (2024a) for more detailed definition of these metrics. For ESMDiff (Lu et al., 2024a), the Jaccard similarity of exposed residue and the Spearman correlation of exposed MI are left empty because it only generates backbone conformation. In order to evaluate the two-state conformation prediction tasks (Tab. 3), we follow Two-state prediction. the evaluation pipeline in EigenFold (Jing et al., 2023): the global and per-target residue flexbility (in terms of RMSD Pearsons correlation r) is calculated after sequence alignment and structural superposition. The TM-ensemble score (at ensemble size 5 following Jing et al. (2023)) is calculated by computing the maximum TM-score (Zhang and Skolnick, 2004) between the ensemble and either ground truth conformation, and averaged across both. We use the same command as above to compute the TMscore."
        },
        {
            "title": "E Additional Experiments",
            "content": "E.1 MD Ensemble Generation Tab. 9 lists the results of SimpleFold and SimpleFold-MD on MD ensemble generation of ATLAS. In particular, no tuning is applied to SimpleFold whereas SimpleFold-MD is tuned on ATLAS training data. It is shown that on MD ensemble generation, SimpleFold also benefits from scaling, namely, larger SimpleFold and SimpleFold-MD achieve better performance. Table 9 Evaluation of SimpleFold (SF) of different sizes on MD ensembles. SF-100M SF-360M SF-700M SF-1.1B SF-1.6B SF-3B SF-MD-100M SF-MD-360M SF-MD-700M SF-MD-1.1B SF-MD-1.6B SF-MD-3B No Tuning Tuned Pairwise RMSD Global RMSF Per target RMSF RMWD RMWD trans contri RMWD var contri MD PCA W2 Joint PCA W2 % PC sim > 0.5 Weak contacts Transient contacts Exposed residue Exposed MI matrix ρ 0.17 0.23 0.59 5.41 4.83 2.24 1.79 4.49 30 0.47 0.25 0.47 0.24 0.21 0.27 0.63 4.36 4.02 1.76 1.54 2.89 29 0.43 0.30 0.48 0.23 0.29 0.33 0.65 4.35 3.95 1.69 1.43 2.82 28 0.43 0.31 0.46 0.24 0.30 0.36 0.64 4.26 3.84 1.68 1.58 2.91 32 0.44 0.30 0.50 0.24 0.38 0.42 0.63 4.20 3.79 1.74 1.57 2.65 34 0.36 0.28 0.41 0. 0.44 0.45 0.60 4.22 3.74 1.75 1.62 2.59 37 0.36 0.27 0.39 0.14 0.19 0.26 0.62 5.88 5.32 2.21 1.86 4.78 28 0.52 0.25 0.55 0.29 0.27 0.34 0.67 4.71 4.22 1.91 1.34 3.36 30 0.55 0.32 0.62 0.31 0.30 0.38 0.67 4.56 4.19 1.80 1.51 3.37 37 0.57 0.33 0.60 0.33 0.32 0.39 0.68 4.12 3.60 1.79 1.39 2.85 37 0.58 0.35 0.62 0.35 0.40 0.45 0.68 4.07 3.44 1.78 1.37 2.29 37 0.58 0.36 0.63 0. 0.45 0.48 0.67 4.17 3.40 1.88 1.34 2.18 38 0.56 0.34 0.60 0.32 E.2 LDDT Loss LDDT loss plays an important role in SimpleFold training. In practice, we find LDDT loss is required to generate structures with refined local atomic positions, which largely affects the LDDT metric in folding tasks. We also find that in the second training phase when finetuning the pretrained model on high-quality data, PDB and SwissProt (filtered at pLDDT > 85). Adding loss weight α = 1 + 8ReLU(t 0.5) (Eq. 3) helps getting better results than keeping α = 1 as pretraining. Tab. 10 shows the effect of different LDDT loss weighting strategies in finetuning. Applying loss weight schedule 1 + 8 ReLU(t 0.5) achieves best overall performance. Table 10 Ablation of LDDT loss weighting on CAMEO22. Model SimpleFold-700M SimpleFold-700M SimpleFold-700M 1 + 8 ReLU(t 0.5) 0.0 1. α(t) TM-score GDT-TS 0.785 / 0.845 0.831 / 0.907 0.785 / 0.844 0.831 / 0.913 0.784 / 0.844 0.826 / 0.904 LDDT 0.711 / 0.746 0.767 / 0.797 0.762 / 0.788 LDDT-Cα 0.847 / 0.882 0.846 / 0.884 0.848 / 0.884 RMSD 4.445 / 2.423 4.586 / 2.742 4.476 / 2.588 E.3 Inference Settings Tab. 11, Tab. 12, and Tab. 13 show the ablation of inference settings of SimpleFold-700M on CAMEO22, CASP14, and Apo/Fold-switch, respectively. By default, we set number of steps to 500, τ = 0.01, and w(t) = 1t for folding tasks while set τ = 0.8 for multi-state tasks to encourage stochasticity in inference. Table 11 Ablation of inference settings on CAMEO22. # Steps 500 500 500 500 500 500 500 500 500 500 250 200 150 100 50 τ 0.0 0.01 0.02 0.05 0.1 0.2 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0. w(t) 1t 1t 1t 1t 1t 1t tan ( π(1t) 1 1t2 1t 1t 1t 1t 1t 1t 2 ) log time TM-score GDT-TS 0.790 / 0.844 0.788 / 0.845 0.788 / 0.845 0.787 / 0.848 0.785 / 0.839 0.781 / 0.832 0.788 / 0.845 0.768 / 0.818 0.788 / 0.841 0.790 / 0.848 0.785 / 0.850 0.788 / 0.843 0.785 / 0.845 0.779 / 0.839 0.605 / 0.599 0.831 / 0.918 0.829 / 0.915 0.831 / 0.919 0.831 / 0.913 0.830 / 0.913 0.826 / 0.909 0.833 / 0.917 0.820 / 0.904 0.829 / 0.916 0.832 / 0.919 0.828 / 0.916 0.831 / 0.915 0.826 / 0.912 0.821 / 0.902 0.654 / 0.618 T T T T Table 12 Ablation of inference settings on CASP14. # Steps 500 500 500 500 500 500 500 500 500 500 250 200 150 100 50 τ 0.0 0.01 0.02 0.05 0.1 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 w(t) 1t 1t 1t 1t 1t 1t tan ( π(1t) 1 1t2 1t 1t 1t 1t 1t 1t 2 ) log time TM-score GDT-TS 0.591 / 0.678 0.591 / 0.668 0.589 / 0.667 0.587 / 0.665 0.585 / 0.668 0.585 / 0.647 0.591 / 0.660 0.572 / 0.642 0.590 / 0.669 0.586 / 0.671 0.593 / 0.683 0.588 / 0.699 0.572 / 0.624 0.558 / 0.584 0.402 / 0.305 0.684 / 0.762 0.680 / 0.767 0.677 / 0.770 0.675 / 0.778 0.675 / 0.779 0.673 / 0.780 0.683 / 0.768 0.671 / 0.737 0.680 / 0.767 0.677 / 0.763 0.679 / 0.777 0.677 / 0.767 0.657 / 0.742 0.633 / 0.680 0.481 / 0.358 T T T LDDT 0.777 / 0.815 0.775 / 0.809 0.775 / 0.808 0.775 / 0.808 0.773 / 0.807 0.768 / 0.805 0.775 / 0.803 0.005 / 0.001 0.775 / 0.808 0.776 / 0.811 0.776 / 0.808 0.777 / 0.808 0.774 / 0.806 0.769 / 0.802 0.615 / 0.641 LDDT-Cα 0.851 / 0.887 0.850 / 0.886 0.850 / 0.886 0.849 / 0.885 0.848 / 0.884 0.845 / 0.882 0.848 / 0.884 0.826 / 0.857 0.849 / 0.885 0.851 / 0.886 0.849 / 0.884 0.850 / 0.888 0.850 / 0.885 0.847 / 0.884 0.717 / 0.746 RMSD 4.497 / 2.410 4.557 / 2.423 4.501 / 2.519 4.461 / 2.429 4.574 / 2.452 4.597 / 2.558 4.504 / 2.403 4.665 / 2.443 4.571 / 2.421 4.473 / 2.427 4.626 / 2.445 4.417 / 2.491 4.581 / 2.478 4.922 / 2.450 10.971 / 11. LDDT 0.628 / 0.662 0.630 / 0.674 0.629 / 0.676 0.624 / 0.661 0.621 / 0.662 0.617 / 0.652 0.629 / 0.638 0.005 / 0.002 0.626 / 0.668 0.626 / 0.656 0.627 / 0.677 0.624 / 0.660 0.625 / 0.641 0.615 / 0.657 0.452 / 0.374 LDDT-Cα 0.713 / 0.762 0.714 / 0.763 0.712 / 0.758 0.711 / 0.767 0.706 / 0.766 0.708 / 0.764 0.714 / 0.753 0.691 / 0.742 0.712 / 0.756 0.709 / 0.758 0.715 / 0.764 0.713 / 0.758 0.709 / 0.750 0.701 / 0.759 0.554 / 0.465 RMSD 9.184 / 4.226 9.289 / 4.431 9.319 / 4.645 9.521 / 4.867 9.391 / 5.029 9.167 / 5.018 8.787 / 4.294 9.414 / 4.876 9.313 / 4.388 9.317 / 4.281 9.374 / 4.765 9.363 / 4.544 11.305 / 7.234 12.370 / 6.615 17.792 / 17.868 Table 13 Ablation of inference settings on Apo and Fold-switch. τ Res. flex. (global) Res. flex. (per-target) TM-ens Res. flex. (global) Res. flex. (per-target) TM-ens 0.2 0.4 0.6 0.8 1. 0.466 0.538 0.531 0.552 0.562 Apo/holo 0.484 / 0.478 0.501 / 0.512 0.513 / 0.510 0.524 / 0.538 0.525 / 0.520 0.868 / 0.901 0.869 / 0.901 0.870 / 0.901 0.870 / 0.899 0.867 / 0.896 0.297 0.314 0.302 0.307 0.319 Fold-switch 0.281 / 0.245 0.305 / 0.228 0.313 / 0.289 0.328 / 0.310 0.337 / 0.329 0.699 / 0.750 0.697 / 0.748 0.695 / 0.734 0.693 / 0.713 0.687 / 0."
        },
        {
            "title": "F Additional Visualization",
            "content": "F.1 Folding Figure 9 Examples of folding results from SimpleFold with ground truth shown in light aqua and prediction in deep tea (first row from CAMEO22 targets and second row from CASP14 targets). F.2 Ensemble Generation Figure 10 Examples of ensemble generation results from SimpleFold. We align 5 generated conformations of the same protein for visulization. F.3 Failure Cases Fig. 11 shows some examples of failure cases from CAMEO22 and CASP14. In particular, we show predictions with TM-score smaller than 0.6 and also include predictions from ESMFold (Lin et al., 2023b). In these shown cases, SimpleFold mostly predicts the secondary structures correctly. However, the relative positions between the different secondary structure domains are not well modeled. Interestingly, this failure mode can also be observed in ESMFold, e.g., 7SZ2-B and 7WF9-A. We attribute this to the ESM2 embedding shared by SimpleFold and ESMFold. This indicates future direction to build more powerful protein language models for representation learning that further benefits protein folding models. 27 Figure 11 Examples of failure cases (TM-score < 0.6) of SimpleFold predictions with ground truth shown in light aqua and prediction in deep tea (first row from CAMEO22 targets and second row from CASP14 targets). We also include predictions from ESMFold for comparison. Apple and the Apple logo are trademarks of Apple Inc., registered in the U.S. and other countries and regions."
        }
    ],
    "affiliations": [
        "Apple"
    ]
}