{
    "paper_title": "A Refined Analysis of Massive Activations in LLMs",
    "authors": [
        "Louis Owen",
        "Nilabhra Roy Chowdhury",
        "Abhay Kumar",
        "Fabian Güra"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Motivated in part by their relevance for low-precision training and quantization, massive activations in large language models (LLMs) have recently emerged as a topic of interest. However, existing analyses are limited in scope, and generalizability across architectures is unclear. This paper helps address some of these gaps by conducting an analysis of massive activations across a broad range of LLMs, including both GLU-based and non-GLU-based architectures. Our findings challenge several prior assumptions, most importantly: (1) not all massive activations are detrimental, i.e. suppressing them does not lead to an explosion of perplexity or a collapse in downstream task performance; (2) proposed mitigation strategies such as Attention KV bias are model-specific and ineffective in certain cases. We consequently investigate novel hybrid mitigation strategies; in particular pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT) successfully balances the mitigation of massive activations with preserved downstream model performance in the scenarios we investigated. Our code is available at: https://github.com/bluorion-com/refine_massive_activations."
        },
        {
            "title": "Start",
            "content": "Louis Owen, Nilabhra Roy Chowdhury, Abhay Kumar, Fabian Güra BluOrion {louis.owen, nilabhra.chowdhury, abhay.kumar, fabian.guera}@bluorion.com March 28,"
        },
        {
            "title": "Abstract",
            "content": "Motivated in part by their relevance for low-precision training and quantization, massive activations in large language models (LLMs) have recently emerged as topic of interest. However, existing analyses are limited in scope, and generalizability across architectures is unclear. This paper helps address some of these gaps by conducting an analysis of massive activations across broad range of LLMs, including both GLU-based and non-GLU-based architectures. Our findings challenge several prior assumptions, most importantly: (1) not all massive activations are detrimental, i.e. suppressing them does not lead to an explosion of perplexity or collapse in downstream task performance; (2) proposed mitigation strategies such as Attention KV bias are model-specific and ineffective in certain cases. We consequently investigate novel hybrid mitigation strategies; in particular pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT) successfully balances the mitigation of massive activations with preserved downstream model performance in the scenarios we investigated. Our code is available at: https://github.com/bluorion-com/refine_ massive_activations."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have revolutionized natural language processing, but their internal dynamics remain poorly understood. [1] highlights striking phenomenon: small subset of activations exhibit values significantly larger than others, and this behavior is observed across various LLMs. These activations, referred to as massive activations, function as implicit bias terms, concentrating attention on specific tokens. Despite these extensive contributions, several critical questions remain unanswered. In particular, there seems to be inconsistency across experimental setupsfor instance, fixed bias analyses are confined to LLaMA [2], Attention KV bias mitigation is tested only on GPT-2 [3], and by default, all analyses are conducted without leading Beginning-of-Sentence (BOS) token, with its inclusion considered only as part of ablation studies. Massive activations are not merely an academic curiosity; they have significant practical implications for model deployment and optimization. For instance, recent investigations into Gemma-3 [4] revealed that its massive activations can cause numerical instability during inference and fine-tuning with float16 precision [5]. Specifically, the activations exceeded float16s maximum range of 65,536, resulting in infinity values and NaN gradients. This issue was traced back to the interaction between successive layer normalization operations in the decoder layers, where the output layer norm produced excessively large values. [6] further extended the discussion of massive activations by linking them to challenges in quantization. Specifically, it highlighted how excessive magnitudes of activationsreferred to as activation spikesin Gated Liner Units (GLU) variants of Feed Forward networks (FFNs) cause severe local quantization errors, degrading the performance of quantized LLMs. The authors observed systematic pattern: activation spikes occur in the FFNs of specific layers, particularly in early and late layers, and are dedicated to few tokens rather than being spread across sequences. To mitigate these issues, they proposed empirical methods such as Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP), and demonstrated their effectiveness in isolating activation spikes during quantization of models that exhibit massive activations pre-quantization. Refined Analysis of Massive Activations in LLMs Preprint Non-GLU-based Models Intervention WikiText Original Set to zero Set to mean 30.42 30.57 30.42 Intervention WikiText Original Set to zero Set to mean 10.89 109.65 10.89 GPT-2 C4 37.61 37.71 37.61 Falcon-7B C4 21.89 154.28 21.89 PG-19 53.72 53.85 53. PG-19 22.07 167.36 22.05 WikiText 64.40 64.30 64.40 WikiText 4.91 5.28 4.91 Phi-2-2.7B C4 87.96 87.93 87.96 Falcon-2-11B C4 9.84 10.84 9.85 PG-19 WikiText 64.16 64.17 64.16 10.99 10.99 10. OPT-6.7B C4 14.65 14.64 14.65 PG-19 13.49 13.48 13.49 PG-19 9.87 11.86 9.88 GLU-based Models Intervention WikiText Original Set to zero Set to mean 5.13 8976.89 5. Intervention WikiText Original Set to zero Set to mean 6.40 2.76 1024 6.40 Intervention WikiText Original Set to zero Set to mean 9.79 10.21 9.79 Intervention WikiText Original Set to zero Set to mean 7.14 15549.53 7. Intervention WikiText Original Set to zero Set to mean 4.99 2.62 107 4.99 LLaMA-2-7B C4 7.65 9974.18 7.66 Gemma-7B C4 10.71 4.77 1025 10.71 Gemma-3-1B C4 15.48 16.07 15.48 OLMo-7B-0724 C4 10.60 13510.87 10.60 Mistral-7B-v0.3 C4 8.48 3.74 107 8.48 PG-19 8.11 6726.09 8.11 PG-19 11.39 6.41 1027 11.38 PG-19 16.56 21.01 6. PG-19 10.29 7573.01 10.29 PG-19 8.47 2.92 107 8.47 PG-19 11.97 11442.38 11.97 WikiText 9.07 6249.02 9.07 LLaMA-3.2-1B C4 14.63 3874.56 14.63 Gemma-2-2B C4 13.18 144.34 13.18 Gemma-3-4B C4 11.74 301.86 11.81 OLMo-2-1124-7B C4 12.35 - - WikiText 8.05 101.73 8. WikiText 6.88 172.25 11.79 WikiText 5.76 - - PG-19 WikiText 14.93 2789.20 14.93 7.26 18440.82 7.26 PG-19 WikiText 13.67 144.99 13.69 6.38 7.09 6. PG-19 WikiText 11.69 325.20 15.92 5.49 5.52 5.49 PG-19 WikiText 10.29 - - 6.08 6.23 6.08 LLaMA-3.2-3B C4 11.88 17798.59 11.88 Gemma-2-9B C4 11.33 12.84 11.33 Gemma-3-12B C4 10.03 10.09 10.05 Phi-4-14B C4 11.88 12.13 11.88 PG-19 10.55 11.31 10. PG-19 8.99 9.16 9.03 PG-19 9.63 10.04 9.63 Table 1: Perplexity scores for various pre-trained LLMs subject to massive activation intervention with the BOS token included. - indicates that no massive activations were found. Results are colorcoded to indicate the level of impact: purple denotes highly detrimental effects, while orange signifies medium detrimental effects. For consistency, we used context length of 4096 tokens for all models, except for GPT-2 and OPT-6.7B, where context lengths of 1024 and 2048 tokens were used, respectively, due to model limitations. By default, all analyses were conducted using float16 precision, except for Gemma-7B and all Gemma-3 models, which required float32. For Gemma-7B, this was due to the original weights being in float32, while for the Gemma-3 models, the excessively high activation magnitudes exceeded the range supported by float16. This paper addresses some gaps in literature by conducting comprehensive analysis of massive activations across broad range of LLMs, including both GLU-based and non-GLU-based architectures. We investigate several mitigation strategies aimed at managing massive activations, such as Attention KV bias [1], Target Variance Rescaling (TVR) [7], and Dynamic Tanh (DyT) [8]. These strategies are evaluated not only for their effectiveness in reducing activation magnitudes, but also for their impact on downstream task performance and attention dynamics. By systematically analyzing these approaches, we aim to advance the understanding of massive activations and their implications for LLM design. Furthermore, this work identifies unresolved questions and proposes directions for future research, paving the way for more robust and architecture-agnostic strategies to manage activation dynamics in LLMs."
        },
        {
            "title": "2 Prior Work",
            "content": "The authors of [1] identified phenomenon where small subset of activations exhibits significantly larger values than others, which they termed massive activations. Specifically, they defined an LLM as having massive activations if the maximum magnitude of its hidden states exceeds 100 and is at least 1,000 times greater than the median magnitude of the hidden states. Mathematically, this can be expressed as: max(h) > 100 and max(h) 1000 median(h), (1) 2 Refined Analysis of Massive Activations in LLMs Preprint where represents the hidden states of the model, and denotes the element-wise absolute value operation. Additionally, they observed the following three key characteristics of massive activations: 1. Constant Values Across Layers Massive activations remain largely constant throughout most intermediate layers. They emerge in the initial layers and diminish in the final layers. For some models (e.g., LLaMA2-7B and LLaMA2-13B [9]), they emerge rapidly within single layer, while for others (e.g., Phi-2 [10] and OPT [11]), they accumulate gradually over many layers. 2. Fixed Location In terms of the sequence dimension, massive activations are located on specific types of tokens. Tokens associated with massive activations are typically starting word tokens, delimiter tokens, or tokens with weak semantics (e.g., and, of, from). These tokens are few in number, but play critical role in shaping model behavior. In terms of the feature dimension, massive activations are consistently present in very few fixed dimensions. 3. Fixed Bias Massive activations act as important bias terms, not just as redundant activations with no effect; the model seems to re-purposing the tokens linked to them to store these biases. This phenomenon is observed through intervention analysis: setting massive activation values to zero severely degrades model performance, while replacing them with their mean values (computed over input samples) does not cause significant harm. This indicates that their values are constant and input-agnostic. Massive activations also seem to be closely connected to self-attention mechanisms. stark contrast in attention patterns emerges when comparing layers before and after the appearance of massive activations. Specifically, in layers following the first occurrence of massive activations, attention becomes concentrated on tokens associated with these activations. For multi-token input sequence which triggers massive activations, the tokens associated with massive activations exhibit drastically different key and value states compared to the key and value states of other tokens in the same sequence. The high activation values skew the denominator term of normalization layers such as LayerNorm [12] and RMSNorm [13]. This results in drastically different feature representations for the tokens associated with massive activation. These tokens are utilized to form constant bias term during attention computation, which acts as an implicit bias term in the model. This conclusion was drawn from study on attention output decomposition, where the attention output is decomposed into two parts: value updates from the tokens where attention is concentrated, and value updates aggregated from other tokens: Attention(Q, K, )k = i vi = pk vi + pk pk vi, ik iC /C (2) is the attention distribution of query token to token i, and vi is the value state of token i. The where pk analysis showed that value updates from are nearly identical across tokens, effectively functioning as additive bias terms despite not being explicitly imposed. The authors furthermore proposed method to eliminate massive activations by augmenting the selfattention mechanism with additional key and value embeddings, which are explicitly designed to act as biases. Rather than re-purposing existing tokens in the input sequence, this method introduces additional learnable parameters k, Rd for each attention head. Specifically, given the input query, key, and value matrices Q, K, RT d, where denotes the sequence length and represents the embedding dimension, the augmented attention mechanism with explicit biases is computed as follows: Attention(Q, K, ; k, v) = softmax (cid:2)K k(cid:3) ! (cid:20) vT (cid:21) , (3) where and are concatenated with the key and value matrices and , respectively. This formulation allows the proposed attention mechanism to serve as drop-in replacement for standard attention, without requiring modifications to other components of the Transformer architecture. Notably, this approach effectively adds an extra token on-the-fly, ensuring that the biases are incorporated seamlessly into the computation. However, this extra token is transient in nature: it only persists until the matrix multiplication between QK and is completed. Subsequently, due to the properties of matrix multiplication, this token is no longer explicitly retained in the output, as its influence is implicitly integrated into the resulting attention values. 3 Refined Analysis of Massive Activations in LLMs Preprint"
        },
        {
            "title": "3 Experiments",
            "content": "Our goal in this paper was to systematically assess the existence, characteristics, and impact of massive activations while addressing several gaps in prior analyses. We adopted the definition proposed by [1], where an LLM is considered to have massive activations if the maximum magnitude value of the hidden states exceeds 100 and is at least 1000 times greater than the median magnitude value of the hidden states (see also Equation 1). This definition ensures consistency with prior studies while enabling reproducibility across diverse architectures. Using this criterion, we ran experiments in two key areas: intervention analysis and mitigation strategies. Its important to note that by default prior work [1] conducted analyses without including the BOS token. However, this approach may not align with how most models are trained and supposed to be configured for inference, as inputs typically include the BOS token. While they observed that massive activations persisted even when the BOS token was included, their findings were limited to specific models (LLaMA2-7B, LLaMA2-13B, and Mixtral-8x7B [17]). They acknowledged this limitation and suggested extending their analysis to broader range of models as part of future work. In this study, we evaluated each model under two conditions: with and without the BOS token. This setup allowed us to explore how architectural differences and input variations influenced the emergence and behavior of massive activations. 3."
        },
        {
            "title": "Intervention analysis",
            "content": "One key claim by [1] that we sought to validate is that massive activations always act as fixed bias terms, essentially re-purposing specific tokens to store these biases. If suppressed, these biases can negatively impact model performance, particularly in terms of perplexity and downstream task performance. In short, this hypothesizes that massive activations are always detrimental. It is worth noting that the original authors based their conclusions primarily on experiments conducted with LLaMA2-7B and LLaMA2-13B. However, there is no guarantee that the same behavior generalizes to other architectures. We adopted the intervention methodology introduced in [1]. Specifically, we modified the inference process of LLMs by intervening on massive activations at one layer. For hidden states exhibiting massive activations, we manually set these activations to chosen fixed values (e.g., zero or their mean). The altered hidden state was then fed into the next layer, allowing computation to proceed as normal. Following the original study, we evaluated perplexity on WikiText [18], C4 [19], and PG-19 [20]. Set to zero. To assess the importance of massive activations, we removed them by setting their values to zero in the hidden state at the point where they first appeared. This effectively eliminated massive activations. If these activations are indeed detrimental, we would expect the measured perplexity to increase significantly. Conversely, if they are not detrimental, the perplexity should remain relatively stable. Set to mean. To test whether small variances in massive activation values contribute to their role, we replaced these values with their empirical means. If the perplexity remains similar after this intervention, it suggests that the values of massive activations are constant and input-agnostic, functioning similarly to bias terms. Unlike the original approach, which focused solely on token indices from particular sentence (e.g., Summer is Warm. Winter is Cold.)1, we first identified all locations of massive activations across the model from 100 samples of RedPajama [25]. These activations were then categorized into two groups: starting tokens (e.g., index 0) and non-starting tokens (index > 0). This categorization allowed us to systematically analyze how massive activations behave at different token positions, effectively controlling for the significant differences in scale of starting and non-starting positions. We then repeated this procedure on the test datasets WikiText, PG-19, and C4, organizing once again massive activations into the two buckets of starting and non-starting tokens, and replace the corresponding tokens in sample with the respective mean values derived from RedPajama2. The models under investigation include both GLU-based and non-GLU-based architectures. Among the GLU-based models, we examine LLaMA-2-7B [9], LLaMA-3.2-1B and LLaMA-3.2-3B [30], Mistral-7B-v0.3 [31], Gemma-7B [14], Gemma-2-2B and Gemma-2-9B [32], OLMo-7B-0724 [15], OLMo-2-1124-7B [33], and Phi-4 [34]. For non-GLU-based models, we include GPT-2 [3], Falcon-7B and Falcon-2-11B [16], OPT-6.7B [11], and Phi-2 [10]. 1See code here. 2See code here. 4 Refined Analysis of Massive Activations in LLMs Preprint"
        },
        {
            "title": "Methods",
            "content": "Another claim we investigated is that augmenting self-attention with explicit key and value biases (see Equation (3)) can eliminate massive activations. However, it is important to note that this conclusion by [1] is based solely on experiments conducted on the GPT-2 architecture. The generalizability of this mitigation strategy to other architectures seems to be untested so far. In this work, we extended the evaluation of mitigation strategies to larger and more mainstream architecture, namely LLaMA-1B. We tested various strategies, such as Attention KV Bias, which repurposes existing tokens to act as biases; Target Variance Rescaling (TVR), which rescales weights to control extreme values; Dynamic Tanh (DyT), which is replacement for all layer norm modules; and hybrid approaches that combine TVR with other strategies, such as KV Bias + TVR and DyT + TVR. The choice of methods to evaluate is motivated by the intuition (and some prior indication) that correlate massive activations with large weight variance (TVR) and layer normalization (DyT), respectively."
        },
        {
            "title": "Benchmarks",
            "content": "Unlike prior studies that relied primarily on validation loss as the sole evaluation metric, we adopted comprehensive evaluation framework encompassing diverse set of tasks designed to measure various aspects of language understanding and reasoning. These tasks include commonsense reasoning benchmarks such as HellaSwag [26], PIQA [22], SIQA [27], and WinoGrande [23]; question-answering tasks such as TriviaQA [28]; and multi-step reasoning tasks such as ARC-Challenge (ARC-C) and ARC-Easy (ARC-E) [24]."
        },
        {
            "title": "Training Setup",
            "content": "GPT-2. We used the default GPT-2-124M model configuration and the default GPT-2 tokenizer with 1024 context length as available on Huggingface3. We utilized only the FineWebEdu-Deduplicated subset of the SmolLM [29] dataset, training the model for total of 50 billion tokens. We always prepend BOS token to every document. All weights were initialized from normal distribution of zero mean and 0.02 standard deviation, with special residual initialization strategy proposed in [3] applied. All training with the GPT-2 architecture was performed using the hyperparameters presented in Appendix 6.1. LLaMA-1B. As for the one-billion-parameter LLaMA model, the training data consists of the SmolLM dataset, which combines three diverse sources: FineWebEdu-Deduplicated, Cosmopedia-V2 and Python-Edu; we randomly sample 100 billion tokens (100BT) from this corpus. The context length is fixed at 2048 tokens per sequence. We always prepend BOS token in every document. All 2D modules weights, except stated otherwise, were initialized from normal distribution of zero mean and 0.006 standard deviation, with Layer Index Rescaling (LIR) applied [7]. Details for the hyperparameters, model configuration, and tokenizer are presented in Appendix 6.2."
        },
        {
            "title": "4.1 Not all Massive Activations are detrimental",
            "content": "First, we observe that not all massive activations are detrimental, especially for most of the non-GLU LLMs (see Table 1 and Table 6 for results with and without BOS token, respectively). While some models experienced significantly worse perplexity when massive activations were suppressed by setting their values to zero, others showed minimal or no degradation in performance. Despite this variability, we have yet to identify clear indicator that distinguishes between detrimental and non-detrimental massive activations. However, one weak pattern emerges: models which have quickly increasing massive activations across the first few layers, quickly decreasing massive activations for the final few layers, and almost constant massive activations for the middle layers, tend to exhibit more detrimental effects. An example for this is LLaMA-2-7B as illustrated in Figure 1a. On the other hand, Falcon-7B displays extreme emergence and diminishment, but with slightly gradual steps; its massive activations are only moderately detrimental (see Figure 1b). On the other end of the spectrum, Falcon-2-11B exhibits multiple step-wise increases on intermediate layers; its massive activations appear to be non-detrimental (see Figure 1c). However, there are exceptions to to the 3https://huggingface.co/openai-community/gpt2 5 Refined Analysis of Massive Activations in LLMs Preprint (a) (b) (c) Figure 1: Top activation magnitude across layers for (a) Llama-2-7B; (b) Falcon-7B; (c) Falcon2-11B. The input sentence is Summer is warm. Winter is cold with BOS token included. pattern: For example, Gemma-7B with BOS token has detrimental massive activation (see Table 1), but the pattern of the top activation magnitude across layers does not follow the standard pattern demonstrating spike in magnitude on the last layer (see Figure 10d). Top activation magnitude plots for all investigated LLMs with and without BOS token can be found in Appendix 6.4 and 6.5."
        },
        {
            "title": "4.2 BOS Token is significant for some LLMs",
            "content": "Second, we find that the BOS token plays significant role in Gemma, though its influence varies across different model generations and sizes. Figure 2 illustrates this effect: when the BOS token is excluded, massive activations are not observed in Gemma-7B, Gemma-2-2B, and Gemma-2-9B. However, when the BOS token is included, massive activations emerge in these models. Additionally, while massive activations persist in the Gemma-3 models (Gemma-3-1B, Gemma-3-4B, and Gemma-3-12B) even without the BOS token (see Figures 8g, 8h, 8i), their magnitudes are significantly reduced compared to when the BOS token is present (see Figures 10g, 10h, 10i). Notably, from perplexity perspective (see Tables 1 and 6), excluding the BOS token also leads to worsened performance across all Gemma models, underscoring the importance of the BOS token in this model family. (a) (b) (c) Figure 2: Comparison of top activation magnitude across layers for (a) Gemma-7B; (b) Gemma2-2B; (c) Gemma-2-9B; with and without BOS token. The input sentence is Summer is warm. Winter is cold."
        },
        {
            "title": "4.3 Attention KV Bias is not a general Mitigation Strategy",
            "content": "Our investigation into the effectiveness of attention key-value bias as mitigation strategy yielded mixed results. We retrained multiple GPT-2 and LLaMA-1B models from scratch following the setup describe in section 3.2. For GPT-2, we were able to reproduce the findings from prior studies, confirming that KV bias effectively reduces the magnitude of massive activations. Figure 3a illustrates the significant reduction in top activation magnitudes when KV bias is applied. However, downstream task performance on benchmarks such as HellaSwag consistently underperformed throughout the whole training process (see Figure 3b). This underscores the limitations of relying solely on validation loss as performance metric. In contrast, the same KV bias formulation failed to mitigate massive activations in LLaMA-1B. As shown in Figure 3c, the top activation magnitudes remained largely unchanged; downstream task performance 6 Refined Analysis of Massive Activations in LLMs Preprint (a) (c) (b) (d) Figure 3: Comparison of top activation magnitude across layers for (a) GPT-2 and (c) LLaMA-1B with and without KV Bias. Evolution of HellaSwag scores for (b) GPT-2 and (d) LLaMA-1B with and without KV Bias. The input sentence is Summer is warm. Winter is cold with BOS token included. We presented the corresponding analysis without BOS token in Appendix 6.7.3. underperformed as well (see Table 7 and Figure 3d). This suggests that KV bias as mitigation strategy for massive activations is not universally effective across architectures. To better understand the impact of KV bias on attention patterns, we analyzed the distribution of attention weights before and after applying the mitigation strategy. Figure 13 shows 2D heatmaps of attention weights for GPT-2 with the BOS token included, illustrating the average attention logits across all heads, tokens, and layers. The results reveal that attention becomes concentrated on the BOS token starting from layer 3 (note that massive activations first appear in layer 2; see Figure 3a). While KV bias successfully reduces the magnitude of massive activations, it does not eliminate attention concentration. Instead, the extra token continues to attract significant proportion of attention across layers (see Figure 14). similar pattern is observed when the BOS token is excluded (see Figures 11 and 12). In contrast, the effect of KV bias on LLaMA-1B differs from its impact on GPT-2, particularly when the BOS token is included. For LLaMA-1B, attention is initially concentrated on the extra token in the layers immediately following the emergence of massive activations (specifically, from layer 3 to layer 6). Beyond layer 6, attention shifts back to the BOS token (see Figure 22). This highlights the key role played by the BOS token in LLaMA-1B. When the BOS token is included, the baseline model exhibits strong attention concentration on the BOS token across all layers (see Figure 21). However, this pattern changes when the BOS token is excluded. As shown in Figure 15, the attention distribution becomes less concentrated and splits between two tokens: the starting token and the full-stop token."
        },
        {
            "title": "4.4 Alternative Mitigation Strategies",
            "content": "We explored alternative mitigation strategies trying to address both massive activations and attention concentration while preserving downstream task performance. We focused our experiments on LLaMA-1B, using it as representative model to evaluate the effectiveness of various approaches. While this scope allows for an in-depth analysis of LLaMA-1Bs behavior, future work should extend these investigations to other architectures to validate the generalizability of our findings. 7 Refined Analysis of Massive Activations in LLMs Preprint Baseline KV Bias DyT TVR KV Bias + TVR DyT + TVR Mitigates Massive Activations? Max Activation Magnitude 1416 1512 47 235 158 45 Median Activation Magnitude 0.8 3.1 1.8 0.2 0.7 1.3 Mean Downstream Task Accuracy 50.3 49.6 49.9 52.5 52.0 50.3 Table 2: Summary of the effectiveness of various mitigation strategies for massive activations in LLaMA-1B, along with their impact on downstream task performance. Breakdown of downstream tasks performance is presented in Appendix 6.8."
        },
        {
            "title": "4.4.1 Dynamic Tanh (DyT)",
            "content": "We studied the effects of replacing the RMSNorm layers in the model with Dynamic Tanh (DyT) [8]. To ensure convergence, we incorporated embedding scaling as proposed by the authors. Following their recommendations, we set the hyperparameters based on the width and depth of LLaMA-1B. Specifically, for the DyT layers in the attention modules, we used α = 1.0, while for the DyT layer before the final projection layer, we used α = 0.5. The embedding scaler was initialized with d, where denotes the embedding dimension, as suggested in the paper. We initialized all 2D module weights in the decoder layer with zero mean and standard deviation of 0.02, with LIR [7] applied. This specific value was determined through hyperparameter tuning experiments, where we observed that other initialization standard deviations resulted in drastically worse downstream task performance. Figure 4: Comparison of top activation magnitude across layers for LLaMA-1B baseline and with DyT. The input sentence is Summer is warm. Winter is cold with BOS token included. We presented the corresponding analysis without BOS token in Appendix 6.7.3. Figure 4 illustrates the reduction in top activation magnitudes, confirming DyTs effectiveness in mitigating massive activations. However, this improvement came at cost: downstream task performance deteriorated, though it remained slightly better than with KV bias alone (see Table 2). Furthermore, attention concentration persisted even with DyT, albeit at reduced level (see Figures 19 and 25). This finding challenges the hypothesis that RMSNorm or LayerNorm play key role in enabling tokens associated with massive activations to exhibit drastically different key and value states [1]. These differences were previously suggested to lead to the formation of constant bias term during attention computation. By entirely replacing normalization operations, DyT was expected to drastically disrupt this mechanism. However, the empirical evidence suggests otherwise, indicating that the relationship between normalization, massive activations, and attention concentration is more complex than previously hypothesized."
        },
        {
            "title": "4.4.2 Target Variance Rescaling (TVR)",
            "content": "Target Variance Rescaling (TVR), introduced in [7], aides in stabilizing the variance of model weights during pre-training. In their experiments on LLaMA-1B, TVR successfully reduced extreme activation values by approximately tenfold. However, lowering activation magnitudes does not necessarily equate to removing massive activations as per our definition. To validate TVRs effectiveness, we conducted experiments using target standard deviation of 0.01, as recommended in the original paper, and applied it to LLaMA-1B. 8 Refined Analysis of Massive Activations in LLMs Preprint Figure 5: Comparison of top activation magnitude across layers for LLaMA-1B with TVR, KV Bias + TVR, and DyT + TVR. The input sentence is Summer is warm. Winter is cold with BOS token included. We presented the corresponding analysis without BOS token in Appendix 6.7.3. The results, shown in Figure 5, confirm that TVR significantly reduces extreme activation values. However, massive activations persist under our definition (the maximum activation magnitude value is still 1000 times greater than the median magnitude value), indicating that TVR alone does not fully eliminate this phenomenon. Despite this limitation, downstream task performance remained comparable to or even slightly improved over the baseline (see Table 2). These findings suggest that TVR can mitigate massive activations to some degree without compromising model performance. Nevertheless, attention concentration persists, as illustrated in Figures 17 and 23. To further enhance TVRs effectiveness, we combined it with KV bias to create hybrid mitigation strategy. As shown in Figure 5, this combination achieved an even greater reduction in extreme activation values compared to using either method alone, hence resulting in the elimination of massive activation as per our definition. Downstream tasks performance also remained stable, with some tasks even showing improvements over the baseline (see Table 2). These results demonstrate that combining TVR and KV bias can effectively mitigate some undesirable effects of massive activations without compromising model performance. Nevertheless, attention concentration still persisted with similar pattern found in incorporating KV bias without TVR (Figures 18 and 24). Finally, we combined DyT with TVR to explore whether TVR could recover the lost downstream task performance observed with DyT alone. In this hybrid approach, we initialized all 2D module weights in the decoder layer with zero mean and standard deviation of 0.02, while applying LIR as before. Additionally, we set the TVR target standard deviation to 0.02. Figure 6: Comparison of top activation magnitude across layers for LLaMA-1B between DyT and DyT + TVR. The input sentence is Summer is warm. Winter is cold with BOS token included. We presented the corresponding analysis without BOS token in Appendix 6.7.3. This combination proved highly effective: Table 2 demonstrates that downstream task performance recovers to levels comparable to the baseline. More importantly, DyT + TVR continues to suppress extreme activation values effectively (see Figures 5 and 6). Additionally, while attention concentration is further reduced compared to only incorporating DyT without TVR, it still persists (Figures 20 and 26). Consistent with the hypothesis proposed by the original authors [1], these findings suggest that LLMs have strong intrinsic propensity to form attention concentration patterns during pre-training. 9 Refined Analysis of Massive Activations in LLMs Preprint In nutshell, these findings highlight the potential of hybrid strategies to achieve balanced mitigation of massive activations. While individual methods like TVR, KV bias, and DyT offer partial solutions, their combinationssuch as KV bias + TVR, and DyT + TVRprovide more comprehensive approaches that preserve model performance while addressing undesirable massive activations."
        },
        {
            "title": "4.5 Other Characteristics",
            "content": "Our analysis of massive activations reveals both consistencies and contradictions with prior findings across various LLM architectures. One key characteristic previously identified is that massive activation values tend to be mostly constant across layers [1]. However, we observed deviations from this pattern in certain models. For instance, models such as Gemma-2-2B, Gemma-3-1B, Gemma-3-4B, and Gemma-3-14B display uptrend increase in activation magnitudes across layers (see Figures 8 and 10). This behavior might be influenced by architectural design choices, as these models employ both preand post-layernorm operations in their MLP and attention modules. In contrast, other models that rely solely on pre-layernorm do not exhibit this trend, suggesting that normalization strategies could play role in shaping activation dynamics. Another characteristic pertains to the fixed sequence dimension locations of massive activations. Prior study [1] suggested that these activations are primarily associated with starting, delimiter, or weak semantic tokens. However, our findings reveal exceptions in models like Gemma-3-4B, Gemma-3-12B, and Falcon-7B. For these models, massive activations are linked not only to starting or delimiter tokens, but also to other tokens within the sequence, such as \"polished\", \"mass\", \"cold\". This challenges the assumption that massive activations are strictly tied to specific token types, highlighting the need for architecture-specific investigations. Despite these contradictions, several characteristics remain consistent with prior observations. Massive activations are consistently small in number, and occur in very few fixed embedding positions across different models. This alignment reinforces the robustness of earlier findings while underscoring the importance of understanding how these activations interact with model architecture and training dynamics. For additional insights and relevant illustrations, we refer readers to the supplementary materials available in our repository: https://github.com/bluorion-com/refine_massive_activations."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "In this work, we conducted systematic analysis of massive activations in LLMs, addressing gaps in prior studies by evaluating their existence, characteristics, and impact across diverse range of architectures. Our findings challenge several previously published assumptions about massive activations, and highlight the need for architecture-specific mitigation strategies. First, we demonstrated that not all massive activations are detrimental to model performance. While some models exhibit significant degradation in perplexity and downstream task performance when massive activations are suppressed, others remain almost completely unaffected. This suggests that the role of massive activations is more nuanced than previously thought, warranting further investigation into distinguishing factors between detrimental and non-detrimental cases. Second, our results indicate that previously presented mitigation strategies are not universally generalizable. In particular, Attention KV bias fails to mitigate massive activations effectively in LLaMA-1B, underscoring its limitations in certain architectures. However, on the LLaMA-1B model specifically, when combined with Target Variance Rescaling (TVR), this approach successfully suppresses extreme activation values without compromising downstream task performance. This highlights the potential of hybrid strategies in addressing massive activations while preserving model performance. Furthermore, we explored the DyT (Dynamic Tanh) architecture as an alternative approach to mitigating massive activations. While DyT effectively removes these activations, it comes at the cost of reduced performance on downstream tasks in our experiments. Interestingly, combining DyT with TVR recovers downstream task performance, suggesting that TVR plays crucial role in balancing activation suppression and functional preservation. These findings underscore the importance of developing architecture-aware mitigation strategies and exploring combinations of techniques to achieve optimal results. Moving forward, several directions for future work emerge: Refined Analysis of Massive Activations in LLMs Preprint Investigating the underlying mechanisms that differentiate detrimental from non-detrimental massive activations, potentially through fine-grained analysis of their distribution and persistence across layers. Extending the evaluation of hybrid strategies such as KV Bias + TVR and DyT + TVR to broader range of architectures and tasks to validate their generalizability. Analyzing in more depth how the attention concentration phenomenon exhibits under multiple different mitigation techniques on broader range of LLMs. Future research in this area has the potential to unlock new insights into model behavior and pave the way for more reliable and scalable AI systems. Notably, given the relevance of massive activations to low-precision training and quantization, our findings provide valuable directions for addressing numerical stability challenges in these contexts. 11 Refined Analysis of Massive Activations in LLMs Preprint References [1] Mingjie Sun, Xinlei Chen, Zico Kolter, and Zhuang Liu. Massive activations in large language models. In First Conference on Language Modeling, 2024. [2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971, 2023. [3] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. [4] Gemma Team. Gemma 3, 2025. [5] Daniel Han. LinkedIn i-investigated-why-gemma-3-wasnt-working-activity-7308570201720070144-FYOO. 2025. https://www.linkedin.com/posts/danielhanchen_ Investigating Available activations and at: gemma infinite fixing Post. in 3, [6] Jaewoo Yang, Hayun Kim, and Younghoon Kim. Mitigating quantization errors due to activation spikes in glu-based llms, 2024. [7] Louis Owen, Abhay Kumar, Nilabhra Roy Chowdhury, and Fabian Güra. Variance control via weight rescaling in llm pre-training, 2025. [8] Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, and Zhuang Liu. Transformers without normalization, 2025. [9] AI @ Meta Llama Team. Llama 2: Open foundation and fine-tuned chat models, 2023. [10] Mojan Javaheripi, Sébastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio César Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, et al. Phi-2: The surprising power of small language models. Microsoft Research Blog, 2023. [11] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022. [12] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. [13] Biao Zhang and Rico Sennrich. Root mean square layer normalization, 2019. [14] Gemma Team. Gemma: Open models based on gemini research and technology, 2024. [15] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. Olmo: Accelerating the science of language models, 2024. [16] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon series of open language models, 2023. [17] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts, 2024. [18] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. [19] Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: case study on the colossal clean crawled corpus, 2021. 12 Refined Analysis of Massive Activations in LLMs Preprint [20] Jack Rae, Anna Potapenko, Siddhant Jayakumar, Chloe Hillier, and Timothy Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. [21] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019. [22] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In AAAI Conference on Artificial Intelligence, 2019. [23] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99106, August 2021. [24] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018. [25] Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, April 2023. [26] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really finish your sentence? In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy, July 2019. Association for Computational Linguistics. [27] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 44634473, Hong Kong, China, November 2019. Association for Computational Linguistics. [28] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, Vancouver, Canada, July 2017. Association for Computational Linguistics. [29] Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Smollmcorpus, July 2024. [30] AI @ Meta Llama Team. The llama 3 herd of models, 2024. [31] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. [32] Gemma Team. Gemma 2: Improving open language models at practical size, 2024. [33] Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious, 2025. [34] Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. Phi-4 technical report, 2024. [35] Abhay Kumar, Louis Owen, Nilabhra Roy Chowdhury, and Fabian Güra. Zclip: Stabilizing pre-training of large language models, 2025. 13 Refined Analysis of Massive Activations in LLMs Preprint"
        },
        {
            "title": "6.1 Training Hyperparameters for GPT-2",
            "content": "The hyperparameters details for GPT-2 are presented in Table 3."
        },
        {
            "title": "Hyperparameter\nOptimizer",
            "content": "Value Fused AdamW (β1 = 0.9, β2 = 0.95, ϵ = 1 107) Learning Rate Schedule Linear warmup followed Max Learning Rate End Learning Rate Warmup Tokens Weight Decay Global Batch Size Precision Gradient Clipping Dropout rate Bias by cosine decay 6 104 6 105 1 billion tokens (1BT) 0.1 (AdamW implementation) 2048 Mixed Precision BFloat16 1.0 0.0 not used Table 3: GPT-2 Training Hyperparameters"
        },
        {
            "title": "6.2 Training Hyperparameters, Model Config, and Tokenizer Details for LLaMA-1B",
            "content": "Table 4 shows details of the model architecture used in our experiment setup. The tokenizer used in this work is truncated version of the Llama3 tokenizer, which includes vocabulary size of 65,536 tokens. Training hyperparameters details is presented in Table 5. Parameter Hidden Size Intermediate Size Number of Hidden Layers Number of Attention Heads Number of Key-Value Heads Activation Function RMSNorm Epsilon Vocabulary Size Maximum Position Embeddings Value 2048 5440 16 16 16 SwiGLU 1 105 65,"
        },
        {
            "title": "Hyperparameter\nOptimizer",
            "content": "Value Fused AdamW (β1 = 0.9, β2 = 0.95, ϵ = 1 107)"
        },
        {
            "title": "Max Learning Rate\nEnd Learning Rate\nWarmup Tokens\nWeight Decay\nGlobal Batch Size\nPrecision",
            "content": "by cosine decay 6 104 6 105 1 billion tokens (1BT) 0.1 (AdamW implementation) 4032 Mixed Precision BFloat16 Table 4: LLaMA-1B Model Configuration Table 5: LLaMA-1B Training Hyperparameters 14 Refined Analysis of Massive Activations in LLMs Preprint 6."
        },
        {
            "title": "Intervention Analysis Results without BOS token",
            "content": "Table 6 shows the intervention analysis results without BOS token for various LLMs. Non-GLU-based Models Intervention WikiText Original Set to zero Set to mean 30.22 30.21 30.21 Intervention WikiText Original Set to zero Set to mean 10.88 117.27 10. GPT-2 C4 37.33 37.31 37.31 Falcon-7B C4 21.91 163.36 21.92 PG-19 53.28 53.29 53.28 PG-19 21.98 174.86 22.13 WikiText 62.13 62.26 61.94 WikiText 4.87 5.29 4.87 Phi-2-2.7B C4 83.70 83.96 83.73 Falcon-2-11B C4 9.78 10.90 9. PG-19 WikiText 62.85 62.94 62.88 10.97 10.98 10.97 OPT-6.7B C4 14.54 14.54 14.54 PG-19 13.35 13.35 13.35 PG-19 9.79 11.98 9.85 GLU-based Models Intervention WikiText Original Set to zero Set to mean 5.12 8115.29 5.12 Intervention WikiText 3.11 108 Original - Set to zero - Set to mean Intervention WikiText Original Set to zero Set to mean 11.67 11.72 11.65 Intervention WikiText Original Set to zero Set to mean 7.10 9014.45 7.10 Intervention WikiText Original Set to zero Set to mean 5.00 1.80 106 4.98 LLaMA-2-7B C4 7.62 8625.69 7.63 Gemma-7B C4 9.61 107 - - Gemma-3-1B C4 18.26 18.50 18.26 OLMo-7B-0724 C4 10.55 9500.15 10.55 Mistral-7B-v0.3 C4 8.44 2.06 106 8.47 PG-19 8.08 5745.95 8.08 PG-19 6.00 107 - - PG-19 20.99 21.05 20.99 PG-19 10.24 5497.85 10.25 PG-19 8.43 1.59 106 8.45 PG-19 11.97 14938.57 11.99 WikiText 9.07 7364.78 9.09 WikiText 57.14 - - WikiText 8.47 266.79 9.00 LLaMA-3.2-1B C4 14.56 5669.25 14.78 Gemma-2-2B C4 65.50 - - Gemma-3-4B C4 14.31 540.92 15.25 OLMo-2-1124-7B C4 12.29 - - WikiText 5.73 - - PG-19 WikiText 14.88 3865.99 15.02 7.28 22144.77 7.29 PG-19 WikiText 48.23 - - 122.60 - - PG-19 WikiText 15.33 460.32 15.99 14.35 15.87 14.91 PG-19 WikiText 10.24 - - 6.06 6.18 6.05 LLaMA-3.2-3B C4 11.83 22513.59 11.85 Gemma-2-9B C4 235.10 - - Gemma-3-12B C4 24.91 27.92 26.75 Phi-4-14B C4 11.87 12.13 11. PG-19 254.67 - - PG-19 25.66 27.51 26.31 PG-19 9.57 10.00 9.58 Table 6: Perplexity scores for various pre-trained LLMs subject to massive activation intervention with the BOS token excluded. - indicates that no massive activations were found. Results are color-coded to indicate the level of impact: purple denotes highly detrimental effects, while orange signifies medium detrimental effects. For consistency, we used context length of 4096 tokens for all models, except for GPT-2 and OPT-6.7B, where context lengths of 1024 and 2048 tokens were used, respectively, due to model limitation. By default, all analyses were conducted using float16 precision, except for Gemma-7B and all Gemma-3 models, which required float32. For Gemma-7B, this was due to the original weights being in float32, while for the Gemma-3 models, the excessively high activation magnitudes exceeded the range supported by float16. 15 Refined Analysis of Massive Activations in LLMs Preprint"
        },
        {
            "title": "6.4 Top Activation Magnitude Plots without BOS token",
            "content": "Figure 7 and 8 illustrate the top activation magnitude plots for non-GLU and GLU-based LLMs without BOS token, respectively. (a) (b) (c) (d) (e) Figure 7: Top activation magnitude across layers for Non-GLU-based LLMs: (a) GPT-2; (b) Phi-2; (c) OPT-6.7B; (d) Falcon-7B; (e) Falcon-2-11B. The input sentence is Summer is warm. Winter is cold with BOS token excluded. 16 Refined Analysis of Massive Activations in LLMs Preprint (a) (b) (c) (d) (e) (f) (g) (h) (j) (k) (i) (l) (m) Figure 8: Top activation magnitude across layers for GLU-based LLMs: (a) LLaMA-2-7B; (b) LLaMA-3.2-1B; (c) LLaMA-3.2-3B; (d) Gemma-7B; (e) Gemma-2-2B; (f) Gemma-2-9B; (g) Gemma-3-1b; (h) Gemma-3-4b; (i) Gemma-3-12b; (j) OLMo-7B-0724; (k) OLMo-2-1124-7B; (l) Phi-4; (m) Mistral-7B-v0.3. The input sentence is Summer is warm. Winter is cold with BOS token excluded. 17 Refined Analysis of Massive Activations in LLMs Preprint"
        },
        {
            "title": "6.5 Top Activation Magnitude Plots with BOS token",
            "content": "Figure 9 and 10 illustrate the top activation magnitude plots for non-GLU and GLU-based LLMs with BOS token, respectively. (a) (b) (c) (d) (e) Figure 9: Top activation magnitude across layers for Non-GLU-based LLMs: (a) GPT-2; (b) Phi-2; (c) OPT-6.7B; (d) Falcon-7B; (e) Falcon-2-11B. The input sentence is Summer is warm. Winter is cold with BOS token included. 18 Refined Analysis of Massive Activations in LLMs Preprint (a) (b) (c) (d) (e) (f) (g) (h) (j) (k) (i) (l) (m) Figure 10: Top activation magnitude across layers for GLU-based LLMs: (a) LLaMA-2-7B; (b) LLaMA-3.2-1B; (c) LLaMA-3.2-3B; (d) Gemma-7B; (e) Gemma-2-2B; (f) Gemma-2-9B; (g) Gemma-3-1b; (h) Gemma-3-4b; (i) Gemma-3-12b; (j) OLMo-7B-0724; (k) OLMo-2-1124-7B; (l) Phi-4; (m) Mistral-7B-v0.3. The input sentence is Summer is warm. Winter is cold with BOS token included. 19 Refined Analysis of Massive Activations in LLMs Preprint"
        },
        {
            "title": "6.6.1 Self Attention Plots for Retrained GPT-2 without BOS token",
            "content": "Figures 11 - 12 illustrates the self attention distribution for the GPT-2 model that we trained from scratch. BOS token is not included in all of these visualizations. Note that in GPT-2 the BOS token shares the same token id with the EOS token. Figure 11: Average attention logits over all heads for GPT-2 (baseline) without BOS token. (a) 20 Refined Analysis of Massive Activations in LLMs Preprint Figure 12: Average attention logits over all heads for GPT-2 (with KV Bias) without BOS token. (a) 21 Refined Analysis of Massive Activations in LLMs Preprint"
        },
        {
            "title": "6.6.2 Self Attention Plots for Retrained GPT-2 with BOS token",
            "content": "Figures 13 - 14 illustrates the self attention distribution for the GPT-2 model that we trained from scratch. BOS token is included in all of these visualizations. Note that in GPT-2 the BOS token shares the same token id with the EOS token. Figure 13: Average attention logits over all heads for GPT-2 (baseline) with BOS token. 22 Refined Analysis of Massive Activations in LLMs Preprint Figure 14: Average attention logits over all heads for GPT-2 (with KV Bias) with BOS token. 23 Refined Analysis of Massive Activations in LLMs Preprint"
        },
        {
            "title": "6.7.1 Self Attention Plots for Retrained LLaMA-1B without BOS token",
            "content": "Figures 15 - 20 illustrates the self attention distribution for our LLaMA-1B that we trained from scratch. BOS token is not included in all of these visualizations. Figure 15: Average attention logits over all heads for LLaMA-1B (baseline) without BOS token. 24 Refined Analysis of Massive Activations in LLMs Preprint Figure 16: Average attention logits over all heads for LLaMA-1B (with KV Bias) without BOS token. 25 Refined Analysis of Massive Activations in LLMs Preprint Figure 17: Average attention logits over all heads for LLaMA-1B (with TVR) without BOS token. 26 Refined Analysis of Massive Activations in LLMs Preprint Figure 18: Average attention logits over all heads for LLaMA-1B (with KV Bias + TVR) without BOS token. 27 Refined Analysis of Massive Activations in LLMs Preprint Figure 19: Average attention logits over all heads for LLaMA-1B (with DyT) without BOS token. 28 Refined Analysis of Massive Activations in LLMs Preprint Figure 20: Average attention logits over all heads for LLaMA-1B (with DyT + TVR) without BOS token. 29 Refined Analysis of Massive Activations in LLMs Preprint"
        },
        {
            "title": "6.7.2 Self Attention Plots for Retrained LLaMA-1B with BOS token",
            "content": "Figures 21 - 26 illustrates the self attention distribution for our LLaMA-1B that we trained from scratch. BOS token is included in all of these visualizations. Figure 21: Average attention logits over all heads for LLaMA-1B (baseline) with BOS token. 30 Refined Analysis of Massive Activations in LLMs Preprint Figure 22: Average attention logits over all heads for LLaMA-1B (with KV Bias) with BOS token. 31 Refined Analysis of Massive Activations in LLMs Preprint Figure 23: Average attention logits over all heads for LLaMA-1B (with TVR) with BOS token. 32 Refined Analysis of Massive Activations in LLMs Preprint Figure 24: Average attention logits over all heads for LLaMA-1B (with KV Bias + TVR) with BOS token. 33 Refined Analysis of Massive Activations in LLMs Preprint Figure 25: Average attention logits over all heads for LLaMA-1B (with DyT) with BOS token. 34 Refined Analysis of Massive Activations in LLMs Preprint Figure 26: Average attention logits over all heads for LLaMA-1B (with DyT + TVR) with BOS token. 35 Refined Analysis of Massive Activations in LLMs Preprint"
        },
        {
            "title": "6.7.3 Top Activation Magnitude Plots for Retrained LLaMA-1B without BOS token",
            "content": "Figure 27: Comparison of top activation magnitude across layers for LLaMA-1B between baseline and with KV Bias. The input sentence is Summer is warm. Winter is cold without BOS token included. Figure 28: Comparison of top activation magnitude across layers for LLaMA-1B without TVR (baseline) and with TVR. The input sentence is Summer is warm. Winter is cold without BOS token included. Figure 29: Comparison of top activation magnitude across layers for LLaMA-1B between baseline and with KV Bias + TVR. The input sentence is Summer is warm. Winter is cold without BOS token included. 36 Refined Analysis of Massive Activations in LLMs Preprint Figure 30: Comparison of top activation magnitude across layers for LLaMA-1B between baseline and DyT. The input sentence is Summer is warm. Winter is cold without BOS token included. Figure 31: Comparison of top activation magnitude across layers for LLaMA-1B between baseline and DyT + TVR. The input sentence is Summer is warm. Winter is cold without BOS token included. 37 Refined Analysis of Massive Activations in LLMs Preprint"
        },
        {
            "title": "6.8 Breakdown of Downstream Tasks Performance for Various Mitigation Strategies",
            "content": "Table 7 contains the breakdown of downstream tasks performance for various mitigation strategies. Baseline KV Bias DyT TVR KV Bias + TVR DyT + TVR HellaS 52.5 50.3 50.7 55.1 54.8 50.9 PIQA 71.2 71.5 71.2 73.4 72.5 70.9 SIQA 40.1 41.3 40.0 42.7 42.0 41.7 WinoG 53.6 53.6 51.3 57.0 57.9 51. TQA 39.2 35.5 39.8 38.9 37.9 40.1 ARC-E 61.8 62.6 63.0 64.3 62.4 61.7 ARC-C Mean base 0.0 50.3 -0.7 49.6 -0.4 49.9 2.2 52.5 1.7 52.0 0.0 50.3 33.9 32.3 33.0 36.1 36.3 34.8 Table 7: Downstream tasks performance benchmark for different massive activation mitigation strategies on LLaMA-1B."
        }
    ],
    "affiliations": [
        "BluOrion"
    ]
}