{
    "paper_title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models",
    "authors": [
        "Marianne Arriola",
        "Aaron Gokaslan",
        "Justin T Chiu",
        "Zhihan Yang",
        "Zhixuan Qi",
        "Jiaqi Han",
        "Subham Sekhar Sahoo",
        "Volodymyr Kuleshov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/"
        },
        {
            "title": "Start",
            "content": "Published as conference paper at ICLR 2025 BLOCK DIFFUSION: INTERPOLATING BETWEEN AUTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS Marianne Arriola Aaron Kareem Gokaslan Justin Chiu Zhihan Yang Zhixuan Qi Jiaqi Han Subham Sekhar Sahoo Volodymyr Kuleshov"
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code1, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms"
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models are widely used to generate images (Ho et al., 2020; Dhariwal & Nichol, 2021; Sahoo et al., 2024b) and videos (Ho et al., 2022; Gupta et al., 2023), and are becoming increasingly effective at generating discrete data such as text (Lou et al., 2024; Sahoo et al., 2024a) or biological sequences (Avdeyev et al., 2023; Goel et al., 2024). Compared to autoregressive models, diffusion models have the potential to accelerate generation and improve the controllability of model outputs (Schiff et al., 2024; Nisonoff et al., 2024; Li et al., 2024; Sahoo et al., 2024c). Discrete diffusion models currently face at least three limitations. First, in applications such as chat systems, models must generate output sequences of arbitrary length (e.g., response to users question). However, most recent diffusion architectures only generate fixed-length vectors (Austin et al., 2021; Lou et al., 2024). Second, discrete diffusion uses bidirectional context during generation and therefore cannot reuse previous computations with KV caching, which makes inference less efficient (Israel et al., 2025). Third, the quality of discrete diffusion models, as measured by standard metrics such as perplexity, lags behind autoregressive approaches and further limits their applicability (Gulrajani & Hashimoto, 2024; Sahoo et al., 2024a). This paper makes progress towards addressing these limitations by introducing Block Discrete Denoising Diffusion Language Models (BD3-LMs), which interpolate between discrete diffusion and autoregressive models. Specifically, block diffusion models (also known as semi-autoregressive models) define an autoregressive probability distribution over blocks of discrete random variables (Si et al., 2022; 2023); the conditional probability of block given previous blocks is specified by discrete denoising diffusion model (Austin et al., 2021; Sahoo et al., 2024a). Developing effective BD3-LMs involves two challenges. First, efficiently computing the training objective for block diffusion model is not possible using one standard forward pass of neural 5 2 0 2 M 2 1 ] . [ 1 3 7 5 9 0 . 3 0 5 2 : r Correspondence to Marianne Arriola: marriola@cs.cornell.edu Cornell Tech, NY, USA. 1Code: https://github.com/kuleshov-group/bd3lms Stanford University, CA, USA. Cohere, NY, USA. 1 Published as conference paper at ICLR 2025 Figure 1: Block diffusion sequentially generates blocks of tokens by performing diffusion within each block and conditioning on previous blocks. By combining strength from autoregressive and diffusion models, block diffusion overcomes the limitations of both approaches by supporting variable-length, higher-quality generation and improving inference efficiency with KV caching and parallel sampling. network and requires developing specialized algorithms. Second, training is hampered by the high variance of the gradients of the diffusion objective, causing BD3-LMs to under-perform autoregression even with block size of one (when both models should be equivalent). We derive estimators of gradient variance, and demonstrate that it is key contributor to the gap in perplexity between autoregression and diffusion. We then propose custom noise processes that minimize gradient variance and make progress towards closing the perplexity gap. We evaluate BD3-LMs on language modeling benchmarks, and demonstrate that they are able to generate sequences of arbitrary length, including lengths that exceed their training context. In addition, BD3-LMs achieve new state-of-the-art perplexities among discrete diffusion models. Compared to alternative semi-autogressive formulations that perform Gaussian diffusion over embeddings (Han et al., 2022; 2023), our discrete approach features tractable likelihood estimates and yields samples with improved generative perplexity using an order of magnitude fewer generation steps. In summary, our work makes the following contributions: We introduce block discrete diffusion language models, which are autoregressive over blocks of tokens; conditionals over each block are based on discrete diffusion. Unlike prior diffusion models, block diffusion supports variable-length generation and KV caching. We introduce custom training algorithms for block-autoregressive models that enable efficiently leveraging the entire batch of tokens provided to the model. We identify gradient variance as limiting factor of the performance of diffusion models, and we propose custom data-driven noise schedules that reduce gradient variance. Our results establish new state-of-the-art perplexity for discrete diffusion and make progress toward closing the gap to autoregressive models."
        },
        {
            "title": "2 BACKGROUND: LANGUAGE MODELING PARADIGMS",
            "content": "Notation We consider scalar discrete random variables with categories as one-hot column vectors in the space = {x {0, 1}V : (cid:80) xi = 1} for the simplex . Let the -th category denote special [MASK] token, where is its one-hot vector. We define x1:L as sequence of tokens, where xℓ for all tokens ℓ {1, . . . , L}, and use to denote the set of all such sequences. Throughout the work, we simplify notation and refer to the token sequence as and an individual token as xℓ. Finally, let Cat(; p) be categorical distribution with probability . 2 Published as conference paper at ICLR"
        },
        {
            "title": "2.1 AUTOREGRESSIVE MODELS\nConsider a sequence of L tokens x = (cid:2)x1, . . . , xL(cid:3) drawn from the data distribution q(x). Autore-\ngressive (AR) models define a factorized distribution of the form",
            "content": "log pθ(x) = (cid:88) ℓ=1 log pθ(xℓ x<ℓ), (1) where each pθ(xℓ x<ℓ) is parameterized directly with neural network. As result, AR models may be trained efficiently via next token prediction. However, AR models take steps to generate tokens due to the sequential dependencies."
        },
        {
            "title": "2.2 DISCRETE DENOISING DIFFUSION PROBABILISTIC MODELS",
            "content": "Diffusion models fit model pθ(x) to reverse forward corruption process (Sohl-Dickstein et al., 2015; Ho et al., 2020; Sahoo et al., 2024b). This process starts with clean data and defines latent variables xt = (cid:2)x1 (cid:3) for [0, 1], which represent progressively noisier versions of x. Given discretization into steps, we define s(j) = (j 1)/T and t(j) = j/T . For brevity, we drop from t(j) and s(j) below; in general, denotes the time step preceding t. , . . . , xL The D3PM framework (Austin et al., 2021) defines as Markov forward process acting indepens) where Qt RV is the diffusion matrix. dently on each token xℓ: q(xℓ t; Qtxℓ The matrix Qt can model various transformations, including masking, random token changes, and related word substitutions. s) = Cat(xℓ xℓ An ideal diffusion model pθ is the reverse of the process q. The D3PM framework defines pθ as pθ(xs xt) = (cid:89) ℓ=1 pθ(xℓ xt) = (cid:34) (cid:89) (cid:88) ℓ=1 q(xℓ xℓ t, xℓ)pθ(xℓ xt) (cid:35) , (2) where the denoising base model pθ(xℓ xt) predicts clean token xℓ given the noisy sequence xt, and the reverse posterior q(xℓ t, x) is defined following Austin et al. (2021) in Suppl. B.3. xℓ The diffusion model pθ is trained using variational inference. Let KL[] denote the Kullback-Leibler divergence. Then, the Negative ELBO (NELBO) is given by (Sohl-Dickstein et al., 2015): (cid:34) L(x; θ) = Eq log pθ(xxt(1)) + (cid:88) j= DKL[q(xs(j)xt(j), x)pθ(xs(j)xt(j))] + DKL[q(xt(T )x)pθ(xt(T ))] (cid:35) (3) This formalism extends to continuous time via Markov chain (CTMC) theory and admits score-based generalizations (Song & Ermon, 2019; Lou et al., 2024; Sun et al., 2022). Further simplifications (Sahoo et al., 2024a; Shi et al., 2024; Ou et al., 2025) tighten the ELBO and enhance performance."
        },
        {
            "title": "3 BLOCK DIFFUSION LANGUAGE MODELING",
            "content": "We explore class of Block Discrete Denoising Diffusion Language Models (BD3-LMs) that interpolate between autoregressive and diffusion models by defining an autoregressive distribution over blocks of tokens and performing diffusion within each block. We provide block diffusion objective for maximum likelihood estimation and efficient training and sampling algorithms. We show that for block size of one, the diffusion objective suffers from high variance despite being equivalent to the autoregressive likelihood in expectation. We identify high training variance as limitation of diffusion models and propose data-driven noise schedules that reduce the variance of the gradient updates during training. 3.1 BLOCK DIFFUSION DISTRIBUTIONS AND MODEL ARCHITECTURES We propose to combine the language modeling paradigms in Sec. 2 by autoregressively modeling blocks of tokens and performing diffusion within each block. We group tokens in into blocks of 3 Published as conference paper at ICLR 2025 length with = L/L (we assume that is an integer). We denote each block x(b1)L:bL from token at positions (b 1)L to bL for blocks {1, . . . , B} as xb for simplicity. Our likelihood factorizes over blocks as log pθ(x) = (cid:88) b=1 log pθ(xb x<b), (4) and each pθ(xb x<b) is modeled using discrete diffusion over block of tokens. Specifically, we define reverse diffusion process as in (2), but restricted to block b: pθ(xb xb t, x<b) = q(xb xb t, xb)pθ(xb xb t, x<b) (5) (cid:88) xb We obtain principled learning objective by applying the NELBO in (3) to each term in (4) to obtain log pθ(x) LBD(x; θ) := (cid:88) b=1 L(xb, x<b; θ), (6) where each L(xb, x<b; θ) is an instance of (3) applied to log pθ(xb x<b). Since the model is conditioned on x<b, we make the dependence on x<b, θ explicit in L. We denote the sum of these terms LBD(x; θ) (itself valid NELBO). t, x<b) using Model Architecture Crucially, we parameterize the base denoiser models pθ(xb xb t, x<b), single neural network xθ. The neural network xθ outputs not only the probabilities pθ(xb xb but also computational artifacts for efficient training. This will enable us to compute the loss LBD(x; θ) in parallel for all blocks in memory-efficient manner. Specifically, we parameterize xθ using transformer (Vaswani et al., 2017) with block-causal attention mask. The transformer xθ is applied t, x<b) to tokens, and tokens in block attend to tokens in blocks 1 to b. When xθ is trained, xb yields predictions for denoised tokens in block based on noised xb θ(xb and clean x<b. In autoregressive generation, it is normal to cache keys and values for previously generated tokens to avoid recomputing them at each step. Similarly, we use Kb, Vb to denote the keys and values at block b, and we define xθ to support these as input and output. The full signature of xθ is logits, Kb, Vb xb xb θ(xb t, K1:b1, V1:b1) := xb θ(xb t, x<b), (7) logits are the predictions for the clean xb, and Kb, Vb is the key-value cache in the forward where xb pass of xθ, and K1:b1, V1:b1 are keys and values cached on forward pass of xθ over x<b (hence the inputs x<b and K1:b1, V1:b1 are equivalent). 3.2 EFFICIENT TRAINING AND SAMPLING ALGORITHMS Ideally, we wish to compute the loss LBD(x; θ) in one forward pass of xθ. However, observe that denoising xb requires forward pass on this noisy input, while denoising the next blocks requires running xθ on the clean version xb. Thus every block has to go through the model at least twice. Training Based on this observation, we propose training algorithm with these minimal computational requirements (Alg. 1). Naively, we would compute the logits by applying xb t, K1:b-1, V1:b-1) in loop times. Instead, we precompute keys and values K1:B, V1:B for the full sequence in first forward pass (, K1:B, V1:B) xθ(x). In the second forward pass, we compute denoised t, K1:b-1, V1:b-1). Each token passes through xθ θ(xb predictions for all blocks simultaneously using xb twice. θ(xb Vectorized Training We propose vectorized implementation that computes LBD(x; θ) in one xB forward pass on the concatenation xnoisy of clean data with noisy data xnoisy = x1 tB t1 obtained by applying noise level tb to each block xb. We design an attention mask for xnoisy such that noisy tokens attend to other noisy tokens in their block and to all clean tokens in preceding blocks (see Suppl. B.6). Our method keeps the overhead of training BD3-LMs tractable and combines with pretraining to further reduce costs. 4 Published as conference paper at ICLR 2025 Sampling We sample one block at time, conditioned on previously sampled blocks (Alg 2). We may use any sampling procedure SAMPLE(xb θ, K1:b-1, V1:b-1) to sample from the conditional distribution pθ(xb t, x<b), where the context conditioning is generated using cross-attention with pre-computed keys and values K1:b1, V1:b1. Similar to AR models, caching the keys and values saves computation instead of recalculating them when sampling new block. sxb Notably, our block diffusion decoding algorithm enables us to sample sequences of arbitrary length, whereas diffusion models are restricted to fixed-length generation. Further, our sampler admits parallel generation within each block, whereas AR samplers are constrained to generate token-by-token. Algorithm 1 Block Diffusion Training Algorithm 2 Block Diffusion Sampling Input: datapoint x, # of blocks B, forward noise process qt(x), model xθ, loss LBD repeat qtb (xb) Sample t1, . . . , tB U[0, 1] {1, ..., B} : xb tb , K1:B, V1:B xθ(x) θ(xb logit, , xb b: xb tb Let xlogit x1 logit xB Take gradient step on θLBD(xlogit; θ) , K1:b-1, V1:b-1) logit KV cache until converged Input: # blocks B, model xθ, diffusion sampling algorithm SAMPLE x, K, for = 1 to do output & KV cache θ, K1:b-1, V1:b-1) xb SAMPLE(xb , Kb, Vb xb x1:b1 xb (K, V) (K1:b1 Kb, V1:b1 Vb) θ(xb) end for return x"
        },
        {
            "title": "4 UNDERSTANDING LIKELIHOOD GAPS BETWEEN DIFFUSION & AR MODELS",
            "content": "4.1 MASKED BD3-LMS The most effective diffusion language models leverage masking noise process (Austin et al., 2021; Lou et al., 2024; Sahoo et al., 2024a), where tokens are gradually replaced with special mask token. Here, we introduce masked BD3-LMs, special class of block diffusion models based on the masked diffusion language modeling framework (Sahoo et al., 2024a; Shi et al., 2024; Ou et al., 2025). t; αtxℓ + (1 αt)m) for More formally, we adopt per-token noise process q(xℓ tokens ℓ {1, . . . , L} where is one-hot encoding of the mask token, and αt [0, 1] is strictly decreasing function in t, with α0 = 1 and α1 = 0. We employ the linear schedule where the probability of masking token at time is 1 αt. We adopt the simplified objective from Sahoo et al. (2024a); Shi et al. (2024); Ou et al. (2025) (the full derivation is provided in Suppl. B.3): txℓ) = Cat(xℓ log pθ(x) LBD(x; θ) := (cid:88) b= Et[0,1]Eq α 1 αt log pθ(xbxb t, x<b) (8) where α is the instantaneous rate of change of αt under the continuous-time extension of (3) that takes . The NELBO is tight for = 1 but becomes looser approximation of the true log-likelihood for (see Suppl. B.5). 4.2 CASE STUDY: SINGLE TOKEN GENERATION Our block diffusion parameterization (8) is equivalent in expectation to the autoregressive NLL (1) in the limiting case where = 1 (see Suppl. B.4). Surprisingly, we find two point perplexity gap between our block diffusion model for = 1 and AR when training both models on the LM1B dataset. Although the objectives are equivalent in expectation, we show that the remaining perplexity gap is result of high training variance. Whereas AR is trained using the cross-entropy of tokens, our block diffusion model for = 1 only computes the cross-entropy for masked tokens xℓ = ℓ {1, . . . L} Table 1: Test perplexities for singletoken generation (PPL; ) across 16B tokens on LM1B. AR + random batch size BD3-LM = 1 + tuned schedule PPL () 22.88 24.37 25.56 22.88 Published as conference paper at ICLR 2025 Figure 2: Train NLLs for modeling the per-token likelihood on LM1B. Models are trained on 16B tokens. Training under the discrete diffusion NELBO, where half of the tokens in batch are masked on average, has similar training variance to an AR model with random batch size. so that EtU [0,1]q(xℓ = mxℓ) = 0.5. Thus, training on the diffusion objective involves estimating loss gradients with 2x fewer tokens and is responsible for higher training variance compared to AR. To close the likelihood gap, we train BD3-LM for = 1 by designing the forward process to = mxℓ) = 1. Under this schedule, the diffusion objective becomes fully mask tokens, i.e. q(xℓ equivalent to the AR objective (Suppl. B.4). In Table 1, we show that training under the block diffusion objective yields the same perplexity as AR training. Empirically, we see that this reduces the variance of the training loss in Figure 2. We verify that tuning the noise schedule reduces the variance of the objective by measuring Varx,t [LBD(x; θ)] after training on 328M tokens: while training on the NELBO results in variance of 1.52, training under full masking reduces the variance to 0.11. 4.3 DIFFUSION GAP FROM HIGH VARIANCE TRAINING Next, we formally describe the issue of gradient variance in training diffusion models. Given our empirical observations for single-token generation, we propose an estimator for gradient variance that we use to minimize the variance of diffusion model training for 1. While the NELBO is invariant to the choice of noise schedule (Suppl. B.3), this invariance does not hold for our Monte Carlo estimator of the loss used during training. As result, the variance of the estimator and its gradients are dependent on the schedule. First, we express the estimator of the NELBO with batch size K. We denote batch of sequences as = (cid:2)x(1), x(2), . . . , x(K)(cid:3), with each x(k) iid q(x). We obtain the batch NELBO estimator below, where t(k, b) is sampled in sequence and block b: LBD(X; θ) := l(X; θ) = 1 (cid:88) (cid:88) k=1 b=1 α t(k,b) 1 αt(k,b) log pθ (cid:16) x(k),b x(k),b t(k,b), x(k),<b(cid:17) (9) The variance of the gradient estimator over batches for each batch Xm {1, . . . , } is: (cid:13) (cid:13) θl(Xm; θ) (cid:13) (cid:13) (cid:13) VarX,t [θl(X; θ)] 1 1 θl(Xm; θ) 1 (cid:88) (cid:88) m=1 m=1 (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 (10)"
        },
        {
            "title": "5 LOW-VARIANCE NOISE SCHEDULES FOR BD3-LMS",
            "content": "5.1 INTUITION: AVOID EXTREME MASK RATES We aim to identify schedules that minimize the variance of the gradient estimator and make training most efficient. In masked setting, we want to mask random numbers of tokens, so that the model 6 Published as conference paper at ICLR 2025 learns to undo varying levels of noise, which is important during sampling. However, if we mask very few tokens, reconstructing them is easy and does not provide useful learning signal. If we mask everything, the optimal reconstruction are the marginals of each token in the data distribution, which is easy to learn, and again is not useful. These extreme masking rates lead to poor high-variance gradients: we want to learn how to clip them via simple and effective new class of schedules."
        },
        {
            "title": "5.2 CLIPPED SCHEDULES FOR LOW-VARIANCE GRADIENTS",
            "content": "We propose class of clipped noise schedules that sample mask rates 1 αt U[β, ω] for 0 β, ω 1. We argue that from the perspective of deriving Monte Carlo gradient estimates, these schedules are equivalent to continuous schedule where the mask probability is approximately 0 before the specified range such that 1 α<β ϵ and approximately 1 after the specified range is linear within the range: α 1 α>ω 1 ϵ. Consequently, α 1/(β ω)."
        },
        {
            "title": "5.3 DATA-DRIVEN CLIPPED SCHEDULES ACROSS BLOCK SIZES",
            "content": "As the optimal mask rates may differ depending on the block size L, we adaptively learn the schedule during training. While Kingma et al. (2021) perform variance minimization by isolating variance term using their squared diffusion loss, this strategy is not directly applicable to our variance estimator in Equation 10 since we seek to reduce variance across random batches in addition to random tb. Instead, we optimize parameters β, ω to directly minimize training variance. To limit the computational burden of the optimization, we use the variance of the estimator of the diffusion ELBO as proxy for the gradient estimator to optimize β, ω: minβ,ω VarX,t [L(X; θ, β, ω)]. We perform grid search at regular intervals during training to find the optimal β, ω (experimental details in Sec. 6). In Table 2, we show that variance of the diffusion NELBO is correlated with test perplexity. Under range of clipped noise rate distributions, we find that there exists unique distribution for each block size {4, 16, 128} that minimizes both the variance of the NELBO and the test perplexity. Table 2: Perplexities (PPLs; ) and variances of the NELBO VarX,t [LBD(X; θ)] (Var. NELBO; ). Models are trained on LM1B using linear schedule for 65B tokens, then finetuned for 10B tokens. U[0, .5] U[.3, .8] U[.5, 1] U[0, 1] 128 16 4 PPL Var. NELBO PPL Var. NELBO PPL Var. NELBO PPL Var. NELBO 31.72 31.27 29.23 1.03 7.90 32.68 31.78 31.19 29.37 1.35 3.62 10.39 31.92 31.29 29.16 1.83 3.63 8. 31.78 31.33 29.23 3.80 7.39 23."
        },
        {
            "title": "6 EXPERIMENTS",
            "content": "We evaluate BD3-LMs across standard language modeling benchmarks and demonstrate their ability to generate arbitrary-length sequences unconditionally. We pre-train base BD3-LM using the maximum block size = for 850K gradient steps and fine-tune under varying for 150K gradient steps on the One Billion Words dataset (LM1B; Chelba et al. (2014)) and OpenWebText (OWT; Gokaslan et al. (2019)). Details on training and inference are provided in Suppl C. To reduce the variance of training on the diffusion NELBO, we adaptively learn the range of masking rates by optimizing parameters β, ω as described in Section 5.3. In practice, we do so using grid search during every validation step (after 5K gradient upTable 3: Test perplexities (PPL; ) of models trained for 65B tokens on LM1B. Best diffusion value is bolded. PPL () Autoregressive Transformer-X Base (Dai et al., 2019) Transformer (Sahoo et al., 2024a) 23.5 22. Diffusion D3PM (absorb) (Austin et al., 2021) 82.34 32.68 SEDD (Lou et al., 2024) 31.78 MDLM (Sahoo et al., 2024a) Block diffusion (Ours) BD3-LMs = 16 = 8 = 4 30.60 29.83 28.23 7 Published as conference paper at ICLR 2025 dates) to identify β, ω: minβ,ω VarX,t [L(X; θ, β, ω)]. During evaluation, we report likelihood under uniformly sampled mask rates (8) as in Austin et al. (2021); Sahoo et al. (2024a)."
        },
        {
            "title": "6.1 LIKELIHOOD EVALUATION",
            "content": "On LM1B, BD3-LMs outperform all prior diffusion methods in Table 3. Compared to MDLM (Sahoo et al., 2024a), BD3-LMs achieve up to 13% improvement in perplexity. We observe similar trend on OpenWebText in Table 4. Table 4: Test perplexities (PPL; ) on OWT for models trained for 524B tokens. Best diffusion value is bolded. We also evaluate the ability of BD3-LMs to generalize to unseen datasets in zero-shot setting, following the benchmark from Radford et al. (2019). We evaluate the likelihood of models trained with OWT on datasets Penn Tree Bank (PTB; (Marcus et al., 1993)), Wikitext (Merity et al., 2016), LM1B, Lambada (Paperno et al., 2016), AG News (Zhang et al., 2015), and Scientific Papers (Pubmed and Arxiv subsets; (Cohan et al., 2018)). In Table 5, BD3LM achieves the best zero-shot perplexity on Pubmed, surpassing AR, and the best perplexity among diffusion models on Wikitext, LM1B, and AG News. 24.10 SEDD (Lou et al., 2024) MDLM (Sahoo et al., 2024a) 22.98 BD3-LMs = 16 = 8 = 4 22.27 21.68 20.73 AR (Sahoo et al., 2024a) PPL () 17.54 Table 5: Zero-shot validation perplexities () of models trained for 524B tokens on OWT. All perplexities for diffusion models are upper bounds. PTB Wikitext LM1B Lambada AG News Pubmed Arxiv AR SEDD MDLM BD3-LM = 4 81.07 96.33 90.96 96.81 25.32 35.98 33.22 31. 51.14 68.14 64.94 60.88 52.13 48.93 48.29 50.03 52.11 67.82 62.78 61. 48.59 45.39 43.13 42.52 41.22 40.03 37.89 39.20 6.2 SAMPLE QUALITY AND VARIABLE-LENGTH SEQUENCE GENERATION Table 6: Generation length statistics from sampling 500 documents from models trained on OWT. One key drawback of many existing diffusion language models (e.g,. Austin et al. (2021); Lou et al. (2024)) is that they cannot generate full-length sequences that are longer than the length of the output context chosen at training time. The OWT dataset is useful for examining this limitation, as it contains many documents that are longer than the training context length of 1024 tokens. We record generation length statistics of 500 variable-length samples in Table 6. We continue sampling tokens until an end-of-sequence token [EOS] is generated or sample quality significantly degrades (as measured by sample entropy). BD3-LMs generate sequences up to 10 longer than those of SEDD (Lou et al., 2024), which is restricted to the training context size. SEDD BD3-LM = 16 OWT train set AR Median Max # tokens # tokens 717 1021 798 131K 131K 1024 9982 We also examine the sample quality of BD3-LMs through quantitative and qualitative analyses. In Table 7, we generate sequences of lengths = 1024, 2048 and measure their generative perplexity under GPT2-Large. To sample = 2048 tokens from MDLM, we use their block-wise decoding technique (which does not feature block diffusion training as in BD3-LMs). We also compare to SSD-LM (Han et al., 2022), an alternative block diffusion formulation. Unlike our discrete diffusion framework, SSD-LM uses Gaussian diffusion and does not support likelihood estimation. Further, BD3-LM adopts an efficient sampler from masked diffusion, where the number of generation steps (NFEs) is upper-bounded by since tokens are never remasked (Sahoo et al., 2024a; Ou et al., 2025). For SSD-LM, we compare sample quality using = 1K diffusion steps per block, matching their experimental setting (yielding 40K NFEs), and = 25 where NFEs are comparable across methods. Published as conference paper at ICLR 2025 Table 7: Generative perplexity (Gen. PPL; ) and number of function evaluations (NFEs; ) of 300 samples of lengths = 1024, 2048. All models are trained on OWT (AR, SEDD, MDLM are trained on 524B tokens, SSD-LM is pre-trained on 122B tokens). Best diffusion value is bolded. We provide further inference details on reporting generative perplexity and NFEs in Supp. C.5. Model AR Diffusion SEDD MDLM Block Diffusion SSD-LM = BD3-LMs = 16 = 8 = 4 = 1024 = 2048 Gen. PPL NFEs Gen. PPL NFEs 14.1 52.0 46. 37.2 281.3 33.4 30.4 25.7 1K 1K 1K 40K 1K 1K 1K 1K 13.2 41. 35.3 281.9 31.5 28.2 23.6 2K 2K 80K 2K 2K 2K 2K BD3-LMs achieve the best generative perplexities compared to previous diffusion methods. Relative to SSD-LM, our discrete approach yields samples with improved generative perplexity using an order of magnitude fewer generation steps. We also qualitatively examine samples taken from BD3-LM and baselines (AR, MDLM) trained on the OWT dataset; we report samples in Suppl. D. We observe that BD3-LM samples have higher coherence than MDLM samples and approach the quality of AR. 6.3 ABLATIONS We assess the impact of the design choices in our proposed block diffusion recipes, namely 1) selection of the noise schedule and 2) the efficiency improvement of the proposed training algorithm relative to naive implementation. SELECTING NOISE SCHEDULES TO REDUCE TRAINING VARIANCE Compared to the linear schedule used in Lou et al. (2024); Sahoo et al. (2024a), training under clipped noise schedules is the most effective for reducing the training variance which correlates with test perplexity. In Table 8, the ideal clipped masking rates, which are optimized during training, are specific to the block size and further motivate our optimization. Relative to other standard noise schedules (Chang et al., 2022), clipped masking achieves the best training variance and test perplexity. Since heavier masking is effective for the smaller block size = 4, we compare with logarithmic and square root schedules that also encourage heavy masking. As lighter masking is optimal for block size = 16, we compare with square and cosine schedules. EFFICIENCY OF TRAINING ALGORITHM In the BD3-LM training algorithm (Sec. 3.2), we compute xlogit using two options. We may perform two forward passes through the network (precomputing keys and values for the full sequence x, then computing denoised predictions), or combine these passes by concatenating the two inputs into the same attention kernel. We find that performing this operation in single forward pass is often more efficient as we reduce memory bandwidth bottlenecks by leveraging efficient attention Table 8: Effect of the noise schedule on likelihood estimation. We finetune BD3-LMs on 3B tokens from LM1B and evaluate on linear schedule. For clipped schedules, we compare optimal clipping for = 4, 16. Noise schedule PPL Var. NELBO = 4 Clipped U[0.45, 0.95] U[0.3, 0.8] Linear U[0, 1] Logarithmic Square root = 16 Clipped U[0.45, 0.95] U[0.3, 0.8] Linear U[0, 1] Square Cosine 29.21 29.38 30.18 30.36 31.41 31.42 31.12 31.72 31.43 31.41 6.24 10.33 23.45 23.53 26. 3.60 3.58 7.62 13.03 13.00 9 Published as conference paper at ICLR 2025 kernels (Dao et al., 2022; Dong et al., 2024), see Suppl. B.7. Instead of paying the cost of two passes through the denoising network, we only pay the cost of more expensive attention operation. Empirically we see that this vectorized approach has 20-25% speed-up during training relative to performing two forward passes."
        },
        {
            "title": "7 DISCUSSION AND PRIOR WORK",
            "content": "Comparison to D3PM Block diffusion builds off D3PM (Austin et al., 2021) and applies it to each autoregressive conditional. We improve over D3PM in three ways: (1) we extend D3PM beyond fixed sequence lengths; (2) we study the perplexity gap of D3PM and AR models, identify gradient variance as contributor, and design variance-minimizing schedules; (3) we improve over the perplexity of D3PM models. Our work applies to extensions of D3PM (He et al., 2022; Lou et al., 2024) including ones in continuous time (Campbell et al., 2022; Sun et al., 2022). Comparison to MDLM BD3-LMs further make use of the perplexity-enhancing improvements in MDLM (Sahoo et al., 2024a; Shi et al., 2024; Ou et al., 2025). We also build upon MDLM: (1) while Sahoo et al. (2024a) point out that their NELBO is invariant to the noise schedule, we show that the noise schedule has significant effect on gradient variance; (2) we push the state-of-the-art in perplexity beyond MDLM. Note that our perplexity improvements stem not only from block diffusion, but also from optimized schedules, and could enhance standard MDLM and D3PM models. Comparison to Gaussian Diffusion Alternatively, one may perform diffusion over continuous embeddings of discrete tokens (Li et al., 2022; Dieleman et al., 2022; Chen et al., 2022). This allows using algorithms for continuous data (Song et al., 2020; Ho & Salimans, 2022), but yields worse perplexity (Graves et al., 2023; Gulrajani & Hashimoto, 2024). Comparison to Semi-Autoregressive Diffusion Han et al. (2022; 2023) introduced block formulation of Gaussian diffusion. Our approach instead extends Austin et al. (2021), and features: (1) tractable likelihood estimates for principled evaluation; (2) faster generation, as our number of model calls is bounded by the number of generated tokens, while SSD-LM performs orders of magnitude more calls; (3) improved sample quality. AR-Diffusion (Wu et al., 2023) extends SSD-LM with left-to-right noise schedule; Chen et al. (2025) apply semi-autoregressive noise to decision traces and videos; Hao et al. (2024); Kong et al. (2025) consider extensions to latent reasoning. Autoregressive diffusion models (Hoogeboom et al., 2021b;a) extend any-order AR models (AOARMs; Uria et al. (2014)) to support parallel sampling. Zheng et al. (2024) prove equivalence between MDLM and AO-ARM training. Further extensions of ARMs that compete with diffusion include iterative editing (Gu et al., 2019), parallel and speculative decoding (Gu et al., 2017; Santilli et al., 2023; Cai et al., 2024; Gloeckle et al., 2024), consistency training (Kou et al., 2024), guidance (Sanchez et al., 2023), and cross-modal extensions (Liu et al., 2023; Tian et al., 2025). Limitations Training BD3-LMs is more expensive than regular diffusion training. We propose vectorized algorithm that keeps training speed within <2x of diffusion training speed; in our experiments, we also pre-train with standard diffusion loss to further reduce the speed gap. Additionally, BD3-LMs generate blocks sequentially, and hence may face the same speed and controllability constraints as AR especially when blocks are small. Their optimal block size is task specific (e.g., larger for greater control). BD3-LMs are subject to inherent limitations of generative models, including hallucinations (Achiam et al., 2023), copyright infringement (Gokaslan et al., 2024), controllability (Schiff et al., 2024; Wang et al., 2023) and harmful outputs (Bai et al., 2022)."
        },
        {
            "title": "8 CONCLUSION",
            "content": "This work explores block diffusion and is motivated by two problems with existing discrete diffusion: the need to generate arbitrary-length sequences and the perplexity gap to autoregressive models. We introduce BD3-LMs, which represent block-wise extension of the D3PM framework (Austin et al., 2021), and leverage specialized training algorithm and custom noise schedules that further improve performance. We observe that in addition to being able to generate long-form documents, these models also improve perplexity, setting new state-of-the-art among discrete diffusion models. 10 Published as conference paper at ICLR"
        },
        {
            "title": "ACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING",
            "content": "This work was partially funded by the National Science Foundation under awards DGE-1922551, CAREER awards 2046760 and 2145577, and by the National Institute of Health under award MIRA R35GM151243. Marianne Arriola is supported by NSF Graduate Research Fellowship under award DGE-2139899 and Hopper-Dean/Bowers CIS Deans Excellence Fellowship. We thank Databricks MosaicML for providing access to computational resources."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:1798117993, 2021. Pavel Avdeyev, Chenlai Shi, Yuhao Tan, Kseniia Dudnyk, and Jian Zhou. Dirichlet diffusion score model for biological sequence generation. In International Conference on Machine Learning, pp. 12761301. PMLR, 2023. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:2826628279, 2022. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1131511325, 2022. Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling, 2014. Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2025. Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202, 2022. Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. discourse-aware attention model for abstractive summarization of long documents. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), 2018. doi: 10.18653/v1/n18-2097. URL http://dx.doi.org/10.18653/v1/n18-2097. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond fixed-length context. arXiv preprint arXiv:1901.02860, 2019. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. 11 Published as conference paper at ICLR 2025 Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021. URL https://arxiv.org/abs/2105.05233. Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. Continuous diffusion for categorical data. arXiv preprint arXiv:2211.15089, 2022. Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: programming model for generating optimized attention kernels. arXiv preprint arXiv:2412.05496, 2024. Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024. Shrey Goel, Vishrut Thoutam, Edgar Mariano Marroquin, Aaron Gokaslan, Arash Firouzbakht, Sophia Vincoff, Volodymyr Kuleshov, Huong Kratochvil, and Pranam Chatterjee. Memdlm: De novo membrane protein design with masked discrete diffusion protein language models. arXiv preprint arXiv:2410.16735, 2024. Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http: //Skylion007.github.io/OpenWebTextCorpus, 2019. Aaron Gokaslan, Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel, Jonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov. Commoncanvas: Open diffusion models trained on creative-commons images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82508260, 2024. Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, and Faustino Gomez. Bayesian flow networks. arXiv preprint arXiv:2308.07037, 2023. Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017. Jiatao Gu, Changhan Wang, and Junbo Zhao. Levenshtein transformer. Advances in neural information processing systems, 32, 2019. Ishaan Gulrajani and Tatsunori Hashimoto. Likelihood-based diffusion language models. Advances in Neural Information Processing Systems, 36, 2024. Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José Lezama. Photorealistic video generation with diffusion models, 2023. URL https: //arxiv.org/abs/2312.06662. Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control. arXiv preprint arXiv:2210.17432, 2022. Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov, and Marjan Ghazvininejad. David helps goliath: Inference-time collaboration between small specialized and large general diffusion lms. arXiv preprint arXiv:2305.14771, 2023. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. Diffusionbert: Improving generative masked language models with diffusion models. arXiv preprint arXiv:2211.15029, 2022. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 12 Published as conference paper at ICLR Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. arXiv:2204.03458, 2022. Emiel Hoogeboom, Alexey Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. Autoregressive diffusion models. arXiv preprint arXiv:2110.02037, 2021a. Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:1245412465, 2021b. Daniel Israel, Aditya Grover, and Guy Van den Broeck. Enabling autoregressive models to fill in masked tokens. arXiv preprint arXiv:2502.06901, 2025. Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. Deqian Kong, Minglu Zhao, Dehong Xu, Bo Pang, Shu Wang, Edouardo Honig, Zhangzhang Si, Chuan Li, Jianwen Xie, Sirui Xie, et al. Scalable language models with posterior inference of latent thought vectors. arXiv preprint arXiv:2502.01567, 2025. Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, and Hao Zhang. CLLMs: Consistency large language models. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=8uzBOVmh8H. Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35: 43284343, 2022. Xiner Li, Yulai Zhao, Chenyu Wang, Gabriele Scalia, Gokcen Eraslan, Surag Nair, Tommaso Biancalani, Shuiwang Ji, Aviv Regev, Sergey Levine, et al. Derivative-free guidance in continuous and discrete diffusion models with soft value-based decoding. arXiv preprint arXiv:2408.08252, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=CNicRIVIPA. Mitch Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313330, 1993. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. Hunter Nisonoff, Junhao Xiong, Stephan Allenspach, and Jennifer Listgarten. Unlocking guidance for discrete state-space diffusion and flow models. arXiv preprint arXiv:2406.01572, 2024. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean In The Thirteenth International Conference on Learning Representations, 2025. URL data. https://openreview.net/forum?id=sMyXP8Tanm. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15251534, Berlin, Germany, August 2016. Association for Computational Linguistics. URL http://www. aclweb.org/anthology/P16-1144. 13 Published as conference paper at ICLR 2025 William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Subham Sekhar Sahoo, Marianne Arriola, Aaron Gokaslan, Edgar Mariano Marroquin, Alexander Rush, Yair Schiff, Justin Chiu, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024a. URL https://openreview.net/forum?id=L4uaAR4ArM. Subham Sekhar Sahoo, Aaron Gokaslan, Christopher De Sa, and Volodymyr Kuleshov. Diffusion models with learned adaptive noise. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. URL https://openreview.net/forum?id=loMa99A4p8. Subham Sekhar Sahoo, John Xavier Morris, Aaron Gokaslan, Srijeeta Biswas, Vitaly Shmatikov, and Volodymyr Kuleshov. Zero-order diffusion guidance for inverse problems, 2024c. URL https://openreview.net/forum?id=JBgBrnhLLL. Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka Ammanamanchi, and Stella Biderman. Stay on topic with classifier-free guidance. arXiv preprint arXiv:2306.17806, 2023. Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodola. Accelerating transformer inference for translation via parallel decoding. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1233612355, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.689. URL https://aclanthology.org/2023.acl-long. 689. Yair Schiff, Subham Sekhar Sahoo, Hao Phung, Guanghan Wang, Sam Boshar, Hugo Dalla-torre, Bernardo de Almeida, Alexander Rush, Thomas Pierrot, and Volodymyr Kuleshov. Simple guidance mechanisms for discrete diffusion models. arXiv preprint arXiv:2412.10193, 2024. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=xcqSOfHt4g. Phillip Si, Allan Bishop, and Volodymyr Kuleshov. Autoregressive quantile flows for predictive uncertainty estimation. In International Conference on Learning Representations, 2022. Phillip Si, Zeyi Chen, Subham Sekhar Sahoo, Yair Schiff, and Volodymyr Kuleshov. SemiIn Inautoregressive energy flows: exploring likelihood-free training of normalizing flows. ternational Conference on Machine Learning, pp. 3173231753. PMLR, 2023. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time discrete diffusion models. arXiv preprint arXiv:2211.16750, 2022. 14 Published as conference paper at ICLR Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2025. Benigno Uria, Iain Murray, and Hugo Larochelle. deep and tractable density estimator. In International Conference on Machine Learning, pp. 467475. PMLR, 2014. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Yingheng Wang, Yair Schiff, Aaron Gokaslan, Weishen Pan, Fei Wang, Christopher De Sa, and Volodymyr Kuleshov. InfoDiffusion: Representation learning using information maximizing diffusion models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 3633636354. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/wang23ah.html. Tong Wu, Zhihao Fan, Xiao Liu, Hai-Tao Zheng, Yeyun Gong, yelong shen, Jian Jiao, Juntao Li, zhongyu wei, Jian Guo, Nan Duan, and Weizhu Chen. AR-diffusion: Auto-regressive diffusion model for text generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=0EG6qUQ4xE. Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NIPS, 2015. Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024. 15 Published as conference paper at ICLR"
        },
        {
            "title": "CONTENTS",
            "content": "1 Introduction 2 Background: Language Modeling Paradigms"
        },
        {
            "title": "2.1 Autoregressive Models .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.2 Discrete Denoising Diffusion Probabilistic Models",
            "content": ". . . . . . . . . . . . . . . . . 3 Block Diffusion Language Modeling"
        },
        {
            "title": "3.2 Efficient Training and Sampling Algorithms . . . . . . . . . . . . . . . . . . . . .",
            "content": "4 Understanding Likelihood Gaps Between Diffusion & AR Models 4.1 Masked BD3-LMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Case Study: Single Token Generation . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Diffusion Gap from High Variance Training . . . . . . . . . . . . . . . . . . . . . 5 Low-Variance Noise Schedules for BD3-LMs 5.1 Intuition: Avoid Extreme Mask Rates . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Clipped Schedules for Low-Variance Gradients . . . . . . . . . . . . . . . . . . . 5.3 Data-Driven Clipped Schedules Across Block Sizes . . . . . . . . . . . . . . . . . 6 Experiments 6.1 Likelihood Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Sample Quality and Variable-Length Sequence Generation . . . . . . . . . . . . . 6.3 Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Discussion and Prior Work 8 Conclusion Block Diffusion NELBO Masked BD3-LMs B.1 Forward Process . B.2 Reverse Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Simplified NELBO for Masked Diffusion Processes . . . . . . . . . . . . . . . . . B.4 Recovering the NLL from the NELBO for Single Token Generation . . . . . . . . B.5 Tightness of the NELBO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.6 Specialized Attention Masks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.7 Optimized Attention Kernel with FlexAttention . . . . . . . . . . . . . . . . . . . Experimental Details 16 2 3 3 3 3 5 5 5 6 6 7 7 7 8 8 10 10 17 17 18 18 19 20 20 21 Published as conference paper at ICLR 2025 C.1 Datasets . . . C.2 Architecture . C.3 Training . . . . . . . . . . . . . . . . . . C.4 Likelihood Evaluation . C.5 Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Samples"
        },
        {
            "title": "A BLOCK DIFFUSION NELBO",
            "content": "24 24 24 24 25 Below, we provide the Negative ELBO (NELBO) for the block diffusion parameterization. Recall that the sequence x1:L = (cid:2)x1, . . . , xL(cid:3) is factorized over blocks, which we refer to as for simplicity, drawn from the data distribution q(x). Specifically, we will factorize the likelihood over blocks of length L, then perform diffusion in each block over discretization steps. Let DKL[] to denote the Kullback-Leibler divergence, t, be shorthand for t(i) = i/T and s(i) = t(i 1)/T [1, ]. We derive the NELBO as follows: log pθ(x) = = = (cid:88) b= (cid:88) b=1 (cid:88) b=1 (cid:20) (cid:88) b= log pθ(xbx<b) log Eq log Eq pθ(xb q(xb pθ(xb t(1):t(T )x<b) t(1):t(T )xb) t(T )x<b) (cid:81)T (cid:81)T i=1 pθ(xb t(i)xb s(i)xb s(i)) i=1 q(xb t(i), x<b) Eq log pθ(xbxb (cid:124) (cid:123)(cid:122) Lrecons t= 1 + (cid:124) t{ 2 ,..., 1 ,1} , x<b) (cid:125) EqT DKL (cid:0)q(xb t=1xb) pθ(xb (cid:123)(cid:122) Lprior (cid:0)q(xb (cid:123)(cid:122) Ldiffusion (cid:21) t=1)(cid:1) (cid:125) sxb t, xb) pθ(xb sxb t, x<b)(cid:1) (cid:125) (11) + DKL (cid:124) MASKED BD3-LMS We explore specific class of block diffusion models that builds upon the masked diffusion language modeling framework. In particular, we focus on masking diffusion processes introduced by Austin et al. (2021) and derive simplified NELBO under this framework as proposed by Sahoo et al. (2024a); Shi et al. (2024); Ou et al. (2025). First, we define the diffusion matrix Qt for states {1, . . . , }. Consider the noise schedule function αt [0, 1], which is strictly decreasing function in satisfying α0 = 1 and α1 = 0. Denote the mask index as = . The diffusion matrix is defined by Austin et al. (2021) as: [Qt]ij = 1 αt 1 αt if = = if = = if = m, = (12) 17 Published as conference paper at ICLR The diffusion matrix for the forward marginal Qts is: [Qts]ij = 1 αts 1 αts if = = if = = if = m, = (13) where αts = αt/αs. B.1 FORWARD PROCESS Under the D3PM framework (Austin et al., 2021), the forward noise process applied independently for each token ℓ {1, . . . L} is defined using diffusion matrices Qt RV as t; Qtxℓ(cid:1) , with Qt = Qt(1)Qt(2) . . . Qt(i) txℓ) = Cat (cid:0)xℓ q(xℓ (14) B.2 REVERSE PROCESS Let Qts denote the diffusion matrix for the forward marginal. We obtain the reverse posterior q(xℓ t, xℓ) using the diffusion matrices: xℓ q(xℓ sxℓ t, xℓ) = q(xℓ txℓ sxℓ) s, xℓ)q(xℓ q(xℓ txℓ) (cid:32) = Cat xℓ s; xℓ Qtsxℓ (xℓ t)Q xℓ (cid:33) (15) where denotes the Hadmard product between two vectors. B.3 SIMPLIFIED NELBO FOR MASKED DIFFUSION PROCESSES Following Sahoo et al. (2024a); Shi et al. (2024); Ou et al. (2025), we simplify the NELBO in the case of masked diffusion processes. Below, we provide the outline of the NELBO derivation; see the full derivation in Sahoo et al. (2024a); Shi et al. (2024); Ou et al. (2025). We will first focus on simplifying the diffusion loss term Ldiffusion in Eq. 11. We employ the SUBSparameterization proposed in Sahoo et al. (2024b) which simplifies the denoising model pθ for masked diffusion. In particular, we enforce the following constraints on the design of pθ by leveraging the {xℓ, m} ℓ {1, . . . , L}. fact that there only exists two possible states in the diffusion process xℓ 1. Zero Masking Probabilities. We set pθ(xℓ = mxℓ t) = 0 (as the clean sequence doesnt contain masks). 2. Carry-Over Unmasking. The true posterior for the case where xℓ = m) = 1 (if token is unmasked in the reverse process, it is never remasked). Thus, we simplify the denoising model by setting pθ(xℓ = is q(xℓ = xℓ = m) = 1. = xℓ txℓ txℓ As result, we will only approximate the posterior pθ(xℓ the ℓ-th position in block {1, . . . , B}. The diffusion loss term becomes: = xℓxℓ = m). Let xb,ℓ denote token in Ldiffusion = = (cid:88) b= (cid:88) b=1 EtEqT (cid:2)DKL (cid:2)q(xb sxb t, xb)pθ(xb sxb t, x<b)(cid:3)(cid:3) (cid:88) (cid:104) DKL q(xb,ℓ xb,ℓ , xb,ℓ)pθ(xb,ℓ xb (cid:105) t, x<b) EtEqT ℓ=1 DKL is simply the discrete-time diffusion loss for the block b; hence, from Sahoo et al. (2024a) (Suppl. B.1), we get: = = (cid:88) b=1 (cid:88) b= EtEqT (cid:88) ℓ=1 αt αs 1 αt log pθ(xb,ℓ xb,ℓ , x<b) EtEqT (cid:20) αt αs 1 αt log pθ(xb xb (cid:21) t, x<b) (16) 18 Published as conference paper at ICLR 2025 Lastly, we obtain tighter approximation of the likelihood by taking the diffusion steps (Sahoo et al., 2024a), for which (αt αs) = α t: (cid:20) α 1 αt (cid:21) t, x<b) log pθ(xb xb Et[0,1]Eq Ldiffusion = (cid:88) (17) b=1 For the continuous time case, Sahoo et al. (2024a) (Suppl. A.2.4) show the reconstruction loss reduces to 0 as xb = Cat(.; xb). Using this, we obtain: .; xb (cid:17) (cid:16) t(1) limT Cat t= 1 Lrecons = Eq log pθ(xbxb t(1), x<b) t(1) = xb, x<b) = log pθ(xbxb = 0 The prior loss Lprior = DKL ensures q(xb t=1xb) = Cat(.; m) and pθ(xb t=1xb) pθ(xb (cid:0)q(xb (18) t=1)(cid:1) also reduces to 0 because αt=1 = 0 which t=1) = Cat(.; m); see Sahoo et al. (2024a) (Suppl. A.2.4). Finally, we obtain simple objective that is weighted average of cross-entropy terms: LBD(x; θ) = (cid:88) b=1 Et[0,1]Eq (cid:20) α 1 αt log pθ(xb xb (cid:21) t, x<b) (19) The above NELBO is invariant to the choice of noise schedule αt; see Sahoo et al. (2024a) (Suppl. E.1.1). B.4 RECOVERING THE NLL FROM THE NELBO FOR SINGLE TOKEN GENERATION Consider the block diffuson NELBO for block size of 1 where = 1, = L. The block diffusion NELBO is equivalent to the AR NLL when modeling single token: log p(x) (cid:88) Et[0,1]Eq (cid:20) α 1 αt log pθ(xb xb t, x<b) (cid:21) b=1 = 1 and αt = 1 t, α Et[0,1]Eq (cid:20) 1 log pθ(xb xb t, x<b) (cid:21) (cid:88) b=1 (cid:88) = = Et[0,1] b=1 Expanding Eq[.], (cid:88) = Et[0,1] b=1 Eq (cid:2)log pθ(xb xb t, x<b)(cid:3) (cid:20) q(xb = mxb) log pθ(xb xb = m, x<b) 1 1 + q(xb = xbxb) log pθ(xb xb (cid:21) = xb, x<b) (20) Recall that our denoising model employs the SUBS-parameterization proposed in Sahoo et al. (2024b). The carry-over unmasking property ensures that log pθ(xb xb = xb, x<b) = 0, as an unmasked token is simply copied over from from the input of the denoising model to the output. Hence, (20) reduces to following: log pθ(x) (cid:88) Et[0,1] 1 q(xb = mxb) log pθ(xb xb = m, x<b) b=1 = mxb) = t, we get: q(xb (cid:88) = Et[0,1] log pθ(xb xb = m, x<b) b=1 Published as conference paper at ICLR 2025 = (cid:88) b=1 log pθ(xb m, x<b) (21) For single-token generation (L = 1) we recover the autoregressive NLL. B.5 TIGHTNESS OF THE NELBO For block sizes 1 L, we show that -log p(x) LK LK+1. Consider = 1, where we recover the autoregressive NLL (see Suppl B.4): L1 = (cid:88) b= log Et[0,1]Eq α 1 αt pθ(xb xb t, x<b) = (cid:88) b=1 log pθ(xb m, x<b) Consider the ELBO for block size = 2: L2 = L/2 (cid:88) b= log Et[0,1]Eq α 1 αt pθ(xb xb t, x<b) (22) (23) We show that L1 L2, and this holds for all 1 by induction. Let xb,ℓ correspond to the token in position ℓ [1, L] of block b. We derive the below inequality: (cid:88) b=1 log pθ(xb m, x<b) = = = L/2 (cid:88) b=1 L/2 (cid:88) b=1 L/2 (cid:88) log Et[0,1]Eq 1 1 αt pθ(xb xb t, x<b) log Et[0,1]Eq 2 (cid:89) i=1 1 1 αt pθ(xb,ℓ xb t, x<b) 2 (cid:89) log Et[0,1]Eq b=1 i=1 L/2 (cid:88) 2 (cid:88) b=1 i=1 log Et[0,1]Eq 1 1 αt 1 1 αt pθ(xb,ℓ xb t, x<b) pθ(xb,ℓ xb t, x<b) (24) B.6 SPECIALIZED ATTENTION MASKS t, x<b) for all blocks [1, B] simultaneously We aim to model conditional probabilities pθ(xb xb by designing an efficient training algorithm with our transformer backbone. However, modeling all conditonal terms requires processing both the noised sequence xb and the conditional context x<b for all b. Rather than calling the denoising network times, we process both sequences simultaneously by concatenating them xfull xt as input to transformer. We update this sequence xf ull of length 2L tokens using custom attention mask Mfull {0, 1}2L2L for efficient training. The full attention mask is comprised of four smaller attention masks: Mfull = (cid:20)MBD MOBC 0 MBC (cid:21) where MBD and MOBC are used to update the representation of xt and MBC is used to update the representation of x. We define these masks as follows: MBD (Block-diagonal mask): Self-attention mask within noised blocks xb (cid:26) 1 if i, are in the same block otherwise [MBD]ij = 20 Published as conference paper at ICLR 2025 MOBC (Offset block-causal mask): Cross-attention to conditional context x<b [MOBC]ij = (cid:26) 1 0 if belongs in block before otherwise MBC (Block-causal mask): Attention mask for updating xb [MBC]ij = (cid:26) 1 0 if belongs in the same block as i, or block before otherwise We visualize an example attention mask for = 6 and block size = 2 in Figure 3. Block Diagonal (MBD) Offset Block Causal (MOBC) Block Causal (MBC) x1 x2 x3 x1 x2 x3 x1 x2 x3 x1 x2 x3 Figure 3: Example of Specialized Attention Mask B.7 OPTIMIZED ATTENTION KERNEL WITH FLEXATTENTION As Figure 3 demonstrates, our attention matrix is extremely sparse. We can exploit this sparsity to massively improve the efficiency of BD3-LMs. FlexAttention (Dong et al., 2024) is compiler-driven programming model that enables efficient implementation of attention mechanisms with structured sparsity in PyTorch. It provides flexible interface for defining custom attention masks while maintaining high performance comparable to manually optimized attention kernels. Below in Fig. 4 we define block-wise attention mask, block_diff_mask, based on its definition as Mfull {0, 1}2L2L in Suppl. B.6. We fuse the attention operations into single FlexAttention kernel designed to exploit the sparsity in our attention matrix to increase computational efficiency. By doing so, we perform the following optimizations: Precomputed Block Masking: The create_block_mask utility generates sparse attention mask at compile-time, avoiding per-step computation of invalid attention entries. Through sparsity-aware execution, FlexAttention kernels reduce the number of FLOPs in the attention computation. Reduced Memory Footprint: By leveraging block-level sparsity, the attention mechanism avoids full materialization of large-scale attention matrices, significantly reducing memory overhead. FlexAttention minimizes memory accesses by skipping fully masked blocks. Optimized Computation via torch.compile: The integration of torch.compile enables kernel fusion and efficient execution on GPUs by generating optimized Triton-based kernels. This efficiently parallelizes masked attention computations using optimized GPU execution paths. Published as conference paper at ICLR 2025 def block_diff_mask(b, h, q_idx, kv_idx, block_size, n): \"\"\" Constructs the specialized block diffusion attention mask composed of - **Block Diagonal Mask (M_BD)**: Self-attention within noised blocks - **Offset Block Causal Mask (M_OBC)**: Cross-attention for conditional context - **Block Causal Mask (M_BC)**: Attention to update three masks: Args: b, h: Batch and head indices (ignored for mask logic). q_idx, kv_idx: Query and Key indices. block_size: Defines the block structure. n: Sequence length of x_0 and x_t Returns: boolean attention mask. \"\"\" # Indicate whether token belongs to xt (0) or x0 (1) x0_flag_q = (q_idx >= n) x0_flag_kv = (kv_idx >= n) # Compute block indices block_q = torch.where(x0_flag_q == 1, block_kv = torch.where(x0_flag_kv == 1, (q_idx - n) // block_size, q_idx // block_size) (kv_idx - n) // block_size, kv_idx // block_size) # **1. Block Diagonal Mask (M_BD) ** block_diagonal = (block_q == block_kv) & (x0_flag_q == x0_flag_kv) # **2. Offset Block-Causal Mask (M_OBC) ** offset_block_causal = ( (block_q > block_kv) & (x0_flag_q == 0) & (x0_flag_kv == 1) ) # **3. Block-Causal Mask (M_BC) ** block_causal = ( (block_q >= block_kv) & (x0_flag_q == 1) & (x0_flag_kv == 1) ) # **4. Combine Masks ** return block_diagonal offset_block_causal block_causal Figure 4: We can adapt the masking strategy from Fig. 3 to FlexAttention compatible sparse masking function as above. This enables the creation of customized JIT attention operation that uses significantly less memory with up to 5X speedup over the naive native scaled_dot_product_attention implementation in PyTorch ( 2.5) on A5000 GPU with = 1024 and batch size = 16. 22 Published as conference paper at ICLR 2025 from torch.nn.attention.flex_attention import flex_attention, create_block_mask from functools import partial # Define block-wise attention mask my_block_diff_mask = partial(block_diff_mask, seq_len=seq_len, block_size =block_size) # Generate optimized sparse block mask block_mask = create_block_mask(my_block_diff_mask, None, None, seq_len*2, seq_len*2, device=device) # Compute attention using FlexAttention # Use no-cudagraphs to avoid an extra copy on small compile graphs. # Use max-autotune if compiling larger model all at once. @torch.compile(fullgraph=True, mode=\"max-autotune-no-cudagraphs\") def single_pass_block_diff_attn(q, k, v, block_mask): return flex_attention(q, k, v, block_mask=block_mask) Figure 5: Attention computation using FlexAttention with our proposed custom mask. This implementation exploits FlexAttentions ability to dynamically optimize execution based on the provided sparsity pattern. By precomputing block-level sparsity and leveraging efficient kernel fusion, it enables scalable attention computation for long sequences. Overall, this approach provides principled method to accelerate attention computations while preserving structured dependency constraints. End-to-end, replacing FlashAttention kernels using custom mask with FlexAttention kernels leads to 15% speedup in model forward pass. We use single A5000 for = 1024 and batch size = 16. 23 Published as conference paper at ICLR"
        },
        {
            "title": "C EXPERIMENTAL DETAILS",
            "content": "We closely follow the same training and evaluation setup as used by Sahoo et al. (2024a). C.1 DATASETS We conduct experiments on two datasets: The One Billion Word Dataset (LM1B; Chelba et al. (2014)) and OpenWebText (OWT; Gokaslan et al. (2019)). Models trained on LM1B use the bert-base-uncased tokenizer and context length of 128. We report perplexities on the test split of LM1B. Models trained on OWT use the GPT2 tokenizer Radford et al. (2019) and context length of 1024. Since OWT does not have validation split, we leave the last 100k documents for validation. In preparing LM1B examples, Sahoo et al. (2024a) pad each example to fit in the context length of = 128 tokens. Since most examples consist of only single sentence, block diffusion modeling for larger block sizes > 4 would not be useful for training. Instead, we concatenate and wrap sequences to length of 128. As result, we retrain our autoregressive baseline, SEDD, and MDLM on LM1B with wrapping. Similarly for OWT, we do not pad or truncate sequences, but concatenate them and wrap them to length of 1024 similar to LM1B. For unconditional generation experiments in Section 6.2, we wish to generate sequences longer than the context length seen during training. However, Sahoo et al. (2024a) inject beginning-of-sequence and end-of-sequence tokens ([BOS], [EOS] respectively) at the beginning and end of the training context. Thus, baselines from Sahoo et al. (2024a) will generate sequences that match the training context size. To examine model generations across varying lengths in Section 6.2, we retrain our AR, SEDD, and MDLM baselines without injecting [BOS] and [EOS] tokens in the examples. We also adopt this preprocessing convention for training all BD3-LMs on OWT. C.2 ARCHITECTURE The model architecture augments the diffusion transformer (Peebles & Xie, 2023) with rotary positional embeddings (Su et al., 2021). We parameterize our autoregressive baselines, SEDD, MDLM, and BD3-LMs with transformer architecture from Sahoo et al. (2024a) that uses 12 layers, hidden dimension of 768, and 128 attention heads. We do not include timestep conditioning as Sahoo et al. (2024a) show it does not affect performance. We use the AdamW optimizer with batch size of 512 and constant learning rate warmup from 0 to 3e-4 for 2.5K gradient updates. C.3 TRAINING We train base BD3-LM using the maximum context length = for 850K gradient steps. Then, we fine-tune under varying using the noise schedule optimization for 150K gradient steps on the One Billion Words dataset (LM1B) and OpenWebText (OWT). This translates to 65B tokens and 73 epochs on LM1B, 524B tokens and 60 epochs on OWT. We use 3090, A5000, A6000, and A100 GPUs. C.4 LIKELIHOOD EVALUATION We use single Monte Carlo estimate for sampling to evaluate the likelihood of token block. We adopt low-discrepancy sampler proposed in Kingma et al. (2021) that reduces the variance of this estimate by ensuring the time steps are more evenly spaced across the interval [0,1] following Sahoo et al. (2024a). In particular, we sample the time step for each block {1, . . . , B} and sequence {1, . . . , K} from different partition of the uniform interval t(k, b) U[ (k1)B+b1 ]. KB , (k1)B+b KB This low-discrepancy sampler is used for evaluation. For training, each masking probability may be sampled from clipped\" range 1 αt U[β, ω]. During training, we uniformly sample [0, 1] under the low-discrepancy sampler. We then apply linear interpolation to ensure that the masking probability is linear within the desired range: 1 αt = β + (ω β)t. 24 Published as conference paper at ICLR 2025 When reporting zero-shot likelihoods on benchmark datasets from Radford et al. (2019) using models trained on OWT, we wrap all sequences to 1024 tokens and do not add [EOS] between sequences following Sahoo et al. (2024a). C."
        },
        {
            "title": "INFERENCE",
            "content": "Generative Perplexity We report generative perplexity under GPT2-Large from models trained on OWT using context length of 1024 tokens. Since GPT2-Large uses context size of 1024, we compute the generative perplexity for samples longer than 1024 tokens using sliding window with stride length of 512 tokens. Nucleus Sampling Following SSD-LM (Han et al., 2022), we employ nucleus sampling for BD3LMs and our baselines. For SSD-LM, we use their default hyperparameters = 0.95 for block size = 25. For BD3-LMs, AR and MDLM, we use = 0.9. For SEDD, we find that = 0.99 works best. In Table 7, BD3-LMs and MDLM use = 5K diffusion steps. BD3Number of Diffusion Steps LMs and MDLM use efficient sampling by caching the output of the denoising network as proposed by Sahoo et al. (2024a); Ou et al. (2025), which ensures that the number of generation steps does not exceed the sample length L. Put simply, once token is unmasked, it is never remasked as result of the simplified denoising model (Suppl. B.3). We use MDLMs block-wise decoding algorithm for generating variable-length sequences, however these models are not trained with block diffusion. SSD-LM (first row in Table 7) and SEDD use = 1K diffusion steps. Since block diffusion performs diffusion steps for each block {1, . . . , B}, SSD-LM undergoes BT generation steps. Thus to fairly compare with SSD-LM, we also report generative perplexity for = 25 diffusion steps so that the number of generation steps does not exceed the sequence length (second row in Table 7). Improved Categorical Sampling of Diffusion Models We employ two improvements to Gumbelbased categorical sampling of diffusion models as proposed by Zheng et al. (2024). First, we use the corrected Gumbel-based categorical sampling from Zheng et al. (2024) by sampling 64-bit Gumbel variables. Reducing the precision to 32-bit has been shown to significantly truncate the Gumbel variables, lowering the temperature and decreasing the sentence entropy. Second, Zheng et al. (2024) show that the MDLM sampling time scales with the diffusion steps , even though the number of generation steps is bounded by the sequence length. For sample length and vocabulary size , the sampler requires sampling O(T LV ) uniform variables and performing logarithmic operations on them. We adopt the first-hitting sampler proposed by Zheng et al. (2024) that requires sampling O(LV ) uniform variables, and thus greatly improves sampling speed especially when L. The firsthitting sampler is theoretically equivalent to the MDLM sampler and leverages two observations: (1) the transition probability is independent of the denoising network, (2) the transition probability is the same for all masked tokens for given t. Thus, the first timestep where token is unmasked can be analytically sampled as follows (assuming linear schedule where αt = 1 t): tn1 = tnu1/n, (25) where {L, . . . , 1} denotes the number of masked tokens, un U[0, 1] and tn1 corresponds to the first timestep where 1 tokens are masked. Variable-Length Sequence Generation For arbitrary-length sequence generation using BD3-LMs and AR in Table 6, we continue to sample tokens until the following stopping criteria are met: 1. an [EOS] token is sampled 2. the average entropy of the the last 256-token chunk is below 4 where criterion 2 are necessary to prevent run-on samples from compounding errors (for example, sequence of repeating tokens). We find that degenerate samples with low entropy result in significantly low perplexities under GPT2 and lower the reported generative perplexity. Thus, when sample meets criterion 2, we regenerate the sample when reporting generative perplexity in Table 7. 25 Published as conference paper at ICLR"
        },
        {
            "title": "D SAMPLES",
            "content": "<endoftext>s architect, lawyer and San Giovanni concerto art critic Paolo Capacotti, gained attention from fellow gallery members and even invited him to present retrospective, publishing issues and newspaper interviews.[10] On 6 September, Kissi and his assistants agreed to move to Angelos Marcus Collection,[10] which included Giorgio Avolivo Arth and Moscolliso (later owned by the artist Belzina Massingolo) and Pan Giazzoglio Romeam-Guessle. The businessman, Giovanni Paletti, an outstanding collector, owned the museum and the painting. The level of criminal activity around the museum has continued to increase, which is part of several attempts to counter centennial rumors including the possibility that museum staff and visitors are tortured and even exposed to del Cavello for the only full year of Francesco Belzinas life (1999).[4] On the evening of 22 October 2005 it was reported that earlier that evening, guards had come on duty and began flinging an electric field with umbrellas from the balcony. As the fire continued, some of the guards sparked an apparent spat from the window of the cathedral. They remained idly watched by pile of trash left after piano key by Pietro Jolla, who died on 21 October 2005.[10] Just before 3:00 to 3pm on Monday, 27 October 2005, strong winds brought the trash on to the residence that opened on 17 October. Some ruined books and statues were hurled in front from every direction of the window. Some claimed that customer Jacques Monet had beaten the hand of photographer Franco Campetti and in some cases had stuck broken candle in the doorway of the museum. Andr Romeam-Guessle responded by laughing when he spoke. Giancio Giuliano, the artistic director of the Museum, even tried to told journalists and press that the patient in the trisomy machine [sic] carried some corpses four hours into the museum, but the whole time it was the guy who stroked the young man who who broke him. In 2008, Giuliano told the same press that the hours of the destruction are truly \"wrong for their morality\" and further stated that We are never satisfied with our decision. We made an informed decision to build the museum after destruction.[5] Deaths [ edit ] little after 12:00 am 17 October 2005, Giuliano and his partner Monica Concerta, noticed that the trash was being thrown by passers-by. Captain Iamienowska leaned over to his film camera and said, in joking manner, that Iannorello, the chair of the Muscéei, was thief that director Frank Nolan said \"he would later be arrested.\" When Iamienowska arrived, the people in question were interviewed by Captain Anderson Tulaqyuk, co-man who was initially lying on the scene and whom Iamienowska said was able to stop them from passing in the vicinity. Iguano proceeded to collect the trash and the police arrived, and closed the door of the museum.[6] During the war, the statue structure was partially removed and its cannons damaged. On the eve of the war, the U.S. Army and Canadian Air Company, who once owned lot on the Coopers of Paris near Leopold Street, sheltered the POWs, who were briefly overworked. This following years led to condemnation of the organization and artists, including John DiDione,[7] returned to the compound to sell the museum items (including many of them originally stolen over the years-more than million returned); on 12 October, the sculptor Jack Uellein installed temporary dome on Del Cavella.[10] Among the changes over the years, Diena featured the lion in the middle of the other two halves of the dome and the lion in the back of the Viduccio.[11] In March 2008, Arvnzo Cissino announced that seven studio studios were built between the summer of 1985 and the last one was completed on November 17, 2005.[10] In the fourth quarter of the mid-2000s, number of new structures were constructed by various museums, but most notably by Arvnzo Cissino, the brothers-soldier, as portrayed by architect Duvois Mayaven in the illusion of the El Valleratas Humanities and Cultural Center, reconstructed by architect Duvois Benjamin. Just as the very significant additions to the current management masts of the del Museum are such private three-bedroom rooms as Tradellos cabin, as well as the architectural aspect from the museums south wall. Also new are the rockings performed by MC Urquhart, Aiba Ruiz and March Flez, during music festival held in the US in 2006. Urquhart was later credited with establishing record label Empire Records as an influence on his music. In<endoftext> Figure 6: Sample from MDLM (Sahoo et al., 2024a) of length = 1024 and = 5K diffusion steps. The generative perplexity of this sample under GPT2-Large is 69.26 and its entropy is 5.6. 26 Published as conference paper at ICLR 2025 <endoftext>When her child was planning to travel to Mexico, she thought it could be easy for her to find money to pay the trip. But that didnt happen, either. The girl arrived on her doorstep the day before after they were scheduled to return to home and - despite her own passport blocked. It wasnt easy. Even her mother reported bad visit. gate to the airportthe gate and flight instructions - were locked at nine a.. \"I had no way to the gate,\" she told The Local. \"I feel like should keep my passport and their fees.\" At first the police arrived, British man told an e-mail that parents should only have money for themselves to be fine, but could never explain the situation. Later, she told her daughter that her passport had been detained because she wanted to visit her family at hotel. She felt embarrassed and confused. Nearly two months later, she said, she lost 35(40) on rent and imagined that the traffic-police and customs agents in Bangkok would end up delaying flights and forcing her to stay home. She was worried that her father would refuse allowing her daughter to spend few days in the country. Meanwhile, the police were sent to search. \"It wasnt easy for them when child feels like home for the first time,\" said Mahavram Kaas, spokesman for the Ministry of Foreign Affairs. Hes referring to tour arranged by the French and French foreign ministry, known as Courage in the Child. That tour cost the region 2 million worth of tickets, and cost the Calais family about $25 million in lodging expenses, according to statement by both the ministrys behalf (he told the Portuguese police agents) and Londons Embassy in London (he told the French ambassador they provided payment for 70,000, which would be used to pay the travel costs for their visits). In 2011, family from Calais had moved to the UK aged 15. Their sister left to remain in France at 5 years old. Her brother tried to answer that question. He explained it to reporters at his service station at Calais airport. \"You take the morning. Its named after you and your little girls,\" he said. The last mother was having 19-month stay with her daughter that night, the police said. The first time she was back her husband took their daughter on boat to the UK, said the mayor of the British Transport Agency. That meant she had plenty of cash-to-go and no money to borrow when she opened an account at Kathmandu Airport few hours after booking her flight. She panicked. She called the was Daleys Nessie (small cash register), saying she was getting better. Her doctors visited her when she returned and her boyfriend quit his job for four months after the visit, she said. \"How do you feel like you are safe?\" text from friend left her to the police. \"She says must go get my wallet,\" said Ajaz. Soon after, she booked plane ticket to Paris and took metro train to Calais. One night, French policeman would knock on the door of local council building, open the mailman and the phone and tell her she knew that she could not leave at least one week without food. The four months her daughter spent in the UK was exhausting and hard, and it reached the stage where she realized she could barely stay in Britain. \"Now no choice but to go home. Then we regret having daughter,\" he said. He thought for minute. \"Youve broken your heart.\" \"Today, my daughter and my boyfriend decided to stay in this country for over two months,\" he wrote in an email with his daughter in his hand. \"All our flights cancelled and no security. Shame.\" Caines family were also put on leave. The French police paid for her car after she rented it, and her female officer used it for the opening ceremony of her press conference in Thailand. The police are still arranging for her family to have their official visit. Although her son is back at work now and his old job, her daughter needs to stay in hospital in Algiers to continue her education. But, finally, her parents will be making their girl home. The daughter was 18 when they opened her case. She was born two months ago. She doesnt talk about it because it feels like she was still child, living in Thailand with small child. Her mother, Anzsa Gurdon, came to England as three-year-old after her brother, Ehab Rahman, was working as British worker in Calais while living in London and studying abroad in Dubai. As mother earns 2,000 pounds month, they receive well-paid living in secure accommodation, some even with public transport buses. If they make money, their child stays in the UK, they can set up companies with kids to take care of their children. Other countries sometimes also give birth to parents forced to provide child care. The parents are often refugees from their home countries, theyre left without family, and have forced to leave families. As one former refugee fled Syria, his family was in detention, because when and if their child had arrived, they would be living somewhere. The detention centers in Western Europe often have higher rate for asylum seekers. Often there are higher \"safe houses\" for young people, and then the people in the center get older when their child comes to stay, but the only family that is year older is not allowed to have children, and usually only if they stay six months. Children are also detained and are asked to show identification. Because in most cases the refugees ask only about their identity, they dont have access to their own documents, and have no other documentation. An activist working for Ireland says hes against offshore processing. He thinks the charity problem here is like diseases which dont sufficiently seek out international funding. think too many countries want to employ \"humanitarian\" children. Now the job Before the refugee crisis in Calais, 1,823 children were living in the UK, the UNHCR website shows. good chunk of those children had landed in refugee camps in Africa, where mostly African migrants were sending children from Syria and Libya to their camps, but those numbers didnt fare so well. \"At the moment when met the French, it was horrific, they wanted me to put my children in van. But was only kidding. Its something called 10-year vans,\" Aakaz said during an interview. \"I want to keep my children for 10 years. Thats something. Its like Christmas. The dream of 10 years. . . . For me, the idea that this is good opportunity, heres chance,\" is that she can sleep with her children. At this point, its much more than just about \"alternatives\". They now have to decide, at some point, whether they want to take the chance. Is it big deal or not? Only in Calais She explained whats agreed to so the children can go home and can have better future. \"Im very determined. My children want to go home but its my lifes personal decision,\" she said. \"Five months. want them to be home, 5 months. If theyre not getting jobs properly, also want to stay home. But feel good about what Ive got. She is from poor country. dont owe anything to anyone. But have to work for them. feel like can just go to the road and provide accommodation for my children and their children.\" But they all dont work well because their families have her as head. \"They want me to have job in England, but feel like its my home, and Im not scared of work,\" she said. \"And feel that the opposite would be possible. think in the future that can have job or two there.\" Second World Friday event will be held outside Calais on Saturday, donating 100 euros to the coming week in the money brought up to them by UNHCR through the King Wahab Samba Global Fund. Friends and family expressed their \"weakness\" like many survivors in many countries.\"We just dont want to accept what has to happen. We want to put the people back there as soon as possible,\" said one man. Her brother, who is the son of long term migrant, said: \"The story of the refugee is not mothers story. The story of the refugee is childrens story.\" \"In Calais its too young for these kids. They play outside or work outside, they just eat, right? dont think much has changed. This child with all her food and sleep, shes too young for life without any protection. We dont need any protection at all. We need anything that would be safer.\"<endoftext> Figure 7: Sample from BD3-LM for block size = 16 of length = 2031 under = 5K diffusion steps (trained with context length of = 1024). The generative perplexity of this sample under GPT2-Large is 24.3, and its entropy is 5.5. 27 Published as conference paper at ICLR 2025 <endoftext>, but Wilson took over the program and turned it around. \"Hes done tremendous job,\" Caldwell said. \"Hes done fantastic job.\" The offense has always had an abundance of weapons, but it became evident that they werent going to have weapon to actually go after players from the slot. Now theyre in two different weapons sets. The top group features Dez Bryant and Mohamed Sanu, and the bottom group features an assortment of weapons and pass rushers. The job has become far more complex. The other players can make plays on the ball and get those targets at higher rate. Sanu is more of classic, get to the quarterback and leave the corner open. Dontari Poe got the job done this year and became one of the more effective players at the position, even in the passing game. However, Dallas has got to figure out how to get their franchise wideouts to contribute on the field. That can be tough. Adding Poe can help get the receiving corps going. C.J. Spiller is two-time Pro Bowler, but if the Cowboys want to upgrade their receiving corps, hes going to have to step up in big way. \"Weve got to be little more aggressive with the type of weapons that we have,\" Caldwell said. \"I think thats part of the reason why our last two games, especially when youre playing in Washington, D.C., youve got to be aggressive, make sure youre hitting at every catch. When you are, youre giving up lot of yards.\" Part of that means taking the quarterback out of the equation and having him beat coverage lot more. In the NFC West, you want your offensive weapons to do better job of running through coverage. The biggest threat that Dallas has is QB in Ben Roethlisberger. Roethlisberger is far and away the best quarterback in the league, but lot of the credit has to go to his receiver group. Martavis Bryant and Antonio Brown are both big-time receivers, and last year they were in the top 10 of yards per catch and receiving yards in the league. That production will never be sustainable, but if youre going to be an elite offense, its going to take lot of catching up. Roethlisberger is an All-Pro receiver, and hes not the most dynamic option. But it would take something like Bryant or Brown at better position, and at slightly lower price, to make him the most productive receiver on the offense. The truth is that Roethlisberger isnt going to be great. He may only have 18 games left in his career, but hes been doing it since he was rookie in 1991. But thats not the worst thing in the world. Roethlisbergers ability to hit guys on the outside with good movement, vision and running ability is what the Cowboys need in order to keep up with the competition. If he keeps getting better, he could become the best receiver in the league. Follow @walterfootball for updates. Like our Facebook page for more Cowboys news, commentary and conversation.The owner of 1H10 Tree in Charlotte Gardens is taking legal action against the city. Derek Jarman says hes been forced to evict his neighbour, Bob, after he took to social media to threaten to burn down his neighbors house. \"Im incredibly furious with the city,\" Jarman told 7.30. \"Ive been trying to keep my eyes on the prize.\" Tree in Charlotte Gardens saying it had seen 9,000+ people enjoying great weekend The company that owns 1H10 named Bob after bee and said the tree was frequently targeted because of its unusual location. Bob said he had his concerns about the tree when he was contacted in October. He said they had had an ongoing conversation about my neighbor. He called, hung up and he was very threatening in the 30 days before they turned the tree over to him. neighbour posted the following online message on 8 October. \"I am shocked about the serious problems you are having with your neighbour that has caused you all (sic). You and the 2 of you are making money at the expense of the good people of Charlotte Gardens.\" Bob says he was furious and said hed just got off the phone with the city manager. \"I told her, no, Im going to bring lawsuit, and called the solicitor and tried to get my phone, just hoping the solicitor would help me out. called again, and asked if could go to court and to try and get an injunction. \"They told me you cannot, and they said, we cant, we cant because youre sending people to the police.\" Tree in Charlotte Gardens (Facebook) He also said hed threatened the city attorney if he didnt stop the building from burning down. The internet user tweeted: \"Is on the tree, but after said threw this away, heres spot to burn, the building started to burn.\" Tree in Charlotte Gardens (Facebook) Bob said the neighbour had threatened to burn down the tree, the windows, the living room and his entire backyard. \"It was more than threat,\" he said. \"He was very strong person. Hes already damaged so many people in this building. Its not going to go away.\" Tree in Charlotte Gardens (Facebook) Jarman says he tried to talk the building owner out of the move, but the building owners behaviour had \"deleted him.\" \"Im going to stop him by letter telling him not to come to my house any more,\" he said. \"I have three kids, and if Bob is going to be in my house, need to make sure have someone who can go in there and protect me. \"My son does really good job of protecting me, and Im not going to let that get in the way of that.\" Tree in Charlotte Gardens (Facebook) Jarman said Bob had pulled him up on social media, calling him \"white nut\" and saying: \"For Gods sake, stop calling me white nut. \"I should have shut him up on Facebook.\" He said he sent Bob the letter and thanked him for the support. \"He should have done it because hes real artist and hes real artist,\" he said. He declined to name the architect of the new tree, but says the firm is the same one that designs buildings. The building is burning down, neighbour says Bobs neighbour, Michael Banks, says the fire is an insult to his daughter. \"There are two black women that live next door to me and they told me you cant do that, and then the fire went up and then the building burnt down,\" he said. \"You cant burn down house if you dont burn down the house.\" Coun-Pete Lawrence, the Northumberland MP for Wood Green, says he has concerns about Bobs neighbours. \"Its very, very sad commentary on the state of society and democracy in general,\" he said. \"Its interesting in community thats 50,000-plus people, youve got your regular residents and well-meaning neighbours who are apparently oblivious to the destruction of their own home. \"To me, thats appalling and it is probably shocking amount of devastation that its left behind. \"I would expect there to be outrage as well.\" Bob Jarman fears for his life after the tree was torched Bob says he has told the Northumberland Council that he had already received $1,000 in legal action from the building owner, when he told them about the incident. The building owner has declined to comment on the situation. The builder is currently assessing its legal options. \"Weve got to sort this out and have an understanding with the builder, Mr Banks,\" he said. \"Weve got to make sure we cant get into into legal battle with that person and make that person change his mind. \"We dont want to do anything to cause scene or anybody in the street to be upset.\" Bob Jarman hopes to have an understanding with the builder on its legal options, who have refused to comment. Topics: state-parliament, smoking-and-doubt, black-wales-6168, united-kingdom, england First postedWhen the other guys are away playing, do short commercial to get you fired up for the next work day. Once you make it home, get few junkies for them. Theyll be very happy to have you, for at least day. They might not be so happy after couple of days. Have bunch of friends and get ready to keep it going. What are you waiting for? Make this long, one-off<endoftext> Figure 8: Sample from an AR model (Sahoo et al., 2024a) with length = 2003 (trained with context length of = 1024). The generative perplexity of this sample under GPT2-Large is 10.6 and its entropy is 5.5."
        }
    ],
    "affiliations": [
        "Cohere, NY, USA",
        "Cornell Tech, NY, USA",
        "Stanford University, CA, USA"
    ]
}