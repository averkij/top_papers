{
    "paper_title": "HoliTom: Holistic Token Merging for Fast Video Large Language Models",
    "authors": [
        "Kele Shao",
        "Keda Tao",
        "Can Qin",
        "Haoxuan You",
        "Yang Sui",
        "Huan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video large language models (video LLMs) excel at video comprehension but face significant computational inefficiency due to redundant video tokens. Existing token pruning methods offer solutions. However, approaches operating within the LLM (inner-LLM pruning), such as FastV, incur intrinsic computational overhead in shallow layers. In contrast, methods performing token pruning before the LLM (outer-LLM pruning) primarily address spatial redundancy within individual frames or limited temporal windows, neglecting the crucial global temporal dynamics and correlations across longer video sequences. This leads to sub-optimal spatio-temporal reduction and does not leverage video compressibility fully. Crucially, the synergistic potential and mutual influence of combining these strategies remain unexplored. To further reduce redundancy, we introduce HoliTom, a novel training-free holistic token merging framework. HoliTom employs outer-LLM pruning through global redundancy-aware temporal segmentation, followed by spatial-temporal merging to reduce visual tokens by over 90%, significantly alleviating the LLM's computational burden. Complementing this, we introduce a robust inner-LLM token similarity-based merging approach, designed for superior performance and compatibility with outer-LLM pruning. Evaluations demonstrate our method's promising efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational costs to 6.9% of FLOPs while maintaining 99.1% of the original performance. Furthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a 1.32x acceleration in decoding throughput, highlighting the practical benefits of our integrated pruning approach for efficient video LLMs inference."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 4 3 3 1 2 . 5 0 5 2 : r HoliTom : Holistic Token Merging for Fast Video Large Language Models Kele Shao1,2, Keda Tao2, Can Qin3, Haoxuan You4, Yang Sui5, Huan Wang2, 1Zhejiang University 2Westlake University 3Salesforce AI Research 4Columbia University 5Rice University https://github.com/cokeshao/HoliTom Figure 1: Left: We introduce HoliTom, training-free holistic token merge method for fast video LLMs. Its key innovation lies in its global, redundancy-aware outer-LLM spatio-temporal compression and robust, token similarity-based inner-LLM compression. Right: The Efficiency/Performance trade-off curve of multiple training-free methods on four widely used video understanding benchmarks: MVBench, EgoSchema, LongVideoBench, and VideoMME. Our method, HoliTom, surpasses the SoTA approaches by maintaining 99.1% average performance while reducing FLOPs to 6.9%."
        },
        {
            "title": "Abstract",
            "content": "Video large language models (video LLMs) excel at video comprehension but face significant computational inefficiency due to redundant video tokens. Existing token pruning methods offer solutions. However, approaches operating within the LLM (inner-LLM pruning), such as FastV, incur intrinsic computational overhead in shallow layers. In contrast, methods performing token pruning before the LLM (outer-LLM pruning) primarily address spatial redundancy within individual frames or limited temporal windows, neglecting the crucial global temporal dynamics and correlations across longer video sequences. This leads to sub-optimal spatiotemporal reduction and does not leverage video compressibility fully. Crucially, the synergistic potential and mutual influence of integrating these strategies remain unexplored. To further reduce redundancy, we introduce HoliTom, novel trainingfree holistic token merging framework. HoliTom employs outer-LLM pruning through global redundancy-aware temporal segmentation, followed by spatialtemporal merging to reduce visual tokens by over 90%, significantly alleviating the LLMs computational burden. Complementing this, we introduce robust innerLLM token similarity-based merging approach, designed for superior performance and compatibility with outer-LLM pruning. Evaluations demonstrate our methods promising efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational costs to 6.9% of FLOPs while maintaining 99.1% of the original performance. Furthermore, we achieve 2.28 reduction in Time-To-First-Token (TTFT) and 1.32 acceleration in decoding throughput, highlighting the practical benefits of our integrated pruning approach for efficient video LLMs inference. Corresponding authors: wanghuan@westlake.edu.cn Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Video large language models (video LLMs) [19, 53, 41, 7, 8, 21, 23, 46, 50] have shown remarkable potential in understanding complex video content. However, their practical deployment is hindered by significant computational inefficiency. This inefficiency stems from processing high volumes of video tokens generated by encoding sampled frames, leading to substantial overhead, particularly due to the quadratic complexity of the attention mechanism in the LLMs. For videos with numerous frames, the input token count can easily reach tens of thousands, making inference computationally expensive. While prior works [6, 48, 45, 36] have explored model compression and token pruning, achieving desirable balance between efficiency and performance in video tasks remains an open challenge. Thus, developing effective methods to reduce video token redundancy while preserving critical semantic information is crucial for the widespread adoption of video LLMs. Table 1: Compression scope of visionlanguage model acceleration methods. This table outlines where different methods apply compression. Spatial and Temporal refer to compression of the input visual data, while Inner-LLM indicates compression mechanisms applied within the models processing. Token pruning is promising direction. These approaches generally fall into two categories depending on where pruning occurs. Inner-LLM pruning methods, such as FastV [6], TopV [47], and PDrop [45], operate within the LLM layers. However, they incur intrinsic computational and memory costs in the initial layers before pruning takes effect, limiting overall FLOPs reduction. Outer-LLM pruning methods process tokens before the main LLM computation. Some methods address spatial redundancy (VisionZip [48], PruMerge [32]), others tackle temporal aspects within limited temporal windows (DyCoke [36], PruneVid [15]), thus preventing global understanding of video dynamics and comprehensive spatio-temporal optimization. Furthermore, despite the potential for synergy, no prior work has systematically explored integrating inner-LLM and outer-LLM pruning strategies or analyzed their mutual benefits. The current methods, while offering some benefits, still leave room for improvement. FastV [6] PDrop [45] LLaVA-PruMerge [32] VisionZip [48] DyCoke [36] FastVID [33] Ours Spatial Temporal Inner-LLM Methods To address these limitations, we propose holistic token pruning for video LLMs that leverages external and internal strategies. Our method first tackles temporal redundancy through global redundancy-aware video segmentation process, followed by spatio-temporal merging. This external step reduces visual tokens to less than 10%, significantly alleviating the computational burden on the subsequent LLM. Complementing this, we introduce new and robust inner-LLM token similaritybased merging method, specifically designed for integration with our outer-LLM pruning method, enabling mutual benefits. This integrated strategy offers more holistic and efficient solution to handle long videos with LLMs, as summarized in Tab. 1, which contrasts the compression achieved by our approach in both the spatio-temporal domain and within the inner-LLM against other methods. Empirical evaluations validate the effectiveness of our proposed method in achieving compelling efficiency-performance trade-off. Specifically, as shown in Fig. 1 (right), on the LLaVA-OneVision7B model [19], our approach reduces computational costs to just 6.9% of the original FLOPs while remarkably preserving 99.1% of the original models performance. Moreover, we observe significant gains in inference efficiency, achieving 2.28 reduction in Time-To-First-Token (TTFT) and 1.32 acceleration in decoding throughput. These results clearly demonstrate the substantial practical advantages of our holistic token merging framework for efficient video LLM inference. Our key contributions are summarized as follows: 1. We analyze the phenomenon of temporal redundancy in the context of video LLMs and propose global redundancy-aware temporal merging method to effectively address the inefficiency in video LLMs before LLM processing in plug-and-play fashion. 2. We introduce robust inner-LLM similarity-based merging technique specifically designed for integration with the outer-LLM pruning method, facilitating synergistic optimization. 3. Extensive evaluations on LLaVA-OneVision and LLaVA-Video demonstrate that our integrated pruning framework achieves state-of-the-art efficiency-performance trade-off, significantly reducing computational costs and accelerating inference while preserving model performance."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Video Large Language Models The rapid progress of multimodal large language models has led to the integration of video encoders, creating video LLMs that excel in video understanding and question answering tasks [46, 14, 19, 2, 41, 3, 21, 22, 23, 50, 35, 37, 17, 1]. However, the substantial number of tokens generated by processing numerous video frames hinders inference efficiency, thereby impeding the widespread adoption of video LLMs. Existing approaches have attempted to mitigate this issue. For instance, VideoLLaMA [50] employs Q-Former module [20] to aggregate video tokens, while MovieChat [35] introduces memory module to merge and store token representations. Although pooling mechanisms in LLaVA-OneVision [19] reduce token counts, each video frame still produces hundreds of tokens for downstream processing. Consequently, handling tens of thousands of visual tokens for long video inputs substantially increases inference time and memory consumption. While works such as VILA [25] and NVILA [26] aim to optimize token usage, these methods often require model fine-tuning, demanding considerable hardware resources [16, 23, 25, 26, 40]. This underscores critical need for developing more efficient, training-free token compression methods specifically for video LLMs, bypassing the need for costly model adaptations and significant hardware investment."
        },
        {
            "title": "2.2 Visual Token Compression",
            "content": "Token compression has emerged as an effective strategy for reducing token redundancy in vision transformers and large language models. ToMe [4] merges similar tokens in ViTs to alleviate spatial redundancy, while TempMe [34] focuses on minimizing temporal redundancy by merging adjacent video clips. TESTA [30] achieves up to 75% reduction in processed tokens by employing temporal and spatial aggregation modules. For MLLMs, FastV [6] prunes non-essential visual tokens in early layers of LLM. TopV [47] proposes an optimization framework to prune unnecessary visual tokens. DyMU [42] introduces token merging in the visual encoder and virtual unmerging in the LLM decoder. PDrop [45] performs progressive pruning of tokens at different stages within the LLM. LLaVA-PruMerge [32] and VisionZip [48] leverage attention weight analysis in visual encoders to eliminate spatial redundancy. However, the inherent temporal dependencies between video frames necessitate specialized compression designs. Consequently, recent methods specifically for video token compression have gained increasing attention. DyCoke [36] consolidates tokens across frames and implements dynamic key-value cache reduction. PruneVID [15] clusters video tokens, whereas FastVID [33] enhances compression by combining temporal segmentation with spatio-temporal token merging. In this paper, we propose new token merging strategy specifically designed for video LLMs, which fully considers spatio-temporal characteristics to maximize performance retention."
        },
        {
            "title": "3 Method",
            "content": "3.1 Background on Video LLMs Inference The inference process of video LLMs involves three key stages: before LLM, prefilling, and decoding. (1) Before LLM. Given an input video with frames, vision encoder processes each frame to produce Nv embedding vectors. These are projected into the text embedding space, yielding visual tokens Hv RBNvd, where represents the dimension of the hidden state space. text prompt = {ti}Nq i=1 is tokenized and embedded into text tokens Hq RNqd similarly. Finally, the visual and text tokens are concatenated to form = concat[Hv, Hq], which serves as the LLM input. (2) Prefilling Stage. During prefilling, each transformer layer of the LLM performs self-attention operations on the concatenated input H. It begins with linear transformations to compute query Rdd are Ql = HWl learnable projection matrices. The resulting key-value pairs (Kl and Vl) are then cached (KV cache) to enhance the efficiency of token generation during the subsequent decoding phase. K, and value Vl = HWl Q, key Kl = HWl , where Wl K, Wl Q, Wl (3) Decoding Stage. The decoding stage generates tokens autoregressively, leveraging the KV cache. At each time step t, only the new token ht is processed to compute its key and value representations, avoiding recalculating attention weights over the entire history. The KV cache is updated by appending the new key-value pairs: [K, htWK] , [V, htWV ]. This caching mechanism substantially reduces the computational complexity of the generation process. 3 Figure 2: Overview of our HoliTom method. HoliTom compresses video LLMs across three scopes; the first two are outer-LLM pruning. Temporal Merging maximizes temporal compression via global redundancy-aware segmentation, merging similar tokens into their first occurrence. Spatial Merging further reduces redundancy by applying tailored spatial compression based on the characteristics of remaining temporal variations. Inner-LLM Merging leverages attention within the LLM to identify key tokens and merges less important, similar tokens, streamlining information within the LLM. 3.2 Global Redundancy-Aware Temporal Merging Temporal redundancy describes feature persistence at fixed spatial locations across consecutive frames. We identify this redundancy for the k-th feature between frames and + 1 using their respective feature vectors hm,k and hm+1,k. feature is considered temporally redundant if its normalized similarity, sim(hm,k, hm+1,k), exceeds defined threshold τ [0, 1]. For temporal segment defined by start frame ts and an end frame te (covering [ts, te)), the total number of prunable tokens, g(ts, te), is calculated. This involves counting tokens (ts, te) that are consecutively redundant across all frames from ts to te 1, and then multiplying by the number of subsequent frames (te ts 1) within the segment where these tokens can be pruned. Our method prunes the redundant by merging these subsequent occurrence tokens into their first appearance at start frame ts, treating them as temporal redundant tokens, as shown in Fig. 2. The formulation is: g(ts, te) = (cid:32) Nv(cid:88) te2 (cid:89) k=1 m=ts (cid:124) I(sim(hm,k, hm+1,k) > τ ) (te ts 1), (1) (cid:33) (cid:123)(cid:122) (ts,te) (cid:125) where Nv is the total number of features per frame, and I() the indicator function. Given video of frames, our objective is to find segmentation into consecutive segments [ti, ti+1) (with t1 = 1, tK+1 = + 1, and ti < ti+1) that maximizes the total prunable features: (cid:88) (2) i=1 This optimization is solved using dynamic programming to achieve global optimization. Let dp[i] be the maximum prunable features for video ending at frame (exclusive, i.e., considering frames 1, ..., 1), where frame marks the exclusive end of the last segment. The value prev[i] stores the optimal starting frame of this final segment [j, i). The state transition is given by: dp[i] = max 1j<i {dp[j] + g(j, i)}, with prev[i] = arg max {dp[j] + g(j, i)}. (3) 1j<i The base case is dp[1] = 0. The maximum prunable features for the entire video are dp[B + 1]. The optimal segmentation is reconstructed by backtracking from + 1 using the prev array. 4 max K,{ti}K+1 i=1 g(ti, ti+1). 3.3 Spatial Merging After temporal merging, tokens are classified as non-redundant or redundant temporal tokens. We first process the former. Inspired by works [32, 48, 52, 38], we utilize the CLS tokens for spatial feature selection. For vision encoders like Siglip [49] that do not have an explicit CLS token, method is detailed to derive CLS-equivalent attention. Specifically, we compute the attention matrix: = Softmax(QK / d) RBNvNv , (4) where is the state dimension. Token importance is quantified by averaging the attention weights each token receives from all other tokens within the same frame in the vision tower, yielding an score vector Aavg RBNv . Tokens receiving higher average attention are considered more salient. Consistent with video LLMs of applying spatial pooling (e.g., after the projector to reduce tokens), we reshape Aavg to its original spatial grid dimensions (H = Nv) and apply an analogous pooling operation. This results in spatially downsampled importance map Aavg RBHW . Ultimately, we select the visual features corresponding to the highest scores in Aavg as the representative and most informative spatial tokens, known as attention-based select, discarding all others. The computation of vision tower attention is intra-frame. Averaging these attention weights across frames lacks theoretical justification, invalidating the attention-based method for redundant temporal tokens. To process these features, we employ cluster-based merging method utilizing density peak clustering based on k-nearest neighbors (DPC-KNN) [12, 31]. Given set of redundant temporal tokens [v1, v2, ..., vN ] within the first frame of the segmentation. For each token vi, we calculate its local density ρi, distance to the closest higher-density token δi and the final density score γi = ρi δi: ρi = exp 1 (cid:88) d(vi, vj) , δi = vj kNN(vi) max j=i min j:ρj >ρi d(vi, vj) if ρi = max d(vi, vj) otherwise ρk . (5) Tokens with high γi are selected as cluster centers. After selecting the cluster centers, each remaining feature is assigned to the cluster whose center is closest in feature space. Finally, the representative feature for each cluster is then computed by averaging the features assigned to it. Ultimately, the compressed features, derived from this clustering process, along with the non-redundant features, are concatenated according to their original spatial order, thereby preserving positional characteristics. 3.4 Inner-LLM Merging Inefficient visual attention in large vision language models has been widely discussed [6, 45]. Existing methods, such as FastV [6] and PDrop [45], directly discard redundant visual tokens, which may lead to performance degradation due to information loss. Unlike these approaches, our proposed method addresses it by merging the information from potentially redundant tokens instead of simply discarding them. Specifically, at the K-th layer of the LLM, to reduce the number of visual tokens by R%, we employ token selection strategy based on attention scores. We use the attention weights of the last token to rank all vision tokens at layer K. The R% of visual tokens exhibiting the lowest attention scores are identified as candidates for merging. We find its most similar visual token within the set of tokens designated for retention. For retained token vr and its associated set of low attention tokens Vm = {vm1 , vm2 , ..., vmn }. The updated retained token is: = average(vr, vm1 , ..., vmn ). This selective merging preserves relevant features from tokens that would otherwise be removed, mitigating information loss while achieving the desired token reduction. (6)"
        },
        {
            "title": "4 Experimental Results",
            "content": "4.1 Experimental Settings Benchmarks. We evaluate our method on four widely-used video understanding benchmarks: MVBench [22], EgoSchema [27], LongVideoBench [43], and VideoMME [14]. Comprising videos of varying lengths and complex scenarios, these benchmarks provide comprehensive testbed for assessing the effectiveness and generalization of our method. 5 Table 2: Comparison of state-of-the-art methods across benchmarks. Best and most efficient results are in bold, second best underlined. Here, \"HoliTom\" means the full version of our method; \"HoliTom (w/o M)\" means our method without inner-LLM merging, for reference. Method LLaVA-OV-7B FastV [6] PDrop [45] DyCoke [36] VisionZip [48] PruneVid [15] FastVID [33] HoliTom (w/o M) HoliTom VisionZip [48] PruneVid [15] FastVID [33] HoliTom (w/o M) HoliTom VisionZip [48] PruneVid [15] FastVID [33] HoliTom (w/o M) HoliTom VisionZip [48] PruneVid [15] FastVID [33] HoliTom (w/o M) HoliTom Prefilling FLOPs (T) FLOPs Ratio Before LLM Retained Ratio MVBench EgoSchema LongVideo Bench VideoMME Avg. Score % 40.8 9.3 10.5 8.7 8.7 8.7 8.7 8.7 7.1 7.0 7.0 7.0 7.0 5.8 5.2 5.2 5.2 5.2 4.3 3.4 3.4 3.4 3.4 2.8 100% 22.8% 25.7% 21.3% 21.3% 21.3% 21.3% 21.3% 17.4% 17.2% 17.2% 17.2% 17.2% 14.2% 12.7% 12.7% 12.7% 12.7% 10.5% 8.3% 8.3% 8.3% 8.3% 6.9% 100% 100% 100% 25% 25% 25% 25% 25% 25% 20% 20% 20% 20% 20% 15% 15% 15% 15% 15% 10% 10% 10% 10% 10% 58.3 55.9 56.1 53.1 57.9 57.4 56.5 58.5 58.4 57.7 57.2 56.3 58.5 58.7 56.5 56.8 56.0 58.1 58.1 53.5 56.2 55.9 56.9 57.3 60.4 57.5 58.0 59.5 60.3 59.9 - 60.8 61. 59.8 59.7 - 60.7 61.0 59.8 59.7 - 61.0 61.2 58.0 59.8 - 61.1 61.2 56.4 56.7 54.1 49.5 56.5 55.7 56.3 56.5 56.7 55.2 54.7 57.1 56.3 57.1 54.4 55.4 56.2 57.0 56. 49.3 54.5 56.3 56.5 56.3 58.6 56.1 56.4 54.3 58.2 57.4 58.0 59.1 58.9 57.9 56.9 57.9 58.6 58.6 56.1 56.6 57.7 58.1 57.3 53.4 56.0 57.3 56.9 56.8 58.4 56.5 56.2 54.1 58.2 57.6 - 58.7 58. 57.7 57.1 - 58.5 58.8 56.7 57.1 - 58.5 58.2 53.5 56.6 - 57.8 57.9 100 96.7 96.2 92.6 99.7 98.6 - 100.5 100.7 98.8 97.8 - 100.2 100.7 97.1 97.8 - 100.2 99. 91.6 96.9 - 99.0 99.1 Compared Methods. We compare our proposed HoliTom against 6 strong training-free baselines: 1) FastV [6], identifies key tokens during prefilling using attention scores between predicted and vision tokens; 2) PDrop [45], prunes visual tokens within partitioned LLM stages, guided by image and instruction tokens; 3) Visionzip [48], prunes tokens before LLM via spatial token merging; 4) DyCoke [36], employs temporal merging before LLM and dynamic KV cache pruning in decoding; 5) PruneVid [15], minimizes video redundancy via spatial-temporal token clustering; and 6) FastVID [33], concurrent work, partitions videos and applies density-based token pruning. Due to the lack of public code, we compare FastVID to its reported results. For all other baselines and our method, experiments use their open-source code under identical hardware condition. Inference Cost Evaluation. We evaluate the inference cost of transformer layers, each composed of multi-head attention (MHA) and feed-forward network (FFN) modules. Following previous work [6, 45, 36], the FLOPs for processing ni vision tokens in layer i, with hidden state size and FFN intermediate size m, are defined as 4nid2 + 2n2 + 2nidm. For an LLM with transformer layers, the total FLOPs span the prefilling and decoding phases, calculated as: (cid:88) i=1 (4nid2 + 2n2 + 2nidm) (cid:125) (cid:123)(cid:122) (cid:124) Prefilling FLOPs per layer (cid:124) + R((4d2 + 2dm) + 2(dni + d(R + 1))) . (7) 1 (cid:123)(cid:122) Decoding FLOPs per layer (cid:125) For consistency, the decoding calculation is fixed for predicting = 100 tokens, accounting for the the KV cache. In video LLMs, the decoding phase FLOPs contribute only approximately 2% of the total. Consequently, our primary optimization focus is on the prefilling stage. When considering prefilling optimization, inner-LLM pruning methods like FastV [6], the FLOPs incurred in the first 2 shallow layers can amount to 2.9 TFLOPs in LLaVA-OneVision-7B. Even pruning 100% token in the layer, these methods cannot match the potential efficiency compared to outer-LLM pruning methods. Thus, outer-LLM pruning offers more impactful optimization approach for this domain. Implementation Details. Our method is implemented on LLaVA-OneVision-7B/72B [19] and LLaVA-Video-7B [53] models. Evaluation uses NVIDIA A100 GPUs, while inference is on an RTX A6000. Inference cost is measured by prefilling FLOPs, with baselines configured for comparable FLOPs (details in the appendix A). The default τ is 0.8; for 10% compression, τ is 0.65. Following official practice, LLaVA-OneVision models utilize 32 input video frames (Nv = 196), while LLaVAVideo uses 64 frames (Nv = 169). All benchmarks are conducted using LMMs-Eval [51, 18]. 6 Table 3: Cross-backbone method comparison. Performance comparison of our method against state-of-the-art methods across different backbones, demonstrating consistent effectiveness. Model Method Prefilling FLOPs (T) FLOPs Ratio Before LLM Retained Ratio MVBench EgoSchema LongVideo Bench VideoMME Avg. Score % LLaVAOneVision-72B LLaVAVideo-7B Vanilla FastV [6] Visionzip [48] PruneVid [15] HoliTom (w/o M) HoliTom Vanilla FastV [6] PDrop [45] VisionZip [48] HoliTom (w/o M) HoliTom 429.3 86.4 59.0 59.0 59.0 51.6 80.2 17.1 19.5 9.3 9.3 7.6 100% 20.1% 13.7% 13.7% 13.7% 12.0% 100% 21.3% 24.3% 11.6% 11.6% 9.5% 100% 100% 15% 15% 15% 15% 100% 100% 100% 15% 15% 15% 60.9 56.1 58.4 56.8 58.5 58.7 60.4 54.3 55.9 56.7 57.8 57.7 61.1 57.1 59.3 57.7 60.0 60.1 57.2 54.1 54.3 54.7 54.8 54. 62.7 57.0 57.4 57.4 57.8 57.2 58.9 55.0 54.7 54.7 55.6 56.2 65.7 61.2 63.8 62.7 64.1 64.3 64.3 58.8 61.9 60.7 61.9 62.1 62.6 57.9 59.7 58.6 60.1 60.1 60.2 55.6 56.7 56.7 57.5 57. 100 92.5 95.4 93.6 96.0 96.0 100 92.4 94.2 94.2 95.5 95.8 Figure 3: Left: Performance of our method vs. FastV when pruning various layers at rate R=50%. Right: Performance comparison with varying pruning rates at fixed layer (K=14). Figure 4: Performance vs. number of frames for our method and other token compression methods. 4.2 Main Results Comparison with State-of-the-Art Methods. Tab. 2 benchmarks Holitom against state-of-the-art approaches on the LLaVA-OneVision-7B model, analyzing performance and inference cost (FLOPs) at various token retention ratios (25%, 20%, 15%, and 10%) prior to LLM processing. Inner-LLM pruning methods, such as FastV [6] and PDrop [45], often struggle to balance performance and efficiency, especially at lower token retention ratios (25%). DyCoke [36], which segments video frames into groups of 4 and prunes all but the first frame, is limited by its design, capping its lowest retention ratio at 25%. Spatial pruning methods like VisionZip [48] show significant performance drop (up to 8.4%) at 10% retention. This decline stems from relying solely on spatial compression, less effective at preserving crucial temporal information needed for performance under aggressive pruning. Crucially, even without our inner-LLM merging technique, our method achieves state-ofthe-art performance and efficiency consistently across the evaluated retention ratios. This highlights the superior robustness and adaptability of our approach compared to prior methods. Our inner-LLM merging method further enhances efficiency, driving optimization further. For instance, we retain only 6.9% of the original FLOPs, while preserving 99.1% of the baseline performance. Performance Comparison Across Different Backbones. Tab. 3 assesses our methods performance across various backbones. For the powerful LLaVA-OneVision-72B model, sensitive to aggressive compression, our approach reduces computational cost to 11.3%, keeping 96% of its original performance. LLaVA-Video-7B presents greater compression challenge due to its higher initial pooling rate (169 vs. 196 tokens/frame in LLaVA-OneVision). Despite this, our method achieves reduction to just 9.5% of the original FLOPs, retaining 95.8% performance and outperforming existing methods. Overall, achieving significant token compression with minimal performance drop is indeed tougher for LLaVA-OneVision-72B and LLaVA-Video-7B than for LLaVA-OV-7B. HoliTom vs. FastV under Outer-LLM Compression. Building on the challenges faced by innerLLM pruning methods discussed in Section 4.1, we compare our inner-LLM merging method with FastV, specifically in scenarios where outer-LLM compression is already applied. In this compressed context, the property \"an image is worth 1/2 tokens after layer 2\" [6] is not consistently observed. This is because outer-LLM compression concentrates information, making trivial token discarding more difficult using attention mechanisms. As illustrated in Fig. 3, our method demonstrates superior performance compared to FastV when pruning 50% at shallower layers. Furthermore, at equivalent layers, our approach consistently surpasses FastV across wide range of pruning rates, underscoring its effectiveness. This effectiveness stems from our inner-LLM merging method, which better preserves information rather than directly discarding it. 7 Figure 5: Achieving superior inference. \"Other\" indicates token pre-processing time (e.g., pooling). Our proposed method reduces Time-To-First-Token (TTFT) by 2.28 and achieves 1.32 higher decoding throughput, outperforming all other token compression methods and the vanilla model. Table 4: Ablation study on merging modules. Our temporal merging module reduces FLOPs to 75.7% without performance loss, alleviates the performance degradation caused by aggressive spatial pruning. The integration of all 3 modules achieves the best performance-efficiency trade-off. Method Vanilla Only Temporal Only Spatial HoliTom (w/o M) HoliTom Prefilling FLOPs (T) FLOPs Ratio Before LLM Retained Ratio MVBench EgoSchema LongVideo Bench VideoMME Avg. Score % 40.8 30.9 5.2 5.2 4. 100% 75.7% 12.7% 12.7% 10.5% 100% 79% 15% 15% 15% 58.3 58.9 57.9 58.1 58.1 60.4 60.5 60.8 61.0 61.2 56.4 56.5 54.2 57.0 56.4 58.6 59.1 56.8 58.1 57. 58.4 58.8 57.4 58.5 58.2 100 100.7 98.3 100.2 99.7 Performance Scaling with more frames. Our method scales performance robustly with increasing input frames  (Fig. 4)  . challenge for video LLMs is that uniformly sampled frames may miss crucial information required for accurate answers. Therefore, an effective token pruning method is essential to process more frames and capture sufficient context. Fig. 4 shows our approach consistently outperforms other compression methods across frame rates. At 16 frames, where less temporal redundancy exists, our method, while slightly below the vanilla, still outperforms all other compression techniques. With 64 frames, our method is more efficient and achieves superior performance over the vanilla model. Furthermore, when processing 128 frames, our token compression approach avoids the maximum context length limitations that bottleneck vanilla models. This capability is particularly beneficial for tasks that require an extensive temporal context or to answer complex questions with long text, resulting in improved performance. Discussion: Improved Performance after Token Compression Tabs. 2, 4, and Fig. 4 present key finding: models employing our token compression technique outperform the original models on various benchmarks. This surprising result underscores fundamental principle for achieving superior performance at the input stage: the value of key information over exhaustive information. Excessive, irrelevant, or redundant data acts as noise, obscuring essential signals critical for effective processing. This information overload impedes the capacity of the model to accurately identify and process critical details, thereby degrading understanding and response generation. By providing refined input that retains pertinent information while shedding redundant information, our compression method facilitates deeper comprehension and yields more accurate, relevant outputs. Collectively, these results underscore the efficacy of our technique in distilling key information and demonstrate that intelligent input refinement is crucial for superior model performance. 4.3 Efficiency Results Fig. 5 summarizes the impact of various token compression methods on the inference efficiency of video LLMs. As shown, all the evaluated methods demonstrably reduce LLM prefilling time. Our method, in particular, reduces it to just 13.7% of the original. For VisionZip [48], PruneVid [15], and our HoliTom require token pre-processing, which introduces additional \"other\" time. Furthermore, both PruneVid and our method produce variable number of tokens per frame, complicating batch processing, which contributes to extra overhead. Our method, designed to maximize temporal redundancy pruning, leads to finer-grained segmentation, further influencing this observation. Nevertheless, our method achieves the maximum reduction in Time-To-First-Token latency, reducing it by 2.28, while maintaining optimal performance. Although we did not specifically optimize for decoding, our models decoding speed still benefits from the reduced number of vision tokens. Our throughput increased by 1.32 compared to the original model, the highest among all methods evaluated. 8 Figure 6: Histogram of temporal pruning rates across four benchmarks (τ = 0.80). The average pruning ratio for each benchmark is annotated in the top right. MVBench (16s duration) exhibits the highest ratio, reflecting greater temporal redundancy, while EgoSchema is the least. Table 5: Ablation study on video segmentation methods. This table compares different video segmentation strategies: Fixed-interval segmentation partitions the video at equal intervals; DySeg adaptively segments based on transition similarity; and our proposed global redundancy-aware segmentation. Methods MVBench EgoSchema Fixed-interval DySeg [33] HoliTom (w/o M) 57.0 56.8 56.9 60.9 60.8 61.1 LongVideo Bench VideoMME Avg. 53.8 54.1 56.5 56.4 56.6 56. 57.0 57.1 57.8 Figure 7: Ablation study on τ . Performance of our method is analyzed with varying τ at target before LLM retained ratio of 15%. 4.4 Ablation Study Ablation study on merging modules. Tab. 4 provides detailed ablation study on the contribution of our proposed merging modules. We first evaluated the temporal merging module (τ = 0.8), designed to eliminate temporal redundancy, which demonstrated efficiency gains while preserving performance. Across the four benchmarks, our method achieved 100.7% of the baseline performance while reducing FLOPs to 75.7%. Note that the reported average pruning rate is calculated over four datasets. The average pruning rate varied across datasets is illustrated in Fig. 6. For instance, MVBench(16s), with its shortest duration, exhibits the highest temporal redundancy, allowing approximately 43% pruning, whereas EgoSchema contains the least, permitting only about 9.3%. We then investigate combining temporal with spatial pruning. Applying our temporal pruning method significantly mitigates the performance degradation typically associated with aggressive spatial pruning alone. Furthermore, incorporating the inner merging module allowed us to push the efficiency boundaries even further, ultimately retaining 99.7% performance with mere 10.5% of the original FLOPs. Ablation study on temporal segmentation method. Tab. 5 compares different temporal segmentation methods. Fixed-interval Segmentation generates 8 segments with an interval of 4. DySeg [33] selects segment start points using the 8 largest inter-frame differences and includes frames below 0.90 similarity threshold. Our proposed global redundancy-aware segmentation maximally leverages spatial redundancy and achieves better performance. Ablation study on τ . The hyperparameter τ controls the sensitivity of the temporal pruning mechanism, with lower values leading to more aggressive pruning. For fixed retained ratio, τ also governs the balance between the amount of spatial and temporal pruning applied. Fig. 7, demonstrates the easy tunability of τ , with peak performance observed around τ = 0.8. This value is adopted uniformly without performance degradation, as shown in Tab. 4. For 10% pruning target, we set τ = 0.65 to mitigate performance degradation from aggressive spatial pruning."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper presents HoliTom, new training-free holistic token merging framework for boosting the efficiency of video LLMs by effectively handling redundant visual tokens. HoliTom achieves this through synergistic integration of outer-LLM spatio-temporal reduction, drastically reducing initial token counts, and robust inner-LLM token merging mechanism tailored for compatibility and further optimization. Evaluated on prominent video LLMs, HoliTom achieves state-of-the-art efficiency-performance trade-off, substantially reducing computational costs (e.g., to 6.9% FLOPs) while preserving high performance (e.g., 99.1% accuracy), and accelerating inference (2.28 TTFT, 1.32 throughput). These results underscore the effectiveness of HoliTom in enabling practical and efficient video LLMs inference for complex, long-form video understanding."
        },
        {
            "title": "References",
            "content": "[1] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, and Mohamed Elhoseiny. Minigpt4-video: Advancing multimodal llms for video understanding with interleaved visual-textual tokens. arXiv preprint arXiv:2404.03413, 2024. [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. In ICLR, 2023. [5] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. Videollm-online: Online video large language model for streaming video. In CVPR, 2024. [6] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In ECCV, 2024. [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [8] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. [9] Rohan Choudhury, Guanglei Zhu, Sihan Liu, Koichiro Niinuma, Kris Kitani, and László Jeni. Dont look twice: Faster video transformers with run-length tokenization. In NeurIPS, 2024. [10] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In ICLR, 2024. [11] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In NeurIPS, 2022. [12] Mingjing Du, Shifei Ding, and Hongjie Jia. Study on density peaks clustering based on k-nearest neighbors and principal component analysis. Knowledge-Based Systems, 99:135145, 2016. [13] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training compression for generative pretrained transformers. In ICLR, 2023. [14] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [15] Xiaohu Huang, Hao Zhou, and Kai Han. Prunevid: Visual token pruning for efficient video large language models. arXiv preprint arXiv:2412.16117, 2024. [16] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In CVPR, 2024. [17] Yang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, et al. Video-lavit: Unified video-language pre-training with decoupled visual-motional tokenization. In ICML, 2024. [18] Bo Li, Peiyuan Zhang, Kaichen Zhang, Fanyi Pu, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Accelerating the development of large multimoal models, 2024. 10 [19] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. [21] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. [22] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, 2024. [23] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In ECCV, 2024. [24] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. In MLSys, 2024. [25] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In CVPR, 2024. [26] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. Nvila: Efficient frontier visual language models. arXiv preprint arXiv:2412.04468, 2024. [27] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. In NeurIPS, 2023. [28] Rui Qian, Shuangrui Ding, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Dispider: Enabling video llms with active real-time interaction via disentangled perception, decision, and reaction. In CVPR, 2025. [29] Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Shuangrui Ding, Dahua Lin, and Jiaqi Wang. Streaming long video understanding with large language models. In NeurIPS, 2024. [30] Shuhuai Ren, Sishuo Chen, Shicheng Li, Xu Sun, and Lu Hou. Testa: Temporal-spatial token aggregation for long-form video-language understanding. In EMNLP, 2023. [31] Alex Rodriguez and Alessandro Laio. Clustering by fast search and find of density peaks. science, 344(6191):14921496, 2014. [32] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. [33] Leqi Shen, Guoqiang Gong, Tao He, Yifeng Zhang, Pengzhang Liu, Sicheng Zhao, and Guiguang Ding. Fastvid: Dynamic density pruning for fast video large language models. arXiv preprint arXiv:2503.11187, 2025. [34] Leqi Shen, Tianxiang Hao, Sicheng Zhao, Yifeng Zhang, Pengzhang Liu, Yongjun Bao, and Guiguang Ding. Tempme: Video temporal token merging for efficient text-video retrieval. In ICLR, 2025. [35] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In CVPR, 2024. [36] Keda Tao, Can Qin, Haoxuan You, Yang Sui, and Huan Wang. Dycoke: Dynamic compression of tokens for fast video large language models. In CVPR, 2025. [37] Keda Tao, Haoxuan You, Yang Sui, Can Qin, and Huan Wang. Plug-and-play 1. x-bit kv cache quantization for video large language models. arXiv preprint arXiv:2503.16257, 2025. [38] Ao Wang, Fengyuan Sun, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding. [cls] token tells everything needed for training-free efficient mllms. arXiv preprint arXiv:2412.05819, 2024. [39] Haicheng Wang, Zhemeng Yu, Gabriele Spadaro, Chen Ju, Victor Quétu, Shuai Xiao, and Enzo Tartaglione. Folder: Accelerating multi-modal large language models with enhanced performance. arXiv preprint arXiv:2501.02430, 2025. 11 [40] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024. [41] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [42] Zhenhailong Wang, Senthil Purushwalkam, Caiming Xiong, Silvio Savarese, Heng Ji, and Ran Xu. Dymu: Dynamic merging and virtual unmerging for efficient vlms. arXiv preprint arXiv:2504.17040, 2025. [43] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. In NeurIPS, 2024. [44] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In ICML, 2023. [45] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. In CVPR, 2025. [46] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. [47] Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Chendi Li, Jinghua Yan, Yu Bai, Ponnuswamy Sadayappan, Xia Hu, et al. Topv: Compatible token pruning with inference time optimization for fast and low-memory multimodal vision language model. In CVPR, 2025. [48] Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. In CVPR, 2025. [49] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. [50] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. In EMNLP, 2023. [51] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024. [52] Qizhe Zhang, Aosong Cheng, Ming Lu, Zhiyong Zhuo, Minqi Wang, Jiajun Cao, Shaobo Guo, Qi She, and Shanghang Zhang. [cls] attention is all you need for training-free visual token pruning: Make vlm inference faster. arXiv preprint arXiv:2412.01818, 2024. [53] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024."
        },
        {
            "title": "A Supplemental Implementation Details",
            "content": "Our method is implemented on the LLaVA-OneVision-7B/72B [19] and LLaVA-Video-7B [53] models. Evaluation utilized NVIDIA A100 (80GB) GPUs; inference was performed on an NVIDIA RTX A6000 GPU. To ensure fair comparison of computational cost, we used total prefilling FLOPs as the primary metric. Baselines are configured for comparable FLOPs: FastV [6] prunes 80% of tokens at layer 2; PDrop [45] retains 50%, 25%, and 12.5% of vision tokens at layers 2, 7, and 14, respectively; VisionZip [48] and PruneVid [15] maintain consistent proportion of input tokens with our method. Performance results for FastVID [33] are adopted directly from their original paper. For our proposed method, the default threshold τ is 0.8. In experiments targeting 10% compression ratio, τ was set to 0.65. Experimental setups include pruning K=18 layers of the 7B model and K=60 layers of the 72B model, both at ratio of R=50%. Following the official LLaVA-OneVision specifications, the default input video frames are 32 and Nv = 196. For LLaVA-Video, the default input consisted of 64 video frames with Nv = 169. All benchmark evaluations are performed using the LMMs-Eval [51, 18]. Figure 8: Histogram of temporal pruning rates across four benchmarks (τ = 0.65). The average pruning ratio for each benchmark is annotated in the top right. MVBench (16s duration) exhibits the highest ratio, reflecting greater temporal redundancy, while VideoMME is the least (τ = 0.65). Supplemental Ablation Study on τ In section 4.4, we discussed the selection of τ (τ = 0.8) and the corresponding histogram of temporal pruning rates on four benchmarks for retain ratio of 15%. Next, we detail the selection of the hyperparameter τ for 10% retain ratio and present the corresponding histogram. As illustrated in Fig. 9, peak performance is observed around τ = 0.65. The Fig. 8 presents the histogram of temporal pruning rates on the four datasets when τ = 0.65. It is evident that controlling τ regulates the aggressiveness of temporal pruning; larger τ results in more aggressive pruning."
        },
        {
            "title": "C Compatible with Flash Attention",
            "content": "Figure 9: Ablation study on τ . Performance of our method is analyzed with varying τ at target before LLM retained ratio of 10%. Our approach introduces two distinct merging strategies: inner-LLM and outer-LLM. The inner-LLM strategy, similar to prior work [6, 45, 36], is designed for integration with highly optimized attention implementations (e.g., Flash Attention [11, 10]). This requires obtaining attention scores from specific layer only once during the prefilling stage, an operation introducing negligible computational overhead compared to total inference cost. In contrast, our outer-LLM merging strategy operates externally to the model, decoupled from the attention mechanisms of LLM."
        },
        {
            "title": "D Limitations and Future Work",
            "content": "While our work demonstrates an adequate acceleration of video LLMs by token merging for inference, it is important to outline its current limitations. First, the approach is primarily designed for fixedlength video clips and does not natively support online, arbitrary-length streaming video input. This poses challenges for real-time processing [5, 29, 28] and maintaining long-term context understanding. Second, as shown in Fig. 5, similar to other methods [6, 45, 48, 36] in the token pruning area, our 13 approach does not optimize the latency of the vision tower. Further work, such as quantization [24, 44, 13] and methods to accelerate the vision tower [9, 39, 42], is worth exploring for further optimization."
        },
        {
            "title": "E Broader impacts",
            "content": "This work significantly enhances video LLM efficiency, addressing key barrier to deployment and scalability. By reducing computational needs, it broadens access to advanced video AI, enabling wider application and fostering innovation."
        },
        {
            "title": "F More Visualizations",
            "content": "Figure 10: Comparison on Challenging Video Understanding. Green: correct results, Red: incorrect results. Our method is able to produce correct answers on challenging video tasks. Figure 11: Qualitative generation comparison. Green indicates correctly detailed descriptions. Our method achieves high-quality, accurate text generation even when retaining only 15% of input tokens."
        }
    ],
    "affiliations": [
        "Columbia University",
        "Rice University",
        "Salesforce AI Research",
        "Westlake University",
        "Zhejiang University"
    ]
}