{
    "paper_title": "iFormer: Integrating ConvNet and Transformer for Mobile Application",
    "authors": [
        "Chuanyang Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a new family of mobile hybrid vision networks, called iFormer, with a focus on optimizing latency and accuracy on mobile applications. iFormer effectively integrates the fast local representation capacity of convolution with the efficient global modeling ability of self-attention. The local interactions are derived from transforming a standard convolutional network, \\textit{i.e.}, ConvNeXt, to design a more lightweight mobile network. Our newly introduced mobile modulation attention removes memory-intensive operations in MHA and employs an efficient modulation mechanism to boost dynamic global representational capacity. We conduct comprehensive experiments demonstrating that iFormer outperforms existing lightweight networks across various tasks. Notably, iFormer achieves an impressive Top-1 accuracy of 80.4\\% on ImageNet-1k with a latency of only 1.10 ms on an iPhone 13, surpassing the recently proposed MobileNetV4 under similar latency constraints. Additionally, our method shows significant improvements in downstream tasks, including COCO object detection, instance segmentation, and ADE20k semantic segmentation, while still maintaining low latency on mobile devices for high-resolution inputs in these scenarios."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 9 6 3 5 1 . 1 0 5 2 : r Published as conference paper at ICLR 2025 IFORMER: FORMER FOR MOBILE APPLICATION INTEGRATING CONVNET AND TRANSChuanyang Zheng chuanyang zheng@sjtu.edu.cn https://github.com/ChuanyangZheng/iFormer"
        },
        {
            "title": "ABSTRACT",
            "content": "We present new family of mobile hybrid vision networks, called iFormer, with focus on optimizing latency and accuracy on mobile applications. iFormer effectively integrates the fast local representation capacity of convolution with the efficient global modeling ability of self-attention. The local interactions are derived from transforming standard convolutional network, i.e., ConvNeXt, to design more lightweight mobile network. Our newly introduced mobile modulation attention removes memory-intensive operations in MHA and employs an efficient modulation mechanism to boost dynamic global representational capacity. We conduct comprehensive experiments demonstrating that iFormer outperforms existing lightweight networks across various tasks. Notably, iFormer achieves an impressive Top-1 accuracy of 80.4% on ImageNet-1k with latency of only 1.10 ms on an iPhone 13, surpassing the recently proposed MobileNetV4 under similar latency constraints. Additionally, our method shows significant improvements in downstream tasks, including COCO object detection, instance segmentation, and ADE20k semantic segmentation, while still maintaining low latency on mobile devices for high-resolution inputs in these scenarios."
        },
        {
            "title": "INTRODUCTION",
            "content": "Building lightweight neural networks facilitates real-time analysis of images and videos captured by mobile applications such as smartphones. This not only enhances privacy protection and security by processing data locally on the device but also improves overall user experience. Through the decades, convolutional neural networks (CNNs) (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016) have emerged as the primary choice for balancing latency and performance on resource-constrained mobile devices. However, significant limitation of CNNs is their reliance on local sliding window mechanism, which imposes crucial inductive biases that may hinder modeling flexibility. Recently, the soaring development of vision transformers (ViTs) (Dosovitskiy et al., 2020) has begun to dominate various computer vision tasks, including image classification (Zhai et al., 2022), object detection (Liu et al., 2021a), and semantic segmentation (Xie et al., 2021). The core mechanism underlying ViTs is self-attention, which dynamically learns interactions between all image patches. This enables the model to focus on important regions adaptively and capture more global features. Nevertheless, deploying ViTs on mobile devices with limited resources poses significant challenges. On the one hand, the quadratic computational complexity of attention renders them unsuitable for large feature maps, which are common in the early stages of viFigure 1: Comparison of latency and accuracy between our iFormer and other existing methods on ImageNet-1k. The latency is measured on an iPhone 13. Our iFormer is Pareto-optimal. 1 Published as conference paper at ICLR 2025 sion networks. On the other hand, the multi-head mechanism requires reshaping operations, leading to increased memory usage. Many research efforts are devoted to combining the advantages of both CNNs and ViTs in designing lightweight networks while mitigating inefficient operations in mobile applications. Some studies (Zhang et al., 2023; Wang et al., 2024; Ma et al., 2024) revisit the architectural designs of lightweight CNNs from ViT perspective and incorporate key components that contribute to the performance of ViTs into CNNs. Although these pure lightweight CNNs show improved performance compared to previous mobile networks (Howard et al., 2017; Zhang et al., 2018; Sandler et al., 2018), they still lag behind the powerful self-attention in ViTs. Another line of works (Mehta & Rastegari, 2021; Chen et al., 2022b; Li et al., 2023; Cai et al., 2023; Shaker et al., 2023; Vasu et al., 2023a; Qin et al., 2024) proposes innovative attention mechanisms to address the limitation of standard attention (Vaswani, 2017) and blend convolutions to achieve better balance between latency and performance. These attention mechanisms either reduce the number of queries and keys (Shaker et al., 2023; Qin et al., 2024), limit the attention span (Wan et al., 2023), or adopt linear attention (Cai et al., 2023), which may compromise performance to some extent. In this work, we present the iFormer, herd of lightweight models that integrates the strengths of both CNNs and ViTs, achieving state-of-the-art balance between latency and accuracy. Specifically, we employ hierarchical architecture consisting of four stages. In the earlier, high-resolution stages, we utilize fast convolution to extract local representations. To construct the convolutional block, we start with modern ConvNeXt (Liu et al., 2022), which incorporates series of design decisions inspired by ViTs. Then we progressively lighten the ConvNeXt to create streamlined lightweight network, optimizing it for real-time mobile latency on an iPhone 13, in contrast to the FLOPs and parameters used in prior works (Mehta & Rastegari, 2021; Chen et al., 2022b). This results in fast convolutional architecture with strong performance. To further enhance the dynamic properties and its ability to model long-range contexts, we incorporate self-attention in the later low-resolution stages. However, direct implementation of standard multi-head self-attention (MHA) brings notable memory overheads and slows down inference speed on mobile devices. We identify that the increased latency stems primarily from the reshaping operations in MHA. More analyses reveal that multiple attention heads behave similarly. Therefore, we propose simple yet effective single-head modulation self-attention (SHMA), which significantly minimizes memory costs while preserving strong performance. Fig. 4 provides an illustration of SHMA. In detail, SHMA learns spatial context interactions through optimized self-attention. Concurrently, parallel feature extraction branch is employed to capture informative features. Finally, we fuse the outputs of these two branches to facilitate more flexible and dynamic exchange of information, compensating for the slight performance degradation of the single-head attention when compared to MHA. Benefiting from the fast local representation capacity of convolution and the efficient global modeling proficiency of the proposed SHMA, iFormer outperforms existing pure lightweight CNNs and hybrid networks across multiple visual recognition tasks, including image classification, object detection, instance segmentation, and semantic segmentation. For instance, in the context of image classification as shown in Fig. 1, iFormer-M achieves Top-1 accuracy of 80.4% with only 1.10 ms on an iPhone 13 without advanced training strategies such as knowledge distillation (Touvron et al., 2021a) or reparameterization (Ding et al., 2021). Notably, our model obtains 0.5% improvement in Top-1 accuracy compared to the recent MNV4-Conv-M (Qin et al., 2024), while being 1.4 faster than FastViT-SA12 (Vasu et al., 2023a) with similar accuracy. These results demonstrate the effectiveness of the proposed network in capturing both local and global feature representations."
        },
        {
            "title": "2.1 EFFICIENT CONVOLUTIONAL NETWORKS",
            "content": "In the past 2010s, computer vision was dominated by CNNs, and so were efficient networks. The first remarkable breakthrough in mobile CNNs is MobileNets (Howard et al., 2017), which hatches the concept of decomposing standard convolution into depthwise and pointwise counterparts. Subsequently, MobileNetV2 (Sandler et al., 2018) introduces an inverted residual bottleneck block to push the state-of-the-art for mobile models. Numerous studies have aimed to accelerate CNNs via various approaches, such as channel shuffle in ShuffleNet (Zhang et al., 2018; Ma et al., 2018) and cheap linear transformations in GhostNet (Han et al., 2020). Meanwhile, Neural architecture search 2 Published as conference paper at ICLR 2025 (NAS) has emerged as method for automating the design of neural networks, optimizing for performance under resource constraints. EfficientNet (Tan & Le, 2019), MobileNetV3 (Howard et al., 2019), and FBNet (Wu et al., 2019) all achieve rather good performance. Besides, MobileOne (Vasu et al., 2023b) proposes to train model using reparameterizable branches, which are merged during inference. Recently, following the revolution of ViTs, several methods reexamine the design spaces and training strategies (Liu et al., 2024) for mobile CNNs. For instance, RepViT (Wang et al., 2024) integrates efficient architectural designs from ViTs into MobileNetV3, outperforming existing lightweight CNNs. Other approaches, such as FocalNet (Yang et al., 2022a), Conv2Former (Hou et al., 2024), and EfficientMod (Ma et al., 2024), fuse features from context modeling and feature projection branches, also known as modulation mechanism, to enhance the model with dynamic properties analogous to attention. However, pure CNNs remain inherently spatially localized and their reliance on stationary weights restricts their flexibility. Although modulation can partially mitigate this limitation by enhancing dynamic capacity, they still exhibit deficiencies in building global interactions."
        },
        {
            "title": "2.2 EFFICIENT VISION TRANSFORMERS",
            "content": "The success of Vision Transformer (Dosovitskiy et al., 2020) offers compelling demonstration of the potential to apply transformer to computer vision tasks. Following this, ViT and its numerous variants (Liu et al., 2021a; Dong et al., 2022; Li et al., 2022a) sweep across various scenarios. However, the quadratic complexity of self-attention behind ViTs poses significant challenges for efficiency. The following researches seek to boost ViT efficiency through efficient attention mechanisms (Wang et al., 2021; Zhu et al., 2023; Hatamizadeh et al., 2023), model compression (Liu et al., 2021b; Zheng et al., 2022), knowledge distillation (Hao et al., 2021), and token reduction (Rao et al., 2021; Bolya et al., 2022). Recent studies further introduce ViTs into mobile applications. One mainstream of work combines efficient convolution and ViT to create lightweight hybrid networks (Mehta & Rastegari, 2022; Vasu et al., 2023a). MobileViT (Mehta & Rastegari, 2021) directly integrates MobileNetv2 blocks and ViT blocks, while Mobile-Former (Chen et al., 2022b) features parallel design of MobileNet and ViT with two-way bridge connecting the two. To further accelerate inference, some approaches replace the standard attention (Vaswani, 2017) with efficient variants within the hybrid networks. These include reducing the number of delegate tokens for computing attention (Pan et al., 2022), employing channel attention (Maaz et al., 2022), substituting projection in attention with efficient ghost modules (Ma et al., 2022), and utilizing linear attention mechanisms (Zhao et al., 2022). Besides manual designs, EfficientFormer (Li et al., 2022b; 2023) and MobileNetV4 (Qin et al., 2024) search for efficient architectures in unified space encompassing both convolution operators and transformer operators. Another stream of work focuses on efficient attention mechanisms and directly employs them throughout the entire network (Shaker et al., 2023; Cai et al., 2023). For example, CMT (Guo et al., 2022) takes advantage of depth-wise convolution to downsample key and value to reduce computation. GhostNetV2 (Tang et al., 2022) applies two fully connected layers along the horizontal and vertical directions to compute attention, decoupled version of MLP-Mixer (Tolstikhin et al., 2021). Recently, SHViT observes computational redundancy in the multi-head attention module and proposes to apply sing-head attention. In contrast to these existing approaches, we introduce novel efficient attention module without sacrificing informative interactions, thereby maintaining strong representational capacity. Regarding attention design, ours is bit similar to SHViT but is considerably superior as shown in Table 18 in the supplementary material. The key difference lies in the novel modulation attention. In addition, we explore efficient attention mechanisms in an on-device environment while SHViT focuses on general-purpose GPUs, fundamentally different hardware."
        },
        {
            "title": "3 METHOD",
            "content": "We present the overall architecture of our iFormer in Fig. 4, which offers Pareto-optimal accuracylatency trade-off on mobile applications. Our exploration towards streamlined lightweight network unfolds as follows: 1) establishing the baseline and measure metric in Sec. 3.1. 2) exploring acceleration techniques consisting of macro and micro designs in Sec. 3.2. 3) injecting global attention in Sec. 3.3. Finally, we create new family of efficient hybrid vision transformers tailored for mobile applications in Sec. 3.3. detailed trajectory illustrating the evolution from general hierarchical CNN to fast hybrid vision transformer is depicted in Fig. 2. 3 Published as conference paper at ICLR 2025 Figure 3: The distribution of average cosine similarity among multiple heads within the MHA mechanism. As the layer depth increases, the similarity goes higher. Table 1: Latency comparison between multi-head and single-head baseline."
        },
        {
            "title": "Models",
            "content": "Latency (ms) Top-1 Acc. (%) MHA Baseline SHA Baseline 1.12 (1.25) 1.40 79.9 79.8 Figure 2: Illustration of the evolution from the ConvNeXt baseline towards the lightweight iFormer. The orange bars are model accuracies and the light blue bars are model latencies. We also include red latency outline for better visualization."
        },
        {
            "title": "3.1 PREPARING CONVNEXT",
            "content": "Our goal is to create an efficient multiscale network, where spatial dimensions of intermediate repIn this hierarchical architecture, early network layers resentations shrink as inference proceeds. have larger spatial dimensions and fewer channels (e.g. 565648), which renders them memorybound. Highly optimized convolution is more appropriate for these layers. Guided by this principle, we choose pure convolutional network as our base architecture, specifically ConvNeXt (Liu et al., 2022) which absorbed several key components from ViTs and competes favorably against ViTs. We gradually lighten the network to achieve more favorable balance between latency and accuracy. For speed metric, we utilize on-device latency, measured on an actual iPhone 13 and compiled by Core ML Tools (CoreML), rather than FLOPs and parameter counts in previous methods (Mehta & Rastegari, 2021; Chen et al., 2022b; Zhang et al., 2022), which are not well correlated with latency. Regarding performance, we follow the training recipe in ConvNeXt while removing the layer scale to align prior methods (Li et al., 2022b; Wang et al., 2024) for fair comparison. Please refer to Sec. in the supplementary material for more details. To initiate our study, we systematically scale down the ConvNeXt by reducing the number of blocks and the width. This results in lightweight model with latency of 1.07 ms and Top-1 accuracy of 74.9%, serving as our initial baseline."
        },
        {
            "title": "3.2 LIGHTENING BASELINE",
            "content": "Seeing Better with Early Convolutions Following ViTs, ConvNeXt adopts an aggressive patchify strategy as the stem cell, specifically by splitting the input image into series of nonoverlapping patches via 4x4 non-overlapping convolutional layer. However, some studies (Xiao et al., 2021; Chen et al., 2022a) indicate that an early convolutional stem can increase optimization stability and facilitate faster model convergence. Moreover, compared to general models, lightweight models typically have fewer parameters and reduced capacity. An aggressive nonoverlapping layer may lead to the premature loss of rich information. Consequently, we opt to replace the non-overlapping patchify stem with stack of overlapping convolutional layers, as shown in Fig. 4. This modification elevates the top-1 accuracy to 76.7% with neglectable increase in latency of 0.1 ms. Normalization An obvious difference between ConvNeXt and previous CNNs is the normalization layer. ConvNeXt utilizes Layer Normalization (LN) (Ba et al., 2016), commonly used in Natural Language Processing (NLP), whereas the latter uses Batch Normalization (BN) (Ioffe, 2015). Albeit its superior performance, LN requires on-the-fly statistics calculation in inference along with 4 Published as conference paper at ICLR 2025 Figure 4: Overview of iFormer architecture, detailed convolutional stem, block design, and SHMA. The hatched area in SHMA indicates extra memory-intensive reshaping operations that are eliminated by SHMA. S() denotes the softmax function. is the ratio for reducing channels of query and key. It is set to 2 in iFormer. We omit BN following project or convolution for simplicity. division and square root operations, leading to inefficiency on mobile hardware (Yang et al., 2022b). On the contrary, BN operates with fixed statistics during inference as an offline method and can be seamlessly fused with other linear operations, providing free lunch. This significantly reduces computational demands and memory overheads on mobile devices. Therefore, we substitute LN with BN throughout the network and merge it during inference. Additionally, we also substitute non-overlapping downsample layers with overlapping counterparts. These adjustments result in reduction of overall latency to 0.77 ms while enhancing the Top-1 accuracy slightly to 77.10%. Going Deeper There is considerable evidence indicating that increasing the depth of model can enhance its capacity and yield performance benefits (Touvron et al., 2021b; Yang et al., 2022a). Most lightweight models typically stack more blocks to boost performance within constrained resources, as exemplified by the MobileNet series (Howard et al., 2019; Qin et al., 2024). In this study, we explore the potential of deepening ConvNeXt by increasing the number of blocks in each stage from (2,2,6,2) to (3,3,9,3). This increase in depth leads to substantial improvement, raising the accuracy from 77.1% to 78.8%, although causing temporary increase in latency to 1.05 ms. Stage Ratio The stage ratio in ConNeXt is not optimized for lightweight models. substantial number of depthwise convolutions in the early stages incurs significant memory transfer costs. Meanwhile, the presence of many blocks with channel expansion ratio of 4 in the Feed-Forward Network (FFN) in the last stage, which already has high channel dimension, imposes substantial computational demands. These factors lead to sub-optimal allocation of computational resources. To address these issues, we propose reallocating more computational resources to the third stage while reducing memory access costs in the early stage. Specifically, the blocks in each stage is adjusted from (3,3,9,3) to (2,2,18,2). As expected, this achieves better balance between latency and performance, with Top-1 accuracy increasing to 79.1% while enjoying lower latency of 1.01ms. Kernel Size Here we examine the effects of different kernel sizes in mobile settings and observe that utilizing larger kernel size introduces nearly no latency burden, as shown in Table 2. So we maintain the convolutional kernel size at 77 in each basic block, consistent with ConvNeXt. Furthermore, previous approaches use kernel size of 33 in the convolutional stem. This small receptive field may hinder feature representation during the early downsampling process. As previously noted, the early layers are memory-bound, allowing for opportunities to employ computeintensive operations (i.e., dense convolution). Therefore, we enlarge the kernel size of the dense Table 2: Latency under different convolutional kernel sizes. Latency (ms)"
        },
        {
            "title": "Kernel Size",
            "content": "33 77 1.00 1.01 5 Published as conference paper at ICLR 2025 convolutional layer in the stem cell to 55. As illustrated in Fig. 2, this change has no impact on inference latency while enhancing Top-1 accuracy by 0.3%."
        },
        {
            "title": "3.3 SINGLE HEAD MODULATION ATTENTION",
            "content": "Single-Head vs. Multi-Head ViTs typically apply MHA, which projects the queries, keys, and values multiple times with different learnable linear projections and performs multiple attention functions simultaneously. In practice, the multi-head mechanism requires the reshaping of feature maps first, causing large memory access and transfer costs. This can seriously impact inference latency, especially on resource-constrained mobile devices. To investigate this issue, we substitute the last half of the convolutional blocks in the third stage and all blocks in the last stage with standard ViT blocks, as depicted in Fig. 4. We refer to this hybrid network as the MHA baseline. Next, we build another network by substituting the MHA with Single-Head self-Attention (SHA), referring to it as the SHA baseline. The comparison is shown in Table 1. The SHA baseline shows 1.25 acceleration over its MHA counterpart on the iPhone 13. This verifies that additional reshaping operations in MHA incur significant memory access costs, leading to considerable decline in inference speed. This naturally calls for optimizing MHA. Recent methods (Pan et al., 2022; Qin et al., 2024) primarily focus on downsampling the query or the key, which may hurt global attention capacity. Instead, we aim to reduce the redundant reshaping of MHA while preserving all token-to-token interactions. Previous works (Michel et al., 2019; Yun & Ro, 2024) indicate that single attention head can approach the performance of multiple heads in general plain transformer models, such as DeiT. To investigate this on the mobile application, we analyze the average cosine similarity of multiple heads within the same layer of the aforementioned MHA baseline, which is hierarchical lightweight network, and present our findings in Fig. 3. We clearly see that the average cosine similarity reaches 50% and even 75% in the final layer. Furthermore, the SHA baseline, as shown in Table 1, exhibits only negligible accuracy drop of 0.1%. These suggest that SHA achieves more favorable balance between accuracy and latency, obtaining an accuracy of 79.8% with latency of 1.12 ms. Modulation Attention We further introduce novel modulation attention to boost performance and strengthen flexibility in modeling, as illustrated in Fig. 4. Formally, we start from the abstracted modulation mechanism (Ma et al., 2024), similar to the gate mechanism Shazeer (2020). Assume we are given an input feature map RCHW where C, H, and denote the channels, height, and width of the feature map. The modulated output can be written as follows: xo = (x) ctx(x), (1) where () denotes the feature mapping branch and ctx() is the context modeling branch. The output xo is the fused features from both branches via efficient element-wise multiplication. The key idea of our approach is to modulate the feature using SHA instead of relying on convolutional layers, as seen in previous works (Yang et al., 2022a; Ma et al., 2024). Since SHA captures global interactions through self-attention, it excels in extracting rich contextual information and better controlling the flow of information. This process can be expressed as follows: ctx(x) = SHA(WQx, WKx, WVx), (2) where WQ, WK, WV are the project weights for query, key, and value, respectively. For simplicity, we omit the bias term. To minimize inference costs, we utilize single projection layer in the feature mapping branch. To enhance expressivity and improve optimization stability, we apply individual nonlinear activation functions to both branches, as follows: xo = σ(WMx) σ(ctx(x)), (3) where σ is the sigmoid function and WM denotes the feature projection weight. We also experiment with various activation functions for modulation in Sec. 5 and observe that the sigmoid works rather well. Finally, the output from the modulation attention is projected in manner as standard attention. Equipped with Single-Head Modulation Attention (SHMA), our model improves the accuracy to 80.4% with an intermediate latency of 1.15 ms. This performance notably surpasses that of the recent MobileNetV4, which achieves an accuracy of 79.9%. 6 Published as conference paper at ICLR 2025 Reducing Width Until now, we have developed lightweight network that performs pretty well, but at bit slow speed. To push the trade-off toward the state-of-the-art, we revise the width configuration in the SHMA. The modulation mechanism enriches the output by enabling more dynamic modeling in both spatial and channel dimensions, making it possible to use weaker SHA and FFN. In light of this, we reduce the head dimension in the SHMA (i.e., WQ, WK) to small factor of the feature dimension, further details can be found in Table 15 in the supplementary material. Simultaneously, we shrink the expansion ratio in FFN following SHMA from 4 to 3. This process obtains lower latency of 1.10 ms, although slight drop of 0.2% in accuracy. Positional Embedding Last but not least, positional information plays crucial role in selfattention as it regards input as set of tokens. Adding positional embedding will help the attention learn permutation-variant features. We apply conditional positional encodings (CPE) (Chu et al., 2021) that are dynamically generated and conditioned on the local neighborhood of the input tokens, as illustrated in Fig. 4. The integration of CPE further enhances our models performance, achieving Top-1 accuracy of 80.4% with only 1.10 ms, establishing state-of-the-art trade-off. iFormer The result of these modifications is an extremely fast and efficient hybrid network, which we denote iFormer. The overall architecture is depicted inFig. 4. It integrates fast local convolutional layers in the early stages that operate on higher resolution and global SHMA in later stages which processes lower resolution. Besides, we create series of iFormer models tailored to various hardware resource constraints. For detailed architectural hyperparameters of these model variants, please refer to Table 15 in the supplementary material."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4."
        },
        {
            "title": "IMAGE CLASSIFICATION",
            "content": "Settings. We first evaluate our models on classification on ImageNet-1K (Deng et al., 2009). To ensure fair comparison with prior studies, we follow the previous training recipe (Touvron et al., 2021a; Liu et al., 2022) and train all models for 300 epochs with standard image size of 224x224. Please refer to Sec. in the supplementary material for details. Besides Top-1 validation accuracy, we also report the latency measured on an iPhone 13 with models compiled by Core ML Tools (CoreML) under batch size of 1, as done in (Li et al., 2023; Wang et al., 2024; Vasu et al., 2023b). Its worth highlighting that we do not apply any advanced strategies such as distillation (Li et al., 2023) and reparameterization (Ding et al., 2021)."
        },
        {
            "title": "Epochs",
            "content": "Top-1 (%) Latency (ms) Reso. EfficientFormerV2-S1 (2023) EfficientFormerV2-S1 (2023) MobileViGv2-S*(2024) FastViT-T12* (2023a) RepViT-M1.1* (2024) iFormer-M Table 4: Results with distillation on ImageNet-1K. * indicates the model is trained with strong training strategy (i.e., reparameterization). Table 3 summarizes comparison between our iFormer and state-of-the-art lightweight models, organized by latency. iFormer demonstrates Paretooptimal trade-off between accuracy and latency. For example, iFormer-M obtains 80.4% top-1 accuracy with latency of only 1.1 ms, surpassing recent MobileNetV4-Conv-M and RepViT-M1 by 0.5% and 1.0%, respectively. This is noteworthy considering that MobileNetV4 requires longer training schedule (500 vs. 300) and takes larger input resolution (256 vs. 224). When compared to other recent models using reparameterization, including FastViT-T12, GhostNetV3-1.3, and MobileOne-S3, iFormer-M achieves superior accuracy while maintaining lower latency. Moreover, iFormer outperforms various hybrid networks. Thanks to the efficient SHMA, iFormer-L achieves more outstanding performance than other attention variants, such as multi-query attention in MNV4-Hybrid-M, additive attention in SwiftFormer-L1, and linear attention in EfficientVIT-B1-r288. SHViT-S4 (2024) EfficientFormerV2-S2 (2023) MobileViGv2-M(2024) FastViT-SA12* (2023a) EfficientFormerV2-S2 (2023) RepViT-M1.5* (2024) iFormer-L 1.48 1.60 1.70 1.50 1.60 1.54 1.60 80.2 81.6 81.7 81.9 82.0 82.3 82.7 300 300 300 300 450 300 300 224 224 224 256 224 224 224 79.0 79.7 79.8 80.3 80.7 81.1 1.02 1.02 1.24 1.12 1.04 1. 224 224 224 256 224 224 300 450 300 300 300 300 7 Published as conference paper at ICLR 2025 Table 3: Classification results on ImageNet-1K. indicates models that are trained with variety of advanced training strategies including complex reparameterization, distillation, optimizer, and so on. We provide more comprehensive comparison in Sec. in the supplementary material."
        },
        {
            "title": "Model",
            "content": "Params (M) GMACs Latency (ms) Reso."
        },
        {
            "title": "Epochs",
            "content": "Top-1 (%) MobileNetV2 1.0x (2018) MobileNetV3-Large 0.75x (2019) MNV4-Conv-S (2024) iFormer-T MobileNetV2 1.4x (2018) MobileNetV3-Large 1.0x (2019) SwiftFormer-XS (2023) SBCFormer-XS (2024) GhostNetV3 1.0x (2024) MobileOne-S2 (2023b) RepViT-M1.0 (2024) iFormer-S EfficientMod-xxs (2024) SBCFormer-S (2024) MobileOne-S3 (2023b) SwiftFormer-S (2023) GhostNetV3 1.3x (2024) FastViT-T12 (2023a) RepViT-M1.1 (2024) MNV4-Conv-M (2024) iFormer-M Mobile-Former-294M (2022b) MobileViT-S (2021) MobileOne-S4 (2023b) SBCFormer-B (2024) GhostNetV3 1.6x (2024) EfficientViT-B1-r288 (2023) FastViT-SA12 (2023a) MNV4-Hybrid-M (2024) SwiftFormer-L1 (2023) EfficientMod-s (2024) RepViT-M1.5 (2024) iFormer-L 3.4 4.0 3.8 2. 6.9 5.4 3.5 5.6 6.1 7.8 6.8 6.5 4.7 8.5 10.1 6.1 8.9 6.8 8.2 9.2 8.9 11.4 5.6 14.8 13.8 12.3 9.1 10.9 10.5 12.1 12.9 14.0 14.7 0.30 0.16 0.20 0.53 0.59 0.22 0.60 0.70 0.17 1.30 1.10 1.09 0.60 0.90 1.90 1.00 0.27 1.40 1.30 1.00 1. 0.29 2.00 2.98 1.60 0.40 0.86 1.90 1.20 1.60 1.40 2.30 2.63 0.73 0.67 0.60 0.60 1.02 0.76 0.95 0.79 0.99 0.92 0.85 0.85 1.29 1.02 1.16 1.12 1.24 1.12 1.04 1.08 1.10 2.66 3.55 1.74 1.44 1.49 3.87 1.50 1.75 1.60 2.57 1.54 1.60 224 224 224 224 224 224 224 224 224 224 224 224 224 224 224 224 256 224 256 224 224 256 224 224 224 288 256 256 224 224 224 224 500 600 500 300 500 600 300 300 600 300 300 300 300 300 300 300 600 300 300 500 450 300 300 300 600 450 300 500 300 300 300 300 72.0 73.3 73.8 74.1 74.7 75.2 75.7 75.8 77.1 77.4 78.6 78.8 76.0 77.7 78.1 78.5 79.1 79.1 79.4 79.9 80.4 77.9 78.4 79.4 80.0 80.4 80.4 80.6 80.7 80.9 81.0 81.2 81.9 Results with distillation on ImageNet-1K. We conducted rigorously fair training as the previous methods above. Recently, some works report enhanced performance leveraging more advanced training strategies. We investigate whether these training recipes can also improve iFormer. Following previous works (Li et al., 2023; Wang et al., 2024), we employ the RegNetY-16GF (Radosavovic et al., 2020) model with top-1 accuracy of 82.9% as the teacher model for distillation. Our findings reveal that iFormer improves obviously over its counterpart without distillation. For example, iFormer-L shows 1.0% increase under the same latency. iFormer also outperforms EfficientFormerV2-S2, despite the latter being trained with 1.5 longer schedule."
        },
        {
            "title": "4.2 OBJECT DETECTION AND INSTANCE SEGMENTATION",
            "content": "To validate the effectiveness of iFormer on downstream tasks, we train Mask R-CNN (He et al., 2017) with iFormer as the backbone for 12 epochs (1), using the MMDetection toolkit (Chen et al., 2019). We also report backbone latency measured at resolution of 512512 on an iPhone 13. The results are presented in Table 5. In comparison to lightweight models, iFormer-M surpasses FastViT-SA12 by +1.9%/+2.0% in APbox /APmask while running 1.32 faster. iFormer-L also obtains +0.1%/+0.6% in APbox /APmask than EfficientMod-S, which utilizes convolutional modulation mechanism to learn dynamics similar to self-attention. Notably, EfficientMod-S operates 3.7 slower when processing high-resolution input, underscoring that the proposed novel attention mechanism is more suitable for mobile networks. Meanwhile, when compared to general networks that are not optimized for mobile applications, iFormer demonstrates significant advantages. For instance, iFormer-L exceeds the performance of ConvNeXt-T with improvements of +1.2%/+1.4% in 8 Published as conference paper at ICLR 2025 Table 5: Object detection & instance segmentation results on MS COCO 2017 using Mask RCNN. Semantic segmentation results on ADE20K using the Semantic FPN framework. We measure all backbone latencies with image crops of 512512 on iPhone 13 by Core ML Tools. Failed indicated that the model runs too long to report latency by the Core ML."
        },
        {
            "title": "Backbone",
            "content": "Param (M) Latency (ms)"
        },
        {
            "title": "Semantic",
            "content": "APbox APbox 50 APbox"
        },
        {
            "title": "APmask APmask",
            "content": "50 APmask 75 mIoU EfficientNet-B0 (2019) ResNet18 (2016) PoolFormer-S12 (2022) EfficientFormer-L1 (2022b) FastViT-SA12 (2023a) RepViT-M1.1 (2024) iFormer-M ResNet50 (2016) PoolFormer-S24 (2022) ConvNeXt-T (Liu et al., 2022) EfficientFormer-L3 (2022b) RepViT-M1.5 (2024) PVTv2-B1 (2022) FastViT-SA24 (2023a) EfficientMod-S (2024) Swin-T (2021a) iFormer-L 5.3 11.7 11.9 12.3 10.9 8.2 8. 25.5 21.4 29.0 31.3 14.0 14.0 20.6 32.6 28.3 14.7 4.55 2.85 5.70 3.50 5.27 3.18 4.00 7.20 10.0 13.6 8.40 5.00 27.00 8.97 24.30 Failed 6.60 31.9 34.0 37.3 37.9 38.9 39.8 40.8 38.0 40.1 41.0 41.4 41.6 41.8 42.0 42.1 42.2 42.2 51.0 54.0 59.0 60.3 60.5 61.9 62. 58.6 62.2 62.1 63.9 63.2 64.3 63.5 63.6 64.4 64.2 34.5 36.7 40.1 41.0 42.2 43.5 44.8 41.4 43.4 45.3 44.7 45.3 45.9 45.8 45.9 46.2 46.0 29.4 31.2 34.6 35.4 35.9 37.2 37.9 34.4 37.0 37.7 38.1 38.6 38.8 38.0 38.5 39.1 39.1 47.9 51.0 55.8 57.3 57.6 58.8 59. 55.1 59.1 59.3 61.0 60.5 61.2 60.5 60.8 61.6 61.4 31.2 32.7 36.9 37.3 38.1 40.1 40.7 36.7 39.6 40.4 40.4 41.5 41.6 40.5 41.2 42.0 41.9 - 32.9 37.2 38.9 38.0 40.6 42.4 36.7 40.3 41.4 43.5 43.6 42.5 41.0 43.5 41.5 44.5 APbox /APmask, while requiring fewer parameters and only 50% mobile latency, suggesting iFormers efficient design in feature extraction and strong potential for mobile applications."
        },
        {
            "title": "4.3 SEMANTIC SEGMENTATION",
            "content": "We conduct experiments on the ADE20K (Zhou et al., 2017) using the Semantic FPN (Kirillov et al., 2019), based on the MMSegmentation toolkit (Contributors, 2020). Thanks to its efficient attention design, iFormer outperforms all competing methods in mIoU with similar and much lower latency. For example, iFormer-L surpasses FastViT-SA24 by +3.5% in mIoU with 1.36 faster inference speed. In addition, iFormer-M demonstrates superior mIoU compared to general networks, which typically exhibit substantially greater latency when processing higher-resolution inputs on mobile devices. Although PVTv2-B utilizes downsampled attention, it still requires 27 ms for latency. Similarly, Swin-T involves intensive operations in window partitioning, making it less suitable for mobile applications. Running at 6.6 ms, iFormer-L achieves +2.0% better mIoU than PVTv2-B1 and +3.0% better than Swin-T. These results suggest that the proposed attention mechanism offers significant benefits for tasks requiring the perception of fine-grained details."
        },
        {
            "title": "5 ABLATION STUDIES",
            "content": "Table 6: Activation function comparison in SHMA. PostBN indicates that BN is applied after projection. Pre-LN means that LN is implemented before the projection, as in standard MHA (Vaswani, 2017). Activation Function Here we explore whether an activation function without an upper bound can enhance the SHMA by allowing neurons to express arbitrarily large values. We compare the widely used Sigmoid Linear Unit (SiLU) (Shazeer, 2020) with the sigmoid function and present the results in Table 6. Directly replacing the activation function in SHMA with SiLU will encounter diverging loss during training. The underlying cause is primarily attributed to the element-wise multiplication of the unbounded context branch. To address this, we replace Post-BN in SHMA with Pre-LN, as LN adaptively normalizes each token feature. The modified model experiences slight decrease in accuracy but incurs an additional 0.07 ms latency, primarily brought by LN. The results suggest that the sigmoid function not only mitigates training instability but also facilitates better convergence. SiLU + Post-BN SiLU + Pre-LN Sigmoid + Post-BN Params (M) GMACs Latency (ms) Top-1 Acc. (%) Diverged 80.3 80.4 1.10ms 1.17ms 1.10ms"
        },
        {
            "title": "SHMA Setting",
            "content": "1.60 1.64 1.60 8.9 8.9 8.9 9 Published as conference paper at ICLR 2025 Choice of Conv v.s. ViT Blcoks In Section 3.3, we replace the convolutional blocks in Stages 3 and 4 with the proposed SHMA block. We provide further ablation studies on the choice of ratio for the ViT blocks. Specifically, We choose the model after enlarging the kernel size as starting point, then we progressively replace the convolutional blocks in Stages 3 and 4. We do not modify Stages 1 and 2 as their larger spatial dimensions would considerably increase the memory requirements for the self-attention mechanism."
        },
        {
            "title": "Ratio Setting",
            "content": "Params (M) GMACs Latency (ms) Top-1 Acc. (%) Table 7: Different ratio of ViT Block. Baseline Replacing 22% Conv Blocks in Stage 3 as SHA Replacing 22% Conv Blocks in Stage 3 as SHMA Replacing 50% Conv Blocks in Stage 3 as SHA Replacing 50% Conv Blocks in Stage 3 as SHMA Replacing 78% Conv Blocks in Stage 3 as SHA Replacing 78% Conv Blocks in Stage 3 as SHMA Replacing 100% Conv Blocks in Stage 3 as SHA Replacing 100% Conv Blocks in Stage 3 as SHMA 9.4M 9.1M 9.2M 8.8M 8.9M 8.3M 8.5M 7.9M 8.3M 1760M 1.0ms 1724M 1.02ms 1739M 1.04ms 1689M 1.04ms 1712M 1.07ms 1635M 1.12ms 1685M 1.17ms 1599M 1.17ms 1665M 1.25ms Replacing 100% Conv Blocks in Stage 3 as SHMA and 100% in Stage 10.0M 1792M 1.15ms 79.4 79.5 79.6 79.5 79.8 79.3 79.6 78.1 79.0 80.4 We present our findings in Table 7. Given that Stage 4 contains only two blocks, we do not conduct further splitting for the ratio. As illustrated in Table 7, although the ViT block has lower FLOPs, it still incurs increased runtime. Substituting all the convolutional blocks in Stage 3 results in the worst performance and the highest latency. Instead, by replacing half of the convolutional blocks in the third stage and all blocks in the final stage, we can better integrate these two operators, thus achieving favorable trade-off between accuracy and latency. Scaling to Larger Model Although iFormer is designed for mobile-device applications, the combination of fast local representation capacity of convolution and the efficient global modeling proficiency of the proposed SHMA enables its scalability for broader range of applications. To demonstrate the scalability of iFormer, we developed larger model named iFormer-H with 99M parameters and trained it for 300 epochs following the same strategy outlined in Section B. It is important to note that we add drop path and layer scale, which are commonly used in the training of larger models (Liu et al., 2022; Tu et al., 2022; Shi, 2024)."
        },
        {
            "title": "Model",
            "content": "Table 8: Scaling to the larger model with 99M parameters. We summarize the results in Table 8. highlight from the results is that iFormer is not specifically designed or trained for this scale. Despite this, iFormer-H outperforms ConvNeXt, achieving 1.0% increase in accuracy while maintaining similar number of FLOPs. Additionally, it demonstrates comparable performance to TransNeXt-Base, despite utilizing fewer FLOPs. These findings indicate the potential for broader applications of iFormer. We plan to explore larger models suitable for mobile devices in future work. Further ablation studies can be found in Sec. in the supplementary material. ConvNeXt-Base (2022) TransNeXt-Base (2024) iFormer-H (ours) MaxViT-Base (2022) Params (M) GMACs (G) Top-1 Acc. (%) 15.4 18.4 15.5 24.0 83.8 84.8 84.8 84. 89 90"
        },
        {
            "title": "6 CONCLUSION",
            "content": "This work proposes iFormer, which integrates highly optimized convolutional operations for the early layers alongside novel and efficient single-head modulation attention for the later layers. iFormer achieves SOTA Pareto-front in terms of Top-1 accuracy and mobile latency. We also validate the effectiveness of iFormer on downstream dense prediction tasks, including COCO object detection, instance segmentation, and ADE20K semantic segmentation. These inspiring results highlight the potential for mobile applications. We hope iFormer can facilitate the application of artificial intelligence on more mobile devices. In future work, we will seek to alleviate inference bottlenecks sociated with high-resolution images. Meanwhile, we plan to optimize iFormer for more hardware platforms, such as Android devices and NVIDIA Jetson Nano. 10 Published as conference paper at ICLR"
        },
        {
            "title": "REFERENCES",
            "content": "William Avery, Mustafa Munir, and Radu Marculescu. Scaling graph convolutions for mobile vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 58575865, 2024. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Han Cai, Junyan Li, Muyan Hu, Chuang Gan, and Song Han. Efficientvit: Lightweight multi-scale In Proceedings of the IEEE/CVF International attention for high-resolution dense prediction. Conference on Computer Vision, pp. 1730217313, 2023. Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, et al. Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019. Qiang Chen, Qiman Wu, Jian Wang, Qinghao Hu, Tao Hu, Errui Ding, Jian Cheng, and Jingdong In Proceedings of the Wang. Mixformer: Mixing features across windows and dimensions. IEEE/CVF conference on computer vision and pattern recognition, pp. 52495259, 2022a. Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, and Zicheng In Proceedings of the IEEE/CVF Liu. Mobile-former: Bridging mobilenet and transformer. conference on computer vision and pattern recognition, pp. 52705279, 2022b. Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, and Chunhua Shen. Conditional positional encodings for vision transformers. arXiv preprint arXiv:2102.10882, 2021. MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. https://github.com/open-mmlab/mmsegmentation, 2020. CoreML. In https://github.com/apple/coremltools. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1373313742, 2021. Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: general vision transformer backbone with cross-shaped windows. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1212412134, 2022. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, and Chang Xu. Cmt: Convolutional neural networks meet vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1217512185, 2022. Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu. Ghostnet: More features from cheap operations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 15801589, 2020. Zhiwei Hao, Jianyuan Guo, Ding Jia, Kai Han, Yehui Tang, Chao Zhang, Han Hu, and Yunhe Wang. Learning efficient vision transformers via fine-grained manifold distillation. arXiv preprint arXiv:2107.01378, 2021. 11 Published as conference paper at ICLR 2025 Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, Jose Alvarez, Jan Kautz, and Pavlo Molchanov. Fastervit: Fast vision transformers with hierarchical attention. arXiv preprint arXiv:2306.06189, 2023. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 29612969, 2017. Qibin Hou, Cheng-Ze Lu, Ming-Ming Cheng, and Jiashi Feng. Conv2former: simple transformerstyle convnet for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 13141324, 2019. Andrew Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. Sergey Ioffe. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dollar. Panoptic feature pyramid networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 63996408, 2019. Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Mvitv2: Improved multiscale vision transformers for classification and detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 48044814, 2022a. Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren. Efficientformer: Vision transformers at mobilenet speed. Advances in Neural Information Processing Systems, 35:1293412949, 2022b. Yanyu Li, Ju Hu, Yang Wen, Georgios Evangelidis, Kamyar Salahi, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Rethinking vision transformers for mobilenet size and speed. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1688916900, 2023. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1001210022, 2021a. Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. Post-training quantization for vision transformer. Advances in Neural Information Processing Systems, 34:28092 28103, 2021b. Zhenhua Liu, Zhiwei Hao, Kai Han, Yehui Tang, and Yunhe Wang. Ghostnetv3: Exploring the training strategies for compact models. arXiv preprint arXiv:2404.11202, 2024. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1197611986, 2022. Xiangyong Lu, Masanori Suganuma, and Takayuki Okatani. Sbcformer: Lightweight network capable of full-size imagenet classification at 1 fps on single board computers. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 11231133, 2024. Published as conference paper at ICLR 2025 Hailong Ma, Xin Xia, Xing Wang, Xuefeng Xiao, Jiashi Li, and Min Zheng. Mocovit: Mobile convolutional vision transformer. arXiv preprint arXiv:2205.12635, 2022. Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In Proceedings of the European conference on computer vision (ECCV), pp. 116131, 2018. Xu Ma, Xiyang Dai, Jianwei Yang, Bin Xiao, Yinpeng Chen, Yun Fu, and Lu Yuan. Efficient modulation for vision networks. arXiv preprint arXiv:2403.19963, 2024. Muhammad Maaz, Abdelrahman Shaker, Hisham Cholakkal, Salman Khan, Syed Waqas Zamir, Rao Muhammad Anwer, and Fahad Shahbaz Khan. Edgenext: efficiently amalgamated cnnIn European conference on computer transformer architecture for mobile vision applications. vision, pp. 320. Springer, 2022. Sachin Mehta and Mohammad Rastegari. Mobilevit: light-weight, general-purpose, and mobilefriendly vision transformer. arXiv preprint arXiv:2110.02178, 2021. Sachin Mehta and Mohammad Rastegari. Separable self-attention for mobile vision transformers. arXiv preprint arXiv:2206.02680, 2022. Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? Advances in neural information processing systems, 32, 2019. Moritz Nottebaum, Matteo Dunnhofer, and Christian Micheloni. Lowformer: Hardware efficient design for convolutional transformer backbones. arXiv preprint arXiv:2409.03460, 2024. Junting Pan, Adrian Bulat, Fuwen Tan, Xiatian Zhu, Lukasz Dudziak, Hongsheng Li, Georgios Tzimiropoulos, and Brais Martinez. Edgevits: Competing light-weight cnns on mobile devices with vision transformers. In European Conference on Computer Vision, pp. 294311. Springer, 2022. Danfeng Qin, Chas Leichner, Manolis Delakis, Marco Fornoni, Shixin Luo, Fan Yang, Weijun Wang, Colby Banbury, Chengxi Ye, Berkin Akin, et al. Mobilenetv4-universal models for the mobile ecosystem. arXiv preprint arXiv:2404.10518, 2024. Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollar. Designing In Proceedings of the IEEE/CVF conference on computer vision and network design spaces. pattern recognition, pp. 1042810436, 2020. Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic token sparsification. Advances in neural information processing systems, 34:1393713949, 2021. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 45104520, 2018. Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Swiftformer: Efficient additive attention for transformer-based real-time mobile vision applications. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1742517436, 2023. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Dai Shi. Transnext: Robust foveal visual perception for vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1777317783, 2024. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 19, 2015. Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pp. 61056114. PMLR, 2019. 13 Published as conference paper at ICLR 2025 Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Chao Xu, and Yunhe Wang. Ghostnetv2: Enhance cheap operation with long-range attention. Advances in Neural Information Processing Systems, 35:99699982, 2022. Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in neural information processing systems, 34:24261 24272, 2021. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pp. 1034710357. PMLR, 2021a. Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herve Jegou. Going deeper with image transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 3242, 2021b. Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxvit: Multi-axis vision transformer. In European conference on computer vision, pp. 459 479. Springer, 2022. Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. Fastvit: In Proceedings of the fast hybrid vision transformer using structural reparameterization. IEEE/CVF International Conference on Computer Vision, pp. 57855795, 2023a. Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. Mobileone: An improved one millisecond mobile backbone. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 79077917, 2023b. Ashish Vaswani. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017. Qiang Wan, Zilong Huang, Jiachen Lu, Gang Yu, and Li Zhang. Seaformer: Squeeze-enhanced axial transformer for mobile semantic segmentation. arXiv preprint arXiv:2301.13156, 2023. Ao Wang, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding. Repvit: Revisiting mobile cnn from vit perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1590915920, 2024. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 568578, 2021. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media, 8(3):415424, 2022. Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efficient convnet design In Proceedings of the IEEE/CVF conference on via differentiable neural architecture search. computer vision and pattern recognition, pp. 1073410742, 2019. Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollar, and Ross Girshick. Early convolutions help transformers see better. Advances in neural information processing systems, 34:3039230400, 2021. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in neural information processing systems, 34:1207712090, 2021. Jianwei Yang, Chunyuan Li, Xiyang Dai, and Jianfeng Gao. Focal modulation networks. Advances in Neural Information Processing Systems, 35:42034217, 2022a. Published as conference paper at ICLR 2025 Qiming Yang, Kai Zhang, Chaoxiang Lan, Zhi Yang, Zheyang Li, Wenming Tan, Jun Xiao, and Shiliang Pu. Unified normalization for accelerating and stabilizing transformers. In Proceedings of the 30th ACM International Conference on Multimedia, pp. 44454455, 2022b. Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1081910829, 2022. Seokju Yun and Youngmin Ro. Shvit: Single-head vision transformer with memory efficient macro design. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 57565767, 2024. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1210412113, 2022. Haokui Zhang, Wenze Hu, and Xiaoyu Wang. Edgeformer: Improving light-weight convnets by learning from vision transformers. arXiv preprint arXiv:2203.03952, 2, 2022. Jiangning Zhang, Xiangtai Li, Jian Li, Liang Liu, Zhucun Xue, Boshen Zhang, Zhengkai Jiang, Tianxin Huang, Yabiao Wang, and Chengjie Wang. Rethinking mobile block for efficient In 2023 IEEE/CVF International Conference on Computer Vision attention-based models. (ICCV), pp. 13891400. IEEE Computer Society, 2023. Tianfang Zhang, Lei Li, Yang Zhou, Wentao Liu, Chen Qian, and Xiangyang Ji. Cas-vit: Convolutional additive self-attention vision transformers for efficient mobile applications. arXiv preprint arXiv:2408.03703, 2024. Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient In Proceedings of the IEEE conference on convolutional neural network for mobile devices. computer vision and pattern recognition, pp. 68486856, 2018. Mingshu Zhao, Yi Luo, and Yong Ouyang. Repnext: fast multi-scale cnn using structural reparameterization. arXiv preprint arXiv:2406.16004, 2024. Youpeng Zhao, Huadong Tang, Yingying Jiang, Qiang Wu, et al. Lightweight vision transformer with cross feature attention. arXiv preprint arXiv:2207.07268, 2022. Chuanyang Zheng, Kai Zhang, Zhi Yang, Wenming Tan, Jun Xiao, Ye Ren, Shiliang Pu, et al. Savit: Structure-aware vision transformer pruning via collaborative optimization. Advances in Neural Information Processing Systems, 35:90109023, 2022. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 633641, 2017. Lei Zhu, Xinjiang Wang, Zhanghan Ke, Wayne Zhang, and Rynson WH Lau. Biformer: Vision transformer with bi-level routing attention. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1032310333, 2023. 15 Published as conference paper at ICLR"
        },
        {
            "title": "B EXPERIMENTAL SETTINGS",
            "content": "B."
        },
        {
            "title": "IMAGE CLASSIFICATION",
            "content": "Table 9: ImageNet-1K training settings. training config resolution weight init optimizer base learning rate weight decay optimizer momentum batch size training epochs learning rate schedule warmup epochs warmup schedule layer-wise lr decay randaugment mixup cutmix random erasing label smoothing iFormer-T/S/M/L/H 2242 trunc. normal (0.2) AdamW 4e-3 (T/S/M/L) 8e-3 0.05 β1, β2=0.9, 0.999 4096 [T/S/M/L] 8192 [H] 300 cosine decay 20 linear None (9, 0.5) 0.8 1.0 0.25 0.1 stochastic depth layer scale head init scale gradient clip exp. mov. avg. (EMA) 0.0 [T/S/M] 0.1 [L] 0.6 [H] None [T/S/M/L] 1e-6 [H] None None None We mainly follow the training recipe of ConvNeXt, while removing stochastic depth, layer scale, and exponential moving average to ensure fair comparison with prior works. The models are trained for 300 epochs on 8 NVIDIA GPUs with total batch size of 4096. We employ the same learning rate across all models. It is possible to further improve performance by adjusting the learning rates for different model variants, which we will explore in the future. For distillation, we use the RegNetY-16GF model as the teacher model and apply hard distillation loss, following the approach of DeiT (Touvron et al., 2021a). During inference, the average output of the classification head and the distillation head is used as the final output. B.2 OBJECT DETECTION AND SEMANTIC SEGMENTATION For object detection experiments, we train MaskR-CNN models on the COCO 2017 dataset for 12 epochs using standard training settings from the MMDetection toolkit. For semantic segmentation experiments, we train Semantic FPN models on the ADE20K dataset for 40,000 iterations using standard training settings from the MMSegmentation toolkit. The input images are cropped to resolution of 512512 during training. For backbone latency, we keep the same input size as training (i.e., 512512) and measure the mobile latency on an iPhone 13 compiled by Core ML Tools."
        },
        {
            "title": "C MORE ABLATION STUDIES",
            "content": "Different Ways for Reducing Latency Here we provide comparison of different methods for reducing latency, contrasting them with the approach discussed in Sec. 3.3. Specifically, we reduce the baseline latency to similar latency by directly removing blocks, cutting down FFN expansion width, 16 Published as conference paper at ICLR 2025 orchestrated and reducing both attention head dimension and FFN expansion dimension simultaneously. From the results in Table 10, we observe that the removal of single block in the final stage can lead to severe drop in accuracy (-0.7%), indicating that greater depth enhances the models capacity. Concurrently reducing all FFN expansion widths causes non-trivial performance degradation (-0.6%). In contrast, we observe that an reduction in both attention head and FFN expansion dimensions accuracy milder yields decline (-0.2%). These results demonstrate that comprehensive reduction across different components offers better flexibility and performance. Baseline Number of Blocks FFN Width Attn. Head and FFN Width Table 10: Different ways for reducing latency. Params (M) GMACs Latency (ms) Top-1 Acc. (%) 1.15 1.07 1.07 1.10 1.79 1.70 1.62 1.64 80.4 79.7 79.8 80.2 10.0 8.4 8.6 8."
        },
        {
            "title": "Reducing Setting",
            "content": "Depthwise Convlution in FFN Recent works (Cai et al., 2023; Qin et al., 2024) attempt to insert depthwise convolution (DW Conv) within the FFN to perform spatial mixing on the expanded features activations. We hypothesize that implementing more effective spatial mixing before the FFN diminishes its significance. In our iFormer, depthwise convolution with kernel size of 7 is employed for spatial modeling in the early layers, while powerful SHMA is utilized in the later layers. This approach provides significantly enhanced spatial mixing capacity than previous methods. As shown in Table 11, enhancing all FFN with depthwise convolution, including those within the convolutional blocks, results in +14% increase in FLOPs and an additional latency cost of 0.33 ms. This increase is expected since the intermediate layers in the FFN possess an expanded feature dimension. However, the Top-1 accuracy only exhibits marginal improvement of +0.1%. Table 11: Comparison of FFN with and without depthwise convolution. DW Conv in FFN Params (M) GMACs Latency (ms) Top-1 Acc. (%) with w/o. 80.5 80.4 1.43 1. 1.83 1.60 9.6 8.9 Training for Longer Schedule Another commonly used advanced training is an extended schedule (450 vs. 300). Here we provide additional experiments for both image classification and downstream tasks where we train iFormer with distillation for 450 epochs. To ensure fair comparison with previous methods, we develop larger model dubbed as iFormer-L2. We report the image Table 12: Training with distillation for 450 epochs on ImageNet-1K."
        },
        {
            "title": "Model",
            "content": "Params (M) Latency (ms) Reso. Epochs Top-1 (%) ConvNeXt-B (2022) EfficientFormerV2-L (2023) iFormer-L2 89.0 26.1 24.5 7.54 2.40 2.30 224 224 224 300 450 83.8 83.5 83.9 classification results on the ImageNet-1k dataset in Table 12. It shows that training iFormer-L2 for 450 epochs yields improved performance, obtaining Top-1 accuracy of 83.9%, even surpassing the ConvNeXt-Base model. Table 13: Object detection & Semantic segmentation results using backbone pretrained for 450 epochs."
        },
        {
            "title": "Backbone",
            "content": "Param (M) Latency (ms)"
        },
        {
            "title": "Semantic",
            "content": "APbox APbox 50 APbox"
        },
        {
            "title": "APmask APmask",
            "content": "50 APmask 75 mIoU ResNet50 (2016) PoolFormer-S24 (2022) ConvNeXt-T (Liu et al., 2022) EfficientFormer-L3 (2022b) RepViT-M1.5 (2024) PVTv2-B1 (2022) FastViT-SA24 (2023a) EfficientMod-S (2024) Swin-T (2021a) iFormer-L EfficientFormerV2-L (2023) iFormer-L2 25.5 21.4 29.0 31.3 14.0 14.0 20.6 32.6 28.3 14. 26.1 24.5 7.20 12.30 12.6 8.40 5.00 27.00 8.97 24.30 Failed 6.60 12.5 9.06 300 300 300 300 300 300 300 300 300 300 450 450 38.0 40.1 41.0 41.4 41.6 41.8 42.0 42.1 42.2 42. 44.7 44.6 58.6 62.2 62.1 63.9 63.2 64.3 63.5 63.6 64.4 64.2 66.3 66.7 41.4 43.4 45.3 44.7 45.3 45.9 45.8 45.9 46.2 46.0 48.8 49.1 34.4 37.0 37.7 38.1 38.6 38.8 38.0 38.5 39.1 39. 40.4 41.1 55.1 59.1 59.3 61.0 60.5 61.2 60.5 60.8 61.6 61.4 63.5 64.0 36.7 39.6 40.4 40.4 41.5 41.6 40.5 41.2 42.0 41.9 43.2 44.1 36.7 40.3 41.4 43.5 43.6 42.5 41.0 43.5 41.5 44. 45.2 46.2 17 Published as conference paper at ICLR 2025 4.67 . SHMA projects the input into higher dimension of 1 Figure 5: Comparison of SHMA and SHA in SHViT. In SHViT, rC channels are utilized for spatial attention, where is set to 1 2 (i.e., R=2) and avoids split and concatenation operations. Furthermore, we integrate iFormer-L2 into the Mask-RCNN and Semantic FPN framework for downstream tasks. As anticipated, the model with the more powerful iFormer-L2 backbone achieves SOTA performance, obtaining significant enhancement over models pretrained for 300 epochs. It also outperforms its EfficientFormerV2-L counterpart by +0.7% in APmask and +1.0% in mIoU, while being 1.4 faster. These experiments collectively show that advanced training strategies can be easily employed to improve the performance of iFormers."
        },
        {
            "title": "D RELATION TO SHVIT",
            "content": "We clarify the difference between SHA in iFormer and its counterpart in SHViT (Yun & Ro, 2024) from the following two aspects: First, in terms of motivation, iFormer explores efficient attention mechanisms specifically tailored for the on-device environment, whereas SHViT is geared towards general-purpose GPUs, which may exhibit different hardware characteristics. Second, in terms of methodology, as shown in Fig. 5, we utilize single head attention with more channels (R is set to 2.), while SHViT employs fewer than 1/4 of channels for attention. The reduced number of channels can result in lower rank of the attention matrix, potentially degrading its expressiveness. Additionally, the split and concatenate operations in SHViT introduce extra runtime. Table 14: Process of converting SHA in iFormer towards SHViT. Intermediate models are only measured by latency."
        },
        {
            "title": "Modification",
            "content": "Params(M) GMACs Latency (ms) Top-1(%) SHA Baseline without Modulation + split + attention on 1/4 channels + concat 9.9M 9.9M 8.3M 8.7M 1758M 1758M 1547M 1579M 1.12ms 1.18ms 1.02ms 1.11ms 79.4 - - 79.5 We also conduct more fair comparison with SHViT. We start from the SHA baseline referenced in Table 1, specifically denoted as SHA in Figure 2. The transition to SHViT involves the following steps: 1) splitting the input into two smaller tensors, X1 and X2, along the channel dimension; 2) applying single head attention to the tensor X1, which contains fewer than 1/4 of channels present in the original input tensor; and 3) concatenating the attention output with the residual input X2. As summarized in Table 14, split and concatenate operations introduce additional runtime. Furthermore, the performance of the SHA in the SHViT exhibits decline compared to its counterpart in iFormer under similar latency conditions (79.8 v.s. 79.5). This degraded performance may be attributed to the reduced number of channels in the attention mechanism."
        },
        {
            "title": "E ARCHITECTURE DETAILS",
            "content": "In Table 15, we show the different architecture configurations of the iFormer model variants."
        },
        {
            "title": "F IFORMER FOR HIGHER RESOLUTION",
            "content": "Self-attention exhibits quadratic complexity with respect to the number of tokens, i.e., the resolution of the input image. This issue is exacerbated in dense prediction tasks, which usually require high18 Published as conference paper at ICLR 2025 iFormer architecture configurations. BN stands for Batch Normalization. SHMA Table 15: stands for Singe-Head Modulation Attention. DW stands for Depthwise convolution. and means the stride and output dimension in convolution. hd denotes the head dimension in SHMA and the number of attention heads in all variants is 1. means the expansion ratio in FFN. Output Size (Downs. Rate) Stem 5656 (4) Stage 1 Stage 2 5656 (4) 2828 (8) Stage 1414 (16) iFormer-T iFormer-S iFormer-M iFormer-L (cid:2)Conv-BN-GELU 55 s2 d16(cid:3) 1 (cid:21) (cid:20)Conv-BN-GELU 55 s2 d64 Conv-BN 11 s1 1 (cid:2)Conv-BN-GELU 55 s2 d16(cid:3) 1 (cid:21) (cid:20)Conv-BN-GELU 55 s2 d64 Conv-BN 11 s1 d32 1 (cid:2)Conv-BN-GELU 55 s2 d24(cid:3) 1 (cid:21) (cid:20)Conv-BN-GELU 55 s2 d96 Conv-BN 11 s1 d48 1 (cid:2)Conv-BN-GELU 55 s2 d24(cid:3) 1 (cid:21) (cid:20)Conv-BN-GELU 55 s2 d96 Conv-BN 11 s1 1 2 Conv-BN 77 s1 d32 Conv-BN-GELU 11 s1 d96 Conv-BN 1x1 s1 d32 (cid:2)Conv-BN 33 s2 d64(cid:3) 1 Conv-BN 77 s1 d64 Conv-BN-GELU 11 s1 d192 Conv-BN 1x1 s1 d64 2 2 Conv-BN 77 s1 d32 Conv-BN-GELU 11 s1 d128 Conv-BN 1x1 s1 d32 (cid:2)Conv-BN 33 s2 d64(cid:3) 1 Conv-BN 77 s1 d64 Conv-BN-GELU 11 s1 d256 Conv-BN 1x1 s1 d64 2 (cid:2)Conv-BN 33 s2 d128(cid:3) 1 Conv-BN 77 s1 d128 Conv-BN-GELU 11 s1 d384 Conv-BN 11 s1 d128 6 (cid:2)Conv-BN 33 s2 d176(cid:3) 1 Conv-BN 77 s1 d176 Conv-BN-GELU 11 s1 d704 Conv-BN 1x1 s1 d176 9 CPE 33 SHMA hd64 FFN r2 CPE 33 SHMA hd88 FFN r3 3 Conv-BN 77 s1 d128 Conv-BN-GELU 11 s1 d384 Conv-BN 1x1 s1 d128 (cid:2)Conv-BN 33 s2 d256(cid:3) 1 Conv-BN 77 s1 d176 Conv-BN-GELU 11 s1 d704 Conv-BN 11 s1 d176 (cid:2)Conv-BN 33 s2 d320(cid:3) 1 1 Conv-BN 77 s1d48 Conv-BN-GELU 11 s1 d192 Conv-BN 1x1 s1 d48 (cid:2)Conv-BN 33 s2 d96(cid:3) 1 Conv-BN 77 s1 d96 Conv-BN-GELU 11 s1 d384 Conv-BN 1x1 s1 d96 2 2 2 Conv-BN 77 s1 d48 Conv-BN-GELU 11 s1 d192 Conv-BN 1x1 s1 d48 (cid:2)Conv-BN 33 s2 d96(cid:3) 1 Conv-BN 77 s1 d96 Conv-BN-GELU 11 s1 d384 Conv-BN 1x1 s1 d96 2 (cid:2)Conv-BN 33 s2 d192(cid:3) 1 Conv-BN 77 s1 d192 Conv-BN-GELU 11 s1 d768 Conv-BN 1x1 s1 d192 9 CPE 33 SHMA hd96 FFN 4 Conv-BN 77 s1 d192 Conv-BN-GELU 11 s1 d768 Conv-BN 11 s1 d192 (cid:2)Conv-BN 33 s2 d384(cid:3) 1 1 (cid:2)Conv-BN 33 s2 d256(cid:3) 1 Conv-BN 77 s1 d256 Conv-BN-GELU 11 s1 d1024 Conv-BN 1x1 s1 d256 8 CPE 33 SHMA hd128 FFN r3 8 Conv-BN 77 s1 d256 Conv-BN-GELU 11 s1 d1024 Conv-BN 11 s1 d256 (cid:2)Conv-BN 33 s2 d384(cid:3) 1 1 Stage 4 77 (32) CPE 33 SHMA hd64 FFN 2 CPE 33 SHMA hd80 FFN 2 CPE 33 SHMA hd96 FFN r3 CPE 33 SHMA hd96 FFN r3 2 Params (M) GMacs 2.9 0.53 6.5 1.09 8. 1.64 14.7 2.63 Table 16: Comparison of different attention designs in iFormer-M. For the sake of simplicity, we exclude other blocks that are not related to attention. ws is the window size for window attention."
        },
        {
            "title": "Chunk Hybrid SHMA",
            "content": "CPE 33 Window Partitioning, ws16 Window SHMA hd96, ws16 FFN r3 CPE 33 Chunk Window Partitioning, ws16 Window SHMA hd96, ws16 FFN r3 1 CPE 33 Window SHMA hd96, ws16 FFN r3 CPE 33 SHMA hd96 FFN r3 4 CPE 33 Window SHMA hd96, ws16 FFN r3 2 CPE 33 Window Reversing, ws16 SHMA hd96 FFN r3 1 CPE 33 Chunk Window Reversing, ws16 SHMA hd96 FFN r3 1 CPE 33 Window Partitioning, ws16 Window SHMA hd96 FFN 1 CPE 33 Chunk Window Partitioning, ws16 Window SHMA hd96 FFN r3 1 CPE 33 Window Reversing, ws16 SHMA hd64 FFN r3 CPE 33 Chunk Window Reversing, ws16 SHMA hd64 FFN r3 1 CPE 33 SHMA hd64 FFN 2 Stage 3 1414 (16) Stage 4 77 (32) resolution input such as 512512 in semantic segmentation and generate large amount of 1024 image tokens even in the third stage. Consequently, this will cause huge memory and computation costs in mobile devices. Table 17: Latency comparison of different attention mechanisms. To mitigate these issues, we resort to window attention as proposed in Swin (Liu et al., 2021a). However, default window attention only performs local self-attention within windows, thus lacking interactions between tokens from different windows which will impair modeling capacity. Swin introduces shifted window attention to alleviate this limitation. Unfortunately, the shifting operation inevitably incurs additional memory costs. In contrast to Swin, we implement"
        },
        {
            "title": "1.10\nFailed\n11.46\n4.0",
            "content": "224 512 512 512 Latency (ms)"
        },
        {
            "title": "Attention",
            "content": "19 Published as conference paper at ICLR 2025 hybrid attention design. Specifically, we compute window attention within windows, except for the last attention block in each stage. This approach enables iFormer to capture more global features essential for dense prediction tasks. At the same time, since window partitioning and reversing also incur memory access costs, we minimize the usage of them to once per stage. We replace the standard SHMA in iFormer with hybrid window SHMA, as shown in Table 16. From the latency comparison in Table 17, we see that simply applying SHMA will encounter memory bottleneck on mobile devices. Instead, our hybrid SHMA can significantly reduce memory access costs, achieving mobile latency of 11.46 ms. However, hybrid SHMA still lags much behind the recent FastViT-SA12, which has latency of 5.27 ms. We identify the speed bottleneck as stemming from the window partitioning and reversing operations, even though we only implement them once in each stage. As the feature map size increases, the reshaping involved in these operations demands considerable memory, thereby slowing inference in resource-constrained mobile devices. To address this issue, we propose method called Channel Chunking (CC). Formally, given 2D input feature map RCHW , the standard window partitioning divides the feature map into non-overlapped regions, each corresponding to window that contains feature vectors. This step is accomplished by reshaping as xP HW 2 CP . Then we apply SHMA within each window. To reduce the memory requirements associated with reshaping, we propose to split the feature map along the channel dimension into series of smaller chunks as follows: 1, ..., xS xS = Chunking(x), (4) where is the chunk size, = is the number of chunks. We set n=16 for the input image of 512512 in our object detection and semantic segmentation experiments. Then we apply window partitioning sequentially to these smaller chunks and concatenate them. This process can be mathematically expressed as follows: xP = Concat(xP where xP ), = WindowPartitioning(xS , ..., xP ), (5) These smaller chunks can be processed rapidly. As shown in Table 17, the chunking strategy allows the model to achieve 2.9 speed up in inference speed. Correspondingly, the window reversing operation is performed by reshaping multiple windows xP HW 2 CP into 2D feature map RCHW . These results demonstrate that our proposed Channel Chunking Hybrid SHMA significantly enhances the iFormers ability to process high-resolution images efficiently. Computation Complexity Given an input RCHW and window size of P, as detailed in Section E, the computational complexity of iFormer is as follows: Ω(SHMA) =4HW 2(QKV and output projection)+ HW C(element-wise product of modulation)+ 2P 2HW C(self-attention), Ω(FFN) = 8HW 2. (6) (7) In image classification, we do not utilize window attention since the feature size is 14 14 in stage 3 (it equals to the window attention when P=14). In downstream tasks, we adopt window size of P=16."
        },
        {
            "title": "G COMPREHENSIVE COMPARISON",
            "content": "In Table 18, we provide more comprehensive comparison between iFormer and other lightweight models on ImageNet-1k classification. 20 Published as conference paper at ICLR 2025 Table 18: Comprehensive comparison between iFormer and the previously proposed models on ImageNet-1K. Failed indicated that the model runs too long to report latency by the Core ML, often caused by excessive memory access."
        },
        {
            "title": "Model",
            "content": "Params (M) GMACs Latency (ms) Reso."
        },
        {
            "title": "Epochs",
            "content": "Top-1 (%) MobileNetV2 1.0x (2018) SHViT-S1 (2024) MobileNetV3-Large 0.75x (2019) MNV4-Conv-S (2024) iFormer-T ShuffleNetV2 1.0 (2018) MobileNetV2 1.4x (2018) MobileNetV3-Large 1.0x (2019) SwiftFormer-XS (2023) SBCFormer-XS (2024) GhostNetV3 1.0x (2024) EfficientNet-B0 (2019) MobileOne-S2 (2023b) LowFormer-B0 (2024) CAS-ViT-XS (2024) EMO-5M (2023) RepViT-M1.0 (2024) iFormer-S ShuffleNetV2 1.5 (2018) EdgeViT-XXS (2022) SHViT-S2 (2024) EfficientMod-xxs (2024) SBCFormer-S (2024) MobileOne-S3 (2023b) SwiftFormer-S (2023) GhostNetV3 1.3x (2024) EfficientNet-B1 (2019) FastViT-T12 (2023a) RepViT-M1.1 (2024) RepNeXt-M3 (2024) FastViT-S12 (2023a) MNV4-Conv-M (2024) iFormer-M MobileViT-XXS (2021) MobileViTV2-0.5 (2022) ShuffleNet v2 2.0 (2018) EdgeViT-XS (2022) Mobile-Former-294M (2022b) MobileViTV2-1.0 (2022) EfficientMod-xs (2024) MobileViT-S (2021) CMT-Ti (2022) Mobile-Former-508M (2022b) SHViT-S4 (2024) EfficientViT-B1-r224 (2023) MobileOne-S4 (2023b) LowFormer-B1 (2024) SBCFormer-B (2024) EfficientNet-B2 (2019) CAS-ViT-S (2024) GhostNetV3 1.6x (2024) EfficientViT-B1-r288 (2023) FastViT-SA12 (2023a) MNV4-Hybrid-M (2024) SwiftFormer-L1 (2023) EfficientMod-s (2024) SBCFormer-L (2024) RepViT-M1.5 (2024) LowFormer-B1.5 (2024) RepNeXt-M4 (2024) CAS-ViT-M (2024) iFormer-L 3.4 6.3 4.0 3.8 2. 2.3 6.9 5.4 3.5 5.6 6.1 5.3 7.8 14.1 3.2 5.1 6.8 6.5 3.5 4.1 11.4 4.7 8.5 10.1 6.1 8.9 7.8 6.8 8.2 7.8 8.8 9.2 8.9 1.3 1.4 7.4 6.7 11.4 4.9 6.6 5.6 11.3 14 16.5 9.1 14.8 17.9 13.8 9.2 5.8 12.3 9.1 10.9 10.5 12.1 12.9 18.5 14.0 33.9 13.3 12.4 14.7 0.73 0.74 0.67 0.60 0.60 0.74 1.02 0.76 0.95 0.79 0.99 0.89 0.92 1.45 0.85 Failed 0.85 0.85 1.16 1.41 1.10 1.29 1.02 1.16 1.12 1.24 1.29 1.12 1.04 1.04 1.26 1.08 1. 2.12 9.47 1.94 1.79 2.66 Failed 2.13 3.55 Failed 3.33 1.48 2.38 1.74 1.90 1.44 1.69 1.82 1.49 3.87 1.50 1.75 1.60 2.57 1.89 1.64 3.02 1.47 2.46 1.60 224 224 224 224 224 224 224 224 224 224 224 224 224 224 224 224 224 224 224 224 224 224 224 224 224 224 240 256 224 224 256 256 224 256 256 224 224 224 256 224 256 160 224 224 224 224 224 224 260 224 224 288 256 256 224 224 224 224 224 224 224 224 500 300 600 500 300 500 600 300 300 600 350 300 300 300 300 300 300 300 300 300 300 300 300 300 600 350 300 300 300 300 500 300 300 300 300 300 450 300 300 300 300 450 300 350 300 300 300 350 300 600 450 300 500 300 300 300 300 300 300 300 300 72.0 72.8 73.3 73.8 74.1 69.4 74.7 75.2 75.7 75.8 77.1 77.1 77.4 78.4 77.5 78.4 78.6 78.8 72.6 74.4 75.2 76.0 77.7 78.1 78.5 79.1 79.1 79.1 79.4 79.4 79.8 79.9 80. 69.0 70.2 74.9 77.5 77.9 78.1 78.3 78.4 79.2 79.3 79.4 79.4 79.4 79.9 80.0 80.1 80.2 80.4 80.4 80.6 80.7 80.9 81.0 81.1 81.2 81.2 81.2 81.4 81.7 0.30 0.24 0.16 0.20 0.53 0.15 0.59 0.22 0.60 0.70 0.17 0.39 1.30 0.94 0.56 0.90 1.10 1.09 0.30 0.60 0.37 0.60 0.90 1.90 1.00 0.27 0.70 1.40 1.30 1.30 1.80 1.00 1.64 0.40 0.50 0.59 1.10 0.29 1.80 0.80 2.00 687 0.51 0.99 0.52 2.98 1.41 1.60 1.00 0.93 0.40 0.86 1.90 1.20 1.60 1.40 2.70 2.30 2.57 2.30 1.89 2."
        }
    ],
    "affiliations": []
}