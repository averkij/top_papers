{
    "paper_title": "Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation",
    "authors": [
        "Xin Yan",
        "Yuxuan Cai",
        "Qiuyue Wang",
        "Yuan Zhou",
        "Wenhao Huang",
        "Huan Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Presto, a novel video diffusion model designed to generate 15-second videos with long-range coherence and rich content. Extending video generation methods to maintain scenario diversity over long durations presents significant challenges. To address this, we propose a Segmented Cross-Attention (SCA) strategy, which splits hidden states into segments along the temporal dimension, allowing each segment to cross-attend to a corresponding sub-caption. SCA requires no additional parameters, enabling seamless incorporation into current DiT-based architectures. To facilitate high-quality long video generation, we build the LongTake-HD dataset, consisting of 261k content-rich videos with scenario coherence, annotated with an overall video caption and five progressive sub-captions. Experiments show that our Presto achieves 78.5% on the VBench Semantic Score and 100% on the Dynamic Degree, outperforming existing state-of-the-art video generation methods. This demonstrates that our proposed Presto significantly enhances content richness, maintains long-range coherence, and captures intricate textual details. More details are displayed on our project page: https://presto-video.github.io/."
        },
        {
            "title": "Start",
            "content": "Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation Xin Yan* Yuxuan Cai Qiuyue Wang Yuan Zhou Wenhao Huang Huan Yang 01.AI 4 2 0 2 2 ] . [ 1 6 1 3 1 0 . 2 1 4 2 : r Figure 1. Presto is capable of generating long videos with rich content and long-range coherence."
        },
        {
            "title": "Abstract",
            "content": "tails. More details are displayed on our project page: presto-video.github.io. We introduce Presto, novel video diffusion model designed to generate 15-second videos with long-range coherence and rich content. Extending video generation methods to maintain scenario diversity over long durations presents significant challenges. To address this, we propose Segmented Cross-Attention (SCA) strategy, which splits hidden states into segments along the temporal dimension, allowing each segment to cross-attend to corresponding sub-caption. SCA requires no additional parameters, enabling seamless incorporation into current DiT-based architectures. To facilitate high-quality long video generation, we build the LongTake-HD dataset, consisting of 261k content-rich videos with scenario coherence, annotated with an overall video caption and five progressive sub-captions. Experiments show that our Presto achieves 78.5% on the VBench Semantic Score and 100% on the Dynamic Degree, outperforming existing state-of-the-art video generation methods. This demonstrates that our proposed Presto significantly enhances content richness, maintains long-range coherence, and captures intricate textual de- *Work done while interning at 01.AI. Correspondence to: hyang@fastmail.com. 1. Introduction Video diffusion models [9, 25, 29, 33, 36] have shown an impressive ability to generate high-quality videos based on single text prompt [17, 27]. However, most current approaches primarily focus on generating short video clips ranging from 3 to 8 seconds, limiting the expressiveness and richness of the resulting content. To generate longer videos, early approaches incorporate an additional interpolation or extrapolation phase to extend short clips, using techniques like noise scheduling [12, 20, 34] or attention manipulation [15, 32]. While these methods work well for generating minute-long videos, they struggle to extend beyond the scene content, as they are constrained by the limited capacity of the original short clips. An alternative approach adopts more direct strategy, typically by adding new modules to extend the video length in an auto-regressive manner [8, 37, 44]. However, this introduces the challenge of error propagation. Unlike short clips, long videos require balance between content diversity and long-range coherence, posing signif1 icant challenges for current video generation methods. To generate long videos with rich content, we recognize the importance of expanding the text input by incorporating multiple texts, as demonstrated in previous approaches [4, 18]. While combining videos generated from each individual text can enhance content diversity, it often leads to abrupt transitions between different scenarios. One alternative is to simultaneously incorporate multiple texts into the video generation model. This method provides the model with broader range of textual inputs, enabling the generation of more content-rich videos, in contrast to the traditional approach of using single text input, which limits the available information. By modeling the texts concurrently, this approach helps ensure long-term coherence in the generated videos, providing seamless viewing experience. Existing long video generation methods overlook the importance of high-quality data [4, 8], leading to low consistency and content diversity in generated videos. To implement model capable of generating content-rich and coherent long videos, large-scale, high-quality video dataset is crucial. This dataset must include content-rich, long videos with long-range coherence, along with multiple distinct yet coherent textual descriptions for each video. To this end, we develop systematic data curation pipeline to collect content-rich video-prompt pairs from public datasets, contributing to the creation of our LongTake-HD dataset. We filter 261k single-scene video clips from 8.9M publicly accessible videos as the pre-training set. Then, we apply an additional set of meticulous filtering steps to select the finest instances, resulting in fine-tuning dataset of 47k clips. This refined dataset ensures that our method can generate high-quality, extended-duration, and content-rich videos. To tackle the challenge of long videos, we propose Presto, novel method capable of generating 15-second videos with rich content and long-range coherence, as shown in Fig. 1. Instead of using single long caption for each video, we divide the visual content into segments and generate progressive sub-captions that align with the unfolding storyline. Next, we modify the cross-attention mechanism in the Diffusion Transformer (DiT). To adapt the DiT model for long video generation, we refine the text embedding process and the cross-attention mechanism to effectively handle multiple progressive text conditions alongside temporal information. In particular, we introduce Segmented Cross-Attention (SCA), which divides the hidden states into segments along the temporal dimension and cross-attends each segment with its corresponding sub-caption. SCA introduces no additional parameters or modules and can be seamlessly integrated into existing DiT-based methods with minimal fine-tuning. We explore three distinct SCA strategies to manage the interaction between text embeddings and segmented latent features: Isolate Segmented Cross-Attention (ISCA), Sequential Segmented Cross-Attention (SSCA), and Overlap Segmented Cross-Attention (OSCA). Our experiments demonstrate that OSCA enhances the content richness and longrange coherence in the generated long videos. Experimental results demonstrate the effectiveness of our methods. Presto achieves 78.5% score on the VBench Semantic Score, outperforming both the leading opensource model, Allegro, and the commercial system, Gen3. Notably, Presto achieves perfect 100% score on the Dynamic Degree metrics, showcasing its outstanding ability to capture dynamics and transitions. The quantitative results highlight our approachs strength in capturing intricate textual details and generating videos with rich content. Furthermore, our user study indicates that Presto excels in scenario diversity, scenario coherence, and textvideo alignment when compared to various open-source and commercial-level works. The key contributions of our work are outlined as follows: We propose large-scale video dataset LongTake-HD, with 261k high-quality cases curated from publicly sourced videos, characterized by long-duration, contentrich, and long-range coherent videos, and each paired with progressive sub-captions. We proposed Presto for long video generation with simple yet effective Segmented Cross-Attention strategy which extends the DiT architecture to accommodate multiple text prompts, enabling the generation of videos with rich content and long-range coherence. 2. Related Work Long-Video Generation is challenging task in video generation, requiring videos to be both rich and coherent. One paradigm [12, 15, 20, 32, 34] is to modify noise scheduling or attention mechanisms during the inference stage, which are highly constrained by the original video clips, often resulting in limited content diversity. Another approach [8, 37, 44] directly generates long videos, by introducing additional modules with auto-regressive generation, requiring substantial training resources. Multiple-Text-to-Video Generation (MT2V) [4, 18, 22] aligns with our work, as we also incorporate multiple text prompts. While MT2V aims to create videos from multiple inputs, our model uniquely adheres to the traditional T2V framework by requiring only single user prompt. Additionally, MT2V methods such as TALC [4], typically combine multi-scene captions rigidly (e.g., panda is running in the park, sunny. and golden retriever is running in the park, autumn.), while our progressive caption method eliminates redundant descriptions across sub-captions and focuses on scenario transitions. Our multiple sub-captions can coalesce into continuous narrative, improving the coherence of the video content. 2 Video Generation Datasets are crucial for pre-training high-quality video generation models. Existing text-video datasets [3] have made substantial progress in terms of dataset size, such as Panda-70M [5], HD-VILA [42], and HD-VG [35], which contain 70M, 100M, and 130M video clips respectively. Recent works such as OpenVid1M [16] and FlintstonesHD [44] have attempted to construct small, yet higher-quality datasets. In contrast, we propose LongTake-HD, focusing on the finest quality videos with rich content and long-range scenario coherence, as well as multiple progressive sub-captions per video. 3. LongTake-HD Dataset Data curation plays crucial role in training our proposed model. To generate content-rich videos with long-range coherence, it is essential to have both high-quality video content and detailed, descriptive captions. Beginning with dataset of 8.9 million publicly available raw videos, we filtered it down to single-scene video clips that exhibit diverse content, with resolution of 7201280, minimum duration of 15 seconds, and high aesthetic quality. This process yielded 261k instances, each comprising content-rich video paired with five coherent and progressively structured sub-captions, as well as an overall video caption. To further enable dataset stratification for different training stages, we utilized the full set of 261k instances during the pre-training phase and applied rigorous filtering criteria to extract the finest quality 47k instances for the fine-tuning set. We refer to this curated dataset as LongTake-HD. Further details and specific thresholds are shown in Sec. and Tab. 4 of Appendix. 3.1. Collecting Content-Diverse Video Clips Existing publicly available datasets, such as HD-VILA100M [42], HD-VG-130M [35], Panda 70M [5], and WebVid-10M [3], offer vast array of diverse and comprehensive video data that reflect the natural distribution of real-world content. However, these raw datasets often contain substantial amounts of noisy and low-quality material, lacking in careful curation for content quality and caption coherence. Inadequate data curation processes can lead to the inclusion of noisy, disjointed, or irrelevant video data, which negatively impacts the training of models, particularly for long-form videos. Starting from dataset of 8.9 million publicly accessible raw videos, we apply video data filtering pipeline including: (1) duration, speed, and resolution filtering; (2) scene segmentation; (3) low-level metrics filtering; and (4) aesthetic and motion contents filtering. Duration, speed, and resolution. We exclude videos shorter than 15 seconds, to ensure an adequate video length. To maintain smoothness, we filter out videos with frame rate lower than 23 FPS. We also remove videos with resolutions below 720p to preserve the visual quality of the dataset. Additionally, videos with aspect ratios less than 1 (i.e., vertical videos) are excluded to ensure consistency throughout the dataset. Scene segmentation. We detect the scene cuts in videos and filter out those with abrupt transitions using PySceneDetect [19]. To further strengthen the process and remove any lingering cuts or transitions, we manually discard the first and last 10 frames of each clip. After completing the scene segmentation, the remaining data filtering is applied to the individual single-scene video clips. Low-Level metrics. We use brightness and artifacts as key metrics for low-level filtering. We compute the average grayscale value of video frames and remove those that are overly dark or bright. Detection tools [2, 40] are applied to identify artifacts, such as watermarks and text, that are unrelated to the actual video content. Aesthetics and motion contents. We employ the LAION Aesthetics Predictor [28] to evaluate the aesthetic quality of video frames and remove those with low aesthetics scores. Optical flow is calculated using Unimotion [41], and clips with higher flow scores are retained to ensure substantial level of motion amplitude. Furthermore, we compute the coefficient of variation for all optical flow values within each clip, defined as the ratio of the standard deviation to the mean [7]. This standardized measure of dispersion allows us to assess the smoothness and consistency of motion dynamics, helping to avoid abrupt shifts in motion intensity. 3.2. Obtaining Coherent Video Captions We apply captioning techniques to both images and videos. First, we conduct image-level diversity filtering based on the sampled keyframes. Next, we generate captions for each keyframe and perform semantic filtering on these captions. Finally, we leverage Large Language Models (LLMs) to create multiple progressive sub-captions that include camera motions. Diversity filtering for keyframes. We employ comprehensive approach to evaluate the diversity and coherence of images, using combination of low-level and semantic metrics. Specifically, we apply the Peak Signal-to-Noise Ratio (PSNR) [11] for pixel-wise filtering, the Structural Similarity Index Measure (SSIM) [38] for structure-wise filtering, and the Perceptual Similarity (LPIPS) [46] for semanticwise filtering. Additionally, some sampled frames may contain minimal information, such as black screens or blurry images. To address this, we use the image file size as filtering criterion, since frames with low information content typically result in smaller file sizes when compressed in the PNG format [6, 10, 49, 50]. Semantic filtering from captions. We utilize Large Vision-Language Models (LVLMs) as caption generators to create detailed descriptions for both the entire video and 3 its sampled frames. The captions for individual frames offer in-depth descriptions of the visual elements in each keyframe and represent the corresponding short video segments, while the video caption emphasizes the dynamics and transitions across the video, incorporating both spatial and temporal details. To generate embeddings for all keyframe captions, we employ MPNet [31] from SentenceTransformers [23, 24] and compute the cosine similarity [30] between each pair of captions. Additionally, we filter out negative captions, which occur when LVLMs fail to generate responses for frames that contain sensitive or abstract content. Progressive sub-captions generation strategy. We propose progressive sub-captions generation approach to create coherent and non-redundant sub-captions that align with the video storyline. We show simple example of three generated progressive sub-captions below: sub-caption 1: close-up shot of the ground, focused on small, slightly elevated, textured mound of soil. single ant emerges from tiny opening at the top of the mound, its tiny antennae gently probing the air as it moves cautiously forward. sub-caption 2: The camera gradually pulls back, maintaining focus on the ant as it traverses the surface of the mound. sub-caption 3: Continuing to pull back, the ant diminishes in prominence and focus transitions to reveal larger view of the immediate ground area. As in the example above, the first sub-caption describes the main subject, ant, and the environment, soil. When it comes to the second and third sub-captions, it continues the previous story and elaborates further on the transitions, e.g. traverses the surface of the mound, and larger view of the immediate ground area. This narrative-style subcaption annotation strategy helps enhance the long-range coherence of the generated videos, and distinguishes our approach from existing video datasets and MT2V methods. For given long video, we first divide the video clip into segments and generate independent captions for each segment using our captioning model. We also create an overall description of the entire video, capturing both spatial and temporal details. Next, we refine each sub-caption using causal approach. We employ Large Language Model (LLM) to adjust each sub-caption, taking into account all previous sub-captions as well as the overall video description, ensuring that each sub-caption represents distinct episode within the broader storyline. Additionally, we explicitly incorporate camera motion into each sub-caption to enable fine-grained control over the camera. This strategy results in set of coherent and progressively linked subcaptions for diffusion model training. During inference, when given short prompt from the user, we again use the LLM as director to generate scripts with consistent and detailed descriptions. For captioning both the full video and selected frames, we use Aria [13] as our captioning model, and GPT-4o [1] as the LLM for refining the sub-captions. The detailed prompt templates for these two models are provided in Listing 1 and Listing 2 of Appendix. 4. Method 4.1. Overview We provide brief overview of latent diffusion models (LDMs) for text-to-video generation. LDMs extend traditional transformer models to handle the generative task and conduct the diffusion process in the latent space. First, pre-trained autoencoder is utilized to compress the raw image or videos from pixel to latent space and text encoder takes the text input and creates text embeddings. diffusion transformer (DiT) takes the visual input with noise and performs denoising process during training. Specifically, as shown in Fig. 2(a), the diffusion transformer consists of stack of self-attention and cross-attention blocks, which capture the spatial and temporal dependencies within the video as long as text embeddings condition. During inference, the diffusion transformer starts from an instance sampled from random Gaussian noise and applies the diffusion process iteratively across multiple timesteps, refining the output at each step. To adapt the DiT model for long video generation, we modify the text embedding process and the cross-attention mechanism to effectively incorporate multiple progressive text conditions with temporal information. Specifically, we split the latent features into segments along temporal dimensions in the cross-attention mechanism and study three different strategies to implement the interaction between text embedding and segmented latent features. The proposed Segmented Cross-Attention (SCA), especially the overlap variant, improves the content richness and longrange coherence in generated long videos by large margin. We study this proposed strategy and different variants in the next subsection. 4.2. Segmented Cross-Attention standard paradigm of text-to-video generation relies on single text prompt input, which is typically encoded into fixed-sized embedding RLD via text encoder. Text embeddings that exceed this size are truncated. This limitation can lead to severe information loss in long video generation, considering the length of our progressive subcaptions. Moreover, single long text embedding presents challenges in capturing intricate details, as latent representations within hidden states struggle to effectively capture the intricate details of lengthy text embedding through cross-attention mechanisms. Inspired by window attention[14], which limits the scope of attention to local regions, we propose the Segmented Cross-Attention (SCA) method. This method splits the hid4 Figure 2. (a) The overall architecture of our Presto, which integrates multiple text inputs concurrently. (b) The Segmented Cross-Attention strategy has three variants: 1) Isolated Segmented Cross-Attention (ISCA) directly splits the hidden states along the temporal dimension. The output is concatenated by multiple segments output. 2) Sequential Segmented Cross-Attention (SSCA) where each segment will see all the previous text conditions. All the overlapped regions are averaged and concatenated with other regions. 3) Overlap Segmented CrossAttention (OSCA) that is adopted in our method. Only frames at the segment boundary will cross-attend with multiple text conditions. den states into temporal-local segments to better interact with the progressive sub-captions via cross-attention. For each group of progressive sub-captions, we separately encode sub-captions with text encoder, and thus obtain i=1 RLD. Given group of text embeddings {ci}N the hidden state with frames in temporal dimension, we also evenly split into non-overlapped segments {zi}N i=1 along the time dimension, aligning in quantity with the group of sub-captions. Thus, segment zi encapsulates the frame information ranging from /N to (i + 1) /N . The core idea for our SCA is to constrain each segment zi to only access its corresponding text condition. We study three strategies for segmented attention computing including: 1) Isolate Segmented Cross-Attention (ISCA), 2) Sequential Segmented Cross-Attention (SSCA), and 3) Overlap Segmented Cross-Attention (OSCA), as shown in Fig. 2(b). For Isolate Segmented Cross-Attention (ISCA) shown in the first row of Fig. 2(b), we treat it as the basic setting of our SCA, in which each segment zi only cross-attends to its corresponding text condition ci in cross-attention layers. Due to the lack of internal interactions among segments, ISCA tends to generate video with rich content but lacks long-range coherence. Another strategy to improve temporal coherence is to enable long-range interactions between latent features and text embeddings, as shown in the second row of Fig. 2(b). We refer this variant to Sequential Segmented Cross-Attention (SSCA), which concatenates the latent segment zi sequentially with its latter segments {zj}N j>i and takes the average of all attention outputs in the final. Such strategy greatly improves the long-range coherence. However, the content richness drops because the sequential interactive introduces more information to each segment and blends its content diversity. Finally, we adopt simple yet effective strategy that performs feature fusion on adjacent segments, which is Overlap Segmented Cross-Attention (OSCA), as shown in the third row of Fig. 2(b). We relax the non-overlapping segment method above into the overlapping one by introducing δ < [T /N ] overlapping frames for each segment zi. Through overlapping, frames at the boundary of two adjacent segments will attend to multiple text conditions. The cross-attention outputs in the overlapped regions are then averaged, promoting smoother transitions between segments. OSCA allows each segment to crossattend to its relevant text embeddings, while self-attention facilitates global information exchange across segments, ensuring overall consistency. This interplay between local and global interactions helps Presto effectively capture the storyline and ensures content diversity and scenario coherence in long-form video generation. 4.3. Implementation Our work is built upon Allegro[48], an open-source video diffusion model with 2.8B parameters. Allegro generates high-quality videos up to 88 frames and 720p resolution from simple text input. Text inputs are handled by T5 [21] text encoder. video caption is decoupled into five progressive sub-captions and hidden state is separated into five segments, which correspond to the notations above. For post-processing, we adopt EMA-VFI[45] as the frame in5 terpolation model, to further normalize the video speed and extend video length. During inference, for single prompt from user input, we leverage GPT-4o as the refiner to generate five progressive sub-captions. The training of Presto can be separated into two stages: Text-to-Video Pre-training, and Text-to-Video Fine-tuning. The pre-training stage is built upon the Allegro model with 88 frames and 720 1280 resolution. The pre-training dataset contains 261k instances and we sample frames from videos at 6 FPS during this stage. Presto is trained for 1500 steps on 64 Nvidia H100 GPUs with batch size of 256 and constant learning rate of 1e-4, processing total of 384k videos. For fine-tuning, we pick the most content-diverse 47k instances from the pre-training dataset and fine-tune for another 500 steps with batch size of 256 learning rate of 1e-4, processing total of 128k videos. 5. Experiment In this section, We demonstrate the long video generation capability of Presto via both quantitative and qualitative evaluation in Sec. 5.2 and Sec. 5.3 respectively. Moreover, we provide an ablation study in Sec. 5.4 on key components of our model, including training data, progressive subcaption strategy, and Segmented Cross-Attention. We exhibit more results in Sec. and Sec. of Appendix, and discuss the limitations and failure cases in Sec. D. 5.1. Baseline Models To evaluate the effectiveness of our Presto on content diversity and long-range coherence, we compare it with the state-of-the-art text-to-video models. We select the best open-source model, Allegro [48], and commercial system, Runaway Gen-3 [26], as our baseline models. We also compare with the recent MT2V method, TALC [4]. To highlight the importance of scenario coherence, we add naive approach of Merge Videos in qualitative evaluation, by utilizing multiple texts to generate multiple short clips. 5.2. Quantitative Evaluation Setup We use VBench for our automatic quantitative evaluation. VBench offers 946 official prompts to validate different aspects of generated videos and is widely adopted benchmark in video generation methods. We directly report the results for models that exist in the VBench Leaderboard. We report several specific dimensions as well as the holistic dimensions in VBench, as shown in Tab. 1, which demonstrate the exceptional capability of generating long videos with rich content and long-range coherence while adhering to the input text. Presto outperforms all state-of-the-art video generation models on Semantic Score. Specifically, Presto notably surpasses Allegro with +5.5%, the commercial Gen-3 with +3.3%, and the previous MT2V method TALC with +34.1%. We attribute the performance improvement to the leverage of the progressive sub-captions generation strategy, which decouples the text input and improves the text information. Besides, we achieve full mark in Dynamic Degree metrics, reflecting the superior ability to capture dynamics and preserve camera control. Compared with TALC which achieves relatively high score of 98.6% in Dynamic Degree, we achieve significantly better performance on all metrics, highlighting the long-range coherence aided by the meticulous data curation and Segmented Cross-Attention. For the degradation in quality scores, we hypothesize that it arises from the increased difficulty in maintaining consistency when the dynamics are complex and varied. 5.3. Qualitative Evaluation Setup Evaluating the quality of generated videos is highly subjective task, as automatic benchmarks often dis-align with human judgment. User study is prevalent paradigm for qualitative assessment in previous work [43, 47, 48]. In this study, we collect 62 diverse text prompts encompassing wide range of aspects, including humans, animals, landscapes, and so on. Human annotators are tasked with blindly comparing pairs of videos and making preference judgment between two cases. To assess the enhanced content diversity achieved by our model, we evaluate three key dimensions: Scenario Diversity, Scenario Coherence, and Text-Video Adherence. We allow the tie situation when the differences are indistinguishable. We recruited 12 annotators, with each instance reviewed by three individuals, resulting in total of 2,232 ratings. We report the win rate (%) with each method, and calculate an overall score by considering all three dimensions. as shown in Tab. 2. Our model surpasses all baselines on the Overall Score, indicating that our Presto can generate videos with improved scenario diversity and coherence with text-following capabilities, even better than the commercial model Gen3. Specifically, Presto excels Gen-3 in scenario diversity, slightly outperforms in text-video adherence, and closely matches in scenario coherence. For the SOTA open-source model, Allegro, Presto wins in all dimensions, especially on the scenario diversity. For TALC, Presto significantly outperforms in all metrics, further demonstrating the importance of curated data. We also consider the naive approach of utilizing multiple texts by Merging Videos, to serve as reference for our metrics. Our method reaches similar results as Merging Videos on scenario diversity but significantly outperforms it in scenario coherence. We further exhibit the real cases in our user study, as shown in Fig. 3. For the first case, our Presto is the only one that captures intricate text details of People hurry along the sidewalk. while all other methods fail to generate walking people. For the second case, the generated video from our Presto achieves the largest scenario motion while with 6 Figure 3. Qualitative comparison with the baselines in our user study. Our Presto can capture intricate text details and generate long videos with long-range coherence and rich content. For the first case, ours is the only method that captures the text details of People hurry along the sidewalk, while other methods fail to generate walking people. For the second case, our generated videos are of the largest camera motion and the best scenario coherence."
        },
        {
            "title": "Methods",
            "content": "Gen-3 Allegro TALC Presto Dynamic Temporal Human Action Style Degree 96.4 24.7 60.1 91.4 24.4 55.0 89.0 18.0 98.6 93.0 25.8 100.0 Specific Dimensions Object Class 87.8 87.5 45.3 93."
        },
        {
            "title": "Color",
            "content": "80.9 82.8 57.3 98."
        },
        {
            "title": "Holistic Dimensions",
            "content": "Overall Consist. 26.7 26.4 19.5 27.8 Semantic Quality Score 84.1 83.1 62.5 80.6 Score 75.2 73.0 44.4 78.5 Overall Score 82.3 81.1 58.9 80.2 Table 1. Quantitative results of dimension performance on VBench. higher score indicates better performance in particular dimension. We focus on the semantic dimension suite to demonstrate our Presto is capable of generating content-rich videos with consistency. Dimensions Methods Gen-3 Allegro Merge Videos TALC Overall Score Lose 38.8 27.0 29.3 3.1 Tie 16.2 18.1 14.9 5.1 Win 45.0 54.9 55.8 91.8 Scenario Diversity Lose 27.4 21.1 44.8 4.1 Tie 13.5 10.9 9.7 5.3 Win 59.1 68.0 45.5 90. Scenario Coherence Tie Lose Win 16.4 48.5 35.1 45.1 22.3 32.6 71.5 9.7 18.8 95.3 2.9 1.8 Text-Video Adherence Tie Lose Win 40.9 18.7 40.4 51.4 21.1 27.4 50.3 25.5 24.2 89.5 7.0 3.5 Table 2. Qualitative results of win rate (%) on user study. We ask users to evaluate two given videos based on three dimensions: Scenario Diversity, Scenario Coherence, and Text-Video Adherence. The Overall Score is calculated by considering all of the three dimensions. the best long-range coherence. Considering Merge Video, which may generate highly diverse content, while failing to maintain consistency as five shots are totally different and unrelated. 5.4. Ablation Study Method O(verlap) SCA Overall Score Dynamic Degree 74.7 100.0 Segmented Cross-Attention (SCA) Strategy S(equential) SCA I(solated) SCA 73.7 73.1 LongTake-HD Dataset Curation w/o Meticulous Filtering Single Long Condition 72.0 71.8 100.0 100.0 97.2 100.0 Table 3. Ablation results of the model design for different Segmented Cross-Attention Strategies, and LongTake-HD dataset curation. We report the performance of models with 360p resolution and 40 frames on VBench. We now ablate on the key proposed components, including both Presto model design and LongTake-HD dataset curation. We use VBench to evaluate generated videos on two dimensions: 1) Overall Score; and 2) Dynamic Degree. As video generation methods consume huge computational resources in model training, we standardized our video generation model training to 360p resolution with 40 frames in this ablation study. Tab. 3 presents the ablation results on the automatic evaluation benchmark. Segmented Cross-Attention (SCA) Strategy. We ablate the three strategies of SCA including Overlap Segmented Cross-Attention (OSCA), Sequential Segmented Cross-Attention (SSCA), and Isolated Segmented CrossAttention (ISCA), as we mentioned in Sec. 4 and Fig. 2(b). All three strategies achieve 100% in Dynamic Degree score, indicating the general concept of SCA plays significant role in capturing dynamics. OSCA achieves an excellent result of 74.7% Overall Score, which is the best version among all strategies. This design not only maintains content richness but also facilitates transitions between segments by introducing proximal overlapping. SSCA achieves 73.7% Overall Score, slightly behind the OSCA. We attribute this degradation to the information from previous frames disrupting the interaction of later segmented latent features. ISCA achieves 73.1% Overall Score, performing poorly in the experiments, for it prohibits the information exchange in adjacent segments. This ablation further underscores the significance of the overlapping technique in OSCA, facilitating smoother transitions between segments. w/o Meticulous Filtering. We study the effect of our meticulously curated dataset LongTake-HD, with the same strategy of OSCA. We randomly select the same amount of pre-training data from the unfiltered video dataset, without the filtering of content diversity in Sec. 3.1 and captions in Sec. 3.2. Note that we still adopt the basic filtering for videos, including duration, speed, resolution, and low-level metrics, to ensure the basic visual quality, and ablate the effect of metrics contributing to rich content. The model with worse data achieves 72.0% on the overall score, largely lagging behind 2.7% in the same strategy with well-curated data. Moreover, the performance on Dynamic Degree will also drop by 2.8%, highlighting the effectiveness of our data 8 curation pipeline. Single Long Condition. To diminish the effect of the increased text length in our sub-captions, and demonstrate the effectiveness of separate modeling for each sub-caption in our Segmented Cross-Attention, we test the method of naive concatenation on text condition. Specifically, this approach directly concatenates sub-captions along the sequence length dimension, forming single long text condition. This long condition will attend to the whole hidden states, the same as standard text-to-video generation approaches. The results show that this naive concatenation method yields the lowest Overall Score with 2.9% drop compared to our proposed separate modeling strategy, indicating that our Segmented Cross-Attention effectively enhances the overall video quality. 6. Conclusion We introduced Presto, simple yet effective method for generating long-range coherent, content-rich, long videos. Our Presto achieves 78.5% and 100% on the VBench Semantic Score and Dynamic Degree, surpassing the existing SOTA video generation approaches. Presto utilizes the Segmented Cross Attention mechanism to integrate multiple texts concurrently, which can be seamlessly adopted in the existing diffusion model with DiT architecture. We also curate high-quality video-texts dataset LongTake-HD from public datasets and sources. We leave more exploration about the attention mechanism and model structure (e.g., auto-regressive generation) for long video generation as our future work."
        },
        {
            "title": "Acknowledgement",
            "content": "The authors appreciate Wei Chen, Huiguo He, Mindy Lin, Zeyu Liu, Yuhang Zhang, Yuanzhi Zhu for their valuable input and suggestions."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 4, 12 [2] Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. Character region awareness for text detection. In CVPR, pages 93659374, 2019. 3 [3] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In ICCV, pages 17281738, 2021. 3 [4] Hritik Bansal, Yonatan Bitton, Michal Yarom, Idan Szpektor, Aditya Grover, and Kai-Wei Chang. TALC: Time-aligned arXiv captions for multi-scene text-to-video generation. preprint arXiv:2405.04682, 2024. 2, 6 [5] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70M: Captioning 70M videos with multiple In CVPR, pages 1332013331, cross-modality teachers. 2024. 3 [6] Peter Deutsch. RFC1951: Deflate compressed data format specification version 1.3, 1996. [7] Brian Everitt. The Cambridge dictionary of statistics. Cambridge University Press 1998, 2002, 2006, 2006. 3 [8] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. 1, 2 [9] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. NeurIPS, 35:86338646, 2022. 1 [10] David Huffman. method for the construction of minimum-redundancy codes. Proceedings of the IRE, 40(9): 10981101, 1952. 3 [11] Quan Huynh-Thu and Mohammed Ghanbari. Scope of validity of psnr in image/video quality assessment. Electronics letters, 44(13):800801, 2008. 3 [12] Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han. Fifo-diffusion: Generating infinite videos from text without training. arXiv preprint arXiv:2405.11473, 2024. 1, 2 [13] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-ofexperts model, 2024. 4, 12 [14] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. [15] Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. arXiv preprint arXiv:2407.19918, 2024. 1, 2 [16] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. OpenVid-1M: large-scale high-quality dataset for text-tovideo generation. arXiv preprint arXiv:2407.02371, 2024. 3 [17] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 1 [18] Gyeongrok Oh, Jaehwan Jeong, Sieun Kim, Wonmin Byeon, Jinkyu Kim, Sungwoong Kim, Hyeokmin Kwon, and Sangpil Kim. MTVG: Multi-text video generation with text-tovideo models. arXiv preprint arXiv:2312.04086, 2023. 2 [19] PySceneDetect Contributors. PySceneDetect. https:// www.scenedetect.com, 2024. 3 [20] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169, 2023. 1, 2 [21] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 21(140):167, 2020. 5 [22] Vasco Ramos, Yonatan Bitton, Michal Yarom, Idan Szpektor, and Joao Magalhaes. Contrastive sequential-diffusion learning: An approach to multi-scene instructional video synthesis. arXiv preprint arXiv:2407.11814, 2024. 2 [23] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2019. 4, 12 [24] Nils Reimers and Iryna Gurevych. Making monolingual sentence embeddings multilingual using knowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2020. 4, 12 [25] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. MM-Diffusion: Learning multi-modal diffusion models for joint audio and video generation. In CVPR, pages 10219 10228, 2023. [26] RunwayML. Gen-3 Alpha. https://runwayml.com/ research/introducing-gen-3-alpha, 2024. 6 [27] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 35:3647936494, 2022. 1 [28] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. NeurIPS, 35:25278 25294, 2022. 3 [29] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-A-Video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 1 [30] Amit Singhal et al. Modern information retrieval: brief overview. IEEE Data Eng. Bull., 24(4):3543, 2001. 4, [31] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MPNet: Masked and permuted pre-training for language understanding. arXiv preprint arXiv:2004.09297, 2020. 4, 12 [32] Zhenxiong Tan, Xingyi Yang, Songhua Liu, and Xinchao Wang. Video-infinity: distributed long video generation. arXiv preprint arXiv:2406.16260, 2024. 1, 2 [33] Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal. MCVD: Masked conditional video diffusion for prediction, generation, and interpolation. NeurIPS, 35:2337123385, 2022. 1 [34] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264, 2023. 1, 2 [35] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. VideoFactory: Swap attention in spatiotemporal diffusions for text-to-video generation. arXiv preprint arXiv:2305.10874, 2023. 3 [36] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. VideoComposer: Compositional video synthesis with motion controllability. NeurIPS, 36, 2024. 1 [37] Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos with autoregressive language models. arXiv preprint arXiv:2410.02757, 2024. 1, 2 [38] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 13(4):600612, 2004. 3 [39] Zhou Wang, Alan Bovik, and Eero Simoncelli. Structural approaches to image quality assessment. Handbook of image and video processing, 7(18), 2005. [40] Watermark-Detection Contributors. Watermark-Detection. https : / / github . com / boomb0om / watermark - detection, 2022. 3 [41] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying flow, stereo and depth estimation. TPAMI, 2023. 3 [42] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In CVPR, pages 50365045, 2022. 3 [43] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 6 [44] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. 1, 2, 3 [45] Guozhen Zhang, Yuhan Zhu, Haonan Wang, Youxin Chen, Gangshan Wu, and Limin Wang. Extracting motion and appearance via inter-frame attention for efficient video frame interpolation. In CVPR, pages 56825692, 2023. [46] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep In CVPR, pages 586595, features as perceptual metric. 2018. 3 [47] Zhicheng Zheng, Xin Yan, Zhenfang Chen, Jingzhou Wang, Qin Zhi Eddie Lim, Joshua B. Tenenbaum, and Chuang Gan. Contphy: Continuum physical concept learning and reasoning from videos, 2024. 6 [48] Yuan Zhou, Qiuyue Wang, Yuxuan Cai, and Huan Yang. Allegro: Open the black box of commercial-level video gen10 eration model. arXiv preprint arXiv:2410.15458, 2024. 5, 6 [49] Jacob Ziv and Abraham Lempel. universal algorithm for IEEE Transactions on inforsequential data compression. mation theory, 23(3):337343, 1977. 3 [50] Jacob Ziv and Abraham Lempel. Compression of individual IEEE transactions on sequences via variable-rate coding. Information Theory, 24(5):530536, 1978. 3 11 Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation"
        },
        {
            "title": "Appendix",
            "content": "A. Details of LongTake-HD Dataset"
        },
        {
            "title": "Filtering",
            "content": "Pre-training Fine-tuning In this section, we show more details of our filtering steps, contributing to the LongTake-HD dataset with rich content and long-range coherence. Thresholds for each step are displayed in Tab. 4. We visualize the discarded samples and selected samples of each filtering step in Fig. 4. Moreover, we exhibit real case with coherent video frames and progressive captions in our LongTake-HD in Fig. 5. Pixel-wise Filtering. We use the Peak Signal-to-Noise Ratio (PSNR) to ensure the sampled keyframes are pixelwisely diverse and coherent. We filter out the cases with high PSNR values, indicating the keyframes are not diverse enough, as visualized in Fig. 4(a). Structure-wise Filtering. We employ the Structural Similarity Index Measure (SSIM) to measure the structural-wise similarity of the keyframe diversity. We filter out similar cases with higher SSIM values, and the cases with SSIM values lower than 0, which indicates that the image structures are inverted [39], as visualized in Fig. 4(b). Semantics-wise Filtering. We adopt the Perceptual Similarity (LPIPS) to evaluate the semantic diversity and coherence of sampled keyframes. We visualize discarded case and selected case in Fig. 4(c). Motion-wise Filtering. We utilize Unimotion to calculate the optical flow values of each video clip per second. Videos with higher flow values are both coherent and dynamic across scenarios, as visualized in Fig. 4(d). Text-wise Filtering. We utilize Aria [13] as our captioning model, and utilize MPNet [31] from SentenceTransformers [23, 24] to compute the cosine similarity [30] of each text pair. We filter out the cases with higher text similarity, as displayed in Fig. 4(f), to enhance the diversity in text captions. We further utilize GPT-4o [1] as the LLM for refining the sub-captions. Prompt templates for these two steps are displayed in Listing 1 and Listing 2. Negative Cases. We show the negative cases of keyframes and captions in Fig. 4(e) and Fig. 4(g) respectively. Blurry or unrelated keyframes are discarded, by analyzing the compressed image file size. Negative captions when LLMs refuse to respond, or with sensitive information, will be filtered out to improve the quality of captions. B. More Qualitative Comparisons We show more qualitative results compared with different baselines in Fig. 7 and Fig. 8. Our generated videos have the largest scenario motion and maintain long-range coherence. Content-Diverse Video Clips"
        },
        {
            "title": "Width\nHeight\nFPS\nDuration\nGrayscale\nLAION Aesthetics\nTolerance Artifacts\nUnimatch Flow",
            "content": "1280 720 [24, 60] 15 [20, 180] 4.8 5%"
        },
        {
            "title": "PSNR\nSSIM\nLPIPS\nText Similarity",
            "content": "[4, 20] [0, 0.7] 0.4 0.75 1280 720 [24, 60] 15 [20, 180] 5.0 5% 50 [4, 20] [0, 0.7] [0.5, 0.8] [0, 0.75] Table 4. Data filtering thresholds across various stages. All thresholds are manually determined by the specific characteristics of the dataset. C. Style Control and Camera Control To exhibit the superior capability of style control and camera control of our proposed Presto, we select series of prompts from the VBench, all centered around the same theme, shark is swimming in the ocean, but with variations in camera poses and styles. As shown in Fig. 9, the results demonstrate that our model accurately adheres to the style and camera specifications provided in user input text. D. Limitations Although our proposed Presto can generate long videos with long-range coherence and rich content, certain limitations remain. First, the generated videos sometimes exhibit slight degradation in visual fidelity compared to the base model. We attribute this to the exclusive use of publicly accessible videos for training, which, while diverse and coherent, still do not match the higher quality of the private datasets leveraged by the base model. Second, in cases involving extreme scenario motion, some regions may display artifacts such as blurring or ghosting, as visualized in Fig. 6. These artifacts are likely consequence of our model prioritizing scenario consistency and smoothness, which occasionally compromises spatial sharpness in high-motion backgrounds. Last, our model is not suitable for generating still frames. 12 Figure 4. The discarded and selected data samples of different filtering steps in LongTake-HD. We discard cases with similar keyframes and poor content diversity and filter out similar and negative captions. The selected cases have rich video content, coherent scenario motion, and progressive captions. We visualize the samples in the LongTake-HD Pre-training set and apply more rigorous filtering to develop the LongTake-HD Fine-tuning set. 13 Figure 5. The progressive sub-captions and coherent video frames of our LongTake-HD dataset. Our captions are more detailed in camera motion, as highlighted in the red text. 1 % Prompt Template for Image Caption 2 <IMAGE> 3 Describe the image in as much detail as possible. Incorporate the alt text if it provides information related to the visual scene. 4 alt text: <ALT_TEXT> 5 6 % Prompt Template for Video Caption 7 Write concise, continuous prompt describing the video for generation, including objective facts, main subjects, their movements and positions, interactions, human actions, data sources , lighting, environment, camera angles, movements, background, atmosphere, photography style, fashion, and temporal information. Use professional or simple language for camera angles and movements. 8 <VIDEO> Listing 1. Prompt template for video and image captioning. 14 1 % Prompt Template for Sub-captions Refinement in LongTake-HD Dataset 2 System Prompt: 3 You are helpful video director. Refine the five scene descriptions to become more coherent based on the provided five frame desciptions and the video description. 4 5 User Prompt: 6 will show you five scene descriptions in progressive frame level, as well as the video description. The refinement should follow these rules: 7 1. Refinement should be based on the corresponding frame description, and can add information based on the video description. Do NOT imagine or add other new information. Do NOT change the order of each description. 8 2. There needs to be connections between the five scenes. Analyze the scenario transitions ( such as camera movement, background changes, and object movement), and add them to each description. The camera movement should be smooth. 9 3. The five scenes must form continuous story, which means repeated object descriptions and details may be omitted. You need to accurately, objectively, and succinctly describe everything. The scene descriptions need to be concise. Do NOT add too many details unrelated to the video content description. 10 4. Frame descriptions are independent, so there may be duplication. You need to analyze the possible states of different frames based on the video description. Do NOT incorporate later details into the previous frames description. 11 The whole video description: <VIDEO_CAPTION> 12 Five descriptions at different frames: <FRAME_CAPTIONS> 13 14 % Prompt Template for Sub-captions Generation in Inference Stage 15 System Prompt: 16 You are helpful video director. 17 18 User Prompt: 19 Based on the video content description, you need to write five coherent scene descriptions to create silent video. These five descriptions are independent, but there needs to be connection between the five scenes. The five scene descriptions should include detailed scenario transitions (such as camera movement, background changes, and object movement). The camera movement should be smooth. Avoid drastic angle changes and transitions, such as shifting from frontal view directly to side view. You can add details and objects, but the five scenes must form continuous story, which means repeated object descriptions and details may be omitted. Five scene descriptions should NOT differ too much. Ensure similarity to enable smooth transitions between scenes. If the description is brief, you can add details, but stay conservative, and only create simple, easily generated scenes. Its also acceptable for multiple scenes to share higher degree of similarity. You need to accurately, objectively, and succinctly describe everything. The scene descriptions need to be concise. Do NOT add details unrelated to the video content description. Do NOT speculate. Do NOT add scene titles, directly return five scene descriptions. 20 The video content description: <VIDEO_DESCRIPTION> Listing 2. Prompt Template for GPT-4o Refinement. Figure 6. Our Presto can generate long videos with high scenario motion, and prioritize scenario smoothness. However, in the case of extreme scenario motion, the main object will retain details and sharpness (as shown in the green box), while the moving background makes it easier to display artifacts such as blurring or ghosting (as shown in the red box). 15 Figure 7. Qualitative comparison with the baselines in our user study. 16 Figure 8. Qualitative comparison with the baselines in our user study. 17 Figure 9. More results of VBenchs prompts centering around the same theme. Presto can generate videos with accurate camera control and style control."
        }
    ],
    "affiliations": [
        "01.AI"
    ]
}