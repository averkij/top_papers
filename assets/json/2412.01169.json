{
    "paper_title": "OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows",
    "authors": [
        "Shufan Li",
        "Konstantinos Kallidromitis",
        "Akash Gokul",
        "Zichun Liao",
        "Yusuke Kato",
        "Kazuki Kozuka",
        "Aditya Grover"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce OmniFlow, a novel generative model designed for any-to-any generation tasks such as text-to-image, text-to-audio, and audio-to-image synthesis. OmniFlow advances the rectified flow (RF) framework used in text-to-image models to handle the joint distribution of multiple modalities. It outperforms previous any-to-any models on a wide range of tasks, such as text-to-image and text-to-audio synthesis. Our work offers three key contributions: First, we extend RF to a multi-modal setting and introduce a novel guidance mechanism, enabling users to flexibly control the alignment between different modalities in the generated outputs. Second, we propose a novel architecture that extends the text-to-image MMDiT architecture of Stable Diffusion 3 and enables audio and text generation. The extended modules can be efficiently pretrained individually and merged with the vanilla text-to-image MMDiT for fine-tuning. Lastly, we conduct a comprehensive study on the design choices of rectified flow transformers for large-scale audio and text generation, providing valuable insights into optimizing performance across diverse modalities. The Code will be available at https://github.com/jacklishufan/OmniFlows."
        },
        {
            "title": "Start",
            "content": "OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows Shufan Li1, Konstantinos Kallidromitis2, Akash Gokul3, Zichun Liao1 Yusuke Kato2, Kazuki Kozuka 2, Aditya Grover1 1 UCLA 2Panasonic AI Research 3Salesforce AI Research *Equal Contribution Correspondence to jacklishufan@cs.ucla.edu 4 2 0 2 2 ]"
        },
        {
            "title": "M\nM",
            "content": ". [ 1 9 6 1 1 0 . 2 1 4 2 : r Figure 1. OmniFlow is capable of diverse range of any-to-any generation tasks. OmniFlow supports generation of any output modalities given any input modality, such as text-to-image, text-to-audio, audio-to-image generations. It also supports tasks in multiple input modalities such as text+audio-to-image."
        },
        {
            "title": "Abstract",
            "content": "We introduce OmniFlow, novel generative model designed for any-to-any generation tasks such as text-to-image, text-to-audio, and audio-to-image synthesis. OmniFlow advances the rectified flow (RF) framework used in text-toimage models to handle the joint distribution of multiple modalities. It outperforms previous any-to-any models on wide range of tasks, such as text-to-image and text-to-audio synthesis. Our work offers three key contributions: First, we extend RF to multi-modal setting and introduce novel guidance mechanism, enabling users to flexibly control the alignment between different modalities in the generated outputs. Second, we propose novel architecture that extends the text-to-image MMDiT architecture of Stable Diffusion 3 and enables audio and text generation. The extended modules can be efficiently pretrained individually and merged with the vanilla text-to-image MMDiT for fine-tuning. Lastly, we conduct comprehensive study on the design choices of rectified flow transformers for large-scale audio and text generation, providing valuable insights into optimizing performance across diverse modalities. The Code will be available at https://github.com/jacklishufan/OmniFlows. 1. Introduction Generative modeling has witnessed considerable advancements in recent years. Notably, diffusion models such as DALLE-3 [40], Stable Diffusion 3 [11], AudioLDM2 [33] achieves state-of-the art performance on text-to-image and text-to-audio tasks. However, these models can only perform single task while requiring considerable computing resources and data for training. To achieve any-to-any generations, previous works such as CoDi [46] and UIO [36] typically combine set of modality-specific encoders (e.g. ViT [1]) and decoders (e.g. Stable Diffusion [44]). However, this design limits these models ability to integrate information across modalities and generate multi-modal outputs coherently. For example, to perform audio+text-to-image (A+TI) generation, CoDi simply takes weighted average of the audio embedding and text embedding to condition an image generator. However, there is no guarantee that the averaged embedding can faithfully represent the two input modalities, as arbitrarily many modality embeddings can average to the same embedding. An alternative approach for any-to-any generation is to use single multi-modal model to learn the joint distribution of multiple modalities. This approach has often led to strong performance as it allows information to flow across modalities. However, existing single-model designs typically involve training from scratch, and thus require considerable amount of data. Existing works in this area, such as UniDiffuser [4] and Chameleon [47] only experiment with text and image modalities. They also require considerable compute resources. To the best of our knowledge, there has yet to be unified open-sourced multi-modal generative model that supports text, image, and audio simultaneously. We propose OmniFlow, unified multi-modal generative model for any-to-any generation. Unlike previous unified multi-modal models, OmniFlow does not need to be trained from scratch with large amount of data because of its modular design, saving considerable computing resources for its training. OmniFlow is inspired by the MMDiT architecture used in Stable Diffusion 3 [11], which performs text-to-image generation using two-stream network that combines text-input stream and an image-output stream through series of joint attention blocks. OmniFlow builds on MMDIT by incorporating additional input and output streams, extending its text-to-image capability to support any-to-any generation. Crucially, since the parameters for each stream are mostly independent, we can pretrain them separately or initialize them with pretrained single-task expert model (e.g. SD3). To effectively train OmniFlow, we propose novel multimodal rectified flow formulation that incorporates diverse set of tasks, such as text-to-audio and audio-to-image, into unified learning objective. Multi-modal rectified flow is built upon decoupled, time-differentiable interpretation between the distribution of multi-modal data pair and i.i.d. Gaussian noise. In this formulation, each of the any-to-any generation tasks can be represented by path connecting two noise levels. For example, given text, image, and audio modalities, the task of text+audio-to-image (T+AI) can be represented by path between the distribution of (clean text, clean audio, Gaussian noise) to (clean text, clean audio, clean image). We conducted extensive evaluations of OmniFlow. Experiment results show that OmniFlow outperforms previous any-to-any models on wide range of tasks, including text-toimage and text-to-audio generation. Compared to single-task specialist models, OmniFlow achieves competitive performance with state-of-the-art methods. In summary, our contributions are three-fold: First, we extend rectified flow formulation to the multimodal setting and support flexible learning of any-to-any generation in unified framework. Second, we proposed OmniFlow, novel modular multimodal architecture for any-to-any generation tasks. It allows multiple modalities to directly interact with each other while being modular enough to allow individual components to be pretrained independently or initialized from task-specific expert models. Lastly, to the best of our knowledge, we are the first work that provides systematic investigation of the different ways of combining state-of-the-art flow-matching objectives with diffusion transformers for audio and text generation. We provide meaningful insights and hope to help the community develop future multi-modal diffusion models beyond text-to-image generation tasks. 2. Backgrounds 2.1. Flow-Based Generative Models Flow-based generative models [23, 31, 34, 48], represent the coupling of data points x0 and noise distribution x1 using an ordinary differential equation (ODE): dxt = vθ(xt, t)dt (1) is defined as xt = (1 t)x0 + tx1, which corresponds to wRF = 1t . Esser et al. [11] summarized many configurations of common methods under this unified formulation, including (LDM)-Linear [44] and Cosine [39]. They also explored logit-normal distribution of timestep for textto-image generation. We explore all these variants in the context of multi-modal generation, particularly for audio and text, as it is unclear if the results from text-to-image domain can be directly generalized. 2.2. Any to Any Generation Prior works have explored any-to-any generation. CoDi [46] achieved it first by combining multiple modality-specific encoders (e.g. ViT) and decoders (e.g. Stable Diffusion) through bridge alignment. However, its design has limited cross-modality interaction. For example, to achieve text+audio-to-image (T+AI generation), it simply computes the weighted average of text embeddings and audio embedding. Unified-IO [36] models any-to-any generation as sequence-to-sequence problem, and uses an autoregressive model to achieve any-to-any generation such as textto-image or text-to-audio. Our work is the first to use multi-modal flow matching objective for any-to-any tasks. Additional works focus exclusively on unifying text-toimage and image-to-text generation. Chameleon [47] uses an LLM-like large autoregressive model to handle multimodal data. It represents images as VQGAN tokens [50]. Transfusion [52] adopted similar design, but uses nonautoregressive diffusion loss for image modeling, while maintaining an autoregressive loss for text generation. Despite their successes, these unified multi-modal models require considerable training resources, because they are less modular than previous works that combine multiple models. OmniFlow achieves good balance by separating parameters for each individual modality, while allowing features from each modality to freely interact with each other at every layer. 3. Method 3.1. Multi-Modal Rectified Flow Figure 2. Pipeline of OmniFlow. Previous any-to-any models such as CoDi [46] (Top) concatenate multiple modality-specific encoders and decoders, and naively average the embedding of multiple modalities to achieve joint conditioning. By contrast, OmniFlow (Bottom) is unified, modular multi-modal model, where features from different modalities directly interact with each other through joint attention layers. OmniFlow is inspired by the modular design of Stable Diffusion 3 [11] (Middle), text-to-image model. where the velocity is parameterized by neural network. Directly solving this equation is expensive. However, we can define forward process xt = a(t)x0 + b(t)x1 to directly regress conditional vector field using the Conditional Flow Matching (CFM) objective [48] as follows: LCFM = ( b(t)λ(t) 2 )Et,x1,xtx1ϵθ(xt, t) x1 (2) where λ(t) = log α(t)2 λ(t)b(t) (vθ(xt, t) α(t) β(t)2 is the signal-to-noise ratio (SNR), ϵθ(xt, t) = 2 α(t) xt) is parameterized by of vθ. The optimum of this objective remains unchanged when introducing time-dependent weighting, and hence we can rewrite it following [22] as: Lw(x0) = where, w(t) = Et, x1 w(t)λ(t)ϵΘ(zt, t) ϵ2 1 2 2 λ(t)b(t)2 for CFM and x1 (0, I) follows noise distribution. This formulation gives unified representation for variety of generative modeling approaches. For example, rectified flows forward process (3) 1, x0 2, ..x0 We consider the joint distribution (x0 n) πdata over the space of paired multi-modal data where xi Rdi is sample of modality represented by vector of di 2, ..x1 n) π1 be the i.i.d Gaussian 1, x1 dimension. Let (x1 distribution where x1 (0, I) is Gaussian vector of di dimension. Given empirical observations x0 πdata, and x1 π1, we consider the decoupled, continuous, timedifferential interpolation given by: 2 , . . . , xti , t1, . . . , ti) 1 , xt2 = vi(xt1 xti ti xti tj xti = (1 ti)x0 = 0; = i + tix1 (4) (5) (6) where the independence condition of Eq (2) indicates xti only moves when ti moves. Over this interpretation space, we can use path τ : (t1, t2..tn); [0, 1] [0, 1]n to model any-to-any generation tasks involving these modalities. For example, given (x1, x2, x3) pdata where x1, x2, x3 are image, text, and audio modalities. We can model text-to-image(TI) tasks as path τt2i such that τt2i(0) = (0, 0, 1), which represents clean text-image pair and τt2i(1) = (1, 0, 1), which represents clean text. We can similarly model the joint sampling of text, image and audio set as path from (0, 0, 0) to (1, 1, 1) and text+image-toaudio (T + A) as path from (0, 0, 0) to (0, 0, 1). The flow matching objective would be solving least squares regression problems for each modality of the form: Eτ min vi θ (cid:90) τ Ex0,x1vi vθ,i(xt1 1 , xt 2 , ...xtn , t1..tn)2ds (7) x1 where vi = x0 , and vθ,i is neural network parameterized by θ. We use the same network θ to predict outputs for all modalities 1, 2..N . The outer expectation is over some prior of paths encoding generation tasks we are interested in. The integral is calculated over path τ (t) = (t1, ...tn), and ds = ti dt. Concretely, we consider three modalities: image, text, audio in our experiments as modalities: 1, 2 and 3 respectively. We consider the distribution of all possible linear paths τ (t) = (t1, t2, t3) in [0, 1]3 following the rectified flow formulation. They can encode diverse set of tasks such as text-to-image or text+image-to-audio. During the training, we do not necessarily need all modalities for each data point. For data points that only contain subset of three modalities (e.g. text-image pairs), we can set the time step of remaining modalities (e.g. audio) to 1, which correspondences to complete Gaussian noise. The full training algorithm is given as follows: Algorithm 1 Multi-Modal Rectified Flow i1, x0 Input: Dataset consists of modality 1, ...N ,where each sample = (x0 i2, ..) consists of subset (or all) of modalities i1, i2.. {1, 2, ..N }. ) vti Output: vθ,i : (xt1 1, 2..N , parameterized by θ Initialize θ for each = 2 , ...xtn 1 , xt2 i2, ..) 1: while not converged do Sample = (x0 i1, x0 2: x0 0; {1, 2..N } {i1, i2...} 3: Sample path τ .* 4: Sample Uniform([0, 1]) (t1...tN ) τ (t) xti xti = (cid:80) Perform optimizer step using θL = (1 ti)x0 + tix1 i{i1,i2..}vi vθ,i(xt1 5: 6: 7: 8: 9: 10: end while 11: Return θ 12: ; 1, 2..N 1 , ...xtn , t1..tn)2 * τ encodes task involving only modality i1, i2.., hence tj = 1; / {i1, i2..} At inference, we simply pick path and use the network prediction to solve for Eq. (5). Notably, for standard textto-image generation with (x1, x2) pairs where x1 is image and x2 is text, and x3 is the missing audio modality, picking linear path from (1, 0, 1) to (0, 0, 1) is equivalent to the standard single-modality rectified flow (TextImage) formulation used by Stable Diffusion 3 [11]. 3.2. Multi-Modal Guidance To flexibly control the multi-modal generation process, we extend the classifier free guidance (CFG)[16] to multi-modal rectified flow setting. Recall that CFG of single modalities are formulated as follows: ˆvθ(xt, c) = vθ(xt, c) + (α 1)(vθ(xt, c) vθ(xt)) (8) i, x0 ) vθ(xt where is condition and xt is the noised latent at timestep of the single-modal output. We extend this formulation to multi-modal setting by defining δij = vθ(xt i), which represents the influence of input modality to output modality i. In particular, we obtain vθ(xt i) by setting inputs of modalities not present in the formula to Gaussian noise. For example, 1, x0 given three modalities x1, x2, x3, we can obtain vθ(xt 2) by computing vθ(xt 1) by comput1, x1 ing vθ(xt 3 is just Gaussian noise. Given the set of δij, we can guide the output generation 3) and obtain vθ(xt 3). Note that x1 ) and vθ(xt 1, x0 2, x1 2, 2, x1 i, x0 of modality by the following formula: ˆvθ(xt1 1 ...xtn ) = vθ(xt 1 ...xtn ) + (cid:88) (αij 1)δij (9) j=i (a) Overall Pipeline of OmniFlow (b) Design of Omni-Transformer Block Figure 3. Architecture of OmniFlow. Left: We highlight the architecture of OmniFlow. Right: We show the design of an individual Omni-Transformer Block. where αij is the equivalent of α in multi-modal setting. This scheme allows the user to precisely control the interaction between each of the input and output modalities. When there are only two modalities, our multi-modal guidance Eq. (9) is equivalent to the standard single-modal classifier-free guidance Eq. (8). 3.3. Model Architecture We propose OmniFlow, modular, effective extension to the MMDiT architecture used in Stable Diffusion 3. Concretely, given multi-modal inputs that consist of text, image, and audio, we first convert them to latents x1, x2, x3 using modality-specific VAEs. We then add random Gaussian noise to the latents following the forward process defined in Eq. (6). We use the three sinusoidal embeddings to encode, t1, t2, t3 which correlate to the noise scale for each modality. These three timestep embeddings are passed to an MLP to obtain y, single embedding representing all modality-specific time steps. The final input to OmniFlow are the unified timestep embedding y, and noised latents (x1, x2, x3). These four input vectors are passed to consecutive Omni-Transformer blocks. The final hidden states of each modality, are then processed by the linear output layer to obtain predictions of v. Within each Omni-Transformer block, the inputs x1, x2, x3 are processed by modality specific projections to obtain q1, k1, v1, q2, k2, v2, q3, k3, v3. We then concatenate the queries, keys, and values to obtain = Concat(q1, q2, q3), = Concat(k1, k2, k3), = Concat(v1, v2, v3). The joint attention output for ith modality outi is given by: outi = SoftMax( qT )V (10) where is the dimension of each attention head. The output is passed to feed forward network (FFN) to get the final output of the Omni-Transformer block. Following the design of DiT [41], we use the unified time embedding to modulate the qkv projection and FFN. We add skip connections after the joint attention operation and after the FFN. We illustrate the model architecture in Fig. 3. Notably, different modalities are handled by different projection and feed-forward layers with independent parameters. The only multi-modal operation is the joint attention, with no trainable parameters of its own. This allows us to pretrain layers of different modalities individually and combine them for finetuning, which significantly improves the training efficiency. 4. Setup 4.1. Training Dataset We use text-image pairs, text-audio pairs, and audio-image pairs during training. We also make use of small amount of text-image-audio triplets. The text-image pairs include 5M images sampled from COYO-700M dataset [5], 2M images sampled from LAION-Aesthetic-3M subset [25], 7M images from LAION-COCO subset [26], the full CC12M dataset [6], and 2M high-quality image dataset generated by fluxdev and DALLE-3 [14]. We put high weights on images from LAION-Aesthetic-3M and the 2M high-quality images to maintain good aesthetic quality in the output. The textaudio pairs include the full training set of AudioSet [12], Audiocaps [21] and WavCaps [37]. The audio-image pairs include the training data of VGGSound [7] and SoundNet [2]. While SoundNet contains 2M images and is larger than VGGSound, we set the sample weight of VGGSound and SoundNet to 2:1 since SoundNet contains many improperly resized images with bad aspect ratios. To generate text-image-audio triplets, we use BLIP [28] to generate synthetic captions for videos in VGGSound and SoundNet. We provide further details of the dataset construction in the Appendix. 4.2. Training Recipe At high level, we initialize OmniFlow with the text and image modules of Stable-Diffusion 3 (Model 1). We first train separate text-to-audio model with text-audio pairs (Model 2). Then, we merge Model 1 and Model 2 to obtain combined model with text, image, and audio modules (Model 3). Since Model 1 and Model 2 have separate text modules, we average their weights during the merge process. Finally, we fine-tune Model 3 on diverse set of any-to-any tasks using the methods described in Sec. 3.1. Due to our modular design, we can initialize and pretrain each module individually. This saves immense computational cost when compared to previous unified multi-modal models (e.g. UniDiffuser [4]) which are trained from scratch. We use global batch size of 64 and train Model 2 and Model 3 for 100k, and 150k steps each. We provide further training and implementation details in the Appendix. 5. Main Results 5.1. Evaluation Metrics We perform extensive experiments on paired generation (textto-image, text-to-audio) and generic any-to-any generation such as text-to-audio+image (TI+A), audio-to-text+image (AT+I). For text-to-image generation, we report FID [15] and CLIP [43] scores on MSCOCO-30K benchmark [30]. Following the official implementation, the cosine similarities between CLIP embeddings are multiplied by 100. We also report results on the GenEval benchmark [13]. For audio generation, we report FAD [20] and CLAP [10] score on AudioCaps. Results are reported with 16kHz sampling rate. We also use CLAP scores for caption evaluations. 5.2. Text-to-Image Generation Model UniDiffuser CoDi UIO-2XXL SDv1.5 SDXL* SD3-Medium* OmniFlow* Param FID CLIP 9.71 30.93 0.9B 30.69 11.26 4.3B 13.39 6.8B - 30.63 11.12 0.9B 31.36 16.49 2.6B 30.65 20.94 2B 31.54 13.40 3.4B Table 1. Text-to-Image Generation on MSCOCO-30K Benchmark. *Indicates models pretraining data consists of high quality images and captions that do not follow the distribution of COCO dataset, which can negatively affect FID scores. Model Param Images Gen. SD1.5 SDv2.1 SDXL DALL-E 2 SD3-Medium SD3-Large CoDi UniDiff. OmniFlow Chameleon Transfusion Text-to-Image Specialist 4.0B 0.9B 2.3B 0.9B 1.6B 2.6B 2.6B 4.2B 1B 2B 2.0B 8B Generalist 4.3B 0.9B 3.4B 7B 7B 400M* 2B 30M* 3.5B 3.5B .43 .50 .55 .52 .62 .68 .38 .43 .62 .39 .63 Table 2. Text-to-Image Generation on GenEval Benchmark. We compare the model size, number of training images and GenEval benmark Score. * Indicates fine-tuning dataset. CoDi and MMDiTO are both initialized with pretrained text-to-image diffusion models (SD and SD3). We report results on MSCOCO-30k in Tab. 1, and results on GenEval in table Tab. 2. On MSCOCO-30k, we achieve lower FID than state-of-the-art models such as SDXL and SD3-Medium. While our FID number is higher than some previous models such as SDv1.5, it should be noted that more recent models such as SDXL and SD3 tend to have higher FID numbers because they are trained on high-quality text-image pairs that do not match the distribution of COCO images [42]. Notably, SD3 has FID of 20.94 while SDv1.5 has 11.12, even though SD3 is considered better model according to human evaluations. SDXL, which is widely recognized as the state-of-the-art open-sourced model before the release of SD3, also has higher FID than SDv1.5. In terms of CLIP scores, OmniFlow significantly outperforms previous models. In particular, when contrasted with generalist models UniDiffuser and CoDi, we achieve gain of +0.61 and +0.85 respectively, showing superior textto-image alignment. On GenEval Benchmark, which better measures the text-to-image capabilities, OmniFlow achieves score of 0.62, competitive score even when compared to the state-of-the-art specialist SD3-Medium. In addition, OmniFlow significantly outperforms previous any-to-any baselines at the same scale, such as CoDi (+.24) and UniDiffuser (+.19). Compared with larger models trained on lot more images, OmniFlow outperforms Chameleon-7B and achieves competitive performance as Transfusion-7B. Notably, unlike Chameleon, Transfusion, and UniDiffser which need to be trained from scratch, OmniFlow achieves high performance with only 30M training images, highlighting the effectiveness of our modular design. While the design of CoDi also allows it to make use of pretrained text-toimage model as its initialization, it is trained with considerably more images than OmniFlow while performing worse. 5.3. Text-to-Audio Generation Model Param FAD CLAP AudioGen-L[24] Make-an-Audio[19] AudioLDM-L[32] Make-an-Audio 2[18] AudioLDM 2-Full-L[33] Text-to-Audio Specialist 1B 0.4B 0.7B 0.9B 0.7B Generalist CoDi OmniFlow UIO-2XXL 3.4B 3.4B 6.7B 1.82 2.66 1.96 2.05 1.86 1.80 1.75 2.64 - - .141 .173 . .053* .183 - Table 3. Text-to-Audio Generation on AudioCaps Evaluation Set. Comparison of FAD and CLAP scores for various audio generators. *Reproduced from official checkpoint, see Appendix for details. We report text-to-audio generation results on AudioCaps in Tab. 3. Compared with previous state-of-the-art, OmniFlow achieves strong performance on FAD and CLAP scores. It outperforms AudioLDM2 on FAD (-0.11) and achieves equivalent performance on CLAP (+0.001). When compared with generalist models, OmniFlow significantly outperforms CoDi on both FAD (-0.05) and CLAP (+.13) metrics. 5.4. Receipes for Audio and Text Diffusions We explore various recipes for training audio and text diffusion transformers for multi-modal generation, which is relatively under-explored area. Concretely, we explored five formulations mentioned in the section Sec. 2.1. For these Audio Gen. Text Gen. CLAP eps/linear v/cos v/linear rf/uniform rf/lognorm FAD Continuous Flow Matching 2.08 2.01 1.86 1.82 1.79 Discrete Text Diffusion - - .141 .203 .126 .227 .254 .180 .163 SEDD[35] MDLM[45] Table 4. Various Formulations for Audio and Text Generation. We report FAD for audio generation and CLAP for text generation on AudioCaps dataset. experiments, we used model with only audio and text modules (Model 2 in Sec. 4.2) and trained for 50k steps. We report FAD score for text-to-audio generation and CLAP score for audio-to-text generation. Amongst all five formulations, rf/lognorm performs the best with the lowest FAD (1.79) and highest CLAP score (.254). We also explored two discrete space diffusion models, SEDD [35] and MDLM [45] which showed advantages over continuous-space diffusion models in recent literature. Specifically, we use the absorbing state version of SEDD. For these experiments, the text-vae encoder is replaced with token-embedding layer, and, text-vae decoder is replaced with simple linear output layer to predict token logits. We also replace the flow-matching loss on the text-embedding with the loss function of SEDD and MDLM respectively, which operates on token logits instead of continuous embeddings. We report the CLAP score on audio-to-text generation. We do not see considerable advantages over continuous alternatives. 6. Sampling On the sampling side, we explored the effect of guidance and timestep shift. The timestep shift was originally introduced by SD3 to balance the sampling process of images at different resolutions. Concretely, it augments the inference schedule as: ˆt = γt 1 + (1 γ)t (11) where γ = (cid:112) , with being the target sample resolution and being reference resolution. For audio and text generation, there is no concept of varying resolution, as the input audio spectrogram and text embedding have fixed resolutions. However, we empirically observe applying shift can improve the generation quality. Concretely, incorporating the shift term γ > 1 will lead to concave schedule, where the denoising process progresses slowly at the beginning and accelerates towards the end. We find that this improves sample quality for text-to-audio and audio-to-text generation tasks. We employ the multi-modal guidance mentioned in Sec. 3.2. For simple audio-to-text and text-to-audio generation, our formulation is reduced to standard classifierfree guidance. We show the effect of guidance and timestep shift in Fig. 4. Generally, we find that shift=3.0 works well for both tasks. For audio generation, guidance scale of 8 achieves the highest performance. For text generation, guidance scale of 4 achieves the best result. (a) Text-to-Audio Generation. (b) Audio-to-Text Generation. Figure 4. Effect of CFG and Shift for audio and text generation. We evaluate the impact of guidance and timestep shift on text-toaudio and audio-to-text tasks. To explore the effect of multi-modal guidance in Sec. 3.2, we provide qualitative results for audio+image-to-text (A+IT) task. Recall that we use x1, x2, x3 to denote image, text, and audio modalities. The multi-modal guidance for this task can be controlled by α21 and α23 where α21 controls text-image alignment and α23 controls text-audio alignment. For simplicity, we denote α21 as αim and α23 as αau. We vary αim, αau between the interval [1.0, 2.0] such that αim + αau = 3.0. We show the results in Fig. 5. Qualitatively, higher αau will make the models output resemble more the audio captions, and αim will make the models output resembles more the image captions. Interestingly, we observe that it also reflects the subtle differences in the style of audio and image captions in the training data (e.g. whether the first letter is capitalized). By varying these two parameters, users can achieve flexible control of generation. 6.1. Qualitative Comparison We directly compare OmniFlow with two recent any-to-any generation methods: CoDi [46] and UniDiffuser [4]. In addition to the quantitative results, we present qualitative text-toimage comparisons in Fig. 6. These examples demonstrate that OmniFlow achieves significant improvement in generation quality compared to previous any-to-any models. Specifically, in the first example (top), our model successfully follows the prompt while maintaining high aesthetic quality, accurately capturing both the cats features and its Figure 5. Effect of Multi-Modal Guidance. In this example, the user can flexibly control the alignment between output text and input image, audio independently by varying αau and αim. Higher αim will make the output texts resemble image captions, with visual descriptions such as lined up, driving down. Higher αau will make the output texts resemble audio captions, with descriptions such as accelerating, revving. Figure 6. Qualitative Comparison with baselines on text-toimage generation. OmniFlow achieves better image quality and prompt alignment when compared to previous generalist models. mirrored reflection. In contrast, CoDi is unable to change the cats eyes, and UniDiffuser fails to depict the cat looking at the mirror. similar trend is evident in the third example: OmniFlow correctly positions lanterns tied to rope, while UniDiffuser places them on the river. Finally, in the lighthouse example, CoDi fails to incorporate seagulls, and UniDiffuser ignores the adjective gentle, instead producing an image with rough waves and an out-of-focus lighthouse. Our results show that OmniFlow achieves much higher generation quality compared with previous any-to-any models, both in terms of image-text alignment and image fidelity. 7. Conclusion We present OmniFlow, unified early-fusion multi-modal generative model for any-to-any generation tasks. OmniFlow adapts modular design that enables individual components to be pretrained separately, while allowing features from different modalities to directly interact with each other, through joint attention mechanism. We conduct extensive experiments to show that OmniFlow outperforms previous any-to-any models on wide range of challenging generation tasks, including text-to-image and text-to-audio generation. We provide further analysis on the limitation of OmniFlow in the Appendix."
        },
        {
            "title": "References",
            "content": "[1] Dosovitskiy Alexey. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010.11929, 2020. 2 [2] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Soundnet: Learning sound representations from unlabeled video. Advances in neural information processing systems, 29, 2016. 6, 12 [3] JISHENG BAI, Haohe Liu, Mou Wang, Dongyuan Shi, Wenwu Wang, Mark Plumbley, Woon-Seng Gan, and Jianfeng Chen. Audiosetcaps: Enriched audio captioning dataset generation using large audio language models. In Audio Imagination: NeurIPS 2024 Workshop AI-Driven Speech, Music, and Sound Generation, 2024. 12 [4] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu. One transformer fits all distributions in multi-modal diffusion at scale. In International Conference on Machine Learning, pages 16921717. PMLR, 2023. 2, 6, 8, 15 [5] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/ coyo-dataset, 2022. 6 [6] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text preIn CVPR, training to recognize long-tail visual concepts. 2021. [7] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: large-scale audio-visual dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 721725. IEEE, 2020. 6, 12 [8] Wenxi Chen, Ziyang Ma, Xiquan Li, Xuenan Xu, Yuzhe Liang, Zhisheng Zheng, Kai Yu, and Xie Chen. Slam-aac: Enhancing audio captioning with paraphrasing augmentation and clap-refine through llms. arXiv preprint arXiv:2410.09503, 2024. 15 [9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instructionfinetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. 12 [10] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. Clap learning audio concepts from natural language supervision. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 6 [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, 3, 4, 12 [12] Jort Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 776780. IEEE, 2017. [13] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-toimage alignment. Advances in Neural Information Processing Systems, 36, 2024. 6 [14] Jacky Hate. Text-to-image-2m dataset, 2024. Accessed: 202411-14. 6 [15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [16] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 4 [17] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 13 [18] Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang, Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma, and Zhou Zhao. Make-an-audio 2: Temporal-enhanced text-toaudio generation. arXiv preprint arXiv:2305.18474, 2023. [19] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. In International Conference on Machine Learning, pages 1391613932. PMLR, 2023. 7 [20] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Frechet audio distance: metric for evaluating music enhancement algorithms. arXiv preprint arXiv:1812.08466, 2018. 6 [21] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 119132, 2019. 6 [22] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36, 2024. 3 [23] Leon Klein, Andreas Kramer, and Frank Noe. Equivariant flow matching. Advances in Neural Information Processing Systems, 36, 2024. 2 [24] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Defossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. arXiv preprint arXiv:2209.15352, 2022. [25] LAION. Aesthetics for open source, 2023. Accessed: 202411-14. 6 [26] LAION. Laion coco: 600m synthetic captions from laion2ben, 2023. Accessed: 2024-11-14. 6 [27] Chunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiujun Li, Yizhe Zhang, and Jianfeng Gao. Optimus: Organizing sentences via pre-trained modeling of latent space. arXiv preprint arXiv:2004.04092, 2020. 13 [28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified visionIn International language understanding and generation. conference on machine learning, pages 1288812900. PMLR, 2022. [29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. 13, 15 [30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 6 [31] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2 [32] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. 7, 13 [33] Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. 2, 7 [34] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [35] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. In Forty-first International Conference on Machine Learning, 2024. 7, 14 [36] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2643926455, 2024. 2, 3 [37] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark Plumbley, Yuexian Zou, and Wenwu Wang. Wavcaps: chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. 6 [38] MidJourney AI. ai, 2024. https://www.midjourney.com/. 15 Image generated using midjourney Accessed on November 21, 2024. URL: [39] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 81628171. PMLR, 2021. [40] OpenAI. Dall-e 3, 2023. 2 [41] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 5 [42] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 6 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 6 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3, 13 [45] Subham Sekhar Sahoo, Marianne Arriola, Aaron Gokaslan, Edgar Mariano Marroquin, Alexander Rush, Yair Schiff, Justin Chiu, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 7, 13 [46] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 8, 14, 15, 17 [47] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 2, [48] Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. Conditional flow matching: Simulation-free dynamic optimal transport. arXiv preprint arXiv:2302.00482, 2(3), 2023. 2, 3 [49] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 45664575, 2015. 14 [50] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. 3 [51] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024. 13 [52] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 3 [53] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, WANG HongFa, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. In The Twelfth International Conference on Learning Representations, 2023. OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows"
        },
        {
            "title": "Size Modality",
            "content": "T,I LAION-Aesthetics-3M 2M* T,I 12M T,I 5M T,I 7M T,A,I 2M 0.2M T,A,I 2M 2M 46K 0.4M CC12M COYO-700M(Subset) LAION-COCO SoundNet VGGSound T2I-2M AudioSet AudioCaps WavCaps T,I T,A T,A T,A Table 5. List of all datasets used in training. *Some image URLs are no longer accessible. We generate synthetic captions using BLIP. A. Implementation Details A.1. Dataset In Tab. 5, we list the size of all the dataset used in the training process. We filter out all images whose shortest side is less than 256. To obtain data with all modalities (image, audio, text), we use BLIP to generate synthetic captions for images in the SoundNet[2] and VGGSound[7] dataset, which are extracted from videos. Since AudioSet only comes with class labels, we use synthetic captions generated by audiolanguage models provided by AudioSetCaps[3]. A.2. Schedules Recall from Section 3 that we can represent different tasks with different paths in [0, 1]3. We visualize this in Fig. 7. We adopted simple linear tasks for any-to-any generation tasks so that for simple cases like text-to-image and text-to-audio, our formulation matches the standard rectified flow. A.3. Training Pipeline We initialize our model with SD3 (Model 1). We first train the model on text-audio pairs to obtain Model 2. The text branch of Model 2 is initialized with weights of SD3, while the audio branch is randomly initialized. After the training, we merge Model 1, which contains text branch and an image branch, and Model 2, which contains text branch and an audio branch, to Model 3, which contains text, image, and audio branches. The text branch of Model 3 is obtained by averaging the weights of the text branches from Model 1 and 2. Finally, we train the Model 3 on all datasets mentioned in Suppl. A.1. This training pipeline is illustrated in Fig. 8. We train Model 2 for 100k steps and Model 3 for 150k steps. We use 8 A6000 GPUs with per GPU batch size of Figure 7. Paths encoding different any-to-any generation tasks. (t1, t2, t3) represents the noise level of image, text and audio modalities. (0, 0, 0) represents clean (image, text, audio) triplets, and (1, 1, 1) represents pure Gaussian noise. Figure 8. Training Pipeline of OmniFlow. We initialize our model with SD3 (Model 1). We then train the model on text-audio pairs to obtain Model 2. We merge Model 1 and Model 2 to obtain Model 3. The final model is obtained by further training Model 3 on anyto-any generation tasks. 8. We use AdamW optimizer with learning rate of 1e-5 for Model 2 and 5e-6 for Model 3. The learning rate undergoes linear warmup in the first 1000 steps and cosine decay throughout the rest of the training. We adopt exponential moving average (EMA), which are updated every 100 training steps with decay factor of 0.999. A.4. Text VAE We train text VAE on caption data using Flan-T5-L [9]. Recall that SD3[11] makes use of three text encoders: CLIP-L, the text VAE on all caption data mentioned in Suppl. A.1 for 2 epochs, with learning rate of 1e-5, global batch size of 256 using AdamW optimizer. When using the VAE encoder as the text encoder of OmniFlow, we pad the embedding to 4096 with zeros to maintain the input dimension of SD3. Additionally, we also incorporate the CLIP-L and CLIP-G encoders of SD3 as auxiliary text encoders to stabilize the training. We apply random dropout to these encoders during the training. During the inference, the CLIP encoders are not used if the input does not contain clean texts (e.g. Image-to-Text task). A.5. Audio VAE We directly adapt the audio VAE used by AudioLDM [32]. A.6. Omni-Transformer We followed the architectural design of SD3 for image and text modules and initialize them with SD3 weights. The audio modules are initialized with identical setup to the image modules. Specifically, it has 24 layers and hidden size of 1536. The positional embedding layer has patch size of 2. Since the audio VAE outputs feature map of dimension 256 16, the positional embedding layer will convert each audio to sequence of length 128 8 = 1024. A.7. Pooled Conditional Embeddings SD3 makes use of additional pooled embeddings from CLIPViT-L/14 and CLIP-ViT-G/14 in addition to the sequence embeddings. We maintain them as is, with additional dropout during the training. We additionally incorporate an Audio Encoder to create pooled embeddings for audio inputs [53]. These embeddings are not used when clean data of respective modality is not available. A.8. Baselines In this section, we describe the specific variants studied in Tab. 4. Except for the discrete text diffusions (SEDD and MDLM), these variants fit into the unified formulation of Eq. (3) by varying its parameters. linear is variant of DDPM used in LDM [44]. It discretizes the timesteps to 0, 1...T 1 and uses the formulation bt = (cid:112)1 α2 i=0(1 βi), and 1 ((cid:112)βT 1 β0))2. We explored ϵβt = ( prediction and v-prediction objectives for this variant. , where at = β0 + (cid:113)(cid:81)t cosine is defined by the forward process xt = cos( π 2 t)x0+ = sin( π 2 t)x (12) The weighting function is wt = eλt/2 for v-prediction objectives[17]. SEDD and MDLM are recently proposed discrete textdiffusion models. We consider MDLM[45] and the absorbing Figure 9. Architecture of Text VAE and Text Encoders in OmniFlow. SD3 (Top) uses three text encoders: CLIP-L, CLIP-G, and T5-XXL. OmniFlow (Middile) replaces the 4.7B T5-XXL with VAE encoder based on Flan-T5-L. CLIP encoders become optional and are not used for tasks without clean text inputs. The decoder of VAE (Bottom) is based on TinyLlama-1.1B. The VAE embedding is used as the prefix for decoding. CLIP-G and T5-XXL. We replace the 4.7B T5-XXL with Flan-T5-L [27] to save computation cost and use it as part of text VAE. Specifically, given an input caption of length L, it is first encoded by Flan-T5-L to obtain vector of size 1024. We then pass it to QFormer[29] and obtain an output vector of size 32 64. This vector is used as the VAE embedding. In the decoding process, the VAE embedding is first processed by linear projection layer to obtain vector of size 32 2048. This is used as the prefix embedding for TinyLlama-1.1B decoder [51]. These architecture designs are shown in Fig. 9. Note that while we introduced 1.1B text-decoder, the overall system actually has fewer parameters since we replaced the 4.7B T5-XXL with 783M Flan-T5-L. We employ the auto-encoding training objective of OPTIMUS [27]. We freeze the Flan-T5-L encoder and fine-tune the QFormer and TinyLlama decoder end-to-end. We train Figure 11. Synthetic Experiments on three 1D-modalities. We consider the joint distribution of three toy modalities (x1, x2, x3), each represented by vector of dimension 1. Hence, triplet consisting of three modalities be represented by point in R3 We assume the joint distribution is uniform distribution in the neighborhood of tetrahedron (Left). We experiment with training OmniFlowusing triplets, pairs, and only individual modalities. Models trained with triplets of three modalities best represent the original distribution. B.2. Necessity of text, image, audio triplets. Compared with previous works such as CoDi[46] which uses weighted average of embeddings to mix multiple input modalities, OmniFlow requires directly training on triplets consisting of all modalities (image, text, audio). To study the necessity of this requirement, we conduct synthetic toy examples on three modalities (x1, x2, x3), each represented by one-dimensional vector. triplet of three modalities can then be represented by point (X, Y, Z) in 3D space. We show this experiments in Fig. 11. We assume the ground truth data distribution follows uniform distribution in small neighborhood adjacent to tetrahedron (Leftmost Figure). We experiment with training an 8-layer MLP with triplets (x1, x2, x3) (Second-Left Figure), pairs of (x1, x2),(x1, x3),(x2, x3) (Second-Right Figure), and only individual modalities (x1), (x2), (x3) (Rightmost Figure). For each model, we plot 50k samples generated by the model. Qualitatively, models trained on triplets best represent the data distribution. This makes sense as pairs are essentially projections on XY, XZ, YZ planes and individual modalities are projections on X, Y, axis. These projections are not sufficient to recover the original distribution represented in this 3D space. C. Quantative Text Evaluation We report quantitative results of image captioning on COCOKarpathy-Test dataset and audio captioning on Audiocaps dataset. We report CLIP score, CLAP score, and CIDEr[49] on these two benchmarks. We compare against generalist models such as CoDi and Uni-Diffuser. Uni-Diffuser, released two checkpoints v0 and v1, where v1 is fine-tuned on internal data. We compare against v0 for fairness. OmniFlow outperforms CoDi on both tasks, and outperforms UniDiffuser in CIDEr score (+26.8). It has lower CLIP score (-2.5). We consider the performance of OmniFlow as competitive, considering OmniFlow is trained on signifiFigure 10. Discrete Diffusion Variant of OmniFlow. In this setup, we remove the text VAE and directly pass token embedding to the Omni-Transformer layers. [m] indicates mask token. state variants of SEDD[35] in our experiments.1 These models directly define forward process in the discrete token space, where clean text tokens are progressively replaced with special [MASK] token. We adapt our implementation for these methods by removing the text VAE and introducing token embedding layer. This design is shown in Fig. 10. B. Additional Discussions B.1. Sampling it does not directly model p(x0 OmniFlow does not directly model the marginals of two modalities. For example, given three modalities 2) = (x1, x2, x3), (cid:82) 3Rd3 p(x0 2)dA, where d3 is the dimension of x1 x1 3. Integrating over x1 3 is infeasible. Instead, we sample p(x0 3x0 3x0 1, x1 2) = (0, I) and sample p(x 2) by first sample x1 1x1 2) using path from (1,0,1) to (0,0,1). 3 q(x1 1, x1 3, x0 3x 1x0 1SEDD also has uniform variant, where the tokens are not replaced with [MASK] token, but randomly token sampled from the vocabulary. COCO-Karpathy AudioCaps Images Parms. CLAP CIDEr CLIP CIDEr BLIP-2[29] SLAM-AAC[8] 129M - OmniFlow CoDi Unidiffuser UIO2-XXL Transfusion 30M 400M 2B 1B* 3.5B Specialist - - 2.7B 7B Generalist 0.254 0.206 - - - 3.4B 4.3B 0.9B 6.8B 7B - 84.1 48.0 7.9 - 48.9 - - - 26.8 25.9 29.3 - - 145.8 - 47.3 17.2 20.5 125.4* 35.2 Table 6. X-to-Text Performance comparison on AudioCaps and COCO Captions. * UIO2s training data includes COCO. The fine-tuning dataset also includes 53M image understanding data, including 14 image captioning datasets. evaluated with official checkpoints. fine-tuned on respective datasets (COCO and Audiocaps). cantly less data than UniDiffuser and can also perform audio captioning task. We note that the performance of generalist models significantly lags behind specialist models that are fine-tuned of respective datasets, suggesting rooms for further improvements. We provide further discussion in the limitation section. D. Additional Qualitative Results D.1. Text-to-Image Fig. 14 demonstrates range of qualitative text-to-image examples for OmniFlow. We depict wide variety of people, scenes and objects to demonstrate the robustness of our approach. D.2. Image-to-Text We provide side-by-side image-to-text comparison between OmniFlow , CoDi [46] and UniDiffuser [4] using synthetic high quality images from the Midjourney Explore page [38]in Fig. 12. D.3. Audio-to-Text In Tab. 7, we show qualitative results on Audiocaps audioto-text task. OmniFlow can generation captions that match the ground truth. While CoDi can correctly grasp the main objects in the audio such as car, bird, sheep, computer, it struggles with generating captions that accurately reflect the scene. D.4. Text-VAE AutoEncoding In Tab. 8, we show reconstruction examples of Text VAE. The reconstruction mostly adheres to the semantics of the ground truth, with minor differences. For example, it may change well-furnished to well-decorated. Figure 12. Qualitative comparison of OmniFlow with baselines on image-to-text generation. Images are provided from the Midjourney Explore page [38]. E. Limitations On text generation tasks, our models performance is not state-of-the-art and has considerable room for improvements. We believe this is the side effect of incorporating large-scale data with many noisy texts of different styles (e.g. alt texts, human written prompts) that differs from the distribution of standard benchmark datasets such as MSCOCO. Additionaly, for image-to-text task specifically, OmniFlow is exposed to considerably less image-text pairs (30M) during the ID yVjivgsU2aA CoDi Four car driver trying forcoming for speeding car. OmniFlow race car engine revs and tires squeal. 8F-ndyrEWJ8 Fire police cars stop and red traffic on different highway. 350OCezayrk Four motor car driving for completing an automobile service. fire siren goes off loudly as man shouts and low hum of an engine is running throughout the whole time. vehicle engine is revving and idling. LCwSUVuTyvg Door, blue hat and winter door is being slammed. jacket. 7XUt6sQS7nM The sheep of the woman are the Multiple sheep bleat nearby. PVvi2SDOjVc smJ66Tb3c CMNlIW6Lkwc sheep of the sheep. Car going for car coming home. Three cars coming for blue car coming down road after the highway. Men in the bird while the man in the boat. Two men in the fire and two men are coming towards the other man in the game. car horn beeps. Two men talk over blowing wind and bird chirps. man speaks, followed by loud bang and people laughing. JQz40TkjymY Writing computers for people in Typing on computer keyboard. U90e2P9jy30 writing. man shouts the word to the person on the sidewalk to walk to get him to the door the hand to fall down on the sidewalk in. 5I8lmN8rwDM Stationary fire drill technician drilling down hose pipe while wearing safety gear. Railroad safety drill for motorcycle with hose or oil checking equipment. Birds on blue birds. NlKlRKz8OKI Basketballs being dribbled and people talking. drill runs continuously. GT An engine running followed by the engine revving and tires screeching. distant police siren, then racing car engine noise, and man calling in police code over his radio. motor vehicle engine starter grinds, and mid-size engine starts up and idles smoothly. Glass doors slamming and sliding shut. sheep is bleating and crowd is murmuring. car engine idles and then the horn blows. man is speaking with bird sounds in the background followed by whistling sound. man talking as camera muffles followed by loud explosion then group of people laughing and talking. Typing on computer keyboard. Several basketballs bouncing and shoes squeaking on hardwood surface as man yells in the distance. Drilling noise loud and continues. woman talks and then an animal chewing. woman speaks with flapping wings and chirping birds. Table 7. Qualitative comparisons of CoDi and OmniFlow on Audiocaps audio captioning task. Audios are randomly sampled. Audiocaps provide five ground truth captions per audio. For better presentation, we only list one in this table. training compared with previous generalist models such as CoDi(400M) and UniDiffuser(2B). There is also the question of balancing datasets of different caption qualities. For example, WavCaps is weakly-labeled dataset, but is 10x larger than higher quality AudioCaps. Additional consideration is required in order to generate captions that can achieve high scores on audiocaps benchmark. Despite these limitations, we show that OmniFlow can generate reasonable image and audio captions through quantitative and qualitative experiments. Our work focuses on develop an effective recipe for any-to-any generalist models. We leave optimizing for text generations to future works. On Image generation tasks, while OmniFlow can generate high quality images, it has the same limitations as any text-toimage models. For example, it may inherit unintended biases from the training dataset. It may also struggle in prompts that the vanilla SD3 model also struggles with. GT Reconstruction Crispy chicken tenders alongside portion of bbq sauce. Crispy chicken tenders alongside portion of bbq sauce. well-decorated living room with patterned curtain well-furnished living room with patterned curtain panel hanging from the window, small white side table rod, small white side table holding vase of flowers, holding vase of flowers, and tufted gray sofa. and tufted gray sofa. young man wearing black shirt and holding an AmerA young man wearing black shirt and holding an American flag. ican flag. An artistic painting of futuristic city by the water. An artistic painting of futuristic city by the water. Cozy and stylish living room with green velvet sofa, Cozy and well-designed living room with green velvet glass coffee table displaying potted plants, and large sofa, glass coffee table displaying potted plants, and skylight overhead. large skylight overhead. silver Acura RLX sedan driving on the passenger side silver Audi Rs4 sedan driving on the passenger side near mountainous coastline. near mountainous coastline. Table 8. Text VAE reconstruction results. We show reconstruction results (Left) and the ground truth text (Right).The reconstruction mostly adheres to the semantics of the ground truth, with minor differences. F. Miscellaneous F.1. Reproducibility of CoDi To accurately reproduce the results of CoDi [46], we follow the weights and instructions as indicated in the i-Code-V3 GitHub repository 2. However, we encounter reproducibility issues, similar to open issues reported by others, which have remained unresolved 3. G. Reproducibility Statement All dataset used in this work are public and accessible from the Internet, except for synthetic captions of SoundNet and VGGSound we generated. We will release the code, checkpoints, and generated captions for these two dataset. H. Failure Cases In Fig. 13 we present several failure cases of OmniFlow when performing text-to-image generation. In the snow globe example, the model fails to interpret the prompt specifying swirling embers instead of snowflakes, mistakenly generating snow instead. Another issue arises with the dancer, where the prompt movements blurred with motion is inaccurately represented as an additional arm. Lastly, the Koi pond and ramen examples highlight unnatural outputs, with the former resembling poorly edited image of fish in pond and the latter depicting oversized bowls of noodles placed unnaturally on the street. 2https://github.com/microsoft/i-Code/tree/main/i-Code-V3 3https://github.com/microsoft/i-Code/issues/134 Figure 13. Examples of failure cases encountered during the text-to-image generation process of OmniFlow. Figure 14. Qualitative examples of the text-to-image capability of OmniFlow."
        }
    ],
    "affiliations": [
        "Panasonic AI Research",
        "Salesforce AI Research",
        "UCLA"
    ]
}