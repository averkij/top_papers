{
    "paper_title": "Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model",
    "authors": [
        "Yueqin Yin",
        "Shentao Yang",
        "Yujia Xie",
        "Ziyi Yang",
        "Yuting Sun",
        "Hany Awadalla",
        "Weizhu Chen",
        "Mingyuan Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning from human feedback (RLHF) has been widely adopted to align language models (LMs) with human preference. Prior RLHF works typically take a bandit formulation, which, though intuitive, ignores the sequential nature of LM generation and can suffer from the sparse reward issue. While recent works propose dense token-level RLHF, treating each token as an action may be oversubtle to proper reward assignment. In this paper, we seek to get the best of both by training and utilizing a segment-level reward model, which assigns a reward to each semantically complete text segment that spans over a short sequence of tokens. For reward learning, our method allows dynamic text segmentation and compatibility with standard sequence-preference datasets. For effective RL-based LM training against segment reward, we generalize the classical scalar bandit reward normalizers into location-aware normalizer functions and interpolate the segment reward for further densification. With these designs, our method performs competitively on three popular RLHF benchmarks for LM policy: AlpacaEval 2.0, Arena-Hard, and MT-Bench. Ablation studies are conducted to further demonstrate our method."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 0 9 7 2 0 . 1 0 5 2 : r a"
        },
        {
            "title": "Segmenting Text and Learning Their Rewards for Improved RLHF\nin Language Model",
            "content": "Yueqin Yin1,2,, Shentao Yang1,*, Yujia Xie2,, Ziyi Yang2,, Yuting Sun2, Hany Awadalla2, Weizhu Chen2, and Mingyuan Zhou1, yueqin.yin@utexas.edu, shentao.yang@mccombs.utexas.edu {yujiaxie, ziyiyang, yutingsun, hanyh, wzchen}@microsoft.com mingyuan.zhou@mccombs.utexas.edu 1The University of Texas at Austin 2Microsoft Abstract Reinforcement learning from human feedback (RLHF) has been widely adopted to align language models (LMs) with human preference. Prior RLHF works typically take bandit formulation, which, though intuitive, ignores the sequential nature of LM generation and can suffer from the sparse reward issue. While recent works propose dense token-level RLHF, treating each token as an action may be oversubtle to proper reward assignment. In this paper, we seek to get the best of both by training and utilizing segment-level reward model, which assigns reward to each semantically complete text segment that spans over short sequence of tokens. For reward learning, our method allows dynamic text segmentation and compatibility with standard sequence-preference datasets. For effective RL-based LM training against segment reward, we generalize the classical scalar bandit reward normalizers into location-aware normalizer functions and interpolate the segment reward for further densification. With these designs, our method performs competitively on three popular RLHF benchmarks for LM policy: AlpacaEval 2.0, Arena-Hard, and MT-Bench. Ablation studies are conducted to further demonstrate our method."
        },
        {
            "title": "Introduction",
            "content": "To align language models (LMs, e.g., OpenAI, 2023; Reid et al., 2024) with human values, reinforcement learning (RL, Sutton and Barto, 2018) methods have been widely adopted to optimize the non-differentiable human preference, leading to the paradigm of reinforcement learning from human feedback (RLHF, Ouyang et al., 2022; Bai et al., 2022b). prevailing approach in RLHF is to optimize the LMs by proximal policy optimization (PPO, Schulman et al., 2017) against bandit reward model learned from human preference data, with KL regularization towards pre-specified target distribution to avoid over-optimization on the reward model (Ziegler et al., 2019; Stiennon et al., 2020; Castricato et al., 2022). While this bandit approach is easier for reward modeling and has achieved remarkable success, language generation is intrinsically sequential, rather than simultaneous. Thus, from the view of optimizing human preference, assigning bandit reward to entire text sequence induces the sparse reward (delayed feedback) issue (Andrychowicz et al., 2017; Marbach and Tsitsiklis, 2003), that often hurts RL-based LM training by increasing gradient variance and lowering sample efficiency (Takanobu et al., 2019; Wang et al., 2020; Guo et al., 2022; Snell et al., 2022). Equal contribution. Corresponding Author. This work was done during an internship at Microsoft. 1 As efforts to mitigate this sparse reward issue, prior works have developed methods to ground the sequence-level preference label into dense token-level reward model (Yang et al., 2023; Zhong et al., 2024). While dense per-token reward signal reduces the optimization complexity (Laidlaw et al., 2023), each action, however, is then defined as single token, i.e., sub-word that is finer-grained than word, especially with the BPE-style tokenizers (Gage, 1994; Sennrich et al., 2016). For instance, Llama 3.1s tokenizer (Dubey et al., 2024) has tokens as {Brit, ce, cod, neo, redd,...} that have less clear semantic meaning per se in any given context. The contribution of those tokens to the text sequence will inevitably depend on later tokens, making reward/credit assignment harder, especially under the prevailing RLHF paradigm of implementing the reward model as an off-the-shelf decoder-only transformer (e.g., Ouyang et al., 2022; Bai et al., 2022b; Menick et al., 2022). Further, token-level reward implicitly assumes that the basic unit of text sequence is token, which may not follow linguistics, where more meaningful decomposition of text may be phrase (including word ) that can be more semantically complete and generally consists of short sequence of tokens. To retain the optimization benefit of dense reward for RLHF, while mitigating its potential reward assignment issue and linguistic counter-intuition, in this paper, we seek to train and utilize segmentlevel reward model, which assigns reward to each semantically meaningful segment of text sequence that constitutes small number of (or just one) tokens. With this construction, we define the action space in RLHF as text segment, interpolating between the finest per token and the coarsest full sequence and potentially getting the benefit of both worlds: easier RL-based LM training owing to denser feedback and more accurate training guidance by the semantic completeness of each action. Technically, we are motivated by prior works (Malinin and Gales, 2018; Li et al., 2024a) to implement dynamic text sequence segmentation by thresholding the entropy of LMs predictive distributions, under the assumption that tokens within semantically complete text segment can be more certainly predicted by prior tokens, while the start of new segment is not (Wang et al., 2024b). To allow training the segment-level reward model by the standard sequence-preference labels via Bradley-Terry (BT, Bradley and Terry, 1952) loss, we differentiably aggregate segment rewards in text sequence into parametrized sequence evaluation. The learned segment-level reward model is then utilized in PPO-based policy learning, where we observe the unsuitability of classical reward normalizers, i.e., the mean and standard deviation (std) of full sequence rewards. We address this issue by generalizing the classical bandit normalizers of scalar mean and std into mean and std function that output the reward normalizers at arbitrary locations of the text sequence. In addition, we enhance PPO training by within-segment reward interpolation, which further densifies training signal and improves results. We test our method on the performance of the PPO-trained LM policy. On three popular RLHF benchmarks for LM policy: AlpacaEval 2.0, Arena-Hard, and MT-Bench, our method achieves competitive performance gain against both the classical bandit reward approach and recent token-level reward approach. We conduct wide array of ablation studies to verify our design choices and provide further insight into our method."
        },
        {
            "title": "2.1 Notations and Background",
            "content": "In this section, we will define generic notations, provide background on the classical bandit RLHF, and then discuss RL formulation of LM generation underlying recent efforts on dense-reward RLHF. Generic Notations. Both reward modeling and policy learning require text prompt and the corresponding response y. Reward model training turns the supervised fine-tuned model πSFT( ) (without the final unembedding layer) into parametrized scalar-output model rϕ(, ) with parameter ϕ that scores its input. The LM policy πθ, parametrized by θ, is then optimized against rϕ . Bandit Reward Model Training. Reward model training assumes dataset Dpref = {(x, yw, yl)} 2 Figure 1: Overview of training and utilizing our segment-level reward model. Numerics in the plot are artificial. In the figure, each text segment has different color, and its starting word is underscored. of prompt and the corresponding winning/chosen response yw and losing/rejected response yl, where the label of winning/losing comes from human evaluation on the entire text sequence yw and yl. In the classical bandit RLHF, reward model rϕ is trained by the binary classification BT loss Lbandit(ϕ) = E(x,yw,yl)Dpref (cid:2)log σ (cid:0)rϕ(x, yw) rϕ(x, yl)(cid:1)(cid:3) , (1) where σ(u) = 1/(1 + exp (u)) denotes the sigmoid function. PPO-based Bandit Policy Learning. In policy learning, set Dpol = {x} of text prompts is given. The LM policy πθ is trained to generate outputs on Dpol optimizing the bandit reward from rϕ , with KL penalty towards πSFT to avoid reward over-optimization. Collectively, the objective is maxθ ExDpol,yπθ( x) [rϕ(x, y) β log (πθ(y x)/πSFT(y x))] , (2) where β is the KL coefficient. In practice, for PPOs training stability, the value of rϕ(x, y) is de-mean and de-std normalized based on statistics calculated on calibration dataset, e.g., Dpref . RL Formulation of LM Generation. By its sequential nature, LM generation is formulated as Markov Decision Process (MDP) = (S, A, P, R, γ) (Sutton and Barto, 2018). Concretely, for state space S, the state at timestep t, st, consists of the prompt and all generated tokens so far a<t =: [a0, . . . , at1] with a<0 =: , i.e., st =: [x, a<t]. is the action space, where the action at at step is short-sequence/segment of tokens from the vocabulary in our segment-level reward/policy setting, whereas at is single token in the token-level reward/policy setting. Transition function deterministically appends the newly sampled tokens after the previous ones, i.e., st+1 = [st, at] = [x, at]. r(s, a) : scores the action choice (segment/token selection) at state/context and is typically substituted by the learned reward model rϕ . γ [0, 1] is the discount factor. In what follows, we will focus on our segment-level reward/policy setting where each action at is semantically complete text segment, consisting of non-deterministic number of consecutive tokens. The response for prompt then contains variable number of segments/actions, generically denoted as = [a0, . . . , aT 1] where is the number of segments in and varies across responses. When necessary, we denote single token in as yi whose generation context is [x, y<i]. Fig. 1 overviews key components in our method. detailed algorithm box is deferred to Appendix A. 3 Figure 2: Examples of data in our reward training dataset, motivating us to use Average as the aggregation function (). It is clear that the average quality of the chosen responses is better than the rejected ones, as they contain some key phrases (highlighted in red) for responding to the given prompt. We note that the choice of () ought to be task and dataset specific, as discussed in Yang et al. (2023)."
        },
        {
            "title": "2.2 Reward Model Training",
            "content": "In training our segment-level reward model, we follow the data assumption set forth in Overview. Section 2.1, where the dataset Dpref = {(x, yw, yl)} contains only binary sequence-level preference labels, without any process supervision (Uesato et al., 2022). The reward model rϕ(st, at) is configured to output scalar reward for each text segment choice at at the generation context st. rϕ is trained such that its induced parameterized text sequence evaluations, aggregated from all segment-level rewards in the respective sequence, align with the preference labels in Dpref . This is inspired by the imitation learning literature (e.g., Christiano et al., 2017; Brown et al., 2019, 2020) and prior token-level reward modeling in RLHF (Yang et al., 2023). Collectively, the BT loss for training our segment-level reward function rϕ is Lseg(ϕ) = E(x,yw,yl)Dpref [log σ (cid:0)eϕ(x, yw) eϕ(x, yl)(cid:1)], {yw, yl}, eϕ(x, y) = ({rϕ(st, at)}aty) , (3) where eϕ denotes the parameterized sequence evaluation induced by rϕ, constructed by aggregating all segment-level rewards {rϕ(st, at)}aty in the text sequence by selected aggregation function (). Below, we discuss in detail the segmentation method for text sequence and the choice of (). Entropy-based Segmentation. As discussed in Section 1, we intend to split the given text sequence {yw, yl} into semantically complete segments, so that the reward assignment to each action (segment) can be easier, especially under the common implementation of the reward model as an auto-regressive LM. Recent works on LMs (e.g., Li et al., 2024a; Wang et al., 2024b) have noticed that tokens within semantically complete segment can be more predictable by the corresponding generation context, since they are continuation of the designated semantics; while the starting token of new segment is comparably less predictable, as its semantic binding with prior words is relatively weaker. For auto-regressive LMs, the predictability of each token can be conveniently measured by the entropy of the next-token-prediction distribution from which the token is sampled (Malinin and Gales, 2018). To make text sequence segmentation one-time data pre-processing in reward model training, we choose to use the prediction distribution from the supervised fine-tuned model πSFT, from which the reward model is initialized before training. With selected entropy cutoff cent, token yi starts new segment if the Shannon entropy H() of πSFTs predictive distribution of the i-th token surpasses cent, i.e., H(πSFT( x, y<i)) > cent, in which case yi1 ends the previous segment. Choice of the Aggregation Function (). Aggregation function () provides inductive bias on the relation between the quality of each segment/action and the preferability of overall text sequence. Since probes into what kind of text sequences will be preferred, its selection should ideally be task and dataset specific, to avoid mis-specification and the subsequent unintended bias in reward model training. While several designs have been proposed in literature (Christiano et al., 2017; Kim et al., 2023; Yang et al., 2023), after looking into the dataset, in our experiments, we select Average to differentiably highlight the better average quality of the chosen responses over the rejected ones. With this choice of (), the parametrized sequence evaluation eϕ(x, y) in Eq. (3) is constructed as eϕ(x, y) = ({rϕ(st, at)}aty) = 1 (cid:80)T 1 t=0 rϕ(st, at) . (4) Fig. 2 provides example data-points from the reward training dataset in our experiments (Section 4), supporting our choice of Average as the aggregation (). Other datasets may require different (). An Alternative Interpretation. Comparing our segment-level reward training loss Eq. (3) with the classical bandit loss Eq. (1), one may alternatively interpret eϕ and ({rϕ}) in Eq. (3) as re-parametrization of the learned sequence-level feedback that differentiably aggregates the quality/contribution of each text segment, and thereby connects denser evaluation rϕ of each semantically complete text segment with the information in ground-truth sequence-level preference label."
        },
        {
            "title": "2.3 PPO-based Policy Learning",
            "content": "In policy learning, we again follow the classical bandit setting in Section 2.1 to Overview. Overview. optimize the LM policy πθ on given prompt set Dpol = {x}. But unlike the bandit objective in Eq. (2), we adopt the full RL setting (Sutton and Barto, 2018) to maximize πθs expected sum of per-segment/step rewards. This enables directly plugging our segment-level reward model rϕ into most off-the-shelf RLHF PPO implementation. With this, the policy learning objective for πθ is maxθ xDpol,y(cid:81)T 1 t=0 πθ(at st) (cid:104)(cid:80)T 1 t=0 rϕ(st, at) β log (πθ(y x)/πSFT(y x)) (cid:105) , (5) where again, each at is segment of tokens (chopped by πSFT), st = [x, a0, . . . at1] is the generation context at step t, and = [a0, . . . , aT 1] is the response to prompt sampled from the learning πθ. Recall from Section 2.1 that the output values from the reward model rϕ need to be normalized for the stability of PPO training. With our segment-level reward model, it is no longer suitable to normalize each rϕ(st, at) by the mean and std of entire sequences rewards as in the bandit setting, since the latter may not be on proper scale and/or well-defined. Further, the on-policy nature of PPO induces an extra complexity: each step of PPO samples new text sequences, whose total length, segment lengths, and segment locations are all stochastic and can differ from the reward calibration dataset, e.g., Dpref . Appendix provides an extended discussion on reward normalization in PPO-based LM training. Below, we discuss our approach to construct the reward value normalizers, followed by interpolating the segment-level reward into per-token signal to helpfully provide an even denser training guidance. Location-aware Reward Normalizers via Regression. While the length of the sampled response and the lengths and locations of segments {at} in are all stochastic, we know that each at is somewhere in y. Correspondingly, each input (st, at) to rϕ is linked to normalized location (0, 1] of y, and can be simply defined as t/T , where is the index of the segment at in y, since PPO routine has fully sampled y. On each datapoint in the calibration set, normalized location (0, 1] again, with the linked segment-level reward available. Across all data points in the calibration set, we construct new dataset Dnorm = {(p, µp, σp)}, where runs over all values of normalized location in the calibration set, µp and σp respectively denote sample mean and sample std of all segment-level rewards corresponding to in the calibration set. With Dnorm, we run simple linear regression to estimate the relation between the log-transformed normalized location log(p) and the mean/std of segment-level rewards at p. Specifically, the regression formula is given by: Mean(p) = wµ log(p) + bµ, Std(p) = wσ log(p) + bσ, (6) 5 where the independent variable is log(p) and the regression coefficients (wµ, bµ) and (wσ, bσ) can be calculated in closed form. Note that the classical bandit normalizers of the mean and std of full sequences rewards correspond to evaluate Mean(p) and Std(p) at = 1.0. In this regard, our mean and std functions in Eq. (6) generalize the classical scalar normalizers into location-aware functions able to output proper reward normalizers at an arbitrary (normalized) location of the text sequence. With Mean() and Std() and the corresponding p, rϕ(st, at) is normalized by rϕ(st, at) (rϕ(st, at) Mean(p))/Std(p). Within-segment Reward Interpolation. Depending on the specific tokenizer in use, we observed that semantically complete text segments may contain up to around twenty tokens. The corresponding action space might still be large and the resulting segment-level design might not sufficiently address the sample inefficiency issue in the classical bandit RLHF and could again lead to inferior PPO-based RL training. To further densify the RL training signal, we evenly split the segment-level reward rϕ(st, at) for segment at to each token yi at. This induces token-level credit assignment that yi at, (cid:101)rϕ([x, y<i], yi) = rϕ(st, at)/at, where [x, y<i] is the generation context of token yi. (cid:101)rϕ can then directly substitute rϕ in Eq. (5), since (cid:80)T t=0 rϕ(st, at) = (cid:80)T 1 Note that t=0 ((cid:80) rϕ(st, at)/at). yiat (cid:101)rϕ is still intrinsically segment level, since all token selections yi within segment at receive the same feedback, i.e., the average of segment-level reward rϕ(st, at). This is in contrast to prior works on token-level reward models (Yang et al., 2023; Zhong et al., 2024), where each token selection is evaluated separately and thus their token-level feedback/rewards vary for each token. Summary. With the learned segment-level reward model rϕ from Section 2.2, in PPO training of the LM policy πθ, we first normalize each rϕ(st, at) in the sampled sequence by the corresponding normalizers Mean(p) and Std(p). Normalized segment-level rewards are then interpolated into the per-token feedback (cid:101)rϕ directly into an off-the-shelf RLHF PPO routine. signal (cid:101)rϕ. Finally, we plug"
        },
        {
            "title": "3 Related Work",
            "content": "In the classical RLHF paradigm, policy LM is optimized against bandit Reward Models in RLHF. reward model trained firstly by binary classification loss on the preference dataset, with KL penalty to specified prior distribution to avoid reward over-optimization (Ziegler et al., 2019; Stiennon et al., 2020; Jaques et al., 2020; Bai et al., 2022a; Ouyang et al., 2022; Castricato et al., 2022). Under the same bandit formulation, recent works have enhanced the bandit reward model by directly modeling the probability of one response being preferred over the other (Jiang et al., 2023; Zhao et al., 2023; Liu et al., 2023; Dong et al., 2024) or factorizing human preference into multiple facets via multi-objective modeling (Touvron et al., 2023; Wang et al., 2023, 2024c,a). Despite its popularity, from the angle of RL-based optimization of human preference captured by the reward model, such bandit reward may lead to inferior training, due to the sparse reward issue intrinsic to the bandit formulation of LM generation and credit assignment (e.g., Takanobu et al., 2019; Guo et al., 2022). Viewing the weakness of bandit RLHF, efforts have been making to densify the reward signal for RLHF LM training. Yang et al. (2023) and Chan et al. (2024) train token-level reward models by the binary preference classification loss. Zhong et al. (2024) and Rafailov et al. (2024) use an LM trained by DPO (Rafailov et al., 2023) firstly for token-level reward assignment, which is later used in PPO training or search-based algorithms. Guo et al. (2023), Cao et al. (2024), and Yoon et al. (2024) assign continuous or fixed fine-grained rewards (e.g., 1) by accessing an external powerful large LM or the oracle environmental reward; while Chen et al. (2024) require the extra task and datasets of erroneous solution rewriting. Apart from potential extra requirements, as discussed in Section 1, the semantic incompleteness of token in text may challenge the efficacy of per-token credit assignment, especially with the prevailing implementation of reward model as decoder-only transformer that cannot look ahead into later tokens. In contrast, by defining text segment as the basic unit of text sequence that can be semantically more complete than token, our segment-level reward may provide more accurate guidance for RL-based LM training, while not losing the benefit of denser 6 feedback. Close to our segment-level reward, process reward models (PRMs, e.g., Uesato et al., 2022; Lightman et al., 2023) in reasoning-alike tasks also assign single reward value to short sequence of tokens. The training of PRMs, however, typically requires human annotation on each step of the reasoning-alike process. This may not be feasible in general text generation tasks, e.g., text summarization or dialog, where each step/text segment lacks clear human evaluation criterion while the full generations can be more easily compared or evaluated. By contrast, as seen in Section 2, our method is developed for the most basic yet general RLHF setting, where human preference is only manifested in dataset of binary sequence-level preference. And the dataset is collected from multiple sources and contains multiple forms of prompt-responses. Learning-from-preference. Learning-from-preference classically takes two-stage approach where reward model is first trained on dataset of binary or multiple ranking via maximizing the choice model likelihood (Bradley and Terry, 1952; Plackett, 1975; Luce, 2012), before optimizing the RL/control policy against the learned reward model by RL algorithms (Akrour et al., 2011, 2012; Fürnkranz et al., 2012). Earlier application in deep learning mainly focuses on relatively simple neural-network policy for robotics/control tasks (Christiano et al., 2017; Ibarz et al., 2018; Bıyık et al., 2019; Brown et al., 2019, 2020; Lee et al., 2021; Shin et al., 2021; Hejna and Sadigh, 2023a,b). Implanting its success in robotics, in natural language generation, this two-stage learning-from-preference paradigm has been scaled up and popularized in the post-training stage to align LMs with specific human values, with applications ranging from text summarization (Ziegler et al., 2019; Stiennon et al., 2020), prompt generation (Yang et al., 2023), to (task-oriented) conversational agent (e.g., Ouyang et al., 2022; Bai et al., 2022a; Menick et al., 2022; Feng et al., 2023; OpenAI, 2023), and with the RL paradigm of both model free (Levine et al., 2020; Yang et al., 2022b) and model based (Yang et al., 2022a,c). To alleviate the complexity in fitting an explicit reward model, motivated by the theory of maximumentropy control and RL (Ziebart et al., 2008; Ziebart, 2010; Finn et al., 2016), direct preference optimization methods (DPO, e.g., Rafailov et al., 2023; Tunstall et al., 2023; Azar et al., 2023; Yuan et al., 2023; Zhao et al., 2023; Ethayarajh et al., 2024; Yin et al., 2024) were recently proposed to directly train LMs on preference dataset by using their log-density-ratio as the classification logit, which have been adapted to train text-to-image diffusion models (e.g., Wallace et al., 2023; Yang et al., 2024; Li et al., 2024b; Gu et al., 2024). In this paper, we contribute to the literature of learning-from-preference by re-thinking suitable definition of action space in the RL formulation of LM generation and preference alignment. Motivated by semantic completeness in linguistics, we define each action as text segment, spanning across small amount of tokens and interpolating between prior works action space of either the finest per token or the coarsest full sequence. With this design, our method may benefit from both denser reward signal for easier RL-based LM training and the semantic completeness of each action for more accurate training guidance, as experimentally verified in Section 4. Training Signals for RL-based Language Model (LM) Training. In RL-based LM fine-tuning, classical training signal for adapting LMs to the specific downstream task is the native trajectory-level downstream test metrics (e.g., Ryang and Abekawa, 2012; Ranzato et al., 2015; Rennie et al., 2017; Paulus et al., 2017; Shu et al., 2021; Lu et al., 2022). This approach intrinsically uses bandit formulation of LM generation that treats the entire generated sequence as single action. As discussed in Section 1, ignoring the sequential nature of LM generation, this bandit training signal delays the feedback to each token/phrase selection, and can thus incur optimization difficulty (Guo et al., 2022; Snell et al., 2022). With various forms of stronger data or compute requirements, task-specific per-step training signals have been proposed to mitigate this sparse reward issue. Assuming abundant golden expert data for supervised (pre-)training, Shi et al. (2018) construct per-step reward via inverse RL (Russell, 1998); Guo et al. (2018) use hierarchical approach; Yang et al. (2018) learn LM discriminators; Lin et al. (2017) and Yu et al. (2017) use the expensive and high-variance Monte Carlo rollout to estimate per-step reward from sequence-level adversarial reward function trained in the first place; while Le et al. (2022) use some rule-based intermediate training signal derived from the oracle sequence-level evaluation, without explicitly learning per-step reward. 7 Similarly, in RLHF, to move forward from the classical bandit formulation, methods have recently been proposed to ground sparse preference labels into dense per-step feedback, with applications in task-oriented dialog systems (e.g., Ramachandran et al., 2021; Feng et al., 2023) and variable-length text-sequence generation (Yang et al., 2023). Our paper seeks to reconcile dense v.s. sparse training signal in RLHF by distributing feedback to the level of semantically complete text segment, interpolating between the densest token level and the sparsest sequence level and ideally getting the benefit of both worlds: easier RL training and accurate optimization signal. Meanwhile, as seen in Section 2, our method adheres to the classical two-stage RLHF paradigm without requiring extra data or compute. Other LM Preference Alignment Methods. Apart from RL methods, strategies have been developed to align LMs with preference by adding external filters on top of the pretrained LMs, for safety checking the generations or the training texts (e.g., Xu et al., 2020). Vanilla maximum likelihood estimation has also been adopted for training LMs on curated datasets (Hancock et al., 2019; Solaiman and Dennison, 2021; Scheurer et al., 2022), or instruction fine-tuning LMs on massive highly-curated sets of tasks (Sanh et al., 2022; Chung et al., 2022). With extra requirements on data, modelling, and/or compute, recent LM works also conduct preference alignment by formulating text generation as constraint satisfaction problem on LMs generation distribution (e.g., Khalifa et al., 2021; Korbak et al., 2022; Go et al., 2023), or utilizing the preference dataset in LMs pre-training stage (Korbak et al., 2023). In this paper, we seek to refine RL-based LM preference alignment by re-thinking the suitable action space in the RL formulation that allows both denser immediate feedback while not jeopardizing the feedback accuracy. Our segment-level design is validated through numeric and example in Section 4."
        },
        {
            "title": "4.1 Experimental Setups and Implementation",
            "content": "Datasets. For reward model training, we use the preference-700K dataset1, which is diverse collection of open-source preference datasets, such as HH-RLHF (Bai et al., 2022a), Stanford Human Preferences Dataset (SHP) (Ethayarajh et al., 2022), and HelpSteer (Wang et al., 2023). PPO-based LM policy training is conducted on the Ultrafeedback dataset (Cui et al., 2023), from which we only use the prompts to sample responses during the PPO training routine. Benchmarks and Evaluations. The (PPO-trained) LM policy is evaluated on three popular openended instruction-following benchmarks: AlpacaEval 2.0 (Li et al., 2023), Arena-Hard (Li et al., 2024c), and MT-Bench (Zheng et al., 2023), where GPT-4o is used as the judge. Our reported scores follow each benchmarks default protocol. AlpacaEval 2.0 consists of 805 test cases, on which we report the length control win rate (LC), raw win rate (WR), and the average response length in number of characters (# char). The LC metric is specifically designed to be robust against model verbosity. We follow the convention (e.g., Dong et al., 2024) to employ alpaca_eval_gpt4_turbo_fn as the annotator for AlpacaEval 2.0. Arena-Hard consists of 500 challenging user queries, and we report both the win rate (WR) against the reference model GPT-4-0314 and the average number of tokens in the responses (# token). MT-Bench is multi-turn dialogue dataset that includes 80 test cases. We report the average MT-Bench scores for two dialogue turns, using GPT-4o as the judge model. Implementation. We implement our method onto the open-sourced 3.8B Phi3-mini Instruct (Abdin et al., 2024) and the SFT checkpoint of Phi3.1-mini Instruct, as well as the popular SFT checkpoint of Llama-3-8B (Dubey et al., 2024) released by RLHFlow (Dong et al., 2024)2. The backbone model is used as the starting points of both reward model training and PPO-based LM policy learning, in the latter initializing the models for value function, learning policy, and reference policy. Our implementation is built upon the open-source RLHF framework OpenRLHF (Hu et al., 2024). We maximally follow the default 1https://huggingface.co/datasets/hendrydong/preference_700K 2https://huggingface.co/RLHFlow/LLaMA3-SFT-v2 8 Table 1: Performance comparison among different action definitions on PPO-trained LM policy, with the backbone model being Phi3-mini Instruct. # {char, token} measures the average response length in the benchmark tests. Highest value of each column is in bold. Action Definition Phi3-mini Instruct Bandit (Sequence) Sentence Token Segment (Ours) Bandit as Segment Segment as Bandit AlpacaEval 2. Arena-Hard MT-Bench LC(%) WR(%) # char WR% # token GPT-4o 18. 27.05 25.56 27.82 14.41 29.07 32.92 26.46 31.05 34.53 14.39 27. 6.46 28.20 1473 2164 2626 1940 2257 691 25.1 31.3 32.8 27.2 34.0 11.1 30.9 490 671 533 593 308 620 7.33 7.46 7.51 7.58 7. 6.61 7.38 hyperparameters in OpenRLHF. For example, in PPO training, we retain the default KL coefficient value β = 0.01. In the main experimental results, we train the reward model for one epoch and the PPO model for two epochs, i.e., two passes through the dataset, using entropy cutoff cent = 1.75. Section 4.3 presents ablation studies on the choice of cent. Due to space limit, we defer further implementation details to Appendix C. For reproducibility, our source code and model checkpoints are released."
        },
        {
            "title": "4.2 Main Experimental Comparisons",
            "content": "Baselines. To demonstrate our unique consideration of RLHFs action space, in the main experiment, we compare our design of segment-level action space with the coarsest bandit/sequence-level action space, the coarser sentence-level space, and the finest token-level space, in terms of performance of the PPO-trained LM policy. For PPO training, corresponding reward model is first trained under the specified action definition. The sentence-level models are implemented by splitting the text sequences using sentence splitters {\".\", \"!\", \"?\", \"n\", \";\", \"...\", \",\", \":\"} and/or their foreign language equivalents. To further illustrate our segment-level reward model and denser segment-level reward assignment, we additionally compare with two hybrid approaches: (A) using the bandit reward model for segment-level reward assignment in the PPO training (Bandit as Segment); and (B) using the segment-level reward model only for bandit reward assignment in the PPO training (Segment as Bandit), where the bandit reward is implemented by the parametrized sequence evaluation eϕ in Eq. (4). For all baselines, we follow the standard training receipts and tune them to the extent of ensuring fair comparison. Results. Table 1 compares the performance of our PPO-trained LM policy with other definitions of RLHFs action space, as well as the two hybrid approaches, under the backbone model of Phi3-mini Instruct. Our key findings are summarized as follows. (1) Our segment-level approach improves RLHF training while not suffering from length hacking. As seen in Table 1, our LM policy performs better than the baselines across all three RLHF policy evaluation benchmarks: AlpacaEval 2.0, Arena-Hard, and MT-Bench. Notably, our models average response length on AlpacaEval 2.0 and Arena-Hard is not significantly larger than the baseline models, in contrast to the LM policy from the sentence-level action space. Together, these results manifest the merit of our segment-level approach in truly improving the quality of the generated responses while not cheating the benchmark evaluations by response-length hacking (Dubois et al., 2024). As discussed in Section 1, we attribute the gain of our segment-level approach over the baselines to simultaneously achieving both denser reward signals for PPO-based RLHF training and more accurate reward assignment by the design of semantically complete action space. (2) Not all finer action spaces can help RLHF training over the classical bandit formulation. Apart from our denser segment-level approach, in Table 1, we see that the other two finer action space specifications: Table 2: Performance comparison among different action definitions on PPO-trained LM policy, with the backbone model being the 3.8B SFT checkpoint of Phi3.1-mini Instruct. Table format follows Table 1. AlpacaEval 2.0 Arena-Hard MT-Bench Action Definition LC (%) WR (%) # char WR (%) # token GPT-4o Phi3.1-mini SFT Bandit (Sequence) Token 14.93 19.39 22.48 10.19 14.78 19.25 1271 1542 1687 Segment (Ours) 26.19 23.85 1795 14.5 19.5 23.2 28.5 476 524 585 7.00 7.26 7.43 7.49 Table 3: Performance comparison among different action definitions on PPO-trained LM policy, with the backbone model being the 8B SFT checkpoint of Llama-3 released by RLHFlow. Table format follows Table 1. Action Definition AlpacaEval 2. Arena-Hard MT-Bench LC (%) WR (%) # char WR (%) # token GPT-4o Llama-3-8B-SFT Bandit (Sequence) Token 16.31 21.20 23.84 9.50 20.99 20.87 1221 2218 1744 Segment (Ours) 25.11 28. 2264 10.4 18.7 26.0 30.4 469 513 622 6.82 7.11 7.13 7.15 per-sentence and per-token, both fail to generally improve over the classical design of bandit/sequence-level action space, especially on AlpacaEval 2.0 and Arena-Hard. This provides an extra verification to our receipt of segment-level action space for RLHF PPO training, which provides more break-down action definition compared to the coarser sentence level, and more complete semantics compared to the token-level design. (3) segment-level reward model is necessary for segment-level reward assignment, vice versa. One may wonder if we can use the classical bandit reward model to assign segment-level reward in the PPO training. As shown by the results of Bandit as Segment in Table 1, this approach performs significantly worse than the original pure bandit, which in turn under-performs our segment-level design. These comparisons justify the necessity to train segment-level reward model for segment-level reward assignment. Conversely, using our segment-level reward model to provide only bandit feedback in PPO training (Segment as Bandit) leads to slight performance degradation over pure bandit design. Compared with our main results, we see that Segment as Bandit does not fully benefit from our proposal of (consistent) segment-level action space. Its weaker results again highlight the gain of denser reward assignment in PPO-based RLHF training. (4) The benefit of segment-level design extends to SFT model and the larger 8B model. We swap the backbone model to the SFT checkpoint of Phi3.1-mini Instruct, as shown in Table 2, and to the larger 8B SFT checkpoint of Llama-3, as presented in Table 3. It is clear the the gain of our segment-level action space over the prior bandit and token-level design is not scoped within the already DPOed Phi3-mini Instruct. Rather, our advantage extends to both the SFT checkpoint of Phi3.1-mini Instruct and the larger Llama-3-8B-SFT, which verifies the value and versatility of our method in the practical post-training pipeline. Appendix B.2 provides generation examples from our main LM policy. Table 6 in Appendix B.1 compares the LM policies in Table 1 on tasks on OpenLLM Leaderboard. Both show that our method, while achieving strong RLHF training, does not suffer from the alignment tax (Askell et al., 2021)."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "This section considers the following four research questions to better understand our method. To save compute, all ablation studies are conducted on the 3.8B Phi3-mini Instruct used in Table 1. 10 Figure 3: Examples of dense reward assignment for text sequences encountered in PPO training, comparing our segment-level reward model and the recent token-level design on normal text (Top) and text with verbosity/repetition underlined. (Bottom). Darker color indicates higher reward. In the bottom half, repeated sentences are (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) Table 4: Comparison of different constructions of segment-level reward normalizers, on performance of the resulted PPO-trained LM policies. Highest numeric of each metric is in bold. Reward Normalizer AlpacaEval 2. Arena-Hard MT-Bench LC (%) WR (%) # char WR (%) # token GPT-4o No Reward Normalization Global Statistics of All Statistics of the Last Rewards 19.64 17.34 20.30 24.10 22.11 24.72 2446 2420 2551 Regression-based (Section 2.3) 31.05 34. 2257 29.9 31.3 29.2 34.0 625 639 641 593 7.25 7.14 7. 7.65 (a): Can our segment-level reward model reasonably distinguish key segments? In Fig. 3, we qualitatively compare dense reward assignments from our segment-level reward model versus the recent token-level approach on two text sequences appeared in PPO training, representing normal text (Top) and broken text with verbosity/repetition (Bottom), where repetitions are underlined. The color blocks in Fig. 3 demonstrate that our entropy-based approach segments the text into meaningful semantic units, whereas token may represent only part of word. Further, Fig. 3 confirms that our segment-level reward model assigns higher rewards to key segments in the responses. Meanwhile, we see that the token-level model does not have consistent reward assignment over even word often less understandably (only) highly rewards some letters of word (e.g., Truths, meditation, compassion, achieving). The benefit of our motivation of semantically complete action space is further manifested by the example of repeated sentences in Fig. 3 Bottom, where our model assigns consistent low rewards to the repeated sentences, effectively refraining the LM from verbosity/repetition. By contrast, due to the semantic incompleteness of each action, the token-level reward model still assigns high rewards to tokens in the repetitions, even in the second repeat, which is undoubtedly undesirable. (b): How will PPO training perform if we use different constructions of reward normalizers? Recall that in our PPO training (Section 2.3), we use simple linear regression to fit location-aware mean and std functions that provide reward normalizers at arbitrary locations of the text sequence. To study if this design is over-engineering, we compare our main method with three simpler constructions of segment-level reward normalizers: (A) no reward normalization; (B) using the scalar global mean and std over all segment-level rewards in the reward calibration dataset; and (C) using the scalar mean and std 11 Table 5: Comparison of different within-segment reward interpolation strategies. Shown are the results of the resulted PPO-trained LM policies on AlpacaEval 2.0 and Arena-Hard. Highest numeric of each metric is in bold. Interpolation Strategy AlpacaEval 2.0 Arena-Hard MT-Bench LC (%) WR (%) # char WR (%) # token GPT-4o No Interpolation Repeat Segment Reward 25.98 26.34 34.53 23.48 2666 1795 Even Split (Section 2.3) 31.05 34.53 2257 39.1 23.0 34.0 675 593 7.45 7.42 7.65 over the last segment-level rewards in each response of the calibration set, mimicking the normalizers in the classical bandit approach. Table 4 compares the resulted LM policies. In Table 4, we clearly see that normalizing (dense) reward by improper reward statistics is akin to no reward normalization, as all three baselines have significantly lower benchmark scores that our regression-based approach and undesirable longer response lengths. As discussed in details in Appendix E, the linguistic structure of the response leads to certain correlation between the mean and std of segment-level reward values and the normalized location of segment in the response, e.g., in the early or middle or later part. This necessitates our design of location-aware reward normalizers that are able to capture the reward statistics at each arbitrary location of the sampled text sequence, since constant normalization statistics can be insufficient to properly normalize the rewards of text segments at different parts of the text sequence, as verified in Table 4. Future work may extend our linear regression-based normalizer functions in Section 2.3 with non-linearity and/or more features. (c): What will happen if we use different strategies for within-segment reward interpolation? Recall from Section 2.3 that, to further densify the learning signal in RLHF for enhancing training, we interpolate the segment-level rewards by evenly splitting the reward of segment to each of its constituting token. We now compare this even-split interpolation strategy with two other intuitive alternatives: (A) no interpolation on the segment-level rewards, use 0 for technical padding in PPO (No Interpolation); (B) repeat the segment-level reward of segment to each token in it (Repeat Segment Reward). Table 5 shows the performance of the resulted PPO-trained LM policies. In conjunction with our main result Table 1, in Table 5, we see that these two alternatives still provide (relatively) effective RLHF training on Phi3.1-mini Instruct, in reference to the results of the classical bandit approach in Table 1. Nevertheless, we see that the generation length from No Interpolation is significantly longer, while Repeat Segment Reward is too short. Probing into those long text sequences encountered in PPO training, we found that they typically contain some very negative segment-level rewards that refrains the behavior of long generation from being learned by the policy LM. Such very negative reward signals may be diluted by the technical zero-padding in No Interpolation, leading to overly long text generation, whereas they are overly amplified in Repeat Segment Reward, resulting in too-strong punishment for long texts and hence too-short generations. By contrast, the even-split interpolation strategy in our main method provides densified learning signal of proper scale, which we attribute to the implicit (segment-) length normalization inherited from the operation of dividing by segment length in an even split. Future work may design proper non-even split of segment-level reward over each token in the text segment. (d): With different entropy cutoff cent for text segmentation, how will our method perform? As discussed in Section 4.1, for main results, we use entropy cutoff cent = 1.75 for entropy-based text segmentation. To investigate the impact of cent, in Fig. 4, we vary the value of cent {1.5, 1.75, 2.0, 2.25}, and compare the performance of the resulted PPO-trained LM policies as well as the average segment length of the PPO-trained LM policy. As seen in Fig. 4, similar to the discussion of token-level approach in Section 1, smaller cent = 1.5, which chops text sequence into finer pieces with smaller average segment length, may result in semantically less complete segments, leading to less accurate reward modeling and the subsequent weaker LM policy. reasonably larger entropy cutoff, such as cent [1.75, 2.25] that corresponds to an average segment length of 12 (a) Avg. Seg. Len (token) (b) AlpacaEval 2 (LC) (c) AlpacaEval 2 (WR) (d) Arena-Hard (e) MT-Bench Figure 4: Performance comparison among different entropy cutoffs cent for entropy-based text segmentation, comparing PPO-trained LM policys benchmark scores and average segment length (Avg. Seg. Len) in terms of number of tokens. 10 to 22 in Fig. 4a (or about 3 to 7 words), leads to much better PPO-trained LMs. This coincides with the advantage of our segment-level design over the prior token-level design in Tables 1-3 and verifies our goal of more semantically complete action space."
        },
        {
            "title": "5 Conclusion and Limitations",
            "content": "In this paper, we propose to train and utilize segment-level reward model for improved RLHF in LMs, motivated by both denser reward signal in RL-based LM training and semantic completeness of each action for accurate reward assignment. Our method and insight are validated through extensive experiments, ablation studies, and backbone models of different sizes. Nevertheless, as an early effort in reconsidering the action space in RLHF, our experiments are currently confined to PPO training on free-form dialog-alike dataset and instruction-following benchmark evaluations. Our future work includes scaling up to even larger LMs, testing our method on other types of tasks such as math problem solving and code generation, and applying to other RL algorithms, such as REINFORCE."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Riad Akrour, Marc Schoenauer, and Michele Sebag. Preference-based policy learning. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2011, Athens, Greece, September 5-9, 2011. Proceedings, Part 11, pages 1227. Springer, 2011. Riad Akrour, Marc Schoenauer, and Michèle Sebag. April: Active preference learning-based reinforcement learning. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2012, Bristol, UK, September 24-28, 2012. Proceedings, Part II 23, pages 116131. Springer, 2012. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Advances in neural information processing systems, 30, 2017. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. general language assistant as laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. general theoretical paradigm to understand learning from human preferences. arXiv preprint arXiv:2310.12036, 2023. 13 Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b. Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open LLM leaderboard. Hugging Face, 2023. Erdem Bıyık, Daniel Lazar, Dorsa Sadigh, and Ramtin Pedarsani. The green choice: Learning and influencing human decisions on shared roads. In 2019 IEEE 58th conference on decision and control (CDC), pages 347354. IEEE, 2019. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Daniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond suboptimal In International conference on demonstrations via inverse reinforcement learning from observations. machine learning, pages 783792. PMLR, 2019. Daniel Brown, Wonjoon Goo, and Scott Niekum. Better-than-demonstrator imitation learning via automatically-ranked demonstrations. In Conference on robot learning, pages 330359. PMLR, 2020. Meng Cao, Lei Shu, Lei Yu, Yun Zhu, Nevan Wichers, Yinxiao Liu, and Lei Meng. Drlc: Reinforcement learning with dense rewards from llm critic. arXiv preprint arXiv:2401.07382, 2024. Louis Castricato, Alexander Havrilla, Shahbuland Matiana, Michael Pieler, Anbang Ye, Ian Yang, Spencer Frazier, and Mark Riedl. Robust preference learning for storytelling via contrastive reinforcement learning. arXiv preprint arXiv:2210.07792, 2022. Alex Chan, Hao Sun, Samuel Holt, and Mihaela van der Schaar. Dense reward for free in reinforcement learning from human feedback. arXiv preprint arXiv:2402.00782, 2024. Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. Zhipeng Chen, Kun Zhou, Wayne Xin Zhao, Junchen Wan, Fuzheng Zhang, Di Zhang, and Ji-Rong Wen. Improving large language models via fine-grained reinforcement learning with minimum editing constraint. arXiv preprint arXiv:2401.06081, 2024. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023. Dao, DY Fu, Ermon, Rudra, and Flashattention Ré. Fast and memory-efficient exact attention with io-awareness. URL https://arxiv. org/abs/2205.14135, 2022. 14 Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with V-usable information. In International Conference on Machine Learning, pages 59886008. PMLR, 2022. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Yihao Feng, Shentao Yang, Shujian Zhang, Jianguo Zhang, Caiming Xiong, Mingyuan Zhou, and Huan Wang. Fantastic rewards and how to tame them: case study on reward learning for task-oriented dialogue systems. In The Eleventh International Conference on Learning Representations, 2023. Chelsea Finn, Paul Francis Christiano, P. Abbeel, and Sergey Levine. connection between generative adversarial networks, inverse reinforcement learning, and energy-based models. ArXiv, abs/1611.03852, 2016. Johannes Fürnkranz, Eyke Hüllermeier, Weiwei Cheng, and Sang-Hyeun Park. Preference-based reinforcement learning: formal framework and policy iteration algorithm. Machine learning, 89:123156, 2012. Philip Gage. new algorithm for data compression. The Users Journal, 12(2):2338, 1994. Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Nahyeon Ryu, and Marc Dymetman. Aligning language models with preferences through f-divergence minimization. arXiv preprint arXiv:2302.08215, 2023. Yi Gu, Zhendong Wang, Yueqin Yin, Yujia Xie, and Mingyuan Zhou. Diffusion-rpo: Aligning diffusion models through relative preference optimization. arXiv preprint arXiv:2406.06382, 2024. Geyang Guo, Ranchi Zhao, Tianyi Tang, Wayne Xin Zhao, and Ji-Rong Wen. Beyond imitation: Leveraging fine-grained quality signals for alignment. arXiv preprint arXiv:2311.04072, 2023. Han Guo, Bowen Tan, Zhengzhong Liu, Eric Xing, and Zhiting Hu. Efficient (soft) q-learning for text generation with limited good data. Findings of the Association for Computational Linguistics: EMNLP 2022, pages 69696991, 2022. Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. Long text generation via adversarial training with leaked information. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, and Jason Weston. Learning from dialogue after deployment: Feed yourself, chatbot! arXiv preprint arXiv:1901.05415, 2019. Donald Joseph Hejna and Dorsa Sadigh. Few-shot preference learning for human-in-the-loop rl. In Conference on Robot Learning, pages 20142025. PMLR, 2023a. Joey Hejna and Dorsa Sadigh. Inverse preference learning: Preference-based rl without reward function. arXiv preprint arXiv:2305.15363, 2023b. 15 Jian Hu, Xibin Wu, Weixun Wang, Xianyu, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward learning from human preferences and demonstrations in atari. Advances in neural information processing systems, 31, 2018. Natasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Shane Gu, and Rosalind Picard. Human-centric dialog training via offline reinforcement learning. arXiv preprint arXiv:2010.05848, 2020. Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023. Muhammad Khalifa, Hady Elsahar, and Marc Dymetman. distributional approach to controlled text generation. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=jWkw45-9AbL. Changyeon Kim, Jongjin Park, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. Preference transformer: Modeling human preferences using transformers for RL. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Peot1SFDX0. Diederik P. Kingma and Jimmy Ba. Adam: Method for Stochastic Optimization. In International Conference on Learning Representations, 2014. Tomasz Korbak, Hady Elsahar, Germán Kruszewski, and Marc Dymetman. On reinforcement learning and distribution matching for fine-tuning language models with no catastrophic forgetting. arXiv preprint arXiv:2206.00761, 2022. Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher Buckley, Jason Phang, Samuel Bowman, and Ethan Perez. Pretraining language models with human preferences. arXiv preprint arXiv:2302.08582, 2023. Cassidy Laidlaw, Stuart Russell, and Anca Dragan. Bridging rl theory and practice with the effective horizon. arXiv preprint arXiv:2304.09853, 2023. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:2131421328, 2022. Kimin Lee, Laura M. Smith, and P. Abbeel. Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training. In International Conference on Machine Learning, 2021. URL https://api.semanticscholar.org/CorpusID:235377145. Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. Bolian Li, Yifan Wang, Ananth Grama, and Ruqi Zhang. Cascade reward sampling for efficient decoding-time alignment. arXiv preprint arXiv:2406.16306, 2024a. Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Yusuke Kato, and Kazuki Kozuka. Aligning diffusion models by optimizing human utility. arXiv preprint arXiv:2404.04465, 2024b. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The arena-hard pipeline, 2024c. 16 Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models, 2023. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, and Ming-Ting Sun. Adversarial ranking for language generation. Advances in neural information processing systems, 30, 2017. Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657, 2023. Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning. arXiv preprint arXiv:2205.13636, 2022. Duncan Luce. Individual choice behavior: theoretical analysis. Courier Corporation, 2012. Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. Advances in neural information processing systems, 31, 2018. Peter Marbach and John Tsitsiklis. Approximate gradient methods in policy-space optimization of markov reward processes. Discrete Event Dynamic Systems, 13:111148, 2003. Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching language models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022. OpenAI. Gpt-4 technical report, 2023. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. Romain Paulus, Caiming Xiong, and Richard Socher. deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017. Robin Plackett. The analysis of permutations. Journal of the Royal Statistical Society: Series (Applied Statistics), 24(2):193202, 1975. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= HPuSIXJaa9. Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From to Q: Your language model is secretly Q-function. arXiv preprint arXiv:2404.12358, 2024. Govardana Sachithanandam Ramachandran, Kazuma Hashimoto, and Caiming Xiong. Causal-aware safe policy improvement for task-oriented dialogue. arXiv preprint arXiv:2103.06370, 2021. MarcAurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015. 17 Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Steven Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 70087024, 2017. Stuart Russell. Learning agents for uncertain environments. In Proceedings of the eleventh annual conference on Computational learning theory, pages 101103, 1998. Seonggi Ryang and Takeshi Abekawa. Framework of automatic text summarization using reinforcement learning. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 256265, Jeju Island, Korea, July 2012. Association for Computational Linguistics. URL https://aclanthology.org/D12-1024. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander Rush. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=9Vrb9D0WI4. Jérémy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback, 2022. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17151725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://aclanthology.org/P16-1162. Zhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing Huang. Toward diverse text generation with inverse reinforcement learning. arXiv preprint arXiv:1804.11258, 2018. Daniel Shin, Daniel Brown, and Anca Dragan. Offline preference-based apprenticeship learning. arXiv preprint arXiv:2107.09251, 2021. Raphael Shu, Kang Min Yoo, and Jung-Woo Ha. Reward optimization for neural machine translation with learned metrics. arXiv preprint arXiv:2104.07541, 2021. Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural language generation with implicit language learning. arXiv preprint arXiv:2206.11871, 2022. Irene Solaiman and Christy Dennison. Process for adapting language models to society (PALMS) with valuestargeted datasets. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=k-ghaB9VZBw. 18 Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. Advances in Neural Information Processing Systems, 33:30083021, 2020. Richard Sutton and Andrew Barto. Reinforcement learning: An introduction. MIT press, 2018. Ryuichi Takanobu, Hanlin Zhu, and Minlie Huang. Guided dialog policy learning: Reward estimation for multi-domain task-oriented dialog. arXiv preprint arXiv:1908.10719, 2019. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. arXiv preprint arXiv:2311.12908, 2023. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. arXiv preprint arXiv:2406.12845, 2024a. Huimin Wang, Baolin Peng, and Kam-Fai Wong. Learning efficient dialogue policy from demonstrations through shaping. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 63556365, 2020. Xinpeng Wang, Bolei Ma, Chengzhi Hu, Leon Weber-Genzel, Paul Röttger, Frauke Kreuter, Dirk Hovy, and Barbara Plank. \" my answer is c\": First-token probabilities do not match text answers in instruction-tuned language models. arXiv preprint arXiv:2402.14499, 2024b. Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, et al. Helpsteer: Multi-attribute helpfulness dataset for steerlm. arXiv preprint arXiv:2311.09528, 2023. Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv preprint arXiv:2406.08673, 2024c. Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. Recipes for safety in open-domain chatbots. arXiv preprint arXiv:2010.07079, 2020. Shentao Yang, Yihao Feng, Shujian Zhang, and Mingyuan Zhou. Regularizing model-based policy stationary distribution to stabilize offline reinforcement learning. In International Conference on Machine Learning, pages 2498025006. PMLR, 2022a. Shentao Yang, Zhendong Wang, Huangjie Zheng, Yihao Feng, and Mingyuan Zhou. regularized implicit policy for offline reinforcement learning. arXiv preprint arXiv:2202.09673, 2022b. 19 Shentao Yang, Shujian Zhang, Yihao Feng, and Mingyuan Zhou. unified framework for alternating offline model training and policy learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022c. Shentao Yang, Shujian Zhang, Congying Xia, Yihao Feng, Caiming Xiong, and Mingyuan Zhou. Preferencegrounded token-level guidance for language model fine-tuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=6SRE9GZ9s6. Shentao Yang, Tianqi Chen, and Mingyuan Zhou. dense reward view on aligning text-to-image diffusion with preference. In Forty-first International Conference on Machine Learning, 2024. Zichao Yang, Zhiting Hu, Chris Dyer, Eric Xing, and Taylor Berg-Kirkpatrick. Unsupervised text style transfer using language models as discriminators. Advances in Neural Information Processing Systems, 31, 2018. Yueqin Yin, Zhendong Wang, Yi Gu, Hai Huang, Weizhu Chen, and Mingyuan Zhou. Relative preference optimization: Enhancing llm alignment through contrasting responses across identical and diverse prompts. arXiv preprint arXiv:2402.10958, 2024. Eunseop Yoon, Hee Suk Yoon, SooHwan Eom, Gunsoo Han, Daniel Wontae Nam, Daejin Jo, Kyoung-Woon On, Mark Hasegawa-Johnson, Sungwoong Kim, and Chang Yoo. Tlcr: Token-level continuous reward for fine-grained reinforcement learning from human feedback. arXiv preprint arXiv:2407.16574, 2024. Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy gradient. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017. Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. RRHF: Rank responses to align language models with human feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=EdIGMCHk4l. Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. Han Zhong, Guhao Feng, Wei Xiong, Li Zhao, Di He, Jiang Bian, and Liwei Wang. Dpo meets ppo: Reinforced token optimization for rlhf. arXiv preprint arXiv:2404.18922, 2024. Brian Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. Carnegie Mellon University, 2010. Brian D. Ziebart, Andrew Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In Proc. AAAI, pages 14331438, 2008. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "A Algorithm Box",
            "content": "Algorithm 1 summarizes our method in Section 2 on training the segment-level reward model and utilizing it in PPO-based RLHF LM training. Note that all operations in Algorithm 1 can be efficiently conducted in batch mode, parallel for multiple sample points at once. Algorithm 1 Training and Utilizing Our Segment-level Reward. Input: Binary preference dataset Dpref = {(x, yw, yl)} for reward model training, prompt set Dpol = {x} for policy learning, supervised fine-tuned model πSFT, reward model training steps Mrew, LM policy training steps Mpol, entropy cutoff cent, KL coefficient β for RLHF PPO training. Initialization: Initialize the segment-level reward model rϕ and LM policy πθ from πSFT, fix the aggregation function () as the Average in Eq. (4), initialize other components in the off-the-shelf RLHF PPO routine as specified. // Training the segment-level reward model Use πSFT and cent to split the responses {(yw, yl)} in Dpref = {(x, yw, yl)} into segments. for iter {1, . . . , Mrew} do Sample minibatch = {(xi, yw , yl With () and τ , calculate eϕ(xi, yw Optimize reward model rϕ by Eq. (3). i)}i Dpref . ) and eϕ(xi, yl i) by Eq. (4) for (xi, yw , yl i) B. end for // Utilizing the segment-level reward model in PPO-based LM policy learning Estimate the reward normalizer functions Mean(p) and Std(p) as described in Section 2.3. for iter {1, . . . , Mpol} do Sample minibatch = {xi}i Dpol. Sample response yi πθ( xi) for each xi Use πSFT and cent to segment each yi; record the completion portion of each segment. Use rϕ to assign segment-level reward to each segment at in each yi Normalize each segment reward rϕ(st, at) as rϕ(st, at) (rϕ(st, at) Mean(p))/Std(p). Interpolate rϕ(st, at) to each token yi, as at y, yi at, rϕ([x, y<i], yi) = rϕ(st, at)/at With KL coefficient β, optimize policy LM πθ against rϕ by the PPO routine. end for"
        },
        {
            "title": "B Additional Results",
            "content": "B.1 Tabular Results Table 6: Evaluation results of downstream tasks on the HuggingFace OpenLLM Leaderboard (Beeching et al., 2023), comparing LM policies in Table 1. Action Definition ARC TruthfulQA Winograd HellaSwag MMLU GSM8K Average Phi-Instruct Bandit (Sequence) Sentence Token Segment (Ours)"
        },
        {
            "title": "Bandit as Segment\nSegment as Bandit",
            "content": "64.76 64.76 63.40 62.71 62.71 64.16 64.33 54.44 55.11 53.99 53. 54.74 54.62 54.81 74.51 74.35 72.93 71.43 72.06 74. 74.74 79.03 79.32 79.34 79.46 79.23 78.95 79. 70.41 70.42 70.42 70.55 70.42 70.55 70.39 81. 77.8 84.1 87.3 86.7 81.0 78.6 70.79 70.29 70.70 70. 70.98 70.66 70.35 B.2 Examples of Generated Responses Below, we provide examples of generated responses from our LM policy in the main results  (Table 1)  on the benchmarks AlpacaEval 2.0 (first two examples) and Arena-Hard (last two examples). Prompt: When are hops added to the brewing process? Response: Hops, the flowers of the hop plant Humulus lupulus, are crucial ingredient in brewing beer, primarily used for their bittering, flavoring, and aromatic properties. They are typically added to the brewing process at various stages, depending on the desired outcome in terms of bitterness, flavor, and aroma. The timing of hop addition is critical aspect of brewing that significantly influences the final beers character. 1. Boiling Stage (Early Boil): This is the most common stage for adding hops. Adding hops early in the boil (usually within the first 15 minutes) primarily contributes to the beers bitterness. The longer the hops are boiled, the more bitterness is extracted due to the breakdown of alpha acids, which are responsible for this bitterness. This process is known as isomerization. Early boil hops can also contribute some flavor, but the impact on aroma is minimal because the volatile compounds responsible for aroma are largely evaporated during this long boil. 2. Middle of the Boil: Adding hops in the middle of the boil (around 15 to 30 minutes) can strike balance between flavor and aroma. The bitterness level is moderate, and the hop flavors and some aromatic compounds are preserved better than in the early boil, thanks to the shorter exposure time. 3. Late Boil (Last 10-15 minutes): Hops added late in the boil contribute significantly to the beers aroma and flavor, with minimal to no contribution to bitterness. This is because the shorter boiling time allows the volatile aromatic compounds to remain intact, while the alpha acids responsible for bitterness are less extracted. This technique is often used in styles that emphasize hop aroma and flavor, such as India Pale Ales (IPAs) and New England IPAs (NEIPAs). 4. Fermentation Stage: Some brewers also add hops during the fermentation process, technique known as dry hopping. This is done after the primary fermentation has completed, and the beer is transferred to secondary fermenter or directly to the bottle/keg. Dry hopping enhances the beers aroma without significantly affecting its bitterness. The extended contact time allows the hop oils to infuse the beer, imparting fresh, fruity, and floral aroma. 5. Post-Fermentation (Aging): In some cases, especially with barrel-aged beers, hops may be 22 added during the aging process to impart additional flavor and aroma characteristics. This can be more experimental approach, as the interaction between the hops and the aging environment (e.g., oak barrels, different beer styles) can lead to unique and complex profiles. The choice of when to add hops is critical decision that brewers make based on the style of beer they are aiming to produce and the specific flavor, aroma, and bitterness profiles they desire. Experimentation and experience play significant roles in fine-tuning this aspect of the brewing process to achieve the perfect balance for each batch. Prompt: Introduce the financial markets and institutions by Frederic S. Mishkin. Response: In his comprehensive work, The Economics of Money, Banking, and Financial Markets, Frederic S. Mishkin delves into the intricate world of financial markets and institutions, providing thorough examination of their roles, functions, and significance in the global economy. The book, published in 2000, is well-regarded resource for students, professionals, and policymakers alike, as it explores the complexities of financial systems and their impact on economic growth, stability, and development. Financial markets and institutions are integral components of the broader economic system, serving as the backbone for the efficient allocation of resources, risk management, and the facilitation of transactions between various participants. Mishkins book offers deep dive into the various types of financial markets, such as capital markets, money markets, and foreign exchange markets, as well as the institutions that operate within these environments, including banks, investment firms, and regulatory bodies. The book begins by establishing solid foundation in the basic concepts and principles of money, banking, and financial markets, ensuring that readers have clear understanding of the fundamental ideas that underpin the entire field. Mishkin then progresses to explore the role of financial intermediaries, such as banks and other institutions, in connecting savers and borrowers, and how these entities contribute to the overall efficiency and stability of the financial system. One of the key themes in Mishkins work is the importance of financial markets and institutions in promoting economic growth and development. He examines the ways in which these entities facilitate the flow of funds from savers to productive investments, thereby supporting innovation, entrepreneurship, and job creation. Furthermore, Mishkin discusses the role of financial markets in providing risk management tools, such as derivatives and insurance products, which help individuals and businesses protect themselves against potential losses. Another crucial aspect of Mishkins exploration is the examination of the regulatory framework that governs financial markets and institutions. The author emphasizes the need for effective oversight and regulation to ensure the stability of the financial system, protect consumers, and prevent financial crises. Throughout the book, Mishkin provides real-world examples and case studies to illustrate the consequences of both well-functioning and poorly regulated financial markets, highlighting the importance of maintaining balanced and robust regulatory environment. In addition to discussing the theoretical aspects of financial markets and institutions, Mishkin also addresses practical considerations, such as the impact of technology on the financial industry, the role of central banks in monetary policy, and the challenges posed by globalization and financial integration. By offering comprehensive and well-rounded perspective on the subject, Mishkins work serves as an invaluable resource for anyone seeking to understand the complexities and nuances of financial markets and institutions in the modern economic landscape. 23 Prompt: Provide me with breakfast recipe that is quick to make and is high in protein (at least 30 grams) and has variety of ingredients. Response: Certainly! Heres quick and delicious high-protein breakfast recipe called \"Spinach and Feta Egg Muffins.\" This recipe is not only packed with protein but also incorporates variety of ingredients, including spinach, feta cheese, and eggs. Its perfect for healthy, on-the-go breakfast or satisfying meal prep option. Ingredients: 6 large eggs 1 cup fresh spinach, chopped 1/2 cup feta cheese, crumbled 1/4 cup red onion, finely chopped 1/4 cup cherry tomatoes, halved 1/4 cup green bell pepper, finely chopped 1/4 cup mushrooms, finely chopped 1/4 cup fresh parsley, chopped Salt and pepper, to taste 1/4 teaspoon garlic powder (optional) 1/4 teaspoon paprika (optional) 1/4 teaspoon dried oregano (optional) Instructions: 1. Preheat your oven to 375F (190C) and lightly grease muffin tin with cooking spray or small amount of oil. 2. In large mixing bowl, whisk the eggs until well combined. 3. Add the chopped spinach, feta cheese, red onion, cherry tomatoes, green bell pepper, mushrooms, and parsley to the egg mixture. Stir until all the ingredients are evenly distributed. 4. Season the mixture with salt, pepper, garlic powder, paprika, and dried oregano (if using) to taste. 5. Pour the egg mixture into the prepared muffin tin, filling each cup about 3/4 full. 6. Place the muffin tin in the preheated oven and bake for 20-25 minutes, or until the egg muffins are set and the tops are lightly golden. 7. Remove the muffin tin from the oven and let the egg muffins cool for few minutes before transferring them to wire rack to cool completely. 8. Once cooled, you can store the egg muffins in an airtight container in the refrigerator for up to 3-4 days or freeze them for up to 3 months. 24 9. To reheat, simply place the egg muffins in microwave-safe dish and heat for 15-20 seconds, or until warmed through. Enjoy your high-protein, delicious, and nutritious Spinach and Feta Egg Muffins! Prompt: Write python function that solves quadratic equation. Response: To solve quadratic equation of the form ax2 + bx + = 0, we can use the quadratic formula: b2 4ac 2a This formula gives us the roots of the quadratic equation, where a, b, and are coefficients of the equation, and represents the variable. The term under the square root, b2 4ac, is known as the discriminant. The discriminant determines the nature of the roots: = - If the discriminant is positive, there are two distinct real roots. - If the discriminant is zero, there is exactly one real root (also known as repeated or double root). - If the discriminant is negative, there are no real roots, but two complex roots. Heres Python function that calculates the roots of quadratic equation, handling all three cases: import cmath def solve_quadratic(a, b, c): \"\"\" Solves quadratic equation of the form ax^2 + bx + = 0. Parameters: - a: Coefficient of x^2 - b: Coefficient of - c: Constant term Returns: - tuple containing the roots of the equation. If the roots are complex, they are returned as complex numbers. \"\"\" # Calculate the discriminant discriminant = cmath.sqrt(b**2 - 4*a*c) # Calculate the two roots using the quadratic formula root1 = (-b + discriminant) / (2*a) root2 = (-b - discriminant) / (2*a) return (root1, root2) # Example usage a, b, = 1, 5, 6 # Coefficients for the equation x^2 + 5x + 6 = 0 roots = solve_quadratic(a, b, c) print(f\"The roots of the equation are: {roots[0]} and {roots[1]}\") # For complex discriminant a, b, = 1, 2, 5 # Coefficients for the equation x^2 + 2x + 5 = 0 roots = solve_quadratic(a, b, c) print(f\"The roots of the equation are: {roots[0]} and {roots[1]}\") This function uses the cmath module to handle complex square roots, ensuring that it can return complex roots when necessary. The function calculates and returns both roots, which can be real or complex numbers, depending on the discriminant."
        },
        {
            "title": "C More Implementation Details",
            "content": "Table 7: Model hyperparameters used in reward model training. Table 8: Model hyperparameters used in PPO-based LM policy training. Hyperparameter Value Hyperparameter Phi-mini Llama-3-8B 128 Batch Size 1 Training Epochs 1728 Max prompt length 2048 Max total length 3 DeepSpeed ZeRO stage Adam Optimizer Learning Rate 1e-6 Gradient clipping norm 1.0 1.75 Entropy threshold cent 128 1024 16 2 1024 1024 2 5e-7 9e-6 Batch Size Rollout batch size Micro rollout batch size Training Epochs Max prompt length Max generation length DeepSpeed ZeRO stage Actor learning rate Critic learning rate Gradient clipping norm 1.0 Entropy threshold cent Value clipping KL coefficient β 1.75 0.25 0.01 128 1024 4 1 1024 1024 2 5e-7 9e-6 1.0 2.0 0.25 0.01 Implementation Details. We tabulate detailed parameter settings in Table 7 and Table 8. Most of them are the same as the default setting in OpenRLHF. Both the reward model and PPO training employ the Adam optimizer (Kingma and Ba, 2014), with β1 = 0.9 and β2 = 0.95. To save GPU memory, we use gradient checkpointing (Chen et al., 2016) and flash attention (Dao et al., 2022). For reward model training, we set the maximum prompt sequence length as 1792 tokens, with the total sequence length (including both prompt and response) capped at 2048 tokens. During data preprocessing, we apply left truncation to the prompt and right truncation to the response. If the EOS token in the response is truncated, we manually change the last token in the truncated response to the EOS token. The global mini batch size for reward model training is set to 128, with each GPU processing micro batch size of 8. To facilitate distributed training, we utilize DeepSpeed ZeRO-3. For our segment-level reward model, the entropy threshold is set to cent = 1.75 for training with the Phi-series models and cent = 2 for the Llama-3-8B model. The baseline bandit reward model is technically implemented as setting the entropy threshold cent = 1000, restricting reward computation to the EOS token only, while the baseline token-level reward model is implemented as setting the entropy threshold cent = 0, ensuring that reward is computed for each token in the text sequence. For PPO training, the replay buffer size (rollout_batch_size) is set to 1024, while the batch size per GPU for generation (micro_rollout_batch_size) is configured as 16 for Phi-mini and 4 for Llama-3-8B. The maximum prompt sequence length is set as 1024 tokens, and the maximum generated sequence length is also set to 1024 tokens. In PPOs on-policy sampling, for each prompt in the mini-batch, single response is sampled via top-p sampling with = 1.0 and sampling temperature 1.0. We use DeepSpeed ZeRO-2 for distributed training. The actor learning rate is set to the default value of 5 107, and the critic learning rate is also the default value of 9 106. The clipping coefficient for value loss (value clip) is set to 0.25 for PPO training based on segmentand token-level reward model, and as default to 0.2 for bandit-reward-based PPO training. The clipping coefficient for policy loss (eps clip) is set to 0.2. The KL coefficient is kept to the default value of β = 0.01. 26 Chat Template. Below is the chat template used for PPO training, reward model training, and evaluation sampling with Phi-3. The original Phi-3-Mini chat template includes an additional EOS token, which we removed in experiments due to incompatibility, leaving only the <end> token. <user> {the user query} <end> <assistant> {Assistant Response} <end> For Llama-3, the chat template is as follows: <begin_of_text><start_header_id>user<end_header_id> {the user query}<eot_id><start_header_id>assistant<end_header_id> {Assistant Response}<eot_id> Computation of Location-Aware Reward Normalizers via Regression The computation process for the location-aware reward normalizers involves several key steps, starting with data sampling and culminating in regression modeling for the estimation of reward normalization functions. First, 60,000 data points are randomly sampled from the Preference-700K dataset, which includes pairs of prompts, chosen responses, and rejected responses. Each response is processed by segment reward model, where the segments within the response are indexed by their respective normalized location percentages. Specifically, the normalized location (0, 1] is computed for each segment at as = , where is the index of the segment within the sequence and represents the total number of segments in the response. The model then records the reward for each segment, producing set of data points that consist of the segments normalized location and its corresponding reward. Next, this collected data is stored in dictionary format, which is then saved as JSON file. The data is subsequently loaded for further analysis, where the normalized locations (denoted as p) are sorted and the corresponding mean and standard deviation of rewards at each location are extracted. To estimate the relationship between the normalized location and the reward statistics, we employ linear regression approach using the HuberRegressor from the sklearn library, which is robust to outliers. We perform the regression on the log-transformed normalized locations, log(p), to model the dependence of the mean reward µp and the standard deviation σp of rewards at each location. The regression formulas are given by: Mean(p) = wµ log(p) + bµ Std(p) = wσ log(p) + bσ Here, wµ and bµ are the regression coefficients for the mean reward, and wσ and bσ are those for the standard deviation. These coefficients are computed via closed-form solutions derived from the linear regression process. Once the regression coefficients are obtained, we use them to compute the normalized rewards for each segment during the reinforcement learning process. The normalized reward rϕ(st, at) is adjusted according to the location-aware normalizers: rϕ(st, at) rϕ(st, at) Mean(p) Std(p)"
        },
        {
            "title": "E More on the Reward Normalizers in PPO Training",
            "content": "To center the assigned rewards from the reward model and reduce their variance, in most open-source (bandit) RLHF PPO implementations (e.g., ?Hu et al., 2024), the bandit reward of the newly sampled response is first Z-score normalized, before being fed into the PPO routine. Concretely, for the prompt and sampled response y, the bandit reward rϕ(x, y) is normalized as rϕ(x, y) (rϕ(x, y) µ)/σ, where µ and σ are respectively the mean and standard deviation of (bandit) rewards in the reward calibration dataset. The PPO routine starts by using this normalized rϕ(x, y), e.g., first subtract it by the KL regularization, and then calculate the advantage estimates and value function training target, etc. For the segment-level action space, we will then need to normalize the reward rϕ(st, at) for each segment at. As shown in Table 4 (Global Statistics of All), the most intuitive idea of simply using the global mean and standard deviation over all segment-level rewards in the reward calibration dataset does not train good LM. Looking into the responses sampled in PPO training and in the reward calibration dataset, we find that, for example, the beginning segments of the responses are typically greeting alike phrases that are less informational and/or essential to respond to the given prompt, which tend to receive relatively lower rewards. If we normalize the segment-level rewards of those early segments by the global mean and standard deviation, those normalized rewards will be significantly negative, rather than centered around 0. This will undesirably refrain the generation of necessary greeting alike phrases, resulting in an impolite LM and thus inferior benchmark results. More generally, the linguistic structure of the response leads to certain correlation between the mean and standard deviation of segment-level reward values and the normalized location of segment in the response, e.g., in the early or middle or later part. This observation motivates us to design location-aware reward normalizers that can approximately capture the reward statistics at an arbitrary location of the response, so that the normalized segment-level rewards can be more centered and less varying. It is important to have proper reward normalizers at an arbitrary location of the response, because the response sampled in PPO training will have stochastic total length, nondeterministic number of segments, and less-controllable length of each segment. These considerations motivate our design of the regression-based reward normalizer functions in Section 2.3."
        }
    ],
    "affiliations": [
        "Microsoft",
        "The University of Texas at Austin"
    ]
}