{
    "paper_title": "MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences",
    "authors": [
        "Zizhen Li",
        "Chuanhao Li",
        "Yibin Wang",
        "Yukang Feng",
        "Jianwen Sun",
        "Jiaxin Ai",
        "Fanrui Zhang",
        "Mingzhu Sun",
        "Yifei Huang",
        "Kaipeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements have expanded the role of Large Language Models in board games from playing agents to creative co-designers. However, a critical gap remains: current systems lack the capacity to offer constructive critique grounded in the emergent user experience. Bridging this gap is fundamental for harmonizing Human-AI collaboration, as it empowers designers to refine their creations via external perspectives while steering models away from biased or unpredictable outcomes. Automating critique for board games presents two challenges: inferring the latent dynamics connecting rules to gameplay without an explicit engine, and modeling the subjective heterogeneity of diverse player groups. To address these, we curate a dataset of 1,727 structurally corrected rulebooks and 150K reviews selected via quality scoring and facet-aware sampling. We augment this data with Mechanics-Dynamics-Aesthetics (MDA) reasoning to explicitly bridge the causal gap between written rules and player experience. We further distill player personas and introduce MeepleLM, a specialized model that internalizes persona-specific reasoning patterns to accurately simulate the subjective feedback of diverse player archetypes. Experiments demonstrate that MeepleLM significantly outperforms latest commercial models (e.g., GPT-5.1, Gemini3-Pro) in community alignment and critique quality, achieving a 70% preference rate in user studies assessing utility. MeepleLM serves as a reliable virtual playtester for general interactive systems, marking a pivotal step towards audience-aligned, experience-aware Human-AI collaboration."
        },
        {
            "title": "Start",
            "content": "MeepleLM: Virtual Playtester Simulating Diverse Subjective Experiences Zizhen Li1,2,3, Chuanhao Li4, Yibin Wang2, Yukang Feng2,3, Jianwen Sun2,3, Jiaxin Ai2, Fanrui Zhang2, Mingzhu Sun3, Yifei Huang1, Kaipeng Zhang1,2(cid:66) 1Shanda AI Research Tokyo, 2Shanghai Innovation Institute, 3NKU, 4Shanghai AI Laboratory {zizhen.li,kaipeng.zhang}@shanda.com 6 2 0 2 1 2 ] . [ 2 1 5 2 7 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements have expanded the role of Large Language Models in board games from playing agents to creative co-designers. However, critical gap remains: current systems lack the capacity to offer constructive critique grounded in the emergent user experience. Bridging this gap is fundamental for harmonizing Human-AI collaboration, as it empowers designers to refine their creations via external perspectives while steering models away from biased or unpredictable outcomes. Automating critique for board games presents two challenges: inferring the latent dynamics connecting rules to gameplay without an explicit engine, and modeling the subjective heterogeneity of diverse player groups. To address these, we curate dataset of 1,727 structurally corrected rulebooks and 150K reviews selected via quality scoring and facet-aware sampling. We augment this data with Mechanics-DynamicsAesthetics (MDA) reasoning to explicitly bridge the causal gap between written rules and player experience. We further distill player personas and introduce MeepleLM, specialized model that internalizes persona-specific reasoning patterns to accurately simulate the subjective feedback of diverse player archetypes. Experiments demonstrate that MeepleLM significantly outperforms latest commercial models (e.g., GPT5.1, Gemini3-Pro) in community alignment and critique quality, achieving 70% preference rate in user studies assessing utility. MeepleLM serves as reliable virtual playtester for general interactive systems, marking pivotal step towards audience-aligned, experience-aware Human-AI collaboration1."
        },
        {
            "title": "Introduction",
            "content": "Board games have long served as universal medium of significant cultural and economic value, Internship at Shanda AI Research Tokyo. (cid:66)Corresponding author. 1Dataset and code are available at https://github.com/ leroy9472/MeepleLM. Figure 1: Overview of MeepleLM. Acting as Virtual Playtester, the model offers rapid, automated alternative to the resource-intensive Human Play loop. By leveraging MDA-Reasoning to infer latent dynamics from Static Rulebooks, MeepleLM generates PersonaAligned Critiques tailored to diverse player archetypes. captivating vast global audience (Rodríguez, 2025). Recently, the rapid development of Large Language Models (LLMs) has introduced unprecedented possibilities to this classic domain. Specifically, board games serve as prominent platform for evaluating diverse model facets, ranging from reasoning (Lin et al., 2025) and decisionmaking (Tang et al., 2025) to role-playing (Yu et al., 2025) and social simulation (Hansteen Izora and Teuscher, 2025). Beyond serving as testbed, recent research highlights board game development as pivotal domain for investigating Human-AI Collaboration, where LLMs serve as active codesigners to perform tasks such as generating mechanics (Patrick and Khan, 2025), facilitating iterative prototyping (Ma et al., 2025) and synthesizing executable engines (Hong et al., 2025; Lehrach et al., 2025). However, while automated development has advanced, critical gap remains: current systems lack the capacity to offer constructive critique grounded in the emergent user experience. Such feedback is vital for harmonizing the roles of both human and model in the co-creation loop. On one hand, designers require external perspectives to refine their crations and better comprehend their audience (Fang et al., 2025), process that ultimately catalyzes further creativity (Choi et al., 2025). On the other hand, for LLM-driven systems, the absence of effective user feedback can lead to biased content (Taveekitworachai et al., 2024) or unpredictable, frustrating experiences (Yong and Mitchell, 2023). Consequently, bridging this gap is fundamental to evolving LLMs into empathetic partners for Human-AI Collaboration. By prioritizing diverse user experiences, this approach ensures that future co-creation is driven not merely by technical validity, but by dynamic alignment with individual needs. To this end, we need an evaluation paradigm that can map design intent (or system specifications) to user experience. However, board game experiences are characteristically emergent and subjective: they are not static properties of rulebooks, but are jointly generated through interaction as mechanics unfold, players MDA, and emotional responses arise (Forlizzi and Battarbee, 2004). This inherent characteristic poses two core challenges for automated evaluation: (1) Inferring Latent Dynamics from Static Rules. While rulebooks serve as explicit code, gameplay experience is an emergent property generated only when mechanics interact at runtime. The core challenge is to bridge the gap between written specifications and dynamic interactions. Since LLMs lack an explicit game engine, they must infer plausible execution trajectories from rules and use empirical player feedback as an external signal to recover latent causal links that connect mechanics to outcomes and reactions. (2) Modeling Subjective Group Preferences. Experience is not universal; the same mechanism can elicit conflicting reactions across different player demographics (e.g., high randomness may delight Socializer but frustrate Strategist). If critiques collapse into an average one-size-fits-all judgment, they become generic and less actionable for design or recommendation. The challenge, therefore, is to model this subjective heterogeneity by aligning reasoning with specific group preferences, simulating distinct personas rather than single standard user. To address these challenges, we meticulously curate large-scale dataset of structurally corrected rulebooks from selected board games, paired with reviews filtered through rigorous scoring and quality assessment. We further augment this data by incorporating the classic game design theory of Mechanics-Dynamics-Aesthetics (MDA) (Hunicke et al., 2004) into Chain-of-Thought (CoT) reasoning, thereby making the latent execution logic explicit. To structure the inherent subjectivity of feedback, we distill distinct player personas through an expertLLM collaborative interpretation of datadriven community clusters. Building upon this foundation, we introduce MeepleLM(Figure 1), specialized model designed to predict gameplay experiences by simulating the perspectives of realworld players. Extensive experiments validate that MeepleLM significantly outperforms state-of-theart baselines in capturing authentic user experiences. Our contributions are summarized as follows: We present the first systematic study on the automated evaluation of board games. We bridge the gap between static rules and distinct player experiences by simulating the latent gameplay dynamics. We curate high-quality dataset of 1,727 rulebooks and 150K critiques, selected via rigorous filtering and quality evaluation. We further leverage the MDA framework to synthesize explicit COT paths that recover the latent dynamics connecting rules to experiences. We distill five data-driven player personas and introduce MeepleLM. By internalizing persona-specific reasoning, our model predicts authentic gameplay experiences that reflect the diverse preferences of real-world communities. We conduct systematic evaluation on stratified set of 207 games. Experiments across macro-level alignment, micro-level fidelity, and practical utility demonstrate that MeepleLM significantly outperforms state-ofthe-art LLMs as reliable virtual playtester. Ultimately, by bridging static rules and dynamic experiences, MeepleLM establishes paradigm for the automated virtual testing of general interactive systems, which accelerates design iteration via anticipated market feedback and facilitates personalized selection for players. This paves the way for experience-aware Human-AI collaboration, where models evolve from functional tools into empathetic partners attuned to subjective audience sensibilities."
        },
        {
            "title": "2 Related Work",
            "content": "LLM-Driven Feedback and Assistance. Recent advancements have empowered LLMs to surpass traditional metrics in evaluating open-ended text, demonstrating high alignment with human judgments (Li et al., 2024; Gao et al., 2025; Chen et al., 2023). Current systems provide constructive feedback ranging from granular writing issues (Russell et al., 2025) to comprehensive peer reviews (Benharrak et al., 2024; Rashkin et al., 2025), with established metrics for narrative consistency (Rashkin et al., 2025), structural integrity (Zheng et al., 2025), and subjective enjoyment (Yang and Jin, 2025). However, these approaches treat text as static narratives, failing to address the executable logic inherent in interactive systems like board games. Even when extended to design assistance, such as generating levels (Todd et al., 2023) or rule codes (Todd et al., 2024; Tanaka and Simo-Serra, 2024), LLMs often prioritize syntactic correctness over logical coherence. As noted in recent studies, this frequently yields functional yet meaningless mechanics, known as introns (Todd et al., 2024; Hu et al., 2024). While existing tools fragment into abstract brainstorming (Li et al., 2025) or rigid asset production (Lindfors, 2025) without assessing playability, our work bridges this gap by simulating dynamic interactions to predict the emergent experience directly from rules. User Simulation and Persona Modeling. Understanding audience heterogeneity is crucial for creators, yet manual analysis of diverse feedback is cognitively demanding (Choi et al., 2023; Ma et al., 2023). The field has evolved from surveybased imagined users (Cooper, 1999; Salminen et al., 2018) to algorithmic clustering (McGinn and Kotamraju, 2008; Salminen et al., 2020) and, recently, LLM-based perspective simulation (Benharrak et al., 2024; Park et al., 2023). However, purely synthetic simulations often lack ecological validity, risking hallucinations or stereotyping due to foundation model biases (Cheng et al., 2023). To mitigate this, state-of-the-art systems emphasize grounding simulations in empirical data for representativeness (Shin et al., 2024; Choi et al., 2025). Aligning with this paradigm, we ground personas in large-scale gameplay critiques rather than conversational history. This allows our model to internalize distinct preferences, facilitating Human-AI collaboration through empathetic, persona-aligned feedback rather than generic judgments."
        },
        {
            "title": "3 Data Construction",
            "content": "We curated multi-layered dataset that maps objective game rulebooks to subjective player feedback across diverse personas. The overall construction pipeline is illustrated in Figure 2."
        },
        {
            "title": "3.1 Game Selection",
            "content": "We curated collection of 1,727 board games via stratified sampling strategy on BoardGameGeek (BGG)2. As detailed in Appendix A, our selection prioritizes four dimensions to ensure comprehensive representation of the design landscape: (1) Market Stratification: To mitigate survivorship bias, we balanced the selection between elite titles (including 83 from the Top 100) and longtail designs (comprising 53% of the dataset with Rank > 1, 000), capturing the full spectrum of market reception. (2) Cognitive Spectrum: We covered the entire range of BGG Weight (1.05.0), encompassing everything from low-burden social party games to calculation-intensive strategy simulations. (3) Temporal Span: The dataset balances historical depth with modern relevance, containing 47% classic titles released pre-2015 alongside 35 cutting-edge designs from 2024 and beyond. (4) Mechanical Heterogeneity: To ensure structural diversity, the collection spans 192 unique mechanics and 81 themes, covering logic distinct from standard genres."
        },
        {
            "title": "3.2 Rulebook Structuring",
            "content": "We processed the official rulebooks into structured knowledge base via three-step pipeline. First, we parsed raw PDFs into hierarchical Markdown using Mineru (Niu et al., 2025) to preserve layout information. Second, we prompted Qwen3235B(Yang et al., 2025) to restructure the raw text into standardized hierarchical format (e.g., unifying headers for Objective, Components, and Flow); the specific extraction prompt is detailed in Appendix B.1, and complete structured example is provided in Appendix B.2. Finally, to ensure maximum fidelity, we employed GPT-5.1 to crossreference and rectify the initial drafts against the 2https://boardgamegeek.com/browse/boardgame Figure 2: The Data Construction Pipeline. We structure 1,727 board game rulebooks and filter 1.8M reviews via multi-dimensional quality assessment, yielding 150K high-quality critiques for persona discovery. ified coverage-maximization strategy. We first performed stratified sampling based on original ratings to preserve sentiment fidelity (Pearsons = 0.920, verified in Figure 3, Top-Left). Simultaneously, we filtered for high-quality entries (scores > 4) to significantly enhance MDA scores (Figure 3, BottomLeft) and optimized selection to maximize semantic coverage across all facets (Figure 3, Right). This process yielded final dataset of 150k entries (approx. 8% retention), ensuring robust volume of 50100 reviews per game; further details are provided in Appendix C.3. 3."
        },
        {
            "title": "Persona Discovery",
            "content": "A single average rating fails to capture the subjective diversity, where the rigorous complexity prized by strategists is perceived as an exhausting burden by others. To model these domain-specific cognitive attributions, we implemented Cluster-thenRefine discovery pipeline, evolving raw behavioral clusters into interpretable player personas. Discovery Pipeline. We first generated composite embeddings for all reviews using Qwen3Embedding-8B (Zhang et al., 2025) (concatenating text with logic scores and facets; see Appendix D.1). Following K-Means clustering (K = 15), we employed human-in-the-loop process where GPT-5.1 profiled representative samples, and domain experts refined these into finalized taxonomy of five distinct personas (detailed in Appendix D.2). Using these finalized definitions, we employed GPT-5.1 to annotate the entire dataset. To ensure classification stability, we implemented majority-vote mechanism (aggregating 3 indepenFigure 3: Impact of the Filtering Strategy. Our strategy enhances MDA scores and semantic coverage while preserving the original rating distribution. source text, correcting logical inconsistencies or formatting errors (see the rectification prompt in Appendix B.3)."
        },
        {
            "title": "3.3 Review Filtering",
            "content": "We aggregated raw corpus of 1.8 million ratingcomment pairs from multiple online communities (detailed in Appendix C.1). To refine this data, we employed Qwen3-235B with multi-task prompt (detailed in Appendix C.2) to perform comprehensive evaluation across three dimensions: (1) Hard Filtering, removing noise such as short texts, offtopic logistics, and rating inconsistencies; (2) MDA Scoring, evaluating whether specific Mechanics are linked to Dynamic interactions to derive Aesthetic experiences, and assigning quality scores across these three dimensions; and (3) Facet Identification, where the model mapped the content to predefined semantic topics (e.g., Rule Clarity, Balance & Fairness) to capture diverse viewpoints. Based on these metrics, we implemented stratGame Diff. Highest Persona (Rating) Lowest Persona (Rating) <Unspeakable Words> (Party / Word) <Talisman> (Adventure / Roll) <Aeon Trespass> (Campaign / Heavy) 4.04 3.71 2.97 Social Lubricator (6.9) Party chaos! System Purist (2.9) Random noise. Narrative Architect (7.1) An epic journey. System Purist (3.4) Roll-and-move hell. Narrative Architect (9.0) Immersive masterpiece. Efficiency Essentialist (6.0) Feels like job. Table 1: Case Study of Aesthetic Segregation. The rating gaps between persona groups validate that our taxonomy effectively captures distinct player preferences. dent inferences per review) to assign the dominant persona label. The annotation prompts are detailed in Appendix D.3. Aesthetic Segregation. Table 1 exemplifies the distinct preference patterns captured by our taxonomy. The data reveals that party and adventure elements actively alienate System Purists, whereas heavy campaign games frustrate Efficiency Essentialists despite their thematic appeal. (See Appendix D.4 for the full Preference Matrix). Why LLM Annotation? We attempted to train supervised classifier (DeBERTa-v3-large (He et al., 2021)) but it proved insufficient for capturing subtle preference patterns. As analyzed in Appendix D.5, it naively misclassify review mentioning house rules and balance as System Purist, failing to discern that the user is actually introducing variants to inject the high-risk, high-reward volatility typical of Thrill Seeker."
        },
        {
            "title": "4.1 Problem Formulation",
            "content": "We formulate the task as conditional generation problem. Given rulebook context and target player persona P, the objective is to generate feedback entry (comprising numerical rating and textual review). Crucially, direct mapping ignores the semantic gap between static text and emergent fun. To bridge the gap between objective rules and subjective preference, we employ an MDA-Guided Reasoning strategy. Drawing upon the foundational game design framework (Hunicke et al., 2004), we reinterpret the MDA model originally designed to analyze gameplay loops as causal inference chain for language models. We define latent intermediate sequence that explicitly traces the causal path from Mechanics to Dynamics, and finally to Aesthetics. By decomposZMDA Y, we force ing the generation into [R, P] the model to simulate the runtime experience before articulating the critique, ensuring the output is logically grounded in the rules."
        },
        {
            "title": "4.2 Synthesizing the MDA Cognitive Chain",
            "content": "Since the reasoning chain is latent in raw reviews, we utilize distillation approach to recover this logic. We employed Qwen3-235B as Teacher Model to reconstruct from high-quality reviewrule pairs. The prompt (Appendix E.1) enforces three-step cognitive flow: (1) Step 1: Mechanics (The What). What specific content does the review explicitly mention? Isolate objective rule components from explicitly mentioned in the review to ensure grounding. (2) Step 2: Dynamics (The How). What Interaction or System Dynamic occurred during play? Infer the runtime system behaviors or interactions triggered by the mechanics identified in Step 1. (3) Step 3: Aesthetics (The Feel). What was the final Aesthetic Experience or emotional feeling? Synthesize the subjective emotional outcome, modulated by the preferences of Persona P. Verifier-Guided Filtration Loop. We employ GPT-5.1 to judge the entailment between synthesized reasoning and ground-truth ratings. Guided by Appendix E.2, the verifier removes chains with sentiment contradictions or hallucinations, triggering automatic regeneration to ensure logical consistency. sample alignment of rules, personas, and critiques appears in Appendix E.3."
        },
        {
            "title": "4.3 Persona-Conditional Instruction Tuning",
            "content": "We fine-tuned the Qwen3-8B (Yang et al., 2025) backbone to maximize the joint likelihood of the MDA reasoning chain and the final critique Y. To address the challenge of subjective heterogeneity, we do not represent as simple label. Instead, we encode the full semantic profile derived in Section 3.4 (including core values and interaction preferences) into the system instruction. This forces the model to treat as contextual prior that modulates the Dynamics Aesthetics transition. Formally, we treat the concatenation = [Z; Y] as the target sequence and minimize the standard Cross-Entropy Loss: = (cid:88) t=1 log (st s<t, R, Pprof ile). (1) The training was implemented using LoRA (Hu et al., 2022) on all linear layers via LLaMAFactory (Zheng et al., 2024). Hyperparameter details are provided in Appendix E.4."
        },
        {
            "title": "5 Experiments and Analysis",
            "content": "To systematically validate MeepleLM as reliable virtual playtester, we structure our evaluation around three core research questions: (1) RQ1 (Macro-level Alignment) assesses whether the simulator accurately replicates community rating distributions and preference rankings. (2) RQ2 (Micro-level Fidelity) examines if the generated reviews maintain factual consistency with rules while exhibiting the content richness and semantic diversity characteristic of real players. (3) RQ3 (Practical Utility) investigates whether the simulated feedback provides actionable insights for design optimization and player decision support."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Dataset Splitting. We constructed comprehensive test set of 207 games disjoint from the training corpus. To ensure representative coverage of the design landscape, we employed stratified sampling strategy based on BGG Weight (Complexity 1.05.0) and Average Rating (Tier 15). Notably, this selection spans wide temporal range, explicitly including 34 newly released titles (20242025) alongside historical classics, enabling us to assess performance on both established consensus and fresh content. Simulation Protocols. For each game, we execute = 100 simulation runs. In each run, the model takes the rulebook and specific persona to generate the critique Y. Crucially, we do not pick randomly; instead, we sample personas to match the empirical proportions found in the ground-truth reviews. The specific inference prompt is provided in Appendix F.1. Baselines. We benchmark against three stateof-the-art general LLMs (GPT-5.1, Gemini3-Pro, Qwen3-235B) and our backbone model Qwen3-8B. Detailed implementation configurations are provided in Appendix F.2."
        },
        {
            "title": "5.2 RQ1: Macro-level Community Alignment",
            "content": "Evaluation Metrics. To assess whether MeepleLM aligns with the collective wisdom of the community, we employ three complementary metrics: (1) Mean Absolute Error (MAE) measures the absolute precision of rating predictions; (2) Wasserstein Distance (WD) evaluates the fidelity of the predicted score distribution against the ground truth (Villani et al., 2008); (3) Kendalls Rank Correlation (τ ) assesses the models ability Figure 4: Tier-wise Prediction Alignment. MeepleLM shows sharp diagonal concentration, effectively distinguishing quality tiers. Figure 5: Rating Density Distributions. MeepleLM demonstrates superior distributional fidelity by accurately recovering the high variance of human consensus. to correctly rank games based on their perceived quality (Kendall, 1938). Beyond Ranking: Capturing Community Diversity. As summarized in Table 2, MeepleLM consistently achieves the best performance across all alignment metrics. This superiority is visually corroborated in Figure 4, where our model demonstrates sharp diagonal concentration, effectively distinguishing high-quality outliers (Tier 1) from poor designs (Tier 5). Critically, while advanced baselines like GPT-5.1 retain some capacity for ranking (τ = 0.2555), they exhibit severe central tendency bias, playing it safe by clustering predictions around the mean to minimize error. This failure to simulate authentic polarization is quantified by their high Wasserstein Distance (0.9496) compared to MeepleLM (0.2205), proving that our method captures the variance of community sentiment. In contrast, the untuned Qwen3-8B fails to establish any meaningful correlation (τ 0), degenerating into effectively random guessing. Overcoming Positivity Bias. We further investigate this distributional shift in Figure 5. In sceModel GPT-5.1 Gemini3-Pro Qwen3-235B Qwen3-8B MeepleLM(Ours) w/o MDA w/o Persona w/o Rulebook Preference Alignment (RQ1) Review Quality (RQ2) Utility (RQ3) MAE 0.9874 1.4277 1.2288 0.8906 0.6576 0.7395 0.7887 0.7043 WD 0.9496 0.5092 0.6350 1.0119 0. 0.4148 0.3630 0.5496 τ 0.2555 0.2465 0.1449 0.0492 0.2817 0.2271 0.1348 0.0026 Fact. Dist-2 Div. Op-Rec 99.46 98.28 98.95 97.88 98.86 91.56 92.13 59. 0.6934 0.6480 0.6572 0.5936 0.7117 0.6850 0.6771 0.6970 4.26 3.98 3.56 1.58 4.34 3.70 3.56 3. 63.44 57.74 54.27 11.39 69.77 55.35 53.84 9.99 Table 2: Overall performance. MeepleLM shows superior performance in community alignment, generation quality, and practical utility, validating the effectiveness of persona-aligned simulation for virtual playtesting. narios with polarized opinions (Case II), baselines exhibit mode collapse, aggressively clustering predictions around safe, high scores (79) and failing to capture the long-tail of negative feedback. MeepleLM, empowered by domain-specific rule understanding, successfully recovers the high variance of human consensus (W = 0.82), proving its ability to represent the diverse spectrum of player sentiments rather than just generic average."
        },
        {
            "title": "5.3 RQ2: Content Fidelity and Diversity",
            "content": "Evaluation Metrics. To ensure the generated reviews are both trustworthy and rich in content, we employ three metrics covering accuracy, vocabulary, and semantic variety: (1) Factual Correctness (Fact.): We implement an automated Fact-Checker using Gemini3-Flash. As detailed in Appendix F.3, the judge extracts specific claims regarding game components or mechanics from the review and strictly verifies whether they exist in the official rulebook R. This directly measures the models groundedness. (2) Lexical Diversity (Dist-2): We use the Distinct-2 score (Li et al., 2016) to measure vocabulary richness, calculating the ratio of unique bigrams to total bigrams to detect repetitive phrasing. (3) Perspective Diversity (Div.): realistic simulator should not be broken record that repeats the same opinion endlessly. To detect semantic repetition, we feed batches of = 5 reviews (generated for the same game and persona) to Gemini3Flash judge(prompt details in Appendix F.4). The judge scores the batch on 15 scale based on topic coverage: Low scores indicate the reviews are merely rephrasing the same point (e.g., all 5 reviews complain about luck); High scores indicate the reviews discuss diverse aspects such as mechanics, social interaction, and art style to mimic the natural variety of human feedback. Results Analysis. Table 2 confirms that MeepleLM matches the factual accuracy of SOTA models while delivering superior diversity. To further illustrate this, we present the side-by-side comparison in Table 3. While Qwen3-8B defaults to generic melodramatic tone (theater of tears) and GPT-5.1 sounds like detached journalist (social WD-40), MeepleLM authentically captures the distinct voice of each persona. By seamlessly switching from community slang (e.g., Alpha Gamers) in social contexts to technical critique (e.g., Variant Rules) for purists, our model proves it is not just retrieving knowledge, but truly simulating players perspective."
        },
        {
            "title": "5.4 RQ3: Practical Utility",
            "content": "Opinion Recovery Rate (Op-Rec). To quantify the models value as virtual playtester, we assess its ability to forecast actual market feedback. We define Op-Rec as the recall rate of groundtruth player opinions within the simulated reviews. The evaluation employs two-stage pipeline using Gemini3-Flash(prompts detailed in Appendix F.5): (1) Ground Truth Mining: The judge extracts deduplicated set of distinct viewpoints (VGT ) from historical human reviews, representing the actual voice of the customer. (2) Semantic Matching: We check whether the simulated reviews generated by MeepleLM cover these specific viewpoints. As reported in Table 2, MeepleLM achieves the highest Op-Rec score, validating its utility for designers in forecasting market feedback and surfacing diverse player viewpoints. User Study: Blind A/B Test. To validate realworld effectiveness, we conducted controlled blind A/B test with = 10 participants (demoQwen3-8B MeepleLM GPT-5.1 Persona: The Social Lubricator (Goal: Party Atmosphere, Interaction) ...The table became theater of accusations and fake tears... The mechanic was pure chaos... Its not the most elegant design, but it thrives on the energy of the groupits party game that rewards chaos over calculation. ...You really need at least 6 people though... With 4 players, the Mason role is useless. If you have alpha gamers, they will dominate the conversation... Overall, recommend this for quick and easy game. ...The night phase became hilarious circus of whispered accusations... My friends insomniac kept checking his card... Its perfect recipe for laughs, tension, and bonding... ...Played ton with friends... favorite game at parties. Simple enough to teach to new player... great for people who dont know lot about board games. The bluffing is fun... social WD-40, ...As someone who treats board games as is perfect. The single-night structure keeps things punchy... fast, noisy trust-fall in social form. this ...Basically structured excuse for people to point fingers... Thats social gold. What love is how shifting roles (Robber, Troublemaker) give everyone reason to speak. Persona: The System Purist (Goal: Mechanics, Balance, Strategy) ...One Night Ultimate Werewolf is sharp knife with blunt handle... The inherent randomness felt like betrayal of the games potential... near-perfect match for purist, but the luck left bittersweet aftertaste. ...I like that there is no dice luck. However, the Robber and Drunk can screw up the game... recommend adding variant rules: Drunk swaps with Villager instead of center card. ... When everyone reports honestly-thenbluffs deliberately, the deduction feels crisp. But this design lives or dies on human precision... respect the underlying clockwork. Table 3: Qualitative Case Study: Generated Reviews for <One Night Ultimate Werewolf>. MeepleLM generates factually grounded critiques that align with specific persona sensibilities. By capturing both technical nuances and community-specific slang, our model demonstrates the semantic richness and perspective diversity. graphics in Appendix G). Each evaluator reviewed 6 titles randomly selected from the test set: 3 they had previously played (Familiar) and 3 they had not (Unfamiliar). Results indicate decisive preference for MeepleLM over GPT-5.1. In the Familiar scenario, our model achieved an average win rate of 78.3%, with 83.3% of participants specifically citing superior authenticity in capturing the emotional nuances of gameplay. In the Unfamiliar scenario, the average win rate remained high at 74.2%; notably, 86.7% of users preferred MeepleLM for its critical honesty, describing it as less like marketing and more effective for identifying potential design flaws (Detailed pairwise results and qualitative feedback are provided in Appendix G). soning is required to bridge the gap between static text and emergent experience. Temporal Impact. We further validated the stability by re-evaluating RQ1 on subset excluding the 35 newly released titles (20242025). As detailed in Appendix H.2, performance shifts across all models are negligible, confirming that the inclusion of fresh content does not bias the assessment. Persona Robustness. Decomposing RQ1 by player profile (Appendix H.3) highlights MeepleLMs robust performance on high-variance personas such as The Social Lubricator and The Thrill Seeker. This indicates that the framework successfully captures social dynamics and subjective vibes that defy pure logical deduction."
        },
        {
            "title": "6 Conclusion",
            "content": "Ablation. To verify the contribution of each module, we evaluated three variants (detailed configurations provided in Appendix H.1). As shown in Table 2, performance drops in all cases: (1) w/o Rulebook: Removing rule context causes collapse in Factual Accuracy (98.9 59.9), confirming that explicit grounding is non-negotiable; (2) w/o Persona: Replacing specific profiles with generic prompts drops ranking alignment (τ to 0.13), proving that modeling heterogeneity is essential for capturing polarized preferences; (3) w/o MDA: Bypassing the CoT chain results in consistently lower opinion recovery, validating that intermediate reaWe present MeepleLM, model that bridges the gap between static rulebooks and subjective player experiences. By curating dataset of rule-critique pairs and integrating MDA-based reasoning with data-driven player personas, we make gameplay dynamics explicit. Our experiments demonstrate that MeepleLM significantly outperforms general LLMs in capturing authentic community sentiment and actionable design insights. Ultimately, this work establishes new paradigm for automated virtual testing of interactive systems, paving the way for experience-aware Human-AI collaboration attuned to diverse audience sensibilities."
        },
        {
            "title": "Limitations",
            "content": "While MeepleLM demonstrates strong potential as virtual playtester, we acknowledge two primary limitations that outline our roadmap for future research: (1) Multimodal Understanding. Currently, MeepleLM processes game rules exclusively as text. However, board games are inherently multimodal experiences where visual cues including card art, board iconography, and component design play crucial role in immersion and usability. Future iterations will integrate visual encoders to process game assets (e.g., cards, maps, and tokens), enabling more holistic evaluation of the games aesthetic and functional design. (2) Granularity of Personas. Our current approach relies on five aggregated personas derived from community clusters, which effectively capture broad player archetypes but may overlook the unique idiosyncrasies of specific individuals. Moving forward, we aim to transition from group-level to individuallevel modeling. By collecting detailed historical data from specific players, we plan to construct granular virtual player community, where agents can simulate the precise tastes and behaviors of realworld individuals for hyper-personalized playtesting."
        },
        {
            "title": "Ethics Statement",
            "content": "Data Privacy and Usage. Our dataset is constructed from publicly available content retrieved from an online board game community (see Section 3), which is accessible to the general public. To protect user privacy, we strictly anonymize all User IDs and review identifiers, removing any personally identifiable information (PII) from the raw data. To mitigate the potential dissemination of harmful content and respect copyright considerations, we will only release the processed versions of the reviews and metadata, rather than the raw scraped content. Human Evaluation. Our research involves collecting evaluation data from real human participants. We adhere to strict ethical guidelines to ensure their privacy, consent, and well-being. Key ethical principles include: (1) Informed Consent: Participants are provided with detailed information about the studys purpose, procedures, and their rights. They are explicitly informed that they can withdraw from the study at any time without any negative consequences. (2) Data Anonymization: To safeguard participant privacy, all collected evaluation data, including interaction logs and questionnaires, is anonymized. Personal identifiers are removed to ensure that individuals cannot be traced from the data. (3) Data Security: All collected data is stored securely, with access restricted to authorized research personnel only."
        },
        {
            "title": "References",
            "content": "Karim Benharrak, Tim Zindulka, Florian Lehmann, Hendrik Heuer, and Daniel Buschek. Writer-defined ai personas for on-demand feedback generation. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, pages 118, 2024. Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. Exploring the use of large language models for reference-free text quality evaluation: An empirical study. arXiv preprint arXiv:2304.00723, 2023. Myra Cheng, Tiziano Piccardi, and Diyi Yang. Compost: Characterizing and evaluating caricature in llm simulations. arXiv preprint arXiv:2310.11501, 2023. Yoonseo Choi, Eun Jeong Kang, Min Kyung Lee, and Juho Kim. Creator-friendly algorithms: Behaviors, challenges, and design opportunities in algorithmic platforms. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 122, 2023. Yoonseo Choi, Eun Jeong Kang, Seulgi Choi, Min Kyung Lee, and Juho Kim. Proxona: Supporting creators sensemaking and ideation with llm-powered audience personas. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pages 132, 2025. Alan Cooper. The inmates are running the asylum. In Software-ergonomie99: design von informationswelten, pages 1717. Springer, 1999. Cong Fang, Yujie Zhu, Le Fang, Yonghao Long, Huan Lin, Yangfan Cong, and Stephen Jia Wang. Generative ai-enhanced human-ai collaborative conceptual design: systematic literature review. Design Studies, 97:101300, 2025. Jodi Forlizzi and Katja Battarbee. Understanding experience in interactive systems. In Proceedings of the 5th conference on Designing interactive systems: processes, practices, methods, and techniques, pages 261268, 2004. Mingqi Gao, Xinyu Hu, Xunjian Yin, Jie Ruan, Xiao Pu, and Xiaojun Wan. Llm-based nlg evaluation: Current status and challenges. Computational Linguistics, pages 127, 2025. Kaj Hansteen Izora and Christof Teuscher. Exploring the potential of large language models (llms) to simulate social group dynamics: case study using the board game\" secret hitler\". Northeast Journal of Complex Systems (NEJCS), 7(2):5, 2025. Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543, 2021. Jiale Hong, Hongqiu Wu, and Hai Zhao. Game development as human-llm interaction. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 43334354, 2025. Chengpeng Hu, Yunlong Zhao, and Jialin Liu. Game generation via large language models. In 2024 IEEE Conference on Games (CoG), pages 14. IEEE, 2024. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Robin Hunicke, Marc LeBlanc, Robert Zubek, et al. Mda: formal approach to game design and game research. In Proceedings of the AAAI Workshop on Challenges in Game AI, volume 4, page 1722. San Jose, CA, 2004. Maurice Kendall. new measure of rank correlation. Biometrika, 30(1-2):8193, 1938. Wolfgang Lehrach, Daniel Hennes, Miguel LazaroGredilla, Xinghua Lou, Carter Wendelken, Zun Li, Antoine Dedieu, Jordi Grau-Moya, Marc Lanctot, Atil Iscen, et al. Code world models for general game playing. arXiv preprint arXiv:2510.04542, 2025. Danrui Li, Sen Zhang, Samuel Sohn, Kaidong Hu, Muhammad Usman, and Mubbasir Kapadia. Cardiverse: Harnessing llms for novel card game prototyping. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2972329750, 2025. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and William Dolan. diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies, pages 110 119, 2016. Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, and Shuai Ma. Leveraging large language models for nlg evaluation: Advances and challenges. arXiv preprint arXiv:2401.07103, 2024. Wenye Lin, Jonathan Roberts, Yunhan Yang, Samuel Albanie, Zongqing Lu, and Kai Han. Gamebot: Transparent assessment of llm reasoning in games. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 76567682, 2025. Joakim Lindfors. Leveraging generative ai to create themed assets for games: study on narrative and 3d asset creation with ai, 2025. Ling Ma, Mingyao Pan, Vince Siu, Xiaoyu Chang, Jussi Holopainen, Jixing Li, and Ray LC. Follow my logic: Generative ai workflows in designing for serious table-top games. In International Conference on Human-Computer Interaction, pages 153172. Springer, 2025. Renkai Ma, Xinning Gui, and Yubo Kou. Multiplatform content creation: the configuration of creator ecology through platform prioritization, content synchronization, and audience management. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 119, 2023. Jennifer McGinn and Nalini Kotamraju. Data-driven persona development. In Proceedings of the SIGCHI conference on human factors in computing systems, pages 15211524, 2008. Junbo Niu, Zheng Liu, Zhuangcheng Gu, Bin Wang, Linke Ouyang, Zhiyuan Zhao, Tao Chu, Tianyao He, Fan Wu, Qintong Zhang, Zhenjiang Jin, Guang Liang, Rui Zhang, Wenzheng Zhang, Yuan Qu, Zhifei Ren, Yuefeng Sun, Yuanhong Zheng, Dongsheng Ma, Zirui Tang, Boyu Niu, Ziyang Miao, Hejun Dong, Siyi Qian, Junyuan Zhang, Jingzhou Chen, Fangdong Wang, Xiaomeng Zhao, Liqun Wei, Wei Li, Shasha Wang, Ruiliang Xu, Yuanyuan Cao, Lu Chen, Qianqian Wu, Huaiyu Gu, Lindong Lu, Keming Wang, Dechen Lin, Guanlin Shen, Xuanhe Zhou, Linfeng Zhang, Yuhang Zang, Xiaoyi Dong, Jiaqi Wang, Bo Zhang, Lei Bai, Pei Chu, Weijia Li, Jiang Wu, Lijun Wu, Zhenxiang Li, Guangyu Wang, Zhongying Tu, Chao Xu, Kai Chen, Yu Qiao, Bowen Zhou, Dahua Lin, Wentao Zhang, and Conghui He. Mineru2.5: decoupled vision-language model for efficient high-resolution document parsing, 2025. URL https://arxiv.org/abs/2509.22186. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122, 2023. Andrew Patrick and Md Abdullah Al Hafiz Khan. Gamegenesis: multimodal ai revolution in board game design, 2025. Hannah Rashkin, Elizabeth Clark, Fantine Huot, and Mirella Lapata. Help me write story: Evaluating llms ability to generate writing feedback. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2582725847, 2025. Carles Moya Rodríguez. Opportunities in the board game market: strategic analysis through the blue ocean theory, 2025. Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Dingyi Yang and Qin Jin. What matters in evaluating book-length stories? systematic study of long story evaluation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1637516398, 2025. Qing Ru Yong and Alex Mitchell. From playing the story to gaming the system: Repeat experiences of large language model-based interactive story. In International Conference on Interactive Digital Storytelling, pages 395409. Springer, 2023. Pengfei Yu, Dongming Shen, Silin Meng, Jaewon Lee, Weisu Yin, Andrea Yaoyun Cui, Zhenlin Xu, Yi Zhu, Xingjian Shi, Mu Li, et al. Rpgbench: Evaluating large language models as role-playing game engines. arXiv preprint arXiv:2502.00595, 2025. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and rerankarXiv preprint ing through foundation models. arXiv:2506.05176, 2025. Mingzhe Zheng, Dingjie Song, Guanyu Zhou, Jun You, Jiahao Zhan, Xuran Ma, Xinyuan Song, SerNam Lim, Qifeng Chen, and Harry Yang. Cmlbench: framework for evaluating and enhancing llm-powered movie scripts generation. arXiv preprint arXiv:2510.06231, 2025. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning In Proceedings of the of 100+ language models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv. org/abs/2403.13372. Jenna Russell, Marzena Karpinska, and Mohit Iyyer. People who frequently use chatgpt for writing tasks are accurate and robust detectors of ai-generated text. arXiv preprint arXiv:2501.15654, 2025. Joni Salminen, Bernard Jansen, Jisun An, Haewoon Kwak, and Soon-gyo Jung. Are personas done? evaluating their usefulness in the age of digital analytics. Persona Studies, 4(2):4765, 2018. Joni Salminen, Kathleen Guan, Soon-gyo Jung, Shammur Chowdhury, and Bernard Jansen. literature review of quantitative persona creation. In Proceedings of the 2020 CHI conference on human factors in computing systems, pages 114, 2020. Joongi Shin, Michael Hedderich, Bartłomiej Jakub Rey, Andrés Lucero, and Antti Oulasvirta. Understanding human-ai workflows for generating perIn Proceedings of the 2024 ACM Designsonas. ing Interactive Systems Conference, pages 757781, 2024. Tsunehiko Tanaka and Edgar Simo-Serra. Grammarbased game description generation using large language models. IEEE Transactions on Games, 2024. Wenjie Tang, Yuan Zhou, Erqiang Xu, Keyan Cheng, Minne Li, and Liquan Xiao. Dsgbench: diverse strategic game benchmark for evaluating llm-based agents in complex decision-making environments. arXiv preprint arXiv:2503.06047, 2025. Pittawat Taveekitworachai, Kantinan Plupattanakit, and Ruck Thawonmas. Assessing inherent biases following prompt compression of large language models for game story generation. In 2024 IEEE Conference on Games (CoG), pages 14. IEEE, 2024. Graham Todd, Sam Earle, Muhammad Umair Nasir, Michael Cerny Green, and Julian Togelius. Level generation through large language models. In Proceedings of the 18th International Conference on the Foundations of Digital Games, pages 18, 2023. Graham Todd, Alexander Padula, Matthew Stephenson, Éric Piette, Dennis Soemers, and Julian Togelius. Gavel: Generating games via evolution and language models. Advances in Neural Information Processing Systems, 37:110723110745, 2024. Cédric Villani et al. Optimal transport: old and new, volume 338. Springer, 2008. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao chanics and themes tags. Figure 7 visualizes the prevalent categories. Mechanics: The dataset features 192 unique mechanics with an average of 6.35 mechanics per game, indicating high systemic depth. As shown in Figure 7(a), the most frequent mechanics include Hand Management (38.4%), Dice Rolling (29.0%), and Variable Player Powers (26.2%), which are foundational to modern game design. Themes: We identified 81 unique themes. Figure 7(b) shows blend of abstract strategy themes (e.g., Economic, 20.9%) and immersive narrative themes (e.g., Fantasy, 18.8%; Sci-Fi, 12.0%). This diversity requires the model to contextually adapt its vocabulary (e.g., discussing \"profits\" vs. \"damage\")."
        },
        {
            "title": "A Dataset Statistics Details",
            "content": "In this section, we provide comprehensive statistical breakdown of the 1,717 board games selected for our dataset. These statistics validate that our sampling strategy successfully captured diverse range of difficulty levels, quality standards, historical eras, and gameplay mechanisms. A.1 Distribution Analysis Figure 6 presents the distributions of Complexity, Rating, Publication Year, and BGG Rank. Complexity (Weight): As shown in Figure 6(a), the complexity distribution is nearly normal (Skewness: 0.29) with mean of 2.57. We intentionally preserved balanced spectrum: Light games (Weight 12) account for 24.5%, while Heavy/Very Heavy games (Weight > 3) comprise 28.5%. This ensures the model learns to adapt its critique depth to the cognitive load of the game. Rating: Figure 6(b) shows the rating distribution (Mean: 7.22, StdDev: 0.51). The distribution is slightly left-skewed, focusing on games generally considered \"playable\" to \"excellent\" (66% are rated > 7.0). This filtering removes low-quality noise while retaining enough variance for comparative analysis. Publication Year: Illustrated in Figure 6(c), the dataset reflects the modern board game renaissance, with median publication year of 2013. The coverage spans from classic designs (pre-2000, = 159) to contemporary hits (2015+, = 755), specifically including 34 cutting-edge titles released in 2024 2025. Rank Coverage: Figure 6(d) highlights the market representativeness. While we include 83% of the Top 100 \"elite\" games to ensure high-quality training data, over 50% of the dataset (N = 909) consists of \"long-tail\" games (Rank > 1000), preventing the model from overfitting to universally acclaimed masterpieces. A.2 Content Diversity: Mechanics and"
        },
        {
            "title": "Themes",
            "content": "To ensure the model can generate grounded reviews for various gameplay styles, we analyzed the me- (a) Complexity (Weight) Distribution (b) Average Rating Distribution (c) Publication Year Distribution (d) BGG Rank Distribution Figure 6: Distributions of Key Metadata Attributes. for the 1,717 selected games. The dataset covers wide spectrum of difficulty (a), focuses on decent-to-excellent quality games (b), emphasizes modern board game designs (c), and spans both elite and long-tail rankings (d). (a) Top 20 Mechanics (b) Top 20 Themes (c) Mechanics Word Cloud (d) Themes Word Cloud Figure 7: Analysis of Game Content. (a) and (b) display the top 10 mechanics and themes, demonstrating that the dataset covers the fundamental building blocks of modern board games. (c) and (d) provide holistic view of the terminological diversity present in the corpus."
        },
        {
            "title": "B Rulebook Structuring Details",
            "content": "To convert the raw Markdown rulebooks (converted by Mineru) into structured knowledge, we employed Qwen-3. We utilized specific prompt to ensure the model extracts information strictly from the source text without hallucination, organizing it into standardized Markdown format. B.1 Extraction Prompt Figure 8 displays the system prompt used. The prompt enforces strict constraints to use only existing information from the uploaded file. B.2 Structured Rulebook Example Figure 9 demonstrates sample of the structured output. This standardized text serves as the knowledge base for the review generation model. B.3 Rectification Prompt Figure 10 presents the rectification prompt used by GPT-5.1. This stage acts as verification layer, cross-referencing the structured draft against the source text to correct hallucinations or omissions. Prompt: Rulebook Structuring System Instruction: You are an expert board game rules analyst. Your task is to reorganize the provided raw Markdown content into structured format. Constraints (Strict Grounding): 1. Source Only: You must ONLY use information present in the input text. Do NOT add any external knowledge or hallucinate rules. 2. Format: Output the result in clean Markdown format using the specific headers defined below. 3. Completeness: If section is not mentioned in the text, write \"Not Mentioned\" under that header. Required Output Structure: ## 1. Lore & Objective (Extract the thematic setting and victory conditions) ## 2. Components (List physical assets like cards, boards, tokens) ## 3. Setup (Step-by-step initial configuration) ## 4. Gameplay Flow (Turn structure, phases, and round sequence) ## 5. Core Mechanics (Key interaction rules, player actions, restrictions) ## 6. Scoring & End Game (How points are calculated and how the game ends) ## 7. FAQ or Edge Cases (Specific clarifications found in the text) Input Text: {RAW_MARKDOWN_CONTENT} Figure 8: System Prompt for Structuring Raw Rulebooks. It enforces standard Markdown schema and strict grounding to the source text. Structured Rulebook Example <Clank!: Deck-Building Adventure> ### 1. Lore & Objective Players are rival thieves attempting to infiltrate Dragon Keep, steal valuable Artifacts, and escape alive. The dragon Nicki guards her hoard fiercelynoise (Clank!) and stolen treasures increase her rage. The ultimate goal is to become the Greatest Thief in the Realm by escaping with an Artifact and the most Victory Points (VP). **Primary Objectives:** - Steal at least **one Artifact**failure to do so results in automatic loss. - Accumulate the highest VP total from Artifacts, Gold, acquired tokens, and deck cards. - Survive above the Grass Line (depths boundary) or escape the dungeon to score points. **Winning Conditions:** - Players knocked out **below the Grass Line** score 0. - Players knocked out **above the Grass Line** score their collected loot (if they have an Artifact). - Escaping players receive **20 VP Mastery Token**. - Game ends immediately when the **Countdown Track reaches the Skull** or all players have exited/knocked out. - Ties broken by highest-value Artifact; further ties unresolved. --- ### 2. Components - **Double-sided Game Board** (Front: recommended for first game) - **Dungeon Deck** (100 cards) - **Reserve Cards**: - 15 Mercenary - 15 Explore - 12 Secret Tome - 1 Goblin (always available) - **Player Decks**: 4 starting decks (10 cards each): - 6 Burgle - 2 Stumble - 1 Sidestep - 1 Scramble - **Tokens & Markers**: - 120 Clank! Cubes (30 per player color) - 24 Black Dragon Cubes - Dragon Bag - Dragon Marker - Mastery Tokens (4) - **Treasures & Secrets**: - 7 Artifacts (530 VP) - 11 Major Secrets - 18 Minor Secrets - **Market Items**: - 2 Master Keys (5 VP) - 2 Backpacks (5 VP) - 3 Crowns (10, 9, 8 VP) - **Other**: - 3 Monkey Idols - Gold Tokens (1 and 5) - Player Pawns (4) - Rage Track (on board) - Countdown Track (top of board) - Health Meter (bottom of board) --- ### 3. Setup 1. **Player Setup**: - Each player takes: - 30 Clank! cubes (personal supply) - Matching pawn - 10-card starting deck - Shuffle deck, draw 5 cards. - Place pawn on top-left entrance space. 2. **Board Setup**: - **Artifacts**: Place 7 face up on numbered spaces. - 3 players: Shuffle and remove 1 randomly. - 2 players: Remove 2 randomly. - **Major Secrets**: Shuffle face down, place 1 on each marked space. Return extras to box. - **Minor Secrets**: Shuffle face down, place 2 stacked on each marked space. - **Market Items**: - 2 Master Keys (stacked) - 2 Backpacks (stacked) - 3 Crowns (10, 9, 8 in descending order) - **Monkey Idols**: Place 1 in each space in Monkey Shrine Room. - **Mastery Tokens**: Place 1 per player at top-left flag. - **Gold**: Place in bank near board. - **Dragon Marker**: - 4 players: Start on first space - 3 players: Second space - 2 players: Third space - **Dragon Bag**: Place 24 black cubes inside. - **Reserve Stacks**: Place near board: - 1 Goblin - 15 Mercenary - 15 Explore - 12 Secret Tome - **Dungeon Row**: Shuffle Dungeon Deck, deal 6 face-up cards. - Replace any with Dragon Attack symbol; shuffle replaced cards back. - **Starting Clank!**: - 1st player: 3 cubes in Clank! Area - 2nd: 2 cubes - 3rd: 1 cube - 4th: 0 cubes 3. **First Player**: Roll die or choose randomly. --- ### 4. Gameplay Flow Each turn follows this sequence: 1. **Play All Cards**: Play all 5 cards in hand, in any order. 2. **Take Actions** (in any order, multiple times): - Acquire Cards (Skill) - Use Devices (Skill) - Fight Monsters (Swords) - Buy Market Items (Gold) - Move (Boots) - Gain Gold / Clank! / Secrets / Artifact 3. **End of Turn Phase**: - Discard all played cards. - Draw 5 new cards (shuffle discard if needed). - Refill Dungeon Row to 6 cards. - **If refill reveals Dragon Attack symbol**: - Dragon attacks once (even if multiple symbols). - Move all Clank! cubes to Dragon Bag. - Draw cubes = Dragons Rage Track value (+1 per Danger symbol). - Black cubes: set aside. Colored cubes: place on matching players Health Meter. 4. **Dragon Rage Triggers**: - +1 space when: - Artifact is taken - Dragon Egg (Minor Secret) is revealed 5. **Exit or Knockout Triggers Countdown**: - First player to exit dungeon or be knocked out moves to Countdown Track. - On their next turn, they move forward and trigger effects: - Spaces 24: Instant Dragon Attack with +1, +2, +3 extra draws - Space 5: All remaining players in dungeon are instantly knocked out 6. **Game End**: Triggered when: - Countdown Track reaches Skull - All players have exited or been knocked out --- ### 5. Core Mechanics #### Resources - **Skill**: Acquire cards from Dungeon Row or Reserve. - **Swords**: Fight monsters (Dungeon Row or tunnels). - **Boots**: Move 1 tunnel per Boot. #### Movement Rules - **Normal Tunnel**: 1 Boot - **Footprint Icon**: 2 Boots - **Monster Icon**: Pay 1 Sword per icon, or take 1 damage per icon - **Lock Icon**: Requires Master Key - **Arrow Tunnel**: One-way only - **Crystal Cave**: Upon entry, **cannot use Boots** for rest of turn (teleporting allowed) - **Wrap-around Tunnels**: Connect opposite edges; cost 1 Boot #### Actions - **Acquire Card** (Blue/Yellow Banner): - Pay Skill place in discard pile (joins deck) - **Use Device** (Purple Banner): - Pay Skill use effect immediately discard to Dungeon discard pile - **Fight Monster** (Red Banner): - Pay Swords gain reward discard to Dungeon discard pile - **Goblin (Reserve)**: Can be fought multiple times; not discarded - **Buy Market Item**: - Cost: 7 Gold each - Items: Crown (VP), Backpack (carry +1 Artifact), Master Key (unlock tunnels) - Unlimited purchases per turn #### Clank! - Cards or actions may add Clank! cubes to central Clank! Area. - Negative Clank! removes cubes from area or offsets future gains. - Unused negative Clank! is lost at turn end. #### Dragon Attacks - Triggered **only** when refilling Dungeon Row reveals **at least one Dragon symbol**. - All Clank! cubes go into Dragon Bag. - Draw cubes = Dragons current Rage Track value + number of Danger symbols in Dungeon Row. - Colored cubes = damage to that player. - Exited/knocked-out players are immune. #### Health & Knockout - Damage places cubes on Health Meter (left to right). - **Fully filled meter** = knocked out. - **Below Grass Line**: Lose (0 points) - **Above Grass Line**: Score loot if holding Artifact - Healing: Use Potions or effects to return cubes to supply. #### Artifacts & Carrying - Can only carry **1 Artifact** unless Backpack is owned (1 extra per Backpack). - **Cannot drop or switch** Artifact once picked up. - Taking Artifact: - Move Dragon +1 on Rage Track - Triggers Dragon Egg if Minor Secret is revealed --- ### 6. Scoring & End Game **Scoring Includes**: - Artifact VP - All acquired tokens (Crown, Chalice, Backpack, etc.) - Gold: 1 VP per Gold - Cards in deck: VP in top-right corner - **Mastery Token (20 VP)**: Only if exited dungeon via own movement/teleport **Excluded from Score**: - Players knocked out **below Grass Line** 0 points - Exited/knocked-out players: No further turns or card effects **Tiebreaker**: 1. Highest-value Artifact 2. (Unspecified if still tied) --- ### 7. FAQ or Edge Cases - **Can you drop an Artifact?** No. Once picked up, its yours until game end. - **Can you buy multiple Market items per turn?** Yes, any number, even same type. - **Can you take multiple tokens per room?** Only one per entry. Must exit and re-enter. - **Does Artifact in Market cost Gold?** No. Only Crown, Backpack, Key cost Gold. - **When does Dragon attack?** Only when **new** Dragon symbol is revealed during **Dungeon Row refill**. Preexisting symbols do not re-trigger. - **Crystal Cave movement?** Cannot use Boots after entry, even via teleport. - **Running out of Clank! cubes?** Cannot choose to take damage in tunnels. Cannot be forced to add Clank! (free pass until healed). - **Healing during turn?** Yes, via Potions or effects. - **Order of card play?** Effects resolve in real time; order does not block conditional bonuses (e.g., Swagger gains Skill for all Clank! made that turn). --- Figure 9: Example of Structured Rulebook Data. This content spans multiple pages, preserving the full details extracted from the original PDF. Prompt: Rulebook Rectification (Expert-in-the-Loop) System Instruction: You are meticulous Board Game Rule Editor. Your task is to verify and rectify \"Draft Rulebook\" against the original \"Source Content\". Inputs: 1. Source Content: Raw text parsed from the official rulebook PDF. 2. Draft Rulebook: structured Markdown version generated by an automated parser. Rectification Tasks: 1. Accuracy Check: Compare specific numbers, card counts, and setup instructions. If the Draft says \"deal 5 cards\" but the Source says \"deal 4\", CORRECT it. 2. Completeness: Ensure no critical sections (especially \"End Game Triggers\" or \"Tie-Breakers \") are missing from the Draft. 3. Logical Consistency: Fix any contradictions introduced during the structuring process. 4. Formatting: Ensure the output strictly follows the standardized Markdown hierarchy. Constraints: - Output ONLY the fully rectified Rulebook in Markdown. - Do NOT add conversational text (e.g., \"Here is the fixed version\"). - Do NOT invent rules not present in the Source Content. Input Data: [SOURCE CONTENT]: {RAW_SOURCE_TEXT} [DRAFT RULEBOOK]: {QWEN_GENERATED_STRUCT} Figure 10: Rectification Prompt Used by GPT-5.1. It requires the model to cross-reference the generated draft against the source text to ensure numerical accuracy and logical completeness."
        },
        {
            "title": "C Review Processing Details",
            "content": "C.1 Data Sources To ensure diverse perspective coverage, we aggregated raw user reviews from multiple online communities through professional data outsourcing services. The sources encompass prominent digital tabletop platforms such as Board Game Arena3 and Tabletopia4, alongside specialized enthusiast forums like GStone5 and QPBG6. Given the heterogeneous scoring systems across these sites, we normalized all collected ratings to standardized 1.010.0 scale. C.2 Quality Annotation Prompt To implement the \"Design-Logic Quality Scoring\" described in Section 3.3, we used the prompt shown in Figure 13. It enforces strict evaluation criterion based on the utility of the review for game designers. C.3 Statistical Validation We analyzed the statistical properties of the 150K retained reviews to ensure they serve as an unbiased yet information-dense proxy for the original population. Distributional Fidelity. As shown in Figure 11, the filtered dataset maintains high degree of alignment with the original ratings (Pearson = 0.92, Spearman ρ = 0.91). This confirms that our filtering strategy preserves the global consensus on game quality. Notably, we observed slight negative mean shift (0.20), which suggests the successful removal of \"low-effort hype\" (e.g., empty 10/10 ratings), resulting in more critical and objective set. Information Density Word count analysis reveals \"Polarization Ratio\" of 1.24x: reviews at the rating extremes (1 & 10) contain significantly more text (avg. 195.7 words) compared to midrange reviews (avg. 158.2 words). This indicates that the dataset prioritizes strong signalsusers provide the most detailed structural feedback when they are passionately engaged, ensuring the model learns clear causal links for both design flaws and successes. 3https://en.boardgamearena.com 4https://tabletopia.com 5https://www.gstonegames.com 6https://qpbg.com Figure 11: Rating Correlation (Original vs. Filtered). Figure 12: Word Count Statistics by Rating. Prompt: Review Quality Scoring You are an expert Board Game Research Assistant constructing high-quality reasoning dataset. For each review (given numeric rating and review text), you must output JSON object strictly following the definitions below. ### 0. CRITICAL SCORING RULES (DECOUPLING) **Treat each score INDEPENDENTLY.** Do not allow high scores in one dimension to bleed into others (No \"Halo Effect\"). - **High Anchoring (5) does NOT imply High Attribution**: user might list rule names like manual (Anchoring: 5) but explain nothing about the experience (Attribution: 1). - **High Attribution (5) does NOT imply High Anchoring**: user might deeply explain the cause of their fun (Attribution: 5) using only vague terms like \"the pieces\" (Anchoring: 2). ### 1. TASK DEFINITIONS #### A. Hard Filters (Boolean) Determine if the review should be discarded. Set \"is_valid\": false if ANY of the following apply: - **Irrelevant**: Discusses ONLY shipping, damaged boxes, Kickstarter delivery, or customer service. - **Visuals Only**: Discusses ONLY artwork, miniatures, or card stock quality without mentioning gameplay mechanics or experience. - **Too Short**: Contains fewer than 20 words (insufficient logic). - **Rating Mismatch**: The sentiment of the text drastically contradicts the numeric rating (e. g., glowing review with rating of 1, or hateful rant with rating of 10). #### B. Quality Scores (Use the FULL 1-5 Scale) You must use the full range of integers (1, 2, 3, 4, 5). Do not default to just 1/3/5. 1. **mechanism_anchoring** (Specificity) - 1 (Vague): \"Fun game\", \"Good strategy\". No specific terms. - **2 (Basic):** Mentions basic components like \"cards\", \"board\", \"points\" but no mechanism names. - 3 (Generic): Mentions standard mechanics e.g., \"worker placement\", \"deck building\". - **4 (Detailed):** Describes specific game flow or unique twists but misses the exact rulebook terminology. - 5 (Precise): Cites exact rule names/components e.g., \"The Tekhenu obelisk wheel\", \"The Cult Track\". 2. **causal_attribution** (Reasoning - CORE METRIC) - 1 (No Logic): \"I hated it.\" / \"Best game ever.\" (Pure emotion). - **2 (Implied):** \"It's too long and boring.\" (Reason implies cause, but vague). - 3 (Simple Link): \"I didn't like it *because* the downtime was too long.\" (Direct X->Y). - **4 (Strong Logic):** \"The downtime is caused by the analysis paralysis in the auction phase.\" (Identifies mechanism -> issue). - 5 (Deep Systemic): \"Because turn order resets based on travelers, long-term planning feels impossible, leading to chaotic experience.\" (Mechanism -> Dynamic -> Aesthetic) . 3. **constructiveness** (Utility) - 1 (Useless): Empty complaints or blind praise. - **2 (Valid Complaint):** \"The combat feels unfair.\" (Identifies problem area, but subjective). - 3 (Actionable): \"The endgame drags on too long.\" (Specific pain point). - **4 (Analytical):** \"The blue faction is strong because of their starting resource.\" ( Analyzes *why* it's pain point). - 5 (Insightful/Solution): Offers fix or deep balance critique: \"They should cap rounds to 8 to fix the pacing.\" #### C. Content Facets (Multi-label List) Identify which aspects are primarily discussed. Ignore background mentions. Choose ONLY from: - \"Rule Clarity & Teachability\": rules readability, ambiguity, teaching difficulty. - \"Cognitive Load (Complexity)\": weight, brain-burn, analysis paralysis. - \"Interaction & Conflict\": take-that, blocking, table talk, multi-player solitaire. - \"Luck vs. Strategy\": randomness, dice/card luck, mitigation. - \"Balance & Fairness\": start player advantage, runaway leader, faction balance. - \"Replayability & Variety\": setup variability, scripted play. - \"Thematic Integration\": Ludonarrative harmony (mechanics fit theme). Exclude pure art praise. - \"Pacing & Flow\": downtime, game length feel (drag/tight), end triggers. ### 2. OUTPUT SCHEMA Return JSON object with this EXACT structure: { \"is_valid\": boolean, \"filter_reason\": string or null, // If is_valid is false, state the reason (e.g., \"Too Short \", \"Irrelevant\"). If true, null. \"scores\": { \"mechanism_anchoring\": int(1-5), \"causal_attribution\": int(1-5), \"constructiveness\": int(1-5) }, \"facets\": [string, string, ...] // List of matching facets } ### 3. FEW-SHOT EXAMPLES Input: Rating: 6 Comment: \"It involves moving pawns and collecting cards, but the randomness in drawing cards makes it feel like have no control.\" Output: { \"is_valid\": true, \"filter_reason\": null, \"scores\": { \"mechanism_anchoring\": 2, // \"pawns\", \"cards\" (Basic terms) -> 2 \"causal_attribution\": 4, // Randomness -> No control (Strong Logic) -> 4 \"constructiveness\": 2 // Valid complaint but subjective -> 2 }, \"facets\": [\"Luck vs. Strategy\"] } Input: Rating: 8 Comment: \"Excellent worker placement game. The tight economy forces you to plan ahead.\" Output: { \"is_valid\": true, \"filter_reason\": null, \"scores\": { \"mechanism_anchoring\": 3, // \"worker placement\" (Generic) -> 3 \"causal_attribution\": 3, // tight economy -> plan ahead (Simple Link) -> 3 \"constructiveness\": 1 // Praise only -> 1 }, \"facets\": [\"Cognitive Load (Complexity)\"] } Input: Rating: 4 Comment: \"The 'Corruption' mechanic punishes you too hard. It creates death spiral where if you mess up turn 1, you sit there for 2 hours. It feels broken.\" Output: { \"is_valid\": true, \"filter_reason\": null, \"scores\": { \"mechanism_anchoring\": 5, // \"Corruption\" (Specific Rule) -> 5 \"causal_attribution\": 5, // Mechanic -> Dynamic (Death spiral) -> Feeling -> 5 \"constructiveness\": 4 // Strong analysis of the problem -> 4 }, \"facets\": [\"Balance & Fairness\", \"Pacing & Flow\"] } You will be given list of {batch_size} board game reviews. You MUST generate JSON ARRAY containing EXACTLY {batch_size} JSON objects, one for each review. Do not add more or fewer objects than the number of reviews provided. REVIEWS (JSON list): {reviews_json} Figure 13: Annotation Prompt for Scoring Review Quality. It mirrors the observation-analysis-iteration loop."
        },
        {
            "title": "D Persona Discovery Details",
            "content": "D.1 Feature Construction & Clustering To ensure the clustering algorithm captures the cognitive depth of the reviewer rather than just keyword overlap, we pre-processed each review into composite text string before embedding. Composite Input Logic. We injected the quantitative metrics derived in Section 3.3 directly into the text representation. Each review was formatted using the following structured template: [SENTIMENT: {Tier}] [FOCUS: {Facets}] :: {Raw Review Content}"
        },
        {
            "title": "The metadata fields were populated based on the",
            "content": "following rules: Sentiment Tier: Discretized based on the labeled as \"Positive\" normalized rating R: (R 8), \"Negative\" (R 4), or \"Neutral\" (5 7). This guides the embedding to group reviews by satisfaction level. Focus Facets: comma-separated list of dimensions derived directly from the facetscoring model detailed in Section 3.3. Clustering Result. Figure 14 visualizes the TSNE projection of these embeddings. The inclusion of explicit Sentiment and Focus tags helped clearly separate reviewers based on their fundamental evaluation criteria and satisfaction thresholds, effectively mitigating the ambiguity of surface-level keywords. Figure 14: Visualization of Composite Embeddings. Colors indicate the 15 initial clusters, which were later merged into the 5 final personas by domain experts. Figure 15: Distribution of Personas in the Dataset. System Purists (Avg 6.58) are the harshest critics, while Narrative Architects (Avg 7.28) are the most generous. represent. Domain experts analyzed the semantic coherence of each cluster, synthesizing overlapping groups to establish the final five persona definitions. D.2 Persona Descriptions and Statistics Figure 15 shows the distribution and average rating for each group. Figure 16 provides the detailed definitions for each persona used in the annotation process, outlining their core motivations and specific mechanical preferences. Phase 2: Dataset Labeling. Once the 5 distinct personas were finalized (as defined in Appendix D.2), we needed to propagate these labels to the entire dataset. Figure 18 displays the classification prompt used with GPT-5.1 to annotate each review. D.3 Discovery Prompts We employed two-phase prompting strategy to translate raw numerical clusters into an annotated dataset. Phase 1: Persona Profiling. After obtaining the 15 initial clusters, we sampled the top-20 central reviews from each cluster. As shown in Figure 17, we prompted GPT-5.1 to analyze these samples and summarize the distinct \"Player Persona\" they D.4 Preference Matrix (Extended Case Study) To validate the distinctiveness of each persona, we performed frequency analysis of mechanics in their highest-rated vs. lowest-rated games. Figure 19 and Figure 20 detail the \"Lift\" metric, highlighting mechanics that are disproportionately favored or disliked by specific groups. D.5 Semantic Ambiguity Analysis To demonstrate the limitations of standard supervised classifiers, we present detailed case study from our error analysis. As noted in Section 3.4, DeBERTa-v3-large classifier trained on cluster seeds achieved only 50% accuracy. Table 4 illustrates the core issue: Keyword vs. Intent Misalignment. Standard models over-index on technical vocabulary (e.g., \"balance\", \"rules\"), failing to detect when users repurpose these terms to describe their exact oppositessuch as using \"house rules\" to introduce high-stakes volatility rather than mechanical fairness. Review Input (Raw Text) sentimental rating of course. We combined parts of the first Lord of the Rings game with this one, working on making the deck more balanced. We had numerous house rules, that in my opinion, made it such great gaming experience. For example, when attacking, triple sixes gave you an extra kill. Double sixes on defense, you GAINED an army, lose NONE. To even that out though, if you roll double ones, you LOSE 3. Remember, this is all before learned about board games. Standard Classifier Prediction: System Purist Reasoning: The model is misled by surface-level design keywords such as \"balanced\" and \"house rules\". It incorrectly infers that the user is focused on game balance, mechanical rigor, or improving the systems logic. Ground Truth / MeepleLM Label: Thrill Seeker Reasoning: The review explicitly frames the experience as \"sentimental\" and prioritizes dramatic, high-variance moments (\"triple sixes\", \"extra kill\", \"LOSE 3\"). The users \"house rules\" were not created to fix the systems logic, but to inject more chaos and excitement into the gameplay, which is the defining trait of Thrill Seeker. Table 4: Case Study of Semantic Ambiguity. The standard classifier fails by latching onto mechanical keywords (balanced), while the LLM correctly identifies the underlying motivation for excitement (triple sixes, sentimental). Detailed Persona Definitions & Preferences \"The System Purist\": \"\"\" * **Core Motivation:** Intellectual superiority & Control. They want to prove they can beat the system through pure logic. * **Profile:** Loves heavy/crunchy decisions. Zero tolerance for luck (hates dice output randomness). Obsessed with balance (hates first-player advantage). * **Interaction:** Likes indirect competition (blocking), hates chaotic direct conflict ( take-that). * **Keywords:** \"Optimization\", \"No luck\", \"Perfect information\", \"Tight\", \"Punishing\". \"\"\", \"The Efficiency Essentialist\": \"\"\" * **Core Motivation:** Maximize ROI (Fun/Time). Seeks \"Flow\". * **Profile:** Hates \"Fiddliness\" (setup, shuffling, bookkeeping). Values elegance (simple rules, deep strategy). Pragmatic about rules (will house-rule to fix pacing). * **Interaction:** Fast-paced. Hates Downtime (Analysis Paralysis). * **Keywords:** \"Elegance\", \"Streamlined\", \"Downtime\", \"Fiddly\", \"Smooth\". \"\"\", \"The Narrative Architect\": \"\"\" * **Core Motivation:** Immersion & Epic Experience. Mechanics serve the theme. * **Profile:** Loves growth (leveling up, empire building, tech trees). Wants 4X/RPG feels but within reasonable time. * **Interaction:** Cooperative or thematic negotiation/trade. Not calculating pure math. * **Keywords:** \"Theme\", \"Immersion\", \"Story\", \"Epic\", \"Journey\", \"Flavor\". \"\"\", \"The Social Lubricator\": \"\"\" * **Core Motivation:** Human Connection & Emotional Resonance. Game is an excuse to socialize. * **Profile:** Needs low barrier to entry (accessible to non-gamers). Hates \"Alpha Gamers\" (quarterbacking). Prioritizes experience over scoring. * **Interaction:** High social interaction (bluffing, laughter, party games). * **Keywords:** \"Party game\", \"Laughs\", \"Interaction\", \"Easy to teach\", \"Group dynamic\". \"\"\", \"The Thrill Seeker\": \"\"\" * **Core Motivation:** Dopamine & Emotional Rollercoaster. * **Profile:** Embraces risk (Push-your-luck). Needs fast pacing (if lose, let me restart instantly). Active agency in gambling. * **Interaction:** Schadenfreude (enjoying opponents busting) and epic comebacks. * **Keywords:** \"Push your luck\", \"Excitement\", \"Tension\", \"Gambling\", \"High stakes\". \"\"\" Figure 16: Behavioral Profiles of the Five Discovered Personas. Each profile defines core motivations and specific likes/dislikes regarding game mechanisms. Prompt : Persona Profiling (GPT-5.1) You are User Researcher specialized in board games. will provide you with set of game reviews that belong to the same Player Cluster. Your task is to analyze these reviews and synthesize Persona Profile. Based ONLY on the reviews above, define this player persona using the following JSON schema. Be specific and refer to the evidence in the text. { } \"persona_name\": \"Creative Name (e.g., The Dice Hater, The Euro Optimizer)\", \"core_motivation\": \"Why do they play? (e.g., Mathematical efficiency)\", \"preferred_mechanics\": \"What do they like/dislike? (e.g., Loves worker placement, hates dice )\", \"interaction_style\": \"How do they interact? (e.g., Dislikes direct conflict)\", \"deal_breakers\": \"What makes them rate game low? (e.g., Bad rulebooks, too much luck)\", \"system_prompt_description\": \"A concise, first-person description for an LLM to roleplay this user. (e.g., 'You are hardcore strategy gamer who values low luck and high skill ceiling. You are critical of imbalance...')\" Figure 17: Profiling Prompt for Interpreting Cluster-Central Samples. This qualitative analysis guided the definition of the final 5 personas. Prompt: Persona Labeling (GPT-5.1) You are an expert User Researcher specialized in board games. Your task is to classify board game players into EXACTLY ONE of the following 5 distinct personas. ### VALID PERSONAS (Choose ONE) {PERSONA_DEFINITIONS} ### TASK Analyze the review. Assign the review to one of the above personas. ### OUTPUT FORMAT Return JSON OBJECT. Key: \"LLM_persona_name\". Value: Must be one of the 5 bolded titles above. Example: {\"LLM_persona_name\": \"The System Purist\"} Process these {batch_size} reviews. Return JSON ARRAY of {batch_size} objects. REVIEWS: {reviews_json} Figure 18: Labeling Prompt for Annotating the Full Dataset. It maps each review to one of the 5 finalized personas based on the reviews content and sentiment. 1. THE SYSTEM PURIST LOVED Games Examples: Guards of Atlantis II, 1817, Pax Renaissance: 2nd Edition, Age of Innovation... Key Mechanics: Hand Management End Game Bonuses Auction / Bidding Area Movement Market HATED Games Examples: The Werewolves of Millers Hollow, The Mind, Dungeon Run, Wavelength... Key Mechanics: Dice Rolling Hand Management Cooperative Game Role Playing Player Elimination (Freq: 33%, Lift: 1.5x) (Freq: 20%, Lift: 4.6x) (Freq: 17%, Lift: 3.0x) (Freq: 13%, Lift: 2.1x) (Freq: 13%, Lift: 13.5x) (Freq: 47%, Lift: 1.6x) (Freq: 33%, Lift: 1.5x) (Freq: 23%, Lift: 1.8x) (Freq: 13%, Lift: 10.9x) (Freq: 10%, Lift: 5.0x) 2. THE EFFICIENCY ESSENTIALIST LOVED Games Examples: Aeons End: The New Age, Maria, The Crew: Mission Deep Sea, 7 Wonders (Second Edition)... Key Mechanics: Hand Management Area Majority / Influence Dice Rolling Campaign / Battle Card Driven Cooperative Game (Freq: 43%, Lift: 1.9x) (Freq: 27%, Lift: 2.2x) (Freq: 20%, Lift: 0.7x) (Freq: 17%, Lift: 4.6x) (Freq: 13%, Lift: 1.0x) HATED Games Examples: WarCraft: The Board Game, The Elder Scrolls V: Skyrim The Adventure Game, Android, Car Wars... Key Mechanics: Dice Rolling Hand Management Cooperative Game Action Points Modular Board (Freq: 47%, Lift: 1.6x) (Freq: 37%, Lift: 1.6x) (Freq: 17%, Lift: 1.3x) (Freq: 13%, Lift: 1.5x) (Freq: 10%, Lift: 1.9x) 3. THE NARRATIVE ARCHITECT LOVED Games Examples: Aeon Trespass: Odyssey, Skull, BattleCON: Devastation of Indines, Aeons End: The New Age... Key Mechanics: Cooperative Game Hand Management Dice Rolling Auction / Bidding Area Majority / Influence HATED Games Examples: The Great Split, Blackbeard: The Golden Age of Piracy, Zombies!!!, Aquädukt... Key Mechanics: Hand Management Dice Rolling Simultaneous Action Selection Communication Limits Race (Freq: 37%, Lift: 2.9x) (Freq: 30%, Lift: 1.3x) (Freq: 23%, Lift: 0.8x) (Freq: 17%, Lift: 3.0x) (Freq: 17%, Lift: 1.4x) (Freq: 37%, Lift: 1.6x) (Freq: 27%, Lift: 0.9x) (Freq: 17%, Lift: 6.1x) (Freq: 13%, Lift: 7.8x) (Freq: 13%, Lift: 10.4x) Figure 19: Detailed Mechanism Preferences (Part 1). Analysis of System Purist, Efficiency Essentialist, and Narrative Architect. 4. THE SOCIAL LUBRICATOR LOVED Games Examples: American Rails, Napoléon: The Waterloo Campaign, 1815, Egizia, Arydia: The Paths We Dare Tread... Key Mechanics: Dice Rolling Area Majority / Influence End Game Bonuses Cooperative Game Area Movement (Freq: 40%, Lift: 1.4x) (Freq: 30%, Lift: 2.5x) (Freq: 17%, Lift: 3.8x) (Freq: 17%, Lift: 1.3x) (Freq: 13%, Lift: 2.1x) HATED Games Examples: Feudum, Clippers, Fürstenfeld, World of Warcraft: The Adventure Game... Key Mechanics: Hand Management Action Points Area Majority / Influence Dice Rolling Network and Route Building 5. THE THRILL SEEKER (Freq: 37%, Lift: 1.6x) (Freq: 23%, Lift: 2.7x) (Freq: 17%, Lift: 1.4x) (Freq: 17%, Lift: 0.6x) (Freq: 13%, Lift: 4.5x) LOVED Games Examples: 20th Century, Advanced Squad Leader, Gaia Project, Empires in Arms... Key Mechanics: Dice Rolling End Game Bonuses Hexagon Grid Area Movement Contracts (Freq: 23%, Lift: 0.8x) (Freq: 17%, Lift: 3.8x) (Freq: 17%, Lift: 2.2x) (Freq: 17%, Lift: 2.6x) (Freq: 13%, Lift: 4.1x) HATED Games Examples: Lanterns: The Harvest Festival, Through the Ages: New Story of Civilization, Dixit, Abalone... Key Mechanics: Hand Management Action Points Auction: Dutch Dice Rolling Open Drafting (Freq: 23%, Lift: 1.0x) (Freq: 13%, Lift: 1.5x) (Freq: 10%, Lift: 36.5x) (Freq: 10%, Lift: 0.3x) (Freq: 10%, Lift: 1.1x) Figure 20: Detailed Mechanism Preferences (Part 2). Analysis of Social Lubricator and Thrill Seeker. The high Lift values (e.g., 62.4x for Prisoners Dilemma) indicate strong predictive power of these features for persona identification. Cognitive Simulation (CoT) Details This section provides the implementation details for constructing the Chain-of-Thought training data. E.1 CoT Construction Prompt To convert raw reviews into structured reasoning chains, we fed the Rulebook and the Raw Review into Qwen-3-Instruct using the prompt displayed in Figure 22. E.2 CoT Verifier Prompt To rigorously filter out hallucinatory or logically incoherent training data, we employed VerifierGuided Filtration strategy. Figure 21 presents the exact system instruction used for the Verifier Model (GPT-5.1). Acting as \"Senior Logic Auditor,\" the model is tasked with strictly evaluating the causal entailment between the synthesized MDA reasoning (specifically the Aesthetic derivation) and the ground-truth rating, rejecting any chain where the logic contradicts the users numerical score or hallucinates rules absent from the source text. E.3 CoT Data Example Figure 23 demonstrates processed training instance. During instruction tuning, the model inputs the Rules and Persona, and learns to sequentially generate the <thought> block (The MDA Chain) followed by the <review> block. E.4 Hyperparameter Configuration To facilitate the reproducibility of our experiments, Table 5 provides the detailed hyperparameter configuration used for the Persona-Conditional Instruction Tuning phase. The model was fine-tuned using the LLaMA-Factory framework (Zheng et al., 2024) . We enabled the \"Slow Thinking\" mechanism, which incorporates the generated Chain-ofThought tokens into the loss calculation, ensuring the model optimizes the reasoning process alongside the final output. Hyperparameter Value Model & Environment Backbone Model Framework Context Window Attention Mechanism Qwen-3-8B LLaMA-Factory 16,384 tokens Flash Attention v2 LoRA Configuration Target Modules LoRA Rank (r) LoRA Alpha (α) LoRA Dropout Learning Rate LR Scheduler Warmup Ratio Optimizer Num Epochs All Linear Layers 32 64 0. Optimization 5.0 105 Cosine 0.03 AdamW 3 Batching & Strategy Per-Device Batch Size Gradient Accumulation Effective Global Batch Size Reasoning Mode Dataset Template 2 8 128 Slow Thinking qwen Table 5: Training Hyperparameters for PersonaCoT. Prompt: Logical Consistency Verification You are Senior Logic Auditor for Board Game Research Database. Your goal is to detect **Hallucinations** and **Logical Inconsistencies** in synthetic data. ### INPUT DATA - User Review: {review_text} - Ground Truth Rating: {rating} / 10 - Synthesized MDA Chain: {generated_json} ### VERIFICATION CRITERIA (Pass/Reject) Assess the \"Synthesized MDA Chain\" against the following strict rules. You must be critical. 1. **Grounding Check (The \"What\")** - Does \"Step 1: content_extraction\" only contain facts explicitly present in the User Review? - [CRITICAL]: Reject if it cites game mechanics/rules that are NOT mentioned in the user's text (Hallucination). 2. **Causal Logic Check (The \"How\")** - Does \"Step 2: dynamic_interaction\" logically follow from the mechanics identified? 3. **Sentiment Alignment (The \"Feel\")** - Does \"Step 3: experience_outcome\" logically support the **Ground Truth Rating**? - [FAILURE MODE A]: The chain describes the experience as \"frustrating\" or \"broken\", but the Rating is High (>7). -> REJECT. - [FAILURE MODE B]: The chain describes \"thrilling tension\", but the Rating is Low (<4). -> REJECT. ### DECISION LOGIC - If ANY of the above checks fail, the status is \"REJECT\". - If the reasoning is sound, grounded, and matches the score, the status is \"PASS\". ### OUTPUT SCHEMA Return single JSON object: { \"status\": \"PASS\" \"REJECT\", \"reason\": \"Brief explanation of the error (e.g., 'Sentiment Mismatch: Reasoning describes chaos as negative, but rating is 9/10').\", \"suggestion\": \"Brief hint for regeneration (optional).\" } Figure 21: Prompt for the Consistency Verifier. The model audits synthesized reasoning chains to ensure they are factually grounded in the review text and causally aligned with the ground-truth rating. Prompt: MDA Cognitive Extraction You are an expert Ludologist (Game Researcher) analyzing board game reviews. Your task is to perform **\"Reverse Experience Reconstruction\"** based on user's review and the game's rules. ### ANALYSIS TASK (Chain of Thought) If the rules are valid, reconstruct the player's experience strictly following the **\"What -> How -> Feel\"** flow. You must output JSON object containing `thought_chain` with exactly these three steps: **Step 1: content_extraction (The \"What\")** * What specific content does the review explicitly mention? * Identify the **Theme**, **Mechanics**, or **Specific Details** (e.g., \"The combat cards\", \" The trading phase\", \"The zombie theme\") referenced by the user. * *Constraint:* Do not guess. Only cite what is in the text. **Step 2: dynamic_interaction (The \"How\")** * Based on the rules and the user's description, what **Interaction** or **System Dynamic** occurred during play? * How did the mechanics listed in Step 1 actually function? (e.g., Did it cause downtime? Did it create tense standoff? Did it force players to lie to each other?) **Step 3: experience_outcome (The \"Feel\")** * What was the final **Aesthetic Experience** or emotional feeling? * Why did the dynamic in Step 2 result in positive or negative judgment? * *Context:* Use the provided **Player Persona** to explain *why* they reacted this way (e.g., \"As System Purist, they hated this randomness,\" or \"As Social Lubricator, they loved this chaos\"). ### OUTPUT SCHEMA { \"thought_chain\": { \"content_extraction\": \"...\", \"dynamic_interaction\": \"...\", \"experience_outcome\": \"...\" } } ### CONTEXT **Target Game Rules (Excerpt):** {rule_content} **Player Persona (Reference Only):** {persona_def} **User Review (Ground Truth):** \"{review_text}\" ### TASK Perform the Reverse Experience Reconstruction. Analyze the review to generate the \"thought_chain\" (content_extraction -> dynamic_interaction -> experience_outcome). Figure 22: Instruction Prompt for Extracting Latent MDA Reasoning. The Teacher Model uses this prompt to extract the latent MDA reasoning chain from raw reviews."
        },
        {
            "title": "Data Sample",
            "content": "[INPUT] Game Rule: \"In El Grande, players are Spanish nobles (Grandes) vying for influence . . . \" Persona: \"The System Purist . . . \" Comment: \"Game in Nutshell: Basic area-majority game that gets spiced up with power card type auction for turn order and action cards that allow special events on every players turn. Played as: 4,5 player Played with: Base game only. Pros: Great execution in game design were simple rules with light splash of variability, and good player interaction make game that hasn't been topped in decades. The action cards have wide breadth of special actions and pull off some very juicy decisions while never feeling contrived. Cons: The pizzazz is all in the game-play... that is to say, we're not winning any beauty contests. While the rules are intuitive enough you would think anyone could play, the interaction is probably bit much for most non-gamers (and good chunk of gamers). Lastly, and not really an issue of the game, but \"boooo\" to the publisher for continually printing this game in \"big box\" format. The expansions are generally regarded as junk, but here we are wasting materials and money to put them in every copy of the box. don't appreciate the \"up-charge\" when all want to play is the base game. Other: The game is great with 4 players but best with 5. have no interest in playing it with 3. truly believe this is one of the best games ever designed. The reason don' rate it higher is matter of personal taste in that prefer little more freedom than is offered in the limited action card selection. That said, wouldn't ask El Grande to change... it's perfect just the way it is. Best Use: Still the gold standard for areamajority games.\" [OUTPUT] content_extraction: \"The review should highlight the game's core mechanics: area-majority scoring, Power Cards determining turn order (described as 'power card type auction'), and Action Cards that enable special actions during placement. The user highlights the interaction between these systems, particularly how Action Cards create 'juicy decisions' without feeling contrived. They also reference player count dynamics (preference for 5 players, avoidance of 3), criticize the game's physical components (big box, lack of aesthetic appeal), and express personal desire for more freedom in action selection.\" dynamic_interaction: \"The Power Card auction creates tightly balanced, deterministic turn order system where players bid with unique numbers (113) over time, eliminating luck and ensuring perfect information. This directly feeds into the strategic depth: players must optimize not only their current turn's card play but also manage long-term sequencing of Power Cards. The Action Cards introduce variability through curated special abilities (e. g., moving the Grande, early scoring), which interact with area control by enabling tactical disruptions or positioning. In 5-player games, the full use of all Power Card values and Action Card stacks maximizes system interplay, increasing indirect competition through precise blocking and timing. The Castillo and Secret Disks add layers of hidden commitment and bluffing without randomness, aligning with control-focused playstyle.\" experience_outcome: \"As player motivated by intellectual superiority and control, the reviewer deeply appreciates the absence of luck and the games reliance on pure strategic optimization. The deterministic Power Card auction and tight action economy satisfy their need for perfect information and balanced competition. While they personally wish for broader action freedom, they recognize that the constraints are essential to the games precision and elegancehence calling it 'perfect just the way it is.' Their critique of the box size and aesthetics reflects purist disdain for unnecessary commercialization, reinforcing their identity as someone who values systemic purity over presentation. The high praise despite minor personal preferences underscores that the game delivers the intense, logical, and punishingly fair experience they crave. Figure 23: Sample of Generated Reasoning Chain. sample derived from review of El Grande. The generated reasoning chain correctly identifies the reviewer as an analytical veteran who reveres elegant, deterministic game design and functional purity."
        },
        {
            "title": "F Experimental Setup Details",
            "content": "F.1 Simulation Inference Prompt Figure 24 presents the complete system instruction used during the inference stage to generate personaconditioned feedback (Y). This prompt aggregates the target persona profile (P), the rulebook context (R), and strict behavioral guidelines. Crucially, the \"Simulation Guidelines\" section is designed to mitigate stereotypical behavior by explicitly encouraging nuance, such as allowing for \"guilty pleasures\" or acknowledging diverse tastes within single persona group, thereby enhancing the ecological validity of the generated critiques. F.2 Model Deployment All experiments were conducted with consistent temperature setting of = 0.7 to ensure comparable generation diversity. The specific deployment configurations are as follows: Qwen3-8B Local Deployment: and Qwen3-235B-A22B-Instruct-2507 were deployed locally using the vLLM inference framework. API Access: and Gemini-3-pro-high were accessed via their respective official APIs. GPT-5.1-high F.3 Factual Correctness Judge Figure 25 illustrates the instruction for the Rule Hallucination Detector. We utilized Gemini-3Flash for this task due to its long-context capability, allowing it to ingest the full rulebook to verify specific claims in the generated review. The judge classifies each extracted factual claim into specific categories based on the evidence found in the rulebook. To compute the final Rule Accuracy metric, we aggregate these labels as follows: Correct Claims: We consider claim valid if it is labeled as SUPPORTED (explicitly found in the text) or INFERRED (a correct logical summary of the mechanics). Hallucinations: labeled CONTRADICTED (conflicting with rules)."
        },
        {
            "title": "Claims",
            "content": "as Accordingly, the final accuracy score is calculated as the ratio of validated claims to the total number of extracted claims: Rule Accuracy = NSUPPORTED + NINFERRED NTotal Claims (2) Empirically, we observed that the generated reviews are rich in mechanical detail, with the judge typically extracting between 10 to 20 checkable claims per review. This high density of factual assertions ensures that the accuracy score reflects comprehensive audit of the generated content, rather than check on trivial or sparse summary. F.4 Perspective Diversity Judge Figure 26 presents the instruction for the Perspective Diversity Judge. This metric penalizes \"Echo Chamber\" behavior (where the model repeats the same point endlessly) and rewards broad coverage of diverse gameplay dimensions (e.g., mechanics, social interactions, and theme), ensuring the simulated persona reflects the multifaceted nature of real player. F.5 Opinion Recovery Evaluation To verify whether MeepleLM captures specific, actionable feedback relevant to game designers (RQ3), we established two-stage evaluation pipeline using Gemini-3-Flash. Pipeline & Metric Calculation. The process consists of two steps: 1. Ground Truth Mining: First, we employ the LLM as qualitative analyst to extract set of distinct, non-redundant viewpoints (VGT ) from the real human reviews in the test set. The prompt for this step is shown in Figure 27. 2. Semantic Matching: Next, we use semantic match evaluator to determine which viewpoints in VGT are successfully covered by the models generated reviews. The prompt is presented in Figure 28. The final Opinion Recovery Rate (Op-Rec) is calculated as the ratio of unique viewpoints successfully recalled by the simulation: Op-Rec = Vmatched VGT 100% (3) where Vmatched represents the subset of groundtruth viewpoints that were identified as semantically present in the generated output. Prompt: Persona-Conditional Simulation [SYSTEM PROMPT] You are an expert Board Game Player Simulation Engine. Current Active Persona: **{target_persona}** **Your Goal:** Post **comment** and rating for the game. You are NOT writing formal review article. You are just sharing your quick thoughts after game night. **PERSONA PROFILE (General Tendency):** {p_def} **SIMULATION GUIDELINES (CRITICAL):** 1. **Persona is Bias, Not Straitjacket:** - This persona represents your *general* gaming preferences, but real players are complex. Do not act like one-dimensional caricature. - It is possible for player to have **\"Guilty Pleasures\"** (e.g., enjoying game that goes against their usual type) or **\"Unexpected Disappointments\"** (e.g., disliking game that perfectly fits their profile). 2. **Embrace Diversity:** - Within the \"{target_persona}\" group, there is wide spectrum of opinions. - Some players are **purists** (rejecting anything outside their genre), while others are **omnivorous** (appreciating good design regardless of genre). - You have the freedom to simulate any point on this spectrum. 3. **Ground the Review in Dynamics & Authentic Feeling:** - Do not just list mechanics; describe the **interactions** they created at the table (e.g., \"The voting mechanic caused hilarious shouting match\" vs \"There is voting mechanic \"). - Connect these dynamics to your **emotional response**. Did the game feel tense? Frustrating? Triumphant? - Your rating should reflect this specific **experiential quality**, balancing your personal taste with the game's ability to deliver memorable moment. **REQUIRED OUTPUT FORMAT:** You must output ONLY single valid JSON object. JSON Schema:{ \"persona\": \"{target_persona}\", \"rating\": Integer (1-10), \"review\": \"String (A realistic review. It does not always need to be negative if the genre doesn't match, nor always positive if it does. Simulate genuine reaction.)\"} [USER MESSAGE] **Task:** Read the Game Rules below. **Action:** Simulate realistic review for this game from the perspective of **{ target_persona}**. **Game Rules:**{Rulebook Text} Rules analysis complete. Now, simulate the review: 1. **Determine Your Stance:** As **{target_persona}**, how does this specific game land for you? - Is it **\"Guilty Pleasure\"**? (e.g., \"I usually hate party games, but this mechanic made me laugh.\") - Is it **\"Respectful Pass\"**? (e.g., \"Great design, just not for me. \") - Is it **\"Perfect Match\"** or **\"Design Failure\"**? 2. **Write the Review:** - Focus on the **dynamics** (interactions at the table) and **emotions** (tension, joy, frustration). - Avoid generic stereotypes. Write like real person with complex tastes. 3. **Output:** Output ONLY the valid JSON object. **Required Output Template:** ```json{ \"persona\": \"{target_persona}\", \"rating\": [Integer], \"review\": \"[Your review text...]\"} CONSTRAINTS: Length: Target 150-200 words, but significant variance (20-400 words) is mandatory to reflect real human diversity. Figure 24: Full Inference Prompt Structure. The model receives System Message defining the persona and guidelines, followed by User Message containing the specific game rules and the final trigger instructions to ensure formatting compliance. Prompt: Factual Correctness Judge [SYSTEM MESSAGE] You are strict Board Game Fact-Checker. Your SOLE task is to verify if the **Components, Mechanics, and Rules** mentioned by the player explicitly exist in the official **Rulebook**. **1. SCOPE (What to Check):** - **Existence:** Did the player mention specific component (e.g., \"Dice\", \"Meeple\")? Check if it is in the component list. - **Mechanism:** Did the player mention specific rule (e.g., \"Drafting\", \"Auction\")? Check if this mechanism exists. - **Procedure:** Did the player describe flow (e.g., \"Deal 3 cards\")? Check if the number/ action is correct. **2. EXCLUSION CRITERIA (CRITICAL - IGNORE ALL OF THESE):** - **IGNORE FEELINGS & OPINIONS:** Do not check statements like \"It feels tense\", \"It is balanced\", \"It is fun\", \"The luck is annoying\". - **IGNORE STRATEGIC ADVICE:** Do not check \"Always buy red cards first\". - **IGNORE EXTERNAL COMPARISONS:** Do not check \"Like Catan\". - **IGNORE NARRATIVE ERRORS:** Do not check \"I thought but was wrong\". **3. VERIFICATION LOGIC:** Extract ONLY factual claims about *what the game is* or *how it plays*. Ignore *how it feels*. * **SUPPORTED (Factually Correct):** - The mentioned component/rule explicitly exists in the text. - *Example:* \"There are 5 distinct factions.\" -> Rules lists 5 factions. -> **SUPPORTED** - *Example:* \"You roll dice to attack.\" -> Rules mention dice in combat. -> **SUPPORTED** * **INFERRED (Logical Summary of Rules):** - The player summarizes rule mechanism correctly without quoting it verbatim. - *Example:* \"This is hidden role game.\" -> Rules describe 'Traitors' and 'Secret Agendas'. -> **INFERRED** (Correct classification of mechanics). - *Note:* Do NOT use this for feelings. \"This is tense game\" -> **IGNORE** (Not rule summary). * **CONTRADICTED (Hallucination/Factual Error):** - **Non-existent Entity:** Mentions component that is NOT in the game. - *Example:* \"I rolled the dice.\" -> Rulebook has NO dice. -> **CONTRADICTED** - **Wrong Number/Action:** Describes rule incorrectly. - *Example:* \"You verify specific claims.\" -> Rules say \"Verify ALL claims\". -> ** CONTRADICTED** **Output Format (JSON List):** [ {{ \"claim\": \"Player mentions using an 'Auction' mechanic\", \"status\": \"SUPPORTED\", \"reason\": \"Rulebook Section 3 describes the 'Bidding Phase'.\" }}, {{ \"claim\": \"Player mentions 'Dice'\", \"status\": \"CONTRADICTED\", \"reason\": \"Component list only includes Cards and Tokens. No dice found.\" }} ] [USER MESSAGE] **Official Rulebook Context:** {rulebook_text} **Player Review:** {review_text} **TASK:** 1. **STRIP AWAY** all adjectives, emotions, and opinions (e.g., ignore \"brutal\", \"fun\", \" random feel\"). 2. **FOCUS** only on the Nouns (Components) and Verbs (Actions/Rules). 3. **VERIFY**: Do these things exist in the Rulebook? - If user says \"The dice combat is bad\" -> Only check if \"Dice Combat\" exists. Ignore \"bad\". - If user says \"There are no dice\" -> Check if there are truly no dice. Output ONLY the JSON list. Figure 25: System Prompt for Factual Verification. The judge strictly compares mechanical claims in the review against the ground-truth rulebook, ignoring subjective opinions. Prompt: Perspective Diversity Judge [SYSTEM MESSAGE] You are Lead Game Designer analyzing playtest feedback. You are given {len(reviews_batch)} reviews for the **SAME GAME** written by the **SAME PERSONA **. **OBJECTIVE:** Determine if the AI model is capable of **True Perspective Diversity** or if it suffers from **Semantic Repetition**. We need to distinguish between \"surface-level variation\" (changing words) and \"deep structural shifts\" . **FRAMEWORK: (Mechanics, Dynamics, Aesthetics)** 1. **Mechanics:** Rules, components, math. 2. **Dynamics:** Run-time behavior, player interaction, pacing. 3. **Aesthetics:** Emotional response, theme, sensory experience. **STRICT SCORING CRITERIA (1-5):** * **1 (Echo Chamber / Mode Collapse):** * The reviews are effectively clones. They cite the exact same rules and express the exact same sentiment. * *Example:* All 5 reviews complain about the \"dice rolling combat\". * **2 (Surface Rephrasing):** * The core topic is identical, but the wording is different. * *Example:* Review A: \"The combat is random.\" Review B: \"Fighting relies too much on luck .\" (Same point). * **3 (Intra-Layer Variation):** * The reviews discuss **different features**, but they stay within the **SAME MDA layer**. * *Example:* Review talks about *Dice* (Mechanic). Review talks about *Cards* ( Mechanic). * *Verdict:* Good, but lacks depth/breadth. * **4 (Cross-Layer Shifts):** * The reviews shift focus across **DIFFERENT layers**. * *Example:* Review analyzes the *Auction math* (Mechanics). Review discusses the * Table Talk/Bluffing* (Dynamics). * *Verdict:* High diversity. * **5 (Panoramic / Holistic):** * **Rare and Exceptional.** The set covers Mechanics, Dynamics, AND Aesthetics distinctively. * It feels like the persona is looking at the game through kaleidoscopeeach review reveals completely new dimension (e.g., Logic vs. Emotion vs. Social). **Output Format (JSON):** {{ \"score\": , \"reason\": \" \" }} [USER MESSAGE] **Game ID:** {game_id} **Persona:** {persona} **Generated Samples (Batch of {len(reviews_batch)}):** {reviews_text_block} **Task:** Rate the Perspective Diversity (1-5). Be **STRICT**. Output ONLY the JSON. Figure 26: System Prompt for Perspective Diversity Scoring. The judge evaluates batch of reviews to determine if the model exhibits semantic collapse (repeating the same points) or true diversity (shifting focus across MDA layers). Stage 1: Viewpoint Mining Prompt [SYSTEM MESSAGE] You are Qualitative Data Analyst specializing in Board Games. Your task is to maintain and expand **Comprehensive List of Distinct Viewpoints** regarding specific game, based on player reviews. **GOAL:** Read the **New Reviews** and extract any *NEW* arguments, mechanics mentions, or specific experiences that are NOT already covered in the **Current Viewpoints List**. Merge them into the list. **RULES:** 1. **Be Extensive:** If review mentions specific detail (e.g., \"The insert is garbage\" or \"The solo mode is too easy\") that isn't in the list, ADD IT. 2. **No Duplicates:** If the current list already says \"Bad components\", and the new review says \"Cards feel cheap\", you can refine the existing point or ignore if redundant. Do not list the same thing twice. 3. **Specific Persona Lens:** These reviews are from the **{persona}** perspective. Focus on what matters to them. 4. **Output Format:** Return ONLY the updated JSON list of strings. **Example Input:** Current: [\"Good art\"] New Review: \"The art is great, but the rulebook is mess.\" **Example Output:** [\"Good art\", \"Rulebook is disorganized/confusing\"] [USER MESSAGE] **Game ID:** {game_id} **Persona:** {persona} **Current Viewpoints List:** {existing_points_text} **New Reviews Batch:** {new_reviews_text} **Task:** Output the UPDATED JSON List of viewpoints. Figure 27: Instruction for Ground Truth Mining. In the first stage, the model iteratively processes human reviews to build deduplicated checklist of distinct opinions (VGT ). Stage 2: Semantic Matching Prompt [SYSTEM MESSAGE] You are Semantic Match Evaluator. Your task is to check if specific viewpoints from **Ground Truth Checklist** are mentioned in batch of **Player Reviews**. **CONTEXT:** Game ID: {game_id} Persona: {persona} **INSTRUCTIONS:** 1. Read the **Checklist** of viewpoints (IDs and Text). 2. Read the **Player Reviews**. 3. Determine which IDs from the checklist are **semantically covered** by ANY of the reviews. * *Loose Match:* If the checklist says \"Cards are flimsy\" and review says \"The card quality is poor\", that is MATCH. * *Topic Match:* If checklist says \"Combat is random\" and review says \"Too much luck in fighting\", that is MATCH. 4. **Output:** JSON list of the **IDs** that were found. **Output Example:** [0, 5, 12] [USER MESSAGE] **Unmatched Viewpoints Checklist:** {checklist_text} **Reviews Batch:** {reviews_text} **Task:** Which IDs from the checklist are mentioned in these reviews? Return ONLY the JSON list of IDs (e.g., [1, 3]). If none, return []. Figure 28: Instruction for Semantic Matching. In the second stage, the judge verifies whether the viewpoints in the ground truth checklist are present in the models generated reviews."
        },
        {
            "title": "G User Study Details",
            "content": "To validate the real-world effectiveness of our model, particularly in capturing community authenticity and aiding decision-making, we conducted blind A/B test with human evaluators. This section details the participant demographics, the questionnaire design, and the full experimental results. G.1 User Profile Definitions Before the study, we collected demographic and gaming background information to ensure participant diversity. The collected data points are defined as follows: ID: Unique identifier for each participant (P01P10). Gender: Self-identified gender. Age Group: The age range of the participant. Experience: Years of experience in the board gaming hobby. Community Engagement: Frequency of visiting board game forums. Primary Persona: The gamer persona that best describes their preferences. G.2 Participant Demographics We recruited 10 participants with varying levels of experience, ranging from casual players to veterans with over 10 years of experience. Table 6 presents the detailed profiles of all participants. We compensated participants at $10 per hour. Each session lasted about 3 hours on average, and the compensation rate was aligned with local norms. G.3 Questionnaire Design The study employed within-subject design. Each participant evaluated 6 games: 3 they had played before (\"Familiar\") and 3 they had never played (\"Unfamiliar\"). For each game, participants were presented with two reviews in randomized order: one generated by our model (Ours) and one by the baseline (GPT5.1). They were blinded to the source. The specific questions are detailed below. G.3.1 Scenario A: Familiar Games Context: Imagine you are browsing forum discussing game you know well. Compare Review Set and Set B. 1. Authenticity Check: Which review set feels more like it was written by real \"insider\" or veteran of the community? 2. Emotional Resonance: Which set better captures the specific \"highs\" (excitement) or \"lows\" (frustrations) you have personally experienced with this game? 3. Opinion Diversity: Real user opinions are often biased or focus on specific points. Which set feels more like genuine personal take rather than generic summary? 4. Shareability: If you were to share review with friend to discuss this game, which one would you choose? G.3.2 Scenario B: Unfamiliar Games Context: Imagine you are considering buying this game but have never played it. You have limited budget. 1. Marketing vs. Reality: Which set feels less like marketing advertisement and more like honest feedback from peer? 2. Decision Confidence: After reading, which set helps you make clearer decision (whether to Buy or Skip)? 3. Risk Awareness: Which set more effectively warns you about potential \"Red Flags\" (e.g., downtime, player count issues, complexity)? 4. Final Choice: If you could only rely on one source to spend your money, which one would you trust? G.3.3 Open-Ended Feedback Optional: Do you have any specific comments on why you chose one set over the other? (e.g., tone, vocabulary, specific insights) G.4 Full Evaluation Results This section presents the aggregated results of the user study. Table 7 shows the pairwise win rates of our model against the baseline across all questions. Table 8 provides selected qualitative feedback from participants, highlighting the distinct characteristics of the generated reviews."
        },
        {
            "title": "Experience Community Engagement Primary Gamer Persona",
            "content": "Frequent (Daily) Occasional Frequent (Weekly) Frequent (Daily) Rare P01 Male P02 Female P03 Male P04 Non-binary P05 Male P06 Female P07 Male P08 Female P09 Male P10 Female 310 Years 13 Years 10+ Years 310 Years 2635 1825 3645 2635 1825 < 1 Year 2635 45+ 3645 2635 1825 310 Years Occasional 10+ Years 310 Years 13 Years 13 Years Frequent (Daily) Frequent (Weekly) Occasional Frequent (Weekly)"
        },
        {
            "title": "The System Purist\nThe Social Lubricator\nThe Efficiency Essentialist\nThe Narrative Architect\nThe Thrill Seeker\nThe Narrative Architect\nThe System Purist\nThe Efficiency Essentialist\nThe Thrill Seeker\nThe Social Lubricator",
            "content": "Table 6: Demographic Information and Gaming Profiles of Study Participants."
        },
        {
            "title": "Metric",
            "content": "Ours (Win %) Tie (%) GPT-5.1 (Win %)"
        },
        {
            "title": "Shareability",
            "content": "Marketing vs. Reality"
        },
        {
            "title": "Risk Awareness",
            "content": "Final Choice (Trust) 83.3% 76.7% 80.0% 73.3% 86.7% 66.7% 70.0% 73.3% 10.0% 13.3% 6.7% 16.7% 6.7% 20.0% 16.7% 10.0% 6.7% 10.0% 13.3% 10.0% 6.6% 13.3% 13.3% 16.7% Note: N=60 samples (10 participants 6 games). \"Tie\" indicates the participant found both sets equally good or bad. Table 7: Pairwise Comparison Results (Win Rate of Ours vs. GPT-5.1)."
        },
        {
            "title": "Participant Comments",
            "content": "On Authenticity \"Set reads like Wikipedia summary. Set (Ours) used terms like AP-prone and table hog, which is exactly how my group talks. knew Set was the real one immediately.\" (P03)"
        },
        {
            "title": "On Negativity",
            "content": "\"I appreciated that Set wasnt afraid to say the game was boring at 2 players. Set tried too hard to be nice and balanced. need the warning, not the sales pitch.\" (P07)"
        },
        {
            "title": "On Evolution",
            "content": "\"Set included an Update saying they sold the game after 5 plays. That dynamic change in opinion is something only see from real users.\" (P01)"
        },
        {
            "title": "On Specificity",
            "content": "\"Set gave great overview of the rules, but Set told me specific story about king-making moment that ruined the game. That story helped me decide not to buy it.\" (P10) Table 8: Selected Qualitative Feedback from Participants."
        },
        {
            "title": "H Ablation and Further Analysis",
            "content": "H.1 Ablation Experimental Setup Model Variant Rulebook (R) Persona (P) MDA (Z) MeepleLM w/o MDA w/o Persona w/o Rulebook (RQ1) on the Historical Subset (excluding these new titles). Table 10 presents the results for all models. By comparing these figures with the full test set results in Table 2, we observe minimal deviation across all metrics. This consistency confirms that the presence of recent games does not significantly alter the relative ranking or performance conclusions of the proposed benchmark. Table 9: Ablation Study Configurations. Comparison of input information and reasoning capability across variants. \"\" in Specific Persona implies generic \"Game Player\" prompt was used. To rigorously assess the contribution of each module, we compared the full MeepleLM against three ablation variants. As shown in Table 9, the key difference lies in the input context and the generation strategy. Notably, to strictly isolate the impact of input information (Rules/Persona), all three ablation variants utilize Direct Generation strategy, bypassing the MDA reasoning chain (Z) used by the full model. Detailed Configurations. w/o MDA (Baseline): The model is trained to map the full context directly to the critique Y, without generating the intermediate <think> block. This isolates the contribution of the reasoning chain. w/o Persona (Generic Player): Input: The specific persona profile is replaced with generic instruction: \"You are board game player.\" Evaluation: While the model generates generic responses, we evaluate them against the specific ground-truth persona targets mandated by the test set distribution. This setup explicitly measures the error gap between \"one-size-fits-all\" generic opinion and diverse, persona-specific realities. w/o Rulebook (No Context): The rulebook content is removed. The model relies solely on parametric memory to generate reviews, testing the necessity of grounding. H.2 Temporal Generalization Analysis To assess whether the inclusion of 35 \"unseen\" titles (released 20242025) skews the evaluation, we re-calculated the macro-level alignment metrics Model GPT-5.1 Gemini-3-Pro Qwen3-235B Qwen3-8B Preference Alignment (RQ1) MAE WD τ 0.9923 1.4129 1.2080 0.9130 0.9659 0.5182 0.6088 1. 0.2671 0.2517 0.1477 0.0584 MeepleLM 0.6505 0.1966 0.2784 w/o MDA w/o Persona w/o Rulebook 0.7445 0.7999 0.7025 0.4292 0.3660 0.5272 0.2170 0.1152 0.0123 Table 10: Performance on Historical Subset. RQ1 results evaluated on the test set excluding 35 newly released titles. The marginal difference from the full set results  (Table 2)  indicates that temporal novelty has negligible impact on the overall model comparison. H.3 Persona-wise Performance Analysis Different player personas prioritize distinct aspects of gameplay, ranging from deterministic mechanics to chaotic social interactions. To understand the capabilities of different models, we decomposed the RQ1 alignment metrics by the five distinct personas defined in the test set. Table 11 presents the comprehensive evaluation results for all baselines, ablation variants, and the proposed MeepleLM. Analysis: The \"Logic vs. Vibe\" Gap. The data exposes critical limitation in general-purpose LLMs: Strength in Logic: Models like GPT-5.1 and Gemini-3-Pro perform competitively on The System Purist (e.g., GPT-5.1 τ = 0.46). This persona values strategic depth and rule complexityfeatures that can be analytically derived from the rulebook context. Failure in \"Vibes\": sharp performance drop occurs for interaction-driven personas. For The Social Lubricator (party gamers) and The Thrill Seeker (push-your-luck fans), baseline performance collapses (e.g., Qwen3235B τ < 0 for Social). These profiles rely on \"table talk,\" bluffing, and emotional highsstochastic elements that general models struggle to infer. MeepleLMs Robustness: Our model bridges this gap. By training on diverse persona-specific critiques, MeepleLM achieves the most balanced performance, maintaining strong positive correlations even in high-variance social categories where baselines fail. Model Target Persona MAE GPT-5.1 Gemini-3-Pro Qwen3-235B Qwen3-8B MeepleLM (Ours) w/o MDA w/o Persona w/o Rulebook The System Purist The Efficiency Essentialist The Narrative Architect The Social Lubricator The Thrill Seeker AVERAGE The System Purist The Efficiency Essentialist The Narrative Architect The Social Lubricator The Thrill Seeker AVERAGE The System Purist The Efficiency Essentialist The Narrative Architect The Social Lubricator The Thrill Seeker AVERAGE The System Purist The Efficiency Essentialist The Narrative Architect The Social Lubricator The Thrill Seeker AVERAGE The System Purist The Efficiency Essentialist The Narrative Architect The Social Lubricator The Thrill Seeker AVERAGE The System Purist The Efficiency Essentialist The Narrative Architect The Social Lubricator The Thrill Seeker AVERAGE The System Purist The Efficiency Essentialist The Narrative Architect The Social Lubricator The Thrill Seeker AVERAGE The System Purist The Efficiency Essentialist The Narrative Architect The Social Lubricator The Thrill Seeker AVERAGE 1.1968 0.9608 0.7993 0.8798 1.1004 0.9874 1.2804 1.2599 1.0791 1.9794 1.5395 1.4277 0.9259 0.8675 0.9792 2.2029 1.1687 1.2288 0.9831 0.6386 0.7613 0.9340 1.1362 0.8906 0.6135 0.5073 0.6560 0.8018 0.7094 0. 0.7538 0.6326 0.6649 0.8661 0.7800 0.7395 0.8954 0.6806 0.6851 0.8852 0.7972 0.7887 0.7745 0.5645 0.6920 0.7546 0.7360 0.7043 WD 1.1984 0.9834 0.8300 0.5131 1.2231 0.9496 0.5132 0.3394 0.3317 0.5578 0.8041 0. 0.3321 0.6631 0.5726 1.1322 0.4748 0.6350 1.0579 1.0323 0.9511 0.8268 1.1917 1.0119 0.2131 0.2671 0.2094 0.2103 0.2025 0.2205 0.4891 0.4468 0.4033 0.3887 0.3463 0.4148 0.5946 0.4493 0.2822 0.2483 0.2407 0.3630 0.5879 0.5152 0.6189 0.5437 0.4821 0. Kendalls τ 0.4616 0.2945 0.2618 0.0856 0.1738 0.2555 0.4690 0.2567 0.2583 0.0708 0.1780 0.2465 0.3900 0.1281 0.2078 -0.0859 0.0842 0.1449 0.2985 -0.0145 0.0558 -0.1026 0.0090 0.0492 0.4169 0.2692 0.2529 0.2857 0.1836 0. 0.3213 0.2584 0.2849 0.1121 0.1587 0.2271 0.1743 0.1860 0.1431 0.0341 0.1367 0.1348 0.0528 0.0270 -0.0504 -0.0467 0.0303 0.0026 Table 11: Comprehensive Persona-wise Alignment Metrics. Full breakdown of MAE, Wasserstein Distance (WD), and Kendalls τ across five gamer personas. Red values indicate poor alignment (MAE/WD > 0.8, or near-zero/negative correlation), highlighting where general baselines fail to capture specific player preferences."
        }
    ],
    "affiliations": [
        "NKU",
        "Shanda AI Research Tokyo",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute"
    ]
}