{
    "paper_title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
    "authors": [
        "Dawei Li",
        "Bohan Jiang",
        "Liangjie Huang",
        "Alimohammad Beigi",
        "Chengshuai Zhao",
        "Zhen Tan",
        "Amrita Bhattacharjee",
        "Yuxuan Jiang",
        "Canyu Chen",
        "Tianhao Wu",
        "Kai Shu",
        "Lu Cheng",
        "Huan Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). However, traditional methods, whether matching-based or embedding-based, often fall short of judging subtle attributes and delivering satisfactory results. Recent advancements in Large Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs are leveraged to perform scoring, ranking, or selection across various tasks and applications. This paper provides a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to advance this emerging field. We begin by giving detailed definitions from both input and output perspectives. Then we introduce a comprehensive taxonomy to explore LLM-as-a-judge from three dimensions: what to judge, how to judge and where to judge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and highlight key challenges and promising directions, aiming to provide valuable insights and inspire future research in this promising research area. Paper list and more resources about LLM-as-a-judge can be found at \\url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge} and \\url{https://llm-as-a-judge.github.io}."
        },
        {
            "title": "Start",
            "content": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, Kai Shu, Lu Cheng, Huan Liu Arizona State University, University of Illinois Chicago, University of Maryland, Baltimore County, Illinois Institute of Technology, University of California, Berkeley, Emory University 4 2 0 2 5 2 ] . [ 1 4 9 5 6 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). However, traditional methods, whether matching-based or embedding-based, often fall short of judging subtle attributes and delivering satisfactory results. Recent advancements in Large Language Models (LLMs) inspire the \"LLM-as-ajudge\" paradigm, where LLMs are leveraged to perform scoring, ranking, or selection across various tasks and applications. This paper provides comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to advance this emerging field. We begin by giving detailed definitions from both input and output perspectives. Then we introduce comprehensive taxonomy to explore LLM-as-a-judge from three dimensions: what to judge, how to judge and where to judge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and highlight key challenges and promising directions, aiming to provide valuable insights and inspire future research in this promising research area12."
        },
        {
            "title": "Introduction",
            "content": "Assessment and evaluation have long been essential yet challenging tasks in machine learning and natural language processing (NLP), particularly for scoring and comparing various attributes (e.g., quality, relevance, and helpfulness) of list of given candidates (Sai et al., 2022; Chang et al., 2024). Traditional evaluation methods rely on static metrics like BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which measure quality by calculating word overlap between output and reference texts. While these automatic metrics are 1More resources on LLM-as-a-judge are on the website: https://llm-as-a-judge.github.io 2We release at: the about LLM-as-ahttps://github.com/llm-as-a-judge/ paper list judge Awesome-LLM-as-a-judge Figure 1: Overview of various input and output formats of LLM-as-a-judge. computationally efficient and used in many generation applications (Zhang et al., 2022, 2023a, 2024c), their reliance on n-gram matching and reference-based designs significantly limits their applicability in dynamic and open-ended scenarios (Liu et al., 2016; Reiter, 2018). With the rise of deep learning models (Devlin et al., 2019; Reimers and Gurevych, 2019), many embedding-based assessment methods (e.g., BERTScore (Zhang et al., 2020) and BARTScore (Yuan et al., 2021)) have also emerged. Although these small model-based metrics shift from word-level to embedding-level representations and offer greater flexibility, they still struggle to capture subtle attributes like helpfulness and harmlessness beyond relevance. The recent advanced large language models (LLMs) such as GPT-4 (Achiam et al., 2023) and o1 (Ope), have demonstrated striking performance in instruction following, query understanding and response generation. This progress has motivated researchers to propose the concept of LLM-as-ajudge (Zheng et al., 2023), which leverages the powerful LLMs to score, rank and select from group of candidates (Figure 1). The strong performance of LLMs (Brown et al., 2020) combined with well-designed assessment pipelines (Li et al., Attributes (3) Helpfulness (3.1) Harmlessness (3.2) Reliability (3.3) Relevance (3.4) Feasibility (3.5) Constitutional AI (Bai et al., 2022), RLAIF (Lee et al., 2023), MT-Bench (Zheng et al., 2023), Just-Eval (Lin et al., 2023), Starling (Zhu et al., 2024a), AUTO-J (Li et al., 2024e), OAIF (Guo et al., 2024), LLaMA Guard (Inan et al., 2023), TRUSTGPT (Huang et al., 2023b), Moral Choice (Scherrer et al., 2023), SORRY-Bench (Xie et al., 2024a), FLASK (Ye et al., 2024b), R-judge (Yuan et al., 2024b), Do-not-answer (Wang et al., 2024i), RAIN (Li et al.) FactScore (Min et al., 2023), HALU-J (Wang et al., 2024a), HalluJudge (Luo et al., 2024), HalluQA (Cheng et al., 2023), SaySelf (Xu et al., 2024b), (Wei et al., 2024b), Self-Alignment for Factuality (Zhang et al., 2024f), FaithScore (Jing et al., 2024), FENCE (Xie et al., 2024b) LLM-Eval (Lin and Chen, 2023a), MoT (Li and Qiu, 2023), (Thomas et al., 2024), (Ma et al., 2024), LLMRank (Hou et al., 2024), LLM Evaluation (Chiang and Lee, 2023), (Yang and Lin, 2024), (Chen et al., 2024a) (Abbasiantaeb et al., 2024), DALK (Li et al., 2024c), MJ-Bench (Chen et al., 2024e), RAP (Hao et al., 2023), ToT (Yao et al., 2023a), Auto-GPT (Yang et al., 2023), GoT (Besta et al., 2024), Diffagent (Zhao et al., 2024b), Routellm (Ong et al., 2024), MAD (Liang et al., 2023), SMoA (Li et al., 2024b) Overall Quality (3.6) (Gao et al., 2023), Just-Eval (Lin et al., 2023), ICE (Jain et al., 2023a), LLM-Eval (Lin and Chen, 2023b), GEMBA (Kocmi and Federmann, 2023), KIEVAL (Yu et al., 2024), OAIF (Guo et al., 2024), Comp-Analysis (Zhang et al., 2024a), LostITS (Huang et al., 2024) Tuning (4.1) Data Source (4.1.1) Tuning Techniques (4.1.2) Manually-labeled (4.1.1) AttrScore (Yue et al., 2023), PandaLM (Wang et al., 2024h), InstructScore (Xu et al., 2023), SELF-JUDGE (Lee et al., 2024), X-Eval (Liu et al., 2024a), CritiqueLLM (Ke et al., 2024), FLAMe (Vu et al., 2024), Synthetic Feedback (4.1.1) JudgeLM (Zhu et al., 2023), AUTO-J (Li et al., 2024e), Meta-Rewarding (Wu et al., 2024), Self-Taught (Wang et al., 2024f), HALU-J (Wang et al., 2024a), OFFSETBIAS (Park et al., 2024), SORRY-Bench (Xie et al., 2024a), LLaVA-Critic (Xiong et al., 2024b), PROMETHEUS2 (Kim et al., 2024), InstructScore (Xu et al., 2023), Supervised Fine-Tuning (4.1.2) PerSE (Wang et al., 2023b), INSTRUCTSCORE (Xu et al., 2023), CRITIQUELLM (Ke et al., 2024), PandaLM (Wang et al., 2024h), X-Eval (Liu et al., 2024a), AUTO-J (Li et al., 2024e), JudgeLM (Zhu et al., 2023), SORRY-Bench (Xie et al., 2024a), AttrScore (Yue et al., 2023), FLAMe (Vu et al., 2024), PROMETHEUS2 (Kim et al., 2024), SELF-JUDGE (Lee et al., 2024), CritiqueLLM (Ke et al., 2024), X-Eval (Liu et al., 2024a), Preference Learning (4.1.2) HALU-J (Wang et al., 2024a), OFFSETBIAS (Park et al., 2024), Themis (Hu et al., 2024), Meta-Rewarding(Wu et al., 2024), Self-Taught (Wang et al., 2024f), PORTIA(Li et al., 2024j) Swapping Operation (4.2.1) MT-Bench (Zheng et al., 2023), RLAIF (Lee et al., 2023), SALMON (Sun et al., 2024), SELF-JUDGE (Lee et al., 2024), Starling (Zhu et al., 2024a) Rule Augmentation (4.2.2) Constitutional AI (Bai et al., 2022), MoT (Li and Qiu, 2023), (Beigi et al., 2024a), AUTO-J (Li et al., 2024e), (Yu et al., 2024), CEB (Wang et al., 2024e), SALMON (Sun et al., 2024), SELF-JUDGE (Lee et al., 2024), DALK (Li et al., 2024c), (Qian et al., 2024), RevisEval (Zhang et al., 2024e), LLM-as-a-personalized-judge (Dong et al., 2024) (Bai et al., 2023a), (Murugadoss et al., 2024), (Gao et al., 2023), Prometheus (Kim et al.), KIEVAL (Liu et al., 2024b), OAIF (Guo et al., 2024), (Lahoti et al., 2023), RLAIF (Lee et al., 2023), LRQ-Fact u - - - L Methodology (4) Prompting (4.2) Multi-Agent Collaboration (4.2.3) PRD (Li et al., 2023), ChatEval(Chan et al., 2023), CoEvol (Li et al., 2024g) LRQ-Fact (Beigi et al., 2024a), Cascaded Selective Evaluation (Jung et al., 2024), Fellowship (Arif et al., 2024), (Zhang et al., 2023b), (Wu et al., 2023), MPA (Zhu et al., 2024b), JudgeLM (Zhu et al., 2023), Demonstration (4.2.4) Multi-Turn Interaction (4.2.5) Comparison Acceleration (4.2.6) ICE (Jain et al., 2023b), Little Giants (Kotonya et al., 2023), ALLURE (Hasanbeig et al., 2023), MSoR (Song et al., 2024) LLM-as-an-examine (Bai et al., 2023b), KIEVAL (Yu et al., 2024), Auto-Arena (Zhao et al., 2024c), (Moniri et al., 2024) (Liu et al., 2023a), OSP (Zhai et al., 2024), Starling (Zhu et al., 2024a), SELF-JUDGE (Lee et al., 2024) Application (5) Evaluation (5.1) Alignment (5.2) Retrieval (5.3) Reasoning (5.4) (Bi et al., 2023), (Fei et al., 2023), (Zhou et al., 2023), (Wang et al., 2023a), (Nan et al., 2024), (Zheng et al., 2023), (Gao et al., 2023), (Wu et al., 2023), (Cheng et al., 2023), (Lin and Chen, 2023b), (Mondorf and Plank, 2024), (Badshah and Sajjad, 2024), (Wang et al., 2024a), (Li et al., 2024f), (Parmar et al., 2024) , (Xu et al., 2024a), (Xiong et al., 2024b), (Chen et al., 2024c), (Zhao et al., 2024a), (Isaza-Giraldo et al., 2024), (Wang et al., 2024j), (Xie et al., 2024a), (Chan et al., 2023), (Moniri et al., 2024), (Xia et al., 2024), (Fatemi et al., 2024), (Bai et al., 2023a), (Kumar et al., 2024), (Bai et al., 2022), (Lee et al., 2023), (Sun et al., 2024), (Guo et al., 2024), (Arif et al., 2024), (Li et al., 2024g), (Yuan et al., 2024c), (Wu et al., 2024), (Pace et al., 2024), (Lee et al., 2024), (Tong et al., 2024), (Zhai et al., 2024), (Liu et al., 2024c), (Liang et al., 2024c), (Zhang et al., 2024f), (Zeng et al., 2024), (Ahn et al., 2024), (Weyssow et al., 2024), (Wang et al., 2024d) (Sun et al., 2023), (Thomas et al., 2023), (Ma et al., 2023), (Tang et al., 2024b), (Qin et al., 2024), (Ma et al., 2024), (Hou et al., 2024), (Li and Qiu, 2023), (Tang et al., 2024a), (Asai et al., 2024) (Zhuang et al., 2024a), (Rackauckas et al., 2024), (Zhang et al., 2024b), (Wang et al., 2024b), (Jeong et al., 2024), (Zhuang et al., 2024b), (Chen et al., 2024d) (Li et al., 2024c), (Yao et al., 2023b), (Creswell et al., 2023), (Wei et al., 2022b), (Yao et al., 2023a), (Yang et al., 2023), (Sha et al., 2023), (Hao et al., 2023), (Zhou et al., 2024b), (Lahoti et al., 2023), (Liang et al., 2023), (Li et al., 2024b), (Besta et al., 2024), (Ong et al., 2024), (Zhao et al., 2024b), (Kawabata and Sugawara, 2024), (Xie et al., 2024b), (Lightman et al., 2023), (Li et al.), (Setlur et al., 2024) Figure 2: Taxonomy of research in LLM-as-a-judge that consists of judging attribution, methodology and application. 2023; Beigi et al., 2024b; Bai et al., 2023a) leads to fine-grained and detailed judgment for various evaluation applications, significantly addressing the limitations of traditional evaluation methods, setting new standard for what is achievable in NLP evaluation. Beyond evaluation, LLM-as-a-judge has also been widely adopted across the entire LLM lifecycle, including tasks like alignment (Bai et al., 2022; Lee et al., 2023), retrieval (Li and Qiu, 2023; Li et al., 2024c), and reasoning (Liang et al., 2023; Zhao et al., 2024b). It empowers LLMs with series of advanced capabilities such as selfevolution (Sun et al., 2024), active retrieval (Li et al., 2024c) and decision-making (Yang et al., 2023), driving their transformation from traditional models to intelligent agents (Zhuge et al., 2024). However, as LLM-as-a-judge develops rapidly, challenges like judging bias and vulnerability (Koo et al., 2023a; Park et al., 2024) are also emerging. systematic review of current techniques and future challenges would therefore be valuable for advancing LLM-based judgment methods. In this survey, we delve into the details of LLMas-a-judge, aiming to provide comprehensive overview of LLM-based judgment. We begin with formal definition of LLM-as-a-judge by discussing its various input and output formats (Section 2). Next, we propose an in-depth and comprehensive taxonomy to address the three key questions (Section 3 - Section 5): input and output formats separately in Section 2.1 and Section 2.2. Attribute: What to judge? We delve into specific attributes assessed by the judge LLMs, including helpfulness, harmlessness, reliability, relevance, feasibility and overall quality. Methodology: How to judge? We explore various tuning and prompting techniques for LLMas-a-judge systems, including manually-labeled data, synthetic feedback, supervised fine-tuning, preference learning, swapping operation, rule augmentation, multi-agent collaboration, demonstration, multi-turn interaction and comparison acceleration. Application: Where to judge? We investigate the applications in which LLM-as-a-judge has been employed, including evaluation, alignment, retrieval and reasoning. Additionally, we collect existing benchmarks that evaluate LLM-as-a-judge from various perspectives in Section 6. Finally, we propose current challenges and promising direction for future research in LLM-as-a-judge in Section 7, including bias & vulnerability, dynamic & complex judgment, self-judging and human-LLM co-judgment. Differences from Other LLM-related Surveys. LLMs have been popular topic in recent years and got many related surveys (Zhao et al., 2023a; Chang et al., 2024; Xiong et al., 2024a). While there are several surveys focusing on LLM-based evaluation for natural language generation (NLG) evaluation (Gao et al., 2024a; Li et al., 2024i), this work aims to provide comprehensive overview of the LLM-as-a-judge approach. As mentioned, LLM-as-a-judge has already been applied in broader range of scenarios beyond evaluation, making it essential to offer general perspective for its summarization and categorization. Besides, there are also some surveys focus on LLM-powered applications, such as LLMs-based data annotation (Tan et al., 2024b), data augmentation (Zhou et al., 2024c), and self-correction (Pan et al., 2024). there is still lack of systematic However, and comprehensive survey dedicated to LLM-as-ajudge."
        },
        {
            "title": "2 Preliminary",
            "content": "In this section, we aim to provide detailed definition of LLM-as-a-judge, discussing the various 2."
        },
        {
            "title": "Input",
            "content": "Given judge LLM J, the assessment process can be formulated as: = J(C1, ...Cn). (1) Here Ci is the ith candidate to be judged and is the judging result. In this section, we categorize two kinds of input formats based on their different candidate number n. Point-Wise: When = 1, it becomes pointwise judgment where the judge LLMs will solely focus on one candidate sample (Gao et al., 2023). Pair/ List-Wise: When 2, it becomes pairwise (n = 2) or list-wise (n > 2) judgment where multiple candidate samples are provided together for the judge LLMs to compare and make comprehensive assessment (Zheng et al., 2023)."
        },
        {
            "title": "2.2 Output",
            "content": "In this section, we discuss three kinds of output of the judgment based on the different formats of R. Score: When each candidate sample is assigned continuous or discrete score: = {C1 : S1, ..., Cn : Sn}, it becomes score-based judgment. It is the most common and widely employed protocol that utilizes LLM to make score for making quantitative comparisons (Li et al., 2024a) or attribute detection (Xie et al., 2024a). Ranking: In ranking-based judgment, the output is ranking of each candidate sample, represented as = {Ci > ... > Cj}. This comparative approach is useful in scenarios where establishing rank order among candidates is required (Li et al., 2023). Selection: In selection-based judgment, the output involves selecting one or more optimal candidates, represented as = {Ci, ..., Cj} > {C1, ...Cn}. This method is particularly useful in decision-making (Yao et al., 2023a) or contentfiltering (Li et al., 2024c) contexts."
        },
        {
            "title": "3 Attribute",
            "content": "In this section, we categorize current research in LLM-as-a-judge from attribute perspectives. Figure 3 gives an overview summarization of what aspects can be assessed by the judge LLMs. to evaluate harmlessness. State-of-the-art LLMs are capable of being used effectively for content moderation, either off the shelf when guided with some policy guidelines3, or when fine-tuned on safe/unsafe data (Inan et al., 2023; Zhang et al., 2024e)4. Ye et al. (2024b) explore the feasibility of using LLMs to evaluate harmlessness in finegrained manner, among other attributes, and find that proprietary models perform much better than open source ones. Wang et al. (2024i) use OpenAIs GPT-4 to evaluate harmlessness and further compare the performance with that of smaller pretrained language model fine-tuned for this specific task. Additionally, Bai et al. (2022) use principles to guide LLMs to make harmlessness evaluations for alignment purposes, paradigm they call Constitutional AI. (Phute et al., 2023) use the same LLM to evaluate its responses as harmful, and provide insights and best practices for GPT-3.5 and Llama-2. Xie et al. (2024a) perform comprehensive comparison of several LLMs on benchmark of LLM safety refusals and find small LLMs are effective safety judges when used in fine-tuned settings. During inference time, (Li et al.) propose Rewindable Auto-regressive INference (RAIN), which allows LLMs to conduct self-evaluation and rewind for AI safety."
        },
        {
            "title": "3.3 Reliability",
            "content": "Reliability is crucial attribute for LLMs, enabling them to generate factual and faithful content while also expressing uncertainty or acknowledging gaps in knowledge about certain topics. Regarding factuality, Wang et al. (2024a) introduce HALU-J, critique-based hallucination judge that enhances factuality assessment by selecting relevant evidence and providing detailed critiques. Cheng et al. (2023) design an automated evaluation method using GPT-4 to judge whether models output is hallucinated. Additionally, several works adopt judge LLMs for long-form factuality evaluation. In the context of dialogues, Luo et al. (2024) collect large-scale benchmark for automatic dialogue-level hallucination evaluation. Based on this dataset, they introduce HalluJudge, specialized judge language model for evaluating hallucinations at the dialogue level. Min et al. (2023) develop FactScore, fine-grained method for evaluating the factuality 3https://openai.com/index/using-gpt-4-for-contentmoderation/ 4https://about.fb.com/news/2023/12/purple-llama-saferesponsible-ai-development/ Figure 3: LLMs are capable of judging various attributes."
        },
        {
            "title": "3.1 Helpfulness",
            "content": "Modern state-of-the-art LLMs undergo instruction tuning and alignment processes, enabling them to follow user instructions and respond effectively. This alignment step relies on large quantities of helpful and harmless data, typically gathered as human preference data, which is then used in reinforcement learning for alignment training. Given the high cost of obtaining such alignment data, recent efforts have explored using LLMs to label helpfulness, as well as to generate or evaluate alignment data (Bai et al., 2022). Authors in (Guo et al., 2024) use an LLM in an online manner to obtain preferences for direct alignment of another LLM. Some recent work shows that helpfulness feedback from AI, i.e., LLMs are comparable to human feedback (Lee et al., 2023). There have also been successful LLMs (Zhu et al., 2024a) with superior performance that have been fine-tuned with AI feedback data thus demonstrating the feasibility and usefulness of this method. Alongside these works in alignment, general purposed frameworks leveraging LLMs as evaluators are also crucial in evaluating the helpfulness of candidate responses (Zheng et al., 2023; Lin et al., 2023; Li et al., 2024e)."
        },
        {
            "title": "3.2 Harmlessness",
            "content": "Evaluating harmlessness in text data is important for both content moderation and for creating or curating synthetic datasets. Given how expensive and time-consuming human labeling efforts are, and inspired by prior work that looks into moral beliefs embedded in LLMs (Scherrer et al., 2023), many recent works have investigated the use of LLMs of long-form generation by first splitting content into atomic-level sentences and retrieving relevant corpus from Wikipedia to assess their factuality. Building on this approach, Wei et al. (2024b) propose equipping judge LLMs with Google search API to enable more flexible and efficient factuality assessments. Jing et al. (2024) expand this finegrained reliability evaluation to the multimodal area and introduce FaithScore. Zhang et al. (2024f) adopt similar strategy in LLM alignment by creating synthetic alignment dataset, which involves evaluating and filtering each generated sample using claim extraction and self-judging techniques. Xie et al. (2024b) train an external critique-based LLM as the judge to provide claim-level claimlevel factuality feedback in the generation stage to improve response factuality. For uncertainty judgment, Xu et al. (2024b) present SaySelf, novel training framework that teaches LLMs to express more fine-grained confidence estimates with selfconsistency prompting and group-based calibration training."
        },
        {
            "title": "3.4 Relevance",
            "content": "Relevance measures the extent to which generated or retrieved content aligns with the original query. Traditional relevance assessment methods often rely on keyword matching (Robertson et al., 2009) or semantic similarity (Gao et al., 2021), which struggle to capture subtle differences or nuances in context. Relevance assessment with judge LLMs has been explored and validated to be more finegrained and effective manner across various applications (Chiang and Lee, 2023). In conversation evaluation, Lin and Chen (2023a) first propose to replace the expensive and time-consuming human annotation with LLM judgment in relevance assessment, providing conversation context and generated response for the judge LLM to evaluate. Similarly, Abbasiantaeb et al. (2024) apply LLM-as-ajudge in conversation search, collaborating with human annotators to address issues related to incomplete relevance judgments. In retrieval-augmented generation (RAG) scenarios, Li and Qiu (2023) utilize LLMs to determine which historical memory is most relevant for solving the current problem. Following this concept, Li et al. (2024c) also propose to adopt LLM as re-ranker to judge and filter out noisy and irrelevant knowledge in sub-knowledge graph. Recently, LLM-as-a-judge has also been used in multimodal applications for relevance judgment. Both Yang and Lin (2024) and Chen et al. (2024a) adopt multimodal LLM to build the automatic assessment benchmark. In contrast, Chen et al. (2024e) employ the multimodal reward model to assess the relevance in text-to-image generation. Besides, LLM-as-a-judge has also been explored in many traditional retrieval applications for relevance assessment, such as search (Thomas et al., 2024), retrieval (Ma et al., 2024) and recommendation (Hou et al., 2024)."
        },
        {
            "title": "3.5 Feasibility",
            "content": "The potential of LLMs can be further released by complicated and well-designed reasoning pipelines. In these agentic LLMs, assessing the feasibility of candidate actions or steps is crucial for the success of planning, reasoning, and decision-making. While some works leverage metrics or external tools for this feasibility assessment (Huang et al., 2023a; Yuan et al.), many others leverage LLMs themselves to select the most appropriate and reasonable actions to perform. Hao et al. (2023) first propose to prompt the LLM to do self-evaluation and generate feasibility judgment as reward signal to perform Monte Carlo Tree Search (MCTS). Similarly, Yao et al. (2023a) suggest adopting the LLM as the state evaluator for potential step searching in their proposed tree-of-thought (ToT) framework. Besta et al. (2024) replace the tree structures used in previous studies with graph structures and employ the LLM to assign score to each thought based on its feasibility or correctness. In multi-agent collaboration systems, both Liang et al. (2023) and Li et al. (2024b) propose to leverage the judge LLM to select the most feasible and reasonable solutions among multiple candidates responses. Besides, there are also works adopt the judge LLMs to perform feasibility assessment in API selection (Zhao et al., 2024b), tool using (Yang et al., 2023) and LLM routing (Ong et al., 2024)."
        },
        {
            "title": "3.6 Overall Quality",
            "content": "As previously mentioned, LLM-as-a-judge can be employed to perform multi-aspect and fine-grained assessments across various tasks and applications. However, in many cases, general assessment is still required to represent the candidates overall quality for comparison or ranking purposes. One straightforward approach to obtain this overall score is to calculate the average of the aspectspecific scores (Lin et al., 2023; Lin and Chen, 2023b). Additionally, several other studies present assessment results for each attribute and prompt the LLM judge to generate an overall quality judgement (Yu et al., 2024; Guo et al., 2024; Zhang et al., 2024a). Beyond summarizing overall quality from multiple attributes, other studies focus on directly producing an overall judgment. For instance, in traditional NLP tasks like summarization (Gao et al., 2023; Jain et al., 2023a) and machine translation (Huang et al., 2024; Kocmi and Federmann, 2023), the evaluation dimensions are less diverse compared to more open-ended, long-form generation tasks. Consequently, LLM-as-a-judge is typically used in these cases to directly prompt the LLM to generate an overall judgment."
        },
        {
            "title": "4 Methodology",
            "content": "In this section, we present commonly adopted methods and tricks for LLM-as-a-judging, splitting them into tuning approaches (Section 4.1) and prompting strategies (Section 4.2)."
        },
        {
            "title": "4.1 Tuning",
            "content": "To enhance the judging capabilities of general LLM, various tuning techniques have been employed in different studies. In this section, we discuss these tuning approaches for LLM-as-a-judge from two perspectives: data sources (Section 4.1.1) and training methods (Section 4.1.2). Table 1 collects all the research papers focused on tuning the judge LLMs."
        },
        {
            "title": "4.1.1 Data Source",
            "content": "Manually-labeled Data: To train judge LLM with human-like criteria, one intuitive method is to collect manually-labeled samples and corresponding judgments. Many previous works have leveraged and integrated existing sources to build comprehensive datasets for tuning judge LLMs. Vu et al. (2024) construct large and diverse collection of over 100 quality assessment tasks, comprising more than 5 million human judgments, curated and standardized using publicly available human evaluations from prior research. Similarly, Wang et al. (2024h) propose PandaLM and collect diverse set of human-annotated test data, where all contexts are human-generated and labels align with human preferences. To enhance the policys judging ability in alignment data synthesis, Lee et al. (2024) augment the supervised fine-tuning (SFT) dataset with pairwise judgment task, where the instruction is to select the chosen response from set of options. There are also some works that collect datasets for fine-grained judgment feedback. Xu et al. (2023) introduce InstructScore, an explainable text generation evaluation metric, and curate the MetricInstruct dataset, which covers six text generation tasks and 23 datasets. Liu et al. (2024a) collect ASPECTINSTRUCT, the first instructiontuning dataset tailored for multi-aspect NLG evaluation, spanning 27 diverse evaluation aspects across 65 tasks. Yue et al. (2023) first propose the evaluation of attribution and fine-tune judge LLMs with data from related tasks such as question answering, fact-checking, natural language inference, and summarization. One distinct approach is from Ke et al. (2024), which first prompts GPT-4 to produce the feedback and manually check its generated texts for each user query, revising them if necessary to improve the quality. Synthetic Feedback: While manually-labeled feedback is high-quality and accurately reflects human judgment preferences, it has limitations in terms of quantity and coverage. As result, some researchers have turned to synthetic feedback as data source for tuning judge LLMs. One approach in this direction relies on the judge LLMs themselves to generate the synthetic feedback. For example, Wu et al. (2024) construct pairwise feedback for judgment enhancement by prompting the policy LLMs to evaluate their own judgments. Wang et al. (2024f) prompt the LLM to generate noisy version of the original instruction and use the corresponding response to this corrupted instruction as the inferior response. Wang et al. (2024a) prompt GPT-4-Turbo to generate multiple pieces of evidence based on the original evidence for each instance, categorizing them into completely irrelevant evidence, partially irrelevant evidence and highly related evidence to train hallucination judgment LLMs. Park et al. (2024) build OFFSETBIAS, pairwise preference dataset that leverages GPT-4 to generate bad, off-topic and erroneous responses and perform difficulty filtering. For safety judging, Xie et al. (2024a) adopt GPT-4 as the classifier to map each data point to predefined safety category to train an automated evaluator. Different from previous works, Li et al. (2024e) adopt GPT-4 to synthesize both pairwise and pointwise data to train generative judge LLM. For pointwise data, they adopt divide-andconquer strategy, where two critiques are collected from GPT-4 for single response, combined into Source Annotator Type Scale Technique Trick Data Tuning Method Base LLM Method AttrScore et al., 2023) (Yue PandaLM (Wang et al., 2024h) AUTO-J (Li et al., 2024e) JudgeLM (Zhu et al., 2023) Self-Judge et al., 2024) (Lee X-EVAL et al., 2024a) (Liu FLAMe (Vu et al., 2024) InstructScore (Xu et al., 2023) CritiqueLLM (Ke et al., 2024) (Wang Meta-Rewarding (Wu et al., 2024) Self-Taught Evaluator (Wang et al., 2024f) HALU-J et al., 2024a) OffsetBias (Park et al., 2024) SorryBench (Xie et al., 2024a) LLaVA-Critic (Xiong 2024b) PROMETHEUS2 et al., 2024) (Kim al., et Manual Human Manual Human Synthetic GPT-4 Synthetic GPT-4 QA, NLI, Fact-Checking, Summarization Instruction Following Real-world Scenarios Instruction Following 63.8K 300K 4K 100K Manual Human Preference Learning 65/57K 55K 5M+ 20K 5K Manual Human Dialogue, Summarization, Data-to-Text Manual Human Various Tasks Manual& Synthetic Human& GPT-4 Manual Human Synthetic LLaMA3 Various Tasks Instruction Following, real-world scenarios Preference Learning 20K Synthetic Mixtral Various Tasks 20K Synthetic GPT-4o Fact Extraction 2.6K Synthetic GPT-4, Claude3 Preference Learning 8.5K Synthetic GPT-4 Safety 2.7K Synthetic GPT-4o Preference Learning 113K SFT SFT SFT SFT SFT SFT SFT SFT SFT Preference Learning Preference Learning Preference Learning SFT SFT Preference Learning Synthetic GPT-4 Preference Learning 300K SFT Themis (Hu et al., 2024) Manual & Synthetic Human & GPT-4 Various Tasks 67K Preference Learning - - - - Multiple LLMs Multiple LLMs LLaMA-2 Vicuna JSFT LLaMA-2 Two-Stage Instruction Tuning Flan-T5 Multi-task Training PaLM-2 Meta-Feedback LLaMA Prompt Simplify, Swapping Augmentation ChatGLM3 Meta-Rewarding LLaMA-3 Self-Taught LLaMA-3 DPO Mistral Debiasing Augmentation - LLaMA-3 Multiple LLMs DPO LLaVA-v.1.5 Joint Training, Weight Merging Multi-perspective Consistency Verification, Rating-oriented DPO Mistral LLaMA-3 Table 1: Overview of tuning methods in LLM-as-a-judge. more comprehensive critique, and final rating is provided Following them, Kim et al. (2024) use GPT-4 to augment the preference learning dataset with detailed human evaluation criteria and verbal feedback. In the multi-modal domain, Xiong et al. (2024b) propose LLaVA-Critic and adopt GPT-4o to generate reasons behind given scores or preference judgments for training data construction. Moreover, By harnessing both explicit human instruction and the implicit knowledge of GPT4, Xu et al. (2023) fine-tune judge LLM based on LLaMA, producing both score for generated text and human-readable diagnostic report. Zhu et al. (2023) introduce JudgeLM and propose comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges."
        },
        {
            "title": "4.1.2 Tuning Techniques",
            "content": "Supervised Fine-tuning: Supervised fine-tuning (SFT) is the most commonly used approach to facilitate the judge LLMs to learn from pairwise (Wang et al., 2024h; Li et al., 2024e; Wang et al., 2023b; Zhu et al., 2023) or pointwise (Xie et al., 2024a; Wang et al., 2023b; Yue et al., 2023) judgment data. Among many works that adopted SFT, Vu et al. (2024) propose supervised multitask training to tune their Foundational Large Autorater Models (FLAMe) across multiple mixed datasets of various tasks. To equip the judge LLM with both pairwise and pointwise judging capabilities, Kim et al. (2024) novelly propose joint training and weight merging approaches during the tuning stage and find the latter does not improve evaluation performances in the majority of cases. To obtain judge model that can not only generate responses but also compare pairwise preferences, Lee et al. (2024) devise Judge-augmented Supervised Finetuning (JSFT) with an augmented preference learning dataset. During the training phase, Ke et al. (2024) enhance their model by adding simplified prompts to distinguish different parts of inputs and augment pairwise training data by swapping the order of two generated texts and exchanging the corresponding content in critiques. Xu et al. (2023) further fine-tune their INSTRUCTSCORE model on self-generated outputs to optimize feedback scores, resulting in diagnostic reports that are better aligned with human judgment. Liu et al. (2024a) also propose two-stage supervised finetuning approach, first applying vanilla instruction tuning to equip the model with the ability to follow instructions for diverse evaluations. They then perform further tuning with auxiliary aspects to enrich the training process, incorporating an additional instruction-tuning stage to leverage potential connections to the target evaluation aspect. Preference Learning: Preference learning is closely aligned with judgment and evaluation tasks, especially comparative and ranking judgment. In addition to works that directly adopt or augment preference learning datasets for supervised finetuning judge LLMs, several studies apply preference learning techniques to enhance LLMs judging capabilities. To enhance the quality of judgment provided by HALU-J, Wang et al. (2024a) further tune it with Directed Preference Optimization (DPO) (Rafailov et al., 2023) after the SFT stage under the multiple-evidence setting. Similarly, Park et al. (2024) apply DPO with synthetic bad responses that contain critical errors but exhibit stylistic qualities favored by judge models, helping to mitigate bias in the judge LLMs. Wu et al. (2024) novelly propose meta-rewarding, which leverages the policy LLMs to judge the quality of their own judgment and produce pairwise signals for enhancing the LLMs judging capability. This concept is also adopted by Wang et al. (2024f), who propose self-taught evaluators that use corrupted instructions to generate suboptimal responses as inferior examples for preference learning. Recently, Hu et al. (2024) propose Themis, an LLM dedicated to NLG evaluation, which has been trained with designed multi-perspective consistency verification and rating-oriented preference alignment methods. Li et al. (2024j) propose PORTIA, an alignmentbased approach designed to mimic human comparison behavior to calibrate position bias in an effective manner."
        },
        {
            "title": "4.2 Prompting",
            "content": "Designing appropriate prompting strategies and pipelines at the inference stage could improve judgment accuracy and mitigate bias. In this section, we summarize and categorize existing prompting strategies for LLM-as-a-judge (Figure 4)."
        },
        {
            "title": "4.2.1 Swapping Operation",
            "content": "Previous studies have demonstrated that LLMbased judges are sensitive to the positions of candidates, and the quality ranking of candidate responses can be easily manipulated by merely altering their order in the context (Wang et al., 2023c; Raina et al., 2024; Zhu et al., 2023). To mitigate this positional bias and establish more fair LLM judging system, the swapping operation (Zheng et al., 2023) has been introduced and widely adopted. This technique involves invoking the judge LLM twice, swapping the order of the two candidates in each instance. In evaluations, if the results are inconsistent after the swap, it is labeled \"tie,\" indicating that the LLM is unable to confidently distinguish the quality of the candidates (Zheng et al., 2023). Several studies have also incorporated swapping operations in selfalignment (Lee et al., 2023; Sun et al., 2024; Lee et al., 2024) to obtain more accurate pairwise feedback from the judge LLM. Zhu et al. (2024a) proposed CoT-like prompting technique to mitigate the positional bias by asking the model to first provide all pairwise ranking, then summarize with ranking list."
        },
        {
            "title": "4.2.2 Rule Augmentation",
            "content": "Rule-augmented prompting involves embedding set of principles, references, and evaluation rubrics directly within the prompt for the judge LLM. This approach is commonly employed in LLM-based evaluations, where judge LLMs are guided to assess specific aspects (Li et al., 2024e; Bai et al., 2023a; Yu et al., 2024; Qian et al., 2024) and provided with detailed rubrics (Gao et al., 2023; Kim et al.; Wang et al., 2024e; Murugadoss et al., 2024) to ensure fair comparison. distinct approach is seen in Liu et al. (2024b), where the judge LLM is prompted to generate its own scoring criteria through in-context learning on set of few-shot examples. In alignment with LLM-as-a-judge, Bai Figure 4: Overview of prompting strategies for LLM-as-a-judge. et al. (2022) first propose to introduce list of principles (e.g., helpfulness, harmlessness, honesty) for the judge LLM to compare the two candidates more precisely and directionally. Following them, subsequent works (Lee et al., 2023, 2024; Guo et al., 2024; Sun et al., 2024; Beigi et al., 2024a) enhance this principle-driven prompting by incorporating more detailed explanations for each aspect of the principle or rubric. Besides, both Li and Qiu (2023) and Li et al. (2024c) propose to prompt LLMs to retrieve appropriate demonstrations/ knowledge triples based on the candidates helpfulness in solving specific problems. To obtain diverse responses from LLMs, Lahoti et al. (2023) prompt multiple LLMs to judge the diversity of each candidate and select the most diverse one for further polishing. Zhang et al. (2024e) propose RevisEval, which leverages the self-correction capabilities of LLMs to adaptively revise the response, then treat the revised text as the principles for the subsequent evaluation. Recently, Dong et al. (2024) investigate the reliability of LLM-as-a-personalized-judge, providing persona as part of principles for LLMs to make personalized judgments."
        },
        {
            "title": "4.2.3 Multi-agent Collaboration",
            "content": "Accessing results from single LLM judge may not be reliable due to various biases inherent in LLMs (Wang et al., 2023c; Liusie et al., 2024; Zhu et al., 2023; Koo et al., 2023a; Liu et al., 2023c). To address this limitation, Li et al. (2023) introduced the Peer Rank (PR) algorithm, which considers each peer LLMs pairwise preferences for all answer pairs and produces final ranking of models. Building on this foundation, several architectures and techniques for multi-agent LLMs have emerged, including mixture-of-agent (Zhang et al., 2023b), role play (Wu et al., 2023), debating (Chan et al., 2023), and voting (Zhu et al., 2024b). Jung et al. (2024) proposed Cascaded Selective Evaluation, where less expensive models serve as initial judges, escalating to stronger models only when necessary (Beigi et al., 2024a). Additionally, some works apply multi-agent collaboration for alignment data synthesis, leveraging multiple LLM judges to refine responses (Arif et al., 2024) or provide more accurate pairwise feedback (Li et al., 2024g)."
        },
        {
            "title": "4.2.4 Demonstration",
            "content": "In-context samples or demonstrations (Brown et al., 2020; Dong et al., 2023; Agarwal et al.) provide concrete examples for LLMs to follow and have been shown to be crucial factor in the success of in-context learning for LLMs. Several studies have introduced human assessment results as demonstrations for LLMs-as-judges, aiming to guide the LLMs in learning assessment standards from few concrete in-context examples. Jain et al. (2023b) is the first to explore the efficacy of large language models as multi-dimensional evaluators using incontext learning, eliminating the need for large training datasets. Kotonya et al. (2023) conduct systematic experiments with various prompting techniques, including standard prompting, prompts informed by annotator instructions, and chain-ofthought prompting, combining these methods with zero-shot and one-shot learning to maximize evaluation effectiveness. To improve the robustness of LLM evaluations, Hasanbeig et al. (2023) propose ALLURE, an approach that iteratively incorporates demonstrations of significant deviations to enhance the evaluators robustness. Additionally, Song et al. (2024) introduce and study two manyshot in-context learning (ICL) prompts, using two versions of many-shot ICL templates to help mitigate potential biases in LLMs."
        },
        {
            "title": "4.2.5 Multi-turn Interaction",
            "content": "In evaluation, single response may not provide enough information for an LLM judge to thoroughly and fairly assess each candidates performance. To address this limitation, multi-turn interactions are commonly adopted to offer more comprehensive evaluation. Typically, the process begins with an initial query or topic, followed by dynamically interacting between the judge LLM and candidate models. Bai et al. (2023b) propose multi-round setting where the evaluator takes on an interviewer role, posing increasingly sophisticated follow-up questions based on the models previous answers. Similarly, Yu et al. (2024) introduce KIEval, Knowledge-grounded Interactive Evaluation framework, which novelly incorporates an LLM-powered interactor to enable dynamic, contamination-resilient assessments. Additionally, some approaches facilitate debates among candidates in multi-round format. For example, Zhao et al. (2024c) design framework where two LLMs engage in multi-round peer battle around query, allowing their true performance differences to surface. Moniri et al. (2024) propose an automated benchmarking system where LLMs debate, with the final assessment carried out by another LLM judge."
        },
        {
            "title": "4.2.6 Comparison Acceleration",
            "content": "Among the various comparison formats in LLM-asa-judge (e.g., point-wise and list-wise), pair-wise comparison is the most common approach for directly comparing two models or generating pairwise feedback. However, when multiple candidates need to be ranked, this method can be quite timeconsuming (Zhai et al., 2024). To mitigate the computational overhead, Zhai et al. (2024) propose ranked pairing method in which all candidates are first compared against blank baseline response. Each candidates rank is then determined by how well they perform in comparison to the baseline. Zhu et al. (2024a) proposed CoT like prompting technique to mitigate the positional bias by forcing the model to provide all pairwise ranking first, then summarize these pairwise rankings with list. In addition, Lee et al. (2024) utilize tournament-based approach (Liu et al., 2023a; Zhao et al., 2023b) for rejection sampling during inference to speed up the pair-wise comparison. They construct tournament tree where the leaf nodes represent sampled responses, and non-leaf nodes are selected based on the outcome of judgments between child nodes."
        },
        {
            "title": "5 Application",
            "content": "While LLM-as-a-judge was initially proposed for the evaluation application, its usage scope has been largely expanded to many other scenarios like alignment, retrieval and reasoning. Therefore, as Figure 5 shows, we provide comprehensive introduction to how LLM-as-a-judge can be applied in various applications."
        },
        {
            "title": "5.1 Evaluation",
            "content": "Traditional evaluation in NLP relies on predefined criteria, typically through metrics, to assess the quality of machine-generated text. Some prominent metrics such as BLEU, ROUGH, and BERTScore have been widely used in the area. However, metricbased evaluation overemphasizes lexical overlap and similarity, which may fall short when many valid responses and more nuanced semantic attributes are required to be considered (Post, 2018; Sai et al., 2022). To address these limitations, LLM-as-a-judge has been used to serve as an automated judge for enhancing evaluations in many tasks (Lin and Chen, 2023b; Mondorf and Plank, 2024). LLM-as-a-judge enables human-like qualitative evaluations rather than simple quantitative comparisons of how well machine-generated outputs match the ground truth. This section discusses how LLM-as-a-judge has been utilized to evaluate open-ended generation, reasoning, and more emerging NLP tasks."
        },
        {
            "title": "5.1.1 Open-ended Generation Tasks\nOpen-ended generation refers to tasks where the\ngenerated content is expected to be safe, accurate,\nand contextually relevant, though there isn’t a sin-\ngle “correct” answer. Such tasks include dialog\nresponse generation, summarization, story gener-\nation, and creative writing (Badshah and Sajjad,\n2024; Bai et al., 2023a; Kumar et al., 2024). Un-\nlike conventional metrics-based evaluation meth-",
            "content": "Figure 5: Overview of application and scenario for LLM-as-a-judge. ods, LLM-as-a-judge provides more nuanced, adaptable, and customized evaluation. As noted by Zheng et al. (2023), LLMs like GPT-4 perform comparably to humans when judging openended text generation. In practice, LLM-as-a-judge has been applied to evaluate outputs from single model to compare outputs from multiple models in competitive setting. For example, Gao et al. (2023) use ChatGPT to perform human-like summarization evaluation. Similarly, Wu et al. (2023) propose comparison-based framework to let LLMs act as judges with multiple role-playing to evaluate the summarization quality in specific dimension and generate its evaluations. Modern LLMs are good at generating detailed and long-form responses. However, as the output length increases, so does the likelihood of hallucinations. To better understand this phenomenon, Cheng et al. (2023) and Zhang et al. (2024d) introduce an evaluation method that uses GPT-4 to judge whether generated outputs include logically structured yet nonsensical statements. Wang et al. (2024a) propose critique-based judging system that evaluates hallucinations by selecting pertinent evidence and providing in-depth critiques. Beyond hallucinations, significant concern is the generation of harmful (e.g., encouragement of self-suicide) and unsafe (e.g., guidance of illegal activity) responses by LLMs. Addressing this, Li et al. (2024f) introduce MD-Judge and MCQ-Judge for evaluating safety-related QA pairs, particularly focusing on queries crafted to provoke unsafe responses. This approach supports seamless and reliable evaluation. However, an overly cautious stance toward unsafe queries can lead to excessive refusal responses, hindering normal functionality and negatively affecting user experience. To explore this issue, Xie et al. (2024a) conduct meta-evaluation across various LLM-as-a-judge frameworks, assessing the refusal tendencies of current LLMs in response to potentially unsafe queries. Recent research has also leveraged LLM-as-ajudge to evaluate the general capabilities of generative models. This approach often adopts debatebased framework, where multiple LLMs generate responses that are subsequently evaluated by separate judging LLM. For example, Chan et al. (2023) introduce multi-agent debate framework designed to facilitate autonomous discussions and assess the quality of generated responses from different LLMs in open-ended text generation tasks. Similarly, Moniri et al. (2024) propose an automated debate framework that evaluates LLMs not only on domain knowledge but also on their abilities in problem definition and inconsistency recognition."
        },
        {
            "title": "5.1.2 Reasoning Tasks",
            "content": "The reasoning capability of LLMs can be evaluated through their intermediate thinking process and final answers on specific reasoning tasks (Mondorf and Plank, 2024). Recently, LLM-as-a-judge has been used to evaluate the logical progression, depth, and coherence of the models intermediate reasoning paths. For mathematical reasoning tasks, Xia et al. (2024) introduce an automatic evaluation framework using judge LLM specifically designed to assess the quality of reasoning steps in problem-solving processes. LLM-as-a-judge can also be applied to more complex reasoning tasks like temporal reasoning, where models need to understand the relationships of different events over time. Fatemi et al. (2024) build synthetic datasets specifically tailored to evaluate LLMs temporal reasoning abilities across varied scenarios, testing their proficiency in reasoning with sequences, causality, and dependencies among temporally ordered events. The vast amount of training data presents challenge in determining whether models are reasoning through deep logical understanding or merely leveraging memorized patterns (Parmar et al., 2024). Wang et al. (2023a) design debate-style framework to evaluate LLMs reasoning capability. Given specific question, the LLM and the user adopt opposing positions and discuss the topic to reach correct decision. Nan et al. (2024) develop multi-agent evaluation framework that simulates the academic peer-review process. This framework engages LLMs-as-Judges in collaborative review, offering more nuanced understanding of LLMs reasoning capabilities in data-driven tasks."
        },
        {
            "title": "5.1.3 Emerging Tasks",
            "content": "As the capabilities of LLMs evolve rapidly, machines are increasingly being employed for tasks previously considered exclusive to humans, especially in context-specific areas. prominent task is in social intelligence, where models are presented with complex social scenarios requiring the understanding of cultural values, ethical principles, and potential social impacts. For example, Xu et al. (2024a) evaluate the social intelligence of LLMs, highlighting that while these models have made strides, they still lag significantly behind in social intelligence compared to their academic problemsolving abilities. Similarly, Zhou et al. (2023) introduce SOTOPIA and SOTOPIA-EVAL to simulate complex social interactions between LLM agents and evaluate their social intelligence. In their work, GPT-4 is used as proxy for human judgment in assessing goal completion, financial management, and relationship preservation within simulated interactions. Another line of research has been directed towards evaluating Large Multimodal Models (LMMs) and Large Vision-Language Models (LVLMs). For example, Xiong et al. (2024b) explore LMM-as-a-judge to evaluate the performance of multimodal models, providing both final score and the underlying rationale for evaluations, promoting transparency and consistency. Chen et al. (2024c) propose the first benchmark for the automatic evaluation of LVLMs, specifically for selfdriving corner cases. They find that evaluations performed by LLMs-as-judges align more closely with human preferences than those conducted by LVLM-as-judges. Recently, we have seen more customized utilization of LLM-as-a-judge to evaluate emerging tasks such as code understanding (Zhao et al., 2024a), legal knowledge (Fei et al., 2023), game development (Isaza-Giraldo et al., 2024), ocean science (Bi et al., 2023), healthcare conversations (Wang et al., 2024j), debating judgment (Liang et al., 2024a) and more. This trend reflects the growing adaptability of LLMs-as-judges in evaluating diverse and specialized domains."
        },
        {
            "title": "5.2 Alignment",
            "content": "Alignment tuning (Wei et al., 2022a; Ouyang et al., 2022) is vital technique to align LLMs with human preferences and values. key component of this process is the collection of high-quality, pairwise feedback from humans, which is essential for reward modeling (Schulman et al., 2017) or direct preference learning (Rafailov et al., 2023). Recently, there have been increasing research interests focused on automating this pairwise feedback mechanism by adopting LLM-as-a-judge in alignment tuning."
        },
        {
            "title": "5.2.1 Larger Models as Judges",
            "content": "One intuitive idea for adopting LLM-as-a-judge in alignment tuning is to leverage the feedback from larger, more powerful LLMs to guide smaller, less capable models. (Bai et al., 2022) first propose employing the AIs feedback to build harmless AI assistant. They train the reward model using synthetic preference data based on the preference of pre-trained language model. Building on this, Lee et al. (2023) discover that the RLAIF method can achieve comparable performance with RLHF even when the LLM judge is not strong enough. They also introduced DIRECT-RLAIF, which directly employs an off-the-shelf LLM as the judge model to mitigate reward staleness in reward models. To avoid reward hacking in alignment, Sun et al. (2024) devise an instructable reward model trained on synthetic preference data. It enables humans to perform RL-time interventions to better align the target policy with human values. Apart from the abovementioned studies, Guo et al. (2024) introduce online AI feedback (OAIF), directly utilizing the preference signals from an annotation model to train the target model. There are also works that utilize multi-agent cooperation for better judgment in alignment tuning. Arif et al. (2024) construct synthetic preference optimization dataset using multi-agent workflows and adopt LLMs as judges with diverse prompting strategies and pipelines. Similarly, (Li et al., 2024g) employ multiple LLMs to debate with each other, iteratively improving response quality, while creating judge LLM to select preferred responses for enhanced instruction tuning. To align the generated code with human preference, Weyssow et al. (2024) introduce CodeUltraFeedback, preference coding dataset constructed using the LLM-as-a-Judge methodology. This synthetic dataset is later used to fine-tune and align small code LLMs using SFT and DPO. Recently, Wang et al. (2024d) propose BPO, using GPT-4 as the judge and constructing synthetic pairwise feedback for knowledge depth and breadth balance in the alignment process."
        },
        {
            "title": "5.2.2 Self-Judging",
            "content": "Another line of work aims to utilize the preference signal from the same LLM for self-improving. Yuan et al. (2024c) first propose the concept of self-rewarding LLM, where pairwise data is constructed by having the LLM itself serve as the judge. Following them, Wu et al. (2024) introduce metarewarding that judges the LLMs judge and uses the feedback to refine their judging skills. Their LLMas-a-meta-judge approach significantly enhances the models capacity to evaluate and follow instructions. To improve synthetic data quality, Pace et al. (2024) combine the Best-of-N and Worst-of-N sampling strategies and introduce the West-of-N approach. Lee et al. (2024) devise Judge augmented Supervised Fine-Tuning (JSFT) to train single model to act as both policy and judge. To fully utilize this judge model, they also propose the SelfRejection by Tournament method to select the best response in the inference time. Unlike the above approaches, which use the LLM as judge for pairwise data construction, Tong et al. (2024) apply LLM-as-a-judge as self-filtering method to ensure the quality of synthetic data pairs in alignment tasks for reasoning. To reduce the computation overhead in pairwise judgment, Zhai et al. (2024) propose ranked pairing method for self-preferring language models, which accelerates the comparison process by measuring the strength of each response against baseline. Liu et al. (2024c) introduce meta-ranking, enabling weaker LLMs to serve as reliable judges and provide trustworthy feedback. They also apply their meta-ranking method in post-SFT training, combining it with KahnemanTversky Optimization (KTO) for improved alignment. To enhance the quality of the synthetic instruction tuning data, Liang et al. (2024c) introduce an iterative self-enhancement paradigm (I-SHEEP). During training, they adopt LLM-as-a-judge to score the synthetic responses and set threshold to collect high-quality query-response pairs for the subsequent training iteration. Several works have also employed LLM-as-ajudge in specific domains or for particular attributes. Zhang et al. (2024f) propose self-evaluation mechanism, judging responses factuality by generating question-answer pairs. They then utilize these self-annotated responses to fine-tune the model via the DPO algorithm for better factuality. In robotics, Zeng et al. (2024) iteratively update the reward function with the self-ranking responses from LLMs, boosting learning efficiency without human supervision. In the multimodal domain, Ahn et al. (2024) propose iterative self-retrospective judgment (i-SRT), which employs self-reflection to improve both response generation and preference modeling."
        },
        {
            "title": "5.3 Retrieval",
            "content": "The role of LLM-as-a-judge in retrieval encompasses both traditional document ranking and the more dynamic, context-adaptive RetrievalAugmented Generation (RAG) approaches. In traditional retrieval, LLMs enhance ranking accuracy through advanced prompting techniques, enabling them to order documents by relevance with minimal labeled data. Complementarily, RAG frameworks capitalize on LLMs capacity to generate content guided by retrieved information, supporting applications where complex or evolving knowledge integration is essential. Together, these techniques underscore LLMs adaptability as judges in retrieval tasks, extending from foundational ranking to domain-specific, knowledge-augmented applications."
        },
        {
            "title": "5.3.1 Traditional Retrieval",
            "content": "Recent research has explored the role of LLMs as judges for ranking documents in information retrieval, aiming to boost ranking precision and reduce reliance on extensive training data. For example, Sun et al. (2023) explore the potential of generative LLMs like GPT-4 for relevance ranking in information retrieval. They propose permutationbased approach to rank passages by relevance, instructing LLMs to output ordered permutations of passages, thus enhancing ranking precision. Complementing this, Zhuang et al. (2024a) introduce method that embeds fine-grained relevance labels within LLM prompts, enabling the models to distinguish subtle relevance variations and produce more refined document ordering. Further innovations in listwise ranking are illustrated by Ma et al. (2023), who present the Listwise Reranker with Large Language Model (LRL), tool that directly reorders document identifiers without relying on task-specific training data. Additionally, Zhuang et al. (2024b) contribute Setwise prompting strategy tailored for zero-shot ranking, which streamlines ranking operations by decreasing LLM inference frequency and token usageenhancing efficiency without sacrificing performance. To address positional biases, common challenge in listwise ranking tasks, Tang et al. (2024b) introduce permutation self-consistency technique that averages across multiple list orders to yield order-independent rankings. This approach effectively reduces positional bias, an issue particularly problematic in LLM-driven listwise ranking. Finally, Qin et al. (2024) critique the limitations of pointwise and listwise ranking prompts in existing methods, noting that typical LLMs often lack the depth to grasp complex ranking tasks. To mitigate this, they propose Pairwise Ranking Prompting (PRP) with medium-sized, open-source LLMs as an effective, cost-efficient alternative to larger proprietary models. Beyond general retrieval tasks, LLMs have demonstrated their utility as judges in specialized applications. For example, Ma et al. (2024) outline few-shot workflow that employs generalpurpose LLM for relevance judgments in legal information retrieval. This model achieves high consistency with expert annotations by breaking down tasks into stages, facilitating the integration of expert reasoning to improve relevance assessment accuracy in legal contexts. In recommender systems, Hou et al. (2024) examine LLMs potential to rank items by framing recommendations as conditional ranking tasks. This framework factors in user interaction histories alongside candidate items, addressing LLMs known biases, such as tendencies to favor popular or top-positioned items. Specialized prompting and bootstrapping techniques are employed to correct these biases and improve interpretive accuracy. Finally, in the realm of search systems, Thomas et al. (2023) find that LLMs perform comparably to human labelers in predicting searcher preferences, making them valuable for identifying high-performing systems and flagging challenging queries. This study underscores the effectiveness of LLMs as judges in complex retrieval tasks, enabling more nuanced and accurate relevance evaluations across diverse applications."
        },
        {
            "title": "5.3.2 Retrieval-Augmented Generation (RAG)",
            "content": "Recent developments in Retrieval-Augmented Generation (RAG) have explored the capacity of LLMs to self-evaluate and self-improve without the need for annotated datasets or parameter adjustments (Chen et al., 2024d). Li and Qiu (2023) introduce the Memory-of-Thought (MoT) framework, two-stage self-reflective model that enhances an LLMs reasoning abilities autonomously. In the first stage, the model generates high-confidence reasoning on an unlabeled dataset, storing this as memory. During the testing phase, the model recalls this memory by judging the relevance of each of them with the current question and choose the most relevant ones as demonstration. In similar vein, Tang et al. (2024a) propose Self-Retrieval, an innovative architecture that consolidates information retrieval (IR) capabilities within single LLM, utilizing natural language indexing to internalize the corpus. This approach transforms retrieval into document generation and self-assessment process, achieving fully endto-end IR workflow within one model. Furthermore, Asai et al. (2024) present SELF-RAG (SelfReflective Retrieval-Augmented Generation), model that enhances LLM response quality and factuality through retrieval and self-reflection cycle. By using reflection tokens to guide adaptive responses, SELF-RAG allows the model to dynamically judge and adjust its responses based on task-specific requirements. In the domain of question answering, LLMs are increasingly employed as evaluative agents to assess answer relevance, quality, and utility in real-time. Rackauckas et al. (2024) introduce an LLM-based evaluation framework that generates synthetic queries from actual user interactions and domain-specific documents. Within this framework, LLMs function as judges, evaluating retrieved documents and ranking RetrievalAugmented Generation (RAG) agent variants via RAGElo, an Elo-based automated competition. This structure offers scalable solution for quality control in QA systems. Additionally, Zhang et al. (2024b) conduct an extensive study on LLMs ability to assess relevance versus utility in open-domain QA. Their findings demonstrate that LLMs can effectively distinguish between the two and are highly adaptable when presented with counterfactual passages. This utility assessment capability allows LLMs to provide more nuanced and contextually relevant responses during evaluations. RAG systems tailored to specific domains reveal the potential for LLMs to navigate complex queries by integrating specialized knowledge structures. In domain-specific retrieval, Wang et al. (2024b) present BIORAG, an advanced RAG framework that enhances vector retrieval with hierarchical knowledge structures. BIORAG adopts selfaware evaluated retriever to continuously judge the adequacy and relevance of the information it has collected, thus improving the accuracy of the retrieved document. For biomedical research, Li et al. (2024c) introduce DALK (Dynamic CoAugmentation of LLMs and Knowledge Graphs), novel system that combines an LLM with continuously evolving Alzheimers Disease (AD) knowledge graph derived from the scientific literature. Using the novel self-aware knowledge retrieval method, DALK employs the judging capability of LLMs to do noise filtering, enhancing the LLMs inferential performance in AD-related queries. Similarly, Jeong et al. (2024) propose SelfBioRAG, framework that adapts RAG principles to biomedical applications. Self-BioRAG employs the LLM to select the best evidence and generate the answer based on the selected evidence and encoded knowledge."
        },
        {
            "title": "5.4 Reasoning",
            "content": "Unlocking the reasoning capability of LLMs offers way to relieve the limitations of scaling laws, which alone may not fully reveal the models potential. Reasoning is critical aspect of LLMs because it directly affects their ability to solve complex problems, make decisions, and provide accurate, context-aware responses. Recently, many studies about LLMs reasoning capability have focused on how to leverage LLM-as-a-judge to select reasoning paths (Section 5.4.1) and utilize external tools (Section 5.4.2)"
        },
        {
            "title": "5.4.1 Reasoning Path Selection",
            "content": "Wei et al. (2022b) introduces the concept of Chain-of-Thought (CoT) prompting to encourage models to generate step-by-step reasoning processes. While other more complex cognition structures (Yao et al., 2023a; Hao et al., 2023) have been proposed to boost LLMs reasoning capabilities, one crucial challenge is how to select reasonable and reliable reasoning path or trajectory for LLMs to follow. To address this problem, LLM-asa-judge has been adopted by many works. Some works focus on sample-level selection in the reasoning process. (Kawabata and Sugawara, 2024) introduce REPS (Rationale Enhancement through Pairwise Selection), judging and selecting valid rationales via pairwise self-evaluation with LLMs and training verifier based on these data. Another reasoning path selection problem for LLMs is diversity, Lahoti et al. (2023) find that LLMs grasp the concept of diversity and can identify aspects where responses lack diversity. By selecting and aggregating multiple critiques, LLMs can achieve similar gains, compared to multiple iterations of critique and revision. In multi-agent collaboration frameworks, Liang et al. (2023) propose multi-agent debating (MAD), novel paradigm to facilitate debating and discussion among multiple agents. They leverage the judge LLM to select the most reasonable response as the final output at the end of the debating process. Similarly, Li et al. (2024b) propose new roles in layer-based multiagent collaboration, employing the judge LLM to select the high-quality and reasonable response and thus enhancing the whole systems token utilizing efficiency by considerable margin. Additionally, there are also many works that focus on step-level reasoning path selection, leveraging the judge LLM as the process reward model (PRM) to evaluate state scores. Creswell et al. (2023) decompose the reasoning process into Selection and Inference. In the selection step, they leverage the LLM itself to judge and evaluate each potential reasoning trace, selecting the appropriate one for the following inference step. Xie et al. (2024b) propose the Kwai-STaR framework, which transforms LLMs into state-transition reasoners to judge and select the best reasoning state for themselves in mathematical reasoning. Lightman et al. (2023) train LLM as the PRM to conduct inference-time supervision and perform best-of-N sampling strategy during reasoning stage. Following them, Setlur et al. (2024) further propose process advantage verifiers (PAVs), generating rewards based on the changes in the likelihood of producing correct responses in the future. Other works simulate advanced cognitive structures as the reasoning process. Hao et al. (2023) employ LLMs as world model to simulate the state of the environment and perform Monte Carlo Tree Search (MCTS) to improve performance on tasks requiring deliberate path selection. Furthermore, Besta et al. (2024) takes the outputs generated by LLMs as an arbitrary graph. An LLM thought is modeled as vertex, while an edge is dependency between thoughts. This framework enables systematic judge of coherence and logical reasoning for each reasoning state. Yao et al. (2023a) propose Treeof-Thoughts (ToT), where each thought serves as an intermediate step toward problem-solving. It decomposes reasoning into steps, self-evaluates and judges progress at each state, and uses search algorithms with LMs to judge thought paths through lookahead and backtracking."
        },
        {
            "title": "5.4.2 Reasoning with External Tools",
            "content": "Yao et al. (2023b) first propose to use LLMs in an interleaved manner to generate reasoning traces and task-specific actions. Reasoning traces help the model to judge and update action plans, while actions enable it to interact with external sources. Further, Auto-GPT was introduced by (Yang et al., 2023) to deliver more accurate information with LLM-as-a-judge for tool-using. By equipping with range of external complex tools, LLMs become more versatile and capable, improving planning performance by judging and reasoning which tools to use. Sha et al. (2023) explore LLMs potential in reasoning and judging, employing them as decisionmaking components for complex autonomous driving scenarios that require human commonsense understanding. Zhou et al. (2024b) utilize selfdiscovery process where LLMs perform judgment based on the given query and select the most feasible reasoning structure for the following inference stage. While LLMs have demonstrated impressive judging capability across variety of tools. However, selecting which model or API to use often involves trade-off between performance and cost. More powerful models, though effective, are also more expensive, while less capable models are more costefficient. To address this dilemma, the authors in Ong et al. (2024) propose routing model that can dynamically choose between stronger or weaker LLM during the judging process, aiming to balance the cost and response quality. Similarly for efficiency consideration, DiffAgent was introduced in Zhao et al. (2024b) to act as an agent designed to judge and select different text-to-image APIs with user-specific prompts. DiffAgents judgment aligns more closely with human preferences, outperforming traditional API selection methods."
        },
        {
            "title": "6 Benchmark: Judging LLM-as-a-judge",
            "content": "The evaluation of large language models (LLMs) as judges necessitates robust and purpose-driven benchmarks to capture the multifaceted nature of this task. We categorize existing benchmarks into the following dimensions: general performance, bias quantification, domain-specific performance, multimodal evaluation, multilingual capabilities, evaluation instruction following, vulnerability assessment, and challenging task performance. This taxonomy exhibits the diverse goals of LLM-asa-judge evaluation frameworks, providing structured lens through which to analyze their design, scope, and impact. Table 2 demonstrates the collection of benchmarks for LLM-as-a-judge."
        },
        {
            "title": "6.1 General Performance",
            "content": "Benchmarks focusing on general performance aim to evaluate the overall competence of LLMs across variety of tasks. These benchmarks typically measure agreement with human judgments, accuracy, and correlation. Notable examples include: MT-Bench and Chatbot Arena (Zheng et al., 2023), which assess conversational settings using metrics such as consistency, bias, and error. These benchmarks further explore specific biMethod Data Type Scale Reference Metrics Purpose MT-Bench (Zheng et al., 2023) Chatbot Arena (Zheng et al., 2023) CodeJudgeEval (Zhao et al., 2024a) JUDGEBENCH (Tan et al., 2024a) SOS-BENCH (Penfever et al., 2024) LLM-judgeeval (Wei et al., 2024a) DHP (Wang et al., 2024g) EVALBIASBENCH (Wang et al., 2024c) Raju et al. (2024) MLLM-as-ajudge (Chen et al., 2024a) MM-EVAL (Son et al., 2024b) KUDGE (Son et al., 2024a) Murugadoss et al. (2024) Thakur et al. (2024) JudgeBench (Tan et al., 2024a) Arena-Hard Auto (Li et al., 2024h) R-Judge (Yuan et al., 2024b) Multi-turn Conversation Single-turn Conversation Human Expert Consistency, Bias, Error 30K User Consistency, Bias, Error General Performance, Position/Verbosity/Selfenhancement Bias General Performance, Position/Verbosity/Selfenhancement Bias Code 457 Execution System Accuracy, F1 General Performance Various Tasks 70K Human Cohens kappa, Correlation General Performance Various Tasks 152K Human Normalized Accuracy General Performance Summarization, Alignment 1K Human Accuracy, Flipping Noise, Position Bias, Length Bias General Performance Various Tasks 400 Human Discernment Score General Performance Alignment 80 Human Accuracy Various Bias Various Tasks 1.5K Human Various Tasks 30K Human Separability, Agreement, BrierScore Human Agreement, Analysis Grading, Hallucination Detection Domain-specific Performance Multimodal Various Tasks 5K Human Accuracy Multilingual Question Answering 3.3K Human & GPT-4o Accuracy, Correlation Non-English & Challenging Various Tasks - Human Correlation Question Answering 400 Human Scotts π, Percent Agreement Evaluation Instruction Following Vulnerability Various Tasks 350 GPT-4o Accuracy Challenging Alignment Multi-turn Interaction 500 569 GPT-4Turbo Separability, Agreement Challenging Human F1, Recall, Spec, Effect Safety Shi et al. (2024) Alignment 100K Human CALM (Ye et al., 2024a) Various Tasks 14K Human Repetition Stability, Position Consistency, Preference Fairness Robustness/Consistency Rate, 0riginal/ Hacked Accuracy Position Bias Bias Quantification Table 2: Overview of various benchmarks and datasets for LLM-as-a-judge. ases, including position bias, verbosity, and selfenhancement tendencies. JUDGE-BENCH (Tan et al., 2024a), DHP (Wang et al., 2024g), and SOS-BENCH (Penfever et al., 2024), which operate at larger scales, utilizing metrics like Cohens kappa, Discernment Score, and normalized accuracy to benchmark general LLM performance. LLM-judge-eval (Wei et al., 2024a), which evaluates tasks such as summarization and alignment with additional metrics like flipping noise and length bias."
        },
        {
            "title": "6.2 Bias Quantification",
            "content": "Mitigating bias in LLM judgments is critical to ensuring fairness and reliability. Typical benchmarks include EVALBIAS-BENCH (Wang et al., 2024c) and CALM (Ye et al., 2024a), focus explicitly on quantifying biases, including those emerging from alignment and robustness under adversarial conditions. Besides, Shi et al. (2024) evaluate metrics such as position bias and percent agreement in question-answering tasks."
        },
        {
            "title": "6.3 Challenging Task Performance",
            "content": "Benchmarks designed for difficult tasks push the boundaries of LLM evaluation. For example, Arena-Hard Auto (Li et al., 2024h) and JudgeBench (Tan et al., 2024a) select harder questions based on LLMs performance for conversational QA and various reasoning tasks, respectively. CALM (Ye et al., 2024a) explores alignment and challenging scenarios, using metrics such as separability, agreement, and hacked accuracy to evaluate performance in manually identified hard data sets."
        },
        {
            "title": "6.4 Domain-Specific Performance",
            "content": "Domain-specific benchmarks provide task-focused evaluations to assess LLMs effectiveness in specialized contexts. Concretely, Raju et al. (2024) measure separability and agreement across tasks, leveraging metrics such as Brier scores for insights in specific domains such as coding, medical, finance, law and mathematics. CodeJudge-Eval (Zhao et al., 2024a) specifically evaluates LLMs for judging code generation with execution-focused metrics such as accuracy and F1 score."
        },
        {
            "title": "6.5 Other Evaluation Dimensions",
            "content": "Beyond general performance and bias quantification, several benchmarks address additional evaluation dimensions essential for assessing specific aspects of using LLMs as judges: Multimodal: MLLM-as-a-judge (Chen et al., 2024a) extends evaluation frameworks to tasks involving multiple data modalities, focusing on agreement with human judgments, grading analysis, and hallucination detection. Multilingual: Benchmarks such as MM-EVAL (Son et al., 2024b) and KUDGE (Son et al., 2024a) evaluate multilingual and non-English performance, measuring metrics like accuracy and correlation, particularly in challenging scenarios. Instruction Following: Murugadoss et al. (2024) examine the extent to which LLMs adhere to specific evaluation instructions, utilizing correlation metrics to quantify performance."
        },
        {
            "title": "7 Challenges & Future Works",
            "content": "In this section, we outline the current challenges for LLM-as-a-judge and promising direction for future works to explore, including bias & vulnerability, dynamic & complex judgment, self-judging and human-LLM co-judgment."
        },
        {
            "title": "7.1 Bias & Vulnerability",
            "content": "The use of LLMs-as-a-judge inherently frames evaluation as generation task, introducing significant challenges related to bias and vulnerability. These biases often stem from the models training data, which frequently embeds societal stereotypes tied to demographic identities such as race, gender, religion, culture, and ideology (Sheng et al., 2021). Such biases can significantly compromise fairness and reliability when LLMs are deployed for diverse judging tasks. In addition to these general biases, specific evaluative biases emerge when LLMs act as judges. Order Bias is prominent issue where the sequence of candidates influences preferences (Zheng et al., 2023; Wang et al., 2023c; Koo et al., 2023a; LLMS). This bias can distort evaluation outcomes, particularly in pairwise comparisons, and is more pronounced when the quality gap between competing responses is small (LLMS; Wang et al., 2023c).Egocentric Bias arises when LLMs favor outputs generated by the same model, compromising objectivity (Liu et al., 2023c; Koo et al., 2023a; Wataoka et al., 2024). This issue is particularly pronounced when evaluation metrics are designed using the same model, leading to inflated scores for outputs from the originating model (Liu et al., 2023c). Length Bias, another prevalent challenge, skews evaluations by disproportionately favoring longer or shorter responses regardless of quality (Zheng et al., 2023; Koo et al., 2023a). Additional biases, such as Misinformation Oversight Bias, Authority Bias, and Beauty Bias, further complicate LLM evaluations. For instance, misinformation oversight bias reflects tendency to overlook factual errors, authority bias favors statements from perceived authoritative sources, and beauty bias prioritizes visually appealing content over substantive quality (Chen et al., 2024b,d; Stephan et al., 2024). Similarly, Verbosity Bias shows preference for lengthier explanations, often equating verbosity with quality, which can mislead the judgment process (Yuan et al., 2024a). Furthermore, Sentiment Bias skews evaluations based on emotional tone, favoring responses with positive phrasing (Ye et al., 2024a). LLM judges are also highly susceptible to adversarial manipulations. Techniques like JudgeDeceiver highlight the risks posed by optimizationbased prompt injection attacks, where carefully crafted adversarial sequences can manipulate LLM judgments to favor specific responses (Shi et al., 2024). Similarly, universal adversarial phrases can drastically inflate scores in absolute scoring paradigms, revealing vulnerabilities in zero-shot assessment setups (Liusie et al., 2023; Raina et al., 2024; Doddapaneni et al., 2024). These manipulations raise concerns about the reliability of LLM judges in high-stakes scenarios such as leaderboards and academic assessments (Shi et al., 2024; Raina et al., 2024). To address these biases and vulnerabilities, frameworks like CALM (Ye et al., 2024a) and BWRS (Gao et al., 2024b) offer systematic approaches for bias quantification and mitigation. Techniques such as Multiple Evidence Calibration (MEC), Balanced Position Calibration (BPC), and Human-in-the-Loop Calibration (HITLC) have proven effective in aligning model judgments with human evaluations while reducing positional and other biases (Wang et al., 2023c). Additionally, cognitive bias benchmarks like COBBLER have identified six key biases, including salience bias and bandwagon effect, that need systematic mitigation in LLM evaluations (Koo et al., 2023b). Future Direction. One promising direction for future research is to integrate Retrieval-Augmented Generation (RAG) frameworks into LLM evaluation processes (Chen et al., 2024d). By combining generative and retrieval capabilities, these frameworks can reduce biases such as self-preference and factuality issues by grounding evaluations in external, verifiable data sources. Another promising avenue is the use of bias-aware datasets, like OFFSETBIAS, to systematically address inherent biases in LLM-as-a-judge systems (Park et al., 2024). Incorporating such datasets into training pipelines allows LLMs to better distinguish superficial qualities from substantive correctness, thereby enhancing fairness and reliability. Exploring finetuned LLMs as scalable judges, exemplified by the JudgeLM framework, represents another intriguing direction (Zhu et al., 2023). Techniques like swap augmentation and reference support embedded in this framework can systematically mitigate biases, improve evaluation consistency, and extend the applicability of LLM-based judgments to open-ended tasks. Additionally, advancing zeroshot comparative assessment frameworks offers significant promise (Liusie et al., 2023). These frameworks can refine pairwise comparison techniques and implement debiasing strategies, improving fairness and reliability across diverse evaluation domains without the need for extensive prompt engineering or fine-tuning. Finally, methods like JudgeDeceiver-resistant calibration and adversarial phrase detection strategies need further exploration to secure LLM-as-a-judge frameworks from attacks (Shi et al., 2024; Raina et al., 2024; Zhou et al., 2024a; Li et al., 2024d)."
        },
        {
            "title": "7.2 Dynamic & Complex Judgment",
            "content": "Earlier works about LLM-as-a-judge usually adopt static and straightforward approaches that directly prompt the judge LLM to perform the assessment (Zheng et al., 2023). Recently, more dynamic and complex judgment pipelines have been proposed to address various limitations, improving the robustness and effectiveness of LLM-as-ajudge. One approach in direction is to follow the concept of LLM-as-a-examiner, where the system dynamically and interactively generates both questions and judgments based on the candidate LLMs performance (Yu et al., 2024; Bai et al., 2023a). Other works focus on making judgments based on two or more candidate LLMs battle and debating results (Moniri et al., 2024; Zhao et al., 2024c). These dynamic judgment methods largely improve the judge LLMs understanding of each candidate and potentially prevent the data contamination problem (Deng et al., 2024; Golchin and Surdeanu) in LLMs evaluation. Additionally, building complex and complicated judgment pipelines or agents is another popular research area (Li et al., 2023; Chan et al., 2023; Zhuge et al., 2024). These methods typically involve multi-agent collaboration, along with well-designed planning and memory systems, enabling the judge LLMs to handle more complex and diverse judgment scenarios. Future Direction. One promising direction for future research is to equip LLMs with human-like judgment capabilities (Yuan et al., 2024a; Liang et al., 2024b). These designs can borrow insights from human behavior when making judgment, such as anchoring and comparing, hindsight and reflection, and meta-judgment. Another intriguing avenue would be to develop an adaptive difficulty assessment system using LLMs (Hu, 2024). This system would adjust the difficulty of questions based on the current performance of the candidates. Such an adaptive and dynamic system could address significant limitation in LLM evaluation, as static benchmarks often fail to accurately assess LLMs with varying capabilities."
        },
        {
            "title": "7.3 Self-Judging",
            "content": "LLM-based evaluators, such as GPT-4, are widely used for assessing outputs but face significant challenges, particularly Egocentric Bias, where models favor their own responses over those from external systems (Liu et al., 2023b; Bai et al., 2023a; Zheng et al., 2023). This self-preference undermines impartiality and creates chicken or the egg dilemma: robust evaluators are essential for developing powerful LLMs, yet advancing LLMs depends on unbiased evaluators. Other issues include Self-Enhancement Bias, where models overrate their own outputs (Li et al., 2023), and Reward Hacking, where over-optimization of specific signals leads to less generalizable evaluations (Wu et al., 2024). Additionally, reliance on Static Reward Models limits adaptability, while biases like Positional and Verbosity Biases distort judgments by favoring response order or length over quality (Yuan et al., 2024c; Wang et al., 2024f). The high cost and limited scalability of human annotations further complicate the creation of dynamic and reliable evaluation systems (Srivastava et al., 2022; Liang et al., 2022). Future Direction. One promising direction for future research is the development of collaborative evaluation frameworks like Peer Rank and Discussion (PRD) (Li et al., 2023). These frameworks leverage multiple LLMs to collectively evaluate outputs, using weighted pairwise judgments and multi-turn dialogues to reduce self-enhancement bias and align evaluations closer to human standards. Another intriguing avenue is the adoption of Self-Taught Evaluator frameworks, which generate synthetic preference pairs and reasoning traces to iteratively refine model evaluation capabilities (Wang et al., 2024f). This approach eliminates dependency on costly human annotations while ensuring that evaluation criteria adapt to evolving tasks and models. The integration of Self-Rewarding Language Models (SRLM) offers another promising path (Yuan et al., 2024c). By employing iterative mechanisms like Direct Preference Optimization (DPO), these models continuously improve their instruction-following and reward-modeling capabilities, mitigating issues like reward hacking and overfitting. Building on SRLM, the use of Meta-Rewarding mechanisms introduces meta-judge role to evaluate and refine judgment quality (Wu et al., 2024). This iterative process addresses biases such as verbosity and positional bias, enhancing both alignment and the ability to evaluate complex tasks. Finally, leveraging synthetic data creation for generating contrasting responses offers scalable solution for training evaluators (Wang et al., 2024f). By iteratively refining evaluations on synthetic preference pairs, models can progressively improve their robustness and adaptability. Combining these approaches with diverse benchmarks (Srivastava et al., 2022; Liang et al., 2022), multifaceted evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023; Ouyang et al., 2022) can ensure evaluations that are fair, reliable, and aligned with human expectations across various domains."
        },
        {
            "title": "7.4 Human-LLMs Co-judgement",
            "content": "As mentioned earlier, the biases and vulnerabilities in LLM-as-a-judge can be addressed through human involvement in the judgment process for further intervention and proofreading. However, only few studies have focused on this approach. Wang et al. (2023c) introduce human-in-the-loop calibration, which employs balanced position diversity entropy to measure the difficulty of each example and seeks human assistance when necessary. In the context of relevance judgment, Faggioli et al. (2023) propose human-machine collaboration spectrum, which categorizes different relevance judgment strategies based on the extent to which humans rely on machines. Future Direction. As data selection (Xie et al., 2023; Albalak et al., 2024) becomes an increasingly popular research area for improving the efficiency of LLMs training and inference, it also holds the potential for enhancing LLMs evaluation. LLMas-a-judge can draw insights from data selection to enable judge LLMs to serve as critical sample selector, choosing small subset of samples for human annotators to evaluate based on specific criteria (e.g., representativeness or difficulty). Additionally, the development of human-LLM co-judgment can benefit from mature human-in-the-loop solutions in other fields, such as data annotation (Tan et al., 2024b) and active learning (Margatina et al., 2023)."
        },
        {
            "title": "8 Conclusion",
            "content": "This survey explores the intricacies of LLMas-a-judge. We begin by categorizing existing LLM-based judgment methods based on input formatspoint-wise, pair-wise, and list-wiseand output formats, including scoring, ranking, and selection. Then, we propose comprehensive taxonomy for LLM-as-a-judge, encompassing judging attributes, methodologies and applications. After this, detailed collection of benchmarks for LLMas-a-judge has been introduced, accompanied by thoughtful analysis of current challenges and future directions, aiming to provide more resources and insights for future works in this emerging area."
        },
        {
            "title": "References",
            "content": "Openai o1 system card. Zahra Abbasiantaeb, Chuan Meng, Leif Azzopardi, and Mohammad Aliannejadi. 2024. Can we use large language models to fill relevance judgment holes? ArXiv preprint, abs/2405.05600. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. ArXiv preprint, abs/2303.08774. Rishabh Agarwal, Avi Singh, Lei Zhang, Bernd Bohnet, Luis Rosias, Stephanie CY Chan, Biao Zhang, Aleksandra Faust, and Hugo Larochelle. Many-shot in-context learning. In ICML 2024 Workshop on In-Context Learning. Daechul Ahn, Yura Choi, San Kim, Youngjae Yu, Dongyeop Kang, and Jonghyun Choi. 2024. i-srt: Aligning large multimodal models for videos by iterative self-retrospective judgment. ArXiv preprint, abs/2406.11280. Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. 2024. survey on data selection for language models. ArXiv preprint, abs/2402.16827. Samee Arif, Sualeha Farid, Abdul Hameed Azeemi, Awais Athar, and Agha Ali Raza. 2024. The fellowship of the llms: Multi-agent workflows for synthetic preference optimization dataset generation. ArXiv preprint, abs/2408.08688. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations. Sher Badshah and Hassan Sajjad. 2024. Referencein automatic ArXiv preprint, guided verdict: evaluation of free-form text. abs/2408.09235. Llms-as-judges Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. ArXiv preprint, abs/2212.08073. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei Hou. 2023a. Benchmarking foundation models with language-model-as-an-examiner. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei Hou. 2023b. Benchmarking foundation models with language-model-as-an-examiner. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Alimohammad Beigi, Bohan Jiang, Dawei Li, Tharindu Kumarage, Zhen Tan, Pouya Shaeri, and Huan Liu. 2024a. Lrq-fact: Llm-generated relevant questions for multimodal fact-checking. ArXiv preprint, abs/2410.04616. Alimohammad Beigi, Zhen Tan, Nivedh Mudiam, Canyu Chen, Kai Shu, and Huan Liu. 2024b. Model attribution in llm-generated disinformation: domain generalization approach with supervised contrastive learning. ArXiv preprint, abs/2407.21264. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. 2024. Graph of thoughts: Solving elaborate problems with large language models. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 1768217690. AAAI Press. Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, and Huajun Chen. 2023. Oceangpt: large language model for ocean science tasks. ArXiv preprint, abs/2310.02031. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based evaluators through multi-agent debate. In The Twelfth International Conference on Learning Representations. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3):145. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. 2024a. MLLM-asa-judge: Assessing multimodal LLM-as-a-judge with vision-language benchmark. In Forty-first International Conference on Machine Learning. Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. 2024b. Humans or llms as the judge? study on judgement biases. ArXiv preprint, abs/2402.10669. Kai Chen, Yanze Li, Wenhua Zhang, Yanxin Liu, Pengxiang Li, Ruiyuan Gao, Lanqing Hong, Meng Tian, Xinhai Zhao, Zhenguo Li, et al. 2024c. Automated evaluation of large vision-language models on self-driving corner cases. ArXiv preprint, abs/2404.10595. Yen-Shan Chen, Jing Jin, Peng-Ting Kuo, Chao-Wei Huang, and Yun-Nung Chen. 2024d. Llms are biased evaluators but not biased for retrieval augmented generation. ArXiv preprint, abs/2410.20833. Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, et al. 2024e. Mj-bench: Is your multimodal reward model really good judge for text-to-image generation? ArXiv preprint, abs/2407.04842. Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu, Mozhi Zhang, Junliang He, Mianqiu Huang, Zhangyue Yin, Kai Chen, et al. 2023. Evaluating hallucinations in chinese large language models. ArXiv preprint, abs/2310.03368. Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1560715631, Toronto, Canada. Association for Computational Linguistics. Antonia Creswell, Murray Shanahan, and Irina Higgins. 2023. Selection-inference: Exploiting large language models for interpretable logical reasoning. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. 2024. Investigating data contamination in modern benchmarks for large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 87068719, Mexico City, Mexico. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Sumanth Doddapaneni, Mohammed Safi Ur Rahman Khan, Sshubam Verma, and Mitesh Khapra. 2024. Finding blind spots in evaluator llms with interpretable checklists. ArXiv preprint, abs/2406.13439. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2023. survey on in-context learning. ArXiv preprint, abs/2301.00234. Yijiang River Dong, Tiancheng Hu, and Nigel Collier. 2024. Can llm be personalized judge? ArXiv preprint, abs/2406.11657. Guglielmo Faggioli, Laura Dietz, Charles LA Clarke, Gianluca Demartini, Matthias Hagen, Claudia Hauff, Noriko Kando, Evangelos Kanoulas, Martin Potthast, Benno Stein, et al. 2023. Perspectives on large language models for relevance judgment. In Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval, pages 39 50. Bahare Fatemi, Mehran Kazemi, Anton Tsitsulin, Karishma Malkan, Jinyeong Yim, John Palowitch, Sungyong Seo, Jonathan Halcrow, and Bryan Perozzi. 2024. Test of time: benchmark for evaluating llms on temporal reasoning. ArXiv preprint, abs/2406.09170. Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, and Jidong Ge. 2023. Lawbench: Benchmarking legal knowledge of large language models. ArXiv preprint, abs/2309.16289. Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, and Xiaojun Wan. 2024a. Llm-based nlg evaluation: Current status and challenges. ArXiv preprint, abs/2402.01383. of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10511068, Singapore. Association for Computational Linguistics. Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. 2023. Human-like summarization evaluation with chatgpt. ArXiv preprint, abs/2304.02554. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 68946910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Yicheng Gao, Gonghan Xu, Zhe Wang, and Arman Cohan. 2024b. Bayesian calibration of win rate estimation with llm evaluators. ArXiv preprint, abs/2411.04424. Shahriar Golchin and Mihai Surdeanu. Time travel in llms: Tracing data contamination in large language models. In The Twelfth International Conference on Learning Representations. Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. 2024. Direct language model alignment from online ai feedback. ArXiv preprint, abs/2402.04792. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173, Singapore. Association for Computational Linguistics. Hosein Hasanbeig, Hiteshi Sharma, Leo Betthauser, Felipe Vieira Frujeri, and Ida Momennejad. 2023. Allure: auditing and improving llm-based evaluation of text using iterative in-context-learning. arXiv eprints, pages arXiv2309. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2024. Large language models are zero-shot rankers for recommender systems. In European Conference on Information Retrieval, pages 364381. Aaron Hu. 2024. Developing an ai-based psychometric system for assessing learning difficulties and adaptive system to overcome: qualitative and conceptual framework. ArXiv preprint, abs/2403.06284. Xinyu Hu, Li Lin, Mingqi Gao, Xunjian Yin, and Xiaojun Wan. 2024. Themis: reference-free nlg evaluation language model with flexibility and interpretability. ArXiv preprint, abs/2406.18365. Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2023a. Large language models can self-improve. In Proceedings Xu Huang, Zhirui Zhang, Xiang Geng, Yichao Du, Jiajun Chen, and Shujian Huang. 2024. Lost in the source language: How large language models evaluate the quality of machine translation. In Annual Meeting of the Association for Computational Linguistics. Yue Huang, Qihui Zhang, Lichao Sun, et al. 2023b. Trustgpt: benchmark for trustworthy and responsible large language models. ArXiv preprint, abs/2306.11507. Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et al. 2023. Llama guard: Llm-based inputoutput safeguard for human-ai conversations. ArXiv preprint, abs/2312.06674. Andrés Isaza-Giraldo, Paulo Bala, Pedro Campos, and Lucas Pereira. 2024. Prompt-gaming: pilot study on llm-evaluating agent in meaningful energy game. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, pages 1 12. Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, and Chunting Zhou. 2023a. Multi-dimensional evaluation of text summarization with in-context learning. In Findings of the Association for Computational Linguistics: ACL 2023, pages 84878495, Toronto, Canada. Association for Computational Linguistics. Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, and Chunting Zhou. 2023b. Multi-dimensional evaluation of text summarization with in-context learning. In Findings of the Association for Computational Linguistics: ACL 2023, pages 84878495, Toronto, Canada. Association for Computational Linguistics. Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, and Jaewoo Kang. 2024. Improving medical reasoning through retrieval and self-reflection with retrievalaugmented large language models. Bioinformatics, 40(Supplement_1):i119i129. Liqiang Jing, Ruosen Li, Yunmo Chen, and Xinya Du. 2024. Faithscore: Fine-grained evaluations of hallucinations in large vision-language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 50425063. Jaehun Jung, Faeze Brahman, and Yejin Choi. 2024. Trust or escalate: Llm judges with provable guarArXiv preprint, antees for human agreement. abs/2407.18370. Akira Kawabata and Saku Sugawara. 2024. Rationaleaware answer verification by pairwise self-evaluation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1617816196. Pei Ke, Bosi Wen, Andrew Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, et al. 2024. Critiquellm: Towards an informative critique generation model for evaluation of large language model generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1303413054. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained evaluation capability in language models. In The Twelfth International Conference on Learning Representations. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024. Prometheus 2: An open source language model specialized in evaluating other language models. ArXiv preprint, abs/2405.01535. Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 193203, Tampere, Finland. European Association for Machine Translation. Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang. 2023a. Benchmarking cognitive biases in large language models as evaluators. ArXiv preprint, abs/2309.17012. Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang. 2023b. Benchmarking cognitive biases in large language models as evaluators, 2023. ArXiv preprint, abs/2309.17012. Neema Kotonya, Saran Krishnasamy, Joel Tetreault, and Alejandro Jaimes. 2023. Little giants: Exploring the potential of small LLMs as evaluation metrics in summarization in the Eval4NLP 2023 shared task. In Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems, pages 202218, Bali, Indonesia. Association for Computational Linguistics. Shachi Kumar, Saurav Sahay, Sahisnu Mazumder, Eda Okur, Ramesh Manuvinakurike, Nicole Beckage, Hsuan Su, Hung-yi Lee, and Lama Nachman. 2024. Decoding biases: Automated methods and llm judges for gender bias detection in language models. ArXiv preprint, abs/2408.03907. Preethi Lahoti, Nicholas Blumm, Xiao Ma, Raghavendra Kotikalapudi, Sahitya Potluri, Qijun Tan, Hansa Srinivasan, Ben Packer, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023. Improving diversity of demographic representation in large language models via collective-critiques and self-voting. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1038310405, Singapore. Association for Computational Linguistics. Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. 2023. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. ArXiv preprint, abs/2309.00267. Sangkyu Lee, Sungdong Kim, Ashkan Yousefpour, Minjoon Seo, Kang Min Yoo, and Youngjae Yu. 2024. Aligning large language models by on-policy selfjudgment. ArXiv preprint, abs/2402.11253. Dawei Li, Zhen Tan, and Huan Liu. 2024a. Exploring large language models for feature selection: datacentric perspective. ArXiv preprint, abs/2408.12025. Dawei Li, Zhen Tan, Peijia Qian, Yifan Li, Kumar Satvik Chaudhary, Lijie Hu, and Jiayi Shen. Improving multi-agent large lan2024b. Smoa: guage models with sparse mixture-of-agents. ArXiv preprint, abs/2411.03284. Dawei Li, Shu Yang, Zhen Tan, Jae Young Baik, Sunkwon Yun, Joseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-Tran, Ying Ding, et al. 2024c. Dalk: Dynamic co-augmentation of llms and kg to answer alzheimers disease questions with scientific literature. ArXiv preprint, abs/2405.04819. Haitao Li, Junjie Chen, Qingyao Ai, Zhumin Chu, Yujia Zhou, Qian Dong, and Yiqun Liu. 2024d. Calibraeval: Calibrating prediction distribution to mitigate selection bias in llms-as-judges. ArXiv preprint, abs/2410.15393. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, hai zhao, and Pengfei Liu. 2024e. Generative judge for evaluating alignment. In The Twelfth International Conference on Learning Representations. Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, and Jing Shao. 2024f. Salad-bench: hierarchical and comprehensive safety benchmark for large language models. ArXiv preprint, abs/2402.05044. Renhao Li, Minghuan Tan, Derek Wong, and Min Yang. 2024g. Coevol: Constructing better responses for instruction finetuning through multi-agent cooperation. ArXiv preprint, abs/2406.07054. Ruosen Li, Teerth Patel, and Xinya Du. 2023. Prd: Peer rank and discussion improve large language model based evaluations. ArXiv preprint, abs/2307.02762. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. 2024h. From crowdsourced data to highquality benchmarks: Arena-hard and benchbuilder pipeline. ArXiv preprint, abs/2406.11939. Xiaonan Li and Xipeng Qiu. 2023. MoT: Memory-ofthought enables ChatGPT to self-improve. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6354 6374, Singapore. Association for Computational Linguistics. Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, and Hongyang Zhang. Rain: Your language models can align themselves without finetuning. In The Twelfth International Conference on Learning Representations. Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, and Shuai Ma. 2024i. Leveraging large language models for nlg evaluation: Advances and challenges. Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, and Yang Liu. 2024j. Split and merge: Aligning position biases in llmbased evaluators. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1108411108. Jingcong Liang, Rong Ye, Meng Han, Ruofei Lai, Xinyu Zhang, Xuanjing Huang, and Zhongyu Wei. 2024a. Debatrix: Multi-dimensinal debate judge with iterative chronological analysis based on llm. arXiv preprint arXiv:2403.08010. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. ArXiv preprint, abs/2211.09110. Sirui Liang, Baoli Zhang, Jun Zhao, and Kang Liu. 2024b. Abseval: An agent-based framework for script evaluation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1241812434. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023. Encouraging divergent thinking in large language models through multi-agent debate. ArXiv preprint, abs/2305.19118. Yiming Liang, Ge Zhang, Xingwei Qu, Tianyu Zheng, Jiawei Guo, Xinrun Du, Zhenzhu Yang, Jiaheng Liu, Chenghua Lin, Lei Ma, et al. 2024c. I-sheep: Self-alignment of llm from scratch through an iterative self-enhancement paradigm. ArXiv preprint, abs/2408.08072. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. arXiv preprint arXiv:2305.20050. Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. 2023. The unlocking spell on base llms: Rethinking alignment via In The Twelfth International in-context learning. Conference on Learning Representations. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Yen-Ting Lin and Yun-Nung Chen. 2023a. LLM-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. In Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023), pages 47 58, Toronto, Canada. Association for Computational Linguistics. Yen-Ting Lin and Yun-Nung Chen. 2023b. LLM-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. In Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023), pages 47 58, Toronto, Canada. Association for Computational Linguistics. Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 21222132, Austin, Texas. Association for Computational Linguistics. Minqian Liu, Ying Shen, Zhiyang Xu, Yixin Cao, Eunah Cho, Vaibhav Kumar, Reza Ghanadan, and Lifu Huang. 2024a. X-eval: Generalizable multi-aspect text evaluation via augmented instruction tuning with auxiliary evaluation aspects. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 85608579, Mexico City, Mexico. Association for Computational Linguistics. Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter Liu, and Jialu Liu. 2023a. Statistical rejection sampling improves preference optimization. ArXiv preprint, abs/2309.06657. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023b. G-eval: NLG evaluation using gpt-4 with better human alignIn Proceedings of the 2023 Conference on ment. Empirical Methods in Natural Language Processing, pages 25112522, Singapore. Association for Computational Linguistics. Yiqi Liu, Nafise Sadat Moosavi, and Chenghua Lin. Llms as narcissistic evaluators: When ArXiv preprint, 2023c. ego inflates evaluation scores. abs/2311.09766. Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. 2024b. Calibrating LLMbased evaluator. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 26382656, Torino, Italia. ELRA and ICCL. Zijun Liu, Boqun Kou, Peng Li, Ming Yan, Ji Zhang, Fei Huang, and Yang Liu. 2024c. Meta ranking: Less capable language models are capable for single response judgement. ArXiv preprint, abs/2402.12146. Adian Liusie, Potsawee Manakul, and Mark Gales. 2024. LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 139151, St. Julians, Malta. Association for Computational Linguistics. Adian Liusie, Potsawee Manakul, and Mark JF Gales. Zero-shot nlg evaluation through pairArXiv preprint, 2023. ware comparisons with llms. abs/2307.07889. SESSMENTS BY LLMS. Juding the judges: Asystematic investigation of position bias in pairwise comparative as. Wen Luo, Tianshu Shen, Wei Li, Guangyue Peng, Richeng Xuan, Houfeng Wang, and Xi Yang. 2024. Halludial: large-scale benchmark for automatic ArXiv dialogue-level hallucination evaluation. preprint, abs/2406.07070. Shengjie Ma, Chong Chen, Qi Chu, and Jiaxin Mao. 2024. Leveraging large language models for relevance judgments in legal case retrieval. ArXiv preprint, abs/2403.18405. Xueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. 2023. Zero-shot listwise document reranking with large language model. ArXiv preprint, abs/2305.02156. Katerina Margatina, Timo Schick, Nikolaos Aletras, and Jane Dwivedi-Yu. 2023. Active learning principles for in-context learning with large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 50115034, Singapore. Association for Computational Linguistics. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1207612100, Singapore. Association for Computational Linguistics. Philipp Mondorf and Barbara Plank. 2024. Beyond accuracy: Evaluating the reasoning behavior of large language modelsa survey. ArXiv preprint, abs/2404.01869. Behrad Moniri, Hamed Hassani, and Edgar Dobriban. 2024. Evaluating the performance of large language models via debates. ArXiv preprint, abs/2406.11044. Bhuvanashree Murugadoss, Christian Poelitz, Ian Drosos, Vu Le, Nick McKenna, Carina Suzana Negreanu, Chris Parnin, and Advait Sarkar. 2024. Evaluating the evaluator: Measuring llms adherence to task evaluation instructions. ArXiv preprint, abs/2408.08781. Linyong Nan, Ellen Zhang, Weijin Zou, Yilun Zhao, Wenfei Zhou, and Arman Cohan. 2024. On evaluating the integration of reasoning and action in LLM agents with database question answering. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 45564579, Mexico City, Mexico. Association for Computational Linguistics. Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. 2024. Routellm: Learning to route llms with preference data. ArXiv preprint, abs/2406.18665. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Alizée Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei Severyn. 2024. West-of-n: Synthetic preference generation for improved reward modeling. ArXiv preprint, abs/2401.12086. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2024. Automatically correcting large language models: Surveying the landscape of diverse automated correction strategies. Transactions of the Association for Computational Linguistics, 12:484506. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi. 2024. Offsetbias: Leveraging debiased data for tuning evaluators. ArXiv preprint, abs/2407.06551. Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, and Chitta Baral. 2024. Logicbench: Towards systematic evaluation of logical reasoning ability of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13679 13707. Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, Hong Kong, China. Association for Computational Linguistics. John Penfever et al. 2024. Style over substance: Failure modes of llm judges in alignment benchmarking. ArXiv preprint, abs/2410.17578. Mansi Phute, Alec Helbling, Matthew Daniel Hull, ShengYun Peng, Sebastian Szyller, Cory Cornelius, and Duen Horng Chau. 2023. Llm self defense: By self examination, llms know they are being tricked. In The Second Tiny Papers Track at ICLR 2024. Matt Post. 2018. call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186 191, Brussels, Belgium. Association for Computational Linguistics. Shenbin Qian, Archchana Sindhujan, Minnie Kabra, Diptesh Kanojia, Constantin Orašan, Tharindu Ranasinghe, and Fred Blain. 2024. What do large language models need for machine translation evaluaIn Proceedings of the 2024 Conference on tion? Empirical Methods in Natural Language Processing, pages 36603674. Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, and Michael Bendersky. 2024. Large language models are effective text rankers with pairwise ranking prompting. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 15041518, Mexico City, Mexico. Association for Computational Linguistics. Zackary Rackauckas, Arthur Câmara, and Jakub Zavrel. 2024. Evaluating rag-fusion with ragelo: an automated elo-based framework. ArXiv preprint, abs/2406.14783. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Vyas Raina, Adian Liusie, and Mark Gales. 2024. Is llm-as-a-judge robust? investigating universal adversarial attacks on zero-shot llm assessment. ArXiv preprint, abs/2402.14016. Ehud Reiter. 2018. structured review of the validity of BLEU. Computational Linguistics, 44(3):393401. Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389. Ananya Sai, Akash Kumar Mohankumar, and Mitesh Khapra. 2022. survey of evaluation metrics used for nlg systems. ACM Computing Surveys (CSUR), 55(2):139. Nino Scherrer, Claudia Shi, Amir Feder, and David M. Blei. 2023. Evaluating the moral beliefs encoded in llms. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. ArXiv preprint, abs/1707.06347. Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 78817892, Online. Association for Computational Linguistics. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Jonathan Berant, and Aviral Kumar. Agarwal, 2024. Rewarding progress: Scaling automated proarXiv preprint cess verifiers for llm reasoning. arXiv:2410.08146. Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping Luo, Shengbo Eben Li, Masayoshi Tomizuka, Wei Zhan, and Mingyu Ding. 2023. Languagempc: Large language models as decision makers for autonomous driving. ArXiv preprint, abs/2310.03026. Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. 2021. Societal biases in language generation: Progress and challenges. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 42754293, Online. Association for Computational Linguistics. Ravi Raju, Swayambhoo Jain, Bo Li, Jonathan Li, and Urmish Thakkar. 2024. Constructing domainspecific evaluation sets for llm-as-a-judge. ArXiv preprint, abs/2408.08808. Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, and Neil Zhenqiang Gong. 2024. Optimization-based prompt injection attack to llmas-a-judge. ArXiv preprint, abs/2403.17710. Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, and Seunghyeok Hong. 2024a. Llm-as-a-judge & reward model: What they can and cannot do. ArXiv preprint, abs/2409.11239. Guijin Son et al. 2024b. Mm-eval: multilingual metaevaluation benchmark for llm-as-a-judge and reward models. ArXiv preprint, abs/2410.17578. Mingyang Song, Mao Zheng, and Xuan Luo. 2024. Can many-shot in-context learning help long-context llm judges? see more, judge better! ArXiv preprint, abs/2406.11629. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. ArXiv preprint, abs/2206.04615. Andreas Stephan, Dawei Zhu, Matthias Aßenmacher, Xiaoyu Shen, and Benjamin Roth. 2024. From calculation to adjudication: Examining llm judges on mathematical reasoning tasks. ArXiv preprint, abs/2409.04168. Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT good at search? investigating large language models as re-ranking agents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1491814937, Singapore. Association for Computational Linguistics. Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Daniel Cox, Yiming Yang, and Chuang Gan. 2024. Salmon: Selfalignment with instructable reward models. In The Twelfth International Conference on Learning Representations. Sijun Tan et al. 2024a. Judgebench: benchmark for evaluating llm-based judges. ArXiv preprint, abs/2410.12784. Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. 2024b. Large language models for data annotation: survey. ArXiv preprint, abs/2402.13446. Qiaoyu Tang, Jiawei Chen, Bowen Yu, Yaojie Lu, Cheng Fu, Haiyang Yu, Hongyu Lin, Fei Huang, Ben He, Xianpei Han, et al. 2024a. Self-retrieval: Building an information retrieval system with one large language model. ArXiv preprint, abs/2403.00801. Raphael Tang, Crystina Zhang, Xueguang Ma, Jimmy Lin, and Ferhan Ture. 2024b. Found in the middle: Permutation self-consistency improves listwise ranking in large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 23272340, Mexico City, Mexico. Association for Computational Linguistics. Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, and Dieuwke Hupkes. 2024. Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges. ArXiv preprint, abs/2406.12624. Paul Thomas, Seth Spielman, Nick Craswell, and Bhaskar Mitra. 2023. Large language models can accurately predict searcher preferences, 2023. ArXiv preprint, abs/2309.10621. Paul Thomas, Seth Spielman, Nick Craswell, and Bhaskar Mitra. 2024. Large language models can accurately predict searcher preferences. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 19301940. Yongqi Tong, Sizhe Wang, Dawei Li, Yifan Wang, Simeng Han, Zi Lin, Chengsong Huang, Jiaxin Huang, and Jingbo Shang. 2024. Optimizing language models reasoning abilities with weak supervision. ArXiv preprint, abs/2405.04086. Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, and Yun-Hsuan Sung. 2024. Foundational autoraters: Taming large language models for better automatic evaluation. ArXiv preprint, abs/2407.10817. Binjie Wang, Steffi Chern, Ethan Chern, and Pengfei Liu. 2024a. Halu-j: Critique-based hallucination judge. ArXiv preprint, abs/2407.12943. Boshi Wang, Xiang Yue, and Huan Sun. 2023a. Can ChatGPT defend its belief in truth? evaluating LLM reasoning via debate. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1186511881, Singapore. Association for Computational Linguistics. Chengrui Wang, Qingqing Long, Xiao Meng, Xunxin Cai, Chengjun Wu, Zhen Meng, Xuezhi Wang, and Yuanchun Zhou. 2024b. Biorag: rag-llm framework for biological question reasoning. ArXiv preprint, abs/2408.01107. Danqing Wang, Kevin Yang, Hanlin Zhu, Xiaomeng Yang, Andrew Cohen, Lei Li, and Yuandong Tian. 2023b. Learning personalized story evaluation. ArXiv preprint, abs/2310.03304. Peifeng Wang, Austin Xu, Yilun Zhou, Caiming Xiong, and Shafiq Joty. 2024c. Direct judgement preference optimization. ArXiv preprint, abs/2409.14664. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023c. Large language models are not fair evaluators. ArXiv preprint, abs/2305.17926. Sizhe Wang, Yongqi Tong, Hengyuan Zhang, Dawei Li, Xin Zhang, and Tianlong Chen. 2024d. Bpo: Towards balanced preference optimization between knowledge breadth and depth in alignment. arXiv preprint arXiv:2411.10914. Song Wang, Peng Wang, Tong Zhou, Yushun Dong, Zhen Tan, and Jundong Li. 2024e. Ceb: Compositional evaluation benchmark for fairness in large language models. ArXiv preprint, abs/2407.02408. Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and Self-taught evaluators. ArXiv Xian Li. 2024f. preprint, abs/2408.02666. Yicheng Wang, Jiayi Yuan, Yu-Neng Chuang, Zhuoer Wang, Yingchi Liu, Mark Cusick, Param Kulkarni, Zhengping Ji, Yasser Ibrahim, and Xia Hu. 2024g. Dhp benchmark: Are llms good nlg evaluators? ArXiv preprint, abs/2408.13704. Yidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. 2024h. PandaLM: An automatic evaluation benchmark for LLM instruction tuning optimization. In The Twelfth International Conference on Learning Representations. Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. 2024i. Do-not-answer: Evaluating safeguards in LLMs. In Findings of the Association for Computational Linguistics: EACL 2024, pages 896911, St. Julians, Malta. Association for Computational Linguistics. Ziyu Wang, Hao Li, Di Huang, and Amir Rahmani. 2024j. Healthq: Unveiling questioning capabilities of llm chains in healthcare conversations. ArXiv preprint, abs/2409.19487. Koki Wataoka, Tsubasa Takahashi, and Ryokan Ri. 2024. Self-preference bias in llm-as-a-judge. ArXiv preprint, abs/2410.21819. Hui Wei, Shenghua He, Tian Xia, Andy Wong, Jingyang Lin, and Mei Han. 2024a. Systematic evaluation of llm-as-a-judge in llm alignment tasks: Explainable metrics and diverse prompt templates. ArXiv preprint, abs/2408.13006. Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022a. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022b. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, et al. 2024b. Long-form factuality in large language models. ArXiv preprint, abs/2403.18802. Martin Weyssow, Aton Kamanda, and Houari Sahraoui. 2024. Codeultrafeedback: An llm-as-a-judge dataset for aligning large language models to coding preferences. arXiv preprint arXiv:2403.09032. Ning Wu, Ming Gong, Linjun Shou, Shining Liang, and Daxin Jiang. 2023. Large language models are diverse role-players for summarization evaluation. In CCF International Conference on Natural Language Processing and Chinese Computing, pages 695707. Springer. Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. 2024. Meta-rewarding language models: Self-improving alignment with llm-as-ameta-judge. ArXiv preprint, abs/2407.19594. Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. 2024. Evaluating mathematical reasoning beyond accuracy. ArXiv preprint, abs/2404.05692. Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. 2023. Data selection for language models via importance resampling. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Tinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang, Udari Madhushani Sehwag, Kaixuan Huang, Luxi He, Boyi Wei, Dacheng Li, Ying Sheng, et al. 2024a. Sorry-bench: Systematically evaluating large language model safety refusal behaviors. ArXiv preprint, abs/2406.14598. Yiqing Xie, Wenxuan Zhou, Pradyot Prakash, Di Jin, Yuning Mao, Quintin Fettes, Arya Talebzadeh, Sinong Wang, Han Fang, Carolyn Rose, et al. Improving model factuality with fine2024b. arXiv preprint grained critique-based evaluator. arXiv:2410.18359. Siheng Xiong, Delin Chen, Qingyang Wu, Longxuan Yu, Qingzhen Liu, Dawei Li, Zhikai Chen, Xiaoze Liu, and Liangming Pan. 2024a. Improving causal reasoning in large language models: survey. arXiv preprint arXiv:2410.16676. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. 2024b. Llava-critic: Learning to evaluate multimodal models. ArXiv preprint, abs/2410.02712. Ruoxi Xu, Hongyu Lin, Xianpei Han, Le Sun, and Yingfei Sun. 2024a. Academically intelligent llms ArXiv are not necessarily socially intelligent. preprint, abs/2403.06591. Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, and Jing Gao. 2024b. Sayself: Teaching llms to express confidence with self-reflective rationales. ArXiv preprint, abs/2405.20974. Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, and Lei Li. 2023. INSTRUCTSCORE: Towards explainable text generation evaluation with automatic feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 59675994, Singapore. Association for Computational Linguistics. Hui Yang, Sifu Yue, and Yunzhong He. 2023. Autogpt for online decision making: Benchmarks and additional opinions. ArXiv preprint, abs/2306.02224. Jheng-Hong Yang and Jimmy Lin. 2024. Toward automatic relevance judgment using visionlanguage models for imagetext retrieval evaluation. ArXiv preprint, abs/2408.01363. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. 2023b. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Justice Chao Huang, Pin-Yu Chen, et al. 2024a. or prejudice? quantifying biases in llm-as-a-judge. ArXiv preprint, abs/2410.02736. Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. 2024b. FLASK: Fine-grained language model evaluation based on alignment skill sets. In ICLR 2024 Workshop on Large Language Model (LLM) Agents. Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, and Shikun Zhang. 2024. Kieval: knowledge-grounded interactive evaluation framework for large language models. ArXiv preprint, abs/2402.15043. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, et al. Advancing llm reasoning generalists with preference trees. In AI for Math Workshop@ ICML 2024. Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Boyuan Pan, Heda Wang, and Kan Li. 2024a. Batcheval: Towards human-like text evaluation. ArXiv preprint, abs/2401.00437. Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, et al. 2024b. Rjudge: Benchmarking safety risk awareness for llm agents. ArXiv preprint, abs/2401.10019. Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 2726327277. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024c. Self-rewarding language models. ArXiv preprint, abs/2401.10020. Xiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su, and Huan Sun. 2023. Automatic evaluation of attribution by large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 46154635, Singapore. Association for Computational Linguistics. Yuwei Zeng, Yao Mu, and Lin Shao. 2024. Learning reward for robot skills using large language models via self-alignment. ArXiv preprint, abs/2405.07162. Yuanzhao Zhai, Zhuo Zhang, Kele Xu, Hanyang Peng, Yue Yu, Dawei Feng, Cheng Yang, Bo Ding, and Huaimin Wang. 2024. Online self-preferring language models. ArXiv preprint, abs/2405.14103. Chen Zhang, Luis Fernando DHaro, Yiming Chen, Malu Zhang, and Haizhou Li. 2024a. comprehensive analysis of the effectiveness of large language models as automatic dialogue evaluators. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 19515 19524. AAAI Press. Hengran Zhang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, and Xueqi Cheng. 2024b. Are large language models good at utility judgments? In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 19411951. Hengyuan Zhang, Dawei Li, Yanran Li, Chenming Shang, Chufan Shi, and Yong Jiang. 2023a. Assisting language learners: Automated trans-lingual definition generation via contrastive prompt learning. arXiv preprint arXiv:2306.06058. Hengyuan Zhang, Dawei Li, Shiping Yang, and Yanran Li. 2022. Fine-grained contrastive learning for definition generation. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 10011012. Hengyuan Zhang, Chenming Shang, Sizhe Wang, Dongdong Zhang, Feng Yao, Renliang Sun, Yiyao Yu, Yujiu Yang, and Furu Wei. 2024c. Shifcon: Enhancing non-dominant language capabilities with shift-based contrastive framework. arXiv preprint arXiv:2410.19453. Hengyuan Zhang, Yanru Wu, Dawei Li, Sak Yang, Rui Zhao, Yong Jiang, and Fei Tan. 2024d. Balancing speciality and versatility: coarse to fine framework for supervised fine-tuning large language model. arXiv preprint arXiv:2404.10306. Qiyuan Zhang, Yufei Wang, Tiezheng Yu, Yuxin Jiang, Chuhan Wu, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, et al. 2024e. Reviseval: Improving llm-as-a-judge via response-adapted references. ArXiv preprint, abs/2410.05193. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, and Helen Meng. 2024f. Self-alignment for factuality: Mitigating hallucinations in llms via self-evaluation. ArXiv preprint, abs/2402.09267. Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. 2023b. Wider and deeper llm networks are fairer llm evaluators. ArXiv preprint, abs/2308.01862. John Zhao et al. 2024a. Codejudge-eval: benchmark for evaluating code generation. ArXiv preprint, abs/2401.10019. Lirui Zhao, Yue Yang, Kaipeng Zhang, Wenqi Shao, Yuxin Zhang, Yu Qiao, Ping Luo, and Rongrong Ji. 2024b. Diffagent: Fast and accurate text-to-image In Proapi selection with large language model. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 63906399. Ruochen Zhao, Wenxuan Zhang, Yew Ken Chia, Deli Zhao, and Lidong Bing. 2024c. Auto arena of llms: Automating llm evaluations with agent peerbattles and committee discussions. ArXiv preprint, abs/2405.20267. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023a. survey of large language models. arXiv preprint arXiv:2303.18223. Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter Liu. 2023b. Slic-hf: Sequence likelihood calibration with human feedback. ArXiv preprint, abs/2305.10425. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Hongli Zhou, Hui Huang, Yunfei Long, Bing Xu, Conghui Zhu, Hailong Cao, Muyun Yang, and Tiejun Zhao. 2024a. Mitigating the bias of large language model evaluation. ArXiv preprint, abs/2409.16788. Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, HengTze Cheng, Quoc Le, Ed Chi, Denny Zhou, Swaroop Mishra, and Huaixiu Steven Zheng. 2024b. Selfdiscover: Large language models self-compose reasoning structures. ArXiv preprint, abs/2402.03620. Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, et al. 2023. Sotopia: Interactive evaluation for social intelligence in language agents. ArXiv preprint, abs/2310.11667. Yue Zhou, Chenlu Guo, Xu Wang, Yi Chang, and Yuan Wu. 2024c. survey on data augmentation in large model era. ArXiv preprint, abs/2401.15422. Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, Karthik Ganesan, Wei-Lin Chiang, Jian Zhang, and Jiantao Jiao. 2024a. Starling-7b: Improving helpfulness and harmlessness with rlaif. In First Conference on Language Modeling. Kaijie Zhu, Jindong Wang, Qinlin Zhao, Ruochen Xu, and Xing Xie. 2024b. Dynamic evaluation of large language models by meta probing agents. In Fortyfirst International Conference on Machine Learning. Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. Judgelm: Fine-tuned large language models are scalable judges. ArXiv preprint, abs/2310.17631. Honglei Zhuang, Zhen Qin, Kai Hui, Junru Wu, Le Yan, Xuanhui Wang, and Michael Bendersky. 2024a. Beyond yes and no: Improving zero-shot LLM rankers via scoring fine-grained relevance labels. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 358370, Mexico City, Mexico. Association for Computational Linguistics. Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon. 2024b. setwise approach for effective and highly efficient zero-shot ranking with large language models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3847. Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, et al. 2024. Agent-as-ajudge: Evaluate agents with agents. ArXiv preprint, abs/2410.10934."
        }
    ],
    "affiliations": [
        "Arizona State University",
        "Emory University",
        "Illinois Institute of Technology",
        "University of California, Berkeley",
        "University of Illinois Chicago",
        "University of Maryland, Baltimore County"
    ]
}