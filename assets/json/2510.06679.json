{
    "paper_title": "DreamOmni2: Multimodal Instruction-based Editing and Generation",
    "authors": [
        "Bin Xia",
        "Bohao Peng",
        "Yuechen Zhang",
        "Junjia Huang",
        "Jiyang Liu",
        "Jingyao Li",
        "Haoru Tan",
        "Sitong Wu",
        "Chengyao Wang",
        "Yitong Wang",
        "Xinglong Wu",
        "Bei Yu",
        "Jiaya Jia"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in instruction-based image editing and subject-driven generation have garnered significant attention, yet both tasks still face limitations in meeting practical user needs. Instruction-based editing relies solely on language instructions, which often fail to capture specific editing details, making reference images necessary. Meanwhile, subject-driven generation is limited to combining concrete objects or people, overlooking broader, abstract concepts. To address these challenges, we propose two novel tasks: multimodal instruction-based editing and generation. These tasks support both text and image instructions and extend the scope to include both concrete and abstract concepts, greatly enhancing their practical applications. We introduce DreamOmni2, tackling two primary challenges: data creation and model framework design. Our data synthesis pipeline consists of three steps: (1) using a feature mixing method to create extraction data for both abstract and concrete concepts, (2) generating multimodal instruction-based editing training data using the editing and extraction models, and (3) further applying the extraction model to create training data for multimodal instruction-based editing. For the framework, to handle multi-image input, we propose an index encoding and position encoding shift scheme, which helps the model distinguish images and avoid pixel confusion. Additionally, we introduce joint training with the VLM and our generation/editing model to better process complex instructions. In addition, we have proposed comprehensive benchmarks for these two new tasks to drive their development. Experiments show that DreamOmni2 has achieved impressive results. Models and codes will be released."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 9 7 6 6 0 . 0 1 5 2 : r Work in progress DREAMOMNI2: MULTIMODAL INSTRUCTION-BASED EDITING AND GENERATION Bin Xia1,4, Bohao Peng1, Yuechen Zhang1, Junjia Huang4, Jiyang Liu4, Jingyao Li1, Haoru Tan3, Sitong Wu1, Chengyao Wang1, Yitong Wang4, Xinglong Wu4, Bei Yu1, and Jiaya Jia2 1CUHK 2HKUST 3HKU 4ByteDance Inc https://github.com/dvlab-research/DreamOmni2 Figure 1: The gallery of overview: Enabling multimodal instruction-based editing and generation, extending beyond concrete objects to abstract attributions."
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advancements in instruction-based image editing and subject-driven generation have garnered significant attention, yet both tasks still face limitations in meeting practical user needs. Instruction-based editing relies solely on language instructions, which often fail to capture specific editing details, making reference images necessary. Meanwhile, subject-driven generation is limited to combining concrete objects or people, overlooking broader, abstract concepts. To address these challenges, we propose two novel tasks: multimodal instruction-based editing and generation. These tasks support both text and image instructions and extend the scope to include both concrete and abstract concepts, greatly enhancing their practical applications. We introduce DreamOmni2, tackling two primary challenges: data creation and model framework design. Our data synthesis 1 Work in progress pipeline consists of three steps: (1) using feature mixing method to create extraction data for both abstract and concrete concepts, (2) generating multimodal instruction-based editing training data using the editing and extraction models, and (3) further applying the extraction model to create training data for multimodal instruction-based editing. For the framework, to handle multi-image input, we propose an index encoding and position encoding shift scheme, which helps the model distinguish images and avoid pixel confusion. Additionally, we introduce joint training with the VLM and our generation/editing model to better process complex instructions. In addition, we have proposed comprehensive benchmarks for these two new tasks to drive their development. Experiments show that DreamOmni2 has achieved impressive results. Models and codes will be released."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in unified generation and editing models (OpenAI, 2025; Google, 2025b) have gained significant attention and praise in the market. The success of these models can be attributed to several factors: (1) They greatly improve user experience by simplifying the process, allowing users to perform various design tasks within single model without the need to switch between different ones. (2) Unified models reduce deployment costs for service providers. (3) Academically, they contribute to the exploration of AGI and world models, enabling the accurate understanding of user instructions and the creation or modification of real-world visual content. Current released works (Batifol et al., 2025; Wu et al., 2025a; Deng et al., 2025) mainly focus on instruction-based editing and subject-driven generation with text prompt and single source image input, but both have limitations in application and advancing intelligence. (1) For instruction-based editing (Brooks et al., 2023; Liu et al., 2025; Xia et al., 2025b), instructions alone often fail to fully capture the users intent. For example, when user says, make the bag in the image have the same pattern as the dress in the given image, its difficult to describe the complex pattern of dress with words. Thus, accurate editing requires multimodal instructions, including reference images and text. Notably, this challenge involves not only modifying objects but also any abstract attributes, such as texture, material, posture, hairstyle, and design style, which are difficult to describe with words. (2) Subject-driven generation models (Xiao et al., 2025; Wu et al., 2025c) and even commercial unified models (Google, 2025b) mainly focus on generating content from specific concrete objects or people, with limited research on referencing more general abstract attributions from input images. To create more intelligent and all-encompassing unified creation tool, we propose DreamOmni2. The biggest challenge lies in the training data, so we introduce comprehensive data pipeline for multimodal instruction-based editing and generation, consisting of the following steps  (Fig. 2)  : (1) We propose feature mixing scheme to exchange attention features between two batches, allowing the model to generate pairs of images with the same abstract attribute or concrete object. Compared to the previous diptych method (Wu et al., 2025c) for generating image pairs, our scheme achieves higher success rate, produces images with greater resolution, and completely eliminates any content blending at the edges when the pair of images is split. (2) Using the pairs generated in Step 1, we train generation-editing model as an extraction model. This model extracts concrete objects or abstract attributions from the given image and generates another based on instructions. Compared to previous methods (Wu et al., 2025c; Chen et al., 2025) relying on segmentation and detection, our extraction model offers three key advantages: it can handle abstract concepts, occluded objects, and generate more diverse reference images. We then generate multimodal instruction-based editing training data, which includes target image, source image, an editing instruction, and multiple reference images. We use text-to-image (T2I) model to generate target image based on multiple keywords or select one from real image database. The extraction model then generates reference images for one of the keywords. Additionally, we use an instruction-based editing model (Batifol et al., 2025) to transform the content defined by the selected keyword into something different, obtaining the source image. (3) We create multimodal instruction-based generation data by applying the extraction model to generate several reference images based on keywords from the source images created in Step 2. Thus, we build data for generating images from multiple reference images. Furthermore, the current SOTA unified generation and editing models (Batifol et al., 2025) still cannot handle multiple image inputs. To this end, we propose the Dreamomni2 framework. First, we 2 Work in progress propose an index encoding and position encoding shift scheme. Index encoding helps the model identify the input images index, improving its understanding of the referenced image in the instructions. Position encoding is shifted based on previous inputs, preventing pixel confusion and the copy-and-paste effect in the generated results. In addition, we propose joint training scheme for the generation/editing model and VLM. While instructions in generation and editing models are typically simple, real-world user instructions are often irregular and logically complex. VLM pretrained on large-scale corpora can better understand these complex intentions, translating instructions into form the model can comprehend, significantly improving performance in real-world scenarios. Furthermore, for these two new tasks, we have built the DreamOmni2 benchmark using real image data. This allows for more accurate assessment of the models generalization and performance in real-world scenarios. Our main contributions are fourfold: We propose two highly practical tasks: multimodal instruction-based editing and generation guided by any concrete or abstract concept. Introducing these two tasks makes current unified generation and editing models smarter and more versatile creative tools. We propose three-stage data creation pipeline. Leveraging this pipeline, we have built high-quality, comprehensive multimodal instruction-based editing and generation dataset. We present the DreamOmni2 framework, which introduces the index encoding and position encoding shift scheme, enabling the model to handle multi-reference image inputs. Additionally, we propose joint training scheme for the generation/editing model and VLM, enhancing the models ability to understand complex user instructions. For these two new tasks, we propose DreamOmni2 benchmark built from real image data. Experiments demonstrate the effectiveness of DreamOmni2 in real-world scenarios."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Instruction-based Editing refers to modifying an image based on users language instruction (Deng et al., 2025; Sheynin et al., 2023; Xia et al., 2024). The main challenge of this task lies in the creation of high-quality and accurate editing datasets. As pioneering work, InstructP2P (Brooks et al., 2023) introduced an instruction-based image editing dataset by fine-tuning GPT-3 and Promptto-Prompt (Hertz et al., 2022) with SD 1.5 (Rombach et al., 2022). Since then, many other approaches (Zhang et al., 2024; Xia et al., 2025b; Wei et al., 2024; Ge et al., 2024) for creating datasets have emerged, such as employing people to create data, using inpainting methods, collage-based methods, and using different expert models. Recently, DreamVE (Xia et al., 2025a) has unified instruction-based image and video editing. However, language-based editing is limited, as many details in real-world scenarios cant be captured with words, requiring reference images for better description. To this end, we propose multimodal instruction-based editing, enabling guidance from concrete objects or abstract attributes in reference images. This makes unified image generation and editing models (Batifol et al., 2025; Wu et al., 2025a) more comprehensive and practical. Subject-driven Generation has been extensively studied. Methods like Dreambooth (Ruiz et al., 2023) and textual inversion (Gal et al., 2022) fine-tune models on multiple images of the same subject, enabling subject-driven generation. However, this requires users to prepare several images and perform fine-tuning for each new subject, which is not user-friendly. Later approaches like IP-adapter (Ye et al., 2023) and BLIP-diffusion (Li et al., 2023) used visual encoders to compress the subject of reference image into vector and inject it into diffusion model, enabling subjectdriven generation without fine-tuning. IC LoRA (Huang et al., 2024) and Ominicontrol (Tan et al., 2024) further explored the inherent image reference capabilities of DIT models (Peebles & Xie, 2023). Recently, unified generation and editing models (Xia et al., 2025b; Batifol et al., 2025; Wu et al., 2025a; Xiao et al., 2025) have adopted the simple approach of encoding reference images as visual tokens, concatenating them with text and noise tokens, and feeding them into the DIT model. However, prior methods focus mainly on concrete objects, limiting their ability to capture broader abstract concepts. In this paper, we propose multimodal instruction-based generation, task that enables referencing any concrete objects or abstract attributes in reference images to generate new ones. This extends the scope of subject-driven generation and enhances its practicality. 3 Work in progress (1) In stage 1, we use Figure 2: The Overview of DreamOmni2s training data construction. feature mixing scheme to leverage the base models T2I capabilities, creating high-quality data pairs with concrete objects and abstract attributes. (2) In stage 2, we generate multimodal instructionbased editing data. Using stage 1 data, we train an extraction model to simulate objects or attributes in the target image and generate reference image based on instructions. Additionally, we use an instruction-based editing model to modify the extracted objects or attributes in the target image to be different, creating the source image. This generates training pairs from reference and source images to the target image. (3) In stage 3, we extract objects from stage 2s source images to create new reference images, forming training data for generating target images from reference images."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 SYNTHETIC DATA Multimodal instruction-based editing and generation are new tasks, with the main challenge being the lack of training data. For multimodal instruction-based editing, the previous data creation pipeline (Brooks et al., 2023; Wei et al., 2024) involves generating triplets of instructions, source images, and target images. However, this approach does not allow for creating data that incorporates reference images as condition for editing. For multimodal instruction-based generation, the previous subject-generation data pipeline (Wu et al., 2025c; Chen et al., 2025) relies on segmentation detection models to create reference images. This approach makes it difficult to synthesize data for generating reference abstract attributions or occluded concrete objects. To address the training data problem for these two tasks, we propose comprehensive synthetic data pipeline. Specifically, as illustrated in Fig. 2, our approach consists of three stages. In the first stage, 4 Work in progress Figure 3: Data distribution and samples for multimodal instruction-based editing and generation training data. Our dataset is comprehensive and diverse, including the generation and editing of concrete objects as well as abstract attributions, such as local and global attributions. we introduce feature mixing scheme, where dual-branch structure is employed to simultaneously generate both the source image and the target image as follows: Attntar(Q, K, ) = softmax (cid:19) (cid:18) QK , (1) tar; Qt where = [Qn tar], = [K tar are the text features from the target branch, while Qn from the target branch. layer as the tar, and tar are the noise features src are the noise features from the source branch at the same tar. [; ] indicates token (or called length) dimension concatenation. tar; tar, and src], and = [V src and tar and src]. Qt tar, tar; tar, tar; tar; Our feature mixing scheme leverages the models inherent T2I capability to generate paired training data. Compared to the previous UNO (Wu et al., 2025c) diptych generation method, our feature mixing scheme has several clear advantages: (1) The diptych method halves the image resolution by forcing two images into one, while Feature Mixing generates in two branches without reducing resolution. (2) The diptych approach often misplaces the dividing line, leading to content blending. Our method avoids this issue. (3) Data generated by the feature mixing scheme is of higher quality and accuracy than that from the diptych approach. Then, we use the data to train extraction models. Our training data not only enhances the base model (Batifol et al., 2025)s ability to extract concrete objects but also enables it to capture abstract concepts, capability it previously lacked. Afterward, as shown in Fig. 2 stage 2, we create multimodal instruction-based editing data. Specifically, we first create target images, using both T2I model-generated data and real images. For T2I-generated images, we randomly select diverse element keywords (e.g., objects or attributes) and use an LLM to compose prompt, which the T2I model then uses to generate the target image. For real images, we directly use VLM to extract keywords. T2I data is more flexible, allowing any concept combination, while real images reflect natural distributions. Thus, we combine both types of data. Next, using the extraction model trained in stage 1, we extract an object or attribution from the target image based on selected keyword to create reference image. We then apply instructionbased editing model (Batifol et al., 2025) to alter the selected keyword in the target image, obtaining the source image. Finally, we use an LLM to generate the editing instructions, forming training tuple consisting of the source image, instruction, reference image, and target image. After that, as shown in Fig. 2 stage 3, we create multimodal instruction-based generation data. We use the extraction model to extract keywords from the source image in stage 2, generating reference images. By combining these with the reference images from stage 2, we can obtain training tuples consisting of multiple reference images, an instruction, and target image. Our created dataset is shown in Fig. 3. Our dataset includes both real and synthetic target data, covering wide range of object categories for generation and editing, including various abstract attributions and concrete objects. Additionally, we provide comprehensive set of reference images, with cases ranging from one to five references, enabling the model to handle wide variety of tasks. 3.2 FRAMEWORK AND TRAINING The unified generation and editing base model (Batifol et al., 2025) can only process single input In multimodal instruction-based image. To this end, we propose the DreamOmni2 framework. 5 Work in progress Table 1: Comparison between our DreamOmni2 benchmark and existing related benchmarks. Benchmarks Task Type Num Reference Editing Target Concrete Object Abstract Attribution DreamBooth (Ruiz et al., 2023) OmniContext (Wu et al., 2025b) DreamOmni2 (Ours) Generation Generation Generation & Editing Single Multiple Multiple tasks, users typically reference images as image 1, image 2 for convenience. However, in DIT, positional encoding alone cannot accurately distinguish the index of reference images. Therefore, we solve this by adding an index encoding to positional channels. Although index encoding helps distinguish reference images, we found that the position encoding still requires an offset based on the size of the previously input reference images. By adding this offset to the position encoding, we observed reduction in copy-and-paste artifacts and pixel confusion between reference images. Currently, training instructions for generation and editing models are usually well-structured with fixed format. However, real-world user instructions are often irregular or logically inconsistent, creating gap that can hinder the models understanding and reduce performance. To address this, we propose joint training of the VLM and generation models, enabling the VLM to interpret complex user instructions and output them in the structured format used in training, helping the editing and generation model better understand user intent. For multimodal instruction-based editing, the predefined output format combines user instructions with refined image descriptions, while for multimodal instruction-based generation, the VLM directly outputs refined image description. During training, we fine-tune Qwen2.5-VL (Wang et al., 2024) 7B to learn the predefined standard output format, with learning rate of 1 105, using approximately 10 A100 hours. We then train the editing and generation models using LoRA on Flux Kontext (Batifol et al., 2025) to perform multimodal instruction-based editing and generation with the predefined standard instruction format. Notably, by using LoRA for training, we can retain the original instruction-editing capabilities of Kontext. As soon as reference image is detected, our LoRA is activated, seamlessly integrating multimodal instruction-based editing and generation into the unified model. Additionally, we train LoRA for generation and editing separately, as the distinction between generation and editing lies in whether the consistency of the source image is preserved. Since instructions often do not clarify whether the user intends to edit or generate, separate training allows users to make their own choice. Both DreamOmni2 editing and generation LoRA are trained on batch size of 16 and learning rate of 5 106, consuming about 384 A100 hours. 3.3 BENCHMARK Currently, no benchmark exists for multimodal instruction-based editing and generation. As shown in Tab. 1, DreamBooth (Ruiz et al., 2023) only supports single-image generation. Although OmniContext (Wu et al., 2025b) includes some multi-reference testing cases, it focuses solely on concrete object combinations and does not evaluate multimodal instruction-based editing or the inclusion of abstract attributes. To address this, we propose the DreamOmni2 benchmark to drive progress in these areas. Our benchmark is comprehensive, consisting of real images to accurately assess the models performance in real-world scenarios. The test cases cover variety of categories, including the reference generation and editing of abstract attributions (global and local) and concrete objects. More details about our DreamOmni2 benchmark can be found in the Appendix."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Evaluation on Multimodal Instruction-based Image Editing. As shown in Tab. 2, we compare several competitive models that natively support multiple image inputs, such as DreamO (Mou et al., 2025), Omnigen2 (Wu et al., 2025b), and Qwen-Image-Edit-2509 (Wu et al., 2025a). Although Kontext (Batifol et al., 2025) and Qwen-Image-Edit (Wu et al., 2025a) do not natively support multiple image inputs, we applied the method from Diffusers (von Platen et al., 2022), which combines multiple images into one input. We also compared closed-source commercial models, such as Nano Banana (Google, 2025b) and GPT-4o (OpenAI, 2025). We tested editing examples with concrete objects and abstract attributions on DreamOmni2 benchmark. The models were evaluated for success rates by Gemini 2.5 (Google, 2025a) and Doubao 1.6 (ByteDance, 2025), and several professional engineers manually assessed the results. As shown in Tab. 2, our DreamOmni2 achieved the best 6 Work in progress Figure 4: Visual comparison of multimodal instruction-based editing. Compared to other competitive methods and even closed-source commercial models (GPT-4o and Nano Banana), DreamOmni2 shows more accurate editing results and better consistency. Table 2: Quantitative comparison of multimodal instruction-based editing. We use Gemini (Google, 2025a) and Doubao (ByteDance, 2025) to evaluate the success editing ratio of different models on concrete objects and abstract attributions, respectively. In addition, Human refers to professional engineers assessing the editing success rates of all models. Method GPT-4o (OpenAI, 2025) Nano Banana (Google, 2025b) UNO (Wu et al., 2025c) DreamO (Mou et al., 2025) Omnigen2 (Wu et al., 2025b) Qwen-Image-Edit (Wu et al., 2025a) Kontext (Batifol et al., 2025) Qwen-Image-Edit-2509 (Wu et al., 2025a) DreamOmni2 (Ours) Concrete Object Abstract Attribution Gemini Doubao Human Gemini Doubao Human 0.6829 0.6829 0.0000 0.0244 0.2195 0.0976 0.0488 0.2683 0.5854 0.7805 0.7073 0.0244 0.0732 0.2927 0.1463 0.1220 0.2927 0. 0.5610 0.5366 0.0000 0.0000 0.2927 0.0244 0.0976 0.2195 0.6098 0.7195 0.6463 0.0061 0.0183 0.0427 0.0244 0.0183 0.0488 0. 0.7439 0.5488 0.0183 0.0183 0.0793 0.0183 0.0122 0.1159 0.6280 0.5793 0.3293 0.0000 0.0000 0.0305 0.0000 0.0122 0.0427 0. performance in human evaluations. In VLM tests, DreamOmni2 significantly outperformed opensource models and achieved results close to those of commercial models. In fact, GPT-4o and Nano Banana often introduced unintended changes or inconsistencies in the edited attribution, which were not aligned with the reference images. These issues are difficult for VLMs to detect accurately. Additionally, GPT-4o caused the edited images to appear yellowed. 7 Work in progress Figure 5: Visual comparison of multimodal instruction-based generation. Our DreamOmni2 significantly outperforms current open-source models and achieves generation results comparable to closed-source commercial models (GPT-4 and Nano Banana). Qualitative results are shown in Fig. 4, where we present visualizations of editing cases involving various concrete objects and abstract attributes. It is clear that DreamOmni2 produces more accurate edits with better consistency. This further demonstrates the impressive performance of our approach in multimodal instruction-based editing. Evaluation on Multimodal Instruction-based Image Generation. As shown in Tab. 3, our method outperforms the commercial model Nano Banana in both human evaluations and assessments by Doubao 1.6 and Gemini 2.5, achieving results comparable to GPT-4o. Compared to open-source models like DreamO, Omnigen2, and Qwen-Edit-2509, which focus primarily on generating images with multiple concrete objects, DreamOmni2 still significantly outperforms them in both generation accuracy and object consistency, even within their specialized domains. This further underscores the effectiveness of DreamOmni2 in multimodal instruction-based generation. Quantitative results, as shown in Fig. 5, indicate that open-source models struggle with generating abstract attributes. Even in generating concrete objects, which these models are specifically optimized for, DreamOmni2 outperforms them in both instruction adherence and object consistency. Furthermore, DreamOmni2 even outperforms the commercial model Nano Banana. Joint Training. As shown in Tab. 4, we validate the impact of joint training of generation or editing and VLM. Scheme 1 represents the base model, Kontext. In Scheme 2, we train the generation and editing models with basic instructions without introducing VLM. In Scheme 3, we train the VLM with standard descriptive instructions and input the VLM-generated descriptions into Kontext. In 8 Work in progress Table 3: Quantitative comparison of multimodal instruction-based generation. We use Gemini (Google, 2025a) and Doubao (ByteDance, 2025) to evaluate the success editing ratio on concrete objects and abstract attributions, respectively. In addition, Human refers to professional engineers assessing the editing success rates of all models. Method GPT-4o (OpenAI, 2025) Nano Banana (Google, 2025b) UNO (Wu et al., 2025c) DreamO (Mou et al., 2025) Omnigen2 (Wu et al., 2025b) Qwen-Image-Edit (Wu et al., 2025a) Kontext (Batifol et al., 2025) Qwen-Image-Edit-2509 (Wu et al., 2025a) DreamOmni2 (Ours) Concrete Object Abstract Attribution Gemini Doubao Human Gemini Doubao Human 0.6250 0.5000 0.0000 0.0417 0.2083 0.0417 0.2500 0.1250 0.5833 0.6250 0.5417 0.0000 0.0833 0.2500 0.1250 0.3750 0.2917 0. 0.5610 0.5366 0.0000 0.0000 0.2927 0.0244 0.0976 0.2195 0.6098 0.6889 0.5556 0.0333 0.0667 0.1000 0.0889 0.0556 0.1111 0. 0.6333 0.5111 0.0556 0.0222 0.0778 0.1000 0.1222 0.1556 0.6333 0.5793 0.3293 0.0000 0.0000 0.0305 0.0000 0.0122 0.0427 0. Table 4: The validation of joint training for generation or editing models and VLM. Method Scheme 1 Scheme 2 Scheme 3 Scehme 4 (Ours) Generation or Editing Model Training VLM Training Concrete Object Abstract Attribution Concrete Object Abstract Attribution Editing Generation 0.1220 0.3659 0.2439 0.6585 0.0122 0.3171 0.3415 0.6280 0.3750 0.4583 0.5417 0.6667 0.1222 0.3444 0.4778 0. Scheme 4, we perform joint training of the VLM and our generation or editing model on our data. Comparing Scheme 2 with Scheme 1, we see that our data significantly enhances the models ability to handle multimodal instruction-based editing and generation. Comparing Scheme 3 with Scheme 4, we observe that introducing VLM helps the generation and editing models better understand realworld user complex instructions, improving performance. Moreover, our joint training scheme in Scheme 4 outperforms Scheme 2 and Scheme 3, demonstrating its effectiveness. Index and Position Encoding. As shown in Tab. 5, we compare different encoding schemes to help the model adapt to multiple image inputs. Comparing Scheme 3 and Scheme 1, we find that adding index encoding enables the model to understand which image corresponds to references like Image 1, Image 2, and Image 3 in user instructions, resulting in more accurate generation and editing. Additionally, when comparing Scheme 3 and Scheme 4, we observe that with the inclusion of index encoding, multiple images require position encoding shifts instead of using the same position encoding. This adjustment prevents the copy-and-paste effect and improves the models editing and generation performance. Therefore, in DreamOmni2, we incorporate index encoding along with position encoding shifts for multiple reference images. Table 5: The validation of different encoding schemes for multiple image inputs. Method Scehme 1 Scehme 2 Scehme 3 Scheme 4 (Ours) Index Encoding Position Encoding Shift Concrete Object Abstract Attribution Concrete Object Abstract Attribution Editing Generation 0.2439 0.4634 0.3415 0.6585 0.2805 0.5427 0.3902 0. 0.2917 0.5417 0.4167 0.6667 0.2222 0.5111 0.4556 0."
        },
        {
            "title": "5 CONCLUSION",
            "content": "Current instruction-based editing relies on language, but it often struggles to clearly describe desired edits. Therefore, reference images are needed to guide the process. Additionally, subject-driven generation models typically focus on concrete objects and cannot generate images based on abstract concepts. To this end, we propose two new tasks: multimodal instruction-based editing and generation, where references include both concrete objects and abstract attributions. These tasks face two main challenges: training data and the framework supporting multi-image input. For training data, we introduce three-stage data synthesis pipeline. In stage 1, we use feature mixing approach to create data for an extraction model, which can generate images with the same elements (objects or attributes) as the given image. In stage 2, we use the extraction and instruction-based editing models to create multimodal instruction-based editing data. In stage 3, we apply the extraction model 9 Work in progress to stage 2 data to generate multimodal instruction-based generation data. For the framework, we design an index encoding and position encoding shift scheme to help the model distinguish multiple images and avoid the copy-and-paste effect. We also propose joint training scheme for the generation/editing model and the VLM, improving the models ability to understand complex real-world instructions. Extensive experiments show the impressive performance of DreamOmni2."
        },
        {
            "title": "REFERENCES",
            "content": "Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, pp. arXiv2506, 2025. 2, 3, 5, 6, 7, 9 Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. 2, 3, 4 ByteDance. Doubao. https://www.doubao.com/, 2025. 6, 7, 9 Bowen Chen, Mengyi Zhao, Haomiao Sun, Li Chen, Xu Wang, Kang Du, and Xinglong Wu. Xverse: Consistent multi-subject control of identity and semantic attributes via dit modulation. arXiv preprint arXiv:2506.21416, 2025. 2, 4 Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 2, Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 3 Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024. 3 Google. Gemini. https://deepmind.google/models/gemini/, 2025a. 6, 7, 9 Google. Nano banana. https://aistudio.google.com/models/ gemini-2-5-flash-image, 2025b. 2, 6, 7, 9 Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 3 Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong In-context lora for diffusion transformers. arXiv preprint Feng, Yu Liu, and Jingren Zhou. arXiv:2410.23775, 2024. Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. NeurIPS, 2023. 3 Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 2 Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, et al. Dreamo: unified framework for image customization. arXiv preprint arXiv:2504.16915, 2025. 6, 7, 9 OpenAI. Gpt-4o image generation. https://openai.com/index/ introducing-4o-image-generation/, 2025. 2, 6, 7, 9 William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 3 Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, 2022. 3 10 Work in progress Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. 3, 6 Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. arXiv preprint arXiv:2311.10089, 2023. 3 Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024. Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and https://github.com/ Thomas Wolf. Diffusers: State-of-the-art diffusion models. huggingface/diffusers, 2022. 6 Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 6 Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In ICLR, 2024. 3, 4 Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. 2, 3, 6, 7, 9 Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. 6, 7, 9 Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more generalization: Unlocking more controllability by in-context generation. ICCV, 2025c. 2, 4, 5, 7, Bin Xia, Shiyin Wang, Yingfan Tao, Yitong Wang, and Jiaya Jia. Llmga: Multimodal large language model based generation assistant. In ECCV, 2024. 3 Bin Xia, Jiyang Liu, Yuechen Zhang, Bohao Peng, Ruihang Chu, Yitong Wang, Xinglong Wu, Bei Yu, and Jiaya Jia. Dreamve: Unified instruction-based image and video editing. arXiv preprint arXiv:2508.06080, 2025a. 3 Bin Xia, Yuechen Zhang, Jingyao Li, Chengyao Wang, Yitong Wang, Xinglong Wu, Bei Yu, and Jiaya Jia. Dreamomni: Unified image generation and editing. In CVPR, 2025b. 2, 3 Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In CVPR, 2025. 2, Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. 3 Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. NeurIPS, 2024. 3 11 Work in progress"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 DREAMOMNI2 BENCHMARK Our DreamOmni2 benchmark includes 205 multimodal instruction-based editing test cases and 114 instruction-based generation test cases. Visualizations of the editing and generation test cases are shown in Fig. 7 and Fig. 6, respectively. The benchmark covers wide range of test cases, with input reference images ranging from one to five, and encompasses diverse local and global attributes, as well as concrete objects. The DreamOmni2 Benchmark will be released. Figure 6: Examples of multimodal instruction-based generation in DreamOmni2 benchmark. Figure 7: Examples of multimodal instruction-based editing in DreamOmni2 benchmark. A.2 MORE MULTIMODAL INSTRUCTION-BASED EDITING CASES As shown in Fig. 8, Fig. 9, Fig. 10, Fig. 11, Fig. 12, Fig. 13, Fig. 14, Fig. 15, Fig. 16, Fig. 17, Fig. 18, Fig. 19, Fig. 20, and Fig. 21, we present additional visual cases of DreamOmni2 on the multimodal instruction-based editing task. 12 Work in progress A.3 MORE MULTIMODAL INSTRUCTION-BASED GENERATION CASES As shown in Fig. 22, Fig. 23, Fig. 24, Fig. 25, Fig. 26, Fig. 27, Fig. 28, Fig. 29, and Fig. 30, we present additional visual cases of DreamOmni2 on the multimodal instruction-based generation task. 13 Work in progress Figure 8: Multimodal instruction-based editing cases of DreamOmni2. 14 Work in progress Figure 9: Multimodal instruction-based editing cases of DreamOmni2. 15 Work in progress Figure 10: Multimodal instruction-based editing cases of DreamOmni2. 16 Work in progress Figure 11: Multimodal instruction-based editing cases of DreamOmni2. 17 Work in progress Figure 12: Multimodal instruction-based editing cases of DreamOmni2. 18 Work in progress Figure 13: Multimodal instruction-based editing cases of DreamOmni2. 19 Work in progress Figure 14: Multimodal instruction-based editing cases of DreamOmni2. 20 Work in progress Figure 15: Multimodal instruction-based editing cases of DreamOmni2. 21 Work in progress Figure 16: Multimodal instruction-based editing cases of DreamOmni2. 22 Work in progress Figure 17: Multimodal instruction-based editing cases of DreamOmni2. 23 Work in progress Figure 18: Multimodal instruction-based editing cases of DreamOmni2. 24 Work in progress Figure 19: Multimodal instruction-based editing cases of DreamOmni2. 25 Work in progress Figure 20: Multimodal instruction-based editing cases of DreamOmni2. 26 Work in progress Figure 21: Multimodal instruction-based editing cases of DreamOmni2. 27 Work in progress Figure 22: Multimodal instruction-based generation cases of DreamOmni2. 28 Work in progress Figure 23: Multimodal instruction-based generation cases of DreamOmni2. 29 Work in progress Figure 24: Multimodal instruction-based generation cases of DreamOmni2. 30 Work in progress Figure 25: Multimodal instruction-based generation cases of DreamOmni2. 31 Work in progress Figure 26: Multimodal instruction-based generation cases of DreamOmni2. 32 Work in progress Figure 27: Multimodal instruction-based generation cases of DreamOmni2. 33 Work in progress Figure 28: Multimodal instruction-based generation cases of DreamOmni2. 34 Work in progress Figure 29: Multimodal instruction-based generation cases of DreamOmni2. 35 Work in progress Figure 30: Multimodal instruction-based generation cases of DreamOmni2."
        }
    ],
    "affiliations": [
        "ByteDance Inc",
        "CUHK",
        "HKU",
        "HKUST"
    ]
}