{
    "paper_title": "Impossible Videos",
    "authors": [
        "Zechen Bai",
        "Hai Ci",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Synthetic videos nowadays is widely used to complement data scarcity and diversity of real-world videos. Current synthetic datasets primarily replicate real-world scenarios, leaving impossible, counterfactual and anti-reality video concepts underexplored. This work aims to answer two questions: 1) Can today's video generation models effectively follow prompts to create impossible video content? 2) Are today's video understanding models good enough for understanding impossible videos? To this end, we introduce IPV-Bench, a novel benchmark designed to evaluate and foster progress in video understanding and generation. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing 4 domains, 14 categories. It features diverse scenes that defy physical, biological, geographical, or social laws. Based on the taxonomy, a prompt suite is constructed to evaluate video generation models, challenging their prompt following and creativity capabilities. In addition, a video benchmark is curated to assess Video-LLMs on their ability of understanding impossible videos, which particularly requires reasoning on temporal dynamics and world knowledge. Comprehensive evaluations reveal limitations and insights for future directions of video models, paving the way for next-generation video models."
        },
        {
            "title": "Start",
            "content": "Zechen Bai * Hai Ci * Mike Zheng Shou Show Lab, National University of Singapore https://showlab.github.io/Impossible-Videos/ 5 2 0 2 8 ] . [ 1 8 7 3 4 1 . 3 0 5 2 : r Figure 1: Impossible Video Examples with Impossible Type and Explanation. 1 Impossible Videos"
        },
        {
            "title": "Abstract",
            "content": "Synthetic videos nowadays is widely used to complement data scarcity and diversity of realworld videos. Current synthetic datasets primarily replicate real-world scenarios, leaving impossible, counterfactual and anti-reality video concepts underexplored. This work aims to answer two questions: 1) Can todays video generation models effectively follow prompts to create impossible video content? 2) Are todays video understanding models good enough for understanding impossible videos? To this end, we introduce IPV-BENCH, novel benchmark designed to evaluate and foster progress in video understanding and generation. IPV-BENCH is underpinned by comprehensive taxonomy, encompassing 4 domains, 14 categories. It features diverse scenes that defy physical, biological, geographical, or social laws. Based on the taxonomy, prompt suite is constructed to evaluate video generation models, challenging their prompt following and creativity capabilities. In addition, video benchmark is curated to assess Video-LLMs on their ability of understanding impossible videos, which particularly requires reasoning on temporal dynamics and world knowledge. Comprehensive evaluations reveal limitations and insights for future directions of video models, paving the way for next-generation video models. 1. Introduction Video data has been long-standing focus in the research community, offering the potential to capture richer and more structured information compared to text (Yang et al., 2024a; Liu et al., 2024c). Over time, this domain has expanded from early tasks like action recognition (Zhu et al., 2020) to more advanced applications such as video captioning (Bai & An, 2018), question answering, video generation (Ning et al., 2023), and video-based world modeling (Liu et al., 2024c), showcasing its versatility and growing importance in advancing AI capabilities. Although video data seems to be abundant on the Internet, it suffers from issues of low quality and diversity, etc. Recent efforts have been trying to alleviate these issues by generating videos either from neural generation models or simulation engines (Agarwal et al., 2025). However, the primary goal is still to replicate real-world scenes in *Equal contribution . Correspondence to: Mike Zheng Shou <mike.zheng.shou@gmail.com>. 2 controlled way, which severely limits the broader imagination space and applications. In this work, we introduce the concept of impossible videos, which particularly captures counterfactual and anti-reality scenes that are impossible in real world. We argue that impossible videos can form an effective testbed to assess video models. As an out-of-realworld-distribution data, it requires the model to not simply memorize real-world data and retrieve similar information based on the input, but to genuinely learn from real-world data and reason upon the input. Currently, mainstream video models can be categorized into two majoy categories based on tasks: understanding and generation (Xie et al., 2024; Wu et al., 2024). Thus, this work aims to probe the boundary of the two types of models by answering the following questions. First, can todays video generation models effectively follow text prompts to create impossible video content? Recent research focus of video generation has been evolving from fundamental video quality (Ning et al., 2023) (e.g., aesthetic quality, motion smoothness) to advanced semantic quality (e.g., physical laws, subject consistency) (Bansal et al., 2024). State-of-the-art models have been positioned as world simulator (OpenAI, 2025). Ideally, the model should be able to generate either physically coherent or anti-reality videos with detailed control of the text prompt, enabling wider range of applications, such as filming, advertising, etc. Prompting the model to generate impossible videos challenges the model to break the rule yet faithfully following the prompt. Second, are todays video understanding models good enough for understanding impossible videos? Advanced video understanding models, especially video large language models (Video-LLMs), are mostly trained on largescale video datasets, demonstrating remarkable performance on popular benchmarks with real-world videos. Impossible videos pose specific challenges on reasoning temporal dynamics and world knowledge. To achieve these goals, we propose benchmark, IPVBENCH, focusing on ImPossible Videos. To the best of our knowledge, this is the first work focusing on this topic. We start with building comprehensive taxonomy covering diverse aspects of impossible videos, including scenes violating physical laws, biological laws, geographical laws, and social laws. Based on the taxonomy, we construct IPV-TXT, prompt suite that consists of 260 text prompts; IPV-VID, video set with 902 high-quality videos. Both the text prompts and videos are organized following the structure of the taxonomy, describing or displaying impossible scenes with particular consideration on temporal dynamics. Examples of impossible videos are shown in Fig. 1. Based on this benchmark, we conduct comprehensive evaluImpossible Videos Table 1: Comparison of IPV-BENCH and Existing Benchmarks. Benchmark GenVideo (Chen et al., 2024a) GenVidBench (Ni et al., 2025) LOKI (Ye et al., 2024) VBench (Huang et al., 2024) VideoPhy (Bansal et al., 2024) PhyGenBench (Meng et al., 2024a) SEED-Bench (Li et al., 2023) Video-Bench (Ning et al., 2023) MV-Bench (Li et al., 2024) TempCompass (Liu et al., 2024d) IPV-Bench (Ours) AIGC Detection Tasks Video Understanding Video Generation Real World Videos Video Data Generated Videos Impossible Videos Text Prompts Text Data Text Descriptions Impossible Text ations for mainstream video understanding models and generation models, suggesting that most video models fall short on impossible videos. Extensive analysis further reveals several insights of certain limitation and future direction. We will make the data public to inspire future research. In summary, our contributions includes: - To the best of our knowledge, this is the first work to identify and investigate impossible videos, which explores critical yet absent space for advanced video understanding and generation research. To this end, we construct benchmark IPV-BENCH. - IPV-BENCH is underpinned by comprehensive taxonomy. Based on the taxonomy, prompt suite and set of high-quality videos is collected and carefully annotated, supporting downstream evaluations. - Based on IPV-BENCH, we conduct extensive evaluation and analysis on mainstream video understanding and generation models, unveiling current limitations and revealing future directions. 2. Related Work This work evaluates video models across two key domains: video understanding and video generation. Tab. 1 presents comprehensive comparison of IPV-BENCH and existing benchmarks, highlighting their relationships and key differences. We then outline the primary objectives of existing models and benchmarks in each domain. Video Understanding remains fundamental challenge in computer vision, encompassing tasks such as action recognition (Zhu et al., 2020), object localization (Fan et al., 2023; Bai et al., 2025), tracking (Zhao et al., 2023b), temporal grounding (Lin et al., 2023b), captioning (Wang et al., 2020; Bai et al., 2021), and, more recently, AI-Generated video detection (Ye et al., 2024). Video Large Language Models (Video-LLMs) (Tang et al., 2023), powered by Large Language Models (LLMs) (Zhao et al., 2023a), leverage language as universal interface to facilitate wide range of video-related tasks. Most open-source Video-LLMs extend from multimodal large language models (MLLMs) originally designed for images, such as LLaVA (Liu et al., 2024b). Some closed-source models, such as GPT-4o (Hurst et al., 2024), although initially designed for images, can also function as video models by processing multiple frames as input. Popular benchmarks such as VideoMME (Fu et al., 2024), MV-Bench (Li et al., 2024), assess models on range of tasks (e.g., multiple-choice and open-ended questions) and scenarios (e.g., daily life, sports, and films). These datasets primarily consist of publicly available videos sourced from the internet. To our knowledge, no existing benchmark explicitly includes dedicated set of impossible or counterfactual videos for evaluation, which are crucial for assessing model generalization, robustness and reasoning abilities. Video Generation has garnered significant attention in both academia and industry, with text-to-video generation serving as foundational task (Wu et al., 2023). Notable open-source models include Stable Video Diffusion (Blattmann et al., 2023), CogVidX (Yang et al., 2024b), Open-Sora (Zheng et al., 2024a), Show-1 (Zhang et al., 2024), and HunyuanVideo (Kong et al., 2024), among others. Proprietary models include Kling (KLING, 2025), Sora (OpenAI, 2025), and Hailuo (Hailuo, 2025), among others. One of the primary challenges in video generation is achieving high visual quality, including factors such as resolution, realism, and temporal consistency (Huang et al., 2024). To address this, benchmarks such as VBench (Ning et al., 2023) comprehensively evaluate various aspects of visual quality. However, with advancements in video generation models, research focus has shifted towards ensuring semantic coherence, particularly in maintaining adherence to physical laws. Recent benchmarks such as PhyGenBench (Meng et al., 2024a) and VideoPhy (Bansal et al., 2024) have emerged to evaluate models ability to generate physically plausible videos. Beyond physics-constrained generation, an often-overlooked aspect is physics-defying content, or more broadly, the generation of impossible scenes, which plays significant role in creative domains such as film and advertising. Creativity is tangentially related to this challenge; however, it is only briefly considered in some comprehensive benchmarks (Zeng et al., 2024) and has yet to be systematically studied. 3 Impossible Videos Figure 2: Overview of the IPV-BENCH Benchmark. IPV-BENCH is structured with comprehensive taxonomy, enabling the creation of diverse prompt suite (IPV-TXT) and high-quality video dataset (IPV-VID). These components facilitate the evaluation of popular video generation and understanding models. 3. IPV-BENCH We first develop taxonomy that systematically categorizes various types of impossible scenes. This taxonomy serves as the foundation for two critical components of the benchmark: 1) IPV-TXT, suite of high-quality text prompts describing impossible scenes that cannot occur in the real world. 2) IPV-VID, curated collection of high-quality videos depicting impossible scenes, each with corresponding annotations. An overview of the taxonomy and the roles of its components is presented in Fig. 2. 3.1. IPV Taxonomy and Prompt Suite Overview. As illustrated in Fig. 2, the taxonomy is structured around four major categories: Physical Laws, Biological Laws, Geographical Laws, and Social Laws. Each category is divided into multiple subcategories, which are further subdivided, forming detailed hierarchical structure. Building upon this hierarchy, our IPV-TXT benchmark incorporates 260 high-quality text prompts that describe various counterfactual and other unusual scenarios. 1. Physical Laws covers 6 common laws: Mechanics, Thermal, Optics, Fluid, Material Properties, and Conservation Laws. This categorization considers most physical phenomenons. We instantiate text prompts to explicitly describe scene defying specific physical law. For example, person pours milk into glass cup half filled with milk, but the amount of milk in the glass cup does not change at all describes video that violates the law of conservation of mass. 2. Biological Laws categorizes 3 sub-categories: 1) Biological Capability covers scenes that exceeds human or animals capabilities. For example, person flapped his arms like wings and successfully flew into the sky.; 2) Morphology consider impossible body composition. E.g., horse walks on the grassland, gradually growing from four legs to eight legs; and 3) Anthropomorphism includes non-living objects exhibit anthropomorphic behavior, e.g., fried egg with face on it is opening its mouth and speaking something. 3. Geographical Laws considers the impossible phenomenons in natural environment, including Climate and Weather anomalies, and Terrain and Environmental anomalies. For example, mountain flattens into perfect plateau, leaving all its trees and snow intact on the new flat surface. 4. Social Laws includes 3 types of counterfactual phenomenons violating social laws: 1) Commonsense defines unusual scenes that violates our daily routine or customs. For example, The programmer in front of the computer suddenly started eating the keyboard and quickly ate half of it. 2) Magical Effects emphasize creative content and effects that cannot be easily interpreted by certain scientific law. For example, e.g., hand turns on flashlight and shines it on glass cup, and the cup immediately breaks into pieces. 3) Historical anomalies highlight interesting type of scene that displays impossible combinations of human-object or human-human across different historical periods. For example, Albert Einstein and Donald Trump are shaking hands at the White House. Construction. We propose comprehensive framework for developing both the taxonomy and the IPV-TXT benchmark. During the development process, the taxonomy structure and text prompts evolve together, informing and refining each other. Specifically, our methodology consists of the following steps: 1) Initialization. We establish the initial taxonomy structure by reviewing relevant literature (Bansal et al., 2024; Meng et al., 2024a;b; Zheng et al., 2024b) and leveraging large language models (LLMs), such as GPT-4o, for additional insights. Beyond the widely studied domain of physical laws, we expand our taxonomy to include broader categories, incorporating everyday scenarios and intriguing effects that can be applied in downstream applications. 2) Iterative Refinement. We refine the taxonomy and prompts through an iterative process. At each iteration, 4 Impossible Videos Figure 3: Questionnaire used for collecting impossible text prompts for IPV-TXT. we collect set of text prompts to expand the existing taxonomy structure. If certain text prompts do not fit within the structure, this suggests the need for adjustments to the taxonomy, ensuring that both the taxonomy and prompts evolve together. New prompts drive the taxonomy toward greater comprehensiveness, while the updated taxonomy encourages human to explore more intriguing examples. LLM Refinement. We carefully design prompts to guide various LLMs in generating broader range of example scenarios. We encourage the models to generate original and creative examples rather than merely substituting nouns in existing examples. We employ three LLMsnamely GPT4o, Claude-3.5, and Gemini-1.5to maximize diversity. Crowdsourcing Refinement. To further improve the taxonomy, we aim to collect more outlier samples to challenge and refine the current structure until it accommodates the majority of samples. To this end, we employ crowdsourcing to collect data samples from diverse set of contributors. As shown in Fig. 3, we employ questionnaire to collect data from 15 participants from diverse academic and professional backgrounds, including computer science, economics, arts, and education. The participants have acknowledged that the collected data will be used for research purposes. On average, participants required 31 minutes to complete the questionnaire. This time commitment demonstrates the complexity and value of the collected text prompts. Finally, we have collected 150 impossible text prompts, which contribute to refining the taxonomy. It is essential to explicitly specify LLMs are not allowed in the questionnaire to guarantee the authenticity of collected data samples. 3) Quality Control. We conduct thorough review of the prompts across multiple dimensions. Clarity. The text 5 Figure 4: Distribution of the Prompt Suite Across the Taxonomy. prompts should be expressed in clear and comprehensible manner. Complicated or confusing prompts will be manually revised or removed. Accessibility. The described scene should be accessible in daily life. We exclude some obscure or esoteric samples. Relevance. The text prompt should be relevant to its belong category (and sub-category) in the taxonomy. We also merge some similar or duplicated samples at this step. Visualizability. The described scene should be able to be displayed by video. Some non-visual scenes, such as sound-related descriptions, will be excluded. 4) Linguistic Enhancement. We further refine the linguistic clarity and expressiveness of the text prompts to enhance their suitability for video generation models. Specifically, we reference popular prompt rewriting strategies (Kong et al., 2024; Yang et al., 2024b) and design prompt to instruct GPT-4o to enhance the text prompts. Compared to the original descriptions, the rewritten prompts feature relatively longer sequences, greater detail, and more structured format. This methodology yields robust and comprehensive taxonomy, and prompt suite, which can be used for assessing T2V models capability on generating impossible videos, providing valuable tool for advancing this domain. The distribution of the prompt suite is shown in Fig. 4. 3.2. IPV-VID We construct IPV-VID, novel video benchmark designed to assess the capabilities of popular VideoLLMs in reasoning about temporal dynamics and world knowledge using impossible videos. IPV-VID is structured according to the established taxonomy. Given the unique nature of video data, IPV-VID places particular emphasis on temporal anomalies, such as motion and stage changes, which are challenging to identify from individual frames alone. Impossible Videos 3.2.1. VIDEO COLLECTION. T2V Generation. We prompt 10 state-of-the-art T2V models to generate comprehensive set of videos, including open-sourced models (Open-Sora (Zheng et al., 2024a), HunyuanVideo (Kong et al., 2024), CogVidX (Yang et al., 2024b), Mochi 1 (GenmoAI, 2025), LTX (GenmoAI, 2025), and Pyramid-Flow (Jin et al., 2024)) and closedsourced models (Sora (OpenAI, 2025), Kling (KLING, 2025), Luma (LumaLabs, 2025), and Hailuo (Hailuo, 2025)). The IPV-TXT prompt suite is utilized as text prompts in textto-video generation. Using 260 high-quality text prompts from the IPV-TXT suite, we generate total of 2,600 synthetic videos. Web Video Collection. To enhance the scale and diversity of the benchmark, we supplement our dataset with videos collected from the Internet. The primary sources include community websites for commercial video generation models, such as Sora and Hailuo, among others. Additionally, we gather videos from Twitter (X) shared by users, explicitly mentioning that they were generated by specific AI models. During the manual collection process, we adhere to an implicit criterion: videos must exhibit phenomena that are impossible in the real world. From these sources, we collect total of 155 videos. Real-world Videos. To mitigate evaluation bias and ensure balanced distribution, we incorporate real-world videos into the benchmark. These videos are used alongside synthetic videos to evaluate the performance of VideoLLMs. We utilize OpenVid (Nan et al., 2024) as the foundational dataset for real-world videos. Using AI-generated videos as queries, we leverage the CLIP (Radford et al., 2021) model to retrieve content-consistent videos from OpenVid. During retrieval, we apply filtering criteria based on video length, aspect ratio, and aesthetic score to exclude unsuitable videos. Additionally, we exclude videos with cartoon-style visuals, conspicuous logos, subtitles, and similar features. In total, we collect 650 real-world videos for inclusion in the benchmark. Figure 5: Sources of Impossible Videos. 2) Semantic Plausibility. In line with the benchmarks objective, we retain videos with semantically counterfactual contentscenes that are impossible in the real world. This ensures that the selected videos exhibit high visual quality while maintaining low semantic plausibility, thereby requiring models to reason based on semantic content rather than low-level visual features. Fig. 5 illustrates the distribution of retained impossible videos following the filtering process. Step 2: Detailed Annotation. For videos satisfying the aforementioned criteria, we perform detailed annotation with the following labels: 1) Spatial or Temporal Anomaly. This field requires annotators to determine whether the impossibility can be identified through spatial semantic information or necessitates temporal reasoning; 2) Taxonomy Category. Annotators assign category label based on the IPV taxonomy; 3) Explanation. Annotators provide brief textual description of the specific impossible phenomenon depicted in the video. 3.2.2. HUMAN ANNOTATION. 3.2.3. TASK DESIGN. Step 1: Video Filtering. Our objective is to curate collection of high-quality videos that depict impossible or counterfactual scenes. To ensure data quality, we develop custom annotation tool and perform human annotation on the collected videos. detailed description of the annotation tool is provided in the Appendix A.1. The main criteria of IPV-VID contains two aspects: 1) Visual Quality. Videos of low visual quality are excluded, including but not limited to spatial blurring, temporal flickering, poor aesthetic quality, insufficient motion, and inconsistent style. Judgment Task requires models to classify the input video as either synthetic or real by answering the question, Is the provided video generated by AI? To minimize the influence of visual elements and style, ensuring models focus on semantic content, we use synthetic videos without watermarks and exclude real-world videos with cartoon-style visuals, conspicuous logos, subtitles, or similar features. To ensure balanced evaluation, the dataset maintains 1 : 1 ratio of synthetic to real-world videos. This task is framed as binary classification problem and evaluated using average Accuracy and F1-score. Additionally, we report the yes rate in Appendix B.1 to facilitate model diagnosis. 6 Table 2: Evaluation Results of IPV-TXT Across Dimensions. This table compares the performance of state-of-the-art video generation models using the IPV-TXT benchmark as text prompts in the T2V setting. higher score indicates better performance in given dimension. Bold denotes best, underline denotes second. Impossible Videos Model Physical Biological Social Geographical Overall Visual Quality Prompt Visual Following Quality Prompt Visual Following Quality Prompt Visual Following Quality Prompt Visual Following Quality Prompt Following IPV Score Open-source Models LTX (HaCohen et al., 2024) Open-Sora (Zheng et al., 2024a) Pyramid-Flow (Jin et al., 2024) CogVidX-1.5 (Yang et al., 2024b) Mochi 1 (GenmoAI, 2025) HunyuanVid (Kong et al., 2024) Proprietary Models Luma (LumaLabs, 2025) Sora (OpenAI, 2025) Kling (KLING, 2025) Hailuo (Hailuo, 2025) 58.3 63.8 88.3 38.7 68.7 95.1 88.3 98.8 98.8 96.3 14.1 22.1 15.3 29.4 44.2 23.9 11.7 15.3 21.5 30.1 35.3 25.5 92.2 35.3 64.7 88. 90.2 98.0 94.1 94.1 21.6 29.4 21.6 52.9 56.6 35.3 19.6 43.1 33.3 45.1 44.0 20.0 72.0 40.0 80.0 88.0 82.6 100.0 95.7 100.0 16.0 32.0 28.0 56.0 60.0 40. 17.4 30.4 43.5 52.0 57.1 57.1 100.0 81.0 71.4 90.5 85.7 95.2 81.0 90.5 23.8 47.6 52.4 61.9 76.2 42.9 52.4 61.9 42.9 61.9 52.3 51.5 88.5 41.5 69.2 92. 88.0 98.4 96.1 95.8 16.5 26.5 20.8 39.2 50.8 29.2 17.1 26.0 27.5 37.7 10.0 15.8 19.6 16.9 37.3 26.2 14.3 25.2 26.7 36.2 Multi Choice Task (MCQA) task requires models to identify the description that best captures the impossible phenomenon depicted in the video. The question is formulated as follows: Select the best answer to the following multiplechoice question based on the video. To create effective distractors that challenge the model, we carefully design an instructional prompt and leverage the GPT-4o model for distractor generation. The instructional prompt is designed with the following considerations: 1) Ensure all options, including both correct answer and distractors, are similar in length, style, detail degree, and complexity; 2) Ensure distractors also present specific impossible phenomenon; 3) Ensure the impossible phenomenon in distractor shall involve visual elements shown in the given video frame, to avoid the model solve the problem with simple visual element grounding. By incorporating detailed annotations and visual content as references, we mitigate hallucination in GPT-4o, ensuring high-quality distractor generation. qualitative example of distractors is provided in Fig. 7. The instructional prompt is included in Appendix C. The MCQA task is treated as multi-class classification problem and evaluated using mean Accuracy. Open-ended QA Task (OpenQA). We introduce an openended impossible explanation task, which requires models to independently and correctly identify the impossible phenomenon depicted in the video without any hints. The task is framed with the following question: Based on your observation of the video, what content or event makes the video impossible or unusual in the real world?. Compared to the MCQA task, the open-ended explanation task is more challenging, as models must generate responses without reference to candidate options. This task more accurately assesses whether models can genuinely perceive and articulate detailed anomalies, rather than relying on guesswork. For evaluation, we employ an LLM as an evaluator to score model responses by comparing them to the annotated text explanations in the benchmark. Empirically, we observe that directly instructing the LLM to assign scores results in instability. To address this, we propose justification-thenscore approach. In this approach, the evaluator first provides justification by identifying key matches or mismatches between the models prediction and the ground truth. Based on this justification, the evaluator assigns semantic alignment score on scale from 0 to 1, where 1.0 indicates perfect alignment, 0.8-0.9 indicates good alignment, 0.5-0.7 indicates partial alignment, 0.1-0.4 indicates weak alignment, and 0.0 indicates no alignment. The justification step is critical to ensure fair and stable score assignment. By default, we employ GPT-4o as the primary evaluator. To avoid self-evaluation bias, as GPT-4o is also evaluated as VideoLLM, we additionally utilize Claude-3.5-Sonnet as an evaluator. Quality Assessment. Since the task of MCQA and OpenQA is constructed or evaluated with the help of GPT-4o, which risks hallucination, we conduct human assessment. Specifically, for each task, we randomly select subset of 100 samples and ask human to answer the questions, imitating the video understanding models. After that, the accuracy or score serve as golden reference in models evaluation. 4. Evaluate Impossible Video Generation Setup. We evaluate mainstream video generation models, including open-source (Open-Sora 1.2 (Zheng et al., 2024a), HunyuanVideo (Kong et al., 2024), CogVidX (Yang et al., 2024b), Mochi 1 (GenmoAI, 2025), LTX (HaCohen et al., 2024), Pyramid-Flow (Jin et al., 2024)) and closed-source models (Sora (OpenAI, 2025), Kling 1.5 (KLING, 2025), Luma (LumaLabs, 2025), and Hailuo (Hailuo, 2025)). IPV-Score Metric. To evaluate model, we first generate set of videos using the IPV-TXT prompt suite. Human annotators are then employed to label two aspects of each video: 1) Visual quality, assessing whether the video meets high-quality standards, and 2) Impossible prompt following, determining whether the video accurately depicts the impos7 Impossible Videos sible event described in the text prompt. Annotators provide binary labels for each dimension. Using these labels, we introduce the IPV-Score, novel metric to evaluate models ability to generate high-quality videos that faithfully depict impossible events. The IPV-Score is calculated as the percentage of videos that are both high-quality and faithful to the prompt within the entire generated set. Humanand Auto-eval. Tab. 2 presents the performance of the evaluated models. Additionally, we propose an automated evaluation method to assess models capability in generating impossible videos. detailed analysis is provided in the Appendix B.2. 4.1. Results and Analysis Can todays video generation models effectively follow prompts to create impossible video content? As shown in Tab. 2, the top-performing model, Mochi 1, generates highquality impossible videos in only 37.3% of cases. Other models perform even worse, suggesting that current video generation models remain far from achieving satisfactory performance in generating high-quality impossible videos. Unbalanced capability between visual quality and impossible prompt following. For instance, Luma demonstrates remarkable visual quality, surpassing most opensource models, but its prompt-following score is significantly lower. In contrast, some open-source models, such as Mochi 1, exhibit superior prompt-following capabilities, even outperforming many proprietary models. An ideal model should excel in both dimensions, achieving balance that is quantified by our IPV-Score metric. What limits the the model on impossible video generation? Beyond basic prompt comprehension and following, we identify two unique challenges posed by impossible text prompts, as illustrated in Fig. 6. First, impossible prompts may trigger low visual quality. While the model attempts to follow the prompt, the unusual nature of impossible prompts often results in visual artifacts or generation failures. This is likely because impossible prompts represent out-of-distribution data for the model. Second, an overemphasis on adhering to physical laws may constrain the models creative freedom. In many failure cases, videos accurately capture the semantic elements of the prompt but fail to depict the critical impossible phenomenon. Instead, they depict normal scenes that conform to real-world rules. Future video generation models may consider the above factors to achieve better model. Additional examples are included in the Appendix E. 5. Evaluate Impossible Video Understanding We evaluate range of popular Video-LLMs, encompassing both open-source and proprietary models. The three tasks are designed to form hierarchy, increasing in difficulty Figure 6: Failure case of impossible video generation. to comprehensively evaluate model capabilities. Tab. 3 presents the evaluation result of impossible video understanding across models. 5.1. Results Judgment Task. Most models achieve comparable Accuracy and F1-scores on this task. Qwen2-VL achieves the highest accuracy at 76.2%, even outperforming Gemini by 3.1 percentage points. Empirically, we observe that some models exhibit bias, predominantly answering Yes (or No) for the majority of videos. To account for this bias, we also report F1-scores, with Video-LLaVA achieving the highest performance. Detailed results and additional analysis are provided in the Appendix B.1. GPT-4o reject answering Yes or No for this task. To ensure fair comparison, we do not further tune the text prompt for it. Multi Choice Task. Model performance on this task exhibits significant variation. The top-performing model, LLaVA-Next, achieves an accuracy of 86.4%, surpass8 Table 3: Evaluation Results for Impossible Video Understanding. This table compares the performance of sota VideoLLMs using the IPV-Vid benchmark. higher score indicates better performance in given dimension. Bold denotes best, underline denotes second, Open(C) denotes using Claude as the evaluator. Impossible Videos Model Open-source Models Random Human Video-LLaVA (Lin et al., 2023a) Oryx (Liu et al., 2024e) Intern-VL-2.5 (Chen et al., 2024b) NVILA (Liu et al., 2024f) LongVU (Shen et al., 2024) Qwen2-VL (Wang et al., 2024) LLaVA-Next (Liu et al., 2024a) Proprietary Models Judgement Physical Biological Social Geographical Overall Acc. F1 MC Open MC Open MC Open MC Open MC Open Open(C) 50.0 - 72.7 58.6 56.5 72.6 70.3 76.2 73.2 50.0 - 72.9 44.6 69.7 64.0 68.0 71.1 70.4 20.0 - 23.0 52.0 61.9 60.2 69.3 69.1 82.8 - - 13.3 16.7 34.9 26.4 21.8 32.8 37.7 48.5 58.2 20.0 - 34.6 68.8 64.2 63.3 79.6 75.8 92. 90.8 83.8 - - 29.7 33.9 59.6 52.3 38.3 56.3 57.2 66.4 75.5 20.0 - 31.8 70.3 65.5 68.9 77.0 75.0 90.5 89.2 84.5 - - 16.9 32.6 49.6 36.6 35.6 48.3 51. 59.3 64.9 20.0 - 24.1 83.9 77.0 69.0 77.0 75.9 90.8 93.1 92.0 - - 25.9 35.2 48.4 42.9 35.6 53.0 51.4 65.3 71.1 20.0 94.0 26.8 60.4 62.4 62.6 73.3 71.4 86. 84.4 79.7 - 82.7 14.2 18.7 33.0 26.8 21.9 31.7 34.4 42.5 49.1 - - 18.7 22.7 32.5 30.6 25.4 33.7 38.6 36.8 45.1 Gemini-1.5-Flash (Team et al., 2024) GPT-4o (Hurst et al., 2024) 73.1 - 64.0 - 80.5 76.7 Table 4: Video understanding evaluation results on two categories of videos: 1) videos that can be understood through spatial scene understanding and world knowledge, and 2) videos that require temporal reasoning for comprehension. Model Spatial Temporal MC Open MC Open Open-source Models Random Video-LLaVA (Lin et al., 2023a) Oryx (Liu et al., 2024e) Intern-VL-2.5 (Chen et al., 2024b) NVILA (Liu et al., 2024f) LongVU (Shen et al., 2024) Qwen2-VL (Wang et al., 2024) LLaVA-Next (Liu et al., 2024a) Proprietary Models 20.0 34.6 83.2 72.4 75.2 85.7 79.7 95.5 Gemini-1.5-Flash (Team et al., 2024) GPT-4o (Hurst et al., 2024) 93.0 90. - 30.3 41.0 56.6 51.6 42.6 57.6 55.8 67.5 75.2 20.0 23.8 51.4 60.6 57.6 68.5 68.2 82.7 81.1 75.6 - 13.7 17.5 37.2 28.0 22.7 34.4 39.8 50.0 59. scores suggest that most models struggle to comprehend impossible videos, highlighting promising direction for future research. Fig. 8 illustrates an example of the OpenQA task. Comparing the results from the two evaluators, we observe that GPT-4o tends to be stricter, assigning lower scores than Claude-3.5 for most models. However, when evaluating GPT-4os performance on video understanding, the GPT-4o evaluator assigns higher scores than Claude-3.5, suggesting potential self-evaluation bias. Despite this, the discrepancy between evaluators does not alter the conclusion that GPT-4o is the top-performing model for impossible video understanding. 5.2. Analysis Are todays video understanding models good enough for understanding impossible videos? The results from the MCQA and OpenQA tasks collectively provide insights into this question. Overall, proprietary models show promising potential, outperforming most open-source models on this task. However, their ability to independently identify Figure 7: Example of the MCQA task. We highlight the correct option in red and the incorrect option in green. ing both GPT-4o and Gemini. In contrast, Video-LLaVA achieves only 26.8% accuracy, which is close to the random baseline. Most open-source models exhibit substantial room for improvement. Fig. 7 presents an example of the MCQA task, where most models fail to select the correct option. Open-ended QA Task. Among the three tasks, the OpenQA task is the most conceptually straightforward yet practically challenging. The last two column of Tab. 3 indicate that most current models struggle to independently identify counterfactual phenomena in videos. Among open-source models, LLaVA-Next achieves superior scores with both GPT-4o and Claude-3.5 evaluators, aligning with its strong performance on the MCQA task. GPT-4o attains the highest performance with both evaluators, underscoring its robust visual understanding and reasoning capabilities. The overall 9 Impossible Videos ment task appears to be distinct. For instance, Video-LLaVA excels in the Judgment task but underperforms significantly in the other tasks. Discrepancies also exist between MCQA and OpenQA performance. LLaVA-Next shows strong performance on MCQA but underperforms on OpenQA. In contrast, GPT-4o achieves the highest performance on OpenQA but lags slightly behind on MCQA. What makes good model for impossible video understanding? We identify two critical factors: temporal dynamic reasoning and world knowledge reasoning. For instance, in the first example from Fig. 1, the cookie appears plausible in individual frames. However, only by analyzing and reasoning across frames can one detect the self-growing phenomenonan impossible event. Conversely, the last example in Fig. 1 (snowing in Singapore) requires world knowledge for identificationspecifically, the understanding that Singapore is tropical country. Due to space limit, additional case studies are provided in the Appendix F. Challenge and opportunity on temporal reasoning. Of the two key factors, world knowledge is primarily governed by the LLM, whereas temporal reasoning offers greater design flexibility but remains more challenging. Tab. 4 provides separate evaluation of spatialand temporal-focused videos. For all models, scores on temporal-focused videos are consistently lower than those on spatial-focused videos. This clearly demonstrates that temporal dynamic reasoning poses significant challenges for most current models. Video expert models with high frame rates (e.g., LongVU) do not exhibit significant advantage. Interestingly, the top-performing models (e.g., LLaVA-Next and GPT-4o) are all image-based. It is worth noting that GPT-4o is evaluated using only 1 FPS. This observation suggests that more sophisticated temporal modules, rather than simply expanding the context window, may be key to understanding and reasoning about impossible videos. 6. Conclusion In this work, we introduce the concept of impossible videos as novel testbed to challenge and advance video understanding and generation models. Unlike real-world videos, impossible videos present counterfactual and anti-reality scenarios, demanding models to go beyond mere memorization and retrieval, requiring deeper reasoning and generalization. To facilitate research in this direction, we construct IPV-BENCH, comprehensive benchmark comprising well-structured taxonomy, diverse prompt suite (IPVTXT), and high-quality video dataset (IPV-VID). Through extensive evaluations, we demonstrate that current video models struggle with impossible videos, revealing significant gaps in their ability to reason about non-real-world scenarios. Our findings provide valuable insights into the limitations of existing models and highlight promising future research directions. Figure 8: Example of the OpenQA task. We ask state-ofthe-art video understanding models to analyze whether the video is impossible or not. We highlight the correct analysis in red and the incorrect analysis in green. impossible videos remains suboptimal, as evidenced by the OpenQA scores. Most open-source models perform poorly on this task, indicating significant room for improvement. Unbalanced model capabilities across domains and tasks. Tab. 3 reveals that many models exhibit unbalanced capabilities across domains and tasks. Across domains, Physical emerges as the most challenging, with most models achieving the lowest scores in this category. In contrast, the remaining 3 domains exhibit relatively balanced performance across models. We hypothesize that Physical contains more challenging samples that necessitates temporal dynamic reasoning. Across tasks, performance on the MCQA and OpenQA tasks is closely correlated, whereas the Judg10 Impossible Videos"
        },
        {
            "title": "References",
            "content": "Agarwal, N., Ali, A., Bala, M., Balaji, Y., Barker, E., Cai, T., Chattopadhyay, P., Chen, Y., Cui, Y., Ding, Y., et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Bai, S. and An, S. survey on automatic image caption generation. Neurocomputing, 311:291304, 2018. Bai, Z., Nakashima, Y., and Garcia, N. Explain me the painting: Multi-topic knowledgeable art description generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 54225432, 2021. Bai, Z., He, T., Mei, H., Wang, P., Gao, Z., Chen, J., Zhang, Z., and Shou, M. Z. One token to seg them all: Language instructed reasoning segmentation in videos. Advances in Neural Information Processing Systems, 37:68336859, 2025. Bansal, H., Lin, Z., Xie, T., Zong, Z., Yarom, M., Bitton, Y., Jiang, C., Sun, Y., Chang, K.-W., and Grover, A. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.03520, 2024. Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Chen, H., Hong, Y., Huang, Z., Xu, Z., Gu, Z., Li, Y., Lan, J., Zhu, H., Zhang, J., Wang, W., et al. Demamba: Ai-generated video detection on million-scale genvideo benchmark. arXiv preprint arXiv:2405.19707, 2024a. Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Cui, E., Zhu, J., Ye, S., Tian, H., Liu, Z., et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. Fan, K., Bai, Z., Xiao, T., Zietlow, D., Horn, M., Zhao, Z., Simon-Gabriel, C.-J., Shou, M. Z., Locatello, F., Schiele, B., et al. Unsupervised open-vocabulary object localization in videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1374713755, 2023. Fu, C., Dai, Y., Luo, Y., Li, L., Ren, S., Zhang, R., Wang, Z., Zhou, C., Shen, Y., Zhang, M., et al. Video-mme: The first-ever comprehensive evaluation benchmark of arXiv preprint multi-modal llms in video analysis. arXiv:2405.21075, 2024. GenmoAI. Mochi 1: new sota in open-source video generation models. https://www.genmo.ai/blog, 2025. Accessed: 2025-01-28. HaCohen, Y., Chiprut, N., Brazowski, B., Shalem, D., Moshe, D., Richardson, E., Levin, E., Shiran, G., Zabari, N., Gordon, O., et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. Hailuo. Hailuo ai: Transform idea to visual with ai. https: //hailuoai.video/, 2025. Accessed: 2025-01-28. Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Jin, Y., Sun, Z., Li, N., Xu, K., Jiang, H., Zhuang, N., Huang, Q., Song, Y., Mu, Y., and Lin, Z. Pyramidal flow matching for efficient video generative modeling. arXiv preprint arXiv:2410.05954, 2024. KLING. Kling ai: Next-generation ai creative studio. https://klingai.com/, 2025. Accessed: 202501-28. Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. Li, K., Wang, Y., He, Y., Li, Y., Wang, Y., Liu, Y., Wang, Z., Xu, J., Chen, G., Luo, P., et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2219522206, 2024. Lin, B., Ye, Y., Zhu, B., Cui, J., Ning, M., Jin, P., and Yuan, L. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023a. Lin, K. Q., Zhang, P., Chen, J., Pramanick, S., Gao, D., Wang, A. J., Yan, R., and Shou, M. Z. Univtg: Towards unified video-language temporal grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 27942804, 2023b. 11 Impossible Videos Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., and Lee, Y. J. Llava-next: Improved reasoning, ocr, and world knowledge, 2024a. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b. Liu, H., Yan, W., Zaharia, M., and Abbeel, P. World model on million-length video and language with ringattention. arXiv e-prints, pp. arXiv2402, 2024c. Liu, Y., Li, S., Liu, Y., Wang, Y., Ren, S., Li, L., Chen, S., Sun, X., and Hou, L. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024d. Liu, Z., Dong, Y., Liu, Z., Hu, W., Lu, J., and Rao, Y. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024e. Liu, Z., Zhu, L., Shi, B., Zhang, Z., Lou, Y., Yang, S., Xi, H., Cao, S., Gu, Y., Li, D., et al. Nvila: Efficient frontier visual language models. arXiv preprint arXiv:2412.04468, 2024f. LumaLabs. Luma dream machine. https://lumalabs. ai/dream-machine, 2025. Accessed: 2025-01-28. Meng, F., Liao, J., Tan, X., Shao, W., Lu, Q., Zhang, K., Cheng, Y., Li, D., Qiao, Y., and Luo, P. Towards world simulator: Crafting physical commonsensebased benchmark for video generation. arXiv preprint arXiv:2410.05363, 2024a. Meng, F., Shao, W., Luo, L., Wang, Y., Chen, Y., Lu, Q., Yang, Y., Yang, T., Zhang, K., Qiao, Y., et al. Phybench: physical commonsense benchmark for evaluating textarXiv preprint arXiv:2406.11802, to-image models. 2024b. Nan, K., Xie, R., Zhou, P., Fan, T., Yang, Z., Chen, Z., Li, X., Yang, J., and Tai, Y. Openvid-1m: large-scale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371, 2024. Ni, Z., Yan, Q., Huang, M., Yuan, T., Tang, Y., Hu, H., Chen, X., and Wang, Y. Genvidbench: challenging benchmark for detecting ai-generated video. arXiv preprint arXiv:2501.11340, 2025. Ning, M., Zhu, B., Xie, Y., Lin, B., Cui, J., Yuan, L., Chen, D., and Yuan, L. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models. arXiv preprint arXiv:2311.16103, 2023. OpenAI. Sora. https://openai.com/sora/, 2025. Accessed: 2025-01-28. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Shen, X., Xiong, Y., Zhao, C., Wu, L., Chen, J., Zhu, C., Liu, Z., Xiao, F., Varadarajan, B., Bordes, F., et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. Tang, Y., Bi, J., Xu, S., Song, L., Liang, S., Wang, T., Zhang, D., An, J., Lin, J., Zhu, R., et al. Video understanding with large language models: survey. arXiv preprint arXiv:2312.17432, 2023. Team, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Wang, L., Bai, Z., Zhang, Y., and Lu, H. Show, recall, and tell: Image captioning with recall mechanism. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 1217612183, 2020. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. Wu, C., Chen, X., Wu, Z., Ma, Y., Liu, X., Pan, Z., Liu, W., Xie, Z., Yu, X., Ruan, C., et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. Wu, J. Z., Ge, Y., Wang, X., Lei, S. W., Gu, Y., Shi, Y., Hsu, W., Shan, Y., Qie, X., and Shou, M. Z. Tune-avideo: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7623 7633, 2023. Xie, J., Mao, W., Bai, Z., Zhang, D. J., Wang, W., Lin, K. Q., Gu, Y., Chen, Z., Yang, Z., and Shou, M. Z. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Yang, S., Walker, J., Parker-Holder, J., Du, Y., Bruce, J., Barreto, A., Abbeel, P., and Schuurmans, D. Video as the new language for real-world decision making. arXiv preprint arXiv:2402.17139, 2024a. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. Cogvideox: Text-to-video diffusion models with an 12 Impossible Videos expert transformer. arXiv preprint arXiv:2408.06072, 2024b. Ye, J., Zhou, B., Huang, Z., Zhang, J., Bai, T., Kang, H., He, J., Lin, H., Wang, Z., Wu, T., et al. Loki: comprehensive synthetic data detection benchmark using large multimodal models. arXiv preprint arXiv:2410.09732, 2024. Zeng, A., Yang, Y., Chen, W., and Liu, W. The dawn of video generation: Preliminary explorations with sora-like models. arXiv preprint arXiv:2410.05227, 2024. Zhang, D. J., Wu, J. Z., Liu, J.-W., Zhao, R., Ran, L., Gu, Y., Gao, D., and Shou, M. Z. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. International Journal of Computer Vision, pp. 115, 2024. Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. survey of large language models. arXiv preprint arXiv:2303.18223, 2023a. Zhao, Z., Wang, J., Horn, M., Ding, Y., He, T., Bai, Z., Zietlow, D., Simon-Gabriel, C.-J., Shuai, B., Tu, Z., et al. Object-centric multiple object tracking. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1660116611, 2023b. Zheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H., Zhou, Y., Li, T., and You, Y. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024a. Zheng, Z., Yan, X., Chen, Z., Wang, J., Lim, Q. Z. E., Tenenbaum, J. B., and Gan, C. Contphy: Continuum physical concept learning and reasoning from videos. arXiv preprint arXiv:2402.06119, 2024b. Zhu, Y., Li, X., Liu, C., Zolfaghari, M., Xiong, Y., Wu, C., Zhang, Z., Tighe, J., Manmatha, R., and Li, M. comprehensive study of deep video action recognition. arXiv preprint arXiv:2012.06567, 2020. 13 A. Additional Details of Benchmark Construction A.1. Video Annotation Tool Impossible Videos Fig. 9 illustrates screenshot of our data annotation tool. It has been divided into two zones: Display Zone showing the essential information for annotation, including original text prompt, the taxonomy label of the prompt, and the video. Annotation Zone provides several fields for annotations, including data for both IPV-VID curation and labels for video generation models evaluation. This all-in-one evaluation tool greatly eases the annotation efforts, supporting multiple ways for downstream usage. Specifically, the questions for each filed is carefully designed to fit human customs. For example, although we aim to annotate if this video is impossible, we find it is more intuitive to answer if this video is reasonable in practice. Figure 9: Screenshot of the annotation tool. B. Additional Results and Analysis B.1. AI-Generated Video Judgment Tab. 5 presents more detailed results on the task of AI-Generated video judgment, helping understanding unique properties of each model. Video-LLaVA is representative balanced model, with similar accuracy on both fake and real videos, and Yes Rate around 50%. In contrast, Intern-VL, NVILA and Gemini are significantly biased. Intern-VL is biased to answer Yes with high Yes Rate 93.5%. NVILA, and Gemini are biased toward the other direction, preferring answer No with lower Yes Rate. The trends can also be reflected in the huge accuracy difference between fake and real videos. B.2. Evaluating Impossible Video Generation An Automatic Strategy In this section, we introduce an automatic evaluation strategy as surrogate to human score presented in the main text. We first generate collection of videos based on the IPV-Txt prompt suite for each model. Then, we compute the the following metrics to evaluate the visual quality and impossible prompt following capability, respectively. 1) Visual Quality. We use the popular video quality assessment suite, VBench (Huang et al., 2024), to build compound metric that measures overall video quality without considering the prompt. Specifically, we combine the six factorsSubject 14 Table 5: Detailed Evaluation Results for AI-Generated Video Judgment Task. Impossible Videos Model Video-LLaVA (Ning et al., 2023) Oryx (Liu et al., 2024e) Intern-VL-2.5 (Chen et al., 2024b) NVILA (Ni et al., 2025) LongVU (Shen et al., 2024) Qwen2-VL (Wang et al., 2024) LLaVA-Next (Liu et al., 2024a) Gemini-1.5-Flash (Team et al., 2024) Overall Acc. Fake Acc. Real Acc. F1-Score Yes Rate 72.7 58.6 56.5 72.6 70.3 76.2 73.2 73.1 72.9 33.2 99.7 48.4 62.7 58.3 63.3 47.6 72.5 84.3 12.8 97.1 78.0 94.3 83.2 98.8 72.9 44.6 69.7 64.0 68.0 71.1 70.4 64.0 50.3 24.5 93.5 25.8 42.5 32.1 40.2 24. Figure 10: Spearmans correlation coefficient  between automatic evaluation score and human annotation score on 10 different video generation models. Consistency, Background Consistency, Motion Smoothness, Aesthetic Quality, Imaging Quality, and Dynamic Degreefrom VBench to form our final metric. Similar to VBench, we calculate the weighted average of these six factors as the final evaluation score. We tailor the weights of these factors to better suit our video domain, such as reducing the weight of Aesthetic Quality since our impossible videos mostly follow realistic style. The weights we use for each factor are: 2.0, 2.0, 0.2, 0.2, 2.0, 1.0. 2) Impossible Prompt Following. To assess whether the impossible event described in the text prompt is faithfully represented in the generated video. We utilize GPT-4o to provide binary judgment for each video and calculate the following ratio as the final score. To achieve accurate judgment, we propose three-step strategy to break down the task. Specifically, we prompt GPT-4o with the text prompt and frames sampled at 1 FPS. We instruct GPT-4o to: 1) identify and summarize the impossible event in the text prompt, 2) ground the impossible event in the video, and 3) confirm the visibility of all key elements that constitute violation of common sense, reason, and conclude with Yes or No. We also provide two additional chain-of-thought examples as demo cases. Please see Fig. 12 for qualitative examples. We compare the baseline prompting strategy with our prompting strategy on videos generated by Kling, where our approach better aligns with human annotations. See Tab. 6. Table 6: Comparison between our prompt strategy and baseline prompt strategy for impossible prompt following evaluation. Human Alignment GPT-4o GPT-4o + our prompt strategy 0.72 0.80 11.1% 3) IPV-Score. We calculate the IPV-score as the product of the visual quality score and the impossible prompt following score, designed to assess the models ability to generate high-quality, impossible videos. Since both scores are evaluated independently, their product effectively models the joint distribution of these factors. Before performing the multiplication, we further scale the ranges of both to better align them. 15 B.2.1. HUMAN ALIGNMENT Impossible Videos We calculate the Spearmans correlation coefficient () between our automatic evaluation score and human annotation score. Fig. 10 shows the corresponding results. We can see that the Spearmans correlation coefficient between the visual quality score, prompt following score, and IPV-score with human annotations remains above 0.8, demonstrating consistency with human annotators. C. Instructional Prompt of MCQA Task In Fig. 11, we present the complete instructional prompt used in the MCQA task. It has particularly designed rules for constructing distractors that challenge the reasoning capability of video understanding models. D. Qualitative Examples Fig. 13 illustrates more video examples from the IPV-BENCH. E. Case Study of Impossible Video Generation In this section, we present three common failure examples of impossible video generation in Fig. 14. Rows 1-2 show cases with high visual quality but that do not faithfully follow the video prompt; the generated videos adhere to the physical laws of the real world. Rows 3-4 display cases with high visual quality, but they do not faithfully follow the video prompt to generate the designated impossible event, instead introducing other common-sense violations. For example, in the third row, the airplane engine is asymmetrical, and in the fourth row, one corner of the cookie moves independently. Rows 5-8 show videos that follow the video prompt but suffer from poor visual quality, with issues like blurriness or an animated style. F. Case Study of Impossible Video Understanding In this section, we present series of case studies of impossible video understanding in Fig. 15, Fig. 16, Fig. 17, Fig. 18, Fig. 19, Fig. 20. We observe that either strong open-source model, LLaVA-Next (Liu et al., 2024a) or close-source model, GPT-4o (Hurst et al., 2024) suffer from difficulty on understanding impossible phenomenon, particularly on temporal dynamics. Impossible Videos Figure 11: Instructional Prompt of MCQA Task. 17 Impossible Videos Figure 12: CoT examples we use to prompt GPT-4o for impossible prompt following evaluation. Videos may contain impossible events that are outside the scope of the prompt. We ignore such events when evaluating impossible prompt following. Impossible Videos Figure 13: More examples of impossible videos. 19 Impossible Videos Figure 14: Failures in Generating Impossible Videos. Impossible Videos Figure 15: Case study of impossible video understanding. 21 Impossible Videos Figure 16: Case study of impossible video understanding. Impossible Videos Figure 17: Case study of impossible video understanding. 23 Impossible Videos Figure 18: Case study of impossible video understanding. Impossible Videos Figure 19: Case study of impossible video understanding. 25 Impossible Videos Figure 20: Case study of impossible video understanding."
        }
    ],
    "affiliations": [
        "Show Lab, National University of Singapore"
    ]
}