{
    "paper_title": "Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data",
    "authors": [
        "Shuhao Gu",
        "Jialing Zhang",
        "Siyuan Zhou",
        "Kevin Yu",
        "Zhaohu Xing",
        "Liangdong Wang",
        "Zhou Cao",
        "Jintao Jia",
        "Zhuoyi Zhang",
        "Yixuan Wang",
        "Zhenchong Hu",
        "Bo-Wen Zhang",
        "Jijie Li",
        "Dong Liang",
        "Yingli Zhao",
        "Yulong Ao",
        "Yaoqi Liu",
        "Fangxiang Feng",
        "Guang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) have recently made significant progress, but the limited scale and quality of open-source instruction data hinder their performance compared to closed-source models. In this work, we address this limitation by introducing Infinity-MM, a large-scale multimodal instruction dataset with 40 million samples, enhanced through rigorous quality filtering and deduplication. We also propose a synthetic instruction generation method based on open-source VLMs, using detailed image annotations and diverse question generation. Using this data, we trained a 2-billion-parameter VLM, Aquila-VL-2B, achieving state-of-the-art (SOTA) performance for models of similar scale. This demonstrates that expanding instruction data and generating synthetic data can significantly improve the performance of open-source models."
        },
        {
            "title": "Start",
            "content": "Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data Shuhao Gu1*, Jialing Zhang1,2*, Siyuan Zhou1,3*, Kevin Yu1,4*, Zhaohu Xing1,5, Liangdong Wang1, Zhou Cao1, Jintao Jia1,4, Zhuoyi Zhang1,4, Yixuan Wang1,4, Zhenchong Hu1,4, Bo-Wen Zhang1, Jijie Li1, Dong Liang1, Yingli Zhao1, Yulong Ao1, Yaoqi Liu4, Fangxiang Feng3, Guang Liu1 1BAAI, 2BJTU, 3BUPT, 4ICT/CAS, 5HKUST(GZ) 4 2 0 2 4 2 ] . [ 1 8 5 5 8 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) have recently made significant progress, but the limited scale and quality of open-source instruction data hinder their performance compared to closed-source models. In this work, we address this limitation by introducing Infinity-MM, large-scale multimodal instruction dataset with 40 million samples, enhanced through rigorous quality filtering and deduplication. We also propose synthetic instruction generation method based on open-source VLMs, using detailed image annotations and diverse question generation. Using this data, we trained 2-billion-parameter VLM, Aquila-VL-2B, achieving state-of-the-art (SOTA) performance for models of similar scale. This demonstrates that expanding instruction data and generating synthetic data can significantly improve the performance of open-source models."
        },
        {
            "title": "Introduction",
            "content": "Recently, Vision-Language Models (VLMs) (Li et al., 2023; Liu et al., 2024b; Dai et al., 2023; Zhu et al., 2024; Bai et al., 2023b; Wang et al., 2023b; Xiao et al., 2024; OpenAI, 2024; Yao et al., 2024; Wang et al., 2024a; Chen et al., 2024b; Li et al., 2024a) have made significant progresses, drawing increasing attention. With the ongoing advancements in foundational language models, multimodal architectures, multimodal training data, and evaluation benchmarks, the capabilities of multimodal models have greatly improved. Among these developments, the expansion of training data scale, the enhancement of data quality, and the optimization of training strategies have emerged as key factors in boosting model performance (Liu et al., 2024b, 2023a; Tong et al., 2024; Li et al., 2024a,b). Currently, the two primary methods for data acquisition are manual data collection and annotation, as well as using models to synthesize instructions. *Core contributors with equal contributions. Project Lead, liuguang@baai.ac.cn Many works have focused on exploring more effective ways to generate and utilize training data. For instance, Liu et al. (2023a) leverages GPT-4 to generate various types of instructions, including dialogues, detailed descriptions, and complex reasoning, based on textual descriptions of images. Building on this, Li et al. (2024b) further expands the data scale, leading to performance improvements. Tong et al. (2024) enhances model performance by increasing the dataset size and adjusting the data type ratios, while Li et al. (2024a) introduces \"high-quality knowledge learning\" phase to further enrich the models knowledge base. In addition, several works explore using closed-source commercial models to generate synthetic instruction data, such as generating captions with GPT-4o or GPT-4v models (Chen et al., 2023, 2024a) or OCR data (Carter, 2024), as well as conversation data (Wang et al., 2023a). Despite these advancements, existing open-source data and instruction datasets remain insufficient to support models in achieving optimal performance. Models trained solely on open-source data still significantly lag behind SOTA closed-source models or open-source models trained on proprietary data. The limitations in both the quantity and quality of open-source data are key factors constraining model performance. To further enhance the performance of opensource models, this work explores improving model effectiveness by expanding the scale of instruction data and increasing the diversity of instruction types. We have extensively collected existing open-source multimodal instruction data, constructing dataset of approximately 40 million samples, and applied rigorous quality filtering and deduplication processes. The model trained on this dataset demonstrated excellent performance, achieving very high level of accuracy. Building on this, we propose multimodal instruction synthesis method based on open-source VLM models. By providing highly detailed annotations for images and generating diverse questions for each image to ensure comprehensive coverage of the information, we can produce higher-quality instruction data, further improving the models ability to understand and follow instructions. Ultimately, we successfully trained 2-billion-parameter VLM model based on open-source data and synthetic data generated by open-source models, achieving SOTA performance comparable to SOTA opensource models of similar scale. The key contributions of this research include: We collected, organized, and open-sourced large-scale multimodal instruction dataset, Infinity-MM, consisting of tens of millions of samples. Through quality filtering and deduplication, we ensured the datasets high quality and diversity. We proposed synthetic data generation method based on open-source models and labeling system, capable of producing highquality instruction data and effectively expanding the scale of instruction datasets. Based on Infinity-MM, we successfully trained 2-billion-parameter VLM model, Aquila-VL-2B, achieving state-of-the-art performance among models of the same scale."
        },
        {
            "title": "2 Related Work",
            "content": "Vision-Language Model VLMs can be categorized into three types based on their capabilities. The first type focuses on understanding multimodal information, such as videos and images (Radford et al., 2021; Alayrac et al., 2022; Liu et al., 2024b; Li et al., 2023; Diao et al., 2024). These models typically take multimodal data as input and produce natural language output, characterized by their ability to integrate and process information from different modalities in unified manner. The second type emphasizes visual generation, primarily aimed at producing high-resolution images and videos (Shi et al., 2020; Peebles and Xie, 2023; Ramesh et al., 2021; Ding et al., 2021). The third type combines both visual understanding and generation capabilities (Sun et al., 2024b,a; Wang et al., 2024b; Zhou et al., 2024; Xie et al., 2024). In this work, we focus on enhancing the models ability to comprehend multimodal information. Multi-modal Instruction Data Currently, considerable amount of research has explored leveraging closed-source commercial models (mainly the GPT-4 series) to generate synthetic instruction data. The first category of work primarily utilizes GPT-4o or GPT-4v to generate specific types of data, such as captions (Chen et al., 2023, 2024a), OCR (Carter, 2024) and conversations (Wang et al., 2023a). Another category of work attempts to generate more complex dialogue or other types of instructions. For example, Liu et al. (2023a) uses GPT-4 to generate various types of instructions based on textual descriptions of images. Wang et al. (2023a) directly uses GPT-4V to generate instructions from images, though it generates only one instruction per image except for the text description for the image. In this work, we focus on how to leverage open-source models to generate high-quality multimodal instruction data."
        },
        {
            "title": "3 Data",
            "content": "First, we extensively collect existing open-source multimodal datasets and categorize them based on task and quality. Subsequently, we will introduce the process of synthesizing data. Finally, we performed unified deduplication and filtering of all collected data. The next three subsections will each address specific aspects in detail."
        },
        {
            "title": "3.1 Categories of Multimodal Datasets",
            "content": "We systematically gathered available open-source multimodal datasets and categorized them. These datasets were classified into four categories, as outlined in Table 1. Image-Caption Data We collected the ImageCaption dataset generated by Emu2 (Sun et al., 2024a). Caption generation is relatively fundamental task, making it well-suited for the initial training of large multimodal models. General Visual Instruction Data We collected various general task data encompassing OCR, mathematical reasoning, chart comprehension, and other tasks. Training large multimodal models with this data equips them with the fundamental capabilities to tackle multimodal tasks effectively. Selective Visual Instruction Data The sources of this data are Llava-OneVision (Li et al., 2024a), Docmatix (Laurençon et al., 2024) Infinityand the subjective components of Instruct (BAAI, 2024b). Verifications on these data have shown that the quality of this data is superior to that of general task instruction data. Data Category Image-Cpation Data Size 10M General Visual Instruction Data 24.4M Selective Visual Instruction Data 6M GPT4 & Synthetic Data 3M Data Composition Caption Data 10M General Data 7.1M General OCR Data 2.6M Doc/Chart/Screen Data 5.8M Math/Reasoning Data 1.3M Text Instruct Data 7.6M LLaVA-onevision Data 3.5M Infinity-Instruct(subjective part) 1.3M Docmatix Data 1.2M Data Generated by GPT4 1.7M Synthetic Data 0.8M Specific Task Data 0.4M Infinity-Preference Data 0.1M Table 1: The quantity and composition of the training data. GPT4 & Synthetic Data This part mainly includes data generated by GPT-4 and the synthetic instruction data introduced in Section 3.2, along with small amount of data specifically tailored for targeted tasks. Experimental results indicate that training with datasets containing synthetic data can further enhance model performance."
        },
        {
            "title": "3.2 Synthetic Data Generation",
            "content": "In this study, we propose multimodal instruction data synthesis method based on open-sourced VLMs. Our goal is to ensure that the generated instructions are closely aligned with the content of the images, while maintaining diversity in instruction types and ensuring the accuracy of instruction responses. The overall process of the method is shown in Figure 1. The images of the synthetic data are extracted from the instruction dataset synthesized using the GPT-4 series models, which is of high quality. However, due to budget constraints, the scope and quantity of the synthetic data are limited. Therefore, we aim to leverage open-source models to synthesize more high-quality data, combining it with the original data to further enhance model performance."
        },
        {
            "title": "3.2.1\nWe first utilize the RAM++ model (Huang et al.,\n2023) to automatically annotate images by extract-\ning key information such as objects, actions, and\nscenes. These tags form the semantic foundation\nof the images, providing a critical basis for subse-\nquent instruction generation. The RAM++ model\ndemonstrates excellent performance when process-\ning large-scale image datasets, accurately capturing",
            "content": "essential details in multimodal scenes. This lays solid foundation for generating precise and contextually relevant multimodal instructions. To systematize the instruction generation process, we designed three-level instruction tagging system that covers different types of instructions. Following Liu et al. (2023b), the first-level tags of the instruction tagging system are divided into six categories, which are: Coarse Perception Fine-grained Perception (single-instance) Fine-grained Perception (cross-instance) Relation Reasoning Attribute Reasoning Logic Reasoning The middle level further refines task characteristics, while the bottom level provides detailed classification based on specific task requirements. We employed commercial closed-source model to extend and enhance this system, ensuring its comprehensiveness and rationality. The complete tagging system can be found in Appendix D."
        },
        {
            "title": "3.2.2 Question Generation",
            "content": "We randomly selected portion of the open-source data we collected as seed data and annotated both the images and instructions using the method described in the previous section. We then established set of mapping rules by analyzing the correlations Figure 1: Illustration of synthetic data generation method. between image tags and instruction tags. Specifically, we calculated the TF-IDF values for the image tags corresponding to each instruction type and ranked the results. Higher TF-IDF values indicate that images with those tags are more suitable for generating that particular type of instruction. Using these rules, we can automatically determine the appropriate instruction type to generate when processing new images. This approach significantly enhances the alignment between generated instructions and image content. During question generation, we input both the images and the target instruction type into the VLM model, prompting the model to generate questions based on the image. Additionally, we randomly select two examples from the seed data to input into the model alongside the image for reference, enabling few-shot generation. For the questions generated by the VLM, we further input both the image and the question back into the question VLM to evaluate the relevance of the question to the image, filtering out lower-quality questions."
        },
        {
            "title": "3.2.3 Answer Generation",
            "content": "After generating the questions, we proceeded to generate the corresponding instruction answers. The goal at this stage was not only to ensure the accuracy of the generated answers but also to account for the diversity of different instruction types. To achieve this, we introduced various prompts to increase answer diversity. Specifically, we employed three different types of prompts: one instructed the model to provide short answers using single words or phrases; another prompted the model to first generate simple explanation before giving the answer; and the third prompted the model to provide detailed explanation followed by the answer. Figure 2: The distribution of instruction types of the synthetic data. We then input the image, question, and generated answer into the VLM model to filter out instructions and answers that did not align with the image content or task. Finally, we obtained approximately 10M question-answer pairs. To further ensure the quality of the generated data, we input the images, questions, and answers into the Qwen2-VL-2B model to compute the data loss and filtered out about 3M samples. We combined multiple QA pairs corresponding to the same image into multi-turn instruction data, resulting in approximately 800K training instructions. The distribution of instruction types in the final synthetic data is shown in Figure 2."
        },
        {
            "title": "3.3 Data Processing",
            "content": "After collecting all the data, we proceeded with data processing. First, to facilitate large-scale training, we standardized the format of data from various sources. Then, to improve training efficiency and enhance model performance, we conducted series of data-cleaning steps. Specifically, we removed duplicate Image-Text pairs and filtered out Resolution #tokens i Samples D Trainable 1.5B LLM M Batch Size LR Epoch i Stage-1 384 729 10M Projector 4.13M 512 1.00E-03 1 Stage-2 StageStage-4 384{(11),...,(22)} Max 5729 384{(11),...,(33)} Max 6729 384{(11),...,(44)} Max 7729 384{(11),...,(66)} Max 10729 384{(11),...,(66)} Max 10 8.2M 8.2M 8.2M 6M 3M Full Model 1.9B 512 1.00E-05 1 Full Model 1.9B 512 1.00E-05 1 Full Model 1.9B 512 1.00E-05 1 Full Model 1.9B 512 1.00E-05 1 Full Model 1.9B 512 1.00E-05 1 Table 2: Configuration for training Aquila-VL-2B across various stages. images with high similarity based on their pHash values. Additionally, we used Qwen2-VL-2B to calculate the loss for each sample and excluded the top 5% with the highest loss, as high loss in welltrained multimodal models often indicates noisy data or outliers. To prevent data contamination, we also deduplicated the training set against images found in the test set. We have included more detailed data information in Appendix C."
        },
        {
            "title": "4.1 Model Architecture",
            "content": "Aquila-VL builds upon LLaVA-OneVision architecture(Li et al., 2024a), comprising language tower, vision tower, and projector. Language Tower We chose Qwen-2.5 (Bai et al., 2023a) as the language tower for its outstanding performance among open-source models and its availability in various sizes. Vision Tower We utilized SigLIP (Zhai et al., 2023), with approximately 400 million parameters, as the vision tower to extract visual features from input images and videos. Projector We utilized two-layer MLP (Liu et al., 2024b) with GELU (Hendrycks and Gimpel, 2023) activation to project visual features into the word embedding space."
        },
        {
            "title": "4.2 Training Details",
            "content": "We implemented curriculum learning approach to train Aquila-VL-2B in phases following Wang et al. (2024a); Li et al. (2024b). Our training is divided into four stages, progressively increasing the task difficulty, image resolution, and data quality. The training setup is presented in Table 2. the word embedding space. Both the vision tower and language tower are frozen during this phase. Stage 2: We utilized general visual instruction data for further training to equip the model with fundamental capabilities for solving multimodal tasks. The data was divided into three subsets, and during each stage of training, the maximum visual resolution was progressively increased to enhance the models comprehension of visual information. Stage 3: We employed selective visual instruction data for training and further increased the maximum resolution to improve performance. Stage 4: We fine-tuned the model using training data from GPT-4 and synthetic data. Experiments demonstrate that scaling with synthetic data can further enhance model performance. We used the official codes of LLaVAOneVision1. Besides, we also supported the training of Aquila-VL-2B in FlagScale (BAAI, 2024a), which is comprehensive toolkit tailored for large models based on open-source projects. Under identical configurations, end-to-end training with FlagScale achieves 1.7x acceleration compared to DeepSpeed. To efficiently handle large-scale training data without encountering out-of-memory (OOM) errors, we have developed an optimized multi-modal data loader, leveraging Megatron Energon (Nvidia, 2024). This loader processes datasets in pre-formatted structure, prepared offline, to ensure efficient data consumption during training. The model was trained on both Nvidia A100 and chips from Chinese manufacturer. Stage 1: We train the projector using 10M imagecaption data to align the visual feature space with 1https://github.com/LLaVA-VL/LLaVA-NeXT/tree/ main/scripts/train Models Params (B) Average MMBenchV1.1test MMStar MMMUval MathVistatestmini HallusionBench AI2Dtest OCRBench MMVet DeepSeek-VL-1.3B (Lu et al., 2024) MiniMonkey (Huang et al., 2024) MiniCPM-V-2 (Yao et al., 2024) PaliGemma-3B-mix-448 (Beyer* et al., 2024) Phi-3-Vision (Abdin et al., 2024) InternVL2-2B (Chen et al., 2024b) H20VL-Mississippi-2B (Galib et al., 2024) XinYuan-VL-2B (Cylingo, 2024) Qwen2-VL-2B (Wang et al., 2024a) Aquila-VL-2B 2.0 2.2 2.8 2.9 4.2 2.1 2.1 2.1 2.1 2.1 39.6 52.7 47.9 46.5 53.6 53.9 54.4 56.1 57.2 59.5 63.8 68.9 65.8 65.6 65.2 69.6 64.8 75.4 72.7 75.2 39.9 48.1 39.1 48.3 47.7 49.8 49.6 51.9 47.8 54.9 33.8 35.7 38.2 34.9 46.1 36.3 35.2 43.6 41.7 47.4 29.8 45.3 39.8 28.7 44.6 46.0 56.8 47.1 47.9 59. 27.6 30.9 36.1 32.2 39.0 38.0 36.4 36.0 41.5 43.0 51.5 73.7 62.9 68.3 78.4 74.1 69.9 74.2 74.6 75.0 413 794 605 614 637 781 782 782 810 772 29.2 39.8 41.0 33.1 44.1 39.7 44.7 42.7 50.7 44.3 Table 3: Performance comparison between Aquila-VL-2B and other models. The results are cited from the official leaderboad of VLMEvalKit and Galib et al. (2024). Models Average MMBenchV1.1test MMStar MMMUval MathVistatestmini HallusionBench AI2Dtest OCRBench MMVet Aquila-VL-2B w/o Synthetic Data 59.5 57.1 75.2 71.2 54.9 53. 47.4 44.2 59.0 58.4 43.0 37.9 75.0 74.3 772 773 44.3 40. Table 4: Comparison of the impact on model performance with and without synthetic data."
        },
        {
            "title": "5 Evaluation",
            "content": "In this section, we first evaluate the performance of the model through comparative analysis of multiple benchmarks, demonstrating the advantages of our approach. Subsequently, we conduct detailed examination of the models specific capabilities, including general visual perception, document understanding and mathematical reasoning. Finally, we carry out an ablation study to investigate several key components of our approach."
        },
        {
            "title": "5.1 Compare to SOTAs",
            "content": "We assessed the visual capabilities of Aquila-VL2B using range of visual benchmarks provided by the VLMEvalKit (Duan et al., 2024). Aquila-VL2B demonstrates highly competitive performance at the same scale, achieving new state-of-the-art results. Specifically, we evaluated the capabilities of Aquila-VL-2B across three task categories. General Visual Question Answering We conducted extensive evaluations across diverse array of general visual question answering benchmarks: MMStar, HallusionBench, MMVet (Yu et al., 2023), and MMBench-1.1 (Liu et al., 2023b). Aquila-VL-2B demonstrated strong performance across these benchmarks, achieving or surpassing state-of-the-art results in most cases at the same scale. On MMStar, which evaluates multimodal capabilities by integrating visual and textual information, Aquila-VL-2B achieved remarkable score of 54.9, surpassing previous state-of-the-art results and demonstrating its strong proficiency in handling diverse multimodal tasks. On HallusionBench, which evaluates image-context reasoning, Aquila-VL-2B achieved score of 43.0, surpassing both the previous state-of-the-art and strong baselines, demonstrating its superior ability in understanding and reasoning within complex visual contexts. On MMVet, which evaluates large multimodal models for integrated vision-language capabilities across 16 complex multimodal tasks, Aquila-VL-2B achieved score of 44.3, demonstrating its ability to address diverse multimodal challenges. On MMBench, which evaluates finegrained abilities across 20 dimensions, Aquila-VL2B exhibited strong performance, achieving score of 76.3 on the English test set, matching the stateof-the-art, and 74.1 on the Chinese test set, demonstrating its robust capabilities in this benchmark. Knowledge and Mathematical Reasoning We conducted experiments on the AI2D (Kembhavi et al., 2016), MMMU (Yue et al., 2023), and MathVista datasets to evaluate the models capabilities in knowledge and mathematical reasoning. The MMMU dataset is new benchmark designed to assess multimodal models on extensive multidisciplinary tasks that require college-level subject knowledge and deliberate reasoning. AquilaVL-2B achieved strong score of 47.4, surpassing state-of-the-art results at the same scale and demonstrating its proficiency in addressing complex multimodal challenges. MathVista is comprehensive benchmark comprising 6,141 diverse examples of mathematical and visual tasks. The Aquila-VL-2B series exhibited exceptional performance on the MathVista benchmark, achieving score of 59.0, thereby outperforming other large vision language models (LVLMs). AI2D focuses on multiple-choice questions related to scientific diagrams containing text. Aquila-VL-2B exhibited outstanding performance at comparable scale, achieving score of 75.0, which represents the performance."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, to enhance the performance of opensource models, we built the Infinity-MM multimodal instruction dataset with tens of millions of samples, increasing the data volume to improve model efficacy. Besides, we proposed method for synthesizing instruction data based on open-source models, which further generated high-quality instruction data and expanded the dataset size. Ultimately, we trained the Aquila-VL-2B model using Infinity-MM, achieving state-of-the-art performance for models of comparable size."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio César Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. 2024. Phi-3 technical report: highly capable language model locally on your phone. Mayank Agrawal, Anand Singh Jalal, and Himanshu Sharma. 2024. Enhancing scene-text visual quesFigure 3: The change of model performance with training data size. highest performance in this benchmark, highlighting its competitive strengths. Text Reading We assessed Aquila-VL-2Bs capabilities in text reading and diagram comprehension using the OCRBench dataset. OCRBench is mixed-task dataset that emphasizes mathematical formula parsing and information extraction, in addition to text-based visual question answering (VQA). The performance results highlight potential areas for further optimization."
        },
        {
            "title": "5.2 Effects of Sythesis Data",
            "content": "To assess the impact of synthetic data on model performance, we conducted an ablation study. In this experiment, we removed all synthetic data and trained the model using only the original GPTgenerated data. The results, as shown in Table 4, revealed significant decline in overall model performance after removing the synthetic data. This demonstrates that the synthetic data played crucial role in enhancing the models performance, further validating the effectiveness of our approach in data augmentation and diversity."
        },
        {
            "title": "5.3 Data Scaling",
            "content": "To further analyze the impact of data size scaling on model performance, we conducted detailed study on how model performance varies with the amount of training data. The results, shown in Figure 3, indicate consistent improvement in performance as the training data increases. This trend clearly demonstrates that expanding the scale of instruction data has significant positive effect on model performance. This observation suggests that as more diverse instruction data is introduced, the models ability to handle complex tasks is enhanced. Therefore, scaling up the instruction data is an effective strategy for improving overall model tion answering with relational reasoning, attention and dynamic vocabulary integration. Comput. Intell., 40(1). Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. 2022. Flamingo: visual language model In Advances in Neural Infor few-shot learning. formation Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. BAAI. 2024a. Flagscale. BAAI. 2024b. arXiv:2406. Infinity instruct. arXiv preprint Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023a. Qwen technical report. arXiv preprint arXiv:2309.16609. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023b. Qwen-vl: frontier large vision-language model with versatile abilities. CoRR, abs/2308.12966. Lucas Beyer*, Andreas Steiner*, André Susano Pinto*, Alexander Kolesnikov*, Xiao Wang*, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bošnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai*. 2024. PaliGemma: versatile 3B VLM for transfer. arXiv preprint arXiv:2407.07726. Jimmy Carter. 2024. Textocr-gpt4v. https: //huggingface.co/datasets/jimmycarter/ textocr-gpt4v. Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. 2024a. Allava: Harnessing gpt4v-synthesized data for lite vision-language model. Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric P. Xing, and Liang Lin. 2021. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. In Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pages 513523. Association for Computational Linguistics. Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2023. Sharegpt4v: Improving large multimodal models with better captions. arXiv preprint arXiv:2311.12793. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. 2024b. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821. Cylingo. 2024. Xinyuan-vl-2b. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. 2023. Instructblip: Towards general-purpose visionIn Adlanguage models with instruction tuning. vances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José M. F. Moura, Devi Parikh, and Dhruv Batra. 2017. Visual dialog. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 10801089. IEEE Computer Society. Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, and Xinlong Wang. 2024. Unveiling encoder-free vision-language models. CoRR, abs/2406.11832. Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. 2021. Cogview: Mastering text-to-image generation via transformers. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 1982219835. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. 2024. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. Shaikat Galib, Shanshan Wang, Guanshuo Xu, Pascal Pfeiffer, Ryan Chesler, Mark Landry, and Sri Satish Ambati. 2024. H2ovl-mississippi vision language models technical report. Agrim Gupta, Piotr Dollár, and Ross B. Girshick. 2019. LVIS: dataset for large vocabulary instance segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 53565364. Computer Vision Foundation / IEEE. William Havard, Laurent Besacier, and Olivier Rosec. 2017. SPEECH-COCO: 600k visually grounded spoken captions aligned to MSCOCO data set. CoRR, abs/1707.08435. Dan Hendrycks and Kevin Gimpel. 2023. Gaussian error linear units (gelus). Mingxin Huang, Yuliang Liu, Dingkang Liang, Lianwen Jin, and Xiang Bai. 2024. Mini-monkey: Multiscale adaptive cropping for multimodal large language models. arXiv preprint arXiv:2408.02034. Xinyu Huang, Yi-Jie Huang, Youcai Zhang, Weiwei Tian, Rui Feng, Yuejie Zhang, Yanchun Xie, Yaqian Li, and Lei Zhang. 2023. Open-set image tagging with multi-grained text supervision. arXiv e-prints, pages arXiv2310. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Min Joon Seo, Hannaneh Hajishirzi, and Ali Farhadi. 2016. diagram is worth dozen images. In ECCV (4), volume 9908 of Lecture Notes in Computer Science, pages 235251. Springer. Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. 2024. Building and better understanding vision-language models: insights and future directions. CoRR, abs/2408.12637. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024a. Llava-onevision: Easy visual task transfer. CoRR, abs/2408.03326. Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. 2024b. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895. Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1973019742. PMLR. Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. 2024a. MMC: advancing multimodal chart understanding with large-scale instruction tuning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 12871310. Association for Computational Linguistics. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024b. Improved baselines with visual instruction tuning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 2628626296. IEEE. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a. Visual instruction tuning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. 2023b. Mmbench: Is your multi-modal model an all-around player? CoRR, abs/2307.06281. Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. 2024. Deepseek-vl: Towards real-world vision-language understanding. Nvidia. 2024. Megatron-energon. OpenAI. 2024. Gpt-4v system card. William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 41724182. IEEE. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 87488763. PMLR. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image genIn Proceedings of the 38th International eration. Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 88218831. PMLR. Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. 2024. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. CoRR, abs/2406.17294. Zhan Shi, Xu Zhou, Xipeng Qiu, and Xiaodan Zhu. 2020. Improving image captioning with better use of caption. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 74547464, Online. Association for Computational Linguistics. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. 2024a. Generative multimodal models are in-context learners. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1439814409. IEEE. Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. 2024b. Emu: Generative pretraining in multimodalIn The Twelfth International Conference on ity. Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. 2024. Cambrian-1: fully open, vision-centric exploration of multimodal llms. CoRR, abs/2406.16860. Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. 2023a. To see is to believe: Prompting GPT-4V for better visual instruction tuning. CoRR, abs/2311.07574. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024a. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. CoRR, abs/2409.12191. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. 2023b. Cogvlm: Visual expert for pretrained language models. CoRR, abs/2311.03079. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. 2024b. Emu3: Nexttoken prediction is all you need. Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. 2024. Florence-2: Advancing unified representation for variety of vision tasks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 48184829. IEEE. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. 2024. Show-o: One single transformer to unify multimodal understanding and generation. CoRR, abs/2408.12528. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800. Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. 2024. mplug-owi2: Revolutionizing multi-modal large language model with modality collaboration. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1304013051. IEEE. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023. Mm-vet: Evaluating large multimodal models for integrated capabilities. CoRR, abs/2308.02490. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2023. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. CoRR, abs/2311.16502. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. 2024. Transfusion: Predict the next token and diffuse images with one multi-modal model. CoRR, abs/2408.11039. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2024. Minigpt-4: Enhancing vision-language understanding with advanced large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net."
        },
        {
            "title": "A Comprehensive Benchmark\nComparisons to SOTAs",
            "content": "In Table 5, we provide more comprehensive assessment of our models performance through extensive comparisons across multiple benchmarks against SOTA models. These benchmarks encompass diverse range of tasks, including visual recognition, reasoning, and document understanding, offering robust evaluation of the models capabilities."
        },
        {
            "title": "B Video Understanding",
            "content": "To enhance Aquila-VL-2Bs ability to process multi-image and video data, we extracted total of 937K multi-image and video samples from the LLaVA-OneVision dataset, and combined them with 1M single-image samples drawn from Stage 4 for further training. The results, as shown in Table 6, demonstrate that even prior to incorporating the multi-image and video data, our model already exhibited solid ability to handle video imagery with satisfactory performance. After introducing the additional multi-image and video data for further training, the models capacity to process such data was significantly improved."
        },
        {
            "title": "C Data composition",
            "content": "We listed the sources, sizes, and types of all our data in Table 7."
        },
        {
            "title": "D Instruction Label System",
            "content": "D.1 Coarse Perception Image Scene Identify people Other scene descriptions Identify background Identify diagram Identify action Identify season Identify vegetation type Other Identify objects in scene Identify overall theme Identify natural elements Identify objects Identify time of day Identify activities Identify number of people Identify environment type Count people Identify main subject Identify clothing Identify geometric properties Identify vegetation presence Identify animals Identify furniture Describe background Identify key elements Identify transportation Identify background details Identify presence of objects Identify natural environment scenery Other image scenes Identify stage Identify indoor scene Other image scene descriptions Identify temperature state Identify presence Describe scene Identify structures Identify geographic location Identify weather condition Identify presence of people Identify event type Identify activity Identify location Identify time Identify buildings Image Quality Assess color and balance Assess focus Other image quality assessments Identify quality issues Assess brightness/ contrast Assess color Assess overall quality Assess lighting Capability Benchmark MiniCPM-V-2 InternVL2-2B XinYuan-VL-2B Qwen2-VL-2B-Instruct Aquila-VL-2B GeneralVQA Knowledge& Mathematical Text-rich MMBench-ENtest MMBench-CNtest MMBench_V1.1test MMT-Benchtest RealWorldQA HallusionBench SEEDBench2Plus LLaVABench MMStar POPE MMVet MMMUval ScienceQAtest AI2Dtest MathVistatestmini MathVersetestmini MathVision DocVQAtest InfoVQAtest ChartQAtest TextVQAval OCRVQAtestcore VCRen easy OCRBench Avg Score 69.4 65.9 65.2 54.5 55.4 36.8 51.8 66.1 41.6 86.6 44. 39.6 80.4 64.8 39.0 19.8 15.4 71.0 40.0 59.6 74.3 54.4 27.6 613 53.5 73.4 70.9 69.7 53.3 57.3 38.1 60.0 64.8 50.2 85.3 41.1 34.9 94.1 74.4 45.0 24.7 12.6 86.9 59.5 71.4 73.5 40.2 51.6 58.8 78.9 76.1 75.4 57.2 63.9 36.0 63.0 42.4 51.9 89.4 42.7 43.6 86.6 74.2 47.1 22.2 16.3 87.6 59.1 57.1 77.6 67.6 67.7 782 60.9 74.9 73.9 72.7 54.8 62.6 41.5 62.4 52.5 47.8 88.0 50. 41.9 78.1 74.6 47.9 21.0 17.5 89.9 65.4 73.5 79.9 68.7 68.3 810 62.1 78.8 76.4 75.2 58.2 63.9 43.0 63.0 68.4 54.9 83.6 44.3 47.4 95.2 75.0 59.0 26.2 18.4 85.0 58.3 76.5 76.4 64.0 70.0 64.1 Table 5: Comprehensive Benchmark Comparisons of Aquila-VL-2B Model and State-of-the-art. Benchmark MiniCPM-V-2 InternVL2-2B Qwen2-VL-2B-Instruct Aquila-VL-2B Aquila-VL-2B-video Video-MME(w/o subs) 38.6 45.9 55.6 48.4 51.5 Table 6: Performance of Aquila-VL-2B and other models on video benchmarks. Data Source Emu2 (Sun et al., 2024b) LVIS-Instruct(Gupta et al., 2019) LLaVA-CC3M-Pretrain-595K(Li et al., 2024b) Visdial(Das et al., 2017) Sharegpt4(Chen et al., 2023) STVQA(Agrawal et al., 2024) MMC-INST(Liu et al., 2024a) MathV360K(Shi et al., 2024) MMC-Alignment(Liu et al., 2024a) DocReason(Ye et al., 2024) ALLaVA(Chen et al., 2024a) Cocotext(Havard et al., 2017) Docvqa(Ye et al., 2024) Geoqa+(Chen et al., 2021) DocDownstream(Ye et al., 2024) Cambrian (Tong et al., 2024) DocStruct4M(Ye et al., 2024) LLaVA-onevision (Li et al., 2024a) Docmatix(Laurençon et al., 2024) Infinity-Instruct (BAAI, 2024b)"
        },
        {
            "title": "Our Synthetic Data",
            "content": "Size 10M 223K 595K 116K 3.2M 43K 500K 338K 250K 26K 1.7M 163K 16K 72K 700K 8.3M 4M 4M 1.2M 7M 0.8M Type Caption General General General General General Doc/Chart/Screen Math/Reasoning Doc/Chart/Screen Doc/Chart/Screen General General Doc/Chart/Screen Math/Reasoning Doc/Chart/Screen General, General OCR, Math/Reasoning Doc/Chart/Screen, Text Instruct General OCR, Doc/Chart/Screen General, General OCR, Math/Reasoning Doc/Chart/Screen, Text Instruct Doc VQA Text Instruct Fine-grained Perception(single-instance) Attribute Reasoning Fine-grained Perception(Cross-instance) Relation Reasoning Coarse Perception, Logic Reasoning Table 7: Data Source, Size and Type of Training Data Assess overall clarity Assess composition Assess clarity Detect noise Assess sharpness Image Topic Identify food Identify book-related content Identify animals Identify medical condition Identify geometric properties Identify people Identify portrait Identify objects Identify main subject Other image topics Identify text Identify event Identify diagram content Identify book Identify color Identify content Identify caption Identify infographic/ cartoon style Identify life cycle stage Identify chart content Identify image content Identify book content Identify vehicles Describe image Identify plant Identify sports Image Emotion Detect overall emotion Other image emotion Read emotions from faces Image Style Other image styles Identify image category D.2 Fine-grained Perception (single-instance) Object Localization Locate object Determine coordinates Count objects Identify specific object Describe region Detect presence Provide bounding box Determine orientation Provide bounding box coordinates Count people Other object localization tasks Provide descriptions Count animals Provide region description Provide short description Identify region Other localization tasks Attribute Recognition Recognize texture Recognize material Recognize pattern Recognize clothing Recognize geometric properties Recognize object presence Recognize appearance characteristics Recognize size Recognize objects Recognize color Other attributes Recognize formulas/tables/charts Recognize orientation Recognize shape Recognize category Count objects OCR Recognize printed text Recognize text Transcribe text from image Extract text from image Recognize text in images Transcribe text in image Extract text from images Recognize formulas/ tables/ charts Key Information Extraction Transcribe text Other OCR tasks Identify specific object direct Detect presence direct D.3 Fine-grained Perception (cross-instance) Spatial Relationship Determine relative position Determine spatial arrangement Other spatial relationships Determine coordinates Count objects Action Recognition Recognize actions in video and text Recognize sequence of actions Recognize human-human interactions Recognize human actions Recognize animal actions Recognize human-object interactions Recognize actions Attribute Comparison Compare text Other attribute comparison Compare preferences Compare ages Compare materials Compare values Compare material Compare shapes Compare shapes/ colors/ textures/ sizes Compare quantities Compare sizes Compare colors Determine relative position direct D.4 Relation Reasoning Social Relation Other social relations Identify family/ friendship/ professional/ hostile relationships Physical Relation Identify spatial/ mechanical/ cause-effect relationships Identify cause-effect relationships Other physical relations Nature Relation Other nature relations D.5 Attribute Reasoning Identity Reasoning Other identity reasoning Predict occupation/ role/ social status Function Reasoning Predict function of objects Other function reasoning New tag Physical Property Reasoning Other physical properties Recognize geometric properties Other physical property reasoning Attribute Reasoning Recognize formulas/ tables/ charts D.6 Logic Reasoning Structuralized Image-Text Understanding Parse tables Other image-text understanding Parse geometric diagrams Other Structuralized Image-Text Understanding Parse sales data Parse bar charts Parse line charts Parse text Other charts Parse other charts Parse diagrams Parse mathematical problem Parse bar/ pie/ line charts Parse formulas Parse function plots Parse charts Parse word problems Future Prediction Predict trend/ social interaction/ physical movement/environmental changes Other future predictions Predict action sequence Action Prediction Predict series of actions"
        }
    ],
    "affiliations": [
        "BAAI",
        "BJTU",
        "BUPT",
        "HKUST(GZ)",
        "ICT/CAS"
    ]
}