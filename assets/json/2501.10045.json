{
    "paper_title": "HiFi-SR: A Unified Generative Transformer-Convolutional Adversarial Network for High-Fidelity Speech Super-Resolution",
    "authors": [
        "Shengkui Zhao",
        "Kun Zhou",
        "Zexu Pan",
        "Yukun Ma",
        "Chong Zhang",
        "Bin Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The application of generative adversarial networks (GANs) has recently advanced speech super-resolution (SR) based on intermediate representations like mel-spectrograms. However, existing SR methods that typically rely on independently trained and concatenated networks may lead to inconsistent representations and poor speech quality, especially in out-of-domain scenarios. In this work, we propose HiFi-SR, a unified network that leverages end-to-end adversarial training to achieve high-fidelity speech super-resolution. Our model features a unified transformer-convolutional generator designed to seamlessly handle both the prediction of latent representations and their conversion into time-domain waveforms. The transformer network serves as a powerful encoder, converting low-resolution mel-spectrograms into latent space representations, while the convolutional network upscales these representations into high-resolution waveforms. To enhance high-frequency fidelity, we incorporate a multi-band, multi-scale time-frequency discriminator, along with a multi-scale mel-reconstruction loss in the adversarial training process. HiFi-SR is versatile, capable of upscaling any input speech signal between 4 kHz and 32 kHz to a 48 kHz sampling rate. Experimental results demonstrate that HiFi-SR significantly outperforms existing speech SR methods across both objective metrics and ABX preference tests, for both in-domain and out-of-domain scenarios (https://github.com/modelscope/ClearerVoice-Studio)."
        },
        {
            "title": "Start",
            "content": "HIFI-SR: UNIFIED GENERATIVE TRANSFORMER-CONVOLUTIONAL ADVERSARIAL NETWORK FOR HIGH-FIDELITY SPEECH SUPER-RESOLUTION"
        },
        {
            "title": "Bin Ma",
            "content": "Tongyi Lab, Alibaba Group, Singapore {shengkui.zhao, b.ma}@alibaba-inc.com 5 2 0 2 7 1 ] . [ 1 5 4 0 0 1 . 1 0 5 2 : r ABSTRACT The application of generative adversarial networks (GANs) has recently advanced speech super-resolution (SR) based on intermediate representations like mel-spectrograms. However, existing SR methods that typically rely on independently trained and concatenated networks may lead to inconsistent representations and poor speech quality, especially in out-of-domain scenarios. In this work, we propose HiFi-SR, unified network that leverages end-to-end adversarial training to achieve high-fidelity speech super-resolution. Our model features unified transformer-convolutional generator designed to seamlessly handle both the prediction of latent representations and their conversion into time-domain waveforms. The transformer network serves as powerful encoder, converting lowresolution mel-spectrograms into latent space representations, while the convolutional network upscales these representations into highresolution waveforms. To enhance high-frequency fidelity, we incorporate multi-band, multi-scale time-frequency discriminator, along with multi-scale mel-reconstruction loss in the adversarial training process. HiFi-SR is versatile, capable of upscaling any input speech signal between 4 kHz and 32 kHz to 48 kHz sampling rate. Experimental results demonstrate that HiFi-SR significantly outperforms existing speech SR methods across both objective metrics and ABX preference tests, for both in-domain and out-of-domain scenarios. Index Terms speech super-resolution, generative adversarial networks, transformer, neural vocoder 1. INTRODUCTION Speech super-resolution (SR) aims to reconstruct high-resolution speech signal from low-resolution input that retains only portion of the original samples. Also referred to as bandwidth extension, this process enriches low-frequency content with high-frequency details. High-resolution speech signals, such as those at 48 kHz, not only deliver superior listening experience but also improve speech intelligibility. Consequently, SR is crucial technique for enhancing the quality of low-resolution speech, with applications in speech quality enhancement [1], historical recording restoration [2], and text-tospeech synthesis [3]. Speech SR is particularly challenging due to the need to manage the high temporal resolution of speech signals, which contain structural patterns across various time scales with both shortand long-term dependencies. Early research in this field primarily relied on statistical methods, leading to slow progress [47]. Recently, learning-based approaches using deep neural networks (DNNs) have shown promising advancements. Most learning-based methods focused on non-generative networks with target resolution of 16 kHz [812]. For example, AECNN [12] utilized an autoencoder for waveform-to-waveform mapping, while TFNet [9] employed dual-branch convolutional neural networks (CNNs) that perform mapping in both time and frequency domains. More recent studies have successfully adopted generative models to achieve higher target resolutions of 48 kHz [1317]. NU-WAV [13] utilizes diffusion probabilistic model to generate high-resolution waveforms from low-resolution inputs. WSRGlow [14] employs glow-based generative model to generate high-resolution samples conditioned on low-resolution inputs. While both NU-WAV and WSRGlow have succeeded in achieving 48 kHz super-resolution, they are constrained by their ability to train on only one fixed input sampling rate at time. Additionally, their performance falls short compared to the latest models NVSR [16] and AudioSR [17], which leverage generative adversarial networks (GANs) and mel-spectrogram representation. Both NVSR and AudioSR decompose the task into two stages: predicting high-resolution mel-spectrograms from lowresolution ones and then reconstructing the time-domain waveform from the high-resolution mel-spectrogram. We find that dividing the SR task into separate steps can introduce inconsistent representations. For instance, the output mel-spectrogram from the first stage may not be optimally aligned with the vocoders input requirements, potentially affecting the output quality. Furthermore, when the input speech differs significantly from the training data, the separately trained models may struggle to generalize effectively. In this work, we propose unified network that leverages endto-end adversarial training to achieve high-fidelity and more generalized speech super-resolution at 48 kHz. Unlike NVSR and AudioSR, our approach features unified transformer-convolutional generator that seamlessly handles both the prediction of latent representations and their conversion into time-domain waveforms. This design allows the latent representations to move beyond mel-spectrogram constraints, enabling the transformer network to optimize them for optimal alignment with the convolutional network during waveform generation. The transformer network taking from MossFormer2 [18] is particularly effective at capturing long-term dependencies, beneficial for inferring high-frequency structures, making it proper encoder choice for converting low-resolution mel-spectrograms into latent space representations. Our convolutional network, using the HiFi-GAN generator [19], ensures high-quality waveform generation. To further enhance high-frequency fidelity, we incorporate multi-band, multi-scale time-frequency discriminator and multiscale mel-reconstruction loss within the adversarial training framework. We demonstrate that our proposed approach, termed HiFi-SR, can upscale any input speech signal between 4 kHz and 32 kHz to 48 kHz sampling rate. Experimental results show that HiFi-SR significantly outperforms existing speech SR methods across both objective metrics and ABX preference tests, in both in-domain and out-of-domain scenarios. Fig. 1. Overview of our proposed generative transformer-convolutional adversarial network for speech super-resolution (HiFi-SR). The transformer-convolutional generator includes hybrid MossFormer and recurrent network followed by reused HiFi-GAN generator. Three discriminators of MSD, MPD and MBD are combined with feature matching loss Lf and mel-spectrogram loss Lm for high-fidelity adversarial training. 2. METHOD When using mel-spectrogram as input to generate waveform output, our proposed HiFi-SR model adopts optimization strategies similar to neural vocoders like MelGAN [20] and HiFi-GAN, which primarily focus on mel-spectrogram inversion for waveform reconstruction. However, SR requires not only waveform reconstruction but also precise high-resolution prediction. 2.1. Transformer-Convolutional Generator To this end, we propose replacing the fully convolutional generators found in HiFi-GAN with transformer-convolutional generator as shown in Figure 1. Our generator combines transformer network and convolutional feed-forward network, taking mel-spectrogram as input and producing raw waveform as output. To accommodate varying input sampling rates, we first up-sample signals with lower sampling rates to 48 kHz before extracting mel-spectrograms. Our transformer network reuses the MossFormer2 block developed in our previous work [18]. The MossFormer2 block is repeated times to enhance the modelling capability. Before the first block, the mel-spectrogram is projected into higher-dimensional space using linear layer. As detailed in [18,21], each MossFormer2 block combines MossFormer and recurrent block. The MossFormer component employs joint local and global self-attention to fully capture long-term global dependencies within the input sequence. It also utilizes an attentive gating mechanism that reduces the number of selfattention heads to one, significantly simplifying the multi-head attention requirement. The recurrent block, based on the feedforward sequential memory network (FSMN) [22], incorporates dilations to achieve broader receptive fields. This recurrent block is crucial for capturing recurrent patterns related to phonetic structures, prosody, and semantic associations in speech signals, thereby improving the prediction accuracy of high-frequency details. The transformer network outputs an enriched latent representation of the mel-spectrogram input, which is then fed into convolutional network for waveform synthesis. Our convolutional network is based on the HiFi-GAN generator [19], consisting of series of transposed convolutional layers that upsample the input sequence until the output sequence length matches that of the high-resolution waveform. Each transposed convolutional layer is followed by multi-receptive field fusion (MRF) module. The MRF module is used to capture patterns of varying lengths by summing outputs from multiple residual blocks, each with different kernel sizes and dilation rates to create diverse receptive field patterns. We adjusted the hidden dimension hu, transposed convolution kernel sizes ku, MRF kernel sizes kr, and MRF dilation rates Dr for optimal performance in our SR experiments. 2.2. Discriminator Design As demonstrated in MelGAN and HiFi-GAN, the design of the discriminator is critical for generating high-fidelity audio waveforms. We utilize the multi-scale discriminator (MSD) from MelGAN and the multi-period discriminator (MPD) from HiFi-GAN to capture periodic speech patterns at different levels. The MSD operates on three input scales 1, 2, and 4 using average pooling, while the MPD processes disjoint samples with periods of [2,3,5,7,11]. While both MSD and MPD contribute to high audio fidelity, we observe that over-smoothing artifacts can still appear in the high-frequency regions of the generated spectrograms. The multi-resolution discriminator (MRD) proposed in BigVGAN [23] could mitigate such artifacts by operating on linear spectrograms. However, MRD discards phase information, limiting its ability to penalize phase modeling errors at high frequencies. To address these issues, we adopt multi-band, multi-scale timefrequency discriminator (MBD), inspired by audio codec discriminators [24, 25]. The MBD takes the concatenated real and imaginary parts of the complex short-time Fourier transform (STFT) as input. We use five STFT window lengths [4096,2048,1024,512,256], with frequency bands split at [0.0,0.1,0.25,0.5,0.75,1.0]. Each time scale and sub-band shares identical network blocks, consisting of 2D convolutional layer with 3 8 kernel and 32 channels, followed by 2D convolutions with dilation rates of 1, 2, and 4 in the time Fig. 2. Spectrogram illustrations of different system outputs for sample input from the VocalSet singing test set. It demonstrates that HiFiSR significantly outperforms the baseline NVSR model. dimension, and stride of 2 along the frequency axis. final 2D convolution with 3 3 kernel and stride of (1, 1) generates the final prediction. In our SR experiments, we combine MSD, MPD, and MBD for enhanced performance. 2.3. Training Objective To optimize the generator and the discriminators, our training loss combines GAN loss, multi-scale mel-spectrogram loss, and feature matching loss, as detailed below. GAN Loss: For our generator and discriminators, we employ the least-squares objective from LS-GAN [26], which has proven highly effective for adversarial training. The losses for MSD, MPD, and MBD are computed in the same manner, with their individual losses summed to form the final discriminator loss: Ki(cid:88) (1 Di,k(x))2 + (Di,k(G(s)))2(cid:105) LAdv(D) = 3 (cid:88) E(x,s) (cid:104) , i=1 k=1 3 (cid:88) Ki(cid:88) i=1 k= LAdv(G) = (cid:104) (1 Di,k(G(s)))2(cid:105) . Es (1) (2) Here, Di,k denotes sub-discriminator, where = 1, 2, 3 corresponds to the three discriminator types of MSD, MPD, and MBD, and refers to the k-th scale or band. Ki represents the total number of scales or bands for the i-th discriminator. Multi-Scale Mel-Spectrogram Loss: In addition to the GAN loss, we incorporate multi-scale mel-spectrogram loss to promote frequency modeling across multiple time scales, as suggested for codecs [25]. The mel-spectrogram loss is known to improve stability, fidelity, and convergence speed [19]. In our model, we apply an L1 loss across 7 mel-spectrogram bins [5,10,20,40,80,160,320], computed using window lengths of [32,64,128,256,512,1024,2048] with hop length of wj/4, where {wj, = 1, 2, ..., 7} represents the different window lengths. The multi-scale mel-spectrogram loss is defined as: Lm(G) = 7 (cid:88) j=1 E(x,s) (cid:104) Melj(x) Melj(G(s))1 (cid:105) (3) Feature Matching Loss: We also incorporate feature matching loss to stabilize the training process. As demonstrated in [19], this loss improves the quality of generated outputs by ensuring that the generator produces feature representations similar to those of real data at various levels within the discriminators. The feature matching loss is defined as: 3 (cid:88) Ki(cid:88) Lf (G) = i=1 k=1 E(x,s) (cid:104) 1 Li Li(cid:88) l=1 Dl i,k(x) Dl i,k(G(s))1 (cid:105) , 1 i,k (4) where Li,k denotes the number of layers in the {i, k}-th discriminator, Dl i,k denote the output feature and the feature length in the l-th layer of the {i, k}-th discriminator. i,k and Table 1. Objective evaluation results for 48 kHz speech superresolution from input sampling rates of 4 kHz, 8 kHz, 16 kHz, and 24 kHz on the VCTK test set. The evaluation metric is the average LSD across all utterances, with lower values indicating better performance. Nu-wave and WSRGlow have fixed input resolutions. Model Unprocessed Nu-wave WSRGlow AudioSR-Speech NVSR HiFi-SR w/o MBD HiFi-SR w/o Lm(G) HiFi-SR (proposed) No. Parameters - 3.0M4 229.9M4 - 99.0M 101M 101M 101M 4 kHz 6.08 1.42 1.12 1.15 0.98 0.97 0.98 0.95 8 kHz 5.15 1.42 0.98 1.03 0.91 0.88 0.89 0.86 16 kHz 4.85 1.36 0.85 0.82 0.81 0.79 0.80 0.77 24 kHz AVG 4.98 1.36 0.94 0.92 0.85 0.83 0.84 0.82 3.84 1.22 0.79 0.69 0.70 0.69 0.70 0. Final Loss: The final objectives for the generator and discriminators are defined as follows: LG = LAdv(G) + λmLm(G) + λf Lf (G), LD = LAdv(D) (5) (6) where we set λm = 7 and λf = 1.5 to balance the weighted losses. 3. EXPERIMENT 3.1. Dataset To evaluate our proposed approach, we created training set from the VCTK speech corpus [27], which includes recordings from 108 English speakers with total of 44 hours of speech at 48 kHz. Consistent with the data preparation strategy used in [16], we used recordings from 100 speakers for training and the remaining 8 speakers for testing. To assess the generalizability of HiFi-SR to unseen speech types and data types, we created two additional test sets. The EXPRESSO dataset [28], containing 17 hours of expressive reading speech from 4 North American English speakers, was used, with 10% of recordings from each speaker and style forming 1.7-hour EXPRESSO test set. The VocalSet [29], dataset of cappella singing voices from 20 professional singers (11 male, 9 female), was also used, with recordings from 2 male and 2 female singers making up 2-hour VocalSet test set. 3.2. Evaluation Metrics For the objective evaluation metric, we use Log-spectral distance (LSD) to evaluate the SR performance following [16, 17]. Let and ˆS stand for the magnitude spectrograms of the target speech and the generated speech ˆs. LSD is defined as follows: LSD(S, ˆS) = (cid:118) (cid:117) (cid:117) (cid:116) 1 1 (cid:88) t=1 (cid:88) = (cid:104) log10 (cid:105)2 S(t, )2 ˆS(t, )2 (7) LSD is frequency-domain metric that measures the logarithmic distance between two magnitude spectra in dB. When the two spectra are identical, LSD reaches its minimum value of 0 dB. We report Fig. 3. Comparison results of NVSR and HiFi-SR on EXPRESSO test set with 48 kHz target sampling rate and four input sampling rates. Fig. 4. Comparison results of NVSR and HiFi-SR on VocalSet test set with 48 kHz target sampling rate and four input sampling rates. the average LSD across all tested audio files. For subjective evaluation, we conducted an ABX listening test, where raters selected their preferred audio output based on sound quality. Eight listeners participated in the test, each evaluating 50 audio pairs. 3.3. Traning Details Our baseline models include Nu-wave, WSRGlow, NVSR, and AudioSR, all targeting sampling rate of 48 kHz. For the VCTK test set, we used the baseline results reported in their respective publications. For the EXPRESSO and VocalSet test sets, we employed the NVSR pre-trained models based on the open-source code1. Following the method described in [16], we simulated training and test sets by applying various low-pass filters to 48 kHz audio data to obtain lower sampling rates between 4 kHz and 32 kHz. We used 80-band mel-spectrograms with 256 lower temporal resolution. For the HiFi-SR model setup, we used = 24 MossFormer2 blocks with embedding size of 512. In the HiFi-GAN generator, we set hu = 512, ku = [16, 16, 4, 4], kr = [3, 7, 11], and Dr = [[[1, 1], [3, 1], [5, 1]] 3] following [19]. The networks were trained using the AdamW optimizer [30] with β1 = 0.8, β2 = 0.99, and weight decay λ = 0.01. The initial learning rate was 2 104, decayed by factor of 0.999 every epoch. Our training was conducted on single NVIDIA A800 GPU with batch size of 16 for 500k steps. 1https://github.com/haoheliu/ssr eval Fig. 5. ABX subjective test results of NVSR and HiFi-SR on mixed EXPRESSO and VocalSet test set with 48 kHz target sampling rate and four input sampling rates. 3.4. Results and Discussion For objective evaluation, Table 1 compares the performance on the matched VCTK test set using the LSD metric. HiFi-SR achieves an average LSD of 0.82, outperforming all baseline models. The closest competitor is NVSR, with an average LSD of 0.85. We attribute this improvement to our proposed transformer-convolutional generator and adversarial training strategy. To further verify the effectiveness of our training strategies, we conducted ablation studies by removing MBD and the multi-scale mel-spectrogram loss Lm(G). As shown in Table 1, without MBD, the average LSD slightly increases to 0.83, while removing Lm(G) increases it to 0.84. On the EXPRESSO and VocalSet test sets, we compared HiFi-SR against the competitive NVSR model to assess generalization capabilities. The results, displayed in Figures 3 and 4, show that HiFi-SR outperforms NVSR by larger margin on the out-of-domain test sets, demonstrating the superiority of our unified framework over NVSRs separated-module approach. For subjective evaluation, the ABX test results are presented in Figure 5. We evaluated both the EXPRESSO and VocalSet test sets by randomly selecting 25 audio outputs from each set for both HiFiSR and NVSR models, resulting in 50 audio pairs per sampling rate. Participants were asked to choose the audio output with better sound quality or indicate no preference. As shown in Figure 5, participants showed higher preference for HiFi-SR audio outputs compared to NVSR, with HiFi-SR achieving preference rate of over 52.50% across all four input sampling rates. This demonstrates that our unified HiFi-SR model generalizes better than the NVSR model on outof-domain test sets. We visualize the spectrograms of processed sample from both NVSR and HiFi-SR in Figure 2. The output of HiFi-SR is noticeably closer to the ground truth. 4. CONCLUSIONS In this paper, we presented HiFi-SR, unified network developed to address the challenges of speech super-resolution, particularly in out-of-domain scenarios. By leveraging transformer-convolutional generator and end-to-end adversarial training, HiFi-SR effectively handles both the prediction of latent representations and their conversion into time-domain waveforms, ensuring consistent and highfidelity speech reconstruction. Our experimental results show that HiFi-SR outperforms existing speech SR methods, achieving superior performance in both objective metrics and ABX preference tests. The models ability to generalize well to out-of-domain data further highlights the robustness of our approach. 5. REFERENCES [1] S. Chennoukh, A. Gerrits, G. Miet, and R. Sluijter, Speech enhancement via frequency bandwidth extension using line spectral frequencies, in Proc. of ICASSP, 2001. [2] H. Liu, Q. Kong, Q. Tian, Y. Zhao, D. Wang, C. Huang, and Y. Wang, VoiceFixer: Toward general speech restoration with neural vocoder, arXiv preprint:2109.13731, 2021. [3] K. Nakamura, K. Hashimoto, K. Oura, Y. Nankaku, and K. Tokuda, mel-cepstral analysis technique restoring high frequency components from low-sampling-rate speech, in Proc. of ISCA, 2014. [4] Y. M. Cheng, D. OShaughnessy, and P. Mermelstein, Statistical recovery of wideband speech from narrowband speech, IEEE Transactions on Speech and Audio Processing, vol. 2, no. 4, pp. 544548, 1994. [5] H. Pulakka, U. Remes, K. Palomaki, M. Kurimo, and P. Alku, Speech bandwidth extension using gaussian mixture modelbased estimation of the highband mel spectrum, in Proc. of ICASSP, 2011. [6] A. H. Nour-Eldin and P. Kabal, Memory-based approximation of the gaussian mixture model framework for bandwidth extension of narrowband speech, in Proc. of INTERSPEECH, 2011. [7] M. T. Turan and E. Erzin, Synchronous overlap and add of spectra for enhancement of excitation in artificial bandwidth extension of speech, in Proc. of INTERSPEECH, 2015. [8] V. Kuleshov, S. Z. Enam, and S. Ermon, Audio super resolution using neural networks, in Workshop of ICLR, 2017. [9] T. Y. Lim, R. A. Yeh, Y. Xu, M. N. Do, and M. HasegawaTime-frequency networks for audio superJohnson, resolution, in Proc. of ICASSP, 2017. [10] X. Li, V. Chebiyyam, K. Kirchhoff, and A. Amazon, Speech audio super-resolution for speech recognition, in Proc. of INTERSPEECH, 2019. [11] N. Hou, C. Xu, V. T. Pham, J. T. Zhou, E. S. Chng, and H. Li, Speaker and phoneme-aware speech bandwidth extension with residual dual-path network, in Proc. of INTERSPEECH, 2020. [12] H. Wang and D. Wang, Towards robust speech superresolution, IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 20582066, 2021. [13] J. Lee and S. Han, Nu-wave: diffusion probabilistic model for neural audio upsampling, arXiv:2104.02321, 2021. [14] K. Zhang, Y. Ren, C. Xu, and Z. Zhao, WSRGlow: glowbased waveform generative model for audio super-resolution, arXiv:2106.08507, 2021. [15] S. Han and J. Lee, dio upsampling model for various sampling rates, preprint:2206.08545, 2022. NUWave 2: general neural auarXiv [16] H. Liu, W. Choi, X. Liu, Q. Kong, Q. Tian, and D. Wang, Neural vocoder is all you need for speech super-resolution, in Proc. of INTERSPEECH, 2022. [17] H. Liu, K. Chen, Q. Tian, W. Wang, and M. D. PlumbAudioSR: Versatile audio super-resolution at scale, ley, arXiv:2309.07314, 2023. [18] S. Zhao, Y. Ma, C. Ni, C. Zhang, H. Wang, T. H. Nguyen, K. Zhou, J. Yip, D. Ng, and B. Ma, MossFormer2: Combining transformer and RNN-free recurrent network for enhanced time-domain monaural speech separation, arXiv:2312.11825, 2023. [19] J. Kong, J. Kim, and J. Bae, HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis, arXiv:2010.05646, 2020. [20] K. Kumar, R. Kumar, T. de Boissiere, L. G., W. Z. Teoh, J. S., A. de Brebisson, Y. Bengio, and A. Courville, MelGAN: Generative adversarial networks for conditional waveform synthesis, arXiv:1910.06711, 2019. [21] S. Zhao and B. Ma, MossFormer: Pushing the performance limit of monaural speech separation using gated single-head transformer with convolution-augmented joint self-attentions, arXiv:2302.11824, 2023. [22] S. Zhang, M. Lei, Z. Yan, and L. Dai, Deepfsmn for large vocabulary continuous speech recognition, arXiv:1803.05030, 2018. [23] S. Lee, W. Ping, B. Ginsburg, B. Catanzaro, and S. Yoon, Bigvgan: universal neural vocoder with large-scale training, arXiv:2206.04658, 2023. [24] A. Defossez, J. Copet, G. Synnaeve, and Y. Adi, High fidelity neural audio compression, arXiv:2210.13438, 2022. [25] R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and K. Kumar, High-fidelity audio compression with improved RVQGAN, arXiv:2306.06546, 2023. [26] X. Mao, Q. Li, H. Xie, R. Y-K. Lau, Z. Wang, and S. P. Smolley, Least squares generative adversarial networks, in Proc. of ICCV, 2017. [27] J. Yamagishi, C. Veaux, and K. MacDonald et al., CSTR VCTK corpus: English multi-speaker corpus for cstr voice cloning toolkit, 2019. [28] T. A. Nguyen, W.-N. Hsu, A. DAvirro, and B. Shi et al., Expresso: benchmark and analysis of discrete expressive speech resynthesis, arXiv: 2308.05725, 2023. [29] J. Wilkins, P. Seetharaman, A. Wahl, and B. Pardo, Vocalset: singing voice dataset, in Proc. of International Society for Music Information Retrieval, 2018. [30] Ilya Loshchilov and Frank Hutter, Decoupled weight decay regularization, arXiv: 1711.05101, 2019."
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group, Singapore"
    ]
}