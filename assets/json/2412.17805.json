{
    "paper_title": "Large Motion Video Autoencoding with Cross-modal Video VAE",
    "authors": [
        "Yazhou Xing",
        "Yang Fei",
        "Yingqing He",
        "Jingye Chen",
        "Jiaxin Xie",
        "Xiaowei Chi",
        "Qifeng Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Learning a robust video Variational Autoencoder (VAE) is essential for reducing video redundancy and facilitating efficient video generation. Directly applying image VAEs to individual frames in isolation can result in temporal inconsistencies and suboptimal compression rates due to a lack of temporal compression. Existing Video VAEs have begun to address temporal compression; however, they often suffer from inadequate reconstruction performance. In this paper, we present a novel and powerful video autoencoder capable of high-fidelity video encoding. First, we observe that entangling spatial and temporal compression by merely extending the image VAE to a 3D VAE can introduce motion blur and detail distortion artifacts. Thus, we propose temporal-aware spatial compression to better encode and decode the spatial information. Additionally, we integrate a lightweight motion compression model for further temporal compression. Second, we propose to leverage the textual information inherent in text-to-video datasets and incorporate text guidance into our model. This significantly enhances reconstruction quality, particularly in terms of detail preservation and temporal stability. Third, we further improve the versatility of our model through joint training on both images and videos, which not only enhances reconstruction quality but also enables the model to perform both image and video autoencoding. Extensive evaluations against strong recent baselines demonstrate the superior performance of our method. The project website can be found at~\\href{https://yzxing87.github.io/vae/}{https://yzxing87.github.io/vae/}."
        },
        {
            "title": "Start",
            "content": "Large Motion Video Autoencoding with Cross-modal Video VAE Yazhou Xing* Yang Fei Yingqing He"
        },
        {
            "title": "Jiaxin Xie",
            "content": "Xiaowei Chi Qifeng Chen"
        },
        {
            "title": "The Hong Kong University of Science and Technology",
            "content": "4 2 0 2 3 2 ] . [ 1 5 0 8 7 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Learning robust video Variational Autoencoder (VAE) is essential for reducing video redundancy and facilitating efficient video generation. Directly applying image VAEs to individual frames in isolation can result in temporal inconsistencies and suboptimal compression rates due to lack of temporal compression. Existing Video VAEs have begun to address temporal compression; however, they often suffer from inadequate reconstruction performance. In this paper, we present novel and powerful video autoencoder capable of high-fidelity video encoding. First, we observe that entangling spatial and temporal compression by merely extending the image VAE to 3D VAE can introduce motion blur and detail distortion artifacts. Thus, we propose temporal-aware spatial compression to better encode and decode the spatial information. Additionally, we integrate lightweight motion compression model for further temporal compression. Second, we propose to leverage the textual information inherent in text-to-video datasets and incorporate text guidance into our model. This significantly enhances reconstruction quality, particularly in terms of detail preservation and temporal stability. Third, we further improve the versatility of our model through joint training on both images and videos, which not only enhances reconstruction quality but also enables the model to perform both image and video autoencoding. Extensive evaluations against strong recent baselines demonstrate the superior performance of our method. The project website can be found at https://yzxing87.github.io/vae/. 1. Introduction Given the significant attention in the field of video generation, Latent Video Diffusion Models (LVDMs) [4, 5, 7, 14, 36] have emerged as popular framework. They have been successfully applied to powerful text-to-video models such as Sora [6], VideoCrafter [7, 8], and CogVideoX [31]. Different from directly generating video pixels, LVDMs gen- *equal contribution corresponding authors erate latent video representations in compact latent space. This is achieved by first training Video VAE to encode videos into this latent space. Thus, Video VAE, as key and fundamental component of LVDMs, has attracted great attention recently. An effective Video VAE can help to reduce the training costs of video diffusion models while improving the final quality of the generated videos. Initially, series of studies adopt the image VAE from Stable Diffusion [25] for video generation tasks, including AnimateDiff [13], MagicVideo [36], VideoCrafter1 [7], and VideoCrafter2 [8]. However, directly adopting an image VAE and compressing video on frame-by-frame basis leads to temporal flickering due to the lack of temporal correlation. Additionally, the information redundancy along the temporal dimension is not reduced, leading to low training efficiency for subsequent latent video diffusion models. From the introduction of Sora, which compresses videos both temporally and spatially through Video VAE, series of studies have emerged that aim to replicate Sora and train their own Video VAEs, including Open Sora [35], Open Sora Plan [19], CV-VAE [34], CogVideoX [31], EasyAnimate [30], and Cosmos Tokenizer [23]. However, the performance of the current video VAE suffers from many problems, including motion ghost, low-level temporal flickering, blurring (faces, hands, edges, texts), and motion stuttering (lack of correct temporal transition). In this work, we propose novel cross-modal Video VAE with better spatial and temporal modeling ability in order to solve the aforementioned challenge problems and obtain robust and high-quality Video VAE. First, we examine different designs for spatial and temporal compression, including simultaneous spatial-temporal (ST) compression and sequential ST compression. We observed that simultaneous ST compression achieves better low-level temporal smoothness and texture stability, while sequential ST compression achieves better motion recovery, particularly in scenarios of large motion. Thus, we propose novel architecture that integrates the advantages of both methods and enables effective video detail and motion reconstruction. Second, we observed that the normally used datasets for text-to-video generation contain text-video pairs. Also, dur1 Figure 1. Our reconstruction results compared with line of three recent strong baseline approaches. The ground truth frame is (0). Our model significantly outperforms previous methods, especially under large motion scenarios such as people doing sports. ing decoding, text description exists as it serves as the input in the first stage, i.e., the video latent generation stage. To this end, we integrate the text information into the encoding and decoding procedure and propose the first Crossmodal Video VAE. We carefully study how text guidance can be integrated into the spatiotemporal backbone and the mechanism of spatial and temporal semantic guidance. In addition, our cross-modal video VAE supports imagevideo joint training. To achieve this, we design our network with fully spatiotemporal factorized architecture, and we feed image and video batches alternately to the network. During image batches, the data only forwards the spatial part of the network, with the temporal modules being skipped. During video batches, the video forwards both spatial and temporal modules. We also demonstrate that image joint training is crucial for training video VAE. In summary, our contributions are as follows: We propose an effective and robust Video VAE, conduct extensive experiments, and achieve the state-of-the-art. We propose an optimal spatiotemporal modeling approach for Video VAE. We propose the first cross-modal video VAE that leverages the information from other modalities, i.e., text descriptions, to the best of our knowledge. 2 Our video VAE is designed and trained to be versatile to conduct both image and video compression. 2. Related Work Video Variational Autoencoder Video Variational Autoencoders (VAEs) [17] can be broadly categorized into discrete and continuous types. Discrete video VAEs compress videos into discrete tokens by learning codebook for quantization and have achieved state-of-the-art performance in video reconstruction, as demonstrated by models like MAGVIT-v2 [32]. However, these VAEs are not suitable for Latent Video Diffusion Models (LVDMs) [14] due to the lack of necessary gradients for backpropagation, which hinders smooth optimization. In contrast, continuous Video VAEs compress videos representations that are widely into continuous latent In earlier video generation studies, adopted in LVDMs. including Stable Video Diffusion [4], the Video VAE was directly adapted from the image VAE used in Stable Diffusion [25], achieving compression ratio of 1 8 8 by processing each frame independently. To further reduce the temporal redundancy, more recent studies [19, 30, 31, 34, 35] have trained their VAEs to achieve more efficient compression ratio of 4 8 8. Despite these advancements, all of the aforementioned video VAEs struggle with accurately reconstructing videos with large motions due primarily to their limited ability to handle the temporal dimension effectively. high-quality Video VAE that can robustly reconstruct videos with significant motion is critical in the LVDM pipeline, as it ensures efficient latent space compression, maintains temporal coherence and reduces computational overhead [27]. Without robust VAE, large motions in videos can lead to poor latent representations, negatively impacting the quality and overall performance of the LVDMs. Latent Video Diffusion Models Latent Video Diffusion Models (LVDMs) are widely used in foundational video generation models including Sora [6], OpenSora [35], Open Sora Plan [19], VideoCrafter1 [7], VideoCrafter2 [8], Latte[20], CogVideoX [31], DynamiCrafter [28], Vidu [3], Hunyuan Video [18], controllable video generation [15, 21, 22], and multimodal video generation models [16, 29]. The general pipeline for these LVDMs consists of two primary steps. First, the raw video is compressed into latent space via video Variational Autoencoder (VAE), significantly reducing computational complexity. In the second step, diffusion model operates within this latent space, learning the desired transformations. The performance of LVDMs is critically dependent on video VAEs, as the quality of the generated video is heavily influenced by the latent space representation and the encoding-decoding capabilities of the VAE. In image generation tasks, Stable Diffusion series [1, 24, 25] has excelled, largely due to its efficient VAE that reconstructs diverse image types with high fidelity. However, no existing VAE in video generation achieves comparable quality, particularly due to challenges in compressing the temporal dimension. This limitation hinders the performance of LVDMs, especially in high-motion scenarios. 3. Method 3.1. Overview The video autoencoding problem can be defined as follows. Let RCT HW represent video or image tensor, where C, , H, and denote the number of channels, frame(s), height, and width, respectively. We want to train an encoder that compresses the input tensor into compact latent representation RCT W . The learned compact latent can be further reconstructed back to RGB space with decoder D: = E(X), ˆX = D(Z). (1) Our goal is to design and learn such an autoencoder that can reduce the spatial and temporal dimension of video data in latent space and reconstruct the video with highly spatial and temporal fidelity, especially for large-motion scenarios. We first examine two inherited video VAE designs from the pre-trained Stable Diffusion model. We then combine the best of two designs and propose our spatiotemporal modeling that can reconstruct high-dynamic contents with fine details. We then investigate the text-conditioned video autoencoding and propose an effective text-guided video VAE architecture. Moreover, we propose joint image and video compression training method, that enables text-aided joint image and video autoencoding. Our method does not rely on causal convolution as adopted by prior works. Finally, we carefully study the effects of different loss functions on the reconstruction performance and present the state-of-the-art video VAE architecture. 3.2. Optimal Spatiotemporal Modeling Designing video VAE that is inherited from pre-trained 2D spatial VAE is good practice to leverage the spatial compression prior. There are typically two options to inflate 2D spatial VAE to its 3D video counterpart. Option 1: Simultaneous Spatiotemporal Compression One common way to inherit the weight from pre-trained 2D VAE is to inflate the 2D spatial blocks to 3D temporal blocks and simultaneously do the spatial and temporal compression. We first examine this design. Specifically, we replace the 2D convolution in SD VAE with 3D convolution of kernel size (1,3,3), whose weights are initialized from the 3 Figure 2. Comparison of our optimal spatiotemporal modeling and the two other options. Simultaneous modeling is achieved by inflating pre-trained 2D spatial VAE to 3D VAE. Sequential modeling indicates first compressing the spatial dimension with spatial encoder and then compressing the temporal information with temporal encoder. We identify the issues of these two options and propose to combine both advantages and achieve much better video reconstruction quality. Our VAE also benefits from cross-modality, i.e., text information. 2D convolution. Then we add an additional temporal convolution layer with kernel size (3,3,3) to learn spatiotemporal patterns. In this middle block of the inflated VAE, we inflate the 2D attention to 3D attention and we also include temporal attention to capture both the spatial and temporal information. We keep other components unchanged to maximumly leverage the learned prior of SD VAE. Option 2: Sequential Spatiotemporal Compression Another reasonable way to cooperate the SD VAE to video VAE is to keep the SD VAE unchanged: first utilize the SD VAE to compress the input video frame-by-frame, and then learn temporal autoencoding process to further compress the temporal redundancy, as shown in Fig. 2. Specifically, we adopt lightweight temporal autoencoder for temporal compression. The encoder consists of one convolutional layer to process the input, and two or three 3D ResNet blocks with convolutional downsampling layers to compress the temporal redundancy. Notably, we design the decoder to be asymmetric as the encoder, i.e., there will be two 3D ResNet blocks following each upsampling layer in the decoder. Through this asymmetric design, our decoder can potentially gain some hallucination ability beyond the reconstruction. Surprisingly, we find this sequential spatiotemporal design can better compress and recover the dynamic of the input video than option 1, but is not good at recovering spatial details, which is proved by consistent improvement under large-motion video autoencoding as shown in Fig. 4. Our Solution We find simultaneous spatiotemporal compression leads to better detail-recovering capability, and the sequential spatiotemporal compression will exceed at motion-recovering ability. Thus, we propose to combine the best of two worlds, and introduce the two-stage spatiotemporal modeling for video VAE. As the first stage, we inflate the 2D convolution to 3D convolution with kernel size (1,3,3), and similarly to option 1, we add additional temporal convolution layers through 3D convolution. We denote our first-stage model as temporal-aware spatial autoencoder. Different from option 1, we only compress the spatial information and do not compress the temporal information at the first stage, but introduce another temporal encoder to further encode the temporal dimensions, which serves as the second stage compression. We follow the same design of option 2 for our temporal encoder and decoder. After that, we decode the reconstructed latent of the second stage to the RGB space, with the inflated 3D decoder. We jointly train the inflated 3D VAE and the temporal autoencoder. The main idea is illustrated in Fig. 2 and Fig. 3. 4 temporal autoencoder. We adopt LayerNorm as the normalization function. We use Flan-T5 [12] as the text embedder. projection convolution is applied to the result, which is then added to the input via residual connection. 3.4. Joint Image and Video Compression In contrast to existing architectures such as MagVitV2 [32], OD-VAE [9], and OPS-VAE [35], which use Causalconv3D layers, we rely primarily on standard Conv3D layer. notable feature of our architecture is the ability to mask out the temporal autoencoder, allowing the first-stage model to operate as standalone image compressor. During training, our model is flexible to take both image and video as input: when the current batch is composed of images, we will disable the temporal convolution and temporal attention layers, as well as the temporal autoencoder. We train our model on both the image dataset and video dataset to let the model learn the image and video compression ability simultaneously. Besides, training on more high-quality images can also help improve the video autoencoding performance. We quantitatively evaluate the performance of our joint image and video compression in Table 1. 3.5. Loss Functions We use the reconstruction loss, the KL divergence loss, and the video adversarial loss (3D GAN loss) to optimize our model. The reconstruction loss, Lrecon, ensures that the generated frames are perceptually and structurally similar to the input frames. It combines pixel-wise error term with perceptual loss, weighted by hyperparameter. The KL divergence loss, LKL, regularizes the latent space by encouraging it to conform to prior distribution, ensuring smoothness and continuity in the learned latent representations. Given the hierarchical structure of our latent space, we only regu8 larize the innermost latent Z2, with dimensions 8 , where , H, and represent the temporal, height, and width dimensions, respectively. The 3D GAN loss, LGAN, is introduced to enhance the realism of the generated video sequences, leveraging discriminator to distinguish between real and generated sequences. The total loss function is expressed as: 4 Ltotal = Lrecon + λKLLKL + λGANLGAN. (5) 4. Experiments 4.1. Experimental Setup Datasets We conduct experiments on three datasets: the public Panda2M [10] and MMTrailer [11] datasets, and private text-video dataset with over 6M pairs. To evaluate reconstruction performance, we use three test sets: the WebVid test set, the Inter4K test set (similar to [34]), and large motion test set. The WebVid test set contains 1,000 Figure 3. The architecture of our temporal-aware spatial autoencoder. We expand the 2D convolution of SD VAE [25] to 3D convolution and append one additional 3D convolution as temporal convolution after the expanded 3D convolution, which forms the STBlock3D. We also inject the cross-attention layers for crossmodal learning with textual conditions. Formulation Recall RCT HW represent video, where C, , H, and denote the number of channels, frames, height, and width, respectively. The i-th frame of the video is denoted as xi RCHW . The temporalaware spatial encoder encodes into latent representation Z1 RcT hw, where is the number of latent channels, and = 8 , as formulated by: 8 , = Z1 = E1(X). (2) Next, the temporal autoencoder encodes Z1 into Z2 Rcthw, where is the number of latent channels for Z2 and = 4 , as given by: Z2 = E2(Z1). (3) Reconstruction is achieved by decoding Z2 back into the original video space, ˆX RCT HW , through the following inverse process: ˆX = D1(D2(Z2)) = D1(Z1). (4) 3.3. Cross-modal Modeling Since textual information is native component for text-tovideo generation datasets, we examine if the textual information can improve the autoencoding process of the model. To achieve that, we split the feature maps into patches as tokens after each ResNet block in the encoder and decoder, and compute the cross attention by taking visual tokens as query (Q) and value (V), the text embeddings as key (K). We try to keep the patch size trackable for each layer. Specifically, we use patch size to 88, 44, 22, and 11 for each layer in the temporal-aware spatial autoencoder respectively. We directly use each pixel as one patch in the 5 Figure 4. Comparisons among simultaneous spatiotemporal modeling, sequential spatiotemporal modeling and our proposed solution. 256x256, 16-frame videos from the WebVid dataset [2]. The Inter4K test set consists of 500 640x864, 16-frame videos from the Inter4K dataset [26]. To assess the models ability to handle challenging motion patterns, we introduce large motion test set. This set includes 80 videos from WebVid and 20 from Inter4K, manually selected for their complex motion dynamics. Implementation Details We initialize our 4-channel and 16-channel latent Video VAEs from SD-1.4 [25] and SD3.5 [1], respectively. For both models, we enable the video GAN loss after 50K warmup steps. We initially train the 4-channel and 16-channel latent Video VAEs for 230K and 310K steps, respectively. Subsequently, we conduct joint image-video training, using an 8:2 video-to-image ratio to balance video and image reconstruction. For each training step, we sample 16 videos from Panda2M and our private text-video dataset, concatenating their frames into single image batch. By masking the temporal dimension and bypassing the temporal autoencoder, we treat these images as independent static frames, allowing the model to learn from both temporal and spatial information. The 4-channel and 16-channel latent Video VAEs undergo additional joint training for 100K and 185K steps, respectively. For the 6 Model Downsample Factor #Channels WebVid Test Set [2] Inter4K Test Set [26] Large Motion Test Set Open-Sora-Plan (OD VAE [9]) Open-Sora (OPS VAE [35]) CV-VAE [34] Video VAE w/o Joint Training (Ours) Video VAE (Ours) Cross-Modal VAE (Ours) Cosmos-Tokenizer [23] CogVideoX-VAE [31] EasyAnimate-VAE [30] CV-VAE [34] Video VAE w/o Joint Training (Ours) Video VAE (Ours) Cross-Modal VAE (Ours) 4x8x8 4x8x8 4x8x8 4x8x8 4x8x8 4x8x8 4x8x8 4x8x8 4x8x8 4x8x8 4x8x8 4x8x8 4x8x8 PSNR () SSIM () LPIPS () PSNR () SSIM () LPIPS () PSNR () SSIM () LPIPS () 4 4 4 4 4 4 16 16 16 16 16 16 16 29.1646 29.3753 28.6795 30.2091 30.3140 30.1110 31.2545 32.8940 32.1233 32.2766 33.8844 34.1558 34.5022 0.8334 0.8284 0.8154 0.8656 0.8676 0.8608 0.8861 0.9208 0.9085 0.9080 0.9334 0.9362 0. 0.0789 0.1240 0.1072 0.0566 0.0538 0.0544 0.1030 0.0504 0.0405 0.0546 0.0344 0.0271 0.0323 28.6690 29.2721 27.7437 28.9048 28.9227 29.0357 31.2002 32.5122 31.5066 31.6129 32.9416 33.3184 33.5687 0.8381 0.8431 0.8124 0.8543 0.8565 0.8510 0.8957 0.9229 0.9048 0.9060 0.9297 0.9328 0. 0.0906 0.1316 0.1284 0.0688 0.0665 0.0678 0.1071 0.0532 0.0572 0.0642 0.0409 0.0316 0.0379 27.5697 27.7586 26.9456 27.3917 27.6236 27.1754 30.1619 31.0906 30.5213 30.7136 31.8471 32.1503 32.2387 0.8045 0.8032 0.7849 0.8078 0.8136 0.7999 0.8675 0.8978 0.8846 0.8868 0.9073 0.9122 0. 0.1065 0.1540 0.1411 0.0867 0.0841 0.0846 0.1194 0.0685 0.0598 0.0726 0.0499 0.0409 0.0481 Table 1. Quantitative comparison with state-of-the-art methods. Model # Ch PSNR () SSIM () LPIPS () SD1.4 [4] Ours w/o JT Ours SD3.5 [1] Ours w/o JT Ours 4 4 4 16 16 16 30.2199 15.1001 30.8650 36.5208 9.2603 35.3437 0.8974 0.5561 0. 0.9646 0.2770 0.9590 0.0440 0.4339 0.0397 0.0116 0.6802 0.0167 Table 2. JT means joint training. We evaluate image reconstruction performance w/ or w/o our joint image-video training strategy. cross-modal VAE, both models are initialized with their pretrained weights. We train them on video-text pairs for 160K steps, enabling the model to learn the alignment between visual and textual modalities. 4.2. Comparison with State-of-the-arts We compare our proposed Video VAE models with the state-of-the-art video compression models: Open-SoraPlan [19], Open-Sora [35], CV-VAE [34] on 4-channel latent models, and Cosmos-Tokenizer [23], CogVideoX [31], EasyAnimate [30], CV-VAE [34] on 16-channel models. Quantitative Evaluation We use PSNR, SSIM, and LPIPS [33] to quantitatively measure the quality of the reconstructed videos. We compare our method with baselines on our three test sets, as listed in Table 1. Among these, our 4-channel latent Video VAE demonstrates superior performance across most datasets and metrics. Specifically, our model achieves the best reconstruction quality on the WebVid test set, shown as more than 1dB improvements over baselines and significant improvement on the LPIPS metrics, which indicates our reconstruction is both with highfidelity and better perceptual quality. similar conclusion can be made on the Inter4K test set. On the Large-Motion test set, our model maintains strong performance with significant SSIM and LPIPS improvement, showcasing its robustness in handling complex motion scenarios. For models with 16-channel latent space, our model consistently outperforms these baselines across all test sets. For example, on the WebVid test set, our model achieves more than 2dB in terms of PSNR, significantly higher than Cosmos-Tokenizer and CogVideoX. Moreover, our model achieves the best SSIM and LPIPS, demonstrating substantial improvements in both fidelity and perceptual quality. In summary, our Video VAE models consistently outperform existing baselines across all test sets and metrics, highlighting their effectiveness in both low-channel (4-channel latent) and high-channel (16-channel latent) configurations. Qualitative Evaluation We provide qualitative comparisons with the baselines in Fig. 1. Our method demonstrates significantly improved motion recovery, greatly reducing ghosting artifacts even in rapid motion scenarios. In contrast, Open-Sora-Plan and CV-VAE struggle to reconstruct fast-moving objects, leading to ghosting artifacts. Additionally, Open-Sora VAE introduces color reconstruction errors, as seen in the clothing of the moving figure. Increasing the latent channels to 16 improves motion reconstruction across all baselines, but noticeable detail errors remain. Our 16channel model further mitigates these errors, resulting in more accurate detail reconstruction. We further compare the reconstruction results with and without the cross-modal training, as shown in Fig. 5. 4.3. Ablation Study Joint Training We evaluate the effectiveness of our image-video joint training by comparing the performance of our 4-channel latent and 16-channel latent Video VAEs with the video-only training VAE, as well as the image VAE, SD 1.4 and SD 3.5, respectively. The results are shown in Table 1 and Table 2. The video reconstruction comparison is conducted on the three benchmark datasets. The 7 Model PSNR () SSIM () LPIPS () Simultaneous Sequential Ours 24.0593 23.3681 24.6722 0.7315 0.6917 0.7234 0.1293 0.1481 0.1162 Table 3. Ablation study comparing simultaneous modeling, sequential modeling, and ours on the large-motion test set. Architecture Variants We evaluate the effectiveness of different spatiotemporal compression strategies, including simultaneous spatiotemporal compression, sequential spatiotemporal compression, and our proposed solution. These architecture variants are tested on the Large-Motion Test Set to determine which model handles challenging scenarios most effectively, as shown in Table 3. Model / Kernel Size PSNR () SSIM () LPIPS () Image GAN Loss Video GAN Loss 31.9133 32.0262 TemporalConv(3, 1, 1) 30.3332 TemporalConv(5, 1, 1) 30.8745 TemporalConv(7, 1, 1) 31.2922 TemporalConv(5, 3, 3) 31.3516 TemporalConv(7, 3, 3) 31.7444 0.9071 0.9089 0.8898 0.9004 0.9025 0.9011 0. 0.0436 0.0426 0.0489 0.0475 0.0458 0.0437 0.0436 Table 4. Ablation study comparing temporal-aware spatial autoencoder with image/video GAN loss, and different kernel sizes. Component Ablation We perform ablation studies on several key components of our model. First, we investigate the impact of the kernel size in the temporal convolutional layer of temporal-aware spatial autoencoder. The results of this study are shown in Table 4. Additionally, we explore the significance of the loss function by comparing the performance of temporal-aware spatial autoencoder trained with either the raw image GAN loss or the video GAN loss, with the results also presented in Table 4. These ablations are conducted on validation set comprising 98 videos, each with resolution of 256x256 pixels and length of 16 frames, sourced from the MMTrailer dataset. 5. Conclusion We propose novel video variational autoencoder (VAE) to address high-fidelity video autoencoding and compression, especially for videos with large motion. Our approach extends pre-trained image VAEs to the video domain by decoupling spatial and temporal compression, mitigating motion blur and detail loss. We design temporal-aware spatial encoder and lightweight motion compression model to enhance motion modeling, temporal consistency, and detail preservation. To improve Figure 5. The effectiveness of the cross-modal learning for our video VAE. The introduction of textural information improves the detail recovery. We visualize the learned attention map using keywords of the input prompts. Figure 6. The effectiveness of joint image and video training. image reconstruction comparison is conducted on set of 500 images with resolution of 480x864, randomly sampled from UHD-4K video dataset. During inference, we mask out the temporal autoencoder and the temporal part of the temporal-aware spatial autoencoder, ensuring that the models process the images without considering temporal information, effectively treating them as independent images. The joint training can further boost the performance of video reconstruction, which is consistent in both the 4channel and 16-channel experiments. For the image reconstruction, our 4-channel latent Video VAE slightly outperforms SD1.4, and also improves on SSIM and LPIPS, indicating better perceptual quality. For the 16-channel VAE, while our model achieves competitive results in terms of PSNR, it falls slightly short of SD3.5. However, our model still demonstrates strong performance in terms of SSIM and LPIPS, suggesting that our joint training approach maintains high perceptual quality despite the slight drop in PSNR. We further show the visual effectiveness of the joint image and video training in Fig 6. Overall, these results demonstrate that our joint image-video training strategy allows the model to retain strong image reconstruction capabilities while simultaneously learning to handle video data. 8 reconstruction quality and versatility, we leverage detailed captions and employ joint image-video training. Extensive experiments on challenging datasets demonstrate superior performance over state-of-the-art baselines. Our model sets new standard for video compression by efficiently handling spatiotemporal compression while benefiting from cross-modal learning and joint training."
        },
        {
            "title": "References",
            "content": "[1] Stability AI. Stable diffusion 3.5 large, 2023. 3, 6, 7 [2] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17281738, 2021. 6, 7 [3] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. 3 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1, 3, 7 [5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 1 [6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 1, 3 [7] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 1, 3 [8] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024. 1, [9] Liuhan Chen, Zongjian Li, Bin Lin, Bin Zhu, Qian Wang, Shenghai Yuan, Xing Zhou, Xinhua Cheng, and Li Yuan. Od-vae: An omni-dimensional video compressor for improving latent video diffusion model, 2024. 5, 7 [10] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple In Proceedings of the IEEE/CVF cross-modality teachers. Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. 5 [11] Xiaowei Chi, Yatian Wang, Aosong Cheng, Pengjun Fang, Zeyue Tian, Yingqing He, Zhaoyang Liu, Xingqun Qi, Jiahao Pan, Rongyu Zhang, Mengfei Li, Ruibin Yuan, Yanbing Jiang, Wei Xue, Wenhan Luo, Qifeng Chen, Shanghang Zhang, Qifeng Liu, and Yike Guo. Mmtrail: multimodal trailer video dataset with language and music descriptions, 2024. 5 [12] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instructionfinetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. 5 [13] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textIn The to-image diffusion models without specific tuning. Twelfth International Conference on Learning Representations. 1 [14] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity arXiv preprint video generation with arbitrary lengths. arXiv:2211.13221, 2022. 1, [15] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et al. Animate-a-story: Storytelling with retrieval-augmented video generation. arXiv preprint arXiv:2307.06940, 2023. 3 [16] Yingqing He, Zhaoyang Liu, Jingye Chen, Zeyue Tian, Hongyu Liu, Xiaowei Chi, Runtao Liu, Ruibin Yuan, Yazhou Xing, Wenhai Wang, et al. Llms meet multimodal generation and editing: survey. arXiv preprint arXiv:2405.19334, 2024. 3 [17] Diederik Kingma and Max Welling. Auto-encoding variational bayes. stat, 1050:1, 2014. 3 [18] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3 [19] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. 1, 3, 7 [20] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 3 [21] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Poseguided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 41174125, 2024. 3 [22] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. arXiv preprint arXiv:2406.01900, 2024. 3 [23] NVIDIA. Cosmos tokenizer: suite of image and video neural tokenizers, 2024. 1, 9 [24] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 3 [25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 3, 5, 6 [26] Alexandros Stergiou and Ronald Poppe. Adapool: Exponential adaptive pooling for information-retaining downsampling. arXiv preprint, 2021. 6, 7 [27] The Movie Gen team @ Meta. Movie gen: cast of media foundation models. 2024. [28] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors, 2023. 3 [29] Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, and Qifeng Chen. Seeing and hearing: Open-domain visualaudio generation with diffusion latent aligners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71517161, 2024. 3 [30] Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang. Easyanimate: high-performance long video generation method based on transformer architecture, 2024. 1, 3, 7 [31] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer, 2024. 1, 3, 7 [32] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusiontokenizer is key to visual generation. International Conference on Learning Representations, 2024. 3, 5 [33] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [34] Sijie Zhao, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Muyao Niu, Xiaoyu Li, Wenbo Hu, and Ying Shan. Cv-vae: compatible video vae for latent generative video models. arXiv preprint arXiv:2405.20279, 2024. 1, 3, 5, 7 [35] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 1, 3, 5, 7 [36] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video arXiv preprint generation with latent diffusion models. arXiv:2211.11018, 2022."
        }
    ],
    "affiliations": []
}