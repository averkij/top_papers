{
    "paper_title": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS",
    "authors": [
        "Weijie Wang",
        "Donny Y. Chen",
        "Zeyu Zhang",
        "Duochao Shi",
        "Akide Liu",
        "Bohan Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a promising solution for novel view synthesis, enabling one-pass inference without the need for per-scene 3DGS optimization. However, their scalability is fundamentally constrained by the limited capacity of their encoders, leading to degraded performance or excessive memory consumption as the number of input views increases. In this work, we analyze feed-forward 3DGS frameworks through the lens of the Information Bottleneck principle and introduce ZPressor, a lightweight architecture-agnostic module that enables efficient compression of multi-view inputs into a compact latent state $Z$ that retains essential scene information while discarding redundancy. Concretely, ZPressor enables existing feed-forward 3DGS models to scale to over 100 input views at 480P resolution on an 80GB GPU, by partitioning the views into anchor and support sets and using cross attention to compress the information from the support views into anchor views, forming the compressed latent state $Z$. We show that integrating ZPressor into several state-of-the-art feed-forward 3DGS models consistently improves performance under moderate input views and enhances robustness under dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K. The video results, code and trained models are available on our project page: https://lhmd.top/zpressor."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 4 3 7 3 2 . 5 0 5 2 : r ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS Weijie Wang1(cid:66) Donny Y. Chen2,3(cid:66) Zeyu Zhang3 Duochao Shi1 Akide Liu3 Bohan Zhuang1 1ZIP Lab, Zhejiang University 2ByteDance Seed 3Monash University"
        },
        {
            "title": "Abstract",
            "content": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as promising solution for novel view synthesis, enabling one-pass inference without the need for per-scene 3DGS optimization. However, their scalability is fundamentally constrained by the limited capacity of their encoders, leading to degraded performance or excessive memory consumption as the number of input views increases. In this work, we analyze feed-forward 3DGS frameworks through the lens of the Information Bottleneck principle and introduce ZPressor, lightweight architectureagnostic module that enables efficient compression of multi-view inputs into compact latent state that retains essential scene information while discarding redundancy. Concretely, ZPressor enables existing feed-forward 3DGS models to scale to over 100 input views at 480P resolution on an 80GB GPU, by partitioning the views into anchor and support sets and using cross attention to compress the information from the support views into anchor views, forming the compressed latent state Z. We show that integrating ZPressor into several state-of-the-art feed-forward 3DGS models consistently improves performance under moderate input views and enhances robustness under dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K. The video results, code and trained models are available on our project page: https://lhmd.top/zpressor."
        },
        {
            "title": "Introduction",
            "content": "Novel view synthesis (NVS) has played an important role in many everyday applications and is expected to become even more crucial in the future as foundational technique for augmented reality (AR) and virtual reality (VR). It has also received growing attention in the research community with the introduction of 3D Gaussian Splatting (3DGS) [1] and series of subsequent developments [26]. Although 3DGS achieves real time rendering and high visual quality, its reliance on slow per-scene tuning significantly limits its practical use in real world scenarios. To address this limitation, feed-forward 3DGS [7, 8] has been introduced to improve the usability of 3DGS. Unlike conventional 3DGS approaches that rely on slow per-scene backward optimization, feed-forward 3DGS introduces an encoder to extract scene dependent features from input images, allowing the model to benefit from large scale training and predict 3DGS in single forward pass. Despite notable progress [913], these methods remain constrained to small number of input views, limiting their ability to fully utilize datasets with dense multiple input views [1417]. For example, our experiments show that the state-of-the-art model DepthSplat [12] suffers significant performance drop and increased computational cost as input views become denser (see Tab. 1 and Fig. 1), . While better engineering might alleviate this memory issue to some extent, it cannot address the huge performance degradation. Upon examining the architecture of several representative feed-forward 1(cid:66)Corresponding authors: wangweijie@zju.edu.cn, donny.chen@bytedance.com. Preprint. Under review. Figure 1: We visualize the result of DepthSplat [12] with 36 view input for novel view synthesis and after adding ZPressor. We report PSNR, inference time, and memory usage before and after simply adding the ZPressor, where the radius of the bubble corresponds to memory. 3DGS models [7, 8, 12], we identify the limited capacity of the encoder as the root cause. By design, it struggles to scale with denser input due to information overload and high computational cost. Rather than introducing yet another ad-hoc encoder design, this work revisits the feed-forward 3DGS framework with inspiration from the Information Bottleneck (IB) [18] principle. IB offers theoretical foundation for learning compact representations that preserve only task-relevant information. In the context of NVS with dense input views, we hypothesize that latent representation can be learned to capture essential scene information while discarding redundant details in dense multi-view inputs. By encouraging the formation of such compressed yet informative representation, we aim to improve the scalability of feed-forward 3DGS models (detailed in Sec. 3.2). Building on this insight, we propose lightweight module, termed ZPressor, designed to be seamlessly integrated into the encoder of existing feed-forward 3DGS models to enhance their scalability. Unlike typical efforts that might rely on engineering optimizations such as memory-efficient attention [19] or activation checkpointing [20], our approach adopts principled perspective grounded in representation learning, aiming to address core architectural limitations under dense input views settings. To put this idea into practice, ZPressor implements the IB principle by explicitly compressing input view information, as illustrated in Fig. 2. Specifically, we divide the input views into two groups: anchor views and support views. Anchor views serve as the compression states, while information from support views is compressed into them. To ensure the compressed representation retains sufficient scene information, we select anchor views using farthest point sampling to maximize coverage with restricted views. The remaining views are assigned to their nearest anchor based on camera distance, and their features are fused into the anchors through stack of customized crossattention blocks. In essence, ZPressor takes multiple input views features and their corresponding camera poses as input and produces compact latent representation that preserves scene information. This design is architecture-agnostic and thus can be integrated into various feed-forward 3DGS models. To validate the effectiveness of ZPressor, we integrate ZPressor into several state-of-the-art feed forward 3DGS models, including pixelSplat [7], MVSplat [8], and DepthSplat [12], and conduct extensive experiments on large-scale benchmarks such as DL3DV-10K [17] and RealEstate-10K [21]. Results show that integrating ZPressor consistently boosts the performance of baseline models under moderate number of input views (e.g., 12 views), and helps them maintain reasonable accuracy and computational cost even with very dense inputs (e.g., 36 views, as shown in Fig. 1), where the original models typically degrade dramatically or run out of memory. Our contributions are threefold: 2 We provide fundamental analysis of why existing feed-forward 3DGS models struggle with dense input views, through the lens of the Information Bottleneck principle. Inspired by IB, we propose ZPressor, an architecture-agnostic module that can be integrated into the encoder of existing feed-forward 3DGS models to compress input view information. Extensive experiments on several large-scale benchmarks show that ZPressor consistently improves the performance of baseline models with moderate number of input views, and further enhances robustness under dense input settings, where existing models typically degrade significantly."
        },
        {
            "title": "2 Related Work",
            "content": "Information bottleneck and its applications. The challenge of managing and processing vast quantities of information is central theme in the development of large-scale machine learning models, particularly in the visual domain [2225]. The Information Bottleneck principle [18] formalizes the problem of extracting compressed representation from the input X, such that is maximally informative about the target . The IB principle was subsequently extended to the domain of deep learning [2628], the Deep Variational Information Bottleneck [29] providing tractable lower bound on the IB objective, bridging the gap between the theoretical IB principle and practical deep learning applications. series of works have applied the IB principle to multi-view inputs [3032], extract information that is common or shared across multiple views while discarding view-specific or redundant information. The drive towards efficient 3D scene reconstruction, especially within the context of 3DGS [1], has also seen the adoption of information-theoretic ideas. StreamGS [33] tackle redundancy in image streams by merging superfluous Gaussians through cross-frame feature aggregation, while other works [34, 35] focus on compressing the learned 3D Gaussians. While demonstrating the effectiveness and necessity of compression in multi-view data and 3D reconstruction, none of the existing works have explored it in the context of feed-forward 3DGS. Our work aims to bridge this gap by introducing the lens of IB for information compression in this area. Optimization-based NeRF and 3DGS. Traditional novel view synthesis (NVS) methods primarily rely on image blending techniques [36, 37]. More recently, neural network-based approaches [3841] have advanced NVS by integrating it with deep learning models. In particular, NeRF [41] employs an MLP to map 3D spatial locations and viewing directions to radiance color and volume density. Numerous works [4247] have sought to improve NeRFs efficiency and reconstruction quality. However, its reliance on volume rendering [48] hinders rendering speed, limiting its practicality in real-world applications. Recently, 3D Gaussian Splatting (3DGS)[1] and its variants[2, 35, 4951] have emerged as efficient solutions for large-scale scene reconstruction and synthesis, offering explicit representations and fast rasterization-based rendering that outperform NeRFs slower volumetric approach. Nonetheless, its requirement for slow per-scene optimization still poses challenges for deployment in downstream tasks. Feed-Forward NeRF and 3DGS. To address the limitation of slow per-scene optimization, PixelNeRF [52] pioneered the feed-forward NeRF (a.k.a. generalizable NeRF) by introducing an additional network that directly encodes input views into NeRF representation. This allows the model to benefit from large-scale training and predict scene representation in single forward pass. This direction has since seen significant advancements [5357], offering promising path toward practical NeRF deployment. This paradigm has recently been extended to real-time rendering with 3DGS [1] replacing NeRF [41]. Among them, pixelSplat [7] pioneered the feed-forward 3DGS approach by combining epipolar transformers with depth prediction to predict 3D Gaussians from two input views. MVSplat [8] proposed an efficient cost-volume-based fusion strategy to improve multi-view reconstruction, while DepthSplat [12] leveraged monocular depth features to better recover fine 3D structures from sparse inputs. Despite the growing number of feed-forward 3DGS models [5862], most follow pixel-aligned design, where the number of 3D Gaussians scales linearly with the number of input views. This leads to significant memory and computational overhead as input views increase. While works like FreeSplat [10] and GGN [11] attempt to reduce the number of Gaussians by merging them via cross-view projection checking, they lack principled framework. In contrast, our work provides theoretical perspective on the information overload problem in feed-forward 3DGS by introducing the IB [18] principle. And we propose an architecture-agnostic module ZPressor that can be seamlessly integrated into existing feed-forward 3DGS models to improve performance under dense input view settings. 3 Figure 2: Overview of ZPressor for Feed-Forward 3DGS. Our proposed ZPressor is plug-andplay module designed for feed-forward 3DGS frameworks. It addresses the challenge of processing dense input views by strategically grouping input view features based on selected anchor views, then features within each respective group are compressed as Z."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Overview of ZPressor Our ZPressor is plug-and-play module for compressing the multi-view inputs of feed-forward i=1 where Vi RHW 3 3DGS, as illustrated in Fig. 2. Formally, given input views = {Vi}K and their corresponding camera poses = {Pi}K i=1, our ZPressor takes the extracted features from each view as input: = {Fi}K i=1 = Φimage(V, P), Fi where Φimage is pretrained image encoder. Then our ZPressor adaptively compresses these heavy multi-view features into compact ones = ZPressor(X ). Subsequently, we directly unprojects these compact latent representations into 3D space using the camera poses and pixel-aligned Gaussian prediction network Ψpred is employed to estimate the Gaussian parameters: C (1) = {(µi, Σi, αi, ci)}HW = Ψpred(Z, P). i=1 The Gaussian parameters include mean µ, opacity α, covariance matrix Σ, and color c, while this pixel-aligned prediction results in linear increase in Gaussian primitives with more input views, constraining the models input capacity. (2) 3. Information Analysis of Feed-Forward 3DGS Existing feed-forward 3D Gaussian Splatting networks suffer from dramatic performance drop and an exponential increase in computational cost when the amount of input view information grows (see Tab. 1), primarily due to information redundancy and the lack of adaptive information compression mechanisms. Specifically, the total information of the scene is represented by the joint entropy H(F1, F2, ..., FK), which is not merely the sum of the individual entropies of the features from all views, i.e., (cid:80)K i=1 H(Fi). Therefore, there is significant amount of redundant information in the features, and it is crucial to remove the irrelevant information after feature extraction while preserving its predictive power, which allows for the efficient use of information from the input views. priciple way for modeling this is the Information Bottleneck (IB) [18], which minimizes the IB scores as min IB = β I(X , Z) (cid:125) (cid:124) (cid:123)(cid:122) Compression Score I(Z, Y) (cid:124) (cid:123)(cid:122) (cid:125) Prediction Score , (3) where β 0 controls the balance between compression and prediction, and I(, ) is the mutual information. The Compression Score component, βI(X , Z), encourages to be concise representation 4 of the input . Minimizing I(X , Z) means reducing the amount of information carries about , which leads to better compression and enhanced efficiency. The Prediction Score component, I(Z, Y), measures the predictive power of the latent feature with respect to the target variable Y. Maximizing this term ensures that retains sufficient task-relevant information about Y, which is vital for maintaining or improving prediction accuracy. As shown in Fig. 2, the Prediction Score is typically modelled by the Gaussian predictor. In the next section, we introduce ZPressor, novel module applied concatenated with the image encoder to model the Compression Score through three consecutive designs, yielding compact presentation. 3.3 Information Compression Module: ZPressor To ensure architecture-agnostic integration, compression is performed along the view dimension, rather than being entangled with the design of specific model. Concretely, given the encoded views , we first divide them into anchor views Xanchor = {Fai}N i=1 and support views Xsupport = {Fsj }M j=1, where ai, sj {1, . . . , K}, XanchorXsupport = , and XanchorXsupport = . Here, Xanchor are expected to capture the essential information of the scene, while Xsupport contain supporting context but may include redundancy. Compression is then achieved by fusing the features of support views into their corresponding anchor views. This raises three design questions: 1) how to select the anchors Xanchor, 2) how to assign Xsupport to specific anchors, and 3) how to fuse the information from Xsupport into their designated Xanchor. i=1 RK3 Anchor view selection (Fig. 2 Step 1). Given set of camera positions = {Ti}K calculated from camera parameters P, where : such that is bijective mapping, we first add random view to the anchor view list = {Ta1}, where Ta1 Uniform(T ). Subsequent anchor views are iteratively selected as the view with the greatest distance to the current anchor view set S: Tai+1 = arg max Tj S min TkS d(Tj, Tk) , (cid:18) (cid:19) (4) where d(, ) denotes the Euclidean distance. This procedure is repeated until anchors are selected, resulting in diverse and representative set Xanchor. Support-to-anchor assignment (Fig. 2 Step 2). Once anchors are selected, each support view is assigned to its nearest anchor based on camera position. This ensures that support views, which capture complementary scene details, are grouped with the most spatially relevant anchor views, thereby ensuring the effectiveness of information fusion. Formally, the cluster assignment to the i-th anchor view can be denoted as: Ci = {f (T) Xsupport Tai Taj , = i} (5) Views information fusion (Fig. 2 Step 3). Upon obtaining the anchor-support view-based clusters, we aim to fuse the information within each cluster. The fusion mechanism should satisfy the following properties: 1) use the anchor views as the base, while effectively integrating information from the support views to enhance them, and 2) capture the similarity between the two sets of views, maintaining compactness while avoiding redundancy. Building on the above analysis, the fusion within each cluster can be effectively achieved via crossattention: = Cross-Attn(Q, K, ), Xanchor, K, Xsupport, (6) where the features Xanchor extracted from the anchor views are used as queries, while the features Xsupport from the support views provide keys and values. In this way, information from the support views is effectively integrated into the anchor views, satisfying the first property. Additionally, it ensures gradient flow from the prediction to both anchor and support views, enabling the model to capture correlations between the two sets of views, thus satisfying the second property. Moreover, 5 since most existing feed-forward 3DGS encoders adopt Transformer-based architectures, the attention module can be readily integrated into these frameworks. Training. Upon obtaining the compressed information Z, we can apply the IB principle to regularize the information flow. Recall that, as in Eq. (3), our current goal should be to minimize the Compression Score I(Xanchor, Xsupport; Z) and maximize Prediction Score I(Z; Y). For the Compression Score, we apply constraint on its complexity, i.e., setting the number of anchor views to value acceptable for training. For the Prediction Score, we have I(Z; Y) = H(Y) H(Y Z), (7) where H(Y) is constant representing the 3D Gaussians that model the underlying 3D scene. Hence, maximizing I(Z; Y) is equivalent to minimizing H(Y Z), which essentially encourages the predicted 3D Gaussians to resemble the original scene. Then we incorporate the IB principle into feed-forward 3DGS training, using the efficient variational estimate of Eq. (3) following [26, 29]. Since 3DGS is typically trained by rendering and supervising on the 2D image space, we optimize the feed-forward 3DGS with ZPressor by the variational rendering loss, e.g., MSE and LPIPS. In our implementation, we append an additional self-attention layer to further enhance information flow within each cluster. In addition, we stack several blocks (each containing both crossand self-attention) to further improve the effectiveness of the IB principle. These two general-purpose engineering techniques help boost performance, as shown in Tab. 4. comprehensive description of ZPressors architecture is available in the supplementary material."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings Datasets. We validate the effectiveness of our ZPressor on the NVS task, following existing works [7, 8, 12], and conduct experiments on two large-scale datasets: DL3DV-10K (DL3DV) [17] and RealEstate10K (RE10K) [21]. DL3DV is challenging large-scale dataset that contains 51.3 million frames from 10,510 real scenes. We used 140 benchmark scenes for testing and the remaining 9896 scenes for training, with filtering applied to ensure that there is strictly no overlap between the training and test sets. RE10K offers large-scale collection of indoor home tour clips, comprising 10 million frames from around 80,000 video clips sourced from public YouTube videos. It is split into 67,477 training and 7,289 testing scenes. Both datasets feature real-world captured scenes, with camera intrinsics and extrinsics reconstructed using COLMAP [63, 64]. Baselines and metrics. To evaluate the effectiveness and flexibility of our proposed ZPressor, we integrate it as module into three representative baselines, including DepthSplat [12], MVSplat [8], and pixelSplat [7]. For fair comparisons, we insert ZPressor into the official implementations of each baseline and strictly follow the experimental settings described in their respective papers. Specifically, we compare with DepthSplat on DL3DV, following its training strategy by first pre-training on RE10K and then fine-tuning on DL3DV. Comparisons with MVSplat and pixelSplat are conducted on RE10K. Following these baselines, we report quantitative results using PSNR, SSIM [65], and LPIPS [66]. As our module focuses on information compression, we additionally report model efficiency in terms of runtime and memory consumption. Implementation details. We use the same computing resources to train the baseline and our method. Due to the memory limit, we use 6 context views for DepthSplat and MVSplat, and 4 context views for pixelSplat. For all of our experiments, we adopted the same learning rate as the baseline, utilized the AdamW optimizer, and trained the models for 100,000 steps on A800 GPUs. Following the setting of the baseline, we use the 256 256 input resolution on RE10K, and 256 448 input resolution on DL3DV. All training losses match those of the baseline, with no additional data or regularization introduced. More details are provided in Appendix B. 4.2 SoTA Comparisons and IB Analysis Comparisons with SoTA models. We train all models on DL3DV and RE10K using 12 input views with 6 anchor views set to our ZPressor, and evaluate them under varying numbers of input views ranging from 8 to 36. As shown in Tab. 1 and Tab. 2, integrating ZPressor into DepthSplat, MVSplat, 6 Table 1: Quantitative comparisons on DL3DV [17]. We evaluate both DepthSplat [12] and DepthSplat with ZPressor with 12, 16, 24, 36 input views and test on eight target novel views. Views Methods PSNR SSIM LPIPS 36 views 24 views 16 views 12 views DepthSplat DepthSplat + ZPressor DepthSplat DepthSplat + ZPressor DepthSplat DepthSplat + ZPressor DepthSplat DepthSplat + ZPressor 19.23 23.88+4.65 20.38 24.26+3.88 22.07 24.25+2. 23.32 24.30+0.97 0.666 0.815+0.149 0.711 0.820+0.109 0.773 0.819+0.046 0.807 0.821+0.014 0.286 0.150-0. 0.253 0.147-0.106 0.195 0.147-0.047 0.162 0.146-0.017 Table 2: Quantitative comparisons on RE10K [21]. We test pixelSplat [7] and MVSplat [8] on eight target views, \"OOM\" represent that model cannot infer on an 80G GPU. Views Methods PSNR SSIM LPIPS 36 views 24 views 16 views 8 views pixelSplat pixelSplat + ZPressor MVSplat MVSplat + ZPressor pixelSplat pixelSplat + ZPressor MVSplat MVSplat + ZPressor pixelSplat pixelSplat + ZPressor MVSplat MVSplat + ZPressor pixelSplat pixelSplat + ZPressor MVSplat MVSplat + ZPressor OOM 26.59 24.19 27.34+3. OOM 26.72 25.00 27.49+2.49 OOM 26.81 25.86 27.60+1.74 26.19 26.86+0.67 26.94 27.72+0.78 OOM 0.849 0.851 0.893+0.042 OOM 0.851 0.871 0.895+0.024 OOM 0.853 0.888 0.896+0. 0.852 0.854+0.002 0.902 0.897-0.005 OOM 0.225 0.155 0.113-0.042 OOM 0.223 0.137 0.111-0.026 OOM 0.221 0.120 0.110-0.010 0.215 0.219+0.004 0.107 0.109+0.002 and pixelSplat consistently improves their performance across all input view settings and evaluation metrics, demonstrating the effectiveness of our approach. Notably, the performance gain becomes more significant as the number of input views increases. This is because existing feed-forward 3DGS models struggle with dense inputs due to information overload, leading to performance degradation. In contrast, ZPressor mitigates this issue by compressing the input through redundancy suppression while preserving essential information, improving model robustness, and maintaining strong performance under dense input settings. Moreover, we observe that pixelSplat fails to run with more than 8 input views due to out-of-memory (OOM) caused by the large number of predicted pixel-aligned 3D Gaussians. In contrast, our ZPressor helps merge input information, reducing the number of predicted Gaussians and enabling testing with up to 36 views. These observations are further supported by the qualitative comparisons shown in Fig. 3 and Fig. 4, where DepthSplat and MVSplat exhibit noticeable artifacts under 36 input views, whereas their ZPressor-augmented versions produce significantly cleaner renderings. More results are provided in Appendix D. Analysis of model efficiency. Compressing the input view information not only improves robustness and performance, but also enhances efficiency. To validate this, we compare the model efficiency of DepthSplat with and without ZPressor under 480P resolution, evaluating the number of 3D Gaussians, the test-time inference latency, and the peak memory usage. As shown in Fig. 5, the benefits of integrating ZPressor are clear. In particular, as the number of context views increases, the baseline models predicted 3D Gaussian numbers, memory usage, and inference time all grow linearly, whereas ZPressor helps maintain stable resource consumption across all aspects. Analysis of the bottleneck constraint. In the context of NVS, the information corresponds to the overall region that the scene covers. Since we evaluated on the static video data DL3DV, longer sequences usually cover larger regions. Therefore, we use the frame distance between input views as proxy for both scene coverage and information content. 7 [12] Figure 3: Qualitative comparison on DL3DV [17] under dense input conditions (36 views). DepthSplat [12] performs poorly due to redundancy in dense views, ZPressor effectively compresses this information, achieving significantly improved visual results. [8] Figure 4: Qualitative comparison on RE10K [21] with 36 input views. MVSplat [8] with ZPressor performs the best in all cases. We can then analysis the effect of bottleneck constraint (the number of anchor views) under scenes with varying information content by adjusting the frame distance between input views. As shown in Fig. 6, we conduct experiments under two settings, Context Gap 50 (CG50, in blue) and Context Gap 100 (CG100, in pink), where context gap refers to the frame distance between input views. CG100 thus contains more information than CG50. Under CG50, increasing the number of anchor views from 7 to 9 leads to performance drop, suggesting that 7 clusters are sufficient and additional ones introduce redundancy. In contrast, under CG100, increasing anchors from 7 to 9 improves performance,indicating higher information bottleneck for scenes with more information. 8 Figure 5: Efficiency analysis. We report the number of Gaussians (K), inference time (ms) and peak memory (GB) of DepthSplat [12] and DepthSplat with ZPressor. Table 3: Analysis of Information Fusion. default denotes our setting where support views are fused into anchor views. w/o fusion removes the fusion step, and fuse anchors fuses repeated anchor views instead. default performs best, indicating that ZPressor improves performance by effectively fusing complementary information from the support views. Method PSNR SSIM LPIPS default fuse anchors w/o fusion 24.30 24.23 23. 0.821 0.817 0.810 0.146 0.148 0.162 Figure 6: Analysis of the bottleneck constraint. We compare the performance of ZPressor in different scale of scene coverage. These results highlight the effectiveness of our ZPressor in implementing the IB principle and show that the information bottleneck is critical in balancing compression and information preservation. Analysis of information fusion. To further confirm that our ZPressor effectively fuses information from support views into anchor views rather than simply discarding it, we conduct experiments by varying the fusion strategy. As shown in Tab. 3, removing the information fusion step (w/o fusion) leads to performance drop, highlighting the importance of fusing support views into anchor views. To ensure that the introduction of support information does result in performance improvement, we conduct control experiment by fusing repeated anchor views instead of support views (fuse anchors). Since this does not introduce new information, its performance is lower than our default setting, which achieves the best results by fusing complementary information. These comparisons further validate that our design effectively implements the IB principle. 4.3 Ablation Study As mentioned at the end of Sec. 3, the default ZPressor uses several stacked attention blocks, each combining self-attention and cross-attention. This section presents an ablation study to validate our design. We report results on DL3DV with 12 input views, following the setting in Tab. 1, using DepthSplat as the backbone. As shown in Tab. 4, removing the stacking design and using only one block (w/o multi-blocks) slightly degrades performance, suggesting that stacking improves fusion of information from support to anchor views. Additionally, removing the self-attention (w/o self attention) also reduce performance, showing that self-attention complements cross-attention by enhancing internal feature interactions. Overall, all variants of ZPressor outperform the DepthSplat baseline, confirming the existence of an information bottleneck in feed-forward 3DGS models and the effectiveness of our ZPressor in addressing it. 9 Table 4: Ablation study of our method with DepthSplat [12] on the DL3DV dataset [17]. Models are evaluated by rendering eight novel views using 12 input views. Methods PSNR SSIM LPIPS Time (s) Peak Memory (GB) DepthSplat + ZPressor w/o multi-blocks w/o self-attention DepthSplat 24.30 24.18 23.85 23.32 0.821 0.817 0.810 0.808 0.146 0.149 0.156 0. 0.184 0.140 0.183 0.260 3.80 3.79 3.80 6."
        },
        {
            "title": "5 Conclusion\nWe have provided a fundamental analysis of the model capacity limitations in existing feed-forward\n3DGS models through the lens of the Information Bottleneck principle. Building on this insight,\nwe introduced ZPressor, a lightweight, architecture-agnostic module that efficiently compresses\nmulti-view inputs, enabling models to overcome inherent limitations and scale to handle more input\nviews. We validated our ZPressor by integrating it into several representative feed-forward 3DGS\nmodels. Our experiments on several large-scale benchmarks demonstrate that ZPressor not only\nconsistently improves the performance of existing models under moderate view settings, but also\nhelps them maintain competitive efficiency under denser inputs. We believe that ZPressor significantly\nenhances the scalability and practicality of feed-forward 3DGS models, opening the door to more\neffective applications in real-world scenarios.",
            "content": "Limitation and discussion. Our ZPressor may be less effective in extremely dense view settings. For example, given 1000 input views, ZPressor can only compress them to around 50 views in order to maintain the information compactness as regularized by the IB principle. However, handling 50 views of 3D Gaussians still presents significant computational challenges for typical GPUs. Future work could explore combining ZPressor with 3D Gaussian merging or memory-efficient rendering to extend feed-forward 3DGS to handle extremely dense input views."
        },
        {
            "title": "References",
            "content": "[1] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [2] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 conference papers, pages 111, 2024. [3] Nicolas Moenne-Loccoz, Ashkan Mirzaei, Or Perel, Riccardo de Lutio, Janick Martinez Esturo, Gavriel State, Sanja Fidler, Nicholas Sharp, and Zan Gojcic. 3d gaussian ray tracing: Fast tracing of particle scenes. ACM Transactions on Graphics (TOG), 43(6):119, 2024. [4] Alexander Mai, Peter Hedman, George Kopanas, Dor Verbin, David Futschik, Qiangeng Xu, Falko Kuester, Jonathan Barron, and Yinda Zhang. Ever: Exact volumetric ellipsoid rendering for real-time view synthesis. CoRR, 2024. [5] Jorge Condor, Sebastien Speierer, Lukas Bode, Aljaz Bozic, Simon Green, Piotr Didyk, and Adrian Jarabo. Dont splat your gaussians: Volumetric ray-traced primitives for modeling and rendering scattering and emissive media. ACM Transactions on Graphics, 2025. [6] Shrisudhan Govindarajan, Daniel Rebain, Kwang Moo Yi, and Andrea Tagliasacchi. Radiant foam: Real-time differentiable ray tracing. arXiv preprint arXiv:2502.01157, 2025. [7] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In CVPR, pages 1945719467, 2024. [8] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In ECCV, pages 370386. Springer, 2024. [9] Christopher Wewer, Kevin Raj, Eddy Ilg, Bernt Schiele, and Jan Eric Lenssen. latentsplat: Autoencoding variational gaussians for fast generalizable 3d reconstruction. In European Conference on Computer Vision, pages 456473. Springer, 2024. [10] Yunsong Wang, Tianxin Huang, Hanlin Chen, and Gim Hee Lee. Freesplat: Generalizable 3d gaussian splatting towards free view synthesis of indoor scenes. NeurIPS, 37:107326107349, 2024. [11] Shengjun Zhang, Xin Fei, Fangfu Liu, Haixu Song, and Yueqi Duan. Gaussian graph network: Learning efficient and generalizable gaussian representations from multi-view images. NeurIPS, 37:5036150380, 2024. [12] Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, and Marc Pollefeys. Depthsplat: Connecting gaussian splatting and depth. In CVPR, 2025. [13] Chen Ziwen, Hao Tan, Kai Zhang, Sai Bi, Fujun Luan, Yicong Hong, Li Fuxin, and Zexiang Xu. Long-lrm: Long-sequence large reconstruction model for wide-coverage gaussian splats. arXiv preprint arXiv:2410.12781, 2024. [14] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In ICCV, pages 1090110911, 2021. [15] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In ICCV, pages 1222, 2023. [16] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: large-scale dataset of multi-view images. In CVPR, pages 91509161, 2023. [17] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In CVPR, pages 2216022169, 2024. [18] Naftali Tishby, Fernando Pereira, and William Bialek. The information bottleneck method. arXiv preprint physics/0004057, 1999. [19] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In ICLR, 2023. [20] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. CoRR, 2016. [21] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: learning view synthesis using multiplane images. ACM Transactions on Graphics (TOG), 37(4):112, 2018. [22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [23] Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers, 2023. [24] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. [25] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. [26] Yonatan Belinkov, James Henderson, et al. Variational information bottleneck for effective low-resource fine-tuning. In ICLR, 2020. [27] Bin Dai, Chen Zhu, Baining Guo, and David Wipf. Compressing neural networks using the variational information bottleneck. In Jennifer Dy and Andreas Krause, editors, ICML, volume 80, pages 11351144. PMLR, 2018. [28] Jinkyu Kim and Mayank Bansal. Attentional bottleneck: Towards an interpretable deep driving network. In CVPR (CVPR) Workshops, June 2020. [29] Alexander Alemi, Ian Fischer, Joshua Dillon, and Kevin Murphy. Deep variational information bottleneck. In ICLR, 2017. [30] Marco Federici, Anjan Dutta, Patrick Forré, Nate Kushman, and Zeynep Akata. Learning robust representations via multi-view information bottleneck. In ICLR, 2020. [31] Qi Zhang, Mingfei Lu, Shujian Yu, Jingmin Xin, and Badong Chen. Discovering common information in multi-view data. Information Fusion, 108:102400, 2024. [32] Yuhan Xie, Yixi Cai, Yinqiang Zhang, Lei Yang, and Jia Pan. Gauss-mi: Gaussian splatting shannon mutual information for active 3d reconstruction, 2025. [33] Yang LI, Jinglu Wang, Lei Chu, Xiao Li, Shiu hong Kao, Ying-Cong Chen, and Yan Lu. Streamgs: Online generalizable gaussian splatting reconstruction for unposed image streams, 2025. [34] Xiangrui Liu, Xinju Wu, Pingping Zhang, Shiqi Wang, Zhu Li, and Sam Kwong. Compgs: Efficient 3d scene representation via compressed gaussian splatting. In ACMMM, pages 29362944, 2024. [35] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, Zhangyang Wang, et al. Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps. NeurIPS, 37:140138 140158, 2024. 12 [36] Shenchang Eric Chen and Lance Williams. View interpolation for image synthesis. In Proceedings of the 20th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 93, page 279288, New York, NY, USA, 1993. Association for Computing Machinery. [37] Steven Seitz and Charles Dyer. View morphing. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 2130, 1996. [38] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes: learning dynamic renderable volumes from images. ACM Transactions on Graphics (TOG), 38(4):114, 2019. [39] Vincent Sitzmann, Michael Zollhöfer, and Gordon Wetzstein. Scene representation networks: Continuous 3d-structure-aware neural scene representations. NeurIPS, 32, 2019. [40] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. NeurIPS, 34:1931319325, 2021. [41] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [42] Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Menglei Chai, Yun Fu, and Sergey Tulyakov. R2l: Distilling neural radiance field to neural light field for efficient novel view synthesis. In ECCV, 2022. [43] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In CVPR, 2022. [44] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised nerf: Fewer views and faster training for free. In CVPR, pages 1288212891, 2022. [45] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):115, 2022. [46] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European conference on computer vision, pages 333350. Springer, 2022. [47] Junyi Cao, Zhichao Li, Naiyan Wang, and Chao Ma. Lightning nerf: Efficient hybrid scene representation for autonomous driving. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1680316809. IEEE, 2024. [48] James Kajiya and Brian Von Herzen. Ray tracing volume densities. ACM SIGGRAPH computer graphics, 18(3):165174, 1984. [49] Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, et al. Instantsplat: Unbounded sparse-view pose-free gaussian splatting in 40 seconds. CoRR, 2024. [50] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffoldgs: Structured 3d gaussians for view-adaptive rendering. In CVPR, pages 2065420664, 2024. [51] Sharath Girish, Kamal Gupta, and Abhinav Shrivastava. Eagles: Efficient accelerated 3d gaussians with lightweight encodings. In European Conference on Computer Vision, pages 5471. Springer, 2024. [52] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In CVPR, pages 45784587, 2021. [53] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering. In CVPR, pages 46904699, 2021. 13 [54] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In ICCV, pages 1412414133, 2021. [55] Mohammad Mahdi Johari, Yann Lepoittevin, and François Fleuret. Geonerf: Generalizing nerf with geometry priors. In CVPR, pages 1836518375, 2022. [56] Yuedong Chen, Haofei Xu, Qianyi Wu, Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. Explicit correspondence matching for generalizable neural radiance fields. arXiv preprint arXiv:2304.12294, 2023. [57] Haofei Xu, Anpei Chen, Yuedong Chen, Christos Sakaridis, Yulun Zhang, Marc Pollefeys, Andreas Geiger, and Fisher Yu. Murf: multi-baseline radiance fields. In CVPR, pages 2004120050, 2024. [58] Xin Fei, Wenzhao Zheng, Yueqi Duan, Wei Zhan, Masayoshi Tomizuka, Kurt Keutzer, and Jiwen Lu. Pixelgaussian: Generalizable 3d gaussian reconstruction from arbitrary views, 2024. [59] Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang, and Songyou Peng. No pose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images. arXiv preprint arXiv:2410.24207, 2024. [60] Zhiyuan Min, Yawei Luo, Jianwen Sun, and Yi Yang. Epipolar-free 3d gaussian splatting for generalizable novel view synthesis. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, NeurIPS, volume 37, pages 3957339596. Curran Associates, Inc., 2024. [61] Gyeongjin Kang, Jisang Yoo, Jihyeon Park, Seungtae Nam, Hyeonsoo Im, Sangheon Shin, Sangpil Kim, and Eunbyung Park. Selfsplat: Pose-free and 3d prior-free generalizable 3d gaussian splatting, 2025. [62] Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, and Jianfei Cai. Mvsplat360: Feed-forward 360 scene synthesis from sparse views. NeurIPS, 37:107064107086, 2024. [63] Johannes Lutz Schönberger and Jan-Michael Frahm. Structure-from-motion revisited. Conference on Computer Vision and Pattern Recognition (CVPR), 2016. In [64] Johannes Lutz Schönberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In ECCV (ECCV), 2016. [65] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [66] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, pages 586595, 2018. [67] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from single image. In ICCV, pages 1445814467, 2021. [68] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International conference on machine learning, pages 1052410533. PMLR, 2020. [69] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. NeurIPS, 35:1634416359, 2022. [70] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019."
        },
        {
            "title": "A More Experimental Analysis",
            "content": "Cross dataset generalization. Following MVSplat [8], we conducted experiments using pretrained model on the RealEstate10K (RE10K) dataset [21] (as detailed in Tab. 2) and tested its performance on the ACID dataset [67] to evaluate the generalization capabilities of our proposed ZPressor across diverse datasets. As demonstrated in Table A, MVSplat with ZPressor exhibits remarkable efficacy in cross-dataset generalization. Notably, this performance advantage becomes progressively more pronounced with an increasing number of input views. Table A: Quantitative comparison on ACID [67] with trained model on RE10K. Trained on indoor scenes (RE10K), MVSplat [8] and pixelSplat [7] with ZPressor perform much better as evaluated on the ACID dataset."
        },
        {
            "title": "Methods",
            "content": "PSNR SSIM LPIPS 36 views 24 views 16 views 8 views pixelSplat pixelSplat + Ours MVSplat MVSplat + Ours pixelSplat pixelSplat + Ours MVSplat MVSplat + Ours pixelSplat pixelSplat + Ours MVSplat MVSplat + Ours pixelSplat pixelSplat + Ours MVSplat MVSplat + Ours OOM 27.78 24.89 28.16+3. OOM 27.91 25.46 28.33+2.87 OOM 27.97 26.08 28.42+2.34 26.69 28.05+1.36 27.89 28.60+0.71 OOM 0.823 0.812 0.853+0.041 OOM 0.825 0.829 0.856+0.027 OOM 0.826 0.844 0.858+0. 0.807 0.828+0.021 0.864 0.860-0.004 OOM 0.238 0.179 0.145-0.034 OOM 0.235 0.167 0.142-0.025 OOM 0.234 0.156 0.141-0.015 0.260 0.234-0.026 0.140 0.140-0."
        },
        {
            "title": "B More Implementation Details",
            "content": "Network architectures. In Algorithm 1, we provide detailed description of how ZPressor is integrated into existing feed-forward 3D Gaussian Splatting (3DGS) frameworks [7, 8, 12]. Initially, we select anchor views and their corresponding support views following Algorithm 2 and Eq. (5). The features associated with these views are then processed by an attention-based network. This network is composed of 6 structurally identical blocks, wherein each block encompasses cross-attention layer, self-attention layer, and an MLP layer. The cross-attention mechanism operates by employing the anchor features as query, while the support features provide the key and value. Subsequent to this fusion, the resulting features are further refined by the self-attention and MLP layers. To ensure training stability, deviating from traditional Transformer architectures, we employ PreLayer Normalization [68] (Pre-LN), which enhances the robustness of the model. Furthermore, system-level advancements have been incorporated to accelerate computation. For example, we employ FlashAttention [19, 69], which uses highly optimized GPU kernels and leverages hardware topology to compute attention in timeand memory-efficient manner. More training details. We use the first model version of DepthSplat [12] from its October 2024 release. Experimental results obtained with this specific version may exhibit slight variations when compared to the current version. ZPressor was incorporated subsequent to the monocular feature extraction performed by CNN. Adhering to its original configuration, experiments were conducted at resolution of 256 448. The model was initially trained on the RE10K [21] for 100,000 steps and subsequently fine-tuned on the DL3DV [17] for an additional 100,000 steps. We employed the AdamW optimizer [70] with 15 Algorithm 1 Overview of Feed-Forward 3DGS framework with ZPressor Input: input views = {Vi}K the number of network blocks h. i=1, camera poses = {Pi}K i=1, the number of anchor views , Output: Gaussian parameters = {(µ, Σ, α, c)}. Φimage(V, P) Xanchor, Xsupport , with Anchor view selection. Assign support views to anchor cluster Xsupport Initialize state Xanchor for 1 to do Cross-Attn(Q, K, ), where K, Xsupport Self-Attn(Q, K, ), where Q, K, MLP(Z) end for {(µi, Σi, αi, ci)} Ψpred(Z, P) return {(µi, Σi, αi, ci)} Algorithm 2 Farthest Point Sampling for Anchor View Selection Input: Set of view camera positions = {T1, T2, ..., TK}, Number of anchor views Output: Indices of the selected anchor views = {Ta1, Ta2, ..., Tan } Initialize the set of anchor view indices Randomly select random anchor view Ta1 , where Ta1 Uniform(T ) Add Ta1 to S: {Ta1 } for 2 to do Initialize dictionary to store minimum distances {} for 1 to do if / then Calculate the minimum distance dk miniS Tk Ti2 Store the distance: D[k] dk end if end for Find the view position Taj with the maximum minimum distance: Taj arg maxk /S D[k] Add aj to S: {Taj } end for return learning rate of 2 104. The total training duration was approximately two days, and the integration of ZPressor did not significantly alter the original training time of DepthSplat. Similarly, for MVSplat [8] and pixelSplat [7], ZPressor was integrated after the monocular feature extraction stage. MVSplat utilizes CNN for feature extraction, whereas pixelSplat employs DINO [24, 25]; this architectural choice in pixelSplat contributes to marginally higher VRAM consumption compared to the other two baselines. We maintained the model parameter settings as published in their respective original works, training models on the RE10K [21] at resolution of 256 256. The learning rate was set to 2 104 for MVSplat and 1.5 104 for pixelSplat, where both of which were trained for 100,000 steps. Notably, due to memory constraints, we trained the pixelSplat model incorporating ZPressor using 4 anchor views, in contrast to the 6 anchor views configured for DepthSplat and MVSplat. The training times for MVSplat and pixelSplat, when augmented with ZPressor, remained comparable to their original durations. We will open-source the complete codebase for ZPressor, our ZPressor-integrated versions of DepthSplat, MVSplat, and pixelSplat, and all associated model checkpoints."
        },
        {
            "title": "C Limitation and Societal Impacts",
            "content": "Limitation analysis. As discussed in Sec. 5, ZPressor exhibits limitations when processing scenarios with an extremely high density of input views. Specifically, its efficacy in compressing the information 16 Figure A: Limitations. Visual results from extremely dense input views show slightly poor presentation effect. from such dense views through limited set of anchor views is diminished. To illustrate this, we conducted an experiment on DepthSplat [12] integrated with ZPressor, using approximately 500 images as input. As depicted in Fig. A, the quality of the rendered novel views was perceptibly affected, which can be attributed to an insufficient number of Gaussian primitives to adequately represent the scene under these dense input conditions. Potential and negative societal impacts. ZPressor can significantly reduce the training costs associated with feed-forward 3DGS networks. It enables the processing of larger number of input views within the same VRAM budget and training duration, delivering high-fidelity rendering results and thereby decreasing energy consumption during the model training process. While the capability to render higher-quality novel views from more densely sampled perspectives positions ZPressor as valuable tool for augmented reality applications, it is important to acknowledge that the fidelity of the rendering can be compromised by the emergence of artifacts, particularly when processing input views of extremely high density. Consequently, in safety-critical applications, such as the training of autonomous driving models, the deployment of ZPressor would necessitate the implementation of additional precautionary measures to mitigate potential risks arising from such limitations."
        },
        {
            "title": "D More Visual Comparisons",
            "content": "This section provides additional qualitative comparison results. We present further visualizations for DepthSplat [12] on the DL3DV [17] and MVSplat [8] on the RE10K [21] in Fig. and Fig. F, with our ZPressor. Furthermore, to illustrate how ZPressor performs with dense input views, we showcase comparative results. For DepthSplat [12], comparisons between the original framework and DepthSplat augmented with ZPressor are presented for scenarios with 24, 16, and 12 input views in Fig. C, Fig. D, and Fig. E. Similarly, for MVSplat [8], visual comparisons between the original framework and MVSplat integrated with ZPressor are displayed for inputs of 24, 16, and 8 views in Fig. G, Fig. H, and Fig. I. The corresponding quantitative results for these multi-view experiments can be found in Tab. 2. 17 [12] Figure B: More qualitative comparisons on DL3DV [17] with DepthSplat [12] under 36 input views. Models with ZPressor performs the best in all cases. [12] Figure C: More qualitative comparisons on DL3DV [17] with DepthSplat [12] under 24 input views. 18 [12] Figure D: More qualitative comparisons on DL3DV [17] with DepthSplat [12] under 16 input views. [12] Figure E: More qualitative comparisons on DL3DV [17] with DepthSplat [12] under 12 input views. 19 [8] Figure F: More qualitative comparisons on RE10K [21] with MVSplat [8] under 36 input views. Models with ZPressor performs the best in all cases. 20 Figure G: More qualitative comparisons on RE10K [21] with MVSplat [8] under 24 input views. [8] 21 [8] Figure H: More qualitative comparisons on RE10K [21] with MVSplat [8] under 16 input views. [8] Figure I: More qualitative comparisons on RE10K [21] with MVSplat [8] under 8 input views."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Monash University",
        "ZIP Lab, Zhejiang University"
    ]
}