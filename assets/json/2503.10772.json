{
    "paper_title": "FlowTok: Flowing Seamlessly Across Text and Image Tokens",
    "authors": [
        "Ju He",
        "Qihang Yu",
        "Qihao Liu",
        "Liang-Chieh Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Bridging different modalities lies at the heart of cross-modality generation. While conventional approaches treat the text modality as a conditioning signal that gradually guides the denoising process from Gaussian noise to the target image modality, we explore a much simpler paradigm-directly evolving between text and image modalities through flow matching. This requires projecting both modalities into a shared latent space, which poses a significant challenge due to their inherently different representations: text is highly semantic and encoded as 1D tokens, whereas images are spatially redundant and represented as 2D latent embeddings. To address this, we introduce FlowTok, a minimal framework that seamlessly flows across text and images by encoding images into a compact 1D token representation. Compared to prior methods, this design reduces the latent space size by 3.3x at an image resolution of 256, eliminating the need for complex conditioning mechanisms or noise scheduling. Moreover, FlowTok naturally extends to image-to-text generation under the same formulation. With its streamlined architecture centered around compact 1D tokens, FlowTok is highly memory-efficient, requires significantly fewer training resources, and achieves much faster sampling speeds-all while delivering performance comparable to state-of-the-art models. Code will be available at https://github.com/bytedance/1d-tokenizer."
        },
        {
            "title": "Start",
            "content": "FlowTok: Flowing Seamlessly Across Text and Image Tokens Ju He1 Qihang Yu1 Qihao Liu2 Liang-Chieh Chen1 1 ByteDance Seed 2 Johns Hopkins University https://tacju.github.io/projects/flowtok.html 5 2 0 2 3 1 ] . [ 1 2 7 7 0 1 . 3 0 5 2 : r Figure 1. Text-to-Image Generation Results by FlowTok. FlowTok projects both text and images into unified, compact 1D latent space, enabling direct flow matching between 1D tokens and facilitating the efficient generation of diverse, high-fidelity images."
        },
        {
            "title": "Abstract",
            "content": "Bridging different modalities lies at the heart of crossmodality generation. While conventional approaches treat the text modality as conditioning signal that gradually guides the denoising process from Gaussian noise to the target image modality, we explore much simpler paradigmdirectly evolving between text and image modalities through flow matching. This requires projecting both modalities into shared latent space, which poses significant challenge due to their inherently different representations: text is highly semantic and encoded as 1D tokens, whereas images are spatially redundant and represented as 2D latent embeddings. To address this, we introduce FlowTok, minimal framework that seamlessly flows across text and images by encoding images into compact 1D token representation. Compared to prior methods, this design reduces the latent space size by 3.3 at an image resolution of 256, eliminating the need for complex conditioning mechanisms or noise scheduling. Moreover, FlowTok naturally extends to image-to-text generation under the same formulation. With its streamlined architecture centered around compact 1D tokens, FlowTok is highly memory-efficient, requires significantly fewer training resources, and achieves much faster sampling speedsall while delivering performance comparable to state-of-the-art models. Code will be available at https://github.com/bytedance/1d-tokenizer. 1 Figure 2. Text as Conditions vs. Direct Flow between Modalities. Top: Conventional text-to-image generation relies on the diffusion process, where text serves as conditioning signal to guide the denoising process. Bottom: The proposed FlowTok enables direct flow between text and image modalities by projecting both into shared, compact 1D latent space, facilitating seamless generation of both. 1. Introduction Bridging different modalities is essential for comprehending the diverse forms of data that represent our world, encompassing both understanding and generation. In multimodal understanding, extensive research has focused on designing architectures that project different modalities into shared latent space [6, 21, 25, 38, 39, 47, 48, 56, 65, 83]. These approaches have significantly advanced cross-modal representation learning and real-world understanding by leveraging common latent space between modalities. In contrast, multimodal generation (e.g., text-to-image generation) follows different paradigm, primarily relying on the diffusion process [37, 54, 61, 73, 74], where the source modality (e.g., text) serves as conditioning signal to guide the denoising process. Various conditioning mechanisms have been explored, including concatenation [12], cross-attention [17, 69], conditioning embeddings [61], and hybrid strategies [27, 41]. While effective, these approaches introduce substantial complexity, requiring intricate conditioning mechanisms and noise scheduling. This naturally raises an important question: Can we unify multimodal understanding and generation by enabling direct transitions within shared latent space? To address this question, we revisit flow matching [7, 51, 55]a modern generative framework that learns direct path from noise to data, enabling faster convergence and accelerated sampling, leading to state-of-the-art multimodal generation results [27, 43]. Unlike diffusion models, flow matching is not constrained to using noise as the source distribution; instead, it only requires the source and target distributions to share the same shape. Pioneering works [7, 52, 72, 80, 96] have demonstrated its effectiveness in learning direct mappings within the same modality (e.g., image-to-image generation). Meanwhile, CrossFlow [53] extends flow matching to cross-modal learning by mapping text into 2D latent space to match the shape of image embeddings, paving the way for new possibilities. However, while this approach simplifies the overall pipeline, it still operates on 2D latent representations. As result, the additional computational overhead introduced by the text variational autoencoder [42] in CrossFlow makes it slower than modern text-to-image diffusion models like SD1.5 and SD2.1 [69], ultimately contradicting its original goal of efficiency. To this end, we introduce FlowTok, minimal framework that enables seamless Flowing of Tokens across text and imagethe two most prevalent modalities  (Fig. 2)  . At the core of FlowTok, both text and images are encoded into compact 1D latent tokens within unified space, enabling direct flow matching between them. On the text side, FlowTok employs pre-trained text encoder [65] to extract initial 1D text embeddings. Since these embeddings typically reside in higher-dimensional space than image latents, FlowTok introduces lightweight text projector to map text embeddings into low-dimensional variational latent space. On the image side, FlowTok builds on recent advancements in image tokenization [41, 92] to encode images into compact 1D latent tokens. Specifically, we enhance TA-TiTok [41] by integrating RoPE [76] and SwiGLU FFN [71], improving positional information handling and reconstruction quality. 2 To enable direct flow matching, we align the number of image latent tokens in TA-TiTok to match the text encoders output sequence length (K = 77 for CLIP text encoder). By integrating these simple yet effective designs across text and image modalities, FlowTok represents both in the same 1D low-dimensional space with shape 77 16 (77 tokens, each with 16 dimensions). This compact representation is 3.3 smaller than typical 2D flow matching shapes [75] of 32 32 4 for image resolutions of 256. This alignment enables fast, direct flow matching and seamless evolution between the two modalities. Unlike standard flow matching models [9, 51, 55], FlowTok eliminates the need for intricate conditioning mechanisms, offering fully self-attention-based generative model. This allows for direct flow across modalities without additional complexity. Unlike CrossFlow [53], which converts text into 2D embeddings, FlowTok retains the 1D structure of text embeddings, avoiding the need for flattening and transformation into 2D. This simplifies the framework while eliminating reliance on heavy parametric contrastive losses for semantic preservation. As result, FlowTok offers streamlined and resourceefficient training process. Its largest variant, FlowTok-H (1.1B), supports batch size of 8K on 8 A100 GPUs without requiring gradient checkpointing or gradient accumulation. In contrast, recent text-to-image models of similar scale typically require 32 to 64 A100 GPUs to train with batch size of only 2K [17, 69]. Moreover, FlowTok converges significantly faster, as shown in Fig. 3a, with FlowTok-H completing training in just 26.1 8-A100 daysfar less than SD 2.1 [69], which requires 1041.6 8-A100 GPU days. Inference is also highly efficient, with FlowTok achieving over 10 the throughput of Show-o [85] and CrossFlow [54], as shown in Fig. 3b. This dramatically reduces computational costs, making text-to-image research far more accessible. To ensure full reproducibility, we train FlowTok exclusively on publicly available datasets, avoiding reliance on high-quality proprietary data. Remarkably, despite its minimalist design and reduced data requirements, FlowTok achieves state-of-the-art text-to-image performance. Beyond text-to-image generation, FlowTok seamlessly extends to image-to-text generation, maintaining strong performance under the same minimalist framework. We believe our work establishes strong foundation for future research in generalized cross-modality generation. To support further advancements, all code will be released. 2. Related Work Flow Matching. Flow matching [7, 51, 55] models generative processes by constructing transport map between two distributions via an ordinary differential equation (ODE). It has recently gained traction as the foundation for stateof-the-art text-to-image and text-to-video synthesis mod- (a) FID vs. Training Costs. (b) FID vs. Inference Speed. Figure 3. COCO Results. FlowTok presents comparable performance to previous methods on COCO while significantly reducing training resource requirements (Fig. 3a) and achieving much faster sampling speed (Fig. 3b). This efficiency stems from its minimalist design centered around 1D tokens, which facilitates direct transformation between text and image modalities, leading to superior performance with enhanced computational efficiency. We note that the compared CrossFlow [53] uses high-quality proprietary data. els [19, 27, 63, 67], offering faster training and sampling compared to conventional diffusion methods [37, 54, 61]. Several works further optimize flow trajectories by minimizing curvature [45, 64, 79]. Despite its theoretical flexibility in handling arbitrary distributions, recent approaches primarily evolve noise into target distributions, often relying on complex control signal conditioning, which complicates the pipeline and overlooks the potential of directly transforming control signals into target distributions. In contrast, only few works [7, 52, 55, 72, 80, 96] explore direct transport within the same modality (e.g., image-to-image [29, 55, 96]), leaving cross-modal transport (e.g., text-to-image) underexplored. In this work, we introduce FlowTok, minimal yet effective framework that enables seamless flow across text and image modalities using 1D tokens. Unlike CrossFlow [53], which follows similar paradigm but relies on 2D latent representations and incurs additional computational costs due to the text variational encoder, FlowTok operates within unified, compact 1D token space. This design achieves 3.3 compression rate in latent size, significantly reducing training costs and accelerating the sampling process, all while maintaining state-of-the-art performance. Text-to-Image Generation. Text-to-image generation has advanced rapidly in recent years, driven by various generative paradigms, including diffusion models [17, 62, 69, 70], flow matching models [19, 27, 86, 94], sequence models [28, 31, 68, 90, 91], and masked generative models [10, 14, 41, 84]. While early works in each category establish the foundation for their respective approaches, subsequent advancements across different model types have primarily emerged from three key areas: careful data collection and advanced image recaptioning for improved data quality [13, 23, 41], architectural and conditioning improve3 ments for faster convergence and better text-image alignment [17, 19, 27, 28], and micro-conditioning for finer control over generated samples [10, 41, 62]. By contrast, this work introduces minimalist framework FlowTok that directly maps text tokens to image tokens, eliminating the need for noise scheduling and complex conditioning mechanisms. This streamlined design enhances both efficiency and simplicity while maintaining competitive performance. 3. Preliminary 1D Visual Tokenization [41, 92] deviates from traditional 2D grid-based latent tokenization by adopting compact 1D representation, eliminating the need to preserve the 2D spatial structure. This work focuses on continuous 1D visual tokens for flow matching. During tokenization, given an input image RHW 3, the image is downscaled by factor of , resulting in patches D. These patches are concatenated with set of latent tokens RKDto form sequence that is passed through Vision Transformer (ViT) [24] encoder, Enc, to generate embeddings. Only the embeddings corresponding to the latent tokens are retained, forming compact 1D latent representation. This representation is modeled as Gaussian distribution with KL divergence regularization, resulting in compact 1D VAE representation, ZI RKD. In the de-tokenization phase, text guidance is applied by incorporating text embeddings generated by pre-trained text encoder [65]. These text embeddings are projected through linear layer to align with the channel dimensions of ViT decoder, resulting in RN D, where is the number of context tokens predefined by the text encoder. The text embedding is then concatenated with the latent tokens ZI and set of mask tokens D. The combined sequence is passed through the decoder Dec, yielding the reconstructed image ˆI. Formally, with denoting concatenation, the tokenization and de-tokenization can be represented as: ZI = Enc(P L), ˆI = Dec(ZI M). Flow matching [51, 55] is framework that learns continuous transformation between source distribution and target distribution. The source distribution is not necessarily required to be Gaussian noise, though we use Gaussian noise as concrete example below. During training, given sample from the target distribution, sampled time step [0, 1], and noise sample (0, I) from the source distribution. An intermediate representation Xt is obtained by: Xt = (1 t) + N. The flow matching model is trained to estimate the velocity field Vt, which describes the direction from the source to the target distribution. Taking the derivative of Xt with respect to t, we have: Vt = dXt dt = X, where Vt indicates the direction from the source to the target distribution such that the induced flow accurately transports the source distribution to the target distribution. Notably, while the source distribution is typically modeled as Gaussian noise in generative frameworks [27], the flow matching formulation generalizes to arbitrary source distributions, provided that the source and target distributions share the same shape. In FlowTok, we directly define unified latent space for image and text modalities, treating them as both source and target distributions. This design enables seamless generation across different modalities. 4. Method In this section, we focus on text-to-image generation as the primary task to illustrate FlowTok. We first detail how images and text are projected into unified, compact latent space as 1D tokens while preserving semantic information (Sec. 4.1). Next, we introduce FlowTok as general framework for seamless flow between text and image tokens and discuss its extension to image-to-text generation under the same formulation (Sec. 4.2). 4.1. Unifying Latent Space of Image and Text The structural discrepancy between text and images presents significant challenge in unifying them within the same latent space for flow matching. Text is inherently semantic, encoded as 1D latent sequence with high-dimensional channels to preserve meaning, whereas images contain spatially redundant information and are typically represented as 2D feature maps with lower channel dimensions to retain spatial priors. To bridge this gap, we propose encoding images into compact 1D tokens by leveraging recent advancements in image tokenization. This formulation helps preserve the 1D structure of text embeddings, requiring only their projection into more compressed set of tokens while ensuring that semantic information is retained. Below, we detail how both images and text are encoded. Encoding Images into Compact Tokens. We build upon the core idea of TA-TiTok [41] with several enhancements to improve our image tokenizer. Specifically, we replace the original learnable 1D positional embedding with RoPE [76] to enhance TA-TiTok performance. Additionally, we substitute the MLP blocks in the Vision Transformer (ViT) [24] with SwiGLU FFN [71], which helps learn more effective latent space [18, 88]. To align with the number of context tokens of the text encoder, we set the number of latent tokens in TA-TiTok accordingly (K = = 77 for Figure 4. Overview of FlowTok. FlowTok is minimal framework that facilitates seamless flow between 1D text tokens and image tokens for both text-to-image and image-to-text generation. Top: For text-to-image generation, the input text is encoded by the CLIP text encoder into Tinit RN , projected into low-dimensional latent space as text tokens ZT RN D, then transformed into image tokens ZI RN of the same shape through flow matching and decoded by 1D Image VAE Decoder to generate the final image. Bottom: For image-to-text generation, an input image is encoded by 1D Image VAE Encoder into ZI, mapped to ZT through flow matching and decoded into text via text decoder. Unlike conventional approaches that rely on 2D noise and image latents (e.g., 32 32 4 for 256-resolution images) with text as conditions, our direct 1D transformation (i.e., 77 16) achieves 3.3 compression rate, significantly reducing memory costs, accelerating training, and enabling faster inference. CLIP [65]). As result, the encoder of TA-TiTok encodes each image into compact 1D token sequence ZI RKD. Transforming Texts into Compact Tokens. We use pretrained text encoder [65] to extract the initial text embedding Tinit RN C, where denotes the number of channels. Notably, is typically much larger than the image latent size D, as it carries richer semantic information. Since our goal is to directly flow the text embedding into the image latent space, we need to ensure both embeddings have the same shape. While we already align with through careful tuning of the image tokenizer, ensuring the image is encoded to match the length of the text tokens. The remaining challenge lies in aligning the number of channels (i.e., and D), which we resolve using text projector. Since only the number of channels in Tinit needs adjustment while preserving its 1D shape, we employ few simple Transformer blocks as the projector. To introduce variability in image generation from the same text, we model the projected text latents ZT RN as Gaussian distribution by applying KL divergence regularization Lkld. crucial aspect of text-to-image generation is ensuring that the generated image accurately reflects the input text description. Since reducing the channel dimensions of text embeddings via learnable projector may result in semantic information loss, we introduce an auxiliary text alignment loss Lalign to preserve semantic consistency. Specifically, we employ lightweight MLP to project Tinit into new space TP RN for alignment. We then flatten and normalize both TP and ZT along the channel dimension and compute contrastive loss between them, inspired by CLIP [65]. Concretely, we calculate the scaled pairwise cosine similarities using learnable temperature parameter τ , followed by symmetric cross-entropy loss: logitsTZ = exp(τ ) (TP ZT logitsZT = exp(τ ) (ZT TT Lalign = (CE(logitsTZ, labels) + CE(logitsZT, labels))/2, ), ), where denotes the transpose operation, CE represents the cross-entropy loss, and labels are assigned based on their batch indices, ensuring that each text token is explicitly trained to align with its corresponding CLIP text embedding within the same batch. We also explore alternative approaches to preserving semantic information, such as aligning with the average-pooled text embedding or using cosine similarity loss with margin. However, we find that the CLIP-style loss achieves the best performance. More details are provided in Sec. 5.3. Through the aforementioned designs, FlowTok efficiently tokenizes text into the same low-dimensional latent space while preserving semantic information. This alignment with the tokenized image latent space establishes foundation for direct flow between compressed text tokens ZT and image tokens ZI. Notably, when using CLIP as the text encoder, FlowTok effectively reduces the latent size compared to traditional 2D flow matching methods. At an image resolution of 256, the latent size is reduced from 32 32 4 to 77 16, achieving 3.3 compression. This reduction significantly lowers memory requirements and accelerates training, en5 model FlowTok-B 12 FlowTok-XL 28 36 FlowTok-H depth width mlp heads #params 153M 698M 1.1B 768 3072 1152 4608 1280 5120 12 16 Table 1. Architecture Configuration of FlowTok. Following prior work, we scale up DiT blocks across three configurations. hancing the frameworks efficiency and scalability. 4.2. FlowTok: General Framework for Seamless"
        },
        {
            "title": "Flow Across Text and Image Tokens",
            "content": "Text-to-Image Generation. As shown in Fig. 4 (top), with both image and text mapped into the same latent space, FlowTok leverages vanilla flow matching [51] by stacking DiT blocks [61]. Notably, source modality (i.e., text) is directly treated as the source distribution for flow matching, removing the need for concatenation or cross-attention within the DiT blocks. This design choice further simplifies the overall framework and streamlines text-to-image generation. Combined with the compact 1D tokens introduced in Sec. 4.1, FlowTok achieves high memory efficiency, supporting batch size of 8K on 8 A100 GPUs. Additionally, it enables fast sampling, running over 10 faster than modern textto-image diffusion models [69], significantly lowering the computational barrier for training large-scale text-to-image generative models. Image-to-Text Generation. FlowTok can also be seamlessly extended to image-to-text generation using compact 1D image and text tokens under the same formulation, as shown in Fig. 4 (bottom). Specifically, the image tokens ZI flow to text tokens ZT, where trained text decoder takes ZT as input and outputs tokenizer indices, which can then be decoded back into the corresponding caption. 5. Experimental Results In this section, we first provide the implementation details of FlowTok (Sec. 5.1), followed by the main results on text-toimage and image-to-text generation (Sec. 5.2). Finally, we present the ablation studies to better understand the design choices of FlowTok in text-to-image generation (Sec. 5.3). 5.1. Implementation Details Image Tokenizer. We build our image tokenizer upon the official TA-TiTok [41] codebase with minimal modifications. The encoder uses ViT-B [24], while the decoder uses ViT-L, both operating with patch size of = 16. To align with the output sequence length of CLIPs text encoder, we set the number of 1D latent tokens to 77 and the token dimension to 16. Additionally, we enhance the tokenizer with RoPE [76] and SwiGLU FFN [71]. Notably, our enhanced tokenizer achieves FID of 1.02 in zero-shot evaluation on the ImageNet validation set, matching the performance of the original TA-TiTok with 128 tokens. Text Projector. We train text projector for text-to-image generation, which transforms CLIP text embeddings into latent representation ZT of shape 77 16, aligning with the image latent space ZI encoded by our image tokenizer. The text projector consists of six Transformer [81] blocks, each comprising multi-head self-attention mechanism and multi-layer perceptron (MLP), both enhanced with skip connections [35] to ensure stable training. Text Decoder. We train text decoder for image-to-text generation, composed of six Transformer [81] blocks, similar to the text projector. The decoder takes the text latent representation ZT as input and outputs the corresponding CLIP text tokenizer indices, which can be further converted into text using the CLIP text tokenizer. FlowTok. We adopt DiT [61] blocks as the fundamental building units of our FlowTok to model token interactions. Specifically, we follow the DiT architecture to implement FlowTok-B for efficient ablation studies and FlowTok-XL for enhanced performance. To further push the performance, we scale up the depth, width, and number of attention heads, constructing FlowTok-H with 1.1B parameters. The detailed model configurations are provided in Tab. 1. Dataset. We employ open-source datasets [41] to facilitate the reproducibility of FlowToks simple framework. Specifically, our image tokenizer is trained on DataComp-1B [30], and our text tokenizer is trained on COCO [50]. For text-toimage generation, inspired by recent works [16, 17, 22, 95], we adopt two-stage training strategy: pre-training and finetuning. The pre-training stage leverages combination of DataComp-1B [30], CC12M [15], and LAION-aesthetic [1], while the fine-tuning stage incorporates additional highquality datasets, including LAION-art [3], LAION-pop [4], JourneyDB [77], and DALLE3-1M [26]. For image-to-text generation, we follow the Karpathy split [40] of COCO [20] to divide the training and validation sets. Detailed dataset information is provided in the Appendix. Training. The training objectives of FlowTok primarily focus on predicting velocity in flow matching, denoted as Lfm. For text-to-image generation, we introduce two additional losses: KL-divergence loss (Lkld) to enforce Gaussian distribution on text tokens and text alignment loss (Lalign) to preserve semantic information as discussed in Sec. 4.1. Formally, the overall training objective is: = Lfm + γ1 Lkld + γ2 Lalign, where γ1 and γ2 control the weighting of losses. By default, we set γ1 to 1104 and γ2 to 1 for text-to-image generation, while both are set to 0 for image-to-text generation. Evaluation. We follow standard evaluation practices to report relevant metrics for both text-to-image and imageto-text generation. Specifically, for text-to-image generamethod params open-data COCO FID-30K MJHQ-30K FID 5.0B GLIDE [58] Dalle2 [66] 6.5B 775M LlamaGen [78] PixArt-α [17] 630M 2.6B SDXL [62] LDM [69] 1.4B Stable-Diffusion-1.5 [69] 860M Stable-Diffusion-2.1 [69] 860M 1.3B Show-o [85] CrossFlow [53] FlowTok-XL FlowTok-H 950M 698M 1.1B text as conditions - - - - - - 7.9 94.1 - - - - - 781.2 - 1041.6 1.0 - text as source distributions 78.8 20.4 26.1 1.1 22.7 18.2 12.24 10.39 - 7.32 - 12.63 9.62 13.45 9.24 9.63 10.06 9.67 - - 25.59 9.85 8.76 - - 26.96 14.99 - 7.68 7.15 Table 2. Zero-Shot Text-to-Image Generation Results on COCO and MJHQ-30K. We compare FlowTok with state-of-the-art methods, categorized into two approaches: (1) text as conditions, where text tokens are used as conditions to guide the generation process, and (2) text as source distributions, where the model directly learns the alignment between text and image distributions. open-data: Models are trained exclusively with publicly available datasets. T: Model training cost, measured in 8 A100 days using float16 precision. I: Model inference throughput, measured at 256px resolution in samples per second on single A100 with batch size 64 using float16 precision. method B@4 R S direct flow from image to text distributions CrossFlow [53] FlowTok-XL 36.4 27.8 57.1 116.2 20.4 37.1 27.8 57.6 117.0 20.5 other methods 30.9 27.5 55.6 108.1 21.0 MNIC [32] MIR [44] 109.5 20.6 - 32.5 27.2 NAIC-CMAL [34] 35.3 27.3 56.9 115.5 20.8 SATIC [97] 111.0 20.5 - 32.9 27.0 37.3 28.1 58.0 118.0 21.6 SCD-Net [57] Table 3. Image-to-Text Generation Results on COCO. FlowTok achieves performance comparable to state-of-the-art methods on image-to-text generation, evaluated on the COCO Karpathy split. For fair comparison, we restrict our evaluation to nonautoregressive methods trained without CIDEr optimization. tion, we report FID-30K on the COCO [50], and FID on MJHQ-30K [46]. For image-to-text generation, we report BLEU-4 [60], METEOR [11], ROUGE [49], CIDEr [82], and SPICE [8] on the COCO Karpathy Split [40]. To incorporate classifier-free guidance (CFG) [36] within FlowTok, we follow CrossFlow[53] and utilize CFG indicator. Unless otherwise stated, we find that using only 20 steps for sampling is sufficient due to the small 1D latent shape of FlowTok. This significantly speeds up the inference process, enabling faster generation without compromising performance. 5.2. Main Results Text-to-Image Generation. We report zero-shot textto-image generation results on COCO [50] and MJHQ30K [46] in Tab. 2. The compared methods are categorized into two groups: text as conditions, where text serves as guiding signal for image generation, and text as source distributions, where text is directly modeled as distribution in the generative process. As observed, FlowTok achieves comparable performance to prior methods in both categories on COCO FID-30K. Specifically, compared to CrossFlow [53], which also uses text as the source distribution, FlowTok-H attains FID-30K of 9.67roughly on par with CrossFlow. When further evaluating FlowTok on MJHQ-30K to assess the aesthetic quality of generated images, we find that, despite being trained solely on publicly available datasets without access to high-quality proprietary data, FlowTok-XL already surpasses other state-of-the-art models, demonstrating its ability to generate diverse, high-quality images. Furthermore, FlowTok-H further improves the FID score to 7.15, underscoring its superior image generation capabilities. Beyond performance, FlowTok requires significantly fewer training resources compared to existing state-of-theart models. Specifically, FlowTok-XL completes training in just 20.4 8-A100 days, while FlowTok-H increases the budget slightly to 26.1 8-A100 days. In contrast, the most efficient text-as-condition model, PixArt-α [17], still demands 94.1 8-A100 days. Compared to CrossFlow [53], which also treats text as source distributions and requires 78.8 8-A100 days, FlowTok is much more efficient. Additionally, FlowTok demonstrates significantly faster inference speeds. At 256px resolution, FlowTok-XL generates 22.7 images per second, while FlowTok-H achieves 18.2 images per second. In contrast, PixArt-α runs at 7.9 images per second, and Show-o at just 1.0 images per second. More notably, within the text as source distributions 7 target Ave Pool MLP COCO FID-30K 36.02 29.14 (a) Text Alignment Target loss type Cosine Contrastive (b) Text Alignment Loss Function COCO FID-30K 31.80 29.14 γ2 COCO FID-30K 1.0 2.0 29.14 30.59 (c) Text Alignment Loss Weight Table 4. Ablation Studies on Text Alignment Loss. We conduct comprehensive ablation studies on three key aspects of the text alignment loss Lalign: the alignment target (Tab. 4a), the choice of loss function (Tab. 4b), and the loss weight γ2 (Tab. 4c), aiming to identify the most effective strategy for preserving semantic information within FlowTok during text-to-image generation. For efficient verification, we report FID-30K on COCO using FlowTok-B, without applying the CFG indicator. category, FlowTok achieves 20 speedup in sampling time compared to CrossFlow, which runs at only 1.1 images per second. This efficiency stems from FlowTok streamlined framework and its effective use of 1D tokens, significantly reducing computational overhead. Image-to-Text Generation. We evaluate image-to-text generation on COCO [50] using the Karpathy split [40], with results summarized in Tab. 3. To ensure fair comparison, we categorize methods into two groups: direct flow from image to text distributions, which represents new paradigm leveraging flow matching for direct image-to-text transformation, and other methods, considering only those trained without CIDEr optimization. Within the direct flow category, FlowTok-XL consistently outperforms its counterpart, CrossFlow [53], across most metrics. Specifically, FlowTokXL achieves BLEU-4 (B@4) score of 37.1, surpassing CrossFlow by 0.7, and CIDEr score of 117.0, exceeding CrossFlow by 0.8. Moreover, compared to state-of-the-art methods from other paradigms, FlowTok-XL demonstrates competitive performance, highlighting direct flow matching as promising approach for image-to-text generation. Notably, FlowTok performs image-to-text generation under the same formulation using compact 1D tokens, theoretically requiring fewer training resources and enabling faster sampling compared to paradigms that operate on 2D latents, as adopted by CrossFlow. However, direct quantitative comparison is not possible, as CrossFlow has not released the corresponding checkpoint for evaluation. 5.3. Ablation Studies We conduct ablation studies on text-to-image generation using FlowTok-B and evaluate them on COCO for efficiency. Our ablations focus on the design of the text alignment loss, as it plays critical role in preserving semantic information. Specifically, we investigate three key aspects: the text alignment target (Tab. 4a), the choice of loss function (Tab. 4b), and the loss weight (Tab. 4c). Details are provided below. Text Alignment Target. We first investigate the choice of alignment target for the projected text tokens ZT in Tab. 4a. straightforward baseline is to directly apply average pooling (row 1) to the original CLIP text embedding Tinit along the channel dimension, reducing the dimensionality from 768 to 16 to match ZT. However, this approach performs significantly worse compared to using simple MLP to learn the alignment target (row 2), as inspired by prior works [33, 59, 93]. We attribute this performance gap to the fact that adjacent channels in the CLIP text embedding are not necessarily correlated, and simple average pooling discards too much semantic information. In contrast, learnable MLP mitigates this information loss, making it more effective choice for defining the text alignment target. Text Alignment Loss Function. Next, we examine the text alignment loss function in Tab. 4b. Besides the contrastive loss adopted from [65], we explore using cosine similarity loss, similar to [87]. Specifically, we compute the cosine similarity between the text tokens and the alignment target, applying penalty to pairs with similarity below threshold. Our experiments show that while both loss functions are effective, the contrastive loss achieves better performance. Text Alignment Loss Weight. Finally, we investigate the impact of the text alignment loss weight, γ2 in Tab. 4c. Our results indicate that setting γ2 to 1.0, equal to the weight of the flow matching loss, is sufficient to preserve semantic information while maintaining high-quality image generation. Increasing γ2 further can cause the text alignment loss to dominate the overall objective during early training stages, potentially hindering final performance. 6. Conclusion In this paper, we introduce FlowTok, minimal yet powerful framework that enables seamless direct flow between 1D text and image tokens. Through carefully designed key modules and loss functions, FlowTok projects both modalities into unified 1D latent space while preserving semantic information, enabling both text-to-image and image-to-text generation under the same formulation. This design makes FlowTok highly memory-efficient, supporting an 8K batch size on just 8 A100 GPUs during training. Additionally, its simplicity accelerates convergencewithin approximately 20 days on 8 A100 GPUs, FlowTok achieves performance comparable to state-of-the-art models that require significantly longer training times. The streamlined design also enables over 10 faster sampling than modern text-to-image generative models. By releasing our code, we aim to further advance research in text-image cross-modal generation."
        },
        {
            "title": "Appendix",
            "content": "The supplementary material provides additional information: Sec. A: More implementation details, including dataset filtering, and FlowTok training hyperparameters. Sec. B: Additional qualitative text-to-image and image-totext generation samples produced by FlowTok. Sec. C: Discussions on limitations and future work of FlowTok."
        },
        {
            "title": "AdamW AdamW",
            "content": "hyper-parmeters pre-training fine-tuning optimizer optimizer-β1 optimizer-β2 weight decay lr lr scheduling lr warmup steps batch size training steps 0.9 0.95 0.03 0.0002 constant 0 4096 150k 0.9 0.95 0.03 0.0004 constant 10K 4096 250K A. More Implementation Details Table 6. Training Hyper-parameters for FlowTok. model dataset filtering resolution aesthetic watermark B. Qualitative Examples of FlowTok Text Decoder Image Tokenizer FlowTok: pre-training FlowTok: fine-tuning COCO [50] DataComp [30] DataComp [30] CC12M [15] LAION-aesthetic [1] DataComp [30] LAION-art [3] LAION-pop [4] DALLE3-1M [26] JourneyDB [77] (5.0) (5.0) (6.0) Table 5. Training Data Details. The filtering criteria applied include resolution (aspect ratio < 2 and longer side 256), aesthetic score (predicted score exceeding the specified value in parentheses), and watermark detection (removal of images predicted to contain watermarks). : We use the re-captioned version released by MaskGen [41], which contains improved captions. Dataset Filtering. In line with previous works [41], we apply three filtering criteria to curate open data for training the image tokenizer and FlowTok: resolution, aesthetic quality, and watermark filtering. The COCO [20, 50] dataset is used directly to train the text decoder without any filtering. Details of the applied filtering criteria are shown in Tab. 5. Specifically, resolution filtering is applied during the training of the image tokenizer and for text-to-image generation. This ensures that the longer side of each image is at least 256 pixels and the aspect ratio is below 2. For text-to-image training, we further apply aesthetic filtering using the LAIONaesthetic-v2 predictor [2] to retain only high-quality images. Images with aesthetic scores above 5.0 are retained during the pre-training stage, while stricter threshold of 6.0 is used during fine-tuning to ensure even higher image quality. Additionally, watermark filtering is implemented for FlowToks text-to-image generation by using the LAIONWatermarkDetector [5], removing images with watermark probabilities exceeding 0.5. Synthetic datasets such as JourneyDB [77] and DALLE3-1M [26] are exempt from these filtering steps, as they inherently meet our high resolution and quality standards. Training Hyper-parameters. Tab. 6 provides the complete list of hyper-parameters used for training FlowTok. Additional Generation Results. Fig. 5, Fig. 6, and Fig. 7 present additional text-to-image generation results produced by FlowTok, demonstrating its ability to generate diverse, high-fidelity images. Meanwhile, Fig. 8 displays the imageto-text generation results, showcasing FlowToks capability to produce accurate and descriptive captions. C. Limitations and Future Work The primary limitation of FlowTok arises during text-toimage generation. To match the compact dimensionality of image latents (e.g., 16), FlowTok projects CLIP text embeddings into the same low-dimensional latent space. While the text alignment loss helps preserve semantic information, some degree of information loss is inevitable during this projection. Consequently, the alignment between text and generated images may be weaker compared to state-of-the-art models employing cross-attention mechanisms. To address this, one potential solution is to introduce stronger alignment loss that better retains textual semantics. more fundamental approach, however, involves increasing the channel dimensionality of image latents by aligning them with vision foundation models [59] during image tokenizer training, as suggested by VA-VAE [89]. This strategy aims to identify an optimal channel dimension for both image and text tokens, achieving balance between preserving semantic information and maintaining efficiency in training and inference. Additionally, FlowTok currently utilizes only the vanilla flow matching technique to validate the frameworks effectiveness. However, many recent advancements in flow matching, such as logit-normal sampling [27], have not yet been explored in our model. Incorporating these techniques could accelerate convergence and enhance performance. Finally, FlowTok serves as starting point for exploring efficient direct evolution between text and image modalities. In the future, we aim to extend to more general framework that can accommodate broader range of modalities, supporting additional tasks under the same unified formulation. 9 Figure 5. Text-to-Image Generation Results by FlowTok. FlowTok generates diverse, high-fidelity images. Figure 6. Text-to-Image Generation Results by FlowTok. FlowTok generates diverse, high-fidelity images. Figure 7. Text-to-Image Generation Results by FlowTok. FlowTok generates diverse, high-fidelity images. Figure 8. Image-to-Text Generation Results by FlowTok. FlowTok generates precise captions."
        },
        {
            "title": "References",
            "content": "[1] LAION2B-en-aesthetic. https://huggingface.co/ datasets/laion/laion2B-en-aesthetic, . 6, 9 [2] LAION-aesthetics predictor V2. https://github.com/ christophschuhmann / improved - aesthetic - predictor, . 9 [3] LAION-art. https://huggingface.co/datasets/ laion/laion-art, . 6, 9 [4] LAION-pop. https : / / huggingface . co / datasets/laion/laion-pop, . 6, [5] LAION-5B-WatermarkDetection. https : / / github . com / LAION - AI / LAION - 5B - WatermarkDetection, . 9 [6] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. NeurIPS, 2022. 2 [7] Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. 2, 3 [8] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In ECCV, 2016. 7 [9] Sara Atito, Muhammad Awais, and Josef Kittler. Sit: Selfsupervised vision transformer. In ECCV, 2024. 3 [10] Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Qing-Guo Chen, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng YAN. Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis. In ICLR, 2025. 3, 4 [11] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, 2005. 7 [12] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In CVPR, 2023. 2 [13] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science, 2(3):8, 2023. 3 [14] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, José Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Textto-image generation via masked generative transformers. In ICML, 2023. [15] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. 6, 9 [16] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. PixArt-Σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In ECCV, 2024. 6 [17] Junsong Chen, Jincheng YU, Chongjian GE, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. PixArt-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024. 2, 3, 4, 6, 7 [18] Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Vitamin: Designing scalable vision models in the vision-language era. In CVPR, 2024. 4 [19] Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, et al. Goku: Flow based video generative foundation models. arXiv preprint arXiv:2502.04896, 2025. 3, 4 [20] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 6, [21] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In ICML, 2021. 2 [22] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Radenovic, Dhruv Mahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Motwani, Yi Wen, Yiwen Song, Roshan Sumbaly, Vignesh Ramanathan, Zijian He, Peter Vajda, and Devi Parikh. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. 6 [23] Xueqing Deng, Qihang Yu, Ali Athar, Chenglin Yang, Linjie Yang, Xiaojie Jin, Xiaohui Shen, and Liang-Chieh Chen. Coconut-pancap: Joint panoptic segmentation and grounded captions for fine-grained understanding and generation. arXiv preprint arXiv:2502.02589, 2025. 3 [24] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 4, 6 [25] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 2 [26] Ben Egan, Alex Redden, XWAVE, and SilentAnDalle3 1 Million+ High Quality Captagonist. https : / / huggingface . co / datasets / tions. ProGamerGov/synthetic-dataset-1m-dalle3high-quality-captions, 2024. 6, [27] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 2, 3, 4, 9 [28] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and 12 Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. 3, 4 [45] Sangyun Lee, Beomsu Kim, and Jong Chul Ye. Minimizing trajectory curvature of ode-based generative models. In ICML, 2023. 3 [29] Johannes Fischer, Ming Gui, Pingchuan Ma, Nick Stracke, Stefan Baumann, and Björn Ommer. Boosting latent diffusion with flow matching. arXiv preprint arXiv:2312.07360, 2023. [30] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. NeurIPS, 2023. 6, 9 [31] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. In ECCV, 2022. 3 [32] Junlong Gao, Xi Meng, Shiqi Wang, Xia Li, Shanshe Wang, Siwei Ma, and Wen Gao. Masked non-autoregressive image captioning. arXiv preprint arXiv:1906.00717, 2019. 7 [33] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. NeurIPS, 2020. 8 [34] Longteng Guo, Jing Liu, Xinxin Zhu, Xingjian He, Jie Jiang, and Hanqing Lu. Non-autoregressive image captioning with counterfactuals-critical multi-agent learning. arXiv preprint arXiv:2005.04690, 2020. 7 [35] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 6 [36] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 7 [37] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 2, 3 [38] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In ICML, 2021. 2 [39] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021. 2 [40] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015. 6, 7, 8 [41] Dongwon Kim, Ju He, Qihang Yu, Chenglin Yang, Xiaohui Shen, Suha Kwak, and Liang-Chieh Chen. Democratizing text-to-image masked generative models with comarXiv preprint pact text-aware one-dimensional tokens. arXiv:2501.07730, 2025. 2, 3, 4, 6, 9 [42] Diederik Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. 2 [43] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2 [46] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. 7 [47] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified visionlanguage understanding and generation. In ICML, 2022. 2 [48] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: simple and perforarXiv preprint mant baseline for vision and language. arXiv:1908.03557, 2019. 2 [49] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, 2004. 7 [50] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 6, 7, 8, 9 [51] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In ICLR, 2023. 2, 3, 4, [52] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A. Theodorou, Weili Nie, and Anima Anandkumar. I2sb: Image-to-image schrödinger bridge. arXiv preprint arXiv:2302.05872, 2023. 2, 3 [53] Qihao Liu, Xi Yin, Alan Yuille, Andrew Brown, and Mannat Singh. Flowing from words to pixels: framework for crossmodality evolution. arXiv preprint arXiv:2412.15213, 2024. 2, 3, 7, 8 [54] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Alleviating distortion in image generation via multi-resolution diffusion models. NeurIPS, 2024. 2, 3 [55] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. 2, 3, 4 [56] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. NeurIPS, 2019. 2 [57] Jianjie Luo, Yehao Li, Yingwei Pan, Ting Yao, Jianlin Feng, Hongyang Chao, and Tao Mei. Semantic-conditional diffusion networks for image captioning. In CVPR, 2023. 7 [58] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [59] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 8, 9 [44] Jason Lee, Elman Mansimov, and Kyunghyun Cho. Deterministic non-autoregressive neural sequence modeling by iterative refinement. arXiv preprint arXiv:1802.06901, 2018. 7 [60] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In ACL, 2002. 7 13 [61] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 2, 3, [62] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 3, 4, 7 [63] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 3 [64] Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky TQ Chen. Multisample flow matching: StraightarXiv preprint ening flows with minibatch couplings. arXiv:2304.14772, 2023. 3 [65] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 2, 4, 5, 8 [66] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2): 3, 2022. 7 [67] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Flowar: Scale-wise autoregressive image generation meets flow matching. arXiv preprint arXiv:2412.15205, 2024. [68] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Beyond next-token: Next-x prediction for autoregressive visual generation. arXiv preprint arXiv:2502.20388, 2025. 3 [69] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 3, 6, 7 [70] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 2022. 3 [71] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. 2, 4, 6 [72] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schrödinger bridge matching. NeurIPS, 2023. 2, [73] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. 2 [74] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. NeurIPS, 2019. 2 [75] stabilityai, 2023. 3 [76] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. 2, 4, 6 [77] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: benchmark for generative image understanding. NeurIPS, 2023. 6, 9 [78] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 7 [79] Alexander Tong, Kilian Fatras, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint arXiv:2302.00482, 2023. [80] Alexander Tong, Nikolay Malkin, Kilian Fatras, Lazar Atanackovic, Yanlei Zhang, Guillaume Huguet, Guy Wolf, Simulation-free schr\" odinger and Yoshua Bengio. arXiv preprint bridges via score and flow matching. arXiv:2307.03672, 2023. 2, 3 [81] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017. 6 [82] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, 2015. 7 [83] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021. 2 [84] Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. Maskbit: Embedding-free image generation via bit tokens. arXiv preprint arXiv:2409.16211, 2024. 3 [85] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 3, 7 [86] Chenglin Yang, Celong Liu, Xueqing Deng, Dongwon Kim, Xing Mei, Xiaohui Shen, and Liang-Chieh Chen. 1.58-bit flux. arXiv preprint arXiv:2412.18653, 2024. 3 [87] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024. 8 [88] Jingfeng Yao and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. arXiv preprint arXiv:2501.01423, 2025. 4 [89] Jingfeng Yao and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. arXiv preprint arXiv:2501.01423, 2025. 9 [90] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. TMLR, 2022. 3 [91] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and LiangChieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024. [92] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. NeurIPS, 2024. 2, 4 [93] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In ICLR, 2025. 8 [94] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In ICLR, 2025. 3 [95] Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogview3: Finer and faster text-to-image generation via relay diffusion. arXiv preprint arXiv: 2403.05121, 2024. 6 [96] Linqi Zhou, Aaron Lou, Samar Khanna, and Stefano Ermon. Denoising diffusion bridge models. arXiv preprint arXiv:2309.16948, 2023. 2, 3 [97] Yuanen Zhou, Yong Zhang, Zhenzhen Hu, and Meng Wang. Semi-autoregressive transformer for image captioning. In ICCV, 2021."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Johns Hopkins University"
    ]
}