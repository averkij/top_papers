{
    "paper_title": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge Reasoning",
    "authors": [
        "Giulio Corallo",
        "Orion Weller",
        "Fabio Petroni",
        "Paolo Papotti"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Incorporating external knowledge in large language models (LLMs) enhances their utility across diverse applications, but existing methods have trade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via similarity search, but key information may fall outside top ranked results. Long-context models can process multiple documents but are computationally expensive and limited by context window size. Inspired by students condensing study material for open-book exams, we propose task-aware key-value (KV) cache compression, which compresses external knowledge in a zero- or few-shot setup. This enables LLMs to reason efficiently over a compacted representation of all relevant information. Experiments show our approach outperforms both RAG and task-agnostic compression methods. On LongBench v2, it improves accuracy by up to 7 absolute points over RAG with a 30x compression rate, while reducing inference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG performs well when sparse evidence suffices, whereas task-aware compression is superior for broad knowledge tasks."
        },
        {
            "title": "Start",
            "content": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge Reasoning Giulio Corallo SAP Labs, EURECOM giulio.corallo@sap.com Orion Weller Johns Hopkins University oweller@cs.jhu.edu Fabio Petroni Samaya AI fabio@samaya.ai Paolo Papotti EURECOM papotti@eurecom.fr 5 2 0 M 6 ] . [ 1 3 7 9 4 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Incorporating external knowledge in large language models (LLMs) enhances their utility across diverse applications, but existing methods have trade-offs. RetrievalAugmented Generation (RAG) fetches evidence via similarity search, but key information may fall outside top ranked results. Long-context models can process multiple documents but are computationally expensive and limited by context window size. Inspired by students condensing study material for open-book exams, we propose taskaware key-value (KV) cache compression, which compresses external knowledge in zeroor few-shot setup. This enables LLMs to reason efficiently over compacted representation of all relevant information. Experiments show our approach outperforms both RAG and task-agnostic compression methods. On LongBench v2, it improves accuracy by up to 7 absolute points over RAG with 30 compression rate, while reducing inference latency from 0.43s to 0.16s. synthetic dataset highlights that RAG performs well when sparse evidence suffices, whereas task-aware compression is superior for broad knowledge tasks."
        },
        {
            "title": "Introduction",
            "content": "Incorporating external information into large language models (LLMs) significantly enhances their utility across various applications, enabling them to generate more informed and accurate outputs. Several methodologies have been devised to facilitate this integration, but each comes with limitations that restrict its effectiveness. Retrieval-Augmented Generation (RAG) is technique that enhances LLMs by leveraging external corpora to retrieve relevant chunks of information (Lewis et al., 2020). However, RAG Figure 1: Overlap match of the words between ground truth and predictions of various KV cache compression methods compared to RAG on synthetic corpus with 32k tokens. Our Few-Shot compression approach achieves results exceeding RAG when the context length is much smaller than the corpus size. is most effective in scenarios with narrow, focused queries that require few pieces of evidence. When dealing with broader queries that demand synthesizing insights from multiple sources across the corpus, retrieval mechanisms may fall short (Barnett et al., 2024). This happens because retrieval typically relies on similarity-based search, which may fail to capture implicit relationships between distant pieces of evidence, making it challenging to surface all relevant context and often introducing noise or redundancy (Yu et al., 2024). Researchers have begun to address these issues, e.g., via improved chunking and pruning strategies (Edge et al., 2024), but handling broad queries with RAG remains challenging. Recent advancements have extended LLMs ability to process longer contexts, pushing the boundaries of how much information they can handle simultaneously (Team et al., 2024; Li et al., 2025a). This progress opens up the possibility of processing entire corpora as input, offering compelling alternative to retrieval-based methFigure 2: An illustration of our compression strategy that reduces the original context (C) from KV cache of 128k tokens to 16k. This process is guided by task instructions (T) and few-shot examples (FS), condensing the essential information needed for factual QA on the corpus documents. At inference time, the LLM can answer multiple questions as if it had access to the entire (uncompressed) corpus. ods. However, this approach comes with significant computational costs, as handling large inputs requires substantial memory resources, particularly on GPUs, which creates scalability bottleneck (Liu et al., 2023). Furthermore, as the context grows, models often struggle to identify the relevant pieces of information buried in all the clutter (Liu et al., 2024). To bridge the gap between massive corpora and limited context windows, researchers are developing compression techniques that condense or filter input text (Jha et al., 2024). Some of this strategy focuses on optimizing the models key-value (KV) cache, ensuring that essential information is retained even within limited contexts. Existing approaches fall into two categories: query-agnostic compression (Zhang et al., 2023; Devoto et al., 2024; Xiao et al., 2023a; Feng et al., 2025), and query-aware compression, which dynamically optimizes content based on the query during inference (Li et al., 2025b; Corallo and Papotti, 2024). While the latter leads to highly relevant outputs, it is computationally prohibitive, as it requires recompressing the input for each query, making it impractical for real-world deployment. In this study, we introduce task-aware, queryagnostic compressed cache, offering balanced Intrade-off between efficiency and relevance. stead of recompressing the input for every query, our approach precomputes compressed cache tailored to broader task context. Our method delivers performance that significantly surpasses existing query-agnostic compression and closely approaches the effectiveness of query-aware compression. Figure 1 shows the quality performance of KV cache compression methods against RAG on 32k token corpus. With high compression rates 64x and 32x (corpus compressed to 512 and 2048 tokens) our method outperforms RAG by about 20 absolute points. With compression rates between 16x and 8x, Few Shots performs on par or better than fitting the original corpus in the model context. Figure 2 shows our compression strategy. The task context can be defined through succinct description (namely Zero Shot) or small set of representative examples in Few-Shot setting. This prompt produces more compact representation of the original context while preserving crucial details. Compression happens only once, creating representation that can be reused for any query within the task domain. This eliminates the need for repeated processing, streamlining inference by bypassing real-time retrieval and prefilling. This approach is not limited to QA but can be applied to wide range of tasks. Experimental results across diverse tasks using Llama 3.1, including the LongBench and LongBench v2 benchmarks, demonstrate that our taskaware compression method consistently outperforms both retrieval-based approaches and fullcontext models in diverse evaluation settings, such as Code Completion and Document QA. Furthermore, experiments on synthetic datasets highlight the superior capability of our method in handling broad, multifaceted queries. Notably, in scenarios requiring the synthesis of widely distributed information, our approach significantly outperforms RAG, establishing compression as key enabler for scaling LLM reasoning beyond retrieval-based methods."
        },
        {
            "title": "2 Background",
            "content": "on the built transformer LLMs architecture (Vaswani et al., 2017) have become the backbone of modern natural language processing. Given sequence of tokens Rn, each transformer layer produces hidden representations via multi-head self-attention mechanism: (cid:16) QK dk Attention(Q, K, V) = softmax V, (cid:17) 2.1 KV-Cache Compression To mitigate the memory load from very long contexts, promising approach is KV-cache compression. Instead of retaining K, for all tokens, one compresses them into smaller matrices: (cid:101)K Rkd and (cid:101)V Rkd with n, that still preserve the essential information needed for generating the final response. Formally, one seeks to minimize divergence measure, dist(cid:0)y K, V, (cid:101)K, (cid:101)V(cid:1)(cid:105) (cid:104) , min (cid:101)K, (cid:101)V where = WQh, = WKh, = WV h, with representing the hidden states (token embeddings) for the input sequence. The dimension dk is where is the hidden size and is the number of attention heads. In knowledge-intensive task setup (Petroni et al., 2020), many instruction-tuned LLMs organize their input as context followed by user prompt. Formally, let denote sequence of tokens. The input sequence can be expressed as: = (cid:104) x(cont), x(prompt)(cid:105) Rn(cont)+n(prompt) , (1) Crucially, x(cont) serves as the knowledge the model has access to when generating the final response. During inference, an LLM operates in two distinct phases: Prefill Stage. The model processes the entire input sequence and caches the key-value (KV) matrices for each layer: Rnd, Rnd. (2) Generation Stage. Tokens are generated autoregressively. For each new token yj, the model computes: where is the models output. Previous work has also recognized that compressing the KV cache of LMs allows for improved performance at much smaller memory costs (Qin et al., 2023; Ge et al., 2023; Corallo and Papotti, 2024). In general there are three levels of compression: (1) task-agnostic compression where no task information is present (Chari et al.; Zhang et al., 2024a), (2) ad-hoc compression, where the compression is tailored for single specific task such as question-answering (Fu et al., 2024; Cai et al., 2024), and (3) query-aware compression where the compression happens w.r.t. specific example (Rehg, 2024; Xu et al., 2024). Our work proposes new compression technique that works for any task specified in the prompt. To explain how compression works, we detail query-aware iterative approach that compresses the KV cache by retaining only the most relevant Key-Value vectors for the query given at inference time (Corallo and Papotti, 2024). Let be the chunk size, and let {c1, c2, . . . } be the segments obtained by slicing the context x(cont). The segment-to-document ratio is defined as = (5) qnew, knew, vnew Rd, and updates the cache as follows: (cid:20) knew , (cid:21) (cid:20) vnew (3) (4) (cid:21) . where is the total document length. At iteration i, the method takes as input (cid:104) (cid:101)Ki1, (cid:101)Vi1 (cid:125) (cid:123)(cid:122) (cid:124) previous compressed cache , , ci (cid:124)(cid:123)(cid:122)(cid:125) current chunk (cid:105) (cid:124)(cid:123)(cid:122)(cid:125) question , Thanks to the cached KV matrices, self-attention complexity reduces from O(n2d) to O(nd), significantly improving efficiency. However, for large n(cont), storing these matrices for every layer can be prohibitively memory-intensive. where (cid:101)Ki1, (cid:101)Vi1 Rri1d denote the compressed cache from the previous iteration, ci Rmd is the chunk of context for the current iteration, and Rqd is the question to be answered. During the forward pass, the multi-head attention scores are computed. Crucially, the crossattention submatrix W(q,c) Rq(ri1+m), captures how each question token attends to both the previous cache and the current chunk. The online method then selects the top token positions (according to the highest attention scores in W(q,c)) to form (cid:101)Ki, (cid:101)Vi. After processing all chunks, the final (cid:101)K, (cid:101)V Rkd provide global representation of the entire context, at substantially reduced size. Agnostic methods use similar principles but in single offline computation of the cache, thus without making use of the query. 2.2 RAG and Knowledge Intensive Tasks Knowledge-intensive tasks require models to utilize external information to arrive at accurate anThis category encompasses tasks like swers. question-answering, summarization, and factchecking (Kwiatkowski et al., 2019; Petroni et al., 2020). While larger models have demonstrated improved capacity to store knowledge whitin their parameters (Tirumala et al., 2022; Biderman et al., 2023; Wang et al., 2024), they face limitations, such as difficulties in updating or modifying the embedded information (De Cao et al., 2021) and hallucinations (Ji et al., 2023). To address this, RAG integrates retrieval mechanisms with generative language models, enhancing the accuracy by explicitely incorporating external knowledge (Lewis et al., 2020; Borgeaud et al., 2022; Gao et al., 2023). However, RAG has its own challenges: crucial information might not be included in the top-ranked retrieval results, preventing the language model from reasoning over it."
        },
        {
            "title": "3 Problem Formulation",
            "content": "is representations fundamental challenge in compressing longachieving querycontext agnostic compression, where precomputed compressed cache (cid:101)K, (cid:101)V remains effective for any query at inference time. However, empirical results indicate that existing query-agnostic methods exhibit significant performance degradation, particularly under high compression rates, often falling behind both full-context processing and RAG (NVIDIA, 2024). On the other hand, query-aware compression, circumvents the challenges of long-context models by (i) processing documents in smaller segments while adjusting positional embeddings accordingly, and (ii) reducing the KV memory footprint in proportion to the compression rate. In practice, query-aware compression can outperform retrieval-based methods like RAG. However, critical drawback is its reliance on specific user query at compression time. While effective for single-query scenarios, this assumption becomes impractical when multiple queries need to be processed, as rerunning the compressor for each query is computationally prohibitive, thus undermining the original goal of avoiding largescale retrieval or excessive context expansion. This raises key research question: Can we design query-agnostic compression method that preserves efficiency while maintaining qualitative competitive performance?"
        },
        {
            "title": "4 Methodology",
            "content": "In this section, we present our task-aware, queryagnostic compression strategy, motivated by the learning capabilities of remarkable in-context modern LLMs. We describe how we obtain single, reusable cache that covers an entire tasks external knowledge."
        },
        {
            "title": "4.1 Motivation via In-Context Learning",
            "content": "LLMs demonstrate remarkable in-context learning capabilities (Brown et al., 2020; Dong et al., 2022): once they read sufficiently large prefix of tokens, they often infer the nature of the task without any parameter updates. In practice, tokens later in the input are easier to predict because the model has more context to condition on. An intuitive analogy is student preparing for exams. When asked new question with no references at hand (i.e., zero-shot inference), both the human student and the LLM rely solely on prior knowledge. If few solved examples are provided (fewshot learning), they adapt by identifying solution patterns from the examples, thereby reducing uncertainty. Giving the student all available reference material (akin to letting an LLM observe the entire corpus) can maximize accuracy. Our objective is to construct compressed representation of the corpus in advance, much like curated cheat sheet that condenses the key information. To explore how LLMs dynamically allocate attention within their input, we analyze cross-attention patterns across different prompting strategies, as Figure 3: We examine how the model attends to context tokens when conditioned on the last token, task description, description with few-shot examples, and description with both few-shot examples and question. As we increase the information in the prompt, the cross attention between the prompt and the context better discriminates the context tokens that are relevant for decoding the answer. The perplexity is calculated on the loss for the answer. shown in Figure 3. Notably, in (c), when task description and few examples are used, the model commits to subset of tokens in the latent space that is similar to (d), the query-aware case, as evidenced by the qualitative similarity in attention distribution and the corresponding reduction in perplexity on the final answer. This suggests that sufficiently structured prompt allows the model to internally resolve much of the task-specific ambiguity, even before an explicit question is introduced. Guided by these insights, our goal is to construct task-aware, query-agnostic compression strategy that enables efficient, reusable caching of task-relevant knowledge."
        },
        {
            "title": "Compression",
            "content": "Concretely, we create compressed cache capable of supporting any query within defined task domain. The procedure is: (1) Define Task Description (T). Rather than targeting single query, we incorporate Task Description specifying the type of questions to be answered (e.g., Answer factual questions about this corpus). When available, we add few examples to better illustrate the target task. (2) Offline Compression. We adapt an existing query-aware method by replacing the query with the Task Description (Corallo and Papotti, 2024). The corpus is split into chunks {c1, c2, . . . }, and we iteratively compress these chunks together with T: (cid:2) (cid:101)Ki1, (cid:101)Vi1, ci, T(cid:3). This yields compressed cache ( (cid:101)K, (cid:101)V) that captures essential domain knowledge. Crucially, this compression is computed only once. (3) Efficient Query Resolution. When facing new query qnew from the same domain, we prepend the precomputed cache: x(prompt) = (cid:2) (cid:101)K, (cid:101)V, qnew (cid:3). No further compression or retrieval is necessary. The LLM conditions on (cid:101)K and (cid:101)V to answer, reusing the relevant external knowledge. We develop two variants of this approach: KVCompress ZS (Zero-Shot Task Description) uses only broad task instructions, and KVCompress FS (Few-Shot Task Description) includes sample task examples (such as QA pairs). Emthis offline, query-agnostic comprespirically, sion speeds up inference while offering more robust coverage of multi-hop or broad queries than RAGs similarity-based lookups. In contexts where RAG struggles to gather widely dispersed evidence or gets confused by near-identical entity names, the global coverage of the compressed cache avoids such pitfalls."
        },
        {
            "title": "5.1 Models",
            "content": "use experiments Our LLAMA-3.1-8BINSTRUCT (Dubey et al., 2024) with 5 knowledgeinfusion baselines: RAG, KVCOMPRESS ZS, KVCOMPRESS FS, KVCOMPRESS FS+Q, and Full Context upper bound. For the compression baselines, we set = 2 in all experiments, as defined in Equation (5). For RAG, we use BGE-LARGE-EN-V1.5 (Xiao et al., 2023b) as the retriever, filling the entire context with the top-k retrieved documents, each containing 256 tokens. The same prompt is used across all models.1 1Detailed prompt configurations for each 5.2 Datasets We evaluate these methods on three datasets: long in-context LongBench v2 (Bai et al., 2024) is multiplechoice benchmark designed to evaluate the reasoning capabilities of LLMs on realistic long-context multitasks. It comprises questions distributed across six major categories: single-document QA, learning, multi-document QA, long-dialogue history understanding, code repository understanding, and long structured data understanding. Context lengths vary significantly, State-of-thefrom 8,000 to 2 million words. art models, without enhanced reasoning prompts, attained only 50.1% accuracy. In our experiments, we evaluate using Exact Match scores. The ground truth for evaluation consists of selecting one option among A, B, C, or D. For the full context evaluation, we use the entire context, truncating to keep the first and last halves if it exceeds the context window length, and constrained generation over the provided options with 1 max_new_tokens (De Cao et al., 2020). LongBench (Bai et al., 2023) is benchmark also designed for long-context understanding. It covers 21 datasets across six tasks, including single-document and multi-document QA, summarization, code completion, few-shot learning, and synthetic task. The benchmark has an average context length of 6,711 words for English tasks. LongBench uses automated evaluation metrics, such as F1 for question answering tasks (Rajpurkar et al., 2016) and ROUGE-L for summarization tasks (Lin, 2004), and Edit Similarity for the Code Completion task (Svyatkovskiy et al., 2020). Our Synthetic Dataset is designed to enable full control over the interconnectivity among text chunks in the corpus for QA task. We use two query types: direct retrieval, which retrieval systems can handle easily, and join-like queries, requiring broader comprehension of the entire corpus. Ground truth answers are generated as lists of entities so that model evaluation is straightforward: predictions and ground truths are normalized (removing punctuation and converting to lowercase) and score is obtained by computing word are available dataset https://anonymous.4open.science/r/ context-compression-2-6B58/conf/custom_ datasets/. configuration our in files: Figure 4: Overview of our synthetic dataset. example, the connectivity level is set to 2. In this Figure 5: Overview of our questions. In this example, the connectivity level is set to 2. overlap between the predicted and ground truth entities. We describe the dataset creation next."
        },
        {
            "title": "6 Synthetic Dataset Construction",
            "content": "We construct synthetic dataset designed to precisely control corpus complexity and the connectivity level between text chunks. By varying interchunk connectivity, we are able to thoroughly evaluate different methods, identifying the exact scenarios where each technique performs well or fails. Figure 4 illustrates the structured design of our corpus. Our dataset will be publicly released to support future research. 6.1 Structured Entities and Corpus Chunks We define three entity types. People. Each person is described through template-structured biographies containing attributes such as name, age, occupation, city, and hobbies. To maintain uniformity and facilitate controlled experiments, each biography text chunk is standardized to length of 256 tokens using additional neutral filler text. Projects. Each project has attributes including title, domain, sponsor, year started, and descriptive summary. Like for people, each text chunk is standardized to 256 tokens. Memberships. membership represents the relationship between people and projects and specifies the role (e.g., Engineer, Manager) and department (e.g., R&D, Marketing) that person holds in project. These text chunks similarly include filler text to meet the fixed-length criterion. We generate multiple corpus instances with varying connectivity levels, ranging from 1 to 8, where level means each person links to exactly projects. Higher connectivity increases dataset complexity by distributing relevant information about person across multiple membership and project chunks, thus challenging the models ability to synthesize scattered information. Each corpus at given connectivity level comprises exactly 32k tokens, ensuring consistent corpus size across experiments."
        },
        {
            "title": "6.2 Controlled Question Types for Evaluation",
            "content": "To rigorously evaluate the performance of both KV-cache compression and retrieval-based methods, we generate two primary question categories (Figure 5). Direct Retrieval Questions. These questions require information localized within single Memberships chunk. Example templates include: Which projects does pname belong to? have does Which pname in role projtitle? Which department is pname part of? Where variables like pname are instantiated at generation time. Answering these queries does not require cross-chunk synthesis. Join-like Questions. Answering these queries require combining information across multiple Memberships and Projects chunks. For example: What are pnames project domains? In which years did pnames projects begin? Who sponsors pnames projects? Addressing these queries tests models capability for multi-hop reasoning and synthesis across distributed knowledge sources. As the connectivity level grows, these join-like questions become increasingly complex, requiring the aggregation of information from multiple chunks. For each connectivity level (1 through 8), we generate 50 distinct queries: 25 direct-retrieval and 25 join-like, totaling 400 distinct queries across all connectivity levels. Additionally, we create targeted variant of direct retrieval questions with highly similar entity names (e.g. Person_01, Person_02, . . . ) to test embedding-based retrieval robustness against closely related entities."
        },
        {
            "title": "7 Results and Discussions",
            "content": "We structure our analysis around five research questions over our synthetic dataset experiments while also drawing connections to the LongBench and LongBench v2 results. When Does RAG Fail? RAG exhibits significant limitations in multi-hop reasoning and highconnectivity scenarios. Our synthetic dataset reveals sharp performance decline for join-like questions those requiring the integration of dispersed information (Figure 6). This degradation stems from RAGs reliance on similarity-based retrieval and frequently omits crucial chunks containing necessary details, thus limiting the models ability to accurately answer these queries. Additionally, RAG struggles with entity disambiguation, particularly for similarly named entities (Figure 7). Embedding-based retrieval frequently misidentifies relevant passages and retrieves irrelevant chunks due to embedding proximity among similarly named entities. These limitations are further reflected in the results for the LongBench v2 hard questions (Figure 8), where KVCOMPRESS ZS already outperforms RAG, despite the absence of few-shot examples. RAG gains only marginal improvements with longer contexts, highlighting the limitations of retrievalbased methods in scenarios that require complex long-context understanding. In LongBench (Figure 9), KVCOMPRESS ZS consistently surpasses RAG in summarization and code completion tasks, which require synthesizing multiple passages or maintaining broader conFigure 6: Performance by increasing target context length (64x to 8x compression rate) and connectivity level for Join-like questions in the synthetic dataset. The dashed line indicates for which connectivity level RAG gets the needed chunk for given context length. Figure 7: Performance by retrieval context length size (128x to 16x compression rate) for Direct Retrieval questions with highly similar entity names in the synthetic dataset. text and is competitive in the other tasks, except the QA, where it is the Few Shot setting that competes with RAG. Takeaway: RAG fails in multi-hop reasoning and entity disambiguation, limiting its effectiveness in synthesis tasks with long-context. When Is RAG Effective? Experiments show that RAG performs well when answers are localized within single document chunk. In direct-retrieval scenarios, where each answer is self-contained, RAGs proves effective. This trend is evident in our synthetic dataset (Figure 10) and in LongBench results (Figure 9), where RAG achieves competitive performance on tasks such as direct question answering. Takeaway: RAG excels when answers are selfcontained within the retrieved chunks, making it effective for narrowly scoped questions. Figure 8: Performance by retrieval context length size (64x to 8x compression rate) for the Hard Questions in LongBench v2. What Are the Benefits of Our Task-Aware, Query-Agnostic Compression Method? Our proposed method departs from traditional retrieval and query-aware compression approaches by precomputing unified, task-aware compressed cache that is reusable across multiple queries. This approach offers several advantages over both full-context processing and RAG: it dramatically reduces memory overhead and inference time while preserving global representation of the corpus. Moreover, it overcomes RAGs limitations in broad query scenarios as shown in Figure 6 and in the LongBench v2 results both in Figure 9 and in Table 1. Besides RAG, we evaluate our task-aware compression strategies against several baseline compression methods, including Expected Attention (NVIDIA, 2024), StreamingLLM (Xiao et al., 2023a), and SnapKV (question-agnostic) (Li et al., 2025b). As shown in Figure 11, our synthetic dataset configured with high-connectivity level of 8 to test multi-hop reasoning reveals critical gap in the effectiveness of these compression techniques. These methods, which treat the Figure 9: Performance results on Longbench. Our FS variant is reported when examples are available (QA tasks). For the QA tasks, the examples used in KVCompress FSt are taken (and removed) at random from the test set. Dataset (Metric) R. Length KVCompress ZS RAG GovReport (Rouge-L ) MultiNews (Rouge-L ) Full context 24.63 512 1000 2000 21.03 21.57 22.58 Full context 18.45 512 1000 2000 16.79 17.86 18.42 Full context 3.25 PassageCount (Accuracy ) 512 1000 2000 5.77 5.50 5.00 LCC (Edit Sim ) Full context 20.13 512 1000 16.62 19.35 20.66 19.14 19.92 21.59 9.35 13.10 17.39 1.50 2.50 4.00 19.36 17.89 18.61 Table 1: Performance of KVCompress ZS and RAG across all query-agnostic datasets in LongBench. instructionis optimal for query-agnostic tasks, In LongBench (Figure 9), as shown in Table 1. for non-QA tasks, it performs comparably or better than RAG; in hard settings like LongBench v2 (Figure 8) it outperforms RAG. KVCOMPRESS FS incorporates few examples and excels in tasks such as questionanswering. As shown in Figures 1 and 9, FS significantly reduces the performance gap with RAG, whereas ZS struggles in these scenarios. Takeaway: ZS is better suited for tasks that are natively query-agnostic and FS is better when dealing with examples such as in QA tasks. Figure 10: Performance by connectivity level (64x compression rate) for the Direct Retrieval questions in the synthetic dataset. context in isolation and are blind to the specific requirements of the query, consistently underperform. Moreover, the results underscore the utility and sensitivity of our synthetic dataset, which effectively captures nuances in method performance, clearly differentiating between naive and more sophisticated compression techniques. This dataset serves as robust benchmarking tool for future research into KV-cache compression methods, particularly for evaluating their comprehensive knowledge reasoning capabilities in challenging scenarios. Takeaway: Task-aware query-agnostic compression is scalable and efficient achieving better performance when RAG fails. How to Choose the Compression Variant? key decision is whether to use the zero-shot (ZS) or few-shot (FS) variant. Our results suggest:"
        },
        {
            "title": "8 Efficiency",
            "content": "Figure 12 reports inference efficiency for increasing corpus size from 16k to 128k tokens, retrieval context size fixed at 8k tokens, prompt length at 512 tokens, and chunk lengths determined by dividing the corpus length by the number of slices (s) as defined in Equation (5). For large corpus lengths, KVCOMPRESS FS+Q shows better inference speed compared to processing the entire context with Flash Attention 2 implementation (Dao, 2023). As expected, KVCOMPRESS FS exhibits the lowest inference latency across all corpus lengths, being up to 2x faster than RAG. This superior performance arises from the offline precomputation of the KV cache in KVCOMPRESS FS, which enable the model to bypass tokenization and prefill at inference time."
        },
        {
            "title": "9 Conclusion and Future work",
            "content": "In this paper, we presented task-aware context compression approach that enhances the ability of LLMs to consume large corpora by efficiently populating the KV cache with condensed contextual representations. Our method enables multistep reasoning while mitigating the limitations of traditional retrieval mechanisms. Empirical results show that our approach outperforms RAG baselines, reduces infrastructure overhead, and improves inference latency. By distilling raw text into compact representations offline, our method establishes new paradigm for corpus-level reasoning, addressing critical constraints in existing long-context models. Future directions include integrating head-wise and layer-wise compression strategies, leveraging prior findings that certain heads and layers contribute less to model performance and can be selectively compressed in favor of more informative ones (Feng et al., 2024; Zhang et al., 2024b). Additionally, our results highlight complementary strength between KV compression and RAG: KV compression excels in broad queries, while RAG is more effective for narrow queries. This raises the question of whether hybrid approach could further enhance retrievalcompressing the corpus offline for global coverage while dynamically fetching top-K chunks online to better address narrow queries. Exploring these strategies could unlock new efficiencies in long-context processing. Figure 11: Comparison against query-agnostic compressors on our synthetic dataset for Join-like queries with connectivity level=8 across decreasing compression levels (64x to 8x). Figure 12: Time to first token with increasing corpus length (context length=8192 and question length=512). What Are the Benefits of Integrating QueryAware Compression Signals? Integrating queryaware signals (as in KVCompress FS+Q) yields substantial performance gains, particularly in scenarios where computation time is less critical, e.g., test-time settings like Deepseek r1 or OpenAI o1 (Guo et al., 2025). By embedding explicit query cues during compression, the model better prioritizes critical information, as shown by improved accuracy across all tasks and datasets. Takeaway: Query-aware compression boosts accuracy at the cost of an increase in processing time. Query-agnostic methods like our proposal are the fastest. All results and takeaways are consistent and held true when we applied the same methods and datasets to QWEN2.5-7B-INSTRUCT (Yang et al., 2024), with its context window extended to 128k using the Yarn technique (Peng et al., 2023), thereby reinforcing the robustness and generalizability of our approach across LLMs."
        },
        {
            "title": "References",
            "content": "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. 2023. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508. Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, et al. 2024. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. arXiv preprint arXiv:2412.15204. Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, and Mohamed Abdelrazek. 2024. Seven failure points when engineering retrieval augmented generation sysIn Proceedings of the IEEE/ACM 3rd tem. International Conference on AI EngineeringSoftware Engineering for AI, pages 194199. Stella Biderman, Usvsn Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff. 2023. Emergent and predictable memorization in large language models. Advances in Neural Information Processing Systems, 36:28072 28090. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, JeanBaptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by reIn Internatrieving from trillions of tokens. tional conference on machine learning, pages 22062240. PMLR. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu, et al. 2024. Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling. arXiv preprint arXiv:2406.02069. Vivek Chari, Guanghui Qin, and Benjamin Van Durme. Kv-distill: Nearly lossless context compression for transformers. Giulio Corallo and Paolo Papotti. 2024. Finch: Prompt-guided key-value cache compression Transactions of for large language models. the Association for Computational Linguistics, 12:15171532. Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691. De Cao, Aziz, and Titov. 2021. Editing factual knowledge in language models. In EMNLP 2021-2021 Conference on Empirical Methods in Natural Language Processing, Proceedings, pages 64916506. Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2020. Autoregressive entity retrieval. arXiv preprint arXiv:2010.00904. Alessio Devoto, Yu Zhao, Simone Scardapane, and Pasquale Minervini. 2024. simple and effective l_2 norm-based strategy arXiv preprint for kv cache compression. arXiv:2406.11430. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. Survey on inContext Learning. arXiv:2301.00234. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024. From local to global: graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130. Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, and Kevin Zhou. 2024. Ada-kv: Optimizing kv cache eviction by adaptive budget allocation for efficient llm inference. arXiv preprint arXiv:2407.11550. Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, and Kevin Zhou. 2025. Identify critical kv cache in llm inference from an output perturbation perspective. arXiv preprint arXiv:2502.03805. Yih, Tim Rocktäschel, et al. 2020. Retrievalaugmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474. Yu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, and Wen Xiao. 2024. Not all heads matter: head-level kv cache compression method with integrated retrieval and reasoning. arXiv preprint arXiv:2410.19258. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023. Retrieval-augmented generation for large lanarXiv preprint guage models: survey. arXiv:2312.10997, 2. Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. 2023. Model tells you what to discard: Adaptive kv arXiv preprint cache compression for llms. arXiv:2310.01801. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Siddharth Jha, Lutfi Eren Erdogan, Sehoon Kim, Kurt Keutzer, and Amir Gholami. 2024. Characterizing prompt compression methods arXiv preprint for long context inference. arXiv:2407.08892. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM computing surveys, 55(12):138. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, et al. 2025a. Minimax-01: Scaling foundation modarXiv preprint els with lightning attention. arXiv:2501.08313. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. 2025b. Snapkv: Llm knows what you are looking for before generation. Advances in Neural Information Processing Systems, 37:2294722970. Chin-Yew Lin. 2004. ROUGE: package for In Text automatic evaluation of summaries. Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173."
        },
        {
            "title": "Lost",
            "content": "Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. 2023. Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache In Thirty-seventh compression at test time. Conference on Neural Information Processing Systems. NVIDIA. 2024. Llm kv cache compression made easy. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. 2020. Kilt: benchmark for knowledge intensive language tasks. arXiv preprint arXiv:2009.02252. streaming language models with attention sinks. arXiv preprint arXiv:2309.17453. Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023b. C-pack: Packaged resources to advance general chinese embedding. Yuhui Xu, Zhanming Jie, Hanze Dong, Lei Wang, Xudong Lu, Aojun Zhou, Amrita Saha, Caiming Xiong, and Doyen Sahoo. 2024. Think: Thinner key cache by query-driven pruning. arXiv preprint arXiv:2407.21018. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. arXiv preprint Qwen2. 5 technical report. arXiv:2412.15115. Shuo Yu, Mingyue Cheng, Jiqian Yang, and Jie Ouyang. 2024. knowledge-centric benchmarking framework and empirical study for retrieval-augmented generation. arXiv preprint arXiv:2409.13694. Rongzhi Zhang, Kuang Wang, Liyuan Liu, Shuohang Wang, Hao Cheng, Chao Zhang, and Yelong Shen. 2024a. Lorc: Low-rank compression for llms kv cache with progresarXiv preprint sive compression strategy. arXiv:2410.03111. Xuan Zhang, Cunxiao Du, Chao Du, Tianyu Pang, Wei Gao, and Min Lin. 2024b. Simlayerkv: simple framework for layer-level kv cache reduction. arXiv preprint arXiv:2410.13846. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. 2023. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710. Guanghui Qin, Corby Rosset, Ethan Chau, and Benjamin Van Durme. Nikhil Rao, 2023. Dodo: Dynamic contextual compresarXiv preprint sion for decoder-only lms. arXiv:2310.02409. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250. Isaac Rehg. 2024. Kv-compress: Paged kvcache compression with variable compression arXiv preprint rates per attention head. arXiv:2410.00161. Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. Intellicode compose: Code generation using transformer. In Proceedings of the 28th ACM joint meeting on European software engineering conference and symposium on the foundations of software engineering, pages 14331443. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. 2022. Memorization without overfitting: Analyzing the training dynamics of large language models. Advances in Neural Information Processing Systems, 35:3827438290. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. AtIn Advances in Neutention is all you need. ral Information Processing Systems, volume 30. Curran Associates, Inc. Xinyi Wang, Antonis Antoniades, Yanai Elazar, Alfonso Amayuelas, Alon Albalak, Kexun Zhang, and William Yang Wang. 2024. Generalization vs memorization: Tracing language models capabilities back to pretraining data. arXiv preprint arXiv:2407.14985. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023a. Efficient"
        }
    ],
    "affiliations": [
        "EURECOM",
        "Johns Hopkins University",
        "SAP Labs",
        "Samaya AI"
    ]
}