{
    "paper_title": "A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation",
    "authors": [
        "Yukang Feng",
        "Jianwen Sun",
        "Chuanhao Li",
        "Zizhen Li",
        "Jiaxin Ai",
        "Fanrui Zhang",
        "Yifan Chang",
        "Sizhuo Zhou",
        "Shenglin Zhang",
        "Yu Dai",
        "Kaipeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Multimodal Models (LMMs) have significantly improved multimodal understanding and generation. However, these models still struggle to generate tightly interleaved image-text outputs, primarily due to the limited scale, quality and instructional richness of current training datasets. To address this, we introduce InterSyn, a large-scale multimodal dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR) method. InterSyn features multi-turn, instruction-driven dialogues with tightly interleaved imagetext responses, providing rich object diversity and rigorous automated quality refinement, making it well-suited for training next-generation instruction-following LMMs. Furthermore, to address the lack of reliable evaluation tools capable of assessing interleaved multimodal outputs, we introduce SynJudge, an automatic evaluation model designed to quantitatively assess multimodal outputs along four dimensions: text content, image content, image quality, and image-text synergy. Experimental studies show that the SEIR method leads to substantially higher dataset quality compared to an otherwise identical process without refinement. Moreover, LMMs trained on InterSyn achieve uniform performance gains across all evaluation metrics, confirming InterSyn's utility for advancing multimodal systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 7 2 4 9 0 . 6 0 5 2 : r A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation Yukang Feng1,2 Jianwen Sun 1,2 Chuanhao Li5 Zizhen Li1,2 Fanrui Zhang1,4 Yifan Chang4 Sizhuo Zhou1,4 Jiaxin Ai1,3 Shenglin Zhang1 Yu Dai1 Kaipeng Zhang2,5 1 Nankai University 4 University of Science and Technology of China 2 Shanghai Innovation Institute 3 Wuhan University 5 Shanghai AI Laboratory"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Multimodal Models (LMMs) have significantly improved multimodal understanding and generation. However, these models still struggle to generate tightly interleaved image-text outputs, primarily due to the limited scale, quality and instructional richness of current training datasets. To address this, we introduce InterSyn, large-scale multimodal dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR) method. InterSyn features multi-turn, instruction-driven dialogues with tightly interleaved imagetext responses, providing rich object diversity and rigorous automated quality refinement, making it well-suited for training next-generation instruction-following LMMs. Furthermore, to address the lack of reliable evaluation tools capable of assessing interleaved multimodal outputs, we introduce SynJudge, an automatic evaluation model designed to quantitatively assess multimodal outputs along four dimensions: text content, image content, image quality, and image-text synergy. Experimental studies show that the SEIR method leads to substantially higher dataset quality compared to an otherwise identical process without refinement. Moreover, LMMs trained on InterSyn achieve uniform performance gains across all evaluation metrics, confirming InterSyns utility for advancing multimodal systems."
        },
        {
            "title": "Introduction",
            "content": "Multimodal understanding and generation are critical capabilities toward artificial general intelligence. In the past two years, multimodal LLMs [13] have shown remarkable performance in multimodal understanding and even surpassed humans in some areas, while we have also seen many impressive advances in high-quality image generation [4, 5]. However, their restriction to generating either text or image outputs in isolation, while real-world scenarios usually require mixed-modality outputs. Recently, pioneer unified multimodal understanding and generation models (uni-MLLMS), such as Chameleon [6] and Janus-Pro [7], have shown great potential. However, they struggle to generate instruction-following interleaved image-text generation. They suffer from manifesting semantic drift, low imagetext synergy, and low image quality. The main challenges are the lack of high-quality training datasets and the absence of reliable evaluation to guide model development. Although related training datasets [812] exist, they suffer from several critical limitations: (1) Unstable quality: Built on web-crawled sources [13, 9] or reused corpora [14, 8] with inconsistent quality and lack standardized quality control mechanisms. Equal contributions. Corresponding author Preprint. Figure 1: InterSyn: Topic hierarchy and interleaved question answering samples (2) Limited domain coverage: Focus on narrow tasks, such as creative content generation [15], story writing [16, 17], or multimodal script writing [13], limiting their applicability to broader real-world scenarios. (3) Low interaction complexity. The majority provide static documents, image-text pairs or singleturn prompts, failing to capture the multi-turn, deeply interleaved conversations required by real scenarios. To address the above issues, we introduce InterSynthe first fully-automated, high-quality largescale dataset for instruction-following, multi-turn question answering with interleaved imagetext responses. InterSyn offers 1.8 single-turn and 50,000 multi-turn dialogues spanning 8 domains, 65 categories, and 3,500 fine-grained topics. Its generation is driven by Self-Evaluation with Iterative Refinement (SEIR) pipeline that embeds self-checking and feedback loops into every iteration, guaranteeing semantic completeness, cross-modal synergy, and contextual relevance with minimal human intervention. Evaluation is another challenge for interleaved image-text generation. Though some related benchmarks [1822] exist, there are some limitations: (1) Limited domain and scale: small, task-specific sets cannot cover realistic multi-turn dialogue needs. (2) Costly manual evaluation: accurate assessment still hinges on human judges, whose expense and delay hinder large-scale, rapid benchmarking. (3) Weak alignment with human preference: current automatic metrics significantly diverge from human judgments on fine-grained multimodal reasoning. (4) Narrow evaluation scope: emphasise surface correctness while overlooking synergy and overall answer quality. We propose SynJudge, reliable and comprehensive judge model for interleaved image-text generation, which aligns with human judgment. SynJudge can provide interpretable and quantitative 2 feedback, facilitating more effective model training and refinement. The interleaved image-text response is assessed along four dimensions: text content completeness(TCC), image content completeness(ICC), image quality(IQ), and image-text synergy(ITS). Unlike traditional imagetext consistency metrics, the imagetext synergy (ITS) metric rewards tight, complementary alignment and penalizes redundancy between the textual and visual modalities. To evaluate the effectiveness of the SEIR pipeline, we conduct human evaluation on both question and answer quality. Human evaluation results indicate that after three rounds of iteration, in terms of question generation, there is 32% improvement compared to the initial version. In terms of answer generation, the improvements in the four dimensions compared to the initial answer are as follows: TCC (15%), ICC (11%), IQ (1%), and ITS (19%). Similarly, we conducted experiments on the gap between several judgment models and human. The results show that the method based on the original MLLM has an average gap of 13% from humans, while SynJudge is closer to human preferences, with deviation of only 5%. Finally, we used InterSyn to conduct fine-tuning experiments on existing models. The experimental results show that the maximum improvements of the fine-tuned models in four capabilities are as follows: TCC (29.7%); ICC (10.3%); IQ (6%); ITS (52.1%). Our main contributions are summarized as follows: We present InterSyn, the first large-scale, multi-turn, instruction-following dataset for interleaved image-text question answering, covering wide range of topics. We propose SEIR, method that ensures high-quality data generation across three refinement steps with minimal manual effort. We introduce SynJudge, multi-dimensional evaluation model for scoring interleaved outputs, enabling fine-grained assessment and effective feedback for model improvement. We conduct comprehensive experiments demonstrating that InterSyn substantially enhances LMM performance in instruction alignment, image-text synergy, and multi-turn reasoning, contributing to the advancement of unified multimodal systems."
        },
        {
            "title": "2 Related Work",
            "content": "Models for Interleaved Image-Text Generation. Recent advances in Multimodal Large Language Models (MLLMs), such as Flamingo [23], InternVL [24, 2], and Qwen-VL [3, 25], have substantially improved multimodal understanding. Meanwhile, diffusion models [26, 5, 27] achieve strong visual generation performance. To unify understanding and generation, models such as MiniGPT-5 [28] and Show-o [29] combine autoregressive text generation with diffusion-based image synthesis. More recent efforts [6, 3032] adopt unified autoregressive frameworks for interleaved generation. However, these models are not explicitly optimized for instruction-following and often struggle to maintain coherence and cross-modal consistency. Datasets for Interleaved Image-Text Generation. High-quality interleaved image-text data is crucial for training multimodal models. Existing largescale datasets like MMC4 [8], OBELICS [9], and CoMM [11] are primarily document-level corpora constructed from web sources, but often suffer from noise, weak alignment and low interaction intensity. Several benchmarks, such as OpenLEAF [18], InterleavedBench [22], and OpenING [19], focus on specific tasks. LeafInstruct [12] constructs an interleaved image-text dataset by filtering samples from existing corpora [8, 33, 14]. However, both benchmarks and datasets remain limited in scale and diversity. Evaluation for Interleaved Image-Text Outputs Early multimodal evaluation metrics independently assessed text quality [34, 35] and image quality [36, 37]. Subsequent metrics [3842] targeted imagetext consistency, yet still inadequately evaluated the quality of interleaved outputs. More recent efforts, including InterleavedEval [12] and CoMM [11], leveraged large multimodal models (LMMs) for holistic assessment, but often exhibit misalignment with human judgment. OpenING [19] proposed IntJudge for pairwise comparisons, but it lacks fine-grained, quantitative scoring for individual responses, limiting its applicability for model training and refinement. 3 Figure 2: Overview of the InterSyn data construction pipeline."
        },
        {
            "title": "3.1 Overview",
            "content": "In this section, we describe the process of constructing the InterSyn dataset. Figure 2 illustrates the overall InterSyn construction pipeline, covering both the preparatory work and the SEIR method."
        },
        {
            "title": "3.2 Dataset Preparatory Work",
            "content": "InterSyns preparatory work involves five major stages: Question Collection. We recruited 25 participants, each providing 40 questions drawn from natural conversational scenarios, resulting in total of 1,000 questions. Question Filtering and Benchmark. We combined LLM-based filtering and expert review to select high-quality questions. Redundant, ambiguous, uncommon, and overly subjective samples were removed based on predefined criteria. In total, 500 diverse and high-quality questionsincluding both original and human-augmented sampleswere selected to construct benchmark. Question Template Extraction. Based on the selected high-quality questions, we extracted generalized question templates to enable scalable question generation. Basic Topic Hierarchy. We performed AI-assisted topic extraction from the filtered questions and manually organized the results to build basic topic hierarchy, ensuring clear logical dependencies and coherent topic relations. Topic Hierarchy Expansion. We further refined and expanded the basic topic hierarchy to improve both coverage and granularity. Combining AI-assisted topic suggestions with expert curation, we constructed well-structured hierarchy that supports diverse and scalable data generation."
        },
        {
            "title": "3.3 SEIR Method",
            "content": "The proposed SEIR method establishes three-stage refinement process for generating high-quality multimodal data, as illustrated in Figure 2. Given conversation turn parameter N+ and global refinement depth parameter N+, for each turn {1, . . . , }, the system generates data through three-stage refinement process, which involves three cascaded modules: Question Refinement (QR), Answer Refinement (AR), and Image Refinement (IR). Each stage of refinement within turn follows Markovian property, where each iteration only depends on the immediate previous state. Across turns, topic consistency is maintained, and cross-turn coherence is achieved through contextual inheritance. This comprehensive approach allows for the generation of rich multimodal data with iterative optimization across multiple conversation turns and at each stage. Let denote the set of question templates, and denote the set of topics. For conversation with turns, let the history up to turn 1 be denoted as H(t1): H(t1) = {(q(i), a(i), i(i))}t1 i=1 (1) where each tuple (q(i), a(i), i(i)) represents the question, text answer, and associated image at turn i. We define H(0) = . For each dataset sample, the data generation pipeline operates as follows: (Z) (Sample-level shared topic) (2) The generation process for each conversation turn (1 ) proceeds as follows: 1. Turn Initialization: Sample question template τ (t) and generate an initial question q(t) 0 : τ (t) (T ) q(t) 0 = ML(pg(τ (t), z, H(t1))) where ML denotes the language model, and pg() represents the generation prompt. 2. Question Refinement (QR): For each refinement iteration {1, . . . , K}: (a) Generate refinement suggestion: s(t,k) = ML(ps(q(t) k1, z, H(t1))) (3) (4) (5) where ps() denotes the suggestion prompt, q(t) k1 denotes the question at iteration 1. (b) Refine question if s(t,k) = : = ML(pr(q(t) q(t) where pr() represents the refinement prompt. The iteration stops when = or no further modifications are suggested (s(t,k) = ), yielding the final question q(t). k1, s(t,k) (6) )) 3. Answer Refinement (AR): Generate initial answer and temp caption pair: (a(t) 0 , γ(t) 0 ) = ML(pg(q(t), H(t1))) represents the initial answer and γ(t) where a(t) 0 0 For each refinement iteration {1, . . . , K}: (a) Generate refinement suggestion: represents the initial temp caption. = ML(ps(a(t) s(t,k) k1, γ(t) k1, q(t), H(t1))) (b) Refine answer and temp caption if s(t,k) = : ) = ML(pr(a(t) The process terminates when = or no further refinements are suggested(s(t,k) yielding the final answer a(t) and the temporary caption γ(t). k1, s(t,k) k1, γ(t) , γ(t) (a(t) )) = ), (7) (8) (9) 5 4. Image Refinement (IR): The temporary caption γ(t) is used as the initial caption c(t) 0 generate the initial image (t) 0 : to c(t) 0 = γ(t), 0 = MG(c(t) (t) 0 ) where MG is the text-to-image generation model. For each refinement iteration {1, . . . , K}: (a) Generate refinement suggestion for image caption: s(t,k) = MV (ps(q(t), a(t), (t) k1, H(t1))) where MV denotes VLM and (t) k1 represents the image at iteration 1. (b) Refine caption if s(t,k) = : c(t) = ML(pr(c(t) k1, s(t,k) )) (c) Generate new image: = MG(c(t) (t) ) The iteration stops when = or no further refinements are suggested (s(t,k) yielding the final caption c(t) and the final image (t). (10) (11) (12) (13) = ),"
        },
        {
            "title": "3.4 Dataset Composition",
            "content": "InterSyn contains approximately 1.8 million single-turn samples and 50k multi-turn dialogues. Data quality is ensured through the SEIR method, which iteratively refines samples to improve answer accuracy, conversational coherence, and image-text synergy. InterSyn offers rare combination of diversity and quality, providing robust foundation for training unified multimodal models with strong instruction-following and contextual reasoning capabilities."
        },
        {
            "title": "4.1.1 Evaluated Models",
            "content": "We evaluate 11 multimodal generators capable of producing interleaved image-text outputs, covering representative spectrum of existing models. For comparative analysis, we categorize them into two groups based on whether they natively support interleaved generation within single stage: Non-Interleaved Generators: These generators produce text and images sequentially through modular pipelines, which include: Emu3 [31], Janus-Pro [7], VILA-U [30], Show-o [29], Show-oTurbo [43], Liquid [44], D-DiT [45], GPT-4o [46] + DALLE3 [5], and Gemini1.5 [47] + FLUX [48]. Interleaved Generators: These generators are designed to generate interleaved text-image outputs within unified process, including VARGPT [49] and Anole [32]."
        },
        {
            "title": "4.1.2 Evaluation Methods",
            "content": "For evaluation, we adopt the benchmark introduced in Section 3.2. These questions are used to prompt all generators. In addition, to validate the effectiveness of the SEIR method, we also generate responses using different refinement iterations. Each generated response is independently evaluated by the following judges across four dimensions: TCC, ICC, IQ and ITS. Human Judge. Human evaluation is conducted first. To ensure scoring reliability, we adopt cross-review protocol: when significant discrepancies arise between annotators, discussions are held to mitigate individual bias and enhance evaluation quality. Model-based Judge. We also use multimodal large language models (MLLMs) for automated assessment. Specifically, GPT-4o [46], InternVL2.5 [2], and QwenVL2.5 [25] are employed to score 6 the generated outputs. While model-based evaluation significantly improves efficiency, it may deviate from human judgment on tasks requiring complex reasoning or fine-grained understanding. SynJudge. To ensure both accuracy and efficiency, we introduce SynJudge, an automated scoring model optimized for human consistency and scalability. SynJudge is trained on 38,400 human annotated samples, with QwenVL2.5 [25] and InternVL2.5 [2] explored as backbone models. Finally, we obtain five model-based judges and human judge consisting of ten experts."
        },
        {
            "title": "4.2 Evaluation Metrics",
            "content": "To quantitatively assess the performance and consistency of judges and generators, we adopt the following metrics: Mean Score. Measures the average quality of generators outputs under each evaluation dimension. Score Variance. Captures the consistency of generators outputs across different questions; lower variance indicates more stable performance. Root Mean Squared Error (RMSE) Quantifies the deviation between model-based judges scores and human scores; lower RMSE implies better alignment with human judgment. Formal definitions of these metrics are provided in the Appendix."
        },
        {
            "title": "4.3.1 Question Quality",
            "content": "We conduct human evaluation to assess the impact of QR. As shown in Figure 3, quality improves over the first three iterations but plateaus thereafter. Based on this, we fix the number of refinements to 3, achieving 99.5% of peak quality while reducing computational cost by 40%. Table 1: Impact of answer refinement (AR) and image refinement (IR) on answer quality. The table reports human evaluation scores across four dimensions (TCC, ICC, IQ, ITS). AR improves TCC, ICC, and ITS, while IR further enhances ICC and ITS, confirming the effectiveness of iterative refinement. AR IR TCC 0 1 2 3 3 3 3 4 5 0 1 2 0 1 2 3 4 3.85 4.19 4.34 4.42 4.42 4.42 4.42 4.44 4.45 ICC 4.01 4.21 4.35 4.11 4.33 4.43 4.47 4.50 4.49 IQ 4.42 4.38 4.41 4.37 4.43 4.39 4.44 4.45 4.43 ITS 3.79 4.07 4.38 4.04 4.35 4.46 4.52 4.53 4.53 Figure 3: Impact of question refinement (QR) on question quality. This plot shows the quality scores across different QR iterations. Quality improves significantly over the first three iterations but plateaus thereafter."
        },
        {
            "title": "4.3.2 Answer Quality",
            "content": "We conduct human evaluation across different iterations of AR and IR, using set of 500 prototype questions generated through three rounds of QR. As shown in Table 1, the results confirm that AR primarily improves content informativeness, while IR enhances multimodal synergy, demonstrating the effectiveness of the SEIR method. Experimental results show that when both AR and IR are set to 4 or 5, the improvements become marginal. Based on this, we fix the number of AR and IR iterations to 3 in dataset construction settings."
        },
        {
            "title": "4.3.3 Judges Comparison",
            "content": "To verify the scoring ability of different judges, we compute the RMSE between five modelbased judges and human across four evaluation dimensions, using human-annotated test set of 9,600 items. Table 2 and Figure 4 present the numerical results and their corresponding visualizations. The radar plot shows that the smaller its range, the better the performance of the judge. Among all judges, QwenVL-trained shows the closest alignment with human judgments and is therefore selected as the final judge, referred to as SynJudge. Figure 4: Visualization of RMSE across evaluation dimensions for different judges. Table 2: RMSE Comparison of different judges across four evaluation dimensions Generator Anole DDiT Emu3 GPT-4o QwenVL InternVL QwenVL_trained InternVL_trained TCC ICC IQ ITS TCC ICC IQ ITS TCC ICC IQ ITS TCC ICC IQ ITS TCC ICC IQ ITS 1.16 1.10 1.13 1.18 0.56 1.34 0.99 1.37 1.12 1.04 1.22 1.04 0.47 0.75 0.81 0.75 0.45 0.79 0.86 0.72 0.61 1.12 1.11 1.34 0.59 1.10 1.12 0.66 0.59 0.91 1.10 0.67 0.33 0.67 0.68 0.35 0.41 0.68 0.77 0.44 0.96 1.11 0.90 1.24 0.76 1.32 0.81 1.22 0.86 0.95 1.00 1.11 0.58 0.70 0.61 0.68 0.57 0.75 0.77 0.84 InterSyn 0.88 0.84 0.74 1.15 0.87 0.89 0.80 1.23 0.82 0.75 0.75 1.04 0.63 0.71 0.60 0.65 0.63 0.56 0.76 0. Gemini+Flux 0.82 0.79 0.68 0.90 0.78 1.16 0.76 0.89 0.76 0.73 0.70 0.91 0.64 0.67 0.61 0.67 0.60 0.65 0.57 0.73 GPT-4o+DALL-E3 0.67 0.73 0.80 0.85 0.62 0.74 0.90 1.08 0.63 0.72 0.84 0.87 0.50 0.59 0.55 0.65 0.47 0.58 0.58 0.61 Janus-Pro 1.31 1.26 1.19 1.38 1.18 1.46 1.27 1.37 1.32 1.05 1.41 1.23 0.47 0.77 0.81 0.79 0.43 0.73 0.83 0.90 Liquid 1.06 1.07 1.05 1.40 0.86 1.00 0.81 1.74 1.14 0.98 1.20 1.33 0.49 0.77 0.76 0.78 0.48 0.64 0.74 0.82 Show-o 1.02 1.19 1.14 1.33 0.96 1.10 1.16 1.23 0.89 1.00 1.20 1.24 0.61 0.79 0.67 0.77 0.65 0.77 0.75 0.94 Show-o-Turbo 1.25 1.09 1.14 1.25 1.22 1.09 0.94 1.43 1.00 0.94 1.21 1.16 0.77 0.81 0.82 0.71 0.77 0.83 0.76 0.75 VARGPT 1.37 0.81 0.62 0.77 0.71 0.80 0.78 1.00 1.52 0.74 0.71 0.75 0.52 0.51 0.41 0.56 0.63 0.64 0.61 0.61 VILA-U 1.00 1.17 1.28 1.39 0.59 1.12 1.20 1.13 0.85 1.02 1.40 0.98 0.42 0.87 0.81 0.71 0.45 0.76 0.82 0."
        },
        {
            "title": "4.3.4 Generator evaluation",
            "content": "We evaluate the responses of 11 generators alongside those produced by our SEIR pipeline using the benchmark introduced in Sec 3.2. Both human judge and SynJudge agree that SEIR-generated samples (InterSyn) achieve the highest mean scores across all evaluation dimensions, outperforming the strongest baseline, GPT-4o+DALL-E, by 0.340.66; the largest margin appears in ITS. InterSyn also shows the low variances (below 0.61), reflecting consistently high-quality outputs and underscoring the robustness of our automatically constructed dataset. Crucially, the sizeable margin that remains between InterSyn and the best existing generator reveals that current state-of-the-art models still struggle with imagetext alignment and complementarity, leaving substantial room for future improvement."
        },
        {
            "title": "4.3.5 Generative Model Fine-Tuning",
            "content": "Table 3: Score Comparison after Fine-Tuning. To verify the effectiveness of the InterSyn dataset, we fine-tune Anole and VILA-U on 50K samples from InterSyn. As shown in Table 3, fine-tuning leads to overall performance improvements across all evaluation dimensions. Specifically, gains in IQ remain limited, likely due to intrinsic constraints in the models visual generation capabilities. In contrast, more substantial improvements are observed in the other three metricsparticularly TCC and ITShighlighting the InterSyn datasets effectiveness in enhancing both semantic alignment and answer completeness. Anole Anole_trained VILA-U VILA-U_trained 2.92 3.1 3.37 3.39 3.09 3.52 2.46 3.19 2.26 2.94 2.19 3. 3.01 3.24 3.72 3.83 Model TCC ICC ITS IQ"
        },
        {
            "title": "5 Ablation Study",
            "content": "To investigate the impact of the SEIR method, we conduct fine-grained ablation by varying AR and IR iterations. Results in Table 5 show that both AR and IR contribute positively to performance. Specifically, increasing AR iterations mainly improves TCC, ICC, and ITS, while IR iterations further enhance ICC and ITS. These results confirm that AR enriches textual content and coherence, while IR reinforces visual relevance and multimodal synergy."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we present InterSyn, large-scale, high-quality multimodal dataset designed for instruction-following and interleaved image-text generation. Constructed via the fully automated SEIR method, InterSyn combines scale, diversity, and fidelity, supporting multi-turn dialogues where each response is refined to achieve not only semantic completeness but also tight image-text synergyensuring that visual and textual modalities complement each other to convey meaning collaboratively. To complement InterSyn, we introduce SynJudge, multi-dimensional automatic evaluator specifically designed to assess interleaved outputs across four key dimensions, including dedicated metric for image-text synergy. Unlike traditional metrics focused on surface-level alignment or consistency, SynJudge emphasizes the semantic interplay between images and text, rewarding complementary relationships while penalizing redundancy or disjointness. Extensive experiments validate the effectiveness of both InterSyn and SEIR. Models fine-tuned on InterSyn consistently outperform strong baselines, showing notable improvements in instruction alignment, multimodal reasoning, and especially the ability to produce coherent, synergistic interleaved content. We believe this work lays solid foundation for future research in scalable multimodal data generation, robust synergy-centric evaluation, and the development of general-purpose multimodal intelligence systems that understand and communicate across modalities in truly integrated manner. Table 4: Generator performance evaluated by human judge and SynJudge. Generator Dimension Human SynJudge TCC ICC IQ ITS TCC ICC IQ Anole DDiT Emu3 VILA-U Liquid Janus-Pro Show-o Show-o-Turbo VARGPT Gemini+Flux GPT-4o+DALL-E InterSyn mean variance mean variance mean variance mean variance mean variance mean variance mean variance mean variance mean variance mean variance mean variance mean variance 2.86 1. 0.38 1.17 3.38 0.86 2.25 2.58 2.88 0.76 2.93 0.95 3.32 1. 3.48 1.12 2.60 0.66 3.94 0.94 4.05 0.37 4.41 0.55 2.69 1. 3.29 0.85 3.87 0.59 3.15 0.97 3.67 0.70 3.16 0.99 3.52 0. 3.53 0.94 0.94 2.32 4.43 0.47 4.41 0.57 4.47 0.53 2.05 2. 0.37 1.21 3.26 1.31 2.11 3.01 3.00 1.42 2.55 1.75 3.10 1. 3.33 1.60 0.55 1.99 3.81 0.90 3.94 0.64 4.51 0.57 2.89 1. 0.28 0.86 3.37 0.74 2.26 2.73 2.87 0.71 2.96 0.96 3.49 1. 3.50 1.06 2.55 0.30 3.97 0.57 3.99 0.65 4.39 0.61 2.81 2. 3.67 0.75 3.92 0.66 3.52 0.92 3.86 0.75 3.30 1.09 3.79 0. 3.89 0.86 0.94 2.39 4.12 0.71 4.10 0.81 4.49 0.63 2.72 1. 3.34 0.92 3.85 0.80 3.17 1.12 3.76 0.85 3.11 1.11 3.57 0. 3.64 1.01 0.87 2.17 4.48 0.64 4.45 0.58 4.44 0.45 2.75 2. 3.51 0.83 3.86 0.86 3.42 0.91 3.82 0.69 3.25 1.21 3.65 0. 3.77 0.88 0.94 2.37 4.06 0.58 4.08 0.48 4.46 0.55 ITS 2.06 2.55 0.26 0.87 3.37 1.16 1.99 2.96 3.06 1. 2.62 1.87 3.30 1.70 3.45 1.44 0.67 2.16 3.84 1.11 3.87 1. 4.53 0.51 Table 5: Ablation study: impact of AR and IR iterations on Anole and VILA-U. null indicates baseline performance without SEIR-based training. (a) Performance of Anole. (b) Performance of VILA-U. AR null 0 1 2 3 3 3 IR TCC ICC IQ null 0 1 2 0 1 2 3 3.09 3.33 3.37 3.41 3.47 3.51 3.49 3.52 3.01 3.17 3.21 3.27 3.20 3.25 3.3 3. 2.92 2.92 3.07 3.03 3.03 3.11 3.08 3.1 ITS 2.26 2.77 2.71 2.79 2.85 2.93 2.91 2."
        },
        {
            "title": "References",
            "content": "AR null 0 1 2 3 3 3 3 IR TCC ICC IQ null 0 0 0 0 1 2 2.46 3.07 3.1 3.17 3.17 3.17 3.21 3.19 3.72 3.83 3.8 3.79 3.81 3.81 3.85 3.83 3.37 3.37 3.34 3.39 3.36 3.4 3.37 3.39 ITS 2.19 3.13 3.2 3.27 3.27 3.3 3.29 3.33 [1] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, 2023. [2] Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, E. Cui, J. Zhu, S. Ye, H. Tian, Z. Liu et al., Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, arXiv preprint arXiv:2412.05271, 2024. [3] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, Y. Fan, K. Dang, M. Du, X. Ren, R. Men, D. Liu, C. Zhou, J. Zhou, and J. Lin, Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, arXiv preprint arXiv:2409.12191, 2024. [4] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel et al., Scaling rectified flow transformers for high-resolution image synthesis, in Forty-first International Conference on Machine Learning, 2024. [5] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo et al., Improving image generation with better captions, Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, vol. 2, no. 3, p. 8, 2023. [6] C. Team, Chameleon: Mixed-modal early-fusion foundation models, arXiv preprint arXiv:2405.09818, 2024. [7] X. Chen, Z. Wu, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, and C. Ruan, Janus-pro: Unified multimodal understanding and generation with data and model scaling, arXiv preprint arXiv:2501.17811, 2025. [8] W. Zhu, J. Hessel, A. Awadalla, S. Y. Gadre, J. Dodge, A. Fang, Y. Yu, L. Schmidt, W. Y. Wang, and Y. Choi, Multimodal c4: An open, billion-scale corpus of images interleaved with text, in Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. [Online]. Available: https://openreview.net/forum?id=tOd8rSjcWz [9] H. Laurençon, L. Saulnier, L. Tronchon, S. Bekman, A. Singh, A. Lozhkov, T. Wang, S. Karamcheti, A. Rush, D. Kiela et al., Obelics: An open web-scale filtered dataset of interleaved image-text documents, Advances in Neural Information Processing Systems, vol. 36, pp. 71 68371 702, 2023. [10] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin, Sharegpt4v: Improving large multi-modal models with better captions, in European Conference on Computer Vision. Springer, 2024, pp. 370387. [11] W. Chen, L. Li, Y. Yang, B. Wen, F. Yang, T. Gao, Y. Wu, and L. Chen, CoMM: coherent interleaved image-text dataset for multimodal understanding and generation, arXiv preprint arXiv:2406.10462, 2024. [12] Z. Xu, M. Liu, Y. Shen, J. Rimchala, J. Zhang, Q. Wang, Y. Cheng, and L. Huang, Lateralization LoRA: Interleaved instruction tuning with modality-specialized adaptations, arXiv preprint arXiv:2407.03604, 2024. 10 [13] Y. Yang, A. Panagopoulou, Q. Lyu, L. Zhang, M. Yatskar, and C. Callison-Burch, Visual goal-step inference using wikiHow, in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 21672179. [Online]. Available: https://aclanthology.org/2021.emnlp-main.165 [14] L. Zhou, C. Xu, and J. J. Corso, Towards automatic learning of procedures from web instructional videos, in Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, S. A. McIlraith and K. Q. Weinberger, Eds. AAAI Press, 2018, pp. 75907598. [Online]. Available: https://doi.org/10.1609/aaai.v32i1.12342 [15] N. Anantrasirichai and D. Bull, Artificial intelligence in the creative industries: review, Artificial intelligence review, vol. 55, no. 1, pp. 589656, 2022. [16] S. Yang, Y. Ge, Y. Li, Y. Chen, Y. Ge, Y. Shan, and Y. Chen, Seed-story: Multimodal long story generation with large language model, arXiv preprint arXiv:2407.08683, 2024. [17] C. Liu, H. Wu, Y. Zhong, X. Zhang, Y. Wang, and W. Xie, Intelligent grimm - open-ended visual storytelling via latent diffusion models, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024, pp. 61906200. [18] J. An, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, L. Wang, and J. Luo, Openleaf: novel benchmark for open-domain interleaved image-text generation, in Proceedings of the 32nd ACM International Conference on Multimedia, ser. MM 24. New York, NY, USA: Association for Computing Machinery, 2024, p. 1113711145. [Online]. Available: https://doi.org/10.1145/3664647.3685511 [19] P. Zhou, X. Peng, J. Song, C. Li, Z. Xu, Y. Yang, Z. Guo, H. Zhang, Y. Lin, Y. He, L. Zhao, S. Liu, T. Li, Y. Xie, X. Chang, Y. Qiao, W. Shao, , and K. Zhang, Gate opening: comprehensive benchmark for judging open-ended interleaved image-text generation, 2024. [20] D. Chen, R. Chen, S. Pu, Z. Liu, Y. Wu, C. Chen, B. Liu, Y. Huang, Y. Wan, P. Zhou, and R. Krishna, Interleaved scene graphs for interleaved text-and-image generation assessment, 2025. [Online]. Available: https://arxiv.org/abs/2411.17188 [21] P. Xia, S. Han, S. Qiu, Y. Zhou, Z. Wang, W. Zheng, Z. Chen, C. Cui, M. Ding, L. Li et al., Mmie: Massive multimodal interleaved comprehension benchmark for large vision-language models, arXiv preprint arXiv:2410.10139, 2024. [22] M. Liu, Z. Xu, Z. Lin, T. Ashby, J. Rimchala, J. Zhang, and L. Huang, Holistic evaluation for interleaved text-and-image generation, arXiv preprint arXiv:2406.14643, 2024. [23] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds et al., Flamingo: visual language model for few-shot learning, in Proceedings of the Advances in Neural Information Processing Systems, 2022. [24] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu et al., Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 24 18524 198. [25] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin, Qwen2.5-vl technical report, arXiv preprint arXiv:2502.13923, 2025. [26] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, Hierarchical text-conditional image generation with clip latents, arXiv preprint arXiv:2204.06125, vol. 1, no. 2, p. 3, 2022. 11 [27] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel et al., Scaling rectified flow transformers for high-resolution image synthesis, in Forty-first International Conference on Machine Learning, 2024. [28] K. Zheng, X. He, and X. E. Wang, MiniGPT-5: Interleaved vision-and-language generation via generative vokens, arXiv preprint arXiv:2310.02239, 2023. [29] J. Xie, W. Mao, Z. Bai, D. J. Zhang, W. Wang, K. Q. Lin, Y. Gu, Z. Chen, Z. Yang, and M. Z. Shou, Show-o: One single transformer to unify multimodal understanding and generation, arXiv preprint arXiv:2408.12528, 2024. [30] Y. Wu, Z. Zhang, J. Chen, H. Tang, D. Li, Y. Fang, L. Zhu, E. Xie, H. Yin, L. Yi et al., Vila-u: unified foundation model integrating visual understanding and generation, arXiv preprint arXiv:2409.04429, 2024. [31] X. Wang, X. Zhang, Z. Luo, Q. Sun, Y. Cui, J. Wang, F. Zhang, Y. Wang, Z. Li, Q. Yu et al., Emu3: Next-token prediction is all you need, arXiv preprint arXiv:2409.18869, 2024. [32] E. Chern, J. Su, Y. Ma, and P. Liu, Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation, arXiv preprint arXiv:2407.06135, 2024. [33] T.-H. Huang, F. Ferraro, N. Mostafazadeh, I. Misra, A. Agrawal, J. Devlin, R. Girshick, X. He, P. Kohli, D. Batra et al., Visual storytelling, in Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: Human language technologies, 2016, pp. 12331239. [34] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, Bleu: method for automatic evaluation of machine translation, in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311318. [35] C.-Y. Lin, Rouge: package for automatic evaluation of summaries, in Text summarization branches out, 2004, pp. 7481. [36] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, Gans trained by two time-scale update rule converge to local nash equilibrium, Advances in neural information processing systems, vol. 30, 2017. [37] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, X. Chen, and X. Chen, Improved techniques for training gans, in Advances in Neural Information Processing Systems, D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, Eds., vol. 29. Curran Associates, Inc., 2016. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2016/file/ 8a3363abe792db2d8761d6403605aeb7-Paper.pdf [38] J. Hessel, A. Holtzman, M. Forbes, R. Le Bras, and Y. Choi, CLIPScore: reference-free evaluation metric for image captioning, in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 75147528. [Online]. Available: https://aclanthology.org/2021.emnlp-main.595 [39] J. Li, D. Li, S. Savarese, and S. Hoi, Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, in International conference on machine learning. PMLR, 2023, pp. 19 73019 742. [40] Z. Lin, X. Chen, D. Pathak, P. Zhang, and D. Ramanan, Revisiting the role of language priors in vision-language models, 2024. [Online]. Available: https://arxiv.org/abs/2306.01879 [41] Y. Chen, L. Liu, and C. Ding, X-iqe: explainable image quality evaluation for text-to-image generation with visual large language models, 2023. [Online]. Available: https://arxiv.org/abs/2305.10843 [42] Y. Lu, X. Yang, X. Li, X. E. Wang, and W. Y. Wang, Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation, Advances in Neural Information Processing Systems, vol. 36, 2024. 12 [43] C. Xu, X. Wang, Z. Liao, Y. Li, T. Hou, and Z. Deng, Show-o turbo: Towards accelerated unified multimodal understanding and generation, 2025. [Online]. Available: https://arxiv.org/abs/2502.05415 [44] J. Wu, Y. Jiang, C. Ma, Y. Liu, H. Zhao, Z. Yuan, S. Bai, and X. Bai, Liquid: Language models are scalable and unified multi-modal generators, 2025. [Online]. Available: https://arxiv.org/abs/2412.04332 [45] Z. Li, H. Li, Y. Shi, A. B. Farimani, Y. Kluger, L. Yang, and P. Wang, Dual diffusion for unified image generation and understanding, 2025. [Online]. Available: https://arxiv.org/abs/2501. [46] OpenAI, Hello GPT-4o, https://openai.com/index/hello-gpt-4o/, 2024, accessed: 2024-05-26. [47] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth et al., Gemini: family of highly capable multimodal models, arXiv preprint arXiv:2312.11805, 2023. [48] Black Forest Labs, Flux, https://github.com/black-forest-labs/flux, 2024, accessed: 2024-1105. [49] X. Zhuang, Y. Xie, Y. Deng, L. Liang, J. Ru, Y. Yin, and Y. Zou, Vargpt: Unified understanding and generation in visual autoregressive multimodal large language model, 2025. [Online]. Available: https://arxiv.org/abs/2501."
        },
        {
            "title": "A Appendix overview",
            "content": "The all supplementary document is organized as follows: Comparison of datasets and Samples of InterSyn are shown in Sec. B. Supplementary analysis of experimental Data are shown in Sec. C. Question templates and topic hierarchy are shown in Sec. D. The evaluation dimensions for question and answer are shown in Sec. E. Prompts used in this work are shown in Sec. F. The benchmark samples are shown in Sec. G. Human annotation platform are shown in Sec. H. Limitations of this study are shown in Sec. I."
        },
        {
            "title": "B Comparison of Datasets and InterSyn Samples",
            "content": "B.1 Comparison of Datasets The table provides comprehensive comparison of InterSyn with representative multimodal datasets and benchmarks. Existing datasets such as MMC4 and OBELICS primarily rely on large-scale web-crawled corpora, often lacking instruction-following capabilities and multi-turn structures. Other resources like CoMM and ShareGPT4V improve data cleanliness but remain limited to single-turn interactions without tight semantic supervision. Recent efforts including LeafInstruct introduce instruction-following supervision but still operate in single-turn formats. Meanwhile, benchmark-oriented resourcessuch as OpenLEAF, ISG-BENCH, MMIE, InterleavedBench, and OpenINGfocus on evaluating generation quality but are constrained by small scale and limited turn complexity. In contrast, InterSyn is the first to offer large-scale, multi-turn, instruction-following dataset specifically designed for interleaved image-text generation. Built with the SEIR method, InterSyn not only ensures high-quality visual-textual synergy but also scales to 1.8 million samplesorders of magnitude larger than existing benchmarks. Its emphasis on dialogue coherence, iterative refinement, and synergistic multimodal responses fills critical gap in current resources, laying the groundwork for developing and evaluating truly unified multimodal generation models. Table 6: Comparison of Multimodal Datasets and Benchmarks. Abbreviations: Cat. = Category; Inst. = instruction-following; MT. = multi-turn; DS. = dataset; BM. = benchmark; Gen. = generation."
        },
        {
            "title": "InterSyn",
            "content": "Cat. Source"
        },
        {
            "title": "Method",
            "content": "DS. Collected ques-"
        },
        {
            "title": "Size",
            "content": "1.8M Inst. MT. tions MMC4 DS. Common"
        },
        {
            "title": "OBELICS",
            "content": "DS. Common"
        },
        {
            "title": "Crawl",
            "content": "DS. WikiHow, StoryBird, eHow, etc. ShareGPT4V DS. GPT-4V captions"
        },
        {
            "title": "LeafInstruct",
            "content": "DS. MMC4, VIST, YouCook2, etc. CLIP-based filtering Multi-granularity filtering Multi-perspective filtering 101.2M Doc. 141M pages 227K Doc. Share-Captioner 1.2M pairs Text & image quality filtering 38, 1 Figure 5: Examples of single-turn conversation"
        },
        {
            "title": "Name",
            "content": "Cat. Source"
        },
        {
            "title": "OpenLEAF",
            "content": "BM. User Queries ISG-BENCH BM. VIST, CoMM, manual Gen."
        },
        {
            "title": "MMIE",
            "content": "BM. WikiHow, VIST, MathVista, etc. GPT-4 Gen. and human review Model Gen. & human review Sampling & reconstruction"
        },
        {
            "title": "Size",
            "content": "660 1,150 20,103 InterleavedBench BM. VIST, WikiHow, etc. GPT-4o Gen. + human review"
        },
        {
            "title": "OpenING",
            "content": "BM. YouTube,"
        },
        {
            "title": "Manual pipeline",
            "content": "5,400 Google, etc. Inst. MT. B.2 Single-Turn Samples Samples of data are shown in Figure 5 2 Figure 6: Examples of multi-turn conversation B.3 Multi-Turn Samples Samples of data are shown in Figure"
        },
        {
            "title": "C Supplementary Analysis of Experimental",
            "content": "C.1 Symbols and Notations for SEIR Method We summarize the key symbols used in the SEIR method below: : Set of question templates. Z: Set of topics. N+: Number of conversation turns. N+: Number of refinement iterations at each stage. H(t1): History of the conversation up to turn 1, represented as {(q(i), a(i), i(i))}t1 i=1. q(t): Final question generated at conversation turn t. q(t) a(t): Final text answer generated at conversation turn t. : Question after refinement iterations at conversation turn t. : Temporary caption after refinement iterations. : Text answer after refinement iterations at conversation turn t. a(t) γ(t): Temporary caption associated with the text answer at conversation turn t. γ(t) c(t): Final image caption at conversation turn t. c(t) : Image caption after refinement iterations. (t): Final generated image at conversation turn t. (t) ML: Language model used for text generation and refinement. MV : Vision-language model used for image caption evaluation and refinement. MG: Text-to-image generation model. pg(): Prompt function for generating model response. ps(): Prompt function for generating refinement suggestions. pr(): Prompt function for applying refinements. :Generated image after refinement iterations. C.2 Evaluation Metrics for Judges and Generators To facilitate quantitative evaluation of judges and generators, we design set of metrics. Mean Calculation Let xi,d denote the score given by judge to the i-th sample generated by generator, under evaluation dimension d, and let be the total number of samples. Then, for each (judge, generator) pair, the mean score is computed as: Sd ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 xi,d (14) The mean score Sd reflects the average performance of generator, as evaluated by specific judge under dimension d. Variance Calculation To estimate the variability of the scores, we compute the variance: σd ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (xi,d Sd)2 (15) The variance σd captures the consistency of the generators performance across different questions. higher variance indicates greater inconsistency in quality. Root Mean Squared Error (RMSE) To measure the agreement between model-based judge and human judge H, we compute the RMSE between their respective scores for each sample: RMSEd = (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) (cid:16) i=1 (cid:17)2 i,d xH xM i,d (16) Here, xM i,d and xH i,d denote the scores assigned by the model-based and human judges respectively. RMSE quantifies the deviation between model-based judges scores and those of human judge in dimension d. Lower RMSE values indicate higher alignment with human preferences, and thus higher reliability of the model-based judge. This evaluation framework provides comprehensive analysis of the performance of the 11 generators across multiple dimensions, ensuring objective comparison from both human and model-based perspectives. 4 C.3 Models Used in the SEIR Method To construct the dataset, we adopt Qwen2.5-32B-Instruct as the language model (ML), InternVL2_526B as the vision-language model (MV ), and FLUX.1-dev as the text-to-image generation model (MG). These open-source models are used as the default configuration in our SEIR framework. Importantly, the framework is modular by designeach component can be substituted with other models, offering flexibility for different deployment environments or research needs. To assess the generality and robustness of SEIR, we systematically evaluated its performance across range of model configurations. Specifically, we experimented with different combinations of open-source LMs (InternLM, DeepSeek-R1, Qwen) and VLMs (InternVL, QwenVL), while keeping the generative model fixed as Flux. In addition, to benchmark against high-performing closed-source alternatives, we included configuration that uses GPT-4o as both the LM and VLM, and DALL-E3 as the generative model. The results, presented in Table 7, demonstrate several key trends: SEIR consistently improves data quality. Across all configurations, we observe noticeable improvements in all four evaluation dimensionsTCC, ICC, IQ and ITSafter applying SEIR. The gains are particularly significant in the TCC, ICC and ITS dimensions, reflecting SEIRs ability to enhance the semantic alignment and cooperative informativeness of multimodal outputs. Initial quality varies across model combinations. Among the open-source configurations, those involving Qwen (e.g., Qwen+InternVL or QwenVL) generally exhibit stronger performance in the No_SEIR stage. In contrast, InternLM+QwenVL and DeepSeek-R1 + InternVL show relatively weaker initial consistency, suggesting differences in language-vision alignment quality across model families. SEIR is especially effective for lower-performing combinations. The relative improvements are more pronounced for model combinations with lower baseline performance. For example, InternLM + QwenVL improves by 0.60 in TCC and 0.70 in ITS, and DeepSeekR1+InternVL shows notable gains across all metrics despite its initially modest performance. SEIR narrows the gap between openand closed-source models. While the GPT4o+DALL-E3 configuration achieves the highest initial quality across all metrics, the application of SEIR allows open-source configurations to reach comparable performance levels. For instance, Qwen+InternVL + Flux achieves 4.42 (TCC) and 4.51 (ITS) after SEIR, which closely rivals the 4.44 and 4.54 obtained by GPT-4o+DALL-E3. Text-Image Synergy (ITS) shows the largest variance. This dimension benefits the most from SEIR optimization, particularly in cases where image-text redundancy or disconnection was prevalent before refinement. The improvement indicates SEIRs effectiveness in jointly adjusting both modalities to produce more complementary multimodal answers. Overall, these results confirm that SEIR is robust and generalizable enhancement framework. It consistently improves dataset quality across wide range of model backbones, and significantly reduces the reliance on expensive closed-source models. Consequently, we adopt the open-source setup of Qwen+InternVL+Flux as our default configuration, balancing quality, flexibility, and costeffectiveness. During the dataset generation and experimental testing process, this work consumed approximately 90,000 H100 hours. C.4 Hyperparameters Used for Training Judge Model We fine-tuned two large multimodal models, InternVL2.5-8B and QwenVL2.5-7B, as judge models to evaluate the interleavde image-text response quality. We followed common practices for large-scale model fine-tuning, applying weight decay regularization, learning rate warmup, and gradient clipping to ensure training stability. All experiments were conducted using mixed-precision training on distributed GPU clusters. For the QwenVL2.5-7B judge model, we adopted multi-GPU training setup using 4 devices with total training batch size of 8, obtained by setting per-device batch size of 1 and gradient accumulation step of 2. The model was trained using the AdamW optimizer with β1 = 0.9, β2 = 0.999, and ϵ = 1 108. The initial learning rate was set to 1 105 and scheduled using Table 7: Comparison of dataset quality before and after SEIR optimization using different model configurations. LM method No_SEIR InternLM InternLM SEIR No_SEIR InternLM SEIR InternLM No_SEIR DeepSeek-R1 SEIR DeepSeek-R1 No_SEIR Qwen SEIR Qwen No_SEIR Qwen SEIR Qwen No_SEIR GPT-4o GPT-4o SEIR VLM InternVL InternVL QwenVL QwenVL InternVL InternVL QwenVL QwenVL InternVL InternVL GPT-4o GPT-4o GM Flux Flux Flux Flux Flux Flux Flux Flux Flux Flux DALL-E3 DALL-E3 TCC ICC IQ 3.98 3.72 4.35 4.34 3.89 3.66 4.32 4.26 3.82 3.65 4.28 4.20 3.93 3.80 4.40 4.37 4.01 3.85 4.47 4.42 4.08 4.05 4.46 4. 4.30 4.38 4.28 4.35 4.37 4.43 4.35 4.36 4.42 4.44 4.41 4.43 ITS 3.70 4.42 3.68 4.38 3.66 4.19 3.75 4.52 3.79 4.51 3.94 4.54 cosine decay strategy with warmup ratio of 10%. fixed random seed of 42 was used for reproducibility. Evaluation was conducted using batch size of 8 per device, resulting in total evaluation batch size of 32. For the InternVL2.5-8B referee model, we adopted multi-GPU training setup, using 4 devices with total training batch size of 8. This was achieved by setting the batch size per device to 1 and the gradient accumulation steps to 2. The model was trained using the AdamW optimizer, with β1 = 0.9, β2 = 0.999, ϵ = 1 108, gradient clipping threshold of 1.0, and weight decay of 0.05. The initial learning rate was set to 4 105, and cosine annealing strategy was employed for adjustment, with warm-up ratio of 3%. For reproducibility, fixed random seed of 42 was used. During evaluation, the batch size per device was 8, resulting in total evaluation batch size of 32. C.5 Detailed Analysis of the Judges Scoring Judge Deviation Analysis. To better understand each judges scoring behavior, we report the distribution of absolute score differences in Tables 812. Each table shows, for given judge, the proportion of samples where the models score differs from the human reference by 0 to 5. These detailed distributions provide fine-grained view of the judges alignment with human evaluators. Judge Accuracy Comparison. Table 13 reports the evaluation accuracy of different judges across models and dimensions. Considering the inherent subjectivity in evaluation, we define deviation within 1 point between model and human scores as an acceptable margin of subjective error. Accordingly, the reported accuracy reflects the proportion of samples where the models score differs from human judgment by no more than one point. InternVL_trained also shows strong performance with an average accuracy of 94.5%, slightly lower but still significantly better than non-fine-tuned baselines. In contrast, GPT-4o and InternVL base models exhibit noticeably lower accuracy, around 86.5%, indicating that although they are strong in generation, they are less reliable for fine-grained evaluation without specialized tuning. Based on these findings, we select QwenVL_trained as the backbone model for constructing SynJudge, ensuring both accuracy and consistency in downstream evaluations. 6 Table 8: Gap proportion between GPT-4o and human scoring. Model Dimension 1 Score 2 3 4 5 Anole GPT-4o+DALL-E DDiT Emu3 InterSyn Gemini+Flux Janus-Pro Liquid Show-o Show-o-Turbo VARGPT VILA-U TCC ICC IQ ITS 0.463 0.525 0.52 0.536 TCC ICC 0.654 0.686 IQ 0.636 0.635 ITS TCC ICC 0.938 0.467 IQ 0.434 0.843 ITS TCC ICC 0.5 0.535 IQ 0.546 0.454 ITS TCC ICC 0.589 0.686 IQ 0.665 0.577 ITS TCC ICC 0.636 0.705 IQ 0.695 0.657 ITS TCC ICC 0.31 0.412 IQ 0.428 0.369 ITS TCC ICC 0.465 0.513 IQ 0.478 0.357 ITS TCC ICC 0.525 0.475 IQ 0.445 0.471 ITS TCC ICC 0.432 0.53 IQ 0.433 0.457 ITS TCC ICC 0.351 0.861 IQ 0.875 0.864 ITS TCC ICC 0.617 0.428 IQ 0.403 0.562 ITS 0.343 0.325 0.309 0. 0.318 0.276 0.295 0.303 0.024 0.379 0.404 0.043 0.398 0.333 0.367 0.358 0.329 0.251 0.281 0.3 0.311 0.237 0.256 0.267 0.419 0.395 0.369 0. 0.389 0.368 0.399 0.414 0.343 0.367 0.382 0.331 0.34 0.332 0.383 0.36 0.356 0.064 0.082 0.071 0.249 0.405 0.37 0.254 0.159 0.106 0.128 0. 0.025 0.031 0.065 0.039 0.017 0.111 0.135 0.023 0.085 0.082 0.074 0.123 0.066 0.037 0.051 0.063 0.037 0.036 0.047 0.045 0.234 0.124 0.164 0. 0.12 0.073 0.094 0.14 0.104 0.101 0.139 0.114 0.17 0.095 0.16 0.122 0.235 0.037 0.034 0.032 0.105 0.117 0.172 0.1 0.031 0.037 0.036 0. 0.003 0.003 0.002 0.019 0.012 0.037 0.023 0.022 0.016 0.037 0.012 0.052 0.013 0.017 0.001 0.037 0.011 0.015 0.002 0.023 0.036 0.06 0.034 0. 0.024 0.039 0.025 0.071 0.026 0.043 0.029 0.06 0.056 0.032 0.019 0.044 0.051 0.03 0.003 0.024 0.024 0.046 0.047 0.038 0.004 0.007 0.007 0. 0.0 0.002 0.0 0.002 0.009 0.006 0.004 0.027 0.001 0.012 0.001 0.013 0.002 0.009 0.001 0.016 0.003 0.007 0.0 0.006 0.001 0.009 0.005 0. 0.002 0.006 0.002 0.016 0.002 0.013 0.005 0.019 0.002 0.011 0.005 0.013 0.007 0.005 0.003 0.008 0.004 0.004 0.007 0.026 0.0 0.0 0.0 0. 0.0 0.002 0.002 0.002 0.0 0.0 0.0 0.042 0.0 0.001 0.0 0.0 0.001 0.0 0.001 0.007 0.002 0.0 0.0 0.002 0.0 0.0 0.0 0. 0.0 0.001 0.002 0.002 0.0 0.001 0.0 0.005 0.0 0.0 0.0 0.004 0.0 0.003 0.003 0.001 0.001 0.0 0.001 0.02 Table 9: Gap proportion between InternVL and human scoring. Model Dimension 0 1 Score 3 4 5 Anole GPT-4o+DALL-E DDiT Emu3 InterSyn Gemini+Flux Janus-Pro Liquid Show-o Show-o-Turbo VARGPT VILA-U TCC ICC 0.523 0.574 IQ 0.504 0.592 ITS TCC ICC 0.658 0.709 IQ 0.647 0.653 ITS TCC ICC 0.942 0.574 IQ 0.485 0.927 ITS TCC ICC 0.586 0.629 IQ 0.531 0.496 ITS TCC ICC 0.589 0.708 IQ 0.692 0.611 ITS TCC ICC 0.629 0.705 IQ 0.694 0.633 ITS TCC ICC 0.37 0.52 IQ 0.378 0.471 ITS TCC ICC IQ ITS 0.438 0.552 0.42 0.433 TCC ICC 0.592 0.556 IQ 0.473 0.49 ITS TCC ICC 0.528 0.581 IQ 0.447 0.492 ITS TCC ICC 0.271 0.857 IQ 0.857 0.872 ITS TCC ICC 0.675 0.553 IQ 0.408 0.684 ITS 0.294 0.286 0.272 0.26 0.326 0.248 0.269 0.281 0.022 0.327 0.347 0. 0.329 0.27 0.328 0.351 0.352 0.239 0.246 0.271 0.332 0.241 0.248 0.279 0.348 0.327 0.31 0.313 0.389 0.339 0.401 0.343 0.314 0.319 0.308 0. 0.333 0.316 0.324 0.347 0.368 0.069 0.083 0.051 0.238 0.316 0.303 0.195 0.147 0.106 0.176 0.116 0.015 0.037 0.075 0.041 0.02 0.081 0.136 0. 0.075 0.072 0.12 0.105 0.048 0.039 0.057 0.075 0.032 0.045 0.056 0.066 0.235 0.124 0.238 0.161 0.139 0.076 0.137 0.149 0.074 0.1 0.181 0. 0.118 0.082 0.199 0.113 0.287 0.057 0.043 0.055 0.069 0.099 0.205 0.087 0.034 0.03 0.042 0.022 0.001 0.003 0.006 0.021 0.005 0.015 0.029 0. 0.008 0.022 0.021 0.044 0.01 0.012 0.003 0.031 0.004 0.005 0.002 0.018 0.046 0.026 0.066 0.049 0.03 0.03 0.033 0.059 0.02 0.02 0.032 0. 0.02 0.017 0.023 0.038 0.062 0.011 0.011 0.014 0.015 0.028 0.076 0.023 0.001 0.003 0.005 0.01 0.0 0.001 0.0 0.0 0.01 0.003 0.003 0. 0.002 0.006 0.0 0.004 0.001 0.001 0.001 0.009 0.002 0.004 0.0 0.002 0.001 0.003 0.008 0.006 0.003 0.003 0.006 0.013 0.0 0.004 0.006 0. 0.001 0.004 0.006 0.007 0.009 0.003 0.003 0.007 0.002 0.004 0.005 0.009 0.001 0.001 0.001 0.0 0.0 0.002 0.003 0.004 0.001 0.0 0.0 0. 0.0 0.001 0.0 0.0 0.0 0.001 0.001 0.003 0.001 0.0 0.0 0.002 0.0 0.0 0.0 0.0 0.001 0.0 0.003 0.003 0.0 0.001 0.0 0. 0.0 0.0 0.001 0.003 0.003 0.003 0.003 0.001 0.001 0.0 0.003 0.002 8 Table 10: Gap proportion between QwenVL and human scoring. Model Dimension 0 1 Score 2 3 5 Anole GPT-4o+DALL-E DDiT Emu3 InterSyn Gemini+Flux Janus-Pro Liquid Show-o Show-o-Turbo VARGPT VILA-U TCC ICC 0.783 0.558 IQ 0.577 0.592 ITS TCC ICC 0.679 0.716 IQ 0.579 0.571 ITS TCC ICC 0.947 0.512 IQ 0.484 0.939 ITS TCC ICC IQ ITS 0.628 0.561 0.58 0.543 TCC ICC 0.574 0.67 IQ 0.667 0.566 ITS TCC ICC 0.607 0.548 IQ 0.677 0.613 ITS TCC ICC 0.433 0.454 IQ 0.429 0.502 ITS TCC ICC 0.621 0.571 IQ 0.656 0.478 ITS TCC ICC 0.542 0.536 IQ 0.457 0.571 ITS TCC ICC 0.436 0.538 IQ 0.554 0.469 ITS TCC ICC 0.707 0.858 IQ 0.852 0.832 ITS TCC ICC 0.823 0.443 IQ 0.447 0.678 ITS 0.189 0.258 0.294 0.189 0.299 0.234 0.325 0.296 0.021 0.339 0.317 0.021 0.316 0.312 0.345 0.277 0.339 0.256 0.264 0. 0.354 0.358 0.254 0.325 0.343 0.29 0.35 0.276 0.298 0.311 0.283 0.239 0.35 0.316 0.364 0.26 0.336 0.311 0.343 0.306 0.241 0.069 0.073 0. 0.151 0.377 0.356 0.165 0.027 0.092 0.102 0.121 0.022 0.041 0.081 0.094 0.013 0.107 0.17 0.017 0.05 0.039 0.073 0.118 0.076 0.051 0.063 0. 0.031 0.04 0.064 0.042 0.196 0.166 0.161 0.132 0.064 0.089 0.044 0.115 0.088 0.105 0.143 0.094 0.187 0.114 0.078 0.137 0.044 0.046 0.052 0. 0.017 0.149 0.15 0.104 0.001 0.06 0.022 0.061 0.0 0.006 0.012 0.029 0.008 0.035 0.029 0.008 0.006 0.041 0.002 0.043 0.009 0.013 0.003 0. 0.004 0.027 0.003 0.012 0.027 0.054 0.053 0.059 0.015 0.024 0.013 0.067 0.016 0.036 0.029 0.051 0.038 0.029 0.024 0.045 0.006 0.015 0.015 0. 0.005 0.031 0.041 0.034 0.0 0.022 0.005 0.032 0.0 0.001 0.003 0.004 0.01 0.007 0.0 0.013 0.0 0.031 0.0 0.015 0.002 0.007 0.001 0. 0.003 0.011 0.002 0.004 0.001 0.025 0.006 0.028 0.002 0.004 0.004 0.09 0.004 0.004 0.006 0.021 0.003 0.007 0.001 0.032 0.002 0.009 0.005 0. 0.003 0.0 0.006 0.013 0.0 0.01 0.0 0.005 0.0 0.002 0.0 0.006 0.001 0.0 0.0 0.002 0.0 0.016 0.0 0.004 0.0 0.003 0.002 0. 0.001 0.016 0.0 0.004 0.0 0.011 0.001 0.003 0.0 0.001 0.0 0.011 0.0 0.003 0.001 0.003 0.0 0.001 0.0 0.011 0.0 0.003 0.003 0. 0.001 0.0 0.0 0.006 9 Table 11: Gap proportion between InternVL_trained and human scoring. Model Dimension 1 Score 2 3 4 5 Anole GPT-4o+DALL-E DDiT Emu3 InterSyn Gemini+Flux Janus-Pro Liquid Show-o Show-o-Turbo VARGPT VILA-U TCC ICC 0.871 0.738 IQ 0.703 0.773 ITS TCC ICC 0.801 0.767 IQ 0.777 0.778 ITS TCC ICC IQ ITS 0.967 0.77 0.72 0.962 TCC ICC 0.775 0.747 IQ 0.748 0.741 ITS TCC ICC 0.726 0.816 IQ 0.737 0.732 ITS TCC ICC 0.786 0.728 IQ 0.746 0.806 ITS TCC ICC 0.855 0.725 IQ 0.699 0.679 ITS TCC ICC 0.846 0.796 IQ 0.716 0.71 ITS TCC ICC 0.788 0.743 IQ 0.718 0.676 ITS TCC ICC 0.718 0.684 IQ 0.713 0.715 ITS TCC ICC 0.797 0.913 IQ 0.922 0.917 ITS TCC ICC 0.906 0.719 IQ 0.702 0.827 ITS 0.106 0.193 0.206 0. 0.191 0.211 0.205 0.201 0.013 0.186 0.215 0.015 0.2 0.196 0.2 0.185 0.25 0.159 0.241 0.233 0.195 0.243 0.236 0.142 0.133 0.209 0.226 0. 0.138 0.169 0.224 0.205 0.176 0.205 0.215 0.214 0.213 0.243 0.242 0.214 0.15 0.04 0.033 0.027 0.078 0.208 0.216 0.122 0.023 0.05 0.068 0. 0.008 0.019 0.014 0.013 0.012 0.034 0.045 0.014 0.021 0.037 0.038 0.039 0.017 0.022 0.008 0.028 0.012 0.02 0.016 0.028 0.012 0.057 0.052 0. 0.012 0.021 0.052 0.066 0.023 0.028 0.059 0.082 0.054 0.052 0.024 0.06 0.047 0.032 0.03 0.038 0.012 0.062 0.059 0.044 0.0 0.012 0.019 0. 0.0 0.001 0.001 0.004 0.007 0.006 0.019 0.007 0.004 0.016 0.009 0.031 0.006 0.002 0.002 0.005 0.004 0.007 0.002 0.02 0.0 0.008 0.019 0. 0.004 0.012 0.006 0.017 0.01 0.019 0.006 0.021 0.013 0.015 0.017 0.01 0.006 0.008 0.011 0.016 0.002 0.01 0.022 0.005 0.0 0.007 0.003 0. 0.0 0.002 0.002 0.003 0.001 0.004 0.001 0.001 0.0 0.004 0.001 0.004 0.001 0.0 0.002 0.001 0.0 0.002 0.0 0.001 0.0 0.001 0.004 0. 0.0 0.001 0.001 0.002 0.003 0.004 0.001 0.004 0.002 0.006 0.004 0.001 0.0 0.004 0.001 0.001 0.002 0.001 0.001 0.001 0.0 0.0 0.001 0. 0.0 0.0 0.001 0.001 0.0 0.0 0.0 0.001 0.0 0.0 0.004 0.0 0.0 0.001 0.01 0.001 0.003 0.0 0.0 0.003 0.0 0.0 0.0 0. 0.0 0.001 0.001 0.0 0.0 0.001 0.001 0.003 0.0 0.0 0.0 0.0 0.0 0.003 0.003 0.001 0.0 0.0 0.0 0.001 Table 12: Gap proportion between QwenVL_trained and human scoring. Model Dimension 0 1 Score 3 4 5 Anole GPT-4o+DALL-E DDiT Emu3 InterSyn Gemini+Flux Janus-Pro Liquid Show-o Show-o-Turbo VARGPT VILA-U TCC ICC 0.857 0.713 IQ 0.701 0.748 ITS TCC ICC 0.783 0.765 IQ 0.757 0.773 ITS TCC ICC 0.974 0.732 IQ 0.733 0.969 ITS TCC ICC 0.757 0.729 IQ 0.749 0.781 ITS TCC ICC 0.699 0.659 IQ 0.704 0.707 ITS TCC ICC 0.735 0.708 IQ 0.712 0.703 ITS TCC ICC 0.841 0.672 IQ 0.667 0.7 ITS TCC ICC 0.833 0.704 IQ 0.699 0.661 ITS TCC ICC 0.774 0.688 IQ 0.722 0.693 ITS TCC ICC 0.69 0.672 IQ 0.624 0.747 ITS TCC ICC 0.82 0.913 IQ 0.922 0.925 ITS TCC ICC 0.89 0.639 IQ 0.646 0.8 ITS 0.121 0.225 0.229 0.189 0.207 0.209 0.227 0.187 0.015 0.223 0.227 0. 0.22 0.233 0.224 0.17 0.279 0.309 0.281 0.268 0.236 0.262 0.263 0.271 0.147 0.262 0.252 0.224 0.152 0.245 0.244 0.269 0.195 0.247 0.248 0. 0.246 0.265 0.299 0.197 0.159 0.047 0.05 0.038 0.097 0.277 0.272 0.127 0.022 0.052 0.05 0.048 0.01 0.023 0.015 0.029 0.007 0.039 0.033 0. 0.021 0.019 0.021 0.04 0.018 0.022 0.014 0.02 0.022 0.023 0.024 0.015 0.011 0.054 0.072 0.064 0.011 0.03 0.044 0.06 0.021 0.043 0.023 0. 0.047 0.039 0.068 0.045 0.017 0.031 0.027 0.019 0.012 0.058 0.072 0.06 0.0 0.006 0.016 0.013 0.0 0.001 0.0 0.009 0.003 0.005 0.004 0. 0.002 0.019 0.005 0.007 0.003 0.01 0.001 0.004 0.004 0.004 0.001 0.01 0.001 0.012 0.007 0.01 0.004 0.018 0.01 0.009 0.01 0.022 0.004 0. 0.016 0.023 0.008 0.009 0.004 0.009 0.001 0.016 0.001 0.025 0.01 0.011 0.0 0.004 0.004 0.001 0.0 0.001 0.001 0.002 0.001 0.001 0.003 0. 0.0 0.0 0.001 0.001 0.001 0.0 0.0 0.001 0.003 0.003 0.0 0.001 0.0 0.0 0.001 0.001 0.0 0.002 0.003 0.001 0.0 0.0 0.002 0. 0.001 0.001 0.001 0.001 0.0 0.0 0.0 0.001 0.0 0.001 0.0 0.002 0.0 0.0 0.0 0.001 0.0 0.001 0.0 0.0 0.0 0.0 0.0 0. 0.0 0.0 0.0 0.001 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.001 0.001 0.0 0.001 0.0 0.0 0.0 0.0 0.001 0. 0.0 0.0 0.0 0.001 0.0 0.0 0.0 0.001 0.0 0.0 0.0 0.0 11 Table 13: Evaluation accuracy comparison across judges Model Dim. GPT-4o InternVL InternVL_trained QwenVL QwenVL_trained 0.977 0.930 0.908 0.933 0.991 0.977 0.981 0. 0.979 0.955 0.934 0.976 0.974 0.942 0.946 0.925 0.975 0.973 0.976 0.964 0.980 0.970 0.981 0.946 0.987 0.932 0.924 0.897 0.983 0.964 0.939 0. 0.963 0.947 0.932 0.889 0.929 0.926 0.954 0.928 0.946 0.953 0.953 0.944 0.982 0.926 0.916 0.948 0.945 0.972 0.816 0.870 0. 0.978 0.949 0.903 0.866 0.967 0.850 0.801 0.958 0.944 0.871 0.924 0.819 0.912 0.925 0.930 0.848 0.961 0.905 0.930 0.936 0.775 0.743 0.778 0. 0.918 0.881 0.939 0.716 0.891 0.852 0.820 0.829 0.772 0.848 0.897 0.775 0.948 0.926 0.924 0.884 0.973 0.818 0.802 0.842 0. 0.977 0.937 0.929 0.936 0.990 0.974 0.983 0.959 0.989 0.954 0.958 0.982 0.975 0.961 0.972 0.950 0.978 0.967 0.983 0.974 0.970 0.969 0.975 0. 0.987 0.933 0.918 0.924 0.984 0.948 0.942 0.929 0.969 0.935 0.969 0.936 0.935 0.936 0.922 0.943 0.978 0.959 0.971 0.962 0.986 0.915 0.917 0. 0.954 Anole GPT-4o+DALL-E DDiT Emu3 InterSyn Gemini+Flux Janus-Pro Liquid Show-o Show-o-Turbo VARGPT VILA-U Average TCC ICC IQ ITS TCC ICC IQ ITS TCC ICC IQ ITS TCC ICC IQ ITS TCC ICC IQ ITS TCC ICC IQ ITS TCC ICC IQ ITS TCC ICC IQ ITS TCC ICC IQ ITS TCC ICC IQ ITS TCC ICC IQ ITS TCC ICC IQ ITS all 0.805 0.848 0.828 0.823 0.971 0.961 0.931 0.938 0.960 0.846 0.837 0. 0.896 0.867 0.912 0.811 0.917 0.936 0.945 0.876 0.946 0.941 0.950 0.924 0.727 0.806 0.796 0.775 0.853 0.880 0.877 0.770 0.867 0.842 0.827 0. 0.771 0.861 0.815 0.816 0.706 0.924 0.956 0.934 0.865 0.832 0.772 0.815 0.865 0.816 0.859 0.776 0.851 0.983 0.956 0.916 0. 0.963 0.901 0.831 0.951 0.915 0.899 0.858 0.846 0.940 0.946 0.937 0.881 0.960 0.945 0.941 0.911 0.716 0.846 0.687 0.783 0.827 0.890 0.820 0. 0.905 0.874 0.781 0.808 0.861 0.895 0.770 0.838 0.638 0.924 0.939 0.922 0.912 0.868 0.711 0.879 0.866 Figure 7: visualization of mean and variance of different generators C.6 Detailed analysis of the capabilities of different Generators Mean and Variance Analysis. Figure 7 presents the mean and variance of evaluation scores across TCC, ICC, IQ, and ITS for different generators. Several important observations emerge from the results. First, DDiT exhibits the lowest mean scores in both TCC (0.38) and ITS (0.37), indicating poor content coverage and weak image-text synergy. Second, VARGPT shows mean scores below 1 across ICC, IQ, and ITS, suggesting significant deficiencies in visual generation capabilities and multimodal alignment. Third, VILA-U demonstrates the highest variance in TCC and ITS among all models, implying that its performance is highly unstable across different questions. In contrast, Gemini+Flux and GPT+DALL-E achieve mean scores above 4.0 across all evaluation dimensions, reflecting generally strong performance. However, their relatively high variance in ITS reveals that they still struggle with maintaining image-text consistency and complementarity across samples. Most notably, InterSyn consistently outperforms all other generators across all four dimensions, achieving the highest mean scores while maintaining the lowest variance. This indicates not only superior quality but also high stability and robustness in both textual and visual generation. These findings collectively highlight the importance of both quality and consistency for robust multimodal generation, and demonstrate the synergy of SEIR in constructing high-quality, stable datasets."
        },
        {
            "title": "D Question Templates and Topic Hierarchy",
            "content": "(cid:4) D.1 Question Templates (cid:7) Do you know ***? Can you draw picture of it for me ? Do you know what *** looks like ? Please draw an image of it for me . very interested in ***. Please help me describe it and draw portrait of it . Can you imagine what *** looks like ? Please draw portrait of it for me . Have you paid attention to ***? Can you tell me something about it ? Besides , can you depict it in painting ? What do you think *** looks like ? Can you draw what this place might look like ? What does *** look like ? Can you draw sketch of it for me ? Do you know about ***? Please draw an image of them for me . heard that *** is very attractive . Can you introduce it to me ? Then draw an image of it for me . need painting of *** now . Please help me describe it and draw it . paying lot of attention to *** now . Do you know it ? By the way , help me draw picture to introduce it . Can you draw picture of ***? Besides , can you give me some popular science knowledge about ***? 13 (cid:5) (cid:4) Have you ever seen the scene of ***? What does *** usually look like ? Please draw picture for me . Will there be ** in the ** of **? Can you draw picture of this scene for me ? What is the most wonderful *** you have ever seen ? Please draw scene picture of *** for me . What kind of wonders can be seen in ***? really curious about what that scene will be like . Can you show it to me ? Can you imagine *** , with ** and ** , creating moment of ** and **? Help me draw them . Can you describe *** for me ? It would be even better if there is painting . Hey , can you tell me legend about *** and draw vivid picture ? ...(cid:6) D.2 Topic Hierarchy (cid:7) { \" Animals \": { \" Terrestrial Animals \": [\" Giant panda \" , \" Snow leopard \" , \" Black bear \" , \" Red panda \" , \" Tibetan antelope \" , \" Argali \"...] , \" Marine Animals \": [\" Blue whale \" , \" Killer whale \" , \" Great white shark \" , \" Humpback whale \" , \" Dolphin \" , \" Octopus \"...] , \" Extinct Animals \": [\" Dinosaur \" , \" Dodo \" , \" Woolly mammoth \" , \" Saber - toothed tiger \" , \" Pterosaur \" , \" Stegosaurus \"...] , \" Domesticated Animals \": [\" Pet cat \" , \" Pet dog \" , \" Pet bird \" , \" Mouse \" , \" Ornamental fish \" , \" Cow \" , \" Sheep \" , \" Pig \"...] } , \" Plants \": { \" Edible Plants \": [\" Rice \" , \" Wheat \" , \" Corn \" , \" Sorghum \" , \" Oat \" , \" Buckwheat \" , \" Quinoa \" , \" Millet \" , \" Barley \"...] , \" Medicinal Plants \": [\" Ginseng \" , \" Wolfberry \" , \" Coptis chinensis \" , \" Notoginseng \" , \" Astragalus membranaceus \" , \" Angelica sinensis \"...] , \" Ornamental Plants \": [\" Rose \" , \" Tulip \" , \" Chrysanthemum \" , \" Peony \" , \" Chinese peony \" , \" Lily \" , \" Narcissus \" , \" Hyacinth \" , \" Iris \"...] } , \" Natural Scenery \": { \" Mountains & Forests \": [\" The flag cloud of Mount Everest in the Himalayas \" , \" Alpine meadows and wildflowers in the Alps \"...] , \" Water & Weather \": [\" Hawaiian volcanic lava flowing into the sea \" , \" The blue - domed church in Santorini , Greece \" , \" dugout canoe in the lagoon of Tahiti \"...] , \" Deserts & Volcanoes \": [\" The sharp ridges on the backlit side of sand dunes \" , \" The winding silhouette of camel caravan \"...] , \" Seasons & Landforms \": [\" Red - crowned cranes dancing in the winter snow in Hokkaido , Japan \" , \" The tulip maze in Keukenhof Gardens in the Netherlands in spring \"...] } , \" Cultural Scenery \": { \" Cities & Villages \": [\" The mirror - like water surface of the terraced fields in Yuanyang , Yunnan \" , \" The lavender fields in Provence , France \"...] , \" Religion & Religious Sites \": [\" Devout believers praying in front of the Western Wall in Jerusalem \" , \" The play of light and shadow under the dome of St ...] , \" Heritages & Wonders \": [\" The giant paintings of the Nazca Lines in Peru seen from above \" , \" The Treasury at the end of the Siq in Petra , Jordan \" , \" The Pyramids of the Sun and Moon in Teotihuacan ,...] } , \" Objects \": { \" Household & Daily Items \": [\" Bench \" , \" Chair \" , \" Sofa \" , \" Coffee table \" , \" Bookshelf \" , \" Wardrobe \" , \" Desk \" , \" Dressing table \" , \" Bed \" , \" Dining table \" , \" Dining chair \"...] , 14 \" Military & Security \": [\" Pistol \" , \" Rifle \" , \" Submachine gun \" , \" Machine gun \" , \" Artillery \" , \" Missile \" , \" Tank \" , \" Armored vehicle \" , \" Fighter jet \"...] , \" Tools & Equipment \": [\" Fire hydrant \" , \" Wrench \" , \" Screw \" , \" Hammer \" , \" Shovel \" , \" Screwdriver \" , \" Tape measure \" , \" Electric drill \" , \" Pliers \" , \" Saw \" , \" File \" , \" Soldering iron \"...] , \" Energy & Industry \": [\" Generator \" , \" Solar panel \" , \" Wind turbine \" , \" Hydraulic generator \" , \" Battery \" , \" Inverter \" , \" Transformer \" , \" Charging pile \" , \" Oil drum \" , \" Gas cylinder \"...] , \" Transportation & Communication \": [\" Bicycle \" , \" Car \" , \" Motorcycle \" , \" Airplane \" , \" Bus \" , \" Train \" , \" Truck \" , \" Ship \" , \" Traffic light \" , \" Tricycle \" , \" Electric scooter \"...] } , \" Activities \": { \" Daily Life & Occupations \": [\" Doctor \" , \" Firefighter \" , \" Farmer \" , \" Teacher \" , \" Lawyer \" , \" Craftsman \" , \" Researcher \" , \" Photographer \" , \" Singer \" , \" Dancer \" , \" Painter \" , \" Journalist \"....] , \" Emotional & Social Interactions \": [\" Hug \" , \" Kiss \" , \" Meet \" , \" Talk \" , \" Lecture \" , \" Study \" , \" Shake hands \" , \" Comfort \" , \" Celebrate \" , \" Take group photo \" , \" Quarrel \" , \" Share \" , \" Wave \"...] , \" Sports & Labor \": [\" Run \" , \" Play basketball \" , \" Play football \" , \" Play volleyball \" , \" Play badminton \" , \" Play tennis \" , \" Play table tennis \" , \" Jump \" , \" Ride bike \" , \" Box \" , \" Wrestle \"...] } , \" Food \": { \" Regional Cuisines \": [\" Mapo Tofu \" , \" Ramen \" , \" Braised Pork Belly in Soy Sauce \" , \" Scrambled Eggs with Tomatoes \" , \" Shredded Pork with Green Peppers \" , \" Braised Beef with Potatoes \"...\"] , \" Baked Goods & Desserts \": [\" Caramel Pudding \" , \" Macaron \" , \" Donut \" , \" Cake \" , \" Yogurt \" , \" French Croissant \" , \" Italian Tiramisu \" , \" German Black Forest Cake \" , \" Japanese Wagashi \"...\"] , \" Processed Foods \": [\" Snacks \" , \" Canned Food \" , \" Frozen Food \" , \" Biscuits \" , \" Chocolate Biscuits \" , \" Ice Cream \" , \" Popcorn \" , \" Potato Chips \" , \" Canned Fish \" , \" Frozen Dumplings \"...\"] , \" Beverages \": [\" Red Wine \" , \" Chinese Baijiu \" , \" Beer \" , \" Coke \" , \" Juice \" , \" Tea \" , \" Milk \" , \" Soda Water \" , \" French Champagne \" , \" Italian Espresso \" , \" Japanese Sake \" , \" Korean Makgeolli \"...\"] , \" Pet Food \": [\" Dog Food \" , \" Cat Food \" , \" Chew Sticks \" , \" Bones \" , \" Pet Canned Food \" , \" Freeze - Dried Chicken Pieces \" , \" Salmon - Flavored Cat Treats \"...\"] } , \" Culture \": { \" Material Culture \": [\" Hanfu ( Han Chinese Clothing ) \" , \" Qipao ( Cheongsam ) \" , \" Kimono \" , \" Indian Sari \" , \" Western Suit \" , \" Wedding Dress \" , \" Tangzhuang ( Tang - style Costume ) \" , \" Mongolian Robe \"...\"] , \" Spiritual Culture \": [\" The Dragon Totem in Ancient China \" , \" The Phoenix Totem \" , \" The Eagle Totem of the Native Americans \" , \" The Wolf Totem \" , \" The Rainbow Serpent Totem of the Australian Aborigines \"...\"] , \" Behavioral Culture \": [\" Traditional Chinese Wedding \" , \" Western Church Wedding \" , \" Coming - of - Age Ceremony \" , \" Crowning Ceremony \" , \" Sacrificial Ceremony \" , \" Japanese Tea Ceremony Etiquette \" , \" The Hongi ( Nose Rubbing ) of the Maori People \" , \" The Namaste of India \" , \" The Apprenticeship Ceremony in Thailand \" , \" The Torch Festival Ceremony of the Yi Ethnic Group \"...\"] } }(cid:6) (cid:5)"
        },
        {
            "title": "E Evaluation Dimensions for Dataset Quality",
            "content": "E.1 Question Evaluation Dimensions Reasonableness of Expression: The question statement is smooth, without any grammatical errors, and the words are used accurately and appropriately. For example, \"Please introduce the Great Wall 15 to me and also give me picture of the Great Wall\" is reasonable expression; while \"Tell me about the Great Wall, and give me picture\" has problem of confused expression. Such questions will affect the models understanding of the intention and make it difficult to give an accurate answer. Clarity of Requirements: Clearly indicate that the model is required to provide both text and image responses simultaneously. For instance, \"Introduce the appearance characteristics of Notre-Dame de Paris and provide high-definition frontal picture\", which clearly puts forward the dual requirements of text description and image acquisition; if the question is just \"What does Notre-Dame de Paris look like\", without clearly stating the image requirement, it does not meet the requirements and cannot effectively guide the model to give comprehensive response. Focus of the Theme: The question revolves around single and clear theme and will not jump between multiple unrelated themes. For example, \"Introduce the geographical features of Mount Fuji and attach distant view of Mount Fuji\", with the theme focused on Mount Fuji; while \"Tell me about Mount Fuji and then talk about the Eiffel Tower, and give two corresponding pictures\", which involves two different themes, may lead to unclear logical answers from the model and is not conducive to the standardized construction of the dataset. Feasibility and Clarity: Based on common sense judgment, the content involved in the question is something that the model has the ability to answer through language and images, and there is no way of multiple interpretations, and the model can accurately grasp the questioners intention. For example, \"Describe the living habits of giant pandas and give picture of panda eating bamboo\", the model can answer based on its existing knowledge reserve and image generation ability, and the intention is clear; however, \"Tell me what its like for person to take bath in volcanic magma and give picture\", such questions seriously deviate from reality and lack scientific basis. The model can neither answer based on existing knowledge, nor is there real-world reference for image generation, which will lead to absurd and meaningless generation results and greatly reduce the reliability and practicality of the dataset. Appropriateness of Length: The length of the question is moderate, which not only contains enough key information to guide the model to generate high-quality answers but also is not too long and complicated for the model to grasp the key points. Generally speaking, short and concise questions are helpful for the model to quickly understand the intention, such as \"Introduce the Forbidden City and give panoramic picture\"; but being too short may lack sufficient information, such as \"Forbidden City, picture\"; and overly long and cumbersome questions, like \"Please introduce in detail the process of the changes of the Forbidden City since its construction in the Ming Dynasty through various dynasties, including the evolution of architectural styles, the transformation of functional uses and other aspects, and provide high-definition panoramic picture that can comprehensively display the current overall layout of the Forbidden City. At the same time, ensure that the picture contains the main palaces, courtyards, city walls and other iconic elements of the Forbidden City\", may cause confusion when the model processes it. The ideal length can be determined according to practical experience and testing. Usually, about 15 - 50 words is more appropriate, which can convey the requirements completely and also facilitate the model to process efficiently. E.2 Interleaved Image-Text Answer Evaluation Dimensions Text Response Quality (0-5 points): This dimension only focuses on the correspondence between the text response and the question, whether the content precisely matches the users needs, and whether the information is complete and error-free. It does not consider the output of any other dimensions and evaluation criteria. 0 points: No text appears; 1 point: The text answer has nothing to do with the question; it is completely wrong, completely divorced from the question, and there is no positive response to the text requirement; there is less content but there are truncations and random spitting characters. 2 points: The text answer can only cover small part of the elements required in the question, and there is large amount of unreasonable content; there is very obvious phenomenon of text truncation that seriously affects the original information; the content is very long or very short, which seriously affects the reading. 3 points: The answer can correspond to key elements, there is small amount of unreasonable content, and there may be omissions of key information; the content is too long or too short, but the information basically corresponds. 4 points: The required elements of the question are basically all corresponding, there is no unreasonable content, there is omission of key information, or the answer is awkward; the content is slightly longer or shorter, but the answer is very correct. 16 5 points: The content of the answer exactly corresponds to the question, there is no unreasonable content, and the answer is smooth and fluent, with full content. Image Content Quality (0-5 points): This dimension only focuses on the correspondence between the image content and the question (considering the content of the picture, the degree to which the image content answers the question). Whether the key parts are retained, and whether there is an obvious lack of objects. 0 points: No image appears; 1 point: The content of the image is completely wrong, and no key elements are depicted at all; the image has no connection to the problem, even if the image itself is of good quality. 2 points: About half of the key elements required for the problem are missing, and there are large number of unreasonable elements; the elements in the figure may have some connection to the problem, but it is almost impossible to identify what they are. 3 points: Only small number of key elements required for the problem are missing in the figure, most of the elements can be fully identified, and there are only few unreasonable content. 4 points: Basically lack the elements required for the problem, and there may be minor flaws in some details. 5 points: All the elements required for the question are completely corresponding, the main body is intact, and the picture content answers the question very well. Image Aesthetic Quality (0-5 points): This dimension only focuses on the performance of the basic generation technology of the image (do not consider the content of the picture). Whether it is clear, whether there are blurred, noisy or out-of-focus areas, truncations or damages (that is, the judgment of image aesthetics and subjective quality). 0 points: No picture; 1 point: The image is very ugly, and it is almost impossible to identify the image content. 2 points: The image looks ugly, the overall image is blurred but can be barely recognized; 3 points: The image is medium in appearance, and the main elements can be distinguished, but other elements are blurred. 4 points: The image looks good, the picture is relatively clear, and there is no visible blurring phenomenon; 5 points: The image looks good, the details are sharp without blur, and the image quality is very high. Text-Image Synergy (05 points): This dimension evaluates the degree of alignment and complementarity between the textual and visual components of response. It focuses not only on how well the entities or scenes described in the text are accurately and completely depicted in the image, but also on whether the text and image together form coherent and mutually supportive answer to the question. 0 points: The image and text are completely unrelated. Additionally, if either the image or the text is missing (i.e., null), the response is assigned 0 points. 1 point: The image and text are minimally related, with only few elements weakly corresponding. The response lacks coherence and fails to effectively address the question. 2 points: Around half of the key elements described in the text are reflected in the image, but significant mismatches remain. The overall synergy is poor. 3 points: Most elements between the text and image are consistent, but few important mismatches or omissions in key entities or scenes reduce the completeness of the response. 4 points: Nearly all elements between the text and image are consistent, with only minor mismatches in non-critical details. The response answers the question well, but there may be redundancy between the two modalities, limiting their complementarity. 5 points: The text and image are perfectly aligned, with all described elements accurately and fully presented. The two modalities work together in complementary way to form complete and informative response without unnecessary duplication."
        },
        {
            "title": "F All Prompts Used in this Work",
            "content": "F.1 Prompts Used in SEIR Method Only simple example of single-round dialogue generation prompt is provided here. The most detailed prompts are given in detail in the open-source code. Detailed prompts can be found in our code. Here is the prompt for the question generation: (cid:7) am building question - answer dataset . The topic of this dataset item is ({ topic }) . Your task is to generate question based on this topic . (cid:4) 17 The length of the question should not exceed 50 words . Here is the question template : { ques_temp }. The new question you generate can refer to the sentence pattern of the question template . The question must meet the following detailed requirements : 1. ** Incorporate Image Request Naturally **: Clearly express the need to generate picture , but use varied and creative expressions to make the request feel natural and human - like . Avoid repetitive phrases like maybe generate picture . Instead , use diverse sentence structures to request the image . 2. ** Varied Sentence Structures **: Diversify how questions are phrased . Use different ways of asking , such as open - ended questions , hypothetical scenarios , or requests for examples . 3. ** Conciseness and Clarity **: Ensure the question is still concise and immediately understandable but without sounding repetitive or formulaic . Avoid redundant language . 4. ** Topic Relevance **: Keep the question focused on the given topic ({ topic }) , ensuring it remains engaging and meaningful . Avoid weak connections to the topic . 5. ** Approachable Tone **: Use conversational , approachable tone that mimics real human interactions . Keep it friendly and engaging , avoiding overly formal or robotic expressions . 6. ** Lexical Simplicity with Creativity **: Use everyday vocabulary with occasional creative language that fits the topic . Ensure accessibility for broad audience while maintaining interest . 7. ** Question Value and Inspiration **: Make the question thought - provoking or creative , capable of inspiring meaningful answers . Avoid overly simple or overly complex questions . 8. ** Image Context **: Clearly specify what kind of picture is expected , but do so creatively . Output only the generated question directly . Do not include explanations , instructions , or any extra text . (cid:6) Here is the prompt to get the the suggestions for the question: (cid:7) am currently constructing question - answer dataset . The first step is to imitate human needs and tone based on certain topic and ask question . This question needs to include the requirement for generating textual content and picture . The topic is : ({ topic }) . The following is question generated based on this topic : { old_q } You need to analyze the quality of this question from human perspective , such as whether the question is too wordy ? Is the question sentence pattern not commonly used in human daily communication ? How well does the question fit the topic ? Does the tone of the question sound human ? Are there any uncommon expressions in the sentence ? Is it meaningless question ? Does the question contain request for generating an image ? Is the generated question easy to answer ? And so on . You need to help me provide revision suggestions . It would be best if the suggestions are concise and brief , and not too long . If you think the original question is not good in other aspects , you need to help me give modification suggestions . Only output the modification suggestions in the end , and there is no need to output the modified results . Your output should conform to this format { json_format } If you think the original question is good enough , you don need to give improvement suggestions . You only need to output None . Therefore , your final output is either None or the modification suggestions . (cid:6) Here is the prompt for the question modification: (cid:7) (cid:5) (cid:4) (cid:5) (cid:4) 18 (cid:5) (cid:4) (cid:5) (cid:4) am currently constructing question - answer dataset . The following is the original question generated by an LLM : { old_q } However , believe the quality of this question can be improved , as it doesn sound like something people would naturally ask in daily communication . have provided some modification suggestions : { mod_q_suggestion }. Please revise the question based on these suggestions and the given topic , making it sound more natural and human - like . Finally , output only the modified question without any additional text .(cid:6) Here is the prompt to get the answer of the question: (cid:7) Currently , constructing question - answer dataset . This is the current question : { final_q } Since this question usually contains requirement for textual answer and image generation . , but you don need to generate the actual image . Instead , you should generate an answer and description of the image according to the question . To ensure high Image - Text Synergy ( ITS ) , write the answer line so it gives the core explanation while referencing key visual elements , and write the caption line so it adds complementary details that the text omits ; the two lines must stay tightly aligned , avoid duplication , and together convey more than either could alone . Therefore , your response should include an answer to the question : answer ; and description of the image : caption . And you are not allowed to output responses like can generate images . You need to pretend that you can . The image description must not exceed 65 words . This last point is very important ! You just need to output in two lines and there should be no other content . The output content : start the first line with answer : , representing the answer ; start the second line with caption : , representing the caption . Your answer should be related to the previous content and must not be repetitive . (cid:6) Here is the prompt for the suggestions for answer modification: (cid:7) Currently , constructing question - answer dataset . Here is the question : { final_q } . The question usually includes the requirement for textual answer and image generation . Then , here is the answer to this question : { old_ac } The answer is divided into two parts , including the textual answer to the question and an image description . Do you think the combination of this answer and image description can fully meet the requirements of the question ? Are the image description and the answer content consistent and not redundant ? How is the correlation among the question , the answer and the image description ? Does it conform to the habits of human answering questions ? If you were knowledgeable human expert , how do you think you would answer this question ? Would the answer seem too wordy ? Would the overlap between the answer and the image description be too high ? Can the image description well summarize picture ? If you were nitpicking critic , do you think there are areas for improvement in this question , the answer and the image description ? Would the image description be too short and not rich enough in content ? Are there any discriminatory elements in the answer and the image description ? And so on . You can give modification suggestions based on the above aspects . Or if you think the answer is unreasonable in other aspects , you also need to give your modification suggestions . 19 (cid:5) (cid:4) (cid:5) (cid:4) In addition , the modification suggestions need to be divided into two parts : the answer and the image description . And the content needs to be concise and condensed , not overly long . Or if you think the answer and the image description are already perfect , you don need to put forward improvement suggestions , and just output None . Therefore , your final output is either None or the modification suggestions . Only output the modification suggestions in the end , and there is no need to output the modified results . Your output should conform to this format { json_format } or None . (cid:6) Here is the prompt for the answer modification: (cid:7) You are tasked with improving the output of model output based on the suggestion feedback . Here is the context and what you need to do step by step : Model Output to Modify ( old_ac ) : { old_ac } This is the current answer generated by the model . The answer is divided into two parts : - answer : This is the text answer to the question . - caption : This is the image description associated with the answer . Modification Suggestion ( mod_ac_suggestion ) : { mod_ac_suggestion } This is the suggestion for improving the model output , including corrections or enhancements to both the answer and caption parts . Your task is to : - According to the provided mod_ac_suggestion , update the answer and caption sections in old_ac . - Ensure that the updated caption does not exceed 65 words . - Follow the specified format strictly . Important : You just need to output in two lines and there should be no other content . The output content : start the first line with answer : , representing the answer ; start the second line with caption : , representing the caption . (cid:6) Here is the prompt for the suggestions for caption modification: (cid:7) Currently , constructing question - answer dataset . Here is the question : { final_q } . This question usually contains request for generating textual content and picture . Then , this is the original answer final_a : { final_a } and the image description old_c : { old_c } generated according to this question , You now need to evaluate the quality of the image description and the image based on the question and the answer . Does the image match the image description ? When proposing revisions , follow the Image - Text Synergy ( ITS ) principle : suggest changes that make the picture ( and its caption ) complement rather than repeat the fixed textual answer , depict the visual elements the answer references , reduce redundancy or irrelevant details , and keep full factual consistency so that image + text together convey more than either could alone . How is the degree of correlation between the image description and the content of the answer to the question ? Can the image description well summarize the content of the picture ? Are there any unreasonable objects or behaviors in the image ? Is the image description clear and not wordy ? And so on . You can give modification suggestions regarding the image description based on the above aspects . Suggestions in other aspects not mentioned above are also highly encouraged to be put forward . The revision suggestions you provide need to be concise and condensed , and shouldn be too long . 20 If you think the image description and the image for this question and answer are already perfect , then you don need to put forward any suggestions and just output None . Therefore , your final output is only None or the modification suggestions . Only output the modification suggestions in the end , and there is no need to output the modified results . Your output should conform to this format { json_format } or None . (cid:6) Here is the prompt for the caption modification: (cid:7) Currently , constructing question - answer dataset . The question usually includes the requirement for textual answer and image generation . Then , this is the image description of the answer : { old_c } Then think the quality of the image description is not very high . have provided some modification suggestions here : { mod_c_suggestion } Please regenerate the image description according to these suggestions . The length of the picture description should not exceed 65 words . In the end , you only need to output the modified image description . (cid:6) F.2 Interleaved Image-text Answer Evaluation Prompt Used by MLLM Here is the prompt for evaluating the interleaved image-text answer: (cid:7) You are an experienced , fair and impartial judge . Next , will provide you with conversation where human interacts with different GPTs on daily topics . In this scenario , the human will pose text question , and the GPT response is based on this question . This response usually includes piece of text and image information , but there may be exceptions where there is only text or only image information . Now you need to reasonably rate the response given by the GPT . < chatbegin > represents the start of the & data , and < chatend > represents the end of the & data . The rating of the response is divided into the following four dimensions , and you should rate the response fairly and impartially according to the criteria of each dimension . Here are the four dimensions for evaluating the response : \"\"\" < Interleaved Image - Text Answer Evaluation Dimensions > \"\"\" The content of your output rating must strictly conform to the following format : [ Text Response Quality : *; Image Response Quality : *; Image Aesthetic Quality : *; Text - Image Synergy : *] your score * for different dimensions , only as score in (0 , 1 , 2 , 3 , 4 , 5) . You need to strictly conduct the grading . Here is the data you need to evaluate , and you need to evaluate the quality of the Answer from the above four dimensions ( both text and image may be \" null \" , and the fact that one of them is \" null \" will not affect the rating of other dimensions .) : (cid:6)"
        },
        {
            "title": "G Benchmark Samples",
            "content": "The partly benchmark examples obtained after modification based on the questions raised by the participants are as follows: (cid:7) Are there fireflies in the forest on summer night ? Can you draw picture of this scene for me ? What could wonderful concert scene be like ? Please draw scene of concert for me . (cid:5) (cid:4) (cid:5) (cid:4) (cid:5) (cid:4) 21 What kind of wonders can be seen in the forest on cold winter night ? very curious about what that scene would be like . Can you show it to me ? Who is the king of the African savanna ? Can you draw picture to depict it ? Can you imagine serene ocean scene with setting sun and some seagulls , creating calm and relaxing moment ? Please draw it for me . Can you describe forest for me ? It would be even better if there is painting . Hey , can you tell me really terrifying legend and draw vivid picture of it ? Can you describe the scenes in futuristic music video ? If possible , can you quickly draw sketch ? really eager to see your ideas ! Hey ! What is the daily life of people in the military usually like ? Also , can you show me what soldier in military uniform looks like ? Describe the scene of huge lightning bolt during storm . Draw picture of this scene . Can you imagine how the concept of the Tree of Life is presented in different religions ? Perhaps painting showing its symbolism would be helpful . Can you describe what the snowy scene in blizzard is like ? want to see such landscape . Introduce delicious snack . Describe its appearance , ingredients , and what makes it so appealing . Also , draw what it looks like . Describe the traditional decorations of the Lantern Festival and show me picture of lively lantern display . Can you describe an autumn scene with vivid orange - red leaves under clear blue sky ? Then draw an image of it . What are some interesting behaviors of cats ? Can you show me picture of cat marking its territory ? Can you quickly draw picture of the Christ the Redeemer statue in Rio de Janeiro and share some interesting facts about it ? Hey ! Can you describe spring garden scene ? really want to hear enough details , and you need to draw it according to the description ! Do you know what happened 100 years ago ? Please draw history - related picture ! Thank you ! need picture of the age of the dinosaurs now . Do you know about past history ? Please draw picture for me . Can you describe what basketball court is like ? Draw basketball moment for me . What is the Lantern Festival like ? Can you show me some pictures of traditional lanterns ? What kind of casual outfit do you think is suitable for wearing on relaxed Saturday afternoon ? Can you draw what it looks like ? Can you describe and perhaps draw picture showing person practicing yoga in tranquil park at sunrise ? need landscape picture of the countryside . Please describe it and draw an image for me . Can you draw picture of an airplane for me ? Also , give me some popular science knowledge about it . ...(cid:6) (cid:5)"
        },
        {
            "title": "H Human Annotation Platform",
            "content": "We develop human annotation platform to evaluate the quality of interleaved image-text responses. Annotators assess each response across four predefined dimensions, focusing on the content and coherence between visual and textual elements. To ensure annotation reliability, cross-validation is conducted on high-rated samples. An overview of the annotation interface is shown in Figure 8. 22 Figure 8: Human annotation platform to evaluate the quality of interleaved image-text responses."
        },
        {
            "title": "I Limitations of This Study",
            "content": "While our work introduces InterSyn, the first large-scale, instruction-following dataset for multi-turn, interleaved image-text dialogues, and proposes SynJudge, comprehensive automatic evaluator emphasizing image-text synergy, several limitations remain that suggest directions for future improvement. First, although our SEIR framework substantially enhances output quality through multi-stage refinement, the visual fidelity of generated images is inherently constrained by the upper bounds of current text-to-image models. This may limit the expressiveness and precision of visual responses, particularly for fine-grained or specialized topics. Second, our current dataset is restricted to one image per dialogue turn, which simplifies the modeling process but diverges from real-world scenarios where understanding or generating multiple images simultaneously is often necessarye.g., comparative reasoning, procedural steps, or spatial reasoning tasks. While we have experimentally validated the feasibility of multi-image dialogue generation using alternative synthesis pipelines, such functionality is not yet reflected in the released dataset. Third, the SynJudge evaluator is currently designed to assess single-image responses, meaning it does not fully capture the additional complexity and multimodal dependencies introduced by multi-image contexts. Extending SynJudge to support multi-image evaluation is promising future direction. Finally, although InterSyn spans diverse domains and fine-grained topics, future work could enhance its coverage of highly structured tasks or multi-modal reasoning chains that involve deeper world knowledge or long-term dialogue coherence. These limitations highlight important opportunities for scaling interleaved image-text datasets and improving evaluators toward more generalizable, high-fidelity multimodal generation systems."
        }
    ],
    "affiliations": [
        "Nankai University",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "University of Science and Technology of China",
        "Wuhan University"
    ]
}