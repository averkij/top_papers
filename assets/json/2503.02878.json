{
    "paper_title": "Language Models can Self-Improve at State-Value Estimation for Better Search",
    "authors": [
        "Ethan Mendes",
        "Alan Ritter"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Collecting ground truth task completion rewards or human demonstrations for multi-step reasoning tasks is often cost-prohibitive and time-consuming, especially in interactive domains like web tasks. To address this bottleneck, we present self-taught lookahead, a self-supervised method that leverages state-transition dynamics to train a value model capable of effectively guiding language model-controlled search. We find that moderately sized (8 billion parameters) open-weight value models improved with self-taught lookahead can match the performance of using a frontier LLM such as gpt-4o as the value model. Furthermore, we find that self-taught lookahead improves performance by 20% while reducing costs 37x compared to previous LLM-based tree search, without relying on ground truth rewards."
        },
        {
            "title": "Start",
            "content": "Language Models can Self-Improve at State-Value Estimation for Better Search 5 2 0 2 4 ] . [ 1 8 7 8 2 0 . 3 0 5 2 : r Ethan Mendes 1 Alan Ritter 1 Abstract Collecting ground truth task completion rewards or human demonstrations for multi-step reasoning tasks is often cost-prohibitive and timeconsuming, especially in interactive domains like web tasks. To address this bottleneck, we present self-taught lookahead, self-supervised method that leverages state-transition dynamics to train value model capable of effectively guiding language model-controlled search. We find that moderately sized (8 billion parameters) open-weight value models improved with self-taught lookahead can match the performance of using frontier LLM such as gpt-4o as the value model. Furthermore, we find that self-taught lookahead improves performance by 20% while reducing costs 37 compared to previous LLM-based tree search, without relying on ground truth rewards. 1. Introduction While current large language models (LLMs) have exhibited impressive intrinsic reasoning capabilities (Wei et al., 2022; Wang et al., 2022), recent work (Yao et al., 2024; Xie et al., 2024) has demonstrated that eliciting more deliberate reasoning through explicit tree search improves performance on complex multi-step reasoning tasks. During this search process, policy LLM proposes possible actions from the current state, and value LLM evaluates and selects the most promising states to explore. Recent work has found the strength of the value model to be strongly correlated with overall performance (Chen et al., 2024; Liu et al., 2024a). Much prior work on LLM-based tree search simply prompts an off-the-shelf LLM to serve as both the policy and value models during the search process, where performance is constrained by the capabilities of this model. Recent approaches have addressed this constraint by utilizing ground truth task completion rewards during Monte Carlo tree search (MCTS) to guide node selection. Zhou et al. (2024) show that MCTS using these rewards can outperform gradient-based methods 1Georgia Institute of Technology. Correspondence to: Ethan Mendes <emendes3@gatech.edu>. 1 that train on human demonstrations or rejection sampled trajectories. However, collecting ground-truth rewards can be costly. For example, in interactive domains like web tasks, data collection can cost thousands of dollars (Yao et al., 2022a). This constraint motivates the need for new methods that can perform self-supervised training of the policy or value LLM without ground truth rewards. In this paper, we introduce self-taught lookahead (STL), technique that enables an LLM-based value model to improve itself by bootstrapping an initial value function and leveraging the environments state transition dynamics without requiring any ground-truth rewards. To the best of our knowledge, this is the first demonstration that an LLM-based value function can self-improve without labels or rewards. We show that using this self-improved value model during search can outperform computationally expensive MCTS methods (Zhou et al., 2024), while reducing inference costs by an order of magnitude. The self-taught lookahead (STL) process (Figure 1) begins by generating self-improvement data through single step of lookahead within tree search. Analogous to the Bellman update, this lookahead refines the estimated value of state by leveraging information about potential future states. However, unlike classical reinforcement learning (RL) methods, which rely on explicit environment rewards, STL uses large language model (LLM) to estimate state values. Specifically, we fine-tune value model to reason about the utility of state by predicting the next best action, its resulting state, and corresponding rationale for the value of that state. During training, the model is fine-tuned using rollouts of states and actions within the environment. At inference time, the value model simulates step of lookahead to provide more accurate value judgments. STL, therefore, more explicitly captures the paradigm of verbal reinforcement learning than previous work (Shinn et al., 2024) by learning from natural language representation of the mechanics of traditional RL algorithm. By learning to simulate the entire process of lookahead rather than only iterated values, STL can leverage the strong generalization improvements of LLMs resulting from this sort of rationalization (Nye et al., 2021; Zelikman et al., 2022). Our results (4) demonstrate that tree search with an STL fine-tuned llama-3.1-8b-instruct value Language Models can Self-Improve at State-Value Estimation for Better Search Figure 1. Self-taught lookahead self-improves the value model by learning from state-transition dynamics. During the data generation phase (top left), tree search is used to discover diverse states. For every observed state encountered during the search, successor states are expanded using base policy πθ and the current value model Vϕk , and formatted textual training example is formed using verbal representations of the next best action and successor state, as well as Vϕk outputted value reasoning (r) and numerical value (v) (top middle). These examples are used to fine-tune Vϕk+1 , which will be used in the next iteration of the algorithm (top right). Value models learned during self-taught lookahead can be used to evaluate unseen states encountered during search on unseen tasks by simulating step of lookahead including the next best action and the best successor state (bottom). model improves performance by 39% or more compared to the base model and matches the performance of search with base gpt-4o value model on unseen tasks. We show that these results hold across web agent and math reasoning domains, including WebShop (Yao et al., 2022a) and Game-of-24 (Yao et al., 2024). Finally, in 5, we demonstrate that search with STL value models achieves Pareto optimality when considering cost and environment usage (states expanded). The results reveal that self-taught lookahead effectively transfers computation from expensive large closed-source models to cheaper opensource alternatives, achieving 37 cost reduction while outperforming previous LLM tree search methods (Zhou et al., 2024) by 20%. Additionally, we analyze scaling trends for self-taught lookahead and find that STL can even be utilized to improve smaller models ( 3 billion parameters). 2. Background 2.1. Guiding Tree Search with Language Models Within state space S, the goal of the tree search is to reach desired state from an initial state s0 S, where is determined based on the natural language task . state si might be step in reasoning chain or an intermediate webpage in web navigation task. While the algorithmic details vary based on the tree search method e.g. breadth-first search (BFS) or Monte Carlo Tree Search (MCTS), to adapt these methods to utilize language models (LMs), we simply need to define how new states (successors) are generated and evaluated. Action generation. Given trajectory of + 1 states during the search process, candidate actions a(j) in the action space are sampled using an LLM-based policy πθ: a(j) πθ(aix, s0, , si), {1, B} (1) where is the branching factor or the number of specified candidate actions. These sampled actions constitute the set Asi . We denote the transition function as : S, so for an action ai, si+1 = (si, ai). State evaluation. value vsix and rationale for the value rsix of state si is generated using LLM-based value model Vϕ : L, where is the space 2 Language Models can Self-Improve at State-Value Estimation for Better Search of natural language sequences: 3.1. Generating Rollouts (rsix, vsix) Vϕ(x, s0, , si) (2) Note that because Vϕ is an LLM, rsix is generated first, and vsix is then conditionally generated. We will denote rsix Res(Vϕ(x, s0, , si)) and vsix Val(Vϕ(x, s0, , si)). While previous work (Zhou et al., 2024; Koh et al., 2024b; Yao et al., 2024; Yu et al., 2024) do generate rationales during state evaluation, usually they are generated purely to leverage the performance improvements of LLMs when asked to rationalize (Wei et al., 2022) and are subsequently discarded. As described in 3, we explicitly use these rationales for self-improvement. 3. Better State-Value Estimation with Self-Taught Lookahead Unlike neural models traditionally used for state-value estimation in the learning to search literature (Silver et al., 2017), an LLM can leverage both conventional numerical values and natural language reasoning to estimate state value. Through our STL method, we train LLM value models that learn to better assign values to states based on their expected future utility by constructing and learning from rationales that explicitly capture domain-specific state transition dynamics. For instance, without understanding these transitions, it might not be clear whether CLOSE (X) button on website interface exits the current view or the entire workflow1 in web tasks . While MCTS-based methods (Zhang et al., 2024a) may learn how transitions affect value assignment implicitly i.e. through the backup of terminal state rewards, we explicitly learn from these transitions via supervised fine-tuning (SFT) on natural language representations of future actions and states, and value estimation rationales. We find that this explicit approach allows us to employ more compute-efficient search methods (5) while maintaining or improving performance (4) and leverage the demonstrated generalization improvements of learning from rationales (Nye et al., 2021; Zelikman et al., 2022). Learning better state-value estimates from the state transition dynamics is also ideal for self-improvement as the environment provides state transition outcomes, so our approach does requires neither ground truth reward, human demonstrations, nor labels. STL thus assumes static policy model πθ and only trains the value model through one or more iterations of self-improvement. We denote the value model initialized with base LLM Vϕ0 and the value model used in subsequent iteration to assign values and generate rationales Vϕk . Figure 1 presents the method in full, but this section enumerates the data generation, training, and inference steps in more detail (see Algorithm 1 in Appendix for concisely written algorithm). 1This is known problem in UX design. An iteration of self-taught lookahead starts with dataset Drolloutk of natural language tasks for the current iteration. For each xi Drolloutk , we roll out the search tree using πθ and Vϕk . Using tree search enables us to collect diverse set of states so that the value model trained on these states values can better generalize to unseen states and tasks. We empirically demonstrate this generalization in 4. When visiting state sj on the trajectory {s0, , sj} during tree search, we compute sjs lookahead value, ysj : ysj γ max aAsj (cid:110) Val(Vϕk (xi, s0, , sj, sj+1)) (cid:111) (3) where γ is the discount factor and sj+1 (sj, a). These lookahead values capture better estimate of the true value of sj as they account for sjs successor states. In 6, we describe how generating and learning from these ys is similar to fitted value iteration. Action-outcome rationales. However, alone, these lookahead values fail to reflect why given state is valuable as they do not capture (1) which action yielded the best (highest value) successor state and (2) why the best successor state was assigned high value by Vϕ. To better capture the state transition dynamics, we also generate action-outcome rationales when visiting state sj. These rationales are of the form {action} {outcome state} {value rationale} where action is sampled from the optimal actions from the max in (3), outcome state is the successor state derived from taking the action, and value rationale is the rationale for the evaluation of this successor state generated by Vϕk . Fine-tuning on these rationales will enable value model to predict the result of taking an action and incorporate this prediction (lookahead) into the current states value estimate. Formally, we can define these action-outcome rationales osj : j+1Res(Vϕk (xi, s0, , sj, sj+1)) osj (4) where denotes concatenation, (cid:110) j+1 (sj, ), and (cid:111) Val(Vϕk (xi, s0, , sj, (sj, a))) (5) arg max aAsj The training data set for iteration is thus set of tuples: Dk = (sk, osk , ysk ). Depending on the task, it might be necessary to automatically filter out tuples that have malformed rationales or account for the same state seen multiple times in different iterations (see 4.2). 3.2. Fine-Tuning the Value Model We start training the new value model Vϕk+1 from the initial or base LLM value model Vϕ0. We can then train using 3 Language Models can Self-Improve at State-Value Estimation for Better Search Table 1. Comparison of methods for the WebShop task. For each method, the models and inference configuration are listed along with the usage of human demonstrations, ground truth rewards, environmental observations, and fine-tuning marked with (used) and (unused). In the Reward and Demo Free setting, only environment observations are accessible. Method Inference Config Human Demo. GT Reward Env. Obs. FT Reflexion (Shinn et al., 2024) LATS (Zhou et al., 2024) IL (Yao et al., 2022a) IL+RL (Yao et al., 2022a) MCTS + DPO (Putta et al., 2024) Greedy Baseline MCTS Baseline STL (Ours) MCTS pass@3 pass@3 MCTS Greedy (pass@3) MCTS (pass@3) Greedy (pass@3) Expert (Yao et al., 2022a) standard supervised fine-tuning negative log-likelihood loss for the generation of both the action-outcome rationale and the lookahead value (osys) of the state. We train the value model to generate the rationale before estimating the value. Automatically constructed text or formatting as seen in Figure 1 are applied for easier learning. 3.3. Search after Self-Taught Lookahead value model resulting from iteration of self-taught lookahead (Vϕk+1 ) can directly replace value model in any search algorithm: 4.2 and 4.1 demonstrates usage in BFS and Greedy search, respectively. As shown in Figure 1 (bottom), Vϕk+1 simulates step of lookahead for the state sn i.e. for (rsnx, vsnx) Vϕk+1(x, s0, , sn), rsnx = an+1sn+1rsn+1x (6) 2022a) which consists of interactive web tasks involving searching for and purchasing an item that matches short natural language specification. This benchmark is an ideal test bed to demonstrate the ability of our approach as, unlike other benchmarks (Zhou et al., 2023; Koh et al., 2024a), ground truth reward is provided for all tasks allowing direct comparison between existing search and RL-based methods that use these rewards and self-taught lookahead which does not. Furthermore, WebShop also has separate training set enabling the demonstration of self-improvement generalization and provides textual representation of webpages that we leverage (see Appendix B.1 for an example). Self-taught lookahead for web tasks. Following the empirical advantages on agent tasks identified by Yu et al. (2024), we generate training data with MCTS by performing step of lookahead at each step during rollout. Note that we use the LLM value model value outputs as proxy reward to guide UCT (Upper Confidence bounds applied to Trees) (Kocsis & Szepesvari, 2006) selection like Yu et al. (2024), instead of ground truth reward (Zhou et al., 2024). We perform self-taught lookahead with gpt-3.5-turbo (Brown et al., 2020) policy to be consistent with previous work (Zhou et al., 2024; Shinn et al., 2024) as well as gpt-4o (OpenAI, 2024) policy and finetune llama-3.1-8b-instruct (Dubey et al., 2024) value function. STL is performed by rolling out 50 tasks from the WebShop training set resulting in close to 1500 training examples, which we find is sufficient for significant performance improvement. Additionally, while we perform data generation during self-taught lookahead with MCTS, we evaluate the agent using the trained value models with greedy search, where the next action is greedily chosen based on the value of the policys proposed actions i.e. at each state si, we pick action ai: (cid:110) (cid:111) Val(Vϕ(x, s0, , si) (7) where an+1sn+1 is simulated step of lookahead and rsn+1x is the value rationale of the simulated successor. ai max aAsi 4. Experiments We benchmark the self-taught lookahead self-improvement approach on applied web agent tasks and traditional math reasoning tasks2. 4.1. Web Tasks As mentioned in 1, it is particularly challenging and expensive to gather ground truth web task completion data (Zhang et al., 2024a) motivating self-improvement techniques like self-taught lookahead. To benchmark our self-taught lookahead method on web tasks, we utilize WebShop (Yao et al., 2Our code is available at https://github.com/ethanm88/selftaught-lookahead. As we will show in 5, greedy search enables significant improvement in efficiency compared to prior methods. Finally, we find that single iteration of self-taught lookahead is sufficient to see significant improvement over using base LLM-initialized value model and that multiple iterations do not yield additional performance improvements due to difficulties in simulating more than one step ahead given the complexity of the environment (in 4.2 we show that this multi-step simulation is possible for simpler tasks). See Appendix for further details about data generation, training, and further discussion about multiple iterations. Baselines. Table 1 presents comparison of the settings of the evaluated methods. We include baselines that use ground truth reward and combination of 4 Language Models can Self-Improve at State-Value Estimation for Better Search Table 2. Score and success rate (SR) on WebShop. Results marked with are taken from previous work (Zhou et al., 2024; Putta et al., 2024). Results correspond to methods specified in Table 1. Value functions marked with are fine-tuned. Due to the computational complexity of MCTS methods, we report their results on only the mini-test set used by Zhou et al. (2024) when results on the full test set are not available. We observe 40% improvement in success rate when using the STL value function compared to the llama-3.1-8b-instruct base value model in the greedy setting. We compute statistical significance of Reward and Demo Free methods compared to the underlined results (p < 0.05, < 0.01, < 0.001) using the paired bootstrap test (Berg-Kirkpatrick et al., 2012). The best results are also bolded. Setting Method Policy Value Prompting + Search Fine-Tuning + RL Reflexion LATS IL IL+RL AgentQ Reward and Demo Free Greedy Baseline MCTS Baseline STL (Ours) gpt-3.5-turbo gpt-3.5-turbo BERT + BART BERT + BART xLAM-v0.1-r-46.7b gpt-3.5-turbo gpt-3.5-turbo gpt-3.5-turbo gpt-3.5-turbo gpt-4o gpt-4o gpt-4o gpt-4o gpt-3.5-turbo gpt-3.5-turbo gpt-4o Human Expert gpt-3.5-turbo gpt-4v llama-3.1-8b-instruct r1-distill-llama-8b gpt-3.5-turbo gpt-4o llama-3.1-8b-instruct r1-distill-llama-8b gpt-3.5-turbo gpt-4o llama-3.1-8b-instruct llama-3.1-8b-instruct llama-3.1-8b-instruct Mini Test Set (50) SR Score Full Test Set (500) Score SR 58.5 75.9 57.5 58.9 70.0 68.4 71.5 72.9* 71.6 71.6 77.4*** 74.4** 71.9 78.3*** 76.0*** 76.1 24.0 38.0 34.0 26.0 26.0 24.0 38.0*** 42.0*** 28.0 32.0* 46.0*** 46.0*** 34.0** 46.0*** 40.0*** 54. 49.5 59.9 62.4 67.7 66.3 70.6*** 71.5*** 67.2 66.5 72.4*** 71.4*** 72.8*** 74.2*** 82.1 16.4 29.1 28.7 50. 26.4 24.6 35.6*** 40.6*** 25.8 25.6 38.8*** 40.8*** 36.6*** 40.6*** 59.6 search and reflection (Shinn et al., 2024; Zhou et al., 2024) by prompting closed-source LLMs, as well as imitation learning and RL methods that train smaller BERT (Devlin et al., 2019) and BART (Lewis et al., 2019) models on human demonstrations and ground truth reward (Yao et al., 2022a). We additionally compare to the current state-of-the-art approach AgentQ (Putta et al., 2024), which finetunes xLAM-v0.1-r-46.7b policy on rolled out MCTS search trees using direct preference optimization (Rafailov et al., 2023). Finally, we also include baselines in the Reward and Demo Free setting where only environment observations are accessible. These baselines include greedy search and MCTS3 with base LLM (llama-3.1-8b-instruct, gpt-3.5-turbo, gpt-4o). We also experiment with the recently released r1-distill-llama-8b (Guo et al., 2025), which allows direct comparison between general-purpose reasoning model and STL. We use the pass@k (Chen et al., 2021) for methods that do not have access to ground truth reward at inference time4. full WebShop test set and on the mini test set of 50 tasks which was used by (Zhou et al., 2024) as we find running LATS and other MCTS methods on the entire test set is computationally expensive. Both of these sets are distinct from those seen during self-taught lookahead. We find that self-taught lookahead leads to greater than 17% improvement in average reward and 39% improvement in success rate relative to base llama-3.1-8b-instruct value model, both of which are statistically significant improvements (p < 0.001) using the paired bootstrap test (BergKirkpatrick et al., 2012). Moreover, although it unsurprisingly underperforms compared to AgentQ, which is trained with environmental rewards, the STL model settings generally match or outperform tree search methods and settings where closed-source LLM is used as value model. Note that we find no statistically significant difference between using gpt-3.5-turbo or gpt-4o policy due to high action diversity which emphasizes the role of the value model over the policy (see Appendix B.2). Results and discussion. The WebShop average reward (Score) and success rate (SR) of all methods evaluated are presented in Table 4. We present results on both the 3We use LLM value as proxy reward to guide UCT similar to the data generation phase of self-taught lookahead 4Note the IL + RL method only has access to ground truth reward at train time. Reasoning Ablation. We also perform ablations on the set of information from the step of lookahead used to fine-tune the value model during self-improvement. Specifically, we compare STL, which uses the lookahead value, the textual representation of the next best action and the successor state, and the value rationale for the successor state, to other settings where only subset of this information is used. The results of this ablation in Table 3 demonstrate that 5 Language Models can Self-Improve at State-Value Estimation for Better Search Table 3. Ablation study on WebShop of the impact of fine-tuning with different combinations of information from lookahead namely lookahead values (LV), textual representation of the next best action and successor state (TR), and the value rationale for the successor state (R). The underlined results are from the base model before any fine-tuning. Fine-tuning Data Setup Score SR(%) llama-3.1-8b-instruct + LV + LV + TR + LV + TR + (STL) 70.4 70.9 74.4 76.4 30.0 34.0 32.0 42.0 regressing solely on lookahead values and also incorporating state transitions from lookahead does improve performance relative to the base model. However, learning from the value rationale of the successor state, as done in STL, yields large improvements in success rate over these other settings. These results substantiate the claims made in 3 about the necessity of learning from action-outcome rationales, key difference between STL and both classical RL (Gordon, 1999) and other LLM tree search works (Feng et al., 2023; Zhang et al., 2024a) which fine-tune an LLM value model on numerical values only. 4.2. Math Reasoning We also study the performance of self-taught lookahead on math reasoning through the Game-of-24 task where inputs consist of four integers between 1 and 13, and the goal is to construct mathematical expression with the numbers to obtain 24. Performance on the Game-of-24 task has shown to benefit from tree search in previous works (Yao et al., 2024; Gandhi et al., 2024). Self-taught lookahead for math reasoning. For the Game-of-24 task, we generate data using breadth-first search (BFS) rather than MCTS to be consistent with the original Tree-of-Thoughts approach. We use gpt-4o policy and llama-3.1-8b-instruct base value function in the first iteration. As described in 3, we replace this base value function with trained model in each subsequent iteration. Self-taught lookahead is run for four iterations with 25 tasks used during each iteration. Note that in this case, we do not have explicit environment observations, but instead use the policys arithmetic while combining two numbers as pseudo-observation. See Appendix for further implementation details. Baselines. We compare the performance of value models learned via self-taught lookahead with BFS baselines that use the same gpt-4o policy but with varied value models. From the original setting proposed by Yao et al. (2024), 6 Figure 2. Breadth-first search performance on Game-of-24 task on sets of tasks both seen and unseen during the self-improvement. one baseline uses the same LLM as the policy (gpt-4o) as the value model. Inspired by Chen et al. (2024), we also include baseline with an algorithmic oracle evaluator. Specifically, the oracle runs simple recursive algorithm to verify whether the current state (set of numbers) can be combined to reach 24. Search performance with this oracle value gives us an upper bound on possible performance improvements due to improving the value function with static policy. Results and discussion. Figure 2 shows the performance of self-taught lookahead and the two baselines of two sets of 48 tasks. The first set includes tasks encountered during self-taught lookahead, comprising four sets of 12 tasks sampled from those observed in each iteration. The second set consists of more challenging tasks (determined by lower human solve percentages) that were not seen during self-taught lookahead. both evaluation BFS with On STL sets, llama-3.1-8b-instruct value model matches or outperforms BFS with gpt-4o value model. However, self-taught lookaheads performance on seen tasks monotonically increases for the first three iterations, while its performance on unseen tasks decreases before increasing in later iterations. This phenomenon is likely due to the base value model generalizing better than self-improved models in the earlier iterations which have seen too few examples and have not received signal from more accurate LLM values of terminal states during lookahead and fine-tuning. Language Models can Self-Improve at State-Value Estimation for Better Search Figure 3. Compute and environmental efficiency during evaluation on WebShop with gpt-3.5-turbo policy. Compute efficiency is measured in total (prompt and completion) tokens broken down by model type (closed and open source). Environmental efficiency is measured by the number of states expanded (webpages visited). 5. Efficiency Analysis In this section, we compare the efficiency of self-taught lookahead with prior methods on the WebShop task. We study efficiency tradeoffs from two perspectives (1) model usage and costs and (2) environment usage. Additionally, in 5.3 we explore how performance changes when scaling the size of the value model trained during self-taught lookahead. 5.1. Compute and Cost Efficiency Keeping compute requirements and costs low during inference time is critical, especially for agents automatSince self-taught lookaing routine, repetitive tasks. head can be used to improve an open-source LLM like llama-3.1-8b-instruct at value estimation, we can transfer computation from more resource-intensive and expensive closed-source models like gpt-4o to these open source models while maintaining performance. Figure 3 demonstrates this transfer, as self-taught lookahead uses more than 50% fewer closed source tokens as greedy search with gpt-35-turbo or gpt-4o value model. We also compute the monetary cost of task performance using OpenAIs5 and Groqs6 cost tables for closed-source models and llama-3.1-8b-instruct respectively. We plot these costs against the average WebShop reward and success rate in Figure 4 (top) and (bottom), respectively. We find that self-taught lookahead is Pareto optimal in both cases and is 37 cheaper than MCTS methods like LATS and 10 cheaper than performing greedy search with gpt-4o value model. 5.2. Environmental Usage Similar to sample efficiency during RL training, we propose that LLM agents should be environmentally efficient during inference. Specifically, in many cases, it may be crucial for an agent taking actions in physical or digital environ5openai.com/api/pricing 6groq.com/pricing - we only use Groq for pricing estimate, as inference and training was run on in-house GPUs 7 ments to be conservative in the number of states it visits while performing task. In the case of digital web agents, taking many steps per task, for instance through exhaustive tree search, may put an unnecessary burden on web servers, especially as agents are deployed at scale. Additionally, allowing web agents to search the environment widely when equipped with personal information or the ability to make purchases may lead to unintended privacy disclosures or financial loss, respectively. Depending on the environment, taking large number of actions may also directly lead to an unreasonable task completion time. Figure 3 and Figure 4 (middle) present the environmental efficiency measured by the count of expanded states or visited sites in the WebShop environment. Considering WebShop score, self-taught lookahead is Pareto optimal and requires expanding half as many states as MCTS-based methods like LATS. Furthermore, unlike LATS, self-taught lookahead does not require any irreversible actions (actually clicking BUY NOW on product page) required to obtain reward. 5.3. Self-Taught Lookahead Scaling Trends Given that 8 billion parameter STL models can match the performance of gpt-4o value model, is it possible to use even smaller models for STL while maintaining good performance? STL requires models to (1) provide generally consistent values out-of-the-box so that it is possible to compare successor states during data generation and (2) learn to generalize to unseen tasks and states, both of which may be challenging for smaller models. We explore this STL scaling trend on WebShop with 8 billion parameter models in the llama 3 family7 (Dubey et al., 2024) and 7 billion parameter models in the qwen-2.5-instruct family (Yang et al., 2024). The results presented in Figure 5 demonstrate that while performance does generally decrease with fewer parameters, smaller models like llama-3.2-3b-instruct and qwen-2.5-3b-instruct can match or even out7As there are no 1B and 3B models in the llama-3.1 and llama-3.2-1b-instruct use we family, llama-3.2-3b-instruct Language Models can Self-Improve at State-Value Estimation for Better Search Figure 5. STL scaling trends on WebShop for llama-3 and qwen-2.5 model families when using gpt-3.5-turbo policy with performance measured by average reward (top) and success rate (bottom). they are concatenated with action-outcome rationales, and together, these sequences are used to fine-tune the LLM value model, training from scratch at each iteration rather than from the previous model checkpoint as in FVI. LLM self-improvement. variety of previous work has shown that LLMs can self-improve with iterative prompting techniques (Huang et al., 2022; Weng et al., 2022; Madaan et al., 2024) and have applied these methods to various domains, from agents (Kim et al., 2024) to privacy protection (Chen et al., 2023b). separate line of work focuses on bootstrapping small training dataset through self-training process to improve either the reasoning policy model (Gulcehre et al., 2023; Singh et al., 2023; Jung et al., 2023) or the verification or reward model (Hosseini et al., 2024) using synthetically generated data. While most selftraining approaches utilize outcome-based reward models, other work (Aksitov et al., 2023; Zhang et al., 2024a) derive process-based rewards like STL to evaluate each step in the reasoning chain. Training reasoning agents. The majority of prior work on training reasoning agents focuses on performing SFT on human-annotated trajectories (Yao et al., 2022a; Lai et al., 2024), synthetically generated trajectories (Chen et al., 2023a; Furuta et al., 2023; Zhang et al., 2024c; Liu et al., 2024b; Patel et al., 2024; Murty et al., 2024; Ou et al., 2024; Su et al., 2025), or combination of both (Zeng Figure 4. Tradeoff between performance and efficiency on WebShop with gpt-3.5-turbo policy. Pareto frontiers of existing methods and baselines are shown, illustrating the optimality of STL when considering the tradeoff between cost and average reward (top), environmental usage and average reward (middle), and cost and success rate (bottom). perform the performance of gpt-4o in terms of average reward, respectively. This result indicates that these smaller models could be reasonably used for STL, increasing the feasibility of large-scale agent deployment to new domains. 6. Related Work Classical reinforcement learning. STL is loosely inspired by the traditional fitted value iteration (FVI) (Gordon, 1999), which generalized value iteration (Bellman, 1957) beyond the tabular setting. In an iteration of FVI, target values are computed using the Bellman update and used to train new value model from the previous model checkpoint using least squares regression. The iterated values in FVI are computed similarly to the lookahead values ysk in 3.1, but with STL, no ground truth reward is assumed, the value model is non-Markovian, and actions are deterministic. Instead of learning directly from iterated values, with STL, 8 Language Models can Self-Improve at State-Value Estimation for Better Search et al., 2023; Zhang et al., 2024b). Other work has trained agents from tree search generated data, but these methods usually require ground truth reward (Gandhi et al., 2024; Putta et al., 2024; Zhang et al., 2024a). While prior work has explored self-improving reasoning agents, these approaches fail to generalize beyond the instructions encountered during self-improvement (Patel et al., 2024) or require fine-tuning frontier models like gpt-4o to achieve generalization (Yu et al., 2024). 7. Conclusion We propose self-taught lookahead (STL) as an efficient method to improve the value model employed during search. This efficiency primarily takes the form of reward and human demonstration-free improvement, where models instead learn from state-transition dynamics. However, we also show that because STL allows us to perform selfimprovement on small models that can be deployed with less exhaustive search algorithms, search with STL models yields significant reductions in both cost and environmental usage. Therefore, the STL framework could help enable the more realistic training and deployment of agent systems."
        },
        {
            "title": "Impact Statement",
            "content": "Our STL method enables LLMs to self-improve at search by capturing the mechanics of traditional RL algorithms in natural language. While we leverage this technique for better state-value estimation, it can likely apply to other settings. Additionally, we show in 5, that STL requires significantly less compute overhead than similarly performing methods, thus reducing the energy consumption required for search during inference, helping to improve the sustainability of agent deployment. Finally, having models self-improve without human supervision may enable agents to take unintended and potentially harmful actions. While investigating these harms is out of the scope of this work, we encourage future research in this area."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Microsofts Azure Accelerate Foundation Models Research Program and NVIDIAs Academic Grant Program for providing computational resources to support this work. This research is supported in part by the NSF under grant number IIS-2052498 and SMA-2418946. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation."
        },
        {
            "title": "References",
            "content": "Aksitov, R., Miryoosefi, S., Li, Z., Li, D., Babayan, S., Kopparapu, K., Fisher, Z., Guo, R., Prakash, S., Srinivasan, P., et al. Rest meets react: Self-improvement for multi-step reasoning llm agent. arXiv preprint arXiv:2312.10003, 2023. Bellman, R. Dynamic Programming. Princeton University Press, 1957. Berg-Kirkpatrick, T., Burkett, D., and Klein, D. An empirical investigation of statistical significance in nlp. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning, pp. 9951005, 2012. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Chen, B., Shu, C., Shareghi, E., Collier, N., Narasimhan, K., and Yao, S. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915, 2023a. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Chen, Y., Mendes, E., Das, S., Xu, W., and Ritter, A. Can language models be instructed to protect personal information? arXiv preprint arXiv:2310.02224, 2023b. Chen, Z., White, M., Mooney, R., Payani, A., Su, Y., and Sun, H. When is tree search useful for llm planning? it depends on the discriminator. arXiv preprint arXiv:2402.10890, 2024. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 41714186, 2019. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Feng, X., Wan, Z., Wen, M., McAleer, S. M., Wen, Y., Zhang, W., and Wang, J. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179, 2023. 9 Language Models can Self-Improve at State-Value Estimation for Better Search Furuta, H., Lee, K.-H., Nachum, O., Matsuo, Y., Faust, A., Gu, S. S., and Gur, I. Multimodal web navigation with instruction-finetuned foundation models. arXiv preprint arXiv:2305.11854, 2023. Gandhi, K., Lee, D., Grand, G., Liu, M., Cheng, W., Sharma, A., and Goodman, N. D. Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683, 2024. Gordon, G. J. Approximate solutions to Markov decision processes. Carnegie Mellon University, 1999. Gulcehre, C., Paine, T. L., Srinivasan, S., Konyushkova, K., Weerts, L., Sharma, A., Siddhant, A., Ahern, A., Wang, M., Gu, C., et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hosseini, A., Yuan, X., Malkin, N., Courville, A., Sordoni, A., and Agarwal, R. V-star: Training verifiers for selftaught reasoners. arXiv preprint arXiv:2402.06457, 2024. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., and Han, J. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022. Jung, J., West, P., Jiang, L., Brahman, F., Lu, X., Fisher, J., Sorensen, T., and Choi, Y. Impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing. arXiv preprint arXiv:2305.16635, 2023. Kim, G., Baldi, P., and McAleer, S. Language models can solve computer tasks. Advances in Neural Information Processing Systems, 36, 2024. Kocsis, L. and Szepesvari, C. Bandit based monte-carlo planning. In European conference on machine learning, pp. 282293. Springer, 2006. Koh, J. Y., Lo, R., Jang, L., Duvvur, V., Lim, M. C., Huang, P.-Y., Neubig, G., Zhou, S., Salakhutdinov, R., and Fried, D. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024a. Koh, J. Y., McAleer, S., Fried, D., and Salakhutdinov, R. Tree search for language model agents. arXiv preprint arXiv:2407.01476, 2024b. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving In Proceedings of the 29th Symwith pagedattention. posium on Operating Systems Principles, pp. 611626, 2023. Lai, H., Liu, X., Iong, I. L., Yao, S., Chen, Y., Shen, P., Yu, H., Zhang, H., Zhang, X., Dong, Y., et al. Autowebglm: Bootstrap and reinforce large language model-based web navigating agent. arXiv preprint arXiv:2404.03648, 2024. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. Liu, J., Cohen, A., Pasunuru, R., Choi, Y., Hajishirzi, H., and Celikyilmaz, A. Dont throw away your value model! generating more preferable text with value-guided montecarlo tree search decoding. In First Conference on Language Modeling, 2024a. Liu, Z., Hoang, T., Zhang, J., Zhu, M., Lan, T., Kokane, S., Tan, J., Yao, W., Liu, Z., Feng, Y., et al. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. arXiv preprint arXiv:2406.18518, 2024b. Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with selffeedback. Advances in Neural Information Processing Systems, 36, 2024. Murty, S., Bahdanau, D., and Manning, C. D. Nnetscape navigator: Complex demonstrations for web agents without demonstrator. arXiv preprint arXiv:2410.02907, 2024. Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. OpenAI. Hello GPT-4o. https://openai.com/ index/hello-gpt-4o/, 2024. Ou, T., Xu, F. F., Madaan, A., Liu, J., Lo, R., Sridhar, A., Sengupta, S., Roth, D., Neubig, G., and Zhou, S. Synatra: Turning indirect knowledge into direct demonstrations for 10 Language Models can Self-Improve at State-Value Estimation for Better Search digital agents at scale. arXiv preprint arXiv:2409.15637, 2024. Patel, A., Hofmarcher, M., Leoveanu-Condrei, C., Dinu, M.-C., Callison-Burch, C., and Hochreiter, S. Large language models can self-improve at web agent tasks. arXiv preprint arXiv:2405.20309, 2024. Putta, P., Mills, E., Garg, N., Motwani, S., Finn, C., Garg, D., and Rafailov, R. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199, 2024. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36: 5372853741, 2023. Xie, Y., Kawaguchi, K., Zhao, Y., Zhao, J. X., Kan, M.- Y., He, J., and Xie, M. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36, 2024. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yao, S., Chen, H., Yang, J., and Narasimhan, K. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757, 2022a. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022b. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et al. Mastering chess and shogi by self-play with general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. Singh, A., Co-Reyes, J. D., Agarwal, R., Anand, A., Patil, P., Liu, P. J., Harrison, J., Lee, J., Xu, K., Parisi, A., et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023. Su, H., Sun, R., Yoon, J., Yin, P., Yu, T., and Arık, S. O. Learn-by-interact: data-centric framework for selfadaptive agents in realistic environments. arXiv preprint arXiv:2501.10893, 2025. Yu, X., Peng, B., Vajipey, V., Cheng, H., Galley, M., Gao, J., and Yu, Z. Exact: Teaching ai agents to explore with reflective-mcts and exploratory learning. arXiv preprint arXiv:2410.02052, 2024. Zelikman, E., Wu, Y., Mu, J., and Goodman, N. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Zeng, A., Liu, M., Lu, R., Wang, B., Liu, X., Dong, Y., and Tang, J. Agenttuning: Enabling generalized agent abilities for llms. arXiv preprint arXiv:2310.12823, 2023. Zhang, D., Zhoubian, S., Yue, Y., Dong, Y., and Tang, J. Rest-mcts*: Llm self-training via process reward guided tree search. arXiv preprint arXiv:2406.03816, 2024a. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Zhang, J., Lan, T., Murthy, R., Liu, Z., Yao, W., Tan, J., Hoang, T., Yang, L., Feng, Y., Liu, Z., et al. Agentohana: Design unified data and training pipeline for effective agent learning. arXiv preprint arXiv:2402.15506, 2024b. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Weng, Y., Zhu, M., Xia, F., Li, B., He, S., Liu, S., Sun, B., Liu, K., and Zhao, J. Large language models are better reasoners with self-verification. arXiv preprint arXiv:2212.09561, 2022. Zhang, J., Lan, T., Zhu, M., Liu, Z., Hoang, T., Kokane, S., Yao, W., Tan, J., Prabhakar, A., Chen, H., et al. xlam: family of large action models to empower ai agent systems. arXiv preprint arXiv:2409.03215, 2024c. Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., and Wang, Y.-X. Language agent tree search unifies reasoning, acting, and planning in language models. In Proceedings of the 41st International Conference on Machine Learning. PMLR, 2024. Language Models can Self-Improve at State-Value Estimation for Better Search Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Ou, T., Bisk, Y., Fried, D., et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. 12 Language Models can Self-Improve at State-Value Estimation for Better Search Algorithm 1 Self-Taught Lookahead Require: Set of tasks Drollout, Base LLM , num iterations n, num tasks per iteration πθ initialize policy model(M ) Vϕ0 initialize value model(M ) DTrain0 {} for 1 to do Drolloutk Drollout[m (k 1) : k] {Select tasks for iteration k} for Drolloutk do rollout tree(πθ, Vϕk1 , x) {Generate rollout search tree for task x} calculate lookahead values(T ) calculate action outcomes rationales(T ) yfiltered, ofiltered task specific filter(y, o) {If applicable, apply task-specific filtering} DTraink add new data(DTraink , (yfiltered, ofiltered)) end for Vϕk fine tune(Vϕ0, DTraink ) {Finetune from base model} end for A. Self-Taught Lookahead Algorithm The STL algorithm is presented in full in Algorithm 1. For information about the task specific filter, see Appendix B.3 and Appendix C.3. B. Self-Taught Lookahead on WebShop In this section, we outline information about WebShop (Yao et al., 2022a) and the implementation details of running STL on the benchmark. B.1. WebShop Task There are two main types of actions in the WebShop task: search[query]: Search actions allow the user to search for particular item with natural language query e.g. search[easy to use medium color face kit less than 40 dollars]. This action can only be taken on the search page which is also the initial / home page of the WebShop interface. click[button]: Click actions are discrete actions but can take many forms which we enumerate below: click[product]: to select relevant product from the search results e.g. click[B09B6SH764] where B09B6SH764 is product code. click[attribute]: to toggle on an attribute or option on the product page of an item e.g. click[small] click[Buy Now]: to buy the selected item - this is terminal action that yields the ground truth reward. This action is not allowed to be taken in STL search but is allowed in other search, RL, and prompting methods (Yao et al., 2022a; Shinn et al., 2024; Zhou et al., 2024). Other navigation buttons: other navigation buttons include click[Back to Search], click[<Prev], click[Next>], click[Description], click[Features]. To simplify trajectories, we generally restrict the ability for models to take these actions in all settings following (Yao et al., 2022a). WebShop provides textual representation of webpages in simple mode. An example of this representation for search results are shown in Figure 6. B.2. Prompts The prompt used to generate actions in WebShop is presented in Figure 7. Notice that we do not use think actions part of the classical ReACT framework (Yao et al., 2022b) like Yao et al. (2022a) or Zhou et al. (2024) because evaluating the value 13 Language Models can Self-Improve at State-Value Estimation for Better Search WebShop Textual Representation [Back to Search] Page 1 (Total results: 50) [Next >] [B0972Q1T8T] Cosycost USB Microphone, Condenser Computer PC Gaming Microphone for PS4/5 Laptop Windows Mac OS Android Phone, Noise Cancelling Instant Mute, Studio Mic for Voice, Music Recording, Podcasting, Streaming $32.99 [B09N3M6H2Z] Wired Stereo Headset Noise Cancelling Microphone with in-line Controls/Volume Controller, All-Day Comfort Design, Works for Playstation, Nintendo Switch, PC with USB Connection (HS-HP101UNCBK) $199.99 [B072L2D6LY] Andrea Communications NC-255VM USB On-Ear Stereo USB Computer Headset with Noise-Canceling Microphone, in-Line Volume/Mute Controls, and Plug $34.59 [B071H84LTJ] Andrea Communications NC-455VM USB Over-Ear Circumaural Stereo USB Computer Headset with Noise-Canceling Microphone, in-Line Volume/Mute Controls, and Plug $49.24 [B08GLJSWJ9] Jiade USB Headset with Noise Canceling Microphone for CallCenter Skype Chat, Computer Phone Headset Voice Recognition Speech Dictation, PC Headphone with Mic Mute Volume Control Binaural Golden $9. Figure 6. Example of simple mode textual representation of the state with the WebShop benchmark. of these actions is difficult as they have no observation. Instead, we prompt the policy model to provide rationale while generating possible actions. Also, note that we prompt the policy multiple times adding to the list of actions that are not allowed and removing from the list of actions that are allowed. This change to selection policy enables action diversity which we find is otherwise low even with prompting the policy at high temperature. This change also likely explains why there is no statistically significant difference between using gpt-3.5-turbo and gpt-4o policy in 4.1. Likewise, the prompt used to evaluate states is presented in Figure 8. Note that this evaluation prompt is only used to prompt base models, STL value models are only prompted with the current trajectory. We note that the Likert scale used was crucial to obtaining consistent value outputs on which we could perform STL. For all value estimates (base model or fine-tuned), we prompt the value model 5 times and use the average score as the state value estimate. During the data generation phase, since we need single rationale to fine-tune on which to construct the action-outcome rationale, we choose the rationale corresponding to the median of the 5 scores. B.3. Implementing STL Unlike with math reasoning tasks, with web tasks, the position of an action in trajectory may influence its value. For instance, selecting certain item from search results early in the trajectory should have higher value than selecting second time in the same trajectory. To account for this difference, we train value models at each position (depth) in the trajectory. Specifically, we limit trajectories to five steps and train four values models depths 1 to 4 only allowing terminating BUY action on the final step. We also filter out malformed rationales from the training data. Specifically, we remove rationales that do not provide the proper format e.g. it does not exactly contain scaffolding like Thus the correctness score is. In total, including scaffolding, the generated self-improvement dataset consists of 1210448 tokens. 14 Language Models can Self-Improve at State-Value Estimation for Better Search WebShop Generation Prompt You are web agent, select the best next action for the search to fulfill the task. Example tasks are shown below. Provide rationale for your selection BEFORE you provide the action. NOTE: You can only select actions that are provided in the Possible Actions list. You MAY NOT select actions in the Not Allowed list. NOTE: You must output BOTH rationale and an action. NOTE: Do not select any of the following actions: Back to Search, Next >, < Prev, Attributes, Description, Features, Reviews, even if they are available on the page. Example Tasks: {few shot examples} New Task: {task} Actions Not Allowed: {not allowed actions} Possible Next Actions (REMINDER: You can only select actions from this list.): {possible actions} REMINDER: Do not select any of the following actions: Back to Search, Next >, < Prev, Attributes, Description, Features, Reviews, even if they are available on the page. Figure 7. Generation prompt for WebShop policy. B.4. Difficulty in Performing STL for multiple iterations. Empirically, we find that STL after second iteration on WebShop has lower performance (average reward of 68.6, and success rate of 26.0) than after single iteration. From manual inspection of the lookahead and rationales generated, we notice that the second step simulated by the value model often does not match the true environment. C. Math Tasks C.1. Game-of-24 Task As introduced by Yao et al. (2024), the Game-of-24 is mathematical reasoning task that involves combining four numbers e.g. 2 3 4 5 together with mathematical operations i.e. +, , /, in order to obtain 24. An action in this task consists of simply applying mathematical operation to combine two numbers e.g. 2 + 3 = 5, the resulting state from the operation is the set of remaining numbers e.g 5 4 5. C.2. Prompts The prompt used to generate actions for the Game-of-24 task is presented in Figure 9. Likewise, the prompt used to evaluate states is presented in Figure 10. Like WebShop, this evaluation prompt is only used to prompt base models, STL value models are only prompted with the current trajectory. However, unlike WebShop, values are not real numbers between 1 and 10, but rather 0.001, 1, and 20 corresponding to the labels of impossible, likely, sure that the remaining numbers can be combined to reach 24. Note these values were used by the Yao et al. (2024) in the original Tree-of-Thoughts paper, but are ad-hoc and are used purely as labels. For all value estimates (base model or fine-tuned), we prompt the value model 3 times and use the median score as the state value estimate. During the data generation phase, since we need single rationale to fine-tune on which to construct the action-outcome rationale, we choose the rationale corresponding to the median of the 3 scores. C.3. Implementing STL Since the state space is quite limited, we combine training examples from the previous 1 iterations with current examples to train the value model in the kth iteration. If the same state is encountered multiple times in different iterations, we defer to the value judgment from the latest iteration. Language Models can Self-Improve at State-Value Estimation for Better Search WebShop Evaluation Prompt Given an item to purchase and trajectory that aims to buy an item that exactly matches the specification, analyze how well the last action and observation align with the task. Provide reflection that concludes with. Thus the correctness score is s, where is either 1, 2, 4, 6, 8, or 10. Use the following scale for scoring: 1: The last action and observed state is entirely irrelevant to the task or captures purchase of an item that is completely unrelated to the specifications. 2: The last action and observed state captures step with low likelihood of leading to purchasing the correct item. 4: The last action and observed state captures step with moderate likelihood of leading to purchasing the correct item. 6: The last action and observed state captures step with high likelihood of leading to purchasing the correct item. 8: The last action and observed state captures step with very high likelihood of leading to purchasing the correct item. 10: The last action and observed state captures step that will certainly lead to purchasing the correct item. Keep reflections short (<100 words). Follow the format of the rationale from the below example task. NOTE: the observation from clicking on the item will be the items product detail page. For instance, click[B078GWRC1J] will show the product detail page for the item with code B078GWRC1J which will include the items name (e.g. Bright Citrus Deodorant by Earth Mama), price ($10.99), and other relevant details as well as options. NOTE: Assume none of the attributes on the product page are selected only provide the reflection for the last action. Example Tasks: {few shot examples} New Task: Respond with the reflection for the last observation of the new task ONLY. As remind,er the last action and observation is as follows: {last action} Your response should start with Reflection: and end with Thus the correctness score is .... Figure 8. Evaluation prompt for WebShop. This prompt was only used to prompt base models. In total, including scaffolding, the generated self-improvement dataset consists of 28708 tokens. D. Model Fine-tuning and Serving warmup-steps learning-rate weight-decay per-device-batch size lora-r lora-alpha STL 10 2e4 0. 8 16 16 Table 4. Hyperparameters during STL training. Fine-tuning the value model for STL is carried out on single NVIDIA A40 GPU. We use LoRA finetuning (Hu et al., 2021) and use models provided by unsloth8. The hyperparameters used are in Table 4. We fine-tuned Game-of-24 value models for 10 epochs and WebShop value models for 20 epochs due to the differences in difficulty for models to learn the format of the action and state representations. 8unsloth.ai 16 Language Models can Self-Improve at State-Value Estimation for Better Search Game-of-24 Generation Prompt Use numbers and basic arithmetic operations (+ - * /) to obtain 2In eachach step, you are only allowed to choose two of the remaining numbers to obtain new number. Follow the example format exactly. {few shot examples} {input} Figure 9. Generation prompt for the Game-of-24 policy. Game-of-24 Evaluation Prompt Evaluate if given numbers can reach 24 (sure/likely/impossible) Follow the example format exactly. Only evaluate the last example. few shot examples {input} Figure 10. Evaluation prompt for Game-of-24. This prompt was only used to prompt base models. Additionally, we serve base and fine-tuned models using vLLM 9 (Kwon et al., 2023) for efficient value estimation of new states during search. E. Significance Testing In 4.1, we use the paired bootstrap test to test the statistical significance of our experimental results. Following BergKirkpatrick et al. (2012), we set = 106. We also run the significance test twice: once for score (average reward) and separate time for success rate. 9docs.vllm.ai/en/latest/"
        }
    ],
    "affiliations": [
        "Georgia Institute of Technology"
    ]
}