{
    "paper_title": "Balanced Multi-Task Attention for Satellite Image Classification: A Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training",
    "authors": [
        "Aditya Vir"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work presents a systematic investigation of custom convolutional neural network architectures for satellite land use classification, achieving 97.23% test accuracy on the EuroSAT dataset without reliance on pre-trained models. Through three progressive architectural iterations (baseline: 94.30%, CBAM-enhanced: 95.98%, and balanced multi-task attention: 97.23%) we identify and address specific failure modes in satellite imagery classification. Our principal contribution is a novel balanced multi-task attention mechanism that combines Coordinate Attention for spatial feature extraction with Squeeze-Excitation blocks for spectral feature extraction, unified through a learnable fusion parameter. Experimental results demonstrate that this learnable parameter autonomously converges to alpha approximately 0.57, indicating near-equal importance of spatial and spectral modalities for satellite imagery. We employ progressive DropBlock regularization (5-20% by network depth) and class-balanced loss weighting to address overfitting and confusion pattern imbalance. The final 12-layer architecture achieves Cohen's Kappa of 0.9692 with all classes exceeding 94.46% accuracy, demonstrating confidence calibration with a 24.25% gap between correct and incorrect predictions. Our approach achieves performance within 1.34% of fine-tuned ResNet-50 (98.57%) while requiring no external data, validating the efficacy of systematic architectural design for domain-specific applications. Complete code, trained models, and evaluation scripts are publicly available."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 7 2 5 5 1 . 0 1 5 2 : r Balanced Multi-Task Attention for Satellite Image Classification: Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training Aditya Vir Department of Computer Science and Engineering Manipal University Jaipur Jaipur, Rajasthan 303007, India aditya23vir@gmail.com October 20,"
        },
        {
            "title": "Abstract",
            "content": "This work presents systematic investigation of custom convolutional neural network architectures for satellite land use classification, achieving 97.23% test accuracy on the EuroSAT dataset without reliance on pre-trained models. Through three progressive architectural iterationsbaseline (94.30%), CBAM-enhanced (95.98%), and balanced multi-task attention (97.23%)we identify and address specific failure modes in satellite imagery classification. Our principal contribution is novel balanced multi-task attention mechanism that combines Coordinate Attention for spatial feature extraction with Squeeze-Excitation blocks for spectral feature extraction, unified through learnable fusion parameter. Experimental results demonstrate that this learnable parameter autonomously converges to α 0.57, indicating near-equal importance of spatial and spectral modalities for satellite imagery. We employ progressive DropBlock regularization (5-20% by network depth) and class-balanced loss weighting to address overfitting and confusion pattern imbalance. The final 12-layer architecture achieves Cohens Kappa of 0.9692 with all classes exceeding 94.46% accuracy, demonstrating confidence calibration with 24.25% gap between correct and incorrect predictions. Our approach achieves performance within 1.34% of fine-tuned ResNet-50 (98.57%) while requiring no external data, validating the efficacy of systematic architectural design for domain-specific applications. Complete code, trained models, and evaluation scripts are publicly available."
        },
        {
            "title": "1 Introduction",
            "content": "Satellite image classification constitutes fundamental task in remote sensing with applications spanning agricultural monitoring, urban planning, environmental assessment, and disaster response [9]. The availability of highresolution multispectral imagery from satellites such as Sentinel-2 has created opportunities for automated land use and land cover (LULC) mapping at unprecedented scales [10]. However, the relative scarcity of large-scale labeled datasets in this domain has led to widespread adoption of transfer learning from ImageNet [11], which may not optimally capture the unique spectral and spatial characteristics of satellite imagery. The EuroSAT dataset [1], comprising 27,000 Sentinel2 RGB images (6464 pixels) across 10 LULC classes, has emerged as standard benchmark for satellite classification evaluation. Current state-of-the-art approaches achieve 98-99% accuracy through fine-tuning pre-trained ResNets, EfficientNets, or Vision Transformers [5]. While effective, these methods obscure fundamental question: what level of performance can be achieved through careful architectural design alone, without leveraging external datasets?"
        },
        {
            "title": "1.1 Motivation",
            "content": "Training CNNs from scratch on domain-specific data offers several advantages: (1) elimination of dependency on proxy tasks such as ImageNet classification, (2) full interpretability of learned features specific to the target domain, (3) applicability to scenarios where pre-training is unavailable (proprietary sensors, confidential datasets, novel modalities), and (4) deeper insights into fundamental architectural requirements for satellite imagery analysis. Despite these benefits, systematic studies on fromscratch training for EuroSAT remain limited, with most work treating it as baseline comparison rather than primary investigation."
        },
        {
            "title": "1.2 Research Questions",
            "content": "This work is guided by three research questions: RQ1: What is the achievable performance ceiling for custom CNNs trained entirely from scratch on EuroSAT, and how does this compare to pre-trained model performance? RQ2: What specific failure modes emerge during from-scratch training, and how can targeted architectural innovations systematically address them? RQ3: Does satellite imagery require balanced attention to orthogonal feature modalities (spatial versus spectral), and can model autonomously learn this balance?"
        },
        {
            "title": "1.3 Contributions",
            "content": "This work makes five principal contributions: Systematic architectural evolution: We document three-stage progression demonstrating iterative problem identification and solution design: baseline (94.30%), attention-enhanced (95.98%), and balanced multi-task attention (97.23%). Novel attention mechanism: We introduce balanced multi-task attention combining Coordinate Attention (spatial features) and Squeeze-Excitation blocks (spectral features) with learnable fusion parameter α that converges to 0.57, demonstrating near-equal importance of both modalities. Trade-off analysis: We empirically demonstrate that single-modality attention (CBAM) resolves targeted confusion patterns (River-Highway) but introduces new failures (vegetation classification), necessitating balanced dual-path design. Comprehensive evaluation: We provide extensive analysis including per-class metrics, confusion pattern evolution and confidence calibration (24.25% gap) quantifying individual component contributions. Reproducibility: We release complete implementations, trained models, hyperparameters, and evaluation scripts enabling exact replication and extension. SeaLake. The original work established ResNet-50 finetuned on ImageNet as achieving 98.57% accuracy. Subsequent research has explored EfficientNets [6] (98.1%), Vision Transformers [5] (99.19%), and ensemble methods (99.41%). However, these approaches uniformly rely on ImageNet pre-training. Other satellite classification datasets include UC Merced Land Use [12] and AID [13], though these focus on aerial RGB imagery with different scale and spectral characteristics compared to Sentinel-2 multispectral data."
        },
        {
            "title": "2.2 Attention Mechanisms in CNNs",
            "content": "Attention mechanisms have become integral to modern computer vision architectures. Squeeze-and-Excitation Networks (SENet) [4] introduced channel-wise attention through global pooling and FC bottleneck layers. CBAM [2] extended this with sequential channel and spatial attention using aggregated pooling operations. Coordinate Attention [3] proposed factorized spatial attention encoding height and width information separately, demonstrating effectiveness for mobile architectures. Our work differs by combining SE and Coordinate Attention through learnable fusion rather than fixed sequential or parallel integration, allowing the model to discover optimal balance between spectral and spatial features."
        },
        {
            "title": "Learning",
            "content": "He et al. [8] challenged conventional wisdom by demonstrating that training from scratch with appropriate normalization can match ImageNet pre-training for object detection tasks. Our work provides empirical evidence that systematic architectural design can achieve 97.23% on EuroSAT from scratchwithin 1.34% of fine-tuned ResNet50demonstrating practical viability for satellite classification."
        },
        {
            "title": "2.1 Satellite Image Classification Bench-",
            "content": "marks Dataset. We utilize the EuroSAT RGB subset containing 27,000 images (6464 pixels, 3 channels) across 10 classes. Data is split 70/15/15 for training/validation/test (18,900/4,050/4,050 samples) with random seed 42 for reproducibility. The EuroSAT dataset [1] consists of 27,000 Sentinel2 RGB images labeled across 10 LULC classes: AnnualCrop, Forest, HerbaceousVegetation, Highway, Industrial, Pasture, PermanentCrop, Residential, River, and Data Augmentation. Training augmentation includes: random rotation by 90 increments (exploiting rotational invariance of satellite imagery), random horizontal/vertical flips, color jittering (brightness/contrast/saturation"
        },
        {
            "title": "3.3 CBAM-Enhanced Architecture (Model",
            "content": "2) 3.3.1 Design Motivated by River-Highway confusion, we implement 7-layer architecture with channel progression [32, 64, 128, 256, 512, 512, 512]. Blocks 2-7 incorporate CBAM (Convolutional Block Attention Module) [2]. CBAM applies sequential channel and spatial attention: Channel Attention: Fc = σ(MLP(AvgPool(X))+MLP(MaxPool(X))) (1) Spatial Attention: Fs = σ(Conv77([AvgPoolc(X); MaxPoolc(X)])) (2) where σ denotes sigmoid activation, MLP employs 16:1 reduction ratio, and [; ] represents concatenation. Total parameters: 7.4M. 3.3.2 Training Configuration Figure 1: Representative samples from the EuroSAT RGB dataset showing all 10 land use and land cover classes. Each class contains approximately 2,700 images at 6464 pixel resolution from Sentinel-2 satellite imagery. 30%), Gaussian blur (kernel=3, σ=0.1-2.0), and random erasing (p=0.3). All images are normalized using ImageNet statistics. Training Infrastructure. PyTorch 2.0, NVIDIA Tesla T4 GPU, mixed precision (FP16) training, batch size 64."
        },
        {
            "title": "3.2 Baseline Architecture (Model 1)",
            "content": "Adam optimizer (lr=1e-3), CosineAnnealingLR scheduler (Tmax=40), Dropout(0.4), 40 epochs. 3.2.1 Design The baseline consists of three convolutional blocks with channel progression [32, 64, 128], each comprising Conv33, BatchNorm, ReLU, and MaxPool22. Global average pooling is followed by FC(512), Dropout(0.5), and FC(10). Total parameters: 2.1M. 3.3.3 Results and Trade-off Discovery Test accuracy: 95.98% (+1.68%). River accuracy improves from 88.0% to 95.5% (+7.5%), Highway from 93.1% to 95.4% (+2.3%), with River-Highway confusions reduced by 70% (278). However, unintended consequences emerge: 3.2.2 Training Configuration HerbaceousVegetation accuracy decreases from Adam optimizer (lr=1e-3), ReduceLROnPlateau scheduler (patience=3, factor=0.5), 30 epochs (1.5 hours). 93.0% to 92.6% (-0.4%) HerbaceousVeg PermanentCrop confusions increase 50% (1218) 3.2.3 Results and Analysis Industrial Residential confusions double (510) Test accuracy: 94.30%. Per-class analysis reveals strong performance on Forest (99.1%) and SeaLake (99.0%), but significant weakness on River (88.0%) and Highway (93.1%). River Highway confusion accounts for 27 misclassifications, representing approximately 12% of total errors. Root Cause Analysis: At 6464 resolution, rivers and highways exhibit similar visual characteristics: gray, linear structures with low contrast and elongated morphology. The baselines limited receptive field (88 at deepest layer) and lack of attention mechanisms prevent discrimination of subtle contextual cues distinguishing these classes. Analysis: CBAMs spatial attention (77 convolution on spatially aggregated features) excels at capturing directional patterns critical for infrastructure classification (rivers, highways). However, this creates feature bias away from spectral and textural information essential for vegetation type discrimination. Different crop types differ primarily in spectral signatures (color, texture) rather than shape, making them vulnerable to spatial attention bias. This empirical observation reveals that satellite imagery exhibits orthogonal feature requirements: infrastructure classes require spatial attention, while land cover classes require spectral attention. Optimizing for one modality degrades performance on the other."
        },
        {
            "title": "3.4 Balanced Multi-Task Attention Archi-",
            "content": "3.4.4 Regularization Strategy tecture (Model 3) 3.4.1 Core Innovation To address the spatial-spectral trade-off, we design balanced multi-task attention mechanism combining two parallel paths: Path 1 (Spatial): Coordinate Attention [3] for directional/linear features Path 2 (Spectral): Squeeze-Excitation blocks [4] for color/texture features Learnable Fusion: Rather than fixed weighting, we introduce learnable parameter α enabling the model to discover optimal balance. 3.4.2 Architecture ResNet-style structure with 4 stages: [3,3,3,2] residual blocks, channel progression [64, 128, 256, 512]. Each residual block incorporates our balanced multi-task attention. Total parameters: 11.2M. 3.4.3 Balanced Multi-Task Attention Mechanism Progressive DropBlock: We implement DropBlock [7] with rates [0.05, 0.10, 0.15, 0.20] across stages 1-4, increasing with network depth. Block size: 77. This spatial dropout removes contiguous regions, forcing robustness to local feature co-adaptationparticularly critical for satellite imagery with variable atmospheric conditions and viewing angles. Class-Balanced Loss: Analysis of confusion patterns from Model 2 informs custom loss weights: 1.3 for frequently confused classes (Herbaceous, PermanentCrop, Industrial), 1.0 for moderate difficulty classes, and 0.8 for classes with consistently high accuracy (Forest, SeaLake, Residential). This prevents over-optimization of easy classes at the expense of challenging ones. 3.4.5 Training Configuration (lr=1e-3, weight decay=0.05), AdamW optimizer CosineAnnealingWarmRestarts (T0=15, Tmult=2), mixed precision (FP16), early stopping (patience=15). Training terminated at epoch 30 with best checkpoint at epoch 15 (97.09% validation). scheduler Coordinate Attention (Spatial Path): Factorizes 2D global pooling into separate height and width encodings:"
        },
        {
            "title": "4 Results",
            "content": "zc = zc = 1 1 1 (cid:88) xc(h, j) j=0 H1 (cid:88) i=0 xc(i, w)"
        },
        {
            "title": "4.1 Overall Performance",
            "content": "Table 1 presents comparative results across all three models. (3) (4) Following transformation: CoordAttn(X) = σ(fh(zh)) σ(fw(zw)) (5) Reduction ratio: 8:1. This design captures directional patterns (rivers flow along cardinal directions; highways follow gridded layouts) more effectively than isotropic convolutions. Squeeze-Excitation Block (Spectral Path): SE(X) = σ(FC2(ReLU(FC1(GAP(X))))) (6) Reduction ratio: 16:1. Channel-wise gating emphasizes spectral variations distinguishing vegetation types, water bodies, and soil characteristics. Learnable Fusion: Table 1: Model Comparison on EuroSAT RGB Test Set Method Pre-training? Test Acc Our Balanced (12-Layer) ResNet-50 (Helber et al.) [1] No Yes 97.23% 98.57% The balanced architecture achieves 97.23%, within 1.34% of fine-tuned ResNet-50 while using no pretraining."
        },
        {
            "title": "4.2 Per-Class Metrics",
            "content": "Table 2 details per-class performance for the final model. Cohens 0.9692. Matthews Correlation Coefficient: All classes achieve 94.46% accuracy. Kappa: 0.9692. BalancedAttn(X) = σ(α)CoordAttn(X)+(1σ(α))SE(X) (7) where α is learnable scalar parameter per block. Through gradient descent, the model autonomously discovers optimal spatial-spectral balance."
        },
        {
            "title": "4.3 Confusion Pattern Analysis",
            "content": "Evolution of critical confusion patterns: River Highway: 4 Figure 2: Evolution of confusion patterns across three architectural iterations. (a) Baseline model exhibits significant River-Highway confusion (27 total misclassifications). (b) CBAM-enhanced model resolves River-Highway confusion (8 errors) but introduces vegetation classification trade-offs. (c) Balanced multi-task attention model simultaneously addresses all confusion patterns, achieving 5 River-Highway errors with recovered vegetation classification accuracy. Table 2: Per-Class Metrics (Balanced 12-Layer Model) Class Acc Prec Rec F1 Forest Industrial SeaLake HerbaceousVeg Residential Pasture Highway River AnnualCrop PermanentCrop 98.64 98.68 98.28 98.25 97.78 97.66 96.68 96.27 95.55 94. 99.09 98.68 98.52 93.36 99.32 95.11 97.67 98.37 96.78 95.47 98.64 98.68 98.28 98.25 97.78 97.66 96.68 96.27 95.55 94.46 98.87 98.68 98.40 95.74 98.54 96.37 97.17 97.30 96.16 94.96 Macro Avg 97.23 97. 97.23 97.12 Baseline: Major confusion (River 88.0%) CBAM: Significant improvement (River 95.5%) Balanced: Further refinement (River 96.27%) Top Misclassifications (Balanced Model): 1. PermanentCrop HerbaceousVeg: 14 errors"
        },
        {
            "title": "4.4 Confidence Calibration",
            "content": "Prediction confidence analysis reveals: Correct predictions: 90.14% average confidence Incorrect predictions: 65.89% average confidence Confidence gap: 24.25% This substantial gap indicates the model exhibits uncertainty awareness, with incorrect predictions showing measurably lower confidencea valuable property for production deployment where low-confidence predictions can be flagged for human review."
        },
        {
            "title": "4.5 Attention Balance Analysis",
            "content": "The learnable fusion parameter α converges to approximately 0.57 by epoch 20, remaining stable thereafter. This corresponds to 57% weighting on Coordinate Attention (spatial) and 43% on SE (spectral), demonstrating nearequal importance with slight spatial bias. This empirical result validates the hypothesis that satellite imagery requires balanced attention to both spatial and spectral modalities. 2. AnnualCrop PermanentCrop: 7 errors"
        },
        {
            "title": "5 Discussion",
            "content": "3. Residential HerbaceousVeg: 6 errors"
        },
        {
            "title": "5.1 Architectural Insights",
            "content": "4. River Highway: 5 errors (vs. 27 baseline) Total errors: 112/4,050 (2.77% error rate). Our systematic evolution reveals several key insights: Heterogeneous Feature Requirements: Satellite imagery exhibits orthogonal feature requirements that single-modality attention cannot simultaneously address. Infrastructure classes depend on spatial patterns (shape, layout, directional structure), while land cover classes depend on spectral signatures (color, texture, spectral indices). Balanced dual-path attention successfully addresses both modalities. Learnable vs. Fixed Fusion: The learnable fusion parameter autonomously discovers near-equal weighting (α 0.57), validating our hypothesis while demonstrating slight spatial bias reflecting the presence of multiple infrastructure classes. This learnable approach outperforms fixed combination (+0.27% ablation), suggesting value in allowing models to discover optimal feature balances rather than imposing architectural priors. Progressive Regularization: DropBlock with progressive rates (5-20% by depth) proves most impactful (+0.65% ablation). Early layers extract general low-level features benefiting all classes; deep layers extract classspecific features prone to overfitting. Progressive regularization appropriately balances feature preservation and overfitting prevention."
        },
        {
            "title": "5.2 Comparison to State-of-the-Art",
            "content": "Our 97.23% accuracy positions within 1.34% of finetuned ResNet-50 (98.57%) [1]. The performance gap represents the cost of eliminating pre-training dependency. This demonstrates that systematic architectural design can substantially close the gap without external data, making from-scratch training viable for scenarios where pretraining is unavailable or suboptimal. The 2% gap to Vision Transformers and ensemble methods is expected, as these leverage fundamentally different architectures (self-attention) and model averaging strategies orthogonal to our CNN-focused investigation."
        },
        {
            "title": "5.3 Limitations and Future Work",
            "content": "Limitations: Transfer to other remote sensing datasets (UC Merced, AID) Temporal fusion for time-series satellite data"
        },
        {
            "title": "6 Conclusion",
            "content": "This work presents systematic investigation of custom CNN architectures for satellite image classification, achieving 97.23% test accuracy on EuroSAT without pretraining. Through three progressive iterations, we identify and address specific failure modes, culminating in balanced multi-task attention mechanism combining Coordinate Attention for spatial features and Squeeze-Excitation blocks for spectral features with learnable fusion. Our key contributions include: (1) empirical demonstration that balanced dual-path attention outperforms single-modality approaches for heterogeneous imagery, (2) learnable fusion discovering near-equal spatialspectral weighting (α 0.57), (3) comprehensive ablation studies quantifying component contributions, and (4) performance within 1.34% of pre-trained ResNet-50 while requiring no external data. This work demonstrates that systematic architectural design can achieve competitive performance for domainspecific applications where pre-training is unavailable, providing template for iterative ML engineering in specialized domains."
        },
        {
            "title": "Code Availability",
            "content": "implementations, scripts Complete els, and https://github.com/virAditya/ satellite-image-classification-eurosat modat: evaluation available trained are RGB-only analysisfull EuroSAT contains 13 spectral bands"
        },
        {
            "title": "Acknowledgments",
            "content": "Single model evaluationensembles typically gain 0.5-1% We thank the EuroSAT dataset creators and the PyTorch development team. Persistent vegetation confusion (PermanentCrop: 94.46%) Future Directions:"
        },
        {
            "title": "References",
            "content": "Extension to multi-spectral bands (expected +1-2%) Ensemble methods for improved robustness Neural architecture search for optimal block configurations [1] P. Helber, B. Bischke, A. Dengel, and D. Borth. Eurosat: novel dataset and deep learning benchmark IEEE for land use and land cover classification. Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):22172226, 2019. [2] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon. Cbam: In ECCV, Convolutional block attention module. pages 319, 2018. [3] Q. Hou, D. Zhou, and J. Feng. Coordinate attention for efficient mobile network design. In CVPR, pages 1371313722, 2021. [4] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In CVPR, pages 71327141, 2018. [5] A. Dosovitskiy et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [6] M. Tan and Q. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, pages 61056114, 2019. [7] G. Ghiasi, T.-Y. Lin, and Q. V. Le. Dropblock: regularization method for convolutional networks. In NeurIPS, pages 1072710737, 2018. [8] K. He, R. Girshick, and P. Dollar. Rethinking imIn ICCV, pages 49184927, agenet pre-training. 2019. [9] X. X. Zhu et al. Deep learning in remote sensing: comprehensive review. IEEE Geoscience and Remote Sensing Magazine, 5(4):836, 2017. [10] M. Drusch et al. Sentinel-2: Esas optical highresolution mission. Remote Sensing of Environment, 120:2536, 2012. [11] J. Deng et al. Imagenet: large-scale hierarchical image database. In CVPR, pages 248255, 2009. [12] Y. Yang and S. Newsam. Bag-of-visual-words for land-use classification. In ACM SIGSPATIAL, pages 270279, 2010. [13] G.-S. Xia et al. Aid: benchmark for aerial scene classification. IEEE Trans. Geoscience and Remote Sensing, 55(7):39653981, 2017."
        }
    ],
    "affiliations": [
        "Department of Computer Science and Engineering Manipal University Jaipur Jaipur, Rajasthan 303007, India"
    ]
}