{
    "paper_title": "Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling",
    "authors": [
        "Yanchen Luo",
        "Zhiyuan Liu",
        "Yi Zhao",
        "Sihang Li",
        "Kenji Kawaguchi",
        "Tat-Seng Chua",
        "Xiang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D molecule generation is crucial for drug discovery and material science, requiring models to process complex multi-modalities, including atom types, chemical bonds, and 3D coordinates. A key challenge is integrating these modalities of different shapes while maintaining SE(3) equivariance for 3D coordinates. To achieve this, existing approaches typically maintain separate latent spaces for invariant and equivariant modalities, reducing efficiency in both training and sampling. In this work, we propose \\textbf{U}nified Variational \\textbf{A}uto-\\textbf{E}ncoder for \\textbf{3D} Molecular Latent Diffusion Modeling (\\textbf{UAE-3D}), a multi-modal VAE that compresses 3D molecules into latent sequences from a unified latent space, while maintaining near-zero reconstruction error. This unified latent space eliminates the complexities of handling multi-modality and equivariance when performing latent diffusion modeling. We demonstrate this by employing the Diffusion Transformer--a general-purpose diffusion model without any molecular inductive bias--for latent generation. Extensive experiments on GEOM-Drugs and QM9 datasets demonstrate that our method significantly establishes new benchmarks in both \\textit{de novo} and conditional 3D molecule generation, achieving leading efficiency and quality."
        },
        {
            "title": "Start",
            "content": "Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling Yanchen Luo1, Zhiyuan Liu2, Yi Zhao1, Sihang Li1, Kenji Kawaguchi2, Tat-Seng Chua2, Xiang Wang1 1University of Science and Technology of China 2National University of Singapore luoyanchen@mail.ustc.edu.cn 5 2 0 2 9 1 ] . [ 1 7 6 5 5 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "3D molecule generation is crucial for drug discovery and material science, requiring models to process complex multi-modalities, including atom types, chemical bonds, and 3D coordinates. key challenge is integrating these modalities of different shapes while maintaining SE(3) equivariance for 3D coordinates. To achieve this, existing approaches typically maintain separate latent spaces for invariant and equivariant modalities, reducing efficiency in both training and sampling. In this work, we propose Unified Variational Auto-Encoder for 3D Molecular Latent Diffusion Modeling (UAE-3D), multi-modal VAE that compresses 3D molecules into latent sequences from unified latent space, while maintaining near-zero reconstruction error. This unified latent space eliminates the complexities of handling multi-modality and equivariance when performing latent diffusion modeling. We demonstrate this by employing the Diffusion Transformera general-purpose diffusion model without any molecular inductive bias for latent generation. Extensive experiments on GEOM-Drugs and QM9 datasets demonstrate that our method significantly establishes new benchmarks in both de novo and conditional 3D molecule generation, achieving leading efficiency and quality."
        },
        {
            "title": "Introduction",
            "content": "The discovery of novel molecules is fundamental to advancements across various scientific fields, particularly in drug and material development. Given significant progress has been made in designing 2D molecular graphs (Jin et al., 2018; Irwin et al., 2022; Polykovskiy et al., 2020; Shi et al., 2020), recent research has increasingly focused on the generation of 3D molecules (Hoogeboom et al., 2022; Huang et al., 2023b). Unlike 2D molecular generation, which focuses on forming valid molecular structures based on chemical bonds, 3D generation 1 Figure 1: (a) 3D molecule is characterized by features of distinct modalities. (b) Conventional approaches use separate latent spaces for equivariant (3D coordinates) and invariant (2D features) components, leading to fragmented molecular representations. (c) Our UAE-3D establishes unified SE(3)-equivariant latent space that jointly encodes both modalities Table 1: Comparing the reconstruction error between UAE-3D and GeoLDMs VAE. Atom/Bond accuracy measures correct atom/bond type predictions. Coordinate RMSD quantifies geometric deviation from the ground truth 3D coordinates. Metric GeoLDM (Xu et al., 2023) UAE-3D (Ours) Atom Accuracy (%) Bond Accuracy (%) Coordinate RMSD (Å) 98.6 96.2 0.1830 100.0 100.0 0.0002 must also predict 3D atomic coordinates that align with the 2D structure. Accurate 3D molecule generation is essential to power many important applications, such as structure-based drug design (Zhang et al., 2023) and inverse molecular design targeting quantum chemical properties (Bao et al., 2023). 3D molecule generation is challenging due to its multi-modal nature. As shown in Figure 1(a), 3D molecule consists of features of three distinct modalities: atom types, atomic coordinates, and chemical bonds. This requires the generation model to handle both discrete (e.g., atom types) and continuous features (e.g., coordinates), while also addressing differences in feature shapes (atom-wise v.s. edge-wise features). Worse still, the modality of 3D coordinate requires special care to respect rotational and translational equivariance, whereas other modalities do not, further complicating the task. Mitigating this multi-modal challenge, prior works mostly process each modality in separate latent spaces: some maintain separate diffusion processes for each modality (Huang et al., 2023b; Vignac et al., 2023; Le et al., 2024; Hua et al., 2023), while others interleave the prediction of different modalities across autoregressive generation steps (Gebauer et al., 2019, 2021). However, we argue that handling each modality separately complicates the model design, reducing both the training and sampling efficiency. Moreover, processing each modality separately risks compromising the consistency between them. These issues raise critical question: can we design unified generative model that seamlessly integrates all three modalities of 3D molecule generation? To answer this research question, we propose to build multi-modal latent diffusion model (LDM) (Rombach et al., 2022) for the unified generative modeling of 3D molecules. LDM extends the diffusion paradigm (Ho et al., 2020) by operating in compressed latent space learned through variational autoencoders (VAEs) (Kingma and Welling, 2014), offering improved computational efficiency and generation quality. For 3D molecules, we can build multi-modal VAE that compresses all three modalities in single unified latent space. Scrutinizing the previous works, we identify the following challenges to achieve this purpose: Building Single Unified Latent Space for 3D Molecules. While previous studies have explored LDMs for 3D molecule and protein generation (Xu et al., 2023; Kong et al., 2024), they fail to build unified latent space that integrates all modalities. Their difficulty arises from the reliance on neural networks with baked-in 3D equivariance (Satorras et al., 2021; Huang et al., 2023b), which mostly maintain separate latent spaces for 3D equivariant and invariant modalities (cf. Figure 1(b)). Ensuring Near-Lossless Compression. Maintaining low reconstruction error for the multimodal VAE is critical, as even small errors can result in invalid or unstable 3D structures. Furthermore, imprecise latents that cannot reproduce the original molecule can propagate their errors to the LDM, easily disrupting the generative modeling. Despite its critical importance, reconstruction error has been largely overlooked in previous works (Xu et al., 2023; Kong et al., 2024) (cf. Table 1). In this work, we introduce Unified Variational Auto-Encoder for 3D Molecular Latent Diffusion Modeling (UAE-3D), VAE that can compress the multi-modal features of 3D molecules into unified latent space while maintaining near zero (100% atom/bond accuracy and 2E-4 coordinate RMSD) reconstruction error. To obtain latent space integrating both 3D equivariant and invariant modalities, we draw inspiration from the bitter lesson (Wang et al., 2024) of 3D molecules: rather than using models with intricate, baked-in 3D equivariance, UAE-3D trains neural network to learn 3D equivariance through our tailor-made SE(3) augmentations, encouraging the transformation on the input coordinates are reflected equivariantly on the output coordinates. Moreover, UAE3D employs the Relational Transformer (Diao and Loynd, 2023) as its encoder, leveraging its scalability and flexibility to incorporate both atomwise and edge-wise features. Transformerbased (Vaswani et al., 2017) decoder is jointly trained with the encoder to reconstruct both 3D invariant and equivariant molecular features. Despite its simplicity, UAE-3D can compress 3D molecules as token sequences in unified latent space, eliminating the complexities of handling multi-modalities and 3D equivariance in latent diffusion modeling. As shown in Table 1, UAE3D reaches lossless reconstruction of atom and bond types, with near-zero coordinate RMSD. To further demonstrate its effectiveness, we employ the Diffusion Transformer (DiT) (Peebles and Xie, 2023), general-purpose diffusion backbone without any molecular inductive bias, to model UAE-3Ds latents. We show that DiT can successfully generate stable and valid 3D molecules, with significantly improved training (by 2.7 times) and sampling efficiency (by 7.3 times). We refer to this LDM pipeline as UDM-3D: Unified Latent Diffusion Modeling for 3D Molecule Generation. Extensive experiments demonstrate that UDM3D, powered by UAE-3D, achieves state-of-theart results in both de novo and conditional 3D molecule generation on the QM9 (Ramakrishnan et al., 2014) and GEOM-Drugs (Axelrod and Gómez-Bombarelli, 2020) datasets. Ablation studies show the effectiveness of our key components. 2 Figure 2: Overview of the UDM-3D and UAE-3D models. The UAE-3D encodes 3D molecules from molecular space into unified latent space, integrating multi-modal features such as atom types, chemical bonds, and 3D coordinates. This unified latent space is then utilized by the UDM-3D, which employs DiT to perform generative modeling. Then the denoised latents are decoded back into 3D molecules."
        },
        {
            "title": "2 Background: 3D Molecular Latent",
            "content": "Gaussian prior:"
        },
        {
            "title": "Diffusion Models",
            "content": "In this work, we explore latent diffusion models (LDMs) for 3D molecule generation. 3D molecular LDM involves 3D molecular VAE to compress 3D molecules into the latent space, where diffusion model performs generative modeling. Below, we briefly introduce both components."
        },
        {
            "title": "2.1 Molecular Variational Auto-Encoding",
            "content": "Notations. 3D molecule is represented by = F, E, X, where RVd1 is the atom feature matrix (e.g., atomic numbers), RVVd2 is the bond feature matrix (e.g., bond connectivity and type), and RV3 is the 3D atom coordinate matrix. d1 and d2 are dimensions of atom and bond features. is Gs set of atoms and denotes the number of atoms. molecular VAE consists of an encoder and decoder D. Given 3D molecule G, the encoder maps it into sequence of latent tokens E(G) = = {zi Rdi V}. Each latent zi is sampled from Gaussian distribution (zi; µi, σi) using the reparameterization trick (Kingma and Welling, 2014), where µi and σi are learned parameters for atom i. The decoder then reconstructs from these latents: ˆG = D(Z). The VAE is trained to minimize reconstruction loss and regularization term to ensure the latent space follows standard LVAE = EGpdata (cid:104) (cid:105) ˆG + DKL(q(ZG)p(z)) , (1) where DKL denotes the Kullback-Leibler divergence; q(ZG) = (cid:81) iV (zi; µi, σi) is the approximated posterior distribution; and ˆG measures the difference between the original and the predicted graph, whose definition varies across different works. In Xu et al. (2023), ˆG combines an MSE loss for 3D coordinates and cross-entropy loss for atom types."
        },
        {
            "title": "2.2 Diffusion Model",
            "content": "Building on the VAEs latent for 3D molecules, molecular LDM performs generative modeling with diffusion model (Rombach et al., 2022; Kingma et al., 2021). In the forward diffusion process, we gradually add noise to the original latent Z(0) = following: Z(t) (Z(t); (cid:112) α(t)Z(0), (1 α(t))I), (2) where (0, 1] is the diffusion timestep, and α(t) is hyperparameter controlling the signal and noise ratio. At the final step (t = 1), Z(1) approaches the prior distribution, an isotropic Gaussian distribution: Z(1) (0, I). In practice, Z(t) is samα(t)Z(0) + pled as Z(t) = 1 α(t)ϵ, where ϵ (0, I). (cid:112) Given the noised latents Z(t), diffusion model ϵθ(Z(t), t) trained to predict the added noise ϵ 3 by minimizing the MSE loss ϵθ(Z(t), t) ϵ2. Once trained, new latents ˆZ(0) are sampled using the diffusion model ϵθ via the iterative denoising process (Ho et al., 2020), and the decoder reconstructs the corresponding 3D molecule via ˆG = D( ˆZ(0)). Separated Latent Spaces. The VAE latents in previous works (Kong et al., 2024; Xu et al., 2023) include two parts: = [ Z; Z], where is for equivariant features and is for 3D invariant features. They also rely on diffusion models In contrast, as with baked-in 3D equivariance. Section 3 shows, our method uses unified latent space, allowing general-purpose diffusion modelwithout any geometric or molecular inductive biasto achieve strong performance."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we propose the UAE-3D, multimodal variational auto-encoder designed to effectively compress the diverse modalities of 3D molecules into unified latent space. Based on UAE-3D, we introduce UDM-3D, an LDM for 3D molecule generation."
        },
        {
            "title": "3.1 UAE-3D: Unified Variational",
            "content": "Auto-Encoder for 3D Molecular Latent Diffusion Modeling UAE-3D is designed to address the complexities of multi-modal and equivariance of molecular data by compressing the atomic features, bond features, and atomic coordinates into unified latent space. This is achieved through three key components: (1) Relatinoal-Transformer (Diao and Loynd, 2023) that effectively integrates the multi-modal features into token sequences, (2) Transformerbased (Vaswani et al., 2017) decoder and the reconstruction loss for 3D molecule reconstruction; and (3) our tailor-made data augmentations to train the model to learn 3D equivariance. Compressing 3D Molecules with Relational Transformer (R-Trans). Given 3D molecular = F, E, X, we compute its initial atom-wise and edge-wise embeddings as: Hn = MLP([X; F]) He = MLP([E; D]) RVd, RVVd, (3) Dij = GBF(Xi Xj2) Rd, (4) (5) where GBF() implements the Gaussian basis functions to expand inter-atomic distances as feature vectors (Thölke and Fabritiis, 2022); is the embedding dimension. The initial node embeddings Hn combines both atomic features and its 3D coordinates, while the edge embeddings He incorporate bond features and inter-atomic distances. Hn and He together effectively represent all 3D molecular information for the subsequent encoding. After obtaining the initial embeddings, we process them through layers of R-Trans blocks (Diao and Loynd, 2023): Hn = R-Trans(Hn, He) RVd, (6) where Hn denotes the updated atom embedding for the next layer, computed as follows: Qij = [Hn ; He ij]Wq, RVVd, (7) [Kij; Vij] = [Hn ; He ij]Wkv, K, RVVd, αij = softmaxj (cid:33) (cid:32) QijK ij , α RVV, (8) (9) ˆHn = (cid:88) αijVij, ˆHn RVd, (10) Hn = MLP( ˆHn ), Hn RVd, (11) where Wq and Wkv are linear projectors. Unlike the original R-Trans, our implementation does not update the edge embedding He across layers. We find that this method is more efficient without compromising performance. Compared to Transformer (Vaswani et al., 2017), R-Trans can effectively integrate edge embeddings He RVVd and atom embeddings Hn RVd, despite their different shapes, making it well-suited for 3D molecule encoding. This integration occurs during the computation of queries, keys and values (Equation (7) and (8)), ensuring the output atom embeddings fully incorporate edge information. Decoder. We employ standard Transformer (Vaswani et al., 2017) as decoder, with three additional predictors to reconstruct complete 3D molecules: atom types, bond types, and atom coordinates. Unlike the encoder, our decoder includes no molecule-specialized design, because it processes sequences of latents. Given the encoders 4 latents Z, our Transformer decoder provides the representation: = Transformer(Z) RVd. (12) We then obtain predictions for three molecular modalities: ˆXi = MLP1(Pi) ˆFi = MLP2(Pi) ˆEij = MLP3(Pi + Pj) R3, RNa, RNb, (13) (14) (15) where Na and Nb are the number of atom and bond types. Reconstruction Loss. We define the distance between our reconstructed graph and the original graph ˆG G, which includes multiple components to ensure the fidelity of the generated 3D molecules: Latom = Lcoordinate = Lbond = Ldistance = 1 (cid:88) iV 1 (cid:88) iV CrossEntropy( ˆFi, Fi), MSE( ˆXi, Xi), (16) (17) 1 V2 (cid:88) i,jV 1 (cid:88) i,jV CrossEntropy( ˆEij, Eij), (18) MSE( ˆXi ˆXj, Dij), (19) where Latom and Lbond are cross-entropy losses for atom and bond types; Lcoordinate measures the MSE between the predicted and the ground truth coordinates; Ldistance is an additionally constraint on the predicted coordinates, to ensure correct interatomic distances. The overall reconstruction loss is weighted sum of these components: Lrecon = γ [Latom, Lbond, Lcoordinate, Ldistance], (20) where γ R4 is hyperparameter vector balancing the reconstruction terms. The final loss for UAE-3D combines Lrecon with KL regularization term: SE(3)-Equivariant Augmentations. To ensure the VAE follows SE(3)-equivariance, we apply SE(3) transformations SE(3) as data augmentation on the molecular 3D coordinates (Abramson et al., 2024). The VAE D(E()) is trained to be equivariant to these transformations: Lrecon = D(E(R G)) G, (22) where = F, E, R(X) denotes applying the transformation on the 3D coordinates of G. In practice, combines random rotation from SO(3) and random translation from (0, 0.01I3), where 0.01 controls the range of the translation augmentation. When these SE(3) augmentations are applied, Equation (22) becomes equivalent to Equation (20), training the VAE to follow SE(3)-equivariance. 3.2 UDM-3D: Unified Latent Diffusion Modeling for 3D Molecule Generation Diffusion Transformer (DiT). Given the unified latent space provided by our UAE-3D model, we now apply the DiT (Peebles and Xie, 2023) as our diffusion model ϵθ for 3D molecular latent generation. DiTs promising performance in image generation underscores its superior capability in modeling latent sequences. Unlike standard Transformer (Vaswani et al., 2017), DiT replaces the layernorm (Ba et al., 2016) by adaptive layernorm (adaLN) (Perez et al., 2018), where the shift and scale parameters are dynamically generated based on the diffusion timestep t: [y; b] = MLP(Embedt(t)), where Embed() embeds timestep into vector. These shift and scale parameters act as gates, selectively activating different dimension given different timesteps, enabling effective dynamic-scale denoising in diffusion. Conditional Generation with Classifier-Free Guidance. For conditional generation, we directly combine the condition embedding Embedc(c) with the time embedding Embedt(t) when generating parameters for the adaLN: [y; b] = MLP(Embedt(t) + Embedc(c)). (23) In this way, we effectively inform the diffusion model ϵθ(Z(t), t, c) with the condition information. LUAE-3D = Lrecon+βEGpdata [DKL(q(ZG)p(Z))] , (21) where the hyperparameter β controls the KL regularization strength. To further enforce conditioning during inference, we employ the classifier-free guidance (Ho and Salimans, 2022) (CFG) to find that has high 5 log p(cZ). By Bayes rule, the gradient of this objective is log p(cZ) log p(Zc) log p(Z). Following Song et al. (2021), we can interprete the diffusion model ϵθs output as score functions, and therefore maximize p(cZ) using the modified denoising function: ϵθ(Z(t), t, c) = (1+w)ϵθ(Z(t), t, c)wϵθ(Z(t), t), (24) where [0, +) is hyperparameter controlling the guidance strength; ϵθ(Z(t), t, c) is the conditional variant of ϵθ, incorporating the property c; ϵθ(Z(t), t) is the unconditioned variant that ignores c. To train ϵθ, we randomly drop condition with certain probability, allowing ϵθ to learn both conditional and unconditional distributions."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we present the experimental results of our proposed unified molecular latent space approach (UAE-3D & UDM-3D) on 3D molecule generation tasks. We comprehensively evaluate UDM-3Ds performance on de novo 3D molecule generation and conditional 3D molecule generation with targeted quantum properties. Our experiments demonstrate significant improvements over state-of-the-art methods while maintaining computational efficiency."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Datasets. Following previous studies (Hoogeboom et al., 2022; Huang et al., 2023b; Vignac et al., 2023), we conduct experiments on the popular QM9 (Ramakrishnan et al., 2014) and the GEOM-Drugs (Axelrod and Gómez-Bombarelli, 2020) datasets. QM9 (Ramakrishnan et al., 2014): Contains 130k small organic molecules (up to 9 heavy atoms, 29 total atoms) with quantum chemical properties. Following standard practice (Hoogeboom et al., 2022; Huang et al., 2023b), we use 100K/18K/13K train/val/test splits. Baselines. For de novo 3D molecule generation, we compare UDM-3D with CDGS (Huang et al., 2023a), JODO (Huang et al., 2023b), MiDi (Vignac et al., 2023), G-SchNet (Gebauer et al., 2019), G-SphereNet (Luo and Ji, 2022), EDM (Hoogeboom et al., 2022), MDM (Huang et al., 2023c), GeoLDM (Xu et al., 2023). For conditional 3D molecule generation, we compare UDM-3D with EDM (Hoogeboom et al., 2022), EEGSDE (Bao et al., 2023), GeoLDM (Xu et al., 2023), and JODO (Huang et al., 2023b). 4.2 De Novo 3D Molecule Generation Experimental Setting. Generating 3D molecule involves generating the structural validity (atom/bond features) and accurate spatial arrangements (3D coordinates). Therefore we adapt the comprehensive metrics from (Huang et al., 2023b; Hoogeboom et al., 2022): 2D Metrics: Atom stability, validity & completeness (V&C), validity & uniqueness (V&U), validity & uniqueness & novelty (V&U&N), similarity to nearest neighbor (SNN), fragment similarity (Frag), scaffold similarity (Scaf), and Fréchet ChemNet Distance (FCD) (Polykovskiy et al., 2020). 3D Metrics: Atom stability, FCD, maximum mean discrepancy (MMD) (Gretton et al., 2012) for the distributions of bond lengths, bond angles, and dihedral angles. The details of these metrics are in Appendix A.1.1. We also report all the metrics for molecules from training set, serving as an approximate upper bound for performance. Table 2 and Table 3 present the performance of UDM-3D on the de novo generation task on QM9 and GEOM-Drugs datasets, respectively. Our model demonstrates leading performances in generating chemically valid and novel molecules and achieves state-of-the-art performance in most of the metrics across both datasets. Two detailed key observations emerge from the results: 1. Breakthrough in Geometric Accuracy. Our unified latent space proves highly effective for 3D geometric modeling which is crucial for accurate 3D molecule generation. UDM-3D achieves significantly better performance than baselines for the distributional distance of bond lengths, bond angles, reducing errors by an order of magnitude. This demonstrates UDM3Ds ability to generate highly accurate 3D structures. For the distributional distance of dihedral angles, key measurement for 3D conformation accuracy (Jing et al., 2022), UDM-3D also achieves the best performance. Notably, these advantages on stereochemical geometric structures persist for complex drug-like molecules in GEOM-Drugs, where UDM-3D improves the distributional distance for bond length by 25 times compared to GeoLDM (9.89E-03 v.s. 6 Table 2: Performance of de novo 3D molecule generation on GEOM-Drugs. * indicates results reproduced using official source codes, while other baseline results are taken from Huang et al. (2023b). 2D-Metric AtomStable V&C V&U V&U&N SNN Frag Scaf FCD Train CDGS JODO MiDi* UDM-3D, ours 1.000 0.991 1.000 0.968 1.000 1.000 0.285 0.874 0.633 0.879 1.000 0.285 0.905 0.654 0.913 0.000 0.285 0.902 0.652 0.958 0.585 0.262 0.417 0.392 0. 0.999 0.789 0.993 0.951 0.989 0.584 0.022 0.483 0.196 0.540 0.251 22.051 2.523 7.054 0.692 3D-Metric AtomStable FCD3D Bond length Bond angle Dihedral angle Train EDM JODO MiDi* GeoLDM UDM-3D, ours 0.861 0.831 0.845 0.750 0.843 0.852 13.73 31.29 19.99 23.14 30.68 17.36 1.56E-04 4.29E-01 8.49E-02 1.17E-01 3.91E-01 9.89E-03 1.81E-04 4.96E-01 1.15E-02 9.57E-02 4.22E-01 5.11E1.56E-04 1.46E-02 6.68E-04 4.46E-03 1.69E-02 1.74E-04 3.91E-01). This quantum leap stems from UDM-3Ds advantage to jointly optimize chemical and 3D geometric constraints when generating within unified latent space, ensuring consistency across different molecular modalities. 2. Novel and High Quality Generation. UDM3D achieves remarkable V&U&N scores of 0.948 (QM9) and 0.958 (GEOM-Drugs) while maintaining stability on par with top baselines like JODO. This demonstrates UDM-3Ds unique capability to generate novel molecules while preserving high-quality structures. The improvement is attributed to the unified latent space: by encoding atom types and bond features in unified space, the model avoids overfitting to common molecular patterns while maintaining chemical validity."
        },
        {
            "title": "4.3 Conditional 3D Molecule Generation",
            "content": "Experimental Settings. We evaluate conditional generation of molecules with target quantum properties using the protocol from (Hoogeboom et al., 2022; Xu et al., 2023). Specifically, our target properties include Cv, µ, α, ϵHOMO, ϵLUMO, ϵ, and we evaluate the Mean Absolute Error (MAE) between target and predicted properties. More details on the properties and settings are provided in Appendix A.1.2. Table 4 presents the MAE results for conditional generation on QM9. UDM-3D achieves the lowest MAE across five properties, demonstrating its effectiveness in generating molecules with desired target properties. Notably, UDM-3D reduces the average MAE by 42.2% compared to GeoLDM, with strong gains (52.7% relative improvement) in ϵ (HOMO-LUMO gap), demonstrating significant improvements in modeling quantum chemical properties. These gains highlight the critical role of our unified latent space in harmonizing multimodal information during conditional generation. While GeoLDM struggles to correlate all the modalities with target quantum properties due to its fragmented latent spaces, our unified latent representation inherently preserves the interplay between 3D geometry, bonds, and electronic characteristics, enabling precise modeling over molecule-property relationships."
        },
        {
            "title": "4.4 Training & Sampling Efficiency",
            "content": "We further analyze the training and sampling efficiency of UDM-3D compared to baselines on the QM9 dataset. These experiments are performed with an NVIDIA A100 GPU. Training Efficiency. As shown in Figure 3a, UDM-3D completes training in 52 hours (14h UAE-3D + 38h UDM-3D), which is 5.3 times faster than GeoLDM (449h) and 2.7 times faster than JODO (139h). This efficiency stems from: (1) Our decoupled training paradigm: UAE-3D first learns 3D molecule compression, allowing the DiT to focus solely on generative modeling of the compressed latents. (2) Efficient diffusion with DiT: Unlike previous molecular diffusion models that require complex architectures for multi-modality and equivariance, DiTs simple and highly paral7 Table 3: Performance of de novo 3D molecule generation on QM9. * indicates results reproduced using official source codes, while other baseline results are taken from Huang et al. (2023b). 2D-Metric AtomStable V&C V&U V&U&N SNN Frag Scaf FCD Train CDGS JODO MiDi* UDM-3D, ours 0.999 0.997 0.999 0.998 0. 0.989 0.951 0.990 0.980 0.983 0.989 0.936 0.960 0.954 0.972 0.000 0.860* 0.780* 0.769 0.948 0.490 0.493 0.522 0.501 0.508 0.992 0.973 0.986 0.979 0.987 0.946 0.784 0.934 0.882 0. 0.063 0.798 0.138 0.187 0.161 3D-Metric AtomStable FCD3D Bond length Bond angle Dihedral angle Train EDM MDM JODO MiDi* GeoLDM UDM-3D, ours 0.994 0.986 0.992 0.992 0.983 0.989 0.993 0.877 1.285 4.861 0.885 1.100 1.030 0.881 5.44E-04 1.30E-01 2.74E-01 1.48E-01 8.96E-01 2.40E-01 7.04E-02 4.65E-04 1.82E-02 6.60E-02 1.21E-02 2.08E-02 1.00E-02 9.84E-03 1.78E-04 6.64E-04 2.39E-02 6.29E-04 8.14E-04 6.59E-04 3.47E-04 Table 4: Mean Absolute Error (MAE) for conditional 3D molecule generation on QM9. Lower values represent better performance."
        },
        {
            "title": "Method",
            "content": "U-Bound EDM EEGSDE GeoLDM JODO UDM-3D, ours L-Bound µ (D) α (Bohr3) Cv mol K(cid:1) (cid:0) cal εHOMO (meV) εLUMO (meV) ε (meV) 1.613 1.123 0.777 1.108 0.628 0.603 0. 8.98 2.78 2.50 2.37 1.42 1.76 0.09 6.879 1.065 0.941 1.025 0.581 0.553 0.040 645 371 302 340 226 216 39 1457 601 447 522 256 247 36 1464 671 487 587 335 313 65 lelizable design allows for faster training and sampling on the unified latent space. Crucially, our speedup does not compromise geometric accuracy, as guaranteed by UAE-3Ds near-lossless molecular reconstruction (cf. Table 1). Sampling Speed. As shown in Figure 3b, UDM3D generates molecules in just 0.081s per sample, making it 7.3x faster than EDM/GeoLDM (0.59s) and 9.8x faster than JODO (0.79s). This speed advantage comes from the unified latent space, which simplifies DiTs modeling and avoids complex neural architectures. Table 5: Comparing DiT with vanilla Transformer for latent diffusion modeling. Models have the same depth and hidden size. A.S. stands for atom stability Arch. A.S.3D A.S.2D V&C V&U V&U&N DiT Trans. 0.993 0.983 0.999 0. 0.983 0.938 0.972 0.922 0.948 0.922 which we attribute to its adaptive LayerNorm layers. These layers enable DiT to effectively handle data with varying noise scales, improving diffusion performance."
        },
        {
            "title": "5 Related Works",
            "content": "DiT Backbone. We conduct an ablation study on DiT by comparing it to vanilla Transformer for de novo 3D molecule generation on the QM9 dataset  (Table 5)  . The results show that DiT outperforms the vanilla Transformer across all metrics, 3D Molecule Generation. Early efforts for 3D molecule generation focus on autoregressive approaches (Gebauer et al., 2019, 2021; Luo and Ji, 2022), constructing 3D molecules sequentially by adding atoms or molecular fragments. With 8 et al., 2023) compresses atomic features and 3D coordinates separately, failing to achieve unified latent space distribution. In 3D protein molecule generation, PepGLAD (Kong et al., 2024) compresses 1D sequences and 3D structures into two separate latent spaces. Our UDM-3D addresses these issues by employing unified latent space, significantly improved generation efficiency. Molecular Variational Autoencoders. Variational Autoencoders (VAEs) have been widely explored for generative modeling (Kingma and Welling, 2014; Simonovsky and Komodakis, 2018; van den Oord et al., 2017; Razavi et al., 2019), providing framework to encode data into structured latent space while enabling both generation and reconstruction (Kingma and Welling, 2014). In molecular generation, VAEs have been applied to 2D molecular graphs, as seen in JT-VAE(Jin et al., 2018), MGCVAE(Lee and Min, 2022), SSVAE(Kang and Cho, 2019), and CGVAE (Liu et al., 2018). However, these methods have several limitations. First, they primarily focus on 2D molecular representations and often rely on nontransformer architectures, limiting their scalability for more complex molecular generation tasks (Du et al., 2022). Additionally, VAE-based methods typically assume simple Gaussian prior, which may fail to accurately model the complex posterior distributions required for effective molecular generation (Dai and Wipf, 2019; Aneja et al., 2021). With recent advancements in Latent Diffusion Models (LDMs) demonstrating success across multiple domains (Vahdat et al., 2021; Zeng et al., 2022), LDMs have been introduced for 3D molecule and protein generation, as seen in GeoLDM(Xu et al., 2023) and PepGLAD(Kong et al., 2024). These approaches leverage diffusion modeling in latent space, offering improved generative capacity and efficiency."
        },
        {
            "title": "6 Conclusion and Future Works",
            "content": "In this paper, we propose UAE-3D to compress the multi-modal features of 3D molecules into unified latent space, and demonstrate the effectiveness of latent diffusion modeling on this space by introducing UDM-3D. By integrating atom types, chemical bonds, and 3D coordinates into single latent space, our model effectively addresses the inherent challenges of multi-modality and SE(3) equivariance for 3D molecule generation. Extensive experiments on GEOM-Drugs and QM9 con- (a) Training time comparison. (b) Sampling speed comparison. Figure 3: Efficiency comparison. (a): Our UDM-3D adopts two-stage pipeline (stacked bar: 14h VAE training + 38h LDM training), while baselines use end-toend training. the success of diffusion models across various domains (Sohl-Dickstein et al., 2015; Ho et al., 2020; Dhariwal and Nichol, 2021), they also become the de facto method for 3D molecule generation (Hoogeboom et al., 2022; Bao et al., 2023). However, the early diffusion works can easily generate invalid molecules because of overlooking the bond information. To bridge this gap, subsequent works (Huang et al., 2023b; Vignac et al., 2023; Hua et al., 2023) additionally consider bond generation by building separate diffusion process. However, generating different modalities in separate diffusion processes unnecessarily complicates the model design, reducing efficiency. It also risks compromising the consistency between these modalities. To address this, our UDM-3D performs generative modeling in UAE-3Ds unified latent space that integrates all molecular modalities, improving efficiency and generation quality. Latent Diffusion Models (LDMs) for 3D Molecule Generation. To improve the efficiency, LDMs employ VAE to compress raw data into lower-dimensional latent space, where diffusion model performs generative modeling (Rombach et al., 2022). This approach has been very popular for image generation (Peebles and Xie, 2023). However, existing LDM-based methods for 3D molecule generation still face challenges in separated latent spaces. For example, GeoLDM (Xu 9 firm leading performances in both de novo and conditional 3D molecule generation, setting new benchmarks for both quality and efficiency. The success of UAE-3D highlights the benefits of building unified latent space for molecular design. Moving forward, we will investigate the transferability of the unified latent space to broader molecular design tasks, including structure-based drug design, and extend it to additional modalities, such as proteins, to further assess its effectiveness."
        },
        {
            "title": "References",
            "content": "Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew Ballard, Joshua Bambrick, and 1 others. 2024. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, pages 13. Jyoti Aneja, Alexander G. Schwing, Jan Kautz, and Arash Vahdat. 2021. contrastive learning approach for training variational autoencoder priors. In NeurIPS, pages 480493. Simon Axelrod and Rafael Gómez-Bombarelli. 2020. GEOM: energy-annotated molecular conformations for property prediction and molecular generation. CoRR, abs/2006.05531. Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. CoRR, Layer normalization. Hinton. 2016. abs/1607.06450. Fan Bao, Min Zhao, Zhongkai Hao, Peiyao Li, Chongxuan Li, and Jun Zhu. 2023. Equivariant energyguided SDE for inverse molecular design. In ICLR. OpenReview.net. Bin Dai and David P. Wipf. 2019. Diagnosing and enhancing VAE models. In ICLR (Poster). OpenReview.net. Prafulla Dhariwal and Alexander Quinn Nichol. 2021. Diffusion models beat gans on image synthesis. In NeurIPS, pages 87808794. Cameron Diao and Ricky Loynd. 2023. Relational attention: Generalizing transformers for graph-structured tasks. In ICLR. OpenReview.net. Yuanqi Du, Tianfan Fu, Jimeng Sun, and Shengchao Liu. 2022. Molgensurvey: systematic survey in machine learning models for molecule design. CoRR, abs/2203.14500. Niklas W. A. Gebauer, Michael Gastegger, Stefaan S. P. Hessmann, Klaus-Robert Müller, and Kristof T. Schütt. 2021. Inverse design of 3d molecular structures with conditional generative neural networks. CoRR, abs/2109.04824. 10 Niklas W. A. Gebauer, Michael Gastegger, and Kristof Schütt. 2019. Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules. In NeurIPS, pages 75647576. Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander J. Smola. 2012. kernel two-sample test. J. Mach. Learn. Res., 13:723773. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In NeurIPS. Jonathan Ho and Tim Salimans. 2022. Classifier-free diffusion guidance. CoRR, abs/2207.12598. Emiel Hoogeboom, Victor Garcia Satorras, Clément Vignac, and Max Welling. 2022. Equivariant diffusion for molecule generation in 3d. In ICML, volume 162 of Proceedings of Machine Learning Research, pages 88678887. PMLR. Chenqing Hua, Sitao Luan, Minkai Xu, Zhitao Ying, Jie Fu, Stefano Ermon, and Doina Precup. 2023. Mudiff: Unified diffusion for complete molecule generation. In LoG, volume 231 of Proceedings of Machine Learning Research, page 33. PMLR. Han Huang, Leilei Sun, Bowen Du, and Weifeng Lv. 2023a. Conditional diffusion based on discrete graph structures for molecular graph generation. In AAAI, pages 43024311. AAAI Press. Han Huang, Leilei Sun, Bowen Du, and Weifeng Lv. 2023b. Learning joint 2d & 3d diffusion models for complete molecule generation. CoRR, abs/2305.12347. Lei Huang, Hengtong Zhang, Tingyang Xu, and KaChun Wong. 2023c. MDM: molecular diffusion model for 3d molecule generation. In AAAI, pages 51055112. AAAI Press. Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. 2022. Chemformer: pre-trained transformer for computational chemistry. Mach. Learn. Sci. Technol., 3(1):15022. Wengong Jin, Regina Barzilay, and Tommi S. Jaakkola. 2018. Junction tree variational autoencoder for molecular graph generation. In ICML, volume 80 of Proceedings of Machine Learning Research, pages 23282337. PMLR. Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi S. Jaakkola. 2022. Torsional diffusion for molecular conformer generation. In NeurIPS. Seokho Kang and Kyunghyun Cho. 2019. Conditional molecular design with deep generative models. J. Chem. Inf. Model., 59(1):4352. Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. 2021. Variational diffusion models. CoRR, abs/2107.00630. Diederik P. Kingma and Max Welling. 2014. Autoencoding variational bayes. In ICLR. Xiangzhe Kong, Yinjun Jia, Wenbing Huang, and Yang Liu. 2024. Full-atom peptide design with geometric latent diffusion. In NeurIPS. Tuan Le, Julian Cremer, Frank Noé, Djork-Arné Clevert, and Kristof T. Schütt. 2024. Navigating the design space of equivariant diffusion-based generative models for de novo 3d molecule generation. In ICLR. OpenReview.net. Myeonghun Lee and Kyoungmin Min. 2022. MGCVAE: multi-objective inverse design via molecular graph conditional variational autoencoder. J. Chem. Inf. Model., 62(12):29432950. Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander L. Gaunt. 2018. Constrained graph variational autoencoders for molecule design. In NeurIPS, pages 78067815. Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. 2020. Graphaf: flowbased autoregressive model for molecular graph generation. In ICLR. OpenReview.net. Martin Simonovsky and Nikos Komodakis. 2018. Graphvae: Towards generation of small graphs using variational autoencoders. In ICANN (1), volume 11139 of Lecture Notes in Computer Science, pages 412422. Springer. Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermoIn ICML, volume 37 of JMLR Workdynamics. shop and Conference Proceedings, pages 22562265. JMLR.org. Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2021. Score-based generative modeling through stochastic differential equations. In ICLR. OpenReview.net. Youzhi Luo and Shuiwang Ji. 2022. An autoregressive flow model for 3d molecular geometry generation from scratch. In International conference on learning representations (ICLR). Philipp Thölke and Gianni De Fabritiis. 2022. Equivariant transformers for neural network based molecular potentials. In International Conference on Learning Representations. William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In ICCV, pages 41724182. IEEE. Arash Vahdat, Karsten Kreis, and Jan Kautz. 2021. Score-based generative modeling in latent space. In NeurIPS, pages 1128711302. Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. 2018. Film: Visual reasoning with general conditioning layer. In AAAI, pages 39423951. AAAI Press. Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, Artur Kadurin, Simon Johansson, Hongming Chen, Sergey Nikolenko, Alan Aspuru-Guzik, and Alex Zhavoronkov. 2020. Molecular Sets (MOSES): Benchmarking Platform for Molecular Generation Models. Frontiers in Pharmacology. Raghunathan Ramakrishnan, Pavlo O. Dral, Matthias Rupp, and O. Anatole von Lilienfeld. 2014. Quantum chemistry structures and properties of 134 kilo molecules. Scientific Data. Ali Razavi, Aäron van den Oord, and Oriol Vinyals. 2019. Generating diverse high-fidelity images with VQ-VAE-2. In NeurIPS, pages 1483714847. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. Highresolution image synthesis with latent diffusion models. In CVPR, pages 1067410685. IEEE. Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. 2021. E(n) equivariant graph neural networks. In ICML, volume 139 of Proceedings of Machine Learning Research, pages 93239332. PMLR. Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. 2017. Neural discrete representation learning. In NIPS, pages 63066315. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS, pages 59986008. Clément Vignac, Nagham Osman, Laura Toni, and Pascal Frossard. 2023. Midi: Mixed graph and 3d denoising diffusion for molecule generation. In ECML/PKDD (2), volume 14170 of Lecture Notes in Computer Science, pages 560576. Springer. Yuyang Wang, Ahmed A. A. Elhag, Navdeep Jaitly, Joshua M. Susskind, and Miguel Ángel Bautista. 2024. Swallowing the bitter pill: Simplified scalable conformer generation. In ICML. OpenReview.net. Minkai Xu, Alexander S. Powers, Ron O. Dror, Stefano Ermon, and Jure Leskovec. 2023. Geometric latent diffusion models for 3d molecule generation. In ICML, volume 202 of Proceedings of Machine Learning Research, pages 3859238610. PMLR. Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. 2022. LION: latent point diffusion models for 3d shape generation. In NeurIPS. Zaixi Zhang, Yaosen Min, Shuxin Zheng, and Qi Liu. 2023. Molecule generation for target protein binding with structural motifs. In ICLR. OpenReview.net."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Experimental Details A.1.1 Evaluation Metrics Our evaluation framework for de novo 3D molecule generation encompasses complementary metrics to assess molecular validity, diversity, and geometric fidelity. These metrics operate at two levels: 2D Structural Analysis: Validity & Stability: Measures adherence to chemical rules: Atom Stability: Percentage of atoms with chemically valid valency Validity & Completeness (V&C): Fraction of fully connected, syntactically correct molecules Diversity: Quantifies exploration of chemical space: V&U/V&U&N: Rates uniqueness and novelty relative to training data Distribution Alignment: Compares generated/test distributions: FCD: Fréchet distance in ChemNet feature space SNN/Frag/Scaf: Tanimoto similarity for nearest neighbors, BRICS fragments, and Murcko scaffolds 3D Geometric Analysis: Conformer Quality: Evaluates reconstructed 3D coordinates: Atom Stability: Consistency of valency in 3D-derived 2D graphs FCD3D: Distribution match of 3D-to-2D reconstructed molecules Geometric Fidelity: Measures spatial arrangement accuracy via Maximum Mean Discrepancy (MMD) for: Bond lengths Bond angles Dihedral angles A.1.2 Targeted Quantum Properties For conditional generation, we optimize molecules toward six key electronic properties from quantum chemistry: Electronic Response: Dipole Moment (µ): Molecular polarity measure Polarizability (α): Induced dipole response to electric fields Thermodynamic Properties: Heat Capacity (Cv): Energy absorption at constant volume Electronic Structure: HOMO (εHOMO)/LUMO (εLUMO): Frontier orbital energies HOMO-LUMO Gap (ε): Critical for reactivity and conductivity The evaluation protocol follows rigorous split-and-validate strategy (Hoogeboom et al., 2022): QM9 training data divided into disjoint subsets Da (50k) and Db (50k) Property predictor ϕc trained exclusively on Da Generated molecules compared against ϕcs predictions on Db (lower-bound baseline) and random predictions without any relation between molecule and property (upper-bound baseline) Performance quantified via Mean Absolute Error (MAE) across all properties This approach ensures fair assessment of conditional generation without data leakage, with ϕcs Db performance establishing the theoretical minimum achievable error. A.2 Implement Details Algorithm 1 Training Algorithm for UAE-3D and UDM-3D Require: Molecular dataset D, encoder E, decoder D, diffusion model ϵθ 1: Initialize encoder E, decoder D, and diffusion model ϵθ 2: Stage 1: Train UAE-3D 3: while not converged do 4: Sample batch of 3D molecules = E(G) {Encode} ˆG = D(Z) {Decode} Lrecon (Eq. 20) {Reconstruction loss} LUAE-3D (Eq. 21) {VAE loss} Update and using LUAE-3D 9: 10: end while 11: Stage 2: Train UDM-3D 12: while not converged do 13: Sample batch of latent sequences Z(0) E(D) Sample diffusion timestep Uniform(0, 1) (cid:112) Z(t) = ˆϵ = ϵθ(Z(t), t) Ldiffusion = ˆϵ ϵ2 {Noise prediction loss} Update ϵθ using Ldiffusion α(t)Z(0) + 18: 19: end while 5: 6: 7: 8: 14: 15: 16: 17: 1 α(t)ϵ, where ϵ (0, I) {Diffusion} Algorithm 2 UDM-3D Sampling with CFG Require: Trained UAE-3D encoder E, DiT ϵθ Require: Guidance strength w, timesteps {tk}K Require: Target property (optional) 1: Z(1) (0, I) {Initial noise} 2: for = downto 1 do 3: k=1 4: 5: 6: 7: 8: tk Compute Guidance: ϵcond ϵθ(Z(t), t, c) ϵuncond ϵθ(Z(t), t, ) ϵ (1 + w)ϵcond wϵuncond {CFG blending} Denoise: Z(t1) DDPM_Step(Z(t), ϵ, t) (Ho et al., 2020) 9: 10: end for 11: Decode: 12: ˆG = ˆF, ˆE, ˆX D(Z(0)) {UAE-3D decoder} Hyperparameters. We refer insterested readers to our open-source code https://anonymous.4open. science/r/UAE-3D/ for comprehensive view of our hyperparameters, such as learning rate and hidden dimension. A.2.1 Algorithm We provide the training and sampling algorithms for UDM-3D in Algorithms 1 and 2, respectively. The training algorithm for UDM-3D involves two stages: training the VAE (UAE-3D) and training the DiT (UDM-3D). The sampling algorithm for UDM-3D with CFG involves iteratively denoising the latent sequence using the DiT and the CFG guidance."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "University of Science and Technology of China"
    ]
}