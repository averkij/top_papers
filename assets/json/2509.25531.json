{
    "paper_title": "MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources",
    "authors": [
        "Huu Nguyen",
        "Victor May",
        "Harsh Raj",
        "Marianna Nezhurina",
        "Yishan Wang",
        "Yanqi Luo",
        "Minh Chien Vu",
        "Taishi Nakamura",
        "Ken Tsui",
        "Van Khue Nguyen",
        "David Salinas",
        "Aleksandra Krasnodębska",
        "Christoph Schuhmann",
        "Mats Leon Richter",
        "Xuan-Son",
        "Vu",
        "Jenia Jitsev"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong model performance. MixtureVitae follows a risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources), alongside targeted instruction, reasoning and synthetic data with documented provenance. We detail a transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and we release the dataset and curation recipes to support reproducible research. In controlled experiments using the open-sci-ref training protocol (fixed architectures at 130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B/300B setting they surpass FineWeb-Edu and approach DCLM in the later stages of training. Performance is particularly strong on math/code and competitive on QA tasks. These results demonstrate that permissive-first, risk-mitigated data provides a practical and legally mitigated foundation for training capable LLMs, reducing reliance on indiscriminate web scraping without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 1 3 5 5 2 . 9 0 5 2 : r Preprint. Under review. MIXTUREVITAE: OPEN WEB-SCALE PRETRAINING DATASET WITH HIGH QUALITY INSTRUCTION AND REASONING DATA BUILT FROM PERMISSIVE-FIRST TEXT SOURCES Huu Nguyen1,3,4,*, Victor May1,*, Harsh Raj1,2,*, Marianna Nezhurina1,3,4,5, Yishan Wang1, 6, Yanqi Luo7, Minh Chien Vu8, Taishi Nakamura4,9, Ken Tsui4,15, Van Khue Nguyen10, David Salinas11,12, Aleksandra Krasnodebska13, Christoph Schuhmann3, Mats Leon Richter14, Xuan-Son (Sonny) Vu16, and Jenia Jitsev1,3,4,5 *Equal contribution Correspondence to: huu@ontocord.ai 1Ontocord 2Northeastern University 3LAION 4Open-Ψ (Open-Sci) Collective 5Juelich Supercomputing Center (JSC), Research Center Juelich (FZJ) 6Carnegie Mellon University 7Salesforce 8Detomo Inc. 9Institute of Science Tokyo 10École Polytechnique, IP Paris 11ELLIS Institute Tuebingen 12University of Freiburg 13NASK 14Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal 15Independent Researcher 16RSS Lab, LTH / DeepTensor AB"
        },
        {
            "title": "ABSTRACT",
            "content": "We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong model performance. MixtureVitae follows risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources), alongside targeted instruction, reasoning and synthetic data with documented provenance. We detail transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and we release the dataset and curation recipes to support reproducible research. In controlled experiments using the open-sci-ref training protocol (fixed architectures at 130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens), models trained on MixtureVitae consistently outperform other permissive datasets across suite of standard benchmarks, and at the 1.7B/300B setting they surpass FineWeb-Edu and approach DCLM in the later stages of training. Performance is particularly strong on math/code and competitive on QA tasks. These results demonstrate that permissive-first, risk-mitigated data provides practical and legally mitigated foundation for training capable LLMs, reducing reliance on indiscriminate web scraping without sacrificing competitiveness. 1 Preprint. Under review."
        },
        {
            "title": "INTRODUCTION",
            "content": "The proliferation of large language models (LLMs) has transformed the landscape of artificial intelligence, yet their development often relies on legally and ethically precarious foundation. The vast majority of performant models are pretrained on massive web scrapes, indiscriminately mixing public-domain content with copyrighted materials such as books, news articles, and personal websites without explicit permission (Raffel et al., 2020; Gao et al., 2020). This practice has led to growing number of copyright infringement lawsuits, creating significant legal uncertainty for both academic researchers and commercial developers and threatening the future of the field. Compounding this uncertainty is the prevailing assumption that state-of-the-art performance is inextricably linked to the sheer scale and diversity offered by these legally ambiguous web scrapes. The absence of high-performance, large-scale pretraining dataset that actively mitigates these risks has forced difficult choice between performance and compliance. This raises critical question: Can powerful language model be trained on dataset that provides more legally robust foundation? To this question, we answer \"yes\": We introduce MixtureVitae, 211.1-billion-token, open-access pretraining dataset constructed to minimize copyright risk. The core of MixtureVitae \"permissivefirst\" data comprises (1) text with clear and permissive licenses (e.g., CC-BY-*, Apache 2.0), public domain text, and copyright exempt text such as US federal works (see Appendix F) and (2) riskmitigated text. Following Phi-4 (Abdin et al., 2024), which shows that the addition of synthetic and web-rewrite data boosts performance, and to address the scarcity of organic reasoning and conversational dialogue in strictly permissive sources, MixtureVitae is significantly augmented with targeted synthetic data, which is derived from permissive models and sources. We call this combination of expressly licensed and risk-mitigated methods the \"permissive-first\" approach. To validate our approach, we train models with 130M, 400M, 1.3B, and 1.7B parameters on MixtureVitae and compare their performance against several prominent open datasets. The results first confirm that MixtureVitae significantly outperforms all other permissively licensed baselines, with the performance gap widening as the model scale increases. The more critical test, however, is against popular non-permissive datasets containing higher proportions of copyrighted material. Here, our models achieve competitive performance, confirming that dataset built on risk-mitigated foundation can produce high-performing models. In summary, our contributions are threefold: We present MixtureVitae, the first large-scale, high-performance permissive-first organicsynthetic pretraining dataset built with transparent, risk-mitigated approach to sourcing. We demonstrate through rigorous evaluation that reliance on indiscriminately scraped, In fact, we high-risk copyrighted data is not prerequisite for training capable LLMs. show strong boost in math/code achieved already in pre-training. Upon acceptance, we will release the full dataset, model checkpoints and our data curation methodologies to the community, providing more legally sound foundation for future LLM research and development."
        },
        {
            "title": "2 DATASET",
            "content": "We adopt permissive-first, risk-mitigated strategy, combining sources with clear permissive licenses (e.g. CC-BY, Apache, public domain) with narrowly justified inclusions (government works, EU TDM-eligible data) and targeted synthetic data. Within this framework, the MixtureVitae dataset is constructed from three primary categories: curated sources for domain-specific expertise, diverse web data for language and general knowledge and instruction-following and reasoning datasets to enhance reasoning and task-completion abilities. The major categories of our corpus are visualized in Figure 1a. We provide granular breakdown showing the token count for each component (Figure 5), the license distribution is, (Figure 1b), and synthetic data usage (Figure 2a). Specific data sources are detailed in the following subsections. 2 Preprint. Under review. (a) Dataset Composition by Top-Level Category and Content Domain (b) Token Distribution by Governing License Figure 1: Composition of the MixtureVitae dataset (permissive-first, risk-mitigated composition). 2.1 DATA SOURCES The following sections describe each of three three categories of data in MixtureVitae: web, curated sources, and instruction and reasoning datasets. 2.1.1 WEB-SCALE CORPORA One subset of our pre-training data is derived from web-scale datasets including Nemotron-CC (Cicero & et al., 2024), MagaCorpus (Xu et al., 2024b), and FineFineWeb (Zhang et al., 2024). It also contain synthetic data generated by rephrasing web text originating from Nemotron-CC and MagaCorpus. 2.1.2 CURATED DATASETS To incorporate domain-specific knowledge and high-quality text, we curate diverse sources: public financial documents from SEC EDGAR (U.S. Securities and Exchange Commission, 2024), multilingual encyclopedic articles from MegaWika (Barham et al., 2023) and TxT360 (Tang et al., 2024), scientific papers from arXiv (Clement et al., 2019) and peS2o (Soldaini & Lo, 2023), medical data from Pubmed (National Library of Medicine (U.S.), 1996), code from The Stack v1 (Kocetkov & et al., 2022), patents from the USPTO database (United States Patent and Trademark Office, 2024) and EuroPat (Heafield et al., 2022), mathematical problems from Deepmind Math Saxton et al. (2019), and video transcripts from both VALID (Nguyen et al., 2024) and the YouTube Commons corpus (Langlais, 2024), news and law data from the Open License Corpus (Min et al., 2024). We source 12.6% of our dataset from The Stack v1, which is permissive-first, risk-mitigated code dataset governed by the OpenRAIL-M license. We discuss its permissiveness situation in Appendix G). 2.1.3 INSTRUCTION AND REASONING DATASETS To enhance instruction-following and reasoning, we follow Abdin et al. (2024) by including considerable synthetic and web-rewrite data. We extensively use fully and partially synthetic data all generated from permissive or public-domain seed data using models under permissive licenses. General Instruction Following We include strong instruction-following baseline with the Magpie Collection (Xu et al., 2024a), its derivatives (e.g., Magpie-Phi3-Pro). This is augmented with preference data from UltraFeedback (Cui et al., 2023) and NVIDIAs SFT data blend NVIDIA (2024), which contains curated mixture of permissively licensed subsets from public datasets, 3 Preprint. Under review. including OASST (Köpf et al., 2023), CodeContests (Li et al., 2022), FLAN (Chung et al., 2022), OpenPlatypus (Lee, 2023), and the training split of GSM8K (Cobbe et al., 2021). Additionally, we augment the P3 (Sanh et al., 2021) dataset with few-shot and multiple-choice format. Reasoning To improve reasoning, we incorporate general corpora such as Glaive-AI Reasoning (AI, 2023) and OpenThoughts (Guha et al., 2025) as well domain-specific datasets: the legal dataset CaseHOLD (Zheng et al., 2021), scientific Q&A from the OpenScience collection (NVIDIA Corporation, 2025), and agent-focused instructions from OpenManus-RL (CharlieDreemur, 2024). Mathematics and Coding To strengthen quantitative reasoning, we combine our internally developed synthetic Math Word Problems dataset (Appendix E) with established datasets like MetaMathQA (Yu et al., 2023) and DM-Math (Saxton et al., 2019), further enriched with large-scale math instruction sets, including OpenMathInstruct-2 (Toshniwal et al., 2024), DART-MATH (Ge et al., 2024), Nemo-Math (Mahabadi et al., 2025), and Prism-Math (NVIDIA, 2025). For coding, we combine the Ling Coder collection Codefuse & Team (2025) with executable instructions from the StarCoder dataset Kocetkov & et al. (2022) to target wide range of software engineering tasks. (a) MixtureVitae composition by origin (total token counts at the top in billions). Each bar represents one of the six primary content domains (as in Figure 1a), segmented by source type: Non-Synthetic (organic human-written text and code), Mixed (sources with partial synthetic data), and Synthetic (data generated by permissive models from permissive seeds). (b) Legal provenance and risk-mitigation tiers of the MixtureVitae corpus. The dataset is segmented into its three constituent legal categories, with all sources falling into permissive-first or risk-mitigated tier. Token counts (billions) and total corpus percentages are shown for each category. Figure 2: Composition and provenance of MIXTUREVITAE: (a): Synthetic-status distribution across the six content domains, (b): licensing tiers and risk posture for the corpus. 2.1.4 LICENSING TIERS AND RISK PROFILES To make the provenance and legal footing of MixtureVitae transparent, we conceptualize all dataset components into tiers based on license type and expected risk profile (see Figure 2a ). Tier 1 Explicit Open Licenses & Public Domain. This tier encompasses text and code under clear permissive licenses (e.g., CC0, CC-BY, Apache 2.0, MIT, BSD) or in the public domain, such as encyclopedic resources, scientific papers, and portions of curated math corpora. Because licenses are explicit and permissive, the legal risk of reuse is minimal. This tier also includes synthetic data generated from permissively licensed models and seed data. Tier 2 Curated Permissive Repositories. This subset consists primarily of permissivelylicensed source code from GitHub, curated in projects such as THE STACK V1 and other BigCode releases. Although still permissive, we annotate them separately to reflect their provenance: repository-level heuristics identify permissive licenses, with opt-outs or known non-permissive inclusions removed. The residual risk is higher than Tier 1 due to file-level uncertainty, but still low relative to unrestricted web scrapes. 4 Preprint. Under review. Tier 3 Civic / Governmental Works. This tier includes materials that are either statutory public domain (e.g., U.S. federal works) or under strong public-purpose rationale for reuse (e.g., government websites, regulatory notices). While not always explicitly licensed, such worktypically created for disseminationis widely recognized as low-risk for inclusion. Filtering with copyright keyword checks further reduces the possibility of inadvertently including restricted content."
        },
        {
            "title": "2.2 DATA PROCESSING PIPELINE",
            "content": "To transform the raw data sources into high-quality and permissively licensed pretraining corpus, we developed multistage data processing pipeline. Our curation pipeline includes the following stages: ensuring permissive licensing, filtering for CSAM and offensive language, improving overall content quality, and reducing data redundancy. The following sections detail each component."
        },
        {
            "title": "2.2.1 PERMISSIVENESS FILTERING",
            "content": "With our permissive-first, risk-mitigated strategy, we conduct pseudo-crawl of the web corpora, prioritizing permissive sources. Specifically, we (i) apply an explicit allowlist of governmental and international domains (Appendix H.1), (ii) curate set of websites with known permissive licenses (Appendix H.2), and (iii) expand this set with risk-mitigated documents by searching for permissive license keywords (e.g., CC-BY-SA), excluding documents with restrictive terms (e.g., all rights reserved). We justify the inclusion of governmental works under strong fair-use rationale, considering their public purpose, content type, and minimal market impact (Appendix F). 2.2.2 SAFETY FILTERING We remove obscene, adult, and CSAM-related content with keyword-based blocklists adapted from prior work (Laurençon et al., 2022; Nakamura et al., 2025). For Wikipedia-based documents, we remove articles about films, sporting events, and biographies of living persons in English with applied targeted filtering, to minimize memorization of facts about people, in case of objection to incorrect facts about people being generated by models trained on MixtureVitae. Besides dataset-level filters, we also evaluate the final models safety profile via standard red-teaming (Appendix C.3). 2.2.3 QUALITY FILTERING Per standard practices (Raffel et al., 2020), we remove documents with base64-encoded text (which can disrupt training) and duplicative headers and footers (e.g., \"Home Search\") from FineFineWeb. 2.2.4 DEDUPLICATION Our deduplication strategy prioritizes diversity over purity, choice informed by recent findings in large-scale data curation. While removing exact repetitions mitigates harmful memorization (Lee et al., 2022), prior research finds that aggressive, global near-duplicate removal can be detrimental. For example, the creators of the FineWeb-Edu dataset (Lozhkov et al., 2024a) reported worsened model performance by global fuzzy deduplication, postulating that it removed too much quality data. Therefore, we adopt local-only approach. We first apply intra-dataset deduplication using prefix- (Lee et al., 2022). We intentionally based exact matching to remove verbatim boilerplate text avoid full, cross-dataset fuzzy deduplication to preserve near-duplicates (e.g., Wikipedia articles with different formatting across sources). We posit that doing so retains stylistic and domain diversity, factor shown to be helpful for model generalization (Chen et al., 2024). 2.2.5 TRAINING EXAMPLE CURATION Our process for creating training examples involves several stages: 1. Heuristic Cleaning: We remove boilerplate content by eliminating repetitive n-gram prefixes and suffixes, following standard web data cleaning pipelines (Raffel et al., 2020). Preprint. Under review. 2. Fine-grained Deduplication: To enhance data quality, we segment documents into sentences and remove duplicate sentences within each document. Documents with high internal repetition (sentence duplication rate > 75%) are discarded entirely, as this has been shown to improve model performance (Lee et al., 2022). 3. Domain-Aware Mixing: To construct the final training examples, we employ domainaware data mixing strategy (Xie et al., 2023). Documents are clustered by their base URL (a proxy for domain), and sentences are concatenated first within their original document, then packed with other documents from the same cluster. This approach is critical for effective model pre-training (Xie et al., 2023)."
        },
        {
            "title": "2.2.6 ADDITIONAL FILTERING FOR SYNTHETIC DATASETS",
            "content": "To ensure the permissiveness of our synthetic instruction datasets, we implement strict data provenance filter. We exclusively use synthetic data generated from seed prompts or data that are sourced from permissively licensed sources or public domain sources; the instructions are generated only by models that are themselves permissively licensed (e.g., under an Apache license)."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 EXPERIMENTAL SETUP To empirically validate the quality of the MixtureVitae pretraining dataset, we conduct large-scale comparative study against selection of prominent open pretraining datasets. We isolate the impact of the dataset on downstream performance using the open-sci-ref training procedure (Nezhurina et al., 2025), which enables systematic control of factors affecting benchmark scores. As in opensci-ref, we fix the model architecture (Table 3, sizes: 0.13B, 0.4B, 1.3B, 1.7B), and training hyperparameters  (Table 4)  , varying only the dataset. This design ensures that any performance difference can be attributed solely to the dataset. Also, following the numbers given in open-sci-ref, we train each model on two token budgets: 50B and 300B tokens to analyze scaling effects. Conducting separate training runs on each budget, rather than using intermediate checkpoints, ensures consistent data distribution and allows for proper optimization of learning rate schedules for each specific token budget (Hoffmann et al., 2022). The 50B token subset of MixtureVitae is uniformly sampled from the original dataset split to analyze scaling effects with respect to token count (see Appendix C.2). The 300B token budget is achieved by uniform repeated sampling. This approach follows established practices where data mixing experiments must consider intended token budget, as some data mixes that are effective at small token budgets do not generalize well to higher budgets (Albalak et al., 2023). Within this controlled evaluation framework, we compare MixtureVitae with the set of public baselines evaluated in open-sci-ref, with the addition of representative selection of permissively licensed datasets. As detailed in Table 2, the comparison set includes two groups: Non-Permissive/Mixed-License Baselines. C4 (Raffel et al., 2020), The Pile (Gao et al., 2020), SlimPajama (Shen et al., 2024), FineWeb-Edu (Lozhkov et al., 2024a), NemotronCC-HQ (Su et al., 2025), DCLM-baseline (Li et al., 2024), HPLT Monolingual Datasets v2.0 (Burchell et al., 2025); Permissive Baselines. CommonCorpus and its English subset (Pleias, 2023), as well as Comma-0.1 (Kandpal et al., 2025). All datasets are tokenized using the GPT-NeoX-20B tokenizer (Black et al., 2022), resulting in vocabulary size of 50,304. The models are trained using Megatron-LM (Shoeybi et al., 2020), and the evaluations are performed using LM Evaluation Harness (Gao et al., 2024). Model performance is evaluated on recognized downstream task benchmarks: MMLU (Hendrycks et al., 2021), COPA (Gordon et al., 2012), LAMBADA (Paperno et al., 2016), OpenBookQA (Mihaylov et al., 2018), Winogrande (Sakaguchi et al., 2020), ARC (Challenge and Easy) (Clark et al., 2018), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), and PIQA (Bisk et al., 2020). 6 Preprint. Under review. To ensure evaluation integrity, we perform comprehensive decontamination analysis against all benchmark test sets, with full details and case studies provided in Appendix D."
        },
        {
            "title": "3.2 EXPERIMENT RESULTS",
            "content": "(a) Average performance on all 10 evaluated tasks. (b) Performance on MMLU. Figure 3: Performance comparison of pretraining datasets for 1.7B-parameter model trained up to 300B token budget, showing downstream accuracy as function of the number of training tokens. At 300B-token budget, MixtureVitae shows strong scaling behavior when compared to the reference permissive dataset and is almost comparable to the non-permissive datasets (Figure 3). Overall average performance. MixtureVitae outperforming all permissive dataset baselines by significant margin, with gaps widening considerably for larger model sizes, in terms of average performance across all 10 tasks (see Figure 3a). Non-permissive datasets, particularly NemotronCC-HQ and DCLM, still achieve the highest overall performance. Approaching 300BT, MixtureVitae catches up to FineWeb-Edu and DCLM. More importantly, while the top-performing models are still trained on non-permissive datasets like Nemotron-CC-HQ and DCLM, our results demonstrate that this performance gap is no longer mandate. MixtureVitae proves that dataset built on fully permissive, risk-mitigated foundation can achieve highly competitive resultssignificantly outperforming all other permissive baselines and landing within small, practical margin of top-tier, legally-ambiguous corpora. This finding directly challenges the prevailing assumption that reliance on high-risk, indiscriminately scraped copyrighted data is prerequisite for training capable LLMs. Our findings hold also at the smaller 50B token budget scale (Appendix C.2). Performance on MMLU. As demonstrated in figure 3b, MixtureVitae performs particularly well relative to others on MMLU, where most baselines fall within near random chance throughout. Among all the baselines, only Nemotron-CC-HQ catches up to MixtureVitae at around 260B and overtakes it past that point. Performance on single tasks. We show performance on each task separately in the appendices  (Fig. 6)  . MixtureVitae outperforms other permissive datasets on MMLU, Arc Challenge, Arc Easy and BoolQ, while closely matching DCLM and FineWeb-Edu. On PIQA, HellaSwag, Winogrande, OpenBookQA, MixtureVitae is on par with Comma-0.1, while both are behind non-permissive datasets. Lambada is the only task where MixtureVitae falls behind Comma-0.1. We thus observe MixtureVitae to be particularly strong on reasoning-related tasks. 3.3 RESULTS ON PROBLEM SOLVING AND INSTRUCTION-BASED DOWNSTREAM TASKS To further demonstrate the performance of the MixtureVitae dataset, we evaluate the model on set of math, code and instruction benchmarks (GSM8k (Cobbe et al., 2021), MBPP (Chen et al., 2021), IF-Eval (Zhou et al., 2023)). The evaluation is done using the final 1.7B model checkpoints after training for 300B tokens using the open-sci-ref protocol (exact evaluation setup in Table 6). 7 Preprint. Under review. Table 1: Performance on math, code and instruct tasks for 1.7B base models, with varying pretraining token budgets and datasets. MixtureVitae has significant lead in math and code, demonstrating the effectiveness of its targeted Reasoning & Instruction data components. Training Dataset Tokens IF-Eval GSM8K HumanEval MBPP Average Models Trained with open-sci-ref for 300B Tokens MixtureVitae 300B 300B Comma-0.1 300B CommonCorpus 300B C4 300B SlimPajama 300B HPLT-2.0 DCLM 300B Nemotron-CC-HQ 300B 0.19 0.19 0.13 0.20 0.14 0.17 0.13 0.09 0.53 0.06 0.02 0.02 0.02 0.02 0.02 0. Models Trained with open-sci-ref for 1T Tokens FineWeb-Edu 1T Nemotron-CC-HQ 1T 1T DCLM Other Models 0.20 0.13 0.15 0.03 0.03 0.03 0.32 0.13 0.05 0.00 0.05 0.00 0.01 0. 0.00 0.01 0.00 0.38 0.22 0.05 0.00 0.00 0.00 0.01 0.00 0.00 0.04 0.01 0.36 0.15 0.06 0.06 0.05 0.05 0.04 0.03 0.06 0.05 0.05 SmolLM2-1.7B 11T 0.18 0.31 0.01 0.35 0. The results  (Table 1)  show dramatic difference on math (GSM8K) and coding (MBPP). MixtureVitae achieves scores of 0.53 and 0.38, respectively. This performance is an order of magnitude higher than any other dataset, all of which fail to show any meaningful coding ability (0.00 on MBPP) and remain near random performance on math (0.02-0.03 on GSM8K). Remarkably, these scores outperform SmolLM2 base model, which uses 11T tokens and multi-stage training procedure. On the general instruction-following task, IF-Eval, the results are more comparable. C4 achieves the highest score (0.20), with MixtureVitae (0.19) and HPLT-2.0 (0.17) performing similarly. To validate these high scores, we perform an extensive decontamination analysis (Appendix D.3). Our performance is maintained when re-evaluating on \"clean\" test sets with all overlapping examples removed  (Table 9)  , confirming our results are not an artifact of test set leakage. 3.4 ABLATIONS To isolate the impact of primary data components in MixtureVitae, we define Web and Instructions subsets (see Figure 1; Instructions encompasses Reasoning & Instruction and Math parts of full mixture) and conduct an ablation study on 100B-token scale. We train three separate models: (1) MixtureVitae (full), the complete dataset; (2) MixtureVitae (w/o Web), removing the Web component; (3) MixtureVitae (w/o Instructions), removing the Instructions component. The average downstream performance of these models (Figure 4a) shows varying contributions by each component: The Instructions data is the most critical driver of performance, as its removal results in the largest, consistent drop of average performance compared to other configurations. Removing Instructions particularly leads to severe drop on GSM8k (from 0.47 to 0.03) and MBPP, as shown in Tab. 4b. Absent the Instructions data, the model fails to match the gains of the full mix, underscoring the essential role of instruction-following data in generalization. Removing the Web component (w/o Web, blue dashed line) also results in performance drop below the full dataset, albeit less dramatically. Tab. 4b shows drop from 0.47 to 0.41 on GSM8k, far less severe than the drop close to 0 for w/o Instructions and only slight changes on code evals. The comparison of ablation effects again highlights the crucial role of Instructions data."
        },
        {
            "title": "4 RELATED WORK",
            "content": "LLM development is intrinsically linked to the scale and quality of pretraining datasets, which have become larger, more diverse, with growing emphasis on provenance and licensing recently. 8 Preprint. Under review. Training Dataset IF-Eval GSM8K MBPP Average MixtureVitae MixtureVitae (w/o Web) MixtureVitae (w/o Instructions) 0.14 0.18 0.19 0.47 0.41 0. 0.34 0.33 0.14 0.25 0.25 0. (b) performance breakdown on math, coding and instruction following tasks for the ablated dataset variants. Best results are in bold. (a) Ablation on full MixtureVitae against two versions, each without main component. Average performance on 10 downstream tasks. Figure 4: An ablation study on components of the MixtureVitae dataset. Figure 4a shows performance average on 10 downstream evals during training, while Table 4b shows scores on further math, code and instruction benchmarks. The evaluation setup is given in Table 6. 4.1 PIONEERING LARGE-SCALE DATASETS Early large-scale text corpora for language modeling often rely on web-crawled data for scale. C4 (Raffel et al., 2020), derived from Common Crawl, is instrumental in training the T5 model, setting standards for large-scale data cleaning and deduplication. Gao et al. (2020) then introduce The Pile, demonstrating the benefit of more varied data mixture on model generalization and downstream performance. Similarly, ROOTS (Laurençon et al., 2022) supports the training of the BLOOM model with its 498 Common Crawl multilingual scrapes. While foundational, these datasets often have complex or unspecified licenses, mixing permissive data with content of unknown or non-commercial licensing, creating potential legal risks for commercial applications. 4.2 OPEN AND REPRODUCIBLE DATASETS Amidst many proprietary \"black box\" datasets, the community has pushed for more openness and reproducibility, moving toward permissive datasets that are also performant, e.g., RedPajama-1T (Weber et al., 2024) and its processing recipes (Touvron et al., 2023), Dolma (Soldaini et al., 2024) and its open-source curation toolkits, SILO (Min et al., 2024). Our work joins this effort, contributing new risk-mitigated dataset featuring explicit consideration for the underlying copyright. 4.3 PERMISSIVELY LICENSED AND SYNTHETIC DATA Growing awareness of copyright and data ownership has spurred interest in datasets built solely from permissively licensed materials. The Stack (Kocetkov et al., 2022) curates such data for codegeneration models, but creating large, diverse, and high-quality corpus for natural language from exclusively permissive sources remains challenge. Recent efforts like Common Corpus (Langlais et al., 2025) and The Common Pile (Kandpal et al., 2025) advance the creation of large-scale corpora of permissively licensed and public-domain text. While foundational, our experiments (Section 3) show that models trained on them can lag in complex reasoning, math, and instruction following, suggesting that strictly permissive human text alone is insufficient to instill these advanced skills. With this scarcity of high-quality reasoning and instruction data, researchers have turned to synthetic data. Alpaca (Taori et al., 2023) and OpenMathInstruct-1 (Toshniwal et al., 2024) use instructional data for fine-tuning. Phi4 proposes using synthetic data for reasoning tasks (Abdin et al., 2024). Our work, MixtureVitae, extends these trends with meticulously curated, permissive-first, riskmitigated dataset augmented with targeted synthetic data, providing strong, legally considered foundation for LLM training to mitigate copyright risks in many existing corpora. 9 Preprint. Under review. We note the concurrent work of Apertus and its permissive recipe (Hernández-Cano et al., 2025), which is unavailable for comparison before submission. For comparison of MixtureVitae to various permissive and non-permissive baselines, see Table 2."
        },
        {
            "title": "5 DISCUSSION & CONCLUSION",
            "content": "We have presented MixtureVitae, large pretraining corpus serving as proof-of-concept: permissively licensed and permissively-sourced synthetic data can achieve high performance. Our experiments confirm that models trained on MixtureVitae not only outperform other permissive datasets but also narrow the performance gap with leading models trained on proprietary or non-permissive data. On math and code tasks, MixtureVitae-trained base model matches or outperforms even base models like SmolLM2, despite their use of significantly larger compute and sophisticated multistage pretraining on non-permissive data. This outcome, together with the performed ablations, validates our data curation strategy and is consistent with the findings of Abdin et al. (2024). It demonstrates that the inclusion of large fraction of reasoning, math, and instruction databoth real and syntheticin the pretraining can already instill skills that are clearly absent when training on standard web-scrape corpora. Our work thus provides path towards creating scalable datasets based on permissive sources, without having to sacrifice performance as compared with non-permissive ones."
        },
        {
            "title": "6 REPRODUCIBILITY STATEMENT",
            "content": "We release our code at https://github.com/ontocord/mixturevitae. 6.1 DATASET AND CURATION RECIPES Public Release: The full 211.1B token dataset, along with the 100B and 50B subsets used for scaling ablations experiments, will be made publicly available upon acceptance of this paper. Curation Methodology: Dataset Composition The detailed list of sources and their composition are shown in Figure 5. Code: We are including our data curation and math word problem generation scripts with the submission. 6.2 TRAINING PROCEDURE To ensure our experiments are directly comparable and reproducible, we adhered to controlled, public framework. Framework: All experiments were conducted using the open-sci-ref training procedure (Nezhurina et al., 2025), which standardizes key factors affecting performance. Architectures: The exact model architectures for all four scales (0.13B, 0.4B, 1.3B, 1.7B) are detailed in Table 3. Hyperparameters: The complete training schedules and hyperparameters (learning rate, batch size, warmup, etc.) for both the 50B and 300B token budgets are specified in Table B.1. Software: Models were trained using Megatron-LM (Shoeybi et al., 2020) with the GPTNeoX-20B tokenizer(Black et al., 2022). Code: We are including our training script with the submission. 6.3 EVALUATION AND ANALYSIS Our evaluation protocol is fully specified to allow for independent verification of our results. Framework: All general and reasoning task evaluations were performed using the public LM Evaluation Harness (Gao et al., 2024). 10 Preprint. Under review. Settings: The exact settings for each benchmark, including the number of few-shot examples, are provided in Table 5 and Table 6. Decontamination: Our 13-gram decontamination protocol is detailed in Appendix D. Code: We are including our evaluation and decontamination scripts with the submission. While model checkpoints and training logs are not included in the initial submission due to size and anonymity constraints, we plan to release these upon publication to facilitate future research."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "Huu Nguyen is thankful for and acknowledges discussions with Robert Kaczmarczyk on the ethical implications of training data; Colin Raffel on the inclusion of large scale instruction data in pretraining; Stella Biderman on various best practices for permissive datasets; Wojciech Kusa and the members of NASK National Research Institute on data safety; Veronika Laippala and Sampo Pyysalo of the University of Turku for their advice, and he espeically thanks his wife Thao Tran for her enduring support. Marianna Nezhurina, David Salinas and Jenia Jitsev acknowledge co-funding by EU from Digital Europe Programme under grant no. 101195233 (openEuroLLM). Marianna Nezhurina and Jenia Jitsev acknowledge co-funding from EuroHPC Joint Undertaking programm under grant no. 101182737 (MINERVA), funding by the Federal Ministry of Education and Research of Germany (BMBF) under grant no. 01IS24085C (OPENHAFM), under the grant 01IS22094B (WestAI - AI Service Center West), and under the grant 16HPC117K (MINERVA). We gratefully acknowledge the Gauss Centre for Supercomputing e.V. for funding this work by providing computing time through the John von Neumann Institute for Computing (NIC) on the supercomputer JUWELS Booster at Jülich Supercomputing Centre (JSC), EuroHPC Joint Undertaking for computing time and storage on the EuroHPC supercomputer LEONARDO, hosted by CINECA (Italy) and the LEONARDO consortium through an EuroHPC Extreme Access grant EHPC-EXT2023E02-068 and through EuroHPC AI Factory Large Scale Access grant EHPC-AIF-2025LS01028, storage resources on JUST granted and operated by JSC and supported by Helmholtz Data Federation (HDF), computing time granted by the JARA and JSC on the supercomputer JURECA at JSC. We thank Robert Kaczmarczyk for coordination and support of EuroHPC Extreme Scale grant applications. Further thanks go for support provided by supercomputing facilities and their teams, especially to Damian Alvarez and Mathis Bode from Juelich Supercomputer Center (JSC, Germany) and to Laura Morselli from CINECA (Italy). We also would like to thank all the members of the Ontocord1, LAION 2, and Open-Ψ (open-sci) communities3 for providing fertile ground for scientific exchange and open-source development."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. Phi-4 technical report, 2024. URL https://arxiv.org/abs/2412. 08905. Glaive AI. reasoning dataset. glaiveai/reasoning-v1-20m, 2023. Glaive-ai https://huggingface.co/datasets/ Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online data mixing for language model pre-training, 2023. URL https://arxiv.org/abs/2312.02406. 1https://huggingface.co/datasets/ontocord 2https://discord.gg/BZqhreFazY 3https://discord.gg/GsKh4mBVcv 11 Preprint. Under review. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Samuel Barham, , Weller, Michelle Yuan, Kenton Murray, Mahsa Yarmohammadi, Zhengping Jiang, Siddharth Vashishtha, Alexander Martin, Anqi Liu, Aaron Steven White, Jordan Boyd-Graber, and Benjamin Van Durme. Megawika: Millions of reports and their sources across 50 diverse languages, 2023. Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Prasoon Varshney, Makesh Narsimhan, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi Mahabadi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury, Omri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Shaona Ghosh, Katherine Luna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo Ribalta, Monika Katariya, Chris Alexiuk, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala Prayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan Catanzaro, Jonah Alben, Yonatan Geifman, and Eric Chung. Llama-nemotron: Efficient reasoning models, 2025. URL https://arxiv.org/abs/2505.00949. Yonatan Bisk, Rowan Zellers, Ronan Lebras, Jian Gau, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. Gpt-neox-20b: An open-source autoregressive language model, 2022. URL https://arxiv.org/abs/2204. 06745. Michael Bommarito II, Jillian Bommarito, and Daniel Martin Katz. The kl3m data project: Copyright-clean training resources for large language models, 2025. URL https://arxiv. org/abs/2504.07854. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165. Laurie Burchell, Ona de Gibert, Nikolay Arefyev, Mikko Aulamo, Marta Bañón, Pinzhen Chen, Mariia Fedorova, Liane Guillou, Barry Haddow, Jan Hajiˇc, Jindˇrich Helcl, Erik Henriksson, Mateusz Klimaszewski, Ville Komulainen, Andrey Kutuzov, Joona Kytöniemi, Veronika Laippala, Petter Mæhlum, Bhavitvya Malik, Farrokh Mehryary, Vladislav Mikhailov, Nikita Moghe, Preprint. Under review. Amanda Myntti, Dayyán OBrien, Stephan Oepen, Proyag Pal, Jousia Piha, Sampo Pyysalo, Gema Ramírez-Sánchez, David Samuel, Pavel Stepachev, Jörg Tiedemann, Dušan Variš, Tereza Vojtˇechová, and Jaume Zaragoza-Bernabeu. An expanded massive multilingual dataset for high-performance language technologies (HPLT). In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1745217485, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176251-0. doi: 10.18653/v1/2025.acl-long.854. URL https://aclanthology.org/2025. acl-long.854/. CharlieDreemur. OpenManus-RL Dataset. CharlieDreemur/OpenManus-RL, 2024. https://huggingface.co/datasets/ Hao Chen, Abdul Waheed, Xiang Li, Yidong Wang, Jindong Wang, Bhiksha Raj, and Marah I. Abdin. On the diversity of synthetic data and its impact on training large language models, 2024. URL https://arxiv.org/abs/2410.15226. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022. Gabriele Cicero and et al. Nemotron-4 340B Technical Report. Technical report, NVIDIA, 2024. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT), pp. 29242936, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018. Colin Clement, Reid Bier, McMahon, et al. On the use of arxiv as dataset. Frontiers in Big Data, 2:5, 2019. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Codefuse and Ling Team. Every sample matters: Leveraging mixture-of-experts and high-quality data for efficient and accurate code llm, 2025. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, XiaoNi Ding, Yitos Budiman, Jiaju Lin, Zedong Wang, Yu-Chen Mack, et al. Ultrafeedback: Boosting language models with highquality feedback, 2023. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. URL https://zenodo.org/records/12608602. 13 Preprint. Under review. Zhenghao Ge, Jun-Jie Huang, Zihan Wu, Chen-Cheng Fan, Ke-Jia Chen, and Dan-Feng Zhang. Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving, 2024. Andrew S. Gordon, Cosmin Adrian Bejan, and Jerry R. Hobbs. Copa: corpus for reading comprehension of commonsense causal reasoning. In Proceedings of the AAAI Workshop on Causal Reasoning, 2012. Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, and Benjamin Feuer. Openthoughts: Data recipes for reasoning models, 2025. Xintong Hao, Ruijie Zhu, Ge Zhang, Ke Shen, and Chenggang Li. Reformulation for pretraining data augmentation, 2025. URL https://arxiv.org/abs/2502.04235. Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. Toxigen: large-scale machine-generated dataset for adversarial and implicit hate speech detection, 2022. URL https://arxiv.org/abs/2203.09509. Kenneth Heafield, Elaine Farrow, Jelmer van der Linde, Gema Ramírez-Sánchez, and Dion Wiggins. The EuroPat corpus: parallel corpus of European patent data. In Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the Thirteenth Language Resources and Evaluation Conference, pp. 732740, Marseille, France, June 2022. European Language Resources Association. URL https://aclanthology.org/2022.lrec-1.78/. Dan Hendrycks, Collin Burns, Steven Basart, Andy Critch, Jerry Li, Dawn Ippolito, Aina Lapedriza, Florian Tramer, Rylan Macfarlane, Eric Jiang, et al. Measuring massive multitask language understanding. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. Alejandro Hernández-Cano, Alexander Hägele, Allen Hao Huang, Angelika Romanou, AntoniJoan Solergibert, Barna Pasztor, Bettina Messmer, Dhia Garbaya, Eduard Frank ˇDurech, Ido Hakimi, Juan García Giraldo, Mete Ismayilzada, Negar Foroutan, Skander Moalla, Tiancheng Chen, Vinko Sabolˇcec, Yixuan Xu, Michael Aerni, Badr AlKhamissi, Ines Altemir Marinas, Mohammad Hossein Amani, Matin Ansaripour, Ilia Badanin, Harold Benoit, Emanuela Boros, Nicholas Browning, Fabian Bösch, Maximilian Böther, Niklas Canova, Camille Challier, Clement Charmillot, Jonathan Coles, Jan Deriu, Arnout Devos, Lukas Drescher, Daniil Dzenhaliou, Maud Ehrmann, Dongyang Fan, Simin Fan, Silin Gao, Miguel Gila, María Grandury, Diba Hashemi, Alexander Hoyle, Jiaming Jiang, Mark Klein, Andrei Kucharavy, Anastasiia Kucherenko, Frederike Lübeck, Roman Machacek, Theofilos Manitaras, Andreas Marfurt, Kyle Matoba, Simon Matrenok, Henrique Mendoncça, Fawzi Roberto Mohamed, Syrielle Montariol, Luca Mouchel, Sven Najem-Meyer, Jingwei Ni, Gennaro Oliva, Matteo Pagliardini, Elia Palme, Andrei Panferov, Léo Paoletti, Marco Passerini, Ivan Pavlov, Auguste Poiroux, Kaustubh Ponkshe, Nathan Ranchin, Javi Rando, Mathieu Sauser, Jakhongir Saydaliev, Muhammad Ali Sayfiddinov, Marian Schneider, Stefano Schuppli, Marco Scialanga, Andrei Semenov, Kumar Shridhar, Raghav Singhal, Anna Sotnikova, Alexander Sternfeld, Ayush Kumar Tarun, Paul Teiletche, Jannis Vamvas, Xiaozhe Yao, Hao Zhao Alexander Ilic, Ana Klimovic, Andreas Krause, Caglar Gulcehre, David Rosenthal, Elliott Ash, Florian Tramèr, Joost VandeVondele, Livio Veraldi, Martin Rajman, Thomas Schulthess, Torsten Hoefler, Antoine Bosselut, Martin Jaggi, and Imanol Schlag. Apertus: Democratizing open and compliant llms for global language environments, 2025. URL https://arxiv.org/abs/2509.14233. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/ 2203.15556. 14 Preprint. Under review. Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and Madian Khabsa. Llama guard: Llmbased input-output safeguard for human-ai conversations, 2023. URL https://arxiv.org/ abs/2312.06674. Nikhil Kandpal, Brian Lester, Colin Raffel, Sebastian Majstorovic, Stella Biderman, Baber Abbasi, Luca Soldaini, Enrico Shippole, A. Feder Cooper, Aviya Skowron, John Kirchenbauer, Shayne Longpre, Lintang Sutawika, Alon Albalak, Zhenlin Xu, Guilherme Penedo, Loubna Ben Allal, Elie Bakouch, John David Pressman, Honglu Fan, Dashiell Stander, Guangyu Song, Aaron Gokaslan, Tom Goldstein, Brian R. Bartoldson, Bhavya Kailkhura, and Tyler Murray. The common pile v0.1: An 8tb dataset of public domain and openly licensed text, 2025. URL https://arxiv.org/abs/2506.05209. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scaling laws for neural language models. Scott Gray, Alec Radford, and Dario Amodei. arXiv:2001.08361 [cs.LG], 2020. Denis Kocetkov and et al. The Stack: 3 TB of Permissively Licensed Source Code. In Proceedings of the Thirty-ninth International Conference on Machine Learning, 2022. Dataset available at https://huggingface.co/datasets/bigcode/the-stack. Denis Kocetkov, Raymond Li, Loubna Ben Allal, et al. The stack: 3 tb of permissively licensed source code, 2022. Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich ard Nagy, et al. Openassistant conversationsdemocratizing large language model alignment. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. Pierre-Carl Langlais. Releasing youtube-commons: massive open corpus for conversational and multimodal data. Hugging Face blog, April 2024. URL https://huggingface.co/blog/ Pclanglais/youtube-commons. Pierre-Carl Langlais, Carlos Rosas Hinostroza, Mattia Nee, Catherine Arnett, Pavel Chizhov, Eliot Krzystof Jones, Irène Girard, David Mach, Anastasia Stasenko, and Ivan P. Yamshchikov. Common corpus: The largest collection of ethical data for llm pre-training, 2025. URL https://arxiv.org/abs/2506.01732. Hugo Laurençon, Lucile Saulnier, Thomas Wang, et al. The bigscience roots corpus: 1.6tb composite multilingual dataset. In Advances in Neural Information Processing Systems, 2022. Ariel Lee. Open-platypus. https://huggingface.co/datasets/garage-bAInd/ Open-Platypus, 2023. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris CallisonBurch, and Nicholas Carlini. Deduplicating training data makes language models better, 2022. Mark Lemley and Bryan Casey. Fair learning. Texas Law Review, 95:1, 2017. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. Advances in Neural Information Processing Systems, 37:1420014282, 2024. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Dadashi, Viorica Fidjeland, Demis Hassabis, Oriol Vinyals, et al. Competition-level code generation with alphacode. In International Conference on Machine Learning, pp. 1275712783. PMLR, 2022. Varvara Logacheva, Daryna Dementieva, Sergey Ustyantsev, Daniil Moskovskiy, David Dale, Irina Krotova, Nikita Semenov, and Alexander Panchenko. ParaDetox: Detoxification with parallel data. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 68046818, Dublin, Ireland, May 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.acl-long.469. Preprint. Under review. Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu: the finest collection of educational content, 2024a. URL https://huggingface.co/datasets/ HuggingFaceFW/fineweb-edu. Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. FineWeb-Edu: the finest collection of educational content. https://huggingface.co/datasets/ HuggingFaceFW/fineweb-edu, 2024b. Rabeeh Karimi Mahabadi, Sanjeev Satheesh, Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc-math: 133 billion-token-scale high quality math pretraining dataset, 2025. Thomas Margoni. The text and data mining exception in the eu copyright directive: game changer for data-driven research and artificial intelligence? SCRIPTed, 16(2):196226, 2019. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 23812391, 2018. Sewon Min, Suchin Gururangan, Eric Wallace, Weijia Shi, Hannaneh Hajishirzi, Noah A. Smith, and Luke Zettlemoyer. Silo language models: Isolating legal risk in nonparametric datastore, 2024. URL https://arxiv.org/abs/2308.04430. Taishi Nakamura, Mayank Mishra, Simone Tedeschi, Yekun Chai, Jason T. Stillerman, Felix Friedrich, Prateek Yadav, Tanmay Laud, Vu Minh Chien, Terry Yue Zhuo, Diganta Misra, Ben Bogin, Xuan-Son Vu, Marzena Karpinska, Arnav Varma Dantuluri, Wojciech Kusa, Tommaso Furlanello, Rio Yokota, Niklas Muennighoff, Suhas Pai, Tosin Adewumi, Veronika Laippala, Xiaozhe Yao, Adalberto Barbosa Junior, Aleksandr Drozd, Jordan Clive, Kshitij Gupta, Liangyu Chen, Qi Sun, Ken Tsui, Nour Moustafa-Fahmy, Nicolo Monti, Tai Dang, Ziyang Luo, Tien-Tung Bui, Roberto Navigli, Virendra Mehta, Matthew Blumberg, Victor May, Hiep Nguyen, and Sampo Pyysalo. Aurora-M: Open source continual pre-training for multilingual language and code. In Proceedings of the 31st International Conference on Computational Linguistics: Industry Track, Abu Dhabi, UAE, 2025. Association for Computational Linguistics. National Library of Medicine (U.S.). Pubmed. https://pubmed.ncbi.nlm.nih.gov/, 1996. Database launched 1996. Accessed: [Date Accessed, e.g., 13-Sep-2025]. Marianna Nezhurina, Joerg Franke, Taishi Nakamura, Timur Carstensen, Niccolò Ajroldi, Ville Komulainen, David Salinas, and Jenia Jitsev. Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison. arXiv:2509.09009, 2025. Huu Nguyen, Ken Tsui, Andrej Radonjic, Harsh Raj, and Christoph Schuhmann. Valid (videoaudio large interleaved dataset), 2024. URL https://huggingface.co/datasets/ ontocord/VALID. NVIDIA. SFT DataBlend v1. https://huggingface.co/datasets/nvidia/sft_ datablend_v1, 2024. NVIDIA. Nemotron-PrismMath Dataset. nvidia/Nemotron-PrismMath, 2025. https://huggingface.co/datasets/ NVIDIA Corporation. OpenScience Dataset, 2025. URL https://huggingface.co/ datasets/nvidia/OpenScience. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), 2016. Pleias. Common corpus. https://thealliance.ai/blog/ pleias-releases-common-corpus-open-multilingual-dataset-for-llm-training, 2023. 16 Preprint. Under review. Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Laurent Sifre, Tom Noland, GOR Young, Ankur Bapna, Ian Caswell, et al. Scaling language models: Methods, analysis & insights from training Gopher. In arXiv preprint arXiv:2112.11446, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial winograd schema challenge at scale. In Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization, 2021. David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557, 2019. Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Zhengzhong Liu, Hongyi Wang, Bowen Tan, Joel Hestness, Natalia Vassilieva, Daria Soboleva, and Eric Xing. Slimpajama-dc: Understanding data combinations for llm training, 2024. URL https://arxiv.org/abs/2309. 10818. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020. URL https://arxiv.org/abs/1909.08053. Luca Soldaini and Kyle Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technical report, Allen Institute for AI, 2023. ODC-By, https://github.com/allenai/pes2o. Luca Soldaini, Rodney Kinney, Akshita Bhagia, et al. Dolma: an open corpus of 3 trillion tokens for language model pretraining, 2024. Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset, 2025. URL https://arxiv.org/abs/2412. 02595. Liping Tang, Nikhil Ranjan, Omkar Pangarkar, Xuezhi Liang, Zhen Wang, Bhaskar Rao Li An, Linghao Jin, Huijuan Wang, Zhoujun Cheng, Cun Mu Suqi Sun, Xuezhe Ma Victor Miller, Yue Peng, Zhengzhong Liu, and Eric Xing. TxT360: Top-Quality LLM Pre-training Dataset Requires the Perfect Blend. 2024. URL https://huggingface.co/spaces/LLM360/ TxT360. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, et al. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. The European Parliament and the Council of the European Union. Directive (EU) 2019/790 of the European Parliament and of the Council of 17 April 2019 on copyright and related rights in the Digital Single Market, 2019. 130/92. Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, et al. Openmathinstruct-1: 1.8 million math instruction tuning dataset, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models, 2023. 17 Preprint. Under review. United States Congress. 17 U.S. Code 107. Limitations on exclusive rights: Fair use. U.S. Government, 1976. Accessed: 2025-08-25. United States Patent and Trademark Office. USPTO Patent Public Data Sets. https:// developer.uspto.gov/product/patent-public-data-sets, 2024. U.S. Securities and Exchange Commission. EDGAR: Electronic Data Gathering, Analysis, and Retrieval System. https://www.sec.gov/edgar, 2024. Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. Do-not-answer: dataset for evaluating safeguards in llms, 2023. URL https://arxiv.org/abs/2308. 13387. Maurice Weber, Dan Fu, Quentin Anthony, et al. Redpajama: an open dataset for training large language models. In Advances in neural information processing systems, 2024. Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V. Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining, 2023. Yijiong Xu, Zihan Wang, Jingyu Hu, Ziqing Yang, Chengyue Wu, Han Yu, Heng Ji, and Yang Liu. Magpie: Alignment data synthesis from scratch by prompting aligned models, 2024a. Yuxuan Xu, Zirui Wang, Ke Zhang, Chen Li, and Song Zhang. MagaCorpus: Large-Scale MulIn Proceedings of the 2024 Joint tilingual Dataset by Combining Publicly Available Corpora. International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pp. 1348813497, 2024b. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models, 2023. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 47914800, 2019. Ge Zhang, Xinrun Du, Zhimiao Yu, Zili Wang, Zekun Wang, Shuyue Guo, Tianyu Zheng, Kang Zhu, Jerry Liu, Shawn Yue, Binbin Liu, Zhongyuan Peng, Yifan Yao, Jack Yang, Ziming Li, Bingni Zhang, Minghao Liu, Tianyu Liu, Yang Gao, Wenhu Chen, Xiaohuan Zhou, Qian Liu, Taifeng Wang, Wenhao Huang, and The M-A-P Team. FineFineWeb: Comprehensive Study on Fine-grained Domain Web Corpus, December 2024. URL https://huggingface.co/ datasets/m-a-p/FineFineWeb. Lucia Zheng, Neel Guha, Daniel E. Ho, Julian Nyarko Dahl, and Brian Chary. CaseHOLD: new dataset for legal reasoning in contract law. In Proceedings of the 18th International Conference on Artificial Intelligence and Law, pp. 326330, São Paulo, Brazil, June 2021. Association for Computing Machinery. URL https://aclanthology.org/2021.icail-1.35. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models, 2023. Preprint. Under review. Appendix: MixtureVitae Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive Text Sources"
        },
        {
            "title": "A DATASET COMPOSITION AND COMPARISON",
            "content": "This appendix provides detailed view of the MixtureVitae corpus, both in relation to other datasets and in its internal construction. Table 2: Comparison of large-scale pretraining datasets, grouped by their licensing philosophy to provide context for our performance results. MixtureVitae is unique in its combination of riskmitigated licensing approach and the inclusion of synthetic data. Dataset Size (Tokens) Primary Data Types Licensing Philosophy Non-Permissive / Mixed-License Baselines Nemotron-cc-hq (Su et al., 2025) DCLM-baseline (Li et al., 2024) Fineweb-edu (Lozhkov et al., 2024b) 1.1T 3.8T 1.3T Web, Synthetic Unspecified Web, Code, Academic Mixed / Unspecified Web (Educational) Unspecified The Pile (Gao et al., 2020) 183.28B Web, Books, Code Mixed / Unspecified SlimPajama (Shen et al., 2024) C4 (Raffel et al., 2020) 627B 156B Web, Books, Code Mixed / Unspecified Web ODC-BY HPLT-2.0 (eng.) (Burchell et al., 2025) 2.86T Web, Books, News Mixed / Unspecified Permissive Baselines CommonCorpus (Pleias, 2023) Comma-0.1 (Kandpal et al., 2025) KL3M (Bommarito II et al., 2025) OLC (Min et al., 2024) 2T 1T 580B 228B Web, Curated Web, Curated Web, Curated Web, Curated Strictly Permissive Strictly Permissive Strictly Permissive Strictly Permissive Our Contribution MixtureVitae Permissive MixtureVitae 200B 211.1B Web, Curated, Synthetic Web, Curated, Synthetic Strictly Permissive Strictly Permissive + Risk-Mitigated Table 2 presents high-level comparison of MixtureVitae against the other prominent pretraining datasets evaluated in our experiments, detailing their respective sizes, primary data types, and licensing philosophies. Figure 5 presents the detailed composition of the MixtureVitae dataset. The individual components are color-coded by their primary dataset category, as presented in the main text. Code & Tech (Blue): This domain is anchored by our largest code sources, Stack V1 and Ling-Coder, and supplemented by StackExchange. Reasoning & Instruction (Green): The largest contributor to this category is Open Thoughts , followed by P3 and NVIDIA OpenScience. Encyclopedic, Papers & Books (Purple): This category is dominated by Wikipedia, the single largest component in the dataset. It is complemented by large-scale text from PubMed and arXiv. Math (Cyan): The math component is diverse mixture of sources, led by the Math and Science (Nemotron) corpus and Prism-Math. Web (Yellow): Our web data is primarily sourced from corpora such as SEC Filings, MegaCorpus, and FineFineWeb. Misc Curated (Pink): This category includes variety of high-quality curated sources, notably Law (Open License Corpus) and YouTube Transcriptions. 19 Preprint. Under review. Figure 5: Detailed composition of the MixtureVitae dataset."
        },
        {
            "title": "B EXPERIMENT SETUP DETAILS",
            "content": "To ensure full reproducibility, this appendix details the complete experimental setup. This includes the model architectures for all scales, the training hyperparameters for both 50B and 300B token budgets, and the specific settings used for all general evaluation benchmarks. Table 3: open-sci-ref (Nezhurina et al., 2025) model architecture and scales. We used tied embedding weights in all experiments. Parameters (B) (Non-Emb + Emb) 0.1 + 0.03 = 0.13 0.35 + 0.05 = 0.40 1.21 + 0.10 = 1. 1.61 + 0.10 = 1.71 Layers Hidden Heads 22 22 24 512 1024 2048 2048 8 32 32 FFN Hidden 2256 3840 8192 Memory FLOPs 0.89 GB 7.8 108 2.4 109 2.88 GB 7.544 GB 7.9 109 9.884 GB 1.0 1010 Table 4: The training schedules used in our experiments. Tokens Global Batch Size (tokens) Iterations 50B 300B 4.12M 4.12M 11,921 72,661 Learning Rate 4 103 4 103 Warmup Cooldown (20%) 1,000 25,000 2,384 14,532 B.1 TRAINING SETUP PARAMETERS This appendix details the exact model architectures and training hyperparameters used for all experiments, ensuring full reproducibility. We adopt the standard architectures and scales from the open-sci-ref framework to allow for fair and direct comparison against other published baselines. All models were trained with tied embedding weights. Model Architecture Table 3 defines the four model scales used in our study. The columns are defined as follows: 20 Preprint. Under review. Parameters (B) (Non-Emb + Emb) The total model parameters in billions, separated into NonEmbedding (Non-Emb) parameters (the core transformer blocks) and Embedding (Emb) parameters (the token lookup tables). As noted in the caption, we used tied embedding weights. Layers The total number of transformer blocks stacked in the model. Hidden The hidden size (or embedding dimension, dmodel) of the model. Heads The number of attention heads in the multi-head attention mechanism. FFN Hidden The inner dimension of the Feed-Forward Network (FFN) layer within each transformer block. Memory The approximate VRAM required to store the model weights, in bfloat16. FLOPs An approximation of the training compute cost using the 6N rule: standard estimate for transformers forward-and-backward pass, where is the number of non-embedding parameters (Kaplan et al., 2020). Training Schedules Table 4 defines the training hyperparameters for our two main experimental runs (50B and 300B tokens). We use single stage training with no post-training. Tokens The total number of tokens in the training run. Global Batch Size (tokens) The total number of tokens processed in single training step (i.e., one gradient update) across all GPUs. Iterations The total number of training steps. Learning Rate The peak learning rate used during training. Warmup The number of initial iterations (steps) over which the learning rate linearly increases from 0 to its peak value. Cooldown (20%) The number of final iterations (the last 20% of training) over which the learning rate decays to zero. B.2 EVALUATION SETTINGS We used the lm-evaluation-harness (Gao et al., 2024) for all general evaluations. The specific tasks and few-shot counts are detailed in Table 5. The settings for the reasoning tasks (e.g., GSM8K, IFEval) are listed separately in Table6. Table 5: General evaluation benchmark settings. All tasks use Accuracy as the primary metric. Task Citation # of Shots Hendrycks et al. (2021) Zellers et al. (2019) MMLU HellaSwag ARC-Challenge Clark et al. (2018) Clark et al. (2018) ARC-Easy Bisk et al. (2020) PIQA Clark et al. (2019) BoolQ Sakaguchi et al. (2020) Winogrande Mihaylov et al. (2018) OpenBookQA Gordon et al. (2012) COPA Paperno et al. (2016) LAMBADA 5 10 10 10 10 10 0 0"
        },
        {
            "title": "C ADDITIONAL EXPERIMENTS",
            "content": "This appendix provides additional experimental results to supplement the findings presented in the main paper. We offer more granular breakdown of the 300B token experiment, analyze performance at smaller 50B token scale to assess the generalization of our results, and report the results of model red-teaming analysis to evaluate the models safety profile. 21 Preprint. Under review. Table 6: Evaluation settings for reasoning tasks. All tasks use Accuracy as the primary metric. To execute the evaluation, we used LM Evaluation Harness Gao et al. (2024)."
        },
        {
            "title": "Citation",
            "content": "# of Shots GSM8k IFEval MBPP Cobbe et al. (2021) Zhou et al. (2023) Austin et al. (2021) 4 0 4 C.1 300B EXPERIMENT - DETAILED RESULTS The detailed results for each evaluated task are given in Figure 6. Figure 6: Comparing performance of 1.7B model trained on MixtureVitae and baseline datasets for 300B token budget. While some evaluations provide clear dataset rankings (e.g. ARC, Hellaswag, Lambada), others do not provide good signal for dataset comparison, on an individual basis. C.2 PERFORMANCE AT 50B TOKENS SCALE. Figure 7: Average performance of permissive datasets after 50B training tokens. MixtureVitae shows an early and consistent lead at larger model scales. To assess performance on smaller reference tokens scale, we also evaluated models trained on 50B token subset of each dataset. The results, shown in Figure 7 and Figure 8, indicate that the advantages of MixtureVitae manifest already at the smaller token scales. Figure 7 shows that MixtureVitae establishes consistent performance lead over other permissive datasets within the first 50B tokens, especially at the 1.3B and 1.7B model scales. The per-benchmark analysis further reinforces this finding (see Figure 8). On MMLU, MixtureVitae is the only permissive dataset to show Preprint. Under review. Figure 8: Per-benchmark performance of permissive datasets after 50B training tokens. MixtureVitaes advantage on MMLU is apparent even at this early stage. significant learning signal early in training, demonstrating that its composition provides immediate benefits, which might be both due to knowledge rich and instruction like content. Arguably, this suggests that the reasoning capability shown by MixtureVitae is not late-stage phenomenon but rather an indication of efficient instillation from the early stages of training. This strong initial performance underscores the learning efficiency of MixtureVitae, making it compelling choice for achieving high performance with less computational cost. C.3 MODEL RED TEAMING To evaluate the safety of the model trained on MixtureVitae for 300B tokens, we performed redteaming analysis to measure the Attack Success Rate (ASR) against three standard benchmarks: toxigen (Hartvigsen et al., 2022), do_not_answer (Wang et al., 2023), and advbench (Zou et al., 2023). The results  (Table 7)  shows that our model is competitive with the baselines. The model responses were evaluated using two safety classifiers: (i) Llama Guard-8B (Inan et al., 2023), used to evaluate the do_not_answer and advbench datasets, while (ii) the toxigen_roberta classifier (Logacheva et al., 2022) was used for the toxigen benchmark. Table 7: Attack Success Rate in %, lower is better. All models are trained with the same open-sciref procedure (300B-token budget.) while varying only the pretraining dataset. Benchmark MixtureVitae Comma CommonCorpus-Eng Nemotron-HQ-CC toxigen do_not_answer advbench 8.07 28.22 86.92 9.04 24.71 92. 12.77 21.62 70.58 10.21 20.98 85."
        },
        {
            "title": "D CONTAMINATION ANALYSIS",
            "content": "D.1 CONTAMINATION DETECTION PROTOCOL To ensure the integrity of our evaluation, we implemented comprehensive decontamination protocol to measure the overlap between our training dataset and all evaluation benchmarks we report results on. This protocol consists of three main stages: Index Construction, Dataset Scanning, and Leakage Reporting. 23 Preprint. Under review. D.1."
        },
        {
            "title": "INDEX CONSTRUCTION",
            "content": "The first stage creates compact, indexed set of unique n-grams from all benchmark evaluation data. 1. Text Normalization: All text from the benchmarks is processed through normalization pipeline, similar to Laurençon et al. (2022): (1) Unicode normalization (NFKC), (2) conversion to lowercase, (3) tokenization, and (4) removal of predefined list of common English stop words. This procedure focuses the resulting n-grams on substantive content. 2. N-gramming and Filtering: We generate 13-grams, common n-gram size for this task Brown et al. (2020); Gao et al. (2020) from the normalized token lists. As in Laurençon et al. (2022), set of regular expressions is used to filter out common boilerplate, exam instructions, and formatting artifacts. 3. Train/Test De-duplication: as in Gao et al. (2020), we compute the set of all 13-gram hashes from the train split and subtract this set from the 13-gram hashes generated from the test split. This ensures our index only contains n-grams that are unique to the evaluation set. D.1.2 DATASET SCANNING The second stage analyzes the target training dataset against the generated index. 1. Document Processing: Each document in the training dataset is processed using the exact same normalization, 13-gramming, and hashing pipeline used for index construction. 2. Contamination Criteria: document is flagged as \"contaminated\" if it meets two criteria, based on the set intersection of its n-gram hashes with the benchmark index: Minimum Hits: The number of distinct matching n-grams is 3. Minimum Coverage: As proposed in Rae et al. (2021), the coverage of matching n-grams is 0.1%. Coverage is defined as: Coverage = distinct_hits total_unique_13grams_in_doc D.1.3 LEAKAGE REPORTING The final stage aggregates the scan results into summary report. 1. Numerator (Leaked N-grams): The procedure aggregates the reports from all scanned partitions. It performs global set union to find all unique n-gram hashes that were found at least once in the target dataset, aggregated by benchmark source. This provides the unique_ngrams_leaked count for each benchmark. 2. Denominator (Total N-grams): The procedure retrieves the pre-computed metadata to obtain the total unique n-gram count for each benchmark. 3. Final Metric: As proposed in Touvron et al. (2023), the Leak Percentage for each benchmark is then calculated as: Leak Percentage = unique_ngrams_leakedbenchmark total_unique_ngrams_in_indexbenchmark 100 D.2 CONTAMINATION REPORT We executed our 13-gram contamination scan across the entire 345 697 271 documents of the MixtureVitae dataset. The global summary of contaminated documents per benchmark is presented in Table 8. The results confirm that for the vast majority of benchmarksincluding ARC, HellaSwag, LAMBADA, OpenBookQA, and PIQAthe document-level contamination rate is negligible (at or below 0.0003%), strongly validating the integrity of our evaluation on these tasks. 24 Preprint. Under review. The scan did, however, flag minor overlap for MMLU (0.0098%) and BoolQ (0.0087%), and more significant overlap for our key code benchmarks: HumanEval (0.0988%) and MBPP (0.0878%). This overlap in code benchmarks is known challenge when including large-scale permissive code corpora like The Stack, which may naturally contain snippets of common coding problems (a \"source overlap\" rather than direct \"test-set leak\"). To ensure this overlap did not artificially inflate our models strong performance on these key tasks, we conducted case studies for the benchmarks with the highest overlap. This analysis is detailed in the following section (Appendix D.3)."
        },
        {
            "title": "Benchmark",
            "content": "Contaminated Docs Contamination Rate (%) ALERT ARC BoolQ GPQA Gsm8k HellaSwag HumanEval Ifeval LAMBADA MBPP MMLU OpenBookQA PIQA SimpleQA 12 17 30 144 1077 230 186 341 554 756 23 303 558 33 922 60 5 98 0.0000% 0.0000% 0.0087% 0.0003% 0.0001% 0.0001% 0.0988% 0.0002% 0.0000% 0.0878% 0.0098% 0.0000% 0.0000% 0.0000% Table 8: Global contamination summary by document count, based on 13-gram overlap scan. This table shows the total number of documents in MixtureVitae that contained at least one overlapping n-gram from each benchmarks test set. The total documents in MixtureVitae is 345 697 271 and the overall contamination rate is 0.1420%. D.3 DECONTAMINATED TEST SET PERFORMANCE To understand how test data leakage affects final performance on downstream tasks, we conducted the following experiment on all models and benchmarks reported in Section 3.3: 1. Identify problems from the test set that have at least one 13-gram match in the training dataset. 2. Evaluate the model on decontaminated benchmark version obtained by removing problems that were identified. As we can see from Table 9, the performance of the evaluated models is consistent between the original and decontaminated versions, aside from some upward bias in the decontaminated versions of MBPP+ and IFEval. Crucially, the result for the model trained on MixtureVitae were not affected by the strict decontamination procedure applied to the benchmarks. This rules out the possibility that the strong performance of MixtureVitae on math and coding is due to benchmark leakage. D.4 DECONTAMINATION CASE STUDY To further alleviate concerns about contamination issues, we performed an experiment where we trained 1.7B model on version of MixtureVitae that excludes three dataset shards that contribute 27% of the total contaminated docs which in particular showed MMLU contamination rates that are high relative to the rest of the dataset, as shown in Table 11. The shards we removed were Misc-Instruct, DART-Math and Nemotron Science & Math. The results are shown in Figure 9 and demonstrate that removing these shards had no effect on MMLU performance. 25 Preprint. Under review. Training Dataset GSM8K GSM8K-CoT MBPP MBPP+ IFEval Orig Decont Orig Decont Orig Decont Orig Decont Orig Decont MixtureVitae SmolLM2 Comma-0.1 CommonCorpus-Eng C4 DCLM FineWeb HPLT Nemotron-CC-HQ SlimPajama 0.53 0.30 0.06 0.02 0.01 0.01 0.02 0.02 0.03 0.02 0.54 0.30 0.06 0.01 0.01 0.02 0.01 0.02 0.02 0.02 0.50 0.28 0.09 0.01 0.01 0.02 0.03 0.02 0.03 0.02 0.50 0.29 0.09 0.01 0.02 0.02 0.03 0.02 0.03 0.02 0.38 0.35 0.21 0.02 0.00 0.01 0.00 0.00 0.00 0.00 0.38 0.35 0.23 0.02 0.00 0.00 0.00 0.00 0.00 0. 0.55 0.48 0.28 0.04 0.00 0.02 0.00 0.00 0.00 0.00 0.59 0.48 0.28 0.05 0.00 0.02 0.00 0.00 0.00 0.00 0.19 0.17 0.18 0.12 0.20 0.12 0.18 0.17 0.09 0.14 0.23 0.20 0.20 0.16 0.21 0.13 0.20 0.21 0.10 0.15 Table 9: Validating math, code, and instruction performance by comparing original (Orig) vs. decontaminated (Decont) test sets for 1.7B models trained for 300B tokens. MixtureVitaes high scores are shown to be genuine, as performance is maintained after removing all overlapping test items. This confirms the models capabilities are not an artifact of test set leakage. Dataset Original Decontaminated MBPP IFEval GSM8K MBPP+ 500 541 1319 378 331 429 1235 339 Table 10: Benchmark test set sizes (number of examples) for the original benchmarks versus the final decontaminated versions. The Decontaminated column shows the reduced set size after removing all examples with detected 13-gram training data overlap."
        },
        {
            "title": "E SYNTHETIC MATH DATA GENERATION",
            "content": "The synthetic math dataset was programmatically generated to produce diverse range of mathematical problems and their solutions. The generation process covers wide array of mathematical domains, including fundamental arithmetic operations, multi-term fractional expressions, and the step-by-step solution of algebraic linear equations. key component of the dataset consists of word problems, where numerical challenges are embedded in narrative scenarios. significant feature of this generation pipeline is the creation of detailed, step-by-step solutions formatted as chain-of-thought. For many problem categories, the scripts produce human-readable explanation of the entire solution process. This is achieved by using variety of randomized natural language templates to describe each logical step, such as carrying digit in addition or isolating variable in an equation. Following the initial generation, final post-processing step is applied to format the dataset for model training. This stage programmatically identifies data entries containing human-like, explanatory text by searching for common instructional words. For these selected entries, descriptive header (e.g., \"Here are examples of addition, division exercises\") is dynamically generated. The conDataset Shard Contaminated Docs Misc-Instruct DART-Math (Ge et al., 2024) Nemotron Science & Math (Bercovich et al., 2025) MAGACorpus (Hao et al., 2025) (All Remaining) 14 649 11 102 4793 241 3137 Table 11: Contamination sources for the MMLU benchmark in MixtureVitae, sorted by the number of contaminated documents, high to low. 26 Preprint. Under review. (a) Average accuracy across all tasks (as listed in Table 5) as function of number of training steps. (b) Accuracy on MMLU as function of number of training steps. Figure 9: Validation of 1.7B model performance. The MixtureVitae (Decontaminated) model (purple, dashed), trained with dataset shards responsible for benchmark leakage removed, performs closely to the full MixtureVitae (green, solid) model. This confirms our results are not an artifact of test set leakage. tent and phrasing of this header are randomized and based on the mathematical operations present within the text, adding significant linguistic diversity. For example, in the generated math problem below, model may be able to generalize to new numbers, but if the problem were to add three students instead of two, the model may not be robust enough to generalize. We leave this analysis for future work. The age difference between Sarah and Asafs age is half the total number of pencils Sarah has. The sum of their ages is 132, and Sarah is 27 years old. If Asaf has 60 more pencils than Sarah, calculate the total number of pencils they have together. Solution: If the sum of their ages is 132, and Sarah is 27 years old, Asaf is 105 years old. The age difference between Sarah and Asafs age is 105-27 = 78. Since the age difference between Sarah and Asafs age is half the total number of pencils Sarah has, Sarah has 2*78 = 156 pencils. If Asaf has 60 more pencils than Sarah, Asaf has 156+60= 216 pencils. Together, they have 156 + 216 = 372 pencils."
        },
        {
            "title": "F OUR POSITION FOR USING GOVERNMENTAL WORKS",
            "content": "In order to increase the diversity of our dataset, we included around 11B tokens of governmental website data from non-federal US government sources. While federal US government sources are non copyrightable, the other governmental website information may not be expressly in the public domain nor licensed. Instead, here we depend on fair use principles (United States Congress, 1976; Lemley & Casey, 2017) and the EU text data mining exceptions (The European Parliament and the Council of the European Union, 2019; Margoni, 2019), which mitigates the risk in using this subset. Our ethical and legal reasoning for using these government web contentsourced from Common Crawl-related datasetsis as follows: Public Purpose Alignment: The content created by governments is normally meant to be shared with the public, and by using the data for training we are assisting this purpose. Purpose of Use: From legal perspective, the government works are being redistributed as part of an open source, no-fee dataset to be used to create models are less likely to be 27 Preprint. Under review. copyright violating. This purpose is clearly not to compete with the governments own usage. Effect on Potential Market: We also think it is more likely to be fair use because the use of government website content is unlikely to have an effect on the potential market for the governments website content because the government is unlikely to be making commercial use to compete with the content as the government is unlikely making commercial use. Nature of the Content: The nature of the content is mostly public announcements, content of public interest, governmental functions or the like. Again, we believe there is strong public policy interest for fair use of this type of information. Amount Used: While we use all or almost all of the content of the government website, the amount of usage is not determinative of fair-use or not fair-use. Federal vs. Non-Federal Works: Lastly, US works created by the federal governments are generally not copyrightable. However, we recognize that this is not the case for other foreign governmental works, or non-federal works. For these reasons, we believe using government website data is lower copyright risk. But, to minimize the risk further such as the potential inclusion of third party copyrighted works on government web-pages, we have included keyword filters such as \"All Rights Reserved\", \"Copyright \", etc. to further filter out government web pages that have these terms. Recent court cases, as of the writing of this paper, Bartz v. Anthropic PBC (district court ruling that use of purchased copies of books for AI training is fair use) and Kadrey v. Meta Platforms, Inc., (district court ruling that training on authors books was transformative fair use) give credence to the case for fair use AI training on web-text data, especially as our small government website subset is both much more limited and of more public nature than in the Bartz and Kadrey case. As with other large permissive datasets, legal risks still remain, including for example trademark risks. While training on Wikipedia article about Spiderman may be low legal risk (CC-BY-SA licensed and transformative and educational summary), model that then generates story with characters named \"Spiderman\", even if those stories are not based on original human created stories, this use may still implicate trademark rights. Addressing those, we will leave for future work. With that said, we do not and cannot guarantee that even with rigorous provenance tracking and standard filtering, that there is no legal risk, so we recommend anyone who uses our datasets to consult their own attorney in their jurisdiction. PROVENANCE AND RATIONALE FOR THE STACK V1 (OPENRAIL-M) Our inclusion of 26.6B tokens sourced from The Stack v1 (Kocetkov et al., 2022), which our chart categorizes by its governing OpenRAIL-M license, warrants this specific note on provenance. The data was included based on the following rationale: Source and Filtering Methodology: The dataset originates from large-scale scrape of GitHub. The BigCode project curated this data by applying filter to include only those repositories that contained clear permissive license file (e.g., MIT, Apache 2.0, BSD) at the root level. Acknowledged Heuristic: This repository-level filtering is heuristic and not file-level guarantee. As acknowledged by the datasets creators, this process cannot perfectly resolve complex cases of multi-licensing within single repository, such as the inclusion of non-permissively licensed vendor libraries or mixed-license assets alongside permissivelylicensed code. Inclusion Justification: Despite this caveat, The Stack v1 represents the largest-available public corpus curated with the explicit goal of permissive filtering. Excluding it would make training high-performance, open, and risk-mitigated code model nearly impossible. Its best-effort permissive curation philosophy directly aligns with our datasets core principle of risk-mitigation. 28 Preprint. Under review."
        },
        {
            "title": "H DATA FILTERING REASONING AND PROTOCOL",
            "content": "To promote transparency, we describe our protocol for defining and checking the lists and content of the psuedo-crawled portion of MixtureVitae. H.1 GOVERNMENTAL AND NGO DOMAIN PATTERNS The following list of URL patterns was used to filter for governmental, non-governmental, and international organization websites from the web datasets. We gathered the list by examining public records, Wikipedia lists, and the like. The list is not as simple as .gov. because international governments use different TLDs. Moreover, some spam websites masquerades as .gov websites. Two of the authors, examined each domain either online or through the Internet Archives Way Back Machine to confirm they belonged to government website. After performing pseudo-crawl on FineFineweb, Nemo-CC and MagaCorpus, the authors manually reviewed the data for quality, and filtered out spam websites with similar website names, which were added to blocklists. The .gov, .gov/, and .mil/ websites are US Federal governmental works. To the extent we could, we filtered any sites that had keywords indicating reservations of rights. We believe this lowers the risk of inadvertent third party copyrighted works appearing on US Federal works, and is in the spirit of the EU text data mining opt-out conventions. .gov (as suffix) .gov/ .mil/ All other websites in this category are specifically international governments or NGOs. .vlada.mk .vlada.cz .kormany.hu regeringen.* (e.g., regeringen.se, regeringen.no) .rijksoverheid.nl .government.nl .bund.de .bundesregierung.de .government.ru .gc.ca .admin.ch www.gob.cl/ www.gob.ec/ guatemala.gob.gt/ presidencia.gob.hn/ www.gob.mx/ presidencia.gob.pa/ www.gob.pe/ gob.es/ argentina.gob.ar/ tanzania.go.tz/ .indonesia.go.id/ .go.kr/ .go.jp/ thailand.go.th/ .europa.eu/ 29 Preprint. Under review. .un/ .int/ .govt. www.gub.uy .gov. .gouv. H.2 CURATED PERMISSIVE DOMAIN LIST The following list of approximately 50 domains was curated based on their known public domain or CC-BY-SA* license status or permissive status. The websites were chosen for their diversity of content. Two of the authors, one of which has legal background examined the websites terms of use, or relevant sections online or on the Way Back Machine to confirm licensing and permission status. After performing pseudo-crawl on FineFineweb, Nemo-CC and MagaCorpus, the authors manually reviewed the data for quality, and filtered out spam websites with similar website names as the below. These spam sites were added to blocklists. .free.law/ .europeana.eu/ .publicdomainreview.org/ .wisdomcommons.org/ .intratext.com/ .mediawiki.org/ .wikimedia.org/ .wikidata.org/ .wikipedia.org/ .wikisource.org/ .wikifunctions.org/ .wikiquote.org/ .wikinews.org/ .wikivoyage.org/ .wiktionary.org/ .wikibooks.org/ .courtlistener.com/4 .case.law/ pressbooks.oer.hawaii.edu/ .huggingface.co/docs/ .opencourselibrary.org/ .medbiq.org/ .doabooks.org/ .bccampus.ca/ open.umn.edu/opentextbooks/ www.gutenberg.org/ .mozilla.org/ www.eclipse.org/ .apache.org/ .python.org/ .pytorch.org/ 4For courtlistener.com, the terms of use says it is CC-BY-ND, but the underlying court cases are public domain, and the content from this website is merely 176KB and is de minimus. 30 Preprint. Under review. .numpy.org/ .scipy.org/ .opencv.org/ .scikit-learn.org/ .pydata.org/ .matplotlib.org/ .palletsprojects.com/ .sqlalchemy.org/ .pypi.org/ .sympy.org/ .nltk.org/ .scrapy.org/ .owasp.org/ .creativecommons.org/ .wikia.com/ .foodista.com/ .fandom.com/ .attack.mitre.org/ The vast majority of these cites are CC-BY sites. But there are some that have other open licenses as shown in Table 12. License BSD 3-Clause Mozilla Public License Python Software Foundation License 2.0 Apache 2.0 MIT License Eclipse Public License MedBiquitous Standards Public License medbiq.org Websites scipy.org, sympy.org, matplotlib.org, scrapy.org, scikit-learn.org, pydata.org, pytorch.org, palletsprojects.com mozilla.org python.org apache.org, nltk.org, opencv.org sqlalchemy.org www.eclipse.org Table 12: Software Licenses and Associated Websites"
        },
        {
            "title": "I AUTHOR CONTRIBUTIONS",
            "content": "Huu Nguyen: Overall lead, data curation and engineering, data review, decontamination analysis, policy design, paper co-writing Victor May: Paper writing lead, project coordination, policy design, decontamination protocol design and analysis Harsh Raj: Led model training, evaluations and ablation experiments, paper co-writing Marianna Nezhurina: Model training, established major parts of the dataset tokenization and training infrastructure (Megatron-LM container based workflow), conducted scaling tests for distributed training, wrote routines for evaluation based on lm-eval-harness, conducted model training, performed evaluations, ablations and paper co-writing Yishan Wang: Paper co-writing Yanqi Luo Paper co-writing Minh Chien Vu Dataset curation, multimodal text generation Taishi Nakamura Implemented checkpoint conversion routines from Megatron to HuggingFace format, paper co-writing 31 Preprint. Under review. Ken Tsui: Data curation, filtering and classifier training Van Khue Nguyen Dataset curation, multimodal text generation David Salinas: Performing evaluations and visualizations, paper co-writing Aleksandra Krasnodebska: Red teaming evaluations, paper co-writing Christoph Schumann: Synthetic math word problems dataset co-design, advising Mats Leon Richter: Dataset curation and processing, advising Xuan-Son (Sonny) Vu: advising and paper co-writing Jenia Jitsev: Supervision, compute resource acquisition. Reference baseline experiments (open-sci-ref) design and model training. Organized dataset acquisition and transfer across the supercomputers. Co-established environments for experiments across various supercomputers. Evaluations co-design, results analysis. Paper co-writing. Table 13: Author contributions to this work. Large squares () indicate major contribution and plus symbols (+) indicate supporting contribution. Dataset Curation & Processing Policy Decontamination Model Training Evaluation & Red Teaming Writing Visualization Leadership & Coordination Advising Huu Nguyen Victor May Harsh Raj Marianna Nezhurina Yishan Wang Yanqi Luo Minh Chien Vu Taishi Nakamura Ken Tsui Van Khue Nguyen David Salinas Aleksandra Krasnodebska Christoph Schuhmann Mats Leon Richter Xuan-Son (Sonny) Vu David Salinas Jenia Jitsev + + + + + + + + + + + + + + + + + + + + + + + +"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Detomo Inc.",
        "ELLIS Institute Tuebingen",
        "Independent Researcher",
        "Institute of Science Tokyo",
        "Juelich Supercomputing Center (JSC), Research Center Juelich (FZJ)",
        "LAION",
        "Montreal Institute for Learning Algorithms, University of Montreal, Université de Montréal",
        "NASK",
        "Northeastern University",
        "Ontocord",
        "Open-Ψ (Open-Sci) Collective",
        "RSS Lab, LTH / DeepTensor AB",
        "Salesforce",
        "University of Freiburg",
        "École Polytechnique, IP Paris"
    ]
}