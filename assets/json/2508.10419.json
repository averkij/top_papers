{
    "paper_title": "ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning",
    "authors": [
        "Juyuan Wang",
        "Rongchen Zhao",
        "Wei Wei",
        "Yufeng Wang",
        "Mo Yu",
        "Jie Zhou",
        "Jin Xu",
        "Liyan Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods can fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition when reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global comprehension, offering a principled, cognitively motivated paradigm for retrieval-based long context comprehension towards stateful reasoning. Our code is publicly released at https://github.com/EternityJune25/ComoRAG"
        },
        {
            "title": "Start",
            "content": "ComoRAG: Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning Juyuan Wang*1 Rongchen Zhao*1 Wei Wei2 Yufeng Wang1 Mo Yu4 Jie Zhou4 Jin Xu1,3 Liyan Xu4 1School of Future Technology, South China University of Technology 2Independent Researcher 3Pazhou Lab, Guangzhou 4Pattern Recognition Center, WeChat AI, Tencent 5 2 0 2 4 1 ] . [ 1 9 1 4 0 1 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Narrative comprehension on long stories and novels has been challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLMs diminished reasoning over extended context and high computational cost, retrieval-based approaches remain pivotal role in practice. However, traditional RAG methods can fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not oneshot process, but dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition when reasoning with memoryrelated signals in the brain. Specifically, when encountering reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into global memory pool, thereby supporting the emergence of coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global comprehension, offering principled, cognitively motivated paradigm for retrieval-based long context comprehension towards stateful reasoning. Our code is publicly released at https://github.com/EternityJune25/ComoRAG."
        },
        {
            "title": "Introduction",
            "content": "The core challenge of long-context narrative comprehension lies not merely in connecting discrete pieces of evidence, task more naturally defined as multi-hop Question Answering (QA), but in performing dynamic cognitive synthesis to grasp necessary background and content progression (Xu et al. 2024a). Unlike multi-hop QA (Yang et al. 2018), which seeks static path through fixed facts, narrative comprehension requires emulating human reader: continuously building and revising global mental model of the *These authors contributed equally. Project lead. Correspondence to: <liyanlxu@tencent.com> Figure 1: Comparison of RAG reasoning paradigms. plot, characters, and their evolving motivations (JohnsonLaird 1983). The complexity of this process is well exemplified by classic narrative question Why did Snape kill Dumbledore? from the Harry Potter series. Answering this requires weaving complete web of evidence from disparate clues spanning multiple booksDumbledores terminal illness, the Unbreakable Vow, and Snapes deeply concealed loyalty. The true significance of these clues is only fully reconciled in hindsight. This capability is what we term stateful reasoning: it demands more than linking static evidence; it requires maintaining dynamic memory of the narrative, one that is constantly updated as new revelations emerge. Long-context LLMs have demonstrated promising performance on benchmarks such as the Needle in Haystack test (Eisenschlos, Yogatama, and Al-Rfou 2023) in recent years. However, their capacity to process long narratives (200k+ tokens) remains limited by finite context windows. Furthermore, as the input length increases, these models are prone to the lost in the middle problem (Liu et al. 2024), which raises perplexity and impairs generation quality. This limitation is particularly pronounced in narrative comprehension tasks which require stateful reasoning. As result, retrieval-augmented generation (RAG) (Lewis et al. 2020) has emerged as an important strategy for tackling long context comprehension with LLMs. However, existing RAG methods still struggle to effectively address this challenge. Advanced single-step retrieval remains limited by its static index. This includes methods such as RAPTOR (Sarthi et al. 2024), which clusters and summarizes text chunks to retrieve at different levels of details; HippoRAGv2 (Gutierrez et al. 2025), which mimics the human hippocampus by building knowledge graph to achieve multi-hop reasoning in single retrieval step. Nonetheless, single-step methods rely on one-shot static retrieval, which may lead to shallow comprehension. For example, in Figure 1(a), the evidence about Snape can mislead the model into making false inference. As remedy, multi-step retrieval methods offer more promising direction, such as IRCoT (Trivedi et al. 2023), which interleaves the retrieval process with Chain-ofThought reasoning (Wei et al. 2022); Self-RAG (Asai et al. 2024), which trains model to adaptively retrieve and reflect on evidence; and MemoRAG (Qian et al. 2025), which uses dual-system architecture to generate clues from compressed global context. These methods all target to obtain richer context through iterative retrieval. However, their retrieval steps are typically independent, which lack coherent reasoning throughout explicit narrative progression, featuring fragmented evidence with stateless comprehension. As illustrated in Figure 1(b), due to lack of dynamic memory, multi-step retrieval fails to integrate contradictory evidence such as Snape protects/bullies Harry and cannot understand the evolution of his actions, ultimately unable to yield the correct answer. In this work, we seek inspiration from the function of Prefrontal Cortex (PFC) in human brains, which employs sophisticated reasoning process called Metacognitive Regulation (Fernandez-Duque, Baird, and Posner 2000). This process is not single action but dynamic interplay between new evidence acquisition, driven by goal-directed memory probes (Dobbins and Han 2006; Miller and Constantinidis 2024), and subsequent knowledge consolidation. During consolidation, new findings are integrated with past information to construct an evolving, coherent narrative. This iterative cycle allows the PFC to continuously assess its understanding and revise its strategy, providing direct cognitive blueprint for our frameworks stateful reasoning approach. We introduce ComoRAG, cognitive-inspired, memoryorganized RAG framework, imitating the human Prefrontal Cortex (PFC) for achieving true stateful reasoning. At its core is dynamic cognitive loop operating on memory workspace, which actively probes and integrates new evidence to build coherent narrative comprehension. This process, as illustrated in Figure 1(c), is closed loop of evolving reasoning states. Faced with complex query like Why did Snape kill Dumbledore?, the systems memory state evolves from an initial causally incomplete event (Snape kills Albus), to an apparent contradiction upon finding contradictory information (Snape protects Harry), and ultimately to logically consistent coherent context through deeper exploration and evidence fusion. Only in this final, complete cognitive state can ComoRAG perform the correct stateful reasoning, deriving the profound insight that it was an act of loyalty, not betrayal. This cognitively-inspired design yields substantial improvements across four challenging long-context narrative benchmarks. ComoRAG is shown to consistently outperform all categories of strong baselines across each dataset. Our analysis reveals several key findings. First, these gains stem directly from the cognitive loop, which transforms static knowledge base into dynamic reasoning engine; for instance, accuracy on EN.MC jumps from static-retrieval baseline of 64.6% to 72.9%, with performance efficiently converging in around 2-3 cycles. Second, our framework excels on narrative queries that require global understanding of plot progression, achieving up to 19% relative F1 improvement on these challenging question types where others falter. Finally, our framework demonstrates remarkable modularity and generalizability. Its core loop can be flexibly integrated to existing RAG methods such as RAPTOR, which directly yields 21% relative accuracy gain). Also, switching to stronger model as the backbone LLM agents can upgrade reasoning in the entire cognitive loop, attaining accuracy from 72.93% to 78.17%. These results collectively validate that ComoRAG provides principled, cognitivelyinspired new paradigm for retrieval-based long narrative comprehension towards stateful reasoning."
        },
        {
            "title": "Narrative Reasoning",
            "content": "Our objective is to design framework for stateful reasoning in RAG scenarios. Especially, it aims to resolve those queries that require global context comprehension in the first place, commonly seen in narratives, where conventional RAG may fail to recognize relevant context based on the surface form of queries. Formally, denote the initial query as qinit, and knowledge source derived upon the original context, our framework leverages series of adaptive operations to yield the final answer, Af inal, through discrete time steps = 1, . . . , with underlying memory control. At the beginning of each step t, determines its focus of reasoninga set of new probing queries (t), representing new information to seek that may logically deepen the query comprehension and ultimately complement the answer resolution. With newly retrieved information by (t) at each step, the framework utilizes the global memory pool Figure 2: An illustration of ComoRAG. Triggered by reasoning impasse (Failure), the Metacognitive Regulation loop conto devise new exploratory probing queries based on past sists of five core operations described in Section 2.3: 1) Self-Probe to form new memory units on memory units; 2) Tri-Retrieve how the latest evidence of new aspects could complement the final query resolution; 4) Mem-Fuse to generate cues integrating to perform query answering using new memory information produced in this cycle. new and past memory units; 5) Try-Answer to retrieve evidence from three knowledge sources; 3) Mem-Encode maintained till the prior step M(t1) , and produces either pool the final answer, or Failure Signal, indicating reasoning impasseand updates the memory pool to M(t) pool, accomplishing cognitive cycle that synergizes between the knowledge source, memory space and retrieval operations."
        },
        {
            "title": "2.2 The Hierarchical Knowledge Source\nTo overcome the limitations of a monolithic representation\nof the given context, our framework first builds a hierarchi-\ncal knowledge index X for retrieval that models the raw text\nfrom three complementary cognitive dimensions, analogous\nto how the PFC integrates different memory types from var-\nious brain regions, particularly supporting cross-layer rea-\nsoning from raw evidence to abstract relationships.",
            "content": "Veridical Layer: Grounding in Factual Evidence. To ensure all reasoning is traceable to source evidence, veridical layer ver is firstly established, constituted by raw text chunks directly, analogous to the precise recall of factual details in human memory. For more accurate retrieval on text chunks, we instruct LLM to generate knowledge triples (subject-predicate-object) for each text chunk. These triples participate in each retrieval, and strengthen the matching between an incoming query and the corresponding text chunk, which is proven effective by HippoRAG (Jimenez Gutierrez et al. 2024). Further details are described in Appendix B. Semantic Layer: Abstracting Thematic Structure. To capture thematic and conceptual connections that transcend across long-range contextual dependencies, semantic layer sem is built, inspired by the prior work RAPTOR that employs GMM-driven clustering algorithm to recursively summarize semantically similar text chunks into hierarchical summary tree. We reckon such semantic abstraction is necessary for deeper comprehension and follow the same formulism. These summary nodes enable the framework to retrieve conceptual information beyond the surface level. Episodic Layer: Reconstructing Narrative Flow. The previous two layers equip views of both factual details and high-level concepts. However, they lack temporal development or plot progression that can be especially crucial for narratives. To enable such view with long-range causal chains, we introduce the episodic layer, epi, which aims to reconstruct the plotline and story arc by capturing the sequential narrative development. The process features sliding window summarization across text chunks; each resulting node is then summary that aggregates the narrative development of continuous or causally related events according to the timeline. Optionally, the sliding window process can be applied recursively to form higher-level views of content progression, extracting different levels of narrative flow as part of the knowledge source."
        },
        {
            "title": "2.3 The Architecture of Metacognitive Regulation\nThe core of ComoRAG is a control loop that fully realizes\nthe concept of metacognitive regulation. It is composed of\na Regulatory Process for reflection and planning at each\nstep, and a Metacognitive Process for executing reasoning\nand memory management with the Memory Workspace.",
            "content": "Dynamic Memory Workspace. The memory workspace contains memory units that serve as the bridge for cohesive multi-step exploration and reasoning by metacogp nitive regulation. Each memory unit functionally concludes one retrieval operation, denoted as tuple of three , Ctype elements: = (p, type ), where is the probing query that triggers this retrieval; type is the homogeneous set of evidence retrieved from single knowledge layer (type {ver, sem, epi}); and Ctype is synthesized cue that reflects how these retrieved evidence by the probe could complement the comprehension and resolution of the original query qinit. Concretely, Ctype is generated by LLM in the role of Comprehension Agent, πcue, denoted as = πcue(qinit, p, type Ctype ). The formation of memory unit (p, type ) by each retrieval is defined as Mem-Encode operation. The memory workspace/pool will be utilized and updated throughout the reasoning cycle described below. , Ctype p The Regulatory Process. The regulatory process is invoked at the beginning of reasoning cycle/step if the preceding cycle t1 is concluded in failure. The core operation, Self-Probe , plans new probing queries of which retrieved information may contribute to the final answer, thereby devising new exploratory paths to break the impasse. It is orchestrated by Regulation Agent, πprobe, whose decisions are informed by the reflection on the prior failure, exploring for more necessary background or relevant information towards full context comprehension to resolve the origitakes three inputs: (1) the ultimate goal nal query. Self-Probe qinit; (2) the complete exploration probing history (t1) hist up to the end of the last step; and (3) the immediate knowledge gaps that caused the failure, concretized by all synthesized cues of memory units generated in the prior step, denoted as {C}(t1). Its output (t) is new, strategic set of retrieving probes for the current cycle t: hist , {C}(t1)(cid:1) (t) = πprobe (cid:0)qinit, (t1) The Metacognitive Process. The metacognitive process takes the new probes for this cycle (t), and performs reasoning towards resolving the original query while keeping track of the progress with the memory space. It comprises series of operations, described in details as follows. (1) Tri-Retrieve : for each probing query (t), retrieval is conducted on each knowledge layer type where type {ver, sem, epi}, such that evidence of high embedding similarity to per layer is retrieved in standard Dense Passage Retrieval paradigm, with each evidence being either the raw text chunk, semantically clustered summary, or narrative flow summary. Mem-Encode : for each and type, the retrieved evidence is immediately processed by the aforementioned Mem-Encode , to generate new memory unit that keeps track of how this specific probing could complement to the final answer. The number of all generated memory units at this step can be denoted as M(t) encode = 3 (t). Mem-Fuse : new memory units in the above step M(t) encode mainly emphasize aspects probed in the current cycle. To fully utilize the past experience and historical knowledge, the framework further identifies relevant synthesized cues from past units in the existing memory pool Mt1 pool, then generates new synthesized cue for fusing past relevant evidence. Let Mt1 pool qinit represent past memory units whose cues are of high embedding similarity with qinit, and denote LLM as Integration Agent πf use that synthesizes these relevant past evidence into high-level background summary, the new cue fusing past memory C(t) use is then: C(t) use = πf use (cid:0)qinit, Mt1 pool qinit (cid:1) (2) Try-Answer : with the new probing evidence in M(t) encode use, QA Agent, πQA, is applied and the past-fusing cue C(t) to these contexts to produce the cycles final output O(t): (cid:0)qinit, M(t) O(t) = πQA encode, C(t) use (3) (cid:1) Specifically, LLM is instructed to take these latest evidence and the past background as the context, and determine whether the original query can be resolved. It either yields the final answer and terminates the entire reasoning loop, or signals Failure and continues to the next step. Mem-Update : this last step in cycle simply incorporates the newly generated memory units into the global pool, with their embedding encoded, for future retrieval and reasoning: M(t) pool M(t1) pool M(t) encode (4) ComoRAG With the above six steps from Tri-Retrieve to Mem-Update , one cycle of the cognitive loop is realized. For the initial step as in = 0, ComoRAG starts with one round of Tri-Retrieve followed by Try-Answer . If Failure is signaled, it initiates the Metacognitive loop of stateful reasoning on exploratory paths, characterized by the interlocking operations with the memory workspace, which enables to tackle complex narrative comprehension. In essence, our framework grasps on the principle that for long context comprehension, especially in narratives where the entire context is cohesively interconnected through the underlying plot progression (Xu et al. 2024a), the query resolution is not linear pipeline; rather, it is dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to the human cognitive process. The overall process is further depicted in the algorithm of Appendix A; detailed prompts used by each LLM agent are provided in Appendix D."
        },
        {
            "title": "NarrativeQA",
            "content": "EN.QA EN.MC DetectiveQA QA Avg. MC Avg."
        },
        {
            "title": "LLM",
            "content": "GPT-4o-mini"
        },
        {
            "title": "Naive RAG",
            "content": "BGE-M3(0.3B) NV-Embed-v2 (7B) Qwen3-Embed-8B"
        },
        {
            "title": "Enhanced RAG",
            "content": "RAPTOR HippoRAGv2 Multi-step RAG Self-RAG MemoRAG RAPTOR+IRCoT HippoRAGv2+IRCoT F1 27.29 23.16 27.18 24. 27.84 23.12 19.60 23.29 31.35 28.98 EM 7.00 15.10 17.80 15.60 17.80 15. 6.40 15.20 16.00 13.00 F1 EM 29.83 12.82 23.71 34.34 25. 26.33 24.45 12.84 19.40 32.09 29.27 16.24 24.57 17.95 19.65 17.09 4.27 11.64 19.36 18.24 ComoRAG (Ours) 31.43 18.60 34.52 25.07 ACC 30. 59.82 61.13 65.50 57.21 60.26 59.83 55.89 63.76 64.19 72.93 ACC 30. 54.54 62.50 61.36 57.95 56.81 52.27 51.13 64.77 62.50 68.18 F1 28. 23.44 30.76 24.99 27.09 23.79 16.22 21.35 31.72 29.13 EM 9.91 15.67 21.19 16. 18.73 16.15 5.34 13.42 17.68 15.62 32.98 21.84 ACC 30. 57.18 61.82 63.43 57.58 58.54 56.05 53.51 64.27 63.35 70.56 Table 1: Evaluation results on four long narrative comprehension datasets. For fair comparison, all methods use GPT-4o-mini as the LLM backbone, and all non-naive RAG methods use BGE-M3 for retrieval (details in Section 3). We highlight the best and second-best results. ComoRAG is shown consistently outperform all baselines across all datasets. EN.MC from BENCH: MC dataset with 229 questions on classic novels of similar length as EN.QA. DetectiveQA (Xu et al. 2024b): MC dataset consisting of detective fiction with average length over 100k tokens. We randomly sample 20% of all stories to reduce the computational cost. For evaluation metrics, we report both F1 and Exact Match (EM) scores for QA datasets, and report Accuracy (ACC) for MC datasets. To ensure fairness in resolving multiple-choice questions, we only expose the options during Try-Answer , such that no retrieval-related actions can utilize potential hints present in the options. Baselines We employ four types of baselines as follows, covering different paradigms for long context QA. LLM: the non-RAG setting, where the entire context (capped by length 128k) is provided to the LLM directly. Naive RAG: the standard RAG setting that splits the raw context by chunks for retrieval. We set the max chunk length as 512 tokens in all experiments. Enhanced RAG: RAG methods with augmented retrieval index, including RAPTOR (Sarthi et al. 2024) that constructs semantic summary tree over text chunks, and HippoRAGv2 (Gutierrez et al. 2025) that builds the knowledge base for entities in text chunks. We also experimented with GraphRAG (Edge et al. 2025); however, it requires exponential computational cost for building the retrieval index, being less practical for full evaluation. We separately report GraphRAG on subset in Appendix B. Multi-step RAG: RAG methods with multi-step or iterative retrieval strategies. IRCoT (Trivedi et al. 2023) leverages Chain-of-Thought (CoT) as intermediate queries that iteratively retrieve evidence. Self-RAG (Asai et al. 2024) trains dedicated critic model to control when to stop retrieval. MemoRAG (Qian et al. 2025) trains model that compresses the global context, which generates clues as intermediate queries. Implementation Details For the Hierarchical Knowledge Source, we follow the procedures of HippoRAGv2 and RAPTOR respectively to build the Veridical and Semantic layers; the Episodic layer employs an adaptive sliding window for narrative summaries described in Appendix B. For LLMs, our main experiments adopt GPT-4o-mini in all approaches to ensure fair comparison. We additionally tested GPT-4.1 and Qwen3-32B (Yang et al. 2025) for generalization analysis in Section 4.3. For all RAG methods, we adopt the popular model BGE-M3 (Chen et al. 2024) for retrieval. Additionally, for naive RAG, we also experiment with larger but less practical embedding models, including NV-Embed-v2 (Lee et al. 2025) and Qwen3-Embed8B (Zhang et al. 2025). The LLM context length for all RAG methods, including ComoRAG, is capped at 6k tokens. For the Metacognitive Regulation loop, we set the framework to iterate for maximum of 5 rounds. More regarding implementation details are further provided in Appendix B."
        },
        {
            "title": "4.1 Main Results",
            "content": "Evaluation results of our main experiments are shown in Table 1. Remarkably, ComoRAG achieves the best performance upon all baselines across all datasets. Despite using the lightweight 0.3B BGE-M3 for retrieval, it significantly outperforms RAG with much larger 8B embedding models. Overall, ComoRAG demonstrates consistent improvement for tackling long narrative comprehension, surpassing strong prior RAG methods of various paradigms. Upon closer examination, ComoRAG exhibits distinct advantages on the two BENCH datasets featuring ultra-long contexts. More broadly, Figure 3 illustrates that ComoRAG is more robust and insensitive to longer contexts, sustaining its efficacy over HippoRAGv2, with the accuracy gap peaking at +24.6% for documents exceeding 150k tokens, which Method EN.MC EN.QA"
        },
        {
            "title": "ComoRAG",
            "content": "Baselines HippoRAGv2 RAPTOR Index w/o Veridical w/o Semantic w/o Episodic Retrieval w/o Metacognition w/o Regulation w/o Both ACC 72.93 60.26 57.21 51.97 64.63 64.63 62.01 55.02 54. F1 EM 34.52 25.07 24.45 26.33 22.24 30.82 31. 26.95 27.95 25.64 17.09 19.65 15.88 22.65 21.47 18.53 20.59 17.35 Table 2: Ablation studies of ComoRAG. Figure 3: Averaged accuracy across different document lengths on Multi-Choice datasets. ComoRAG is shown more robust to long contexts over the baseline. highlights the importance of stateful multi-step reasoning for query resolution over long and coherent contexts."
        },
        {
            "title": "4.2 Ablation Studies",
            "content": "We perform ablation studies on EN.MC and EN.QA datasets by systematically removing key modules in ComoRAG. The results are shown in Table 2. Figure 4: Performance gains from iterative probing. GPT-4.1 marks the evaluation by using the stronger GPT-4.1 as LLM agents in ComoRAG (as opposed to GPT-4o-mini). Hierarchical Knowledge Source All three knowledge layers contribute supplementary enhancements to the final performance, with the Veridical Layer being the most significant retrieval index. It provides the basis for factualgrounded reasoning, as confirmed by the 30% relative performance drop upon its removal. Metacognition Removing the Metacognition process essentially disables the memory workspace, where all agents operate on retrieved evidence directly, without knowledge consolidation by synthesized cues. Disabling this module leads to significant performance drop, as seen by the 22% relative decrease in F1 score on EN.QA, and an approximate 15% decrease in accuracy on EN.MC, underscoring the critical role of dynamic memory organization. Regulation Removing the Regulation process cuts off the goal-oriented guidance, such that each cycle uses the same initial query for new evidence retrieval (duplicated evidence is removed), without generating probing queries that are crucial to new evidence acquisition. Disabling this module severely impacts retrieval efficiency, causing 24% drop in accuracy on EN.MC and 19% drop in F1 score on EN.QA. Notably, removing both Metacognition and Regulation further degrades performance, effectively reducing the system to one-shot resolver without multi-step reasoning. Overall, the ablation study results corroborate that the enhancement offered by ComoRAG stems from the synergy between its memory consolidation and dynamic evidence exploration, facilitated by the hierarchical knowledge index to provide enriched semantic information. Removing any of the core components would significantly weaken its narrative reasoning capabilities."
        },
        {
            "title": "4.3\nIn-Depth Analysis of Iterative Retrieval\nTo further investigate the source of ComoRAG’s effective-\nness, this section presents a quantitative analysis of its core\niterative retrieval process.",
            "content": "Source of Gains: From Static Bottleneck to Dynamic Reasoning Our analysis suggests that the stateful multistep reasoning enabled by the Metacognitive loop is the key factor driving the observed improvement. We first identify static bottleneck: after the initial retrieval using the original query at step 0, the single-step evaluation score shows no significant advantage over strong baselines, with less than 1% compared to the best baseline HippoRAGv2+IRCoT. However, upon activating the cognitive loop, there presents sustained and significant improvement, raising the accuracy to 72.93% on EN.MC, as shown in Figure 4. This further supports the findings from the ablation studies, which demonstrate significant performance drop upon removing the entire loop. Additionally, Figure 4 illustrates that the majority of the improvement occurs within 2-3 cycles, confirming the efficiency of the process. The few remaining unresolved queries are tied to the inherent reasoning limitation of the base LLM, where our next analysis shows that the ceiling performance of ComoRAG can be boosted simply by switching to more capable LLMs. Model-agnostic Generalization ComoRAG demonstrates generalization with different LLM backbones, with stronger LLMs further enhancing the reasoning process and final query resolution. To validate this, we replace GPT-4omini with GPT-4.1 and Qwen3-32B in the Metacognitive loop, using the same knowledge source for retrieval. The"
        },
        {
            "title": "Method",
            "content": "NarQA EN.QA EN.MC DetQA ComoRAG w/ Qwen3-32B w/ GPT-4.1 HippoRAGv2 + Our Loop RAPTOR + Our Loop F1 31.43 32.17 35. 23.12 29.12 27.84 30.55 F1 34.52 35.29 38.82 24.45 31.76 26.33 34. ACC 72.93 74.24 78.17 60.26 68.56 57.21 69.00 ACC 68.18 69.32 76. 56.81 63.64 57.95 62.50 Table 3: Efficacy of ComoRAG on model-agnostic generalization and Plug-and-Play flexibility. results, presented in Figure 4 and the upper section of Table 3, show notable improvement particularly with GPT-4.1, boosting the F1 score on EN.QA from 34.52 to 38.82, and increases the accuracy on EN.MC from 72.93 to 78.17. These results demonstrate that ComoRAG effectively leverages and unleashes the models capabilities during its stateful iterative reasoning process. Plug-and-Play: Flexibility To examine the modularity of our framework, we conduct further experiments by applying the Metacognitive loop of ComoRAG on existing RAG methods. As shown in the bottom section of Table 3, the cognitive loop can be seamlessly integrated with different RAG index including HippoRAGv2 and RAPTOR. This integration consistently results in significant performance improvements across all benchmarks, with accuracy on EN.MC increasing by over 8% for HippoRAGv2 and nearly 12% for RAPTOR (a similar trend is observed on EN.QA). These results demonstrate that ComoRAG could serve as robust and flexible plug-and-play solution to enhance query resolution of existing RAG methods. In-Depth Analysis of Query Resolution 4.4 To deepen the understanding of narrative query resolution, we roughly categorize all questions in our experimented datasets into three query types: factoid, narrative, and inferential, described as follows(details in Appendix C). Factoid Queries: queries answerable by single, specific piece of information, often knowledge-seeking, e.g., What religion is Octavio Amber? Narrative Queries: queries that require an understanding of plot progression as coherent background context, e.g., Where does Trace choose to live at the end of the novel? Inferential Queries: queries demanding reasoning beyond the literal text to understand implicit motivations, e.g., What is the main reason that Nils first visits Aiden in his apartment? To systematically investigate the dynamics of ComoRAG reasoning, we first pose the question: what is the bottleneck in long-narrative reasoning for existing RAG methods? Figure 5 pictures clear diagnosis. While one-shot retrieval suffices for factoid queries, which account for over 60% of initial solution, our iterative cognitive loop is essential for resolving complex narrative queries involving global Figure 5: Distribution of solved question types by processing stage. Figure 6: Benchmarking RAG methods across query types. context comprehension and deeper reasoning. These constitute nearly 50% of the problems that are solved exclusively through the Metacognitive loop. This leads to the second question: how does our frameworks performance on this specific bottleneck compared to strong baselines? Figure 6 demonstrates that our methods advantage is most pronounced precisely in this area. On narrative queries, ComoRAG substantially outperforms the strongest baselines, achieving 19% relative F1 improvement on EN.QA and 16% accuracy gain on EN.MC. By addressing these queries, we demonstrate that the success of our framework is not merely general improvement, but targeted and effective solution to the narrative query type - cornerstone to achieve true narrative comprehension that has posed challenges for prior retrievalbased narrative reasoning approaches. Qualitatively, Figure 2 illustrates the dynamic reasoning mechanism with the query qinit: Mrs. MacIntyre never writes letters, so what is the sudden purpose of buying ink? standard, single-step retrieval would fail on this query, as it would only find vague clue about cut out newspaper, which is insufficient to form an answer. In contrast, ComoRAG initiates an iterative reasoning process by dynamically probing new queries and evidence towards full resolution, thereby constructing complete evidence chain to deduce the final answer: Mrs. McGinty recognized photo, wanted to sell the story, and intended to write to the newspaper. We provide full reasoning details in Appendix E."
        },
        {
            "title": "5 Conclusion\nIn this work, we propose ComoRAG for long narrative rea-\nsoning, aiming to address the “stateless” limitation of con-\nventional RAG. ComoRAG is especially inspired by the hu-",
            "content": "man brains Prefrontal Cortex; through dynamic memory workspace and iterative probes, it fuses fragmented evidence into coherent context to achieve stateful reasoning over narrative progression. Experiments validate that ComoRAG overcomes the bottleneck of existing methods by excelling at complex narrative and inferential queries, marking paradigm shift from information retrieval to cognitive reasoning towards deeper long context comprehension. References Asai, A.; Wu, Z.; Wang, Y.; Sil, A.; and Hajishirzi, H. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In The Twelfth International Conference on Learning Representations. Chen, J.; Xiao, S.; Zhang, P.; Luo, K.; Lian, D.; and Liu, Z. 2024. M3-Embedding: Multi-Linguality, MultiFunctionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. In Ku, L.-W.; Martins, A.; and Srikumar, V., eds., Findings of the Association for Computational Linguistics: ACL 2024, 23182335. Bangkok, Thailand: Association for Computational Linguistics. Dobbins, I. G.; and Han, S. 2006. Cueversus Probedependent Prefrontal Cortex Activity during Contextual ReJournal of Cognitive Neuroscience, 18(9): membering. 14391452. Edge, D.; Trinh, H.; Cheng, N.; Bradley, J.; Chao, A.; Mody, A.; Truitt, S.; Metropolitansky, D.; Ness, R. O.; and Larson, J. 2025. From Local to Global: Graph RAG Approach to Query-Focused Summarization. arXiv:2404.16130. Eisenschlos, J. M.; Yogatama, D.; and Al-Rfou, R. 2023. Needle In Haystack: Where Is It? Finding Factual Associations in Long Texts. arXiv preprint arXiv:2307.09288. Fernandez-Duque, D.; Baird, J. A.; and Posner, M. I. 2000. Executive Attention and Metacognitive Regulation. Consciousness and Cognition, 9(2): 288307. Gutierrez, B. J.; Shu, Y.; Qi, W.; Zhou, S.; and Su, Y. 2025. From RAG to Memory: Non-Parametric Continual Learning for Large Language Models. In Forty-second International Conference on Machine Learning. Jimenez Gutierrez, B.; Shu, Y.; Gu, Y.; Yasunaga, M.; and Su, Y. 2024. Hipporag: Neurobiologically inspired longterm memory for large language models. Advances in Neural Information Processing Systems, 37: 5953259569. Johnson-Laird, P. N. 1983. Mental Models: Towards Cognitive Science of Language, Inference, and Consciousness. Cambridge, MA: Harvard University Press. Kocisky, T.; Schwarz, J.; Blunsom, P.; Dyer, C.; Hermann, K. M.; Melis, G.; and Grefenstette, E. 2017. The NarrativeQA Reading Comprehension Challenge. Transactions of the Association for Computational Linguistics, 6: 317328. Lee, C.; Roy, R.; Xu, M.; Raiman, J.; Shoeybi, M.; Catanzaro, B.; and Ping, W. 2025. NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models. In The Thirteenth International Conference on Learning Representations. Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Kuttler, H.; Lewis, M.; Yih, W.-t.; Rocktaschel, T.; Riedel, S.; and Kiela, D. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neural Information Processing Systems, volume 33, 94599474. Curran Associates, Inc. Liu, N. F.; Lin, K.; Hewitt, J.; Paranjape, A.; Bevilacqua, M.; Petroni, F.; and Liang, P. 2024. Lost in the Middle: How Language Models Use Long Contexts. Transactions of the Association for Computational Linguistics, 12: 157173. Miller, J. A.; and Constantinidis, C. 2024. Timescales of learning in prefrontal cortex. Nature Reviews Neuroscience, 25(9): 597610. Qian, H.; Liu, Z.; Zhang, P.; Mao, K.; Lian, D.; Dou, Z.; and Huang, T. 2025. Memorag: Boosting long context processing with global memory-enhanced retrieval augmentation. In Proceedings of the ACM on Web Conference 2025, 2366 2377. Sarthi, P.; Abdullah, S.; Tuli, A.; Khanna, S.; Goldie, A.; and Manning, C. 2024. RAPTOR: Recursive Abstractive In International Processing for Tree-Organized Retrieval. Conference on Learning Representations (ICLR). Trivedi, H.; Balasubramanian, N.; Khot, T.; and Sabharwal, A. 2023. Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. In Rogers, A.; Boyd-Graber, J.; and Okazaki, N., eds., Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1001410037. Toronto, Canada: Association for Computational Linguistics. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; brian ichter; Xia, F.; Chi, E. H.; Le, Q. V.; and Zhou, D. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. In Oh, A. H.; Agarwal, A.; Belgrave, D.; and Cho, K., eds., Advances in Neural Information Processing Systems. Xu, L.; Li, J.; Yu, M.; and Zhou, J. 2024a. Fine-Grained Modeling of Narrative Context: Coherence Perspective via Retrospective Questions. In Ku, L.-W.; Martins, A.; and Srikumar, V., eds., Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 58225838. Bangkok, Thailand: Association for Computational Linguistics. Xu, Z.; Ye, J.; Liu, X.; Sun, T.; Liu, X.; Guo, Q.; Li, L.; Liu, Q.; Huang, X.; and Qiu, X. 2024b. DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels. ArXiv, abs/2409.02465. Yang, A.; Li, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Gao, C.; Huang, C.; Lv, C.; et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Yang, Z.; Qi, P.; Zhang, S.; Bengio, Y.; Cohen, W.; Salakhutdinov, R.; and Manning, C. D. 2018. HotpotQA: Dataset for Diverse, Explainable Multi-hop Question Answering. In Riloff, E.; Chiang, D.; Hockenmaier, J.; and Tsujii, J., eds., Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 23692380. Brussels, Belgium: Association for Computational Linguistics. Zhang, X.; Chen, Y.; Hu, S.; Xu, Z.; Chen, J.; Hao, M.; Han, X.; Thai, Z.; Wang, S.; Liu, Z.; et al. 2024. Bench: Extending long context evaluation beyond 100k tokens. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1526215277. Zhang, Y.; Li, M.; Long, D.; Zhang, X.; Lin, H.; Yang, B.; Xie, P.; Yang, A.; Liu, D.; Lin, J.; Huang, F.; and Zhou, J. 2025. Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models. arXiv:2506.05176."
        },
        {
            "title": "A ComoRAG Algorithm",
            "content": "Algorithm 1: ComoRAG (Described in Section 2) Require: Initial Query qinit, Knowledge Source , Max"
        },
        {
            "title": "Iterations T",
            "content": "Ensure: The final answer or failure signal 1: function COMORAG(qinit, , ) 2: M(0) pool, (0) hist, {C}(0) , , Initialize Memory Pool, Probing History, and Synthesized Cues 3: 4: 5: 6: 7: (0) Tri-Retrieve ({qinit}, ) O(0) Try-Answer (qinit, (0)) if O(0) = FailureSignal then return O(0) Return immediately if successful end if Triggered only if initial attempt fails 8: M(0) encode Mem-Encode (qinit, (0) pool Mem-Update (M(0) pool, M(0) 9: M(0) (0) hist qinit {C}(0) M(0) pool for = 1, . . . , do hist, (0)) encode) , {C}(t1)) (t) Self-Probe (qinit, (t1) hist (t) Tri-Retrieve (P (t), ) M(t) encode Mem-Encode (qinit, (t), (t)) (cid:1) C(t) use Mem-Fuse (qinit, M(t1) pool qinit encode, C(t) O(t) Try-Answer (qinit, M(t) if O(t) = FailureSignal then return O(t) end if M(t) pool Mem-Update (M(t1) hist (t1) (t) hist (t) {C}(t) M(t) pool pool , M(t) encode) use) 22: 23: 24: 25: end function end for return FailureSignal 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21:"
        },
        {
            "title": "B Implementation Details",
            "content": "B.1 Veridical Layer As described in Section 2.2, ComoRAG empowers Large Language Models by constructing hierarchical knowledge source, whereby the Veridical Layer is foundational component comprising text chunks of the original context. We largely follow the construction process of HippoRAGv2 (Gutierrez et al. 2025) to add mapping between knowledge graphs (KGs) and text chunks to facilitate retrieval. To construct the KG, Large Language Model (LLM) is leveraged to extract (subject-predicate-object) knowledge triples. These triples from document are then aggregated to form unified knowledge graph. Finally, retrieval-optimized encoder adds supplementary edges to this graph by identifying and linking semantically similar entities (synonyms). The retrieval of the Veridical Layer thus follows HippoRAGv2 to utilize KGs towards more accurate retrieval. Statistics for this layer are detailed in Table 4."
        },
        {
            "title": "Count",
            "content": "NarQA EN.QA EN.MC DetQA"
        },
        {
            "title": "Veridical",
            "content": "# of Chunks # of Entities # of Triples 4446 33 810 51 012 26 465 47 074 292 170 401 040 372 339 576 595 2406 30 969 33 696 Table 4: Statistics of the Veridical Layer across Datasets. B.2 Episodic Layer To construct the Episodic Layer, sequence of text chunks is summarized. Since the context lengths can vary significantly, the choice of sliding window size for this summarization presents trade-off: large window can be too coarse for short narratives, while small window may be inefficient and fail to capture long-range dependencies in longform content. Therefore, we dynamically adjust the window size according to the total number of text chunks, , in the document. The specific heuristic is as follows. For short to medium-length narratives (N 200 chunks): stepped window sizes (3, 5, 8, and 10) are used for documents up to 20, 50, 100, and 200 chunks respectively, aiming to preserve details for shorter contexts. For long narratives (N > 200): logarithmic scaling function is applied to prevent the window from becoming excessively large. This sub-linear growth is intended to increase the summary scope for massive texts more slowly. The window size is calculated as follows to keep the window size between 10 to 20: = min(20, max(10, log2(N ) 2)) For each window, the contained text chunks are concatenated and provided to an LLM agent (GPT-4o-mini in our experiments). The agent is instructed to generate concise summary that maintains chronological order and identifies key events and causal relationships. The resulting summaries are then collected and sorted by their original window order to form the nodes of the Episodic Layer. B.3 GraphRAG Experiments GraphRAG is structured-augmented RAG method similar to HippoRAGv2, which involves the construction of comprehensive knowledge graph from source documents, which is then used to identify interconnected information for retrieval. However, its formulation requires heavy computation for building the retrieval index that includes multi-level node relations and summaries. We conducted preliminary experiments on data subset to evaluate its viability. The results, detailed in Table 5, demonstrated that GraphRAG not only had significantly higher token consumption, but also attained lower scores compared to other baselines adopted in our experiments. Considering the trade-offs between its computational cost and performance, we ultimately did not include GraphRAG as primary baseline for full-scale evaluation. B.4 Hyperparameters for ComoRAG The key hyperparameters for our ComoRAG framework are detailed in Table 6. All cognitive agents employ GPT-4oEach question is classified into one of the three categories based on the cognitive processes required to answer it, described in Section 4.4: Factoid: questions answerable by locating single, specific piece of information from the text. Narrative: questions that demand an understanding of plot progression, requiring the aggregation of information from multiple text parts. Inferential: questions that necessitate reasoning beyond the literal text to understand implicit motivations or causal links. The final distribution of the annotated query types is presented in Table 7."
        },
        {
            "title": "Performance Metrics",
            "content": "F1 Score EM Score 33.61 (100.0%) 21.43 (100.0%) 14.20 (42.3%) 8.00 (37.3%)"
        },
        {
            "title": "Token Usage\nTokens",
            "content": "5.90M (100.0%) 27.12M (459.7%) Average Time Taken (sec)"
        },
        {
            "title": "Index\nRetrieve",
            "content": "291 (100.0%) 25 (100.0%) 1936 (665.3%) 29 (116.0%) Table 5: Comparison of Performance, Token Usage, and Average Time for ComoRAG and GraphRAG. mini, with retrieval powered by the widely-used BGE-M3 embedding model. For retrieval settings, The dynamic cognitive loop is configured to run for maximum of 5 iterations, generating up to 3 new probing queries per cycle. The context for QA is capped at 6k tokens, in consistent with all RAG baselines in our experiments. This context is assembled via proportional 8:2:2:1 allocation of evidence from the Veridical, Semantic, Episodic, and fused Historical memory, respectively. The Mem-Fuse Threshold is set to 0.5, indicating the proportion of evidences retrieved from the memory pool that are forwarded to the Integration Agent for memory fusion and summary generation."
        },
        {
            "title": "Value",
            "content": "LLM Agents (πprobe, etc.) GPT-4o-mini Retrieval Model Chunk Size Context Length Random Seed BGE-M3 512 tokens 6,000 tokens"
        },
        {
            "title": "Max Iterations\nMax Probing Queries\nContext Construction",
            "content": "Mem-Fuse Threshold 5 3 Proportional Allocation (8:2:2:1 ratio for V:S:E:H) 0.5 Table 6: Hyperparameter settings for ComoRAG in our experiments. V, S, E, refer to Veridical, Semantic, Episodic, and Historical evidence."
        },
        {
            "title": "Inferential Total",
            "content": "EN.QA EN.MC 224 132 84 46 43 51 351 229 Table 7: Distribution of query types across the two datasets. To facilitate fine-grained analysis of our models performance, we (authors of this work) manually annotated the types of all questions in the EN.QA and EN.MC datasets. Self-Probe"
        },
        {
            "title": "D Prompting Templates",
            "content": "Role: You are an expert in multi-turn retrieval-oriented probe generation. Your job is to extract diverse and complementary retrieval probes from queries to broaden and enrich subsequent corpus search results. Input Materials: Original Query: question or information need that requires comprehensive information retrieval. Context: Available background information, partial content, or relevant summaries. Previous Probes: Previously generated probes from earlier iterations (if any). Task: Based on the query and context, generate up to 3 non-overlapping retrieval probes that explore the query from distinct angles. Critical Requirements: Semantic Differentiation: Ensure new probes are semantically distinct from any previous probes provided. Complementary Coverage: New probes should cover different information dimensions not addressed by previous probes. Relevance Maintenance: All probes must remain directly relevant to answering the original query. Each probe should: Target different information dimensions relevant to the query type: Character-related: actions, motivations, relationships, timeline, consequences Event-related: participants, causes, sequence, location, outcomes Object-related: description, origin, usage, significance, connections Location-related: events occurred, people involved, time periods, significance Expand search scope beyond obvious keywords to capture related content. Avoid semantic overlap with previous probes while maintaining query relevance. Be formulated as effective search terms or phrases. Probe Generation Strategy: When previous probes exist: 1. Analyze Previous Coverage: Identify what semantic domains/angles have been covered. 2. Gap Identification: Find unexplored but relevant information dimensions. 3. Alternative Angles: Generate probes from different conceptual perspectives. 4. Semantic Distance: Ensure sufficient semantic distance from previous probes. When no previous probes exist: Probe 1: Direct elements explicitly mentioned in the query. Probe 2: Contextual elements that might contain the answer. Probe 3: Related concepts or alternative formulations. Output Format: json { \"probe1\": \"Content of probe 1\", ... } Notes: For simple queries, you may generate only 12 probes. If previous probes have covered most relevant angles, generate fewer new probes to avoid redundancy. Prioritize quality and semantic distinctiveness over quantity. Mem-Encode"
        },
        {
            "title": "Instruction Template for Synthesized Cue Generation in Comprehension Agent",
            "content": "Role You are an expert narrative analyst capable of identifying, extracting, and analyzing key information from narrative texts to provide accurate and targeted answers to specific questions. Material You are given the following: 1. final objective to be resolved 2. specific question that needs to be answered 3. Content: Direct excerpts, facts, and specific information from the narrative text"
        },
        {
            "title": "Task",
            "content": "1. Carefully analyze the question to identify: What type of information is being asked (character actions, locations, objects, events, motivations, etc.) Which narrative elements are relevant to answering it The specific details that need to be extracted 2. Systematically scan the content for: Direct mentions of relevant elements (names, places, objects, events) Contextual probes that help answer the question Temporal and spatial relationships Cause-and-effect connections 3. Analyze the identified information considering: Explicit statements (directly stated facts) Implicit information (suggested through context, dialogue, or narrative) Logical connections between different narrative elements Chronological sequence of events if relevant 4. Synthesize findings to construct precise answer to the question. Response Format Provide structured analysis with up to 5 key findings: Key Finding: <Most directly relevant information answering the question> Key Finding: <Supporting evidence or context> Key Finding: <Additional relevant details> Key Finding: <Clarifying information if needed> Key Finding: <Resolution of any ambiguities> Mem-Fuse"
        },
        {
            "title": "Instruction Template for Cue Generation in Integration Agent",
            "content": "Role: You are an expert narrative synthesis specialist who excels at integrating and analyzing information from multiple narrative sources to create coherent and comprehensive insights. Input Material: Previous Analysis: Results from earlier memory fusion operations that contain analyzed narrative information. Current Query: question or information request that needs to be addressed. Task: 1. Review and understand the previous memory fusion outputs: Identify key narrative elements and their relationships. Note any established facts, character developments, or plot points. Recognize patterns and connections across different analyses. 2. Analyze the current query in context: Determine how it relates to previously established information. Identify any new aspects or angles that need to be addressed. Consider how previous insights can inform the current response. 3. Synthesize the information: Integrate relevant previous findings with new analysis. Create coherent narrative that addresses the current query. Ensure continuity and consistency with previous analyses. Highlight any new insights or developments. 4. Provide comprehensive response that: Directly answers the current query. Incorporates relevant previous context. Maintains narrative coherence. Offers clear and insightful analysis. Response Format: Provide cohesive narrative response that integrates previous insights with new analysis to address the current query. Focus on creating flowing, well-structured response. Try-Answer Prompt Template for Query Resolution in QA Agent Role: You are an expert on reading and understanding books and articles. Task: Given the following detailed article, semantic summary, Episodic summary from book, and related question with different options, you need to analyze which option is the best answer for the question. Inputs: Detail Article: {context} Summary by Semantic: {semantic summary} Summary by Episodic: {Episodic summary} History Info: {history info} Question: {question} Limits: Do not infer. Respond only based on the provided content strictly. Pick the choice only if you find at least 2 places that support the answer. Response Format: 1. Content Understanding: Start with brief summary of the content in no more than three sentences. Begin this section with ### Content Understanding. 2. Question Analysis: Based on the question, analyze and list all relevant items using markdown list. Begin this section with ### Question Analyse. 3. Options Analysis: Extract the key points related to 4 options, also using markdown list. Begin this section with ### Options Analyse. Note: Only analyze based on the provided materials, do not make guesses. 4. Final Answer: Provide your final answer with heading. Begin this section with ### Final Answer, followed by the best option in the format of [A] or [B] or [C] or [D]. If you cannot answer, give failure signal: *."
        },
        {
            "title": "E Case Study on Narrative Reasoning",
            "content": "Query: Mrs. MacIntyre never writes letters, so what is the sudden purpose of buying ink? Options: [A] Response to the governments letter [B] Sending birthday card to my niece [C] Write letter to the Sunday Comet newspaper. [D] Blurring Photos Input Data (No Options) ComoRAGs Choice Result Memory Pool (0) - page of the newspaper had portion cut out... pool: Step1 Probes (1): - What did Mrs. McGinty recognize in the clipping, and how did she try to profit from it? ... Retrieved Passages: ...The narrative offers insight into Miss Pamela Hosfords role at the Sunday Comet, as she casually recalls receiving vague letter from woman about photograph but fails to retrieve it... Cues (1): - Key Finding:Mrs. McGinty usually had Joe help her reply to letters.; - Key Finding:Mrs. McGinty may have sent letter to the newspaper.;... Memory Pool (1) - page of the newspaper had portion cut out... - Mrs. MacIntyre sent letter to the Sunday Comet... pool: Step2 Probes (2): - What did Miss Pamela Hosford recall about letter sent to the Sunday Comet, and what might it imply about Mrs. McGinty? ... Retrieved assages: ...Miss Pamela Hosfords role at the Sunday Comet, as she casually recalls receiving vague letter from woman about photograph but fails to retrieve it...She liked eavesdropping, knew who was in the photo, and asked what wed pay... Cues (2): - Key Finding:Mrs. McGinty wanted to make some money from the photo.;... Chosen: C.(Correct) (C) Write letter to the Sunday Comet newspaper: Strong textual probes support this option. Mrs. McGinty cut out part of the newspaper, recognized someone in photo, asked about payment, and unusually bought inksuggesting she intended to write to the paper. Final Answer: [C] Table 8: Case Study on Narrative Reasoning. We present case to demonstrate our models performance in long-context understanding, showing the final round of the Metacognitive Control Loop. Different colors are used to highlight the nature of the processed information: Blue is used for the key evidence that contributes to the correct answer, while Orange is used for the key cues."
        }
    ],
    "affiliations": [
        "Independent Researcher",
        "Pattern Recognition Center, WeChat AI, Tencent",
        "Pazhou Lab, Guangzhou",
        "School of Future Technology, South China University of Technology"
    ]
}