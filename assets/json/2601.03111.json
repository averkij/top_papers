{
    "paper_title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
    "authors": [
        "Yiyuan Li",
        "Zhen Huang",
        "Yanan Wu",
        "Weixun Wang",
        "Xuefeng Li",
        "Yijia Luo",
        "Wenbo Su",
        "Bo Zheng",
        "Pengfei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume."
        },
        {
            "title": "Start",
            "content": "One Sample to Rule Them All:"
        },
        {
            "title": "Extreme Data Efficiency in RL Scaling",
            "content": "Yiyuan Liγ Zhen Huangγ Yanan Wuτ Weixun Wangτ Xuefeng Liγ Yijia Luoτ Wenbo Suτ Bo Zhengτ Pengfei Liuσ γ * Taobao & Tmall Group of Alibaba τ , Shanghai Jiaotong Univeristyσ, GAIRγ yiyuanli@cs.unc.edu"
        },
        {
            "title": "Abstract",
            "content": "6 2 0 2 6 ] . [ 1 1 1 1 3 0 . 1 0 6 2 : r The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in Large Language Models (LLMs) have demonstrated the remarkable effectiveness of reinforcement learning (RL) in enhancing complex reasoning capabilities. Models like o1 (OpenAI, 2024), Deepseek R1 (DeepSeek-AI et al., 2025a), and Kimi1.5 (Team et al., 2025a) have shown that RL training is able to naturally induce sophisticated reasoning behaviors, including self-verification (Weng et al., 2023), reflection (Shinn et al., 2023), and extended chains of thought. While these advances typically rely on large-scale training data, recent work has begun to challenge this paradigm. Li et al. (2025a) demonstrated with their LIMR approach that strategically selected subset of just 1,389 samples can outperform the full 8k sample MATH dataset (Hendrycks et al., 2021). More recently, Wang et al. (2025a) made the surprising observation that even one single sample can produce meaningful improvements in math reasoning through RL, and Wang et al. (2025b) achieved similar gains by distilling high-quality reasoning paths from strong commercial models. However, this finding remains preliminary and math-specific, and leaves the critical questions of cross-domain generalization with internal abilities of LLMs unanswered: whether reasoning improvements beyond math can be achieved in similar manner? Whether strategy exists in directing the optimal sample? Whether such sample can be synthesized to enhance the sample quality? In this paper, we build upon these emerging insights to systematically investigate the phenomenon of one-shot reinforcement learning in broad reasoning tasks termed as polymath learning. Our central finding is that single, carefully selected math reasoning sample is able to produce significant performance gains not only in mathematics but across diverse domains including physics, chemistry, biology, as well as more general reasoning domains. This cross-domain generalization suggests that RL may enhance fundamental reasoning mechanisms rather than merely domain-specific knowledge without saturated domain-specific training. Specifically, our work addresses three research questions: Cross-Domain Generalization: Does single mathematical reasoning sample yield improvements across diverse knowledge domains through polymath learning? We investigate the transfer mechanisms that allow reasoning patterns to transcend domain boundaries and observe that one single math sample selected on the math categories *Corresponding author 1 elicits greater reasoning gains of LLM than comprehensive datasets with thousands of samples, and the reasoning gains even extend to less quantitative subjects and domains that are distant from math. Optimal Sample Selection: What characteristics define the ideal training sample for maximal impact in general reasoning domains? Although the optimal polymath sample varies across domains, we find that their efficacy correlates with the salient math skills critical to reasoning, particularly the algebra and precalculus skills. Synthetic Sample Construction: How can we engineer hybrid meta-sample beyond naturally occurred ones that integrates multiple reasoning skills? We propose synthesis technique through the lens of salient math skills to construct the sample with comprehensive skill coverage and multidisciplinary context. The results illustrate that the multidisciplinary background strengthens the comprehensiveness of the salient skills, and yields greater cross-domain reasoning gains than the natural samples that mainly possess math skills in limited categories and volume. It highlights the power of individual sample amplified by properly enriching its internal multidisciplinary knowledge. By demonstrating that single sample can trigger broad and transferrable reasoning improvements, our findings refine the current understanding of data requirements in RL, suggesting that the field may benefit from shift toward sample engineering: deliberate selection, and synthesis of training samples to unlock reasoning capabilities more efficiently, rather than simply scaling data volume, which may potentially induce generalization degradation (Yang et al., 2024b)."
        },
        {
            "title": "2 Related Work",
            "content": "Reinforcement Learning in Language Models Reinforcement learning has been applied to aligning language models with human intents (Christiano et al., 2017) or instructions (Ouyang et al., 2022) through learning from human feedback. Later, it is extended to strengthen the long-form reasoning ability of models without relying on imitation of high-quality reasoning data, specifically by employing Reinforcement Learning with Verifiable Reward (RLVR) where the model outcomes can be verified and rewarded by verification functions with the advancement in RL algorithms (Schulman et al., 2017; Lambert et al., 2025; Hu et al., 2025a). However, training reliable outcome-based reward models (Cobbe et al., 2021) is challenging, and the rule-based reward function demonstrates effectiveness by simplifying the implementation of critic models and mitigating reward hacking (Shao et al., 2024). In this work, we extend the reasoning ability to broader reasoning domains by learning intensively from one high quality sample. Data Efficiency in Reinforcement Learning Xu et al. (2025) selects variance-based subset responses for GRPO training. Zhang et al. (2025a) employs the most recent reward information for filtering prompts, which is beneficial to GRPO training (Yu et al., 2025b). Other than focusing on the response quality in RL training, Li et al. (2025a) highlights the significance of prompt quality by demonstrating the effectiveness of carefully selected training subset. Further, Shrestha et al. (2025) demonstrates cross-domain reasoning ability with less than 100 samples but requires pre-warmup distillation stage, and Wang et al. (2025a) utilizes only one training sample and achieves notable improvement in mathematical reasoning. And Zhao et al. (2025a) requires no human-expert data but still relies on an external executor to generate valid answers to synthetic coding problems. However, these studies still focusing on the mathematical reasoning domain where the training data originates and neglect its broader impacts on multiple disciplines where the reasoning ability essences. Transfer Learning and Cross-Domain Generalization Afzal et al. (2024) demonstrates that small LLMs can catch up with larger counterparts in domain adaptation with few examples. And Chen et al. (2024) adapts models to new domain by extracting domain-invariant features in existing domain. For reasoning problems, Zhao et al. (2025a) unleashes an improvement in mathematical reasoning soly based on training on programming data, and Huan et al. (2025) demonstrates that RL achieves better generalization from math to other domains than supervised fine-tuning, without deep dive into data efficiency. Li et al. (2025b) investigates the cross-domain impact in math reasoning, but only limits the study within logical-intensive domains like code and puzzle. In polymath learning, we enlarge the reasoning scope to various subjects and investigate the learning impact from one labeled math sample. Sample Selection Strategies The effectiveness of finetuning large language models heavily is heavily dependent on the quality of data selection (Xie et al., 2023). And well selected data samples can elicit powerful fine-tuning performance compared to data volume of magnitudes larger (Wang et al., 2023; Zhou et al., 2023). Xia et al. (2024) relies on the gradient information for data selection, while Liu et al. (2024b) formulates data selection as an optimal transportation problem. The effectiveness of data selection also extends to reasoning problems (Qin et al., 2024; Ye et al., 2025). Liu et al. (2024a); Li et al. (2025c) apply LLM-based scores, justification, solve ratios (Havrilla et al., 2025) and LLM-based role-play (Luo et al., 2025) to estimate sample diversity for data selection. Here we select polymath samples based on the alignment with reinforcement learning dynamics to elicit the reasoning ability in multiple disciplines. And we employ the salient-skill set to for selecting the synthesized data."
        },
        {
            "title": "3 GRPO Basics",
            "content": "Given dataset = {(x, ˆy)} where and ˆy stand for the prompt and golden answer, RLVR relies on policy model πθ(x) to generate correct reasoning trajectories without relying on trajectories generated by human-expert or teacher models (Zhao et al., 2025a). In GRPO (Shao et al., 2024), the advantage value is estimated within group of responses responses {y1, y2, ..., yG} to substitute the critic model in PPO while remaining effectiveness. Specifically, LGRP = E[xD,{yi}πθold (x)]["
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 yi yi (cid:88) t=1 min(ri,tAi, clip(ri,t, 1 ϵ, 1 + ϵ)Ai) βKL(πθπref )] Ai = ri mean(r1, r2, ...rG) std(r1, r2, ...rG) , ri,t = πθ(yi,tx, yi,<t) πθold(yi,tx, yi,<t) Here ri is computed by applying the reward function on the response and the golden answer ri = reward(yi, ˆyi). πθ(yi,tx, yi,<t) identifies the likelihood of the t-th token in i-th response from the policy model. Unlike previous efforts that assembles with comprehensive set of samples, in polymath learning, the dasataset consists of one valid sample and Dpolymath = (x1, ˆy1)."
        },
        {
            "title": "4 Polymath Learning",
            "content": "OpenAI et al. (2024) unlocks complex reasoning ability of LLM through reinforcement learning, and DeepSeek-AI et al. (2025b,a) further demonstrates that such advanced reasoning ability can be elicited directly from pretrained base models using rule-based rewards, without relying on imitation from high-quality supervised reasoning trajectories. Existing explorations mainly focus on math or synthetic logic (Zeng et al., 2025; Pan et al., 2025; Xie et al., 2025) where large volumes of questions with rule-based verifiable answers are accessible. Beyond the success of comprehensive learning: training models with thousands of comprehensive high-quality problems and beyond, Wang et al. (2025a) shows that the reasoning ability can also be boosted by one single math sample with RL. Following this inquiry, we investigate polymath learning: training with one sample that plays polymath role and extends the model reasoning power across domains. Similar to Wang et al. (2025a), we conduct polymath learning from math reasoning problems. Polymath Learning with One Natural Sample LIMR (Li et al., 2025a) displays the potential of improving training efficiency in reinforcement learning by selecting subset of samples from MATH that closely align with the training dynamics of RL. preliminary model is trained in LIMR to record the reward trajectories during optimization. The sample learnability is then computed by comparing its outcome reward with the dataset-wise average of outcome rewards. Higher LIMR scores indicate greater alignment between the model behavior on individual sample and the entire dataset during RL training. However, learning from samples with excessively high LIMR scores risks over-specialization in math reasoning at the expense of the broader reasoning capabilities in other disciplines. Therefore, we select LIMR samples with the lowest scores (0.6) in different math categories as polymath candidates to maintain the same learnability according to preliminary experiments. One polymath sample is displayed in Table 1 and others are included in Appendix M."
        },
        {
            "title": "Polymath Sample in Algebra",
            "content": "[Question] 100-gon P1 is drawn in the Cartesian plane. The sum of the x-coordinates of the 100 vertices equals 2009. The midpoints of the sides of P1 form second 100-gon, P2. Finally, the midpoints of the sides of P2 form third 100-gon, P3. Find the sum of the x-coordinates of the vertices of P3. [Answer] 2009 Table 1: Polymath sample in algebra. Polymath Learning with One Synthetic Sample Synthesizing reasoning trajectories have been shown beneficial in boosting the reasoning ability in LLM in the pretraining (Ishibashi et al., 2025) and supervised-finetuning stage (Singh et al., 2024; Yuan et al., 2024). Careful problem synthesis also scales up the mathematical reasoning ability of models by reinforcement learning (Setlur et al., 2024). Since solving multidisciplinary problems and purely mathematical problems are not require on the same base of expertise, existing problem synthesis approaches based on problem imitation (Toshniwal et al., 2025), mutation (Havrilla et al., 2025) or creation based on seed concept or problem bank (Huang et al., 2025; Liang et al., 2025; Zhao et al., 2025b; Liu et al., 2025) do not directly 3 apply. In practice, we find it challenging to organically integrate and align information from problems in diverse disciplines. Therefore, unlike Setlur et al. (2024) and Wang et al. (2025b), we synthesize the polymath sample based on instruction without relying on existing problems or models finetuned with question-generation (Ding et al., 2025; Wu et al., 2025b). Our final problem synthesis pipeline includes two stages, Candidate problem generation We employ strong models like OpenAI-O3 (OpenAI, 2025a), Gemini2.5Pro (Google, 2025) and DeepSeek-R1 to integrate knowledge from physics, chemistry, and biology. The golden answers are collected from the joint success in problem solving of these models. Specialized problem selection After massive collection of candidate problems, we employ Qwen2.5-72Binstruct to identify the salient math skills related in solving the problem given the problem text. The abundance of skills in different math categories is employed to reflect the complexities and qualities of problems. We then select the problems with the most specialized skills as the synthesized polymath samples, please refer to Appendix for the prompt employed and Appendix for example. We find this instruction-based approach unleashes the creativity of LLMs in producing complex multidisciplinary problems. Specifically, we select the synthesized polymath sample with the most comprehensive skill spectrum (Synthetic Prime, shown in Table 2). Solving the Synthetic Prime requires complex set of knowledge, including the strand sequence (biology), chemical bonds and energy to break bonds (chemistry), accumulating energy by collecting photons and estimating photon energy based on its wavelength (physics). The synthesis prompt is shown in Appendix A. Polymath Sample (Synthetic Prime) [Question] double-stranded DNA fragment of exactly 11 base pairs has the upper strand sequence 5 G G A 3. Each adeninethymine (AT) base pair is held together by **2** hydrogen bonds, and each guaninecytosine (GC) base pair by **3** hydrogen bonds. The DNA molecule is irradiated with monochromatic light of wavelength λ = 400nm. Assume that **100%** of every photons energy is used exclusively to break hydrogen bonds between the two strands. Use the exact data below (treat every value as exact): * Enthalpy of one hydrogen bond = 20kJ mol1 * Planck constant = 6.626 1034J * Speed of light = 3.00 108m s1 * Avogadro constant Na = 6.022 1023mol **Fill in the blank:** What is the minimum number of 400 nm photons required to supply exactly enough energy to dissociate *all* hydrogen bonds in **one** molecule of this DNA fragment? (Answer with single positive integer.) [Answer] 2 Table 2: The synthetic prime polymath sample that incorporates multidisciplinary knowledge."
        },
        {
            "title": "5 Experimental Setup",
            "content": "We choose Qwen2.5-7b-base (Qwen et al., 2025) as the primary model, while Qwen2.5-math models (Yang et al., 2024a) demonstrate inferior performance on non-math benchmarks in preliminary experiments and are therefore not considered. Similar to Wang et al. (2025a), we employ GRPO (Shao et al., 2024) for RL training and augment the polymath sample into the batch of 128, and sample 16 responses per prompt with temperature of 1.0. The prompt template follows the design of Hu et al. (2025b). Following Huan et al. (2025), the model is trained for 140 steps since the reasoning ability saturates. We only employ 0-1 outcome reward with rule-based matching of the final answer according to previous studies (Shao et al., 2024; Yu et al., 2025b), and exclude the format reward and the KL term as they demonstrate inferior performance (Wang et al., 2025a; Yu et al., 2025b). In skill identification, we employ Algebra to include salient skills from Prealgebra, Algebra and Intermediate Algebra to eliminate their large overlaps. Our evaluation covers both math and non-math domains. Specifically, we select MATH500, AIME in 2024 and 2025, MinervaMath (Lewkowycz et al., 2022), GPQA-Diamond (Rein et al., 2024), Scibench (Wang et al., 2024a), MMLU-Pro (Wang et al., 2024b) with randomly select 100 problems for each subject and SuperGPQA (Team et al., 2025b) with 1500 random problems as the evaluation set. The full spectrum of subjects is listed in Appendix E. The model responses are generated with greedy decoding in single attempt, except for AIME, where the results are averaged from 32 attempts with temperature being 0.4 (additional configurations are included in Appendix A). 4 Table 3: The performance of employing different sample strategies on different subject domains. The best performance on each subject domain is bolded. Most natural polymath samples outperforms in-context learning and comprehensive learning with LIMR selection. Most synthetic specialist samples outperforms the corresponding natural sample, and the Synthetic Prime sample demonstrates the best performance. The dataset-wise results is included in Appendix C. Polymath Subject Math Physics Chemistry Biology Science Engineering Computer Science Others Avg - 20.4 4.4 24.5 22. 21.4 22.7 21.7 21.6 22.4 18. 37.2 38.0 15.5 38.0 37.3 36. 37.7 38.0 38.8 35.4 37.3 38. 37.1 37.1 38.3 Geometry Prealgebra Algebra Intermediate Algebra Number Theory Precalculus Probability Prime MATH LIMR Geometry Prealgebra Algebra Intermediate Algebra Number Theory Precalculus Probability Geometry Algebra Number Theory Precalculus Probability Prime"
        },
        {
            "title": "6 Results",
            "content": "8.0 11.2 10.9 8.0 10.9 8. 9.7 4.6 12.8 11.6 9.9 17. 17.4 19.1 16.9 18.4 19.9 15. 16.9 18.2 20.3 16.7 20.6 N=64 Sampling (0-shot) 5.1 0.0 3.7 In-context Learning (1-shot) Natural Sample 4.3 24.4 40. 38.7 21.8 31.9 20.2 6.8 8. 4.5 5.4 5.2 24.4 5.6 Synthetic Sample 2.2 8. 6.0 10.2 10.4 9.5 6.6 6. 7.7 4.6 4.4 7.2 9.4 9. 7.0 8.7 5.9 7.2 4.6 Comprehensive Learning (> 1k shots) Natural Sample 6. 10.0 31.4 8.6 10.0 48.3 11.8 Polymath Learning (1-shot) - Ours Natural Sample 11. 10.0 55.1 13.4 16.7 12.2 13. 13.1 12.4 13.7 11.5 11.5 12. 12.0 15.3 13.9 15.7 51.7 51. 50.0 49.2 50.0 15.1 12.1 13. 13.4 16.0 46.6 14.7 Synthetic Sample 36.1 31.1 31. 32.1 32.9 30.1 54.2 41.2 36. 44.3 46.4 15.6 16.5 15.6 17. 17.8 19.7 16.4 52.5 52.5 47. 48.3 50.0 20.8 3.3 29.0 35. 35.0 32.0 28.0 26.0 22.0 11. 25.8 35.1 37.1 49.5 43.3 42. 42.3 43.3 41.2 13.2 18.6 18. 20.8 19.7 48.5 9.6 6.4 11. 20.3 20.6 15.5 15.8 11.9 13. 14.4 19.4 19.4 15.1 16.1 13. 14.0 7.7 7.7 23.4 31.5 35. 33.5 30.9 31.1 32.2 31.0 31. 11.0 13.9 13.8 16.5 10.8 32. 19.5 25.0 23.8 29.2 27.7 27. 27.7 28.8 27.6 25.7 28.1 27. 29.4 28.1 30."
        },
        {
            "title": "6.1 Cross-Domain Generalization of Learning on Single Polymath Sample",
            "content": "Table 3 reports the reasoning performance aggregated by subject domains (e.g. Math includes all math problems from MATH500, AIME, MinervaMath and other benchmarks). Models trained with various natural and synthetic polymath samples are compared against the base model. In addition to the Synthetic Prime sample, we construct several synthetic specialist samples across different math categories by selecting instances containing the highest number of salient skills identified in those categories. Here, we make several observations. Firstly, the base model exhibits imbalanced reasoning abilities: performing strongly in math but weakly in other domains. Secondly, polymath learning delivers substantial improvements over in-context learning across different subject domains. Thirdly, although comprehensive learning enhances the math reasoning ability of the base model, especially with effective data selection strategies like LIMR, most natural polymath samples demonstrate comparable performance to comprehensive learning on the math domain, and surpass it on non-math domains, underscoring the potential of single high-quality sample in unlocking reasoning ability. Notably, polymath samples in prealgebra and precalculus stand out, exhibiting superior performance due to their wide coverage of salient math skills (Sec 6.2). Lastly, synthetic polymath samples further elevate the reasoning ability. Most specialist samples outperform their natural 5 polymath sample counterparts and demonstrate domain-specific advantages: geometry and algebra samples excel in engineering; number theory sample in math and probability sample in science. Furthermore, the Synthetic Prime sample achieves the strongest overall performance and demonstrates particular strength in physics and chemistry, suggesting that the reasoning potential of individual samples can be amplified through well-incorporation of multidisciplinary knowledge. Therefore we select the Synthetic Prime sample as the primary synthetic sample for subsequent experiments. Unlike data collection approaches that are based on widely crawled sources (Wu et al., 2025a; He et al., 2025; Zhang et al., 2025b), our polymath samples do not rely on seed data to construct or displaying evidence of data contamination. Please refer to Appendix for the specialist samples. The breakdown performance of sampling (0-shot pass rate@64), polymath learning and comprehensive learning by subjects is visualized in Figure 1, with subjects ordered by their similarities to math. The similarity is measured by computing the subject embedding distance between the mean of embeddings of all problems in each subject and the mean of problems in MATH500. We employ Text-Embedding-3-Small (OpenAI, 2025b) with the dimension of 1024 to generate problem representations. The best performance of polymath learning and in-context learning of polymath samples are displayed with triangles and stars, respectively. We include our major findings, Figure 1: The subject-level performance of different learning strategies. OE stands for subjects with open-ended problems. The subjects are sorted by subject embedding distance to MATH500 (the grey dotted line), from low to high. The blue line represents pass ratio from 64 independent attempts of the base model. The stars and triangles represent best performance of in-context learning and polymath learning. Note that we only display the best polymath learning and in-context polymath learning results for demonstration. Strong mathematical but skewed reasoning of the base model Due to the massive mathematical and coding data participated in pretraining (Qwen et al., 2025; Wu et al., 2025a), the Qwen2.5-7b-base model achieves pass rate@64 > 0.5 in MATH500, higher than all other subjects with significant margins. However, the strength in MATH500 does not naturally extend to other subjects. For example, the base model performs poorly on physics, chemistry and biology, but demonstrates relative strength (pass rate@64 close to 0.2) in education, medicine, sociology and management, which does not possess similar proportion of quantitative components as math does. Comprehensive learning provides mathematical dominance, but not multidisciplinary Comprehensive learning with MATH or LIMR sets demonstrate strong performance in MATH500, and remain competitive with the strongest polymath sample in other math subjects (math, minerva). However, their performance on most non-math subjects lags far behind from the best polymath results. The reasoning strengths gained from math-specific training generalize only to limited set of subjects, like economics, health, psychology, education, and history where more than fourfold performance improvement over zero-shot reasoning is observed. Nonetheless, quality-driven data selection remains beneficial in comprehensive learning, with LIMR consistently outperforming MATH in most 6 subjects. The training dynamics further reveals the overfitting of comprehensive learning in multidisciplinary benchmarks (see Appendix for details). The effectiveness of in-context learning of polymath samples The best in-context polymath learning sample outperforms 0-shot pass rate@64 baseline in most subjects, highlighting the efficacy of polymath samples even under gradient-free learning. Moreover, we observe that the specifc polymath samples (e.g. prealgebra or algebra) are able to achieve performance on par with, or superior to, at least one model trained via comprehensive learning in over 50% of subjects, with details included in Appendix L. Better generalization of polymath learning on math-distant subjects Even though the best polymath sample outperforms comprehensive learning in LIMR on math-intensive domains like math and engineering, its advantage is more pronounced on subjects that are semantically distant from math. For example, it demonstrates around 10 points gains in agronomy, literature and sociology. On average, polymath learning with the best natural samples yields 14.5 points improvement over comprehensive learning on the full MATH set on the 50% subjects farthest from MATH500, compared to 7.7 points gain on the 50% subjects closest to MATH500. This pattern suggests that polymath learning promotes stronger reasoning generalization in less math-intensive subjects."
        },
        {
            "title": "6.2 Characteristics of Optimal Polymath Sample",
            "content": "Data diversity is beneficial in training more capable reasoning LLMs (Zhang et al., 2025b), serving both regularization to the neural network (Ba et al., 2025) and mean to mitigate performance saturation especially when leveraging synthetic data sources (Jung et al., 2025). In polymath learning, we extend beyond the diversity at the level of problem or trajectory (Yu et al., 2025a)and instead examine the composition of salient mathematical skills within individual polymath samples. The result in Figure 2 illustrates the key supporting role of algebra and precalculus skills in cross-domain reasoning. Polymath samples demonstrate stronger performance tend to exhibit high prevalences of these skills. Furthermore, synthetic specialist samples with multidisciplinary backgrounds span broader range of skills than math-specialized samples of the same specialty, which accounts for their superior performance. Notably, the Synthetic Prime sample exhibits the highest concentration of salient skills, suggesting that solving such problems requires complex interplay of knowledge and thus provides rich learning signals for training LLMs. The comparison with other out-of-MATH 1-shot sample is included in Appendix H. Figure 2: Skill spectrum between natural and synthetic polymath samples. The polygon represents number of salient skills identified in each math domain (Geo. and Precal. represents Geometry and Precalculus respectively). The real and dashed areas represent the natural and synthetic specialist samples except the last one, which represents the Synthetic Prime sample, and the synthetic samples include more comprehensive salient skill sets than the natural polymath samples. The distribution of salient skills across subject domains further highlights the central roles of algebra and precalculus. Skill abundance also reflects the degree of domain specialization. For instance, in engineering, the most frequent algebraic and geometric skills are unit conversion and trigonometry. Figure 3 shows that algebra and precalculus consistently dominate in skill popularity, underscoring their foundational importance for quantitative reasoning (e.g., unit conversion and arithmetic operations). Moreover, domains with integrative knowledge, such as science and engineering, demand more comprehensive combinations of salient skills compared to discipline-focused domains such as math, physics, chemistry, or biology. 7 Figure 3: Average number of mathematical skills employed per problem in different subject domains. Algebra and Precalculus skills are the most prevalent."
        },
        {
            "title": "7 Generalization of Self-Verification",
            "content": "The verification mechanism act as signal for models to reconsider and refine their initial solutions (DeepSeek-AI et al., 2025a). Verification feedback can further enhance decision-making (Madaan et al., 2023; Shinn et al., 2023). To analyze such behavior, several signature words have been proposed for monitoring self-verification patterns (Xie et al., 2025). Following this, we collect pattern statistics across polymath learning samples, adding the code category to capture python-based program verification and excluding reevaluate for its rare appearance. We find that polymath learning in general demonstrates more frequent self-verification behavior than comprehensive learning. Moreover, the polymath sample in number theory and intermediate algebra exhibit strong tendencies in eliciting the self-checking (re-evaluate) behavior and programming assistance (code) respectively. Moreover, different polymath samples display distinct self-verification preferences depending on the subject domain, with details in Appendix G. Figure 4: Self-verification patterns under different comprehensive and polymath samples across all subjects. Verification patterns like re-evaluate and recheck appear most frequently in polymath learning with the number theory sample, and the intermediate algebra sample elicits the most code blocks in reasoning. Similar to Shao et al. (2025), we observe frequent use of program verification in the polymath sample of intermediate algebra. However, the role of programs varies across domains: the programs in math are primarily used as part of the final answer generation process, including pseudo-execution errors like Timed out; in physics and chemistry, by contrast, the programs are employed more for result validation. Importantly, without the access of external executor, the integration of program does not necessarily yield reasoning gains. Illustrative examples are provided in Appendix N."
        },
        {
            "title": "8 Limitations and Future Work",
            "content": "In polymath learning, we focus our study in the effectiveness of one single training sample in lifting interdisciplinary reasoning ability with reinforcement learning. Due to resource constraints, our study only covers small set of samples without larger-scale experiments in one-shot polymath learning. And the sample selection based on salient skills does not extend to scaled skill-based problem synthesis like Havrilla et al. (2025). Although we observe different verification pattern preferences by choosing polymath samples, we do not observe direct connection between the self-verification and the improvement in reasoning abilities. Besides, the polymath learning experiments are only conducted in open-ended format, while previous studies have demonstrated the benefits of incorporating diverse question-answer formats (Akter et al., 2025), especially for benchmarks that are in multiple-choice formats. Moreover, our study mainly focuses polymath samples in math or employing math skills and does not extend to other domains where reliable rewards are accessible."
        },
        {
            "title": "9 Conclusion",
            "content": "We employ learning rate of 1e-6 during training, with ϵ being 0.2. The maximum generation length is set to 2048. The configuration to collect zero-shot sampling for base model is listed in Table 4. The prompt used is displayed in Table 5, and the prompt to synthesize polymath samples is shown in Table 6. Around 500 candidate problems are synthesized on the candidate problem generation stage. The prompt employed for math skill identification is displayed in Table 7."
        },
        {
            "title": "References",
            "content": "[1] Anum Afzal, Ribin Chalumattu, Florian Matthes, and Laura Mascarell. 2024. AdaptEval: Evaluating large language models on domain adaptation for text summarization. In Proceedings of the 1st Workshop on Customizable NLP: Progress and Challenges in Customizing NLP for Domain, Application, Group, or Individual (CustomNLP4U), pages 7685, Miami, Florida, USA. Association for Computational Linguistics. [2] Syeda Nahida Akter, Shrimai Prabhumoye, Matvei Novikov, Seungju Han, Ying Lin, Evelina Bakhturina, Eric Nyberg, Yejin Choi, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2025. Nemotron-crossthink: Scaling self-learning beyond math reasoning. [3] Yang Ba, Michelle V. Mancenido, and Rong Pan. 2025. Data diversity as implicit regularization: How does diversity shape the weight space of deep neural networks? [4] Yue Chen, Chen Huang, Yang Deng, Wenqiang Lei, Dingnan Jin, Jia Liu, and Tat-Seng Chua. 2024. STYLE: Improving domain transferability of asking clarification questions in large language model powered conversational agents. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1063310649, Bangkok, Thailand. Association for Computational Linguistics. [5] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep In Advances in Neural Information Processing Systems, reinforcement learning from human preferences. volume 30. Curran Associates, Inc. [6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. [7] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang 9 Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025a. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. [8] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. 2025b. Deepseek-v3 technical report. [9] Yuyang Ding, Xinyu Shi, Xiaobo Liang, Juntao Li, Zhaopeng Tu, Qiaoming Zhu, and Min Zhang. 2025. Unleashing llm reasoning capability via scalable question synthesis from scratch. [10] Elvis Dohmatob, Mohammad Pezeshki, and Reyhane Askari-Hemmat. 2025. Why less is more (sometimes): theory of data curation. [11] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. 2025. Omni-MATH: universal olympiad level mathematic benchmark for large language models. In The Thirteenth International Conference on Learning Representations. [12] Google. 2025. Gemini-2.5-pro. [13] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzman, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, 10 Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vıtor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, 11 Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The llama 3 herd of models. [14] Alex Havrilla, Edward Hughes, Mikayel Samvelyan, and Jacob Abernethy. 2025. Sparq: Synthetic problem generation for reasoning via quality-diversity algorithms. [15] Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2025. Deepmath103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. [16] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). [17] Jian Hu, Jason Klein Liu, Haotian Xu, and Wei Shen. 2025a. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models. [18] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. 2025b. Openreasoner-zero: An open source approach to scaling up reinforcement learning on the base model. [19] Maggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig, and Xiang Yue. 2025. Does math reasoning improve general llm capabilities? understanding transferability of llm reasoning. [20] Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, and Weizhu Chen. 2025. Key-point-driven data synthesis with its enhancement on mathematical reasoning. Proceedings of the AAAI Conference on Artificial Intelligence, 39(23):2417624184. [21] Yoichi Ishibashi, Taro Yano, and Masafumi Oyamada. 2025. Mining hidden thoughts from texts: Evaluating continual pretraining with synthetic data for llm reasoning. [22] Jaehun Jung, Seungju Han, Ximing Lu, Skyler Hallinan, David Acuna, Shrimai Prabhumoye, Mostafa Patwary, Mohammad Shoeybi, Bryan Catanzaro, and Yejin Choi. 2025. Prismatic synthesis: Gradient-based data diversification boosts generalization in llm reasoning. [23] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. 2025. Tulu 3: Pushing frontiers in open language model post-training. [24] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative reasoning problems with language models. In Advances in Neural Information Processing Systems, volume 35, pages 38433857. Curran Associates, Inc. [25] Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025a. Limr: Less is more for rl scaling. [26] Yu Li, Zhuoshi Pan, Honglin Lin, Mengyuan Sun, Conghui He, and Lijun Wu. 2025b. Can one domain help others? data-centric study on multi-domain reasoning via reinforcement learning. [27] Zhuo Li, Yuhao Du, Xiaoqi Jiao, Yiwen Guo, Yuege Feng, Xiang Wan, Anningzhe Gao, and Jinpeng Hu. 2025c. Add-one-in: Incremental sample selection for large language models via choice-based greedy paradigm. [28] Xiao Liang, Zhong-Zhi Li, Yeyun Gong, Yang Wang, Hengyuan Zhang, Yelong Shen, Ying Nian Wu, and Weizhu Chen. 2025. Sws: Self-aware weakness-driven problem synthesis in reinforcement learning for llm reasoning. [29] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2024a. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. In The Twelfth International Conference on Learning Representations. [30] Weize Liu, Yongchi Zhao, Yijia Luo, Mingyu Xu, Jiaheng Liu, Yanan Li, Xiguo Hu, Yuchi Xu, Wenbo Su, and Bo Zheng. 2025. Designer: Design-logic-guided multidisciplinary data synthesis for llm reasoning. [31] Zifan Liu, Amin Karbasi, and Theodoros Rekatsinas. 2024b. Tsds: Data selection for task-specific model finetuning. In Advances in Neural Information Processing Systems, volume 37, pages 1011710147. Curran Associates, Inc. [32] Jing Luo, Longze Chen, Run Luo, Liang Zhu, Chang Ao, Jiaming Li, Yukun Chen, Xin Cheng, Wen Yang, Jiayuan Su, Ahmadreza Argha, Hamid Alinejad-Rokny, Chengming Li, Shiwen Ni, and Min Yang. 2025. Personamath: Boosting mathematical reasoning via persona-driven data augmentation. 12 [33] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems. [34] OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quinonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. 2024. Openai o1 system card. [35] OpenAI. 2024. Learning to reason with llms, september 2024. [36] OpenAI. 2025a. Introducing openai o3 and o4-mini. [37] OpenAI. 2025b. Openai text-embedding-3-small. [38] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 2773027744. Curran Associates, Inc. [39] Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. 2025. Tinyzero. https://github.com/Jiayi-Pan/TinyZero. Accessed: 2025-01-24. [40] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. 2024. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982. [41] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. [42] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2024. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. [43] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. [44] Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. 2024. Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. In Advances in Neural Information Processing Systems, volume 37, pages 4300043031. Curran Associates, Inc. [45] Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, and Luke Zettlemoyer. 2025. Spurious rewards: Rethinking training signals in rlvr. [46] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. [47] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: In Thirty-seventh Conference on Neural Information language agents with verbal reinforcement learning. Processing Systems. [48] Safal Shrestha, Minwu Kim, Aadim Nepal, Anubhav Shrestha, and Keith Ross. 2025. Warm up before you train: Unlocking general reasoning in resource-constrained settings. [49] Avi Singh, John Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alexander Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Fathy Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. 2024. Beyond human data: Scaling self-training for problem-solving with language models. Transactions on Machine Learning Research. Expert Certification. [50] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. 2025a. Kimi k1.5: Scaling reinforcement learning with llms. [51] M-A-P Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixing Deng, Shuyue Guo, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Junting Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jingyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, and Ge Zhang. 2025b. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. [52] Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. 2025. Openmathinstruct-2: Accelerating AI for math with massive open-source instruction data. In The Thirteenth International Conference on Learning Representations. 14 [53] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. 2024a. SciBench: Evaluating College-Level Scientific ProblemSolving Abilities of Large Language Models. In Proceedings of the Forty-First International Conference on Machine Learning. [54] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. 2025a. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571. [55] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1348413508, Toronto, Canada. Association for Computational Linguistics. [56] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024b. MMLU-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. [57] Yubo Wang, Ping Nie, Kai Zou, Lijun Wu, and Wenhu Chen. 2025b. Unleashing the reasoning potential of pre-trained llms by critique fine-tuning on one problem. arXiv preprint arXiv:2506.03295. [58] Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. 2025c. Octothinker: Mid-training incentivizes reinforcement learning scaling. [59] Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. In Findings of the Association 2023. Large language models are better reasoners with self-verification. for Computational Linguistics: EMNLP 2023, pages 25502575, Singapore. Association for Computational Linguistics. [60] Mingqi Wu, Zhihao Zhang, Qiaole Dong, Zhiheng Xi, Jun Zhao, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Yanwei Fu, Qin Liu, Songyang Zhang, and Qi Zhang. 2025a. Reasoning or memorization? unreliable results of reinforcement learning due to data contamination. [61] Zijian Wu, Jinjie Ni, Xiangyan Liu, Zichen Liu, Hang Yan, and Michael Qizhe Shieh. 2025b. Synthrl: Scaling visual reasoning with verifiable data synthesis. [62] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. 2024. Less: Selecting influential data for targeted instruction tuning. [63] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. 2023. Data selection for language models via importance resampling. [64] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. 2025. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. [65] Yixuan Even Xu, Yash Savani, Fei Fang, and Zico Kolter. 2025. Not all rollouts are useful: Down-sampling rollouts in llm reinforcement learning. [66] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. 2024a. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. [67] Haoran Yang, Yumeng Zhang, Jiaqi Xu, Hongyuan Lu, Pheng-Ann Heng, and Wai Lam. 2024b. Unveiling the generalization power of fine-tuned large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 884899, Mexico City, Mexico. Association for Computational Linguistics. [68] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. Limo: Less is more for reasoning. [69] Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. 2025a. Flow of reasoning: Training LLMs for divergent reasoning with minimal examples. In Forty-second International Conference on Machine Learning. [70] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. 2025b. Dapo: An open-source llm reinforcement learning system at scale. 15 [71] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2024. Scaling relationship on learning mathematical reasoning with large language models. [72] Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 2025. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https: //hkust-nlp.notion.site/simplerl-reason. Notion Blog. [73] Xiaojiang Zhang, Jinghui Wang, Zifei Cheng, Wenhao Zhuang, Zheng Lin, Minglei Zhang, Shaojie Wang, Yinghan Cui, Chao Wang, Junyi Peng, Shimiao Jiang, Shiqi Kuang, Shouyu Yin, Chaohang Wen, Haotian Zhang, Bin Chen, and Bing Yu. 2025a. Srpo: cross-domain implementation of large-scale reinforcement learning on llm. [74] Xuemiao Zhang, Chengying Tu, Can Ren, Rongxiang Weng, Hongfei Yan, Jingang Wang, and Xunliang Cai. 2025b. Large-scale diverse synthesis for mid-training. [75] Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. 2025a. Absolute zero: Reinforced self-play reasoning with zero data. [76] Xueliang Zhao, Wei Wu, Jian Guan, and Lingpeng Kong. 2025b. PromptCoT: Synthesizing olympiad-level problems for mathematical reasoning in large language models. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1816718188, Vienna, Austria. Association for Computational Linguistics. [77] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less is more for alignment. In Advances in Neural Information Processing Systems, volume 36, pages 5500655021. Curran Associates, Inc."
        },
        {
            "title": "A Configurations",
            "content": "We employ learning rate of 1e-6 during training, with ϵ being 0.2. The maximum generation length is set to 2048. The configuration to collect zero-shot sampling for base model is listed in Table 4. The prompt used is displayed in Table 5, and the prompt to synthesize polymath samples is shown in Table 6. Around 500 candidate problems are synthesized on the candidate problem generation stage. The prompt employed for math skill identification is displayed in Table 7."
        },
        {
            "title": "HYPERPARAMETER VALUE",
            "content": "temperature top top 0.5 10 0. Table 4: Hyperparameters for computing 0-shot pass rate@k of the base model."
        },
        {
            "title": "Prompt for Training",
            "content": "A conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. User: You must put your answer inside boxed{} and Your final answer will be extracted automatically by the boxed{} tag. For multiple choice questions, the final answer in boxed{} should be the option letter (A, B, C, D, etc.). [PROBLEM] Assistant: Table 5: Training Prompt, where [PROBLEM] is the placeholder for the problem."
        },
        {
            "title": "Prompt for Synthesizing Polymath Sample",
            "content": "You are professor proficient in physics, chemistry, and biology, tasked with creating highly integrated problem for students that encompasses knowledge from all three disciplines. This problem should be fill-in-the-blank question, and the final answer must be precise integer (a positive integer between 1-1000). The difficulty of this question should be at the high school to university level. Furthermore, it should not involve any estimation, and complex calculations should be avoided as much as possible to ensure the robustness of the evaluation. Table 6: Prompt for synthesizing polymath sample."
        },
        {
            "title": "Prompt for Skill Identification",
            "content": "Here is reasoning problem, and your job is to identify the concepts and skills in the scope of [CATEGORY] that are related to solve the problem. Please separate the concepts or skills with ;, and if there is no skills or concepts identified, please answer with None. Please put your answer within <answer></answer>. For example: compute derivatives is the skill in precalculus. Question: [QUESTION] Table 7: Prompt for skill identification. The [CATEGORY] and [QUESTION] are the placeholder for math category (e.g. algebra) and problem respectively."
        },
        {
            "title": "B LIMR Score Basics",
            "content": "The LIMR score (Li et al., 2025a) is computed by measuring the sample-wise training reward with the dataset-wise average. Specifically, si = 1 (cid:80)K i=1(rk rk)2 i=1(1 rk)2 (cid:80)K , rk ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 rk 17 where rk is the reward of sample in the k-th epoch, and rk is the average reward of training set in the k-th epoch."
        },
        {
            "title": "C Results by Datasets",
            "content": "Table 8 includes results by datasets on polymath learning and comprehensive learning, with the synthetic sample still performing the strongest. Table 8: Results on different reasoning benchmarks, where OE refers to benchmarks of open-ended problems: MATH500, AIME2024, AIME2025, Minerva and Scibench, while MCQ refers to benchmarks of multiplechoice problems. The best performance is bolded and the best polymath learning performance is underlined if not optimal. Polymath Subject MATH500 AIME2024 AIME2025 Minerva GPQA-Diamond SuperGPQA MMLU-Pro SciBench AVG-OE AVG-MCQ AVG-All N=64 Sampling (0 shot) 13.1 15.7 4.7 In-context Learning (1 shot) Natural Sample - Geometry Prealgebra Algebra Intermediate Algebra Number Theory Precalculus Probability Prime MATH (8k) LIMR (1k) Geometry Prealgebra Algebra Intermediate Algebra Number Theory Precalculus Probability Geometry Algebra Number Theory Precalculus Probability Prime 54.8 60.0 55.0 48.0 59.6 52.8 51.8 54.2 44.2 73.6 74.8 26.6 71.2 72.0 71.2 69.6 71.6 71.6 71.4 71.6 73.8 71.8 71. 71.4 9.0 8.2 9.2 8.2 5.1 8.5 6.7 7.3 4.8 13.0 12.6 0.0 13.3 6.7 13.3 16.7 10.0 13. 10.2 10.2 11.7 11.4 11.6 10.1 7.1 4.7 4.5 3.1 4.5 3.9 3.9 4.0 2.4 7.9 8. 0.0 13.3 0.0 0.0 10.0 10.0 16.7 6.7 6.7 7.1 7.7 7.2 7.2 13.4 15.4 10.7 15.8 12.1 11.8 15.8 13.6 9.6 16.2 14.6 14.1 16.7 13.1 11. 4.5 9.2 10.7 7.3 6.3 4.9 6.3 Synthetic Sample 5.6 15.1 Comprehensive Learning (> 1k shots) Natural Sample 2.8 30.9 30. 19.9 30.9 30.9 28.7 30.9 30.5 29.8 27.2 30.9 29.8 29.4 28.3 30.9 11.7 13.2 10.3 15.8 Polymath Learning (1 shot) Natural Sample 23.9 18.3 16.2 20.3 17.8 18.8 14.2 Synthetic Sample 15.7 20.3 14.2 19.8 16.8 21.3 18.5 19.4 17.3 18.9 18.2 20.9 18.9 16.9 19.3 19.3 21.5 17. 20.5 9.8 6.8 6.4 6.7 5.7 5.9 5.2 5.8 3.8 23.1 22.7 7.9 21.4 22.8 22.0 22.3 22.4 22. 21.4 21.8 23.1 22.8 22.1 22.3 23.6 11.3 15.9 19.0 17.2 16.4 17.4 16.6 16.7 17. 14.1 29.7 29.8 10.9 30.0 26.5 27.0 29.9 28.9 30.8 27.4 28.2 29.1 28.6 28.2 28.4 11.5 18.1 17.0 14.0 15.5 12.3 12. 6.3 14.8 20.2 25.2 24.2 22.8 24.6 23.7 24.6 22.7 21.1 24.4 22.7 25.7 23.6 26.7 16.2 17.5 16.6 16.1 16.2 15.0 15. 11.2 24.1 26.2 16.2 27.9 25.1 26.1 27.6 27.3 27.8 25.0 26.8 26.7 27.5 26.5 27.8 20.5 28.8 25.6 20.5 23.4 19.0 19. 10.6 22.5 31.5 33.1 35.0 34.9 34.5 35.0 34.1 34.9 30.7 33.6 34.6 35.8 36.4 38."
        },
        {
            "title": "D Sample Preference with LIMR Scores",
            "content": "We include the results from selecting different LIMR scores from two math categories, prealgebra and probability. The results in Figure 5 show that the samples with LIMR score equals 0.6 delivers the best performance. Figure 5: Average domain performance over natural samples with different LIMR scores. The performance is reported the same way as in Table 3. The samples with LIMR score being 0.6 perform best."
        },
        {
            "title": "E Full Subject List",
            "content": "The full list of reasoning subjects being evaluated is displayed in Table 9."
        },
        {
            "title": "SOURCE",
            "content": "# SAMPLES"
        },
        {
            "title": "Math",
            "content": "AIME MATH500 Minerva math AIME2024, AIME2025 MATH MinervaMath Scibench, MMLU-Pro"
        },
        {
            "title": "Physics",
            "content": "physics GPQA-Diamond, Scibench, MMLU-Pro"
        },
        {
            "title": "Chemistry",
            "content": "chemistry GPQA-Diamond, Scibench, MMLU-Pro"
        },
        {
            "title": "Science",
            "content": "biology science"
        },
        {
            "title": "Engineering",
            "content": "engineering"
        },
        {
            "title": "Computer Science",
            "content": "computer science"
        },
        {
            "title": "Others",
            "content": "military science business philosophy economics management health psychology medicine education agronomy literature and arts law history sociology other GPQA-Diamond, Scibench, MMLU-Pro"
        },
        {
            "title": "SuperGPQA",
            "content": "MMLU-Pro SuperGPQA MMLU-Pro MMLU-Pro, SuperGPQA MMLU-Pro, SuperGPQA SuperGPQA MMLU-Pro MMLU-Pro SuperGPQA SuperGPQA SuperGPQA SuperGPQA MMLU-Pro, SuperGPQA MMLU-Pro, SuperGPQA SuperGPQA MMLU-Pro 60 500 272 299 413 459 557 447 100 12 100 120 149 28 100 100 155 27 27 93 137 138 8 100 Table 9: Evaluation reasoning benchmarks with subjects included."
        },
        {
            "title": "F Robustness of Experiments",
            "content": "We include the results of comprehensive learning in MATH train set and polymath learning with the Synthetic Prime sample in 3 independent runs on Qwen2.5-7b-base. The results in Table 10 shows that the comprehensive learning on 8k MATH samples demonstrate stronger reasoning in math benchmarks, but polymath learning with the Synthetic Prime sample outperforms comprehensive learning on the MATH training set in most other benchmarks as well as the overall performance. Table 10: The results of comprehensive learning on MATH and polymath learning on the Synthetic Prime sample with 3 independent runs in Qwen2.5-7b-base. The best performance is bold as the on par performance is underlined. Polymath learning on the Synthetic Prime sample outperforms comprehensive learning with MATH on most benchmarks as well as the overall performance. Polymath Subject MATH500 AIME2024 AIME2025 Minerva GPQA-Diamond SuperGPQA MMLU-Pro SciBench AVG-OE AVG-MCQ AVG-All MATH (8k) 73.00.59 15.64.16 6.70.0 Comprehensive Learning (> 1k shots) 11.61.75 11.90. 29.51.24 25.02.94 23.50.37 29.70.73 16.21.53 24.60. Prime 71.70.34 12.21.56 10.04.71 31.01.07 20.30. 20.80.31 38.10.69 21.90.33 29.41.03 26.40.29 28.20. Polymath Learning (1 shot) Self-verification by Subject Domains We list the self-verification statistics by different sbuject domains in Figure 6 and Figure 7. Specifically, we found that verify is more preferred in math problems while re-evaluate appears more frequently in science and engineering. Besides, polymath learning with the intermediate algebra sample elicits the most coding verifications among all the natural and synthetic samples. 19 Figure 6: The verification patterns identified for wait, verify and yet in different subject groups. The wait rates in computer science problems are highly attributed from terms in the question stems. Figure 7: The verification patterns identified for re-evaluate, recheck and code in different subject groups. 21 Polymath Learning with Other 1-shot Sample π1 (see Table 25) is employed in previous success of reinforcement learning with one sample (Wang et al., 2025a,b). It is selected from DeepScaleR (?), curated dataset of challenging mathematical competition problems like AIME and Omni-math (Gao et al., 2025) other than MATH. Results in Table 11 demonstrate the effectiveness of Synthetic Prime sample over both π1 and comprehensive learning with 8k MATH samples in Qwen2.5-base in both 7b and 14b sizes. The skill abundance comparison with the strong synthetic and natural polymath sample (Synthetic Prime sample and prealgebra) in Figure 8 also demonstrates more complex skill combinations than π1 to solve. Table 11: The results between comprehensive learning on 8k MATH samples and polymath learning on the Synthetic Prime sample and π1 in Qwen2.5-7b-base and Qwen2.5-14b-base. The Synthetic Prime sample consistently outperforms the other two data choices across models."
        },
        {
            "title": "Science Engineering Computer Science Others Avg",
            "content": "Qwen2.5-7b-base N=64 Sampling (0-shot) - 20.4 4.4 MATH (8k) 37. 12.8 5.1 4.4 3.7 Comprehensive Learning (> 1k shots) 8.6 10.0 31.4 6.5 0. π1 (DeepScaleR) Prime 35.5 38.3 14.3 20.6 11.3 15.7 Polymath Learning (1-shot) 35.1 15. 28.4 54.2 Qwen2.5-14b-base N=64 Sampling (0-shot) 44.1 20.8 - 37.7 26.2 MATH (8k) 42.7 26.4 28.1 39.0 22.2 Comprehensive Learning (> 1k shots) 64.4 20.5 49.5 44. 41.2 π1 (DeepScaleR) Prime 40.4 44.0 27.6 32.7 20.0 22.7 39.4 42. 51.5 56.7 57.6 58.5 Polymath Learning (1-shot) 3.3 25.8 13.8 48. 20.8 22.3 22.1 31.0 9.6 6.4 23. 19.5 10.4 32.4 24.1 30.8 14.3 28.7 15. 35.8 17.1 20.6 34.5 38.6 Figure 8: The skill spectrum between the π1 sample, the Synthetic Prime sample, and the strongest natural polymath sample in prealgebra. The strongest natural polymath and synthetic samples demonstrate richer and more comprehensive skill coverage than the π1 sample. Performance on MMLU-Pro and SuperGPQA Full Set Table 12 reports the results on full MMLU-Pro and SuperGPQA for comprehensive learning and strong polymath samples trained with Qwen2.5-7B-Base under the same configuration described in Section 5. Polymath learning on the Synthetic Prime sample achieves substantially higher performance than both 0-shot learning and comprehensive learning using thousands of samples. 22 Table 12: Performance of different comprehensive learning and polymath learning samples on the full set of MMLU-Pro and SuperGPQA, the Synthetic Prime sample performs best (bolded)."
        },
        {
            "title": "Data",
            "content": "MMLU-Profull"
        },
        {
            "title": "SuperGPQAfull",
            "content": "0-shot MATH (8k) LIMR (1k) π1 Prealgebra Prime 30.3 31.7 33.0 29.7 33.4 37.6 16.8 16.6 17.2 16.7 19.2 21."
        },
        {
            "title": "J Training Dynamics of Polymath Learning",
            "content": "Figure 9 illustrates the training dynamics of comprehensive learning and polymath learning across strong natural and synthetic samples. We specifically prolong the training to better observe convergence. We observe that comprehensive learning, on either the 8k MATH training set or the LIMR subset, yields progressive improvement on MATH500, but exhibits pronounced overfitting on multidisciplinary benchmarks such as GPQA Diamond, SuperGPQA, and MMLU-Pro. And training with the MATH set exacerbates this effect. Polymath learning, on the other hand, demonstrates substantially greater robustness especially on multidisciplinary reasoning benchmarks even though demonstrate inferior performance on MATH500 compared to comprehensive learning. Moreover, both the Synthetic Prime sample and natural polymath sample in prealgebra deliver stronger multidisciplinary reasoning performance than the π1 sample. Figure 9: The evaluation results of benchmarks between comprehensive learning (MATH and LIMR) and different polymath learning samples (Synthetic Prime sample, natural prealgebra sample, π1) trained in Qwen2.5-7b-base. The results are collected in greedy decoding and rolling smoothing average with window of 5 is applied to AIME2024, AIME2025 and 3 for other benchmarks for demonstration purpose."
        },
        {
            "title": "K Polymath Learning on Additional Models",
            "content": "Table 13 includes comparison between comprehensive learning and polymath learning in additional model choices. Specifically, we select Qwen2.5-14b-base, Llama3.1-8b-instruct (Grattafiori et al., 2024) and OctoThinker-8blong-base (Wang et al., 2025c), which enhances reasoning ability of Llama3.2 through mid-training in long-form reasoning data. The results show that the benefits of polymath learning on the Synthetic Prime extends to the 14b parameter model. Although it does not surpass comprehensive learning in Llama3.1-8b-instruct, it nonetheless yields improvements in multidiscipline reasoning benchmarks (GPQA-Diamond, SuperGPQA) when applied to models strengthened with mid-training. This trend echoes observations in Dohmatob et al. (2025) regarding the relationship between data curation effectiveness and the capability of the underlying model solver. Table 13: Performance of comprehensive learning on 8k MATH samples and the Synthetic Prime sample on reasoning benchmarks with additional model choices. The best performance is bolded and the on-par performance is underlined. The Synthetic Prime sample outperforms comprehensive learning when trained with strong model like Qwen2.5-14b-base and in some non-math benchmarks when trained with OctoThinker-8b-long-base. Polymath Subject MATH500 AIME2024 AIME2025 Minerva GPQA-Diamond SuperGPQA MMLU-Pro SciBench AVG-OE AVG-MCQ AVG-All Qwen-14b-base (2k context) 0-shot MATH (8k) π1 (DeepScaleR) Prime 0-shot MATH (8k) Prime 0-shot MATH (8k) Prime 68.6 77.6 73.8 76.0 50.2 54.2 48.6 8.6 73.0 14. 16.7 20.0 6.7 16.7 3.3 10.0 0.0 3.3 16.7 0.0 3.3 6.7 10.0 10.0 0.0 0.0 0.0 0.0 13.3 0. 26.8 34.2 36.4 35.3 16.8 23.4 21.5 26.1 Llama3.1-8b-instruct (2k context) 29.9 28.4 29.9 37.1 17.3 23.5 20.6 4.6 14.7 11.2 2.8 12.6 1. OctoThinker-8b-long-base (8k context) 9.6 22.4 11.8 0.0 17.8 28.4 0.1 16.3 17.1 42.2 46.9 48.5 53.3 10.1 31.5 12.2 0.4 41.5 33. 19.7 27.2 23.7 23.6 13.2 13.7 13.2 2.0 22.0 5.8 27.0 33.1 30.1 32.3 16.8 20.3 16.5 4.7 29.5 6. 29.6 32.9 33.3 38.8 5.8 19.6 8.4 0.2 25.2 26.2 28.0 33.1 31.3 34.8 12.7 20.0 13.4 3.0 27.9 13."
        },
        {
            "title": "L Reasoning Breakdown by Subject",
            "content": "Figure 10 illustrates the best polymath sample for different subjects. Figure 10: The subject-level performance of different learning strategies. OE stands for subjects with open-ended problems. The subjects are sorted by subject embedding distance to MATH500 (the grey dotted line), from low to high. The blue line represents pass ratio from 64 independent attempts of the base model. The stars and triangles represent best performance of in-context learning and polymath learning. Note that we only display the best polymath learning and in-context polymath learning results for demonstration, and Synthetic represents the Synthetic Prime sample. 24 Self Verification - Math [Question] The intensity of light with wavelength λ traveling through diffraction grating with slits at an angle θ is given by I(θ) = 2 sin2 k/k2, where = (πN sin θ)/λ and is the distance between adjacent slits. helium-neon laser with wavelength λ = 632.8 109 is emitting narrow band of light, given by 106 < θ < 106, through grating with 10,000 slits spaced 104 apart. Use the Midpoint Rule with = 10 to estimate the total light intensity (cid:82) 106 106 I(θ)dθ emerging from the grating. [Answer] 59.4 [Intermediate Algebra] To solve this problem, we follow these steps: 1. Define the function I(θ) = 2 sin2 (cid:0) πN sin θ 2. Substitute the given values = 10000, = 104, and λ = 632.8 109. 3. Use the Midpoint Rule with = 10 to approximate the integral (cid:82) 106 parts and implement it in Python using the sympy library. (cid:1) / (cid:0) πN sin θ (cid:1)2 λ λ . 106 I(θ) dθ. Lets break it down into manageable import sympy as sp # Define the variables theta = sp.symbols(theta) # Given values = 10000 = 10**-4 lambda_val = 632.8 * 10**-9 # Define the function I(theta) = (sp.pi * * * sp.sin(theta)) / lambda_val I_theta = N**2 * sp.sin(k)**2 / k**2 # Define the limits of integration = -10**-6 = 10**-6 = 10 # Midpoint Rule delta_theta = (b - a) / midpoints = [a + (i + 0.5) * delta_theta for in range(n)] integral_approx = sum(I_theta.subs(theta, midpoint) for midpoint in midpoints) * delta_theta # Evaluate the integral approximation integral_approx_evaluated = integral_approx.evalf() print(integral_approx_evaluated) Output: TimeoutError: Timed Out It seems that the computation took too long to complete. This might be due to the complexity of the function and the large number of terms involved in the Midpoint Rule. Lets simplify the problem by breaking it down further and using numerical integration instead of symbolic integration. We can use the scipy.integrate.quad function to perform the numerical integration. This function is designed to handle such integrals efficiently. Heres the updated code using numerical integration: import numpy as np from scipy.integrate import quad # Given values = 10000 = 10**-4 lambda_val = 632.8 * 10**-9 # Define the function I(theta) def I_theta(theta): = (np.pi * * * np.sin(theta)) / lambda_val return N**2 * np.sin(k)**2 / k**2 # Define the limits of integration $a = -10**-6$ $b = 10**-6$ # Perform the numerical integration integral_approx, error = quad(I_theta, a, b) # Print the result print(integral_approx) Output: 10000000000.0 The numerical integration result for the total light intensity ((cid:82) 106 from the grating is 10000000000.0 . 106 I(θ)dθ) emerging Table"
        },
        {
            "title": "M Other Polymath Learning Samples",
            "content": "We list the other samples used for polymath learning in Table 19 to Table 24, and synthetic specialist samples from Table 26 to Table 30. 25 Self Verification - Physics [Question] We know that the negative charge on the electron and the positive charge on the proton are equal. Suppose, however, that these magnitudes differ from each other by 0.00010%. With what force would two copper coins, placed 1.0 apart, repel each other? Assume that each coin contains 3 1022 copper atoms. (Hint: neutral copper atom contains 29 protons and 29 electrons.) [Answer] 1.7 [Intermediate Algebra] To solve this problem, we need to follow these steps: 1. Calculate the charge difference between the electron and the proton. 2. Determine the total charge on each copper coin. 3. Use Coulombs law to find the force between the two copper coins. Lets break it down: 1. **Calculate the charge difference:** - The charge of an electron is = 1.602 1019 C. - The charge of proton is = 1.602 1019 C. - The difference in charge is = 0.00010% 1.602 1019 C. - 0.00010% = 0.000001, so = 0.000001 1.602 1019 = 1.602 1025 C. 2. **Determine the total charge on each copper coin:** - Each copper atom has 29 protons and 29 electrons, so the net charge of neutral copper atom is zero. - Each copper coin contains 3 1022 copper atoms, so the total charge on each coin is 3 1022 1.602 1025 = 4.806 103 C. 3. **Use Coulombs law to find the force between the two copper coins:** - Coulombs law states that the force between two point charges q1 and q2 separated by distance is given by = q1q2 - Here, q1 = q2 = 4.806 103 and = 1.0 m. - So, = 8.99 109 (4.806103)2 r2 , where is the Coulomb constant 8.99 109 m2 C2 . = 8.99 109 2.31 105 = 2.07 105 N. 1.02 Lets implement this in Python to ensure the accuracy of the result: # charge of an electron (and proton) # Constants = 1.602e-19 delta_e = 0.00010 / 100 * num_atoms = 3e22 charge_per_atom = total_charge_per_coin = num_atoms * charge_per_atom = 8.99e9 distance = 1.0 # number of copper atoms in each coin # charge of single copper atom # distance between the coins in meters # charge difference # Coulomb constant # total charge on each coin # Calculate the force using Coulombs law force = * (total_charge_per_coin ** 2) / (distance ** 2) print(force) 206673.0 output: The force between the two copper coins, each containing 3 1022 copper atoms, placed 1.0 meter apart, with charge difference of 0.00010% between the electron and the proton, is 2.07 105 . Table 15 Self-Verification Examples Table 14, Table 15 and Table 16 include examples in math, physics, and chemistry problems where program verification emerges in polymath learning with the polymath sample in intermediate algebra."
        },
        {
            "title": "O Example of Mathematical Skill in the Reasoning Problem",
            "content": "A sample science problem and relevant algebra skills to solve is displayed in Table 18. 26 Self Verification - Chemistry [Question] Assume all gases are perfect unless stated otherwise. Note that 1 atm = 1.013 25 bar. Unless otherwise stated, thermochemical data are for 298.15 K. Concerns over the harmful effects of chlorofluorocarbons on stratospheric ozone have motivated search for new refrigerants. One such alternative is 2,2-dichloro-1,1,1-trifluoroethane (refrigerant 123). Younglove and McLinden published compendium of thermophysical properties of this substance (J. Phys. Chem. Ref. Data 23, 7 (1994)), from which properties such as the Joule-Thomson coefficient µ can be computed. Compute µ at 1.00 bar and 50C given that (H/p)T = 3.29 103 JMPa1 mol1 and Cp, = 110.0 K1 mol1. [Answer] 29.9 [Intermediate Algebra] To compute the Joule-Thomson coefficient µ at 1.00 bar and 50C, we can use the following formula: The Joule-Thomson coefficient can also be expressed in terms of the enthalpy and temperature derivatives as follows: µ = (cid:19) (cid:18) p Given: µ ="
        },
        {
            "title": "1\nCp,m",
            "content": "(cid:18) p (cid:19) (cid:18) p (cid:19) = 3.29 103 MPa1 mol Cp,m = 110.0 K1 mol1 We can substitute these values into the formula to find µ: Lets calculate this using Python to ensure accuracy. µ = 1 110.0 (3.29 103) MPa # Given values dH_dp_T = -3.29e3 Cp_m = 110.0 # MPa$ˆ{-1}$ mol$ˆ{-1}$ # K$ˆ{-1}$ mol$ˆ{-1}$ # Calculate the Joule-Thomson coefficient mu = dH_dp_T / Cp_m print(mu) # MPa$ˆ{-1}$ Output: -29. The Joule-Thomson coefficient µ at 1.00 bar and 50C is: 29.91 MPa1 Table 16: Chemistry example of self-verification in polymath learning. 27 Self Verification - Engineering [Question] Transonic airfoil flow is to be initially analyzed using water tunnel test through air-water analogy. To simulate an airflow with M=1.1,a water depth of 50mm is chosen for the water tunnel test. The water depth at the stagnation point (ζ + h) is (). Options are: (A): 0.1057m (B): 0.0803m (C): 0.0704m (D): 0.1451m (E): 0.1158m (F): 0.0906m (G): 0.0605m (H): 0.1259m (I): 0.1552m (J): 0.1350m [Answer] [Intermediate Algebra] To solve this problem, we need to use the air-water analogy for transonic airfoil flow. The key relationship we will use is the Mach number similarity criterion, which states that the ratio of the water depth at the stagnation point to the water depth in the tunnel should be equal to the ratio of the Mach number of the air flow to the Mach number of the water flow. Given: - The Mach number of the air flow, = 1.1 - The water depth in the tunnel, = 50 mm = 0.05 We need to find the water depth at the stagnation point, ζ + h. The Mach number of water, Mw, is typically taken as 1 (since water is incompressible and its Mach number is considered to be 1 for practical purposes in this context). Using the Mach number similarity criterion: Substitute the given values: Simplify the equation: Multiply both sides by 0.05 to solve for ζ + 0.05: ζ + = Mw ζ + 0.05 0.05 = 1.1 1 ζ + 0.05 0.05 = 1. Calculate the right-hand side: Subtract 0.05 from both sides to find ζ: ζ + 0.05 = 1.1 0.05 ζ + 0.05 = 0.055 ζ = 0.055 0.05 ζ = 0.005 Now, add the water depth to find the total water depth at the stagnation point: ζ + = 0.005 + 0.05 ζ + = 0.055 Convert this to millimeters for comparison with the given options: 0.055 = 55 mm However, the options provided are in meters. Therefore, we need to convert 55 mm back to meters: 55 mm = 0.055 None of the given options match 0.055 exactly. Lets re-evaluate the problem to ensure there are no calculation errors. The correct approach should be: Given the options, the closest match is: 0.0803 Thus, the correct option is: ζ + = 0.055 Table 17: Engineering example of self-verification in polymath learning. 28 Skill Extraction - Science 3m/s 3m/s 3m/s [Question] particle of mass 1 kg is moving in the plane and its potential energy in joule obeys the law = 6x + 8y, where (x, y) are the coordinates of the particle in meter. If the particle starts from rest at (9,3) at time = 0, then (A): The speed of the particle when it crosses the axis is 5 (B): The speed of the particle when it crosses axes is 7 (C): Magnitude of acceleration of particle is 10m/s2 (D): The speed of the particle when it crosses axes is 11 (E): Acceleration of particle is zero 3m/s (F): The speed of the particle when it crosses axes is 8 3m/s (G): The speed of the particle when it crosses axes is 9 (H): The speed of the particle when it crosses axes is 12 (I): The speed of the particle when it crosses axes is 10 [Skills in Algebra] Interpreting physical laws in mathematical form Understanding the relationship between potential energy and force Using the gradient to find force components Applying the work-energy theorem Solving for velocity using energy conservation Understanding the relationship between force and acceleration Solving for acceleration using newtons second law Analyzing motion in two dimensions Solving for the time when particle crosses specific axis Evaluating expressions involving square roots 3m/s 3m/s Table 18: Skills extracted from sample science problem. Other math categories do not contribute relevant math skills."
        },
        {
            "title": "Polymath Sample in Geometry",
            "content": "[Question] white cylindrical silo has diameter of 30 feet and height of 80 feet. red stripe with horizontal width of 3 feet is painted on the silo, as shown, making two complete revolutions around it. What is the area of the stripe in square feet? [asy] size(250);defaultpen(linewidth(0.8)); draw(ellipse(origin, 3, 1)); fill((3,0)(3,2)(-3,2)(-3,0)cycle, white); draw((3,0)(3,16)h-3,0)(-3,16)); draw((0, 15)(3, 12)h0, 16)(3, 13)); filldraw(ellipse((0, 16), 3, 1), white, black); draw((-3,11)(3, 5)h-3,10)(3, 4)); draw((-3,2)(0,-1)h-3,1)(-1,-0.89)); draw((0,-1)(0,15), dashed); draw((3,-2)(3,-4)h-3,-2)(-3,-4)); draw((-7,0)(-5,0)h-7,16)(-5,16)); draw((3,-3)(-3,-3), Arrows(6)); draw((-6,0)(-6,16), Arrows(6)); draw((-2,9)(-1,9), Arrows(3)); label(3, (-1.375,9.05), dir(260), UnFill); label(A, (0,15), N); label(B, (0,-1), NE); label(30, (0, -3), S); label(80, (-6, 8), W); [/asy] [Answer] 240 Table 19: Polymath sample in geometry. 29 Polymath Sample (Counting & Probability) Bicycle license plates in Flatville each contain three letters. The first is chosen from the set {C, H, L, P, R}, the second from {A, I, O}, and the third from {D, M, N, }. When Flatville needed more license plates, they added two new letters. The new letters may both be added to one set or one letter may be added to one set and one to another set. What is the largest possible number of ADDITIONAL license plates that can be made by adding two letters? Answer: 40 Table 20: Polymath sample in counting and probability."
        },
        {
            "title": "Polymath Sample in Intermediate Algebra",
            "content": "[Question] Let a, b, be nonzero real numbers such that + + = 7 and + + = 9. a3 b3 + b3 c3 + c3 a3 ."
        },
        {
            "title": "Find",
            "content": "[Answer] 157 Table 21: Polymath sample in intermediate algebra."
        },
        {
            "title": "Polymath Sample in Precalculus",
            "content": "[Question] For certain value of k, the system + ky + 3z = 0, 3x + ky 2z = 0, 2x + 4y 3z = 0 has solution where x, y, and are all nonzero. Find xz y2 . [Answer] 10 Table 22: Polymath sample in precalculus."
        },
        {
            "title": "Polymath Sample in Number Theory",
            "content": "[Question] The American Mathematics College is holding its orientation for incoming freshmen. The incoming freshman class contains fewer than 500 people. When the freshmen are told to line up in columns of 23, 22 people are in the last column. When the freshmen are told to line up in columns of 21, 14 people are in the last column. How many people are in the incoming freshman class? [Answer] 413 Table 23: Polymath Sample in Number Theory."
        },
        {
            "title": "Polymath Sample in Prealgebra",
            "content": "[Question] region is bounded by semicircular arcs constructed on the side of square whose sides measure 2/π, as shown. What is the perimeter of this region? [asy] path a=(10,0)..(5,5)(5,-5)..cycle; path b=(0,10)..(5,5)(-5,5)..cycle; path c=(-10,0)..(-5,5)(-5,-5)..cycle; path d=(0,-10)..(-5,-5)(5,-5)..cycle; path e=(5,5)(5,-5)(-5,-5)(-5,5)cycle; fill(e,gray(0.6)); fill(a,gray(0.8)); fill(b,gray(0.8)); fill(c,gray(0.8)); fill(d,gray(0.8)); draw(a,linewidth(0.7)); draw(b,linewidth(0.7)); draw(c,linewidth(0.7)); draw(d,linewidth(0.7)); draw(e,linewidth(0.7)); [/asy] [Answer] 4 Table 24: Polymath sample in prealgebra. The π1 Sample [Question] The pressure exerted by wind on sail varies jointly as the area of the sail and the cube of the winds velocity V. When the velocity is 8 miles per hour, the pressure on sail of 2 square feet is 4 pounds. Find the wind velocity when the pressure on 4 square feet of sail is 32 pounds. [Answer] 12. Table 25: The π1 sample."
        },
        {
            "title": "Synthetic Specialist Sample in Precalculus",
            "content": "[Question] chemical factory discharges waste into river at rate of 500 cubic meters per day. The waste has an untreated pollutant concentration of 100 mg/L. The river has flow rate of 24,500 cubic meters per day, and the waste mixes completely and instantly with the river flow. The pollutant degrades following first-order kinetics with half-life of 5 days. The time for water to travel from the discharge point to critical fish habitat is 5 days. To protect an endangered fish species (reflecting ethical considerations of intrinsic value in philosophy), the pollutant concentration at the habitat must not exceed 0.1 mg/L. If the concentration exceeds this limit, the probability of harm to the fish is 0.05 per mg/L of excess concentration. Due to legal regulations (incorporating law), if harm occurs, the factory is fined $10,000 per day. The factory can treat the waste to reduce the pollutant concentration before discharge. The treatment cost is $0.005 per cubic meter per mg/L reduction in concentration (incorporating economics and chemistry). Calculate the optimal initial concentration of pollutant in the treated waste (in mg/L) that minimizes the total daily cost (treatment cost plus expected fine), considering the interdisciplinary aspects of physics (degradation kinetics and flow), biology (fish protection), and mathematics (optimization). [Answer] 10 Table 26: Synthetic Specialist Sample in Precalculus."
        },
        {
            "title": "Synthetic Specialist Sample in Number Theory",
            "content": "[Question] pharmaceutical company develops new drug for treating specific condition. The drug has biological half-life of 4 hours in the human body and volume of distribution of 50 liters. Clinical trials determine that the minimum therapeutic concentration required for efficacy is 10 mg/L. The drug is administered as single intravenous bolus dose at the beginning of each day to maintain concentrations at or above the therapeutic level for exactly 8 hours. The manufacturing cost analysis shows that each 500 mg vial of the drug costs $2.50 to produce, and the entire vial must be used if opened. Regulatory requirements (reflecting legal and ethical considerations) mandate that the drug concentration must not drop below the therapeutic level during the 8-hour treatment period. Considering the exponential decay of the drug concentration, calculate the required dose in milligrams. Then, determine the daily cost in dollars for administering this dose, providing the cost to one decimal place. [Answer] 10 Table 27: Synthetic Specialist Sample in Number Theory."
        },
        {
            "title": "Synthetic Specialist Sample in Geometry",
            "content": "[Question] model cell membrane is represented by cube-shaped vesicle with side length of 10.0 nm. The membrane is phospholipid bilayer made of two leaflets. Each phospholipid occupies exactly 1.50 nm2 of surface area within single leaflet. Assuming both leaflets cover the entire outer surface of the cube and ignoring membrane thickness and edge effects, how many phospholipid molecules are present in the bilayer? [Answer] 800 Table 28: Synthetic Specialist Sample in Geometry."
        },
        {
            "title": "Synthetic Specialist Sample in Probability",
            "content": "[Question] molecular-biology lab purifies circular plasmid that is exactly 3000 base pairs (bp) long. Each base pair contains two deoxyribonucleotides, and every nucleotide carries one phosphate (PO3 While the cells were growing, the medium contained the β-emitter 32P, so every phosphate in the plasmid is 32P-labelled. The radioactive isotope 32P has half-life of 14.0 days. Immediately after purification, tube that contains precisely 100 identical plasmid molecules shows an activity of 1024 disintegrations per minute (dpm). The tube is stored in freezer, andafter an integral number of whole half-livesthe activity is measured again and found to be exactly 4 dpm. 4 ) group. To cross-check the number of phosphate groups, the plasmid DNA is then completely hydrolysed and the liberated phosphate is quantitatively precipitated as calcium phosphate according to The precipitation requires exactly 5.0 107 mol of Ca2+ ions, confirming the amount of DNA present (the stoichiometry is consistent and needs no further calculation here). What is the number of 32P half-lives that have elapsed between the two activity measurements?"
        },
        {
            "title": "3 Ca2+ + 2 PO3−",
            "content": "4 Ca3(PO4)2 [Answer] 8 Table 29: Synthetic Specialist Sample in Probability."
        },
        {
            "title": "Synthetic Specialist Sample in Algebra",
            "content": "[Question] plant that is heterozygous for two independent genes, and (genotype GgHh), is self-pollinated. Exactly 640 seeds are obtained. Biology: Only seeds that are homozygous recessive for both genes (gghh) can synthesize the enzyme Catalase-X. Chemistry: Every gghh seed is placed in its own vial containing 0.0800 mol of hydrogen peroxide. Catalase-X instantly and completely decomposes the peroxide according to"
        },
        {
            "title": "2 H2O2 → 2 H2O + O2(g).",
            "content": "Thus each qualifying vial releases pure O2 gas. Physics: The O2 is dried, transferred to 1.00 rigid cylinder at 298 K, and all molecules are singly ionised (O2 O+ 2 + e). The ions are accelerated so that each has speed that makes its circular path radius exactly 0.0400 in uniform magnetic field = 1.00 perpendicular to their velocity (m(O2) = 32 u, 1 = 1.66 1027 kg, = 1.60 1019 C). Immediately after acceleration an electronic gate allows only the very first O+ 2 ion to continue; all later ions are blocked. That single ion has 50% chance of striking narrow slit that leads to detector; otherwise nothing is recorded. vial is counted as success if its lone transmitted ion hits the detector. All vials operate independently. What is the expected number of successes after all 640 seeds have been processed? [Answer] 20 Table 30: Synthetic Specialist Sample in Algebra."
        }
    ],
    "affiliations": [
        "GAIR",
        "Shanghai Jiaotong University",
        "Taobao & Tmall Group of Alibaba"
    ]
}