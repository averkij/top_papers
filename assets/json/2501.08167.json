{
    "paper_title": "Potential and Perils of Large Language Models as Judges of Unstructured Textual Data",
    "authors": [
        "Rewina Bedemariam",
        "Natalie Perez",
        "Sreyoshi Bhaduri",
        "Satya Kapoor",
        "Alex Gil",
        "Elizabeth Conjar",
        "Ikkei Itoku",
        "David Theil",
        "Aman Chadha",
        "Naumaan Nayyar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Rapid advancements in large language models have unlocked remarkable capabilities when it comes to processing and summarizing unstructured text data. This has implications for the analysis of rich, open-ended datasets, such as survey responses, where LLMs hold the promise of efficiently distilling key themes and sentiments. However, as organizations increasingly turn to these powerful AI systems to make sense of textual feedback, a critical question arises, can we trust LLMs to accurately represent the perspectives contained within these text based datasets? While LLMs excel at generating human-like summaries, there is a risk that their outputs may inadvertently diverge from the true substance of the original responses. Discrepancies between the LLM-generated outputs and the actual themes present in the data could lead to flawed decision-making, with far-reaching consequences for organizations. This research investigates the effectiveness of LLMs as judge models to evaluate the thematic alignment of summaries generated by other LLMs. We utilized an Anthropic Claude model to generate thematic summaries from open-ended survey responses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as LLM judges. The LLM-as-judge approach was compared to human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable alternative to traditional human centric evaluation methods. Our findings reveal that while LLMs as judges offer a scalable solution comparable to human raters, humans may still excel at detecting subtle, context-specific nuances. This research contributes to the growing body of knowledge on AI assisted text analysis. We discuss limitations and provide recommendations for future research, emphasizing the need for careful consideration when generalizing LLM judge models across various contexts and use cases."
        },
        {
            "title": "Start",
            "content": "Rewina Bedemariam1, Natalie Perez1, Sreyoshi Bhaduri, Satya Kapoor, Alex Gil, Elizabeth Conjar, Ikkei Itoku, David Theil, Aman Chadha, Naumaan Nayyar"
        },
        {
            "title": "Abstract",
            "content": "Rapid advancements in large language models have unlocked remarkable capabilities when it comes to processing and summarizing unstructured text data. This has implications for the analysis of rich, open-ended datasets, such as survey responses, where LLMs hold the promise of efficiently distilling key themes and sentiments. However, as organizations increasingly turn to these powerful AI systems to make sense of textual feedback, critical question arises, can we trust LLMs to accurately represent the perspectives contained within these text based datasets? While LLMs excel at generating human-like summaries, there is risk that their outputs may inadvertently diverge from the true substance of the original responses. Discrepancies between the LLM-generated outputs and the actual themes present in the data could lead to flawed decision-making, with far-reaching consequences for organizations. This research investigates the effectiveness of LLMs as judge models to evaluate the thematic alignment of summaries generated by other LLMs. We utilized an Anthropic Claude model to generate thematic summaries from open-ended survey responses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as LLM judges. The LLM-asjudge approach was compared to human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha, validating scalable alternative to traditional human centric evaluation methods. Our findings reveal that while LLMs as judges offer scalable solution comparable to human raters, humans may still excel at detecting subtle, contextspecific nuances. This research contributes to the growing body of knowledge on AI assisted text analysis. We discuss limitations and provide recommendations for future research, emphasizing the need for careful consideration when generalizing LLM judge models across various contexts and use cases. 1. Introduction The rapid evolution of Large Language Models (LLMs) has expanded their potential uses, from generating content to assessing it. As organizations increasingly adopt these models, there is growing need to evaluate the accuracy and alignment of LLM-generated outputs with human perspectives (Long et al., 2024). The concept of using LLMs as evaluative judges dates back to efforts in natural language processing to improve evaluation metrics such as BLEU or ROUGE, which primarily measure word overlap (Wang et al., 2024). However, these traditional evaluation metrics often fall short when it comes to accurately assessing the nuances of natural language generation tasks (Liu et al., 2023). As LLMs increasingly power the analysis of open-ended textual data in organizational settings, ensuring the fairness and accuracy of their outputs becomes crucial. In artificial intelligence, model alignment refers to techniques designed to align LLM behaviors with human values and expectations (Shen et al., 2023). This involves methods like fine-tuning, human feedback, and reinforcement learning to ensure outputs reflect human-like reasoning and decision-making processes (Liu et al., 2024). Model alignment is critical, especially when models like Anthropic Claude are used as judge models in behavioral research (Shen et al., 2023). Our research makes significant contribution by investigating the effectiveness of using LLMs as judges to evaluate the thematic alignment of summaries generated by other LLMs, specifically in the context of organizations using open-text survey responses. Our study is important because it addresses critical gap in the responsible deployment of AI systems for decision-making processes that directly impact organizational decision-making. We employed an Anthropic Claude model to generate thematic summaries from open-ended survey responses and utilized Amazons Titan Express and Nova Pro LLMs and Llama as judges to evaluate these summaries. By comparing the LLM-as-judge approach with human evaluations using Cohen's kappa, Spearmans rho, and Krippendorffs alpha, we present scalable alternative to traditional human-centric evaluation methods. This research is particularly relevant to the AI in Talent Management research community, as it explores the potential for AI systems to serve as impartial arbiters of content accuracy and representation, while also highlighting the ethical considerations and potential biases inherent in such approaches. Our findings demonstrate that LLM judges can produce results comparable to human raters, offering organizations more efficient means of validating AI-generated insights. However, we also critically examine the limitations of this approach and provide recommendations for future research to ensure fairness, accountability, and transparency in the use of LLMs for organizational decision-making. This work 1 Both authors contributed equally to this work. 1 contributes to the ongoing dialogue on responsible AI deployment and the development of trustworthy evaluation mechanisms for AI-generated content in high-stakes environments. The following research questions are explored: 1) To what extent can LLMs replicate human judgment in evaluating thematic alignment, and what factors contribute to discrepancies between LLM and human ratings? 2) What are the implications of higher inter-model agreement compared to human-model agreement for the development and application of LLMs in similar content analysis and theme evaluation tasks? 2. Background \"LLM-as-a-judge\" has emerged as an innovative solution, wherein an LLM evaluates another models output to approximate human labeling (Zheng et al., 2023). This approach offers the promise of automating human judgment at scale, while maintaining high levels of reliability and consistency. For example, Yan (2024) demonstrated that, when properly calibrated, LLMs can reach agreement rates close to those of human annotators, which can be costly and time-intensive. From this lens, the utilization of LLMs can significantly reduce analysis time and turn around rapid results. By comparing an LLM's evaluation of generated content to human evaluations, researchers aim to refine LLM capabilities and ensure alignment with human interpretations (Rao et al., 2024; Wang et al., 2024).This approach could simplify the evaluation process by reducing reliance on human resources, enabling faster iterations in development cycles. Therefore, understanding how well LLMs can replicate and adhere to human judgment is crucial for their effective deployment in organizational settings (Wang et al., 2024). Open-Text Survey Data Open-ended survey data is type of textual data where respondents provide unstructured text-based responses. This type of data is useful for obtaining authentic and sometimes unexpected information that illuminates why or how respondents think, feel, or behave (Rouder et al., 2021). Unlike scaled or categorical survey items, open-ended questions encourage respondents to provide information about their experiences, perceptions, thoughts, values, and/or feelings that researchers might not anticipate or share information about topics that might be sensitive or personal in nature (Allen, 2017). Open-text data can be particularly valuable in telling nuanced stories and highlighting diverse responses (Rouder et al., 2021). Thematic Summaries LLMs can be used for variety of textual analysis, including topic modeling (Bhaduri et al., 2024; Kapoor et al., 2024) and summarizing large amounts of unstructured text-based data (Tang et al., 2023). The objective of textual summarization is to communicate the primary meaning of an original dataset into simplified and straightforward form without sacrificing the integrity of the information being conveyed (Dutta et al., 2022). Scholars in the LLM field have explored textual summarization as valuable use-case for LLMs (Tang et al., 2023). Yet research has revealed factual inconsistencies in using LLMs to generate textual summaries, with range of errors or biases (Tang et al., 2023; Chan et al., 2023; Ashwin et al., 2023). LLMs and Classification Evaluation Strategy There are number of LLM evaluation strategies available, and classification prompts are one way to classify text into predefined categories (Rao et al., 2024). In particular, classification prompts allow researchers to classify content into categories, inductively or deductively (Rao et al., 2024). Historically, text classification has been conducted using traditional ML approaches that are often complex and resource-intensive; tasks include extraction, dimensionality reduction, classifier selection, and model evaluation, not including pre-processing steps (Wang et al., 2024). However, LLM advancements have made text classification easier with only three required elements: 1) data collection, 2) feeding LLM data, and 3) obtaining classification results from an LLM (Wang et al., 2024). Since LLMs have been pre-trained on diverse datasets, researchers have found that they require little, if any, additional training specific to task or domain area (Wang et al., 2024). Given recent LLM advancements and prompt engineering, LLMs offer cheaper alternative to classify thematic summaries based on unstructured open-text survey data. This ability has the potential to help researchers not only analyze textual data, but also classify textual generation into distinct categories (Zhao et al., 2023). In particular, researchers can use classification prompts and evaluation to assess the accuracy, also known as correctness, of LLM classifications, by measuring the proportion of accurately classified thematic summaries (Rao et al., 2024). That said, the absence of robust methodological strategies to evaluate open-text thematic summaries reveals gap in the research field addressing this novel use-case. This study aims to evaluate how well classification prompts can accurately rate thematic summary content alignment. 3. Methods The process of implementing the LLM-as-a-judge methodology involves taking the text output from one AI model and feeding it back into another LLM. The second model, now serving as the judge, evaluates the text based on an evaluation prompt provided by the user. The second LLM then returns score, label, or descriptive judgment depending on the specific evaluation criteria set by the user. This allows for high degree of customization, as users can instruct the LLM to assess specific properties, which makes the approach adaptable to various applications. This framework also means that LLM-as-ajudge is not an evaluation metric in the traditional sense, like accuracy or precision (Yan, 2024). Rather, it serves as general technique for approximating human judgment, where the LLM relies on its training to assess qualities like faithfulness to source or correctness. This way, LLMs act as proxy evaluators, following detailed prompts much like human evaluator would. While this technique does not produce fixed measure, it offers flexible proxy metric that can align with specific use cases. 3.1. Dataset We obtained access to novel, non-open source dataset for this analysis. The benefit of testing across new dataset, not previously used for model pre-training, is that the researchers can determine how well LLMs analyze and generate predictions (outputs) accurately. The data used in this study was collected using census survey that addressed work-related topics and was launched across multi-global population who reported working full-time. The survey contained one free-text question that asked the respondents to share one thing about work they want senior leaders to be aware of. Over 13,000 comments were collected. The researchers pre-processed the responses by removing short samples, noise (e.g., text with only symbols such as periods or dashes and no other content), and personally identifiable information (PII). The 13K dataset was further grouped into 70 smaller datasets by segmenting the data into groups based on business lines, so insights could be shared with business lines and the datasets would not be corrupted with comments from individuals that did not report to certain business lines. Next, previously engineered thematic summary prompt was used to analyze the dataset and create LLM-generated thematic summaries. The thematic summaries were produced based on validated prompt that leverages thematic quality principles, as outlined by Naeem et al., (2023), including JSON formatting instructions focused on identifying theme name (must contain at least one topic and sentiment details), thematic summary (must contain words that reflect genuine experiences or perceptions of respondents, must contain words that are rich in meaning and provide detailed understanding of the topic or focus area, must describe the relationship of theme to related topics that aid to contribute to new insight), and raw verbatim comments (must include verbatim words expressed by respondents). The data was run through different LLM API calls. total of 70 thematic summaries containing three themes per summary were generated (i.e., each theme contained theme name, theme description with 3-4 sentences, and one representative verbatim comment), based on the 70 input datasets. The thematic summaries provide aggregated findings across the top-three most salient and prevalent themes within each of the respective input datasets. This study only used the 70 LLM-generated thematic summaries to evaluate how well the thematic summary content was aligned across theme name, theme description, and representative comments to reflect the theme description and theme name. 3.2. Overview of the Evaluation Our general methodology used Claude model to generate thematic summary outputs from survey comments. For evaluations, this study utilized three-stage methodology to evaluate the thematic alignment of LLM-generated summaries, which involved human evaluators, and several LLM models including Anthropic Claude (v2.1), Claude Sonnet (v3.5) Amazon Titan Express and Nova Pro, and Llama evaluations. Each model was tasked with assessing alignment across three dimensions: theme name, description, and representative quote. Step 1. Human Evaluation as Baseline. Human evaluators were first tasked with reviewing the thematic summaries based on alignment among the theme name, description, and representative verbatim comment/quote generated by Claude model. The human ratings served as the baseline for comparison against LLM assessments. To evaluate the prompts ability to accurately evaluate content alignment, human evaluators used the same content alignment criteria as the LLM-as-a-judge model (see scale below). Step 2. LLM Evaluation: Claude as the Initial Evaluator. The Claude model was provided with the same summaries and instructed to assign alignment scores based on structured evaluation prompt. The prompt specifically asked Claude to rate the thematic coherence across each theme's name, description, and quote. The prompt engineered for this study to classify thematic summaries into predefined categories was based on content alignment using the following rating scale: 1 to 3, with 3 (Not Aligned\"), 2 (\"Somewhat Aligned\"), and 3 (\"Completely Aligned\"). Since each thematic summary contained three dimensions, the prompt evaluated each dimension independently (refer to Appendix 1 for prompt and output examples). Step 3. Multiple Models as LLM-Judge of Claudes Output. Following Claudes evaluation, Titan Express, Claude Sonnet 3.5, Llama 3.3 (70b), and Nova Pro models were tasked with evaluating the scores generated by the human evaluations and across the models to identify baseline for agreement cross the human and model evaluations. Inferential parameters were configured with the following settings: top-p value of 0.9, top-k value: 0.25, and Temperature of 0. This multi-model evaluation aimed to mirror human review process while leveraging different models' unique capabilities. This experiment compared the evaluation results of several distinct LLM ratings of the same outputs. The objective was to determine if LLMs could replicate each others judgments and thus validate the consistency of the LLM-as-a-judge approach. Claude, Titan, Nova, and Llama models served as the evaluators, examining thematic alignment, accuracy, and relevance of the outputs. 3.3. Evaluation Strategy The evaluation strategy for this study involved testing and validating classification prompt focused on content alignment, then using the prompt and running Anthropics Claude on the 70 thematic summaries generated from over 13,000 openended survey responses. blind review was conducted with human evaluators to rate the same 70 thematic summaries. Both the classification prompt and human raters were assigned the same directions and rating criteria to rate the thematic summaries based on content accuracy. Reliability was assessed using four metrics: Percentage Agreement, Cohens kappa, Spearmans rho, and Krippendorffs alpha. Percentage Agreement provides an intuitive measure of exact matches but does not account for chance alignment. Cohens kappa adjusts for chance but assumes nominal categories. Spearmans rho determines if enough data has been rated, while Krippendorffs alpha (Ordinal) accounts for ordinal datas inherent structure, offering more nuanced perspective on agreement (Cohen, 1960; Warrens, 2015; Hayes and Krippendorff, 2007). Evaluation Processes structured evaluation process was employed to determine the alignment between the LLM and human ratings. The process involved three key steps: Step 1. Evaluation Classification Prompt Development. classification prompt was engineered and used by human evaluators through reviewing and rating 70 LLM-generated thematic summary outputs. The aim of the prompt engineering focused on the scope of the evaluation, whether to evaluate the entire summary prompt, or evaluate each theme independently within one summary prompt. The researchers decided to focus on engineering prompt to focus on each theme independently across each thematic summary, which would result in three ratings per thematic summary, since each thematic summary contained three themes. Step 2. Prompt Testing and Validation. The classification prompt underwent testing to ascertain its effectiveness in identifying misalignment. This phase was conducted in partnership between different research teams, ensuring the prompt was accurate and reliable for broader application in model alignment tasks. The aim of the prompt was to accurately classify content alignment within thematic summaries. Multiple iterations were conducted until the research teams determined the details, instructions, and results aligned with the anticipated outputs, including ratings for each independent theme and details that describe the LLMs reason(s) for classifying each theme into particular category (refer to Appendix for prompts used for the evaluations). Step 3. Implementation. After finalizing the classification prompt, all 70 thematic summaries were run through 70 different LLM API calls. The researchers securely stored the ratings for further comparative analysis with the human evaluations, LLM evaluations and the LLM-as-a-judge model. 4. Results Our study sought to answer two research questions: 1) To what extent can LLMs replicate human judgment in evaluating thematic alignment, and what factors contribute to discrepancies between LLM and human ratings? and 2) What are the implications of higher inter-model agreement compared to human-model agreement for the development and application of LLMs in content analysis and theme evaluation tasks? We examined the alignment between ratings provided by LLM and human evaluators for set of 70 thematic summaries. The inter-rater agreement between human evaluators and LLMs demonstrated consistency across various LLM architectures. When comparing the Cohens kappa results between the human ratings and the models, Sonnet 3.5 had the highest rate of 4 agreement with human raters with score of 0.44, indicating moderate agreement (Cohen, 1960). However, nuanced differences emerged when examining Cohen's kappa and Krippendorff's alpha coefficients. This variability in agreement metrics can be attributed to several factors: the differential sensitivity of agreement measures, with Cohen's kappa being more sensitive to marginal distributions and Krippendorff's alpha accounting for different types of data and multiple raters; the inherent stochasticity in LLM outputs, which introduces degree of randomness potentially affecting the stability of agreement metrics; task-specific performance variations, where certain LLMs may exhibit slight advantages in specific evaluation criteria or content domains; and variations in human raters' expertise and their interpretation of annotation guidelines. These findings underscore the importance of employing multiple agreement metrics and considering the underlying factors that influence inter-rater reliability when assessing LLM performance in evaluation tasks. Table 1: Inter-rater Agreement Metrics between Human and LLM Rating. Red: best; Blue: second best"
        },
        {
            "title": "Comparisons",
            "content": "Human vs Claude 2.1 Ratings Human vs Titan Express Ratings Human vs Sonnet 3.5 Ratings Human vs Llama 3.3 70b Ratings Human vs Nova Pro Claude 2.1 vs Titan Express Ratings Claude 2.1 vs Sonnet 3.5 Ratings Claude 2.1 vs Llama 3.3 70b Ratings Claude 2.1 vs Nova Pro Ratings Titan Express vs Sonnet 3.5 Ratings Titan Express vs Llama 3.3 70b Ratings Titan Express vs Nova Pro Ratings Sonnet 3.5 vs Llama 3.3 70b Ratings Sonnet 3.5 vs Nova Pro Ratings Nova Pro vs Llama 3.3 70b Ratings Percentage Agreement 79% 78% 76% 79% 76% 91% 75% 85% 85% 74% 84% 87% 76% 82% 84% Cohen's kappa 0.41 0.35 0.44 0.39 0.34 0.70 0.35 0.50 0.54 0.32 0.44 0.57 0.37 0.56 0.47 Note: This table includes several metrics that are defined below. Spearmans rho 0.62 0.50 0.60 0.63 0.57 0.86 0.43 0.65 0.69 0.47 0.68 0.71 0.55 0.70 0.72 Krippendorff's alpha Ordinal 0.60 0.49 0.60 0.60 0.57 0.87 0.41 0.65 0.69 0.44 0.68 Krippendorff's alpha Nominal 0.42 0.41 0.35 0.39 0.35 0.70 0.35 0.50 0.54 0.30 0.43 0.71 0.49 0.67 0.71 0.57 0.37 0.55 0. Percentage Agreement definition: Percentage agreement tells us how many times two raters provide the same rating (e.g., 1 5) of the same thing, such as two people providing the same 5-star rating of movie. The more times they agree, the better. This is expressed as percentage of the total number of cases rated and calculated by dividing the total agreements by the total number of ratings and multiplying by 100 (McHugh, 2012). Cohens kappa definition: Cohens kappa is essentially smarter-version of percentage agreement. It is like when two people guess how many of their 5 co-workers will wear the color blue in the office each day; sometimes both people guess the same number (e.g., 15) by chance. Cohens kappa takes into account how well the two people agree, beyond any lucky guesses. The coefficients range from -1 to +1, where 1 represents perfect agreement, 0 represents agreement equivalent to chance, and negative values indicate agreement less than chance (Cohen, 1960). Spearmans rho definition: Spearman's rho is like friendship meter for numbers. It shows how well two sets of numbers get along or move together. If one set of numbers goes up and the other set also goes up, they have positive relationship. If one goes up while the other goes down, they have negative relationship. Coefficients range from 1 to +1, with values closer to 1 indicate stronger correlations (Schober et al., 2018). In this study, rho was used as statistical test on Cohens kappa as parameter. Rho allowed researchers to determine if enough data was used to ensure the rating agreement was sound (Shaffer, 2017). Krippendorff's alpha definition: Krippendorff's alpha is test used to determine how much all raters agree on something. Imagine two people taste-testing different foods at restaurant and rating the foods on scale of 15. 5 Krippendorff's alpha provides score to show how much the two people agree on their food ratings, even if they did not taste every dish in the restaurant. The alpha coefficient ranges from 0 to 1, where values closer to 1 indicate higher agreement among raters. Generally, an alpha above 0.80 signifies strong agreement, between 0.67 and 0.80 indicates acceptable agreement, and below 0.67 suggests low agreement (Krippendorff, 2018). If calculated with the rationale that the levels (1, 2 and 3) are ordinal, Krippendorffs Alpha considers not just agreement but also the magnitude of disagreement. It is less affected by marginal distributions compared to kappa and provides more nuanced assessment when ratings are ranked (ordinal). That is, while percentage agreement and kappa treat all disagreements equally, Alpha recognizes the difference between minor (e.g., \"1\" vs. \"2\") and major disagreements (e.g., \"1\" vs. \"3\"; Krippendorff, 2018). Moderate Agreement Between LLM and Human Ratings Our results show that despite the high percentage agreement between the human ratings and those of the models (e.g., two highest percentage agreements Human vs. Claude (v2.1): 79% and Human vs. Llama: 79%), the corresponding Cohen's kappa and Krippendorffs alpha values indicate range of low to high substantial reliability (Kappa range between human and models: 0.340.44; Kappa range between models: 0.320.70; Alpha range between human and models (ordinal): 0.49 0.60; Alpha range between models (ordinal): 0.410.87; Alpha range between human and models (nominal): 0.350.42; Alpha range between models: 0.300.70). The Spearmans rho results suggest moderate to strong correlation between the human and model ratings (0.500.62). In most cases, Spearman's rho is higher than Cohen's kappa. This suggests that while the humans and models might not always provide the exact same rating, they tend to rank items similarly. In addition, while percentage agreement only considers exact matches, metrics like Cohens Kappa and Krippendorffs alpha adjust for chance agreement and weighting of ordinal disagreements, respectively (refer to Table 1). Krippendorffs alpha further accounts for the ordinal nature of the data, reflecting nuanced differences between raters more accurately than percentage agreement (Artsein and Poesio, 2008; Krippendorff, 2018). Variability in Performance Across LLM Models Interestingly, when considering all the metrics, Claude (v.2.1) performed well across all metrics, with one of the highest percentage agreements with the human ratings (79%), second-highest Cohen's Kappa score (0.41), tied for highest Krippendorff's alpha (Ordinal) score (0.60), and highest Krippendorff's alpha (Nominal) score (0.42). The model that performed the second best across all metrics was Llama 3.3 (70b). Sonnet 3.5 and Nova Pro did not perform as well across most metrics. These findings suggest that Claude (v2.1) appears to be the most consistently aligned with human ratings across all metrics, followed by Llama 3.3, with Titan Express, Sonnet 3.5, and Nova Pro presenting mixed results. Although the choice of prioritizing metrics could change the ranking, Claude (v2.1) is most consistent across all metrics. As an older model in the Claude suite, the results suggest that newer models do not always produce better results for all use-cases. Researchers might benefit from examining several models to determine which bests suites their respective use-cases. Higher Inter-Model Agreement Compared to Human-Model Agreement Overall, the results suggest moderate to high level of agreement and reliability between the human and LLM model ratings, as well as between the different LLM model ratings. However, when examining the results more closely, the models indicate being generally more consistent with each other compared with the human ratings, as evidenced by the higher percentage agreement, Cohen's Kappa, and Krippendorff's alpha values for the model-to-model comparisons. Overall, our findings suggest that while models such as Claudes Sonnet 2.1 demonstrates reasonable degree of alignment with human evaluations, there remains room for improvement. The moderate Cohen's Kappa highlights some discrepancies between human and LLM ratings. It is possible that the observed variations could stem from nuanced differences in human interpretation, especially when themes involve complex issues. Examples of discrepancies included instances where the LLM rated themes as completely aligned despite minor misalignments identified by human evaluators. Finally, our study results highlighted areas of alignment and discrepancies between LLM and human ratings: High Agreement Cases. In many instances where both the LLM and human rated the themes as Completely Aligned, the thematic summaries displayed high degree of coherence across theme name, description, and quotes. Both raters frequently aligned in recognizing specific work environment topics. Topics that were aligned included but were not limited to topics addressing role function, leadership, and policies. Discrepancies and Over-Estimation by LLM. In cases where the human rated alignment as somewhat aligned or not aligned, the LLM sometimes rated itself as Completely Aligned. For example, the LLM might have focused on general content similarities, while the human raters noted discrepancies in specific details, such as incomplete 6 coverage of the theme or misaligned quotes. This finding suggests that humans might perform better with more nuanced content compared with LLMs. 5. Discussion The findings confirm that while LLMs such as Claude, Titan, Nova, and Llama can demonstrate high levels of agreement with each other, they face challenges in fully replicating human judgment. In instances of partial alignment, both the Claude, Titan, and Llama models occasionally rated alignment higher than the human evaluators, suggesting potential for overestimating alignment. The Claude LLM displayed tendency to overestimate alignment, which may stem from lack of nuanced understanding of thematic details, particularly in cases where quotes added new dimensions or diverge slightly from the theme description. Furthermore, sophisticated prompts and evaluation criteria that capture the depth of alignment nuances more fully could also improve both the LLM generation and evaluation models. Therefore, human oversight remains essential. Specifically, integrating additional evaluation criteria that capture thematic nuances and content salience could improve alignment with human interpretations. Continuous improvement of the evaluation framework and prompt design is necessary to ensure that LLM outputs remain aligned with organizational needs. This includes developing more comprehensive assessment methods that account for the contextual and qualitative aspects of thematic understanding, beyond just the surface-level alignment. 6. Recommendations To address the observed discrepancies, researchers should consider incorporating additional evaluation metrics beyond content alignment. Possible improvements include investigating and mitigating biases in the evaluation of LLMs, which is crucial for refining the assessment process and ensuring reliable outcomes (Bhaduri et al., 2024). Position bias, which favors options presented earlier, and verbosity bias, which can lead to an overvaluation of longer responses, are among the key challenges that need to be addressed (Saito et al., 2023). Other potential biases include recency bias, confirmation bias, and anchoring bias. Mitigating these biases requires multifaceted approach, encompassing careful design of evaluation protocols, randomization of response order, utilization of diverse evaluators, and development of objective metrics (Saito et al., 2023). Defining comprehensive success metrics for LLMs necessitates interdisciplinary contributions from various fields. Computer science and AI can develop specific capability benchmarks and quantify properties like coherence and factual accuracy. Linguistics can assess grammatical correctness and pragmatic aspects of communication. Psychology can design experiments to measure human preferences and evaluate cognitive load. Philosophy can explore ethical considerations and refine definitions of key concepts. Domain experts can assess task-specific performance, while sociology and anthropology can examine societal implications and cultural sensitivities. Human-computer interaction can focus on user experience, and statistics can develop robust methodologies for data analysis. The integration of these diverse perspectives into cohesive evaluation frameworks represents an ongoing challenge in the rapidly evolving field of LLM development and assessment. 7. Conclusion This study contributes to deeper understanding of how well LLMs align with human judgments in thematic analysis. While the percentage agreement and Cohens Kappa results indicate fair agreement, the findings point to the need for ongoing adjustments and improvements to the LLMs evaluation prompt. The analysis reveals areas for improvement, particularly in instances where the LLM overestimated the degree of alignment compared to human raters. This tendency may stem from the LLM's limitations in fully comprehending the nuanced details and contextual factors that contribute to human interpretations of thematic content. The discrepancies highlight the continued need for human oversight and the refinement of evaluation frameworks to better capture the qualitative aspects of thematic understanding. Future research should explore strategies to further enhance the LLM's alignment with human judgments. This may involve developing more sophisticated prompts and evaluation criteria that account for thematic salience, repetition of quotes, and other contextual factors. Additionally, fine-tuning the LLM to better identify and handle personal identifiable information (PII) and theme recurrence could improve the reliability and trustworthiness of the generated insights. As researchers continue to leverage the power of LLMs in analyzing open-ended survey data, ongoing collaboration between human experts and machine learning systems will be crucial. By iteratively improving the evaluation methods and incorporating human feedback, researchers can unlock the full potential of LLMs to generate high-quality thematic summaries that reliably reflect the perceptions and experiences of survey respondents."
        },
        {
            "title": "References",
            "content": "Allen, M. (2017). The SAGE Encyclopedia of Communication Research Methods. 4 vols. Thousand Oaks, CA: SAGE Publications, Inc. https://doi.org/10.4135/9781483381411. Artstein, R., & Poesio, M. (2008). Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4), 555-596. https://doi.org/10.1162/coli.07-034-R2. Ashwin, J., Chhabra, A., & Rao, V. (2023). Using Large Language Models for Qualitative Analysis can Introduce Serious Bias. arXiv. https://doi.org/10.48550/arXiv.2309.17147. Bhaduri, S., Kapoor, S., Gil, A., Mittal, A., & Mulkar, R. (2024). Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research. arXiv preprint arXiv:2408.11043. https://doi.org/10.48550/arXiv.2408.11043. Bhaduri, S., Kuketz, V., Savoia, S., & Virguez, L. (2024, June). Beyond the Algorithm: Empowering AI Practitioners through Liberal Education. In 2024 ASEE Annual Conference & Exposition. https://doi.org/10.18260/1-2-- 46648. Chan, C. M., Chen, W., Su, Y., Yu, J., Xue, W., Zhang, S., ... & Liu, Z. (2023). ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate. arXiv preprint arXiv:2308.07201. https://doi.org/10.1038/s41746-02300896-7 Chen, B., Zhang, Z., Langrené, N., & Zhu, S. (2023). Unleashing the potential of prompt engineering in Large Language Models: comprehensive review. arXiv preprint arXiv:2310.14735. https://doi.org/10.48550/arXiv.2310.14735. Cohen, J. (1960). Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement, 20(1), 3746. https://doi.org/10.1177/001316446002000104 Dutta, S., Das, A. K., Ghosh, S., & Samanta, D. (2022). Data Analytics for Social Microblogging Platforms. Academic Press. https://doi.org/10.1016/C2021-0-01247-5. Hayes, A. F., & Krippendorff, K. (2007). Answering the call for standard reliability measure for coding data. Communication methods and measures, 1(1), 77-89. https://doi.org/10.1080/19312450709336664. Kapoor, S., Gil, A., Bhaduri, S., Mittal, A., & Mulkar, R. (2024). Qualitative Insights Tool (QualIT): LLM Enhanced Topic Modeling. arXiv preprint arXiv:2409.15626. https://doi.org/10.48550/arXiv.2409.15626. Krippendorff, K. (2011). Computing Krippendorff's alpha-reliability. Departmental Papers (ASC), 43. Krippendorff, K. (2018). Content analysis: An introduction to its methodology. Sage publications. https://doi.org/10.4135/9781071878781. Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., & Zhu, C. (2023). G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634. https://doi.org/10.48550/arXiv.2303.16634. Long, D. X., Kawaguchi, K., Kan, M. Y., & Chen, N. F. (2023). Aligning Large Language Models with Human Opinions through Persona Selection and Value--Belief--Norm Reasoning. arXiv e-prints, arXiv-2311. https://arxiv.org/html/2311.08385v4 McHugh, M.L. (2012). Interrater reliability: the kappa statistic. Biochem Med (Zagreb). https://doi.org/10.11613/BM.2012.031. Naeem, M., Ozuem, W., Howell, K., & Ranfagni, S. (2023). Step-by-Step Process of Thematic Analysis to Develop 8 Conceptual Model in Qualitative Research. International Journal of Qualitative Methods, 22. https://doi.org/10.1177/16094069231205789. Rao, V. N., Agarwal, E., Dalal, S., Calacci, D., & Monroy-Hernández, A. (2024). QuaLLM: An LLM-based Framework to Extract Quantitative Insights from Online Forums. arXiv preprint arXiv:2405.05345. https://arxiv.org/abs/2405.05345 Rouder, J., Saucier, O., Kinder, R., & Jans, M. (2021). What to do with all those open-ended responses? Data visualization techniques for survey researchers. Survey Practice. https://doi.org/10.29115/SP-2021-0008. Saito, K., Wachi, A., Wataoka, K., & Akimoto, Y. (2023). Verbosity bias in preference labeling by large language models. arXiv preprint arXiv:2310.10076. https://doi.org/10.48550/arXiv.2310.10076. Shaffer, D. W. (2017). Quantitative ethnography. http://www.epistemicanalytics.org/publications/. Shen, T., Jin, R., Huang, Y., Liu, C., Dong, W., Guo, Z., ... & Xiong, D. (2023). Large language model alignment: survey. arXiv preprint arXiv:2309.15025. Schober, P., Boer, C., & Schwarte, L. A. (2018). Correlation coefficients: appropriate use and interpretation. Anesthesia & analgesia, 126(5), 1763-1768. https://doi.org/10.1213/ane.0000000000002864. Tang, L., Sun, Z., Idnay, B., Nestor, J. G., Soroush, A., Elias, P. A., ... & Peng, Y. (2023). Evaluating large language models on medical evidence summarization. npj Digital Medicine, 6(1), 158. https://doi.org/10.1038/s41746-023-00896-7. Wang, Z., Pang, Y., & Lin, Y. (2024). Smart Expert System: Large Language Models as Text Classifiers. arXiv preprint arXiv:2405.10523. https://doi.org/10.48550/arXiv.2405.10523. Warrens, M. J. (2015). Five ways to look at Cohen's kappa. Journal of Psychology & Psychotherapy, 5. DOI:10.4172/2161-0487. Yan, Ziyou. (Aug 2024). Evaluating the Effectiveness of LLM-Evaluators (aka LLM-as-Judge). eugeneyan.com. https://eugeneyan.com/writing/llm-evaluators/. Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; Zhang, H.; Gonzalez, J. E.; and Stoica, I. 2024. Judging LLM-as-ajudge with MT-bench and Chatbot Arena. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23. Red Hook, NY, USA: Curran Associates Inc. https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-PaperDatasets_and_Benchmarks.pdf Zhao, Z., Fan, W., Li, J., Liu, Y., Mei, X., Wang, Y., ... & Li, Q. (2023). Recommender systems in the era of large language models (llms). arXiv preprint arXiv:2307.02046. https://arxiv.org/abs/2307.02046."
        },
        {
            "title": "Appendix",
            "content": "Note: The content in this prompt is mock-content generated to provide an example of the thematic structure only; it does not reflect real data used in this study. You are behavioral research scientist whose job is to review outputs for thematic alignment. You will skillfully review, analyze, and determine whether or not the content in the <theme></theme>, <description></description>, and <quote></quote> are aligned in meaning or not using this scale:1 (not aligned), 2 (somewhat aligned), or 3 (completely aligned). Follow these steps in order and do NOT to miss theme2 and theme3: (1) Review the information in <theme1></theme1> (2) Review the information in <theme2></theme2> (3) Review the information in <them3></theme3> (4) Check for alignment between the meaning within each of the themes: <theme1></theme1>, <theme2></theme2>, and <them3></theme3>. (4) Based on each theme reviewed in step 1,2,and 3 assign an alignment rating score for each theme based on the following scale: 1 (not aligned), 2 (somewhat aligned), or 3 (completely aligned). <theme1> name: Learning and development desires description: Many individuals expressed their satisfaction with learning and development opportunities. However, some individuals requested more opportunities to engage in multi-day hands-on learning sessions with expert mentors to aid in learning and skill development. Some individuals suggested more learning and development trainings on more regular cadence. quote: really love the opportunities to learn and grow here. There are lot of options for new and tenured people to learn. That said, would love more hands-on training options with mentors; it would be great to be able to ask questions with an expert on-hand. </theme1> <theme2> name: Recognition perceived as important description: Individuals indicated there are many different recognition opportunities for career development, training, and promotion, some feel this is contributing to motivation and engagement. The ability to provide pathways for recognition and advancement was frequently cited as something that is valued. Some individuals connected recognition and advancement, reinforcing perceptions of value associated with recognition. quote: Lot of opportunities to be recognized, which is something appreciate. Being is important to advancement think. </theme2> <theme3> name: Global connections description: There is perception that the culture is valued as innovative and there are many opportunities to connect with individuals globally. Having the ability to partner, interact, and work with different individuals from around the globe was valued and meaningful. Some individuals feel opportunities to continue cross-cultural exchange opportunities. quote: feel it is amazing to be able to collaborate with peers around the world. Would be great to be able to continue to partner in variety of ways. </theme3> Assistant: Rating score theme1: theme1 score reason Rating score theme2: theme2_score_reason 10 Rating_score_theme3: theme3_score_reason"
        },
        {
            "title": "Output Example",
            "content": "Rating score theme1: 3 theme1 score reason: The theme, description, and quote in theme1 are completely aligned. They all focus on learning and development opportunities, with the description and quote both mentioning satisfaction with current opportunities while also expressing desire for more hands-on, mentor-led training sessions. Rating score theme2: 3 theme2_score_reason: The theme, description, and quote in theme2 are completely aligned. They all emphasize the importance of recognition in the workplace, its connection to career advancement, and how it contributes to motivation and engagement. The quote directly supports the description by highlighting the value placed on recognition opportunities. Rating_score_theme3: 3 theme3_score_reason: The theme, description, and quote in theme3 are completely aligned. They all focus on the value of global connections and cross-cultural collaborations within the organization. The description and quote both express appreciation for the opportunity to work with individuals from around the world and desire to continue and expand these global partnerships."
        }
    ],
    "affiliations": []
}