{
    "paper_title": "GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages",
    "authors": [
        "Amir Hossein Kargaran",
        "Fran√ßois Yvon",
        "Hinrich Sch√ºtze"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The need for large text corpora has increased with the advent of pretrained language models and, in particular, the discovery of scaling laws for these models. Most available corpora have sufficient data only for languages with large dominant communities. However, there is no corpus available that (i) covers a wide range of minority languages; (ii) is generated by an open-source reproducible pipeline; and (iii) is rigorously cleaned from noise, making it trustworthy to use. We present GlotCC, a clean, document-level, 2TB general domain corpus derived from CommonCrawl, covering more than 1000 languages. We make GlotCC and the system used to generate it - including the pipeline, language identification model, and filters - available to the research community. Corpus v. 1.0 https://huggingface.co/datasets/cis-lmu/GlotCC-v1, Pipeline v. 3.0 https://github.com/cisnlp/GlotCC."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 3 ] . [ 1 5 2 8 3 2 . 0 1 4 2 : r GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages Amir Hossein Kargaran Fran√ßois Yvon Hinrich Sch√ºtze LMU Munich & Munich Center for Machine Learning, Munich, Germany Sorbonne Universit√© & CNRS, ISIR, Paris, France amir@cis.lmu.de"
        },
        {
            "title": "Abstract",
            "content": "The need for large text corpora has increased with the advent of pretrained language models and, in particular, the discovery of scaling laws for these models. Most available corpora have sufficient data only for languages with large dominant communities. However, there is no corpus available that (i) covers wide range of minority languages; (ii) is generated by an open-source reproducible pipeline; and (iii) is rigorously cleaned from noise, making it trustworthy to use. We present GlotCC, clean, document-level, 2TB general domain corpus derived from CommonCrawl, covering more than 1000 languages. We make GlotCC and the system used to generate it including the pipeline, language identification model, and filtersavailable to the research community. Corpus Pipeline v. 1.0 v. 3.0 hf.co/datasets/cis-lmu/GlotCC-v1 github.com/cisnlp/GlotCC"
        },
        {
            "title": "Introduction",
            "content": "Progress in building language technologies has largely been limited to about 200 languages (referred to as large languages in this paper) for which text is reasonably accessible [34]. In this regard there is disparity between large languages and minority languages, disparity that becomes much more visible as state-of-the-art language technologies developing from word embedddings and encoder-only models to large autoregressive models become more data-hungry. FastText [14, 28] word vectors supports 160 languages, XLM-R [22] 100 languages, Llama-3 [51] 30 languages. So there is trend for the number of supported languages to decrease over time in state-of-the-art models. While efforts have been made to compile multilingual corpora from books [45, 26], the most common approach to collecting large amounts of raw textual data is to rely on crawled web text [55, 4, 11, 42, 67]. However, there are still many issues to resolve to guarantee the quality of the extracted web text, particularly for minority languages [41]. Most pipelines for web-text extraction rely on language identification (LID) to classify text, e.g., on FastText [14, 35] (as does OSCAR [4]) or on CLD3 [15, 64, 73]. Many errors in existing web corpora are artifacts of their LIDs [20]. For example, FastTexts accuracy suffers from hash collisions: it maps out-of-domain (OOD) n-grams to n-grams seen in training, resulting in many OOD errors. Additionally, commonly used LIDs can identify the (roughly) 200 largest languages but only few minority languages, leading to out-of-model cousin errors [20, 41]. In this paper, we adopt the Ungoliant pipeline [3] for extracting text from CommonCrawl. To address the limitations of current LID models (hash collisions and limited language coverage), we develop new LID model, GlotLID v3.0, an extension of GlotLID v1.0 [37]. This model covers over 2,000 Preprint. linguistic labels. It also mitigates common sources of noise encountered on the web with better rejection model. We also extend Ungoliant with several filtering techniques that remove general web noise, list-like content, documents with repeated words [42, 62] and inconsistent documents, i.e., documents for which LID detects multiple languages, often an indicator of noise. We perform an audit of random 653 language subcorpora of GlotCC, finding that the data is in-language, with macro-average score of 0.93 and median score of 1.0. We publish GlotCC as document-level corpus. This makes it usable for pretraining generative language models as well as for other language technologies that require information beyond the sentence level."
        },
        {
            "title": "2 GlotLID",
            "content": "In our previous work, we introduced GlotLID v1.0 [37], an LID model based on the FastText architecture [14] with good performance for 1665 languages. Upon further analysis, we improved GlotLID to v2.0 and then v3.0. In v2.0, we extended our ISO 639-3 labels to also include the ISO-15924 script; an example is rus-Cyrl (Russian language in Cyrillic script). This new labeling reduces errors and enhances the quality of language resources. For instance, it allows us to restrict the set of language labels that can be assigned to each script, thereby avoiding obvious mismatches between language and script [40]. This can be implemented by employing writing system detection tools, such as GlotScript-T [40], during inference. We also use GlotScript-T and GlotScript-R to filter out noisy training data from the GlotLID training corpus. GlotScript-T determines the script of an input text. GlotScript-R provides the admissible script(s) for each language. In GlotLID v3.0, we increase the number of LID labels to more than 2000 by adding, removing and relabeling resources based on feedback from the community and on our own findings from the continuous analysis we are conducting. 2.1 GlotLID v3.0 vs GlotLID v1.0/v2.0 2.1.1 Increased coverage of minority languages We include in the GlotLID training set new resources for African languages [43, 66, 7, 56], Uralic languages [31, 74], Indonesian languages [70], Indic languages [48, 25] as well as additional indigenous languages [19]. We also consider data for variety of languages from books [45], children storybooks [2], crowdsourcing [10], low-resource crawls [1], and world language repositories [54]. 2.1.2 Better rejection model with the UND label LID is typically understood as closed-set classification problem; most LIDs, including GlotLID, adopt this setup. As LID is in fact an open-set problem [32, 49], when processing web data, there is always the risk of encountering unknown languages (those not occurring in train). To limit errors that are caused by these unknowns, it is customary to reject samples for which the most likely language has low probability, assuming the classification model is well-calibrated [68, 37]. This problem is compounded when using popular LID architectures such as FastText and CLD3, which detect languages based on character n-gram features. To limit memory footprint and speed up inference, FastText hashes character n-grams into predefined range of integers, using this map to retrieve feature embeddings. This procedure applies to utf-8 character ùëõ-grams, where the default for GlotLID is 2 ùëõ 5. This approach makes no distinction between n-grams that have been seen in training and those that have not, as any n-gram is assigned to an existing hash value. Furthermore, as the number of languages increases, so does the number of scripts, and accordingly, the number of possible n-grams. With 160+ scripts, this number is in fact much greater than FastTexts default hash size (2,000,000), increasing the chance of collisions. This first implies that the probability of languages that are well represented in the training data (i.e., high-resource languages) or that have large number of n-grams due to their character set (e.g., Chinese) is overestimated. This also means that closed-set LIDs will provide non-zero probability score even for writing systems that were never seen during training, predicting (sometimes with high probability) languages whose n-grams collide with unseen n-grams. Although GlotLID supports all major scripts, its training data does not contain minor scripts such as Mong (Mongolian), Sylo (Syloti Nagri), Newa (Pracalit), Talu (New Tai Lue) and Gran (Grantha). In 2 order to correctly reject languages written in these scripts, and given that we have no access to reliable training data for them, we introduce set of 157 new und (undetermined) labels. The associated training data is obtained by randomly generating character strings from the corresponding character set. For example, for the Talu script, we use the label und_Talu. und_Talu training data consists of randomly selected Talu characters forming 100,000 sentences. 2.1.3 Removing noise with zxx labels In addition to handling unseen scripts, web crawlers and processing systems also need to be robust to multiple types of noise [20]. To make GlotLID more robust, we create training data for additional types of noise, including mis-rendered PDFs and Mojibake (text decoded using an unintended character encoding). We either create artificial training data or we use an initial seed and query the Google search engine to obtain training data. We include six major sources of web noise that we encountered in our work on GlotLID. 1) Mis-rendered PDF: This is gibberish Latin text that appears when an Arabic PDF is mis-rendered by OCR. To generate representative sentences, we query 1and 2-grams of the letters i, and l, with spaces between them, as these letters are frequent in this context.1 2) S K: This is type of noise where the characters of the text are space-separated [20]. It is easy to generate synthetic training data from any text or alphabet. 3) Binary files: This type of noise occurs when binary files, especially images, become part of the text. It is easy to generate training data by opening any binary file in text editor. 4) Mojibake Latin: This is gibberish text that results from text being decoded using an unintended Latin character encoding. In this type of noise, vowel characters of latin with different accents in repeated form can serve as seed for Google queries, such as √°√†√£√†. 5) Mojibake Arabic: This is gibberish text that results from text being decoded using an unintended Arabic character encoding. In this type of noise, ^, , , and the Arabic characters Tah and Zah appear frequently. Our queries are based on these characters.3 6) Replacement character: The replacement character (U+FFFD) appears repeatedly in this type of text. We simulate it by randomly replacing characters with the replacement character. We create three GlotLID labels for these six noise types: zxx_Latn for noise types [1-4], zxx_Arab for noise type 5 and zxx_Zzzz for noise type 6. 2.1.4 Curation of labels To curate our set of LID labels, we rely on confusion matrices, focusing on languages with low performance, i.e., those that are frequently confused with others. We employ both genealogical analysis and basic sanity checking using web resources about the languages. See Section 7 of [37] for an example of our methodology. Most of the problems identified this way were due to the training data, e.g., the training corpus for given language is actually mix of several languages. We remove labels and their training data if we deem them too noisy based on our analysis. GlotLID v1.0 and v2.0 both support macro languages and individual languages. In cases where the correct class is an individual language, the associated probability is often spread over this language and its macro language. The reason is that many individual language n-grams and words are also frequent in the macro language. In GlotLID v1.0/v2.0, we provide the option of taking the softmax on subset of LID labels (e.g., on just the individual language and the macro language). However, according to community feedback, it is preferable for labels to be mutually exclusive, making it possible to run LID just once over all supported labels. We rectify this problem by (i) re-labeling macro languages as one of the individual languages; (ii) merging the individual languages into the macro language (in case of small differences between them, e.g., we merge individual languages prs_Arab and pes_Arab into macro language fas_Arab); and (iii) deleting the macro language in case we already have good support for its individual languages such as zho_Hani, aze_Latn, est_Latn. However, there are 1Mis-rendered PDF query example: google.com/search?q=i+j+l+ii+jj+ll+ij+ji+lj+jl+li+il 2Mojibake Latin query example: google.com/search?q=√ÉƒÖ√ÉƒÉ√Éƒç√ÉƒÉ 3Mojibake Arabic webpage example: al-jazirah.com/2010/20100802/index.htm 3 some conventional exceptions: we keep both srd_Latn and sdc_Latn. srd_Latn only represents sro_Latn and src_Latn, not sdc_Latn. This decision is based on the Glottolog tree [29] and the linguistic map of Sardinia. We changed tgl_Latn to fil_Latn following Kudugunta et al. [42]. 2.2 Evaluation setup We train GlotLID v3.0 using the same parameters and sampling strategy as GlotLID v1.0 [37] (also described in Table 1) for 1 epoch. In our previous work [37], we found that the variance of the FastText architecture with different initial seeds is negligible. Table 1: GlotLID v3.0 training hyperparameters Argument Description Minimal number of word occurrences -minCount -minCountLabel Minimal number of label occurrences -wordNgrams -bucket -minn -maxn -loss -dim -epoch -lr Max length of word ngram Number of buckets Min length of char ngram Max length of char ngram Loss function Size of word vectors Number of epochs Learning rate Value 1000 0 1 106 2 5 softmax 256 1 . GlotLID is trained on the GlotLID v3.0 corpus, which draws from many different sources; the three main sources are Wikipedia, news websites, and religious texts.4 The new languages (2.1.1), und labels (2.1.2), and zxx labels (2.1.3) are also included. As some of our evaluation benchmarks (2.2.1) might have leaked into other sources included in the GlotLID corpus, such as UDHR in articles (e.g., Wikipedia), translation community resources (e.g., Tatoeba), and news (e.g., BBC), we remove this contamination from the GlotLID corpus. We count benchmark test sentence as occurring in the GlotLID corpus if all of its word four-grams occur in one sentence of the GlotLID corpus. We remove all of the sentences from the GlotLID corpus that meet this condition. After deduplication, we split the corpus into training, validation, and test sets in the ratio 0.8/0.1/0.1. Following prior work [55, 37, 18], we report F1 and false positive rate (FPR) and assume that the set of languages covered by the evaluation benchmark is known. Accordingly, we restrict models predictions to those languages that occur in the intersection of the benchmark and model training data. 2.2.1 Evaluation data We evaluate GlotLID v3.0 on three existing benchmarks with high number of languages: UDHR, FLORES-200 and GlotTest (our in-domain test set). be the number of sentences from language ùëô in the GlotLID corpus test set. Then 1) GlotTest: Let ùëõùëô we sample min(1000, ùëõùëô ) sentences from it. We refer to the resulting dataset as GlotTest. GlotTest supports 2102 LID labels, the same number of labels supported by GlotLID v3.0. This includes the und labels (2.1.2) and zxx labels (2.1.3). 2) UDHR: UDHR consists of about 500 translations of the Universal Declaration of Human Rights. In this work, we use the UDHR test set released in our previous work [37].5 It supports the 415 translations from udhrinunicode.org that are available with valid ISO 639-3 code. 371 of these 415 are covered by GlotLID v3.0 training data. 3) FLORES-200: FLORES-200 [55] comprises 842 articles sourced from English-language Wikimedia projects. Each sentence of these articles was translated into 204 language_script labels. The dataset is split into 997 sentences for development, 1012 for dev-test and 992 for test. FLORES-200 test is not public. As is common practice, we use FLORES-200 dev-test as our FLORES-200 test set. 4github.com/cisnlp/GlotLID/blob/main/sources.md 5hf.co/datasets/cis-lmu/udhr-lid 4 2.2.2 Evaluation results Table 2 reports results on GlotTest, UDHR and FLORES-200. On average, GlotLID v3.0 achieves an F1 score of 0.991 and false positive rate of 0.000003 on GlotTest. The three lowest F1 scores on GlotTest are 0.72, 0.75, and 0.76 for bos_Latn, hrv_Latn, and cnr_Latn, three mutually intelligible varieties of BCMS (Bosnian-Croatian-Montenegrin-Serbian). We made sure that every LID label has performance of at least 0.7 on the GlotLID validation set. Otherwise, we remove or merge the label (see 2.1.4). Table 2: Performance of GlotLID v3.0 Compared to GlotLID v1.0 [37], GlotLID v3.0 shows improvements in F1 of 0.05 for GlotTest, 0.09 for UDHR, and 0.05 for FLORES-200. Although the UDHR results are the lowest among these three benchmarks (F1 of 0.882 vs 0.991 and 0.967), GlotLID v3.0 outperforms the state-ofthe-art also for this dataset [37]. The most likely reason for the lower performance on UDHR is domain shift: the data in the GlotLID corpus for some of the UDHR languages is dominated by religious texts. FPR 0.000003 0.000298 0.000161 GlotTest UDHR FLORES-200 F1 0.991 0.882 0.967 2102 371 199 Benchmark # Labels"
        },
        {
            "title": "3 GlotCC v1.0",
            "content": "The process of GlotCC creation is similar to other pipelines for large-scale web corpus creation [42]. More specifically, we use Ungoliant [3], the OSCAR pipeline [58]. Instead of OSCARs FastText LID (which detects 176 languages), we use GlotLID v3.0. Recall that this change also includes the zxx and und labels that are part of GlotLIDs label set and support robust noise removal (2). We further provide extensions for content classes (3.1) and quality warnings and filtering (3.2, 3.3). We also perform replacement of personally identifiable information (3.5) for GlotCC. We distribute our forked pipeline under the Apache 2.0 license, the same as the Ungoliant license. GlotCC is licensed under CommonCrawl terms: commoncrawl.org/terms-of-use. For this paper, we ran the pipeline on the CommonCrawl CC-MAIN-2024-10 snapshot in its entirety and on parts of CC-MAIN-2023-40 and CC-MAIN-2023-50. We refer to the corpus produced by this process as GlotCC v1.0 (or GlotCC for short). # Labels Macroarea Table 3: Geographic distribution of languages in GlotCC. GlotCC v1.0 contains data (subcorpora) for 1275 LID labels (i.e., language-script pairs such as rus-Cyrl). Table 3 shows their geographic distribution for Glottolog [29] macroareas.6 Based on the Wikipedia list of ISO 6393 codes,7 we also add 12 constructed languages (Constructed). As reflected in Table 4, GlotCC v1.0 considerably increases language coverage compared to OSCAR 23.01, especially for minority languages. The number of languages with more than 102 documents in GlotCC is 145+89+52+29+22+12=349 (vs 132 for OSCAR), and with more than 10 documents is 349+360=709 (vs 142 for OSCAR). This language coverage can be easily increased by applying GlotLID on more CommonCrawl snapshots. One reason for GlotCCs better coverage could be that we lose less minority language content via contamination [13] to large languages. GlotCCs Wikipedia percentage is highest (.2658, .2940) for languages with document count between 102 and 104. Many GlotCC languages with 102 documents come from religious websites (.4441, .4285). GlotCCs coverage of languages with more than 107 documents is lower than OSCARs because we apply more cleaning filters. Eurasia Papunesia Africa North America South America Australia Constructed 395 380 252 123 97 16 12 6Papunesia refers to Insular Southeast Asia and Oceania, excluding Australia. 7wikipedia.org/wiki/Codes_for_constructed_languages 5 Table 4: Partition statistics for OSCAR 23.01 and GlotCC-v1.0. Each partition is defined as: 10ùêΩ > # documents per language 10ùêº where 0 ùêº 7, 1 ùêΩ 9. {ùêº, ùêΩ } Corpus Version # Documents # Languages # Wikipedia # Religious # Words # Lines Total Median Total Median Total Median Total pct. Total pct. {7, 9} {6, 7} {5, 6} {4, 5} {3, 4} {2, 3} {1, 2} {0, 1} {0, 9} OSCAR 23.01 GlotCC-v1.0 OSCAR 23.01 GlotCC-v1.0 OSCAR 23.01 GlotCC-v1.0 OSCAR 23.01 GlotCC-v1.0 OSCAR 23.01 GlotCC-v1.0 OSCAR 23.01 GlotCC-v1. OSCAR 23.01 GlotCC-v1.0 OSCAR 23.01 GlotCC-v1.0 OSCAR 23.01 GlotCC-v1.0 24 12 23 22 25 26 52 14 89 20 145 10 360 10 566 152 2.7B 34.4M 579.5M 22.7M 15.1B - - 1.0T 780.8M 436.4B 80.0M 92.2M 2.4M 3.8M - 3.0B - 27.6B 122.1M 67.8B 9.3M 262.7K 10.7M 334.8K 305.4M 9.1M - - 3.2B 6.9B 12.6B 17.0B 738.8M 2.4B 82.4M 195.7M 919.7K 1.9M 60.1K 338.7K 8.6K 53.9K 368 11.5K 44 1.7K 25.2K 29.6K 3.6K 2.7K 400 326 36 24 4 2 2.8B 684.7M 69.7K - - 55.1M 714.4K 212.0M 5.4M 17.9M 1.3B - 8.2M - 1.4M - 245.0K - 41.5K - 18.5B - 52.2K - 6.5K 10.1M 315.7K 223.9M 1.4M 772.3K 13.4K 39.3M 192.6K - 460 - 26 - 254 13.6K 11.3M 21.5K 1.7M 1.1T 512.6B 431 20.5K 67 1.2K 14.5M 11.6K - 0. - 0.0001 - 0.0001 - 0.0005 - 0.0029 - 0.0606 - 0. - 0.4285 - 0.0009 - 0.0044 - 0.0219 - 0.0922 - 0. - 0.2940 - 0.1044 - 0.0285 - 0.000001 - 0.00000007 3.1 Annotating documents with content classes Similar to OSCAR 23.01, we use the UT1 blocklist [61] to classify websites into different content classes such as adult and blog. The main utility of these UT1-based filters is to warn about potential adult content websites, with 3.5 million domains labeled as adult in UT1. We add to UT1 two additional content classes: wikipedia and religious.8 The religious content class is important to assess how domain-specific particular languages corpus is for some minority languages almost all web content is religious [42]. Following OSCAR 23.01, we do not remove any content classes from GlotCC and instead leave that decision (e.g., removal of adult content) to the user, mainly because of UT1 false positives [4]. 3.2 Quality warnings We add new quality warnings adopted from prior work on data cleaning and web crawling [63, 62, 4, 30, 40, 42]. Specifically, we provide the following quality warnings.9 Tiny: The document has small number of lines. Following [63], we use threshold of three lines. Short sentences: The document has high number ( 50%) of short lines [4]. Header and footer: CommonCrawl contains boilerplate extracted from headers and footers. We give header (resp. footer) warning if short lines occur at the start (resp. end) of the document [4]. Inconsistent: Ungoliant applies LID at both document and line levels. If 60% of lines do not match the document-level label, we mark the document as LID-inconsistent. If 10% of the script content is incompatible with the label predicted by LID, we mark the document as script-inconsistent [40]. List case: Flag sentences with 50% of tokens beginning with capital letter [42]. Some scripts like Hani lack capital letters. However, since Chinese is written without spaces, we can still find lists by identifying portions of the text that are shorter than five characters and are surrounded by spaces. Technical characters: Fires when 20% of characters are numbers/punctuation [42]. The warning script-inconsistent is also raised here since 10% is not written in the main script. Cursed regex: These are substrings and regexes from [42] used for identifying noisy and questionable content. 8We compile the religious category based on popular websites like jw.org and ebible.com. While we believe it to be helpful indicator for the user, it is not perfect, neither in terms of recall (especially for nonChristian content) nor precision (many religious websites also contain non-religious content). 9In the following discussion, we use the terms line and sentence interchangeably. 6 Repetition: Repetition of words and bigrams indicates poor quality [62]. We give the warning repetition if sentence has >20 words and either >50% of the words or >20% of the bigrams are repetitive. This heuristic cannot be applied to languages without word boundaries. Long word: This warning applies if there is word with more than 100 characters; see [42]. Lorem ipsum: If the text contains the placeholder lorem ipsum [63], we flag it as lorem ipsum. Policy: Many texts have boilerplate policy notices [63]. We flag the text as policy if it contains any of the following: terms of use, privacy policy, cookie policy, uses cookies, use of cookies, use cookies. JS warning: Many texts contain warnings that JavaScript should be enabled [63]. If the text contains JavaScript or Javascript, we flag it as js warning. Curly bracket: Curly braces are mostly used in programming languages, not natural language [63]. If the text contains curly braces, we flag it as curly bracket. Adult words: This is collection of pornographic keywords from [42], mostly for Chinese tokens. We also found out that the GPT-4o tokenizer [57] has many adult tokens and gambling terms, so we conducted manual audit of Chinese tokens longer than 3 characters in this tokenizers vocabulary and added them as additional keywords. 3.3 Quality warning filters We sample 20 sentences from three languages for each script: one from the top 10% of the language distribution (i.e., with most data in GlotCC), one from the bottom 75% and one from the remaining 15%. For each sentence, we determine whether it is high-quality content in the target language. For unknown languages, we check the website URL and search for information about the language. We find that the quality warnings generally indicate bad content or erroneously assigned LID labels and therefore remove sentences with quality warnings in GlotCC. There are two exceptions. First, we ignore three warnings: short sentences, header, and footer, because the LID label is correct for most of these documents, or in other cases, the warnings are false positives. Second, for languages without overt word boundaries,10 we keep documents with warnings long word and repetition as computing these warnings is nonsensical if the text is not separated into words. The warning metadata for these five warnings is kept in GlotCC in case users want to use it. 3.4 Deduplication Following OSCAR 23.01, hash is provided for each document in GlotCC. This hash is computed by py-tlsh11 with hyperparameters 256 buckets and 3-byte checksums [4]. However, we do not distribute deduplication as part of the pipeline, because deduplication is costly [26, 4] and hashing algorithm and hyperparameters are application-dependent. 3.5 Personally identifiable information replacement We replace two types of personally identifiable information (PII): email addresses and public network IP addresses. We do not replace phone numbers due to the high false positive rate of regex patterns. We use the PII process implemented by DataTrove [59] (see also FineWeb [60]). Email addresses are replaced with email@example.com or firstname.lastname@example.com, and public network IP addresses are replaced with one of six IP addresses: 22.214.171.124, 126.96.36.199, 188.8.131.52, 184.108.40.206, 220.127.116.11, or 18.104.22.168, which, at the time of corpus creation, were unresponsive to pings. 3.6 Wall time We calculate the pipelines wall time, considering only the LID, without accounting for quality warning filters, the PII process, or any infrastructure-related bottlenecks. Suppose the LID throughput documents per second, and we can run ùëÉ parallel jobs. is, on average, ùëáùëÜ sentences per second and ùëáùê∑ 10wikipedia.org/wiki/Category:Writing_systems_without_word_boundaries 11pypi.org/project/py-tlsh 7 We have ùê∑ documents to annotate, each containing an average of ùëÜ sentences. For each document, in addition to running the LID on the entire document, we also run it on each individual sentence. The estimated processing time in hours is given by: Wall time (hours) = ùê∑ 3600 ùëÉ ) ( ùëÜ ùëáùëÜ + 1 ùëáùê∑ The processing time on Intel Xeon E7-8857 3GHz CPUs with ùëÉ = 48 for one Common Crawl snapshot (ùê∑ = 3.16 109 for CC-MAIN-2024-10) is estimated, using the given values of ùëáùëÜ = 1379, ùëáùê∑ = 245, and ùëÜ = 20, to be approximately 340 hours. 3.7 Self-audit quality review We perform self-audit of GlotCC-V1.0. We have two motivations. First, following [41], we want to ensure that the target language metadata are correct and that there are no systematic issues. Second, we intend to develop additional filters to clean the corpus for future releases. The bottleneck in such an audit is the difficulty of finding native speakers for each language. Therefore, following [42], we conduct the audit by providing high-level comments on the data quality and identifying the language of the data by looking for language clues. We randomly select 653 languages. We sample 20 pages from each (or all if there are fewer than 20) and check the validitiy of GlotCCs LID label. Additionally, we analyze common errors and provide high-level comments. We follow these guidelines in conducting the audit: For unknown languages, we inspect the URL and visit the webpage to find language clues such as language codes (especially in the URL), the lang attribute inside the html tag, country flag, contact address and the name of the language in the text. Otherwise, we search for sentences on the web to consult similar webpages related to that sentence. If the corpus contains noise but the noise appears filterable, we leave high-level note detailing the noise and how it can be filtered. We report the percentage of in-language sentences for each audited language. Overall results. Out of 653 audited languages, we find that, with macro-average score of 0.93 and median score of 1.0, the data is in-language. During the audit, for some languages, we couldnt determine the correct language; therefore, we do not consider those languages in the audited set. We find 10 out-of-model languages: aon (Bumbita Arapesh, Torricelli), bkx (Baikeno, MalayoPolynesian), gup (Gunwinggu, Arnhem, Northern Australia), ibl (Ibaloi, Philippine), kpo (Ikposo, Atlantic-Congo), mcr (Menya, Trans-New Guinea), mge (Mango, Nilo-Saharan), mrh (Mara Chin, Sino-Tibetan), sxw (Saxwe Gbe, Atlantic-Congo) and tao (Yami, Malayo Polynesian). We plan to include these languages in GlotLID v4.0. There are also errors that neither the LID nor the filters captures. For example, repetitive n-grams in list-like content, such as those at the start or end of words from websites like anagrams.app. Based on these audits, we published more clean version of GlotCC-V1.0 to the community. 3.8 Evaluation of LID within the pipeline We compare the NLLB LID [55] and GlotLID within the context of the Ungoliant pipeline. For this comparison, we run the pipeline in the exact same configuration, except that we use NLLB LID for one run and GlotLID for the other. We look at random sample of minority language pages for which the two pipelines make different predictions. In more detail, we select subset of size 1% (a prefix) of the latest CommonCrawl snapshot (CCMAIN-2024-18) and run the GlotCC pipeline on it, once using GlotLID as the LID and once using the NLLB LID as the LID. This produces corpus that is analogous to GlotCC-v1.0, except it is based on (a prefix of) different CommonCrawl snapshot. We count the number of times that an LID label (e.g., rus-Cyrl) is assigned by either GlotLID or NLLB LID in this filter subset; we refer to this number as ùëõùëô 10 times because our main focus is minority languages, not large languages that are already well covered by existing resources. We further only consider pages for which GlotLID and NLLB LID disagree. This for label ùëô. We restrict the comparison to those labels that occur ùëõùëô 8 gives us final set of 260 pages. Note that this set of 260 pages would correspond to 100 100 260 = 2,600,000 pages, had we run the pipeline on the entire CommonCrawl. (We take prefix of size 1% of the latest snapshot and there have been about 100 CommonCrawl snapshots so far.) Table 5: Comparison of GlotLID and NLLB on random subset of 20 pages from minority languages Table 5 reports comparison of GlotLID and NLLB LID for random sample of 20 of the 260 pages; see codebase for detailed report of the comparison. miss refers to cases where the LID did not make call. We see that GlotLID is correct 13 times for pages that NLLB LID treated incorrectly (3 misclassified pages, 10 miss pages) and incorrect 5 times for correct decisions by NLLB LID (3 incorrect calls and 2 incorrect miss decisions). On 2 pages, both GlotLID and NLLB LID make incorrect decisions. GlotLID correct error miss NLLB correct error miss 10 0 - 3 2 0 0 3 The main reason for GlotLIDs clearly better performance is that it makes many more calls than the prior state of the art, without losing overall accuracy. This is in keeping with the substantial expansion of GlotLIDs label set (more than 2,000) compared to prior work."
        },
        {
            "title": "4 Related work",
            "content": "4.1 LID There is wealth of resources to perform LID, but not all of them meet the requirements of good LID for minority corpus creation [37]. All of the following cover at most 218 languages: CLD2 [50], Equilid [36], Langdetect [65], langid.py [47], OpenLID [18], NLLB LID [55], FastText LID [14], CLD3 [15, 64], and HeLI-OTS [33]. Some LIDs are not open-source, e.g., those published by Caswell et al. [20], Bapna et al. [11], Kudugunta et al. [42]. whatlang [16, 17] and idNet [23] are two broad-coverage LIDs that meet many other requirements but are hard to use in many practical scenarios due to software issues and lack of maintenance. Franc [72] is character 3-gram LID with support for 400+ languages; however, it does not provide well-calibrated probabilities. Another LID with similar properties and support for 1600+ languages is FUN-LangID [71]; however, according to the developers, this model is not the best for high performance on F1/FPR. AfroLID [5], which covers African languages, is Transformer architecture and less efficient than its competitors. strong requirement for LIDs is their effective applicability for very large corpora, not least for ecological reasons. GeoLID [24] meets most of the requirements; it supports 900+ languages and the architecture is based on FastText. However, it needs geographic prior information, which makes it more suitable for social media such as (Twitter) that provide such geographic priors. 4.2 Multilingual corpora Much work has been done on mining multilingual corpora from the web. Xue et al. [73] introduce mC4, general 101-language web domain corpus, to train the mT5 model. Similarly, Conneau et al. [22] introduce CC-100 based on the CC-Net repository [69] to train the XLM-R model. The OSCAR corpus [4] supports 150+ languages. The mC4 pipeline uses CLD3, and CC-Net and OSCAR use FastText LID. NLLB Team et al. [55] mine an internal corpus from CommonCrawl with 200+ languages using NLLB LID. The MADLAD-400 corpus [42] is another mined corpus with 450+ languages using an internal 500-language coverage LID and pipeline. The most closely related work to ours is by Bapna et al. [11], who create an internal corpus of 1500+ languages using an internal 1600+ LID and pipeline. There are also high-quality pipelines that focus on creating corpora mostly for English, including Dolma [67], RedPajama-Data-v2 [21], DCLM [46] and FineWeb [60]. There is also some work that does not only mine the web, but builds mulitlingual data upon other datasets by compiling multiple data sources, such as the ROOTS corpus [44], community-built dataset that contains 46 languages. CulturaX [53] combines mC4 3.1.0 and different OSCAR versions and applies additional filters, resulting in 160+ languages. The Glot500 corpus [30] covers 500+ languages (400+ open-access), mostly based on prior work in academia. Serengeti [6] also introduces an internal dataset of 500+ African languages derived from religious texts, news and academia. 9 To the best of our knowledge, GlotCC is the first open-access corpus that is based on an open pipeline (including open-access LID) and that passes the threshold of 160 languages (e.g., CulturaX [53]) in web-mined corpora; in fact, as we show we cover more than thousand languages. As our statistics show, we find many languages on the web that have never before been part of web-mined corpus."
        },
        {
            "title": "5 Limitations",
            "content": "Use cases: Due to certain filtering steps (e.g., curly bracket filter), the GlotCC likely does not contain much math/code content. It is advisable to supplement GlotCC with math/code data if this is the intended use case. Due to the use of LID and script inconsistency filters as indicators of noise, the documents that exist in the corpus are more monolingual than multilingual or code-switched [39] compared to when these filters are not used. Additionally, since we did not customize the processing for each website, some sources, such as Wikipedia, may have better formatting in the original than in GlotCC. Errors: Although we have good in-language content, GlotCC still exhibits many types of errors and noise, including misclassification and out-of-model cousins. Model training: We did not train any language model to better justify the significance of GlotCC, as this would be pointless unless we also evaluate them. Evaluating language models requires evaluation data that we mostly do not have for minority languages (see [38, 9, 8, 30]). At this stage, this endeavor seems unrealistic. We believe that identifying relevant data and performing LID is crucial first step in that direction."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce document-level, general domain web corpus covering more than 1000 languages. We open-source the entire pipeline, including better language identification model, which is more robust in the corpus creation task in terms of noise handling, unseen writing systems, and broad coverage of languages to reduce the chance of encountering unknown languages. We filter the created corpus and perform self-audit to ensure the created corpus is clean. We hope the creation of such corpus and pipeline will benefit language technologies, enabling the inclusion of more minority languages. For future work, we plan to extend this corpus to additional CommonCrawl snapshots."
        },
        {
            "title": "7 Ethics statement",
            "content": "The advancement of NLP technologies has primarily been constrained to languages for which resources are available in good quality and quantity. Many of the worlds minority languages face significant barrier due to the scarcity of high-quality general data sources, making it difficult to develop NLP tools for these languages. Despite concerns that an extractive approach to NLP often does not benefit the affected communities [12], it is still an important goal of both computational and theoretical linguistics to have as good representation of minority languages in available web-based corpora as is permitted by the licenses of content available on the web. By providing cleaned corpus like GlotCC, we take initial steps towards including diverse range of languages in NLP. Despite our strong focus on filtering, the cleaning of GlotCC is constrained by the lack of tools for filtering out undesirable content and noise such as adult content, personal information, lists and this is considerable problem for subset of languages, including languages without explicit word boundaries. Therefore, we recommend users carefully evaluate their specific use case before using GlotCC."
        },
        {
            "title": "Acknowledgments and disclosure of funding",
            "content": "We would like to thank anonymous reviewers. This work was funded by Deutsche Forschungsgemeinschaft (project SCHU 2246/14-1). 10 References [1] GlotSparse, 2023. URL https://huggingface.co/datasets/cis-lmu/GlotSparse. [2] GlotStoryBook, GlotStoryBook. 2023. URL https://huggingface.co/datasets/cis-lmu/ [3] Julien Abadji, Pedro Javier Ortiz Su√°rez, Laurent Romary, and Beno√Æt Sagot. Ungoliant: An optimized pipeline for the generation of very large-scale multilingual web corpus. Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-9) 2021. Limerick, 12 July 2021 (Online-Event), pages 1 9, Mannheim, 2021. Leibniz-Institut f√ºr Deutsche Sprache. doi: 10.14618/ids-pub-10468. URL https://nbn-resolving.org/urn:nbn:de: bsz:mh39-104688. [4] Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Beno√Æt Sagot. Towards Cleaner Document-Oriented Multilingual Crawled Corpus. arXiv e-prints, art. arXiv:2201.06642, January 2022. [5] Ife Adebara, AbdelRahim Elmadany, Muhammad Abdul-Mageed, and Alcides Inciarte. AfroLID: neural language identification tool for African languages. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 19581981, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.128. URL https://aclanthology.org/2022.emnlp-main.128. [6] Ife Adebara, AbdelRahim Elmadany, Muhammad Abdul-Mageed, and Alcides Alcoba Inciarte. SERENGETI: Massively multilingual language models for Africa. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 14981537, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.97. URL https://aclanthology.org/ 2023.findings-acl.97. [7] David Ifeoluwa Adelani, Marek Masiak, Israel Abebe Azime, Jesujoba Oluwadara Alabi, Atnafu Lambebo Tonja, Christine Mwase, Odunayo Ogundepo, Bonaventure F. P. Dossou, Akintunde Oladipo, Doreen Nixdorf, Chris Chinenye Emezue, Sana Sabah al azzawi, Blessing K. Sibanda, Davis David, Lolwethu Ndolela, Jonathan Mukiibi, Tunde Oluwaseyi Ajayi, Tatiana Moteu Ngoli, Brian Odhiambo, Abraham Toluwase Owodunni, Nnaemeka C. Obiefuna, Shamsuddeen Hassan Muhammad, Saheed Salahudeen Abdullahi, Mesay Gemeda Yigezu, Tajuddeen Gwadabe, Idris Abdulmumin, Mahlet Taye Bame, Oluwabusayo Olufunke Awoyomi, Iyanuoluwa Shode, Tolulope Anu Adelani, Habiba Abdulganiy Kailani, Abdul-Hakeem Omotayo, Adetola Adeeko, Afolabi Abeeb, Anuoluwapo Aremu, Olanrewaju Samuel, Clemencia Siro, Wangari Kimotho, Onyekachi Raphael Ogbu, Chinedu E. Mbonu, Chiamaka I. Chukwuneke, Samuel Fanijo, Jessica Ojo, Oyinkansola F. Awosan, Tadesse Kebede Guge, Sakayo Toadoum Sari, Pamela Nyatsine, Freedmore Sidume, Oreen Yousuf, Mardiyyah Oduwole, Ussen Kimanuka, Kanda Patrick Tshinu, Thina Diko, Siyanda Nxakama, Abdulmejid Tuni Johar, Sinodos Gebre, Muhidin Mohamed, Shafie Abdi Mohamed, Fuad Mire Hassan, Moges Ahmed Mehamed, Evrard Ngabire, and Pontus Stenetorp. MasakhaNEWS: news topic classification for African languages. ArXiv, 2023. [8] Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng, Krithika Ramesh, Prachi Jain, Akshay Nambi, Tanuja Ganu, Sameer Segal, Mohamed Ahmed, Kalika Bali, and Sunayana Sitaram. MEGA: Multilingual evaluation of generative AI. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 42324267, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.258. URL https://aclanthology.org/2023.emnlp-main.258. [9] Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Mohamed Ahmed, Kalika Bali, and Sunayana Sitaram. MEGAVERSE: Benchmarking large language models across languages, modalities, models and tasks. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human 11 Language Technologies (Volume 1: Long Papers), pages 25982637, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.143. URL https://aclanthology.org/2024.naacl-long.143. [10] Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common voice: massively-multilingual speech corpus. In Nicoletta Calzolari, Fr√©d√©ric B√©chet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, H√©l√®ne Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 42184222, Marseille, France, May 2020. European Language Resources Association. URL https://aclanthology.org/2020.lrec-1.520. [11] Ankur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, Theresa Breiner, Vera Axelrod, Jason Riesa, Yuan Cao, Mia Xu Chen, Klaus Macherey, Maxim Krikun, Pidong Wang, Alexander Gutkin, Apurva Shah, Yanping Huang, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. Building machine translation systems for the next thousand languages. arXiv preprint arXiv:2205.03983, 2022. [12] Steven Bird. Decolonising speech and language technology. In Donia Scott, Nuria Bel, and Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, pages 35043519, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.313. URL https://aclanthology.org/2020.coling-main.313. [13] Terra Blevins and Luke Zettlemoyer. Language contamination helps explains the cross-lingual capabilities of English pretrained models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 35633574, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.233. URL https:// aclanthology.org/2022.emnlp-main.233. [14] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5: 135146, 2017. doi: 10.1162/tacl_a_00051. URL https://aclanthology.org/Q17-1010. [15] Jan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov, Alex Salcianu, David Weiss, Ryan McDonald, and Slav Petrov. Natural language processing with small feed-forward networks. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 28792885, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1309. URL https://aclanthology.org/D17-1309. [16] Ralf Brown. Language-aware string extractor, 2014. URL https://sourceforge.net/ projects/la-strings/. [17] Ralf Brown. Finding and identifying text in 900+ languages. Digital Investigation, 9:S34S43, 2012. [18] Laurie Burchell, Alexandra Birch, Nikolay Bogoychev, and Kenneth Heafield. An open dataset and model for language identification. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 865879, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-short.75. URL https://aclanthology.org/2023.acl-short.75. [19] Gina Bustamante, Arturo Oncevay, and Roberto Zariquiey. No data to crawl? monolingual corpus creation from PDF files of truly low-resource languages in Peru. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 29142923, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://www.aclweb.org/anthology/2020.lrec-1.356. 12 [20] Isaac Caswell, Theresa Breiner, Daan van Esch, and Ankur Bapna. Language ID in the wild: Unexpected challenges on the path to thousand-language web text corpus. In Donia Scott, Nuria Bel, and Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, pages 65886608, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.579. URL https://aclanthology.org/2020.coling-main.579. [21] Together Computer. Redpajama: an open dataset for training large language models, 2023. URL https://github.com/togethercomputer/RedPajama-Data. [22] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm√°n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 84408451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https://aclanthology.org/2020.acl-main.747. [23] Jonathan Dunn. Mapping languages: The corpus of global language use. Language Resources and Evaluation, 54:9991018, 2020. [24] Jonathan Dunn and Lane Edwards-Brown. Geographically-informed language identification. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 76727682, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.org/ 2024.lrec-main.678. [25] Jay Gala, Pranjal Chitale, Raghavan, Varun Gumma, Sumanth Doddapaneni, Aswanth Kumar M, Janki Atul Nawale, Anupama Sujatha, Ratish Puduppully, Vivek Raghavan, Pratyush Kumar, Mitesh Khapra, Raj Dabre, and Anoop Kunchukuttan. Indictrans2: Towards high-quality and accessible machine translation models for all 22 scheduled indian languages. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=vfT4YuzAYA. [26] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [27] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum√© Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12):8692, 2021. [28] Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov. Learning word vectors for 157 languages. In Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018), 2018. [29] Harald Hammarstr√∂m, Robert Forkel, Martin Haspelmath, and Sebastian Bank. Glottolog 5.0. Max Planck Institute for Evolutionary Anthropology, 2024. doi: 10.5281/zenodo.10804357. URL http://glottolog.org. Available online at http://glottolog.org, Accessed on 2024-06-01. [30] Ayyoob ImaniGooghari, Peiqin Lin, Amir Hossein Kargaran, Silvia Severini, Masoud Jalili Sabet, Nora Kassner, Chunlan Ma, Helmut Schmid, Andr√© Martins, Fran√ßois Yvon, and Hinrich Sch√ºtze. Glot500: Scaling multilingual corpora and language models to 500 languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10821117, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.acl-long.61. [31] Heidi Jauhiainen, Tommi Jauhiainen, and Krister Linden. Wanca in korp: Text corpora for underresourced Uralic languages. In Proceedings of the Research data and humanities (RDHUM) 2019 conference. University of Oulu, 2019. 13 [32] Tommi Jauhiainen, Marco Lui, Marcos Zampieri, Timothy Baldwin, and Krister Lind√©n. Automatic language identification in texts: survey. Journal of Artificial Intelligence Research, 65: 675782, 2019. [33] Tommi Jauhiainen, Heidi Jauhiainen, and Krister Lind√©n. HeLI-OTS, off-the-shelf language identifier for text. In Nicoletta Calzolari, Fr√©d√©ric B√©chet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, H√©l√®ne Mazo, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 39123922, Marseille, France, June 2022. European Language Resources Association. URL https://aclanthology.org/2022. lrec-1.416. [34] Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state and fate of linguistic diversity and inclusion in the NLP world. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 62826293, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.560. URL https://aclanthology.org/2020.acl-main.560. [35] Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H√©rve J√©gou, and Tomas Mikolov. FastText.zip: Compressing text classification models, 2016. URL https://arxiv. org/abs/1612.03651. [36] David Jurgens, Yulia Tsvetkov, and Dan Jurafsky. Incorporating dialectal variability for socially equitable language identification. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 5157, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-2009. URL https://aclanthology.org/P17-2009. [37] Amir Hossein Kargaran, Ayyoob Imani, Fran√ßois Yvon, and Hinrich Schuetze. GlotLID: Language identification for low-resource languages. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 61556218, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.410. URL https://aclanthology.org/2023. findings-emnlp.410. [38] Amir Hossein Kargaran, Ali Modarressi, Nafiseh Nikeghbal, Jana Diesner, Fran√ßois Yvon, and Hinrich Sch√ºtze. MEXA: Multilingual evaluation of english-centric LLMs via cross-lingual alignment, 2024. URL https://arxiv.org/abs/2410.05873. [39] Amir Hossein Kargaran, Fran√ßois Yvon, and Hinrich Schuetze. MaskLID: Code-switching language identification through iterative masking. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 459469, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-short.43. URL https://aclanthology.org/2024.acl-short.43. [40] Amir Hossein Kargaran, Fran√ßois Yvon, and Hinrich Sch√ºtze. GlotScript: resource and In Nicoletta Calzolari, Min-Yen Kan, tool for low resource writing system identification. Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 77747784, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.org/2024.lrec-main.687. [41] Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar UlziiOrshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Beno√Æt Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey Osei, Pedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, Andre Niyongabo Rubungo, Toan Q. Nguyen, Mathias M√ºller, Andr√© M√ºller, Shamsuddeen Hassan Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine √áabuk Ballƒ±, Stella Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa Adeyemi. Quality at Glance: An Audit of Web-Crawled Multilingual Datasets. Transactions of the Association for Computational Linguistics, 10:5072, 01 2022. ISSN 2307-387X. doi: 10.1162/tacl_a_00447. URL https://doi.org/10.1162/tacl_a_00447. [42] Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat. Madlad-400: multilingual and document-level large audited dataset. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 6728467296. Curran Associates, Inc., 2023. URL https://openreview.net/pdf?id=Y45ZCxslFx. [43] Richard Lastrucci, Jenalea Rajab, Matimba Shingange, Daniel Njini, and Vukosi Marivate. Preparing the vukuzenzele and ZA-gov-multilingual South African multilingual corpora. In Rooweither Mabuya, Don Mthobela, Mmasibidi Setaka, and Menno Van Zaanen, editors, Proceedings of the Fourth workshop on Resources for African Indigenous Languages (RAIL 2023), pages 1825, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.rail-1.3. URL https://aclanthology.org/2023.rail-1.3. [44] Hugo Lauren√ßon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz√°lez Ponferrada, Huu Nguyen, J√∂rg Frohberg, Mario ≈†a≈°ko, Quentin Lhoest, Angelina McMillan-Major, Gerard Dupont, Stella Biderman, Anna Rogers, Loubna Ben allal, Francesco De Toni, Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier de la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre, Sebastian Nagel, Leon Weber, Manuel Mu√±oz, Jian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid Almubarak, Minh Chien Vu, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Alexandra Luccioni, and Yacine Jernite. The bigscience roots corpus: 1.6tb composite multilingual dataset. Advances in Neural Information Processing Systems, 35:3180931826, 2022. [45] Colin Leong, Joshua Nemecek, Jacob Mansdorfer, Anna Filighera, Abraham Owodunni, and Daniel Whitenack. Bloom library: Multimodal datasets in 300+ languages for variety of downstream tasks. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8608 8621, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.590. URL https://aclanthology.org/ 2022.emnlp-main.590. [46] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. DataCompLM: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794, 2024. [47] Marco Lui and Timothy Baldwin. langid.py: An off-the-shelf language identification tool. In Min Zhang, editor, Proceedings of the ACL 2012 System Demonstrations, pages 2530, Jeju Island, Korea, July 2012. Association for Computational Linguistics. URL https://aclanthology. org/P12-3005. [48] Yash Madhani, Mitesh M. Khapra, and Anoop Kunchukuttan. Bhasha-abhijnaanam: Nativescript and romanized language identification for 22 indic languages, 2023. [49] Shervin Malmasi. Open-set language identification. arXiv preprint arXiv:1707.04817, 2017. 15 [50] Michael McCandless. Accuracy and performance of googles compact language detector. Blog post, 2010. [51] AI Meta. Introducing Meta Llama 3: The most capable openly available LLM to date. 2024. URL https://ai.meta.com/blog/meta-llama-3. [52] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency, pages 220229, 2019. [53] Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. Culturax: cleaned, enormous, and multilingual dataset for large language models in 167 languages, 2023. [54] Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajiƒç, Christopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. Universal Dependencies v2: An evergrowing multilingual treebank collection. In Nicoletta Calzolari, Fr√©d√©ric B√©chet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, H√©l√®ne Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 40344043, Marseille, France, May 2020. European Language Resources Association. URL https://aclanthology.org/2020.lrec-1.497. [55] NLLB Team, Marta R. Costa-juss√†, James Cross, Onur √áelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm√°n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022. [56] Odunayo Ogundepo, Tajuddeen R. Gwadabe, Clara E. Rivera, Jonathan H. Clark, Sebastian Ruder, David Ifeoluwa Adelani, Bonaventure F. P. Dossou, Abdou Aziz DIOP, Claytone Sikasote, Gilles Hacheme, Happy Buzaaba, Ignatius Ezeani, Rooweither Mabuya, Salomey Osei, Chris Emezue, Albert Njoroge Kahira, Shamsuddeen H. Muhammad, Akintunde Oladipo, Abraham Toluwase Owodunni, Atnafu Lambebo Tonja, Iyanuoluwa Shode, Akari Asai, Tunde Oluwaseyi Ajayi, Clemencia Siro, Steven Arthur, Mofetoluwa Adeyemi, Orevaoghene Ahia, Aremu Anuoluwapo, Oyinkansola Awosan, Chiamaka Chukwuneke, Bernard Opoku, Awokoya Ayodele, Verrah Otiende, Christine Mwase, Boyd Sinkala, Andre Niyongabo Rubungo, Daniel A. Ajisafe, Emeka Felix Onwuegbuzia, Habib Mbow, Emile Niyomutabazi, Eunice Mukonde, Falalu Ibrahim Lawan, Ibrahim Said Ahmad, Jesujoba O. Alabi, Martin Namukombo, Mbonu Chinedu, Mofya Phiri, Neo Putini, Ndumiso Mngoma, Priscilla A. Amuok, Ruqayya Nasir Iro, and Sonia Adhiambo. Afriqa: Cross-lingual open-retrieval question answering for african languages, 2023. [57] OpenAI. GPT4-o (May 13 version), 2024. hello-gpt-4o/. URL https://openai.com/index/ [58] Pedro Javier Ortiz Su√°rez, Beno√Æt Sagot, and Laurent Romary. Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures. Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019, pages 9 16, Mannheim, 2019. Leibniz-Institut f√ºr Deutsche Sprache. doi: 10.14618/ids-pub-9021. URL http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215. [59] Guilherme Penedo, Alessandro Cappelli, Thomas Wolf, and Mario Sasko. Datatrove: large scale data processing, 2024. URL https://github.com/huggingface/datatrove. [60] Guilherme Penedo, Hynek Kydl√≠ƒçek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The FineWeb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557, 2024. [61] Fabrice Prigent. ut1-blacklists. UT1 Blacklists, 2024. URL https://github.com/olbat/ [62] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson dAutume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher, 2021. [63] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [64] Alex Salcianu, Andy Golding, Anton Bakalov, Chris Alberti, Daniel Andor, David Weiss, Emily Pitler, Greg Coppola, Jason Riesa, Kuzman Ganchev, et al. Compact language detector v3. 2018. URL https://chromium.googlesource.com/external/github.com/google/cld_3/. [65] Nakatani Shuyo. Language detection library for java, 2010. URL https://github.com/ shuyo/language-detection. [66] Kathleen Siminyu, Godson Kalipe, Davor Orlic, Jade Abbott, Vukosi Marivate, Sackey Freshia, Prateek Sibal, Bhanu Neupane, David Adelani, Amelia Taylor, et al. AI4DAfrican Language Program. arXiv preprint arXiv:2104.02516, 2021. [67] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024. [68] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: good closed-set classifier is all you need. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=5hLP5JY9S2d. [69] Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm√°n, Armand Joulin, and √âdouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 40034012, 2020. [70] Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Rahmad Mahendra, Fajri Koto, Ade Romadhony, Kemal Kurniawan, David Moeljadi, Radityo Eko Prasojo, Pascale Fung, Timothy Baldwin, Jey Han Lau, Rico Sennrich, and Sebastian Ruder. NusaX: Multilingual parallel sentiment dataset for 10 Indonesian local languages, 2022. [71] Caswell Wormer. Fun LangID, 2023. URL https://github.com/google-research/ url-nlp/tree/main/fun_langid. [72] Titus Wormer. Franc Library, 2014. URL https://github.com/wooorm/franc. 17 [73] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mT5: massively multilingual pre-trained text-to-text transformer. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483498, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.41. URL https://aclanthology.org/2021.naacl-main.41. [74] Lisa Yankovskaya, Maali Tars, Andre T√§ttar, and Mark Fishel. Machine translation for lowresource Finno-Ugric languages. In Tanel Alum√§e and Mark Fishel, editors, Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa), pages 762771, T√≥rshavn, Faroe Islands, May 2023. University of Tartu Library. URL https://aclanthology.org/ 2023.nodalida-1.77."
        },
        {
            "title": "Checklist",
            "content": "1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Section 5. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Section 7. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments (e.g. for benchmarks)... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as URL)? [Yes] See the Abstract, Appendices A.1 and A.2. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See 2.2 (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] GlotCC does not depend on any random seed. For GlotLID see 2.2 (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See 3.6 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [Yes] See Appendix A.1 and Appendix A. (or Section 3). (c) Did you include any new assets either in the supplemental material or as URL? [Yes] (d) Did you discuss whether and how consent was obtained from people whose data youre using/curating? [Yes] See Appendix A.2 (or Section 3). (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] See Appendix A.2, Sections 3.5 and 7. 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] See 3.7. (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [No] The audits are performed by the authors."
        },
        {
            "title": "A Appendix",
            "content": "A.1 GlotLID model card We provide the GlotLID v3.0 model card based on the model card template introduced by Mitchell et al. [52]. Model details Person or organization developing model: LMU Munich and Sorbonne Universit√© Model Date: April 18, 2024 Model Types: Language identification and language modeling. Model Access: github.com/cisnlp/GlotLID and hf.co/cis-lmu/glotlid Information about training algorithms, parameters, fairness constraints or other applied approaches, and features: Provided in the Section 2 Paper: V3: Kargaran et al, GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages, NeurIPS, 2024 V1: Kargaran et al, GlotLID: Language Identification for Low-Resource Languages, EMNLP, 2023 License: Apache License 2.0 Contact: amir@cis.lmu.de Intended use Primary intended uses: Language identification on over 2000 linguistic labels. Primary intended users: Research community. Out-of-scope use cases: The model is trained on general domain data and might not perform well with short sentences or domain-specific data like the medical domain. Factors The classification quality of this model varies based on language, as some languages are easy to distinguish while others are challenging with n-gram models. Metrics We use F1 score and FPR (false positive rate), two widely used metrics in the language identification literature, for our evaluations. Ethical considerations We acknowledge that this model has high error rate for some of the languages. This means that there is potential risk of excluding minority languages during the collection and processing of NLP corpora. Training data We train this model using openly available (but not necessarily freely redistributable) datasets, including resources previously published by researchers, publishers, and translators. The complete list of data sources is available on github. com/ cisnlp/ GlotLID/ blob/ main/ sources. md . Evaluation data For evaluation, we used GlotTest, UDHR, and Flores-200 as described in 2.2.1. Caveats and recommendations We note that if there is setup where the list of languages is known, then the model can limit its predictions to that set of languages. This implies that languages not included in the set will be excluded from the softmax computation. We have provided the code for limiting this on github. com/ cisnlp/ GlotLID . 20 A.2 GlotCC datasheet We provide the GlotCC v1.0 datasheet based on the datasheet template introduced by Gebru et al. [27]. Motivation 1. For what purpose was the dataset created? (Was there specific task in mind? Was there specific gap that needed to be filled? Please provide description.) We created GlotCC as general web-crawled documentlevel dataset covering 1275 languages, with the purpose of breaking barriers by providing training data for language technologies to include broader range of languages. 2. Who created this dataset and on behalf of which entity? Amir Hossein Kargaran, Fran√ßois Yvon, Hinrich Sch√ºtze (LMU Munich, Sorbonne Universit√©) 3. Who funded the creation of the dataset? DFG (grant SCHU 2246/14-1). 4. Any other comments? None. Composition 1. What do the instances that comprise the dataset represent? Each instance is filtered instance from CommonCrawl, with its language annotated by GlotLID. 2. How many instances are there in total? GlotCC has 684.7M documents (18.5B lines, or 512.6B words) total across 1275 languages. 3. Does the dataset contain all possible instances or is it sample (not necessarily random) of instances from larger set? (If the dataset is sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover more diverse range of instances, because instances were withheld or unavailable).) GlotCC is created from CommonCrawl and has been annotated by GlotLID, then filtered. To maintain high level of in-language content, we have employed aggressive filtering, which may result in the exclusion of certain documents in specific language within CommonCrawl. 4. What data does each instance consist of? Each instance is raw text with accompanying metadata, including the timestamp, URL, quality warnings, category, tlsh hash, language identification probability, language identification consistency score, script consistency score, number of sentences, and content length. 5. Is there label or target associated with each instance? If so, please provide description. Yes, GlotCC has language label for each instance. 6. Is any information missing from individual instances? No. 7. Are relationships between individual instances made explicit? No. 8. Are there recommended data splits (e.g., training, development/validation, testing)? No. 9. Are there any errors, sources of noise, or redundancies in the dataset? Although GlotCC have good inlanguage content, GlotCC still exhibits many types of errors and noise, including misclassification and out-of-model cousins. 10. Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? (If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.) Yes. 11. Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals non-public communications)? (If so, please provide description.) It is possible as GlotCC is general web-crawled dataset. 12. Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? (If so, please describe why.) It is possible as GlotCC is general web-crawled dataset. 13. Does the dataset relate to people? Its possible that some instances of GlotCC mention and describe individuals. 14. Does the dataset identify any subpopulations (e.g., by age, gender)? Its possible that some instances of GlotCC mention and describe people of certain subpopulations. 21 15. Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? Its possible that some instances of GlotCC mention and describe individuals. 16. Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? It is possible as GlotCC is general web-crawled dataset. 17. Any other comments? None. Collection 1. How was the data associated with each instance acquired? From CommonCrawl CC-MAIN-2024-10 snapshot eniterly and on parts from CommonCrawl CC-MAIN-2023-40 and CC-MAIN-2023-50. 2. What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? We annotated the CommonCrawl data using GlotLID and then filtered the documents to create GlotCC. 3. If the dataset is sample from larger set, what was the sampling strategy? GlotCC is subset of CommonCrawl documents based on the GlotLID annotations and filtering steps. 4. Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? For the audit, the authors inspected the dataset. 5. Over what timeframe was the data collected? (Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.) Each instance is followed by UTC timestamp that shows the time of the crawl provided by CommonCrawl. 6. Were any ethical review processes conducted (e.g., by an institutional review board)? No. 7. Does the dataset relate to people? Its possible that some instances of GlotCC mention and describe individuals. 8. Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)? We collect the data from CommonCrawl. 9. Were the individuals in question notified about the data collection? No. 10. Did the individuals in question consent to the collection and use of their data? No. 11. If consent was obtained, were the consenting individuals provided with mechanism to revoke their consent in the future or for certain uses? N/A. 12. Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., data protection impact analysis) been conducted? No. 13. Any other comments? None. Preprocessing/cleaning/labeling 1. Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? (If so, please provide description. If not, you may skip the remainder of the questions in this section.) We filter the data based on various quality warning filters. 2. Was the \"raw\" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? We do not change the raw data; we only provide the metadata and determine which CommonCrawl instances to keep. 3. Is the software used to preprocess/clean/label the instances available? Yes, github. com/ cisnlp/ GlotCC 4. Any other comments? None. Uses 1. Has the dataset been used for any tasks already? (If so, please provide description.) No. 2. Is there repository that links to any or all papers or systems that use the dataset? No. 3. What (other) tasks could the dataset be used for? GlotCC can serve as general training dataset for any language included in GlotCC. 22 4. Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? (For example, is there anything that future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide description. Is there anything future user could do to mitigate these undesirable harms?) Despite our strong focus on filtering, the cleaning of GlotCC is constrained by the lack of tools for filtering out undesirable content and noise such as adult content, personal information, lists and this is considerable problem for subset of languages, including languages without explicit word boundaries. Therefore, we recommend users carefully evaluate their specific use case before using GlotCC. 5. Are there tasks for which the dataset should not be used? (If so, please provide description.) N/A. 6. Any other comments? None. Distribution 1. How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? GlotCC is made available through Huggingface datasets hf. co/ datasets/ cis-lmu/ GlotCC-V1 . 2. When will the dataset be distributed? June 2024. 3. Will the dataset be distributed under copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? (If so, please describe this license and/or ToU, and provide link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.) LMU Munich hosts GlotCC on Huggingface. We license the GlotCC metadata under CC0 1.0 (public domain). However, the GlotCC is licensed under the terms of the CommonCrawl Terms of Use. 4. Have any third parties imposed IP-based or other restrictions on the data associated with the instances? GlotCC is licensed under the terms of the CommonCrawl Terms of Use. 5. Any other comments? None. Maintenance 1. Who is supporting/hosting/maintaining the dataset? LMU Munich on Huggingface. 2. How can the owner/curator/manager of the dataset be contacted (e.g., email address)? Amir Hossein Kargaran (amir@cis.lmu.de). 3. Is there an erratum? (If so, please provide link or other access point.) github.com/cisnlp/GlotCC 4. Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? (If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?) Yes, we maintain the data based on discussions on Huggingface, GitHub and those reported via email. 5. If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for fixed period of time and then deleted)? (If so, please describe these limits and explain how they will be enforced.) N/A. 6. Will older versions of the dataset continue to be supported/hosted/maintained? (If so, please describe how. If not, please describe how its obsolescence will be communicated to users.) Yes, any version of the data will be hosted for the sake of reproducibility of results for others using the dataset. 7. If others want to extend/augment/build on/contribute to the dataset, is there mechanism for them to do so? (If so, please provide description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there process for communicating/distributing these contributions to other users? If so, please provide description.) We made the system to generate GlotCC, including the pipeline, language identification model, and filters, available to the research community. This allows them to build upon it or customize it for their use case. 8. Any other comments? None."
        }
    ],
    "affiliations": [
        "LMU Munich & Munich Center for Machine Learning, Munich, Germany",
        "Sorbonne Universit√© & CNRS, ISIR, Paris, France"
    ]
}