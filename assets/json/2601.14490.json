{
    "paper_title": "GutenOCR: A Grounded Vision-Language Front-End for Documents",
    "authors": [
        "Hunter Heidenreich",
        "Ben Elliott",
        "Olivia Dinica",
        "Yosheb Getachew"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 2 ] . [ 1 0 9 4 4 1 . 1 0 6 2 : r GUTENOCR: GROUNDED VISIONLANGUAGE FRONT-END FOR DOCUMENTS Hunter Heidenreich, Ben Elliott, Olivia Dinica, Yosheb Getachew Roots.ai ai-ml@roots.ai Models & Demo: GutenOCR-3B GutenOCR-7B Demo"
        },
        {
            "title": "ABSTRACT",
            "content": "GutenOCR is family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint visionlanguage models expose reading, detection, and grounding through unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with lineand paragraph-level bounding boxes and conditional where is x? queries. We introduce grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.400.82). On Fox and OmniDocBench v1.5, our approach substantially improves regionand line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts. line-, Figure 1: Capability profile their of GutenOCR-3B/7B, Qwen2.5-VL backbones, and Qwen2.5-based OCR baselines on nine grounded OCR tasks spanning our in-domain suite and Fox. Each spoke shows raw score in [0, 1] (higher is better): in-domain page-, and local-reading tasks are scored as 1 CER, full and conditional detection as F1, and Fox page-, region-, line-, and color-guided OCR as 1 CER. GutenOCR substantially improves detection, fine-grained reading, and Fox region/line OCR, while trading off some page-level and colorguided performance relative to OCR-specialized baselines."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 GutenOCR Overview"
        },
        {
            "title": "2.1 Model Overview .",
            "content": ". . . . ."
        },
        {
            "title": "2.2 Task Families and Interfaces",
            "content": "3 Data & Training"
        },
        {
            "title": "3.1 Data curation .",
            "content": "."
        },
        {
            "title": "3.2 Training recipe .",
            "content": "4 Evaluation Setup"
        },
        {
            "title": "4.1 Metrics",
            "content": ". . . 4.2 Benchmarks . 5 Results 5.1 Models . . 5.2 In-Domain . 5.3 Fox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 OmniDocBench v1.5 . . . . . . . . . . . . . . . . . 5.4.1 Text Recognition . 5.4.2 Text Detection . . . . . . . . . . . . . . . . . . . . . . . . 5.4.3 Formula Recognition . 5.5 Training Stage Ablation . 5.6 Qualitative Examples . . . . . . . . . . . . . . . . . . . . . . . 6 Discussion 7 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1 Classical and Modern OCR Pipelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Document Conversion Toolchains and Markup Formats . . . . . . . . . . . . . . . . . . . . . . . . . 7.3 VisionLanguage Models for Documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4 Grounded OCR and Localization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.5 Benchmarks and Evaluation for OCR and Document Understanding . . . . . . . . . . . . . . . . . . 8 Conclusion Interface contract A.1 Task families & I/O schemas . A.2 Geometry and BOX Type . A.3 Layout-Sensitive text2d . A.4 Task-Specific Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 4 4 5 7 7 8 8 9 9 10 11 13 13 14 14 17 17 19 19 20 20 21 21 26 26 27 28 Data B.1 Real corpora: filtering and normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Synthetic corpora: generation pipelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Splits and held-out sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Training C.1 Input Preprocessing & Setup . C.2 Hyperparameters C.3 System Prompt . . . . . . . . . . . . . Evaluation D.1 Parsing Structured Outputs . D.2 Metrics . . . . . . . D.2.1 Text metrics . . . . . . . D.2.2 Detection metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2.3 End-to-end metrics (detection + recognition) . . . . . . . . . . . . . . . . . . . . . . . . . . D.2.4 Edge cases and invalid predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Benchmark Details E.1 Fox Benchmark Details . E.2 OmniDocBench v1.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Models Additional Results G.1 Detailed In-Domain Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.1.1 Reading . . . . . . . G.1.2 Localized Reading . G.1.3 Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.1.4 Conditional Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 28 29 30 30 30 30 31 32 32 32 32 33 33 35 36 37 37 38 38"
        },
        {
            "title": "Introduction",
            "content": "Document images remain primary medium for business transactions, scientific communication, and automated workflows. Optical character recognition (OCR) is therefore the front door for much of the text consumed by large language models (LLMs) and retrieval-augmented generation (RAG) systems: any content that OCR fails to capture is effectively invisible to downstream, text-only systems. Two main approaches currently shape document OCR. Classical pipelineswith distinct stages for detection, recognition, and layout analysisoffer explicit text grounding and controllable reading order, but are brittle on complex layouts and difficult to adapt to new domains or tasks [1, 2, 3, 4]. OCR-free visionlanguage models (VLMs), by contrast, now dominate many document-understanding benchmarks [5, 6, 7, 8], scaling to large datasets, handling diverse visual signals, and operating in complex domains (e.g., chemistry, music, mathematics). However, they typically treat text as latent variable: page-level transcripts are implicit, token-level grounding is at best partial, and reading behavior is controllable only indirectly via prompts. We refer to the OCR component that downstream systems actually interact with as grounded OCR front-end: model that (i) produces page transcripts, (ii) attaches each token or span to 2D bounding boxes, and (iii) can be prompted to read arbitrary regions on the page. 3 This lack of an explicit grounded front-end is particularly acute in production settings, where downstream systems need both high-quality text and fine-grained control over how it is read. They require precise local reading, explicit links between tokens and pixels, and representations that can be tailored (e.g., whitespace-sensitive, layout-aware ordering) for extraction, filtering, and RAG. Non-grounded page-to-Markdown outputs make human oversight costly: reviewers must read both the full page and the generated transcript end-to-end, which is slow and error-prone. In contrast, grounded outputs make errors easier to spot and correct: hallucinated content appears as boxes over empty regions, missing text manifests as gaps in box coverage, and misrecognized spans are localized to specific boxes. Grounding therefore directly supports human-in-the-loop verification and active-learning workflows. Existing OCR-specialized VLMs and end-to-end parsers [7, 8, 9, 10] deliver strong page-to-Markdown quality, but are not designed as grounded front-ends: they often couple OCR with task-specific decoding formats, expose limited or unstable tokenbox alignment, and offer only coarse control over where and how the page is read. When grounded modes are provided, they are frequently tied to bespoke prompt conventions or rigid output schemas that downstream systems must accommodate. Classical OCR engines, in contrast, expose modular, stable APIs for detection, recognition, and layout analysis that can be flexibly composed inside larger business workflows. Rather than forcing legacy systems to retrofit around model-specific formats, we seek VLM-based OCR systems that offer equally versatile, API-like interfaces. We revisit OCR as grounded front-end problem and introduce GutenOCR, VLM-based OCR system trained to behave like traditional OCR pipeline while retaining the flexibility of contemporary visionlanguage architectures [11, 12]. single checkpoint supports full-page reading and detection, localized reading, and conditional detection, and can return either plain transcripts or structured lineand paragraph-level outputs with bounding boxes. By releasing the model and training recipe, we aim to establish an open baseline for VLM-based OCR that combines the strengths of classical pipelines with the expressivity and scalability of general-purpose multimodal models, providing drop-in, human-verifiable front-end for downstream extraction and RAG systems. In summary, this work makes five contributions: Unified grounded OCR front-end. We present GutenOCR, family of 3B and 7B VLM checkpoints that, via prompt-specified inputoutput schemas, support full-page reading and detection, conditional detection, and localized reading. The models return either linear transcripts or structured lineand paragraph-level outputs with bounding boxes, integrating cleanly with existing business workflows and human-in-the-loop verification pipelines. OCR specialization recipe for general-purpose VLMs. We provide training recipe on public data mixture that turn Qwen2.5-VL-3B/7B [11] into grounded OCR-specialized models without modifying the tokenizer, adding adapters, or freezing modules, while supporting long-context page transcripts (up to 8k16k tokens). Metrics and protocol for grounded OCR. We define metric suite and evaluation protocol that jointly measure text accuracy, detection quality, and page-level fidelity across task families using CER, WER, F1@0.5, mCER@0.5, and CERe2e. These metrics separate pure recognition errors from localization and reading-order failures, avoiding good reader, bad paginator pathologies. Empirical analysis across in-domain and external benchmarks. On 10.5K held-out business and scientific pages, GutenOCR-7B substantially improves composite grounded OCR score, localized-reading CER, and line-detection F1 over its Qwen2.5-VL-7B backbone. On Fox [13] and OmniDocBench v1.5 [14], GutenOCR significantly reduces regionand line-level OCR CER and boosts text-detection recall, while revealing clear failure modes in formula-heavy and color-guided OCR settings (see Figure 1). Reproducible open release. We release our training and evaluation code, provide full data provenance (using only publicly available sources), and publish open model weights under license compatible with the upstream data and model licenses."
        },
        {
            "title": "2 GutenOCR Overview",
            "content": "2.1 Model Overview GutenOCR is family of grounded OCR front-ends as defined in Section 1. Each model is single-checkpoint visionlanguage system, obtained by fine-tuning Qwen2.5-VL [11], that exposes classical OCR stages and pointer-like behaviors (for example, Where is in this document?) via prompts. Conceptually, GutenOCR acts as thin, API-like layer between page images and downstream systems, exposing small set of stable OCR primitives (reading, detection, grounding) that can be composed into diverse document workflows. 4 Detect all text Read all text OX(q) query + Image Image + box text Figure 2: Unified GutenOCR interface. single visionlanguage model consumes page image, optionally conditioned on text query or bounding box b, and serves multiple OCR-style tasks (reading, detection, conditional detection, localized reading) through prompt-specified input and output schemas. Sub-panels depict each task family: full page detection (top-left), full page reading (top-right), local reading (bottom-right), and conditional detection (bottom-left). Sample page from [15]. As depicted in Figure 2, each GutenOCR model takes single document page image as input, optionally augmented with text query (for conditional detection) or bounding box (for localized reading), and produces either (i) plain transcripts (text, text2d) or (ii) structured outputs with lineor paragraph-level bounding boxes (lines, paragraphs), depending on the requested mode. All task variants share the same backbone, tokenizer, and prompting interface: the pipeline stages of traditional OCR system (reading, detection, grounding) are exposed as different inputoutput schemas of single model, rather than as separate components. 2.2 Task Families and Interfaces We expose four task families through unified, prompt-based interface: full-page reading, full-page detection, conditional detection, and localized reading. We use this four-way taxonomy throughout the paper for both training (Section 3.2) and evaluation (Section 4.1). Table 1 gives canonical inputoutput schemas and example prompts. For grounded outputs, we use JSON arrays of objects with keys text and bbox, where bbox is [x1, y1, x2, y2] in pixel coordinates. For pure detection tasks the output is JSON array of bbox arrays only. Full-page reading recognizes text over the entire page and can return either linear transcripts (text) or structured outputs (text2d, lines, paragraphs). Full-page detection localizes text or other regions of interest without recognition. Localized reading transcribes user-specified subregions, and conditional detection highlights all instances of query string. All outputs of type BOX are axis-aligned bounding boxes in pixel coordinates, represented as [x1, y1, x2, y2] with x1 < x2, y1 < y2 in the original image reference frame. We do not predict rotated boxes; rotated or skewed text is 5 Task Input Output Example prompt Reading Image text text2d lines paragraphs Detection Image BOX Conditional detection Image + text BOX Read all text in the image and return single plain-text transcription. Return layout-sensitive text2d representation of the image. Extract line-level JSON objects with text and [x1, y1, x2, y2]. Extract paragraph-level JSON objects with text and [x1, y1, x2, y2]. Detect all LINES in the image and return JSON array of [x1, y1, x2, y2] boxes. Detect all PARAGRAPHS in the image and return JSON array of boxes. Detect all math regions in the image and return their boxes as JSON. Given query string x, return boxes for all matching lines in the image. Localized reading Image + box Given bounding box [x1, y1, x2, y2], read only the text inside that region. Table 1: Canonical task families, inputs, outputs, and prompts. lines_text, paragraphs_text represented by the axis-aligned box of its minimal enclosing rectangle. More detailed conventions (e.g., clipping and degenerate boxes) are given in Appendix A.2. Impact of fear on patients delay regarding health care seeking behavior seeking behavior or treatment seeking delay or patient acceptance of health care and fear or anxiety . Two reviewers ( TD and JPvD ) independently assessed the studies that were identified during the screening based on information obtained from the title and the abstract of Results Using the first search strategy , 158 articles were found . Additional screening based on authors detected another 16 articles , which were not included in the MEDLINE or the Figure 3: Example of the layout-sensitive text2d representation. Long runs of spaces encode horizontal alignment (e.g., right-justified page number), while blank lines encode vertical gaps between sections and between the main text and the flow diagram. Page from [16]. Layout-sensitive text2d. The text2d format is whitespace-preserving, layout-aware linearization built from line-level boxes and transcripts. We order lines in 2D reading order and render them into notional character grid, mapping horizontal gaps to spaces and sufficiently large vertical gaps to blank lines. The resulting text2d string uses only spaces and newline characters to encode 2D layout while remaining plain-string API for downstream systems (see Figure 3). The exact construction procedure and its interaction with CER/WER metrics are described in Appendix A.3. Task-specific semantics. Conditional detection tasks take query string and an image as input and must return bounding boxes for all lines whose normalized text contains q; if the query does not appear, the expected output is an empty JSON array []. Localized reading tasks take bounding box and an image as input and require the model to 6 transcribe only the text inside that region. We train both line-level and paragraph-level variants, but for localized reading the API always returns plain-text string (no boxes). We reuse the labels lines/paragraphs only to indicate the expected granularity of the text, not the JSON structure. Further implementation details are deferred to Appendix A.4."
        },
        {
            "title": "3 Data & Training",
            "content": "3.1 Data curation We construct the training corpus by combining large-scale real-world document collections with synthetic data targeted at specific grounding behaviors. Our goal is to train grounded OCR front-endrather than general-purpose visionlanguage modelso we bias the mixture toward scanned documents and synthetic layouts that emphasize lineand paragraph-level geometry, math, and noisy business forms. Table 2 summarizes corpus sizes, OCR engines, and available annotation types; Appendix details preprocessing, splits, and sampling. Real sources. OCR-IDL (IDL) [17] covers noisy, heterogeneous business documents (IDs, invoices, claims) with stamps, handwriting, and capture artifacts. TabMe++ (T++) [18] offers similar content with different OCR backend and layout distribution. PubMed-OCR (PMD) [?] brings long, multi-column scholarly articles with equations, Greek/Latin symbols, figure captions, and references. Training across these sources exposes the model to (i) distinct page organizations (forms, tables, narrative prose), (ii) broad typographic and symbol distributions (small fonts, math, diacritics), and (iii) varied rasterization noise and reading-order conventions. IDL and TabMe++ provide lineand word-level transcripts but no paragraph structure, while PubMed-OCR includes paragraph segmentation in addition to lines and words. Synthetic sources. We complement real pages with synthetic data that explicitly target grounding for lines and math. Grounded LATEX (GL). We mine equation snippets from Wikimedia [19], normalize them to KaTeX-compatible LATEX, and render them into single-page PDFs at random positions, scales, and rotations. Each equation is associated with its tight bounding box in the rendered page, providing localized math supervision. SynthDoG grounding (SDG). Following prior work [5], we adapt SynthDoG to (i) emit line-level bounding boxes, (ii) sample text from FinePDFs [20], and (iii) use word-sensitive wrapping to avoid mid-word line breaks. These synthetic pages provide dense supervision for line detection and structured reading. Dataset Docs Pages OCR engine Paragraphs Lines Words LATEX OCR-IDL TabMe++ SynthDoG Grounding Grounded LATEX PubMed-OCR 4.6 26.0 44.8 122.5 1.2 3.0 1.5 Google Vision OCR Amazon Textract Azure OCR Synthetic Synthetic 209.5 Table 2: Dataset statistics and annotation granularity. checkmark indicates that supervision is available at the corresponding level. We reserve 10.5K pages across IDL, TabMe++, and PubMed-OCR for evaluation (Section 5.2) and train on the remainder. Held-out pages are selected uniformly at random over pages, stratified by source and sequence length. During each training stage, we additionally set aside 2,048 samples as validation set for early stopping; we halt training when the validation loss saturates. Appendix B.3 specifies the exact sampling and filtering criteria. 3.2 Training recipe Base models. We fine-tune the public instruction-tuned Qwen2.5-VL-3B and Qwen2.5-VL-7B checkpoints as backbones [11]. Both support long-context multimodal inputs and are comparable in scale to recent document-focused VLMs, allowing us to study OCR performance as function of model size under fixed training recipe. During training, we rasterize each PDF page to single 72 DPI image and feed it directly to the NaViT-style Qwen2.5-VL vision encoder without cropping or tiling (Appendix C.1). Each training example consists of single page plus text prompt, with the total sequence length capped according to the stage-specific limits in Table 3. Optimization and hardware. We fine-tune using AdamW [21] on 8H100 GPUs; see Appendix C.2 for full hyperparameters and implementation details. 7 Prompt variation and image references. All tasks are prompt-driven. For each (task, input, output) tuple we maintain bank of prompt templates and sample one uniformly at training time. We also randomize how the image is referenced by sampling from small set of determiners and nouns, e.g., this/that/the provided/the attached document/image/picture/file/scan. Thus prompts such as Where is {text} in this document?, Find occurrences of {text} in the above image and return bounding boxes as JSON., or Without returning the text, where are all the LINES in the attached scan? are all seen during training. We observed qualitatively more robust behavior under minor phrasing changes at inference time, while leaving the underlying semantics unchanged. Length-based curriculum. To stabilize optimization and handle complex layouts at long context, we adopt curriculum over total sequence length (image prompt plus full-page reading output). We begin with short sequences and gradually extend the maximum length while shifting the dataset mixture, as summarized in Table 3. Stage 1 2 3a 3b SDG GL Datasets IDL T++ PMD Length range Samples Steps Goal < 2k [2k, 8k] [2k, 8k] [8k, 16k] 2.5M 0.5M 0.3M 0.3M 20K 4K 2K 2K Core training Real specialization PMD exposure PMD specialization Table 3: Training stages and dataset mixture. SDG = SynthDoG Grounding, GL = Grounded LATEX, IDL = OCR-IDL, T++ = TabMe++, PMD = PubMed-OCR. Samples are counted by number of pages seen by the model. In Stage 1, we train on short sequences (< 2,048 tokens) drawn from mixture of synthetic and real sources: SynthDoG Grounding (SDG), Grounded LATEX (GL), OCR-IDL (IDL), and TabMe++ (T++). Stage 2 moves to medium-length sequences (2,0488,192 tokens) and uses only real pages (IDL, TabMe++), dropping synthetic pages and plain pagelevel transcripts to emphasize structured JSON outputs. Stage 3a keeps the same length range, adds PubMed-OCR (PMD), and introduces paragraph-based reading to refine paragraph structure and long-form page handling. Stage 3b then focuses on the longest contexts (8,19216,384 tokens) using only PMD, emphasizing multi-column scientific articles and very long transcripts. At each stage we sample uniformly over the task families in Section 2.2 that are supported by the available annotations (for example, lines/paragraphs from OCR-IDL and TabMe++, paragraph-level reading and text2d from PubMed-OCR, and detection-only tasks from SynthDoG). We refer to the resulting checkpoints as GutenOCR-3B,7B Stage 13b and evaluate all stages under the grounded OCR metrics of Section 4.1."
        },
        {
            "title": "4 Evaluation Setup",
            "content": "4.1 Metrics We evaluate grounded OCR along three axes: (i) Textcharacterand word-level errors on string outputs; (ii) Detection localization quality for boxes; and (iii) End-to-endjoint quality of boxtext pairs and page-level fidelity (including reading order and whitespace). Table 4 summarizes which metrics are reported for each task family. Full formal definitions, normalization, and edge-case handling are deferred to Appendix D.2. Task(s) Output type Reported metric(s) Reading (text/text2d), Localized Reading Detection, Conditional Detection Reading (lines/paragraphs) text boxes text+boxes CER , WER F1@0.5 , Recall@0.5 CERe2e , mCER@0.5 Table 4: Metrics per task family at glance. Arrows indicate whether lower () or higher () values are better. Characterand word-error rates. For any task that returns plain text, we report character error rate (CER) and word error rate (WER). Both are based on Levenshtein edit distance on lightly normalized strings (Unicode NFKC + whitespace cleanup). Intuitively, CER measures per-character mistakes, while WER is stricter and counts word-level insertions, deletions, and substitutions. Detection metrics. For detection-style outputs (plain or conditional), we report F1@0.5 and Recall@0.5. Predicted and ground-truth boxes are matched one-to-one using IoU 0.5; unmatched predictions count as false positives and unmatched ground-truth boxes as false negatives. The exact matching rule and treatment of corner cases (e.g., pages with no boxes) are specified in Appendix D.2. End-to-end structured reading. For structured reading tasks that produce text+boxes (lines/paragraphs), we report two text metrics that reuse the matched box set. mCER@0.5 isolates recognition quality given successful localization: we match predicted and ground-truth boxes at IoU 0.5, compute CER for each matched text pair, and average over the matched set. Unmatched ground-truth boxes do not affect mCER@0.5 directly (but do count as false negatives in detection recall), cleanly separating finding the right region from reading it correctly. CERe2e measures page-level fidelity. We linearize all predictions and references into page-level strings using deterministic reading order and whitespace-preserving layout (as in the text2d output format in Section 2.2), then compute CER on these concatenated transcripts. This metric is sensitive to missing or spurious lines, reading-order errors, and layout-sensitive whitespace. Together, these metrics probe (a) whether models find the right regions, (b) whether they read those regions correctly, and (c) whether the resulting page-level representation preserves reading order and layout-sensitive whitespace. For multi-task summaries (Section 5.2), we use CER (or CERe2e for structured reading) as the reading error ϵ, convert each into per-task score 1 ϵ, and average these with detection and conditional-detection F1 values to obtain composite grounded OCR score in [0, 1]. 4.2 Benchmarks We evaluate GutenOCR on both in-domain business/scientific pages and public document benchmarks. In-domain documents. We first evaluate on held-out pages drawn from the same sources used during training (OCR-IDL, TabMe++, PubMed-OCR). For each model we run the full suite of grounded OCR tasks from Section 2.2: full-page reading in multiple output formats (text, text2d, lines), localized reading given user-specified box, full-page detection, and conditional detection. Metrics follow Table 4: reading tasks are reported as CER/WER errors, while detection tasks are summarized with F1@0.5 and Recall@0.5; structured outputs use mCER@0.5 and CERe2e for end-to-end quality. Structured predictions are parsed through the shared JSON repair and normalization pipeline in Appendix D.1 before scoring. Fox. Fox is multi-page benchmark that probes models ability to focus anywhere on document via region-level, line-level, and color-guided OCR. We follow the authors task definitions and restrict Fox to the four English OCR-style tasks that align with our setting: (i) page OCR, (ii) region-level OCR, (iii) line-level OCR, and (iv) color-guided OCR. For page OCR we report both the reading-order-agnostic soft token F1 (Page F1) and the order-sensitive character error rate (Page CER); for region, line, and color OCR we report CER only  (Table 7)  . Additional benchmark details are given in Appendix E. OmniDocBench v1.5. OmniDocBench v1.5 is diverse PDF benchmark with dense annotations for layout regions, text spans, formulas, and tables across multiple page types. We treat it as an out-of-domain stress test and, restricting to English-only pages, evaluate three behaviors that match our grounded OCR setting: (i) text recognition via CER on cropped text spans, (ii) formula recognition via CDM [22] and CER on cropped equations, and (iii) text detection via recall of line-level text boxes. Because OmniDocBench text spans cover only subset of visible text, we report recall@0.5 (but not precision/F1) for detection under our detect all readable text ontology. Full dataset and metric details are deferred to Appendix E.2."
        },
        {
            "title": "5 Results",
            "content": "We evaluate GutenOCR on three settings: in-domain documents, Fox, and OmniDocBench v1.5. Our experiments are designed to answer three questions: Does GutenOCR improve both reading and grounding over Qwen2.5-VL backbones? How does it compare to OCR-specialized and layout-aware models on standard benchmarks? How do curriculum stages trade off global reading quality vs fine-grained grounding? We first describe the evaluated models (Section 5.1), then in-domain results (Section 5.2), Fox (Section 5.3), OmniDocBench (Section 5.4), and training-stage ablation (Section 5.5). At high level, GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.400.82). On Fox, it substantially improves regionand line-level OCR while sacrificing some page-level linearization and color-guided OCR. On OmniDocBench v1.5, it raises textdetection recall from 0.02 to 0.550.62 but slightly degrades formula recognition, especially at 3B. We unpack these trends below. 5.1 Models We consider three groups of models  (Table 5)  : (i) general-purpose Qwen-based VLMs used via prompting, (ii) OCRspecialized or layout-focused Qwen derivatives, and (iii) our GutenOCR-3B/7B front-ends. Full model descriptions are deferred to Appendix F. Model Params Backbone Type / role Qwen2.5-VL-3B/7B Qwen3-VL-8B Nanonets-OCR2-3B olmOCR 2 Infinity-Parser-7B Chandra OCR DeepSeek-OCR PaddleOCR-VL-0.9B 0.9B GutenOCR 3B / 7B 8B 3B 7B 7B 8B 3B 3B / 7B - - Qwen2.5-VL-3B Qwen2.5-VL-7B Qwen2.5-VL-7B Qwen3-VL-8B DeepSeek-3B-MoE PaddleOCR-VL-0.9B OCR VLM (no layout model) Qwen2.5-VL Prompt-only VLM baseline Prompt-only VLM baseline OCR-specialized baseline OCR-specialized baseline Layout parser baseline Layout-preserving OCR OCR + grounding baseline OCR + grounding (ours) Coord. boxes Table 5: Overview of models considered in our evaluation. Coord. boxes indicates whether the public interface can emit bounding boxes or coordinates without additional tooling. 5.2 In-Domain Model Reading Detection text text2d lines local full conditional Composite Qwen2.5-VL-3B 0.508 Nanonets OCR2-3B 0.369 0.224 GutenOCR-3B (3a) Qwen2.5-VL-7B Infinity-Parser-7B olmOCR 2 GutenOCR-7B (3a) Qwen3-VL-8B Chandra OCR DeepSeek-OCR PaddleOCR-VL 0.333 0.310 0.365 0.202 0.285 0. 0.702 0.399 0.584 0.518 0.293 0.522 0.586 0.530 0.280 0.491 0.593 0.809 0.539 0.379 0.645 0. 0.633 0.386 0.515 0.147 0.306 0.867 0.998 1.000 0.699 0.807 0.109 0.530 0.613 0.745 0.129 0.689 0. 0.940 0.936 0.135 0.001 0.798 0.111 0.098 0.039 0.787 0.049 0.012 0.010 0.001 0.121 0.001 0. 0.285 0.112 0.024 0.882 0.025 0.005 0.003 0.002 0.348 0.277 0.811 0.396 0.386 0.318 0.819 0.384 0. 0.094 0.188 Table 6: Average in-domain performance on OCR-IDL, TabMe++, and PubMed-OCR across six task families. Reading columns report errors (lower is better, ); detection columns report F1 scores (higher is better, ). The composite score averages task-wise values after mapping reading errors ϵ to scores 1 ϵ. Within each column, bold numbers indicate the best value across all models with reported score, while underlined numbers indicate the best value within each backbone cohort (e.g., 3B vs. 7B). Entries that are both bold and underlined are best both overall and within their cohort; blank cells denote metrics that were not evaluated for that model. We first assess in-domain performance on 10.5K held-out pages drawn from the same real-world sources used in training (OCR-IDL, TabMe++, PubMed-OCR). For each model (and, for GutenOCR, each training stage), we evaluate the six task families from Section 4.1: full-page reading in three output formats (text, text2d, lines), localized reading, full-page detection, and conditional detection. Table 6 reports CER for reading tasks and F1 for detection and conditional detection. To aggregate across tasks, we convert each reading error ϵ into score 1 ϵ and then average these scores with the detection and conditional-detection F1 values, yielding the composite column in Table 6. Under this composite, GutenOCR-3B and GutenOCR-7B (Stage 3a) more than double the scores of their Qwen2.5-VL backbones (from 0.348 10 to 0.811 and from 0.396 to 0.819, respectively), driven primarily by large gains in localized reading and detection. Unless otherwise noted, we use the Stage 3a checkpoints as the default GutenOCR-3B and GutenOCR-7B models in subsequent experiments. 5.3 Fox Recall from Section 4.2 that we evaluate on the four English Fox OCR tasks that align with our training objectives: page OCR, region-level OCR, line-level OCR, and color-guided OCR  (Table 7)  . Page-level performance is reported as both reading-order-agnostic token F1 (Page F1) and order-sensitive CER, while the region, line, and color subtasks use CER only (lower is better for CER; higher is better for Page F1). Model Page F1 Page Region Line Color Qwen2.5-VL-3B Nanonets OCR2-3B GutenOCR-3B Qwen2.5-VL-7B olmOCR-2 Infinity-Parser-7B GutenOCR-7B Qwen3-VL-8B Chandra OCR Fox PaddleOCR-VL DeepSeek-OCR 0.961 0.976 0. 0.984 0.983 0.983 0.973 0.971 0.954 0.975 0.906 0.051 0.026 0.138 0.025 0.018 0.024 0.164 0.040 0. 0.046 0.037 0.100 0.260 0.630 0.053 0.163 0.697 0.200 0.067 0.468 0.794 0.059 0.876 0.865 0.817 0.879 0. 0.701 0.878 0.709 0.211 0.820 0.958 0.116 0.980 0.979 0.768 0.475 0.940 0.109 0.558 0.147 0.963 0.200 0. 0.064 0.882 0.881 Table 7: Results on the English Fox OCR subtasks. Page F1 is reading-order agnostic soft token F1 for full-page OCR (higher is better); Page is full-page OCR CER; Region is region-level OCR given provided box; Line is line-level OCR given line pointer; and Color is OCR guided only by the color of box. All entries except Page F1 are character error rates (CER; lower is better). Within each column, bold numbers indicate the best value across all models, while underlined numbers indicate the best value within each cohort (3B, 7B, 8B, and Fox/PaddleOCR-VL/DeepSeek-OCR, as separated by horizontal rules). Entries that are both bold and underlined are best both overall and within their cohort. Models with are copied from the original Fox report; all others are recomputed under unified prompting setup. Fox mixes settings that are close to our training distribution (pageand region-style OCR) with more extrapolative ones (line pointers and color-guided grounding). Page-level results highlight an important distinction between what GutenOCR reads and how it linearizes that content. At 3B, GutenOCR-3B attains the highest Page F1 (0.988), indicating that it reads almost all reference tokens, but its page-level CER is 0.138, much worse than Qwen2.5-VL-3B (0.051) and Nanonets-OCR2-3B (0.026). similar pattern holds at 7B. The gap between Page F1 and Page CER reflects GutenOCRs training objective: it is optimized for layout-sensitive text2d and structured line/paragraph JSON, not for Foxs canonical page-to-Markdown order. Under Page F1 (content only), GutenOCR appears strong; Page CER (content + order) penalizes its layout-driven linearization. This is consistent with our training objectives. GutenOCR is optimized to produce layout-sensitive text2d outputs and structured line/paragraph JSON, not single fixed page-to-Markdown format. The model is trained to follow 2D layout cuescolumns, tables, headings, and whitespacewhen streaming text, which can conflict with the reading order assumed by Foxs reference markup. Under Page F1 (which ignores order), this yields very high agreement on content; under Page CER (which penalizes reordering and extra text), it appears as relatively weak page OCR despite strong underlying reading ability. On region-level OCR (Region), GutenOCR is best aligned with the intent of Fox. GutenOCR-3B attains region CER of 0.053, the best value in Table 7 and slightly better than the Fox model itself (0.059), while GutenOCR-7B reaches 0.067. Both represent large gains over the Qwen2.5-VL backbones (0.260 for 3B and 0.163 for 7B), consistent with our focus on grounded, region-aware OCR. For line-level OCR (Line), where Fox provides pointer to single line, the Fox model remains strongest overall (0.116 CER), but GutenOCR again substantially improves over its backbones. GutenOCR-3B reduces line CER from 0.817 to 0.240, and GutenOCR-7B from 0.701 to 0.211. Explicit line-pointer prompts do not appear in our training mixture, so this behavior likely reflects transfer from region-level detection and localized reading: once the model learns to find and read arbitrary regions, line pointers become natural special case. 11 GT: This section reports tax revenues from busine... Qwen: Revenue (CER: 0.987) Guten: This section reports tax revenues from busine... (CER: 0.004) (a) Region-level OCR Success GT: Administrative Apparatus and Functions Qwen: The best way to understand ISIS as formativ... (CER: 0.988) Guten: Administrative Apparatus and Functions (CER: 0.000) (b) Line Pointer Generalization GT: December 1988 BAS Meeting Qwen: December 1988 BAS Meeting (CER: 0.000) Guten: [[461, 71, 809, 86], [461, 88, 809, 102], [84... (CER: 0.997) (c) Color OCR Catastrophic Forgetting GT: which could be undertaken for the development... Qwen: National advisory committees on human rights... (CER: 0.574, F1: 0.597) Guten: which could be undertaken for the development... (CER: 0.687, F1: 0.983) (d) Content vs Structure Trade-off Figure 4: Qualitative comparison of Qwen2.5-VL-3B and GutenOCR-3B showing complementary strengths and weaknesses. (a) GutenOCR excels at region extraction (CER: 0.004 vs. 0.987). (b) Region training transfers to line pointer tasks (CER: 0.000 vs. 0.988). (c) GutenOCR suffers catastrophic forgetting on color tasks (CER: 0.997 vs. 0.000). (d) Trade-off: GutenOCR achieves higher content F1 (0.983 vs. 0.597) despite worse reading order (CER: 0.687 vs. 0.574) relative to Foxs reference expectations. 12 Color-guided OCR (Color) exposes clear limitation of our current recipe. The Fox model, Qwen2.5-VL-7B, and several OCR-specialized systems leverage color-coded boxes effectively (e.g., 0.064 CER for the Fox model and 0.109 CER for Qwen2.5-VL-7B), whereas both GutenOCR variants perform poorly (0.940 CER for 3B and 0.963 for 7B). Color-conditioned pointers are absent from our training data, and the OCR-focused fine-tuning overwrites the base models heuristic ability to interpret these cues, form of catastrophic forgetting on this narrow but challenging sub-task. Qualitatively, we often observe GutenOCR misreading the instruction as find the words red/green/blue rather than read the text in the red/green/blue box. GutenOCR explicitly trades some page-level CER on Fox for stronger grounded behavior. It preserves high Page F1 (indicating good reading) while reshaping reading order to follow its layout-sensitive text/text2d training, and it substantially strengthens regionand line-level OCR over the Qwen2.5-VL backbones. We qualitatively illustrate these trade-offs in Figure 4 for the 3B-parameter model. 5.4 OmniDocBench v1.5 Recall from Section 4.2 that OmniDocBench v1.5 [14] provides dense annotations for layout regions, text spans, formulas, and tables across nine PDF page types (e.g., books, slides, financial reports, textbooks, exam papers, magazines, academic papers, handwritten notes, and newspapers).1 The official benchmark is designed around end-toend markup generation with structure-aware metrics, whereas our production stack targets JSON-style text+box outputs. Rather than retrofitting GutenOCR to OmniDocBenchs canonical markup, we treat it as an out-of-domain stress test for three core capabilities: 1. text recognition (CER on cropped text spans), 2. formula recognition (CDM and CER on cropped equations), and 3. text detection (recall of line-level text boxes). Throughout, we restrict evaluation to pages whose language tag is purely English. 5.4.1 Text Recognition Model Granularity Background Page Sample White Single Multi Qwen2.5-VL-3B 0.018 0.028 GutenOCR-3B Qwen2.5-VL-7B 0.011 0.024 GutenOCR-7B 0.016 0.021 0.010 0. 0.015 0.021 0.010 0.020 0.029 0.011 0.014 0.012 0.041 0.075 0.026 0. Table 8: Component-level evaluation on the OmniDocBench OCR subset (English-only), grouped by text attributes under the edit-distance metric. All values are CER (lower is better). For text recognition we use the OmniDocBench OCR subset, which provides cropped text regions and attributes (language, background color, rotation). We keep only English spans and evaluate character error rate (CER; lower is better) under the same component-level edit-distance metric as the original paper. Following OmniDocBench, we report results at two granularitiesfull-page OCR and per-sample cropsand further group spans by background type (white, single-color, and multi-color). Table 8 summarizes results for the Qwen2.5-VL backbones and their GutenOCR variants. Across both model sizes, OCR-centric fine-tuning slightly increases CER relative to the base models (e.g., from 0.018 to 0.028 page-level CER for Qwen2.5-VL-3B, and from 0.011 to 0.024 for 7B). The degradation is most pronounced on multi-colored backgrounds, where CER more than doubles for both GutenOCR-3B and -7B, while the Qwen2.5-VL baselines remain comparatively robust. These results complement those from Fox (Section 5.3), where GutenOCR dramatically improves regionand line-level OCR but performs poorly on color-guided pointers. plausible explanation is domain mismatch: the OmniDocBench OCR subset contains many visually diverse pages (magazines, notes, newspapers, slides) that are under-represented in our training corpus, which focuses on business documents and structured forms, with academic pages added only in the final training phase. Color and background cues are also not modeled explicitly in our current recipe, so the fine-tuning does not inherit the base models background robustness on this benchmark. 1See the v1.01.5 changelog in the OmniDocBench supplement for details. Models Book Slides Textbook Exam Magazine Academic Notes Newspaper Qwen2.5-VL-3B 0.014 0.034 GutenOCR-3B 0.007 0.023 0.047 0.039 0.025 0.077 0.007 0.006 0.024 0.018 0.000 0. 0.018 0.008 Qwen2.5-VL-7B 0.011 0.034 GutenOCR-7B 0.016 0.057 Table 9: OmniDocBench OCR subset: CER by page type (English-only; lower is better). 0.167 0.000 0.009 0.014 0.008 0. 0.007 0.004 0.029 0.031 0.005 0.004 5.4.2 Text Detection For text detection, several design choices in OmniDocBench interact with our modeling goals. Text spans are defined at roughly line level and are provided only for subset of visible text: many visually text-like objects (e.g., table cell content, certain chemical formulas, decorative labels) are either annotated only as part of Table object or tagged as Equation Ignore and excluded from text evaluation. GutenOCR, by contrast, aims to detect all readable text, including text inside tables and equations, and does not attempt to reproduce OmniDocBenchs span-selection heuristics. As illustrated in Figure 5, our predictions (blue) cover substantially more text-like regions than the red annotated spans. Because OmniDocBenchs text annotations cover only subset of visible text (for example, many table cells and some equations are unlabeled), we cannot reliably treat non-overlapping predictions as false positives. predicted box that misses an annotated span may be hallucination or may simply hit unannotated but readable text. We therefore evaluate only recall at IoU 0.5, treating the annotated spans as trusted but incomplete ground truth and asking: when OmniDocBench says there is text here, does our detector cover it? Model All Book Slides Textbook Exam Magazine Academic Notes Newspaper Qwen2.5-VL-3B 0.022 0.620 GutenOCR-3B Qwen2.5-VL-7B 0.020 0.549 GutenOCR-7B 0.028 0.679 0.045 0.645 0.005 0.707 0.005 0. 0.055 0.758 0.012 0.639 0.038 0.471 0.015 0.503 0.041 0.797 0.015 0. 0.004 0.493 0.014 0.399 0.000 0.000 0.000 0.000 0.038 0.773 0.035 0. Table 10: OmniDocBench layout subset: text detection recall at IoU 0.5 (R@0.5) by page type (higher is better). Under this recall-only protocol  (Table 10)  , the Qwen2.5-VL backbones almost never produce boxes that align with OmniDocBenchs text spans (R@0.5 0.02 for both 3B and 7B). After GutenOCR fine-tuning, recall increases by more than an order of magnitude, reaching 0.620 for GutenOCR-3B and 0.549 for GutenOCR-7B. This is consistent with our design: the GutenOCR curriculum explicitly trains line-level detection head on diverse business documents, whereas the original Qwen2.5-VL models were never optimized to emit dense text boxes that match an external annotation standard. We emphasize that these numbers should not be interpreted as full detection quality on OmniDocBench: higher recall here does not imply stronger precision on their ontology, and our models deliberately predict many additional boxes (e.g., table cells, formulas) that the benchmark does not label as text. Rather, OmniDocBench serves as an external check that the detection behavior learned on our in-domain datasets transfers reasonably well to visually diverse, attribute-rich document collection. Accordingly, we view OmniDocBench text detection as recall-only stress test rather than full detector ranking: higher R@0.5 indicates better coverage of annotated spans, but does not reflect precision or behavior on unannotated text. 5.4.3 Formula Recognition CDM Model CER Overall Book Slides Textbook Exam Academic Qwen2.5-VL-3B GutenOCR-3B Qwen2.5-VL-7B GutenOCR-7B 0.189 0.294 0.216 0.221 0.936 0.866 0.935 0.927 0.951 0. 0.951 0.940 0.876 0.758 0.877 0.870 0.979 0.975 0.991 0.906 0.958 0. 0.955 0.951 0.914 0.864 0.914 0.915 Table 11: Formula recognition evaluation on the English subset of OmniDocBench. CER is lower-is-better; CDM is higher-is-better. 14 Figure 5: Qualitative OmniDocBench text-detection example. Red boxes denote annotated text spans; blue boxes denote GutenOCR predictions on the same page. Many blue boxes fall on readable text (e.g., table cells and decorative labels) that OmniDocBench does not label as text, so non-overlapping predictions cannot be reliably treated as false positives. This motivates our recall-only evaluation protocol in Section 5.4.2. OmniDocBench includes dedicated formula subset with LaTeX ground truth and evaluates models with the Character Detection Matching (CDM) metric [22], which measures symbol-level correctness under rendering, together with normalized edit distance. We follow this setup, again filtering to English formulas, and report CDM (higher is better) together with CER in Table 11. On formulas, GutenOCR exhibits clear negative transfer relative to its Qwen2.5-VL bases. For the 3B model, CDM drops from 0.936 to 0.866 and CER rises from 0.189 to 0.294; for the 7B model, the degradation is milder but consistent: CDM decreases from 0.935 to 0.927 and CER increases from 0.216 to 0.221. These results suggest that the current GutenOCR training mixture is effectively math-agnostic (likely exacerbated by dropping LaTeX grounding early on): it substantially improves document OCR (Sections 5.25.3) but erodes formula recognition, especially for the 3B model. For equation-heavy use cases, the best-performing checkpoints in our experiments are the un-fine-tuned Qwen2.5-VL backbones, not the GutenOCR variants. Making GutenOCR explicitly math-awarefor example, by incorporating formula-rich supervision similar to the OmniDocBench formula subsetis an important direction for future work. 15 5.5 Training Stage Ablation To study curriculum trade-offs, we compare GutenOCR checkpoints across training stages on the same held-out in-domain pages. Table 12 reports averages over OCR-IDL, TabMe++, and PubMed-OCR for each stage, and Figure 6 summarizes the corresponding trends in composite score and localized-reading error. Model Stage Reading Detection text text2d lines local full conditional Composite Qwen2.5-VL-3B GutenOCR-3B GutenOCR-3B GutenOCR-3B GutenOCR-3B Qwen2.5-VL-7B GutenOCR-7B GutenOCR-7B GutenOCR-7B GutenOCR-7B 0.508 0.218 0.223 0.224 0.253 0.333 0.198 0.210 0.202 0.242 0.584 0.276 0.282 0.293 0.345 0.522 0.255 0.263 0.280 0.323 0.379 0.183 0.185 0.185 0. 0.633 0.141 0.154 0.147 0.169 0.699 0.154 0.142 0.109 0.118 0.530 0.171 0.149 0.129 0.136 0.135 0.799 0.788 0.798 0.790 0.111 0.790 0.778 0.787 0.761 1 2 3a 3b 1 2 3a 3b 0.121 0.854 0.851 0.877 0.868 0.285 0.871 0.876 0.882 0.866 0.348 0.804 0.801 0.811 0.789 0.396 0.816 0.813 0.819 0.793 Table 12: Average in-domain performance on OCR-IDL, TabMe++, and PubMed-OCR across six task families, broken down by training stage. Reading columns report errors (lower is better, ); detection columns report F1 scores (higher is better, ). The composite score averages task-wise values after mapping reading errors ϵ to scores 1 ϵ. Within each column, bold numbers indicate the best value across all models and stages, while underlined numbers indicate the best value within each backbone (3B vs. 7B). Entries that are both bold and underlined are best both overall and within their backbone. Figure 6: Training-stage ablation summary for composite score (left, higher is better) and localized-reading error (right, lower is better) on OCR-IDL, TabMe++, and PubMed-OCR. The x-axis shows the base model and Stages 13; Stage 3a and Stage 3b share the same x-position and are distinguished by line style and marker. Relative to the Qwen2.5-VL backbones, GutenOCR yields large and consistent gains across all in-domain tasks. For the 3B model, the best stage (3a) roughly halves full-page reading errors (e.g., text CER 0.5080.224), cuts localizedreading error from 0.699 to 0.109, and lifts detection and conditional-detection F1 from 0.135/0.121 to 0.8/0.88, more than doubling the composite (0.3480.811). The 7B backbone shows the same pattern: GutenOCR-7B (3a) reduces reading errors by 4080%, raises detection F1 from 0.111 to 0.787 and conditional detection from 0.285 to 0.882, and increases the composite from 0.396 to 0.819. Thus most of the in-domain improvement comes from the OCR-centric training recipe, not from scaling alone. After fine-tuning, model-size differences are modest. At their best stages (3a), GutenOCR-7B improves the composite only slightly over GutenOCR-3B (0.819 vs. 0.811). The 7B model consistently achieves lower errors on global reading tasks (text, text2d, lines) and slightly higher overall composite, while the 3B model achieves lower localizedreading error at every stage and slightly higher unconditional detection F1. This trade-off suggests that GutenOCR-3B is competitive, more efficient choice when local reading and pure detection are prioritized, whereas GutenOCR-7B is preferable when global reading and conditional detection are the main objectives. 16 Within each backbone, the curriculum stages exhibit clear trends. Stage 1 already captures most of the gains in full-page reading and detection relative to the base VLMs. Stage 2 makes only minor adjustments and is largely dominated by Stage 1 or Stage 3a. Adding Stage 3a mainly sharpens localized reading and conditional detection for both backbones and yields the best composite scores, indicating that the additional curriculum primarily refines grounding rather than further reducing global reading error. By contrast, Stage 3b slightly degrades the composite relative to Stage 3a and does not produce any column-best metrics, consistent with mild over-specialization on the longest PubMed-OCR-style contexts. Overall, Table 12 and Figure 6 show that GutenOCR turns general-purpose VLMs into strong, grounded OCR systems across in-domain documents, with small but consistent advantages for the 7B backbone and clear stage-wise curriculum trade-offs. 5.6 Qualitative Examples To exemplify how GutenOCR can be used to further improve annotations with human-in-the-loop, we highlight an example on each real dataset where the 3B parameter model correctly found more lines than were originally annotated by the ground truth OCR engine in Figure 7."
        },
        {
            "title": "6 Discussion",
            "content": "Taken together, our results suggest that treating OCR as grounded front-endrather than as single page-to-markup conversionchanges both what should be optimized and how OCR outputs are consumed in practice. In particular, when OCR is used as the substrate for downstream retrieval, extraction, and document QA, the most costly failures are often not minor transcription noise but missing regions, unstable reading order, and weak provenance between model outputs and the pixels that support them. Grounded front-ends versus page-only OCR. Across our in-domain suite, Fox, and OmniDocBench, GutenOCR behaves differently from both its Qwen2.5-VL backbones and contemporary OCR-specialized VLMs. Rather than optimizing single notion of page-level fidelity, our recipe emphasizes grounded front-end: high recall of text regions, stable lineand paragraph-level outputs, and predictable behavior under localized and conditional reading. This shows up quantitatively in large gains on localized reading and detection metrics (Tables 6 & 7), and qualitatively in cases where GutenOCR recovers legible content that is absent from reference annotations. From systems perspective, these properties are often as important as small differences in page-level CER: downstream components can tolerate some transcription noise, but they are brittle to missing content, scrambled layout, or non-reproducible pointing behavior that prevents targeted re-reading. Human-in-the-loop verification and control. Viewed through the lens of human review, GutenOCR is designed to reduce the effort required to decide whether page is usable for downstream automation. Lineand paragraph-level detection, together with multi-format transcripts (plain text, layout-preserving text, and line-based JSON), let reviewer quickly answer concrete questions: (i) Did we cover all the text? (recall of boxes over the page), (ii) Can we reliably re-read region of interest? (localized reading that is stable across prompts), and (iii) Can we attribute extracted values to evidence? (boxes that pinpoint the supporting span). This asymmetric cost profile motivates our bias toward recall and fine-grained grounding: false positives are typically easy to dismiss visually, whereas false negatives force re-scanning the page and make exceptions difficult to triage. In practice, these properties support review tools, exception workflows, and active-learning loops where the goal is not only accurate text but quickly verifiable text and actionable provenance. By contrast, page-to-Markdown systems often entangle rendering choices with OCR itself, making isolated errors or ambiguous attributions harder to localize without repeatedly comparing long transcript to the original page. Interfaces, semantic surface, and comparison to prior work. large portion of recent work binds OCR to single, task-specific output representation: page-to-Markdown/HTML conversion, fixed layout-to-JSON schemas, or document parsing pipelines that produce one canonical document object for downstream use. These approaches can be strong end-to-end baselines when the downstream task matches the target representation, but they often provide limited support for (i) alternative reading orders, (ii) region-restricted re-reading, and (iii) fine-grained attribution of answers to the specific spans that support them. In particular, document conversion systems (e.g., those centered on structured markup such as DocTags-like formats [23]) provide exportable parses that are valuable for rendering and indexing, but they do not by themselves guarantee evidence-first behaviorthat is, stable contract where extracted fields and QA answers can be traced to precise tokenor span-level locations on the page. 17 (a) IDL (b) TabMe++ (c) PubMed-OCR (d) PubMed-OCR (zoom-in) Figure 7: Qualitative examples of GutenOCR-3Bs lines predictions on test samples. Blue boxes indicate GutenOCRs prediction versus the ground truth in red. Regions of overlap are purple. Areas of disagreement are immediately obvious (and actionable). 18 GutenOCR takes different position in this design space. It offers small set of stable, grounded interfacesfull-page reading in multiple formats, localized reading, full-page detection, and conditional detectionall backed by single checkpoint and coherent prompting scheme. These interfaces act as thin, API-like layer between raw pixels and downstream logic: applications consume structured outputs with text and boxes and are free to render or transform them into whatever schema they require (HTML, Markdown, XML, database records) without changing the underlying OCR model. We view this as exposing broader semantic surfacea set of stable OCR primitives that downstream systems can reliably composethan page-only transcripts, while remaining predictable enough for enterprise integration and human verification. Limitations and open challenges. Our experiments also reveal the limits of the current recipe. First, specialization for business and scientific documents improves reading and grounding on our in-domain suite and on Foxs region and line tasks, but can reduce robustness on visually diverse layouts and color-guided cues, and degrades formula recognition on formula-heavy OmniDocBench pages. Second, our outputs are still primarily textand line-centric: we do not yet explicitly model table structure, math layout, cross-page links, or higher discourse units, even though these are critical for many real workflows (e.g., attributing financial value to the correct row/column context). Addressing these weaknesses will likely require both broader training mixture (e.g., mathand table-rich supervision, colorand background-aware tasks) and extended interfaces that surface this structure without sacrificing grounding and controllability. Finally, because our goal is practical front-end, future work should also characterize efficiency trade-offs (latency/throughput) alongside accuracy and grounding. Toward document holograms. The preceding points suggest target beyond OCR alone. We use the term document hologram to describe persistent, multi-view representation of document that couples (i) fine-grained grounding between content and pixels (down to spans or tokens), with (ii) learned semantic typing and relations over that grounded content, and (iii) queryable views that support region-restricted inspection and controllable reading orders. Crucially, document hologram is not merely an export format or parse tree: it is the internal state that enables evidence-bearing responses, where extracted fields and QA answers come with set of supporting locations (often many-to-many) that user can directly verifyfor example, pointing to the specific cells in large table that justify suspected fraudulent amount. In this sense, GutenOCR should be viewed as front-end module for document holograms rather than complete solution. Our results show that general-purpose VLM can be specialized into grounded OCR front-end that substantially improves structured reading and detection while preserving localized and pointer-based behaviors useful for evidence retrieval. At the same time, the observed failure modesmath and color sensitivity, table-heavy layouts, and cross-page reasoninghighlight what is still missing from higher hologram layers that model structure and semantics on top of grounded text. We leave the design of these higher layers, and the evaluation protocols that directly test evidential QA and fine-grained provenance, to future work."
        },
        {
            "title": "7 Related Work",
            "content": "7.1 Classical and Modern OCR Pipelines Classical OCR systems decompose document understanding into sequence of stages: image normalization and binarization, text region detection, characteror word-level recognition, and downstream post-processing. Early systems such as Tesseract [1] and successors in industrial settings (e.g., PaddleOCR [2] and cloud APIs such as Amazon Textract [3] and Google Vision OCR [4]) largely share this modular structure, even as individual components have shifted from rule-based processing to convolutional detectors and sequence recognizers. Production deployments typically add task-specific logic for page segmentation, tables, forms, and key-value extraction. Despite their maturity, pipeline OCR systems exhibit well-known limitations on complex documents. First, layout handling can be brittle: multi-column articles, nested tables, stamps, marginalia, and rotated inserts can break readingorder heuristics, scrambling text streams and propagating errors downstream [24, 25]. Second, the link between text and visual support is often indirect: while many systems expose wordor line-level geometry, programmatic control over reading behavior (alternative reading orders, region-restricted rereading, whitespace-sensitive linearizations, or query-conditioned extraction) typically requires substantial glue logic on top of OCR outputs [26]. Finally, hybrid document pipelines frequently combine PDF-native extraction (when embedded text is available) with OCR fallback for scanned or rasterized content; recent work on PDF-centric QA highlights the value of exploiting PDF structure and metadata when available [27]. In this work, we adopt the pipeline formulation as our starting point: we aim to preserve the strengths of modular OCR explicit detection, recognition, and structured outputswhile replacing hand-crafted components with unified 19 visionlanguage model. Our GutenOCR system is trained to perform full-page reading, detection, conditional detection, and localized reading within single VLM interface, behaving like traditional OCR pipeline from the outside while enabling fine-grained grounding, controllable reading order, and task-specific document representations. 7.2 Document Conversion Toolchains and Markup Formats complementary line of work targets document conversion: converting PDFs and page images into richly structured, LLM-friendly intermediate representations such as Markdown, HTML, or JSON. Docling provides an open-source conversion toolkit that parses multiple document formats into unified structured representation [28]. Recent compact conversion models such as SmolDocling generate DocTags, an XML-like tagged format designed to preserve document structure and element locations while remaining tokenizer-friendly [23]. IBMs Granite-Docling-258M similarly targets efficient end-to-end conversion into structured machine-readable formats with tight integration into the Docling toolchain [29]. Other open-source tools (e.g., MinerU) focus on practical PDF-to-Markdown/JSON conversion for scientific and technical documents [30, 31]. These systems provide strong end-to-end baselines when downstream application can consume their target schema directly. Our goal is different: rather than committing to single canonical export format, we emphasize small set of OCR primitives as stable contract (detection, reading, localized reading, and conditional detection) that downstream systems can compose into diverse workflows and representations. 7.3 VisionLanguage Models for Documents Recent document understanding systems increasingly rely on VLMs that operate on page images. One family consists of OCR-based models such as LayoutLM and successors [32, 33, 34], DocFormer [35], DiT [36], LiLT [37], and UDOP [38], which consume OCR tokens and bounding boxes. More recently, layout-aware LLM extensions such as DocLLM [39] and DocLayLLM [40] incorporate spatial structure directly into generative LMs, but still assume OCR has produced accurate text and geometry. These approaches achieve strong results on downstream benchmarks (e.g., FUNSD [41], CORD [42], SROIE [43], DocVQA [44]), but are limited by upstream OCR errors and provide only indirect control over region-restricted reading or query-conditioned localization. second family of OCR-free document VLMs operates directly on pixels and emits text or structured markup. Donut [5] is prominent example; related work on screenshot/document parsing (e.g., Pix2Struct [45]) and scientific document conversion (e.g., Nougat [46]) likewise targets page-to-markup generation. Several newer OCR-specialized parsers (e.g., dots.ocr [47], DocPedia [48]) emphasize faithful conversion and reading order, often producing Markdown/HTML/JSON (sometimes with coordinates) and evaluating primarily via text/markup edit distance. More recently, several OCR-specialized multimodal models fine-tune VLM backbones for high-fidelity document transcription and parsing. General OCR Theory (GOT) frames OCR as unified end-to-end OCR-2.0 model that supports whole-page transcription as well as interactive, region-guided recognition (e.g., via coordinates or colors) and formatted outputs [6]. Other systems use reinforcement learning or verifiable reward signals to improve structural fidelity and reading order in page-to-text/markup conversion, including olmOCR [7] and Infinity-Parser [8]. These models provide strong end-to-end conversion baselines, but they typically expose primary canonical output representation (plain text or markup) rather than small set of schema-stable OCR primitives spanning pure detection, localized re-reading, and string-conditioned search. In parallel, general-purpose vision foundation models (e.g., Florence-2 [49], Granite Vision [50], Qwen2.5-VL/Qwen3VL [11, 12], InternVL variants [51, 52, 53]) can perform document QA, captioning, and OCR-like behaviors when suitably prompted. However, their interfaces and training objectives are typically optimized for broad multimodal instruction following rather than well-specified OCR API contract, and coordinate outputs are often exposed through ad-hoc formats. Our work takes different stance: we explicitly retrain general-purpose VLM backbone as grounded OCR front-end with task schemas that mirror classical OCR pipeline (full-page reading, detection, localized reading, and conditional detection). The primary product is set of structured text+box outputs and prompt-selectable behaviors, rather than QA answers or single canonical document linearization. 7.4 Grounded OCR and Localization Before the rise of VLMs, grounded OCR was studied through text spotting systems that jointly detect and recognize text (e.g., EAST [54], CTPN [55], FOTS [56], Mask TextSpotter [57]). These models provide explicit boxtext pairs, but are often tuned for scene text and do not naturally support document abstractions or language-driven queries. More recently, work on reading order and layout-aware extraction (e.g., LayoutReader [24], Reading Order Matters [25], éclair [26]) assumes OCR tokens exist. Meanwhile, phrase-grounding models (e.g., MDETR [58], GLIP [59, 60], Grounding DINO [61, 62]) align language with regions; document-focused tasks appear in benchmarks such as Fox [13]. Foundation models such as Florence-2 expose OCR and region-level prompting as part of unified multi-task interface [49]. Several OCR-focused VLM parsers annotate outputs with coordinates or support prompted localization (e.g., dots.ocr [47], SmolDocling/DocTags [23]). However, these interfaces are typically optimized for end-to-end conversion rather than single checkpoint that unifies pure detection, localized rereading, and string-conditioned search under stable, schema-driven API with standardized conditional-localization metrics. GutenOCR can be viewed as generalization of text spotting and phrase grounding to unified, VLM-based OCR interface: one checkpoint supports (i) full-page detection, (ii) full-page structured reading, (iii) localized reading given user box, and (iv) conditional detection given query string, all expressed through prompt-selectable schemas. 7.5 Benchmarks and Evaluation for OCR and Document Understanding Classical OCR evaluation has been driven by ICDAR-style benchmarks for scene text and document text recognition, which typically separate detection and recognition. Detection is measured with IoU-based precision, recall, and F1 at one or more thresholds; recognition quality is reported as character error rate (CER), word accuracy, or normalized edit distance on isolated words, lines, or cropped regions. End-to-end text spotting benchmarks often combine localization and recognition through matching-based protocols (e.g., end-to-end F-score/Hmean), and more fine-grained schemes such as CLEval score character-level correctness while handling split/merge cases in detection [63]. Document understanding benchmarks broaden the focus from text recognition to classification and extraction. Datasets such as RVL-CDIP [64], FUNSD [41], CORD [42], SROIE [43], and DocVQA [44] evaluate document image classification, form understanding, key-value extraction, and question answering, typically assuming access to an external OCR system. In these settings, OCR quality is hidden variable: models are judged on downstream task accuracy, and errors stemming from misrecognition, mis-localization, or poor reading order are entangled. More recent benchmarks have started to evaluate document parsing and grounding more directly. Fox [13] introduces multi-page, fine-grained tasks such as region-level OCR, line-level OCR, and color-guided OCR, using CER on cropped regions as the primary metric. OmniDocBench v1.5 [14] provides dense annotations for layout regions, text spans, formulas, and tables across diverse set of page types; it reports component-level edit distance for text, CDM and edit distance for formulas [22], and structure-aware metrics such as TEDS for tables [65]. These benchmarks substantially raise the bar on the diversity and richness of evaluation. At the same time, they are primarily organized around either component-wise parsing fidelity (text/formula/table/reading-order modules) or end-to-end markup correctness under particular canonicalization [14]. They do not directly evaluate interactive OCR primitives such as region-restricted rereading or string-conditioned conditional detection under unified, schema-driven contract. Recent OCR benchmarks for large multimodal models also evaluate OCR across multiple task tracks (e.g., multilingual reading, document parsing, and key information extraction), highlighting common failure modes such as grounding errors and repetition hallucinations [66, 67]. Our evaluation protocol builds on these traditions but targets grounded OCR as first-class objective. Following Section 4.1, we measure: (i) text accuracy via balanced CER and WER on whole-page transcripts and localized crops; (ii) detection quality via F1@0.5 and Recall@0.5 over axis-aligned boxes; and (iii) end-to-end quality via CERe2e and mCER@0.5 on aligned boxtext pairs, which together disentangle localization errors (missing, spurious, or mis-ordered boxes) from recognition errors within correctly localized regions. Unlike prior work that reports either text-only metrics or IoU-only metrics, this metric set evaluates (a) whether models find the right regions, (b) whether they read those regions correctly, and (c) whether the resulting page-level representation preserves reading order and layout-sensitive whitespace. In this sense, our metrics are an explicit evolution of classical OCR and text-spotting criteria toward the grounded, layout-aware document representations that document-hologram systems require."
        },
        {
            "title": "8 Conclusion",
            "content": "GutenOCR recasts OCR as grounded front-end problem, training single visionlanguage checkpoint to support full-page reading, detection, localized reading, and conditional detection through unified prompt interface. Across in-domain business and scientific documents, Fox, and OmniDocBench v1.5, it substantially improves structured reading and detection over Qwen2.5-VL backbones and several OCR-specialized baselines, while revealing clear failure modes in formula-heavy and color-guided settings. These results suggest that explicit grounding and controllable 21 reading are as important as page-level transcripts for downstream document systems and point toward richer document hologram representations that we leave for future work."
        },
        {
            "title": "References",
            "content": "[1] Ray Smith. An overview of the tesseract ocr engine. In ICDAR 07: Proceedings of the Ninth International Conference on Document Analysis and Recognition, pages 629633, Washington, DC, USA, 2007. IEEE Computer Society. [2] Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, Yue Zhang, Wenyu Lv, Kui Huang, Yichao Zhang, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, and Yanjun Ma. Paddleocr 3.0 technical report, 2025. [3] Edouard Belval, Thomas Delteil, Martin Schade, and Srividhya Radhakrishna. Amazon textractor. https: //aws-samples.github.io/amazon-textract-textractor/, 2025. [4] Google Cloud. Google cloud vision ocr and document ai ocr. https://cloud.google.com/use-cases/ocr, 2025. Optical Character Recognition (OCR) services provided by Google Cloud, including Cloud Vision API and Document AI. [5] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision (ECCV), 2022. [6] Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, Chunrui Han, and Xiangyu Zhang. General ocr theory: Towards ocr-2.0 via unified end-to-end model, 2024. [7] Jake Poznanski, Luca Soldaini, and Kyle Lo. olmocr 2: Unit test rewards for document ocr, 2025. [8] Baode Wang, Biao Wu, Weizhen Li, Meng Fang, Zuming Huang, Jun Huang, Haozhe Wang, Yanjie Liang, Ling Chen, Wei Chu, and Yuan Qi. Infinity parser: Layout aware reinforcement learning for scanned document parsing, 2025. [9] Haoran Wei, Yaofeng Sun, and Yukun Li. Deepseek-ocr: Contexts optical compression, 2025. [10] Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, Yue Zhang, Yubo Zhang, Handong Zheng, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, and Yanjun Ma. Paddleocr-vl: Boosting multilingual document parsing via 0.9b ultra-compact vision-language model, 2025. [11] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. [12] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report, 2025. [13] Chenglong Liu, Haoran Wei, Jinyue Chen, Lingyu Kong, Zheng Ge, Zining Zhu, Liang Zhao, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Focus Anywhere for Fine-grained Multi-page Document Understanding, May 2024. [14] Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, Jin Shi, Fan Wu, Pei Chu, Minghao Liu, Zhenxiang Li, Chao Xu, Bo Zhang, Botian Shi, Zhongying Tu, and Conghui He. Omnidocbench: Benchmarking diverse pdf document parsing with comprehensive annotations, 2025. [15] Y. T. Zhou, L. L. Chew, S. C. Lin, and B. C. Low. The BNIP-2 and Cdc42GAP homology (BCH) domain of p50RhoGAP/Cdc42GAP sequesters RhoA from inactivation by the adjacent GTPase-activating protein domain. Molecular Biology of the Cell, 21(18):32323246, September 2010. Epub 2010 Jul 21. 22 [16] T. Dubayova, J. P. van Dijk, I. Nagyova, J. Rosenberger, E. Havlikova, Z. Gdovinova, B. Middel, and J. W. Groothoff. The impact of the intensity of fear on patients delay regarding health care seeking behavior: systematic review. International Journal of Public Health, 55(5):459468, October 2010. Epub 2010 May 14. [17] Ali Furkan Biten, Rubèn Tito, Lluis Gomez, Ernest Valveny, and Dimosthenis Karatzas. OCR-IDL: OCR Annotations for Industry Document Library Dataset, February 2022. [18] Hunter Heidenreich, Ratish Dalvi, Rohith Mukku, Nikhil Verma, and Neven Piˇculjan. Large Language Models for Page Stream Segmentation, August 2024. [19] Moritz Schubotz. Complete list of mathematical expressions in all wikimedia projects, including wikipedia. https://doi.org/10.5281/zenodo.15162182, April 2025. [20] Hynek Kydlíˇcek, Guilherme Penedo, and Leandro von Werra. Finepdfs. https://huggingface.co/datasets/ HuggingFaceFW/finepdfs, 2025. [21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. [22] Bin Wang, Fan Wu, Linke Ouyang, Zhuangcheng Gu, Rui Zhang, Renqiu Xia, Bo Zhang, and Conghui He. Image over text: Transforming formula recognition evaluation with character detection matching, 2025. [23] Ahmed Nassar, Andres Marafioti, Matteo Omenetti, Maksym Lysak, Nikolaos Livathinos, Christoph Auer, Lucas Morin, Rafael Teixeira de Lima, Yusik Kim, A. Said Gurbuz, Michele Dolfi, Miquel Farré, and Peter W. J. Staar. Smoldocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion, 2025. [24] Zilong Wang, Yiheng Xu, Lei Cui, Jingbo Shang, and Furu Wei. LayoutReader: Pre-training of Text and Layout for Reading Order Detection. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 47354744, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. [25] Chong Zhang, Ya Guo, Yi Tu, Huan Chen, Jinyang Tang, Huijia Zhu, Qi Zhang, and Tao Gui. Reading Order Matters: Information Extraction from Visually-rich Documents by Token Path Prediction. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1371613730, Singapore, December 2023. Association for Computational Linguistics. [26] Ilia Karmanov, Amala Sanjay Deshmukh, Lukas Voegtle, Philipp Fischer, Kateryna Chumachenko, Timo Roman, Jarno Seppänen, Jupinder Parmar, Joseph Jennings, Andrew Tao, and Karan Sapra. éclair Extracting Content and Layout with Integrated Reading Order for Documents, February 2025. [27] Jon Saad-Falcon, Joe Barrow, Alexa Siu, Ani Nenkova, David Seunghyun Yoon, Ryan A. Rossi, and Franck Dernoncourt. Pdftriage: Question answering over long, structured documents, 2023. [28] Nikos Livathinos, Christoph Auer, Maxim Lysak, Ahmed Nassar, Michele Dolfi, Panos Vagenas, Cesar Berrospi Ramis, Matteo Omenetti, Kasper Dinkla, Yusik Kim, Shubham Gupta, Rafael Teixeira de Lima, Valery Weber, Lucas Morin, Ingmar Meijer, Viktor Kuropiatnyk, and Peter Staar. Docling: An Efficient Open-Source Toolkit for AI-driven Document Conversion. In AAAI Conference on Artificial Intelligence, February 2025. [29] IBM Granite team. ibm-granite/granite-docling-258M. https://huggingface.co/ibm-granite/ granite-docling-258M, 2025. [30] Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, Bo Zhang, Liqun Wei, Zhihao Sui, Wei Li, Botian Shi, Yu Qiao, Dahua Lin, and Conghui He. Mineru: An open-source solution for precise document content extraction, 2024. [31] Junbo Niu, Zheng Liu, Zhuangcheng Gu, Bin Wang, Linke Ouyang, Zhiyuan Zhao, Tao Chu, Tianyao He, Fan Wu, Qintong Zhang, Zhenjiang Jin, Guang Liang, Rui Zhang, Wenzheng Zhang, Yuan Qu, Zhifei Ren, Yuefeng Sun, Yuanhong Zheng, Dongsheng Ma, Zirui Tang, Boyu Niu, Ziyang Miao, Hejun Dong, Siyi Qian, Junyuan Zhang, Jingzhou Chen, Fangdong Wang, Xiaomeng Zhao, Liqun Wei, Wei Li, Shasha Wang, Ruiliang Xu, Yuanyuan Cao, Lu Chen, Qianqian Wu, Huaiyu Gu, Lindong Lu, Keming Wang, Dechen Lin, Guanlin Shen, Xuanhe Zhou, Linfeng Zhang, Yuhang Zang, Xiaoyi Dong, Jiaqi Wang, Bo Zhang, Lei Bai, Pei Chu, Weijia Li, Jiang Wu, Lijun Wu, Zhenxiang Li, Guangyu Wang, Zhongying Tu, Chao Xu, Kai Chen, Yu Qiao, Bowen Zhou, Dahua Lin, Wentao Zhang, and Conghui He. Mineru2.5: decoupled vision-language model for efficient high-resolution document parsing, 2025. [32] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 20, page 11921200. ACM, August 2020. 23 [33] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. LayoutLMv2: Multi-modal pre-training for visually-rich document understanding. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 25792591, Online, August 2021. Association for Computational Linguistics. [34] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for document ai with unified text and image masking. In Proceedings of the 30th ACM International Conference on Multimedia, MM 22, page 40834091, New York, NY, USA, 2022. Association for Computing Machinery. [35] Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R. Manmatha. Docformer: End-to-end transformer for document understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 9931003, October 2021. [36] Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, and Furu Wei. Dit: Self-supervised pre-training for document image transformer. In Proceedings of the 30th ACM International Conference on Multimedia, MM 22, page 35303539, New York, NY, USA, 2022. Association for Computing Machinery. [37] Jiapeng Wang, Lianwen Jin, and Kai Ding. LiLT: simple yet effective language-independent layout transformer for structured document understanding. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 77477757, Dublin, Ireland, May 2022. Association for Computational Linguistics. [38] Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, and Mohit Bansal. Unifying vision, text, and layout for universal document processing. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1925419264, 2023. [39] Dongsheng Wang, Natraj Raman, Mathieu Sibue, Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, and Xiaomo Liu. DocLLM: layout-aware generative language model for multimodal document understanding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 85298548, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [40] Wenhui Liao, Jiapeng Wang, Hongliang Li, Chengyu Wang, Jun Huang, and Lianwen Jin. DocLayLLM: An Efficient Multi-modal Extension of Large Language Models for Text-rich Document Understanding. [41] Guillaume Jaume, Hazim Kemal Ekenel, and Jean-Philippe Thiran. Funsd: dataset for form understanding in noisy scanned documents. In 2019 International Conference on Document Analysis and Recognition Workshops (ICDARW), pages 16. IEEE, 2019. [42] Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, and Hwalsuk Lee. {CORD}: consolidated receipt dataset for post-{ocr} parsing. In Workshop on Document Intelligence at NeurIPS 2019, 2019. [43] Minghui Li, Yuxin Deng, Yujie Guan, Wei Xu, Peng Zhang, Jian Zhang, Xin Zhu, Yuzhen He, Honghui Li, Yong Jiang, et al. Icdar 2019 competition on scanned receipt ocr and information extraction. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 15941599. IEEE, 2019. [44] Minesh Mathew, Dimosthenis Karatzas, R. Manmatha, and C. V. Jawahar. Docvqa: dataset for vqa on document images. 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 21992208, 2020. [45] Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding, 2023. [46] Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical understanding for academic documents, 2023. [47] Yumeng Li, Guang Yang, Hao Liu, Bowen Wang, and Colin Zhang. dots.ocr: Multilingual document layout parsing in single vision-language model, 2025. [48] Hao Feng, Qi Liu, Hao Liu, Jingqun Tang, Wengang Zhou, Houqiang Li, and Can Huang. DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding, November 2024. [49] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing unified representation for variety of vision tasks, 2023. 24 [50] Granite Vision Team, Leonid Karlinsky, Assaf Arbelle, Abraham Daniels, Ahmed Nassar, Amit Alfassi, Bo Wu, Eli Schwartz, Dhiraj Joshi, Jovana Kondic, Nimrod Shabtay, Pengyuan Li, Roei Herzig, Shafiq Abedin, Shaked Perek, Sivan Harary, Udi Barzelay, Adi Raz Goldfarb, Aude Oliva, Ben Wieles, Bishwaranjan Bhattacharjee, Brandon Huang, Christoph Auer, Dan Gutfreund, David Beymer, David Wood, Hilde Kuehne, Jacob Hansen, Joseph Shtok, Ken Wong, Luis Angel Bathen, Mayank Mishra, Maksym Lysak, Michele Dolfi, Mikhail Yurochkin, Nikolaos Livathinos, Nimrod Harel, Ophir Azulai, Oshri Naparstek, Rafael Teixeira de Lima, Rameswar Panda, Sivan Doveh, Shubham Gupta, Subhro Das, Syed Zawad, Yusik Kim, Zexue He, Alexander Brooks, Gabe Goodhart, Anita Govindjee, Derek Leist, Ibrahim Ibrahim, Aya Soffer, David Cox, Kate Soule, Luis Lastras, Nirmit Desai, Shila Ofek-koifman, Sriram Raghavan, Tanveer Syeda-Mahmood, Peter Staar, Tal Drory, and Rogerio Feris. Granite vision: lightweight, open-source multimodal model for enterprise intelligence, 2025. [51] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. [52] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [53] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. [54] Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang Zhou, Weiran He, and Jiajun Liang. East: An efficient In Proceedings of the IEEE Conference on Computer Vision and Pattern and accurate scene text detector. Recognition (CVPR), July 2017. [55] Zhi Tian, Weilin Huang, Tong He, Pan He, and Yu Qiao. Detecting text in natural image with connectionist text proposal network. In European conference on computer vision, pages 5672. Springer, 2016. [56] Xuebo Liu, Ding Liang, Shi Yan, Dagui Chen, Yu Qiao, and Junjie Yan. Fots: Fast oriented text spotting with unified network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. [57] Pengyuan Lyu, Minghui Liao, Cong Yao, Wenhao Wu, and Xiang Bai. Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018. [58] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr modulated detection for end-to-end multi-modal understanding, 2021. [59] Liunian Harold Li*, Pengchuan Zhang*, Haotian Zhang*, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training. In CVPR, 2022. [60] Haotian* Zhang, Pengchuan* Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding. arXiv preprint arXiv:2206.05836, 2022. [61] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. Grounding dino: Marrying dino with grounded pre-training for open-set object detection, 2024. [62] Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, Yuda Xiong, Hao Zhang, Feng Li, Peijun Tang, Kent Yu, and Lei Zhang. Grounding dino 1.5: Advance the \"edge\" of open-set object detection, 2024. [63] Youngmin Baek, Daehyun Nam, Sungrae Park, Junyeop Lee, Seung Shin, Jeonghun Baek, Chae Young Lee, and Hwalsuk Lee. Cleval: Character-level evaluation for text detection and recognition tasks. arXiv preprint arXiv:2006.06244, 2020. [64] Adam Harley, Alex Ufkes, and Konstantinos Derpanis. Evaluation of deep convolutional nets for document image classification and retrieval. In International Conference on Document Analysis and Recognition (ICDAR), 2015. [65] Xu Zhong, Elaheh ShafieiBavani, and Antonio Jimeno Yepes. Image-based table recognition: data, model, and evaluation, 2020. 25 [66] Zhibo Yang, Jun Tang, Zhaohai Li, Pengfei Wang, Jianqiang Wan, Humen Zhong, Xuejing Liu, Mingkun Yang, Peng Wang, Shuai Bai, LianWen Jin, and Junyang Lin. Cc-ocr: comprehensive and challenging ocr benchmark for evaluating large multimodal models in literacy, 2024. [67] Ling Fu, Zhebin Kuang, Jiajun Song, Mingxin Huang, Biao Yang, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, Hao Lu, Zhang Li, Guozhi Tang, Bin Shan, Chunhui Lin, Qi Liu, Binghong Wu, Hao Feng, Hao Liu, Can Huang, Jingqun Tang, Wei Chen, Lianwen Jin, Yuliang Liu, and Xiang Bai. Ocrbench v2: An improved benchmark for evaluating large multimodal models on visual text localization and reasoning, 2025. [68] Souvik Mandal, Ashish Talewar, Siddhant Thakuria, Paras Ahuja, and Prathamesh Juvatkar. Nanonets-ocr2: model for transforming documents into structured markdown with intelligent content recognition and semantic tagging, 2025. [69] Vik Paruchuri. Introducing our newest model: Chandra, Oct 2025."
        },
        {
            "title": "A Interface contract",
            "content": "A.1 Task families & I/O schemas For each task family in Section 2.2, we parameterize the interface by task type (reading, detection, conditional_detection, localized_reading), an input type (image only, image + query string, image + bounding box), and an output type (text, text2d, lines, paragraphs, BOX). At training time we maintain small bank of prompt templates for each (task, input, output) triple and sample one uniformly, to encourage robustness to minor paraphrases. Below we list representative templates for each family; in all cases {FORMAT} is replaced with one of the output types (text, text2d, lines, paragraphs): Full-page reading. Read all text in the provided document and return the result as {FORMAT}. Transcribe the text from this page. Use the {FORMAT} format described above. Without adding explanations, output only the {FORMAT} representation of the page. Full-page detection. Without returning any text, detect all LINES in this document and output their bounding boxes as JSON array. Locate all PARAGRAPHS in the image and return JSON array of boxes [x1, y1, x2, y2]. Do not include text. Detect all math regions in the page and return their bounding boxes as JSON. Conditional detection. Where does the exact string \"{q}\" appear in this document? Return JSON array of bounding boxes for all matching lines. Search for \"{q}\" in the page and output [x1, y1, x2, y2] boxes for each line that contains it. If there are no matches, return []. Localized reading. What text appears inside the region [x1, y1, x2, y2] of this page? Return only the recognized text. Read the text inside the specified box and output lines or paragraphs string, without JSON or extra commentary. During training, we also randomize the noun used to refer to the image (e.g., document, image, page, scan) and simple determiners (e.g., this, that, the attached, the provided). We qualitatively found that this light prompt augmentation improves robustness to prompt phrasing at inference time while keeping the semantics of the task fixed. A.2 Geometry and BOX Type All grounded outputs ultimately reduce to axis-aligned bounding boxes in the coordinate system of the input image. Coordinate system. We use integer pixel coordinates with origin at the top-left corner of the rasterized page. bounding box is represented as with x1 < x2 and y1 < y2. Here increases to the right and increases downward. [x1, y1, x2, y2] Axis-aligned boxes only. We do not predict rotated boxes. Rotated or skewed text is represented by the axis-aligned bounding box of its minimal enclosing rectangle. This choice simplifies evaluation and downstream consumption. Clipping and degeneracies. During post-processing we apply the following normalizations to both predictions and references: Clip all coordinates to the closed interval [0, ] [0, H], where and are the image width and height in pixels. Discard degenerate boxes with non-positive width or height (i.e., x1 x2 or y1 y2). These steps ensure that all boxes used for evaluation have strictly positive area and lie within the image bounds. Grounded output schema. Whenever an output type is grounded and includes text, we use JSON arrays of objects of the form {\"text\": <string>, \"bbox\": [x1, y1, x2, y2]} For pure detection tasks, where no text is required, the output is JSON array of bounding boxes only: [ ] [x1, y1, x2, y2], [x1, y1, x2, y2], ... We apply tolerant JSON-repair step before evaluation and treat irreparable outputs as invalid; see Appendix D.1 for details. A.3 Layout-Sensitive text2d The text2d representation is designed to encode 2D layout using only spaces and newline characters while remaining plain string. It is constructed from line-level grounded outputs of the form {\"text\",\"bbox\"} as follows. Preprocessing. We begin with set of line predictions {(ˆti, bi)}N i=1, where bi = [x1,i, y1,i, x2,i, y2,i]. We discard boxes with non-positive width or height and clip all remaining boxes to the image bounds. We then compute simple global statistics (e.g., median line height and crude character density in characters per pixel) that are used to approximate how many fixed-width character cells fit in typical line. Line ordering. We sort lines according to 2D reading order: first by the vertical position of the line center (top-tobottom), then by the horizontal position of the left edge (left-to-right) within each row. This ordering approximates natural reading order across multi-column layouts and figures. Mapping to notional grid. We conceptually render lines into notional character grid. For each ordered line, we: Map the absolute horizontal position of its left edge to character column index using the estimated character density. Insert spaces to account for horizontal gaps between consecutive lines on the same row (e.g., to right-justify page numbers or align columns). Emit the normalized line text norm(cid:0)ˆti (cid:1) at the corresponding columns. 27 Vertical gaps between consecutive lines are measured in pixels using the distance between their vertical centers. If this gap exceeds threshold proportional to the estimated line height, we insert one or more blank lines in the output. In practice we cap the number of inserted blank lines to small maximum to avoid pathological vertical expansion. Final string. After all lines have been placed, we obtain 2D grid with characters and spaces. We trim trailing spaces on each row and convert the grid to single string by joining rows with newline characters. The result is text2d string that: preserves relative horizontal alignment via runs of spaces, preserves vertical structure via explicit blank lines, and can be consumed by downstream systems as plain-text API. During evaluation, we preserve whitespace in text2d so that layout-sensitive differences (e.g., scrambled columns or collapsed vertical gaps) are reflected in the error. A.4 Task-Specific Semantics Conditional detection. Each conditional-detection example consists of page image, query string q, and target set of line-level boxes. To construct the target, we take the reference line transcripts and apply the same normalization norm() used for text metrics (Appendix D.2), including Unicode NFKC normalization, trimming of leading/trailing whitespace, and collapsing of internal whitespace runs. We apply norm() to both the query and each line transcript and mark line as hit if the normalized query appears as case-sensitive substring of the normalized line. If the query does not appear on the page, the ground truth set is empty and the expected model output is an empty JSON array [] with no additional commentary. When query appears multiple times on the same line, we treat that as single target box; if it appears on multiple lines, all corresponding line boxes are included in the target set. During evaluation we compare predicted and reference boxes with IoU-based matching at threshold of 0.5; unmatched predicted boxes are false positives and unmatched reference boxes are false negatives. Localized reading. For localized reading, the input consists of page image and bounding box = [x1, y1, x2, y2] in pixel coordinates. The model is expected to transcribe only the text inside b. We train both line-level and paragraphlevel variants, but in both cases the output is treated as non-grounded: the model returns single plain-text string rather than JSON array of {\"text\",\"bbox\"} objects. To construct training labels, we intersect with the reference lineor paragraph-level boxes and concatenate the transcripts of blocks whose overlap with exceeds an IoU threshold (we use 0.5 in all experiments). The resulting string is used as the target transcript for that localized reading example. At evaluation time, we compute CER and WER between the model output and this target string, again using norm() to normalize whitespace and Unicode variants. Full-page reading and detection. For full-page reading tasks, the model receives only the page image and is prompted to produce either: linear transcript (text), whitespace-preserving transcript (text2d), or structured array of line-/paragraph-level objects (lines/paragraphs). Structured predictions are parsed through the JSON repair and normalization pipeline described in Appendix D.1. Invalid or irreparable outputs are treated as failures and assigned maximal error according to Appendix D.2. For full-page detection, the model receives only the page image and must output JSON array of bounding boxes that cover all lines or paragraphs of interest, without transcripts. Supervision and metrics follow the detection protocol in Appendix D.2, using IoU-based matching at threshold 0.5."
        },
        {
            "title": "B Data",
            "content": "B.1 Real corpora: filtering and normalization For OCR-IDL and PubMed-OCR we start from document-level PDFs and associated OCR outputs provided by each source. We apply common preprocessing pipeline to obtain page images and normalized ground-truth annotations. 28 All PDFs are rasterized to single-page RGB images at 72 dpi. TabMe++ was rasterized in the initial construction of TabMe and is used as is. Text normalization. For each available transcript (paragraph-, line-, or word-level) we apply the same normalization operator norm() used for text metrics (Appendix D.2), including: Unicode NFKC normalization, conversion of platform-specific quotes, dashes, and bullets to canonical ASCII or Unicode codepoints, trimming of leading/trailing whitespace, and collapsing of internal whitespace runs into single space. This produces consistent supervision across datasets and aligns training-time targets with evaluation-time normalization. Geometry unification. All datasets provide boxes in pixel coordinates, but differ in conventions (e.g., half-open vs. closed intervals, one-based vs. zero-based indices). We map all coordinates to the axis-aligned [x1, y1, x2, y2] convention from Section 2.2, clip to image bounds, and discard degenerate boxes with non-positive width or height. B.2 Synthetic corpora: generation pipelines Grounded LATEX (GL). The GL corpus is designed to provide highly localized supervision for mathematical expressions. 1. Equation mining. We extract raw LATEX fragments from Wikimedia dumps following [19]. 2. Rendering. For each equation we create blank page and render the equation at randomly sampled: rotation angle, and sub-pixel position on the page. SynthDoG grounding (SDG). The SDG corpus adapts SynthDoG [5] to better match our line-centric grounding objectives. 1. Text source. We draw raw text snippets from FinePDFs [20], sampling both narrative and transactional content. Snippets are truncated to at most few hundred characters to keep synthetic pages compact. 2. Layout generation. We use SynthDoGs layout engine to place text blocks, but modify the line-breaking logic to be word-aware: line breaks are only inserted at whitespace, and hyphenation is disabled. This avoids mid-word splits that complicate line-level supervision. 3. Line-level supervision. We instrument the generator to emit line-level bounding boxes and transcripts directly, in addition to any word-level boxes that SynthDoG already produces. Paragraph-level groupings are not used in SDG. 4. Rendering and noise. Pages are rendered at 300 dpi with randomized fonts, font sizes, and background textures (e.g., paper grain). We apply the same noise augmentations as in GL (blur, compression, low-amplitude geometric jitter). B.3 Splits and held-out sets Evaluation pages. We reserve 10,500 pages across IDL, TabMe++, and PubMed-OCR for evaluation (Section 5.2). Pages are sampled uniformly at random from the union of these three corpora. All remaining pages from IDL, TabMe++, and PubMed-OCR are used for training or validation. Training and validation splits. Within each training stage we randomly carve out 2,048 examples from the current training pool as validation set. Sampling is again stratified by dataset so that every source is represented in validation. These validation examples are held fixed for the duration of the stage and are not reused for training. Early stopping. We monitor the validation loss once per fixed number of training steps and terminate the current stage when the loss fails to improve for specified patience window. If the best checkpoint occurs before the end of the stage, we roll back model parameters to that checkpoint before proceeding to the next stage. All random splits are generated with fixed seeds and logged to disk to enable exact reproduction of data partitions."
        },
        {
            "title": "C Training",
            "content": "C.1 Input Preprocessing & Setup During training, we rasterize each PDF page at 72 DPI into single image and feed it directly to the Qwen2.5-VL NaViT-based vision encoder, preserving the original aspect ratio and using the default maximum number of image tokens without additional cropping, tiling, or high-resolution modes. Backbone configuration. We fine-tune the public instruction-tuned checkpoints (Qwen2.5-VL-3B-Instruct and Qwen2.5-VL-7B-Instruct) without modifying the tokenizer or vocabulary. All modules of the model are trainable: we do not freeze the vision encoder or language backbone, and we do not introduce adapters or LoRA layers in the GutenOCR models. Context handling. We train and serve GutenOCR with long-context transcripts, using curriculum over sequence length described in Section 3.2. Across stages, we cap the total sequence length (image prompt plus output tokens) between roughly 2k and 16k tokens, always using single page per example with no dynamic packing across pages. When target transcript would exceed the stage-specific maximum, we truncate the output and discard the remainder. At inference time, we enforce the same total-context limits and cap the number of generated tokens at 4,096 on external benchmarks and 16,384 in our internal long-sequence evaluations. Decoding and task routing. All reported results use deterministic greedy decoding (temperature = 0, no sampling) with the maximum new-token limits described above. Task routing is purely prompt-based: the user specifies the desired task (reading, detection, localized_reading, or conditional_detection) and output schema (text, text2d, lines, paragraphs, BOX, etc.) in the prompt, and the same GutenOCR checkpoint is used for all task families. We do not employ any external head-selection logic or multi-model routing. C.2 Hyperparameters Table 13 summarizes the stage-specific optimization settings used in all experiments. Across stages, we fix the effective batch size to 128 samples (16 per GPU device across 8H100) and use the same optimizer and schedule: AdamW [21] with weight decay 0.01 and maximum learning rate 1 106, linearly warmed up over the first 1,000 updates and then linearly decayed to zero. All modules are fully trainable and we use DeepSpeed ZeRO-3 for memory sharding of parameters, optimizer states, and activations. We train in bf16 with gradient checkpointing and clip all gradients to global norm of 1.0. Stage Per-GPU batch Grad accum #GPUs Eff. batch 1 2 3a 3b 4 2 1 1 8 16 16 8 8 8 8 size 128 128 128 Length range < 2k [2k, 8k] [2k, 8k] [8k, 16k] Approx. dataset mixture GL 48%, IDL 45%, SDG 6%, T++ 0.4% IDL 95%, T++ 5% PMD 32%, IDL 65%, T++ 3% PMD 100% Table 13: Stage-specific optimization settings. Effective batch size is computed as (per-GPU batch) (gradient accumulation) 8 GPUs. Dataset mixtures are approximate fractions of pages sampled per stage. SDG = SynthDoG Grounding, GL = Grounded LATEX, IDL = OCR-IDL, T++ = TabMe++, PMD = PubMed-OCR. C.3 System Prompt Your k o d and a e t a from u t and g . GEOMETRY: r a : p b s d o r : a / l e : e p l ; g ( 0 , 0 ) e ; h image n ; p e t r / a e o bottom , d t d o r : a a [ x1 , y1 , x2 , y2 ] h x1 <x2 , y1 <y2 . a . t i . u h i g b d box h n l l n c g ( no a b s ) . TASK TYPES : d : l t a e _ d : e o : e o t a _ e o : e t r o e n s on e r image . d t h s i d n g box h image . h image h t a r i . t i i e image e on o e x e . 30 OUTPUT TYPES : l s l l TEXT : one i r ; c TEXT2D : one i r ; s e t a s LINES : JSON a o c , d , y u h WORDS: JSON a o c , PARAGRAPHS : JSON a o c , a r i , y u h r p i o LaTeX r i : LATEX : JSON a o c , d , y u h i . e : BOX: JSON a b d b s y : r o n word by word OCR: r o n l by e OCR: [ x1 , y1 , x2 , y2 ] , i . i . t : t : . . . [ one ; s e o u ( c + y ; no r a ) . Non u d p n . \" bbox \" : n e . Non u d p n . [ x1 , y1 , x2 , y2 ] } . When a { \" t \" : i , r o n p g h e OCR: { \" t \" : r , \" bbox \" : [ x1 , y1 , x2 , y2 ] } . When { \" t \" : i , \" bbox \" : [ x1 , y1 , x2 , y2 ] } . { \" t \" : i , \" bbox \" : [ x1 , y1 , x2 , y2 ] } . When a ] . d c n and d o _ e o s n . OUTPUT FORMAT non u d p , u s n : t o z e o e . t 2 ABSTRACT c i n e n 2D o . h t i empty , t an empty i : t t 2 g n o u , u JSON a o c when f i e n s . Each e The \" t \" key i p e h two s : \" t \" and \" bbox \" . what and \" bbox \" key h where . n [ { \" t \" : { \" t \" : \" s \" Second e i f t \" , \" bbox \" : t \" , \" bbox \" : [ 1 0 0 , 2 0 0 , 4 0 0 , 2 5 0 ] } , [ 1 0 0 , 5 0 0 , 4 0 0 , 6 0 0 ] } ] n [ { \" t \" : \" c { }{ } \" , \" bbox \" : [ 5 2 5 , 5 5 8 , 7 5 5 , 6 2 0 ] } ] h t i empty , t an empty JSON a : n [ ] d c n k , u JSON a b d b s y . n [ [ 1 0 0 , 2 0 0 , 4 0 0 , 2 5 0 ] , [ 1 0 0 , 5 0 0 , 4 0 0 , 6 0 0 ] ] l l d d t s , u h c i t w i e c e u n box . t o z e i n b d box . no t e n d h h u n box , u an empty i : t"
        },
        {
            "title": "D Evaluation",
            "content": "D.1 Parsing Structured Outputs Many baselines and all GutenOCR variants are prompted to emit JSON-structured outputs (e.g., arrays of {\"text\", In practice, LLM-generated JSON is frequently imperfect, so we employ simple repair-and- \"bbox\"} pairs). normalization pipeline before evaluation. We first pass the raw model output to tolerant JSON repair library (json_repair), which attempts to recover valid objects even in the presence of unbalanced brackets, missing commas, comments, or extra text around the JSON 31 snippet. If repair fails, the prediction is treated as invalid and assigned the maximal error according to the conventions in Appendix D.2. For successfully repaired outputs, we normalize key names and shapes: keys whose names contain text or label (case insensitive) are mapped to canonical text field; keys containing box are mapped to bbox. Lists of four [x_1,y_1,x_2,y_2]} records when numeric values are interpreted as bounding boxes and wrapped into {\"bbox\": appropriate, and any remaining coordinates are clipped to the image bounds. This normalization allows us to treat structurally similar outputs from different models in uniform way while still penalizing irreparable or semantically malformed predictions. D.2 Metrics Unless stated otherwise, all text metrics are computed on normalized strings norm(). The normalization operator applies Unicode NFKC, trims leading and trailing whitespace, and collapses internal whitespace runs to single space. For text2d we preserve line breaks and intra-line spacing before applying norm() so that layout changes affect the score. We summarize the formal definitions below for completeness. D.2.1 Text metrics For prediction ˆy and reference y, let dlev(, ) denote the standard Levenshtein edit distance and the character length. The character error rate (CER) is CER(ˆy, y) = dlev (cid:0)norm(ˆy) , norm(y)(cid:1) max(cid:0)1, max{norm(ˆy) , norm(y)}(cid:1) . Normalizing by the length of the longer string keeps CER [0, 1] even when one side is empty or much shorter than the other, which is useful under noisy OCR-derived labels. For word error rate (WER), we tokenize norm(y) into words using whitespace and let = norm(y)words be the number of reference words. Let S, D, be the number of substitutions, deletions, and insertions, respectively, when aligning norm(ˆy) to norm(y) at the word level. Then WER(ˆy, y) = + + max(1, ) . D.2.2 Detection metrics Let = {bi}m i=1 be the set of predicted boxes and = {gj}n [x1, y1, x2, y2] in image pixel coordinates. We define the intersection-over-union j=1 the ground-truth boxes, with each box = IoU(b, g) = area(b g) area(b g) . For threshold τ (we use τ = 0.5 throughout), we construct one-to-one matching Mτ using only pairs with IoU τ (Hungarian matching with IoU as affinity). We then define TPτ = Mτ , FPτ = TPτ , FNτ = TPτ . Precision, recall, and F1 at threshold τ are Precτ = TPτ max(1, TPτ + FPτ ) , Recτ = (cid:40) 1, = 0, TPτ /G, > 0, F1τ = 2 Precτ Recτ max(1, Precτ + Recτ ) . In the main text we report F1@0.5 F1τ =0.5 and Recall@0.5 Recτ =0.5. D.2.3 End-to-end metrics (detection + recognition) For structured reading tasks that produce boxtext pairs = {(bi, ˆti)} and = {(gj, tj)} we use two end-to-end metrics that reuse the detection matching M0.5 from Section D.2.2. mCER@0.5 (recognition given correct localization). Let M0.5 be the one-to-one matching between predicted and ground-truth boxes at IoU 0.5. For each (i, j) M0.5 we compute per-span CER ceri,j = dlev max(cid:0)1, max{(cid:12) (cid:0)norm(cid:0)ˆti (cid:1) , norm(tj)(cid:1) (cid:12)norm(cid:0)ˆti (cid:12) , norm(tj)}(cid:1) . (cid:1)(cid:12) mCER@0.5 is the mean of ceri,j over (i, j) M0.5, with edge cases handled as described below. Because unmatched ground-truth boxes do not contribute directly to this average, mCER@0.5 isolates recognition quality conditioned on successful localization; coverage is accounted for by the detection recall. CERe2e (page-level fidelity). To assess the joint effect of localization, reading order, insertions, and deletions, we form page-level strings from and as follows. We sort boxes in deterministic reading order (by the y1 coordinate of their top edge, then by x1 left-to-right), concatenate the corresponding texts with line breaks and layout-sensitive whitespace (as in the text2d construction), and finally apply norm() to both page strings. We then compute CER between these two concatenated strings. CERe2e is therefore sensitive to missing or spurious lines, scrambled ordering, and layout-preserving whitespace errors in addition to pure recognition mistakes. D.2.4 Edge cases and invalid predictions We adopt the following conventions to keep metrics well defined: Empty text. If both normalized strings are empty, we set CER = 0 and WER = 0. If exactly one side is empty, we set CER = 1 and WER = 1. Detection with no boxes. = and = : (Prec, Rec, F1) = (1, 1, 1). = and = : Rec = 1, Prec = 0, so F1 = 0. = and = : Rec = 0 and F1 = 0. mCER@0.5 with no matches. If M0.5 > 0, mCER@0.5 is the average over matched pairs. If = and = , we set mCER@0.5 = 0. In all other cases with M0.5 = 0 (e.g., detector fires but never overlaps ground truth, or misses all ground-truth boxes) we set mCER@0.5 = 1. Invalid structured predictions. If model output cannot be repaired into valid JSON or yields no usable boxes/text after normalization (Appendix D.1), we treat it as maximal-error prediction: CER = 1, WER = 1. For structured tasks we additionally set mCER@0.5 = 1 and CERe2e = 1, and assign F1@0.5 = 0 and Recall@0.5 = 0 for the associated detection metrics. Composite grounded OCR score. For the composite scores reported in Section 5.2, we map each reading error ϵ [0, 1] (CER, WER, mCER@0.5, or CERe2e) to per-task score 1ϵ, and average these scores with the corresponding detection and conditional-detection F1@0.5 values over the task families in Table 4. The resulting composite lies in [0, 1] and weights reading and detection tasks equally."
        },
        {
            "title": "E Additional Benchmark Details",
            "content": "E.1 Fox Benchmark Details This section supplements the main Fox results in Section 5.3 and Table 7 with task configuration, prompting details, and more detailed discussion of the Page F1 vs. Page CER trade-off and of the region, line, and color-guided subtasks. Tasks and filtering. Fox [13] defines collection of multi-page document tasks that mix OCR, retrieval, and visual reasoning. We restrict our evaluation to the four English OCR-style subtasks that align with GutenOCRs training objectives: (i) page OCR, (ii) region-level OCR, (iii) line-level OCR, and (iv) color-guided OCR. We follow the official Fox splits and evaluation scripts, but filter pages to those whose language metadata is purely English. Output canonicalization. For the page OCR subtask, we score predictions as plain text. In practice, some models emit lightweight structure or markup (e.g., JSON wrappers, Markdown, or HTML) rather than raw transcript. To make evaluation robust to these surface forms, we apply uniform post-processing step that attempts to detect and render such outputs into single text string prior to scoring (Markdown is rendered via markdown2, HTML is stripped via BeautifulSoup, and JSON outputs are parsed and linearized into text representation). This canonicalization is applied identically to all models, but it may mildly benefit systems that produce well-formed structured/marked-up outputs by allowing their textual content to be recovered instead of being penalized for formatting alone. 33 Prompting and decoding. All models, including GutenOCR and Qwen-based baselines, are evaluated under unified prompting scheme. For each Fox subtask, we use the official benchmark prompts and output-format instructions as provided by the evaluation scripts. For the regionand line-level OCR tasks (which include an explicit pointer region), the only model-specific adaptation we apply is deterministic conversion of the pointer box into the coordinate convention expected by the model (e.g., absolute pixel coordinates vs. normalized coordinates). Beyond this box-format normalization, prompts are otherwise unchanged across models. For baselines, we apply minimal OCR-focused system prompt (Listing E.1) to encourage strict format compliance (e.g., emitting raw JSON when requested). For GutenOCR, we preserve the exact system prompt used during training (see Appendix C.3), without modification. All results are produced with deterministic greedy decoding (temperature = 0, no sampling) and fixed maximum new-token budget. You an OCR i n . Read and n i e from p i document image . l t s Do add l t s n u o x l . r x beyond r e d p . Page metrics: Page F1 vs. Page CER. Fox reports an order-agnostic Page F1 metric that measures the overlap between predicted and reference tokens after text normalization and an order-sensitive edit-distance metric. In our tables, we retain Page F1 and replace the latter with our normalized character error rate (CER; Section 4.1). Intuitively: Page F1 is forgiving of reading order and extra whitespace. It treats the page as bag (or multiset) of tokens and measures how many reference tokens appear somewhere in the prediction, and vice versa. Page CER is stricter: predictions are compared to the canonical Fox linearization as single string, so reordering blocks, inserting extra text, or missing spans all increase the error. GutenOCR is trained to emit layout-sensitive text2d and structured line/paragraph JSON outputs, not single canonical page-to-Markdown format. On Fox pages, it therefore tends to obey 2D layout cues (columns, tables, headings, whitespace) rather than the specific reading order chosen by the benchmark. As result, GutenOCR-3B and GutenOCR-7B achieve very high Page F1 (close to or above their Qwen2.5-VL backbones) while exhibiting substantially worse Page CER: they read essentially the right content, but prefer to do so in layout-driven order that diverges from Foxs reference markup. In contrast, OCR-specialized baselines like Nanonets-OCR2-3B and olmOCR 2 are explicitly optimized for page-toMarkdown fidelity and thus better match Foxs canonical order, yielding lower Page CER at similar or slightly lower Page F1. We view this discrepancy as reflecting difference in target behavior rather than pure weakness: GutenOCR prioritizes layout-preserving, grounded front-end outputs, whereas Foxs page metric rewards adherence to particular markup convention. Regionand line-level OCR. Region and line subtasks probe the ability to focus anywhere on the document. Fox provides, for each query, either region pointer (a box highlighted on the page) or line pointer, together with target transcript. We evaluate all models by cropping the indicated region/line from the page, passing the full page plus pointer to the model, and computing CER between the normalized prediction and reference text. GutenOCR is closest to these tasks in spirit: it is explicitly trained for region-aware reading and localized reading given box. As reported in Table 7, this specialization yields large gains over the Qwen2.5-VL backbones on both region and line tasks, and GutenOCR-3B attains the best region-level CER among all evaluated models, slightly outperforming the dedicated Fox model itself. The line task is somewhat extrapolative for GutenOCR (we do not train on explicit line-pointer prompts), but we still see substantial improvements, which we attribute to transfer from region-level detection and localized reading: once the model can reliably find and read arbitrary subregions, line pointers become natural special case. Color-guided OCR and catastrophic forgetting. The color-guided task asks the model to read only text in colored box (red/green/blue) while ignoring all other text. Fox and several OCR-specialized baselines perform strongly here; Qwen2.5-VL-7B also demonstrates reasonable zero-shot ability to interpret color-coded pointers, likely inherited from generic vision-and-grounding pretraining. GutenOCR, by contrast, performs poorly on this subtask (high CER for both 3B and 7B). We do not include any color-guided supervision in our training mixture, and the OCR-centric fine-tuning appears to overwrite the heuristics the base VLMs had learned for color-based selection. Qualitatively, we frequently observe GutenOCR interpreting prompts literally (e.g., trying to read the words red box from the page) or returning structured box coordinates instead of the requested text. This is classic example of narrow catastrophic forgetting: the documentation-focused curriculum strengthens grounded OCR in the regime we care about (full-page, region, and localized reading) while eroding an orthogonal emergent behavior (color-conditioned region selection) that was never exercised during fine-tuning. Qualitative examples. Figure 4 illustrates typical Fox behaviors for Qwen2.5-VL-3B vs. GutenOCR-3B: Region/line success. In panels (a) and (b), Qwen2.5-VL-3B either misses or badly misreads the targeted snippet, while GutenOCR returns the correct localized text with near-zero CER, showing transfer of its in-domain grounding abilities. Color failure. Panel (c) shows case where Qwen2.5-VL-3B perfectly follows the color box, whereas GutenOCR essentially ignores the color cue. Content vs. structure. Panel (d) provides concrete Page F1 vs. Page CER example: GutenOCR captures almost all of the reference tokens (high F1) but streams them in layout-driven order that diverges from the Fox linearization, leading to higher CER than the backbone despite better underlying reading. These examples match the quantitative pattern in Table 7: GutenOCR behaves like grounded OCR front-endstrong on region and line reading, layout-sensitive in its page outputs, and relatively agnostic to color cuesrather than like page-to-Markdown system tuned for single canonical reading order. E.2 OmniDocBench v1.5 OmniDocBench v1.5 [14] is multi-domain document benchmark built from PDF pages with dense annotations for layout regions, text spans, formulas, and tables. The corpus covers nine page typesbooks, slides, financial reports, textbooks, exam papers, magazines, academic papers, handwritten notes, and newspaperswith v1.01.5 revision that expands coverage and refines markup conventions.2 The official tasks focus on end-to-end markup generation (e.g., HTML/Markdown) and report combination of edit-distance and structure-aware metrics for text, formulas, and tables. Our production setting targets grounded text+box outputs rather than canonical markup, so we use OmniDocBench as testbed for three task-specific behaviors: text recognition, formula recognition, and text detection. Throughout, we restrict to pages whose language tag is purely English. Text recognition subset. For text recognition we use the OmniDocBench OCR subset, which provides cropped text spans together with attributes such as language, background type, and rotation. Each span is paired with reference transcript, and evaluation is based on component-level edit distance. We follow the official setup and compute character error rate (CER) on either (i) full-page concatenations (page-level OCR) or (ii) individual cropped spans (componentlevel OCR), grouping results by attributes such as background (white, single-color, multi-color) and page type. In all cases we apply the same string normalization norm() as in Appendix D.2 before computing CER. Formula recognition subset. OmniDocBench includes dedicated formula subset with LATEX annotations for rendered equations. The official benchmark evaluates formulas using the Character Detection Matching (CDM) metric [22], which compares rendered strings at the symbol level, together with normalized edit distance. We adopt this setup for the English subset: for each cropped formula we render both the reference and predicted LATEX and compute (i) CER between normalized strings, and (ii) CDM as render-aware similarity score (higher is better). Results in the main text  (Table 11)  break these metrics down by page type to surface domain-specific trends. Layout and text-detection subset. For layout, OmniDocBench provides text spans defined at roughly line level, together with higher-level layout regions such as tables and formula blocks. Crucially for our purposes, the text spans form partial annotation of visible text: Many table cells are represented only through table object rather than individual text spans. Certain formulas and labels are marked with special tags (e.g., Equation Ignore) to exclude them from text-based evaluation. Decorative or low-salience text may be omitted entirely. GutenOCR, by contrast, is trained under detect all readable text ontology: it aims to place line-level boxes on any legible text, including table cells, equation-like expressions, and decorative labels. As result, one-to-one mapping between OmniDocBench spans and our notion of text line does not exist. 2See the v1.01.5 changelog in the OmniDocBench supplement. 35 Recall-only detection protocol. Because OmniDocBench text spans cover only subset of the text that GutenOCR legitimately detects, we cannot reliably treat non-overlapping predictions as false positives. Any predicted box with no matching annotated span might be: (i) true positive under our ontology (e.g., content inside table cell), or (ii) genuine hallucination; the benchmark does not distinguish between these cases. This makes precision and F1 ill-defined for our setting. To avoid over-interpreting partial labels, we treat OmniDocBench text spans as trusted but incomplete ground-truth set and evaluate only recall at IoU 0.5: When OmniDocBench says there is text here, how often does our detector cover it? Concretely, we compute R@0.5 by matching predicted and ground-truth boxes using IoU 0.5 (as in Appendix D.2) and report the fraction of annotated spans that receive match. We do not report precision or F1 on OmniDocBench text detection. Qualitatively, this protocol corresponds to counting red (annotated) boxes that are covered by blue (predicted) boxes in examples such as Figure 5, while ignoring extra blue boxes on unannotated but readable text (table cells, decorative labels, etc.). Higher recall under this metric therefore indicates better coverage of the annotated subset, without making claims about behavior on the unannotated portion of the page."
        },
        {
            "title": "F Models",
            "content": "We group baselines into two categories: (i) general-purpose Qwen-based VLMs used in prompting-only setting, and (ii) OCR-specialized or layout-focused models built on Qwen or other backbones. high-level overview of all models, including parameter counts, backbones, and whether their public interfaces expose coordinates, is given in Table 5. Qwen2.5-VL Qwen2.5-VL [11] is general-purpose visionlanguage model trained on mixture of imagetext, document-understanding, OCR, and spatial-grounding tasks, including referring expressions and box/point localization. In its native interface, spatial outputs are expressed as absolute pixel coordinates in the input image frame (e.g., bounding boxes ([x1, y1, x2, y2])), which matches our detection and localized-reading conventions. We use the off-the-shelf instruct models (3B/7B) as prompting-only baselines for our reading, detection, conditional-detection, and localizedreading tasks, specifying the desired output schema (plain text, layout-sensitive text, or JSON with boxes) in the prompt. Qwen3-VL Qwen3-VL-8B [12] is an 8B-parameter visionlanguage model trained on large-scale OCR, document parsing, long-document VQA, and 2D/3D grounding and counting data, in addition to generic imagetext and VQA corpora. Spatial supervision is provided through both bounding-box and point annotations, represented in normalized coordinate system scaled to [0, 1000] along each image axis, which we map to pixel coordinates for evaluation. We use the off-the-shelf Qwen3-VL-8B instruct model as another prompting-only baseline, again prompting for the desired output schema (plain text, layout-aware text, or JSON with boxes). Nanonets OCR2-3B Nanonets-OCR2-3B [68] is 3B-parameter OCR-specialized visionlanguage model obtained by fine-tuning Qwen2.5-VL-3B-Instruct on > 3M-page corpus of synthetic and manually annotated documents for image-to-Markdown OCR and document-focused visual question answering. The model takes document page as input and produces single structured Markdown string with semantic tags for equations, tables, images, signatures, watermarks, checkboxes, page numbers, and flowcharts, but does not expose bounding-box coordinates or regionlevel grounding outputs. In our evaluation, we primarily use Nanonets-OCR2-3B as strong baseline for page-level reading and tag-level content extraction, and additionally probe its performance on our coordinate-based detection and grounding tasks to assess how much of the underlying Qwen2.5-VL-3B spatial reasoning ability is retained after OCR specialization. olmOCR 2 olmOCR 2 [7] is an OCR-specialized 7B visionlanguage model obtained by further fine-tuning Qwen2.5VL-7B-Instruct with supervised training and reinforcement learning with verifiable rewards (RLVR) on synthetic HTML-rendered pages, using unit tests that score only the correctness of linearized textual transcript (reading order, tables, math rendering, etc.). The public system takes rasterized PDF page as input and emits YAML whose body is plain Markdown text, without explicit bounding boxes or coordinates. We therefore regard olmOCR 2 as strong baseline for page-level reading and use our evaluation to quantify how much of the base models detection and referring ability is lost under OCR-focused specialization. Infinity-Parser-7B Infinity-Parser-7B [8] is layout-aware document-parsing model obtained by fine-tuning Qwen2.5VL-7B on the Infinity-Doc-55K corpus of scanned pages paired with structured Markdown and table/formula annotations, using reinforcement-learning objective that combines normalized edit distance, paragraph-count accuracy, and readingorder preservation. The public model takes page images as input and produces Markdown (with region segments wrapped in <ele> tags) and HTML tables, but does not emit bounding boxes or coordinates. We treat InfinityParser-7B as strong baseline for page-level reading and layout-sensitive parsing, and additionally evaluate it on our grounding tasks to measure how much of the Qwen2.5-VL-7B base models detection and referring ability remains after layout-focused RL tuning. Chandra OCR Chandra [69] is layout-preserving OCR system built on top of the Qwen3-VL visionlanguage model, trained to convert full document pages into structured HTML/Markdown/JSON while handling complex layouts, handwriting, tables, forms, math, and images. The model outputs HTML comprising block-level <div> elements annotated with semantic labels (e.g., text, image, figure, table) and bounding boxes in normalized 1024 1024 coordinate system via data-bbox attributes, which the library rescales to pixel coordinates for downstream processing. We primarily use Chandra as baseline for page-level reading and block-level detection in our taxonomy, and additionally test it on our finer-grained line-level and conditional grounding tasks, which go beyond the capabilities explicitly targeted by its training. DeepSeek-OCR DeepSeek-OCR [9] is 3B-parameter visionlanguage OCR model composed of custom highresolution vision encoder (DeepEncoder) and DeepSeek-3B-MoE decoder, trained on large mixture of document OCR, synthetic OCR 2.0 (charts, chemical formulas, geometry), natural-scene OCR, general vision (caption/detection/grounding), and text-only data. Document pages are supervised with both coarse text-only labels and fine layout annotations in which each paragraph is preceded by its region label and bounding-box coordinates normalized to 1,000 bins, enabling the model to emit layout-aware text paired with coordinates as well as plain transcripts. We use DeepSeek-OCR as baseline for page-level reading and layout-sensitive parsing, and further evaluate it on our detection and conditional grounding tasks to probe its referring and object-localization capabilities. PaddleOCR-VL PaddleOCR-VL-0.9B [10] is compact, document-specialized VLM trained for element-level recognition (OCR, tables, formulas, charts) but not for layout detection or box prediction. In the full PaddleOCR system, separate layout model (PP-DocLayoutV2) first predicts element types, boxes, and reading order; cropped regions are then fed to PaddleOCR-VL-0.9B, which is trainedvia 29M pretraining set plus 2.7M instruction samplesto output structured text such as Markdown/JSON, OTSL tables, and LATEX formulas. In our evaluation we explicitly use only the PaddleOCR-VL-0.9B VLM as another OCR/document-parsing baseline, and do not rely on their separate layout-analysis stage."
        },
        {
            "title": "G Additional Results",
            "content": "G.1 Detailed In-Domain Evaluation G.1.1 Reading Structured, line-based reading. GutenOCR converts both Qwen2.5-VL backbones from good reader, bad paginator into stable structured readers. On the 3B backbone, Stage 1 roughly halves average CERe2e and cuts mCER by about 80% relative to Qwen2.5-VL-3B, with Stages 2 and 3a mostly refining per-line recognition (mCER) at similar page-level error. On the 7B backbone, Stage 1 achieves the lowest overall CERe2e and Stage 3a the lowest mCER, eliminating the large gap between strong per-line accuracy and poor page assembly seen in the raw Qwen2.5-VL-7B model. Stage 3b over-specializes to PubMed-OCR, improving that domain at modest cost to the business datasets. Table 15 shows plain-text OCR where each page is reduced to single transcript. Scaling from 3B to 7B already helps substantially, but GutenOCR fine-tuning dominates: all GutenOCR variants reduce CER and WER versus their backbones. Stage 1 gives the lowest average CER for both sizes, while Stage 3a trades small amount of CER for noticeably better WER, especially on PubMed-OCR. Stage 3b again behaves as PubMed-OCR specialist. Overall, GutenOCR-7B Stage 3a is the strongest linear-text reader, with GutenOCR-3B Stage 1/3a as competitive, cheaper options. Layout-sensitive text2d. Table 16 evaluates layout-preserving text2d, which is stricter than linear text because whitespace carries layout information. Both GutenOCR backbones roughly halve CER and WER relative to their Qwen baselines. Stage 1 gives the lowest CER on average, while Stage 3a improves WER by better handling of spaces and line Model Stage OCR-IDL TabMe++ PubMed-OCR AVG CERe2e mCER CERe2e mCER CERe2e mCER CERe2e mCER Qwen2.5-VL-3B GutenOCR-3B GutenOCR-3B GutenOCR-3B GutenOCR-3B 1 2 3a 3b 0.372 0.140 0.144 0.153 0.191 0.418 0.068 0.065 0.071 0.093 0.284 0.175 0.172 0.170 0. 0.247 0.054 0.048 0.051 0.072 0.479 0.235 0.238 0.233 0.235 0.559 0.113 0.116 0.081 0.077 0.379 0.183 0.185 0.185 0.208 0.408 0.078 0.076 0.067 0.081 1 2 3a 3b 0.638 0.114 0.128 0.128 0.169 Qwen2.5-VL-7B GutenOCR-7B GutenOCR-7B GutenOCR-7B GutenOCR-7B 0.651 0.149 0.156 0.159 0.188 Table 14: Structured, line-based full-page reading on OCR-IDL, TabMe++, and PubMed-OCR. CERe2e is the page-level character error rate on the concatenated transcript; mCER is computed only on lines whose boxes are correctly matched. All values are errors (lower is better, ). Within each column, bold numbers indicate the best value across all models; underlined numbers are best within each backbone cohort (3B vs. 7B). Entries that are both bold and underlined are best both overall and within their cohort. 0.633 0.141 0.154 0.147 0.169 0.235 0.067 0.071 0.060 0.073 0.129 0.045 0.047 0.047 0. 0.196 0.058 0.060 0.062 0.089 0.611 0.159 0.177 0.155 0.150 0.379 0.099 0.107 0.070 0.060 Model Stage OCR-IDL CER WER TabMe++ CER WER PubMed-OCR CER WER AVG CER WER Qwen2.5-VL-3B GutenOCR-3B GutenOCR-3B GutenOCR-3B GutenOCR-3B 0.508 0.186 0.201 0.202 0.230 0.535 0.263 0.274 0.277 0. 0.438 0.207 0.212 0.209 0.234 0.503 0.284 0.287 0.286 0.438 0.577 0.259 0.257 0.260 0.296 0.735 0.528 0.527 0.346 0.359 0.508 0.218 0.223 0.224 0.253 0.591 0.358 0.363 0.303 0. 1 2 3a 3b Qwen2.5-VL-7B GutenOCR-7B GutenOCR-7B GutenOCR-7B GutenOCR-7B 0.262 0.198 0.209 0.206 0.247 Table 15: Full-page reading with linearized text output. We report character error rate (CER) and word error rate (WER), lower is better (). Bold numbers indicate the best value in each column across all models; underlined numbers are best within each backbone cohort. 0.441 0.341 0.352 0.297 0.406 0.333 0.198 0.210 0.202 0.241 0.340 0.275 0.285 0.282 0. 0.418 0.228 0.240 0.222 0.254 0.353 0.244 0.256 0.253 0.448 0.320 0.181 0.181 0.178 0.221 0.629 0.505 0.514 0.343 0.312 1 2 3a 3b breaks, again especially on PubMed-OCR. After fine-tuning, differences between GutenOCR-3B and GutenOCR-7B in this setting are small compared to the gains over the original VLMs. G.1.2 Localized Reading Localized reading from ground-truth regions. Table 17 evaluates recognition given ground-truth boxes. GutenOCR dramatically improves both backbones: average CER drops from 0.7/0.53 to the 0.110.20 range and WER from 0.8/0.67 to 0.200.32. Unlike full-page reading, GutenOCR-3B Stage 3a achieves the best overall CER and WER, with GutenOCR-7B stages close behind. Stage 3b again tilts toward PubMed-OCR, improving that domain while slightly degrading business-document performance. G.1.3 Detection Full-page line detection. Table 18 shows that the raw Qwen2.5-VL models are very weak detectors (F1 0.1). GutenOCR raises both F1 and recall to around 0.750.82 across datasets for both 3B and 7B, turning the backbones into strong line detectors with only small size-dependent differences. Stage 1 offers the best general-purpose detector, Stage 3a slightly favors TabMe++, and Stage 3b leans toward PubMed-OCR, but all GutenOCR stages are large step up from the prompting-only baselines. G.1.4 Conditional Detection Phrase-conditioned detection. Table 19 summarizes phrase-conditioned detection (where is string on this page?). While Qwen2.5-VL-7B is noticeably better than 3B out of the box, both backbones are far from usable. GutenOCR 38 Model Stage OCR-IDL CER WER TabMe++ CER WER PubMed-OCR CER WER AVG CER WER Qwen2.5-VL-3B GutenOCR-3B GutenOCR-3B GutenOCR-3B GutenOCR-3B 0.623 0.234 0.254 0.258 0.318 0.545 0.249 0.264 0.267 0.451 0.505 0.287 0.290 0.293 0.346 0.397 0.272 0.279 0.284 0. 0.623 0.305 0.301 0.327 0.370 0.710 0.520 0.517 0.350 0.372 0.584 0.276 0.282 0.293 0.345 0.562 0.347 0.353 0.300 0.424 1 2 3a 3b Qwen2.5-VL-7B GutenOCR-7B GutenOCR-7B GutenOCR-7B GutenOCR-7B 0.473 0.274 0.282 0.291 0.344 Table 16: Full-page reading with layout-preserving text2d output. We report CER and WER, lower is better (). Bold numbers denote the best value in each column; underlined numbers are best within each backbone cohort. 0.459 0.331 0.344 0.303 0.419 0.522 0.255 0.263 0.280 0.323 0.343 0.260 0.277 0.281 0.473 0.363 0.229 0.245 0.251 0.458 0.580 0.276 0.284 0.318 0. 0.514 0.213 0.223 0.232 0.290 0.670 0.503 0.510 0.377 0.327 1 2 3a 3b Model Stage OCR-IDL CER WER TabMe++ CER WER PubMed-OCR CER WER AVG CER WER Qwen2.5-VL-3B GutenOCR-3B GutenOCR-3B GutenOCR-3B GutenOCR-3B 0.717 0.115 0.108 0.104 0.129 0.825 0.204 0.197 0.192 0. 0.596 0.104 0.092 0.088 0.112 0.708 0.204 0.191 0.190 0.344 0.784 0.244 0.224 0.134 0.113 0.915 0.518 0.505 0.220 0.174 0.699 0.154 0.142 0.109 0.118 0.816 0.309 0.298 0.200 0. 1 2 3a 3b Qwen2.5-VL-7B GutenOCR-7B GutenOCR-7B GutenOCR-7B GutenOCR-7B 0.424 0.128 0.097 0.100 0.123 Table 17: Localized reading from ground-truth regions. We report CER and WER, lower is better (). Bold entries are best overall per column; underlined entries are best within each backbone cohort. 0.667 0.324 0.302 0.223 0.324 0.548 0.231 0.196 0.203 0.369 0.811 0.532 0.513 0.269 0. 0.530 0.171 0.149 0.129 0.136 0.641 0.208 0.196 0.197 0.406 0.646 0.262 0.241 0.178 0.137 0.521 0.122 0.109 0.110 0.147 1 2 3a 3b pushes F1 and recall into the 0.850.90 range across datasets, with Stage 3a giving the best average performance and Stage 3b again specializing toward PubMed-OCR. In this setting the 7B model keeps small but consistent edge over 3B, reflecting its stronger underlying recognition once detection has been trained. Model Stage Qwen2.5-VL-3B GutenOCR-3B GutenOCR-3B GutenOCR-3B GutenOCR-3B 1 2 3a 3b OCR-IDL F1 TabMe++ 0.103 0.820 0.800 0.794 0.756 0.097 0.812 0.798 0.794 0.769 0.253 0.795 0.806 0.813 0.798 0.235 0.761 0.815 0.821 0.809 PubMed-OCR AVG F1 0.048 0.782 0.758 0.788 0.817 F1 0.044 0.771 0.754 0.787 0. 0.135 0.799 0.788 0.798 0.790 0.125 0.794 0.789 0.801 0.798 Qwen2.5-VL-7B GutenOCR-7B GutenOCR-7B GutenOCR-7B GutenOCR-7B 0.162 0.778 0.793 0.801 0.764 Table 18: Full-page line detection. We report F1 and recall (R), higher is better (). Bold numbers indicate the best value across all models; underlined numbers are best within each backbone cohort. 0.110 0.785 0.785 0.795 0.775 0.056 0.769 0.742 0.770 0. 0.161 0.777 0.804 0.814 0.782 0.111 0.790 0.778 0.787 0.761 0.054 0.775 0.735 0.767 0.799 0.114 0.810 0.809 0.800 0.735 0.117 0.817 0.806 0.794 0.720 1 2 3a 3b 39 Model Stage Qwen2.5-VL-3B GutenOCR-3B GutenOCR-3B GutenOCR-3B GutenOCR-3B 1 2 3a 3b OCR-IDL TabMe++ F1 0.089 0.861 0.855 0.859 0.831 0.107 0.863 0.858 0.861 0.834 0.255 0.882 0.898 0.903 0.888 0.300 0.884 0.901 0.905 0.894 PubMed-OCR AVG F1 0.019 0.819 0.799 0.868 0.884 F1 0.024 0.820 0.799 0.870 0.887 0.121 0.854 0.851 0.877 0.868 0.144 0.855 0.853 0.879 0.872 Qwen2.5-VL-7B GutenOCR-7B GutenOCR-7B GutenOCR-7B GutenOCR-7B 0.466 0.887 0.899 0.902 0.876 Table 19: Conditional line detection given query string. We report F1 and recall (R), higher is better (). Bold entries are best overall per column; underlined entries are best within backbone. 0.302 0.872 0.876 0.883 0. 0.489 0.889 0.899 0.904 0.880 0.285 0.871 0.876 0.882 0.866 0.326 0.875 0.881 0.874 0.815 0.307 0.874 0.880 0.873 0.813 0.090 0.852 0.849 0.872 0.911 0.083 0.852 0.849 0.872 0. 1 2 3a 3b"
        }
    ],
    "affiliations": [
        "Roots.ai"
    ]
}