{
    "paper_title": "ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning",
    "authors": [
        "Hanyang Chen",
        "Mark Zhao",
        "Rui Yang",
        "Qinwei Ma",
        "Ke Yang",
        "Jiarui Yao",
        "Kangrui Wang",
        "Hao Bai",
        "Zhenhailong Wang",
        "Rui Pan",
        "Mengchao Zhang",
        "Jose Barreiros",
        "Aykut Onol",
        "ChengXiang Zhai",
        "Heng Ji",
        "Manling Li",
        "Huan Zhang",
        "Tong Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in embodied AI highlight the potential of vision language models (VLMs) as agents capable of perception, reasoning, and interaction in complex environments. However, top-performing systems rely on large-scale models that are costly to deploy, while smaller VLMs lack the necessary knowledge and skills to succeed. To bridge this gap, we present \\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates prior knowledge learning and online reinforcement learning (RL). The first stage, \\textit{Embodied Prior Learning}, distills foundational knowledge from three types of data: (1) Trajectory-Augmented Priors, which enrich existing trajectory data with structured reasoning generated by stronger models; (2) Environment-Anchored Priors, which provide in-environment knowledge and grounding supervision; and (3) External Knowledge Priors, which transfer general knowledge from out-of-environment datasets. In the second stage, we develop an online RL pipeline that builds on these priors to further enhance agent performance. To overcome the inherent challenges in agent RL, including long horizons, sparse rewards, and training instability, we introduce three key designs: self-summarization for context management, dense reward shaping, and turn-level policy optimization. Extensive experiments on both high-level planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate that ERA-3B surpasses both prompting-based large models and previous training-based baselines. Specifically, it achieves overall improvements of 8.4\\% on EB-ALFRED and 19.4\\% on EB-Manipulation over GPT-4o, and exhibits strong generalization to unseen tasks. Overall, ERA offers a practical path toward scalable embodied intelligence, providing methodological insights for future embodied AI systems."
        },
        {
            "title": "Start",
            "content": "ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Hanyang Chen1 , Mark Zhao1, Rui Yang1 , Qinwei Ma1,,, Ke Yang1, Jiarui Yao1, Kangrui Wang2 Hao Bai1, Zhenhailong Wang1, Rui Pan1, Mengchao Zhang3, Jose Barreiros3, Aykut Onol3 ChengXiang Zhai1, Heng Ji1, Manling Li2, Huan Zhang1, Tong Zhang1 1UIUC, 2Northwestern University, 3Toyota Research Institute https://embodied-reasoning-agent.github.io/ Abstract Recent advances in embodied AI highlight the potential of vision language models (VLMs) as agents capable of perception, reasoning, and interaction in complex environments. However, top-performing systems rely on large-scale models that are costly to deploy, while smaller VLMs lack the necessary knowledge and skills to succeed. To bridge this gap, we present Embodied Reasoning Agent (ERA), two-stage framework that integrates prior knowledge learning and online reinforcement learning (RL). The first stage, Embodied Prior Learning, distills foundational knowledge from three types of data: (1) Trajectory-Augmented Priors, which enrich existing trajectory data with structured reasoning generated by stronger models; (2) Environment-Anchored Priors, which provide in-environment knowledge and grounding supervision; and (3) External Knowledge Priors, which transfer general knowledge from out-ofenvironment datasets. In the second stage, we develop an online RL pipeline that builds on these priors to further enhance agent performance. To overcome the inherent challenges in agent RL, including long horizons, sparse rewards, and training instability, we introduce three key designs: self-summarization for context management, dense reward shaping, and turn-level policy optimization. Extensive experiments on both high-level planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate that ERA-3B surpasses both prompting-based large models and previous training-based baselines. Specifically, it achieves overall improvements of 8.4% on EB-ALFRED and 19.4% on EB-Manipulation over GPT-4o, and exhibits strong generalization to unseen tasks. Detailed Ablation studies further validate the effectiveness of different prior datasets and agent RL designs. Overall, ERA offers practical path toward scalable embodied intelligence, providing methodological insights for future embodied AI systems. 5 2 0 2 4 1 ] . [ 1 3 9 6 2 1 . 0 1 5 2 : r Figure 1 (a) Overview of the ERA framework: Embodied Prior Learning (EPL) finetunes on diverse data sources to provide foundational knowledge, and online RL further improves the agent. (b) ERA (i.e, EPL+RL) boosts 3B base model to surpass GPT-4o on hold-out evaluation sets. Equal contribution. Work done during internship at UIUC. Second author. Emails: Hanyang Chen <hc81@illinois.edu>, Mark Zhao, <mark.zhao@mail.utoronto.ca>, Rui Yang <ry21@illinois.edu> 1 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning"
        },
        {
            "title": "1 Introduction",
            "content": "Vision language models (VLMs) have shown remarkable capabilities in instruction following, visual understanding, and commonsense as well as mathematical reasoning (OpenAI, 2024; Liu et al., 2024a; Reid et al., 2024; Bai et al., 2025; Zhu et al., 2025). Building on these strengths, researchers are now exploring how to transform VLMs into embodied agents that can operate in interactive environments and tackle real-world tasks (Driess et al., 2023; Huang et al., 2023, 2024; Mu et al., 2024; Liu et al., 2024b; Kim et al., 2024b; Szot et al., 2025). Unlike single-turn question answering, embodied tasks require an agent to actively perceive, reason, and act within dynamic environment to achieve its goals. This introduces new challenges for VLMs, including long-horizon planning, reliable visual grounding, and spatial awareness (Yang et al., 2025). Recent studies have systematically evaluated VLMs as embodied agents (Liu et al., 2024b; Yang et al., 2025; Cheng et al., 2025; Li et al., 2025b). With carefully designed prompting, large-scale VLMs can solve increasingly complex tasks, but their success comes at high cost: massive models demand expensive hardware, long training cycles, and costly inference, all of which hinder real-world deployment where efficiency is critical. Meanwhile, the performance gap between large and small models remains striking. For example, Claude-3.5-Sonnet achieves 64.0% on EB-ALFRED, compared to only 4.7% for Qwen2.5-VL-7B-Instruct (Yang et al., 2025). This disparity highlights the limitations of smaller models, which often lack embodied knowledge, robust reasoning, and the synergy between high-level planning and low-level grounding. Thus, enabling compact models to master complex embodied tasks remains an open challenge. Recent efforts have explored reinforcement learning (RL) to enhance embodied agents reasoning capabilities (Zhai et al., 2024; Kim et al., 2025; Zhang et al., 2025c; Wu et al., 2025a; Wang* et al., 2025), but most approaches are restricted to static QA-style datasets or focus on either high-level or low-level reasoning in isolation. unified framework that combines supervised fine-tuning (SFT) with RL, while systematically examining embodied data curation and the design of online agentic RL for VLM-based agents, has yet to be developed. In this paper, we address the gap between large and small VLMs in embodied tasks with Embodied Reasoning Agent (ERA), two-stage training framework designed to unlock generalizable embodied skills in VLMs. In essence, ERA first introduces embodied priors into small VLMs and then refines them through online RL. Since general VLMs, especially small ones, lack domain-specific abilities in embodied environments, the first stage, Embodied Prior Learning, injects tailored knowledge to strengthen reasoning, perception, and environmental understanding. We categorize three sources of prior knowledge: (i) Trajectory-Augmented Priors, which enrich existing trajectories with reasoning annotations from stronger VLMs and rule-based visual description augmentation; (ii) Environment-Anchored Priors, which provide in-environment knowledge and grounding in the form of QA pairs beyond agent-collected trajectories; (iii) External Knowledge Priors, which transfer general skills (e.g., mathematical reasoning, spatial reasoning) from large-scale out-of-environment data and can be curated at minimal cost. The second stage applies online RL to further enhance agents performance. Specifically, agents are trained with an improved PPO pipeline that incorporates three key designs: efficient context management via self-summarization, dense reward shaping through sub-goal and behavior-shaping rewards, and turn-level policy optimization. Together, these components enable stable and efficient policy learning in long-horizon, sparse-reward settings. We evaluate ERA on EmbodiedBench (Yang et al., 2025), focusing on both high-level planning (EB-ALFRED) and low-level control (EB-Manipulation), which together offer broad coverage of embodied reasoning tasks. ERA-3B not only surpasses prompting-based large models (e.g., GPT-4o) but also outperform 7B-scale training-based baselines, achieving an average score of 65.2% on EB-ALFRED and 48.3% on EB-Manipulation. Moreover, our ablation studies disentangle the contributions of different priors in the first stage, as well as context management, reward shaping, and turn-level optimization in the RL stage, providing practical insights for building effective training pipelines for embodied agents. Our main contributions are threefold: (1) We present comprehensive study on post-training compact VLMs for embodied agents, integrating prior knowledge curation for supervised fine-tuning with online agentic RL enhanced by key design choices. (2) We introduce principled taxonomy of accessible prior knowledge for embodied agents, offering practical guidance for data curation across different task levels. (3) We demonstrate strong performance on both highand low-level embodied tasks using 3B model, and conduct detailed ablation studies to analyze the contribution of each data source and agentic RL design component. ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Table 1 Comparison of related work on embodied VLM agents. Most existing studies focus on either high-level or low-level tasks, and many incorporate chain-of-thought (CoT) reasoning. Columns under SFT represent three types of prior knowledge, while those under RL indicate whether the method includes interactive training, process-level rewards, or value learning. Our work presents the most comprehensive framework that unifies SFT and online agentic RL, addressing both high-level and low-level embodied tasks. Method Task Level Reasoning Traj.-aug. prior Env.-anchored prior Ext.-knowledge prior In-env. interaction Process-level reward Value Learning SFT RL Reinforced Reasoning (Wu et al., 2025a) High High CoSo (Feng et al., 2025a) Low Embodied-R1 (Yuan et al., 2025) Low Robot-R1 (Kim et al., 2025) High & Low GEA (Szot et al., 2025) Low MolmoAct (Lee et al., 2025) High RL4VLM (Zhai et al., 2024) Low RFTF (Shu et al., 2025) High VAGEN (Wang* et al., 2025) Low VLA-RL (Lu et al., 2025b) Low Emma-X (Sun et al., 2024) ERA (Ours) High & Low"
        },
        {
            "title": "2 Related Work",
            "content": "- - - - - - - - - - - - - - - - - - - - - Foundation Modelbased Embodied Agents. LLMs and VLMs have been explored as embodied agents, enabling them to perceive complex environments and make sequential decisions. Early work relied on prompting strategies to harness the reasoning and planning capabilities of foundation models (Singh et al., 2022; Song et al., 2023; Hu et al., 2023; Kim et al., 2024a; Shin et al., 2025). Building on this foundation, subsequent research introduced mechanisms to improve decision-making, such as code-based tools (Liang et al., 2023; Silver et al., 2024). More recently, the availability of curated embodied datasets has facilitated supervised finetuning, which has proven effective across both low-level robotic control (Zawalski et al., 2024; Zhao et al., 2025; Lee et al., 2025; Liu et al., 2025a; Kim et al., 2024b; Lu et al., 2025a; Huang et al., 2025; Zhang et al., 2025b) and high-level embodied planning (Wu et al., 2023; Chen et al., 2024a; Ji et al., 2025). RL for Embodied Agents. Beyond supervised learning, RL has become central approach for training embodied agents (Su & Zhang, 2023; Zhai et al., 2024; Yang et al., 2024a; Shu et al., 2025; Cao et al., 2025; Liu et al., 2025b; Kim et al., 2025; Szot et al., 2025; Feng et al., 2025a). key strength of RL lies in its ability to exploit suboptimal and even failed trajectories, thereby making efficient use of diverse data sources (Song et al., 2024; Wang et al., 2025). Recent progress further shows that RL can foster reasoning abilities, enabling embodied agents to generalize more effectively to novel tasks (Wu et al., 2025a; Wei et al., 2025; Lin et al., 2025). Meanwhile, studies reveal that smaller LLMs and VLMs often lack basic embodied knowledge, such as spatial reasoning (Gao et al., 2024; Lee et al., 2025; Sun et al., 2024). Therefore, grounding embodied knowledge into VLMs prior to RL training has emerged as promising direction. detailed comparison of related work is provided in Table 1, with additional discussions deferred to Appendix B. Table 1 provides comprehensive comparison with existing works, highlighting how ERA itself apart from prior works in several aspects."
        },
        {
            "title": "3 Problem Formulation",
            "content": "VLM-based Agents. Formally, VLM-based agentic tasks can be modeled as Partially Observable Markov Decision Process (POMDP) augmented with language, represented by the tuple (S, A, Ω, , O, L, R). Here, denotes the full environment state space; is the action space; and Ω is the visual observation space, where each observation It = O(st) is generated from the underlying state. The agent also receives language instruction L, which specifies the goal. The reward function generally provides binary signal: 1 if the current state satisfies the instruction, and 0 otherwise. At timestep t, the agent maintains history ht = (I0, a0, . . . , It1, at1, It) of past observations and actions, and acts according to policy π(at L, ht) parameterized by VLM. The episode terminates either when the instruction is satisfied or when maximum 3 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning horizon is reached. The learning objective is to maximize the expected reward: maxπ Eπ[(cid:80)τ denotes the terminal timestep and γ is the discount factor. t=0 γtrt], where τ High-level and Low-level Embodied Tasks. Following the embodied agent literature (Ma et al., 2024; Yang et al., 2025), we distinguish task levels based on the direct executability of actions in robotic systems. Low-level tasks operate on atomic action commands that can be executed directly by robots, typically specifying fine-grained translational or rotational displacements. For example, robotic arms action can be represented as 7dimensional vector: = [X, Y, Z, Roll, Pitch, Yaw, Gripper], where (X, Y, Z) denotes incremental translations, (Roll, Pitch, Yaw) represent rotational deltas in Euler angles, and Gripper encodes the binary open/closed state of the end-effector. In contrast, high-level tasks consist of semantically meaningful actions that can be decomposed into sequences of low-level primitives. For instance, in EB-ALFRED (Yang et al., 2025), high-level action such asfind HandTowel may require iterating through multiple low-level behaviors, which are ultimately grounded into executable commands within the simulator. Prior work has revealed that high-level tasks primarily emphasize logical reasoning and long-term planning, whereas low-level tasks focus on precise perception and manipulation."
        },
        {
            "title": "4 From Priors to Policies: Training VLMs as Embodied Agents",
            "content": "In this section, we present the Embodied Reasoning Agent (ERA) framework, two-stage training pipeline designed to equip compact VLMs with strong embodied intelligence. ERA consists of: (1) Embodied Prior Learning, which injects structured perception and reasoning capabilities into small VLMs through supervisedfinetuning on curated prior data, and (2) Online Reinforcement Learning, which further enhances embodied performance via interactive online training with process-level rewards and turn-level advantage estimation."
        },
        {
            "title": "4.1 Embodied Prior Learning",
            "content": "Because of the inherent gap between VLM pretraining and embodied agent tasks, small VLMs cannot be directly applied in this domain. Bridging this gap requires learning embodied priors before engaging in environment interactions. common approach to adapt VLMs for embodied tasks is to fine-tune them on task-specific trajectories (Liu et al., 2024b; Yuan et al., 2025; Wu et al., 2025a; Feng et al., 2025a). However, this strategy faces two key limitations. First, data scarcity and cost: collecting high-quality agent trajectories is computationally expensive and time-consuming. Second, limited reasoning supervision: popular datasets such as ALFRED typically record only action sequences without detailed reasoning traces, since annotating such information is difficult and costly. We refer to trajectory data with little or no reasoning supervision as raw trajectory data. To overcome these challenges, we propose to curate embodied prior data from diverse sources rather than relying solely on raw trajectories. We introduce three different prior data sources: trajectory-augmented priors, which enrich existing trajectories with reasoning annotations and detailed visual descriptions; environmentanchored priors, which provide contextual supervision and grounding signals from the same environment beyond the agents recorded trajectories; and external knowledge priors, which transfer general reasoning and perception skills from out-of-domain data. An overview of these datasets is illustrated in Figure 2, and their statistics are summarized in Table 2. Given curated dataset or their combination DEPL = {(xi, yi)}N i=1, where xi is prompt and yi = (yi,1, . . . , yi,yi) is the corresponding response, we perform supervised fine-tuning to inject embodied knowledge into the base model: LEPL(θ) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) yi (cid:88) i=1 j=1 log πθ(yi,j xi, yi,<j), where yi,<j denotes the sequence of tokens preceding the jth token in the response. For downstream deployment, training on trajectory data, either raw or augmented, is indispensable. Thus, instead of mixing the dataset together, we first fine-tune VLMs on environment-anchored or external knowledge priors to acquire general embodied skills, and subsequently train them on raw or augmented trajectories to obtain the final agent model. 4 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Figure 2 Illustration of Embodied Prior Learning (EPL). EPL leverages three data sources: Augmented trajectory priors, environment-anchored priors, and external knowledge priors. Table 2 Statistics of the embodied prior learning datasets used for EB-ALFRED and EB-Manipulation. We report the number of trajectories (#Traj), total samples (#Samples), and average output token length (Avg. Token Len.) for each prior dataset type: Raw trajectory, Trajectory-Augmented Prior (Traj-Aug), Environment-Anchored Prior (Env-Anc), and External Knowledge Prior (Ext-Know). Domain Prior Dataset #Traj #Samples Avg. Token Len. EB-ALFRED EB-Manipulation Raw Traj-Aug Masked Action Modeling Env-Anc Env-Anc Action Seq. Reordering Ext-Know OpenO1-SFT (Open O1 Team, 2024) Raw Traj-Aug Absolute Grounding Env-Anc Relative Grounding Env-Anc Comb. Grounding Env-Anc Ext-Know SpaceThinker (Remyx AI, 2025) 913 913 622 622 8,834 8,834 41,616 6,574 10,000 4,249 4,023 4,444 2,000 8,888 11,413 74.0 159.8 396.1 395.6 1102.6 104.4 284.2 7.6 12.0 1.0 202."
        },
        {
            "title": "4.1.1 Trajectory-Augmented Priors",
            "content": "Existing embodied trajectories typically consist only of observations and action sequences, without the intermediate reasoning steps that are crucial for solving complex agentic tasks. Some recent efforts (Yang et al., 2025) provide limited reasoning signals, such as single high-level rationale corresponding to multi-step plan. However, such coarse supervision does not offer structured guidance at intermediate steps, making it difficult for agents to adapt quickly when errors occur partway through execution. To address this limitation, we construct trajectory-augmented priors by enriching every step of the trajectory with explicit reasoning supervision from large VLMs such as GPT-4o. Specifically, for each timestep t, we , zplan define structured reasoning trace zt = {zvis is visual description of the current state, zref is reflection on the history to detect and correct potential errors, and zplan is step-level plan for achieving the task. By prompting GPT-4o with the language instruction L, current observation It, action history {a0, . . . , at1}, and the current action at, we obtain zt that enriches the trajectory with structured inner monologue (Huang et al., 2022; Xu et al., 2024). This step-level reasoning has been shown to significantly improve generalization in high-level embodied tasks (Feng et al., 2025a; Zhai et al., 2024). Prompting used for generating this reasoning with GPT-4o is provided in Appendix G.5. }, where zvis , zref t 5 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning For low-level manipulation tasks, however, GPT-4o often produces inaccurate visual descriptions, leading to misalignment between perception and action, and consequently, failure to solve low-level tasks. To overcome this issue, we employ rule-based method that generates ground-truth visual descriptions using the backend information of the underlying simulator, ensuring consistency between perception and control. Further details and illustrative examples of this rule-based procedure are provided in Appendix G.6."
        },
        {
            "title": "4.1.2 Environment-Anchored Priors",
            "content": "While agent trajectories provide the most direct supervision for embodied tasks, such data is often limited in scale and expensive to collect. This limitation raises natural question: can we leverage additional supervision that is not trajectory-based, yet still grounded in the same environment, to enhance the agents learning? To address this, we introduce environment-anchored priors: auxiliary data sources that encode environment-level knowledge beyond trajectories, such as semantic question answering, visual grounding, or spatial reasoning. These priors enrich agents with deeper understanding of task semantics, temporal dependencies, visual grounding, and spatial relations. We illustrate these datasets in the middle of Figure 2. For EB-ALFRED, we curate two semantic supervision datasets designed to enhance task comprehension, action feasibility, and embodied planning: masked action modeling and action sequence reordering. Both datasets are derived from the original ALFRED training set, which contains large-scale raw action sequences. Because the original actions do not perfectly align with the EB-ALFRED action space, we apply rule-based matching and replacement to adapt them for effective use. Detailed processing steps are provided in Appendix G.7. Masked Action Modeling. Given an instruction and an action sequence {a0, a1, . . . , aT }, we mask randomly selected timestep {0, . . . , }, replacing at with [MASK]. This produces queryoutput pair: = (L, {a0, . . . , at1, [MASK], at+1, . . . , aT }), = (z, at), where is the masked trajectory with its instruction, and contains the missing action at along with reasoning trace z. The reasoning traces, generated by GPT-4o, explain why at is correct, providing explicit supervision that strengthens both prediction accuracy and interpretability. Action Sequence Reordering. Here, an action sequence {a0, a1, . . . , aT } is randomly shuffled into permuted sequence {am0, am1 , . . . , amT }. The queryoutput pairs are organized as: = (L, {am0 , am1 , . . . , amT }), = (z, {a0, a1, . . . , aT }), where contains the permuted sequence and instruction, and provides the correctly ordered sequence with reasoning trace from GPT-4o. These traces explain the correct temporal order, helping the model capture action dependencies and temporal coherence. For EB-Manipulation, we curate environment-anchored priors that emphasize spatial understanding, critical component towards fine-grained control. Using simulated episodes from the VLMbench training set (Zheng et al., 2022), we combine image observations with ground-truth 3D coordinates to construct three complementary subsets based on the question type: absolute coordinate grounding, relative coordinate grounding, and combined grounding. Detailed dataset descriptions are provided in Appendix G.3. Absolute Coordinate Grounding. This dataset builds direct mappings between objects and their 3D coordinates. Two query formats are included: (1) given scene image (with robot arm and various objects on desk) and an object description, predict its coordinates; or (2) given scene and coordinate, describe the object at that location. Relative Coordinate Grounding. This subset captures spatial relations such as the leftmost and the second leftmost. Each query pairs an image with relational description, and the output is the coordinate of the corresponding object, allowing the agent to learn spatial reasoning and relational grounding. Combined Grounding. This subset combines both absolute and relative grounding information within each query to enable joint reasoning. Each data sample pairs an observation with coordinate and spatial relation (e.g., Is the object at [42, 11, 17] the leftmost?). The response is binary (Yes or No), allowing models to learn both coordinate grounding and relational verification. 6 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning"
        },
        {
            "title": "4.1.3 External Knowledge Priors",
            "content": "While environment-anchored priors enrich embodied agents with knowledge tied to their task environments, they remain limited in scale compared to the massive datasets commonly used in general LLM/VLM training. This raises natural question: can broader datasets, curated outside embodied environments, provide complementary supervision for embodied agent learning? To explore this, we define external knowledge priors: large-scale datasets that potentially transfer general reasoning and cross-domain grounding capabilities from out-of-environment data, thereby enabling agents to generalize beyond what environment-specific data alone can provide. For high-level planning tasks such as EB-ALFRED, we investigate whether external reasoning datasets can strengthen agents planning and logical reasoning abilities. Specifically, we adopt the OpenO1-SFT dataset (Open O1 Team, 2024), text-based corpus designed to activate chain-of-thought reasoning. OpenO1-SFT is multilingual and contains 77,685 examples. From its English subset, we sample 10,000 QA pairs to construct our external knowledge prior dataset. For low-level control tasks such as EB-Manipulation, we examine whether external spatial reasoning datasets can improve agents visual perception and spatial understanding. To this end, we utilize the SpaceThinker dataset (Remyx AI, 2025), synthesized multimodal spatial reasoning corpus. We use all 11,413 QA pairs from this dataset to curate the corresponding external knowledge prior. By incorporating these external knowledge priors, we complement environment-anchored supervision with large-scale reasoning and grounding signals. This combination equips VLMs with richer generalization capabilities that extend beyond the limitations of purely embodied data."
        },
        {
            "title": "4.2 Online Reinforcement Learning",
            "content": "While Embodied Prior Learning (4.1) equips agents with essential foundational skills, the dynamic nature of embodied tasks requires going beyond static priors to achieve adaptive planning, reasoning, and reflection. The natural next stage is online reinforcement learning (RL), where agents interact with the environment through trial and error, gradually discovering how to best leverage and refine their priors for autonomous task completion. However, applying RL in embodied agents introduces three major challenges: (1) Efficient VLM-based agent design: due to the high computational cost of RL training, it is crucial to build an efficient agent framework that manages training overhead through careful design choices, such as context management. (2) Long-horizon tasks with sparse rewards: tasks often require many steps (e.g., 20+) to complete, making it difficult for RL to propagate meaningful supervision signals, even when prior knowledge is available. (3) Stable RL optimization: conventional RL methods for LLMs/VLMs tend to be unstable in embodied agent settings, since token-level value functions capture limited semantic meaning and are not suitable for effective agent policy optimization. To address these challenges, we introduce our online RL training pipeline with three key components: (1) an efficient agent framework that incorporates structured observation description, reflection, planning, and selfsummarization context management; (2) process-level reward function that provides fine-grained supervision at each step; and (3) turn-level advantage value estimation for PPO, which aligns better with agent-level decision-making and delivers more stable training compared to existing token-level or bi-level methods."
        },
        {
            "title": "4.2.1 Agent Framework",
            "content": "We design unified VLM-based embodied agent pipeline, illustrated in Figure 3(a). This pipeline provides powerful framework for processing multimodal inputs, generating structured reasoning and reflection, and producing executable actions. The agent processes three types of inputs: language instructions, visual observations, and interaction history. For visual perception, the agent takes the current step image as input, following prior works agent design (Yang et al., 2025; Xu et al., 2024). Interaction history is maintained by context manager that integrates environment feedback with the agents previous reasoning traces and actions. Below, we describe the key components of our agent framework. Structured Reasoning and Action. At each turn, the agent generates response that combines reasoning trace with an executable action, separated by special tokens: <think start> . . . <think end> for 7 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Figure 3 (a) Our agent framework, and (b) comparison of advantage estimation in turn-level GAE and token-level GAE. A0,0, . . . , AT,n, and A0, . . . , AT refer to Advantage values for each response token. , zref , zplan reasoning, and <action start> . . . <action end> for actions. This ReAct-style (Yao et al., 2023) format leverages the reasoning ability of VLMs to improve decision-making, while also ensuring that actions can be directly parsed by downstream modules. The reasoning trace is further structured into three components, zt = {zvis is reflection on past actions, environment feedback, and progress toward the goal, and zplan is multi-step plan that guides subsequent actions. The action enclosed between action tokens is formatted according to environment requirements: either as an action string for high-level planning tasks (EB-ALFRED) or as 7-dimensional vector for low-level control (EB-Manipulation). This structured design elicits the reasoning capability of foundation models while enabling fine-grained optimization during agent training. is natural language description of the current visual input, zref }, where zvis t Self-Summarization Context Management. The context manager organizes historical information and feeds it into the model to ensure continuity of reasoning. key challenge in long-horizon tasks is the context explosion problem: naively retaining the full history of agent outputs (reasoning and action) and environment feedback, ht = (z1, a1, e1, . . . , zt1, at1, et1), causes input length to grow linearly with turn number t, i.e., O(t). This is computationally inefficient and may harm performance by diverting attention to irrelevant history. Sliding-window approaches are common workaround, but their window sizes are often chosen heuristically rather than principled. In our framework, trajectory-augmented priors train the model to explicitly summarize the interaction history into reflection at each step. This design allows the agent to compress the entire past into its most recent thought zt1 and we only need one-step context ht = (zt1, at1, et1), effectively reducing context size to O(1) while retaining essential information. We refer to this lightweight mechanism as Self-Summarization. It enables efficient long-horizon reasoning by maintaining crucial context without being hindered by non-critical historical details."
        },
        {
            "title": "4.2.2 Reward Design",
            "content": "Embodied tasks are typically long-horizon (e.g., 20 steps) and often suffer from sparse supervision, where rewards are only given upon task success. To provide richer learning signals, we design process-level reward function rt that integrates task completion, intermediate progress, and behavior shaping. At each turn t, the reward is defined as rt = rsuccess + rsubgoal + rbehavior , with the three components described below and further detailed in Appendix D.1.2. Success-based Reward (rsuccess completed before reaching the maximum step limit, and zero otherwise. ). This sparse reward is assigned positive value when the task is successfully Subgoal-based Reward (rsubgoal ). To provide denser feedback for reinforcement learning, subgoal rewards are granted the first time the agent achieves rule-defined subgoals. For high-level planning tasks, subgoals correspond to satisfying one of the conditions specified in the Planning Domain Definition Language (PDDL) 8 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning configuration of the simulator. For low-level manipulation tasks, subgoals are defined as the first successful approach of the end-effector to the instruction-referenced object within predefined distance threshold. Behavior-Shaping Reward (rbehavior ). This component shapes task-specific behaviors by rewarding desirable actions and penalizing undesirable ones. For high-level planning, penalties are applied to invalid actions that the environment cannot execute (e.g., attempting to pick up an object while already holding another, or issuing an erroneous action string). For low-level manipulation, rewards are based on the accuracy of the agents visual grounding, quantified by the ratio of correctly matched attributes against the ground truth. Thresholds on this ratio determine positive or negative rewards. Full implementation details are provided in Appendix D.1.2."
        },
        {
            "title": "4.2.3 Turn-level Policy Optimization",
            "content": "Conventional token-level optimization, widely adopted in RLHF and recent agent RL works (Wu et al., 2025a; Kim et al., 2025), is not well-suited for multi-turn embodied agents. In embodied tasks, interactions and rewards are inherently defined at the turn level. Learning value function for individual reasoning or action tokens is therefore less meaningful and often leads to unstable policy optimization. To address this challenge, we propose turn-level policy optimization scheme, where the agents entire response in turn is treated as single action. At each turn t, rather than estimating values for every token, we pass only the state input xt (including current observation, task instruction, and interaction history) to the value function Vϕ to obtain single estimate Vϕ(xt). Given turn-level rewards {rt}T t=0 for trajectory of + 1 turns, we compute the temporal-difference (TD) residual for each turn: δt = rt + γVϕ(xt+1) Vϕ(xt), with terminal bootstrap Vϕ(xT +1) = 0. The turn-level generalized advantage estimate (GAE) is then calculated as: (cid:88) (γλ)l δt+l, At = l= (1) where λ controls the biasvariance trade-off. This advantage estimate At is shared across all tokens within the response yt, ensuring that credit assignment aligns with the natural unit of environment interaction. Figure 3(b) illustrates the difference between turn-level and token-level value estimation. We perform parallel rollouts with multiple environments and collect an online buffer of turn-level stateresponse pairs for PPO updates. The policy objective is defined as Lpolicy(θ) = E(xt,yt)D 1 yt yt (cid:88) i=1 min (cid:18) πθ(yt,i xt) πθold (yt,i xt) At, clip (cid:18) πθ(yt,i xt) πθold(yt,i xt) (cid:19) (cid:19) , 1 ϵ, 1 + ϵ At , (2) where πθ and πθold are the current and old policies, yt,i is the i-th token in response yt, and ϵ is the clipping threshold. Note that At is shared by all tokens of yt. The value function is trained concurrently by regressing toward detached target: Lvalue(ϕ) = ExtD (cid:0)Vϕ(xt) no grad (At + Vϕ(xt)) (cid:1)2(cid:21) . (cid:20) 1 2 (3) Overall, this turn-wise formulation reduces variance in advantage estimation and leads to more stable policy learning for embodied agents. An empirical comparison between token-level and turn-level GAE is presented in Section 5.4."
        },
        {
            "title": "5 Experiments",
            "content": "We conduct comprehensive experiments on both high-level planning and low-level manipulation tasks, aiming to gain deeper insights into how different design choices contribute to embodied agent learning. Specifically, we address the following research questions: 1 Q1: What performance does ERA achieve compared to strong baselines? 2 Q2: What role do different prior datasets play in agent performance? 9 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Figure 4 Examples of ERA performing step-by-step reasoning and actions: (a) on EB-ALFRED, it identifies and reflects on earlier errors; (b) on EB-Manipulation, it accurately places the star into the correct slot of the shape sorter. ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Table 3 Task success rates on the five subsets of EB-ALFRED and EB-Manipulation. The best result in each column is highlighted in bold. Base, Complex, and Visual are seen subsets, while Common and Spatial are unseen subsets. Model EB-Alfred EB-Manipulation Avg Base Complex Visual Common Spatial Avg Base Complex Visual Common Spatial GPT-4o Claude-3.5-Sonnet Gemini-1.5-Pro Gemini-2.0-flash Llama-3.2-90B-Vision InternVL3-78B Qwen2.5-VL-72B-Ins Qwen2.5-VL-7B-Ins Qwen2.5-VL-3B-Ins RL4VLM (3B) VAGEN (3B) Reinforced Reasoner (7B) Robot-R1 (7B) ERA-3B (EPL-only) ERA-3B (EPL+RL) 56.8 66.4 63.2 51.2 35.2 39. 40.8 5.2 0 51.2 52.8 41. - 56.0 65.2 64 72 62 38 38 50 10 70 70 54 - 68 68 76 72 54 44 42 6 0 70 70 - 66 72 Prompting-based MLLMs 46 58 46 28 42 36 0 54 66 64 48 34 42 8 0 52 52 46 32 38 34 0 Training-based MLLMs 56 58 28 - 52 62 32 38 42 - 44 54 28 28 38 - 50 66 28.9 25.4 21.1 16. 14.9 26.3 16.2 9.6 0 21. 22.9 - 39.6 37.5 14.6 14. 10.4 29.2 12.5 8.3 0 33. 35.4 - 11.7 12.5 40.0 48. 45.8 56.3 29.2 29.2 22.9 14. 16.7 22.9 16.7 8.3 0 29. 31.3 - 6.3 41.7 47.9 19. 19.4 16.7 13.9 10.4 25.0 22. 5.6 0 30.6 29.2 - 16. 47.9 50.0 29.2 16.7 14.6 8. 12.5 22.9 12.5 8.3 0 8. 8.3 - 8.3 37.5 47.9 25. 22.9 35.4 31.3 20.8 31.3 18. 16.7 0 8.3 10.4 - 14. 27.1 39.6 3 Q3: Is Self-Summarization effective for context management? 2 Q4: How do reward design and turn-level value impact RL performance? Experiment Setup. We build ERA on top of Qwen2.5-VL-3B-Instruct and evaluate models on EmbodiedBench, comprehensive benchmark covering both high-level planning (EB-ALFRED) and low-level control (EBManipulation). The benchmark includes diverse subsets designed to assess distinct embodied capabilities. To evaluate both in-distribution learning and out-of-distribution generalization, we use three subsets, i.e., base skills (Base), complex instruction following (Complex), and visual perception (Visual), for training (seen), and hold out commonsense reasoning (Common) and spatial awareness (Spatial) subsets for testing (unseen) ERA and other training-based baselines. We report task success rate as the primary evaluation metric. Additional details on dataset curation, training hyperparameters, and evaluation procedures are provided in Appendix E."
        },
        {
            "title": "5.1 Q1: What performance does ERA achieve compared to strong baselines?",
            "content": "In Table 3, we compare ERA-3B against diverse set of baselines. These include prompting-based models, such as GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro, Gemini-2.0-Flash, Llama-3.2-90B-Vision-Ins (Patterson et al., 2022), InternVL3-78B (Zhu et al., 2025), and Qwen2.5-VL (3B, 7B, and 72B) (Bai et al., 2025). We also evaluate RL-based models, including reproduced RL4VLM (Zhai et al., 2024) and VAGEN (Wang* et al., 2025), which are trained from our best EPL checkpoint and fine-tuned with identical setting as ERA, ensuring fair comparison of the RL stage. In addition, we report results for Reinforced Reasoner (Wu et al., 2025a) on EB-ALFRED and Robot-R1 (Kim et al., 2025) on EB-Manipulation, where scores are taken from their original papers. We also present two trajectory examples in Figure 4, which demonstrate that ERA exhibits sophisticated long-term reasoning, planning, and action execution across both tasks. Overall Results. ERA establishes new state-of-the-art among training-based agents, substantially outperforming previous RL baselines on both high-level planning (+12.4% over VAGEN) and low-level manipulation 11 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Table 4 Ablation results on different prior datasets. We report average success rates on both seen and unseen splits. Traj-Aug, Env-Anc, and Ext-Know denotes Trajectory-Augmented, Environment-Anchored, and External Knowledge Priors. Stage 1 and Stage 2 correspond to EPL and RL, respectively. Numbers in parentheses indicate gains over the raw trajectory baseline. Methods Stage 1 Stage 1 & 2 Stage 1 Stage 1 & 2 EB-ALFRED EB-Manipulation Seen Unseen Seen Unseen Seen Unseen Seen Unseen Base Model (No prior injected) Raw Trajectory (baseline) 59. 32.0 0 64.0 0 36. 25.0 7.3 0 44. 0 21.9 + Traj-Aug + Env-Anc + Ext-Know 62.0 (+2.7) 37.0 (+5.0) 66.7 (+2.7) 49.0 (+13.0) 41.4 (+16.4) 26.1 (+18.8) 50.3 (+6.3) 35.5 (+13.6) 63.3 (+4.0) 39.0 (+7.0) 70.0 (+6.0) 42.0 (+6.0) 25.0 (+0.0) 9.4 (+2.1) 47.2 (+3.2) 22.9 (+1.0) 63.3 (+4.0) 35.0 (+3.0) 68.7 (+4.7) 46.0 (+10.0) 30.3 (+5.3) 18.8 (+11.5) 48.5 (+4.5) 27.1 (+5.2) + Traj-Aug + Env-Anc 62.0 (+2.7) 47.0 (+15.0) 68.7 (+4.7) 60.0 (+24.0) 45.1 (+20.1) 32.3 (+25.0) 51.4 (+7.4) 43.8 (+21.9) + Traj-Aug + Ext-Know 63.3 (+4.0) 44.0 (+12.0) 68.7 (+4.7) 55.0 (+19.0) 37.9 (+12.9) 31.3 (+24.0) 51.4 (+7.4) 37.5 (+15.6) (+25.4% over VAGEN). ERA achieves average success rates of 65.2% on EB-ALFRED and 48.3% on EBManipulation, exceeding proprietary models such as GPT-4o by 8.4 and 19.4 points, respectively. Remarkably, these results are obtained with compact 3B model, underscoring the parameter efficiency of ERA for embodied agents. On EB-ALFRED, ERA-3B is also competitive with the top-performing large proprietary model Claude-3.5-Sonnet (66.4%). Unseen Task Generalization. On EB-ALFRED, RL4VLM and VAGEN perform comparably to ERA on the three seen subsets but fall far behind on the two unseen subsets. For example, ERA achieves 66% on the Spatial subset, outperforming VAGEN (28%) by 38 points. Similar patterns are observed on EB-Manipulation. These results show that ERA learns robust and transferable skills rather than overfitting to the training tasks. Benefits of EPL and RL in ERA. The results further highlight the complementary roles of EPL and RL in ERA. EPL alone provides strong foundation, reaching success rates of 56.0% on EB-ALFRED and 40.0% on EB-Manipulation. Adding the RL stage yields substantial gains, improving average success rates by 9.2 and 8.3 points, respectively. These improvements are especially pronounced on unseen subsets, with average gains of 13.0 points on EB-ALFRED and 11.5 points on EB-Manipulation. Together, these findings confirm that EPL imparts essential foundational knowledge, while online RL effectively refines the policy to boost generalization."
        },
        {
            "title": "5.2 Q2: What role do different prior datasets play in agent performance?",
            "content": "To evaluate the impact of different prior datasets, we conducted an ablation study on performance using various prior datasets both after stage 1 (i.e., EPL) and after stages 1 & 2 (i.e., EPL+RL), as reported in Table 4. Without embodied prior learning, direct RL training from the 3B base model completely fails, yielding scores of 0 on all tasks. Incorporating raw trajectory data substantially improves performance, but generalization to unseen tasks remains limited. In contrast, EPL with each prior dataset consistently improves performance over the raw-trajectory baseline in both stages and significantly reduces the seenunseen gap. Below, we summarize the key findings. Trajectory-Augmented Priors Achieve the Largest Individual Gains in Generalization. Among the three individual datasets, trajectory augmentation yields the strongest improvements on unseen tasks. On EB-ALFRED, augmenting raw trajectories with structured reasoning improves unseen performance by +13.0% after Stage 2, relative to the raw-trajectory baseline. The effect is even stronger on EB-Manipulation, with +13.6% gain on unseen tasks. In comparison, Environment-Anchored and External Knowledge Priors achieve relatively modest improvements of +6.0%/+10.0% on EB-ALFRED (unseen) and +1.0%/+5.2% on EB-Manipulation (unseen). These results highlight the importance of structured reasoning in enhancing transfer to novel tasks. Environment-Anchored Priors Improve Seen and Unseen Tasks Equally, While External Knowledge Priors Favor 12 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Figure 5 Performance correlation between stage 1 (EPL) and stage 2 (EPL+RL) on both seen and unseen sets. The Pearson correlation coefficient is reported in the top-left corner. Figure 6 (a)(b) Reward design ablations and (c)(d) Value estimation method comparisons in ERA. We report average success rates on the unseen subsets of EB-ALFRED and EB-Manipulation. Unseen Tasks. Environment-Anchored Priors produce balanced improvements across seen and unseen subsets. For instance, on EB-ALFRED they deliver +6% gain over the raw-trajectory baseline for both seen and unseen tasks after stage 2. This consistency suggests that environment-anchored data encode environment-level knowledge that is useful across tasks in similar environments. In contrast, External Knowledge Priors lead to larger improvements on unseen tasks than on seen ones. This indicates that while external data are less environment-specific, they capture general reasoning and grounding skills that support generalization to novel tasks. However, their overall gains remain smaller than trajectory augmentation. Combining Trajectory-Augmented and Environment-Anchored Priors Elicit the Best Performance. We next examine whether combining different priors leads to further gains. Notably, the combination of Trajectory-Augmented and Environment-Anchored Priors achieves the strongest overall results: after stages 1 and 2, performance reaches 60% on the unseen subsets of EB-ALFRED (+24% over baseline) and 43.8% on the unseen subsets of EB-Manipulation (+21.9% over baseline). While combining Trajectory-Augmented and External Knowledge Priors also produces substantial improvements, the gains are relatively smaller. These findings suggest that environment-anchored data provide explicit, task-relevant supervision that complements trajectory-based priors, whereas external knowledge offers weaker but more easily obtainable alternative when environmentspecific data are costly to curate. Performance Correlation between EPL and RL. Figure 5 illustrates the relationship between model performance after stage 1 (EPL) and stage 2 (EPL+RL) across different prior datasets. Using the Pearson correlation coefficient, we observe an often near-linear correlation between the two stages, with correlation coefficients ranging from 0.88 to 0.97. This finding indicates that models achieving higher performance during the EPL stage tend to maintain or even amplify their advantage after RL fine-tuning. Such trend indicates that EPL performance is reliable predictor of final RL outcomes, emphasizing the crucial role of embodied prior learning. In other words, investing in stronger prior learning substantially improves the downstream effectiveness of reinforcement learning, leading to more capable and generalizable embodied agents. 13 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning"
        },
        {
            "title": "5.3 Q3: Is Self-Summarization Effective for Context Management?",
            "content": "History Length EB-ALFRED EB-Manipulation SR (%) #Input Tokens Table 5 Comparison of average success rate (SR) and average input tokens with varying number of history steps included in the context. SR is averaged over unseen subsets. To evaluate the effectiveness of the proposed self-summarization mechanism, we conduct an ablation study in Table 5 comparing unseen task performance with and without selfsummarization after Stage 1. The key difference is that the w/o Self-Summarization setting excludes the models generated reflection from the context, while retaining other history information identical to the selfsummarization setting. Overall, interaction history plays crucial role in embodied agents. Removing it causes success rates to drop drastically to 2% and 3.1% on EB-ALFRED and EB-Manipulation, respectively. Incorporating self-summarization substantially improves performance, yielding an 810% increase across different context lengths. Notably, with self-summarization, using only one-step history already outperforms the 35 step history variants, while requiring fewer tokens, likely due to the distraction introduced by redundant information. These findings demonstrate that concise summary generated by the model can provide more efficient history representation, enabling the agent to proactively focus on relevant context without being hindered by lengthy histories. 1 step (Ours) 3 steps 5 steps SR (%) #Input Tokens 1 step 3 steps 5 steps w/o Self-Summarization w/ Self-Summarization 217.4 490.5 680. 157.0 332.7 455.8 305.3 564.4 694.5 399.5 798.3 998.3 32.3 30.3 29.1 29.2 24.4 25.4 No History 47 45 46 40 38 37 0 step 179 3."
        },
        {
            "title": "5.4 Q4: How do reward design and turn-level value impact RL performance?",
            "content": "To assess the effect of two key RL design choices, we conduct ablations in Figure 6, reporting average success rates on unseen subsets of EB-ALFRED and EB-Manipulation. All comparisons are trained under identical settings as ERA, ensuring fair comparison. Synergistic Dense Reward Improves Long-Horizon RL. Reward sparsity poses major challenge for credit assignment, particularly in long-horizon tasks. As shown in Figure 6, supplementing sparse success-based rewards with two process-level signals (subgoal-based and behavior-shaping rewards) substantially improves performance, particularly for high-level planning tasks. On EB-ALFRED, the average success rate rises by 14% (46% 60%) compared to training with only success-based rewards. In contrast, the gain on the shorter-horizon EB-Manipulation benchmark is modest (+2.1%). This disparity shows that dense rewards are especially critical for long-horizon tasks, where they can guide exploration and stabilize credit assignment. Moreover, using only subgoal-based or only behavior-shaping rewards produces limited gains, highlighting the synergistic effect of combining multiple dense reward signals. Turn-Level Value Estimation Enhances Policy Learning. We also compare three value learning schemes: token-level, bi-level (Wang* et al., 2025), and our turn-level GAE. Token-level and bi-level approaches require learning value function over individual tokens, distributing fine-grained credit across reasoning and action tokens. In contrast, turn-level GAE treats the entire response as single action and learns turn-level value function. Results show that turn-level GAE achieves the strongest performance, improving average success rates by 12 points on EB-ALFRED (48% 60%) and by 8.4 points on EB-Manipulation (35.4% 43.8%) compared to token-level GAE. Bi-level GAE offers modest gains over token-level GAE, likely because it incorporates partial turn-level credit assignment, but its reliance on unstable token-level value estimation still limits effectiveness. These results validate that turn-level credit assignment yields more stable policy learning."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we propose two-stage framework for transforming VLMs into capable embodied reasoning agents. In the first stage, we show that enriching existing trajectory data with structured reasoning, incorporating environment-anchored supervision, and leveraging external knowledge each improve agent performance, with 14 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning their combination yielding even greater gains. In the second stage, we highlight the importance of careful design choices in context management, dense reward shaping, and turn-level value learning, providing insights into the factors that drive effective RL for embodied agents. These contributions establish practical and scalable recipe for developing more powerful and efficient VLM-based agents in more realistic settings."
        },
        {
            "title": "Acknowledgments",
            "content": "This material is based upon work supported partially by NSF under Grant No. 2416897, Grant No. 2505932, and by ORN under Grant No. N000142512318. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the funding agencies. This research used both Delta (NSF award OAC 2005572) and DeltaAI (NSF award OAC 2320345) advanced computing systems, as well as TAAC computing resources via NAIRR Pilot NAIRR250157. Huan Zhang acknowledges the support of the University Research Program (URP) from Toyota Research Institute (TRI). This research reflects solely the opinions and conclusions of its authors, not those of TRI or any other Toyota entity."
        },
        {
            "title": "References",
            "content": "Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Foundation models defining new era in vision: survey and outlook. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. Hongye Cao, Fan Feng, Jing Huo, and Yang Gao. Causal action empowerment for efficient reinforcement learning in embodied agents. Science China Information Sciences, 68(5):150201, 2025. Guo Chen, Zhiqi Li, Shihao Wang, Jindong Jiang, Yicheng Liu, Lidong Lu, De-An Huang, Wonmin Byeon, Matthieu Le, Tuomas Rintamaki, et al. Eagle 2.5: Boosting long-context post-training for frontier visionlanguage models. arXiv preprint arXiv:2504.15271, 2025a. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025b. Yaran Chen, Wenbo Cui, Yuanwen Chen, Mining Tan, Xinyao Zhang, Dongbin Zhao, and He Wang. Robogpt: an intelligent agent of making embodied long-term decisions for daily instruction tasks, 2024a. URL https://arxiv.org/abs/2311.15649. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. Zhili Cheng, Yuge Tu, Ran Li, Shiqi Dai, Jinyi Hu, Shengding Hu, Jiahao Li, Yang Shi, Tianyu Yu, Weize Chen, et al. Embodiedeval: Evaluate multimodal llms as embodied agents. arXiv preprint arXiv:2501.11858, 2025. Jae-Woo Choi, Youngwoo Yoon, Hyobin Ong, Jaehong Kim, and Minsu Jang. Lota-bench: Benchmarking language-oriented task planners for embodied agents. arXiv preprint arXiv:2402.08178, 2024. Matt Deitke, Aniruddha Kembhavi, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. RoboTHOR: An open simulation-to-real embodied ai platform. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 15 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv e-prints, pp. arXiv2409, 2024. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: an embodied multimodal language model. In Proceedings of the 40th International Conference on Machine Learning, pp. 84698488, 2023. Lang Feng, Weihao Tan, Zhiyi Lyu, Longtao Zheng, Haiyang Xu, Ming Yan, Fei Huang, and Bo An. Towards efficient online tuning of vlm agents via counterfactual soft reinforcement learning. arXiv preprint arXiv:2505.03792, 2025a. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training, 2025b. URL https://arxiv.org/abs/2505.10978. Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, and Dorsa Sadigh. Physically grounded vision-language models for robotic manipulation, 2024. URL https:// arxiv.org/abs/2309.02561. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, and Yang Gao. Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning, 2023. URL https://arxiv.org/abs/2311.17842. Jialei Huang, Shuo Wang, Fanqi Lin, Yihang Hu, Chuan Wen, and Yang Gao. Tactile-vla: Unlocking visionlanguage-action models physical knowledge for tactile generalization, 2025. URL https://arxiv.org/abs/ 2507.09160. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022. Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973, 2023. Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, Xinda Xue, Qinghang Su, Huaihai Lyu, Xiaolong Zheng, Jiaming Liu, Zhongyuan Wang, and Shanghang Zhang. Robobrain: unified brain model for robotic manipulation from abstract to concrete, 2025. URL https://arxiv.org/abs/2502.21257. Byeonghwi Kim, Jinyeon Kim, Yuyeong Kim, Cheolhong Min, and Jonghyun Choi. Context-aware planning and environment-aware memory for instruction following embodied agents, 2024a. URL https://arxiv.org/ abs/2308.07241. 16 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Dongyoung Kim, Sumin Park, Huiwon Jang, Jinwoo Shin, Jaehyung Kim, and Younggyo Seo. Robot-r1: Reinforcement learning for enhanced embodied reasoning in robotics. arXiv preprint arXiv:2506.00070, 2025. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024b. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An interactive 3d environment for visual AI. In arXiv preprint arXiv:1712.05474, 2017. Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, et al. Molmoact: Action reasoning models that can reason in space. arXiv preprint arXiv:2508.07917, 2025. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024b. Yifan Li, Zhixin Lai, Wentao Bao, Zhen Tan, Anh Dao, Kewei Sui, Jiayi Shen, Dong Liu, Huan Liu, and Yu Kong. Visual large language models for generalized and specialized applications. arXiv preprint arXiv:2501.02765, 2025a. Yun Li, Yiming Zhang, Tao Lin, XiangRui Liu, Wenxiao Cai, Zheng Liu, and Bo Zhao. Sti-bench: Are mllms ready for precise spatial-temporal world understanding? arXiv preprint arXiv:2503.23765, 2025b. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control, 2023. URL https://arxiv.org/ abs/2209.07753. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Mingxian Lin, Wei Huang, Yitang Li, Chengjie Jiang, Kui Wu, Fangwei Zhong, Shengju Qian, Xin Wang, and Xiaojuan Qi. Embrace-3k: Embodied reasoning and action in complex environments. arXiv preprint arXiv:2507.10548, 2025. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https://llava-vl.github.io/blog/ 2024-01-30-llava-next/. Huaping Liu, Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, and Hanbo Zhang. Towards generalist robot policies: What matters in building vision-language-action models. 2025a. Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, and Yu Wang. What can rl bring to vla generalization? an empirical study. arXiv preprint arXiv:2505.19789, 2025b. Xiao Liu, Tianjie Zhang, Yu Gu, Iat Long Iong, Yifan Xu, Xixuan Song, Shudan Zhang, Hanyu Lai, Xinyi Liu, Hanlin Zhao, et al. Visualagentbench: Towards large multimodal models as visual foundation agents. arXiv preprint arXiv:2408.06327, 2024b. 17 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, and Ziwei Wang. Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning. arXiv preprint arXiv:2505.18719, 2025a. Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, and Ziwei Wang. Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning, 2025b. URL https://arxiv.org/abs/2505.18719. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Feng Luo, Rui Yang, Hao Sun, Chunyuan Deng, Jiarui Yao, Jingyan Shen, Huan Zhang, and Hanjie Chen. Rethinking diverse human preference learning through principal component analysis. arXiv preprint arXiv:2502.13131, 2025. Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, and Irwin King. survey on vision-language-action models for embodied ai. arXiv preprint arXiv:2405.14093, 2024. Drew McDermott, Malik Ghallab, Adele Howe, Craig Knoblock, Ashwin Ram, Manuela Veloso, Daniel Weld, and David Wilkins. Pddlthe planning domain definition language. Technical report, Yale Center for Computational Vision and Control / Yale University / DCS, 1998. Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in Neural Information Processing Systems, 36, 2024. Open O1 Team. Open o1, 2024. Dataset. OpenAI. Hello gpt-4o, 2024. URL https://openai.com/index/hello-gpt-4o/. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. David Patterson, Joseph Gonzalez, Urs Holzle, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. The carbon footprint of machine learning training will plateau, then shrink. Computer, 55(7):1828, 2022. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC), pp. 116. IEEE, 2020. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Remyx AI. Spacethinker: Synthetic spatial reasoning traces (vqasynth). Hugging Face dataset, 2025. URL https://huggingface.co/datasets/remyxai/SpaceThinker. Elodie Rohmer, Surya Singh, and Daniel Freese. Coppeliasim (formerly v-rep): versatile and scalable robot simulation framework. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2013. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Jingyan Shen, Jiarui Yao, Rui Yang, Yifan Sun, Feng Luo, Rui Pan, Tong Zhang, and Han Zhao. Micro: Mixture modeling and context-aware routing for personalized preference learning. arXiv preprint arXiv:2505.24846, 2025. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 12791297, 2025. Suyeon Shin, Sujin jeon, Junghyun Kim, Gi-Cheon Kang, and Byoung-Tak Zhang. Socratic planner: Selfqa-based zero-shot planning for embodied instruction following, 2025. URL https://arxiv.org/abs/ 2404.15190. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. Junyang Shu, Zhiwei Lin, and Yongtao Wang. Rftf: Reinforcement fine-tuning for embodied agents with temporal feedback. arXiv preprint arXiv:2505.19767, 2025. Tom Silver, Soham Dan, Kavitha Srinivas, Joshua Tenenbaum, Leslie Kaelbling, and Michael Katz. Generalized planning in pddl domains with pretrained large language models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pp. 2025620264, 2024. Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with graph visual question answering. In European conference on computer vision, pp. 256274. Springer, 2024. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models, 2022. URL https://arxiv.org/abs/2209.11302. Chan Hee Song, Jiaman Wu, Clayton Washington, Brian Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: In Proceedings of the Few-shot grounded planning for embodied agents with large language models. IEEE/CVF international conference on computer vision, pp. 29983009, 2023. Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. Trial and error: Exploration-based trajectory optimization for llm agents. arXiv preprint arXiv:2403.02502, 2024. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. Jianhai Su and Qi Zhang. Subgoal proposition using vision-language model. In CoRL 2023 Workshop on Learning Effective Abstractions for Planning (LEAP), 2023. Hao Sun. Reinforcement learning in the era of llms: What is essential? what is needed? an rl perspective on rlhf, prompting, and beyond. arXiv preprint arXiv:2310.06147, 2023. Hao Sun, Yunyi Shen, and Jean-Francois Ton. Rethinking reward modeling in preference-based large language model alignment. In The Thirteenth International Conference on Learning Representations, 2025. Qi Sun, Pengfei Hong, Tej Deep Pala, Vernon Toh, Tan, Deepanway Ghosal, Soujanya Poria, et al. Emma-x: An embodied multimodal action model with grounded chain of thought and look-ahead spatial reasoning. arXiv preprint arXiv:2412.11974, 2024. Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Emu: Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023a. ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023b. Andrew Szot, Bogdan Mazoure, Omar Attia, Aleksei Timofeev, Harsh Agrawal, Devon Hjelm, Zhe Gan, Zsolt Kira, and Alexander Toshev. From multimodal llms to generalist embodied agents: Methods and lessons. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1064410655, 2025. Jiaqi Tang, Hao Lu, Ruizheng Wu, Xiaogang Xu, Ke Ma, Cheng Fang, Bin Guo, Jiangbo Lu, Qifeng Chen, and Yingcong Chen. Hawk: Learning to understand open-world video anomalies. Advances in Neural Information Processing Systems, 37:139751139785, 2024. Shulin Tian, Ruiqi Wang, Hongming Guo, Penghao Wu, Yuhao Dong, Xiuying Wang, Jingkang Yang, Hao Zhang, Hongyuan Zhu, and Ziwei Liu. Ego-r1: Chain-of-tool-thought for ultra-long egocentric video reasoning. arXiv preprint arXiv:2506.13654, 2025. Kangrui Wang*, Pingyue Zhang*, Zihan Wang*, Yaning Gao*, Linjie Li*, Qineng Wang, Hanyang Chen, Chi Wan, Yiping Lu, Zhengyuan Yang, Lijuan Wang, Ranjay Krishna, Jiajun Wu, Li Fei-Fei, Yejin Choi, and Manling Li. Reinforcing visual state reasoning for multi-turn vlm agents, 2025. URL https: //github.com/RAGEN-AI/VAGEN. Shuhe Wang, Shengyu Zhang, Jie Zhang, Runyi Hu, Xiaoya Li, Tianwei Zhang, Jiwei Li, Fei Wu, Guoyin Wang, and Eduard Hovy. Reinforcement learning enhanced llms: survey. arXiv preprint arXiv:2412.10400, 2024. Siyin Wang, Zhaoye Fei, Qinyuan Cheng, Shiduo Zhang, Panpan Cai, Jinlan Fu, and Xipeng Qiu. World modeling makes better planner: Dual preference optimization for embodied task planning, 2025. URL https://arxiv.org/abs/2503.10480. Tong Wei, Yijun Yang, Junliang Xing, Yuanchun Shi, Zongqing Lu, and Deheng Ye. Gtr: Guided thought reinforcement prevents thought collapse in rl-based vlm agent training. arXiv preprint arXiv:2503.08525, 2025. Di Wu, Jiaxin Fan, Junzhe Zang, Guanbo Wang, Wei Yin, Wenhao Li, and Bo Jin. Reinforced reasoning for embodied planning. arXiv preprint arXiv:2505.22050, 2025a. Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, et al. Gui-actor: Coordinate-free visual grounding for gui agents. arXiv preprint arXiv:2506.03143, 2025b. Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan. Embodied task planning with large language models. arXiv preprint arXiv:2307.01848, 2023. Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Haoran Tan, Chencheng Jiang, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, et al. Octopus: Embodied vision-language programmer from environmental feedback. In European conference on computer vision, pp. 2038. Springer, 2024a. Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. Regularizing hidden states enables learning generalizable reward model for llms. Advances in Neural Information Processing Systems, 37: 6227962309, 2024b. Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, and Jianshu Chen. Rewards-incontext: Multi-objective alignment of foundation models with dynamic preference adjustment. arXiv preprint arXiv:2402.10207, 2024c. 20 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, et al. Embodiedbench: Comprehensive benchmarking multimodal large language models for vision-driven embodied agents. arXiv preprint arXiv:2502.09560, 2025. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12), 2024. Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1380713816, 2024. Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, and Jianye Hao. Embodied-r1: Reinforced embodied reasoning for general robotic manipulation. arXiv preprint arXiv:2508.13998, 2025. Zhengqing Yuan, Yixin Liu, Yihan Cao, Weixiang Sun, Haolong Jia, Ruoxi Chen, Zhaoxu Li, Bin Lin, Li Yuan, Lifang He, et al. Mora: Enabling generalist video generation via multi-agent framework. arXiv preprint arXiv:2403.13248, 2024. Micha(cid:32)l Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. Robotic control via embodied chain-of-thought reasoning. arXiv preprint arXiv:2407.08693, 2024. Simon Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Peter Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, et al. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. Advances in neural information processing systems, 37:110935110971, 2024. Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025a. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self-training via process reward guided tree search. Advances in Neural Information Processing Systems, 37:6473564772, 2024a. Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, and Jianyu Chen. Up-vla: unified understanding and prediction model for embodied agent. arXiv preprint arXiv:2501.18867, 2025b. Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: survey. IEEE transactions on pattern analysis and machine intelligence, 46(8):56255644, 2024b. Liang Zhang, Anwen Hu, Haiyang Xu, Ming Yan, Yichen Xu, Qin Jin, Ji Zhang, and Fei Huang. Tinychart: Efficient chart understanding with visual token merging and program-of-thoughts learning. arXiv preprint arXiv:2404.16635, 2024c. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024d. Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Shicheng Li, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, et al. Mavis: Mathematical visual instruction tuning with an automatic data engine. arXiv preprint arXiv:2407.08739, 2024e. Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, et al. Embodied-reasoner: Synergizing visual search, reasoning, and action for embodied interactive tasks. arXiv preprint arXiv:2503.21696, 2025c. ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 17021713, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Kaizhi Zheng, Xiaotong Chen, Odest Chadwicke Jenkins, and Xin Eric Wang. VLMbench: compositional benchmark for vision-and-language manipulation. In Proceedings of the NeurIPS Datasets and Benchmarks Track, 2022. Kaizhi Zheng, Xuehai He, and Xin Eric Wang. Minigpt-5: Interleaved vision-and-language generation via generative vokens. arXiv preprint arXiv:2310.02239, 2023. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. In The Thirteenth International Conference on Learning Representations. 22 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning"
        },
        {
            "title": "Appendix Contents",
            "content": "Limitation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Environment Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 Additional Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 Agent Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Embodied Prior Learning Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 Error Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 Case Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 23 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning"
        },
        {
            "title": "A Limitation",
            "content": "A key limitation of this work is that all evaluations are conducted in simulated environments, without validation on real-world systems. This reflects common trade-off in agent research: simulations provide standardized and reproducible benchmarks that greatly reduce time, cost, and safety risks, but they inevitably limit real-world applicability. While such practice is common for LLM/VLM-based agents (Zhai et al., 2024; Feng et al., 2025b), real-world testing remains crucial for practical deployment. As future work, we plan to explore deploying our ERA training pipelines in real-world environments."
        },
        {
            "title": "B Additional Related Work",
            "content": "Vision Language Models. Vision Language Models (VLMs) have been popular research domain with their ability to combine multi-modal perception with strong language backbone. Li et al. (2025a) and other works (Yin et al., 2024; Zhang et al., 2024b; Awais et al., 2025) categorize VLMs into subdomains like vision to text (Chen et al., 2024b; Deitke et al., 2024; Bai et al., 2025; Li et al., 2024a,b; Zhang et al., 2024c), vision to action (Sima et al., 2024), and text to vision (Deng et al., 2025; Chen et al., 2025b; Sun et al., 2023a; Zheng et al., 2023), etc. For vision to action, embodied AI serves as perfect area, as it provides natural environment and action interface. Kim et al. (2024b); Huang et al. (2023); Xu et al. (2024); Wu et al. (2024, 2025b) pioneer the relevant field by converting vision input into executable actions. VLMs for reasoning comprise large portion of the suitable applications. For example, existing works (Zhang et al., 2024d; Lu et al., 2023; Zhang et al., 2024e; Zou et al.) utilize VLMs for solving math problems. Besides, video models emerge as an extension to VLMs as well. Chen et al. (2025a), Zhang et al. (2025a) and Tian et al. (2025) synthesize video information into text for further processing. The interleaving interaction between vision information and text reasoning provides better deployment for VLMs (Yuan et al., 2024; Tang et al., 2024). RL for LLMs or VLMs. Reinforcement Learning from Human Feedback (RLHF) has become cornerstone of modern LLM alignment (Sun, 2023; Wang et al., 2024). Early work such as InstructGPT established the paradigm of training reward model from human preferences and using RL to fine-tune the base model, demonstrating substantial improvements in helpfulness and safety (Ouyang et al., 2022; Stiennon et al., 2020; Yang et al., 2024c). In such framework, reward model (Sun et al., 2025; Yang et al., 2024b; Luo et al., 2025; Shen et al., 2025) plays critical role to modeling human preference and guide policy optimization. To avoid explicitly training reward model, implicit reward models like Direct Preference Optimization (DPO (Rafailov et al., 2023)) have been investigated and successfully applied in scenarios with preference data. Since the success of OpenAI-o1 (Jaech et al., 2024) and DeepSeek-R1 (Guo et al., 2025), reinforcement learning (RL) has been dominant technique for LLMs post-training, especially for reasoning models. To better handle long autoregressive sequences, new algorithms like GRPO and GSPO (Guo et al., 2025; Shao et al., 2024; Zheng et al., 2025) improve stability via group-based comparisons, and offline/self-training methods such as ReST and ReST-MCTS (Gulcehre et al., 2023; Zhang et al., 2024a) reduce online interaction costs by iteratively filtering and retraining on high-quality outputs. Beyond textual alignment, RL is increasingly applied to reasoning (Lightman et al., 2023) and multimodal alignment: LLaVA-RLHF and RLHF-V (Sun et al., 2023b; Yu et al., 2024) demonstrate that preference optimization can mitigate hallucinations and strengthen grounding in visual-language tasks."
        },
        {
            "title": "C Environment Details",
            "content": "In this section, we describe the two evaluation environments used in our study. This complements Section 4 by detailing the task interfaces, action spaces, observations, and simulator feedback. The environments span high-level planning and low-level control: EB-ALFRED, household-task benchmark in AI2-THOR with PDDL-specified subgoals and discrete action set; and EB-Manipulation, CoppeliaSim-based robotic control suite that issues 7-DoF end-effector commands with automatic motion planning. C.1 EB-ALFRED for High-level Planning Task EB-ALFRED is built on the ALFRED dataset and the AI2-THOR simulator (Shridhar et al., 2020; Kolve et al., 2017; Deitke et al., 2020), widely recognized for diverse household tasks and realistic environments 24 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning in embodied AI. The benchmark evaluates an agents ability to plan and execute sequences of high-level actions for tasks such as Put washed lettuce in the refrigerator. Each task is formally represented in the Planning Domain Definition Language (PDDL) (McDermott et al., 1998), enabling precise evaluation of task and subgoal completion. The ALFRED dataset spans seven task types: Pick & Place, Stack & Place, Pick Two & Place, Clean & Place, Heat & Place, Cool & Place, and Examine in Light. Following LoTa-Benchs implementation for household task planning (Choi et al., 2024), the simulator supports eight high-level action primitives: pick up, open, close, turn on, turn off, slice, put down, and find. These actions are parameterized by objects (e.g., find an apple or pick up an apple). The simulator provides both egocentric visual observations and textual feedback, indicating whether an action succeeds or fails (e.g., failure to pick up an object because another object is already being held). C.2 EB-Manipulation for Low-level Control Task EB-Manipulation extends VLMbench (Zheng et al., 2022) using the CoppeliaSim/V-REP simulator (Rohmer et al., 2013) to control 7-DoF Franka Emika Panda robotic arm. It comprises four manipulation categories: (1) Pick & Place Objects, (2) Stack Objects, (3) Shape Sorter Placement, and (4) Table Wiping, each with diverse instances varying in color, position, shape, and orientation. The action space is 7-dimensional vector specifying end-effector translation, rotation, and gripper state. Actions are executed with automatic motion planning in ABS EE POSE PLAN WORLD FRAME mode, which drives the trajectory from the current to the target pose, reducing the agents burden to predicting keypoints essential for task completion."
        },
        {
            "title": "D Implementation Details",
            "content": "This section provides comprehensive overview of the implementation details for our proposed ERA framework, complementing the methodology presented in Section 4 and supporting the reproducibility of the experimental results reported in Section 5. We structure this section into two main parts. First, we elaborate on the Algorithm Details, where we discuss the design rationale behind our EPL training curriculum (D.1.1) and provide detailed breakdown of the reward function used in our RL pipeline (D.1.2). Second, we present the Training Details, which includes complete list of hyperparameters and configurations for both the Embodied Prior Learning stage (D.2.1) and the online Reinforcement Learning stage (D.2.2). D.1 Algorithm Details D.1.1 EPL Training Design Details The sequential training order in EPL is deliberately designed to reflect the hierarchical nature of embodied skill acquisition. Since small VLMs lack environment-grounded perception after generic pretraining, we first fine-tune them on environment-anchored priors to establish spatial grounding, object semantics, and actioncontext associations tied to the embodied environment. This early stage provides the model with stable perceptual foundation for interpreting observations and reasoning about spatial / semantic relations. Next, we train on trajectory-augmented priors, which build upon this foundation by injecting step-level reasoning, temporal coherence, and goal-conditioned planning skills. This stage allows the model to integrate grounded perception with structured reasoning traces, thereby enhancing its ability to perform long-horizon tasks. When external knowledge priors are included, they are placed before trajectory-level training to transfer abstract reasoning and cross-domain grounding capabilities from large-scale text or multimodal corpora. Together, this curriculum aligns with the goal of bridging the perceptionreasoningaction gap, ensuring that embodied agents acquire both generalizable and task-specific priors before online interaction. The configurations of the EPL methods in Table 4 are summarized as follows: Raw Trajectory: trained on the raw trajectory dataset for 2 epochs. Raw Trajectory + Traj-Aug: trained on the trajectory-augmented prior dataset for 2 epochs. 25 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Raw Trajectory + Env-Anc: trained first on the environment-anchored prior dataset for 1 epoch, followed by the raw trajectory dataset for 2 epochs. Raw Trajectory + Ext-Know: trained first on the external knowledge prior dataset for 1 epoch, followed by the raw trajectory dataset for 2 epochs. Raw Trajectory + Traj-Aug + Env-Anc: trained first on the environment-anchored prior dataset for 1 epoch, followed by the trajectory-augmented prior dataset for 2 epochs. Raw Trajectory + Traj-Aug + Ext-Know: trained first on the external knowledge prior dataset for 1 epoch, followed by the trajectory-augmented prior dataset for 2 epochs. D.1.2 RL Reward Design Details To provide dense and informative learning signal that balances final task completion with intermediate progress and behavior shaping, we design the reward function rt at each turn as sum of three components: rt = rsuccess + rsubgoal + rbehavior . The specific values for these components are summarized in Table 6. Below, we provide detailed breakdown of each component with examples and implementation details. Reward Hyperparameters. The numerical values for each reward component are detailed in Table 6. These values were determined through empirical tuning to balance the different learning objectives. Table 6 Hyperparameters for the reward components."
        },
        {
            "title": "Value",
            "content": "Success (rsuccess ) Subgoal (rsubgoal ) High-level (EB-ALFRED) +4.0 Low-level (EB-Manipulation) +3.0 High-level (EB-ALFRED) Low-level (EB-Manipulation) +1.0 per new object approached +1.0 per new subgoal Behavior Shaping (rbehavior ) High-level (EB-ALFRED) Low-level (EB-Manipulation) +0.5 if qt > 0.75; 0.5 if qt < 0.25 0.5 for invalid actions (i) Success-based Reward (rsuccess ): sparse, high-magnitude reward is given upon task completion to serve as the primary optimization objective. For high-level planning, reward of rsuccess = +4.0 is awarded if the tasks goal conditions are met. The episode then terminates. For example, for the instruction wash the apple and put it in the refrigerator, the agent receives this reward only when the environment state confirms that the apples property is isWashed and its location is inside the refrigerator receptacle. For low-level manipulation, reward of rsuccess = +3.0 is given upon successful completion. For instance, if the instruction is to stack block on block B, the reward is granted when the environments physics engine determines that block is in stable state on top of block B, satisfying the goal constraints. (ii) Subgoal-based Reward (rsubgoal ): This component provides dense signal for achieving intermediate steps, guiding the agents exploration. For high-level planning, the environment defines set of subgoals that must be completed. The agent receives reward of rsubgoal = +1.0 each time it achieves new, previously uncompleted subgoal. For example, for the task wash the apple and put it in the refrigerator, key subgoal is changing the apples state to isWashed. When the agent successfully executes the wash action on the apple, it receives +1.0 reward for completing this subgoal for the first time. 26 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning For low-level manipulation, we maintain set of target objects Otarget relevant to the task. The agent is rewarded with rsubgoal = +1.0 the first time its end-effector et enters the vicinity of target object Otarget at position p. This is determined by checking if et p2 < δ, where δ is small distance threshold. To encourage exploration, this reward is granted only once per unique target object within an episode. The logic is detailed in Algorithm 1. (iii) Behavior Shaping Reward (rbehavior at the domain-specific level. ): This component penalizes incorrect behavior and rewards correctness For high-level planning, flawed reasoning can lead to semantically invalid actions. Such actions incur = 0.5. These invalid actions are defined by the environments logical constraints. penalty of rbehavior Examples include: Attempting to Pickup an object when another is already held. Attempting to Put an object in receptacle when not holding that object. Attempting to Open receptacle that is already open. Interacting with an object that is not currently visible. This penalty discourages the agent from taking illogical or impossible actions, thereby improving the coherence of its plans. For low-level manipulation, precise control requires accurate visual perception. We reward or penalize the agent based on the quality of its generated visual description. Let the environment contain objects ordered from left to right, with ground-truth attributes (type, color) given by sequence of tuples (d1, . . . , dN ). If the agents description yields predicted tuples ( ˆd1, . . . , ˆdN ), we define the i=1 1{ ˆdi = di}. If the agent fails to generate parsable description, qt is matching ratio as qt = 1 set to 0 to prevent the agent from omitting the description to avoid penalties. The reward is then assigned based on this ratio: (cid:80)N rbehavior = +0.5 0.5 0 if qt > 0.75 if qt < 0.25 otherwise This reward structure incentivizes the agent to develop robust and accurate visual perception skills. The calculation is detailed in Algorithm 2. Hyperparameter Considerations. The relative magnitudes of the reward components are crucial for effective training. rsuccess completion remains the primary goal. should be larger than any potential cumulative reward from other components to ensure task rsubgoal controls the incentive for making intermediate progress. Its magnitude should be significant enough to guide exploration but not so large as to create local optima where the agent is satisfied with only completing subgoals. The penalties for invalid actions and poor visual descriptions should be calibrated to discourage undesired behaviors without making the agent overly risk-averse, which could stifle exploration. The bonuses for accurate descriptions should provide meaningful incentive but not dominate the subgoal or success rewards. careful tuning of these components is necessary to achieve balance between exploration, behavior shaping, and convergence to successful policies. 27 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Algorithm 1 Pseudocode for Low-Level Subgoal Reward 1: Input: observation, state dictionary target objects approached 2: procedure CheckTargetObjectsApproached(observation, target objects approached) 3: 4: if gripper pose not in observation then return False gripper coords observation.gripper pose[:3] for each obj name, status in target objects approached do if status == 0 then Only check un-approached objects 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for return False 17: 18: end procedure obj info observation.object informations[obj name] obj coords obj info.pose[:3] distance gripper coords obj coords2 if distance 0.2 then target objects approached[obj name] 1 return True end if end if Mark as approached New subgoal achieved No new target object was approached D.2 Training Details D.2.1 Embodied Prior Learning For ERA with the Qwen2.5-VL-3B backbone, we set the input resolution to 500 500 pixels to balance performance and efficiency, and freeze the vision transformer (ViT) to preserve pre-trained visual representations. The maximum sequence length is 8,192 tokens. Training uses the Adam optimizer (Kingma & Ba, 2014) with cosine learning rate schedule and warm-up ratio of 5%. We adopt Embodied Prior Learning (EPL) with batch size of 16 per dataset. The peak learning rate is 1 105. Our implementation builds upon the AGUVIS framework (Xu et al., 2024) and incorporates DeepSpeed optimizations (Rajbhandari et al., 2020), BF16 mixed precision, and gradient checkpointing to reduce memory usage. The EPL-only variant is trained on cluster of H200-140G GPUs, where the 3B model uses 2 nodes and completes training in approximately 2 hours for EB-Manipulation and 5 hours for EB-ALFRED. D.2.2 Reinforcement Learning Our online reinforcement learning stage is implemented using custom framework based on VeRL (Sheng et al., 2025), tailored for training VLM-based embodied agents. We employ the Proximal Policy Optimization (PPO) algorithm. key feature of our framework is its ability to perform large-scale parallel rollouts, where multiple agents interact with distinct environment instances simultaneously to accelerate data collection. The following subsections detail the hyperparameters and training procedures for both high-level (EB-ALFRED) and low-level (EB-Manipulation) tasks. Batching Strategy. crucial aspect of our training setup is the distinction between data collection batching and gradient update batching. Rollout Batch Size refers to the number of parallel environments used for data collection in each rollout phase. For the high-level EB-ALFRED task, we use rollout batch size of 50. For the low-level EB-Manipulation task, we use 48 parallel environments. Each environment instance generates trajectory of up to 30 turns for EB-ALFRED and 15 turns for EB-Manipulation. PPO Mini-Batch and Micro-Batch Size. During the update phase, the trajectory data collected from all parallel rollouts is aggregated. From this buffer, we sample PPO mini-batches of 16 turn-level experiences. 28 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning For distributed training, this mini-batch is further divided into micro-batches. For both tasks, we set the per-GPU micro-batch size to 1, meaning each GPU processes one turn-level sample at time for gradient computation. Policy and Value Network Optimization. For both tasks, the actor (policy) and critic (value) networks are initialized from the weights of the model obtained after the Embodied Prior Learning stage. However, optimization details differ significantly between the high-level and low-level tasks to reflect their distinct challenges. For high-level planning (EB-ALFRED), we use an AdamW optimizer with learning rate of 1 106 for the actor and 1 105 for the critic. Throughout the RL stage, the vision tower of the VLM is kept frozen. This encourages the agent to learn high-level reasoning and planning capabilities based on fixed visual features, as the task depends more on symbolic understanding than on fine-tuning perceptual abilities. For low-level manipulation (EB-Manipulation), the actor learning rate is set to 6 107 and the critic learning rate to 1 105. In contrast to the high-level task, we unfreeze and fine-tune the vision tower for both the actor and the critic. This is critical for low-level control, which demands precise spatial understanding and grounding that can be refined through online interaction with the environment. PPO Hyperparameters. Our PPO implementation is built upon the turn-wise advantage estimation described in Section 4.2.3. Key hyperparameters were configured as follows for both high-level and low-level tasks. The discount factor was set to γ = 0.99 and the GAE parameter to λ = 0.99, placing slight emphasis on near-term rewards while still accounting for long-term consequences. During policy updates, we used clipping ratio of ϵ = 0.2 for the PPO objective. The value function loss was also clipped with range of 0.5. For each batch of rollout data, we performed single update epoch (Nepochs = 1). To encourage exploration and prevent policy collapse, we added an entropy bonus to the actors loss, with coefficient of 0.001. Gradient clipping was applied with norm of 1.0 for both the actor and critic to ensure stable training. While our framework supports KL-divergence regularization against the initial SFT policy to prevent large policy deviations, this feature was disabled in our final experiments. Training Procedure. The online training process is organized into PPO iterations, each consisting of rollout phase and an update phase. We run total of 15 PPO iterations for EB-ALFRED and 50 iterations for EB-Manipulation. To ensure that value estimates are reliable before they are used to compute advantages for policy updates, we employ critic warmup phase. For both tasks, the critic network is trained for 3 iterations on data from an initial rollout while the actors policy is held constant. This stabilization of the value function is crucial for effective and stable PPO training. Totally, we use 2 H200-140GB GPU for RL training with roughly 12 hours for EB-ALFRED and EB-Manipulation."
        },
        {
            "title": "E Additional Experiments",
            "content": "This section provides supplementary experimental results and detailed analyses that complement the main findings presented in Section 5. The goal is to offer more granular understanding of how specific design choices within the ERA framework contribute to overall performance and generalization. We structure our analysis around four key areas. First, we investigate the impact of oracle visual descriptions on low-level control, highlighting the critical role of accurate perception. Second, we dissect the environment-anchored prior dataset to evaluate the individual and combined contributions of its components. Third, we provide more detailed breakdown of our reward design ablation, examining the synergistic effects of different reward signals. Finally, we offer comprehensive comparison of token-level, bi-level, and our proposed turn-level Generalized Advantage Estimation (GAE) schemes, further validating the stability and effectiveness of turn-level credit assignment. E.1 Effect of Rule-based Visual Description for Low-level Control While Embodied Prior Learning (EPL) enables agents to learn structured priors from visual trajectories, the raw trajectories often contain noisy or incorrect visual descriptions generated by VLMs. Such inaccuracies 29 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Table 7 Ablation study on the effect of rule-based visual description in augmented-trajectory prior dataset."
        },
        {
            "title": "Method",
            "content": "Raw Trajectory (baseline) + Rule-based visual description + Traj-Aug"
        },
        {
            "title": "Unseen",
            "content": "25.0 39.6 41.4 7.3 22.9 26.1 can misguide perception and hinder effective reasoning about the environment. To isolate and quantify the impact of accurate perception-level grounding, we introduce two variants: (i) + Rule-based visual description, which enhances the raw trajectories with rule-based visual descriptions only, and (ii) + Traj-Aug, which further augments trajectories by inserting step-wise reasoning derived from these rule-based descriptions. This setup enables us to systematically evaluate how precise visual grounding and structured reasoning contribute to embodied learning and generalization. Table 7 highlights the impact of incorporating rule-based visual descriptions into EPL. Starting from the raw trajectory baseline, which yields 25.0% on seen and 7.3% on unseen environments, adding rule-based visual descriptions substantially improves performance to 39.6% and 22.9%, respectively. This demonstrates that accurate visual grounding plays critical role in bridging perception and action. Further enriching trajectories with structured reasoning through Traj-Aug leads to additional gains, reaching 41.4% on seen and 26.1% on unseen environments. These results confirm that both accurate visual descriptions and trajectory-level reasoning are essential for enhancing generalization in embodied agents, with the strongest improvements observed in unseen settings. E.2 Effect of Different Components in the Environment-anchored Prior Dataset Table 8 Ablation study on environment-anchored prior dataset on EB-ALFRED, analyzing the impact of training data from different tasks."
        },
        {
            "title": "Data Type",
            "content": "EB-ALFRED"
        },
        {
            "title": "Masked Action Modeling only\nAction Sequence Reordering only\nAll",
            "content": "41 44 47 38 40 44 44 48 50 Table 9 Ablation study on environment-anchored prior dataset on EB-Manipulation, analyzing the impact of training data from different tasks."
        },
        {
            "title": "Data Type",
            "content": "No Comb. Grounding No Relative Grounding No Absolute Grounding All EB-Manpulation"
        },
        {
            "title": "Avg",
            "content": "36.7 36.6 35."
        },
        {
            "title": "Spatial",
            "content": "45.8 47.9 43.8 45.8 37.5 33.3 33.3 41.7 45.8 47.9 45.8 47.9 35.4 33.3 33.3 37.5 18.8 20.8 18.8 27.1 The Environment-Anchored dataset is designed to enrich embodied prior learning by providing diverse supervision signals derived from the environment. However, it remains unclear which specific components contribute most to improving embodied reasoning. By systematically ablating these components, we aim to understand how different forms of environment-anchored supervision jointly shape embodied representations and generalization across tasks. On EB-ALFRED, the results in Table 8 reveal that incorporating both Masked Action Modeling (MAM) and Action Sequence Reordering (ASR) provides clear additive benefits. Using MAM alone achieves an 30 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning average success rate of 41.0, while ASR alone slightly outperforms it with 44.0. When the two tasks are combined, performance rises to 47.0 on average, gain of +6.0 over MAM alone and +3.0 over ASR alone. The improvements are consistent across subsets: on the Common split, performance increases from 38.0 and 40.0 to 44.0 (+6.0 and +4.0, respectively), and on the Spatial split from 44.0 and 48.0 to 50.0 (+6.0 and +2.0). These results indicate that temporal reasoning (ASR) and context-based inference (MAM) provide complementary supervision signals that jointly enhance high-level planning and perception grounding. On EB-Manipulation, Table 9 demonstrates similar synergy among grounding components. Excluding any single grounding dimension leads to noticeable drop in performance. Without compositional grounding, the average success rate is 36.7; removing relative grounding yields 36.6, and removing absolute grounding further decreases it to 35.0. In contrast, jointly using all three grounding types raises the average to 40.0, gain of +3.3 to +5.0 points. The benefit is most evident on unseen or spatially complex subsets: on the Spatial split, performance improves from 18.820.8 to 27.1 (+6.3 to +8.3), and on the Common split from 33.335.4 to 37.5 (+2.1 to +4.2). These gains highlight that absolute, relative, and compositional grounding capture complementary spatial priors, with their joint supervision enabling stronger generalization across diverse embodied settings. E.3 Effect of Different Reward Design in RL Table 10 Ablation of RL reward sparsity of unseen task performance"
        },
        {
            "title": "Reward Sparsity",
            "content": "EB-ALFRED EB-Manipulation"
        },
        {
            "title": "Spatial",
            "content": "Outcome Only Outcome + Subgoal Outcome + B.S Outcome + Subgoal + B.S 46 49 47 60 42 44 44 54 50 54"
        },
        {
            "title": "Avg",
            "content": "41.7 42.8 41.7 43."
        },
        {
            "title": "Spatial",
            "content": "47.9 45.9 47.9 47.9 35.4 39.6 35.4 39.6 Reward sparsity poses significant challenge for credit assignment in long-horizon embodied tasks. To analyze the contribution of different reward components, we conduct granular analysis on unseen subsets, with detailed results in Table 10. On the long-horizon planning tasks in EB-ALFRED, dense, process-level rewards provide substantial benefits over sparse, success-reward-only baseline. While individual components offer moderate gainssubgoal-based rewards improve the average success rate by 3% and behavior-shaping by 1%their combination yields strong synergistic effect. The full, composite reward function boosts the average success rate from 46% to 60%, 14-point improvement that far exceeds the sum of individual gains. This synergy is particularly impactful on unseen subsets, with performance on Spatial and Common Sense tasks increasing by 16 and 12 points, respectively. These results underscore that composite reward function is critical for robust generalization in complex planning tasks. On the contrast, for shorter-horizon, low-level control tasks as in EB-Manipulation, the impact of dense rewards much more limited. The overall gain is modest (41.7% 43.8%) and the performance on individual subset is close. We hypothesize this is because the shorter task horizons (with the shortest task finished by 45 steps) allow turn-level GAE to assign credit more effectively even with sparser rewards. That being said, the combination of sub-goal reward and behavior-shaping reward in low level task still offers positive effect on agents generalization on unseen tasks. In summary, our analysis confirms that the utility of dense rewards is associated with task horizon length. That being said, different reward components can be designed to target distinct capabilities, and their combination often produces synergistic effects. This highlights the importance of designing composite reward function that provides both positive guidance and negative constraints to facilitate effective and generalizable policy learning. E.4 Effect of Different GAEs in RL Standard token-level advantage estimation creates granularity mismatch for VLM-based agents, where an entire sequence of generated tokens corresponds to single environmental action. This can distribute 31 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Table 11 Ablation of Different GAE in RL Different GAE Token-level GAE Bi-level GAE Turn-level GAE (Ours) Avg 58.8 60.4 65.2 EB-ALFRED EB-Manipulation Base Complex Visual Common Spatial 70 72 72 72 70 72 56 60 40 44 54 56 56 66 Avg 40.0 42.1 48.3 Base Complex Visual Common Spatial 43.9 47.9 56.3 47.9 47.9 47.9 37.5 39.6 50. 45.8 39.6 47.9 25.0 35.4 39.6 Figure 7 Comparing error statistics in two benchmarks. credit improperly and destabilize training. To address this, we compare token-level, bi-level (Wang* et al., 2025), and our proposed turn-level GAE. As shown in Table 11, aligning credit assignment with the unit of interaction via turn-level GAE consistently yields superior performance. On the long-horizon tasks in EB-ALFRED, turn-level GAE significantly improves generalization. It achieves 65.2% average success rate, outperforming token-level GAE by 6.4 points. The gains are most pronounced on the unseen Common Sense and Spatial subsets (+14 and +10 points, respectively), demonstrating that turn-level credit assignment better helps agent learn in complex planning task. The advantage is also evident in the low-level control tasks of EB-Manipulation, where turn-level GAE achieves the highest average success rate (43.9%). It yields substantial improvements on subsets requiring fine-grained perception, with performance increasing by 12.5 points on Visual tasks and 14.6 points on Spatial tasks over the token-level baseline. In contrast, bi-level GAE offers only modest gains, as its method is still limited by reliance on token-level value estimation. In summary, these results provide strong evidence that matching the temporal unit of credit assignment to the agents action abstraction is critical for learning generalizable policies. Turn-level GAE proves to be more stable and effective method for training sequence-generating agents in interactive environments. E.5 Error Analysis To understand how the EPL and RL stages in ERA reduce different types of errors, we conduct an error analysis on unseen subsets of both the high-level planning task EB-ALFRED (100 tasks) and the low-level control task EB-Manipulation (98 tasks). We categorize into 3 types of error: (i) Perception errors: incorrect descriptions of the current state; (ii) Reasoning errors: mistakes in reasoning about the current state or reflecting on history; (iii) Planning errors: mistakes in planning future steps. The results, shown in Figure 7, reveal distinct patterns across task levels. In high-level tasks, reasoning and planning errors are dominant, while in low-level tasks, perception and reasoning errors are more prevalent. Across both settings, EPL and RL consistently reduce errors, but their effects differ in granularity: EPL contributes to reducing all error types, whereas RL is especially effective at lowering reasoning and planning errors. When combined, EPL and RL (i.e., ERA) achieve the lowest error rates across all categories. We provide deeper case analysis for ERA below. 32 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Algorithm 2 Pseudocode for Low-Level Visual Description Reward 1: Input: Agents reasoning output think text, Environment observation 2: 3: procedure GetGroundTruthObjects(observation) 4: 5: 6: 7: 8: coord obj info.pose[1] type, color GetProperties(obj name) Add (y coord, type, color) to objects with objects with [] for obj name, obj info in observation.object informations do Extract y-coordinate for sorting end for Sort objects with by coord return list of (type, color) tuples from sorted list 9: 10: 11: 12: end procedure 13: 14: procedure ParseVisualDescription(think text) 15: 16: 17: 18: 19: 20: desc text Extract visual description section from think text using regex if desc text is empty then return None end if parsed objects [] matches Find all object patterns (e.g., red cube at [...]) in desc text for each match in matches do end for return parsed objects words Split match into words color, type IdentifyColorAndType(words) Add (color, type) to parsed objects 21: 22: 23: 24: 25: 26: end procedure 27: 28: procedure CalculateVisualReward(think text, observation) predicted tuples ParseVisualDescription(think text) 29: if predicted tuples is None then return -0.5 30: end if 31: 32: gt tuples GetGroundTruthObjects(observation) length of gt tuples if == 0 then return 0 end if match count 0 for from 0 to min(len(gt tuples), len(predicted tuples)) - 1 do if predicted tuples[i] matches gt tuples[i] then match count match count + 1 end if end for 33: 34: 35: 36: 37: 38: 39: 40: 41: 42: 43: 44: match count / 45: 46: 47: 48: 49: 50: 51: end procedure if > 0.75 then return +0.5 else if < 0.25 then return -0.5 elsereturn 0 end if Penalize unparsable description 33 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning"
        },
        {
            "title": "F Agent Prompt",
            "content": "We provide the agent input prompts used as textual input to ERA for EB-ALFRED and EB-Manipulation. Training System Prompt for EB-ALFRED ## You are robot operating in home. Given task, you must accomplish the task using defined set of actions to achieve the desired outcome. ## Action Descriptions and Validity Rules Find: Parameterized by the name of the receptacle to navigate to. So long as the object is present in the scene, this skill is always valid. Pick up: Parameterized by the name of the object to pick. Only valid if the robot is close to the object, not holding another object, and the object is not inside closed receptacle. Put down: Parameterized by the name of the object to put down to nearby receptacle. Only valid if the robot is holding an object. Drop: Parameterized by the name of the object to put down. It is different from the Put down action, as this does not guarantee the held object will be put into specified receptacle. Open: Parameterized by the name of the receptacle to open. Only valid if the receptacle is closed and the robot is close to the receptacle. Close: Parameterized by the name of the receptacle to close. Only valid if the receptacle is open and the robot is close to the receptacle. Turn on: Parameterized by the name of the object to turn on. Only valid if the object is turned off and the robot is close to the object. Turn off: Parameterized by the name of the object to turn off. Only valid if the object is turned on and the robot is close to the object. Slice: Parameterized by the name of the object to slice. Only valid if the object is sliceable and the robot is close to the object. ## The available action id (0 - {len(SKILL SET) - 1}) and action names are: {SKILL SET}. ## Guidelines 1. **Output Plan**: Avoid generating empty plan. Each plan should include no more than 20 actions. 2. **Visibility**: Always locate visible object by the find action before interacting with it. 3. **Action Guidelines**: Make sure match the action name and its corresponding action id in the output. Avoid performing actions that do not meet the defined validity criteria. For instance, if you want to put object in receptacle, use put down rather than drop actions. 4. **Prevent Repeating Action Sequences**: Do not repeatedly execute the same action or sequence of actions. Try to modify the action sequence because previous actions do not lead to success. 5. **Multiple Instances**: There may be multiple instances of the same object, distinguished by an index following their names, e.g., Cabinet 2, Cabinet 3. You can explore these instances if you do not find the desired object in the current receptacle. 6. **Reflection on History and Feedback**: Use interaction history and feedback from the environment to refine and improve your current plan. If the last action is invalid, reflect on the reason, such as not adhering to action rules or missing preliminary actions, and adjust your plan accordingly. **Generation Guide** - Include the thinking process between <think start> and <think end>. - Include only the target action in <action start> and <action end>, i.e., the content inside should be nothing more than [action id, action name], where the action id is an integer and the action name is the corresponding name. Do not include any other text, such as quotation marks. ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Training System Prompt for EB-Manipulation ## You are Franka Panda robot with parallel gripper. You can perform various tasks and output sequence of gripper actions to accomplish given task with images of your status. The input space, output action space and color space are defined as follows: ** Input Space ** - Each input object is represented as 3D discrete position in the following format: [X, Y, Z]. - There is red XYZ coordinate frame located in the top-left corner of the table. The X-Y plane is the table surface. - The allowed range of X, Y, is [0, 100]. - Objects are ordered by in ascending order. ** Output Action Space ** - Each output action is represented as 7D discrete gripper action in the following format: [X, Y, Z, Roll, Pitch, Yaw, Gripper state]. - X, Y, are the 3D discrete position of the gripper in the environment. It follows the same coordinate system as the input object coordinates. - The allowed range of X, Y, is [0, 100]. - Roll, Pitch, Yaw are the 3D discrete orientation of the gripper in the environment, represented as discrete Euler Angles. - The allowed range of Roll, Pitch, Yaw is [0, 120] and each unit represents 3 degrees. - Gripper state is 0 for close and 1 for open. ** Color space ** - Each object can be described using one of the colors below: [\"red\", \"maroon\", \"lime\", \"green\", \"blue\", \"navy\", \"yellow\", \"cyan\", \"magenta\", \"silver\", \"gray\", \"olive\", \"purple\", \"teal\", \"azure\", \"violet\", \"rose\", \"black\", \"white\"], ** Generation Guide ** - Include the thinking process between <think start> and <think end> - Include only the target action in <action start> and <action end>, i.e. the content inside <action start> and <action end> should be nothing more than the 7-DoF vector. Do not include any other thing, such as . 35 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning"
        },
        {
            "title": "G Embodied Prior Learning Dataset",
            "content": "G.1 Raw Trajectory We provide an example of raw trajectory collected from EmbodiedBench for both EB-ALFRED and EBManipulation. Raw trajectory for EB-ALFRED Step 1: Input image Prompt instruction: Set plate with spoon in it on the kitchen table interaction history: [] Based on the above information, please provide the action for the next step to complete the task. Think, then act. Generation <think start>This is the first step in the episode, should start by formulating the first plan: visual - description: The image shows corner of room with beige walls and carpeted flooring. There appears to be vent or air return mounted on the wall. The space appears to be empty without any visible furniture or objects. reasoning and reflection: Since the room appears empty, well need to search for and locate the required items (plate, spoon, and dining table) before we can complete the task. Well need to find these items first, then arrange them according to the instruction. language plan: 1. First, locate the plate in the room. 2. Pick up the plate once found. 3. Find the spoon. 4. Pick up the spoon. 5. Find the dining table. 6. Put down the spoon on the plate. 7. Place the plate with the spoon on the dining table.<think end> <action start>[31, find Plate]<action end> Step 2: Input image Prompt visual description: This is the first step in the episode, Since the room appears empty, well need to search for instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: 0, thinking: The should start by formulating the first plan: image shows corner of room with beige walls and carpeted flooring. There appears to be vent or air return mounted on the wall. appears to be empty without any visible furniture or objects. and reflection: and locate the required items (plate, spoon, and dining table) before we can complete the task. them according to the instruction. plate in the room. 2. Pick up the spoon. 4. on the plate. action: executed successfully.}]] Based on the above information, please provide the action for the next step to complete the task. Think, then act. First, locate the 3. Find the spoon. Put down the spoon Place the plate with the spoon on the dining table., 1. Pick up the plate once found. 6. Find the dining table. 5. Well need to find these items first, then arrange [31, find Plate], env feedback: The space reasoning - language plan: Last action 7. Generation <think start>Based on the interaction history and current observation, am in the middle of the last plan and will continue carrying it out.<think end> 36 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning <action start>[122, pick up the Plate]<action end> Step 3: Input image Prompt 1, thinking: instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: current observation, am in the middle of the last plan and will continue carrying it out., action: feedback: Based on the above information, please provide the action for the next step to complete the task. Think, then act. Last action executed successfully.}]] [122, pick up the Plate], env - Based on the interaction history and Generation <think start>Based on the interaction history and current observation, am in the middle of the last plan and will continue carrying it out.<think end> <action start>[19, find Spoon]<action end> Step 4: Input image Prompt 2, thinking: instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: and current observation, am in the middle of the last plan and will continue carrying it out., action: feedback: Based on the above information, please provide the action for the next step to complete the task. Think, then act. Last action executed successfully.}]] Based on the interaction history [19, find Spoon], env - Generation <think start>Based on the interaction history and current observation, am in the middle of the last plan and will continue carrying it out.<think end> <action start>[105, pick up the Spoon]<action end> Step 5: Input image Prompt 3, thinking: instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: and current observation, am in the middle of the last plan and will continue carrying it out., action: env feedback: Plate}]] Based on the above information, please provide the action for the next step to complete the task. Think, then act. [105, pick up the Spoon], Robot is currently holding Based on the interaction history Last action is invalid. Generation <think start>Based on the interaction history and current observation, have completed the previous plan and can now formulate new plan: visual description: The image shows kitchen countertop surface. There appears to be some objects on the counter, including what looks like an apple or round fruit. reasoning and - reflection: From the interaction history, the robot successfully found and picked up plate (Steps 0-1), then found spoon (Step 2). However, when trying to pick up the spoon (Step 3), the action failed because the robot was already holding the plate. To achieve the goal of placing plate with spoon in it on the kitchen 37 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning table, we need to first put down the plate, pick up the spoon, then put the spoon on the plate, pick up the plate again, and finally place it on the dining table. language plan: 1. Find the dining table to put down the plate temporarily. 2. Put down the plate. 3. Find the spoon again. 4. Pick up the spoon. 5. Find the plate. 6. Put the spoon on the plate. 7. Pick up the plate (now with spoon in it). 8. Find the dining table. 9. Put down the plate with spoon on the table.<think end> <action start>[18, find DiningTable]<action end> Step 6: Input image Prompt reasoning and - 4, thinking: visual description: Based on the interaction history From the interaction history, the robot successfully found The image shows kitchen There appears to be some objects on the counter, instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: and current observation, have completed the previous plan and can now formulate new plan: countertop surface. including what looks like an apple or round fruit. reflection: and picked up plate (Steps 0-1), then found spoon (Step 2). when trying to pick up the spoon (Step 3), the action failed because the robot was already holding the plate. plate with spoon in it on the kitchen table, we need to first put down the plate, pick up the spoon, then put the spoon on the plate, pick up the plate again, and finally place it on the dining table. 1. the plate. the plate. with spoon in it). 8. with spoon on the table., action: env feedback: Based on the above information, please provide the action for the next step to complete the task. Think, then act. 5. Find Pick up the plate (now Put down the plate Find the spoon again. Put the spoon on the plate. Find the dining table to put down the plate temporarily. Last action executed successfully.}]] language plan: Put down To achieve the goal of placing [18, find DiningTable], Find the dining table. Pick up the spoon. However, 3. 6. 9. 4. 2. 7. Generation <think start>Based on the interaction history and current observation, am in the middle of the last plan and will continue carrying it out.<think end> <action start>[133, put down the object in hand]<action end> Step 7: Input image Prompt 5, thinking: instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: current observation, am in the middle of the last plan and will continue carrying it out., action: [133, put down the object in hand], env feedback: Based on the above information, please provide the action for the next step to complete the task. Think, then act. Last action executed successfully.}]] Based on the interaction history and Generation <think start>Based on the interaction history and current observation, am in the middle of the last plan and will continue carrying it out.<think end> <action start>[19, find Spoon]<action end> 38 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Step 8: Input image Prompt 6, thinking: instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: and current observation, am in the middle of the last plan and will continue carrying it out., action: feedback: Based on the above information, please provide the action for the next step to complete the task. Think, then act. Last action executed successfully.}]] Based on the interaction history [19, find Spoon], env - Generation <think start>Based on the interaction history and current observation, am in the middle of the last plan and will continue carrying it out.<think end> <action start>[105, pick up the Spoon]<action end> Step 9: Input image Prompt 7, thinking: instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: current observation, am in the middle of the last plan and will continue carrying it out., action: feedback: Based on the above information, please provide the action for the next step to complete the task. Think, then act. Last action executed successfully.}]] [105, pick up the Spoon], env - Based on the interaction history and Generation <think start>Based on the interaction history and current observation, am in the middle of the last plan and will continue carrying it out.<think end> <action start>[31, find Plate]<action end> Step 10: Input image Prompt 8, thinking: instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: and current observation, am in the middle of the last plan and will continue carrying it out., action: feedback: Based on the above information, please provide the action for the next step to complete the task. Think, then act. Last action executed successfully.}]] Based on the interaction history [31, find Plate], env - Generation <think start>Based on the interaction history and current observation, am in the middle of the last plan and will continue carrying it out.<think end> <action start>[133, put down the object in hand]<action end> Raw trajectory for EB-Manipulation Step 1: 39 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Input image Prompt instruction: Pick up the star and place it into the red container. interaction history: [] {object 1: additional info: 18, 18], object 3: [49, 37, 17], object 4: object 5: [54, 81, 18]} Based on the above information, please provide the action for the next step to complete the task. Think, then act. [35, 15, 17], object 2: [76, 69, 17], [57, Generation <think start>This is the first step in the episode, should start by formulating the first plan: visual - description: From left to right, can see red star at [35, 15, 17], teal cube at [57, 18, 18], purple container at [49, 37, 17], red container at [76, 69, 17], and an orange cylinder at [54, 81, 18]. reasoning and reflection: The task is to pick up the star located at [35, 15, 17] and place it into the red container located at [76, 69, 17]. need to move the gripper to the stars position, close the gripper to grasp it, lift it up, move to the red containers position, and then open the gripper to release the star into the container. language plan: 1. Move gripper above the red star at [35, 15, 17]n2. Lower gripper to grasp the starn3. Close gripper to pick up the starn4. Lift the star upn5. Move the star above the red container at [76, 69, 17]n6. Lower the star into the containern7. Open gripper to release the star<think end> <action start>[35, 15, 27, 0, 60, 0, 1]<action end> Step 2: Input image Prompt visual description: reasoning and reflection: This is the first step in the episode, instruction: Pick up the star and place it into the red container. interaction history: [{step id: 1, thinking: should start by formulating the first plan: From left to right, can see red star at [35, 15, 17], teal cube at [57, 18, 18], purple container at [49, 37, 17], red container at [76, 69, 17], and an orange cylinder at [54, 81, 18]. The task is to pick up the star located at [35, 15, 17] and place it into the red container located at [76, 69, 17]. to the stars position, close the gripper to grasp it, lift it up, move to the red containers position, and then open the gripper to release the star into the container. the red star at [35, 15, 17]n2. Close gripper to pick up the starn4. star above the red container at [76, 69, 17]n6. containern7. 27, 0, 60, 0, 1], env feedback: {object 1: additional info: 18, 18], object 3: [49, 37, 17], object 4: object 5: [54, 81, 18]} Based on the above information, please provide the action for the next step to complete the task. Think, then act. Lower gripper to grasp the starn3. Move the Last action was successful.}]] [35, 15, 17], object 2: Open gripper to release the star, action: need to move the gripper Lower the star into the Lift the star upn5. Move gripper above [76, 69, 17], language plan: [35, 15, 1. [57, Generation <think start>Based on the interaction history and current observation, am in the middle of the last plan and will continue carrying it out.<think end> <action start>[35, 15, 17, 0, 60, 0, 0]<action end> Step 3: 40 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Input image Prompt 2, thinking: Based on the interaction history and instruction: Pick up the star and place it into the red container. interaction history: [{step id: current observation, am in the middle of the last plan and will continue carrying it out., action: feedback: {object 1: additional info: 18, 18], object 3: [49, 37, 17], object 4: object 5: [54, 81, 18]} Based on the above information, please provide the action for the next step to complete the task. Think, then act. [35, 15, 17, 0, 60, 0, 0], env - Last action was successful.}]] [35, 15, 17], object 2: [76, 69, 17], [57, Generation <think start>Based on the interaction history and current observation, am in the middle of the last plan and will continue carrying it out.<think end> <action start>[35, 15, 27, 0, 60, 0, 0]<action end> Step 4: Input image Prompt 3, thinking: Based on the interaction history and instruction: Pick up the star and place it into the red container. interaction history: [{step id: current observation, am in the middle of the last plan and will continue carrying it out., action: feedback: {object 1: additional info: 18, 18], object 3: [49, 37, 17], object 4: object 5: [54, 81, 18]} Based on the above information, please provide the action for the next step to complete the task. Think, then act. [35, 15, 27, 0, 60, 0, 0], env - Last action was successful.}]] [35, 15, 17], object 2: [76, 69, 17], [57, Generation <think start>Based on the interaction history and current observation, am in the middle of the last plan and will continue carrying it out.<think end> <action start>[76, 69, 27, 0, 60, 0, 0]<action end> Step 5: Input image Prompt 4, thinking: Based on the interaction history and instruction: Pick up the star and place it into the red container. interaction history: [{step id: current observation, am in the middle of the last plan and will continue carrying it out., action: feedback: {object 1: additional info: 18, 18], object 3: [49, 37, 17], object 4: object 5: [54, 81, 18]} Based on the above information, please provide the action for the next step to complete the task. Think, then act. [76, 69, 27, 0, 60, 0, 0], env - Last action was successful.}]] [35, 15, 17], object 2: [76, 69, 17], [57, Generation 41 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning <think start>Based on the interaction history and current observation, am in the middle of the last plan and will continue carrying it out.<think end> <action start>[76, 69, 17, 0, 60, 0, 1]<action end> G.2 Trajectory-augmented Prior Dataset We provide an example of Trajectory-augmented Prior dataset for both EB-ALFRED and EB-Manipulation. Trajectory-augmented Prior for EB-ALFRED Step 1: Input image Prompt instruction: Set plate with spoon in it on the kitchen table interaction history: [] Based on the above information, please provide the action for the next step to complete the task. Think, then act. Generation <think start>visual description: The image shows corner of room with beige walls and carpeted flooring. There appears to be vent or air return mounted on the wall. The space appears to be empty without any visible furniture or objects. reasoning and reflection: Since the room appears empty, well need to search for and locate the required items (plate, spoon, and dining table) before we can complete the task. Well need to find these items first, then arrange them according to the instruction. language plan: 1. First, locate the plate in the room. 2. Pick up the plate once found. 3. Find the spoon. 4. Pick up the spoon. 5. Find the dining table. 6. Put down the spoon on the plate. 7. Place the plate with the spoon on the dining table.<think end> <action start>[31, find Plate]<action end> Step 2: Input image Prompt There appears The image shows 0, thinking: visual description: The space appears to be reasoning and reflection: instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: corner of room with beige walls and carpeted flooring. to be vent or air return mounted on the wall. empty without any visible furniture or objects. Since the room appears empty, well need to search for and locate the required items (plate, spoon, and dining table) before we can complete the task. according to the instruction. in the room. 2. Pick up the spoon. on the plate. action: executed successfully.}]] Based on the above information, please provide the action for the next step to complete the task. Think, then act. Well need to find these items first, then arrange them language plan: Place the plate with the spoon on the dining table., First, locate the plate Find the spoon. 4. [31, find Plate], env feedback: Pick up the plate once found. Find the dining table. Put down the spoon Last action 1. 3. 7. 5. 6. Generation <think start>visual description: The image shows kitchen counter with black plate, knife, toaster, pan, and small round object. The plate is visible and accessible. reasoning and reflection: The plate has 42 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning been located on the kitchen counter. The next step is to pick up the plate as planned. After picking up the plate, the next task will be to find and pick up spoon, then locate the dining table to complete the task. The previous plan is still valid, and we can continue executing it. language plan: 1. Pick up the plate. 2. Find the spoon. 3. Pick up the spoon. 4. Find the dining table. 5. Put down the spoon on the plate. 6. Place the plate with the spoon on the dining table.<think end> <action start>[122, pick up the Plate]<action end> Step 3: Input image Prompt The image shows 1, thinking: visual description: The plate is visible and accessible. The plate has been located on the kitchen counter. instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: kitchen counter with black plate, knife, toaster, pan, and small round object. reasoning - and reflection: next step is to pick up the plate as planned. the next task will be to find and pick up spoon, then locate the dining table to complete the task. can continue executing it. Find the spoon. Put down the spoon on the plate. the dining table., action: feedback: Based on the above information, please provide the action for the next step to complete the task. Think, then act. The previous plan is still valid, and we 2. 6. [122, pick up the Plate], env - 5. Place the plate with the spoon on Pick up the plate. Find the dining table. Last action executed successfully.}]] After picking up the plate, Pick up the spoon. language plan: The 3. 1. 4. Generation <think start>visual description: The image shows kitchen counter with knife, toaster, plate, and bottle. The plate is visible on the counter. reasoning and reflection: The previous actions successfully located and picked up the plate. Now, the next step is to find spoon. However, the image shows knife instead of spoon. We need to continue searching for the spoon. The dining table is not visible in the current image, so it will need to be located later. language plan: 1. Find Spoon. 2. Pick up the Spoon. 3. Find the dining table. 4. Put down the Spoon on the Plate. 5. Place the Plate with the Spoon on the dining table.<think end> <action start>[19, find Spoon]<action end> Step 4: Input image Prompt The image shows 2, thinking: visual description: reasoning and reflection: instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: kitchen counter with knife, toaster, plate, and bottle. plate is visible on the counter. actions successfully located and picked up the plate. step is to find spoon. spoon. not visible in the current image, so it will need to be located later. 2. language plan: the dining table. 4. Plate with the Spoon on the dining table., action: Spoon], env feedback: Based on the above information, please provide the action for the next step to complete the task. Think, then act. Find Place the [19, find However, the image shows knife instead of We need to continue searching for the spoon. Last action executed successfully.}]] Put down the Spoon on the Plate. The dining table is Pick up the Spoon. Now, the next Find Spoon. The previous The 1. 5. 3. 43 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Generation <think start>visual description: The image shows kitchen counter with spoon, plate, green apple, and coconut. The spoon is on the counter next to the plate. reasoning and reflection: The previous actions successfully located the plate and spoon. The next step is to pick up the spoon, which is visible on the counter. After picking up the spoon, the plan is to place it on the plate and then find the dining table to complete the task. language plan: 1. Pick up the spoon. 2. Place the spoon on the plate. 3. Find the dining table. 4. Place the plate with the spoon on the dining table.<think end> <action start>[105, pick up the Spoon]<action end> Step 5: Input image Prompt 3, thinking: visual description: instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: kitchen counter with spoon, plate, green apple, and coconut. spoon is on the counter next to the plate. reasoning and reflection: previous actions successfully located the plate and spoon. is to pick up the spoon, which is visible on the counter. up the spoon, the plan is to place it on the plate and then find the dining table to complete the task. 2. the plate with the spoon on the dining table., action: pick up the Spoon], env feedback: Robot is currently holding Plate}]] Based on the above information, please provide the action for the next step to complete the task. Think, then act. The image shows The The The next step Pick up the spoon. 4. Place the spoon on the plate. Last action is invalid. Find the dining table. language plan: After picking [105, Place 1. 3. Generation <think start>visual description: The image shows kitchen countertop surface. There appears to be some objects on the counter, including what looks like an apple or round fruit. reasoning and reflection: From the interaction history, the robot successfully found and picked up plate (Steps 0-1), then found spoon (Step 2). However, when trying to pick up the spoon (Step 3), the action failed because the robot was already holding the plate. To achieve the goal of placing plate with spoon in it on the kitchen table, we need to first put down the plate, pick up the spoon, then put the spoon on the plate, pick up the plate again, and finally place it on the dining table. language plan: 1. Find the dining table to put down the plate temporarily. 2. Put down the plate. 3. Find the spoon again. 4. Pick up the spoon. 5. Find the plate. 6. Put the spoon on the plate. 7. Pick up the plate (now with spoon in it). 8. Find the dining table. 9. Put down the plate with spoon on the table.<think end> <action start>[18, find DiningTable]<action end> Step 6: 44 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Input image Prompt reasoning - The image shows 4, thinking: visual description: There appears to be some objects on the instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: kitchen countertop surface. counter, including what looks like an apple or round fruit. and reflection: From the interaction history, the robot successfully found and picked up plate (Steps 0-1), then found spoon (Step 2). However, when trying to pick up the spoon (Step 3), the action failed because the robot was already holding the plate. To achieve the goal of placing plate with spoon in it on the kitchen table, we need to first put down the plate, pick up the spoon, then put the spoon on the plate, pick up the plate again, and finally place it on the dining Find the dining table to put down the plate table. temporarily. Pick up the spoon. 7. table. [18, find DiningTable], env feedback: successfully.}]] Based on the above information, please provide the action for the next step to complete the task. Think, then act. Put the spoon on the plate. Find the dining Put down the plate with spoon on the table., action: Pick up the plate (now with spoon in it). language plan: 2. Last action executed Find the spoon again. Put down the plate. Find the plate. 9. 8. 3. 6. 5. 4. 1. Generation <think start>visual description: The robot is near table with loaf of bread on it. The robot is holding plate. reasoning and reflection: The robot successfully found the dining table and is ready to put down the plate. The next step is to put down the plate, find the spoon, and then place the spoon on the plate. Finally, the robot will pick up the plate with the spoon and place it on the table. language plan: 1. Put down the plate on the table. 2. Find the spoon. 3. Pick up the spoon. 4. Find the plate. 5. Put the spoon on the plate. 6. Pick up the plate with the spoon. 7. Find the dining table. 8. Put down the plate with the spoon on the table.<think end> <action start>[133, put down the object in hand]<action end> Step 7: Input image Prompt The robot is 5, thinking: visual description: The next step is to put down the The robot is holding plate. The robot successfully found the dining table instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: near table with loaf of bread on it. reasoning and reflection: and is ready to put down the plate. plate, find the spoon, and then place the spoon on the plate. the robot will pick up the plate with the spoon and place it on the table. spoon. on the plate. table. [133, put down the object in hand], env feedback: executed successfully.}]] Based on the above information, please provide the action for the next step to complete the task. Think, then act. Put down the plate with the spoon on the table., action: Last action Put down the plate on the table. Find the plate. Pick up the plate with the spoon. language plan: 3. 2. Put the spoon Pick up the spoon. Find the dining Finally, Find the 1. 5. 7. 8. 6. 4. Generation 45 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning <think start>visual description: The plate is on the kitchen table next to loaf of bread. There is stool nearby. reasoning and reflection: The plate has been successfully placed on the kitchen table. The next step is to find and pick up spoon to place it on the plate. The previous plan was interrupted after putting down the plate, so the next action is to locate the spoon and continue with the plan to place it on the plate. language plan: 1. Find the spoon. 2. Pick up the spoon. 3. Find the plate on the kitchen table. 4. Put the spoon on the plate. 5. Pick up the plate with the spoon in it. 6. Put down the plate with the spoon on the kitchen table.<think end> <action start>[19, find Spoon]<action end> Step 8: Input image Prompt The plate is visual description: The previous plan was interrupted after putting down the There is stool nearby. The plate has been successfully placed on the The next step is to find and pick up spoon to place it instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: 6, thinking: on the kitchen table next to loaf of bread. reasoning and reflection: kitchen table. on the plate. plate, so the next action is to locate the spoon and continue with the 2. plan to place it on the plate. Pick up the spoon. the spoon on the plate. Put down the plate with the spoon on the kitchen table., action: [19, find Spoon], env feedback: successfully.}]] Based on the above information, please provide the action for the next step to complete the task. Think, then act. Pick up the plate with the spoon in it. Find the plate on the kitchen table. Last action executed Find the spoon. language plan: Put 4. 5. 3. 1. 6. Generation <think start>visual description: The kitchen counter has spoon, fork, green apple, and brown object. The sink is visible, and there are some utensils near it. reasoning and reflection: The previous plan involved putting down the plate to pick up the spoon. The first three actions were executed successfully, and the robot is now ready to pick up the spoon. The next steps involve placing the spoon on the plate, picking up the plate with the spoon, and placing it on the dining table. language plan: 1. Pick up the spoon. 2. Find the plate. 3. Put the spoon on the plate. 4. Pick up the plate (now with spoon in it). 5. Find the dining table. 6. Put down the plate with spoon on the table.<think end> <action start>[105, pick up the Spoon]<action end> Step 9: 46 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Input image Prompt 7, thinking: visual description: The kitchen The The first three actions were executed successfully, and the The previous plan involved putting down the plate to pick up instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: counter has spoon, fork, green apple, and brown object. sink is visible, and there are some utensils near it. reflection: the spoon. The next steps involve placing robot is now ready to pick up the spoon. the spoon on the plate, picking up the plate with the spoon, and placing Find it on the dining table. the plate. with spoon in it). with spoon on the table., action: env feedback: Based on the above information, please provide the action for the next step to complete the task. Think, then act. 1. Put the spoon on the plate. Pick up the plate (now Put down the plate Last action executed successfully.}]] Pick up the spoon. 4. [105, pick up the Spoon], Find the dining table. reasoning and - language plan: 5. 2. 3. 6. Generation <think start>visual description: The image shows kitchen counter with spoon, fork, and green object on tray. There is also brown object and green apple on the counter. reasoning and reflection: The previous plan involved finding the plate to place the spoon on it. The spoon is already visible on tray, which might be the plate. The next step is to confirm the presence of the plate and place the spoon on it. The plan needs to be adjusted to ensure the spoon is placed on the plate and then the plate is moved to the dining table. language plan: 1. Find the plate (tray) on the counter. 2. Put the spoon on the plate. 3. Pick up the plate with the spoon. 4. Find the dining table. 5. Put down the plate with the spoon on the dining table.<think end> <action start>[31, find Plate]<action end> Step 10: Input image Prompt There reasoning - 8, thinking: The image shows visual description: The previous plan involved finding the plate to place The spoon is already visible on tray, which might instruction: Set plate with spoon in it on the kitchen table interaction history: [{step id: kitchen counter with spoon, fork, and green object on tray. is also brown object and green apple on the counter. and reflection: the spoon on it. be the plate. and place the spoon on it. spoon is placed on the plate and then the plate is moved to the dining table. the spoon on the plate. the dining table. 5. table., action: action executed successfully.}]] Based on the above information, please provide the action for the next step to complete the task. Think, then act. Put down the plate with the spoon on the dining [31, find Plate], env feedback: Find the plate (tray) on the counter. Pick up the plate with the spoon. The next step is to confirm the presence of the plate The plan needs to be adjusted to ensure the language plan: Last Find Put 3. 4. 1. 2. Generation <think start>visual description: The kitchen table has loaf of bread, plate, and spoon on it. The spoon is not on the plate. reasoning and reflection: The robot has successfully found the dining table and put down the plate. It then found and picked up the spoon. The next step is to place the spoon on the plate, pick up the plate with the spoon, and place it on the table. The previous plan was interrupted after picking up the ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning spoon, so the next step is to put the spoon on the plate. language plan: 1. Find the plate. 2. Put the spoon on the plate. 3. Pick up the plate with the spoon. 4. Find the kitchen table. 5. Put down the plate with the spoon on the table.<think end> <action start>[133, put down the object in hand]<action end> Trajectory-augmented Prior for EB-Manipulation Step 1: Input image Prompt instruction: Pick up the star and place it into the silver container. interaction history: [] {object 1: additional info: 20, 18], object 3: [66, 38, 18], object 4: object 5: 7: Based on the above information, please provide the action for the next step to complete the task. Think, then act. [33, 43, 17], [28, 80, 17], object [73, 15, 18], object 2: [57, 74, 17], object 6: [26, 91, 17]} [57, Generation <think start>visual description: From left to right, can see purple cylinder at [73, 15, 18], white container at [57, 20, 18], teal cube at [66, 38, 18], silver container at [33, 43, 17], an orange star at [57, 74, 17], maroon moon at [28, 80, 17], and teal triangular at [26, 91, 17]. reasoning and reflection: The task requires picking up the orange star located at [57, 74, 17] and placing it into the silver container at [33, 43, 17]. will need to move the gripper to the star position, grasp it, lift it up, move it to the silver container position, and then release it. language plan: 1. Move gripper above the orange starn2. Lower gripper and open it to prepare for graspingn3. Close gripper to grasp the starn4. Lift the star upn5. Move the star above the silver containern6. Lower the star into the containern7. Open gripper to release the star<think end> <action start>[57, 74, 27, 0, 60, 90, 1]<action end> Step 2: 48 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Input image Prompt From left to 0, thinking: visual description: reasoning and reflection: instruction: Pick up the star and place it into the silver container. interaction history: [{step id: right, can see purple cylinder at [73, 15, 18], white container at [57, 20, 18], teal cube at [66, 38, 18], silver container at [33, 43, 17], an orange star at [57, 74, 17], maroon moon at [28, 80, 17], and The task teal triangular at [26, 91, 17]. requires picking up the orange star located at [57, 74, 17] and placing it into the silver container at [33, 43, 17]. gripper to the star position, grasp it, lift it up, move it to the silver Move gripper container position, and then release it. above the orange starn2. graspingn3. Move the star above the silver containern6. containern7. 27, 0, 60, 90, 1], env feedback: {object 1: additional info: 21, 17], object 3: [66, 38, 18], object 4: object 5: 7: Based on the above information, please provide the action for the next step to complete the task. Think, then act. [57, 74, Last action was successful.}]] [55, [34, 46, 18], [26, 79, 17], object Open gripper to release the star, action: Lower gripper and open it to prepare for Close gripper to grasp the starn4. [73, 15, 18], object 2: [56, 74, 17], object 6: Lower the star into the will need to move the Lift the star upn5. [26, 91, 17]} language plan: 1. Generation <think start>visual description: From left to right, can see purple cylinder at [73, 15, 18], white container at [55, 21, 17], teal cube at [66, 38, 18], silver container at [34, 46, 18], an orange star at [56, 74, 17], maroon moon at [26, 79, 17], and teal triangular at [26, 91, 17]. reasoning and reflection: The task involves picking up the orange star and placing it into the silver container. have already moved the gripper above the star. The next step is to lower the gripper and open it to prepare for grasping the star. After that, will close the gripper to grasp the star, lift it, move it to the container, and release it. need to ensure that each step is executed precisely to avoid dropping the star or missing the container. language plan: 1. Lower gripper and open it to prepare for graspingn2. Close gripper to grasp the starn3. Lift the star upn4. Move the star above the silver containern5. Lower the star into the containern6. Open gripper to release the star<think end> <action start>[57, 74, 17, 0, 60, 90, 0]<action end> Step 3: 49 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Input image Prompt The 1, thinking: visual description: reasoning and reflection: instruction: Pick up the star and place it into the silver container. interaction history: [{step id: From left to right, can see purple cylinder at [73, 15, 18], white container at [55, 21, 17], teal cube at [66, 38, 18], silver container at [34, 46, 18], an orange star at [56, 74, 17], maroon moon at [26, 79, 17], and teal triangular at [26, 91, 17]. task involves picking up the orange star and placing it into the silver container. have already moved the gripper above the star. step is to lower the gripper and open it to prepare for grasping the star. After that, will close the gripper to grasp the star, lift it, move it to the container, and release it. executed precisely to avoid dropping the star or missing the container. language plan: 1. Close gripper to grasp the starn3. above the silver containern5. Open gripper to release the star, action: 0], env feedback: {object 1: additional info: 21, 17], object 3: [66, 38, 18], object 4: object 5: 7: Based on the above information, please provide the action for the next step to complete the task. Think, then act. Lower gripper and open it to prepare for graspingn2. Lift the star upn4. Move the star [32, 40, 17], [27, 81, 18], object Lower the star into the containern6. Last action was successful.}]] need to ensure that each step is [73, 15, 18], object 2: [56, 73, 16], object 6: [57, 74, 17, 0, 60, 90, [26, 91, 17]} The next [55, Generation <think start>visual description: From left to right, can see purple cylinder at [73, 15, 18], white container at [55, 21, 17], teal cube at [66, 38, 18], silver container at [32, 40, 17], an orange star at [56, 73, 16], maroon moon at [27, 81, 18], and teal triangular at [26, 91, 17]. reasoning and reflection: The task involves transferring the star from its current position to the silver container. have already moved the gripper above the star and lowered it to grasp the star. The next step is to close the gripper to secure the star, lift it, move it to the container, and release it. need to ensure that the gripper is properly aligned and that the star is securely held before moving it to avoid dropping it. language plan: 1. Close gripper to grasp the starn2. Lift the star upn3. Move the star above the silver containern4. Lower the star into the containern5. Open gripper to release the star<think end> <action start>[57, 74, 25, 0, 60, 90, 0]<action end> Step 4: 50 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Input image Prompt From left to 2, thinking: visual description: reasoning and reflection: The next step is to close the gripper to secure have already moved the gripper above the star and lowered instruction: Pick up the star and place it into the silver container. interaction history: [{step id: right, can see purple cylinder at [73, 15, 18], white container at [55, 21, 17], teal cube at [66, 38, 18], silver container at [32, 40, 17], an orange star at [56, 73, 16], maroon moon at [27, 81, 18], and The task teal triangular at [26, 91, 17]. involves transferring the star from its current position to the silver container. it to grasp the star. the star, lift it, move it to the container, and release it. ensure that the gripper is properly aligned and that the star is securely held before moving it to avoid dropping it. gripper to grasp the starn2. the silver containern4. gripper to release the star, action: 0], env feedback: {object 1: additional info: 21, 17], object 3: [66, 38, 18], object 4: object 5: 7: Based on the above information, please provide the action for the next step to complete the task. Think, then act. [31, 40, 18], [27, 81, 18], object Open [57, 74, 25, 0, 60, 90, 1. Move the star above Lower the star into the containern5. Last action was successful.}]] [73, 15, 18], object 2: [58, 72, 24], object 6: Lift the star upn3. [26, 91, 17]} language plan: need to Close [55, Generation <think start>visual description: From left to right, can see purple cylinder at [73, 15, 18], white container at [55, 21, 17], teal cube at [66, 38, 18], silver container at [31, 40, 18], an orange star at [58, 72, 24], maroon moon at [27, 81, 18], and teal triangular at [26, 91, 17]. reasoning and reflection: The task is to place the star into the silver container. have already moved the gripper to the stars position, grasped it, and lifted it up. The next step is to move the star to the position above the silver container, lower it into the container, and release it. need to ensure the star is securely placed inside the container before releasing it. language plan: 1. Move the star above the silver containern2. Lower the star into the containern3. Open gripper to release the star<think end> <action start>[33, 43, 25, 0, 60, 90, 0]<action end> Step 5: 51 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Input image Prompt From left to 3, thinking: have already moved visual description: reasoning and reflection: instruction: Pick up the star and place it into the silver container. interaction history: [{step id: right, can see purple cylinder at [73, 15, 18], white container at [55, 21, 17], teal cube at [66, 38, 18], silver container at [31, 40, 18], an orange star at [58, 72, 24], maroon moon at [27, 81, 18], and The task teal triangular at [26, 91, 17]. is to place the star into the silver container. the gripper to the stars position, grasped it, and lifted it up. next step is to move the star to the position above the silver container, lower it into the container, and release it. is securely placed inside the container before releasing it. plan: into the containern3. [33, 43, 25, 0, 60, 90, 0], env feedback: successful.}]] {object 1: additional info: 21, 17], object 3: [66, 38, 18], object 4: object 5: 7: Based on the above information, please provide the action for the next step to complete the task. Think, then act. need to ensure the star language - Lower the star Open gripper to release the star, action: [34, 41, 25], [27, 80, 18], object Move the star above the silver containern2. [73, 15, 18], object 2: [32, 42, 17], object 6: Last action was [26, 91, 17]} The 1. [55, Generation <think start>visual description: From left to right, can see purple cylinder at [73, 15, 18], white container at [55, 21, 17], teal cube at [66, 38, 18], an orange star at [34, 41, 25], silver container at [32, 42, 17], maroon moon at [27, 80, 18], and teal triangular at [26, 91, 17]. reasoning and reflection: The task is to place the star into the silver container. have already picked up the star and moved it above the container. The next steps involve lowering the star into the container and releasing it. need to ensure the gripper is positioned correctly above the container before lowering it. Once the star is inside the container, will open the gripper to release it and then move the gripper back to neutral position. language plan: 1. Move the star above the silver containern2. Lower the star into the containern3. Open gripper to release the starn4. Move gripper back to neutral position<think end> <action start>[33, 43, 17, 0, 60, 90, 1]<action end> G.3 Environment-Anchored Prior Dataset We provide an example of Environment-Anchored Prior dataset for both EB-ALFRED and EB-Manipulation. 52 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Environment-Anchored Prior Dataset for EB-ALFRED (Masked Action Modeling) Prompt You are household assistant. You are given an instruction: Place the clean bar soap from the bin into the tub. and an incomplete action sequence: [find garbagecan, pick up the soapbar, find sinkbasin, [MASK], find Faucet, turn on the Faucet, turn off the Faucet, find soapbar, pick up the soapbar, find bathtubbasin, put down the object in hand]. Please identify the missing action to complete the sequence. Generation To complete the logical chain for the instruction Place the clean bar soap from the bin into the tub., we need to ensure that the sequence of actions aligns with the goal of cleaning the soap and then placing it in the tub. Lets break down the sequence step by step: 1. **Find garbagecan**: This step is likely to ensure that any debris or used soap can be discarded if necessary. 2. **Pick up the soapbar**: This is the initial step to get the soap ready for cleaning. 3. **Find sinkbasin**: The sinkbasin is where the soap will be cleaned. 4. **[MASK]**: This is the missing action that needs to be identified. 5. **Find Faucet**: The faucet is needed to provide water for cleaning the soap. 6. **Turn on the Faucet**: This action provides the water necessary for cleaning. 7. **Turn off the Faucet**: This action stops the water flow after the soap is cleaned. 8. **Find soapbar**: This step is redundant because the soapbar was already picked up earlier. However, it might be included to ensure the soap is still in hand. 9. **Pick up the soapbar**: This step is also redundant but ensures the soap is ready to be moved. 10. **Find bathtubbasin**: The bathtubbasin is the final destination for the clean soap. 11. **Put down the object in hand**: This action places the clean soap in the bathtubbasin. Given the context, the missing action should logically be an action that prepares the soap for cleaning. The most appropriate action to fill the gap is put down the object in hand. This ensures that the soap is placed in the sinkbasin before the water is turned on, allowing it to be properly cleaned. After the soap is cleaned and the faucet is turned off, the sequence continues with finding the bathtubbasin and placing the clean soap in it. Therefore, the missing action is put down the object in hand. To complete the logical chain for the instruction Place the clean bar soap from the bin into the tub., the missing action is put down the object in hand. The complete and correct sequence is: [find garbagecan, pick up the soapbar, find sinkbasin, put down the object in hand, find Faucet, turn on the Faucet, turn off the Faucet, find soapbar, pick up the soapbar, find bathtubbasin, put down the object in hand]. 53 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Environment-Anchored Prior Dataset for EB-ALFRED (Action Sequence Reordering) Prompt You are household assistant. You are given the instruction: Relocate the remote from the chair to the lamp. The randomized action sequences are [pick up the remotecontrol, find floorlamp, find an armchair, turn on the floorlamp]. Your task is to generate the correct sequence of actions to accomplish the instruction. Generation To accomplish the task of relocating the remote from the chair to the lamp, need to follow logical sequence of actions. First, must locate the armchair where the remote is currently placed. Once have found the armchair, can proceed to pick up the remote control from it. Next, need to find the floorlamp where will place the remote. Finally, to ensure the lamp is functional and the remote is easily accessible, will turn on the floorlamp. This sequence ensures that complete the task as instructed, moving the remote from the chair to the lamp and verifying that the lamp is operational. As household assistant, to accomplish the instruction Relocate the remote from the chair to the lamp., the correct sequence of actions is: [find an armchair, pick up the remotecontrol, find floorlamp, turn on the floorlamp]. Environment-Anchored Prior Dataset for EB-Manipulation (Absolute Coordinate Grounding) Input image: Example 1: Prompt What is the 3D coordinate of the gray moon? Generation [42, 11, 17] Example 2: Prompt What object is located at [42, 11, 17]? Generation The gray moon 54 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Environment-Anchored Prior Dataset for EB-Manipulation (Relative Coordinate Grounding) Input image: Example 1: Prompt What is the 3D location of the leftmost object? Generation [42, 11, 17] Example 2: Prompt What is the 3D location of the rightmost object? Generation [41, 80, 18] Environment-Anchored Prior Dataset for EB-Manipulation (Combination of Absolute & Relative Grounding) Input image: Example 1: Prompt Is the object located at [42, 11, 17] the leftmost in the scene? Generation Yes Example 2: Prompt Is the object located at [42, 11, 17] the rightmost in the scene? Generation No G.4 External Knowledge Prior Dataset We provide an example of External Knowledge Prior dataset for both EB-ALFRED and EB-Manipulation. ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning External Knowledge Prior Dataset for EB-ALFRED (OpenO1-SFT) Prompt ## You are robot who is great at reasoning. Given instruction, you must generate thought process and then an output. Question: To find the number of positive five-digit numbers that have 1, 3, 7, or 5 in them. Generation Alright, need to find the number of positive five-digit numbers that have 1, 3, 5, or 7 in them. First, lets clarify the problem to ensure understand it correctly. It seems that the task is to count all five-digit numbers where at least one of the digits is either 1, 3, 5, or 7. To approach this, can consider the total number of five-digit numbers and then subtract those that do not contain any of the digits 1, 3, 5, or 7. **Step 1: Calculate the Total Number of Five-Digit Numbers** Five-digit numbers range from 10000 to 99999. To find the total number of five-digit numbers: - The first digit (ten-thousands place) can be from 1 to 9 (since it cannot be 0), giving 9 options. - The remaining four digits (thousands, hundreds, tens, and ones places) can each be from 0 to 9, giving 10 options each. Total five-digit numbers = 9 10 10 10 10 = 90, 000 **Step 2: Calculate the Number of Five-Digit Numbers Without 1, 3, 5, or 7** If number does not contain any of the digits 1, 3, 5, or 7, the allowed digits are 0, 2, 4, 6, 8, and 9. - The first digit cannot be 0 (to maintain five-digit number), so it can be 2, 4, 6, 8, or 9. Thats 5 options. - The remaining four digits can be 0, 2, 4, 6, 8, or 9, giving 6 options each. Number of five-digit numbers without 1, 3, 5, or 7 = 5 6 6 6 6 = 5 Calculating 64: 62 = 36 63 = 216 64 = 1296 So, number = 5 1296 = 6480 **Step 3: Subtract to Find the Desired Number** Number of five-digit numbers with at least one digit being 1, 3, 5, or 7 = Total five-digit numbers - Numbers without 1, 3, 5, or 7 = 90, 000 6, 480 = 83, 520 But wait, lets double-check to ensure there are no mistakes in the calculation. **Verification:** Total five-digit numbers: 90,000 Numbers without 1, 3, 5, or 7: 5 (first digit) 64 (remaining digits) = 5 1296 = 6480 Subtracting: 90, 000 6, 480 = 83, 520 Everything seems correct. There are no overlapping cases or double-counting issues since the approach uses complementary counting. ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning External Knowledge Prior Dataset for EB-Manipulation (Spacethinker) Input image Prompt ## You are robot who is great at spatial reasoning. Given instruction, you must generate thought process and then an output. Question: How far apart are the snow-covered parked cars and the snowy street with parked cars? Generation see street covered with snow, and several parked cars also dusted with snow. Additionally, there is cyclist riding along the road. The snow covers both the cars and the roadway, suggesting contiguous snowy scene. Now, determining the distance between snow-covered parked cars and the snowy street with parked carssuggests considering the separation between two distinct areas within this same scene. Since both the cars and the street are snow-covered, it might seem as if they are tightly connected in context; however, finding distance here usually involves looking at the lateral space due to the lack of any actual intervening physical barrier or wide expanse in the image. estimate this minimal distance to be the space separating the parked cars from the street edge directly in front of them, without significant gap in the visible scene. Given these observations, the notion of any small distance like 20.33 inches feels particularly plausible, most likely representing the buffer or visual gap where the snow transitions from the cars to the street with virtually no space. This would particularly make sense if the focus were on something like the offset from the curb which is often minor. Therefore, although exactness in this scenario might be less definitive visually, conceptually small distance effectively represents the snow transition directly from car to the street, accounting for negligible separation. There is essentially no significant distance between the snow-covered parked cars and the snowy street with parked cars, with the understanding that 20.33 inches could be seen as the minimal or visual margin separating the snow on the cars from the street-level snow. G.5 Prompt for Generating Reasoning Augmentation To generate the trajectory-augmented priors as described in Section 4.1.1, we leverage GPT-4o, to enrich raw trajectories with structured reasoning. This process involves prompting the model to generate visual description, reflection on past actions, and language plan for following steps. The following prompt is used to elicit these detailed reasoning traces from GPT-4o, transforming action-only sequences into knowledge-rich data for Embodied Prior Learning. 57 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning"
        },
        {
            "title": "Prompt",
            "content": "For the following task: {} You have generated the following multi-step plan to complete the task: {} You have executed the first {} actions of the plan. The next action to be executed is {}. Now you need to follow the multi-step plan to generate the next multi-step plan including \"visual state - description\", \"reasoning and reflection\", and \"language plan\" in the format of JSON object. Make sure the step number in the language plan starts from 1. G.6 Rule-based Visual Description Generation for EB-Manipulation To obtain consistent and interpretable ground-truth visual descriptions, we develop an algorithm that extracts objectcolor associations from the simulation environment and composes structured, color-aware description of the scene. The algorithm proceeds in two stages: (i) extraction of objectcolor pairs via nearest-neighbor color classification in normalized RGB space, and (ii) synthesis of natural-language description that integrates both semantic and spatial information. Inputs. Simulator object tree ; object type SHAPE. Canonical color map κ : [0, 1]3 (predefined color names RGB in [0, 1]). Aligned coordinate maps: SimCoord : Rk, RealCoord : Rk. Outputs. Sentence S: From left to right, can see . . . Procedure: 1. Extract shapes. Query for all SHAPE objects with exclude base=true to obtain S. 2. Read attributes. For each S, record its simulator name name(s) and normalized color rgb(s) [0, 1]3. 3. Classify color. For each rgb(s), choose fcol(s) = arg mincC rgb(s) κ(c)2. 4. Build color map. Set color dict[name(s)] fcol(s) (fallback unknown if missing). 5. Align names/coords. Iterate over paired entries from (sim name coord) and (real name coord) (the alignment implied by the paired dictionaries). 6. Label composition. For each pair, form color-prefixed label: Preserve full real name for exceptions = {sponge, shape sorter}. Otherwise, drop the first token of the real name and prefix with color (e.g., blue block). 7. Article selection. Choose an if the label begins with vowel, else a. 8. Ordering (optional, recommended). Sort objects by the x-coordinate to enforce true left-to-right enumeration. 9. Sentence assembly. Convert each (label, coord) to the phrase a/an label at coord; join with commas and final and. 58 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Pseudocode for Generating Rule-based Visual Description Algorithm 3 Generate(SimCoord, RealCoord, color dict) Require: Aligned dictionaries of simulator names coords and real names coords; color map color dict Ensure: Sentence c color dict.get(sim name, unknown) if real name {sponge, shape sorter} then 1: [ ] 2: for all paired entries (sim name, x) SimCoord and (real name, y) RealCoord do 3: 4: 5: 6: 7: (real name with first token removed) real name else list of (label, x) preserve full name end if Append (L, x) to 8: 9: 10: end for 11: Optional: sort by the x-axis of (left-to-right) 12: [ ] 13: for all (L, x) do 14: 15: 16: end for 17: Construct sentence by joining elements of D, prefixed with From left to right, can see . 18: return an if first character of is vowel; else Append string at to G.7 Rule-based action mapping for EB-ALFRED To build our environment-anchored prior datasets (masked action modeling and action sequence reordering), we start from the ALFRED training set. central challenge is the mismatch between the action spaces of ALFRED and EB-ALFRED: ALFRED uses high-level, PDDL-style actions (e.g., CleanObject), whereas EB-ALFRED uses more granular set of phrase-based actions (e.g., turn on the faucet). To reconcile this, we designed deterministic, rule-based mapper that converts each high-level ALFRED action into sequence of one or more EB-ALFRED actions. The conversion preserves the semantic intent of the original trajectories while conforming to the target action space. The mapping addresses several cases: Composite Actions:High-level verbs like CleanObject expand into logical sequence of primitives (e.g., locate faucet, turn it on, then turn it off). Context-Dependent Actions: The mapping for PutObject depends on the receptacle. If the target (e.g., microwave) must be opened first, the sequence is prepended with an open the ... action. Stateful Actions: For ToggleObject, the mapping alternates between turn on the ... and turn off the ... based on the inferred object state. The complete mapping from ALFRED actions to EB-ALFRED action sequences is detailed in Table 12. After transforming an original action sequence into the new EB-ALFRED action space using these rules, we executed the mapped sequence in the simulator to verify that all mappings were valid. 59 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Table 12 Rule-based mapping from ALFRED actions to EB-ALFRED action sequences. Arguments like obj and loc are placeholders for specific objects and locations from the original action."
        },
        {
            "title": "ALFRED Action",
            "content": "Mapped EB-ALFRED Action Sequence GotoLocation(loc) find {loc} PickupObject(obj) pick up the {obj} SliceObject(obj) slice the {obj} ToggleObject(obj) Alternates between turn on the {obj} and turn off the {obj}. NoOp() (Ignored; maps to an empty sequence) PutObject(obj, loc) If loc requires opening (e.g., Fridge, Cabinet): CleanObject(obj) CoolObject(obj) HeatObject(obj) 1. 2. Else: 1. open the {loc} put down the object in hand put down the object in hand 1. 2. 3. 4. 5. 6. 1. 2. 3. 4. 5. 6. 7. 1. 2. 3. 4. 5. 6. 7. 8. 9. put down the object in hand find Faucet turn on the Faucet turn off the Faucet find {obj} pick up the {obj} open the Fridge put down the object in hand close the Fridge open the Fridge find {obj} pick up the {obj} close the Fridge open the Microwave put down the object in hand close the Microwave turn on the Microwave turn off the Microwave open the Microwave find {obj} pick up the {obj} close the Microwave 60 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning"
        },
        {
            "title": "H Case Analysis",
            "content": "In this section, we present both successful and unsuccessful planning examples for EPL only and ERA (EPL + RL) across EB-ALFRED and EB-Manipulation. Refer to Figures 8, 9, 10, and 11 for detailed reasoning and planning. Figure 8 Reflection error Example in EB-Manipulation. ERA successfully identified the correct target object: the azure triangular prism, while EPL mistakenly selected the yellow cube. 61 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Figure 9 Successful reflection Example in EB-Manipulation. Both agents were able to identify the silver star as the target object. ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Figure 10 Planning error example in EB-ALFRED. ERA successfully identified the second spray bottle as SprayBottle 2 while EPL repeatedly located the same SprayBottle. 63 ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning Figure 11 Successful reflection example in EB-ALFRED. Both agents were able to identify the FloorLamp as the target object."
        }
    ],
    "affiliations": [
        "Northwestern University",
        "Toyota Research Institute",
        "UIUC"
    ]
}