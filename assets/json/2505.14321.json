{
    "paper_title": "Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or True Temporal Understanding?",
    "authors": [
        "Bo Feng",
        "Zhengfeng Lai",
        "Shiyu Li",
        "Zizhen Wang",
        "Simon Wang",
        "Ping Huang",
        "Meng Cao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing video understanding benchmarks often conflate knowledge-based and purely image-based questions, rather than clearly isolating a model's temporal reasoning ability, which is the key aspect that distinguishes video understanding from other modalities. We identify two major limitations that obscure whether higher scores truly indicate stronger understanding of the dynamic content in videos: (1) strong language priors, where models can answer questions without watching the video; and (2) shuffling invariance, where models maintain similar performance on certain questions even when video frames are temporally shuffled. To alleviate these issues, we propose VBenchComp, an automated pipeline that categorizes questions into different domains: LLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questions can be answered without viewing the video; Semantic questions remain answerable even when the video frames are shuffled; and Temporal questions require understanding the correct temporal order of frames. The rest of the questions are labeled as Others. This can enable fine-grained evaluation of different capabilities of a video LLM. Our analysis reveals nuanced model weaknesses that are hidden by traditional overall scores, and we offer insights and recommendations for designing future benchmarks that more accurately assess video LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 1 2 3 4 1 . 5 0 5 2 : r Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or True Temporal Understanding? Bo Feng, Zhengfeng Lai, Shiyu Li, Zizhen Wang Simon Wang, Ping Huang, Meng Cao Apple {bfeng2, jeff lai, shiyu li, wang zizhen}@apple.com {simon wang2, huang ping, mengcao}@apple.com Equal contributions; Corresponding author;Senior authors"
        },
        {
            "title": "Abstract",
            "content": "Existing video understanding benchmarks often conflate knowledge-based and purely image-based questions, rather than clearly isolating models temporal reasoning ability, which is the key aspect that distinguishes video understanding from other modalities. We identify two major limitations that obscure whether higher scores truly indicate stronger understanding of the dynamic content in videos: (1) strong language priors, where models can answer questions without watching the video; and (2) shuffling invariance, where models maintain similar performance on certain questions even when video frames are temporally shuffled. To alleviate these issues, we propose VBenchComp, an automated pipeline that categorizes questions into different domains: LLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questions can be answered without viewing the video; Semantic questions remain answerable even when the video frames are shuffled; and Temporal questions require understanding the correct temporal order of frames. The rest of the questions are labeled as Others. This can enable finegrained evaluation of different capabilities of video LLM. Our analysis reveals nuanced model weaknesses that are hidden by traditional overall scores, and we offer insights and recommendations for designing future benchmarks that more accurately assess video LLMs."
        },
        {
            "title": "Benchmark",
            "content": "Table 1: A100 GPU hours needed for evaluating video LLMs (Qwen2-VL) across different benchmarks. The rapid progress of video Large Language Models (video LLMs) has led to the emergence of wide range of video understanding benchmarks, such as VideoMME [4], MLVU [5], LongVideoBench [1], EgoSchema [2], and others. While this surge of benchmarks offers broader coverage for evaluating different capabilities, it also introduces considerable computational cost and redundancy. As shown in Table 1, evaluating 2B-parameter model (e.g., Qwen2-VL) across existing video QA benchmarks requires 190.6 A100 GPU hours. This computational cost escalates dramatically to 491.9 hours for 72B LongVideoBench [1] Egoschema [2] NexTQA [3] VideoMME [4] MLVU [5] LVBench [6] PerceptionTest [7] 1337 500 4996 2700 2174 1549 19140 26.4 6.7 58.0 50.7 31.7 22.2 296.3 15.0 2.2 20.0 20.7 15.9 8.7 131. 14.3 2.7 16.0 18.0 14.1 7.5 118.1 Model Size 7B"
        },
        {
            "title": "Number of\nQuestions",
            "content": "32396 190.6 213.7 491."
        },
        {
            "title": "Total",
            "content": "72B 2B Preprint. Figure 1: Examples of LLM-Answerable, Semantic and Temporal questions in VideoMME [4]: (Top) The model uses LLMs prior knowledge to answer correctly without the need of video; (Middle) The model relies on semantic understanding to answer without requiring temporal comprehension; (Bottom) The model relies on comprehensive temporal understanding to answer. model, raising serious concerns about the computational burden of benchmarking video LLMs given the growing number of video understanding datasets. Beyond the computational cost, current video understanding benchmarks often conflate different skills and fail to truly evaluate the video understanding capability. We identify two key limitations that undermine meaningful evaluation. First, some questions can be answered correctly without access to the video, since models rely on their pretrained language priors rather than visual evidence, as shown in Figure 1. These questions primarily test the underlying LLMs factual knowledge and reasoning skills, rather than evaluating the models ability to process and understand visual content. As result, high performance on these questions can misleadingly inflate benchmark scores, giving the false impression of strong video understanding when, in fact, the model may not be attending to the visual input at all. Second, some questions primarily assess static semantic understanding and do not require comprehension of the videos temporal structure. For example, models often achieve similar performance even when the video frames are randomly shuffled, indicating that their predictions rely heavily on spatial or frame-level cues rather than temporal reasoning. This shuffling invariance exposes critical flaw: current benchmarks may significantly overestimate models true temporal understanding, conflating static visual recognition with dynamic sequence reasoning. 2 While many existing benchmarks claim to be comprehensive, there is currently no standardized protocol for assessing their effectiveness. Each dataset emphasizes different aspects of video comprehension, yet lacks clear metric for how well it captures temporal reasoning, which is the core capability that distinguishes video from static images. We introduce VBenchComp, an automated evaluation pipeline that categorizes questions into four distinct domains: LLM-Answerable, Semantic, Temporal, and Other. This structured categorization disentangles the contributions of language priors, static visual understanding, and genuine temporal reasoning, enabling more diagnostic and interpretable evaluation of video models. Based on this, we curate core benchmark subset that emphasizes both semantic and temporal understanding, and introduce dedicated metric, the VBenchComp Score, which provides more focused and light-weighted evaluation protocol to better guide model development and comparison. Importantly, we find that results obtained from this core set are consistent with those from the full benchmark suite, while reducing computational cost significantly."
        },
        {
            "title": "2 Related Works",
            "content": "Video Large Language Models (Video LLMs). Large Language Models (LLMs) have revolutionized natural language understanding, demonstrating exceptional ability to follow human instructions and serving as versatile agents for general-purpose AI assistants [8, 9, 10]. Building on these advancements, Multimodal Large Language Models (MLLMs) [11, 12] have made significant strides in vision-language learning by incorporating visual encoders with LLMs and fine-tuning on visionlanguage instruction data. In addition, video Large Language Models (video LLMs) incorporate visual encoders to extract video features, temporal modeling mechanisms to capture motion dynamics, and large language models to generate responses [13]. For instance, Video-ChatGPT [14] employs CLIP [15] to obtain per-frame representations, which are subsequently processed through spatial and temporal pooling before being fed into an LLM. LLaVA-NeXT-Video [16] builds on LLaVANeXT [12], adapting it for video-based tasks, while its DPO-enhanced variant [16] further refines output quality by aligning responses with AI-generated feedback. To improve temporal consistency, VideoLLaMB [17] incorporates memory tokens within its bridge layers, allowing the model to capture both sequential dependencies and historical visual context. InternVideo2.5 [18] enhances multimodal models by leveraging annotations from dense vision tasks, optimizing preferences directly, and refining spatiotemporal representations through hierarchical token compression. This enables better handling of detailed video content and extended temporal reasoning. Apollo [19] introduces structured methodology for training video LLMs, while Oryx [20] proposes OryxViT, vision transformer pre-trained to encode images at arbitrary resolutions into representations compatible with LLMs. Oryx also integrates dynamic compression module that adjusts visual token density, allowing compression levels from 1x to 16x based on task requirements. VideoLLaMA-3 [21] highlights the importance of high-quality image-text data for both image and video comprehension. Video Understanding Benchmarks. Traditional evaluations of video LMMs rely on classic video QA benchmarks such as MSRVTT-QA [22] and ActivityNet-QA [23], which assess global video understanding through summary-based questions. However, prior work has shown that these datasets can often be answered using only handful of key frames, limiting their effectiveness in evaluating true temporal reasoning [1]. More recent benchmarks, such as NeXT-QA [3] and MVBench [24], focus on short clips (averaging 44 and 16 seconds, respectively), while Video-MME [4] spans diverse set of video domains and durations. LongVideoBench [1] explicitly targets referring reasoning over long videos. While each benchmark aims to provide comprehensive evaluation of video understanding, there is still no standardized protocol to assess their effectivenessparticularly in measuring temporal reasoning. rigorous framework is needed to systematically evaluate these benchmarks, ensuring that they go beyond static frame-based understanding and capture the core challenges of video comprehension."
        },
        {
            "title": "3 A Closer Look at Video Understanding Benchmarks",
            "content": "Evaluating the video understanding capabilities of recent video LLMs is complex and multifaceted task. Although many benchmarks exist, their ability to capture the depth of reasoning needed for real-world video comprehension remains uncertain. Some focus on higher-level skills like temporal reasoning, causal inference, and fine-grained event recognition, while others may primarily focus 3 on semantic understanding. To explore this, we take closer look at several widely used video understanding benchmarks. We select VCGBench [14] and ActivityNet-QA [23] as representative open-ended QA benchmarks. We use NeXT-QA [3], VideoMME [4], EgoSchema [2], MLVU [5], and LongVideoBench [1] to serve as examples of multiple-choice QA benchmarks."
        },
        {
            "title": "3.1 Answering Questions without Videos",
            "content": "Inspired by [25], we begin by evaluating whether questions can be answered without access to the corresponding videos. To do this, we input the questions into various MLLMs without providing the videos. As shown in Figure 2, surprisingly, GPT-4o achieves up to 50% accuracy on both VideoMME and NExT-QA, despite not processing any video data. Similarly, open-sourced models like PLLaVA-34B also achieve 37.0% accuracy on VideoMME without video input. For long video understanding (LongVideoBench [1]), these models even surpass 35% accuracy without feeding in the actual long videos. These results cast serious doubt on the reliability of these benchmarks in accurately assessing models video understanding capabilities. The fact that models can achieve notable performance without processing the video data suggests that these benchmarks may be evaluating factors unrelated to true video comprehensionsuch as reliance on text-based cues or prior knowledge. This raises the question of whether these benchmarks are an effective measure of multimodal models abilities to understand and process video content. Figure 1 shows an example that the question can be directly answered by an LLM without watching the video. Figure 2: Performance of different MLLMs without videos as the input on four benchmarks."
        },
        {
            "title": "3.2 Shuffled Frames but Unshaken Scores",
            "content": "Besides the aforementioned issues on the text, we also identify critical concern regarding the semantic and temporal understanding. In recent video LLMs, multiple frames are typically sampled and fed into the model. However, many questions in these benchmarks may not adequately assess models ability to understand the temporal dynamics of videos. natural approach to evaluating temporal understanding would be to test whether shuffling the frames affects the models final answer. If model can still produce accurate responses despite shuffled frames, it suggests that the question may not require deep understanding of temporal relationships, but rather relies on static or semantic content from the frames themselves. We conduct experiments with variety of representative video LLMs across different model types: 1) closed-source models such as GPT-4o and Gemini-1.5-Pro [29], which set the state-of-the-art standard for video understanding; 2) trainingfree models such as SlowFast-LLaVA [26], which leverage pre-trained visual and language models without additional fine-tuning; 3) LoRA fine-tuned models, e.g., PLLaVA [27], which demonstrate the effectiveness of parameter-efficient adaptation; and 4) video SFT models, such as LLaVA-OneVision (LLaVA-OV) [28], which benefit from supervised fine-tuning with video datasets. This selection enables us to systematically assess the effect of shuffled frames on wide range of models. As shown in Figure 3, we apply frame shuffling twice and observe that the scores of both GPT-4o and Gemini-1.5-Pro remain remarkably stable, indicating that these models are largely unaffected by temporal disruptions. similar pattern is observed in open-source models such as SlowFastLLaVA, PLLaVA, and LLaVA-OV, despite differences in their training paradigms and architectures. Interestingly, this insensitivity to temporal order persists across models of varying sizes. For instance, the large model (LLaVA-OV-72B-Qwen2) and the smaller model (LLaVA-OV-7B-Qwen2) exhibit consistent behavior across all six benchmarks. Surprisingly, in some cases, shuffling the frames even leads to an improvement in performance. For example, Gemini-1.5-Pro achieves higher score on EgoSchema after frame shuffling, and GPT-4o also performs better on NExT-QA under the same condition. This counter-intuitive result raises critical concerns about the validity of these benchmarks in evaluating true video understanding. If models can achieve higher scores after the temporal sequence is disrupted, it suggests that these benchmarks may not be adequately assessing 4 (a) Close-sourced GPT-4o (b) Training-free Model: SlowFast-LLaVA-7B [26] (c) LORA Fine-tuned Model: PLLaVA-7B [27] (d) Video SFT Model: LLaVA-OV-7B-Qwen2 [28] Figure 3: After shuffling the extracted frames, the scores of each model remain unshaken across all benchmarks. *Frame settings: (a), (d) uses 128 frames for VideoMME-long, others use 64 frames; (b) uses 10slow + 50fast frames for all benchmarks; (c) uses 16 frames for all benchmarks. the models ability to comprehend and reason over temporal information, which is core component of video understanding."
        },
        {
            "title": "3.3 Potentially Misleading Scores in Current Video Benchmarks",
            "content": "Based on the above analysis, we find that single final score reported by current video benchmarks may not accurately reflect models true capability in video understanding: many of the tasks may be overly relying on prior language knowledge or semantic understanding rather than requiring genuine video understanding across dynamic frames. As result, models might excel by leveraging spatial correlations and semantic associations within individual frames, bypassing the need to process temporal dependencies. This raises the possibility that benchmark scores could be misleading, potentially leaving the impression that models possess more profound understanding of video content than they actually do. Moreover, these results suggest that current video benchmarks inadvertently prioritize evaluating the LLMs language proficiency and semantic understanding over its temporal comprehension of video content. This overemphasis can lead to biased evaluations, where models with strong language priors or frame-level understanding receive inflated scores, despite having limited capability to capture complex temporal dynamics. Such biases introduce the risk of drawing erroneous conclusions about models progress in video understanding, potentially giving false sense of achievement in the field thus making community risk overestimating the robustness and real-world applicability of these models. Therefore, we advocate for the development of more comprehensive evaluation protocols that disentangle language knowledge, semantic, and temporal understanding, ensuring more accurate and holistic assessment of video models. 5 Figure 4: An overview of our standardized protocol: benchmark questions are categorized into four groups. Questions answerable by both GPT-4o and Gemini without video are classified as LLM-Answerable. For the remaining questions, we apply random shuffles to the extracted frames twice: if both models answer correctly before and after shuffling, the question is classified as Semantic. If one model answers correctly before but fails after shuffling, the question is classified as Temporal. All other questions are categorized as Others."
        },
        {
            "title": "4 A Standardized Protocol for Breaking Down Video LLM Benchmarks",
            "content": "In this section, we propose standardized protocol (as shown in Figure 4) for decomposing video LLM benchmarks into four distinct domains: (1) LLM-answerable questions to focus on the prior language capabilities of the LLM backbone, (2) semantic understanding questions to evaluate the models ability to understand semantic content, (3) temporal understanding questions to measure the models capacity to capture temporal dependencies and dynamic changes, and (4) Other questions that may either require overly advanced comprehensive reasoning or are poorly constructed and thus lack sufficient distinctiveness. Our goal is to disentangle these question types to provide more precise and comprehensive evaluation of video LLMs. It will also assist future benchmarks in refining their question design strategies and focusing more on authentic video-understanding questions."
        },
        {
            "title": "4.1 LLM-Answerable Questions",
            "content": "Answer leakage is critical issue in image-QA benchmarks, where MLLMs can often generate correct answers without relying on the image itself. Instead of genuinely integrating visual and textual information, these models leverage their pre-trained knowledge from LLM to infer answers based solely on the text [25]. This undermines the intended goal of evaluating models multimodal understanding capabilities. Multimodal answer leakage can be summarized into two categories: 1) text-answerable questions, where the question itself provides sufficient information for the model to answer, rendering the associated visual input unnecessary; 2) memorized questions, where the MLLM has previously encountered the same question during training and recalls the corresponding answer from memory rather than reasoning from the given image. As result, certain questions can be answered solely by text-based LLM without requiring visual input. To assess this, we perform text-only evaluation using both GPT-4o and Gemini-1.5-Pro. As shown in Figure 4, if both models correctly answer given question without the video, we classify the corresponding QA pair as an LLM-Answerable question. We then analyze the entire benchmark and compute the proportion of such questions relative to the total, denoted as α."
        },
        {
            "title": "4.2 Semantic Questions: Shuffling Frames but Consistent Answer",
            "content": "After filtering for LLM-answerable questions, we further identify subset of questions that focus specifically on semantic understanding. To achieve this, we introduce diagnostic procedure: for each video-question pair, we first generate answers using Gemini-1.5-Pro and GPT-4o. We then shuffle the extracted frames and query the models again - repeating this process twice. If both models consistently provide correct answers despite the disrupted temporal order (before and after shuffling the extracted frames), we classify the question as semantic, indicating that static visual information from single or certain group of frames alone are sufficient for answering. By applying this procedure across the benchmark, we compute the proportion of such questions, denoted as β, to quantify the prevalence of questions relying solely on semantic understanding. high β suggests that the benchmark may be biased toward spatial or appearance-based cues, potentially inflating models perceived temporal reasoning capability. This highlights the need to construct more temporal-related questions that explicitly require sequential understanding to ensure more rigorous and targeted evaluation of video LLMs."
        },
        {
            "title": "4.3 Temporal Questions",
            "content": "After classifying questions into LLM-Answerable and Semantic categories, the remaining questions are further divided into Temporal and Others. To identify Temporal questions, we apply the following criterion: if GPT-4o or Gemini-1.5-Pro answers the question correctly when provided with frames in their original order but fails to do so after the frames are shuffled, we classify the question as Temporal, indicating that the right sequential information is crucial for the answering process. Unlike semantic or frame-independent tasks, these questions assess whether the model can correctly infer event progression and temporal consistency over time. By introducing controlled perturbationshuffling the frame order, we isolate the questions for temporal understanding capacity, distinguishing them from purely visual or semantic understanding."
        },
        {
            "title": "4.4 Others",
            "content": "Lastly, the remaining questions will be labeled as Others. This category includes questions that are either too difficult to answer for all SOTA models or are so comprehensive that they may require additional modalities, such as audio, to resolve. Questions may depend on recognizing spoken dialogue, distinguishing between environmental sounds, or interpreting non-visual context cues like tone or timing. For example, in VideoMME [4], answering certain questions may depend on recognizing spoken dialogue, distinguishing between environmental sounds, or interpreting non-visual context cues like tone or timing."
        },
        {
            "title": "4.5 VBenchComp: Quantifying Video Benchmark Composition",
            "content": "To systematically analyze and quantify the composition of video LLM benchmarks, we introduce VBenchComp, diagnostic tool that applies our standardized protocol (Figure 4) to decompose the benchmark into its four key domains. VBenchComp computes the ratios of LLM-Answerable, Semantic, Temporal, and Others questions, denoted as α, β, γ, and δ respectively. Benchmark profiling and skill gap identification. VBenchComp not only quantifies benchmark composition but also identifies potential gaps in coverage. For instance, an overrepresentation of LLM-Answerable questions (α) suggests that the benchmark may underestimate the need for genuine multimodal understanding. Conversely, an excess of Semantic questions (β) could create an illusion of strong temporal understanding, when in reality, the model might rely primarily on static frame information. low proportion of Temporal questions (γ) may indicate inadequate assessment of dynamic event comprehension."
        },
        {
            "title": "5.1 An Overview of VBenchComp",
            "content": "We apply the standardized categorization protocol described in Section 4.5 to seven widely-used video question answering benchmarks, quantifying their distributions across four diagnostic categories: 7 Table 2: Compositions of question types across different video understanding benchmarks. Each cell (except Total) shows the count and its percentage of the total."
        },
        {
            "title": "Dataset",
            "content": "LongVideoBench [1] Egoschema [2] NextQA [3] VideoMME [4] MLVU [5] LVBench [6] PerceptionTest [7]"
        },
        {
            "title": "Total",
            "content": "1337 500 4996 2700 2174"
        },
        {
            "title": "Others",
            "content": "308 / 23.03% 363 / 27.15% 133 / 26.60% 182 / 36.40% 1738 / 34.79% 1880 / 37.63% 810 / 30.00% 841 / 31.15% 643 / 29.57% 621 / 28.57% 321 / 20.72% 140 / 9.04% 431 / 32.24% 140 / 28.00% 941 / 18.83% 678 / 25.11% 527 / 24.23% 733 / 47.32% 3642 / 19.03% 6283 / 32.82% 3117 / 16.29% 6098 / 31.86% 235 / 17.58% 45 / 9.00% 437 / 8.75% 371 / 13.74% 383 / 17.62% 355 / 22.92% LLM-Answerable, Semantic, Temporal, and Others. Table 2 summarizes the raw counts and their corresponding percentages relative to the total number of questions in each benchmark. Across all benchmarks, we observe considerable variation in the proportion of question types, which reflects their differing emphases on language, semantic, and temporal capabilities. For instance, NextQA [3], LongVideoBench [1], MLVU [5], Egoschema [2], and VideoMME [4] contain significant portion of LLM-Answerable questions, which indicates potential answer leakage and reliance on language priors. In contrast, benchmarks like LVBench [6] contains relatively fewer LLM-Answerable questions. On the other hand, with the exception of LongVideoBench and LVBench, all other benchmarks have more than 30% of Semantic questions, where frame shuffling has minimal impact on the models ability to produce correct answers. Table 3: Benchmarking public models under VBenchComp categorization. (All settings use 64 frames, except for VideoMME-long, which uses 128.) (a) Egoschema [2] (b) NextQA [3] Model Overall LLM Semantic Temporal Others Size Model Overall LLM Semantic Temporal Others Size 7B 72B Size 7B 72B Size 7B 72B Qwen2-VL [30] LLaVA-OV [28] LLaVA-Video [31] 65.8 66.2 61.8 85.0 75.2 72.2 83.5 83.5 82.4 Qwen2-VL [30] LLaVA-OV [28] LLaVA-Video [31] 77.4 65.2 70.4 95.1 84.6 90.7 (c) VideoMME [4] 87.2 78.9 81.2 37.8 57.8 46.7 64.4 40.0 53.3 33.6 37.9 30.0 49.3 35.0 39.3 7B 72B Qwen2-VL [30] LLaVA-OV [28] LLaVA-Video [31] Qwen2-VL [30] LLaVA-OV [28] LLaVA-Video [31] 81.3 80.3 84.4 84.0 83.2 85.4 88.7 89.8 92. 91.1 93.4 94.0 90.9 91.1 92.3 92.6 93.9 94.7 70.0 65.7 73.7 70.9 66.6 73.7 54.1 48.2 56. 60.0 50.6 56.6 (d) MLVU [5] Overall LLM Semantic Temporal Others Model Overall LLM Semantic Temporal Others Size Model Qwen2-VL [30] LLaVA-OV [28] LLaVA-Video [31] Qwen2-VL [30] LLaVA-OV [28] LLaVA-Video [31] 60.6 59.0 63.9 68.2 68.7 70. 77.8 76.3 79.3 86.8 87.2 88.1 78.4 76.8 82.0 86.3 86.3 88.9 36.7 37.2 42.6 49.6 52.6 51. 31.1 28.2 34.7 33.8 33.6 38.1 7B 72B Qwen2-VL [30] LLaVA-OV [28] LLaVA-Video [31] Qwen2-VL [30] LLaVA-OV [28] LLaVA-Video [31] 62.5 65.2 63.7 67.9 74.2 74.2 77.8 77.1 77.8 81.8 88.1 87.4 79.5 88.0 83.1 85.4 92.5 92. 43.6 47.5 49.6 52.5 62.7 64.0 37.4 36.1 33.6 41.4 44.0 43.8 (e) LongVideoBench [6] (f) PerceptionTest [7] Model Overall LLM Semantic Temporal Others Size Model Overall LLM Semantic Temporal Others Qwen2-VL [30] LLaVA-OV [28] LLaVA-Video [31] Qwen2-VL [30] LLaVA-OV [28] LLaVA-Video [31] 52.8 58.9 59.8 58.0 59.8 62.8 74.4 79.1 81. 82.4 87.3 87.3 70.5 82.1 84.4 76.0 84.4 85.7 42.6 49.8 49.8 46.4 49.8 52.8 27.6 30.2 29. 30.9 32.9 31.3 7B 72B Qwen2-VL [30] LLaVA-OV [28] LLaVA-Video [31] Qwen2-VL [30] LLaVA-OV [28] LLaVA-Video [31] 60.7 58.0 68. 68.1 62.5 69.6 71.9 66.0 75.4 77.7 75.6 76.0 84.2 84.9 87.9 92.1 89.8 92.1 49.7 45.9 60. 62.7 50.6 61.2 35.3 31.9 47.8 40.5 32.8 47."
        },
        {
            "title": "5.2 Benchmarking Public Models Under VBenchComp Categorization",
            "content": "Table 3 benchmarks recent public video-language models under our proposed VBenchComp framework, which categorizes questions into LLM-answerable, Semantic, and Temporal types. This fine-grained categorization provides more diagnostic view of model capabilities compared to single overall score. As shown in Table 3(a), Qwen2-VL-7B slightly outperforms LLaVA-Video-7B in terms of the traditional overall score on Egoschema. However, this superficial advantage is misleading. breakdown of the scores shows that the performance gain is almost entirely due to LLM-answerable questions that do not require visual or temporal understanding. However, the two models perform similarly on Semantic questions, and Qwen2-VL-7B even lags behind on Temporal questions, which indicates weaker grasp of fine-grained video temporal understanding. These findings suggest that Qwen2-VL-7Bs advantage is largely attributable to its stronger language model backbone, rather than superior visual or temporal reasoning. In contrast, LLaVA-Video-7B, though slightly behind overall, demonstrates more balanced capabilities across semantic and temporal dimensions. 8 Interestingly, the comparison flips in VideoMME (Table 3(c)), where LLaVA-Video-7B outperforms Qwen2-VL-7B not just overall, but more meaningfully across both vision-dependent axes. While the two models perform similarly on LLM-answerable questions, LLaVA-Video-7B achieves notably higher scores on both Semantic (82.0 vs. 78.4) and Temporal (42.6 vs. 36.7) categories. This demonstrates that LLaVA-Video-7B possesses stronger visual and temporal understanding, reinforcing the claim that strong language knowledge alone are insufficient for robust video understanding. These results collectively demonstrate core limitation of traditional evaluation: single overall score fails to capture specific model strengths and weaknesses. Only through our VBenchComp categorization can we identify crucial gaps in semantic or temporal understanding that would otherwise be masked. This insight is not only critical for fair benchmarking but also for guiding the development of next-generation video LLMs, where improvement must go beyond language modeling and target true temporal understanding."
        },
        {
            "title": "5.3 VBenchComp Score: Fewer Questions, Deeper Video Understanding",
            "content": "Figure 5: VBenchComp scores are aligned with the original scores but they can better evaluate the overall video LLM performance with less questions. The temporal video understanding capability of models under the trend line can be potentially over-estimated in the original benchmarks. Based on the above analysis, we retain only the Semantic and Temporal questions from each benchmark to compute focused evaluation score, denoted as the VBenchComp score. The results across models are shown in Figure 5. Despite removing nearly 50% of the original questions (as detailed in Table 2), the model rankings remain highly consistent with those based on the original scores. This strong correlation indicates that Semantic and Temporal questions alone are sufficient to preserve the discriminative power of the benchmark. It further suggests that many of the remaining questions may be redundant or less critical for evaluating core model capabilities, and that VBenchComp can serve as more focused yet reliable metric for model comparison."
        },
        {
            "title": "6 Discussion",
            "content": "VBenchComp provides structured and interpretable framework for dissecting the capabilities of video LLMs, highlighting whether models rely on language priors, static semantics, or genuine temporal reasoning. This diagnostic lens not only clarifies what current benchmarks actually measure, but also helps researchers identify blind spots in model behavior. However, our approach is not without limitations. First, while our categorization pipeline is automated and scalable, it heavily relies on GPT-4o and Gemini, which may introduce biases. Second, our core benchmark subset, while compute-efficient and representative in aggregate, may omit edge cases that appear in the full benchmark suite. Finally, VBenchComp focuses primarily on question-answering tasks; generalizing this framework to other video understanding tasks like captioning, retrieval, or grounding remains an important avenue for future work."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Mingze Xu, Junting Pan, Wentao Wu, Dongxu Li, Haotian Zhang, and Zhe Gan for their great help, feedback, and fruitful discussions."
        },
        {
            "title": "References",
            "content": "[1] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding, 2024. [2] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. NeurIPS, 2024. [3] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. NExT-QA: Next phase of question-answering to explaining temporal actions. In CVPR, 2021. [4] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [5] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. [6] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. [7] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36:42748 42761, 2023. [8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. JMLR, 2023. [9] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288, 2023. [10] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv:2303.08774, 2023. [11] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv:2310.03744, 2023. [12] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVANeXT: Improved reasoning, ocr, and world knowledge, 2024. [13] Shimin Chen, Yitian Yuan, Shaoxiang Chen, Zequn Jie, and Lin Ma. Fewer tokens and fewer videos: Extending video understanding abilities in large vision-language models. arXiv:2406.08024, 2024. [14] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-ChatGPT: Towards detailed video understanding via large vision and language models. In ACL, 2024. [15] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [16] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. LLaVA-NeXT: strong zero-shot video understanding model, 2024. [17] Yuxuan Wang, Cihang Xie, Yang Liu, and Zilong Zheng. Videollamb: Long-context video understanding with recurrent memory bridges. arXiv:2409.01071, 2024. 10 [18] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. Internvideo2. 5: Empowering video mllms with long and rich context modeling. arXiv:2501.12386, 2025. [19] Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, et al. Apollo: An exploration of video understanding in large multimodal models. arXiv:2412.10360, 2024. [20] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx MLLM: On-demand spatial-temporal understanding at arbitrary resolution. ICLR, 2025. [21] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. VideoLLaMA 3: Frontier multimodal foundation models for image and video understanding. arXiv:2501.13106, 2025. [22] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In ACM Multimedia, 2017. [23] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. ActivityNet-QA: dataset for understanding complex web videos via question answering. In AAAI, 2019. [24] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. [25] King Zhu, Qianbo Zang, Shian Jia, Siwei Wu, Feiteng Fang, Yizhi Li, Shawn Gavin, Tuney Zheng, Jiawei Guo, Bo Li, et al. Lime: Less is more for mllm evaluation. arXiv preprint arXiv:2409.06851, 2024. [26] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Dehghan. SlowFast-LLaVA: strong training-free baseline for video large language models. arXiv:2407.15841, 2024. [27] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. PLLaVA: Parameter-free llava extension from images to videos for video dense captioning. arXiv:2404.16994, 2024. [28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [29] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [30] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [31] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-LLaVA: Learning united visual representation by alignment before projection. arXiv:2311.10122, 2023."
        }
    ],
    "affiliations": [
        "Apple"
    ]
}