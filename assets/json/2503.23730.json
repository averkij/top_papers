{
    "paper_title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language",
    "authors": [
        "Yoonshik Kim",
        "Jaeyoon Jung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a variety of different benchmarks for evaluating such models. Despite this, we observe that most existing evaluation methods suffer from the fact that they either require the model to choose from pre-determined responses, sacrificing open-endedness, or evaluate responses using a judge model, resulting in subjective and unreliable evaluation. In addition, we observe a lack of benchmarks for VLMs in the Korean language, which are necessary as a separate metric from more common English language benchmarks, as the performance of generative language models can differ significantly based on the language being used. Therefore, we present KOFFVQA, a general-purpose free-form visual question answering benchmark in the Korean language for the evaluation of VLMs. Our benchmark consists of 275 carefully crafted questions each paired with an image and grading criteria covering 10 different aspects of VLM performance. The grading criteria eliminate the problem of unreliability by allowing the judge model to grade each response based on a pre-determined set of rules. By defining the evaluation criteria in an objective manner, even a small open-source model can be used to evaluate models on our benchmark reliably. In addition to evaluating a large number of existing VLMs on our benchmark, we also experimentally verify that our method of using pre-existing grading criteria for evaluation is much more reliable than existing methods. Our evaluation code is available at https://github.com/maum-ai/KOFFVQA"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 0 3 7 3 2 . 3 0 5 2 : r KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language Yoonshik Kim Jaeyoon Jung * MAUM AI Inc. / Republic of Korea {yoonshik1205, jyjung}@maum.ai"
        },
        {
            "title": "Abstract",
            "content": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in variety of different benchmarks for evaluating such models. Despite this, we observe that most existing evaluation methods suffer from the fact that they either require the model to choose from pre-determined responses, sacrificing open-endedness, or evaluate responses using judge model, resulting in subjective and unreliable evaluation. In addition, we observe lack of benchmarks for VLMs in the Korean language, which are necessary as separate metric from more common English language benchmarks, as the performance of generative language models can differ significantly based on the language being used. Therefore, we present KOFFVQA, general-purpose free-form visual question answering benchmark in the Korean language for the evaluation of VLMs. Our benchmark consists of 275 carefully crafted questions each paired with an image and grading criteria covering 10 different aspects of VLM performance. The grading criteria eliminate the problem of unreliability by allowing the judge model to grade each response based on pre-determined set of rules. By defining the evaluation criteria in an objective manner, even small open-source model can be used to evaluate models on our benchmark reliably. In addition to evaluating large number of existing VLMs on our benchmark, we also experimentally verify that our method of using preexisting grading criteria for evaluation is much more reliable than existing methods. Our evaluation code is available at https://github.com/maum-ai/KOFFVQA. 1. Introduction In recent years, there has been much research on Large Vision-Language Models(VLMs), giving rise to models with unprecedented capabilities in various applications [35, 54]. However, evaluating the performance of such genera- *Equal Contribution. tive models remains difficult task [20]. Although there are many benchmarks for evaluating the performance of VLMs, they are limited by the fact that the performance of generative language model is hard to measure quantitatively [8]. Most benchmarks either restrict the model to respond with one of several pre-defined answers [10, 14, 18, 39, 56] or evaluate the outputs of the model using potentially subjective metrics such as LLM judges [37, 55]. An important property of generative language models, including VLMs, is that they have the ability for open-ended generation of long-form responses [54]. When evaluating the general-purpose performance of such models, not being able to harness this ability can be limiting. However, evaluating long-form responses is very challenging [20]. Many benchmarks use an LLM or VLM judge to evaluate such responses, but this can be subjective and unreliable [8, 49, 50]. This approach can also be biased due to the tendency of judge models to give comparatively higher scores to the responses of similar models [44, 58]. In addition, grading VLM benchmark using judge model requires the judgment process to incorporate information from both images and language queries, unlike LLM benchmarks. This can be achieved through methods such as 1) using VLM-as-a-judge approach and providing the images directly to the judge [9, 31], 2) prompting the judge to evaluate the response by comparing it to second baseline or ground-truth response [37, 55], or in the case of specialized benchmarks, 3) using some ground-truth information extracted from the image in the judgment process [34]. However, each of these methods has limitations. In the first approach, when VLMs are used to judge responses, such models can be prone to hallucinations based on visual input [50] or can be misaligned with human intention [9]. In the second approach, comparison with baseline or groundtruth response can be limiting since there may be more than one way to correctly answer question. In the third approach, it is difficult to generalize such methods outside the domain of specialized benchmarks. To address these challenges, we propose novel approach to evaluating such open-ended responses in VLM benchmarks: providing list of objective pre-defined grading criteria paired with partial scores to an LLM judge in order to reliably grade long-form responses to queries. Additionally, we propose that using VLM as judge and providing images to aid in judgment can negatively affect the consistency of evaluation when using our approach due to the tendency of VLMs to hallucinate based on visual input [50] and that our approach does not suffer from this as it uses an LLM judge. Furthermore, despite the fact that the performance of generative language models can differ greatly based on the language being used [8], there are very few benchmarks for VLMs in languages other than English or Chinese. In order to evaluate the performance of VLMs in other languages, such as Korean, there is need for general-purpose benchmarks that can evaluate various aspects of the performance of VLM in those languages. Based on this, we introduce KOFFVQA, carefully hand-crafted general-purpose visual question answering(VQA) benchmark for the Korean language capable of objectively evaluating free-form responses from models. Each question consists of an image, the corresponding question, and list of pre-defined grading criteria for responses. For each given image-question pair, the response of VLM is evaluated by an LLM judge that is simply instructed to score the response based on the given criteria. In addition, the tasks in our benchmark attempt to cover as many aspects of VLM performance relevant to real-world applications as possible. We propose that this benchmark does not suffer from the issues that affect the reliability of evaluating long-form responses, and that it fills the much-needed gap in general-purpose VLM benchmarks for the Korean language. We evaluate 47 total open-source and proprietary VLMs on our benchmark, and also experimentally compare our methodology with existing approaches to VLM benchmarks. Our contributions are as follows: We create and release KOFFVQA, general-purpose Korean-language VLM benchmark that uses questionspecific grading criteria to objectively evaluate free-form responses. Our benchmark consists of 275 human-written questions each paired with an image and their corresponding grading criteria across 10 subcategories designed to evaluate various aspects of VLMs performance. We evaluate 47 openand closed-source VLMs on our benchmark. We find that larger model does not necessarily mean higher performance unlike in English-language benchmarks, and that models that excel in some subcategories do not necessarily excel in others. We compare our approach of benchmark evaluation with the existing method of comparing each response with baseline. We find that our approach significantly increases the consistency of evaluation compared to the existing method. Furthermore, the fact that our approach can utilize LLM-as-a-judge for evaluation mitigates the issues that may arise when using VLM-as-a-judge due to the tendency of VLMs to hallucinate based on visual input. We find that using the LLM-as-a-judge method is more reliable and accurate when using our approach compared to using the VLM-as-a-judge method with image input. 2. Related Work 2.1. VLM Evaluation Benchmarks There has been much work on benchmarks for evaluating large generative vision-language models [9, 10, 15, 18, 37, 39, 40, 48, 5456]. While there is large variety of benchmarks that evaluate various aspects of performance, the open-ended nature of text generation makes it difficult for such generated responses to be evaluated using traditional metrics [8]. Due to this, existing VLM benchmarks largely fall into the following two categories: Multiple Choice Question Answering. These benchmarks provide fixed set of possible answers to each question(including simply yes or no), and prompt the model to choose the correct answer. This is usually achieved by either prompting the VLM to generate response in format that can be easily parsed [10] or using separate LLM to extract the answer choice from the VLMs response [39]. Such benchmarks include MME [14], MMBench [39], MMStar [10], MMMU [56], and HallusionBench [18]. This approach allows for objective evaluation, but is limited in that it cannot evaluate long free-form responses, which are characteristic of generative models. Free-form Question Answering with Subjective Evaluation. This approach involves prompting the VLM to generate free-form response to question, then prompting separate judge model(either an LLM or VLM) to assign score to each generated response, such as by comparing the generated response to reference response [37] or by grading the quality of the response using subjective criteria [9]. Such benchmarks include MM-Vet [55] and LLaVABench [37]. This approach is useful for evaluating the quality of long free-form responses but suffers from the subjectivity of evaluation criteria, which can often lead to inconsistent or biased evaluation [46, 58]. There are some exceptions, most notably benchmarks that require the model to output an exact single word or phrase such as TextVQA [48] and MathVista [40]. 2.2. Fine-grained Evaluation Using LLMor VLM-as-a-judge methods for question answering with subjective evaluation has the aforementioned issues of inconsistent or biased evaluation. As such, there has been some work on mitigating this using fine-grained evaluation criteria. Prometheus [26] and Prometheus-vision [31] are judge models that can evaluate the responses of model on benchmarks on scale of 1 to 5, given single user-specified grading criterion. Evallm [28] provides framework for evaluating and providing feedback on language model outputs based on user-given criteria. BiGGen Bench[27] is benchmark for LLMs that utilizes single instance-specific criterion per instruction to evaluate the response of model on scale of 1 to 5. FLASK-Hard [53] is fine-grained evaluation protocol for LLMs that utilize instance-specific criteria for 12 skill sets that are used to evaluate the responses of LLMs. However, these methods have limitations. First, adherence to the five-point scale makes the approach inflexible. For example, it is difficult to create grading criteria for simple questions that can only be correct or incorrect. Furthermore, the majority of the instance-specific criteria used in the above studies are subjective metrics, and are still open to judge alignment issues. We believe our approach suffers from neither of these limitations, as it uses flexible system for criteria and only uses criteria that can be evaluated objectively by the judge model. Additionally, this allows our approach to utilize even small open-source models for reliable evaluation, which makes the evaluation process much more accessible. 2.3. Korean VLM Benchmarks The performance of generative language models can vary across different languages [8]. Despite this, there has not been much work regarding VLM benchmarks in the Korean language. The VARCO-VISION [22] paper introduces four open multiple-choice benchmarks(K-MMBench, K-SEED, K-MMStar, K-DTCBench), three of which are translated subsets of existing English language benchmarks. VLRBench [36] is VLM benchmark in English, Chinese, and Korean for evaluating retrieval-augmented generation ability. [30] introduces Korean-English bilingual multiplechoice benchmark for specifically evaluating VLMs ability to distinguish between animate and inanimate objects. Moreover, VQA datasets in the Korean language that are not necessarily intended for the evaluation of VLM performance can be used as benchmarks, as in [47]. These include KVQA [24], which is Korean language VQA dataset featuring situations commonly encountered by blind people, and BOK-VQA [25], Korean-English bilingual VQA dataset for knowledge-based VQA. We observe that most of these datasets are not necessarily general-purpose benchmarks designed to evaluate various aspects of VLMs performance. To the best of our knowledge, the only general-purpose Korean-language VLM benchmarks publicly available are K-MMBench, KSEED, and K-MMStar, all of which are translated subsets of English-language benchmarks and only utilize multiplechoice questions. Figure 1. Distribution of question categories and subcategories in the KOFFVQA benchmark. 3. The KOFFVQA Benchmark Existing VLM benchmarks that use the LLMor VLMas-a-judge approach may suffer from the problem of unreliable judgment, and even when utilizing fine-grained criteria, suffer from limitations associated with such approaches. Therefore, we present KOFFVQA, generalpurpose Korean-language VLM benchmark with finegrained objective evaluation criteria designed to evaluate wide range of aspects of VLM performance relevant to realworld applications. 3.1. Tasks In this benchmark, we define 3 main categories and 10 subcategories of questions based on the categorization schemes used by existing general-purpose datasets [10, 20, 39]. The categorization that we use is as follows: 1. Perception Object Attributes. These questions ask about the attributes of objects within an image, such as color, shape, count, or existence. This evaluates the most basic skill necessary to perceive and reason with visual input. Relationship. These questions ask about aspects of complex scene involving multiple elements, such as attribute comparisons or spatial relationships. This evaluates the most basic skill necessary to reason about images that involve multiple visual elements. Recognition. These questions ask the model to recognize well-known visual subjects, such as the names of places, artworks, people, or symbols. This evaluates the level of visual world knowledge that model possesses. Recognition-KO. We create separate subcategory of Figure 2. Three examples from each main category of our benchmark. The left column is the original text in Korean, and the right column provides the English translation. Grading criteria paired with partial points are given to the judge model to evaluate the VLMs response. recognition specific to Korean culture, where the questions ask about visual subjects that are specifically wellknown among speakers of the Korean language. This evaluates the specialized skill necessary to understand scenes that may commonly be presented when being used with the Korean language. KO-OCR. These are questions that simply ask the VLM to extract Korean text from an image. This evaluates the models ability to read Korean text written in Hangul, which is an important skill for understanding scenes involving Korean text. 2. Reasoning Commonsense Reasoning. These questions ask for responses that require the model to reason about the scene presented in the image using commonsense knowledge and logic. This evaluates the model for its ability to understand complex visual scene on high level. Document Understanding. These are questions paired with images of documents with large amount of Korean-language text, such as wiki articles, posters, and infographics. This evaluates the model for its ability to understand and reason about large amount of Koreanlanguage text presented visually. Table Understanding. These are questions paired with images of tables containing structured information in the Korean language. This evaluates the model for its ability to understand and reason about structured visual data. Graph and Chart Understanding. These are questions paired with images of various graphs and charts in the Korean language. This evaluates the model for its ability to understand and reason about graphs and charts that display information visually. 3. Safety and Bias Hallucination and Robustness. These are questions designed to potentially induce language-based hallucinations in VLM by using unusual images. This evaluates the model for its ability to produce correct output based on visual information instead of hallucinating. The number of questions in each subcategory is provided in Figure 1. The above categories are constructed based on how we can categorize aspects of VLMs performance that are unique to VLMs in that they relate to the models ability to perceive and utilize visual input. Such an ability requires that it 1) can correctly perceive scenes and objects, and 2) is able to extract meaning from them. These each correspond to the Perception and Reasoning categories respectively. In addition to the above, the tendency to hallucinate is an important aspect of performance specific to VLMs [38] that is useful to measure on its own. As this can involve aspects of both perception and reasoning but is distinct from either, we set aside third category, Safety and Bias, containing questions to evaluate the tendency for VLMs to hallucinate. We separate the Recognition-KO subcategory from Recognition due to the fact that the ability to recognize visual subjects unique to Korean culture is an important aspect of performance when considering real-world usage of the Korean language. Thus, we opt to evaluate it as separate subcategory. Additionally, when evaluating the ability to extract written text from visual input in the KO-OCR subcategory, we only consider Korean-language text written in Hangul. We do not include questions that prompt the VLM to extract non-Korean text in our benchmark, as such questions would not meaningfully evaluate its performance related to the Korean language. Similarly, we also only include images containing Korean-language text in the Document Understanding, Table Understanding, and Graph and Chart Understanding subcategories. 3.2. Evaluation The evaluation process of KOFFVQA employs the LLMas-a-judge approach, where the model is prompted to evaluate responses based on pre-defined grading criteria. The VLM responses are generated by prompting the model with an image followed by the corresponding question. Since the benchmark allows free-form responses, the VLMs output is unrestricted, enabling the model to generate its own responses without predefined constraints. The response is then fed to the LLM judge along with the predefined grading criteria, ensuring that the LLM judge is guided by these criteria to generate score between 0 and 10 in an easily parsable format. Aside from the LLM judgment process, the language of the response is also determined using langid [42], and the final score is set to zero if the response is not in the Korean language. An exception is when the response entirely consists of numbers and special characters, as this should be considered valid response even if the language cannot be determined. The process of filtering by language in this manner is necessary because model that responds in different language despite not being instructed to when asked question in Korean cannot be said to be performing the intended task. After the final scores for each question are extracted, they are averaged both per subcategory and over the entire benchmark. The average scores are multiplied by 10 in order to shift the range of scores to be between 0 and 100 for more intuitive interpretation and easier comparison with other benchmarks. 3.3. Data Construction Human Annotation. For each sample, human annotator selects images from various online sources based on each subcategory. Images depicting general scenes are sourced from the Open Images v7 dataset [7]. Images for the KOOCR subcategory are obtained from the KAIST Scene Text dataset [23], and some images for the Document Understanding, Table Understanding, and Graph and Chart Understanding subcategories are sourced from the Open Data Portal of the Government of the Republic of Korea. Also, some of the images for the Hallucination and Robustness subcategories are taken from HallusionBench [18]. Based on the selected image and the given subcategory, the annotator creates question and its corresponding grading criteria. Each criterion is paired with partial score such that an entirely correct response would result in score of 10. The criteria written for each question and the partial scores assigned to each criterion are determined based on the following: 1. If there is only one part to the intended answer, and the response can only be either correct or incorrect, there is only one criterion and it is assigned 10 points. 2. When there are multiple parts to the question, or if the response can have varying degrees of correctness, there are multiple criteria. In this case, partial scores are assigned to each criterion based on how important it is in determining overall correctness. (a) When one part of the question is harder to answer than another, that part is assigned larger score. (b) An exception to the above is when the harder part of question is relatively minor detail, and response can be considered mostly correct even if the harder part is incorrect. In this case, the harder part is assigned smaller score. (c) When question can be answered with varying degrees of accuracy, there can be multiple mutually exclusive criteria for each possibility. In this case, the most accurate response is assigned perfect score of 10, and less accurate responses are assigned lower score. Each of the grading criteria is written as an assessment of whether or not particular answer or part of an explanation is included in response. This way, responses generated by VLMs can be objectively judged based on their contents, minimizing the need for the judge model to subjectively interpret the criteria. This, in turn, allows for even small open-source models to be used as judge for reliable evaluation, making the evaluation process more accessible compared to most LLM-as-a-judge approaches. Manual Curation. In the process of creating questions and criteria for our benchmark, we manually edit and filter the data to create meticulously curated benchmark well-suited for machine evaluation. After creating set of questions, we use it to evaluate several VLMs and observe the generated judgments from our judge model. From this, we identify questions that the judge model has trouble correctly grading. This allows us to either edit the criteria so that it accounts for the possibility of the observed misjudgment, or remove the question entirely. This process is iteratively repeated to finally produce the 275 questions and grading criteria in our benchmark. While KOFFVQA consists of human-written and wellcurated questions and grading criteria in order to be as effective as possible, we believe our approach can also be easily scaled up by either using synthetic data for both questions and grading criteria or by generating grading criteria from existing questions using pre-trained models. 4. Experiments and Results 4.1. Evaluation of Existing Models In this section, we present the evaluation results of existing openand closed-source VLMs using our benchmark and analyze the results. We evaluate total of 47 VLMs. The Rank Name 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 gpt-4o-2024-11-20 claude-3-5-sonnet-20241022 Qwen2.5-VL-72B-Instruct gemini-2.0-flash-001 gemini-2.0-flash-exp gpt-4o-2024-08-06 gemini-2.0-pro-exp-02-05 gemini-1.5-pro-002 Qwen2-VL-72B-Instruct gemini-1.5-flash-002 Ovis2-8B gpt-4o-mini-2024-07-18 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-7B-Instruct InternVL2_5-78B VARCO-VISION-14B gemini-2.0-flash-lite-preview-02-05 gpt-4-turbo-2024-04-09 Ovis2-34B Qwen2-VL-7B-Instruct gemini-1.5-flash-8b-001 (B) 73.4 73. Params Overall Obj. Attr. 78.3 81.8 81.8 73.5 73.8 77.5 80.0 71.3 86.7 72.8 77.8 71.3 75.0 78.3 71.3 76.7 59.8 76.7 81.0 73.2 68.7 Score 82.0 80.5 80.1 79.1 78.9 77.6 77.6 77.2 74.8 73.5 69.5 68.3 67.9 67.7 67.2 66.0 65.2 65.2 64.3 63.2 61.9 88.6 8.3 78.4 15.2 34.9 8.3 8.9 Recog. 90.0 90.0 90.0 80.0 80.0 80.0 80.0 90.0 80.0 90.0 65.0 80.0 75.0 50.0 75.0 45.0 75.0 90.0 80.0 50.0 50.0 Recog. -KO 85.0 80.0 45.0 70.0 70.0 90.0 90.0 60.0 45.0 50.0 30.0 35.0 40.0 25.0 25.0 10.0 60.0 60.0 35.0 40.0 15.0 Relat. KO-OCR 80.0 66.0 63.7 65.3 56.7 64.7 58.0 69.3 62.7 68.0 70.7 66.3 62.7 55.3 66.7 58.0 43.7 76.3 77.0 56.0 46.7 91.5 76.5 95.0 93.5 90.0 80.0 85.0 62.5 75.0 72.5 70.0 100.0 80.0 85.0 70.0 85.0 68.0 30.0 45.0 70.0 35.0 Comm. Reas. 85.6 88.9 85.3 72.7 82.7 87.6 87.1 83.3 83.1 78.2 76.2 77.8 76.0 71.6 78.9 70.7 72.7 80.0 80.9 74.9 82. Doc. Table Underst. Underst. 86.7 78.0 83.3 90.0 93.3 77.0 86.0 94.7 64.0 89.3 53.7 63.0 56.0 74.7 60.7 48.3 80.3 47.3 37.7 64.3 91.3 82.3 73.7 84.0 96.7 90.0 82.0 78.7 80.0 84.3 83.3 74.0 47.7 55.0 61.7 51.0 74.0 71.7 39.3 40.0 50.0 51.3 Graph & Chart Halluc. & Robust. 70.0 80.0 80.0 50.0 50.0 70.0 50.0 60.0 70.0 40.0 80.0 70.0 76.0 75.0 85.0 80.0 60.0 80.0 70.0 60.0 55. Underst. 74.7 88.7 76.7 88.0 84.7 68.0 68.0 84.7 61.3 61.3 64.0 61.3 68.0 58.7 68.0 63.3 63.3 64.7 60.0 53.3 53.3 Table 1. Selected evaluation results for the top scoring 21 VLMs out of the 47 total models tested on our benchmark. larger model size does not necessarily correspond to better performance, and models that excel in some subcategories may not do well in others. Due to the page limit, we show the entire models result in https://huggingface.co/spaces/maum-ai/KOFFVQA-Leaderboard models we evaluate are the following: VLMs that officially support Korean or explicitly mention using Korean data in training: Claude models(3.5 Sonnet 20241022, 3 Haiku 20240307, 3 Sonnet 20240229, 3 Opus 20240229) [4, 5], Gemini models(2.0 Pro experimental 02-05, 2.0 Flash 001, 2.0 Flash experimental, 2.0 Flash Lite preview 02-05, 1.5 Pro 002, 1.5 Flash 002, 1.5 Flash 8B 001) [16], GPT-4o models(2024-08-06, 2024-1120, Mini 2024-07-18) [21], GPT-4V [2], Bllossom-AICA5B [47], InternVL 2.5(8B, 26B, 38B, 78B) [11], MiniCPMV 2.6 [52], Qwen2-VL(3B, 7B, 72B) [51], and VARCOVISION 14B [22]. VLMs that do not officially support Korean and do not mention using Korean data in training: Aria [33], Idefics 3 8B [29], InternLM-XComposer 2.5 7B [57], InternVL 2(8B and 76B) [12], Llama 3.2(11B and 90B) [17], LLaVAOneVision(7B and 72B) [32], MAmmoTH-VL 8B [19], Molmo(7B, 72B) [13], Ovis 2(8B and 34B) [41], Ovis 1.6(9B and 27B) [41], Phi 3.5 Vision [1], Pixtral 12B [3], Qwen2.5-VL(2B, 7B, 72B) [6], and SmolVLM [43]. For the judge LLM, we use Gemma 2 9B [45] with the temperature set to zero for this experiment. More advanced proprietary models, such as GPT-4o and Gemini 2.0 Flash, can also be used. We mainly use Gemma 2 9B as it is an open-source model that 1) can be reasonably deployed on local machine, and 2) can meaningfully understand the grading criteria given in Korean in manner that aligns with human intention. Being an open-source model also makes the results easily reproducible. Gemma 2 9B correctly judges responses in approximately 89% of all cases. subset of the evaluation results, ordered by overall score, is provided in Table 1. At first glance, notable observation we can make is that larger model does not necessarily correspond to better performance, contrary to the tendency in English-language benchmarks. We suggest that this is because different models are trained on varying amounts of multilingual data, which can have an impact on performance, especially for less-represented languages such as Korean. Inspecting the responses of each model, we see that many models that excel in English often fail to answer simple questions in KOFFVQA by either generating incoherent Korean or answering the question in English even when prompted in Korean. We can also observe that high overall score does not necessarily imply high score in every subcategory. In fact, we can observe that many models excel in some categories but not in others. This suggests that the categorization system of our benchmark is meaningful and can provide useful information about various aspects of VLMs performance. For instance, the top five models in the Document Understanding subcategory are Gemini models, with scores ranging from 94.67 to 89.33. Even the smallest model, gemini-1.5-flash-8b-001, which top the overall is model(gpt-4o-2024-11-20) in this subcategory. ranked 21st overall, outperforms 4.2. Comparison with Other Evaluation Methods In this section, we perform an experiment to verify that our method of judge model-based evaluation using predetermined grading criteria is more robust and resistant to subjectivity and unreliability compared to existing methods of judge-based evaluation used in many VLM benchmarks. We create three alternate versions of our benchmark: 1) KOFFVQA-V, where the image belonging to each question is given to VLM judge in addition to our grading criteria, 2) KOFFVQA-GT, where no image is given and ground truth response is given instead of grading criteria, and 3) KOFFVQA-GT-V, where the ground truth reGemma 2 9B GPT-4o Gemini 2.0 Flash Gemma 2 9B GPT-4o Gemini 2.0 Flash KOFFVQA KOFFVQA-V KOFFVQA-GT KOFFVQA-GT-V 0.398 - 0.584 - 0.171 0.208 0.456 0.452 0.127 0.254 0.476 0.426 Table 2. Mean standard deviation of scores for 5 repeated evaluations for each question across 275 sampled responses. These are based on individual scores that range from 0 to 10. sponse is given along with the image paired with the question. The ground truth responses we use are minimal natural language responses written by human to satisfy all of the grading criteria for each question. We compare the evaluation results of these with the results from our original KOFFVQA benchmark. We also compare the results of evaluation between three different judge models. We use Gemma 2 9B as judge for KOFFVQA and KOFFVQA-GT, and also GPT-4o(202411-20), and Gemini 2.0 Flash as judge for all four benchmarks. We select GPT-4o and Gemini 2.0 Flash to represent two different robust proprietary VLMs that can be employed as judge for our benchmark. For this experiment, we set the generation temperature to 0.6 for Gemma 2 9B and 1.0 for GPT-4o and Gemini 2.0 Flash, and run the evaluation process 5 times over fixed subset of 275 responses randomly sampled from the evaluation results of our tested VLMs. When creating this subset, we randomly choose the response of one model for each of the 275 questions. As we are only considering the judgments of the judge models and not the language filtering process, we exclude responses that are in non-Korean language when we sample the responses. Our intention is to compare the reliability of the judge model for each grading method by measuring the standard deviations of scores across multiple trials. comparison of the standard deviations of scores for 5 trials averaged across all 275 samples is given in Table 2. In this experiment, we compare the differences in standard deviation between KOFFVQA and KOFFVQA-GT, and between KOFFVQA-V and KOFFVQA-GT-V. We can first observe that for all 3 judge models, providing grading criteria(KOFFVQA, KOFFVQA-V) results in much more consistent evaluation results compared to providing groundtruth answer(KOFFVQA-GT, KOFFVQA-GT-V). This is expected, as grading criteria provide an objective metric with which to grade the responses, as opposed to ground truth answers which do not specify how to grade partially correct answer. 4.3. Influence of Visual Input in Judgment We also intend to prove that prompting the judge with image inputs can be less reliable compared to using only language inputs in judgment using our method due to hallucinations based on visual input. KOFFVQA KOFFVQA-V 89.3% - 95.8% 92.9% 93.0% 91.8% Table 3. Percentages of how often each judge model correctly graded the sampled responses according to the grading criteria. These values are derived by comparing the grading results to human-evaluated scores. An important observation that we can make is that when comparing KOFFVQA and KOFFVQA-V in Table 2, providing images in addition to grading criteria seems to degrade the consistency of evaluation for both GPT-4o and Gemini 2.0 Flash. This is despite the fact that the judges are given identical grading criteria for both benchmarks, and are prompted to grade the responses using only the grading criteria regardless of the contents of the image. We also compare the accuracy of how often each judge model correctly graded the responses using the given criteria when compared to human-evaluated scores. As the criteria provide objective guidelines for grading, true scores for each of the 275 sampled responses can be acquired by manually grading each of the responses using the existing grading criteria. The correctness of evaluation for each judge model in KOFFVQA and KOFFVQA-V are given in Table 3. The grading correctness decreases when an additional image input is given, despite the judge models in both cases having been given identical grading criteria. We further explore the cause of this phenomenon by qualitatively comparing the judgments of each model when grading the same response in both KOFFVQA and KOFFVQA-V. An example of case where GPT-4o grades response correctly in the KOFFVQA case but incorrectly in the KOFFVQA-V case is shown in Figure 3. In this example, when grading the response with only the grading criteria, the judge correctly determines that the answer satisfies both criteria and gets 10 points. On the other hand, when the judge is also given the image input, it attempts to base its judgment on the image and the grading criteria. Since it hallucinates that the middle door is green instead of blue, it incorrectly determines that the response does not satisfy the first grading criterion. Many other samples where the same judge graded the KOFFVQA case correctly but misgraded the KOFFVQA-V case exhibited similar pattern, where the judge hallucinated details of the image and evaluated the response based on this information instead of the given criteria. This shows that the tendency of VLMs to hallucinate based on visual input negatively impacts the grading performance of judge models when used in benchmark that utilizes our approach of using grading criteria. This confirms that there is an issue of VLM judges producing incorrect judgments due to hallucination prompted by visual input. The KOFFVQA benchmark does not suffer from this issue as it uses the LLM-as-a-judge method. Figure 3. An example of response that GPT-4o grades correctly when the image is not given as input but grades incorrectly when the image is given. The left columns are the original text in Korean, and the right columns provide the English translations. When the image is given, the judge model attempts to judge the response based on the image and hallucinates that the door in the middle of the photograph is green. When the image is not given, the judge has no reason to grade the response based on anything other than the given criteria. 4.4. Analysis of failure cases 5. Conclusion During the grading process of the 275 sampled questions in the above experiment, we inspect cases where the judge models graded response differently from the humanevaluated score in the KOFFVQA benchmark. We manually review 11 failure cases of GPT-4o and 17 failure cases of Gemini 2.0 Flash from 275 grading results of the original subset. We do not analyze the judgments from Gemma 2 9B, as the model does not output the reasoning behind its evaluation the majority of the time. Based on the judge models reasoning, we classify the failure cases into the following 3 categories: 1. Arbitrarily overruling criteria. This is when the judge model decides to overrule what is explicitly written in the grading criteria and judge the response based on its own interpretation of the criteria. 2. Ambiguous criteria. This is when the grading criteria are not perfectly clear in meaning in the context of the provided response, and the judge model misinterprets the grading criteria due to this ambiguity. 3. Unexplainable misjudgment. This is when the judge incorrectly grades the response for no apparent reason. GPT-4o has 5 cases in category 1, 2 cases in category 2, and 4 cases in category 3. Gemini 2.0 Flash has 7 cases in category 1, 2 cases in category 2, and 8 cases in category 3. We see that the majority of failure cases is in categories 1 and 3, suggesting that there is even more room for improvement in our LLM-as-a-judge process if judge model can be trained to minimize these errors. In this work, we identify an issue with existing benchmarks for the evaluation of large vision-language models, namely that benchmarks leveraging the open-ended generation ability of VLMs suffer from the issue of unreliable judgment in the evaluation process. In addition, we observe that there is lack of general-purpose benchmarks for VLMs in the Korean language that also include Korea-specific content. To address these issues, we develop KOFFVQA, carefully crafted general-purpose free-form Korean language VQA benchmark using novel approach of providing objective pre-defined grading criteria to an LLM judge model during the evaluation process. We use our benchmark to evaluate the performance of 47 VLMs, including both open and closed models, regardless of their support for Korean, and find it effective in evaluating various aspects of each models performance. We also experimentally compare our partial scoring approach with the existing method of prompting the judge to compare each response with reference response and show that our approach results in more reliable and consistent judgment. We also demonstrate that using VLM judge with image input can degrade grading effectiveness and cause hallucinations. Furthermore, we analyze the failure cases of LLM judge models using our benchmark and suggest that the accuracy of our judge model can be improved by training. We hope that KOFFVQA can provide insights on the performance of various VLMs in the Korean language and serve as reference for the further development of other benchmarks that utilize fine-grained criteria in the future."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 6 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 6 [3] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, arXiv preprint Theophile Gervet, et al. arXiv:2410.07073, 2024. 6 Pixtral 12b. Claude 3.5 Sonnet Model Card Adden- [4] Anthropic. https : / / www - cdn . anthropic . com / dum . fed9cc193a14b84131812372d8d5857f8f304c52/ Model_Card_Claude_3_Addendum.pdf, 2024. 6 [5] Anthropic. The Claude 3 Model Family: Opus, Sonnet, Haiku. https://wwwcdn.anthropic.com/ de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/ Model_Card_Claude_3.pdf, 2024. [6] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6 [7] Rodrigo Benenson and Vittorio Ferrari. From colouring-in to pointillism: revisiting semantic segmentation supervision. arXiv preprint arXiv:2210.14142, 2022. 5 [8] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3):145, 2024. 1, 2, 3 [9] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal In Fortyllm-as-a-judge with vision-language benchmark. first International Conference on Machine Learning, 2024. 1, 2 [10] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. 1, 2, 3 [11] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 6 [12] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. [13] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 6 [14] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 1, 2 [15] Chaoyou Fu, Yi-Fan Zhang, Shukang Yin, Bo Li, Xinyu Fang, Sirui Zhao, Haodong Duan, Xing Sun, Ziwei Liu, Liang Wang, et al. Mme-survey: comprehensive surarXiv preprint vey on evaluation of multimodal arXiv:2411.15296, 2024. 2 llms. [16] Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 6 [17] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv preprint et al. arXiv:2407.21783, 2024. The llama 3 herd of models. [18] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual ilIn Proceedings of lusion in large vision-language models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. 1, 2, 5 [19] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. arXiv preprint arXiv:2412.05237, 2024. 6 [20] Jiaxing Huang and Jingyi Zhang. survey on evaluation of multimodal large language models. arXiv preprint arXiv:2408.15769, 2024. 1, 3 [21] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 6 [22] Jeongho Ju, Daeyoung Kim, SunYoung Park, and Youngjune Kim. Varco-vision: Expanding frontiers in korean visionlanguage models. arXiv preprint arXiv:2411.19103, 2024. 3, [23] Jehyun Jung, SeongHun Lee, Min Su Cho, and Jin Hyung Kim. Touch tt: Scene text extractor using touchscreen interface. ETRI Journal, 33(1):7888, 2011. 5 [24] Jin-Hwa Kim, Soohyun Lim, Jaesun Park, and Hansu Cho. Korean localization of visual question answering for blind In SK T-Brain-AI for Social Good Workshop at people. NeurIPS, 2019. 3 [25] MinJun Kim, SeungWoo Song, YouHan Lee, Haneol Jang, and KyungTae Lim. Bok-vqa: Bilingual outside knowledgebased visual question answering via graph representation pretraining. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1838118389, 2024. 3 [26] Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fineIn The grained evaluation capability in language models. Twelfth International Conference on Learning Representations, 2023. 2 [27] Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, et al. The biggen bench: principled benchmark for fine-grained evaluation of language models with language models. arXiv preprint arXiv:2406.05761, 2024. [28] Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, and Juho Kim. Evallm: Interactive evaluation of large language In Proceedings of model prompts on user-defined criteria. the 2024 CHI Conference on Human Factors in Computing Systems, pages 121, 2024. 3 [29] Hugo Laurencon, Andres Marafioti, Victor Sanh, and Leo Tronchon. Building and better understanding visionlanguage models: insights and future directions. In Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models, 2024. 6 [30] Chanwoo Lee, Hyunjeong Lee, Minsang Kim, Hyun Kim, Haneol Jang, and Cheoneum Park. Harnessing llms for vqa: prompted benchmark with animate/inanimate keywords. In 2024 15th International Conference on Information and Communication Technology Convergence (ICTC), pages 15031507. IEEE, 2024. 3 [31] Seongyun Lee, Seungone Kim, Sue Park, Geewook Kim, and Minjoon Seo. Prometheus-vision: Vision-language model In Findings of the as judge for fine-grained evaluation. Association for Computational Linguistics ACL 2024, pages 1128611315, 2024. 1, 2 [32] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-onevision: Easy visual task transfer. Transactions on Machine Learning Research, 2025. 6 [33] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Fan Zhou, Chengen Huang, Yanpeng Li, et al. Aria: An open multimodal native mixtureof-experts model. arXiv preprint arXiv:2410.05993, 2024. [34] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. 1 [35] Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. survey of multimodal large In Proceedings of the 3rd International language models. Conference on Computer, Artificial Intelligence and Control Engineering, pages 405409, 2024. 1 [36] Hyeonseok Lim, Dongjae Shin, Seohyun Song, Inho Won, Minjun Kim, Junghun Yuk, Haneol Jang, and KyungTae VLR-bench: Multilingual benchmark dataset for Lim. vision-language retrieval augmented generation. In Proceedings of the 31st International Conference on Computational Linguistics, pages 61506168, Abu Dhabi, UAE, 2025. Association for Computational Linguistics. 3 [37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 1, 2 [38] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253, 2024. [39] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2025. 1, 2, 3 [40] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. 2 [41] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797, 2024. 6 [42] Marco Lui and Timothy Baldwin. langid.py: An off-theshelf language identification tool. In Proceedings of the ACL 2012 System Demonstrations, pages 2530, Jeju Island, Korea, 2012. Association for Computational Linguistics. 5 [43] Andres Marafioti, Orr Zohar, Miquel Farre, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, Vaibhav Srivastav, Joshua Lochner, Hugo Larcher, Mathieu Morlon, Lewis Tunstall, Leandro von Werra, and Thomas Wolf. Smolvlm: Redefining small and efficient multimodal models. 2025. 6 [44] Arjun Panickssery, Samuel Bowman, and Shi Feng. Llm evaluators recognize and favor their own generations. Advances in Neural Information Processing Systems, 37: 6877268802, 2025. 1 [45] Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. 6 [46] Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and Lidong Bing. Large language models are not yet humanlevel evaluators for abstractive summarization. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 42154233, 2023. [47] DongJae Shin, HyeonSeok Lim, Inho Won, ChangSu Choi, Minjun Kim, SeungWoo Song, HanGyeol Yoo, SangMin Kim, and KyungTae Lim. X-LLaVA: Optimizing bilingual In Findings of the Assolarge vision-language alignment. ciation for Computational Linguistics: NAACL 2024, pages 24632473, Mexico City, Mexico, 2024. Association for Computational Linguistics. 3, 6 [48] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 2 [49] Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, and Dieuwke Hupkes. Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges. arXiv preprint arXiv:2406.12624, 2024. 1 [50] Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, et al. Evaluation and analysis of hallucination in large vision-language models. arXiv preprint arXiv:2308.15126, 2023. 1, 2 [51] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 6 [52] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 6 [53] Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. FLASK: Fine-grained language model evaluation based on alignment skill sets. In ICLR 2024 Workshop on Large Language Model (LLM) Agents, 2024. 3 [54] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12), 2024. 1, [55] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In International conference on machine learning. PMLR, 2024. 1, 2 [56] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 1, 2 [57] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. 6 [58] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. 1,"
        }
    ],
    "affiliations": [
        "MAUM AI Inc. / Republic of Korea"
    ]
}