{
    "paper_title": "daVinci-Dev: Agent-native Mid-training for Software Engineering",
    "authors": [
        "Ji Zeng",
        "Dayuan Fu",
        "Tiantian Mi",
        "Yumin Zhuang",
        "Yaxing Huang",
        "Xuefeng Li",
        "Lyumanshan Ye",
        "Muhang Xie",
        "Qishuo Hua",
        "Zhen Huang",
        "Mohan Jiang",
        "Hanning Wang",
        "Jifan Lin",
        "Yang Xiao",
        "Jie Sun",
        "Yunze Wu",
        "Pengfei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ..."
        },
        {
            "title": "Start",
            "content": "daVinci-Dev: Agent-native Mid-training for Software Engineering Ji Zeng2,3 Dayuan Fu1,3 Tiantian Mi1,3 Yumin Zhuang2,3 Yaxing Huang3 Xuefeng Li1,2,3 Lyumanshan Ye3 Muhang Xie1,3 Qishuo Hua2,3 Zhen Huang1,3 Mohan Jiang1,2,3 Hanning Wang1,3 Jifan Lin2,3 Yang Xiao3 Jie Sun1,3 Yunze Wu3 Pengfei Liu1,2, 1SII 2SJTU 3GAIR SII Open Source: Code Models ı Datasets"
        },
        {
            "title": "Abstract",
            "content": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineeringa paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, agentic mid-trainingmid-training (MT) on large-scale data that mirrors authentic agentic workflowsremains critically underexplored due to substantial resource requirements, despite offering more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is agent-native datasupervision comprising two complementary types of trajectories: contextually-native trajectories that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and environmentally-native trajectories collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the models agentic capabilities on SWE-Bench Verified. We demonstrate our superiority over the previous open software engineering mid-training recipe KIMI-DEV under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve 56.1% and 58.5% resolution rates, respectively, which are state-of-the-art among open training recipes using agentic scaffolds under their model sizes, despite starting from non-coder Qwen2.5-Base base models. Beyond these agentic capabilities, we also observe performance gains on general code generation and scientific benchmarks. We plan to open-source significant portion of our datasets, recipes, and model checkpointsresources representing substantial computational investment typically unavailable to the broader communityto facilitate further research in this underexplored paradigm. 6 2 0 2 6 2 ] . [ 1 8 1 4 8 1 . 1 0 6 2 : r Figure 1: Overview of our recipe. Left: We curate two complementary datasets using different elements of PRs. Right: Comparison of our best performance and the best performance of the previous open software engineering mid-training recipe KIMI-DEV on SWE-AGENT scaffold. Detailed comparison is reported in Table 1. Corresponding author. 1 1. Introduction"
        },
        {
            "title": "Introduction",
            "content": "The capabilities of code-generating large language models have rapidly expanded from synthesizing isolated functions (Jain et al., 2024; Wang et al., 2024c) to tackling repository-level software engineering tasks (Jimenez et al., 2023). This shift toward agentic software engineering (Jimenez et al., 2023; Badertdinov et al., 2025a; Wu et al., 2025) reflects the demands of real-world development, where resolving issues requires code agents to autonomously and iteratively navigate complex codebases, understand cross-file dependencies, apply edits, and validate changes through test execution. The dominant approach to building such code agents has centered on post-training: supervised fine-tuning (SFT) on curated trajectories (Yang et al., 2025b,c) followed by reinforcement learning (RL) from execution feedback (Team et al., 2025b). While effective, the quantity and diversity are limited for the repositories that can be used in this paradigm. Due to the unclear instructions in the repositories README files and the resource limitation (e.g., GPUs), the number of repositories that can be transformed into executable environments is limited. Moreover, in most cases, only correct trajectories can be used in training, but high-quality agentic trajectories generated by expert human annotators are expensive to collect, while the sophisticated agent systems can only solve small subset of issues, so most environments will also be filtered before the training process (Yu et al., 2025). Such flaw constrains the learning dynamics and degrades performance during SFT or RL training. More fundamentally, post-training is constrained by the base models intrinsic capacities, and certain agentic reasoning abilities may not be learnable through post-training alone (Ye et al., 2025). This raises natural question: Can we instill foundational agentic behaviors earlier in the training pipeline, during mid-training? Mid-training (MT) on domain-specific data has proven transformative for specializing LLMs to domains like mathematics (Lewkowycz et al., 2022; Wang et al., 2025d) and code (Yang et al., 2025c). For agentic software engineering, mid-training offers compelling value proposition: by exposing base models to massive-scale data that mirrors agentic iterationsfile navigation, contextual edits, tool invocations, test-driven iterationswe can build stronger foundations that subsequent post-training can refine more efficiently. Yet despite this potential, agentic mid-training remains critically underexplored. Existing mid-training or pre-training efforts for code models (Yang et al., 2025c) predominantly adopt factorized approach: synthesizing isolated samples for atomic capabilities such as localization and editing, without the procedural context that an agent would encounter before exercising these capabilities. In effect, we argue that existing MT data is not agent-native: it does not preserve the actionobservation loop structure that governs real development. We therefore identify the core challenge as distribution mismatch between conventional training data and the dynamic reality of agentic deployment. Consider typical GitHub Pull Request: while the commit history reveals what files were changed, it obscures how developer (or agent) discovered those files, what context they examined before editing, and how test feedback shaped subsequent revisions. Training on such static snapshotseven at massive scaleleaves models unprepared for the sequential, interactive nature of real development workflows. An agent should not just learn to navigate to the right location, retrieve relevant context, generate correct edits, apply changes, and run unit tests separately, but should coordinate these skills into coherent, iterative problem-solving loop, where feedback from each step informs the next action. To bridge this gap, we present the first systematic study of agentic mid-training for software engineering at scale. Our central thesis is that effective agentic mid-training requires large-scale and diverse agent-native datasupervision that preserves the complete information flow and environmental dynamics an agent experiences during deployment. We formalize this through two complementary trajectory types: Contextually-native trajectories: This type of trajectory emphasizes coverage and diversity. Any supervision instance that preserves the structure of realistic engineering process can be included, regardless of whether it was produced through live execution, ensuring broad source repository coverage and diversity. Supervision is organized around full task-level action sequences, bundling localization steps (e.g., identifying relevant files) together with modification steps (e.g., applying edits). Edit actions can be conducted multiple times with interleaved textual reasoning to reflect coherent software development process. This allows the dataset to capture wide variety of valid contextual patterns and operational permutations. Environmentally-native trajectories: This type of trajectory prioritizes interaction authenticity while also considering quantity. Only trajectories generated through actual interactions with real development environment are eligible for inclusion. These trajectories record genuine observationstool invocations, test executions, runtime errors, and scaffold system feedbackrather than simulated or retrospectively constructed observations. We do not apply any filter strategy, so that the quantity of such trajectories can be much larger than the ones in the SFT stage. This exposes models to the dynamic feedback loops inherent in real development. We materialize these principles through large-scale data synthesis effort that leverages different elements from GitHub Pull Requests to construct two complementary data types, as illustrated in Figure 1. First, we curate 68.6B-token contextually-native trajectories (Dctx) using base files and commits, carefully reconstructing the 2 2. Background and Problem Setup procedural process behind each code change: which context the developer likely examined (related issue and base file content), and how they iteratively refined their solution (temporal commits). This transforms static diffs into contextually-rich trajectories that preserve the natural coupling between navigation and editing, providing broad coverage and diversity across repositories and languages. Second, we construct 3.1B-token environmentallynative trajectories (Denv) from PR-derived software engineering tasks using their Docker environments and unit tests, generating agentic rollouts where our agent interacts with real build systems, test suites, and linters, collecting observations from actual tool outputs. This provides depth and authenticity through genuine execution feedback that cannot be retrospectively reconstructed. Evaluating our models on SWE-Bench Verified, we surpass the previous state-of-the-art open MT recipe, KIMI-DEV, under two post-training settings with an aligned base model and agentic scaffold while reducing the mid-training corpus size by over 50% (73.1B vs 150B tokens). Our best performing 32B and 72B models reach resolution rates of 56.1% and 58.5%, respectively. These scores represent the highest performance among open training recipes using agentic scaffolds for their respective model sizes, significant feat given our initialization from Qwen2.5-Base models instead of newer or code-focused base models. Beyond agentic workflows, this regimen also confers broad benefits, improving performance on scientific and general code generation tasks as detailed in Table 3. To conclude, we make the following contributions: We formulate agentic mid-training and introduce agent-native data as supervision that preserves the information flow of real software engineering. We build large-scale agent-native corpora from public software development traces, including 68.6B-token contextually-native corpus and 3.1B-token set of environmentally-native rollouts, and provide practical training recipe that leverages them. We demonstrate consistent gains on agentic software engineering brought by our agentic mid-training recipe across post-training schemes and model sizes, and provide analysis of robustness, scalability, and generalization. We plan to release the data construction code, training configurations, and substantial portion of the resulting artifacts (e.g., curated datasets and checkpoints) where permitted."
        },
        {
            "title": "2.1 Agentic Software Engineering Tasks",
            "content": "We formalize an agentic software engineering task as tuple (R, q, E), where is repository state, is natural language problem description (e.g., bug report, issue), and is an evaluation oracle (typically test suite). Unlike single-turn generation where all necessary context is provided upfront, agentic tasks require multi-step interaction. At each step t, the agent selects an action based on the conversation history and receives an observation from an observation generator: at πθ(a ht1, q) ot Obs(at, R) (observation) (action selection) where ht1 = {(a1, o1), . . . , (at1, ot1)} accumulates prior interactions. Actions correspond to tool calls such as searching for files, reading code, applying edits, or running tests, while observations return concrete outputs like file contents, compiler errors, or test results. This interaction is necessary because the agent initially does not know which parts of the codebase (potentially thousands of files) are relevant to the issue, and must iteratively refine its solution based on feedback from the evaluation oracle. While the exact sequence varies by task complexity, typical development workflow follows the pattern: localize (identifying relevant files) read (understanding code context) edit (applying modifications) test (validating changes) revise (refining based on feedback). This structure reflects common agent implementations (Yang et al., 2025b) and mirrors natural software development practices, though agents may repeat or interleave these steps as needed. This complete sequence is an agent trajectory τ = (q, R, {(ai, oi)}T i=1, y), where {0, 1} indicates whether the trajectory is successful under its supervision source."
        },
        {
            "title": "2.2 The Distribution Mismatch Problem",
            "content": "As formalized in Section 2.1, agentic software engineering requires multi-step interaction. However, traditional training data predominantly consists of static, completed artifacts that bear no resemblance to this interactive process (Figure 2a). Models are exposed to final outcomescomplete code files, merged commits, finished implementationswithout the sequential action-observation pairs (at, ot) that agents experience at deployment. This creates critical distribution mismatch: training data shows what was ultimately produced, but deployment 3 3. Agent-Native Data: Design and Synthesis Figure 2: Comparison of training data paradigms. (a) Traditional code pre-training uses isolated static files. (b) Factorized approaches train subtasks separately, creating train-test mismatch. (c) Our contextually-native PRs bundle retrieval context with sequential edit trajectory. (d) Our environmentally-native trajectories capture real execution feedback loops. requires agents to learn how to construct solutions through the dynamic workflow of localization, reading, editing, testing, and revision. Moreover, even when workflow structure is preserved, training data typically shows only successful final states, omitting the validation failures, error messages, and iterative refinements that emerge from actual environment interaction. Models trained on such incomplete supervision must learn to orchestrate complete workflows and respond to execution feedback during post-training, rather than internalizing these coordination patterns as foundational behaviors."
        },
        {
            "title": "3 Agent-Native Data: Design and Synthesis",
            "content": "To address the distribution mismatch between static training data and interactive deployment, we construct agentnative datasupervision that preserves the complete action-observation trajectories and environmental feedback agents experience during real problem-solving. Specifically, we construct two complementary types of agent-native trajectories: contextually-native trajectories, which reconstruct complete workflows from GitHub Pull Requests to preserve the full development context, and environmentally-native trajectories, which capture authentic execution feedback through agent rollouts in real executable environments. The combination ensures both breadth (diverse workflow patterns at scale) and depth (authentic execution dynamics), addressing the distribution mismatch from complementary angles."
        },
        {
            "title": "3.1.2 Construction Pipeline\nData Sources. We construct contextually-native trajectories from two complementary subsets: Dctx\ngen (“general”)\nprovides broad coverage of software engineering patterns across diverse languages and frameworks by drawing from\nhighly-starred repositories, while Dctx\n(“Python”) ensures strong alignment with software engineering benchmarks\npy\n(e.g., SWE-Bench Verified) through focused coverage of Python development. The two subsets partially\noverlap in Python repositories but serve complementary purposes: Dctx\ngen establishes cross-language understanding,\nwhile Dctx",
            "content": "py ensures alignment with target evaluation tasks."
        },
        {
            "title": "3.1 Contextually-Native Trajectories",
            "content": "Figure 3: Overview of our dataset generation pipeline. Collection. We collect pull requests through GitHub REST1 and GraphQL APIs2. For each repository, we obtain pull request metadata and selectively query additional endpoints for detailed content, including linked issue descriptions (if exist), relevant file contents at the base commit, and the full commit sequence with corresponding diffs. We determine relevant files deterministically by querying the net diff between base and head commits. To ensure correctness, we align file contents and patches with the parent of the first PR commit, rather than using the base commit recorded in PR metadata, which may not reflect the actual codebase state when the PR was created. gen selects from the top 10,000 most-starred repositories across all languages3. Dctx py Filtering. We apply multi-level filtering criteria to ensure data quality while maintaining coverage. (1) At the repository level, Dctx focuses on public Python repositories (language=Python in metadata API) with at least 5 stars and not archived. The relaxed star threshold for Dctx py balances repository diversity with quality standards. (2) At the pull request level, both subsets retain only merged PRs and exclude bot-created PRs. For Dctx py , we additionally require modifications to be done only in Python source or documentation files, with the number of changed Python files between 1 and 5. Six million out of thirteen million pull requests pass the Dctx py pull request level filters. These constraints ensure focused, manageable tasks suitable for agent learning. Reconstruction. For each filtered PR, we reconstruct an agent-native training sample through the following process: (1) Content enhancement. We use Qwen3-235B-A22B-Instruct-2507 (Yang et al., 2025a) to generate two types of enhancements. First, we create concise summary of the overall PR that captures its intent and main changes. Second, since some commit messages are terse or uninformative (e.g., fix, update), we refine them into more descriptive summaries that explain what each commit accomplishes. Detailed prompts are provided in Appendix B. (2) Relevant file identification. We identify relevant files through reverse engineering: analyzing symmetric diffs across all commits to extract the set of modified files, then retrieving their complete contents at the base commit. (3) Template organization. We organize all extracted information into clearly delineated sections: Repository Context, Issue (when available), Pull Request, Relevant Files Found (complete file contents), LLM-generated Summary, and Edits. The Edits section contains the code modifications: for PRs with multiple commits, we concatenate them in temporal order, with each refined commit message followed by its associated code changes. The two subsets use different structural formats: Dctx gen adopts XML-like tags with traditional patch diffs and additionally includes developer comments and reviews, while Dctx py uses Markdown structure with search-and-replace blocks that more directly represent agent editing actions. See Appendix for detailed format specifications and examples. This organization mirrors the workflow: relevant file paths simulates the localize phase, file contents represent the read phase, edits represent the edit phase, and LLM-generated contents serve as textual reasoning in between. We apply standard post-processing to ensure training efficiency and evaluation integrity. For length filtering, we discard samples exceeding 32k tokens, which retains over 90% of Python pull requests while improving training efficiency. For decontamination, we remove all pull requests from repositories included in SWE-Bench Verified to prevent data leakage and ensure fair evaluation."
        },
        {
            "title": "3.1.3 Corpus Statistics\nAfter applying the pipeline described, we obtain two complementary subsets. The general subset Dctx\ngen (26.7B\ntokens) provides broad software engineering coverage across diverse languages and tooling patterns, drawn from\n4 million PRs in the top 10,000 most-starred repositories. The Python subset Dctx\n(41.9B tokens), comprising 6\npy\nmillion PRs from 7.4 × 105 repositories, focuses on Python development to ensure alignment with benchmarks.",
            "content": "1REST API: https://docs.github.com/en/rest 2GraphQL API: https://docs.github.com/en/graphql 3In the future, we will scale our approach to more repositories."
        },
        {
            "title": "3.2 Environmentally-Native Trajectories",
            "content": "Together, Dctx = Dctx development workflows across diverse repositories and scenarios. gen Dctx py contains 68.6B tokens of contextually-native trajectories and preserves complete"
        },
        {
            "title": "3.2 Environmentally-Native Trajectories\nNotation. We denote the environmentally-native dataset as Denv, consisting of trajectories τ env. We further split it\ninto Denv",
            "content": "fail based on the final test outcome. pass and Denv"
        },
        {
            "title": "3.2.1 Design Rationale\nWhile contextually-native trajectories provide agent-like structure, they lack agent-like dynamics: the model never\nobserves the iterative feedback loop (edit → test → revise) that characterizes real agentic coding in practice.\nTo close this gap, we curate environmentally-native trajectories—collected by running a capable agent in real\nexecutable development environments with authentic test feedback.",
            "content": "Our approach contrasts with trajectories in simulated or synthetic environments (Yang et al., 2025c) where codebase navigation is read-only or test execution is unavailable during rollout. While such approaches produce trajectories with agentic format, they lack agentic feedbackthe model never observes how its edits affect test outcomes or how error messages guide revisions. Environmentally-native trajectories preserve this critical feedback loop by recording actual agent-environment interactions: tool invocations, test executions, and the resulting observations (test outputs, error messages, runtime feedback)."
        },
        {
            "title": "3.2.2 Construction Pipeline\nIn order to ensure authenticity, we choose to derive executable environments from real GitHub pull requests,\nrather than artificially constructed ones. Therefore, we construct our agentic rollout environments following the\nmethodology established in SWE-REBENCH (Badertdinov et al., 2025a). We build a Docker image for each\ntask that reproduces the repository state at a specific commit, alongside unit tests from the actual codebase, and\ninfrastructure to execute tool calls (file edits, shell commands, test runs). Then we deploy GLM-4.6 (Team et al.,\n2025a) within the SWE-AGENT framework (Yang et al., 2025b). For each task, we generate up to 4 rollouts,\nrecording the complete action-observation sequences: the agent’s actions and the environment’s responses (file\ncontents, search results, test outcomes, error messages).",
            "content": "After discarding trajectories exceeding 128k tokens, we classify the remaining trajectories based on final test outcomes into passing trajectories (all tests pass) and non-passing trajectories (tests fail). Both types provide valuable learning signals: passing trajectories demonstrate complete problem-solving cycles, while non-passing trajectories capture realistic debugging scenarios with error feedback. pass (0.7B tokens) where all tests pass, and 5.55 104 non-passing trajectories Denv"
        },
        {
            "title": "4.1 Training Pipeline Terminology",
            "content": "We clarify our position within the standard LLM development pipeline: Pre-training. Large-scale next-token prediction on diverse corpora. Mid-training (MT). An intermediate stage that shifts capability distribution by training on curated domain data at scale (Wang et al., 2025d). Unlike fine-tuning (which teaches specific behaviors), mid-training operates at the knowledge level. Post-training. Supervised fine-tuning (SFT) on demonstrations and/or reinforcement learning."
        },
        {
            "title": "4.2 Experimental Setup\nBase model. Unless otherwise specified, we start from base model Qwen2.5-72B-Base and Qwen2.5-32B-Base.",
            "content": "Evaluation. We evaluate on SWE-Bench Verified using SWE-AGENT (temperature 0, 128k context and 100 steps) and report Pass@1, averaged across 4 runs. We manually fix small number of test cases where the provided ground truth patch cannot pass due to various reasons."
        },
        {
            "title": "4.3 Mid-Training Provides Robust Gains",
            "content": "Model / Variant Qwen 2.5 32B Series Baseline (Weak SFT) Baseline (Strong SFT) Ours (Weak SFT) Ours (Strong SFT) Ours (daVinci-Dev-32B) Qwen 2.5 72B Series Baseline (Weak SFT) Baseline (Strong SFT) Kimi-Dev (Yang et al., 2025c) Kimi-Dev (Yang et al., 2025c) Kimi-Dev (Yang et al., 2025c) Ours (Weak SFT) Ours (Strong SFT) Ours (daVinci-Dev-72B) Mid-training Data Post-training Data Post-training Method SWE-V - - Dctx Dctx Dctx + Denv - - DAgentlessMT DAgentlessMT DAgentlessMT Dctx Dctx Dctx + Denv DSWE-smith Denv pass DSWE-smith Denv pass Denv pass DSWE-smith Denv pass DSWE-smith DAgentlessRL + DSWE-smith DAgentlessRL + Denv pass DSWE-smith Denv pass Denv pass SFT SFT SFT SFT SFT SFT SFT SFT SFT+RL SFT+RL SFT SFT SFT 34.8 53.0 39.5 54.1 56.1 38.0 56.6 46.0 48.6 56.2 46.4 58.2 58.5 Table 1: Ablations and mid-training comparisons on SWE-Bench Verified (SWE-V). Our agentic mid-training on contextually-native trajectories (Dctx) and environmentally-native trajectories (Denv) consistently improves downstream performance, and is competitive with or surpasses prior mid-training recipes. All results use SWEAGENT for evaluation. Trained and tested using our infrastructure. Estimated from Figure 5 in Yang et al. (2025c). Training stages. We consider two stages: (i) mid-training (MT) on large-scale unlabeled corpora (PR data and/or trajectories), and (ii) supervised fine-tuning (SFT) on agentic trajectories. The training configuration is detailed in A. Data components. We use three main data sources. For compactness in tables, we denote datasets with symbols (defined in Section 2.1): For contextually-native trajectories (Dctx), we transform GitHub pull requests into the structured format described in 3.1. In this setting, we use two subsets: (41.9B): Python-focused subset for alignment with software engineering benchmarks. Dctx py Dctx gen (26.7B): general subset drawn from most-starred repositories across all languages. Dctx (68.6B): Dctx gen Dctx py . For environmentally-native trajectories (Denv), we collect rollouts by running SWE-AGENT with GLM-4.6 in executable Docker environments derived from real GitHub pull requests, forming Denv (3.1B raw tokens; 4.5B effective tokens). We upsample Denv pass by 3 during training. For data to activate the model after mid-training, we may use: pass (0.7B): subset of Denv that pass the unit tests Denv DSWE-smith (0.11B tokens): public set of SWE-AGENT trajectories released by Yang et al. (2025b) (mostly generated with Claude 3.7 Sonnet), which we use as an external SFT baseline. Baselines. For Kimi-Dev comparisons, we quote results from Yang et al. (2025c) where applicable, and match our SFT dataset DSWE-smith and parameters (A) close to theirs. For experiments requiring downstream SFT on Denv pass  (Table 1)  , we utilize the official Kimi-Dev-72B checkpoint as the starting point, as their pre-RL mid-training checkpoint is not publicly available."
        },
        {
            "title": "4.3 Mid-Training Provides Robust Gains",
            "content": "Our most important finding is that agent-native mid-training improves performance even when strong trajectory SFT already yields competitive results. For robustness we also validate this across two SFT regimes and against the strongest prior MT recipe (with best effort). The comparison results are shown in Table 1."
        },
        {
            "title": "4.4 Comparison with Open Recipes",
            "content": "Model Qwen 2.5 32B Coder Series R2EGym-Agent (Jain et al., 2025) Openhands-LM (Wang et al., 2025c) SWE-Agent-LM (Yang et al., 2025b) SWE-Mirror-LM (Wang et al., 2025b) Skywork-SWE (Zeng et al., 2025) SWE-Dev (Wang et al., 2025a) Qwen 3 32B Series DeepSWE-Preview (Luo et al., 2025) FrogBoss (Sonwane et al., 2025) SWE-Lego-Qwen3-32B (Tao et al., 2026) Qwen 2.5 32B Series daVinci-Dev-32B (Ours) Qwen 2.5 72B Series Kimi-Dev (Yang et al., 2025c) daVinci-Dev-72B (Ours) Base or Inst. Midtraining Post-training Method Scaffold SWE-V Base Inst. Inst. Inst. Inst. Inst. Inst. Inst. Inst. Base Base Base No No No No No No No No No Yes Yes Yes SFT SFT SFT SFT SFT SFT+RL R2E-Gym OpenHands SWE-Agent MOpenHands OpenHands OpenHands RL SFT SFT OpenHands SWE Agent OpenHands 34.4 37.2 40.2 52.2 38.0 36.6 42.2 54.6 52. SFT SWE-Agent 56.1 SFT+RL SFT Agentless SWE-Agent 48.6 58. Table 2: Comparison with representative methods on SWE-Bench Verified (SWE-V). We include representative works with agentic scaffolds. Robustness across SFT regimes. On the 72B model, our MT consistently boosts performance. With weak SFT, we improve from 38.0% to 46.4% with only Dctx MT, matching the Kimi-Dev baseline despite using fewer than half the tokens (68.6B vs. 150B) and no synthetic reasoning data. With strong SFT, we reach 58.2% with only Dctx MT, outperforming the RL-tuned Kimi-Dev checkpoint and SFT-only baseline. This indicates that our contextually-native representationbundling file context and editssuccessfully bridges the gap between pre-training and agentic fine-tuning. With Dctx + Denv MT, our performance further increases to our strongest result 58.5%, showing that adding environmentally-native trajectories to MT enables the model to internalize the dynamics of the execution environment. Robustness across scales. The benefits of our MT recipes transfer effectively from 72B to the smaller 32B model. On the 32B scale, Dctx MT improves the weak SFT baseline by 4.7% and the strong SFT baseline by 1.1%, and Dctx + Denv MT continues to deliver the best performance 56.1%, 3.1% above the strong SFT baseline. This confirms that the effectiveness of contextually-native trajectories and environmentally native trajectories is not specific to single model capacity."
        },
        {
            "title": "4.4 Comparison with Open Recipes\nWe compare our full recipe against representative open methods on SWE-Bench Verified based on the\nQwen2.5 model family and use agentic scaffolds. Table 2 presents the results.",
            "content": "Results. Within the 72B scale, our daVinci-Dev-72B achieves 58.5%, surpassing 48.6% for Kimi-Dev using the same base model and agentic scaffold. At 32B scale, daVinci-Dev-32B achieves 56.1%, which is state-of-the-art among open training recipes at this scale using agentic scaffolds, despite the fact that prior work uses Qwen2.5-Coder-32B series or Qwen3-32B while our method starts from non-coder Qwen2.5-32B-Base."
        },
        {
            "title": "4.5 Generalization Beyond SWE Tasks",
            "content": "While our agentic mid-training is specialized for software engineering, we investigate whether the agentic capabilities acquired from processing Pull Requests and execution trajectories transfer to broader domains requiring complex logic. We focus our evaluation on two distinct categories: standard code generation and rigorous scientific reasoning. In this experiment we choose clean single stage MT recipe with Dctx py + Denv as dataset. As reported in Table 3, our model demonstrates strong generalization performance, consistently surpassing the base models across both 32B and 72B scales. In code generation, we observe substantial gains on HumanEval (Chen et al., 2021) and EvalPlus (Liu et al., 2023) (after decontamination following XCoder (Wang et al., 2024b)), confirming that our data improves fundamental coding proficiency. More notably, we observe transfer learning to scientific benchmarks such as GPQA (Rein et al., 2023) and SciBench (Wang et al., 2024a). These tasks, which demand expert-level domain knowledge and multi-step reasoning, benefit from the decision-making patterns inherent in our agentic mid-training. This suggests that the logic required for autonomous software engineering fosters fundamental reasoning skills that generalize beyond code. 8 5. Analysis Qwen2.5-32B Qwen2.5-72B Benchmark Base MT Mix Base MT Mix Scientific Benchmarks 38.17 GPQA-Main 33.85 SuperGPQA 18.46 SciBench Code Benchmarks HumanEval EvalPlus DS-1000 58.16 50.13 12.2 38.84 35.94 20.49 81.42 71.31 21.2 +0.67 +2.09 +2. +23.26 +21.18 +9.0 43.30 37.76 19.33 64.27 56.04 21.4 44.87 39.27 19.77 76.73 69.45 24.7 +1.57 +1.51 +0. +12.46 +13.41 +3.3 Table 3: Generalization performance on scientific and code benchmarks. We report the base model performance and the impact of our MT stages. MT Mix refers to the model trained on Dctx py + Denv."
        },
        {
            "title": "5 Analysis",
            "content": "In this section, we analyze the factors contributing to the effectiveness of agentic mid-training. We first examine the efficiency and information density of contextually-native data, then explore the synergistic relationship between our two data types, and finally discuss the scalability of this paradigm."
        },
        {
            "title": "5.1 High Information Density and Efficiency",
            "content": "A key advantage of our approach is token efficiency. Kimi-Devs recipe involves 70B tokens directly derived from PR plus 20B synthetic trajectory/CoT tokens upsampled 4 times, totaling 150B tokens. In contrast, our 68.6B tokens Dctx MT stage consistently outperforms Kimi-Dev as shown in section 4.3, and performance further grows with additional 4.5B effective tokens Denv added to MT training stage. This efficiency stems from our contextually-native representation being closer to software engineering agents test distribution compared to factorized approaches, and our environmentally-native trajectories being more authentic than simulated trajectories."
        },
        {
            "title": "5.2 Synergy: contextually-native data amplifies trajectory learning",
            "content": "While environmentally-native trajectories provide the correct format for agentic interaction, we find they are insufficient for generalization when used in isolation. Table 4 presents an ablation study on the composition of MT data across both 32B and 72B model scales. MT Data Composition Tokens SFT Data 32B Base 72B Base SWE-Verified Ablation: Trajectories vs. PR (Zero-shot / No SFT) Denv Denv + Dctx py 4.5B 46.4B Impact of MT Composition on SFT Dctx py Denv + Dctx py Denv + Dctx 41.9B 46.4B 73.1B Denv pass Denv pass Denv pass 43.7 49.9 52.9 53.6 56.1 47.1 54. 56.5 57.8 58.5 Table 4: Ablation of data components in MT. Top: In zero-shot setting (no SFT), grounding trajectories with PR data yields massive gains (+7.7% on 72B). Bottom: Even when performing SFT, exposing the model to trajectories during MT improves final performance (comparing rows 3 and 4). Trajectories require PR grounding. In the zero-shot setting (top section), training on environmentally-native trajectories alone yields 47.1% (72B). However, mixing in the Python contextually-native subset Dctx py boosts performance to 54.8%a significant +7.7% gain. This suggests that while environmentally-native trajectories teach the model how to interact with the environment, contextually-native data provides the necessary knowledge and code modification diversity required to solve complex issues. Mid-training on trajectories aids SFT. key question in agent training is whether double-dippingtraining on trajectories during MT and then fine-tuning on them during SFTprovides value. Comparing the first two rows of the SFT section  (Table 4)  , we observe consistent improvement when trajectories are included in MT. For the 72B model, adding trajectories to the MT mix improves the final SFT score from 56.5% to 57.8%. This indicates that mid-training allows the model to internalize the dynamics of the execution environment more deeply"
        },
        {
            "title": "5.3 Scalability: from raw PRs to executable tasks",
            "content": "Figure 4: Scaling Law of Agent-Native Mid-training. Pass@1 performance on SWE-Bench Verified during mid-training (MT) on the Dctx py + Denv mixture. The strong log-linear fit indicates that agentic capabilities scale predictably with training steps and data consumption, suggesting the model has not yet saturated. than SFT alone, creating better initialization for the final alignment stage. Finally, our strongest result, 58.5% (72B) and 56.1% (32B) comes from scaling the contextually-native foundation from Dctx (41.9B) to the full Dctx py (68.6B). This demonstrates that while mixing trajectories into MT is beneficial, the sheer scale and diversity of contextually-native supervision remain the dominant factors in model performance."
        },
        {
            "title": "5.3 Scalability: from raw PRs to executable tasks",
            "content": "Our approach is scalable along two axes: data availability and empirical performance scaling. Empirical Scaling. Beyond the theoretical abundance of data, we verify that our model effectively converts additional compute and training steps into performance gains. Figure 4 illustrates the learning curves of both Qwen2.5-72B and Qwen2.5-32B during the MT stage on the Dctx py + Denv mixture. We observe robust loglinear relationship between training steps and Pass@1 performance for both model sizes (R2 0.90). Specifically, the 72B model climbs to 54.9%, while the 32B model follows parallel trajectory to reach 49.9%. Notably, we select the Dctx py + Denv mixture as training dataset in this experiment because only when Denv is added can the models achieve zero-shot agentic capabilities (without SFT) directly from mid-training, and that we train the model using only one stage A.1. The consistent monotonic upward trends, suggests that performance has not saturated. This indicates that further scaling would yield continued improvements. is built from 1.3 107 pull requests (before filtering) in Scaling PR data. The Python-focused subset Dctx py 7.4 105 repositories while the multi-language subset Dctx gen only utilize the 1 104 most starred repositories. However, our survey indicates there are 3 108 pull requests in 109 public repositories, suggesting substantial headroom to scale the corpus by expanding language coverage and relaxing filters. Scaling executable supervision. Recent advances in environment construction (Badertdinov et al., 2025a) demonstrate that PR-derived data can be automatically transformed into deeper supervision: executable tasks with Docker environments and unit tests, constructed via fully automated pipeline. Conceptually, this is more processed form of PR data than our raw PR corpora. Because verification is test-based and environments are executable, scaling increases not only quantity but also authenticity: each additional task comes with real environment feedback rather than synthetic traces. Importantly, SWE-REBENCH builds its public dataset from 3,468 Python repositories and reports 21,336 validated tasks, suggesting substantial headroom for scaling as repository coverage expands. This supports long-term path where raw PR mining provides breadth, while rebench-style processing provides depth and verifiability for agent training."
        },
        {
            "title": "6 Limitations",
            "content": "Data privacy and attribution. We did not explicitly remove developer identifiers from PR text in the general subset Dctx gen , which may raise privacy concerns and could lead to memorization of contributor names. 10 7. Related Work Evaluation sensitivity. Some results depend on patched evaluation harness that fixes small number of benchmark issues. This introduces an additional source of variance. Scope. We focus on single base model family and single benchmark. Extending to other model families and more real-world agentic tasks (Li et al., 2026; Xu et al., 2025; Wu et al., 2025) is left for future work."
        },
        {
            "title": "7 Related Work",
            "content": "Mid-training Recent work increasingly positions mid-training as critical bridge between large-scale pre-training and post-training. Rather than transitioning directly from noisy, web-scale corpora to SFT or RL, mid-training introduces higher-quality, task-structured, or instruction-oriented data at later stages of training, often paired with learning-rate annealing (Mo et al., 2025; Zhang et al., 2025; Tu et al., 2025). For example, OctoThinker (Wang et al., 2025d) argues that mid-training can substantially improve both the sample efficiency and the achievable performance ceiling of subsequent RL by stabilizing internal representations and encouraging reasoning-friendly behaviors. Despite these advances, existing studies provide only limited insight into agentic mid-training data. For example, although Kimi-Dev (Yang et al., 2025c) incorporates data such as file retrieval and file editing during mid-training, these behaviors are treated in isolation and do not constitute coherent, end-to-end agentic process. In contrast to prior work, this paper introduces Agent-Native Mid-Training, paradigm that treats agentic behavior as first-class training objective. We systematically design and construct mid-training data that reflects complete agentic processes and release both the construction methodology and the resulting datasets to the community. Agentic training Agentic training builds upon prior work, SFT and RL. Early agents were predominantly trained by sampling trajectories in specific environments using closed-source large models, followed by applying SFT to distill the collected data into smaller, task-specialized models (Zeng et al., 2024; Chen et al., 2024; Xi et al., 2024; Fu et al., 2025b). SWE-smith (Yang et al., 2025b), BugPilot (Sonwane et al., 2025), SWE-rebench (Badertdinov et al., 2025b), and SWE-Factory (Guo et al., 2026) use these ideas to create datasets in the domain of code agents. With the introduction of GRPO (Shao et al., 2024; Liu et al., 2025), recent work has increasingly focused on training agents capable of multi-step reasoning, tool usage, and explicit interaction with external environments (Team et al., 2025b; Zheng et al., 2025; Li et al., 2025). Despite growing body of research on agentic post-training, systematic studies of agentic mid-training remain notably scarce. Since mid-training can gain more diverse data than the post-training stage, exploring its role and potential benefits becomes important. Data synthesis Early approaches to synthetic data primarily focused on recombining and rewriting large-scale corpora and using reject-sampling to get the final data (Yuan et al., 2023). As the demand for data scale and coverage increased, persona-driven synthesis was introduced (Ge et al., 2025; Fu et al., 2025a), enabling systematic expansion of the task space beyond naturally occurring data. More recently, with the rise of agent-oriented research, synthetic data has shifted from text-level generation to the synthesis of agentic processes. Through interaction within synthetic environments (Liu et al., 2025; Team et al., 2025b; Badertdinov et al., 2025b), models actively generate data containing decision-making trajectories, feedback loops, and long-horizon dependencies. In this paper, synthetic data for mid-training has become an emerging trend. Mid-training synthesis emphasizes agentic data to shape intermediate representations, serving as critical bridge between pre-training and post-training."
        },
        {
            "title": "8 Conclusion",
            "content": "In this work, we demonstrated that the agentic coding capabilities of large language models can be substantially enhanced through rigorous data-centric strategy leveraging GitHub pull requests and executable interaction trajectories. By constructing unified training recipe that combines 68.6B tokens of context-rich PR data with high-quality, verified rollouts collected in executable environments, we obtained daVinci-Dev-72B with strong performance on SWE-Bench Verified (58.5%), surpassing recent baselines such as Kimi-Dev. Across experiments, the key driver is agent-native dataagent-native PR supervision (context-complete samples) plus environmentally-native trajectories (executable, test-verified rollouts). Our analysis highlights two critical insights. First, the structural representation of PR data is paramount; keeping relevant file contents and commit edits together provides cohesive supervision signal that mirrors the localizeread-edit loop of code agents, proving more effective than decomposing PRs into isolated subtasks. Second, not all agentic trajectories are equal. We showed that training on executable, test-verified passing trajectories yields significantly higher gains than training on static or simulated traces. The synergy between these data sourcesusing PRs to establish general software engineering priors and verified trajectories to specialize agentic behavioroffers highly token-efficient path to strong performance. Looking forward, these results suggest scalable paradigm for future code agent development. With the vast availability of public repositories and the increasing feasibility of automated environment verification, there is substantial headroom to expand this approach to broader language ecosystems and more complex software"
        },
        {
            "title": "References",
            "content": "maintenance tasks. As the field shifts from single-turn code generation to autonomous engineering, bridging the gap between static historical data and dynamic execution environments will be essential."
        },
        {
            "title": "Acknowledgments",
            "content": "We express our gratitude to Haoyang Zou, Zengzhi Wang, and Fan Zhou for their constructive feedback and stimulating discussions. We are also grateful to Liming Liu for his guidance and advice."
        },
        {
            "title": "References",
            "content": "[1] Ibragim Badertdinov, Alexander Golubev, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Andrei Andriushchenko, Maria Trofimova, Daria Litvintseva, and Boris Yangel. 2025a. Swe-rebench: An automated pipeline for task collection and decontaminated evaluation of software engineering agents. [2] Ibragim Badertdinov, Alexander Golubev, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Andrei Andriushchenko, Maria Trofimova, Daria Litvintseva, and Boris Yangel. 2025b. Swe-rebench: An automated pipeline for task collection and decontaminated evaluation of software engineering agents. [3] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. [4] Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. 2024. Agent-flan: Designing data and methods of effective agent tuning for large language models. [5] Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma Gongque, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, and Weiran Xu. 2025a. Agentrefine: Enhancing agent generalization through refinement tuning. arXiv preprint arXiv:2501.01702. [6] Dayuan Fu, Yunze Wu, Xiaojie Cai, Lyumanshan Ye, Shijie Xia, Zhen Huang, Weiye Si, Tianze Xu, Jie Sun, Keyu Li, Mohan Jiang, Junfei Wang, Qishuo Hua, Pengrui Lu, Yang Xiao, and Pengfei Liu. 2025b. Interaction as intelligence part ii: Asynchronous human-agent rollout for long-horizon task training. [7] Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. 2025. Scaling synthetic data creation with 1,000,000,000 personas. [8] Lianghong Guo, Yanlin Wang, Caihua Li, Wei Tao, Pengyu Yang, Jiachi Chen, Haoyu Song, Duyu Tang, and Zibin Zheng. 2026. Swe-factory: Your automated factory for issue resolution training data and evaluation benchmarks. [9] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974. [10] Naman Jain, Jaskirat Singh, Manish Shetty, Liang Zheng, Koushik Sen, and Ion Stoica. 2025. R2e-gym: Procedural environments and hybrid verifiers for scaling open-weights swe agents. [11] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770. [12] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative reasoning problems with language models. [13] Keyu Li, Junhao Shi, Yang Xiao, Mohan Jiang, Jie Sun, Yunze Wu, Shijie Xia, Xiaojie Cai, Tianze Xu, Weiye Si, et al. 2026. Agencybench: Benchmarking the frontiers of autonomous agents in 1m-token real-world contexts. arXiv preprint arXiv:2601.11044. [14] Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025. Torl: Scaling tool-integrated rl."
        },
        {
            "title": "References",
            "content": "[15] Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, et al. 2025. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556. [16] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems. [17] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Munoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2024. Starcoder 2 and the stack v2: The next generation. [18] Michael Luo, Naman Jain, Jaskirat Singh, Sijun Tan, Ameen Patel, Qingyang Wu, Alpay Ariyak, Colin Cai, Tarun Venkat, Shang Zhu, Ben Athiwaratkun, Manan Roongta, Ce Zhang, Li Erran Li, Raluca Ada Popa, Koushik Sen, and Ion Stoica. 2025. Deepswe: Training fully open-sourced, state-of-the-art coding agent by scaling rl. Blog post. [19] Kaixiang Mo, Yuxin Shi, Weiwei Weng, Zhiqiang Zhou, Shuman Liu, Haibo Zhang, and Anxiang Zeng. 2025. Mid-training of large language models: survey. [20] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2023. Gpqa: graduate-level google-proof q&a benchmark. [21] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. [22] Atharv Sonwane, Isadora White, Hyunji Lee, Matheus Pereira, Lucas Caccia, Minseon Kim, Zhengyan Shi, Chinmay Singh, Alessandro Sordoni, Marc-Alexandre Cˆote, and Xingdi Yuan. 2025. Bugpilot: Complex bug generation for efficient learning of swe skills. [23] Chaofan Tao, Jierun Chen, Yuxin Jiang, Kaiqi Kou, Shaowei Wang, Ruoyu Wang, Xiaohui Li, Sidi Yang, Yiming Du, Jianbo Dai, Zhiming Mao, Xinyu Wang, Lifeng Shang, and Haoli Bai. 2026. Swe-lego: Pushing the limits of supervised fine-tuning for software issue resolving. [24] 5 Team, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, Mingshu Zhai, Pengfan Du, Qian Dong, Shangde Lei, Shangqing Tu, Shangtong Yang, Shaoyou Lu, Shijie Li, Shuang Li, Shuang-Li, Shuxun Yang, Sibo Yi, Tianshu Yu, Wei Tian, Weihan Wang, Wenbo Yu, Weng Lam Tam, Wenjie Liang, Wentao Liu, Xiao Wang, Xiaohan Jia, Xiaotao Gu, Xiaoying Ling, Xin Wang, Xing Fan, Xingru Pan, Xinyuan Zhang, Xinze Zhang, Xiuqing Fu, Xunkai Zhang, Yabo Xu, Yandong Wu, Yida Lu, Yidong Wang, Yilin Zhou, Yiming Pan, Ying Zhang, Yingli Wang, Yingru Li, Yinpei Su, Yipeng Geng, Yitong Zhu, Yongkun Yang, Yuhang Li, Yuhao Wu, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yuxuan Zhang, Zezhen Liu, Zhen Yang, Zhengda Zhou, Zhongpei Qiao, Zhuoer Feng, Zhuorui Liu, Zichen Zhang, Zihan Wang, Zijun Yao, Zikang Wang, Ziqiang Liu, Ziwei Chai, Zixuan Li, Zuodong Zhao, Wenguang Chen, Jidong Zhai, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, and Jie Tang. 2025a. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. [25] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. 2025b. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534."
        },
        {
            "title": "References",
            "content": "[26] Chengying Tu, Xuemiao Zhang, Rongxiang Weng, Rumei Li, Chen Zhang, Yang Bai, Hongfei Yan, Jingang Wang, and Xunliang Cai. 2025. survey on llm mid-training. [27] Haoran Wang, Zhenyu Hou, Yao Wei, Jie Tang, and Yuxiao Dong. 2025a. Swe-dev: Building software engineering agents with training and inference scaling. [28] Junhao Wang, Daoguang Zan, Shulin Xin, Siyao Liu, Yurong Wu, and Kai Shen. 2025b. Swe-mirror: Scaling issue-resolving datasets by mirroring issues across repositories. [29] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. 2024a. Scibench: Evaluating college-level scientific problemsolving abilities of large language models. [30] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. 2025c. Openhands: An open platform for AI software developers as generalist agents. In The Thirteenth International Conference on Learning Representations. [31] Yejie Wang, Keqing He, Dayuan Fu, Zhuoma Gongque, Heyang Xu, Yanxu Chen, Zhexu Wang, Yujia Fu, Guanting Dong, Muxi Diao, Jingang Wang, Mengdi Zhang, Xunliang Cai, and Weiran Xu. 2024b. How do your code llms perform? empowering code instruction tuning with high-quality data. [32] Yejie Wang, Keqing He, Dayuan Fu, Zhuoma Gongque, Heyang Xu, Yanxu Chen, Zhexu Wang, Yujia Fu, Guanting Dong, Muxi Diao, et al. 2024c. How do your code llms perform? empowering code instruction tuning with really good data. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1402714043. [33] Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. 2025d. Octothinker: Mid-training incentivizes reinforcement learning scaling. [34] Yunze Wu, Dayuan Fu, Weiye Si, Zhen Huang, Mohan Jiang, Keyu Li, Shijie Xia, Jie Sun, Tianze Xu, Xiangkun Hu, et al. 2025. Innovatorbench: Evaluating agents ability to conduct innovative llm research. arXiv preprint arXiv:2510.27598. [35] Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, Songyang Gao, Lu Chen, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. 2024. Agentgym: Evolving large language model-based agents across diverse environments. [36] Tianze Xu, Pengrui Lu, Lyumanshan Ye, Xiangkun Hu, and Pengfei Liu. 2025. Researcherbench: Evaluating deep ai research systems on the frontiers of scientific inquiry. [37] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025a. Qwen3 technical report. arXiv preprint arXiv:2505.09388. [38] John Yang, Kilian Lieret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. 2025b. Swe-smith: Scaling data for software engineering agents. [39] Zonghan Yang, Shengjie Wang, Kelin Fu, Wenyang He, Weimin Xiong, Yibo Liu, Yibo Miao, Bofei Gao, Yejie Wang, Yingwei Ma, Yanhao Li, Yue Liu, Zhenxing Hu, Kaitai Zhang, Shuyi Wang, Huarong Chen, Flood Sung, Yang Liu, Yang Gao, Zhilin Yang, and Tianyu Liu. 2025c. Kimi-dev: Agentless training as skill prior for swe-agents. [40] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387. [41] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. 2025. Dapo: An open-source llm reinforcement learning system at scale. [42] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023. Scaling relationship on learning mathematical reasoning with large language models. [43] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. 2024. AgentTuning: Enabling generalized agent abilities for LLMs. In Findings of the Association for Computational Linguistics: ACL 2024, pages 30533077, Bangkok, Thailand. Association for Computational Linguistics. 14 A. Training Details [44] Liang Zeng, Yongcong Li, Yuzhen Xiao, Changshi Li, Chris Yuhao Liu, Rui Yan, Tianwen Wei, Jujie He, Xuchen Song, Yang Liu, and Yahui Zhou. 2025. Skywork-swe: Unveiling data scaling laws for software engineering in llms. [45] Charlie Zhang, Graham Neubig, and Xiang Yue. 2025. On the interplay of pre-training, mid-training, and rl on reasoning language models. [46] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments."
        },
        {
            "title": "A Training Details",
            "content": "A.1 Dataset Components and Staging PR MT Staging. Our Dctx (68.6B) training was conducted in two sequential stages rather than single mix. We first trained on the general subset Dctx gen (26.7B tokens) to establish broad software engineering baseline. We then performed mid-training (MT) on the Python subset Dctx (41.9B tokens) to specialize the model on agent-native, py Python-centric patterns. Our Dctx + Denv (73.1B) was also conducted in two sequential stages where the first stage is the general subset Dctx gen (26.7B tokens) and the second stage is the other two datasets. SFT Configuration. For all SFT experiments involving our Denv pass or DSWE-smith datasets, we trained for 5 epochs. A.2 Hyperparameters We provide the key hyperparameters used for Mid-training (MT) and Supervised Fine-tuning (SFT) below. MT Hyperparameters. We use global batch size of 1024 samples and peak learning rate of 8 105. The learning rate schedule utilizes warmup ratio of 0.05 (5% of total training steps), followed by cosine decay until all samples are consumed once (1 epoch). No loss mask is applied during MT. SFT Hyperparameters. We use global batch size of 128 samples and peak learning rate of 1 105. The learning rate schedule utilizes warmup ratio of 0.10 (10% of total training steps), followed by cosine decay until all samples are consumed once per epoch. standard loss mask is applied to user and tool tokens during SFT."
        },
        {
            "title": "B LLM prompts used for PR rendering",
            "content": "During context enrichment (Section 3.1), we optionally call an LLM to (i) generate concise pull-request summary and (ii) normalize/optimize commit messages for readability. We use Qwen3-235B-A22b-Instruct-2507 with fixed output budgets (512 tokens for PR summaries; 256 tokens for commit-message refinement)."
        },
        {
            "title": "C Dataset Formats",
            "content": "We include the templates for two types of data in the contextually-native dataset: (i) the General PR format, and (ii) the Python PR agentic format. The General PR format uses XML-like tags similar to The Stack v2 (Lozhkov et al., 2024) and includes rich interaction history (comments and reviews). Events related to pull requests are concatenated in chronological order. Different from The Stack v2, we always include relevant file content and grouped review comments threads. This corpus is sourced from top-starred repositories without the 15 Python-file constraint. The Python PR format uses Markdown structure and represents edits in search-and-replace action space. It includes an LLM-generated PR summary after presenting all related files (simulating overall planning and reasoning phase in an agentic workflow) and enhanced commit messages (simulating textual reasoning before action). Edits are rewritten from git diff format to search-replace format used in many agentic scaffolds. The # Issue section is omitted if no linked issue is found."
        },
        {
            "title": "D Benchmark decontamination",
            "content": "In our training dataset we take measures to remove samples related to the SWE-Bench Verified benchmark as detailed in Section 3.1. For the HumanEval and EvalPlus benchmarks, we follow the decontamination procedure of XCoder (Wang et al., 2024b). Concretely, for each benchmark instance we form the reference text by concatenating the prompt and canonical solution, tokenize it, and compute the set of unique n-grams (n = 13). We then scan the tokenized training corpus and, for every training sample, compute its set of unique 13-grams and the overlap with each benchmark instance. Similarity is measured as leakage ratio: leakage ratio(e, x) = Ge Gx Ge , 15 D. Benchmark decontamination"
        },
        {
            "title": "PR Summary Prompt",
            "content": "Summarize this pull request in 1-4 clear sentences: Repository: {{.RepoName}} Description: {{.RepoDesc}} PR Title: {{.Title}} PR Description: {{.Body}} {{if .Issue}}Related Issue: {{.Issue.Title}} {{.Issue.Body}} {{end}}Changed Files: {{range .ChangedPyFiles}}- {{.}} {{end}} Commits: {{range .Commits}} ## Message: {{.Message}} Changes: {{range .Diffs}} File: {{.Path}} {{.Patch}} {{end}} {{end}} Please provide clear and concise summary (1-4 sentences) of this Pull Request, focusing on: 1. What problem does it solve or what feature does it add? 2. What are the key changes made? 3. Any important implementation details? Summary:"
        },
        {
            "title": "Commit Message Refinement Prompt",
            "content": "Optimize this commit message for clarity and educational value while keeping it concise. PR Context Summary: {{.Summary}} Original commit message: {{.Commit.Message}} Diff Context: {{range .Commit.Diffs}}File: {{.Path}} {{truncatePatch .Patch 2000}} {{end}} Provide an optimized version that: 1. The subject is clear and descriptive 2. If the commit is trivial and the changes are minimal, dont add the footer 3. Otherwise, keep the footer in one sentence Refined commit message: Figure 5: Prompts used for optional LLM-based PR summary generation and commit-message refinement during PR rendering. 16 D. Benchmark decontamination"
        },
        {
            "title": "General PR Example",
            "content": "# Repository Context Name: parcel-bundler/parcel Description: The zero configuration build tool for the web. # Relevant Files Context ## packages/core/parcel-bundler/src/cli.js ... if (command.name() === serve && command.target === browser) { const server = await bundler.serve( command.port 1234, command.https, command.host ); ... Response: <pr>Title: use env port lizzzp1: Adds process.env.PORT as default port option... <pr comment>mischnic: think it should rather be... <pr review>devongovett: Looks good to me. <pr review state>approved <pr commit>Liz P: use env port <commit file>packages/core/parcel-bundler/src/cli.js <patch> @@ -219,7 +219,7 @@ async function bundle(main, command) { if (command.name() === serve && command.target === browser) { const server = await bundler.serve( - + command.port 1234, process.env.PORT 1234, command.https, command.host ); </patch> <pr>devongovett <pr status>closed <pr is merged>True Figure 6: Example of the general PR format. 17 D. Benchmark decontamination"
        },
        {
            "title": "Python PR Example",
            "content": "# Repository Context Name: Pylons/waitress Description: Waitress - WSGI server for Python 3 # Issue ## xa0 and x85 are stripped from header values Given that these bytes are allowed in header values (due to obs-text), they shouldnt be stripped during header-field OWS stripping... # Pull Request ## Bugfix: Dont strip whitespace from values before inserting into environ This fixes small bug where the value of the header would get stripped when inserted into the environ so it no longer matched. Closes #432 # Relevant Files Found ## src/waitress/task.py ... ... for key, value in dict(request.headers).items(): value = value.strip() mykey = rename_headers.get(key, None) # Edits This pull request removes the erroneous .strip() call on header values in the WSGI environ construction. The HTTP specification allows certain non-ASCII bytes (xa0, x85) in header values via obs-text, and these should not be stripped. Remove the strip() call from header value processing in get environment() Edit: src/waitress/task.py Search: for key, value in dict(request.headers).items(): value = value.strip() mykey = rename_headers.get(key, None) if mykey is None: mykey = \"HTTP_\" + key Replace: for key, value in dict(request.headers).items(): mykey = rename_headers.get(key, None) if mykey is None: mykey = \"HTTP_\" + key Figure 7: Example of the Python PR agentic format. 18 D. Benchmark decontamination where Ge is the set of unique 13-grams in the benchmark instance and Gx is the set of unique 13-grams in training sample. For each benchmark instance, we take the maximum leakage ratio over all training samples as its contamination score. We manually selected the contamination threshold as τ = 0.10 based on case studies of high-overlap matches. Using this criterion, we identified 24 contaminated HumanEval instances, which were removed from evaluation."
        }
    ],
    "affiliations": [
        "GAIR",
        "SII",
        "SJTU"
    ]
}