{
    "paper_title": "LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation",
    "authors": [
        "Bo Miao",
        "Weijia Liu",
        "Jun Luo",
        "Lachlan Shinnick",
        "Jian Liu",
        "Thomas Hamilton-Smith",
        "Yuhe Yang",
        "Zijie Wu",
        "Vanja Videnovic",
        "Feras Dayoub",
        "Anton van den Hengel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap"
        },
        {
            "title": "Start",
            "content": "LangMap: Hierarchical Benchmark for Open-Vocabulary Goal Navigation Bo Miao1, Weijia Liu2, Jun Luo3, Lachlan Shinnick1, Jian Liu3, Thomas Hamilton-Smith1, Yuhe Yang4, Zijie Wu5, Vanja Videnovic6, Feras Dayoub1, Anton van den Hengel1 1AIML, Adelaide University 2East China Normal University 3NERC-RVC, Hunan University 4University Western Australia 5Singapore University of Technology and Design 6Breaker Industries Project page: bo-miao.github.io/LangMap 6 2 0 2 2 ] . [ 1 0 2 2 2 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "scene, room, region, and instance. The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: To this end, we present Language as Map (LangMap), large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using 4 fewer words. Comprehensive evaluations of strong zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish rigorous testbed for advancing language-driven embodied navigation. 1. Introduction Goal-oriented navigation (GN) is fundamental to embodied intelligence, underpinning applications such as homeassistant robots. It requires agents to comprehend user instructions, e.g., object categories [1, 2] or reference images [3, 4], and to perceive and reason within 3D environments to reach goals without step-by-step guidance. We focus on language-conditioned goal navigation (LGN), which enables intuitive and scalable human-robot interaction. Previous LGN research has primarily focused on objectFigure 1. HieraNav requires agents to interpret natural language and navigate to goals across four semantic levels: scene, room, region, and instance, where success means reaching target that satisfies the instruction. Instruction-relevant targets are color-coded. goal navigation, where agents locate any instance of given category (e.g., chair). Early benchmarks [2, 5] consider 621 common object categories and evaluate generalization to new environments. To improve scalability, HM3DOVON [1] introduces an open-vocabulary setting with 178 categories for evaluation, while LHPR-VLN [6] incorporates heuristically inferred room types. However, categorylevel navigation prioritizes perception and detection over semantic reasoning. This remains insufficient for real-world scenarios, where users specify context-dependent goals, such as find the phone on the bed with Bluey blanket, that require fine-grained semantic and spatial understanding for disambiguation (see Fig. 1). Recently, GOAT [7] and PSL [8] have attempted to unify categoryand instance-level navigation by leveraging visionlanguage models to automatically generate instructions from HM3D object views [9, 10]. However, current VLMs often fail to capture distinctive cues [11] and exhibit limited spatial reasoning [1215], resulting in descriptions that lack uniqueness and reliable spatial grounding. To assess the quality of these auto-generated instructions, we analyze same-category instances within each scene of GOAT-Bench [7] and examine whether the descriptions 1 instances within each scene to compose unambiguous descriptions, which are further cross-validated for accuracy. Each target features both concise descriptions emphasizing salient cues and detailed descriptions providing richer context, enabling evaluation across different instruction styles. We conduct qualitative and quantitative analyses on LangMap, demonstrating superior annotation quality and outperforming GOAT-Bench by 23.8% in discriminative accuracy using 4 fewer words. We further evaluate zero-shot and supervised models on LangMap, showing that while richer context and memory improve navigation success, long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish rigorous and unified testbed for advancing language-driven embodied navigation. Our main contributions are as follows: We introduce HieraNav, multi-granularity openvocabulary navigation task that unifies goals across four semantic levels: scene, room, region, and instance, to advance context-aware embodied navigation. We present LangMap, large-scale benchmark built on real-world 3D scans with comprehensive human-verified annotations, including region labels and discriminative regionand instance-level descriptions in concise and detailed forms. It spans diverse object categories and multilevel navigation tasks, delivering high-quality annotations that achieve 23.8% higher discriminative accuracy than GOAT-Bench. We evaluate strong zero-shot and supervised models on LangMap, analyzing performance across semantic granularities, long-tailed categories, object sizes, navigation distances, and sequential tasks to uncover key challenges and reasoning limitations. instruction styles, 2. Related Work Goal-oriented Navigation enables embodied agents to interpret instructions and navigate 3D environments to reach goals. Unlike vision-and-language navigation [1618], GN is starting-point independent and requires agents to explore and localize goals without step-by-step guidance. Existing GN tasks specify targets in various formats, such as point coordinates [1922], object categories [1, 2, 5, 23 28], reference images [3, 4, 2931], or VLM-generated instance descriptions [7, 8], typically focusing on categoryor instance-level targets. GN methods generally follow two paradigms: end-to-end reinforcement learning and modular architectures. End-to-end reinforcement learning approaches [3238] learn representations that map inputs to low-level actions but often suffer from low sample efficiency and poor interpretability. Modular architectures [20, 3942] decompose navigation into specialized components and construct explicit scene representations, such as scene Figure 2. Analysis of instance-level instructions in GOAT-Bench. are visually grounded and discriminative. As shown in Fig. 2, our manual inspection of ten scenes (around 30% of the evaluation set) reveals that 39.8% of instance-level instructions contain semantic errors (9.9%) or ambiguities (29.9%), where supposedly unique instance descriptions match multiple objects or exhibit nearly identical wording (see supplementary materials for examples). These findings expose the limitations of current VLM-based pipelines in generating accurate and unambiguous instructions for semantic navigation. In addition, tasks requiring spatial disambiguation across rooms or regions remain unexplored due to the lack of quality semantic annotations. To address these limitations, we argue that robust (i) cover broad spectrum of LGN benchmark should: goal granularities, from coarse scene-level to fine-grained instance-level; (ii) provide human-verified and discriminative descriptions that uniquely identify targets within each scene; and (iii) support open-vocabulary evaluation for both single-goal tasks at different semantic levels and multi-goal episodes with mixed-level goals. We introduce HieraNav, multi-granularity openvocabulary navigation task that unifies goals across four semantic levels: scene, room, region, and instance. As shown in Fig. 1, these levels capture the essential granularities of indoor navigation and reflect the diversity of realworld goal specifications. HieraNav requires agents to interpret natural language instructions, perform spatiotemporal reasoning, and navigate to targets aligned with user intent. To enable rigorous evaluation, we present Language as Map (LangMap), large-scale benchmark built on realworld HM3D scans [9, 10], augmented with comprehensive, human-verified semantic annotations and navigation tasks across all four semantic levels. Unlike prior datasets, LangMap provides region labels, discriminative region descriptions (absent from prior benchmarks), and discriminative instance descriptions covering 414 object categories (3.5 more than the VLM-generated annotations in GOATBench [7]), along with over 18k navigation tasks. Annotations are created via rigorous contrastive protocol in which annotators compare all same-category regions and 2 Figure 3. HieraNav requires agents to interpret natural language instructions and navigate to scene-, room-, region-, and instance-level targets. LangMap establishes the first large-scale benchmark for rigorous and systematic evaluation, featuring single-goal tasks at different semantic levels and multi-goal tasks across mixed levels. Target descriptions incorporate intrinsic attributes, spatial-relational context, and open-world semantics. graphs [43] or top-down maps [44, 45]. Recent advances in VLMs [4650] further enable zero-shot, open-vocabulary navigation [6, 26, 5156], leveraging strong multimodal alignment and broad open-world knowledge. Despite these advances, prior work overlooks languageconditioned goals across multiple semantic levels. We introduce unified multi-granularity open-vocabulary goal navigation task where agents interpret natural-language goals at four semantic levels: scene, room, region, and instance. This design captures key granularities of indoor navigation and reflects the diversity of real-world goal specifications, enabling natural language goal expression and practical, intuitive embodied navigation. Language-conditioned Goal Navigation Benchmarks. Early LGN benchmarks [2, 5] utilize real-world scans [5, 9, 10] but include only 621 common object categories and primarily evaluate generalization to unseen environments. To increase scene diversity, ProcTHOR [58] generates floor plans populated with 3D assets, and OVMM [59] curates human-authored interactive synthetic scenes. However, these synthetic environments often lack realism and suffer from sim-to-real transfer challenges [60]. To expand object diversity, HM3D-OVON [1] presents an openvocabulary benchmark with 178 categories for evaluation. LHPR-VLN [6] incorporates heuristically inferred room types for long-horizon tasks. Nonetheless, these benchmarks emphasize object detection with limited higher-level semantic reasoning. To enable instance-level goals, GOAT [7] and PSL [8] use VLMs to generate instructions from object views. However, VLMs struggle to capture distinctive visualsemantic cues [11] and exhibit limited 3D spatial reasoning [1214], resulting in ambiguous or inaccurate descriptions. To address these limitations, we introduce large-scale benchmark built on real-world 3D scans with comprehensive human-verified semantic annotations. Our benchmark includes both singleand multi-goal navigation tasks across four semantic levels (scene, room, region, and instance) and covers 414 object categories for rigorous and systematic evaluation. 3. Multi-Granularity Open-Vocabulary Goal"
        },
        {
            "title": "Navigation",
            "content": "3.1. HieraNav As shown in Fig. 3, HieraNav requires mobile agent, randomly initialized in 3D environment, to perform either single-goal or multi-goal navigation. Although primarily designed for unseen environments, the task also extends to known scenes. Unlike prior work [1, 7, 8], goals are speci3 Table 1. Dataset statistics of popular goal-oriented navigation evaluation benchmarks. : GOAT-Bench uses VLM-generated descriptions without human verification, resulting in inaccurate and non-discriminative descriptions. LHPR-VLN adds heuristically inferred room types to object categories, and its region-based goal assignment overlooks distinct regions of the same room type, leading to suboptimal evaluation. Description: text that uniquely identifies target instance within scene. Small Obj.: percentage of objects with mean IoU below 3.3%, computed over the agents optimal look-up, forward, and look-down views. LangMap provides comprehensive human-verified annotations, such as natural and discriminative region and instance descriptions essential for HieraNav. We report average word counts for both styles; concise instance descriptions average 5.3 words, balancing practical utility with increased navigational challenge. Evaluation Benchmark"
        },
        {
            "title": "Task Semantic Levels",
            "content": "Object/Instance Annotations Region Annotations Scene Room Region Instance Category Description Words Category Description Words Small Obj. Tasks RoboTHOR [57] ObjectNav-MP3D [5] ObjectNav-HM3D [2] HM3D-OVON [1] LHPR-VLN [6] GOAT-Bench [7] LangMap (Ours) 10 12 926 5.7/21.0 12 21 6 178 - 119 414 1506 7510 29.0 5.3/15. - - - - - - 9000 4.2% 960 - 4.9% 7951 22.2% 18479 fied in natural language across four semantic levels: Scene-level: any object of the target category in the scene (e.g., armchair). Room-level: an object of the target category located in specified room type (e.g., armchair in the bedroom). Region-level: an object of the target category within specific room instance, disambiguated from other rooms of the same type by contextual cues (e.g., armchair in the bedroom with geometric rug). Instance-level: unique object instance identified by discriminative attributes or contextual relations (e.g., armchair beside the bed and balcony, square coffee table, and white bed with teal runner). At each time step t, the agent receives an RGB image It, depth Dt, and odometry Pt = (x, y, θ) relative to the start position. The agent is required to reach the goal specified by natural language instructions. Following standard protocols [1, 7, 8], the action space includes MOVE FORWARD (0.25m), TURN LEFT or TURN RIGHT (30), LOOK UP or LOOK DOWN (30), and STOP. task is successful if the agent executes STOP within 1m of any object matching the goal description within 500 steps. The simulated agent follows the physical specifications of the Stretch robot [61]: height 1.41m, base radius 0.17m, and an RGB-D camera mounted at 1.31m. 3.2. LangMap Statistics LangMap is built on real-world HM3D scans [9, 10] and includes all 36 HM3D-Sem validation scenes. It provides comprehensive human-verified annotations and navigation tasks across four semantic levels, enabling systematic and rigorous evaluation. Table 1 reports detailed statistics, showing that LangMap offers greater annotation and navigation task diversity, higher annotation quality, and larger scale than existing benchmarks. Figure 4. Distribution of region labels in descending frequency. Region Annotations. Unlike prior datasets limited to object annotations, our LangMap provides human-verified region labels for 12 room categories (Bathroom, Bedroom, Dining room, Garage, Hall, Kitchen, Laundry room, Living room, Office, Recreation room, Storage room, and Walk-in closet) and 926 discriminative region descriptions. These annotations underpin roomand region-level navigation tasks and facilitate scene understanding. Fig. 4 shows the distribution of region labels in LangMap, where most regions correspond to common indoor spaces such as halls, bathrooms, and bedrooms, while recreation rooms, storage rooms, and garages are less frequent. Object Diversity. LangMap covers 414 object categories, 3.5 that of the GOAT-Bench val split (119) and 1.6 that of the full GOAT-Bench (261), enabling more semantically diverse evaluation. Compared to [1, 7], it also includes more small-object categories (e.g., calculator, mouse, dumbbell) and retains small instances as targets, resulting in more realistic and challenging setting. Instance counts per category are shown in Fig. 6(a), and the full category list is provided in the supplementary material. 4 Figure 5. Contrastive region annotation. Annotators are provided with region panoramas, corresponding labeled object views, and 3D scene models to compose one concise and one comprehensive description per region, distinguishing it from other same-category regions via visual comparison. subsequent cross-check ensures clarity and quality. tasks across semantic levels during task generation, with most tasks having shortest-path distances between 515m. Together, these human-verified, large-scale, and semantically rich annotations establish LangMap as rigorous testbed for language-conditioned goal navigation. 3.3. Contrastive Region Annotation LangMap provides human-verified annotations to support roomand region-level goal specification. For each object instance, we sample candidate viewpoints and select the one with the highest visible coverage as its representative view. For each region, we collect the 3D coordinates of all contained objects from HM3D-Sem [10], compute pseudocenter as the midpoint of the minimum and maximum coordinates, and capture panoramic observation at this location. Annotators use these region panoramas and object views for room category labeling. Regions spanning multiple room types (e.g., living room connected to kitchen) receive all applicable labels. We then employ contrastive annotation protocol to derive natural and discriminative region descriptions. As shown in Fig. 5, annotators compare all same-category regions within each scene and write concise and detailed descriptions that identify the target region, where the detailed version extends the concise one with additional attributes. VLM-generated descriptions [62] serve as optional references to streamline the process. second round of crosschecking ensures quality. 3.4. Contrastive Instance Annotation Similar to region annotation, we employ contrastive protocol to derive natural and discriminative instance descriptions. To address fine-grained ambiguity, we cluster semanFigure 6. Distribution analysis of LangMap. (a) Instance count per object category (sampled category names shown for readability). (b) Ground-truth geodesic distance distribution for navigation tasks across the four semantic levels. Instance Annotations. LangMap provides concise and detailed discriminative descriptions created through rigorous contrastive human annotation. In contrast, GOATBench [7] relies on VLM-generated descriptions that often contain semantic errors and lack discrimination (see Fig. 2). In LangMap, concise instance descriptions average only 5.3 words, providing minimal yet sufficient cues to identify target instances. This aligns with real-world goal specification and promotes practical and efficient humanrobot interaction. Task Granularity. LangMap includes singleand multigoal tasks spanning four semantic levelsscene, room, region, and instanceenabling evaluation of exploration, memory, and semantic reasoning. As shown in Fig. 6(b), we align ground-truth geodesic path length distributions for 5 Figure 7. Contrastive instance annotation. Using object views, region panoramas, verified region descriptions, and the 3D scene, annotators write concise and detailed descriptions to distinguish each instance from others of the same category. All annotations are cross-checked. tically similar categories using SentenceBERT [63] and refine them into hierarchy where base categories group related fine-grained categories, e.g., yoga mat and gym mat under exercise mat. As shown in Fig. 7, annotators select base category and view all instances from the grouped finegrained categories, along with object snapshots, labeled region panoramas, and verified region descriptions. They identify salient visual cues to compose one concise and one comprehensive description per instance, distinguishing it from others of the same category. The descriptions include intrinsic attributes (e.g., color, material, pattern, shape, size), spatial context (e.g., relative position, room or region context), and open-world semantics (e.g., Eiffel Tower photo). 3.5. Navigation Task Generation LangMap provides singleand multi-goal tasks spanning multiple semantic levels. single-goal task consists of scene, the agents start pose, and language instruction specifying goal at any semantic level, which may correspond to one or more target instances. multi-goal task extends this setup with sequence of instructions across different semantic levels, requiring the agent to reach multiple goals sequentially within single episode. Scene-level (category-level) instruction generation follows [1, 2], but we iterate over all object categories instead of random sampling to prevent duplication. For each category, we randomly sample start pose subject to two constraints: (1) at least one target instance lies on the same floor to avoid stair climbing [1, 2, 7]; and (2) the geodesic distance to the nearest goal is 530 (relaxed to at least 1m if no valid pose exists). For room-level navigation, we restrict targets to instances of the specified category within the given room by intersecting regionobject mappings from HM3D-Sem [10] with our human-verified region labels. For region-level navigation, goals are further constrained by both the room type and the corresponding discriminative region description. For instance-level navigation, discriminative instance descriptions directly serve as instructions. This process yields about 15k non-redundant single-goal tasks across all four semantic levels. For multi-goal task generation, we uniformly sample five tasks across multiple semantic levels on the same floor with non-overlapping targets and concatenate them into sequential episodes, yielding 720 episodes (3.6k individual tasks). 4. Experiments 4.1. Metrics Following prior work [1, 7, 8, 64, 65], we adopt standard evaluation protocols, reporting Success Rate (SR) and Success weighted by Path Length (SPL) as the primary metrics. SR is the fraction of tasks where the agent stops within 1m of the target within 500 steps, and SPL further accounts for path efficiency relative to the optimal trajectory: SR = SPL ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:88) i=1 Si, Si max(Li, ) , (1) (2) where Si = 1 if the i-th task succeeds and 0 otherwise, and Li and denote the executed and optimal path lengths. Per-goal metrics alone struggle to capture multi-goal reliability. An agent can achieve reasonable per-goal SR yet fail to complete full episodes, since any missed sub-goal breaks the sequence, making partial success insufficient for real-world use (e.g., finding cup but failing to reach the coffee machine). We therefore additionally report Sequence Success Rate (SeqSR@n) for multi-goal episodes, which 6 Table 2. Evaluation on LangMap. We report performance at each semantic level and the overall average for single-goal navigation. For regionand instance-level goals, concise descriptions are used to reflect real-world goal expressions. For multi-goal navigation, we present both sequence-level (SeqSR and SeqSR-4) and per-goal (SR and SPL) results. For 3D-Mem, results are reported using 7B and 3B opensource VLMs [49]. Uni-NaVid and MTU3D are evaluated twice, and the results are averaged. Method Object Room Region Instance Overall Multi-Goal SR SPL SR SPL SR SPL SR SPL SR SPL SeqSR SeqSR-4 SR SPL PSL [8] [ECCV2024] 6.0 SenseAct-Mono [7] [CVPR2024] 10.2 3D-Mem-3B [64] [CVPR2025] 20.7 21.3 3D-Mem-7B [64] [CVPR2025] 33.8 Uni-Navid [55] [RSS2025] MTU3D [65] [ICCV2025] 33. 1.4 5.6 3.7 8.4 16.2 16.7 6.6 8.3 20.3 18.8 33.2 31.4 1.9 4.4 4.0 8.1 16.5 15.9 7.3 8.5 13.4 21.7 30.1 32.7 2.1 4.3 2.3 8.8 15.5 16.5 6.4 7.7 10.2 22.7 26.2 23. 1.9 4.0 1.8 9.2 13.8 13.3 6.6 8.7 15.3 21.2 30.3 29.7 1.8 4.6 2.8 8.7 15.3 15.4 0.0 0.1 0.0 0.7 0.8 1.5 0.0 0.6 0.3 7.8 6.0 12.4 8.1 15.5 20.4 36.8 34.1 41. 5.7 8.4 11.3 21.2 12.8 24.5 Table 3. Quantitative comparison of annotation discriminability via one-to-many text-to-view matching (Qwen3-VL-235B). Exclusive Win: instances correctly matched only by the benchmark."
        },
        {
            "title": "Benchmark",
            "content": "Avg. words Accuracy () Exclusive Win () GOAT-Bench [7] LangMap (Ours) 21.1 5.2 55.9% 79.7% 5.3% 29.1% measures the fraction of episodes where at least out of Kj tasks are successfully completed in the correct order: SeqSR@n = (cid:88) Kj (cid:88) j=1 k="
        },
        {
            "title": "1\nM",
            "content": "Sj,k , (3) where is the number of multi-goal episodes, Kj is the number of tasks in episode j, and Sj,k indicates the success of the k-th task. Each episode contains Kj = 5 tasks; SeqSR denotes full completion, while SeqSR-4 measures partial completion with at least four successful tasks. 4.2. Annotation Quality Analysis To quantify discriminability, we perform one-to-many textto-view matching on overlapping instances using Qwen3VL-235B [66]. We restrict the comparison to instance descriptions, as region annotations are unique to LangMap. As shown in Table 3, LangMap achieves 79.7% accuracy with 4 fewer words (vs 55.9% for GOAT-Bench) and maintains non-inferiority in 94.7% of cases, demonstrating the superior precision of our annotations. 4.3. Main Results We evaluate recent advances on LangMap, including MTU3D [65], Uni-NaVid [55], 3D-Mem [64], SenseActNN Monolithic [7], and PSL [8]. Table 2 reports their performance on single-goal navigation across semantic levels and multi-goal navigation with mixed semantic levels. Overall Performance Analysis. MTU3D and Uni-NaVid are trained on million-scale multimodal data, while 3DMem is training-free and relies on frozen VLMs. Among them, PSL shows the weakest results, 3D-Mem achieves moderate performance, and MTU3D and Uni-NaVid lead overall. Most models perform better at coarser semantic levels (object, room, and region) than at the instance level, which involves single target and requires stronger disambiguation. In multi-goal navigation, all methods show improved performance compared to single-goal navigation, benefiting from implicit temporal encoding or explicit memory mechanisms. Nevertheless, the low SeqSR exposes critical reliability gap, indicating that completing all goals within multi-goal episode remains challenging. PSL [8] leverages CLIP [46] for vision-language alignment and trains policy network for navigation. While it performs well on closed-set benchmarks [2, 3], it underperforms on our multi-granularity open-vocabulary benchmark. CLIP exhibits limitations in compositional reasoning [67], such as handling object relations and attributeobject bindings [68, 69], which are essential for fine-grained 3D semantic reasoning. Furthermore, training on limited set of categories restricts PSLs ability to generalize to openvocabulary and compositional goals. 3D-Mem [64] is training-free method that uses frozen VLMs for semantic reasoning and maintains explicit memory of the explored environment. For fair evaluation, we use the open-source Qwen2.5-VL-7B and -3B [49], as closedsource models like GPT-4o are orders of magnitude larger and prohibitively expensive for deployment. As shown in Table 2, 3D-Mem-3B performs comparably to 3D-Mem-7B on objectand room-level navigation, which mainly rely on perception and detection, but underperforms on regionand instance-level navigation, where success depends on finegrained semantic and relational reasoning. While 3D-Mem outperforms PSL, it incurs high latency (about 2.5 min per task) and trails models trained on large-scale multimodal navigation data. However, with explicit memory of detected 7 Table 4. Ablation of description styles. Concise descriptions are used by default. -D denotes using detailed descriptions. Table 5. Ablation of head (top 20%) and long-tail object categories. Table 6. Ablation of object visibility. Small (<3.3% mean IoU). Method Region Region-D Instance Instance-D Method Head Long-tail Method Non-small Small SR SPL SR SPL SR SPL SR SPL SR SPL SR SPL SR SPL SR SPL 9. 7.3 2.1 PSL 2.4 3D-Mem 21.7 8.8 25.4 8.5 22.7 9.2 25.7 9.6 Uni-NaVid 30.1 15.5 31.7 17.4 26.2 13.8 28.8 15.9 MTU3D 32.7 16.5 33.7 16.2 23.8 13.3 28.7 15.2 1.9 2.6 6. 8.5 4.4 2.0 7.3 PSL 1.1 3D-Mem 21.5 9.1 20.2 7.3 Uni-NaVid 31.3 15.9 26.1 13.2 MTU3D 31.5 16.3 23.0 12.2 4. 2.0 7.2 PSL 1.3 3D-Mem 22.1 9.2 17.0 6.6 Uni-NaVid 32.6 16.5 23.0 10.9 MTU3D 32.7 16.8 20.0 10.1 Table 7. Ablation of optimal path lengths. Short (25%), medium (2575%), and long (>75%). Method Short Medium Long Overall SR SPL SR SPL SR SPL SR SPL 3.2 1.8 5.4 13.4 PSL 3D-Mem 36.7 17.1 19.8 8.7 Uni-NaVid 42.2 21.5 29.7 14.7 19.4 10.3 30.3 15.3 29.7 15.4 MTU3D 47.2 22.2 27.9 15.2 15.9 6.6 21.2 1.6 7.3 0.8 3.2 2.3 8.8 9. objects and accumulated spatial context, it achieves competitive performance in multi-goal navigation. Uni-NaVid [55] is VLM-based model trained on 3.6 It premillion navigation samples across multiple tasks. dicts low-level actions directly from language and egocentric RGB streams without using depth information, enabling lightweight and practical deployment. MTU3D [65] performs end-to-end trajectory learning that combines visionlanguage-exploration pre-training over 1 million RGB-D trajectories. Both methods achieve top-tier performance on LangMap, benefiting from large-scale multimodal training that enhances temporal and spatialsemantic reasoning. Uni-NaVid shows marginally higher SR (+0.6%) in single-goal navigation. However, MTU3D performs notably better on multi-goal sequence tasks due to its explicit memory of explored objects for decision making. 4.4. Ablation Study Description Style. In Table 4, we evaluate the impact of description style on navigation performance. Unless otherwise specified, 3D-Mem refers to 3D-Mem-7B. For both regionand instance-level goals, detailed descriptions improve success rate by providing richer contextual cues that help disambiguate semantically similar targets and In contrast, constrengthen visuallanguage grounding. cise descriptions, while containing distinctive cues, are often harder to ground reliably in visual observations. Head vs. Long-Tail Object Categories. We partition object categories into head and long-tail groups based on their frequency in the dataset, following the Pareto principle [70]. The head group (top 20%) covers about 77% of all 8 tasks, while the long-tail group (remaining 80%) accounts for 23%. As shown in Table 5, all methods exhibit substantial performance drops on long-tail categories compared to head categories. Notably, 3D-Mem shows the smallest gap, benefiting from its frozen VLM that preserves broad openworld knowledge. Object Size and Visibility. Table 6 analyzes the impact of object visibility on navigation performance. All models exhibit clear performance degradation when targets are small (mean IoU<3.3%), highlighting the difficulty of detecting and localizing small or low-visibility objects. Path Length. We group tasks by optimal path length using the geodesic distance distribution in Fig. 6(b), with the 25th and 75th percentiles defining short (25%), medium (25 75%), and long (>75%) ranges. As shown in Table 7, performance decreases as target distance increases, reflecting the challenge of long-horizon exploration. Uni-NaVid and MTU3D show the most stable performance, benefiting from large-scale trajectory learning that enhances long-horizon decision-making. 5. Conclusion We introduced HieraNav, multi-granularity openvocabulary navigation task that unifies goals across four semantic levels, capturing the hierarchical and fine-grained distinctions essential for real-world scenarios. We presented LangMap, large-scale benchmark built on realworld 3D scans with comprehensive human-verified annotations and navigation tasks across these levels. Compared with GOAT-Bench, LangMap achieves 23.8% higher discriminative accuracy using 4 fewer words, setting new standard for high-quality and natural semantic annotations. Extensive evaluations show that although memory and large-scale multimodal training improve grounding and planning, robust reasoning over long-tailed, small, context-dependent, and distant goals, as well as multigoal completion, Together, Hierremain challenging. aNav and LangMap establish rigorous and standardized testbed for future research on language-conditioned goal navigation in complex real-world environments."
        },
        {
            "title": "References",
            "content": "[1] N. Yokoyama, R. Ramrakhya, A. Das, D. Batra, and S. Ha, Hm3d-ovon: dataset and benchmark for openvocabulary object goal navigation, in 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 55435550, IEEE, 2024. 1, 2, 3, 4, 6 [2] K. Yadav, J. Krantz, R. Ramrakhya, S. K. Ramakrishnan, J. Yang, et al., Habitat challenge 2023. https: //aihabitat.org/challenge/2023/, 2023. 1, 2, 3, 4, 6, 7 [3] J. Krantz, S. Lee, J. Malik, D. Batra, and D. S. Chaplot, Instance-specific image goal navigation: Training eminstances, arXiv preprint bodied agents to find object arXiv:2211.15876, 2022. 1, 2, 7 [4] J. Krantz, T. Gervet, K. Yadav, A. Wang, C. Paxton, R. Mottaghi, D. Batra, J. Malik, S. Lee, and D. S. Chaplot, Navigating to objects specified by images, in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1091610925, 2023. 1, 2 [5] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang, Matterport3d: Learning from rgb-d data in indoor environments, arXiv preprint arXiv:1709.06158, 2017. 1, 2, 3, 4 [6] X. Song, W. Chen, Y. Liu, W. Chen, G. Li, and L. Lin, Towards long-horizon vision-language navigation: Platform, benchmark and method, in Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1207812088, 2025. 1, 3, [7] M. Khanna, R. Ramrakhya, G. Chhablani, S. Yenamandra, T. Gervet, M. Chang, Z. Kira, D. S. Chaplot, D. Batra, and R. Mottaghi, Goat-bench: benchmark for multimodal lifelong navigation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1637316383, 2024. 1, 2, 3, 4, 5, 6, 7 [8] X. Sun, L. Liu, H. Zhi, R. Qiu, and J. Liang, Prioritized semantic learning for zero-shot instance navigation, in European Conference on Computer Vision, pp. 161178, Springer, 2024. 1, 2, 3, 4, 6, 7 [9] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. M. Turner, E. Undersander, W. Galuba, A. Westbury, A. X. Chang, M. Savva, Y. Zhao, and D. Batra, Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments for embodied AI, in Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. 1, 2, 3, 4 [10] K. Yadav, R. Ramrakhya, S. K. Ramakrishnan, T. Gervet, J. Turner, A. Gokaslan, N. Maestre, A. X. Chang, D. Batra, M. Savva, et al., Habitat-matterport 3d semantics dataset, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 49274936, 2023. 1, 2, 3, 4, 5, 6 [11] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen, Evaluating object hallucination in large vision-language models, arXiv preprint arXiv:2305.10355, 2023. 1, 3 [12] W. Cai, I. Ponomarenko, J. Yuan, X. Li, W. Yang, H. Dong, and B. Zhao, Spatialbot: Precise spatial understanding with vision language models, in 2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 94909498, IEEE, 2025. 1, 3 [13] B. Chen, Z. Xu, S. Kirmani, B. Ichter, D. Sadigh, L. Guibas, and F. Xia, Spatialvlm: Endowing vision-language models with spatial reasoning capabilities, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1445514465, 2024. [14] Q. Guo, S. De Mello, H. Yin, W. Byeon, K. C. Cheung, Y. Yu, P. Luo, and S. Liu, Regiongpt: Towards region understanding vision language model, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1379613806, 2024. 3 [15] B. Miao, M. Feng, Z. Wu, M. Bennamoun, Y. Gao, and A. Mian, Referring human pose and mask estimation in the wild, Advances in Neural Information Processing Systems, vol. 37, pp. 4479144813, 2024. 1 [16] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sunderhauf, I. Reid, S. Gould, and A. Van Den Hengel, Vision-and-language navigation: Interpreting visuallygrounded navigation instructions in real environments, in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36743683, 2018. 2 [17] J. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee, Beyond the nav-graph: Vision-and-language navigation in continuous environments, in European Conference on Computer Vision, pp. 104120, Springer, 2020. [18] A. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge, Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding, arXiv preprint arXiv:2010.07954, 2020. 2 [19] P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, et al., On evaluation of embodied navigation agents, arXiv preprint arXiv:1807.06757, 2018. 2 [20] D. S. Chaplot, D. Gandhi, S. Gupta, A. Gupta, and R. Salakhutdinov, Learning to explore using active neural slam, ICLR, 2020. 2 [21] X. Zhao, H. Agrawal, D. Batra, and A. G. Schwing, The surprising effectiveness of visual odometry techniques for embodied pointgoal navigation, in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1612716136, 2021. [22] R. Partsey, E. Wijmans, N. Yokoyama, O. Dobosevych, D. Batra, and O. Maksymets, Is mapping necessary for realistic pointgoal navigation?, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1723217241, 2022. 2 [23] D. S. Chaplot, D. P. Gandhi, A. Gupta, and R. R. Salakhutdinov, Object goal navigation using goal-oriented semantic exploration, Advances in Neural Information Processing Systems, vol. 33, pp. 42474258, 2020. [24] D. Batra, A. Gokaslan, A. Kembhavi, O. Maksymets, R. Mottaghi, M. Savva, A. Toshev, and E. Wijmans, Objectnav revisited: On evaluation of embodied agents navigating to objects, arXiv preprint arXiv:2006.13171, 2020. 9 [25] J. Zhang, L. Dai, F. Meng, Q. Fan, X. Chen, K. Xu, and H. Wang, 3d-aware object goal navigation via simultaneous exploration and identification, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 66726682, 2023. [26] S. Y. Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song, Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2317123181, 2023. 3 [27] J. Chen, G. Li, S. Kumar, B. Ghanem, and F. Yu, How to not train your dragon: Training-free embodied object goal navigation with semantic frontiers, in Proceedings of Robotics: Science and Systems (RSS), 2023. [28] S. Wani, S. Patel, U. Jain, A. X. Chang, and M. Savva, Multion: Benchmarking semantic map memory using multiobject navigation, in Advances in Neural Information Processing Systems (NeurIPS), 2020. [29] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. FeiFei, and A. Farhadi, Target-driven visual navigation in indoor scenes using deep reinforcement learning, in 2017 IEEE international conference on robotics and automation (ICRA), pp. 33573364, IEEE, 2017. 2 [30] N. Kim, O. Kwon, H. Yoo, Y. Choi, J. Park, and S. Oh, Topological semantic graph memory for image-goal navigation, in Conference on Robot Learning, pp. 393402, PMLR, 2023. [31] X. Sun, P. Chen, J. Fan, J. Chen, T. Li, and M. Tan, Fgprompt: fine-grained goal prompting for image-goal navigation, Advances in Neural Information Processing Systems, vol. 36, pp. 1205412073, 2023. 2 [32] E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva, and D. Batra, Dd-ppo: Learning nearperfect pointgoal navigators from 2.5 billion frames, in International Conference on Learning Representations (ICLR), 2020. 2 [33] A. Mousavian, A. Toshev, M. Fiˇser, J. Koˇsecka, A. Wahid, and J. Davidson, Visual representations for semantic target driven navigation, in International Conference on Robotics and Automation (ICRA), pp. 88468852, 2019. [34] J. Ye, D. Batra, A. Das, and E. Wijmans, Auxiliary tasks and exploration enable objectgoal navigation, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1611716126, IEEE, 2021. [35] Y. Hong, Q. Wu, Y. Qi, C. Rodriguez-Opazo, and S. Gould, Vln-bert: recurrent vision-and-language bert for navigation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1643 1653, 2021. [36] R. Ramrakhya, D. Batra, E. Wijmans, and A. Das, Pirlnav: Pretraining with imitation and rl finetuning for objectnav, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1789617906, 2023. [37] E. Wijmans, I. Essa, and D. Batra, Ver: Scaling on-policy rl leads to the emergence of navigation in embodied rearrangement, in Advances in Neural Information Processing Systems (NeurIPS), vol. 35, pp. 77277740, 2022. [38] K.-H. Zeng, Z. Zhang, K. Ehsani, R. Hendrix, J. Salvador, A. Herrasti, R. B. Girshick, A. Kembhavi, and L. Weihs, Poliformer: Scaling on-policy rl with transformers results in masterful navigators, arXiv preprint arXiv:2406.20083, June 2024. 2 [39] B. Yu, H. Kasaei, and M. Cao, Frontier semantic exploration for visual target navigation, in IEEE International Conference on Robotics and Automation (ICRA), pp. 4099 4105, 2023. [40] N. Yokoyama, S. Ha, D. Batra, J. Wang, and B. Bucher, Vlfm: Vision-language frontier maps for zero-shot semantic navigation, in Proceedings of the IEEE/RSJ International Conference on Robotics and Automation (ICRA), 2024. [41] F. Xie, S. Schwertfeger, and H. Blum, osmag-llm: Zeroshot open-vocabulary object navigation via semantic maps and large language models reasoning, IEEE Robotics and Automation Letters, vol. 11, no. 3, pp. 24262433, 2026. [42] B. Sun, H. Chen, S. Leutenegger, C. Cadena, M. Pollefeys, and H. Blum, Frontiernet: Learning visual cues to explore, IEEE Robotics and Automation Letters, 2025. 2 [43] A. Pal, Y. Qiu, and H. I. Christensen, Learning hierarchical relationships for object-goal navigation, in Proceedings of the Conference on Robot Learning (CoRL), vol. 164, pp. 517528, PMLR, 2021. 3 [44] S. K. Ramakrishnan, D. S. Chaplot, Z. Al-Halah, J. Malik, and K. Grauman, Poni: Potential functions for objectgoal navigation with interaction-free learning, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1889018900, 2022. 3 [45] G. Georgakis, B. Bucher, K. Schmeckpeper, S. Singh, and K. Daniilidis, Learning to map for active semantic goal navigation, in International Conference on Learning Representations (ICLR), 2022. 3 [46] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning transferable visual models from natural language supervision, in Proceedings of the International Conference on Machine Learning (ICML), vol. 139, pp. 87488763, 2021. 3, [47] OpenAI, Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024. [48] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning: Large language and vision assistant, in Advances in Neural Information Processing Systems (NeurIPS), 2023. [49] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, and et al., Qwen2.5-vl technical report, arXiv preprint arXiv:2502.13923, 2025. 7 [50] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, et al., Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, See https://vicuna. lmsys. org (accessed 14 April 2023), vol. 2, no. 3, p. 6, 2023. 3 [51] A. Majumdar, G. Aggarwal, B. Devnani, J. Hoffman, and D. Batra, Zson: Zero-shot object-goal navigation using multimodal goal embeddings, in Advances in Neural Information Processing Systems (NeurIPS), vol. 35, 2022. 3 10 ration and reasoning, in Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1729417303, 2025. 6, [65] Z. Zhu, X. Wang, Y. Li, Z. Zhang, X. Ma, Y. Chen, B. Jia, W. Liang, Q. Yu, Z. Deng, et al., Move to understand 3d scene: Bridging visual grounding and exploration for efficient and versatile embodied navigation, in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 81208132, 2025. 6, 7, 8 [66] S. Bai, Y. Cai, R. Chen, K. Chen, X. Chen, Z. Cheng, L. Deng, W. Ding, C. Gao, C. Ge, W. Ge, Z. Guo, Q. Huang, J. Huang, F. Huang, B. Hui, S. Jiang, Z. Li, M. Li, M. Li, K. Li, Z. Lin, J. Lin, X. Liu, J. Liu, C. Liu, Y. Liu, D. Liu, S. Liu, D. Lu, R. Luo, C. Lv, R. Men, L. Meng, X. Ren, X. Ren, S. Song, Y. Sun, J. Tang, J. Tu, J. Wan, P. Wang, P. Wang, Q. Wang, Y. Wang, T. Xie, Y. Xu, H. Xu, J. Xu, Z. Yang, M. Yang, J. Yang, A. Yang, B. Yu, F. Zhang, H. Zhang, X. Zhang, B. Zheng, H. Zhong, J. Zhou, F. Zhou, J. Zhou, Y. Zhu, and K. Zhu, Qwen3-vl technical report, arXiv preprint arXiv:2511.21631, 2025. 7 [67] T. Thrush, R. Jiang, M. Bartolo, A. Singh, A. Williams, D. Kiela, and C. Ross, Winoground: Probing vision and language models for visio-linguistic compositionality, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 52385248, 2022. 7 [68] A. Kamath, J. Hessel, and K.-W. Chang, Text encoders bottleneck compositionality in contrastive vision-language models, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 4933 4944, 2023. 7 [69] M. Yuksekgonul, F. Bianchi, P. Kalluri, D. Jurafsky, and J. Zou, When and why vision-language models behave like bags-of-words, and what to do about it?, arXiv preprint arXiv:2210.01936, 2022. 7 [70] V. Pareto, Cours deconomie politique, vol. 1. Librairie Droz, 1964. 8 [52] B. Chen, F. Xia, B. Ichter, K. Rao, K. Gopalakrishnan, M. S. Ryoo, A. Stone, and D. Kappler, Open-vocabulary queryable scene representations for real world planning, arXiv preprint arXiv:2209.09874, September 2022. [53] K. Zhou, K. Zheng, C. Pryor, Y. Shen, H. Jin, L. Getoor, and X. E. Wang, Esc: Exploration with soft commonsense constraints for zero-shot object navigation, in Proceedings of the International Conference on Machine Learning (ICML), vol. 202 of Proceedings of Machine Learning Research, pp. 4282942842, 2023. [54] Y. Long, W. Cai, H. Wang, G. Zhan, and H. Dong, Instructnav: Zero-shot system for generic instruction navigation in unexplored environments, in Proceedings of the Conference on Robot Learning (CoRL), vol. 270, pp. 20492060, 2025. [55] J. Zhang, K. Wang, S. Wang, M. Li, H. Liu, S. Wei, Z. Wang, Z. Zhang, and H. Wang, Uni-navid: video-based visionlanguage-action model for unifying embodied navigation tasks, Robotics: Science and Systems, 2025. 7, 8 [56] H. Yin, X. Xu, L. Zhao, Z. Wang, J. Zhou, and J. Lu, Unigoal: Towards universal zero-shot goal-oriented navigation, in Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1905719066, 2025. 3 [57] M. Deitke, W. Han, A. Herrasti, A. Kembhavi, E. Kolve, R. Mottaghi, J. Salvador, D. Schwenk, E. VanderBilt, M. Wallingford, et al., Robothor: An open simulation-toreal embodied ai platform, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 31643174, 2020. [58] M. Deitke, E. VanderBilt, A. Herrasti, L. Weihs, K. Ehsani, J. Salvador, W. Han, E. Kolve, A. Kembhavi, and R. Mottaghi, Procthor: Large-scale embodied ai using procedural generation, Advances in Neural Information Processing Systems, vol. 35, pp. 59825994, 2022. 3 [59] S. Yenamandra, A. Ramachandran, K. Yadav, A. Wang, M. Khanna, T. Gervet, T.-Y. Yang, V. Jain, A. W. Clegg, J. Turner, et al., Homerobot: Open-vocabulary mobile manipulation, arXiv preprint arXiv:2306.11565, 2023. 3 [60] M. Khanna, Y. Mao, H. Jiang, S. Haresh, B. Shacklett, D. Batra, A. Clegg, E. Undersander, A. X. Chang, and M. Savva, Habitat synthetic scenes dataset (hssd-200): An analysis of 3d scene scale and realism tradeoffs for objectgoal navigation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16384 16393, June 2024. 3 [61] C. C. Kemp, A. Edsinger, H. M. Clever, and B. Matulevich, The design of stretch: compact, lightweight mobile manipulator for indoor human environments, in 2022 International Conference on Robotics and Automation (ICRA), pp. 31503157, IEEE, 2022. 4 [62] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, and et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774, 2023. 5 [63] N. Reimers and I. Gurevych, Sentence-bert: Sentence embeddings using siamese bert-networks, arXiv preprint arXiv:1908.10084, 2019. 6 [64] Y. Yang, H. Yang, J. Zhou, P. Chen, H. Zhang, Y. Du, and C. Gan, 3d-mem: 3d scene memory for embodied explo-"
        }
    ],
    "affiliations": [
        "AIML, Adelaide University",
        "Breaker Industries",
        "East China Normal University",
        "NERC-RVC, Hunan University",
        "Singapore University of Technology and Design",
        "University Western Australia"
    ]
}