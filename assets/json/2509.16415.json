{
    "paper_title": "StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes",
    "authors": [
        "Zhengri Wu",
        "Yiran Wang",
        "Yu Wen",
        "Zeyu Zhang",
        "Biao Wu",
        "Hao Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Underwater stereo depth estimation provides accurate 3D geometry for robotics tasks such as navigation, inspection, and mapping, offering metric depth from low-cost passive cameras while avoiding the scale ambiguity of monocular methods. However, existing approaches face two critical challenges: (i) parameter-efficiently adapting large vision foundation encoders to the underwater domain without extensive labeled data, and (ii) tightly fusing globally coherent but scale-ambiguous monocular priors with locally metric yet photometrically fragile stereo correspondences. To address these challenges, we propose StereoAdapter, a parameter-efficient self-supervised framework that integrates a LoRA-adapted monocular foundation encoder with a recurrent stereo refinement module. We further introduce dynamic LoRA adaptation for efficient rank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to enhance robustness under diverse underwater conditions. Comprehensive evaluations on both simulated and real-world benchmarks show improvements of 6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods, while real-world deployment with the BlueROV2 robot further demonstrates the consistent robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter. Website: https://aigeeksgroup.github.io/StereoAdapter."
        },
        {
            "title": "Start",
            "content": "StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes Zhengri Wu1 Yiran Wang2 Yu Wen1 Zeyu Zhang3 Biao Wu1 Hao Tang3 1AI Geeks 3Peking University Equal contribution. Project lead. Corresponding author: bjdxtanghao@gmail.com. 2Australian Centre for Robotics 5 2 0 S 9 1 ] . [ 1 5 1 4 6 1 . 9 0 5 2 : r Abstract Underwater stereo depth estimation provides accurate 3D geometry for robotics tasks such as navigation, inspection, and mapping, offering metric depth from lowcost passive cameras while avoiding the scale ambiguity of monocular methods. However, existing approaches face two critical challenges: (i) parameter-efficiently adapting large vision foundation encoders to the underwater domain without extensive labeled data, and (ii) tightly fusing globally coherent but scale-ambiguous monocular priors with locally metric yet photometrically fragile stereo correspondences. To address these challenges, we propose StereoAdapter, parameterefficient self-supervised framework that integrates LoRAadapted monocular foundation encoder with recurrent stereo refinement module. We further introduce dynamic LoRA adaptation for efficient rank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to enhance robustness under diverse underwater conditions. Comprehensive evaluations on both simulated and real-world benchmarks show improvements of 6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods, while real-world deployment with the BlueROV2 robot further demonstrates the consistent robustness of our approach. Code: https://github. com/AIGeeksGroup/StereoAdapter. Website: https:// aigeeksgroup.github.io/StereoAdapter. I. INTRODUCTION Stereo depth estimation is pervasive in robotics for understanding [1], [2], [3], navigation [4], manipulation [5], and inspection [6], offering metric 3D from low-cost passive binocular cameras and avoiding the scale ambiguity that plagues monocular depth [7], [8]. Underwater depth estimation is equally critical for AUV/ROV mapping, infrastructure inspection (e.g., pipelines and hulls), ecology monitoring, and archaeology, where reliable geometry directly impacts autonomy and safety [9], [10]. Recent efforts connect monocular priors with stereo geometry from different angles: TiO-Depth unifies monocular and binocular self-supervised depth estimation within single two-in-one framework [11], whereas Stereo Anywhere couples stereo geometry with robust priors from monocular vision foundation models (VFMs) to attain strong zero-shot generalization even in textureless, specular, or transparent scenes [12]. However, underwater imaging violates photometric assumptions commonly exploited by terrestrial stereo due to wavelength-dependent attenuation, forward and backscattering, and refraction at waterglass interfaces, which induces severe domain shift [13] and leaves two key challenges: (i) parameter-efficiently adapting large foundation encoders to the underwater domain without extensive labels, and (ii) tightly fusing globally coherent yet scale-ambiguous monocular priors with locally metric but Fig. 1. Comparison between our methods and other baselines: (a) TiO-Depth (b) Stereo Anywhere (c) StereoAdapter (Ours). photometrically fragile stereo correspondences within selfsupervised pipeline. Our motivation is to reconcile the complementary strengths of monocular vision foundation models and stereo geometry while keeping adaptation economical in both labels and parameters. Concretely, we (i) adapt rather than replace strong foundation encoder to underwater appearance using lightweight low-rank modules instead of full fine-tuning, and (ii) treat the resulting monocular prior as initialization and guidance for an iterative stereo matcher so that local metric constraints can correct global-scale or texture-induced errors under purely self-supervised objective. This perspective yields pipeline that is robust to severe underwater domain shift and practical for deployment. To address the above challenges, we propose StereoAdapter, parameter-efficient self-supervised framework that merges LoRA-adapted monocular foundation encoder with recurrent stereo refinement module. Architecturally, StereoAdapter exposes multi-scale features from DepthAnything-style encoder (adapted with LoRA) to produce coarse, globally consistent disparity prior, and fuses it with multi-scale correlation pyramid inside GRU-style updater to recover locally metric depth at full resolution, as shown in Figure 1 (c). Training-wise, we employ dynamic LoRA to automatically select effective ranks per layer and consolidate the surviving components into the base weights, and we optimize with monocular-prior guidance, photometric reconstruction, occlusion-aware masking, and edge-aware smoothness enabling continual adaptation without dense underwater ground truth. On the data side, we introduce UWStereoDepth-40K for pre-training, synthesized with highfidelity underwater stereo in UE5, ensuring robust cross-view consistency while varying attenuation, scattering, particles, and baselines to emulate diverse ROV setups. Furthermore, we conducted comprehensive experiments on both simulated and real-world underwater stereo depth benchmarks, including TartanAir [14] and SQUID [15], achieving improvements of 6.11% and 5.12% in RMSE compared to the state-of-theart methods. We also performed real-world evaluations with the BlueROV2 robot, which demonstrated consistently robust performance. The main contribution of our work can be summarized as follows: We propose StereoAdapter, parameter-efficient selfsupervised framework that integrates LoRA-adapted monocular foundation encoder with recurrent stereo refinement module. We design training strategy that leverages dynamic LoRA adaptation for efficient rank selection and pretraining on the synthetic UW-StereoDepth-40K dataset to enhance underwater robustness. We conduct comprehensive experiments on both simulated and real-world benchmarks including TartanAir [14] and SQUID [15], as well as real-world evaluations with the BlueROV2 robot, demonstrating consistent improvements over state-of-the-art methods. II. RELATED WORK a) Deep Stereo Matching and Self-supervised Learning: Deep learning has revolutionized stereo matching through end-to-end architectures. Early methods like GC-Net [16], PSMNet [17], and GA-Net [18] pioneered cost volume regularization with 3D convolutions. The field shifted toward iterative optimization with RAFT-Stereo [19], which inspired subsequent works including IGEV-Stereo [20] and SelectiveIGEV [21]. Recent approaches explored Transformer architectures [22] and self-supervised pretraining [23]. The pursuit of robust generalization across diverse domains has led to the emergence of foundation models for depth estimation. MiDaS [24] and DPT [25] demonstrated that training on mixed datasets enables remarkable zeroshot transfer capabilities. Depth Anything [26] unleashed the potential of large-scale unlabeled data, while Depth Anything V2 [27] further enhanced fine-grained details and robustness through synthetic-to-real knowledge transfer. These monocular foundation models have recently been integrated into stereo matching frameworks: Stereo Anywhere [12] achieves robust zero-shot stereo matching by combining monocular depth priors with stereo geometry, particularly excelling in scenarios where traditional stereo or monocular methods fail independently. FoundationStereo [28] adapts foundation model architectures specifically for stereo matching tasks, while DEFOM-Stereo [29] introduces dynamic fusion mechanisms to optimally combine monocular and stereo cues based on scene characteristics. The scarcity of ground-truth depth annotations has also motivated self-supervised learning approaches. For monocular estimation, Zhou et al. [30] introduced learning from video sequences through ego-motion estimation, while Monodepth [31] and Monodepth2 [32] leveraged left-right consistency in stereo pairs. Advanced techniques include depth hints [33], reversing distillation cycles between monocular and stereo networks [34], and revealing reciprocal relations between self-supervised stereo and monocular learning [35]. TiO-Depth [11] unified monocular and binocular tasks in single framework, demonstrating their complementary nature. These self-supervised methods provide valuable alternatives when labeled data is scarce, particularly important for challenging domains like underwater environments. b) Domain Adaptation for Stereo Matching: Despite advances in stereo matching, cross-domain generalization remains challenging [36], [37]. While methods like masked representation learning [38] show promise for terrestrial scenes, they struggle with extreme domain shifts. Existing datasets primarily focus on terrestrial environments: synthetic datasets (Scene Flow [39], HyperSim [40]) and real benchmarks (KITTI [41], Middlebury [42], Booster [43]) provide comprehensive coverage but cannot capture underwater challenges. Parameter-efficient fine-tuning offers promising solution for domain adaptation with limited data. LoRA [44] and its variants [45], [46], [47], [48] demonstrated that large models can be adapted with minimal trainable parameters, particularly effective when target domain data is scarce. Recent vision applications [49], [50], [51] show that adaptive parameter distribution improves adaptation efficiency in challenging domains. c) Underwater Depth Estimation: Underwater imaging violates fundamental assumptions of standard vision algorithms due to wavelength-dependent attenuation, backscattering, and refraction [13]. Early underwater datasets [15], [52] lacked stereo depth annotations. UW-Stereo [53] provided 29,568 synthetic stereo pairs, but the sim-to-real gap persists. Recent underwater depth estimation methods address these challenges through various strategies. UWStereo [54] proposed comprehensive adaptation with style, semantic, and disparity modules. UWNet and Fast-UWNet [55] developed attention mechanisms for real-time underwater processing. However, these methods require extensive underwater data or complex adaptation pipelines. The combination of severe domain shift and data scarcity in underwater scenarios motivates our approach: leveraging foundation models robustness, self-supervised learnings adaptability, and parameter-efficient fine-tuning through LoRA [56]. This enables effective underwater depth estimation by adapting pre-trained models with minimal underwater-specific data, bridging the gap between terrestrial knowledge and underwater applications. III. THE PROPOSED METHOD A. Overview We proposed StereoAdapter, which adopts selfsupervised learning pipeline for underwater stereo depth prediction which leverages monocular depth estimation to guide stereo depth prediction as shown in Fig 2 (a). The framework consists of two main training stages. In the first stage, we employ depth foundation model, Depth Anything V2 [57], for monocular underwater depth prediction. We utilize LoRA [44] to adapt the encoder to underwater scenes, and then decode the feature pyramid output from the encoder to obtain scene disparity. In the second stage, the coarsegrained disparity prediction from the first stage is used for disparity initialization and fused with the cost volume pyramid in the recurrent module. Through progressive iterations, we generate accurate fine-grained disparities and convert them to depth. Furthermore, inspired by [56], we design the dynamic LoRA for continuous learning among different domains, enabling the encoder from VFM to better collaborate with stereo disparity prediction, as shown in Fig 2 (b). B. Architecture a) Monocular Depth Estimation: In the monocular depth estimation pathway, we leverage pretrained Depth Anything V2 [57] model. We directly utilize the multi-scale feature pyramids {Fi}4 i=1 at resolutions {H/4, H/8, H/16, H/32} from the DPTs reassemble modthese pretrained representations to underules. To adapt water scenes, we incorporate LoRA [56] modules in the transformer encoder, enabling efficient domain adaptation while preserving the learned geometric priors. This approach allows us to benefit from Depth Anything V2s strong depth representations while adapting to underwater-specific characteristics with minimal additional parameters. The adapted features are then processed through SDFA blocks [58] for progressive aggregation. Each SDFA block combines features from adjacent scales, maintaining spatial coherence while incorporating multi-scale context. The decoder produces discrete disparity volume RN HW , where represents the number of predefined disparity levels. During training, we generate the primary volume Vm through our decoder branches. This volume is directly used for photometric reconstruction following [11]. Specifically, we employ the discrete depth constraint to generate reconstructed image ˆIl from the primary volume of the right view and the original right image Ir. The monocular training objective consists of three components: photometric reconstruction loss Lmono rec measuring the difference between ˆIl and Il, multi-scale edge-aware smoothness loss Lmono Lmono = Lmono rec + λ1Lmono smooth, smooth: (1) where λ1 is predefined weighting parameters. During this stage, only the LoRA parameters and decoder components are optimized while keeping the pretrained encoder weights frozen. b) Correlation Pyramids Building: We construct multiscale correlation pyramids from stereo features while leveraging our mono stages scaled predictions and sparse stereo matching for robust metric depth estimation. Our mono stage directly produces metric-scaled depth, which we further refine using sparse correspondences for improved local accuracy. Stereo Correlation Pyramid. Given feature maps fL, fR RCH/4W/4 extracted from the stereo pair, we compute the 4D correlation volume through the inner product: C(i, j, d) = fL(i, j), fR(i, d), [0, Dmax]. (2) We then construct multi-scale pyramid {C(l)}3 l=0 via average pooling, where each level provides correlation features at different granularities for coarse-to-fine refinement. , Mmono Hybrid Scale Alignment and Refinement. Our mono stage produces metric-scaled depths Mmono that already possess global scale consistency. To ensure metric accuracy and improve local precision, we employ sparse stereo correspondences as verification and refinement anchors. Through feature matching with bidirectional consistency check, we obtain sparse metric depth measurements: dp Pmatched. Dsparse(p) = (3) , We first verify the mono stages scale by comparing with sparse depths: α = 1 Pmatched (cid:88) pPmatched Dsparse(p) Mmono (p) . (4) If α 1 < τ (typically τ = 0.1), the mono stage scale is considered reliable. Otherwise, we compute the corrective scale and shift: min ˆs,ˆt (cid:88) wp (cid:13) (cid:13)(ˆs Mmono (p) + ˆt) Dsparse(p)(cid:13) 2 (cid:13) . (5) pPmatched The initially aligned depths are: (cid:40) ˆM(0) = Mmono , ˆs Mmono if α 1 < τ otherwise (6) + ˆt, Subsequently, we refine these depths through confidenceweighted propagation of local corrections: ˆML(p) = ˆM(0) (p) + (cid:88) wpq (Dsparse(q) ˆM(0) (q)), qPmatched (7) where bilateral weights wpq preserve edges while propagating corrections: (cid:18) (cid:19) (cid:19) (cid:18) wpq = exp exp I(p) I(q)2 2σ2 . (8) q2 2σ2 Fig. 2. Detailed architecture of the StereoAdapter: (a) two-stage self-supervised training pipeline; (b) update mechanism with LoRA. This hybrid approach leverages the global consistency of our mono stage while ensuring local metric accuracy through sparse anchors. c) Stereo Depth Estimation: Our stereo depth estimation module refines the initial metric-scaled predictions from the mono stage through iterative optimization using stereo correspondences. We adopt recurrent refinement framework that leverages both enhanced context features and stereo matching cues. Combined Context Encoder. Inspired by DEFOM-Stereo [29], we directly leverage the VFM encoder from our monocular depth estimation stage.It has already been finetuned with fused weights for underwater scene understanding. This pre-adapted encoder provides robust feature representations that capture both general visual semantics and domain-specific characteristics. For each resolution level {4, 8, 16}, we extract hierarchical features from the DPTs Reassemble modules: h(l) ViT = ReassembleLoRA (IL), (9) where ReassembleLoRA fused LoRA weights from the mono stage. denotes the Reassemble module with In parallel, we employ lightweight CNN-based context encoder to extract complementary local features following : h(l) CNN = CNNl(IL). To effectively combine these features, we apply learnable convolutional blocks for channel alignment and perform element-wise addition: (10) h(l) = Convalign(h(l) ViT) + h(l) CNN. (11) This design efficiently reuses the domain-adapted representations from our mono stage while augmenting them with CNN features for enhanced spatial precision. The resulting multi-scale context features {h(l)}l{4,8,16} initialize the GRUs hidden state and are integrated at each iteration to provide consistent geometric and semantic guidance throughout the refinement process. Iterative Disparity Refinement. We aim to estimate series of refined disparity maps {d(1) = b/ ˆML, d(2), . . . , d(l), . . .} exploiting the guidance from our combined context encoder and stereo correlation pyramid. Starting from the GRU update operator, we employ lookup operator that extracts correlation features c(l) from the multiscale correlation pyramid {C(l)} The correlation features c(l) are processed by two-layer encoder and concatenated with features derived from the current disparity estimation d(l) and the multi-scale context features g(l) from our combined encoder. This concatenation is further processed by 2D convolutional layer, and then fed to the ConvGRU operator: l=0. h(l+1), d(l) = ConvGRU(h(l), [c(l), g(l), ϕ(d(l))]), (12) where ϕ() denotes feature extraction from the current disparity map. We adopt the same upsampling module from [19] to upsample the final disparity to full resolution and then project it back to depth, producing metric-accurate depth estimates suitable for underwater robotics applications. Joint training strategy. We employ comprehensive loss function that leverages monocular predictions as strong priors while enforcing stereo consistency. Let d(L) denote the final disparity after all iterations. importance weights Rr that modulate the contribution of each rank component: The stereo reconstruction loss enforces photometric consistency between the left image and the warped right image: Lrec = αIL L1 + (1 α)SSIM(IL, L), (13) where IL is reconstructed by warping IR using the predicted disparity d(L). To handle occlusions, we leverage the monocular predictions: = Mocc IL + (1 Mocc) Imono , (14) where Mocc initial monocular disparity d(1) = b/ ˆML, and Imono reconstructed using monocular depth. is the occlusion mask computed from the is The disparity guidance loss leverages monocular predictions to regularize stereo refinement: Lguide =xd(1) xd(L)1 + yd(1) yd(L)1 + Mout d(1) d(L)1, (15) where d(1) and d(L) represent the initial monocular and final stereo disparity maps respectively, and denote the horizontal and vertical gradient operators, and Mout masks pixels with invalid reprojections. Additionally, we apply an edge-aware smoothness loss smooth for final prediction. The complete stereo training Lstereo objective is: Lstereo = Lstereo rec + λ3Lstereo smooth + λ4Lguide, (16) where {λ3, λ4} are weighting parameters. During this stage, new set of LoRA weights will be learned for adapting the VFM encoder from monocular to stereo setup. C. Dynamic LoRA Adaptive Rank Selection Mechanism. To address the substantial domain gap between terrestrial pre-training environments and underwater deployment scenarios, we incorporate Low-Rank Adaptation (LoRA) [44] for efficient encoder fine-tuning. For weight matrix W0 Rdk, Traditional LoRA introduces low-rank decomposition matrices Rdr and Rrk to approximate weight updates: = W0x + Wx = W0x + BAx, (17) where (min(d, k)) represents the adaptation rank. However, fixed-rank approaches fail to account for the varying adaptation requirements across different network components and layers. This limitation often leads to either insufficient adaptation capacity with overly conservative ranks or excessive parameter overhead with unnecessarily high ranks. Inspired by [56], we design dynamic LoRA that enables adaptive rank optimization based on the importance of individual singular components to address this limitation. The dynamic LoRA reformulates the low-rank update through learnable importance weighting mechanism. Instead of fixed-rank decomposition, it introduces learnable Wt,m = (cid:88) i=1 Bt,m wt,m At,m , (18) , At,m where wt,m represents the learned importance of the i-th rank component, and Bt,m correspond to the i-th singular vectors of the low-rank decomposition. This formulation draws inspiration from Singular Value Decomposition (SVD) [47], [56], [59], [60], [61], [62], where importance weights correspond to learnable approximations of the diagonal elements in the singular value matrix, enabling the model to dynamically emphasize the most relevant subspace directions for the current adaptation task. Sparsity-Regularized Optimization. The importance weights are initialized randomly and optimized jointly with the low-rank matrices through gradient descent. To ensure only the most impactful rank components are retained while eliminating redundant parameters, our approach incorporates ℓ1 sparsity regularization: Lt train = Lt sup + λ (cid:88) m=1 wt,m1, (19) where Lsup denotes the supervised learning objective, and λ controls the regularization strength. This sparsity constraint promotes automatic rank selection by driving less important weights toward zero, performing continuous rank pruning during optimization. The non-differentiable nature of ℓ1 regularization necessitates specialized optimization techniques. Rather than directly applying this ℓ1 regularization during optimization, the dynamic LoRA employs the proximal gradient method with soft-thresholding operations to handle the sparsity constraint. The weight update rule for importance parameters follows: (20) ) κ), (cid:12) ˆwt,m := 1((cid:12) (cid:12) (cid:12) > κ) ( ˆwt,m + sign( ˆwt,m wt,m where ˆwt,m denotes the importance weight after gradient update from the supervised loss, κ is the threshold parameter, and 1() is the indicator function. The threshold parameter κ is gradually increased during training from zero to maximum value κmax, allowing initial exploration of all rank components before applying progressively stronger sparsity constraints. Additionally, in accordance with the X-TAIL experimental protocol, we employ two-stage training procedure. The dense training stage constitutes 50% of the total iterations during which the soft-thresholding operation is not applied, followed by the sparse training stage where the soft-thresholding mechanism is activated to perform rank pruning. The dense training stage is essential as it allows all rank components to capture task-relevant information before sparsity constraints are imposed, preventing premature elimination of potentially important adaptation directions. Continual Weight Integration. Upon completion of each adaptation phase, ranked components with non-zero importance weights are consolidated into the base network parameters. This integration process eliminates the computational overhead associated with auxiliary adaptation modules during inference while preserving the acquired domain-specific knowledge. The weight merging operation follows: ˆW = W0 + = W0 + (cid:88) i=1 wiBiAi, (21) where only rank components with significant importance weights contribute to the final parameter update. D. Data Synthesis Inspired by UWstereo [53], we construct large-scale synthetic underwater stereo dataset, termed UW-StereoDepth40K, using Unreal Engine 5 (UE5). UE5s advanced rendering capabilities enable physically accurate simulation of underwater visual effects while maintaining precise geometric consistency required for stereo matching applications. The overall data generation pipeline is illustrated in Figure 3. a) Rendering Pipeline: We leverage UE5s powerillumination systems to creful ray tracing and global ate photorealistic underwater environments. Our pipeline encompasses four distinct underwater scenescoral reefs, industrial structures, shipwrecks, and natural seabed environmentspopulated with diverse 3D assets. These virtual resources include high-resolution scanned corals, detailed marine flora, underwater robots, submerged vehicles, and various marine structures collected from professional 3D asset libraries and photogrammetry scans. The key advantage of using UE5 over generative approaches lies in its ability to maintain robust stereo consistency. Unlike diffusion-based methods that introduce stochastic variations between views, UE5 renders both left and right images from precisely calibrated virtual stereo cameras, ensuring pixel-aligned geometric relationships and consistent appearance across the stereo pair. This eliminates the correspondence ambiguity issues inherent in generative approaches while preserving the photorealism necessary for effective domain transfer. b) Environmental Variation: To enhance dataset diversity and improve model generalization, we systematically vary multiple environmental parameters. Camera baselines are sampled from {4cm, 10cm, 20cm, 40cm} to accommodate the diverse baseline configurations found across different underwater ROV platforms. This range covers typical stereo configurations from compact inspection ROVs with narrow baselines to larger survey vehicles with wider stereo separation, ensuring our trained models can generalize across various underwater robotic systems. Additionally, we introduce realistic underwater effects including caustic patterns generated through water surface simulation, floating particles with physics-based motion, and depth-dependent color attenuation. These environmental variations ensure our synthetic data captures the full spectrum of challenges present in real underwater scenarios. c) Dataset Generation and Quality Control: The UWStereoDepth-40K dataset is generated by processing continuous camera trajectories through the virtual environments, Fig. 3. Data synthesis pipeline. Unreal Engine 5 rendering pipeline for UW-StereoDepth-40K dataset. extracting stereo pairs at regular intervals. We implement automatic filtering to remove frames with insufficient texture information and frames with extreme depth distributions that exceed typical underwater robotic operation ranges (>50m). Each stereo pair is rendered at 1280 960 resolution with accompanying dense depth ground truth and semantic segmentation masks. Quality assurance involves both automated metrics and manual inspection. We compute structural similarity indices between stereo pairs to ensure consistency, and employ domain experts to evaluate photorealism and identify potential artifacts. The final dataset comprises 40,000 high-quality stereo image pairs covering diverse underwater scenarios, providing comprehensive training data for robust underwater stereo matching models. IV. EXPERIMENTS A. Datasets and Evaluation Metrics a) Datasets: For training, we utilize our synthetic dataset UW-StereoDepth-40K, generated using Unreal Engine 5, which consists of 40,000 stereo image pairs from various underwater scenes. For evaluation, we conduct experiments on two real-world underwater stereo datasets. The first is subset of TartanAir [14] containing 13,583 underwater stereo image pairs from 22 different sequences. The second is the SQUID dataset [15], which comprises 57 stereo pairs captured from four distinct scenes. b) Evaluation metrics: We adopt standard depth estimation metrics to comprehensively evaluate our method. Following established protocols, we report Relative Error (REL), Squared Relative Error (SQ REL), Root Mean Square Error (RMSE), and Log Root Mean Square Error (LOG RMSE) to assess accuracy. Additionally, we compute threshold accuracy metrics δ < 1.25i (denoted as A1, A2, A3 for = 1, 2, 3 respectively) to measure the percentage of pixels with depth predictions within specified error thresholds. B. Implementation Details a) Visual augmentation: We employ the pre-trained MobileIE [70] as our visual enhancement module to process underwater images and effectively remove water-induced effects. Underwater image degradation is fundamentally physical problemdue to the selective absorption and scattering of light by water medium, raw images typically exhibit severe color shifts, low contrast, and blurred details. These low-level distortions directly impair feature extraction and recognition performance in subsequent high-level vision tasks. Therefore, applying targeted image restoration TABLE EVALUATION ON THE TARTANAIR UNDERWATER SUBSET. Method Training Set REL SQ REL RMSE LOG RMSE A1 A2 A3 LEAStereo [63] PSMNet [17] AANet [64] GwcNet [65] ACVNet [66] RAFT-Stereo [19] HSMNet [67] TiO-Depth [11] FoundationStereo [28] Stereo Anywhere [12] CREStereo [68] Scene Flow Scene Flow Scene Flow Scene Flow Scene Flow Scene Flow Scene Flow KITTI2012 FoundationStereo dataset Scene Flow ETH3D StereoAdapter (Ours) UW-StereoDepth-40K IGEV-Stereo [20] Selective IGEV [21] GMStereo [69] 5 Datasets + TartanAir 5 Datasets + TartanAir 5 Datasets + TartanAir StereoAdapter (Ours) StereoAdapter (Ours) TartanAir UW-StereoDepth-40K + TartanAir Zero-Shot 0.1099 0.0884 0.6096 0.1013 0.0970 0.0814 0.9856 0.7194 0.0542 0.0592 2.5746 0.0527 1.3898 0.8699 8.3687 1.2965 1.1335 0.7342 12.3768 8.6479 0.6701 0.5098 9.8789 4.5610 3.9721 13.0542 4.1829 3.9985 4.0423 15.2865 13.4635 2.9644 3.1572 8.4526 0.5167 2. Fine-Tuning 0.1009 0.1225 0.1561 0.0519 0.0512 1.6475 1.5155 2.2275 0.5041 0.4987 4.7107 4.8742 5. 2.8341 2.7834 0.2063 0.1804 0.9903 0.1855 0.1803 0.1703 4.5961 1.6967 0.1358 0.1544 5.1297 0.1371 0.1909 0.2123 0.2432 0.1330 0.1312 0.8929 0.9122 0.2598 0.9085 0.9063 0.9030 0.0000 0.0053 0.9302 0.9442 0. 0.9512 0.9627 0.3451 0.9612 0.9612 0.9612 0.0000 0.0096 0.9701 0.9787 0.5732 0.9761 0.9804 0.3888 0.9801 0.9813 0.9832 0.0000 0.0550 0.9779 0.9889 0.7001 0.9467 0.9801 0.9873 0.8913 0.8545 0. 0.9489 0.9512 0.9515 0.9375 0.9252 0.9823 0.9836 0.9764 0.9713 0.9651 0.9897 0.9904 the input stage is essential, as it can recover visual at quality without altering semantic content, providing more reliable inputs for downstream tasks. The MobileIE features lightweight architecture with fast inference speed and delivers stable, consistent enhancement results during both training and testing phases. More importantly, its low computational overhead and memory footprint make it particularly well-suited for deployment on resource-constrained mobile robotic platforms, meeting the real-time visual processing requirements of underwater ROVs. b) Model Details: The encoder of our model is initialized with pre-trained DepthAnything v2-B [57] weights. We employ two-stage training strategy: 20 epochs, and followed by 40 epochs for stereo depth estimation. constant learning rate of 1 104 is used with the AdamW optimizer and batch size of 8. Our model is trained on the proposed UW-StereoDepth-40K dataset, with evaluation performed on the TartanAir [14] underwater subset and SQUID [15] datasets. All experiments are conducted on an Intel Xeon Platinum 8469C CPU at 2.60GHz, with single NVIDIA L40 48GB GPU and 64GB of RAM. C. Main Results The 5 Datasets refers to combination of five commonly used stereo matching datasets for training, which are Scene Flow [39], Sintel [71], ETH3D [72], InStereo2K [73], CREStereo [68]. Our experiments demonstrate that the proposed StereoAdapter, trained on the UW-StereoDepth-40K dataset, achieves consistent improvements over existing stereo matching methods across both the TartanAir Underwater part and SQUID benchmarks. As summarized in Tables and II, our approach delivers state-of-the-art zero-shot performance and achieves further gains when fine-tuned with TartanAir, and UWStereoDepth-40K. As shown in Table I, StereoAdapter significantly outperforms existing methods on the TartanAir Underwater subset. In the zero-shot setting, our model achieves the lowest REL (0.0527) and RMSE (2.8947), along with the highest accuracy at A1 (94.67%). When fine-tuned with TartanAir, StereoAdapter further improves across all metrics, reducing RMSE to 2.7834 and achieving 95.12% for A1, 98.36% for A2, and 99.04% for A3. These results demonstrate both the robustness of our adapter design and the value of UWStereoDepth-40K as pretraining dataset for underwater applications. StereoAdapter attains the best overall performance as shown in Table II, achieving an RMSE of 1.8843, which is further reduced to 1.8621 when combined with TartanAir fine-tuning. The model also delivers the highest accuracy across all δ thresholds, reaching 94.13% (A1), 97.48% (A2), and 98.52% (A3). Compared to other baseline methods, StereoAdapter demonstrates superior generalization capability in challenging underwater conditions. Collectively, these findings highlight the critical role of the UW-StereoDepth40K dataset in enabling robust zero-shot generalization and fine-tuned performance for underwater stereo depth estimation. As shown in Figure 5, StereoAdapter generates substantially more accurate and visually coherent depth maps than baseline methods (for example, better scale estimation for far range areas). D. Real World Evaluation a) Platform: As shown in Figure 4, we employ BlueROV2 as the experimental vehicle. Low-level actuation is handled by an STM32 controller, while onboard perception TABLE II ZERO-SHOT EVALUATION ON SQUID DATASET. Method Training Set REL SQ REL RMSE LOG RMSE A1 A2 A3 LEAStereo [63] PSMNet [17] AANet [64] GwcNet [65] ACVNet [66] RAFT-Stereo [19] HSMNet [67] CREStereo [68] IGEV-Stereo [20] Selective IGEV [21] GMStereo [69] TiO-Depth [11] FoundationStereo [28] Stereo Anywhere [12] Scene Flow Scene Flow Scene Flow Scene Flow Scene Flow Scene Flow Scene Flow ETH3D 5 Datasets + TartanAir 5 Datasets + TartanAir 5 Datasets + TartanAir KITTI2012 FoundationStereo dataset Scene Flow StereoAdapter (Ours) StereoAdapter (Ours) UW-StereoDepth-40K UW-StereoDepth-40K + TartanAir 0.5574 0.5182 7.4801 0.2294 1.6030 0.0831 0.9772 2.5746 0.0932 0.0960 3.3442 1.3154 0.1095 0.0952 0.0806 0.0795 3.9434 7.1404 314.1577 1.2275 65.6518 0.6946 7.2766 9.8789 1.4685 0.9617 140.3211 11.6828 0.7012 1.1017 0.7082 0.6823 5.4659 4.9186 34.7612 3.0003 10.3828 1.9625 8.2301 8.4526 2.4741 1.9268 18.7829 7.0930 2.2510 2.4317 1.8843 1. 0.4335 0.5902 1.8994 0.3799 0.7293 0.1441 4.0887 5.1297 0.1523 0.1665 1.0219 0.8121 0.1584 0.1586 0.1469 0.1398 0.6512 0.7139 0.0602 0.7423 0.7019 0.9235 0.0000 0.4890 0.9346 0.9171 0.5300 0.1753 0.8995 0.9179 0.9413 0.9428 0.8042 0.7999 0.1087 0.8517 0.7925 0.9634 0.0000 0.5732 0.9712 0.9555 0.6076 0.3346 0.9433 0.9605 0.9748 0. 0.8869 0.8311 0.1570 0.9005 0.8321 0.9835 0.0000 0.7001 0.9820 0.9720 0.6578 0.5133 0.9501 0.9763 0.9852 0.9867 TABLE III REAL WORLD EVALUATION ON BLUEROV2. Method REL SQ REL RMSE LOG RMSE A1 Stereo Anywhere [12] FoundationStereo [28] TiO-Depth [11] StereoAdapter (Ours) 0.0905 0.1040 1.5697 0.0856 1.0467 0.7363 13.1487 0. 2.5101 2.1385 6.7584 1.9690 0.1507 0.1505 0.8715 0.1428 0.9120 0.8961 0.1525 0.9478 and inference run on Jetson Orin NX (16GB). The robot is equipped with stereo camera Zed 2i; the left/right sensors are triggered by the cameras built-in sync circuitry. All sensing and logging are performed onboard the Orin NX. b) Environment and Obstacles: Experiments are performed in the same indoor rectangular water tank. To emulate underwater obstacle avoidance, we arrange glass cups and rocks of varying shapes to form three distinct obstacle environments within this single scene: dispersed, side-by-side, and clustered. For each environment, the robot is teleoperated along three different pre-defined motion trajectories, and we record one synchronized stereo sequence per trajectory. All data are logged onboard the Orin NX. c) AprilTag Layout and Reference Geometry: Before the experiments, we pre-constructed metrically scaled 3D mesh of the scene. During the underwater runs, camera poses were estimated from AprilTag (family 16h5) detections via PnP and aligned to the mesh. For each frame, we then rendered reference depth map in the left-camera coordinate frame with standard visibility handling, masking pixels without valid mesh hits. These maps serve as the ground truth for evaluating our predicted depth across all three obstacle layouts. We evaluate only on sequences with dense mesh coverage, ensuring effectively dense and reliable ground-truth depth in our demos. d) Protocol: We evaluate all methods on the same indoor tank described above. Within this single scene we realize three obstacle environments (dispersed, side-by-side, clustered) using glass cups and rocks, and for each environment we teleoperate the robot along three distinct motion Fig. 4. Hardware and Evaluation Pipeline for Real-World Experiments. trajectories, yielding nine synchronized stereo sequences in total. All approaches consume the identical rectified stereo pairs at the same input resolution with the same preprocessing. Where method outputs disparity, we convert to metric depth by using the calibrated focal length and baseline b. Unless noted otherwise, evaluation is performed in the left-camera coordinate frame. e) Masking and evaluation scope: We evaluate against the per-frame reference depth rendered from the metrically scaled scene mesh described above. During evaluation, depth is compared in the left-camera coordinate frame; pixels without valid z-buffer hit in the rendered reference are masked and excluded. Metrics are reported over valid pixels only, and we restrict reporting to sequences with dense reference coverage. f) Metrics: We evaluate using the standard depth metrics shown in Table III: Absolute Relative Error (REL), Squared Relative Error (SQ REL), RMSE, LOG RMSE, and accuracy A1 with δ < 1.25. Metrics are computed per frame over valid pixels only (as defined by the referencedepth mask) and averaged across the nine sequences (three environments three trajectories). Lower for REL/SQ REL/RMSE/LOG RMSE; higher is better for A1. g) Baselines: We compare against representative stereo approaches: Stereo Anywhere [12], FoundationStereo [28], is better Fig. 5. Visualization results of Stereo Depth Estimation Methods on SQUID and TartanAir. TABLE IV ABLATION ON RECURRENT REFINEMENT MODULE. TABLE VI ABLATION ON TRAINING STRATEGY GRU layers Hidden Dimension Number of Iteration REL RMSE Batch Size Learning Rate Stage One Epochs Stage Two Epochs REL/RMSE 4 3 3 3 2 128 256 128 128 128 32 32 64 32 32 0.049 0.048 0.533 0.051 0. 2.614 2.625 2.8654 2.783 3.024 4 4 8 8 16 1 104 2 104 1 104 2 104 1 104 20 30 20 30 20 20 60 40 30 20 0.0711/3.183 0.0667/2.951 0.051/2.783 0.054/2.842 0.052/2. TABLE ABLATION ON DYNAMIC LORA SETUPS. Rank κ Threshold Dense Epoch Ratio REL RMSE 16 16 16 32 32 0.005 0.005 0.01 0.005 0.01 0.5 0.45 0.45 0.5 0.5 0.077 0.074 0.049 0.049 0.054 3.214 3.105 2.783 2.814 2.744 and the stereo branch of TiO-Depth [11]. Our method StereoAdapter is evaluated under identical input settings and without augmentation of the test time. E. Ablation Study We conduct comprehensive ablation experiments to evaluate the impact of key design choices in our framework. Specifically, we investigate: (i) the recurrent refinement module configuration, (ii) the Dynamic LoRA adaptation strategy, and (iii) critical training hyperparameters. Table IV shows the ablation results for the recurrent refinement module. We vary the number of GRU layers, hidden dimensions, and refinement iterations. The results demonstrate that deeper networks significantly improve performance, with the 4-layer architecture achieving the best results. Our final configuration with 3 GRU layers, 128 hidden dimensions, and 32 iterations balances performance with computational efficiency. Table reports the results for different Dynamic LoRA configurations. We examine the adapter rank, κ threshold for rank scheduling, and the dense epoch ratio. Our configuration employs rank of 16 with κ = 0.01 and 45% dense epochs; it has the best performance on RMSE and REL. This LoRA configuration provides both high performance and training efficiency. Table VI examines the influence of training hyperparameters including batch size, learning rate, and training epochs. The experiments reveal that moderate batch sizes (8-16) consistently outperform smaller batches, while learning rate of 1 104 ensures more stable convergence compared to 2 104. Notably, Multistage training epoch allocation (20 epochs for stage one, 40 for stage two) allows the model to first learn monocular features before learning on stereo domain, yielding optimal results with batch size 8 (REL=0.051, RMSE=2.783). Our final configuration employs batch size of 8, learning rate of 1 104, 20 epochs for first stage, and 40 epochs for second stage. Overall, our quantitative evaluation confirms that StereoAdapter outperforms existing state-of-the-art methods across multiple metrics while maintaining good efficiency. V. TEST-TIME EFFICIENCY We evaluate on an on-board Jetson Orin NX 16GB in MaxN mode with TensorRT, batch size 1, and input resolution 640320. All methods use authors official implementations with identical pre/post-processing. We report per-frame Fig. 6. Generalization of Stereo Methods to Real-World Experiments TABLE VII AVERAGE PER-FRAME INFERENCE LATENCY (MS) ON JETSON ORIN NX @ 640360, BS=1. Method On-board (ms) FoundationStereo[28] Stereo Anywhere [12] StereoAdapter (Ours) 1815 1440 1113 end-to-end latency in milliseconds (ms). FoundationStereo is the slowest on Orin NX , consistent with its heavy transformer backbone and comprehensive cost aggregation; Stereo Anywhere improves to 1440 ms by using RAFT-style recurrent module; its budget is dominated by running DepthAnything-L for twice and the two monocular passes, and 3D convolution module for feature fusion. In contrast, StereoAdapter is the fastest at 1113 ms: it leverages LoRA-adapted DepthAnything-B encoder only for feature extraction, yielding 327 ms speedup over Stereo Anywhere and 702 ms speedup over FoundationStereo on the same board. VI. LIMITATION AND FUTURE WORK Our approach focuses on lightweight decoder with LoRA-aided specialization and synthetic supervision. This yields strong efficiency and robustness in our evaluated settings, but several limitations remain. (i) Decoder capacity vs. context: the current RAFT-style recurrent head emphasizes local matching with short-range memory; under severe turbidity, non-Lambertian highlights, and large textureless spans, global reasoning is still limited. (ii) Synthetic supervision (UE): while Unreal Engine simulation substantially reduces labeling cost, its data distribution remains narrower and more factorized than real-world underwater imagery. In particular, the pipeline provides limited domain breadth and scene co-occurrence statistics, and simplifies correlations among appearance, geometry, and control; important phenomena (participating-media multiple scattering, polarization effects, dynamic turbidity/particles, sensor/ISP idiosyncrasies, rolling-shutter and lens aberrations) are only partially captured, leaving measurable sim-to-real gap. Future work will broaden both data and model. On the data side, we plan unified training recipe that couples UE procedural generation with coverage-driven domain randomization and physics-based participating-media rendering, calibrated to measured sensors and scene spectra; we will also explore self-training on unlabeled real sequences with confidence filtering to close residual gaps. On the model side, we will investigate unified multi-task objectives (e.g., stereo, depth, normals) to absorb broader cross-task statistics, alongside longer-context, linear-time decoders (such as Mamba/RWKV) and temporal/multi-view extensions with explicit uncertainty estimation and quantization-aware deployment on embedded platforms. VII. CONCLUSION In this work, we present StereoAdapter, parameterefficient self-supervised framework that integrates LoRAadapted monocular foundation encoder with recurrent stereo refinement module for underwater depth estimation. Our method addresses the challenge of adapting large vision models to severe underwater domain shifts by leveraging dynamic LoRA adaptation and pre-training on the synthetic UW-StereoDepth-40K dataset. Comprehensive evaluations demonstrate StereoAdapter achieves 6.11% improvement on TartanAir and 5.12% on SQUID compared to state-of-the-art methods, while real-world deployment on BlueROV2 confirms robust performance with favorable latency. These advancements provide practical solutions for AUV navigation, infrastructure inspection, and marine ecology monitoring, contributing to safer autonomous underwater operations. VIII. APPENDIX: VISUALIZATION a) Real world evaluation: To assess real-world performance, we present qualitative results on BlueROV2 across multiple key frames and compare StereoAdapter with captured ground truth and competing baselines  (Fig. 7)  . In particular, StereoAdapter consistently produces sharper and more geometrically faithful depth maps than competing baselines, especially in regions with textureless water and reflective surfaces where traditional methods often fail. b) Benchmark evaluation: We further report additional qualitative visualizations on TartanAir and SQUID  (Fig. 8)  . Results on TartanAir and SQUID further highlight its ability to generalize across diverse synthetic-to-real domain shifts, capturing fine-grained scene structure with fewer artifacts. c) UW-StereoDepth-40K visualization: Beyond these qualitative comparisons, we showcase representative imagedepth pairs from our proposed UW-StereoDepth-40K dataset  (Fig. 9)  . The examples from UW-StereoDepth-40K demonstrate the datasets variety in viewpoint, illumination, and scene composition, underscoring its value as large-scale resource for training underwater depth estimation models. Fig. 7. Comparison results from real-world experiments Fig. 8. Comparison results from TartanAir and SQUID Fig. 9. Synthetic underwater images from UW-StereoDepth-40K"
        },
        {
            "title": "REFERENCES",
            "content": "[1] T. Huang, Z. Zhang, Y. Wang, and H. Tang, 3d coca: Contrastive learners are 3d captioners, arXiv preprint arXiv:2504.09518, 2025. [2] T. Huang, Z. Zhang, R. Zhang, and Y. Zhao, Dc-scene: Datalearning for 3d scene understanding, arXiv preprint centric arXiv:2505.15232, 2025. [3] T. Huang, Z. Zhang, and H. Tang, 3d-r1: Enhancing reasoning in 3d vlms for unified scene understanding, arXiv preprint arXiv:2507.23478, 2025. [4] Q. Liu, T. Huang, Z. Zhang, and H. Tang, Nav-r1: Reasoning and navigation in embodied scenes, arXiv preprint arXiv:2509.10884, 2025. [5] Z. Song, G. Ouyang, M. Li, Y. Ji, C. Wang, Z. Xu, Z. Zhang, X. Zhang, Q. Jiang, Z. Chen, et al., Maniplvm-r1: Reinforcement learning for reasoning in embodied manipulation with large visionlanguage models, arXiv preprint arXiv:2505.16517, 2025. [6] Z. Song, G. Ouyang, M. Fang, H. Na, Z. Shi, Z. Chen, F. Yujie, Z. Zhang, S. Jiang, M. Fang, et al., Hazards in daily life? enabling robots to proactively detect and resolve anomalies, in Proceedings of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 2025, pp. 73997415. the 2025 Conference of the Nations of [7] R. Hartley and A. Zisserman, Multiple View Geometry in Computer Vision, 2nd ed. Cambridge, UK: Cambridge University Press, 2004. [8] A. Geiger, P. Lenz, and R. Urtasun, Are we ready for autonomous driving? the kitti vision benchmark suite, in Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE, 2012, pp. 33543361. [9] S. Zhang, S. Zhao, D. An, J. Liu, H. Wang, Y. Feng, D. Li, and R. Zhao, Visual slam for underwater vehicles: survey, Computer Science Review, vol. 46, p. 100510, 2022. [10] F. Nauert and P. Kampmann, Inspection and maintenance of industrial infrastructure with autonomous underwater robots, Frontiers in Robotics and AI, vol. 10, p. 1240276, 2023. [11] Z. Zhou and Q. Dong, Two-in-one depth: Bridging the gap between monocular and binocular self-supervised depth estimation, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 94119421. [12] L. Bartolomei, F. Tosi, M. Poggi, and S. Mattoccia, Stereo anywhere: Robust zero-shot deep stereo matching even where either stereo or mono fail, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 10131027. [13] D. Akkaynak and T. Treibitz, revised underwater image formation model, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 67236732. [14] W. Wang, D. Zhu, X. Wang, Y. Hu, Y. Qiu, C. Wang, Y. Hu, A. Kapoor, and S. Scherer, Tartanair: dataset to push the limits of visual slam, in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 49094916. [15] D. Berman, D. Levy, S. Avidan, and T. Treibitz, Underwater single image color restoration using haze-lines and new quantitative dataset, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 8, pp. 28222837, 2020. [16] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy, A. Bachrach, and A. Bry, End-to-end learning of geometry and context for deep stereo regression, in Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017, pp. 66 75. [17] J.-R. Chang and Y.-S. Chen, Pyramid stereo matching network, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 54105418. [18] F. Zhang, V. Prisacariu, R. Yang, and P. H. Torr, Ga-net: Guided aggregation net for end-to-end stereo matching, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 185194. [19] L. Lipson, Z. Teed, and J. Deng, Raft-stereo: Multilevel recurrent field transforms for stereo matching, in International Conference on 3D Vision (3DV), 2021, pp. 218227. [20] G. Xu, X. Wang, X. Ding, and X. Yang, Iterative geometry encoding volume for stereo matching, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 21 91921 928. [21] X. Wang, G. Xu, H. Jia, and X. Yang, Selective-stereo: Adaptive frequency information selection for stereo matching, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 19 70119 710. [22] Z. Li, X. Liu, N. Drenkow, A. Ding, F. X. Creighton, R. H. Taylor, and M. Unberath, Revisiting stereo depth estimation from sequenceto-sequence perspective with transformers, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 61976206. [23] P. Weinzaepfel, V. Leroy, T. Lucas, R. Brégier, Y. Cabon, V. Arora, L. Antsfeld, B. Chidlovskii, G. Csurka, and J. Revaud, Croco: Selfsupervised pre-training for 3d vision tasks by cross-view completion, arXiv preprint arXiv:2210.10716, 2023. [24] R. Ranftl, K. Lasinger, D. Hafner, K. Schindler, and V. Koltun, Towards robust monocular depth estimation: Mixing datasets for zeroshot cross-dataset transfer, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 3, pp. 16231637, 2022. [25] R. Ranftl, A. Bochkovskiy, and V. Koltun, Vision transformers for the IEEE/CVF International dense prediction, in Proceedings of Conference on Computer Vision (ICCV), 2021, pp. 12 17912 188. [26] L. Yang, B. Kang, Z. Huang, X. Xu, J. Feng, and H. Zhao, Depth anything: Unleashing the power of large-scale unlabeled data, in CVPR, 2024. [27] L. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, and H. Zhao, Depth anything v2, arXiv preprint arXiv:2406.09414, 2024. [28] B. Wen, M. Trepte, J. Aribido, J. Kautz, O. Gallo, and S. Birchfield, Foundationstereo: Zero-shot stereo matching, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 5249 5260. [29] H. Jiang, Z. Lou, L. Ding, R. Xu, M. Tan, W. Jiang, and R. Huang, Defom-stereo: Depth foundation model based stereo matching, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 21 85721 867. [30] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, Unsupervised learning of depth and ego-motion from video, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 18511858. [31] C. Godard, O. Mac Aodha, and G. J. Brostow, Unsupervised monocular depth estimation with left-right consistency, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 270279. [32] C. Godard, O. Mac Aodha, M. Firman, and G. J. Brostow, Digging into self-supervised monocular depth prediction, October 2019. [33] J. Watson, M. Firman, G. J. Brostow, and D. Turmukhambetov, Selfsupervised monocular depth hints, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 2162 2171. [34] F. Aleotti, M. Poggi, and S. Mattoccia, Reversing the cycle: Selfsupervised deep stereo through enhanced monocular distillation, arXiv preprint arXiv:2008.07130, 2020. [35] Z. Chen, X. Ye, W. Yang, Z. Xu, X. Tan, Z. Zou, E. Ding, X. Zhang, and L. Huang, Revealing the reciprocal relations between selfsupervised stereo and monocular depth estimation, in 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 15 50915 518. [36] F. Zhang, X. Qi, R. Yang, V. Prisacariu, B. Wah, and P. Torr, Domaininvariant stereo matching networks, arXiv preprint arXiv:1911.13287, 2020. [37] C. Cai, M. Poggi, S. Mattoccia, and P. Mordohai, Matching-space stereo networks for cross-domain generalization, in International Conference on 3D Vision (3DV), 2020, pp. 364373. [38] Z. Rao, B. Dai, J. He, X. Dai, and R. Zhou, Masked representation learning for domain generalized stereo matching, arXiv preprint arXiv:2309.07315, 2023. [39] N. Mayer, E. Ilg, P. Häusser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox, large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 40404048. [40] M. Roberts, J. Ramapuram, A. Ranjan, A. Kumar, M. A. Bautista, N. Paczan, R. Webb, and J. M. Susskind, Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding, arXiv preprint arXiv:2011.02523, 2021. [41] A. Geiger, P. Lenz, and R. Urtasun, Are we ready for autonomous driving? the kitti vision benchmark suite, in Conference on Computer Vision and Pattern Recognition (CVPR), 2012. cient stereo matching, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 19591968. [65] X. Guo, K. Yang, W. Yang, X. Wang, and H. Li, Group-wise correlation stereo network, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 32733282. [66] G. Xu, J. Cheng, P. Guo, and X. Yang, Attention concatenation volume for accurate and efficient stereo matching, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 12 98112 990. [67] H. Zhao, H. Zhou, Y. Zhang, J. Chen, Y. Yang, and Y. Zhao, Highfrequency stereo matching network, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 13271336. [68] J. Li, P. Wang, P. Xiong, T. Cai, Z. Yan, L. Yang, J. Liu, H. Fan, and S. Liu, Practical stereo matching via cascaded recurrent network with adaptive correlation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 16 26316 272. [69] H. Xu, J. Zhang, J. Cai, H. Rezatofighi, F. Yu, D. Tao, and A. Geiger, Unifying flow, stereo and depth estimation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [70] H. Yan, A. Li, X. Zhang, Z. Liu, Z. Shi, C. Zhu, and L. Zhang, Mobileie: An extremely lightweight and effective convnet for realtime image enhancement on mobile devices, in Proceedings of the IEEE/CVF International Conference on Computer Vision, October 2025. [71] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black, naturalistic open source movie for optical flow evaluation, in European Conference on Computer Vision (ECCV), 2012, pp. 611625. [72] T. Schöps, J. L. Schönberger, S. Galliani, T. Sattler, K. Schindler, M. Pollefeys, and A. Geiger, multi-view stereo benchmark with high-resolution images and multi-camera videos, in Conference on Computer Vision and Pattern Recognition (CVPR), 2017. [73] W. Bao, W. Wang, Y. Xu, Y. Guo, S. Hong, and X. Zhang, Instereo2k: large real dataset for stereo matching in indoor scenes, Science China Information Sciences, vol. 63, 2020. [Online]. Available: https://api.semanticscholar.org/CorpusID:221110870 [42] D. Scharstein, H. Hirschmüller, Y. Kitajima, G. Krathwohl, N. Nešic, X. Wang, and P. Westling, High-resolution stereo datasets with subpixel-accurate ground truth, in German Conference on Computer Vision (GCPR), 2014. [43] P. Zama Ramirez, F. Tosi, M. Poggi, S. Salti, S. Mattoccia, and L. Di Stefano, Open challenges in deep stereo: the booster dataset, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [44] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, Lora: Low-rank adaptation of large language models, in International Conference on Learning Representations, 2022. [45] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, Qlora: Efficient finetuning of quantized llms, arXiv preprint arXiv:2305.14314, 2023. [46] S.-Y. Liu, C.-Y. Wang, H. Yin, P. Molchanov, Y.-C. F. Wang, K.-T. Chao, and J. Kautz, Dora: Weight-decomposed low-rank adaptation, arXiv preprint arXiv:2402.09353, 2024. [47] Q. Zhang, M. Chen, A. Bukharin, P. He, Y. Cheng, W. Chen, and T. Zhao, Adalora: Adaptive budget allocation for parameter-efficient fine-tuning, in International Conference on Learning Representations (ICLR), 2023. [48] S. He, D. Zhou, Y. Zhou, Y. Zhou, and D. Tang, Sparseadapter: An easy approach for improving the parameter-efficiency of adapters, arXiv preprint arXiv:2210.04284, 2022. [49] Y. Zhang, X. Lu, and G. Sun, Dares: Dynamic adaptation for robust and efficient stereo matching, arXiv preprint arXiv:2401.06987, 2024. [50] S. Chen, C. Ge, Z. Tong, J. Wang, Y. Song, J. Wang, and P. Luo, Adaptformer: Adapting vision transformers for scalable visual recognition, in NeurIPS, 2022. [51] S. Chang, P. Wang, H. Luo, F. Wang, and M. Z. Shou, Revisiting vision transformer from the view of path ensemble, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 19 88919 899. [52] M. J. Islam, C. Edge, Y. Xiao, P. Luo, M. Mehtaz, C. Morse, S. S. Enan, and J. Sattar, Semantic segmentation of underwater imagery: Dataset and benchmark, in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 17691776. [53] Q. Lv, J. Dong, Y. Li, S. Chen, H. Yu, S. Zhang, and W. Wang, Uwstereo: large synthetic dataset for underwater stereo matching, IEEE Transactions on Circuits and Systems for Video Technology, 2025. [54] X. Ye, Z. Li, B. Sun, Z. Wang, R. Xu, H. Li, and X. Fan, Underwater depth estimation via stereo adaptation networks, IEEE Transactions on Circuits and Systems for Video Technology, vol. 33, no. 8, pp. 42284240, 2023. [55] L. Zhu, Y. Gao, J. Zhang, Y. Li, and X. Li, Reliable and effective stereo matching for underwater scenes, Remote Sensing, vol. 16, no. 23, p. 4570, 2024. [56] H. Lu, C. Zhao, J. Xue, L. Yao, K. Moore, and D. Gong, Adaptive rank, reduced forgetting: Knowledge retention in continual learning vision-language models with dynamic rank-selective lora, arXiv preprint arXiv:2412.01004, 2024. [57] L. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, and H. Zhao, Depth anything v2, arXiv preprint arXiv:2406.09414, 2024. [58] C. Zhao, Y. Zhang, M. Poggi, F. Tosi, X. Guo, Z. Zhu, G. Huang, Y. Tang, and S. Mattoccia, Monovit: Self-supervised monocular depth estimation with vision transformer, 2022. [59] N. Ding, X. Lv, Q. Wang, Y. Chen, B. Zhou, Z. Liu, and M. Sun, Sparse low-rank adaptation of pre-trained language models, arXiv preprint arXiv:2311.11696, 2023. [60] Z. Liu, J. Lyn, W. Zhu, X. Tian, and Y. Graham, Alora: Allocating low-rank adaptation for fine-tuning large language models, arXiv preprint arXiv:2403.16187, 2024. [61] F. Meng, Z. Wang, and M. Zhang, Pissa: Principal singular values and singular vectors adaptation of large language models, arXiv preprint arXiv:2404.02948, 2024. [62] J. Zhang, Y. Zhao, D. Chen, X. Tian, H. Zheng, and W. Zhu, Milora: Efficient mixture of low-rank adaptation for large language models fine-tuning, arXiv preprint arXiv:2410.18035, 2024. [63] X. Cheng, Y. Zhong, M. Harandi, Y. Dai, X. Chang, H. Li, T. Drummond, and Z. Ge, Hierarchical neural architecture search for deep stereo matching, in Advances in Neural Information Processing Systems, vol. 33, 2020. [64] H. Xu and J. Zhang, Aanet: Adaptive aggregation network for effi-"
        }
    ],
    "affiliations": [
        "AI Geeks",
        "Australian Centre for Robotics",
        "Peking University"
    ]
}