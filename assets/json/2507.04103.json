{
    "paper_title": "How to Train Your LLM Web Agent: A Statistical Diagnosis",
    "authors": [
        "Dheeraj Vattikonda",
        "Santhoshi Ravichandran",
        "Emiliano Penaloza",
        "Hadi Nekoei",
        "Megh Thakkar",
        "Thibault Le Sellier de Chezelles",
        "Nicolas Gontier",
        "Miguel Muñoz-Mármol",
        "Sahar Omidi Shayegan",
        "Stefania Raimondo",
        "Xue Liu",
        "Alexandre Drouin",
        "Laurent Charlin",
        "Alexandre Piché",
        "Alexandre Lacoste",
        "Massimo Caccia"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LLM-based web agents have recently made significant progress, but much of it has occurred in closed-source systems, widening the gap with open-source alternatives. Progress has been held back by two key challenges: first, a narrow focus on single-step tasks that overlooks the complexity of multi-step web interactions; and second, the high compute costs required to post-train LLM-based web agents. To address this, we present the first statistically grounded study on compute allocation for LLM web-agent post-training. Our approach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate a Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy reinforcement learning. We find this process highly sensitive to hyperparameter choices, making exhaustive sweeps impractical. To spare others from expensive trial-and-error, we sample 1,370 configurations and use bootstrapping to estimate effective hyperparameters. Our results show that combining SFT with on-policy RL consistently outperforms either approach alone on both WorkArena and MiniWob++. Further, this strategy requires only 55% of the compute to match the peak performance of pure SFT on MiniWob++, effectively pushing the compute-performance Pareto frontier, and is the only strategy that can close the gap with closed-source models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 3 0 1 4 0 . 7 0 5 2 : r How to Train Your LLM Web Agent: Statistical Diagnosis Dheeraj Vattikonda,1,2,5 Santhoshi Ravichandran,1,2 Emiliano Penaloza,1,2,6 Hadi Nekoei1,2,6 Megh Thakkar1 Thibault Le Sellier de Chezelles1,2,3 Nicolas Gontier1 Miguel Muñoz-Mármol1 Sahar Omidi Shayegan1,2,5 Stefania Raimondo1 Xue Liu2,5 Alexandre Drouin1 Laurent Charlin2,4 Alexandre Piché1 Alexandre Lacoste1 Massimo Caccia,"
        },
        {
            "title": "Abstract",
            "content": "LLM-based web agents have recently made significant progress, but much of it has occurred in closed-source systemswidening the gap with open-source alternatives. Progress has been held back by two key challengesfirst, narrow focus on singlestep tasks that overlooks the complexity of multi-step web interactions, and second, the high compute costs required to post-train LLM-based web agents. To address this, we present the first statistically grounded study on compute allocation for LLM web-agent post-training. Our approach uses two-stage pipeline, training Llama 3.1 8B student to imitate Llama 3.3 70B teacher via SFT, followed by on-policy reinforcement learning. We find this process highly sensitive to hyperparameter choices exhaustive sweeps are impractical. To spare others from expensive trial-and-error, we sample 1,370 configurations and use bootstrapping to estimate effective hyperparameters. Our results show that combining SFT with on-policy RL consistently outperforms either approach alone on both WorkArena and MiniWob++. Further, this strategy only requires 55% of the compute to match the peak of pure SFT on MiniWob++, pushing the computeperformance Pareto frontier and is the only strategy that can close the gap with closed-source models. Figure 1: Computeperformance frontier on MiniWoB++ (results averaged over two seeds). The blue curve shows pure SFT on teacher demonstrations. Warm-colored curves represent hybrid runs that branch off from SFT checkpoints and continue training with RL. Early transitions to RL push the Pareto frontierachieving higher success rates for the same computeand is the only approach able to achieve over 30% improvement on both held-out goals (left) and held-out tasks (right) closing the gap between open and closed-source models. *Equal contribution 1ServiceNow Research 2MilaQuebec AI Institute 5McGill University 6Univeristé de Montréal 4HEC Montréal 3Polytechnique Montréal Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Large language model (LLM) agents for web interfaces have advanced rapidly, but open-source systems still trail proprietary ones. Bridging this gap would allow organizations to train smaller, cost-efficient agents tailored to their needs while maintaining data privacy. Yet, despite impressive progress of open-source models in domains like math and code generation, advances in training web-capable LLM agents remain limited by two persistent challenges: lack of attention to multi-turn, long-horizon tasks, and the high cost and low reproducibility of current training pipelines. Most research centers on single-step tasks like code or mathdomains with rapid feedback and simplified credit assignmentwhich fall short of real-world web environments requiring sequential decisions and long-horizon planning. Recent benchmarks like WebArena [25], WorkArena [10], OSWorld [22], and The Agent Company [23] have exposed how brittle current methods become under delayed rewards, sparse feedback, and compounding errors. Addressing these settings demands not just better agents, but reproducible, compute-efficient training pipelinesan area we directly tackle. However, building such pipelines is nontrivial. Modern LLM post-training often involves combination of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), with performance sensitive to large number of interacting hyperparameters [11]. Single-seed results are noisy and misleading, but exhaustive tuning and multi-seed sweeps, e.g. [2], remain infeasible for most labs due to the cost of LLM training. This makes it all the more critical to design pipelines that are not only effective but statistically robust and accessible within realistic compute budgets. In this paper, we tackle these gaps by providing statistically driven diagnosis of training LLM agents for web-based, multi-step tasks. Specifically, we study how to allocate compute between expensive, high-quality off-policy demonstrations from large teacher model and cheaper on-policy rollouts from smaller student model. We analyze this tradeoff across two levels of generalization: held-out goalstasks encountered during training but with novel goalsand held-out tasks, which are entirely unseen during training. To study this compute-performance trade-off we use two-stage training pipeline. First, LLaMA 3.3 70B teacher model generates successful trajectories to warm-start smaller LLaMA 3.1 8B student via SFT. We then branch of various SFT checkpoints where training continues with an on-policy RL phase using Group Relative Policy Optimization (GRPO) [9]. Our central objective is to determine the optimal compute allocation and hyperparameter mix for training web-based LLM agents. To this end, we run 1,370 training configurations, varying key hyperparameters and branching points between SFT and RL. We then apply bootstrap-based analysis to estimate the impact of different hyper-parameters on downstream performance and how they vary across branching SFT checkpoints. This data-driven approach enables us to identify important considerations to get the most out of each run. We use this method to show the optimal mix between SFT and RL in the MiniWob++ environment, achieving better task accuracy at significantly lower cost. Additionally, we provide concrete recommendations on compute allocations between SFT and RL on the more demanding WorkArena environment. Putting things together, we show how our study yields several actionable insights. First, branching into RL earlybut not immediatelyafter SFT leads to better outcomes. This hybrid strategy consistently outperforms pure SFT and pure RL, and reaches the maximal performance of pure SFT while requiring only 55% of the compute, effectively pushing the computeperformance Pareto frontier. It is also the only strategy that can close the gap with closed-source models. Second, curriculum learning and error log feedback help the less SFT warmup has been applied but can become counterproductive thereafter. Third, in GRPO, applying zero-advantage filtering consistently improve performance, while dividing the advantage with the standard deviation and wether to use an importance ratio are dependent on the amount of SFT warmup done. Fourth, decoding temperature is consistently critical, while learning rate and discount rate must also be carefully tuned. These findings are significant for two reasons. First, they give smaller research groups reproducible, budget-aware playbook for pushing open LLM agents closer to state-of-the-art without scaling model size. Second, they address slice of the broader reproducibility concerns recently highlighted in the RL community [3, 11], offering template for rigorous, statistically-grounded hyperparameter tuning and compute allocation in LLM agent training."
        },
        {
            "title": "2 Background",
            "content": "This section consolidates the algorithmic ingredients used throughout the paper: (i) the MDP formulation of web-based language agents, (ii) SFT on expert traces, (iii) GRPO for RL, and (iv) curriculum and normalization techniques that stabilize training. 2.1 Language Agents as MDPs We model each task as Markov Decision Process (MDP) = S, A, P, r, ρ0, γ. state st is textual contextin our case prompt and an action at is textual response generated by the agent. Each action at consists of sequence of tokens o1t:Kt sampled autoregressively from the policy πθ(atst) parameterized by an LLM sampled with temperature ρLLM. The environment then returns scalar reward rt {1, 1} indicating task failure/success. In our setting we assume the environment dynamics are p(st+1 at) . We optimize the policy πθ to maximize the expected discounted return: J(θ) = Eτ πθ γtrt (1) (cid:34) (cid:88) (cid:35) t=0 Here, γ [0, 1] is the discount rate, which controls the agents preference for immediate versus future rewards and τ = (s0, a0, r0, s1, a1, r1, . . . , sT ) refers to trajectory sampled from the policy. 2.2 Off-policy boot-strapping via SFT We first imitate stronger expert policy πE by minimizing the cross-entropy loss LSFT(θ) = Eτ πE (cid:104) (cid:88) t=0 log πθ(at st) (cid:105) . (2) SFT offers high-quality, low-variance gradient but is inherently off-policy which can lead to poor generalization [6]. 2.3 On-Policy Improvement with Multi-Turn GRPO After the initial SFT warmup phase, we continue training using on-policy RL using GRPO as the optimization algorithm. Like REINFORCE [18], GRPO maximizes the expected return using policy gradient objective. However, GRPO introduces additional structure by leveraging per-goal advantage normalization and importance weighting, wherein each trajectory is associated with specific goalin our context, referring to the seed of the target task. For given goal g, the group-normalized advantage function is: At,g = rt,g mean(Rt,g) std(Rt,g) , where Rt,g = (rt,1, . . . , rt,G) is the set of rewards across the goals at timestamp t. In addition to thissimilar to proximal policy optimization (PPO) [2]an importance-ratio is often applied to the GRPO objective: ηt,g = πθ(at st) πθold (at st) , where πθ is the current policy and πθold is the behavior policy used to collect trajectories. Finally, clipped-minimum is also applied to stabilize the training process. Putting things together the GRPO objective for our multi-turn setting is: JMT-GRPO(θ) = Eτ πθold (cid:34) 1 (cid:88) g=1 1 T (cid:88) t=0 min (cid:0)ηt,g At,g , clip(ηt,g , 1ϵ, 1+ϵ)At,g (cid:35) (cid:1) . (3) Traditionally the GRPO objective includes KL penalty between the optimizing and reference policies which we do not use as early experiments showed they did not improve performance, slowed down training and required additional compute budget. 3 Zero-advantage filtering. Tokens with At,g = 0 contribute no learning signal yet still consume memory. Dropping them yields constant effective batch size and modestly accelerates training [24]. 2.4 Curriculum through Variance-Aware Boltzmann Sampling To promote steady learning progress, we design curriculum that prioritizes challenging tasks, neither trivial nor too difficult [19]. Specifically, we select tasks according to Boltzmann distribution centered around target return µtarget which specifies the desired performance threshold, encouraging focus on partially mastered tasks, with temperature parameter ρCurr controlling the sharpness of the distribution, with lower values concentrating probability mass tightly around µtarget. This sampling mechanism dynamically adapts the training distribution, concentrating learning on tasks where the agent is neither already proficient nor entirely unskilled. As result, the agent avoids premature convergence on easy tasks and prevents wasted effort on tasks far beyond its current capabilities."
        },
        {
            "title": "3 Methodology",
            "content": "Our training pipeline consists of two sequential stagesSFT followed by RLframed as resource allocation problem. We also describe our hyperparameter sweep strategy and statistical analysis protocol that consolidates hundreds of runs into reliable conclusions. We evaluate our recipe along two axes: compute cost, measured in FLOPs (using the formula from [1]), and model performance, assessed on both unseen training goals and held-out testing tasks. Stage 1 Supervised Fine-Tuning (SFT). We begin by generating NE expert trajectories using large teacher model. Only successful trajectories are retained after filtering, and the corresponding (s, a) pairs, along with chain-of-thought annotations, form the SFT dataset. Note that computing the cost of the SFT dataset includes both successful and discarded unsuccessful trajectories. We then train smaller student model for TSFT gradient steps. To explore the trade-off between SFT and RL, we branch off times at fixed intervals along the SFT trajectory, yielding checkpoints at timesteps tb [0, TSFT]. Each checkpoint corresponds to student policy πθ(tb), which initializes distinct RL phase. The total compute used up to each branching point, FSFT(tb), includes both teacher inference and student training FLOPs accumulated over tb steps. Stage 2 RL Fine-Tuning. Each policy πθ(tb) is further trained using GRPO for TRL steps. The compute cost of this phase, FRL(TRL), includes both the FLOPs required for data collection (online rollouts) and for student updates across all TRL steps. The total FLOPs for full training run starting from θ(tb) is computed as: FLOPs(tb) = FSFT(tb) + FRL(TRL). (4) By varying tb throughout SFT, we assess how shifting compute between expert-driven supervision and on-policy learning impacts final performance. This setup highlights the trade-off between the high compute cost of expert supervision and the lower-cost, but noisier, nature of on-policy learning. 3.1 Estimating the Uncertainty of the Hyperparameter Selection Process Across the different SFT checkpoints, we sample 1,370 distinct configurations with ten varying hyperparameters (see Appendix for details). Our objective is to study the effect of various hyperparameters (HP) on the downstream success rate of our trained agents. This comes with two important considerations. First, if we change the value of, e.g., the batch size, and we want to know if bigger batch size is better, the learning rate and other parameters need to be readjusted close to their optimal configuration (under fixed budget). Secondly, to account for noise, we would need to restart the same experiment several times to avoid spurious conclusions. In practice, this is out of reach. For more computationally friendly approach, we resort to bootstrapping the collection of trials over different hyperparameter configurations. 4 Algorithm 1 Bootstrap Estimation of Hyperparameter Importance Require: Set of training runs Results, evaluation metric column , hyperparameter of interest H, number of bootstrap iterations Win counts per HP value Score samples per HP value Distinct values of the hyperparameter Get the winning HP value for sampleb sampleb = resample(Results, weights) = argmaxh sampleb[M ] W[h] += 1 for each in Hvals do 1: = dict( ) 2: = defaultdict(List) 3: Hvals = unique(Results[H]) 4: weights = { hi : 1/count(Results[hi]) for each hi Hvals } 5: for = 1 to do 6: 7: 8: 9: 10: 11: 12: end for 13: for each in Hvals do 14: 15: end for 16: return W, with confidence intervals S[h]+ = max sampleb[sampleb[H] == h][M ] Compute 95% confidence interval from S[h] end for Bootstrapping the hyperparameter selection process. From the full set of 1,370 training runs, we perform bootstrap resampling by drawing individual runs (with replacement). For each resample, we identify the best-performing configuration and repeat this process 1,000 times. We also compute the fraction of times each hyperparameter value \"wins\", which serves as an estimated probability that it belongs to the global optimum. This procedure serves two purposes: to estimate the maximum relative improvement specific hyper-parameter provideswhile accounting for variation in other parametersand to offer uncertainty estimates in the form of win-rate distributions between different configurations. In addition, to better understand how optimal hyper-parameters may change depending on the amount of SFT warmup we apply this analysis across various SFT checkpoints. Balancing unequal coverage. Due to random search, some HP values were explored more than others, biasing the winner toward the larger groups. To correct for this, each run is sampled with probability 1/group size, approximating an equal compute budget for every HP value. We provide detailed explanation of this procedure in Algorithm 1. Additionally in Appendix we describe how use this procedure to obtain the results reported in Figure 1."
        },
        {
            "title": "4 Experimental Setup",
            "content": "In this section, we describe the experimental setup used to validate our findings. We detail the models, benchmarks, action spaces, training framework, compute infrastructure, and evaluation protocols, ensuring that all components are aligned with the goals of studying compute-efficient, reproducible training for LLM web agents. Models. We evaluate our approach using Llama 3.3 70B as the expert model to generate demonstration traces, and Llama 3.1 8B as the student model for fine-tuning. Both models operate with 16k token context window to handle the complexity of web-based tasks. Benchmarks. Our experiments focus on two benchmarks. The first is MiniWoB++, suite of 30 medium-horizon web interaction tasks, where we observe that optimal policies typically complete tasks in 2 to 5 steps. The second is WorkArena [10], more challenging benchmark of 33 enterprise knowledge-work tasks, where we observe optimal policies generally complete tasks in 3 to 10 steps. These benchmarks provide representative spectrum of sequential decision-making challenges faced by interactive LLM agents. Both benchmarks are depicted in Figure 2. Observation & Action Spaces. MiniWoB++ provides raw HTML trees, whereas WorkArena leverages accessibility trees (AxTrees), which we truncate to 16k tokens to meet hardware con5 Figure 2: Example tasks in MiniWoB++ [13] (top) and WorkArena [10] (bottom). MiniWoB consists of single-page simple tasks such as selecting particular date and using basic text editor, while WorkArena comprises multi-page complex tasks like filling forms and placing orders in an enterprise environment. Figure 3: Per-task performance of SFT and SFT+RL agents on WorkArena. The Llama 3.1 8B model is initially fine-tuned for 4 epochs on trajectories from teacher Llama 3.3 70B model. Training then continues either with additional SFT or with GRPO fine-tuning up to epoch 20. The teacher models success rate is also shown. straints. The agent operates in discrete action space composed of high-level UI primitives: noop, fill(node, text), click(node), type(node, text), select_option(node, option), scroll(node), and hover(node). This abstraction allows the agent to interact effectively with diverse web interfaces. All agents employ chain-of-thought prompting [21]. We also experiment with error log feedback, allowing the agent to receive explicit error messages when it takes invalid actions. Training Framework. To manage the training pipeline, we use BROWSERGYM [8] for orchestrating Chromium-based web environments and structuring the agents action space, while AGENTLAB [8] handles agent design. Model fine-tuning is conducted with TORCHTUNE [20], utilizing Fully Sharded Data Parallelism (FSDP) to enable scalable training across multiple GPUs. Given the high memory demands of long-sequence processing, we apply activation offloading and gradient checkpointing techniques, achieving approximately 40% reduction in memory usage. Compute Infrastructure. Our computational infrastructure comprises 8 H100-80GB GPUs for expert data generation with the 70B model. For student model training, we allocate 2 H100 GPUs for MiniWoB++ experiments and 4 H100 GPUs for WorkArena experiments, reflecting the increased complexity of the latter. Evaluation Protocol. The environments in this study are stochastic; each task is initialized with random seed. We define goal as specific random seed for given task. The evaluation protocol assesses generalization at two levels: (i) performance on held-out goals within the training tasks, and (ii) performance on entirely new, unseen held-out tasks. We consider both settings important as they capture different aspects of generalizationheld-out goals test reliability on trained tasks, while held-out tasks test the models ability to transfer skills learnt in training to entirely new situations. In both settings we use the the average task success rate as the evaluation metric, which reflects the agents ability to generalize beyond its training distribution. To reduce the impact of evaluation noise during model selection, we apply rolling average with window size of 3 on the held-out goals, and select the checkpoint with the highest smoothed score. 6 Table 1: Comparison of our method with baselines on the held-out train and test splits of WorkArena and MiniWoB++. Baselines are run in the exact same setting as [8]. Model WorkArena MiniWoB++ Held-out Goals Held-out Tasks Held-out Goals Held-out Tasks Claude-3.5-Sonnet GPT-4o GPT-4o-Mini Llama-3.1-70b-Instruct o1-Mini Llama-3.1-405b-Instruct Llama-3.1-8B Instruct (Student) Llama-3.1-8b-SFT (Ours) Llama-3.1-8b-RL (Ours) Llama-3.1-8b-SFT+RL (Ours) Llama-3.3-70b (Teacher) 52.53.2 42.13.2 27.12.9 25.02.8 53.83.2 39.22.4 0.080.1 28.42.3 0.00.0 34.62.4 36.02.4 70.05.5 55.75.9 28.65.4 32.95.6 68.65.5 58.62.5 0.040.1 26.42.2 0.00.0 28.02.2 44.02. 70.52.0 65.72.1 56.22.2 57.02.2 69.72.1 65.92.4 29.52.3 53.42.5 43.52.5 66.32.4 63.22.4 70.44.3 64.34.5 66.14.4 65.24.4 66.14.4 65.22.4 36.42.4 55.62.5 43.52.5 62.92.4 61.92.4 For all our reported runs (SFT, RL, SFT + RL) we report the average of the non-smoothed scores at the selected checkpoint, aggregated over four runs chosen through the described model selection procedure based on our random search. For all other models, we report average scores over 100 independent runs. Compute Allocation Protocol. We track the total floating-point operations (FLOPs) consumed during both SFT and GRPO phases, following the procedure in Section 3 and Appendix B. For RL branching, we start from the SFT run that (i) attains the highest SFT performance and (ii) exhibits stable learning, allowing us to sample checkpoints at regular intervals and cleanly study the effect of SFT warm-up. Because this run sits in the top 10% of all SFT trials by area-under-the-training-curve (AUC)the mean success rate across all checkpoints of the run, serving as proxy for overall training efficiency, we match that selection pressure for RL by averaging the top two seeds out of ten RL trials, thus approximating the 90th-percentile of the RL distribution and ensuring fair compute-aware comparison between strategies."
        },
        {
            "title": "5 Main Results and Compute Trade-Offs",
            "content": "In this section, we present our primary findings and analyze the trade-offs between SFT and RL in terms of both performance and compute efficiency. Performance Overview. Table 1 summarizes the results on MiniWoB++ and WorkArena. Combining SFT with RL consistently yields the best performance among student models. Pure SFT and pure RL each fall short, with RL from scratch particularly struggling due to sparse rewards and unstable learning dynamics. These results highlight the complementary strengths of expert demonstrations and on-policy fine-tuning. On MiniWoB++, this approach not only maximizes student performance but also matches the teacher and significantly closes the gap with proprietary models. In contrast, WorkArena remains more challenging: while SFT+RL improves over SFT alone, student performance still lags behind the teacher and proprietary models, suggesting that stronger supervision or more effective RL strategies may be needed for complex enterprise tasks. We observe that agent performance eventually saturates for both SFT and SFT+RL, especially on more difficult tasks. Further analysis of this saturation behavior is provided at the end of this section. Notably, WorkArenas test set can be easier than its training set for certain agents, which adds nuance to the observed performance gaps and complicates straightforward interpretation of generalization performance. ComputePerformance Trade-Off. Our analysis focuses on the trade-off between costly but high-quality teacher demonstrations and cheaper, noisier on-policy rollouts. To examine this, we branch RL fine-tuning from SFT checkpoints sampled at fixed intervals along the training trajectory. Figure 1 illustrates the computeperformance frontier on MiniWoB++, where runs that combine SFT with RL (warm colors) consistently outperform pure SFT (blue), pushing the Pareto frontier forward. Early branching into RL unlocks substantial gains in compute efficiency, delivering stronger performance at lower compute budgets. For example, SFT+RL reaches the maximal performance of pure SFT on the test set (achieved at approximately 11 exaFLOPs with pure SFT) using only around 6 exaFLOPsrepresenting compute saving of roughly 45% (11 vs 6 exaFLOPs) compute saving of roughly 45%. Notably, this is the only strategy that closes the performance gap with closed-source models like GPT-4o. The trend is consistent across both held-out goals and held-out tasks: warm-started RL yields higher performance than either SFT or RL alone, reinforcing the importance of blending expert supervision with on-policy learning. These findings underscore the need to balance sample efficiency from demonstrations with the compute efficiency of on-policy learninga key consideration for scaling LLM-agent training. Task Performance Saturation and Analysis Despite extensive post-training, agent performance on the WorkArena benchmark plateaus at around 40% after just 910 epochs. This stagnation appears to stem from the intrinsic difficulty of certain taskssuch as sorting and filteringwhich even the Llama 3.3 70B teacher model struggles to solve (see Figure 3). per-task breakdown shows that while both SFT and RL agents gradually close the performance gap with the teacher model, with RL achieving slightly higher final success rate, significant portion of tasks (14 out of 33) remain completely unsolved. These failures are attributed to either the limitations of the teacher model or the sparsity of reward signals, both of which hamper the learning process. On-policy RL exploration proves ineffective in overcoming these challenges due to the lack of foundational skills and informative feedback. These findings underscore the need for more effective methods to address complex tasks under sparse reward settings. Additional per-task performance results for WorkArena and Miniwob are provided in Appendix A. Shortcut-Seeking Agents and Task Integrity During extended training we found that giving agents full administrative privileges invites creativebut problematicshort-cuts. striking example comes from the WorkArena Order hardware devices task: rather than navigate through the catalogue as intended, the agent simply edited its homepage to add direct link to the ordering form. While this hack fulfils the success condition, it violates the tasks spirit and, worse, the modifications persist across sessions. Other agents entering the environment later inherit the altered UI, leading to failures unrelated to their own policies. In general, agents exploited admin rights to create or delete elements, reshaping pages in ways that saved clicks but destabilised the environment. These findings argue for tighter sandboxenough access for exploration and accomplishment, but safeguards against permanent, instance-wide side-effects."
        },
        {
            "title": "6 Ablation and Sensitivity Analysis",
            "content": "We simulate re-running hyperparameter configurations and selecting the best-performing ones using the method described in Section 3.1. This is done across three checkpoints: the base LLaMA 3.1 8B Instruct model and two warm-started variants with an additional 2.5 1018 and 7.6 1018 FLOPs of supervised fine-tuning, respectively, to assess variations across compute budgets. We evaluate the held-out goals and held-out tasks performance of 10 hyper-parameters across 1,370 runs. We report final held-out task performance to verify generalization in Appendix finding no significant deviations between held-out tasks and held-out goals parameters. Figure 4 displays our findings, which we summarize as follows. Curriculum learning is beneficial when starting RL from scratch but becomes detrimental after warm-starting, likely because warmstarted models already perform well on easy tasks, and curriculum forces them to overfocus on harder ones. Error log feedback helps when theres no SFT but otherwise does not, likely because SFT warmup removes many common errors made by weaker models. decoding temperature of 0.25 consistently yields the best results, striking balance between exploration and exploitation; lower values led to under-exploration and were discarded. Grouped-relative advantage helps only after SFT, while using raw rewards works better when starting directly from the Instruct model, possibly due to how advantage scaling interacts with the initial weights. Zero-advantage filtering improves training across most settings by ensuring batches focus on informative updates. Standard-deviation normalized advantage, as noted by [14], seems to aid performance under less finetuned models and decrease in value the more finetuning is done. Importance ratio correction and trust region, though 8 Figure 4: Bootstrap analysis (n = 1000 samples) of hyperparameter optimization across different SFT compute budgets on training held out tasks. Each subplot examines different hyperparameter, including increasing SFT compute: the base instruct model (left), +2.51018 SFT FLOPs (middle), and +7.6e1018 SFT FLOPs (right). For each hyperparameter-compute combination, the top panel shows relative reward performance with error bars indicating 95% confidence intervals, while the bottom panel displays win rates representing the percentage of bootstrap iterations where each parameter value achieved maximum performance. Results demonstrate that optimal hyperparameter values shift as model pre-training compute increases, suggesting that hyperparameter selection should be adapted to the computational budget allocated to SFT. standard, also hurt models with little or no SFT, likely because conservative updates slow down learning. In contrast, for models that start from stronger SFT checkpoint, these mechanisms can help stabilize training and avoid catastrophic updates. For the learning rate, the larger value (1e-16) generally worked better. Effective batch size of 512 appears to be generally be safe and robust choice in all our experiments. Finally, regarding the discount rate, values above 0.9 work well for models with little or no SFT warmup, while heavily warm-started models benefit from lower rate around 0.5likely because it encourages the agent to optimize more aggressively on tasks it already handles well."
        },
        {
            "title": "7 Related Work",
            "content": "Best prectices in deep RL. Building on the recognition of reproducibility challenges and unstable RL training of LLM agents, recent studies have proposed best practices for training LLM agents using RL methods. Dang and Ngo [7] recommend leveraging high quality data, balancing easy and hard problems, and controlling length generation with cosine reward. Yu et al. [24] promote higher clipping in the GRPO loss to promote diversity and avoid entropy collapse, dynamic sampling to improve training efficiency and stability, token level gradients for long CoT sequences, and overlong reward shaping to reduce reward noise. Roux et al. [17] introduce tapered variant of importance sampling to speed up learning while maintaining stable learning dynamics. The proposed method (TOPR) allows the handling of both positive and negative examples in fully offline setting. More generally, Hochlehnert et al. [11] emphasizes the need for greater methodological precision, particularly concerning decoding parameters, random seeds, prompt formatting, as well as the hardware and software frameworks, to guarantee transparent and thorough assessments of model performance. These practices are essential for developing robust and reproducible agents. LLM Agents trained with RL on multi-step environments. Recent advancements have sought to bridge the gap in training LLM agents for multi-step environments, with approaches like WebRL [16] and SWEET-RL [26] demonstrating significant progress. WebRL employs self-evolving curriculum to address the challenges of sparse feedback and task scarcity, successfully enhancing the performance of open LLMs in web-based tasks [16]. Similarly, SWEET-RL introduces hierarchical structure that enables effective credit assignment over multiple turns, improving policy learning and generalization in collaborative reasoning tasks [26]. These studies collectively illustrate the necessity of adapting RL techniques to accommodate the complexities of multi-step interactions, paving the way for more capable and versatile LLM agents. Similar in spirit to our work, [4] propose an empiral study on the inference cost of trained LLM web agents. An extended version of the related work is provided in Appendix C. LLM agents trained with RL in multi-step environments. Recent work has begun closing the gap for LLM agents that must reason over multiple steps. WebRL [16] introduces self-evolving curriculum that tackles sparse feedback and task scarcity, substantially boosting open-source agents on web tasks. SWEET-RL [26] adds hierarchical credit-assignment scheme spanning many turns, improving both learning stability and generalization in collaborative reasoning. Together, these studies underscore the need to adapt RL techniques to the intricacies of long-horizon interaction, paving the way for more capable, versatile agents. Similar in spirit, [4] provide an empirical analysis of inference costs for trained LLM web agents. An extended discussion of related work appears in Appendix C."
        },
        {
            "title": "8 Discussion",
            "content": "Limitations. Our focus is on providing comprehensive perspective on training an LLM-based web agent, studying compute trade-offs, hyperparameter selection, and analyzing failure cases. With this in mind, our results are limited to English-language web interfaces and Llama 3 models in the 8B70B parameter range, where larger models may alter trade-offs. Regarding our statistical method, it does not account for the lack of coverage from the random search. more exhaustive search could discover configurations that would change the conclusions drawn in this study. We note also that significant portion of the reported uncertainty is due to epistemic uncertainty that could be reduced by evaluating more configurations. Conclusion. We present statistically grounded study on training LLM web agents, analyzing the trade-off between SFT and RL. We perform random sweep across 1,370 configurations to identify optimal hyperparameter choices whichimportantly we find can vary across compute budgets. Using the bootstrap prescribed hyper-parameters we show that branching into RL earlybut not immediatelyafter SFT achieves better performancecompute trade-offs, matching peak SFT with 45% less compute and closing the gap with closed-source agents. Our findings offer reproducible, budget-aware blueprint for advancing open-source LLM web agents in complex multi-step environments."
        },
        {
            "title": "References",
            "content": "[1] Dgxc benchmarking. URL https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ dgxc-benchmarking/resources/llama31-8b-dgxc-benchmarking-a. [2] M. Abdin, S. Agarwal, A. Awadallah, V. Balachandran, H. Behl, L. Chen, G. de Rosa, S. Gunasekar, M. Javaheripi, N. Joshi, P. Kauffmann, Y. Lara, C. C. T. Mendes, A. Mitra, B. Nushi, D. Papailiopoulos, O. Saarikivi, S. Shah, V. Shrivastava, V. Vineet, Y. Wu, S. Yousefi, and G. Zheng. Phi-4-reasoning technical report, 2025. URL https://arxiv.org/abs/2504. 21318. [3] R. Agarwal, M. Schwarzer, P. S. Castro, A. C. Courville, and M. Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34:2930429320, 2021. [4] M. Andreux, B. B. Skuk, H. Benchekroun, E. Biré, A. Bonnet, R. Bordie, M. Brunel, P.-L. Cedoz, A. Chassang, M. Chen, et al. Surfer-h meets holo1: Cost-efficient web agent powered by open weights. arXiv preprint arXiv:2506.02865, 2025. [5] L. Boisvert, M. Thakkar, M. Gasse, M. Caccia, T. L. S. D. Chezelles, Q. Cappart, N. Chapados, A. Lacoste, and A. Drouin. Workarena++: Towards compositional planning and reasoning-based common knowledge work tasks, 2024. URL https://arxiv.org/abs/2407.05291. [6] T. Chu, Y. Zhai, J. Yang, S. Tong, S. Xie, D. Schuurmans, Q. V. Le, S. Levine, and Y. Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training, 2025. URL https://arxiv.org/abs/2501.17161. [7] Q.-A. Dang and C. Ngo. Reinforcement learning for reasoning in small llms: What works and what doesnt. arXiv preprint arXiv:2503.16219, 2025. URL https://arxiv.org/abs/2503. 16219. [8] T. L. S. de Chezelles, M. Gasse, A. Lacoste, M. Caccia, A. Drouin, L. Boisvert, M. Thakkar, T. Marty, R. Assouel, S. O. Shayegan, L. K. Jang, X. H. Lù, O. Yoran, D. Kong, F. F. Xu, S. Reddy, G. Neubig, Q. Cappart, R. Salakhutdinov, and N. Chapados. The browsergym ecosystem for web agent research. Transactions on Machine Learning Research, 2025. ISSN 2835-8856. URL https://openreview.net/forum?id=5298fKGmv3. Expert Certification. [9] DeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, X. Zhang, X. Yu, Y. Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu, B. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Ding, H. Xin, H. Gao, H. Qu, H. Li, J. Guo, J. Li, J. Wang, J. Chen, J. Yuan, J. Qiu, J. Li, J. L. Cai, J. Ni, J. Liang, J. Chen, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu, L. Xia, M. Zhang, M. Zhang, M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wang, Q. Chen, Q. Du, R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. L. Jin, R. Chen, S. Lu, S. Zhou, S. Chen, S. Ye, S. Wang, S. Yu, S. Zhou, S. Pan, S. S. Li, S. Zhou, S. Wu, S. Ye, T. Yun, T. Pei, T. Sun, T. Wang, W. Zeng, W. Zhao, W. Liu, W. Liang, W. Gao, W. Yu, W. Zhang, W. L. Xiao, W. An, X. Liu, X. Wang, X. Chen, X. Nie, X. Cheng, X. Liu, X. Xie, X. Liu, X. Yang, X. Li, X. Su, X. Lin, X. Q. Li, X. Jin, X. Shen, X. Chen, X. Sun, X. Wang, X. Song, X. Zhou, X. Wang, X. Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. Zhang, Y. Xu, Y. Li, Y. Zhao, Y. Sun, Y. Wang, Y. Yu, Y. Zhang, Y. Shi, Y. Xiong, Y. He, Y. Piao, Y. Wang, Y. Tan, Y. Ma, Y. Liu, Y. Guo, Y. Ou, Y. Wang, Y. Gong, Y. Zou, Y. He, Y. Xiong, Y. Luo, Y. You, Y. Liu, Y. Zhou, Y. X. Zhu, Y. Xu, Y. Huang, Y. Li, Y. Zheng, Y. Zhu, Y. Ma, Y. Tang, Y. Zha, Y. Yan, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Xu, Z. Xie, Z. Zhang, Z. Hao, Z. Ma, Z. Yan, Z. Wu, Z. Gu, Z. Zhu, Z. Liu, Z. Li, Z. Xie, Z. Song, Z. Pan, Z. Huang, Z. Xu, Z. Zhang, and Z. Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. URL https://arxiv.org/abs/2501.12948. [10] A. Drouin, M. Gasse, M. Caccia, I. H. Laradji, M. D. Verme, T. Marty, L. Boisvert, M. Thakkar, Q. Cappart, D. Vazquez, N. Chapados, and A. Lacoste. Workarena: How capable are web agents at solving common knowledge work tasks?, 2024. [11] A. Hochlehnert, H. Bhatnagar, V. Udandarao, S. Albanie, A. Prabhu, and M. Bethge. sober look at progress in language model reasoning: Pitfalls and paths to reproducibility, 2025. URL https://arxiv.org/abs/2504.07086. [12] J. Y. Koh and et al. Visualwebarena: Evaluating multimodal agents on realistic visually grounded web tasks. In ACL 2024, 2024. URL https://aclanthology.org/2024.acl-long.50. [13] E. Z. Liu, K. Guu, P. Pasupat, T. Shi, and P. Liang. Reinforcement learning on web interfaces using workflow-guided exploration. In International Conference on Learning Representations (ICLR), 2018. URL https://arxiv.org/abs/1802.08802. [14] Z. Liu, C. Chen, W. Li, P. Qi, T. Pang, C. Du, W. S. Lee, and M. Lin. Understanding r1-zero-like training: critical perspective, 2025. URL https://arxiv.org/abs/2503.20783. [15] S. Murty and et al. Nnetnav: Unsupervised learning of browser agents through environment interaction in the wild. arXiv preprint arXiv:2410.02907, 2025. URL https://arxiv.org/ abs/2410.02907. [16] Z. Qi, X. Liu, I. L. Iong, H. Lai, X. Sun, J. Sun, X. Yang, Y. Yang, S. Yao, W. Xu, J. Tang, and Y. Dong. WebRL: Training LLM web agents via self-evolving online curriculum reinforcement learning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=oVKEAFjEqv. [17] N. L. Roux, M. G. Bellemare, J. Lebensold, A. Bergeron, J. Greaves, A. Fréchette, C. Pelletier, E. Thibodeau-Laufer, S. Toth, and S. Work. Tapered off-policy reinforce: Stable and efficient reinforcement learning for llms. arXiv preprint arXiv:2503.14286, 2025. URL https:// arxiv.org/abs/2503.14286. [18] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Proceedings of the 13th International Conference on Neural Information Processing Systems, NIPS99, page 10571063, Cambridge, MA, USA, 1999. MIT Press. [19] M. Thakkar, T. Bolukbasi, S. Ganapathy, S. Vashishth, S. Chandar, and P. Talukdar. Selfinfluence guided data reweighting for language model pre-training. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview. net/forum?id=rXn9WO4M2p. [20] torchtune maintainers and contributors. torchtune: Pytorchs finetuning library, Apr. 2024. URL https//github.com/pytorch/torchtune. [21] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https: //arxiv.org/abs/2201.11903. [22] T. Xie, D. Zhang, J. Chen, X. Li, S. Zhao, R. Cao, T. J. Hua, Z. Cheng, D. Shin, F. Lei, Y. Liu, Y. Xu, S. Zhou, S. Savarese, C. Xiong, V. Zhong, and T. Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments, 2024. [23] F. F. Xu, Y. Song, B. Li, Y. Tang, K. Jain, M. Bao, Z. Z. Wang, X. Zhou, Z. Guo, M. Cao, M. Yang, H. Y. Lu, A. Martin, Z. Su, L. Maben, R. Mehta, W. Chi, L. Jang, Y. Xie, S. Zhou, and G. Neubig. Theagentcompany: Benchmarking llm agents on consequential real world tasks, 2024. URL https://arxiv.org/abs/2412.14161. [24] Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, T. Fan, G. Liu, L. Liu, X. Liu, H. Lin, Z. Lin, B. Ma, G. Sheng, Y. Tong, C. Zhang, M. Zhang, W. Zhang, H. Zhu, J. Zhu, J. Chen, J. Chen, C. Wang, H. Yu, W. Dai, Y. Song, X. Wei, H. Zhou, J. Liu, W.-Y. Ma, Y.-Q. Zhang, L. Yan, M. Qiao, Y. Wu, and M. Wang. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. URL https://arxiv.org/abs/2503.14476. [25] S. Zhou, F. F. Xu, H. Zhu, X. Zhou, R. Lo, A. Sridhar, X. Cheng, T. Ou, Y. Bisk, D. Fried, U. Alon, and G. Neubig. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. URL https://arxiv.org/abs/2307.13854. [26] Y. Zhou, S. Jiang, Y. Tian, J. Weston, S. Levine, S. Sukhbaatar, and X. Li. Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks, 2025. URL https://arxiv.org/abs/ 2503.15478."
        },
        {
            "title": "A Extended Learning and Saturation Analysis",
            "content": "Figure 5: Per task performance of SFT and SFT+RL agents on MiniWob++. Challenges in Agent-Environment Interaction In this section we talk about the general challenges faced by the agent to interact effectively with the environment. Observation/action space mismatch: One of the important thing to note specifically in our web environment is the observation space which the agent uses is bit different from the action space. Multiple times, the agent can see the correct action in the AxTree but the action space, the icon is not visible and to make it visible, the agent needs to scroll down and do the action. This mismatch causes huge problems [12] UI Misuse: The agent tries to interact with items in the environment in ways that it is not designed. For example, the agent trying to fill in checkbox with value True while it should just click on it. [15] Repeating actions: common issue we observed is the repetition of actions across multiple consecutive steps, often accompanied by verbose and redundant chains of thought. The agent frequently restates similar thoughts or re-executes the same actions unnecessarily, leading to inefficiencies and sometimes getting stuck in loops. [15]. Figure 6: Per task performance of SFT and SFT+RL agents on WorkArena."
        },
        {
            "title": "B Deriving Compute Cost",
            "content": "FLOPs Estimation Methodology Flop calculations are based on model architecture, token counts, and average sequence lengths observed during training and evaluation. FLOPs per Token We estimate FLOPs per token using the following formula, adapted from nvidia benchmarking[1]: FLOPsper token = (FLOPsattn + FLOPsMLP + FLOPsembed) (1 + backward multiplier) (5) Where: FLOPsattn = 12 (number of layers) (hidden size)2 (cid:18) 1 + number of query groups number of attention heads + sequence length hidden size (cid:19) FLOPsMLP = 18 (number of layers) (hidden size) FFN FLOPsembed = 6 vocabulary size (hidden size) (6) (7) (8) On-Policy FLOPs (LLaMA-8B) We compute the total FLOPs for each on-policy epoch by summing the training and testing FLOPs: FLOPstrain = Ntrain FLOPs(backward=3) FLOPstest = Ntest FLOPs(backward=0) per token per token FLOPsepoch = FLOPstrain + FLOPstest (9) (10) (11) Where Ntrain and Ntest are the number of tokens used for training and evaluation respectively. Sequence length is measured per epoch from logged metrics. Offline FLOPs (Generation: LLaMA-70B, Training: LLaMA-8B) Offline training includes two compute components: Data Generation (LLaMA-70B, forward-only): FLOPsgen = Ngen FLOPs(70B,backward=0) per token (12) 14 where Ngen = avg seq len samples per epoch (from dataset metadata). Training (LLaMA-8B, with backward pass): FLOPstrain = Ngen FLOPs(8B,backward=3) per token The total FLOPs per offline epoch is: FLOPsepoch = FLOPsgen + FLOPstrain All FLOPs values are reported in exaFLOPs by dividing the total FLOPs by 1018. (13) (14)"
        },
        {
            "title": "C Extended Related Work",
            "content": "The Reproducibility Crisis in RL. The reproducibility crisis in large language models (LLMs) and reinforcement learning (RL) has garnered increasing attention, particularly due to the reliance on single seed results that distort the perceived performance of models. The reproducibility challenge* organized every year is positive step towards addressing this. More concretely, Hochlehnert et al. [11] provide critical examination of how such practices undermine the reliability of published findings, revealing that many reported gains are sensitive to implementation choices, such as random seeds and prompt formatting [11]. Bandit-domain RLHF with LLMs. Previous work in RL for LLMs has predominantly focused on single-step tasks, which have shown effectiveness in mathematical reasoning and code generation [24, 9, 17]. While these approaches exhibit promising results, they are limited in their applicability to real-world scenarios, which often require multistep decision-making capabilities. The narrow focus on bandit-style problems fails to address the complexities inherent in tasks that demand sequential interaction, highlighting significant gap in the current research landscape. Interactive Agent Benchmarks. To assess the capabilities of LLM agents in more realistic environments, benchmarks such as WebArena [25], WorkArena [10, 5], the Agent Company [23], and OSWorld [22] have been designed to evaluate agents on multi-step tasks across various domains. These benchmarks expose the limitations of current LLM agents, revealing that while they may perform well in controlled settings, their performance in practical applications remains subpar, underscoring the need for further advancements in agent robustness and generalization to multi-step planning. Best prectices in deep RL. Building on the recognition of reproducibility challenges and unstable RL training of LLM agents, recent studies have proposed best practices for training LLM agents using RL methods. Dang and Ngo [7] recommend leveraging high quality data, balancing easy and hard problems, and controlling length generation with cosine reward. Yu et al. [24] promote higher clipping in the GRPO loss to promote diversity and avoid entropy collapse, dynamic sampling to improve training efficiency and stability, token level gradients for long CoT sequences, and overlong reward shaping to reduce reward noise. Roux et al. [17] introduce tapered variant of importance sampling to speed up learning while maintaining stable learning dynamics. The proposed method (TOPR) allows the handling of both positive and negative examples in fully offline setting. More generally, Hochlehnert et al. [11] emphasizes the need for greater methodological precision, particularly concerning decoding parameters, random seeds, prompt formatting, as well as the hardware and software frameworks, to guarantee transparent and thorough assessments of model performance. LLM Agents trained with RL on multi-step environments. Recent advancements have sought to bridge the gap in training LLM agents for multi-step environments, with approaches like WebRL [16] and SWEET-RL [26] demonstrating significant progress. WebRL employs self-evolving curriculum to address the challenges of sparse feedback and task scarcity, successfully enhancing the performance of open LLMs in web-based tasks [16]. Similarly, SWEET-RL introduces hierarchical structure that enables effective credit assignment over multiple turns, improving policy learning and generalization *https://reproml.org/ 15 in collaborative reasoning tasks [26]. These studies collectively illustrate the necessity of adapting RL techniques to accommodate the complexities of multi-step interactions, paving the way for more capable and versatile LLM agents. Test Set Hyper-Parameter Bootstrap Analysis We overall find similar results between the held-out train and test tasks with respect to optimal hyper-parameters. While we see no large deviations, we find that some parameters such as curriculum learning from the instruct model and using error logs can have larger beneficial effect on the held-out testing tasks. Figure 7: Bootstrap analysis (n = 1000 samples) of hyperparameter optimization across different SFT compute budgets on test held out tasks. Each subplot examines different hyperparameter, including increasing SFT compute: the base instruct model (left), +2.5e+18 SFT FLOPs (middle), and +7.6e+18 SFT FLOPs (right). For each hyperparameter-compute combination, the top panel shows relative reward performance with error bars indicating 95% confidence intervals, while the bottom panel displays win rates representing the percentage of bootstrap iterations where each parameter value achieved maximum performance. Results demonstrate that optimal hyperparameter values often shift as model pre-training compute increases, suggesting that hyperparameter selection should be adapted based on the computational budget allocated to SFT."
        },
        {
            "title": "E Random Search Space",
            "content": "We conduct random hyperparameter sweep over 1,370 training runs over the following parameter configurations: Decoding temperature (ρLLM): Sampled from {0.1, 0.25, 0.5, 0.75, 1} Curriculum learning: Enabled or disabled (True, False) Curriculum mean (µtarget): {0.25, 0.5, 0.75} Curriculum Temperature (ρCurr): {0.1, 0.3} Discount rate: {0.5, 0.8, 0.9, 0.95, 0.98, 1.0} Grouped-relative advantage: Enabled or disabled Zero-advantage filtering: Enabled or disabled Standard-deviation normalized advantage: Enabled or disabled Effective batch size: {64, 256, 512, 1024} Learning rate: {1e-6, 5e-7} Error log feedback: Enabled or disabled Importance ratio: Enabled or disabled"
        },
        {
            "title": "F Compute Allocation Hyperparameter Selection",
            "content": "For consistency, we select single set of hyperparameters for all runs in Figure 1. To do so, we use the bootstrap algorithm described in Algorithm 1 where we use all the runs over all SFT checkpoints as input into Results. We then analyze the results of this bootstrap and use them to obtain the reported results. The optimal parameters given by this aggregate bootstrap are: Decoding temperature ρLLM: 0.25 Curriculum learning: False Discount rate: 0.90 Grouped-relative advantage: True Zero-advantage filtering: True Standard-deviation normalized advantage: True Effective batch size: 512 Learning rate: 1e-6 Error log feedback: True Importance Ratio: False"
        }
    ],
    "affiliations": [
        "HEC Montréal",
        "McGill University",
        "MilaQuebec AI Institute",
        "Polytechnique Montréal",
        "ServiceNow Research",
        "Univeristé de Montréal"
    ]
}