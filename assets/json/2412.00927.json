{
    "paper_title": "VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation",
    "authors": [
        "Weiming Ren",
        "Huan Yang",
        "Jie Min",
        "Cong Wei",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current large multimodal models (LMMs) face significant challenges in processing and comprehending long-duration or high-resolution videos, which is mainly due to the lack of high-quality datasets. To address this issue from a data-centric perspective, we propose VISTA, a simple yet effective Video Spatiotemporal Augmentation framework that synthesizes long-duration and high-resolution video instruction-following pairs from existing video-caption datasets. VISTA spatially and temporally combines videos to create new synthetic videos with extended durations and enhanced resolutions, and subsequently produces question-answer pairs pertaining to these newly synthesized videos. Based on this paradigm, we develop seven video augmentation methods and curate VISTA-400K, a video instruction-following dataset aimed at enhancing long-duration and high-resolution video understanding. Finetuning various video LMMs on our data resulted in an average improvement of 3.3% across four challenging benchmarks for long-video understanding. Furthermore, we introduce the first comprehensive high-resolution video understanding benchmark HRVideoBench, on which our finetuned models achieve a 6.5% performance gain. These results highlight the effectiveness of our framework."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 ] . [ 1 7 2 9 0 0 . 2 1 4 2 : r VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by VIdeo SpatioTemporal Augmentation Weiming Ren1,2,3, Huan Yang3,, Jie Min1, Cong Wei1,2, Wenhu Chen1,2,* 1University of Waterloo, 2Vector Institute, 301.AI {w2ren,wenhuchen}@uwaterloo.ca, hyang@fastmail.com https://tiger-ai-lab.github.io/VISTA/"
        },
        {
            "title": "Abstract",
            "content": "Current large multimodal models (LMMs) face significant challenges in processing and comprehending long-duration or high-resolution videos, which is mainly due to the lack of high-quality datasets. To address this issue from data-centric perspective, we propose VISTA, simple yet effective VIdeo SpatioTemporal Augmentation framework that synthesizes long-duration and high-resolution video instruction-following pairs from existing video-caption datasets. VISTA spatially and temporally combines videos to create new synthetic videos with extended durations and enhanced resolutions, and subsequently produces questionanswer pairs pertaining to these newly synthesized videos. Based on this paradigm, we develop seven video augmentation methods and curate VISTA-400K, video instructionfollowing dataset aimed at enhancing long-duration and high-resolution video understanding. Finetuning various video LMMs on our data resulted in an average improvement of 3.3% across four challenging benchmarks for long-video understanding. Furthermore, we introduce the first comprehensive high-resolution video understanding benchmark HRVideoBench, on which our finetuned models achieve 6.5% performance gain. These results highlight the effectiveness of our framework. 1. Introduction Recent advancements in large language models (LLMs) and large multimodal models (LMMs) have brought transformative changes to video understanding tasks. Traditionally, video understanding relied on training task-specific models using domain-specific datasets (e.g. action recognition on Kinetics [3], video retrieval on MSR-VTT [54], and video captioning on YouCook2 [67]). In contrast, it is now possible to process video inputs and address diverse tasks using single video LMM [6, 26, 32] through instruction follow- *Corresponding authors. Figure 1. VISTA is simple but effective framework that generates high-quality video instruction data from existing video-caption pairs. Our VISTA-400K dataset enhances model performances on various long and high-resolution video benchmarks. ing. However, most current (open-sourced) video LMMs are optimized for understanding and reasoning over short, low-resolution videos. Processing long-sequence video input, such as long or high-resolution videos, remains significant challenge for video LMMs. Most efforts to enhance long-sequence video understanding have centered around designing better model architectures. Conversely, the creation of higher-quality (long/high-resolution) video instruction-following datasets remains highly under-explored. The primary challenges stem from the scarcity of publicly available (licensefriendly) high-quality video-instruction data. Consequently, open-source video instruction-following datasets often face limitations such as low resolution or short duration. For example, VideoChat2 [23] collects video instructionfollowing dataset by combining videos from multiple domain-specific datasets and rewriting the instructions using ChatGPT [34]. However, the video sources used in this dataset primarily contain short videos. ShareGPT4Video [4] curates video-instruction dataset containing 40K videos with detailed video captions. Nevertheless, the source videos are collected at sparse sampling rate of 0.15 1 fps, and the video contents often lack motion and can appear nearly static. FineVideo [8] is long video dataset featuring diverse video contents and high-quality metadata such as detailed captions and narrative progressions. While it offers satisfactory video durations, the dataset is limited by its resolution, as it predominantly comprises 360p videos. Though some concurrent studies have improved LMMs long-sequence video understanding capabilities, they often release only the model weights, hiding their video training data. For instance, Kangaroo [28]s training data contains 700K long video data, but specifics about the curation of the videos and the construction of the instructions are not provided. Similarly, Qwen2-VL [46], MiniCPM-V-2.6 [57] and Aria [19] all claim that their instruction-following dataset contains long video training data, yet without providing the sources and statistics of the video data. This lack of transparency hinders clear understanding of what types of video instruction data truly benefit long-sequence video understanding tasks and impedes further advancements in improving existing models. In this study, we propose VISTA, simple yet effective video augmentation pipeline to synthesize long and highresolution video instruction data from existing video datasets. VISTA leverages insights from image and video classification data augmentation techniques such as CutMix [59], MixUp [61] and VideoMix [60], which demonstrate that training on synthetic data created by overlaying or mixing multiple images or videos results in more robust classifiers. Similarly, our method spatially and temporally combines videos to create (artificial) augmented video samples with longer durations and higher resolutions, followed by synthesizing instruction data based on these new videos. Our data synthesis pipeline utilizes existing public videocaption datasets, making it fully open-sourced and scalable. This allows us to construct VISTA-400K, high-quality video instruction-following dataset aimed at improving the long and high-resolution video understanding capabilities of video LMMs. By finetuning various video LMMs on our dataset, we observe an average of 3.3% improvement across multiple long video understanding benchmarks. Additionally, we compile new benchmark HRVideoBench that focuses on high-resolution videos. Results on HRVideoBench (+6.5% after finetuning) demonstrate that our method produces models well-suited for high-resolution video understanding. Our contributions are summarized below: 1. We present VISTA-400K, high-quality synthetic video instruction-following dataset. VISTA-400K includes challenging QA pairs designed to enhance video LMMs ability to understand long and high-res video inputs. 2. We collect HRVideoBench, comprehensive benchmark dedicated to evaluating video LMMs capability in understanding fine object details and subtle, localized actions within high-resolution videos. 3. Our VISTA-finetuned models achieve an average gain of 3.3% on four challenging long-video benchmarks and 6.5% on HRVideoBench compared to the vanilla models. Our ablation study indicates that disabling our proposed video augmentations significantly reduces performance. 2. VISTA-400K Dataset Our goal is to create high-quality video instructionfollowing data from existing video-caption datasets. Specifically, we aim to (1) generate extended or higher-resolution videos by spatially or temporally combining existing videos, and (2) produce high-quality question-answer pairs by leveraging existing video captions. Formally, given set of candidate videos = {V1, V2, ..., VN } with captions = {C1, C2, ..., CN }, we seek to create an augmented video and synthesize QA pair (q, a): = Φ(V), (q, a) = Θ(C). (1) For the video augmentation operator Φ, we draw inspiration from data augmentation techniques used in image and video classification, such as CutMix [59] and VideoMix [60], to perform video mixing and combination. We use highly capable language model Gemini-1.5-Pro [44] as Θ to generate synthetic QA pairs. As shown in Figure 2, our dataset contains total of seven subsets, each featuring different video augmentation methods. In this section, we detail the dataset construction process for each subset. 2.1. Long Video Captioning & Event QA Our first approach to synthesizing long video instruction data is to generate longer videos by temporally concatenating multiple short clips, as illustrated by Figure 2-A. We observe that public video-text datasets like InternVid [50] and Panda-70M [5] often contain short clips that are drawn from longer videos, which can be combined to form extended sequences. To harvest long video data, we combine multiple short clips from the same source video, ensuring that the interval between them does not exceed five seconds. This preserves natural scenes and content transitions in the extended videos. Given sampled videos containing multiple short clips and their accompanying captions, we generate two types of instruction-following data using Gemini: 1. Long Video Captioning: given the captions for each short clip, we prompt Gemini to generate longer caption describing the entire video. This task is designed to enhance the summarization ability of video LMMs for input videos with longer durations. 2. Event Relationship QA: we let Gemini generate freeform or multiple choice questions related to the order of the events based on the short clip captions. Figure 2B shows an example of such instruction data. These QA pairs focus on improving video LMMs ability to recognize action and event sequences. 2 Figure 2. Our proposed video augmentation and instruction-following data synthesis schemes for VISTA-400K. Given input videos, We perform spatiotemporal video combinations to produce augmented video samples with longer duration and higher resolution. 2.2. Video Needle-in-a-haystack (NIAH) QA To effectively understand and reason over long or highresolution videos, video LMMs must learn to accurately retrieve relevant information from this long sequence of video tokens, i.e. finding needles in haystack. NIAH experiments have been widely adopted to evaluate LLMs [15] and have also been adapted to LMM evaluations [45, 48]. In this approach, we propose to spatially or temporally combine videos to form various NIAH QA data: 1. Temporal NIAH: as shown in Figure 2-C, this approach randomly inserts short clip within longer video, creating temporal needle in the sequence. Based on this combined video, our instruction data contain questions that ask video LMMs to describe the content of the needle video. This task challenges video LMMs to locate the relevant needle tokens within long video and accurately summarize their content. 2. Two Needle NIAH: we consider variant of Temporal NIAH, where short video clip is split into two parts and inserted at different timestamps within longer clip. 3 Table 1. Statistics of our synthetic video instruction-following dataset. (N) and (H) corresponds to the needle (short or low-res videos) and the haystack (long or high-res videos) in NIAH subsets. Subset Instruction Type Video Source #Videos Avg. Duration Avg. Resolution Long Video Captioning Video Captioning Event Relationship QA Freeform QA/MCQ Freeform QA/MCQ Temporal NIAH Two Needle NIAH Freeform QA Spatial NIAH Spatiotemporal NIAH HR Video Grid QA Panda-70M [5] Panda-70M [5] Panda-70M [5] (N), MiraData [14] (H) Panda-70M [5] (N), FineVideo [8] (H) Freeform QA/MCQ InternVid [50] (N), OpenVid-1M [33] (H) Freeform QA/MCQ OpenVid-1M [33] (N), FineVideo [8] (H) Freeform QA/MCQ InternVid [50] VISTA-400K - - 58,617 56,854 59,751 52,349 59,978 56,494 59,901 403, 33.2s 33.4s 67.6s 112.4s 9.9s 89.9s 3s 48.6s 1277720 1278720 640358 591382 1726971 591383 19201080 1160666 Video LMMs must summarize the short clips content by locating both needles, challenging them to retrieve relevant information from multiple temporal locations within the video sequence. 3. Spatial NIAH: recognizing local and small objects in high-resolution videos is essential for video LMMs, yet obtaining suitable training data for high-resolution videos with detailed captions or QA pairs is challenging. We propose to generate spatial NIAH data to simulate such training data. As depicted in Figure 2-E, we overlay small and low-resolution video onto high-resolution video at random position, and then ask question related to this small video. This approach focuses our QA data on specific needle region, forcing video LMMs to extract local information from high-resolution videos. 4. Spatiotemporal NIAH: finally, we generate synthetic training data by integrating spatial and temporal NIAH. As shown in Figure 2-F, we embed low-resolution, short-duration needle video within longer, highresolution video at random spatial position and timestamp. This combined setup encourages video LMMs to understand video contents across both spatial and temporal dimensions, fostering more comprehensive understanding of complex video inputs. For all four NIAH variants, we obtain freeform QA pairs by prompting Gemini using the captions of the needle videos. We further convert half of these QA pairs into multiple-choice questions by prompting Gemini to generate incorrect distractors derived from the captions of the haystack videos. This method ensures the distractor options are contextually related to the haystack content. Consequently, if the model fails to identify the needle video correctly, it is more likely to choose these distractor options, increasing the challenge and rigour of the NIAH tasks. 2.3. High-Resolution Video Grid QA In this task, we explore the idea of generating highresolution video instruction data by combining multiple low-resolution videos. From large collection of lowresolution video clips, we randomly sample 64 videos and arrange them in 8 8 grid (c.f. Figure 2-G). Each video is resized to 240 135, resulting in combined video resolution of 1920 1080. We then randomly select cell at row and column in the video grid and synthesize question about the content in this specific cell. This design enhances video LMMs ability to understand high-resolution videos by requiring it to locate the correct cell based on indices and accurately interpret the content within that small area. For MCQ data, we randomly select other cells within the grid and use their captions to create distractor options. 2.4. Summary of Video Instruction-Following Data Table 1 provides summary of our video instructionfollowing dataset. Our dataset comprises 400K entries, with each long video over 30 seconds and each highresolution video at least 960p. key advantage of our data synthesis pipeline is that our QA synthesis process (c.f. Equation 1) requires only text processing via the Gemini API, without needing Geminis multimodal functionality. This makes our approach significantly more cost-efficient compared to existing methods [4, 65]. We leverage five datasets: Panda-70M [5], MiraData [14], FineVideo [8], InternVid [50] and OpenVid-1M [33] to generate synthetic video instruction data. However, our method also applies to any raw video source by first using an off-the-shelf video captioning model to generate simple captions, followed by our pipeline to produce high-quality video instruction data. 3. Evaluation: HRVideoBench We observe that existing video understanding benchmarks are inadequate for accurately assessing the ability of video LMMs to understand high-resolution videos, especially the details inside the videos. Prior benchmarks mainly consist of low-resolution videos. More recent benchmarks focus on evaluating the long video understanding capability of video LMMs, which contain questions that typically pertain to short segment in the long video. As result, models high-resolution video understanding performance 4 Table 2. Comparisons between baseline models and VISTA-finetuned models on long/short video understanding benchmarks. The best results among open-source models are bolded. denotes the performance differences before and after finetuning on VISTA-400K. Models Size Video-MME w/o subtitles MLVU LVBench LongVideoBench MVBench NExT-QA Long Video Understanding Short Video Understanding avg short medium long m-avg test GPT-4V [1] GPT-4o [35] Gemini-1.5-Pro [44] VideoChat2 [23] LLaMA-VID [25] ST-LLM [29] ShareGPT4Video [4] LongVILA [55] LongLLaVA [49] Video-XL [41] - - - 7B 7B 7B 7B 7B 7B 7B 59.9 71.9 75.0 39.5 - 37.9 39.9 50.5 52.9 55.5 70.5 80.0 81.7 48.3 - 45.7 48.3 61.8 61.9 64.0 VideoLLaVA [26] 7B VISTA-VideoLLaVA 7B - VideoLLaVA Mantis-Idefics2 [13] VISTA-Mantis - Mantis-Idefics LongVA [63] VISTA-LongVA - LongVA 8B 8B 7B 7B 45.3 39.9 43.7 48.2 +3.8 +2.9 55.9 45.4 48.2 58.4 +2.8 +2.5 61.4 52.4 55.5 66.0 +3.1 +4. Proprietary Models 49.2 53.5 64.6 65.3 - 67.4 - 34.7 33.1 Open-source Models 47.9 33.2 33.2 - 31.3 - 46.4 35.0 - 46.2 45.4 - 64.9 49.2 - 23.9 - - - - - 36.2 38.9 +2.7 37.2 39.6 +2. 45.0 47.4 +2.4 47.3 49.5 +2.2 49.4 55.5 +6.1 56.3 62.1 +5.8 29.3 33.8 +4.5 35.0 36.4 +1. 35.9 39.0 +3.1 55.8 70.3 74.3 37.0 - 36.8 36.3 50.4 51.4 53.2 38.0 43.9 +5.9 43.0 46.7 +3.7 50.9 53.1 +2. val 59.1 66.7 64.0 39.3 - - 39.7 - - 49.5 39.9 42.3 +2.4 45.8 49.1 +3.3 51.8 53.1 +1. test 43.5 - - 51.9 41.3 54.9 51.2 - 54.6 55.3 43.8 47.2 +3.4 51.4 52.5 +1.1 49.2 51.1 +1. mc - 76.0 - 78.6 - - - - - 77.2 61.8 63.0 +1.2 75.8 75.2 -0.6 68.3 69.3 +1. can be undermined if it struggles to sample or retrieve the relevant frames from lengthy video sequence. To address this gap, we introduce HRVideoBench, comprehensive benchmark with 200 multiple-choice questions designed to assess video LMMs for high-resolution video understanding. HRVideoBench focuses on the perception and understanding of small regions and subtle actions in the video. Our test videos are at least 1080p and contain 10 different video types collected with real-world applications in mind. For example, key applications of high-resolution video understanding include autonomous driving and video surveillance. We correspondingly collect POV driving videos and CCTV footage for the benchmark. Our benchmark consists of 10 types of questions, all of which are manually annotated and can be broadly categorized into object and action-related tasks. Further details can be found in the Appendix. The question types are: Object-related Tasks: Object Counting, OCR problem, Object Recognition, Entity Recognition, Object Property Recognition, Object Status Change Recognition. Action-related Tasks: Action Recognition, Moving Direction Identification, Interaction Detection, Temporal Sequence Recognition. 4. Experimental Results 4.1. Evaluation Setup To validate the effectiveness of VISTA-400K, we finetune diverse set of LMMs on our dataset. Specifically, we choose VideoLLaVA [26], Mantis-Idefics2 [13] and LongVA [63] as the base models because these models disclose details about their training dataset. Further details of these models can be found in the Appendix. We directly finetune LongVA and Mantis-Idefics2 on our dataset as they are mainly pretrained on image data. For VideoLLaVA, we additionally include 300K short video from VideoChat2-IT [23] to preserve the models short video understanding capability. We use 8 frames for VideoLLaVA, 24 frames for Mantis-Idefics2, and 64 frames for LongVA for finetuning and evaluation. The input resolution is fixed at 224224 for VideoLLaVA and 336336 for LongVA, while MantisIdefics2 supports dynamic resolutions up to 980980. The finetuned model is denoted as VISTA-[base model]. Our evaluation assesses video LMMs capabilities in long and high-resolution video understanding. We test the finetuned models on four comprehensive long video understanding benchmarks: Video-MME [10], 5 Table 3. Quantitative results on HRVideoBench and open-ended video QA benchmarks. acc. represents accuracy. High-Res Video Understanding Open-Ended Video QA Models HRVideoBench MSVD-QA MSRVTT-QA TGIF-QA ActivityNet-QA avg object VideoLLaVA [26] VISTA-VideoLLaVA - VideoLLaVA 32.5 47.5 +15.0 36.0 50.0 +14.0 Mantis-Idefics2 [13] VISTA-Mantis - Mantis LongVA [63] VISTA-LongVA - LongVA 48.5 51.0 +2.5 48.0 50.0 +2.0 50.9 53.5 +2.6 52.6 56.1 +3.5 action 27.9 44.2 +16.3 45.4 47.7 +2.3 41.9 41.9 +0.0 acc. score acc. score acc. score 60.3 71.5 +11.2 57.4 65.2 +7.8 56.3 61.0 +4. 3.7 4.0 +0.3 3.5 3.8 +0.3 3.5 3.7 +0.2 42.1 58.5 +16.4 34.9 46.4 +11.5 37.7 42.5 +4. 3.0 3.5 +0.5 2.7 3.1 +0.4 2.8 3.0 +0.2 63.5 78.0 +14.5 65.7 71.4 +5.7 55.4 67.5 +12. 3.8 4.3 +0.5 3.8 4.0 +0.2 3.4 3.9 +0.5 acc. 48.6 49.1 +0.5 46.5 48.8 +2. 48.0 51.8 +3.8 score 3.3 3.4 +0.1 3.1 3.3 +0.2 3.2 3.4 +0.2 MLVU [66], LVBench [47] and LongVideoBench [51]. For high-resolution video understanding, we utilize our HRVideoBench. Additionally, we report results on prior short video understanding benchmarks including MVBench [23] and NExT-QA [52], as well as open-ended video QA benchmarks such as MSVD-QA [53], MSRVTT-QA [53], TGIF-QA [12] and ActivityNet-QA [58]. 4.2. Quantitative Results Long Video Understanding Quantitative results for long video understanding are shown in Table 2. All three models demonstrate consistent performance boost across the evaluation benchmarks, with average improvements of 3.3% for Video-MME, 4.7% for MLVU, 3.0% for LVBench, and 2.3% for LongVideoBench. For Video-MME, our method yields the largest average improvement for medium-length questions, which correspond to videos ranging from 4 to 15 minutes. Given that most of the augmented videos in our dataset are under 3 minutes, this result suggests that our approach generalizes effectively to longer videos. One possible explanation for this observation is that longer videos often contain more scene cuts and transitions, which is aligned with our synthetic data where multiple clips with different scenes are concatenated. Our VISTA-LongVA achieves state-of-the-art performance on Video-MME, LVBench and LongVideoBench among open-source models, demonstrating the effectiveness of our approach. High-Resolution Video Understanding We present the HRVideoBench evaluation results for VISTA-finetuned models and the baseline models in Table 3. Among the baseline models, we observe that Mantis-Idefics2, with its higher input video resolution, outperforms LongVA on both object and action-related tasks, despite being considered weaker for long video understanding. This suggests that our HRVideoBench requires detailed comprehension of high-resolution videos, and simply adding more input frames during testing does not necessarily lead to enhanced model performance. Overall, we find that finetuning on our dataset enhances the high-resolution video understanding capabilities for all three models. Additionally, we observe that recognizing subtle actions remains more challenging than identifying objects and their properties, suggesting that there remains significant potential for improvement in highresolution video understanding for video LMMs. Short Video Understanding We further validate our proposed method on several prior short video understanding benchmarks. According to Table 2, all three VISTAfinetuned models show improvement on both MVBench and NExT-QA benchmarks except Mantis. This result is likely due to NExT-QAs training split being included as part of the in-domain training data for Mantis-Idefics2. Consequently, finetuning on our out-of-domain data causes slight decrease in performance. While our results are slightly worse than Video-XL [41], we note that their pretraining and instruction-tuning datasets are larger (>2.7M training samples). Here, we focus more on the relative improvement of our method over the baseline models. Since the benchmarks mentioned above consist solely of multiple-choice questions, we also include several openended QA benchmarks to verify the text generation capabilities of the finetuned models. Following Video-ChatGPT [32], we use GPT-3.5-Turbo [34] to evaluate the accuracy of the responses and rate their quality on scale of 1 to 5. As shown in Table 3, finetuning on our dataset improves both accuracy and generation quality across all four benchmarks for the VISTA-finetuned models. This demonstrates that our dataset can enhance the generation quality of LMMs. 4.3. Ablation Study To verify that each subset in VISTA-400K contributes to the performance gain of long or high-resolution video understanding, we conduct an ablation study and train sev6 Table 4. Ablation study results for VISTA-Mantis. Each w/o [Subset] denotes Mantis-Idefics2 model finetuned on modified VISTA-400K by replacing the corresponding subset with the same amount of training examples from VideoChat2-IT [23]. Models Video-MME HRVideoBench w/o sub. avg VISTA-Mantis w/o Long Video Captioning w/o Event Relationship QA w/o Temporal NIAH w/o Two Needle NIAH w/o Spatial NIAH w/o Spatiotemporal NIAH w/o HR Video Grid QA w/o Video Augmentation 48.2 47.9 47.7 47.5 48.1 47.2 47.7 47. 45.7 avg 51.0 48.0 49.5 48.0 50.5 47.5 50.0 48.0 44.5 eral Mantis-Idefics2 models by excluding each subset from the training data. In addition, we add the same amount of video training data from VideoChat2-IT [23] to each of the ablation experiments to ensure the total number of training examples does not change. The results on VideoMME and HRVideoBench are shown in Table 4. We observe that excluding the long video subsets decreases the model performances on Video-MME. Similarly, excluding high-resolution video subsets leads to drop in scores on HRVideoBench. These observations indicate that all seven video augmentation methods are effective at their corresponding tasks. We also report results for Mantis-Idefics2 model without our proposed video augmentations by using the original videos directly for training before combining them with others (e.g., using the needle video in NIAH tasks to train the model directly). According to the last row in Table 4, this yields very low scores on Video-MME and HRVideoBench, highlighting the necessity of our augmentations to enhance video instruction data quality. Furthermore, previous studies [20, 63] have shown that training on high-resolution image data yields strong long video understanding models. Our ablation study supports this conclusion and demonstrates that the proposed highresolution video augmentation methods enhance the performances of long video understanding tasks. In addition, our results indicate that the reverse is also true: our long video augmentation methods also improve the performance of high-resolution video understanding. We believe this transferability between long and high-resolution video understanding capabilities is because both types of inputs convert to long sequence of video tokens in video LMMs embedding space. As result, identifying short moments from long video and recognizing small regions in highresolution video both correspond to the task of finding key information needles in haystack of video tokens. 4.4. Case Study To qualitatively evaluate our method, we compare the generated responses of VISTA-finetuned models with the baseline models, as illustrated in Figure 3. The helicopter example involves recognizing event orders within long video sequence. The baseline models LongVA and VideoLLaVA exhibit hallucination issues, while our VISTALongVA describes the action accurately. Due to the sparse sampling rate of VideoLLaVA (8 frames per video), VISTAVideoLLaVA misses frames related to the action immediately following the helicopter takes off. Nevertheless, it successfully describes the later event, the helicopter crashes onto rooftop, which the vanilla VideoLLaVA fails to recognize. The table tennis example challenges video LMMs to identify subtle and unusual action within high-resolution video. As demonstrated in the second part of Figure 3, Mantis-Idefics2 fails to recognize this action, instead generating response based on common knowledge about table tennis. Conversely, VISTA-Mantis correctly identifies the unusual action that the player hits the table tennis ball with his leg. 5. Related Work LMMs for Video Understanding Recent research has advanced the ability of large multimodal models (LMMs) to process video inputs and generate natural language responses. Earlier works on video LMMs such as VideoChatGPT [32] and Video-LLaVA [26] employ video encoders that are restricted to limited number of low-resolution frames (e.g. 224 224, eight frames). To enable the processing of longer and higher-resolution videos, several studies [9, 22, 23, 25, 28, 38, 42, 62] have developed token compression and pooling methods to manage the sequence length of video tokens. Video-LLaMA [62], VideoChat [22] and VideoChat2 [23] utilize Q-Former [21] in the visual encoder to compress video tokens into few learnable queries. Video-CCAM [9] further applies causal crossattention masks within the video Q-Former to better capture the temporal order of the video frames. In addition to using Q-Former, VideoLLaMA 2 [6] designs 3D convolution-based STC Connector for efficient token compression. LongVA [63] alternatively increases the maximum number of tokens the model can receive by training long-context LLM with 128K context length, allowing the model to handle longer video sequences. Video Language Datasets Previous efforts on building video-text datasets have typically focused on specialized domain, such as video retrieval [2, 17, 54], video question answering [52, 53, 58] and action recognition [3, 11, 43]. VideoChat2 [23] creates large-scale video instruction tuning dataset by combining multiple specialized datasets and rewriting instructions using ChatGPT [34]. However, the 7 Figure 3. Qualitative comparisons between the baseline models and our VISTA-finetuned models. Red text indicates hallucinations or incorrect responses, while green text highlights the correct responses that correspond accurately to the video content. quality of the videos is suboptimal. More recently, LLaVAHound [64] and ShareGPT4Video [4] generate high-quality video captioning and open-ended QA data using GPT-4V [1]. Nonetheless, the video sources in these datasets often have limited motion degree and variety. concurrent study to ours is the LLaVA-Video-178K dataset [65], which contains 178K high-quality video data and 1.3M instruction-following samples annotated by GPT4o [35]. However, this dataset relies on costly annotation process that involves feeding in video frames to GPT-4o at one fps and generating three levels of video descriptions. In contrast, our work emphasizes the synthesis of high-quality video data from existing short or low-resolution videos, offering more scalable and cost-effective solution for generating synthetic video language data. Video Understanding Benchmarks The advancement of video LMMs has also driven the creation of various video understanding benchmarks. MVBench [23] introduces comprehensive video LMM benchmark by regenerating question-answer pairs based on various existing benchmarks. TempCompass [30] and VideoVista [24] evaluate the temporal reasoning and understanding capabilities of video LMMs. Recently, new benchmarks have emerged to assess video LMMs ability to comprehend extremely long videos, including Video-MME [10], MLVU [66], LVBench [47] and LongVideoBench [51]. Despite these advancements, we note that no existing benchmark specifically evaluates video LMMs ability to understand high-resolution videos, which is crucial task in video understanding that has significant applications in fields such as autonomous driving and sports analysis. In this study, we aim to address this gap by collecting comprehensive benchmark for high-resolution video understanding targeting small objects and localized actions within videos. 6. Conclusion We presented VISTA, video augmentation framework to enhance long-duration and high-resolution video understanding by generating high-quality synthetic video instruction-following data from existing video-caption datasets. VISTA performs spatiotemporal video combination over input videos to create new video samples and utilizes video captions for instruction synthesis. Through extensive experiments on various long video understanding benchmarks and our proposed HRVideoBench, we demonstrated that VISTA-400K consistently improves the performance of three different video LMMs. For future work, we aim to design and explore additional video augmentation methods to further strengthen our approachs robustness."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 5, 8 [2] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moIn Proceedings of ments in video with natural language. the IEEE international conference on computer vision, pages 58035812, 2017. 7 [3] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 62996308, 2017. 1, 7 [4] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understandarXiv preprint ing and generation with better captions. arXiv:2406.04325, 2024. 1, 4, 5, 8 [5] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple In Proceedings of the IEEE/CVF cross-modality teachers. Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. 2, 4 [6] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 1, [7] Tri Dao. Flashattention-2: Faster attention with betarXiv preprint ter parallelism and work partitioning. arXiv:2307.08691, 2023. 1 [8] Miquel Farre, Andi Marafioti, Lewis Tunstall, Leandro https : Von Werra, and Thomas Wolf. //huggingface.co/datasets/HuggingFaceFV/ finevideo, 2024. 2, 4 Finevideo. [9] Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. Video-ccam: Enhancing video-language understanding with causal cross-attention masks for short and long videos. arXiv preprint arXiv:2408.14023, 2024. 7 [10] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 5, 8, 1 [11] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The something something video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017. visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 27582766, 2017. 6, 2 [13] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. 5, 6, 1 [14] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. arXiv preprint arXiv:2407.06358, 2024. 4 [15] Garrett Llmtest needleinahaystack. https : / / github . com / gkamradt / LLMTest _ NeedleInAHaystack, 2024. Accessed: 2024-10-24. 3 Kamradt. [16] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 1 [17] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706715, 2017. 7 [18] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024. 1 [19] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-ofexperts model. arXiv preprint arXiv:2410.05993, 2024. 2 [20] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 7 [21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. [22] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 7 [23] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 1, 5, 6, 7, 8, 2 [24] Yunxin Li, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, and Min Zhang. Videovista: versatile benchmark for video understanding and reasoning. arXiv preprint arXiv:2406.11303, 2024. 8 [25] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pages 323340. Springer, 2025. 5, 7 [12] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in [26] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represen9 tation by alignment before projection. arXiv:2311.10122, 2023. 1, 5, 6, 7 arXiv preprint [27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 1 [28] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. Kangaroo: powerful video-language model supporting long-context video input. arXiv preprint arXiv:2408.15542, 2024. 2, 7 [29] Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li St-llm. Large language models are effective temporal learners. arXiv preprint arXiv:2404.00308, 2024. 5 [30] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. [31] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Da Li, Pengcheng Lu, Tao Wang, Linmei Hu, Minghui Qiu, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207, 2023. 1 [32] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 1, 6, 7, 2 [33] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-tovideo generation. arXiv preprint arXiv:2407.02371, 2024. 4 [34] OpenAI. Chatgpt. https://openai.com/index/ chatgpt/, 2023. 1, 6, 7 [35] OpenAI. Gpt-4o. https://openai.com/index/ hello-gpt-4o/, 2024. 5, 8 [36] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems, 24, 2011. 1 [37] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training In SC20: International Confertrillion parameter models. ence for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. 1 [38] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1431314323, 2024. 7 [39] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 1 [40] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25562565, 2018. [41] Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie Zhou, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. arXiv preprint arXiv:2409.14485, 2024. 5, 6 [42] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. 7 [43] Soomro. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 7 [44] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2, 5 [45] Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, and Hao Wang. Multimodal needle in haystack: Benchmarking long-context capability of multimodal large language models. arXiv preprint arXiv:2406.11230, 2024. 3 [46] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2 [47] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. 6, 8, 2 [48] Weiyun Wang, Shuibo Zhang, Yiming Ren, Yuchen Duan, Tiantong Li, Shuo Liu, Mengkang Hu, Zhe Chen, Kaipeng Zhang, Lewei Lu, et al. Needle in multimodal haystack. arXiv preprint arXiv:2406.07230, 2024. [49] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. Longllava: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture. arXiv preprint arXiv:2409.02889, 2024. 5 [50] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 2, 4 [51] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interarXiv preprint leaved video-language understanding. arXiv:2407.15754, 2024. 6, 8, 2 [52] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining 10 [66] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 6, 8, [67] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018. 1 In Proceedings of the IEEE/CVF contemporal actions. ference on computer vision and pattern recognition, pages 97779786, 2021. 6, 7, 2 [53] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 16451653, 2017. 6, 7, 2 [54] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52885296, 2016. 1, 7 [55] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context arXiv preprint visual language models for long videos. arXiv:2408.10188, 2024. 5 [56] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 1 [57] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [58] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 91279134, 2019. 6, 7, 2 [59] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 60236032, 2019. 2 [60] Sangdoo Yun, Seong Joon Oh, Byeongho Heo, DongyVideomix: Rethinking oon Han, and Jinhyung Kim. data augmentation for video classification. arXiv preprint arXiv:2012.03457, 2020. 2 [61] Hongyi Zhang. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. 2 [62] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 7 [63] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 5, 6, 7, 1 [64] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language model reward. arXiv preprint arXiv:2404.01258, 2024. [65] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 4, 8 11 VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by VIdeo SpatioTemporal Augmentation"
        },
        {
            "title": "Supplementary Material",
            "content": "7. HRVideoBench Details As detailed in Section 3, our HRVideoBench consists of 200 questions covering 10 question types and 10 video types. That is, we collect two questions for each combination of video type and question type. The 10 video types are: POV driving videos Egocentric sports videos Sportscast videos (broadcasting of sports events) Public event recordings Surveillance camera/CCTV footage Wildlife stock videos Aerial videos/Drone videos Factory and industrial stock videos Public transport videos Product review videos For each question in the benchmark, we ensure the video duration falls between 3 to 10 seconds. This relatively short duration is chosen to maximize the likelihood of the frames relevant to the question getting sampled by the models. The final dataset has an average video duration of 5.4 seconds and an average resolution of 30481699. Example questions and answers from our HRVideoBench are shown in Figure 4. 8. Model Training and Evaluation Details mance on various multi-image benchmarks and excels on short video understanding benchmarks, such as MVBench [23]. LongVA [63] is long-context LMM designed for understanding long video content. It first performs continual pretraining using Qwen2 [56] model to support up to 224K context length. Following this, it uses this modified Qwen2 model as the backbone for visual instruction tuning. LongVA is instruction-tuned on pure image data, using the same training data as LLaVA-1.6 [27]. It introduces the UniRes strategy, which divides an image into multiple grids and encodes each grid independently using the vision encoder. During inference, these grids are replaced by different frames from the input video, enabling effective processing of long video sequences. 8.2. Additional Implementation Details For all three models, we conduct full-finetuning for one epoch using 8 Nvidia H800 GPUs. The total training time for 400K data is around one day. We use the Adam [16] optimizer with batch size of 128 during training. The learning rate is set to 5e-6 for VideoLLaVA and 1e-7 for LongVA and Mantis-Idefics2, with cosine learning rate scheduler and warm-up ratio of 0.03 applied to all models. We employ Flash-Attention 2 [7] and DeepSpeed ZeRO-3 [37] to accelerate training. In this section, we provide additional details for training and benchmarking our selected baseline models. 8.3. Evaluation Benchmarks 8.1. Baseline Models VideoLLaVA [26] is video LMM jointly pretrained on image and video data. It uses the pretrained Vicuna v1.5 model as its LLM backbone and LanguageBind as its image and video encoder. The model is pretrained on 558K imagetext pairs from LAION-CC-SBU [36, 39, 40] and 702K video-text pairs from Valley [31]. During the instructiontuning stage, it incorporates 665K image-text pairs from LLaVA-1.5 [27] and 100K video-text pairs from VideoChatGPT [32]. Mantis-Idefics2 [13] is an LMM specialized in processing It is initialized inputs with multiple interleaved images. from Idefics2 [18] and continually pretrained on MantisInstruct, dataset comprising 721K interleaved image-text instruction-tuning examples. This dataset focuses on enhancing multi-image understanding across four dimensions: co-reference, comparison, reasoning, and temporal understanding. Mantis-Idefics2 achieves state-of-the-art perforVideo-MME [10] is comprehensive benchmark designed to evaluate the video analysis capabilities of LMMs. It includes 900 videos and 2700 questions across six visual domains. The questions are categorized based on video durations into short, medium, and long video questions, with median durations of 26s, 164.7s, and 890.7s, respectively. The median duration values for short, medium and long video questions are 26s, 164.7s, and 890.7s, respectively. Video-MME supports two evaluation formats: (1) the w/ subtitle format, which includes both the video subtitles and questions as text inputs, and (2) the w/o subtitle format, which uses only the raw video and questions as inputs. In the main paper, we focus on the w/o subtitle format to emphasize improving the long video understanding capabilities of video LMMs through video augmentation, rather than relying on additional subtitle information. For completeness, we provide results for the w/ subtitle format in Section 9. MLVU [66] is long video understanding benchmark en1 Figure 4. Example questions from our HRVideoBench. Zoom in for better visualizations. compassing diverse tasks and video genres. It features two types of questions: multiple-choice questions and freeform generation questions. The benchmark evaluates LMMs across three dimensions: holistic video understanding, requiring global information from the entire video; singledetail video understanding, focused on short and salient moments within the video; and multi-detail video understanding, involving connections across multiple short clips in the video. In this paper, we report the accuracy scores for the multiple-choice questions from the development set of MLVU. In the paper, we report the accuracy scores for the multiple-choice questions from the dev set of MLVU. LVBench [47] evaluates the comprehension capabilities of video LMMs for extremely long videos. It consists of 1549 QA pairs, with an average video duration of 4101 seconds. The benchmark assesses video LMMs across six core aspects: temporal grounding, video summarization, video reasoning, entity recognition, event understanding, and key information retrieval. We use the full test set for evaluation. LongVideoBench [51] is question-answering benchmark featuring interleaved long video-text input. The dataset contains 3763 videos and 6678 human-annotated multiple-choice questions spanning 17 fine-grained categories. LongVideoBench supports two evaluation formats: the standard input format, where video tokens are processed first followed by question descriptions, and an interleaved video-text format, where subtitles are inserted between video frames. Although Mantis-Idefics2 supports interleaved image-text input, as our VISTA-400K does not include training examples in such format, we still evaluate Mantis-Idefics2 and the finetuned VISTA-Mantis using the standard format. We report the results of the validation split. MVBench and NExT-QA [23, 52] are short video understanding benchmarks, focusing on videos under one minute in duration. MVBench includes 4,000 multiple-choice questions derived from 3,641 video clips, with an average video duration of 16 seconds. NExT-QA comprises 8,564 questions (both multiple-choice and open-ended) sourced from 1,000 videos, averaging 40 seconds in length. In our experiments, we evaluate the models on the full MVBench dataset and the MCQ split of NExT-QA. MSVD-QA, MSRVTT-QA, TGIF-QA and ActivityNetQA [12, 53, 58] are open-ended QA benchmarks designed to evaluate the response generation capabilities of video LMMs. These benchmarks consist of short videos and assess the ability of video LMMs to produce simple, coherent answers. For all four benchmarks, we follow VideoChatGPT [32] and use GPT-3.5-Turbo to evaluate the accuracy and quality of the responses. Specifically, GPT is prompted with the ground truth answer and the models response to determine if the answer is correct (yes/no) and to assign quality score between 1 and 5. Following VideoChatGPT, we evaluate the models on the validation sets of MSVD-QA, MSRVTT-QA and ActivityNet-QA, and use the FrameQA split from TGIF-QAs test set for evaluation. Since GPT-3.5-Turbos API version has changed and the Table 5. Comparison between the baseline VideoLLaVA model, VideoLLaVA finetuned on VISTA-400K and VideoLLaVA finetuned on VISTA-400K + 300K VideoChat2-IT data (VISTA-VideoLLaVA in the main paper) on long video understanding benchmarks. SFT indicates supervised finetuning. Models VideoLLaVA VideoLLaVA (SFT on VISTA-400K) - VideoLLaVA VideoLLaVA (SFT on VISTA-400K + 300K VideoChat2-IT) 43.7 - VideoLLaVA (SFT on VISTA-400K) 48.2 +0.1 +0. Long Video Understanding Video-MME w/o subtitles MLVU LVBench LongVideoBench avg short medium long m-avg 39.9 45. 47.3 43.6 +3.7 +2.0 38.0 43.8 +5.8 43.9 +0.1 36.2 39.8 +3. 38.9 -0.9 45.0 48.7 +3.7 49.5 +0.8 test 29. 32.6 +3.3 33.8 +1.2 val 39.1 41.0 +1.9 42.3 +1. Table 6. Comparison between the baseline VideoLLaVA model, VideoLLaVA finetuned on VISTA-400K and VideoLLaVA finetuned on VISTA-400K + 300K VideoChat2-IT data (VISTA-VideoLLaVA in the main paper) on HRVideoBench. SFT indicates supervised finetuning."
        },
        {
            "title": "VideoLLaVA",
            "content": "VideoLLaVA (SFT on VISTA-400K) - VideoLLaVA VideoLLaVA (SFT on VISTA-400K + 300K VideoChat2-IT) - VideoLLaVA (SFT on VISTA-400K) High-Resolution Video Understanding"
        },
        {
            "title": "HRVideoBench",
            "content": "avg 32.5 44.0 +11.5 47.5 +3.5 object 36. 42.1 +6.1 50 +7.9 action 27.9 46.5 +18.6 44.2 -2. older API versions are no longer accessible, we are unable to reproduce the results for some baseline models. In the paper, we report all scores based on our evaluation script. 9. Additional Experimental Results 9.1. Training Data Ablations for VideoLLaVA As mentioned in Section 4.1, unlike Mantis-Idefics2 and LongVA, we fine-tune VideoLLaVA using combination of our VISTA-400K and 300K short video samples from VideoChat2-IT to preserve its short video understanding capabilities. In this section, we examine how this additional training data impacts the models performance on long and high-resolution video understanding tasks after finetuning. To assess this, we finetune another VideoLLaVA model exclusively on our VISTA-400K and compare the results against the combined training approach in Table 5 and Table 6. As shown in Table 5, finetuning VideoLLaVA exclusively on our VISTA-400K results in consistent improvements across all long video understanding benchmarks. On the other hand, incorporating an additional 300K short video samples does not yield further significant gains in long video understanding. Notably, the Video-MME results indicate that adding short video data slightly detracts from the models performance on long videos, underscoring the importance of our dataset for enhancing long video understanding capabilities. For high-resolution video understanding, according to Table 6, finetuning VideoLLaVA on our data leads to significant improvement (+11.5%) on HRVideoBench. While adding additional short video data further enhances model performance, the improvement is less substantial. These findings suggest that our dataset remains the primary driver of performance gains in high-resolution video understanding. Moreover, incorporating VideoChat2-IT training data leads to decline in performance on action-related questions, highlighting the superior effectiveness of our dataset for tasks requiring temporal understanding. 9.2. Video-MME w/ Subtitles Results We show the results for Video-MME w/ subtitles in Table 7. In this evaluation setting, the videos subtitles are provided as part of the question input to the model. The results indicate that both baseline models and our VISTAfinetuned models can be further enhanced by providing extra subtitle information. Similar to Video-MME w/o subtitles results, our VISTA-finetuned models consistently 3 Table 7. Comparison between VISTA-finetuned models and baseline models on Video-MME w/ subtitle benchmark."
        },
        {
            "title": "Freeform QA Generation Prompt",
            "content": "Models VideoLLaVA VISTA-VideoLLaVA - VideoLLaVA Mantis-Idefics2 VISTA-Mantis - Mantis-Idefics2 LongVA VISTA-LongVA - LongVA Video-MME w/ subtitles avg short medium 41.6 45.1 +3.5 49.0 50.9 +1.9 54.3 59.3 +5.0 46.1 50.2 +4.1 60.4 61.8 +1. 61.6 70.0 +8.4 40.7 45.7 +5.0 46.1 48.6 +2.5 53.6 57.6 +4.0 long 38.1 39.3 +1. 40.3 42.3 +2.0 47.6 50.3 +2.7 achieve better performances compared to the baseline models. This shows that our synthetic data provides consistent and model-agnostic enhancements to the long video understanding capability of video LMMs. 10. Limitations Our method exhibits few limitations. First, since we generate instruction data based on video captions, and most public video-caption datasets contain simple captions for video clips, our synthesized data often contain short responses, leading to shorter response from the finetuned models. This issue could be addressed by recaptioning the raw video data using high-capacity video captioning models. Second, while our synthesized augmented video data have been shown to enhance long and high-resolution video understanding, the current video augmentation paradigm does not fully align with real-world video distributions. Addressing this limitation would require more advanced video combination and blending techniques, such as leveraging segmentation maps to isolate specific regions from one video and seamlessly integrating them into another to create more natural and realistic augmented video samples. 11. Instruction Synthesis Prompt Templates In this section, we list the Gemini prompts we used to synthesize instruction data below. User: Given short paragraph of caption describing video clip, can you try to extract relevant information from the caption and come up with question-answer pair that could possibly reflect the facts of some local and fine-grained scenes in the video? The caption of the video is as follows: <Video Caption>"
        },
        {
            "title": "Please also note that",
            "content": "Please try not to come up with questions that the you cannot answer. caption will not be presented in the actual training data. Return only the question and the answer. Format your output as: ###Question### <your question> ###Answer### <your answer> Assistant: <Synthesized Freeform QA pairs>"
        },
        {
            "title": "MCQ Generation Prompt",
            "content": "User: Given the following Question-Answer pair, turn this short answer question into multiple-choice question by synthesizing three additional incorrect options. Assume the correct option is <Random Option between to D>. Question: <Question> Answer: <Freeform Answer> Your output should be in the format of python list: [ A. <answer1>, B. <answer2>, C. <answer3>, D. <answer4> ] Assistant: <Synthesized MCQ pairs>"
        },
        {
            "title": "Event Relationship QA Generation Prompt",
            "content": "User: Given multiple short captions, each representing short chunk of video in longer video, generate question-answer pair related to the order of the events in the video. Note that because the short captions are from the same video, you can combine in entities with slightly different descriptions different captions, as they most likely represent the same thing. Format the output using the following format: ###Question### <Your question> ###Answer### <Your answer> For example, given captions like: Caption 1: squirrel is sitting on tree branch in forest, surrounded by pine trees and blue sky. Caption 2: cartoon squirrel is holding an egg in tree. Caption 3: cartoon squirrel is standing next to an egg. Your output can be: ###Question### What happens after the squirrel sits on tree branch? ###Answer### The squirrel holds an egg."
        },
        {
            "title": "Long Video Caption Generation Prompt",
            "content": "User: Given multiple short captions, each representing short chunk of video in longer video, create detailed caption by combining the short captions such that the detailed caption describes the whole video. Note that because the short captions are from the same video, you can combine entities with slightly different descriptions in different captions, as they most likely represent the same thing. Return only the caption. (in chronological order) captions The short are listed below: Caption 1. <Caption 1> Caption 2. <Caption 2> ... Caption N. <Caption N> Try to be creative with your question and answer. Assistant: <Synthesized Long Video Caption> (in chronological order) captions short The are listed below: Caption 1. <Caption 1> Caption 2. <Caption 2> ... Caption N. <Caption N> Assistant: <Synthesized Event Relationship QA pairs>"
        }
    ],
    "affiliations": [
        "University of Waterloo",
        "Vector Institute"
    ]
}