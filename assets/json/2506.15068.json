{
    "paper_title": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation",
    "authors": [
        "Zongxia Li",
        "Yapei Chang",
        "Yuhang Zhou",
        "Xiyang Wu",
        "Zichao Liang",
        "Yoo Yeon Sung",
        "Jordan Lee Boyd-Graber"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluating open-ended long-form generation is challenging because it is hard to define what clearly separates good from bad outputs. Existing methods often miss key aspects like coherence, style, or relevance, or are biased by pretraining data, making open-ended long-form evaluation an underexplored problem. To address this gap, we propose PrefBERT, a scoring model for evaluating open-ended long-form generation in GRPO and guiding its training with distinct rewards for good and bad outputs. Trained on two response evaluation datasets with diverse long-form styles and Likert-rated quality, PrefBERT effectively supports GRPO by offering better semantic reward feedback than traditional metrics ROUGE-L and BERTScore do. Through comprehensive evaluations, including LLM-as-a-judge, human ratings, and qualitative analysis, we show that PrefBERT, trained on multi-sentence and paragraph-length responses, remains reliable across varied long passages and aligns well with the verifiable rewards GRPO needs. Human evaluations confirm that using PrefBERT as the reward signal to train policy models yields responses better aligned with human preferences than those trained with traditional metrics. Our code is available at https://github.com/zli12321/long_form_rl."
        },
        {
            "title": "Start",
            "content": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation"
        },
        {
            "title": "Yoo Yeon Sung",
            "content": "Jordan Lee Boyd-Graber University of Maryland, College Park {zli12321, yapeic, yuhang, wuxiyang, zliang14, yysung53, jbg}@umd.edu"
        },
        {
            "title": "Abstract",
            "content": "Evaluating open-ended long-form generation is challenging because it is hard to define what clearly separates good from bad outputs. Existing methods often miss key aspects like coherence, style, or relevance, or are biased by pretraining data, making open-ended long-form evaluation an underexplored problem. To address this gap, we propose PrefBERT, scoring model for evaluating open-ended long-form generation in GRPO and guiding its training with distinct rewards for good and bad outputs. Trained on two response evaluation datasets with diverse long-form styles and Likert-rated quality, PrefBERT effectively supports GRPO by offering better semantic reward feedback than traditional metrics ROUGE-L and BERTScore do. Through comprehensive evaluations, including LLM-as-a-judge, human ratings, and qualitative analysis, we show that PrefBERT, trained on multi-sentence and paragraph-length responses, remains reliable across varied long passages and aligns well with the verifiable rewards GRPO needs. Human evaluations confirm that using PrefBERT as the reward signal to train policy models yields responses better aligned with human preferences than those trained with traditional metrics. Our code is available at https://github.com/zli12321/ long_form_rl."
        },
        {
            "title": "Introduction",
            "content": "Identifying the good and bad generations is the key to the success of RLVR to improve LLMs abilities on structured rule-based tasks such as mathematical problem-solving (Ahn et al., 2024), classification (Rouzegar and Makrehchi, 2024), and game planning (Shao et al., 2019). RL algorithms like Group Relative Policy Optimization (GRPO) excel in mathematical problem-solving tasks by leveraging clear, rule-based reward signals (e.g., correctness) that effectively distinguish between correct and incorrect responses (DeepSeek-AI et al., 2025). However, despite these advances, extending GRPO to open-ended, long-form text generation tasks, such as writing poem, creative compositions, instruction-following, remains largely underexplored. key challenge in applying GRPO to open-ended generation is the lack of objective evaluation criteria, making it difficult to distinguish high-quality responses from poor ones. (Krishna et al., 2021). Unlike structured tasks evaluated by explicit correctness criteria, long-form generation requires models to balance coherence, fluency, and informativeness which are all subjective dimensions that are hard to define with discrete rules. This makes reward design fundamental challenge in GRPO: How do we guide model to write better, not just longer? key barrier is the lack of reliable, scalable evaluation methods. Traditional metrics like ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2020)which measure lexical or embedding-level similarity to referenceoften fail to capture qualities that humans actually care about, such as clarity, relevance, and stylistic appropriateness (Chen et al., 2019; Li et al., 2024). Thus, traditional metrics correlate poorly with human preferences, making them unreliable reward signals in GRPO pipelines. Recent work has applied general reward models (GRM) (Lambert et al., 2024) in RLHF to select preferred responses, but they are not designed as verifiable rewards for GRPO, and their large size introduces latency and high computational cost. As shown in Figure 1, traditional metrics fail to distinguish between clearly better and worse responses, assigning similar scores to both, while GRM rewards often correlate with length rather than content quality. To address open-ended evaluation in GRPO, we propose PrefBERTa lightweight ModernBERT (Warner et al., 2024) model trained on diverse long-form responses and human ratings to provide fine-grained quality evaluation. We hyFigure 1: Our proposed reward model PrefBERT better reflects human judgments by distinguishing the good responses from the bad ones than other traditional metrics, generalized reward model. pothesize that PrefBERT, by capturing human preferences, quality, and semantics, provides stronger reward signals than traditional metrics, enabling GRPO to train models that generate higher-quality responses. Through extensive evaluationsincluding LLMas-a-judge point-wise scoring, pairwise BradleyTerry ranking analyses (Bradley and Terry, 1952), and human rating and qualitative evaluationwe show that leveraging stronger, PrefBERT significantly enhances the quality of open-ended text generation across three long-form datasets ELI5 (Fan et al., 2019), Alpaca (Taori et al., 2023b), LongForm (KÃ¶ksal et al., 2023). Our results show that using improved fine-grained evaluators as reward signals in GRPO leads to better alignment with human preferences for open-ended response generation compared to traditional metric-based rewardsand. Furthermore, smaller models (e.g., Qwen2.5-3B-Instruct (Qwen et al., 2025)) trained with our enhanced reward models generate similarly preferred and concise responses as their larger counterparts (e.g., Qwen-2.5-32/72B-Instruct), and outperform models trained with traditional supervised fine-tuning (SFT) in preference quality. Our contributions are: We introduce PrefBERT, lightweight longform reward model that can be easily extended to GRPO training. We validate using PrefBERT in GRPO to train models on across multiple open-ended generation benchmarks (ELI5, Alpaca, LongForm), showing resulting model have an overall higher alignment with human preferences compared to traditional metrics and SFT training. Through human expert annotations, we further confirm that models trained with PrefBERT align better with human preferences than traditional metrics as rewards, showing promising direction for using GRPO to improve openended generation."
        },
        {
            "title": "2 Related Work",
            "content": "RL for LLM alignment: RL is pivotal in aligning LLMs with human preferences by optimizing non-differentiable objectives, making it valuable for tasks like dialogue (Li et al., 2016), summarization (Roit et al., 2023), and code generation (Le et al., 2022). Popular RLHF methods include DPO, which applies classification loss over preference data, and PPO, which trains reward model to guide generation (Wu et al., 2023). However, both of them require substantial human-annotated data or computational resources. To address this, GRPO (DeepSeek-AI et al., 2025) leverages selfgenerated data and simple, verifiable reward functions to reduce annotation needs, especially for tasks with clear correctness signals like math(Liu et al., 2025). Extensions such as DAPO(Yu et al., 2025), GRPO-LEAD(Zhang and Zuo, 2025) and DISCO (Zhou et al., 2025b) broaden GRPOs capabilities in math problem solving. However, these approaches still rely on rule-based reward designs, leaving their application to open-ended, long-form generation taskswhere there is not fixed correct responseunderexplored. Free-form and open-ended evaluation: Evaluating long-form and open-ended generation in LLMs remains difficult (Krishna et al., 2021; Chen et al., 2019). Unlike short-form tasks with clear correctness signals, long-form outputs, like summaries, dialogues, or open-ended answers, lack binary ground truths and require assessing coherence, factuality, structure, and helpfulness (Chiang et al., 2024; Fabbri et al., 2021; Li et al., 2025a). Traditional metrics such as ROUGE(Lin, 2004), BLEU(Papineni et al., 2002), and BERTScore(Zhang et al., 2020) rely on token overlap or embeddings but poorly reflect semantic or pragmatic qualities, often misaligning with human judgments(Chen et al., 2019). To overcome this, LLM-as-a-judge offers more nuanced evaluation through pairwise comparisons or Likert ratings, aligning better with human preferences (Chiang et al., 2024; Gu et al., 2025; Zheng et al., 2023; Zhou et al., 2025a). However, this approach introduces heavy computational costs, especially in GRPO where multiple generations and evaluations per prompt are needed (Luo et al., 2025), limiting accessibility for users with modest resources. Some efforts fine-tune small models using human ratings to act as open-source judges (Kim et al., 2024; Yang et al., 2024; Chen et al., 2020; Zhou and Ai, 2024), useful for ranking or evaluation (Li et al., 2024; Krumdick et al., 2025). Still, few explore using them as verifiable rewards in training, leaving this an open area for research."
        },
        {
            "title": "3 Conceptual Backgrounds",
            "content": "In this section, we first review GRPO training and existing verifiable rewards for long-form generation, and then introduce how PrefBERT can be used as reward signal to improve learning robustness and effectiveness."
        },
        {
            "title": "3.1 Preliminaries on GRPO",
            "content": "GRPO is an RL algorithm designed to refine language model policies, ÏÏ, using reward signals contextualized within group of candidate responses. Given prompt from dataset D, GRPO samples responses yi = y1, . . . , yG from the old policy ÏÏold(yx). Each response yi is assigned scalar reward r(x, yi) (detailed in the following subsections). The group-normalized advantage A(x, yi) is then computed as: A(x, yi) = r(x, yi) r(x) Ïr(x) , (1) where r(x) = 1 j=1 r(x, yj) and Ïr(x) are the mean and standard deviation, respectively, of rewards r(x, yj) within the group . This normalization contextualizes each advantage relative to the groups current performance. The new policy ÏÏ(yx) is optimized by maximizing the GRPO objective, which combines clipped surrogate loss with Kullback-Leibler (KL) divergence penalty (Kullback and Leibler, 1951) against reference model Ïref(yx) for regularization (Equation 5, in Appendix A). Although originally applied to tasks with explicit, rule-based rewards (e.g., correctness or win/loss), GRPOs reliance on advantage estimation and KL-regularized updates allows it to learn from scalar feedback, making it well-suited for open-ended tasks where response quality lies on spectrum rather than binary correctness."
        },
        {
            "title": "Generation",
            "content": "Current scoring methods for open-ended generation mainly fall into two categories. The first are reference-based metrics, method commonly used in natural language generation. These methods score the generations over metrics like string overlap or embedding similarity. While easy to apply, they correlate poorly with human preferences on long-form outputs (Chen et al., 2019, 2020; Kim et al., 2024; Li et al., 2024; Gu et al., 2025; Li et al., 2025b). The alternative is to use LLMs as evaluators, which compute scores via pairwise preference prompts, Likert-scale tuning (Kim et al., 2024), or general reward models (GRMs) trained on preference data (Lambert et al., 2024). While more adaptive to human preferences, they may still inherit biases from their training data. In our work, we adopt two reference-based metrics, ROUGE and BERTScore, and one reward modelbased method, GRM-LLaMA-3B, as baselines. ROUGE (Lin, 2004) is reference-based metric that measures n-gram overlap between generated and reference texts. Variants include ROUGE1, ROUGE-2, and ROUGE-L, which capture unigram/bigram matches and the longest common subsequence, respectively. BERTScore (Zhang et al., 2020) is referencebased metric that measures semantic similarity between the reference and generation using contextual embeddings. It has shown stronger correlation with human judgments than token overlap metrics like ROUGE on long-form generation and translation tasks. However, its reliability diminishes on modern datasets and models (Bhandari et al., 2020). Generalized Reward Model (GRM-LLaMA-3B) is reference-free, LLM-based reward model (Yang et al., 2024), fine-tuned on the decontaminated Skywork preference dataset (Liu et al., 2024). Each data point includes an input prompt, preferred response (yc), and rejected response (yr). The model is trained to favor preferred outputs by minimizing Bradley-Terry-style loss (Bradley and Terry, 1952) (Equation 6, Appendix A)."
        },
        {
            "title": "3.3 PrefBERT: Training a Lightweight\nBERT-Based Reward Model",
            "content": "While traditional metrics often fail to capture human preferences, reward models can be computationally expensive to scale due to the size of the underlying models. To balance quality and efficiency, we propose PrefBERT, lightweight reference-based model with only 150M parameters, to offer fine-grained reward signals while maintaining training efficiency. Creating training data for PrefBERT: Each training example has reference answer, generated answer, and Likert score from 1-5 that rates the quality of the generated answer against the reference. To ensure balanced quality ratings across both long and short free-form responses, we incorporate training data from the Prometheuspreference (Kim et al., 2024) and MOCHA (Chen et al., 2020).1 We combine the two datasets and split them into 80% for training and 20% for testing. The resulting training set contains 19K examples substantially smaller than the 80K examples used to train GRM-llama-3B. Training BERT (Warner PrefBERT: We et train Modernal., 2024) on triplets 1Specifically, Prometheus-preference contains 200K finegrained Likert preference ratings spanning ten categories of evaluation including e.g. adaptive communication, emotional intelligence; the data is primarily long free-form answers where each answer is above 150 tokens. MOCHA contains mid to long length answer evaluation data to judge the overall correctness of the generated response. , xgen (xref first normalize each gold Likert score to , si) where si {1, . . . , 5}. We ri = si 1 4 [0, 1], where si is the gold Likert scale, ri is the normalized Likert score on the [0, 1] scale, xref is the reference answer, and xgen is the generated response. Thus, given xref and xgen, we concatenate them as single string: xpair = [CLS] xref [SEP] xgen, (2) where xpair is the input string feeds into ModernBERT. Let hi Rd be the pooled ModernBERT embedding of xpair. linear regressor plus sigmoid yields prediction rËi = Ï(whi + b), (3) where Rd and are the regressor weights and bias, Ï(z) = 1/(1 + ez) is the sigmoid activation. rËi (0, 1) is the predicted normalized score, and is taken as the reward signal of GRPO. Training minimizes the mean-squared error ="
        },
        {
            "title": "1\nN",
            "content": "N i=1 (rËi ri )2, (4) where is the batch-averaged MSE loss, and is the number of examples in the batch."
        },
        {
            "title": "4 Experiment Setup",
            "content": "With the background on GRPO and the various reward signals established, we now outline our experimental setup, including the datasets, base models, and training methods."
        },
        {
            "title": "4.1 Free-Form and Open-Ended Datasets",
            "content": "We use three datasets featuring long-form responses that span broad spectrum of topics. They cover diverse styles of long-form responses averaging 185 words, requiring evaluation across dimensions like coherence, conciseness, and informativeness rather than correctness.2 More details on each dataset are in B. ELI5 (Fan et al., 2019) is collection of questions and answers from Reddits r/explainlikeimfive community.3 We sample 10,444 questions as the train set and 1,056 as the test set. 2Examples in Table 4. 3https://www.reddit.com/r/explainlikeimfive/ Alpaca (Taori et al., 2023a) is collection of 52K instruction-response pairs generated by OpenAIs text-davinci-003 in the style of SelfInstruct (Wang et al., 2022). We use 10,444 examples as the train set and 1,334 as the test set. LongForm (KÃ¶ksal et al., 2023) is built from English documents (e.g., Wikipedia (Wikipedia contributors, 2025), C4 (Dodge et al., 2021)) paired with reverse-instruction prompts generated by LLMs. We exclude coding tasks, sampling 8,648 training and 956 test examples. We merge the three sampled datasets together as our free-form train/test set. 4.2 Training Setup GRPO for open-ended generation: We train policy models using GRPO within the OpenRLHF framework (Hu et al., 2024), optimizing each of the four reward signals from Section 3 separately: ROUGE-L, BERTScore, GRM-llama-3B, and PrefBERT. We use two base models, Qwen2.51.5B-Instruct and Qwen2.5-3B-Instruct (Qwen et al., 2025). In the training prompt, we encourage models to be factual, relevant, clear, concise, and complete (more details in Appendix Table 5).4 Supervised Finetuning (SFT): Using the reference responses as ground truth, we run SFT on Qwen2.5-Instruct size 1.5B and 3B."
        },
        {
            "title": "5 Automatic Evaluation",
            "content": "We use our test set to evaluate our trained models, as well as larger off-the-shelf models: Qwen2.5Instruct 7B, 32B, and 72B. For evaluation, we use LLM-as-a-judge to evaluate the quality of the responses for different models as they can be strong alternative evaluators of humans (Chiang and yi Lee, 2023a). Overall, models trained with our lightweight PrefBERT performs competitively with those trained with the much larger GRMllama-3B, and both substantially outperform models trained with token-overlap metrics or SFT. In addition, PrefBERT-trained models at 1.5B and 3B scale rival or exceed the performance of Qwen2.57B-Instruct, despite having far fewer parameters. 4All GRPO models are trained on 4 A6000 GPUs for one epoch, with global batch size of 128, group size of 4, and learning rate of 1e-6. We set both max prompt length and max generation length to 1024. 5All SFT models are trained on 4 A6000 GPUs for three epochs, with with global batch size of 128, learning rate of 1e-5, and max tokens of 4096. 5.1 Evaluation Metrics Point-wise evaluation: Point-wise evaluation assigns an absolute overall quality score to each response on Likert scale (Fabbri et al., 2021). We use GPT-4 as judge to first provide some reasoning, then assign score between 1 to 5 to the generated response, considering aspects like factuality, relevance, clarity and organization, conciseness, and completeness (detailed prompt in Table 7).6 We use two metricsmean Likert score (the average overall score) and success rate (the percentage of responses that receive score 4)to evaluate the quality of model responses. Pairwise preference evaluation: From the Likert scores, we derive pairwise comparisons to compute BradleyTerry win rates. This approach reduces rating noise by focusing on relative preferences rather than absolute scales, which has been shown to yield more reliable comparisons in subjective evaluation settings (Bai et al., 2022; Stiennon et al., 2022). For each prompt, we compare the LLM ratings between every pair of models. tie is recorded when both receive the same rating, and win is assigned to the model with the higher rating. We use the Bradley-Terry model to compute the probability win rate of each model on the three datasets."
        },
        {
            "title": "5.2 Results and Discussion",
            "content": "Table 1 summarizes model performance across instruction-following tasks using Likert scores, success rates, and BradleyTerry win rates and Figure 5.3 shows the training curves of the 3B policy models. Below, we discuss our findings. Policy models trained with PrefBERT and GRMllama-3B achieve the highest ratings from LLMas-a-judge among all evaluated policy models. 3B-PrefBERT and 3B-GRM-llama-3B both have the highest LLM-as-a-judge average ratings and success rates among all policy models. However, PrefBERT is more efficient, having 20 times fewer parameters and was trained on only 19K examples, compared to 80K for GRM-llama-3B. In addition, as discussed later in Section 6.2, human evaluations reveal that GRM-llama-3B models tend to produce long and overly verbose outputs, which likely inflate their scores in Table 1 due to LLM judges bias toward longer responses. 6Chiang and yi Lee (2023b) shows that first analyze the response then give rating score yields the best correlation with human judgments. Model ELI5 LongForm Alpaca Overall ELI5 LongForm Alpaca Overall ELI5 LongForm Alpaca Overall Mean Likert Scores Success Rates with Score 4 (%) BradleyTerry Win Rate (%) Qwen2.5-72B-Instruct Qwen2.5-32B-Instruct Qwen2.5-7B-Instruct Qwen2.5-3B-Instruct Qwen2.5-1.5B-Instruct 3B-GRM-llama-3B 3B-PrefBERT 3B-BERTScore 3B-ROUGE-L 1.5B-GRM-llama-3B 1.5B-PrefBERT 1.5B-ROUGE-L 1.5B-BERTScore 3B-sft 1.5B-sft 4.39 4.28 4.04 3.90 3.61 4.29 4.09 3.79 3.66 4.13 4.10 2.66 2. 2.19 2.18 3.56 3.52 2.95 2.88 2.26 3.37 3.54 2.79 2.69 3.12 2.96 1.98 1.86 2.21 2.15 4.23 4.29 3.82 3.75 3.44 4.07 4.16 3.63 3.51 3.88 3.89 3.04 3. 3.32 3.33 4.13 4.10 3.69 3.59 3.21 Base LLM 96.59 94.15 77.38 66.00 49.16 38.91 37.55 18.83 17.99 10.25 RL-Finetuned Policy Models (GRPO) 3.99 3.98 3.49 3.37 3.79 3.74 2.62 2. 91.97 84.64 60.73 51.16 86.63 87.02 5.72 0.90 30.02 28.87 11.09 7.32 22.38 18.62 1.05 0.42 Supervised Finetuning (SFT) 1.78 2.59 1.67 2.57 2.51 2.63 87.93 89.51 66.64 63.57 47.38 77.51 84.41 59.30 52.55 72.41 73.61 17.92 17. 36.58 37.93 79.25 78.47 59.10 53.22 38.87 71.55 70.70 47.89 40.74 65.73 65.37 8.79 6.50 14.14 14.64 21.94 15.43 7.54 5.18 2.69 15.89 7.92 3.40 2.40 8.93 8.01 0.28 0. 0.12 0.12 19.87 18.24 4.94 4.17 1.49 12.59 18.00 3.19 2.57 6.65 4.81 0.72 0.55 1.13 1.09 16.11 20.68 5.95 5.26 2.89 11.52 12.54 3.72 2.78 6.74 6.59 0.90 0. 1.68 1.73 19.33 18.09 6.37 5.09 2.51 13.53 11.56 3.60 2.72 7.64 6.72 0.69 0.52 0.82 0.82 Table 1: Evaluation of model outputs via GPT-4 as judge across different instruction tuning and reward optimization strategies. Groupings show comparisons between SoTA baselines,RL-finetuned models using various reward functions, and supervised finetuning (SFT). Larger models are generally stronger, though models fine-tuned with better-aligned reward functions (e.g., GRM-llama-3B) may show inflated automatic metrics due to biases like verbosity. PrefBERT-trained models rival much larger models. Despite their smaller size, PrefBERTtrained models at 1.5B and 3B scale match or exceed the performance of Qwen2.5-7B-Instruct across all metrics. For example, 3B-PrefBERT outperforms Qwen2.5-7B-Instruct in Likert score (3.98 vs. 3.69), success rate (70.70% vs. 59.10%), and win rate (11.56% vs. 6.37%). While Qwen2.532B and 72B are stronger overall, these results highlight how well-aligned reward model can enable smaller LLMs to compete with much larger ones on open-ended long-form generations. The pattern that using semantic verifiable reward for training produces better policy models than using traditional metric-based rewards in long-form generation remains consistent across model sizes. Models trained with standard metrics, such as 1.5B-ROUGE-L and 1.5B-BERTScore, perform worse across all evaluation dimensions compared to 1.5B-PrefBERT and 1.5B-GRM-llama3B; this trend also holds for the 3B model sizes. SFT underperforms across the board. Policy models trained with SFT achieve lower scores across all evaluation dimensions than all models trained with GRPO, regardless of the reward used, except for 1.5B-BERTScore. Their success and win rates are the lowest across all datasets. Even the small 1.5B-PrefBERT and 1.5B-GRM-llama-3B substantially outperform the 3B-SFT model on average Likert score by around 1.1, success rate by 50%m and Bradley-Terry win rate by 6%. 5.3 Reward Learning Curves We examine the reward learning curves of models trained with the four types of rewards. Reward curves in Figure 2 during GRPO training show key differences across reward functions. Traditional metrics show minimal reward change, only 0.05 for ROUGE-L and 0.25 for BERTScore, indicating limited model improvement across all global training steps. In contrast, 3B-GRM-llama-3Bs reward is strongly correlated with response length; by step 60, it already generates the maximum allowed tokens (1,024), causing reward values to plateau around 0.6. PrefBERT shows more meaningful reward progression, not strictly tied to length, suggesting it favors responses of an optimal length rather than simply longer outputs."
        },
        {
            "title": "6 Human Evaluation",
            "content": "While LLM-as-a-judge evaluation often correlates with human judgments at the system level (Gu et al., 2025), LLMs tend to prefer longer responses regardless of quality (Zheng et al., 2023). To better assess output quality, we conducted human evaluation of responses from seven models: Qwen2.5-72BInstruct, Qwen2.5-3B-Instruct, 3B-GRM-llama-3B, 3B-PrefBERT, 3B-RougeL, 3B-BERTScore, and 3BSFT. Human preferences largely align with LLMas-a-judge rankings, except for 3B-GRM-llama-3B, whose overly verbose answers lower its human scores. Among the 3B models, 3B-PrefBERT is the top performer. Additionally, we observe that SFT models often produce shallow outputs, whereas GRPO models, trained with strong verifiable reward Figure 2: Reward curves during GRPO training show key differences across reward functions. Traditional metrics show minimal reward changeonly 0.05 for ROUGE-L and 0.25 for BERTScoreindicating limited model improvement across all global training steps. In contrast, 3B-GRM-llama-3Bs reward is strongly correlated with response length; by step 60, it already generates the maximum allowed tokens (1,024), causing reward values to plateau around 0.6. PrefBERT shows more meaningful reward progression, not strictly tied to length, suggesting it favors responses of an optimal length rather than simply longer outputs. signals, better leverages the models internal capabilities to produce higher-quality responses."
        },
        {
            "title": "6.1 Human Evaluation Setup",
            "content": "We randomly sample 150 test prompts (50 from each datasets test set), then collect responses from Qwen2.5-72B-Instruct, Qwen2.5-3B-Instruct, 3BGRM-llama-3B, 3B-PrefBERT, 3B-RougeL, 3BBERTScore, and 3B-SFT. We use an annotation tool (.Figure 3), where for each response, the annotator needs to give Likert score between 1-5 using the same evaluation criteria as the criteria given to LLMs in Table 7. We have four author annotators annotating total of 150 examples. For each prompt, the annotator also needs to give rankings of the responses of the seven models. All the model names are hidden for fair comparison."
        },
        {
            "title": "6.2 Results and Qualitative Analysis",
            "content": "Table 2 shows that Qwen2.5-72B-Instruct achieves the highest average human Likert rating (3.61) and success rate (57%), followed by 3B-PrefBERT with rating of 3.36 and 51% success rate. In contrast, 3B-BERTScore and 3B-ROUGE-L perform only slightly better than the 3B base model, each improving success rates by approximately 3%. 3BPrefBERT, however, achieves much larger gain of around 10%. 3B-SFT receives the lowest human rating, while 3B-GRM-llama-3B ranks second to last in both Likert score and win rate. Except for 3B-GRM-llama-3B, the human evaluation results align closely with the rankings produced by LLM-as-a-judge, supporting the reliability of our automatic evaluation. We elaborate on these human evaluation findings below. Annotators strongly prefer 3B-PrefBERT. In general, we observe consistent ranking between human expert judgments and the rankings produced by LLM-as-a-judge. However, 3B-GRM-llama-3B is notable outlier. While it has the highest scores from LLM judges across all GRPO-trained models  (Table 1)  , annotators consistently rank it low  (Table 2)  . As shown in Table 3, 3B-GRM-llama3B is excessively verbose, averaging 710 words per responsealmost 2.75 times as many as 3BPrefBERT, which averages 258. The tendency for longer responses is consistent with the reward training curves in Figure 2: 3B-GRM-llama-3B receives higher rewards primarily for increased response length, rather than for high answer quality. While this reward hacking can lead LLM-as-a-judge to assign higher scores, human evaluators often dislike these responses due to their verbosity and lack of conciseness and completeness. Additionally, we observe that 3B-GRM-llama-3B requires 10 more hours of training than other 3B policy models, and 1.5B-GRM-llama-3B takes 7 more hours than other 1.5B models. This increased training time is result of length-based rewards, which guide the model to produce longer responsesthereby increasing the time needed for each GRPO inference and overall training. What distinguishes 3B-PrefBERT from the base model? We further analyze 26 of the examples where 3B-PrefBERT is preferred over the base model. The improvements fall into two main categories: instruction following and tone and fluency. On prompts with explicit constraints (e.g., explain in 2 sentences), the base model often Model ELI5 LongForm Alpaca Overall ELI5 LongForm Alpaca Overall ELI5 LongForm Alpaca Overall Mean Likert Scores Success Rates with Score 4 (%) BradleyTerry Win Rate (%) Qwen2.5-72B-Instruct Qwen2.5-3B-Instruct 3B-GRM-llama-3B 3B-PrefBERT 3B-BERTScore 3B-ROUGE-L 3B-sft 3.85 3.31 2.30 3.55 2.95 3. 2.0 3.9 3.3 2.5 3.6 3.3 2.9 2.8 3.4 3.2 2.75 3.5 3.3 3. 1.4 Base LLM 3.61 3.21 70.0 40.0 65.0 55.0 RL Finetuned Policy Models (GRPO) 2.55 3.36 3.23 3.31 15.0 60.0 40.0 53.0 30.0 55.0 45.0 43.5 47.5 30.0 47.5 55.0 42.5 27.5 57.3 37. 16.67 15.24 31.7 51.0 41.46 41.66 12.86 21.19 15.95 15.24 21.54 14.06 8.16 19.72 12.02 19.04 17.62 12. 10.76 21.38 17.62 16.43 17.62 12.62 14.76 17.38 16.43 17.14 Supervised Finetuning (SFT) 25.0 10.0 1. 10.0 13.41 0.03 5.44 3.98 4. Table 2: Human evaluation shows different preference than automatic evaluation, where model trained with GRM-llama-3B is less preferred by exerts but PrefBERT is more preferred than all other policy models. Model Markdown (%) Repetition Rate (%) Response Length Qwen2.5-72B-Instruct Qwen2.5-3B-Instruct 3B-GRM-llama-3B 3B-PrefBERT 3B-BERTScore 3B-ROUGE-L 3B-SFT 47.48 28.89 66.80 81.31 24.00 21.92 15.96 6.25 4.69 4.18 4.34 4.55 8.59 8.29 220 194 710 258 180 182 146 Table 3: Average words per response for each group by model. 3B-GRM-llama-3B generates way more words per response than all other models, where human experts consider as overly verbose and contain unnecessary information. Repetition rate is the percentage of bigrams that are repeated. Markdown is regular expression that checks whether response follows particular structure and returns boolean  (Table 6) . fails to comply, producing responses that are eiIn contrast, 3Bther too long or overly brief. PrefBERT reliably adheres to such constraints. Additionally, its writing is more polished and humanlike. While the base model tends to sound mechanicalproducing fragmented sentences reminiscent of stitched-together search results3B-PrefBERT generates fluent, cohesive answers. See Appendix Figure 6 and Figure 7 for qualitative examples. 3B-PrefBERT and 3B-GRM-llama-3B frequently use structured outputs. In Table 3, we observe that 3B-PrefBERT and 3B-GRM-llama-3B tend to use markdown formatting more frequently than other models (81.31% and 66.80%, respectively). As result, annotators often note that their outputs have better readability. See qualitative examples in Appendix (Figure 4). 3B-ROUGE-L and 3B-BERTScore tend to be generic and sometimes repetitive. These models often respond to prompts such as Categorize the AI technologies mentioned below: Machine Learning, Natural Language Processing, Robotics with generic definitions (e.g., Machine Learning is subset of artificial intelligence that involves training algorithms...) rather than actually categorizing or differentiating between the terms. Additionally, Table 3 shows that 3B-ROUGE-L can be highly repetitive. In 5.3, we find that ROUGE-L and BERTScore show little reward variance across the training curve, which could indicate that the training signal is too weak, potentially causing the trained models to output vague and surface-level outputs. In contrast, 3B-PrefBERT provides clearer categorizations and contextual explanations for each term, demonstrating stronger content logic. See detailed qualitative analysis in Appendix Figure 5 and Figure 8. 3B-SFT responses are often vague and overly simplified. In annotated examples, 3B-SFT responses explicitly avoid answering the questionsometimes stating dont know or offering no meaningful explanation. For instance, in response to the prompt Why is the Big Bang seen as singular event?, the model deflects the question without addressing the core scientific reasoning. Additionally, on LongForm promptsespecially those derived from Alpaca-style or open-ended datasets3B-SFT tends to produce overly simplified, shallow explanations. These responses often lack both technical depth and structural clarity, which diminishes their informativeness and readability. This trend is also reflected in Table 3, where 3B-SFT produces the shortest responses on average. We attribute this issue in part to the nature of the training data from sources such as ELI5, which contains casual, informal responsesmany of which may be low-quality or factually incorrect. This results in model that mimics the tone and content of noisy or imprecise reference answers. While GRPO-trained models demonstrate better performance over SFT in open-ended long-form generation in our experiments, we do not dismiss SFT as an ineffective approach. When high-quality, human-annotated datasets are available, SFT remains valuable strategyparticularly in domains like code generation (Zhou et al., 2023), where reference outputs are well-defined and reliable."
        },
        {
            "title": "7 Conclusion",
            "content": "RLVR especially GRPO has been success for its ability to fully leverage LLMs abilities to selfimprove without massive amount of labeled data on many rule-based evaluation tasks. However, extending GRPO study on long-form and openended generation has been underexplored for the challenges of evaluating long-form responses. We propose using small fine-tuned language model (PrefBERT) to evaluates long-form responses with different dimensions and semantic quality evaluation as reward signals for long-form generations of GRPO and show that model trained with PrefBERT generates responses with overall better quality than models trained with traditional metrics such as ROUGE and BERTScore or generalized preference reward model, even close to the quality of larger models of the same backbone. Our work shows the potential of applying more efficient and robust verifiable reward design into the GRPO pipeline for models to self-improve its long-form and openended generations beyond using traditional reward metrics. Future work can expand upon current work on more diverse open-ended generation tasks such as training more efficient and stronger verifiable reward models and apply them on creative writings, creative research and design, or openended math problems."
        },
        {
            "title": "8 Ethics",
            "content": "Our annotation does not involve in collecting annotators private information and does not involve in extensive tool usage. Thus, our annotation is exempted by the Institutional Review Board (IRB) annotation protocol."
        },
        {
            "title": "9 Limitations",
            "content": "in LLMs on such complex tasks. key limitation of our study is that we did not train or use larger and more powerful language model (e.g., 7B-scale) to serve as verifiable reward provider, primarily due to computational constraints. Larger evaluators, while potentially offering more reliable and semantically accurate rewards, significantly increase GPU memory usage and slow down training. We hypothesize that incorporating stronger evaluator to provide high-quality, verifiable rewards could unlock the full potential of GRPO for aligning LLMs on open-ended tasks."
        },
        {
            "title": "References",
            "content": "Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. 2024. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022. Training helpful and harmless assistant with reinforcement learning from human feedback. Preprint, arXiv:2204.05862. Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Reevaluating evaluation in text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 93479359, Online. Association for Computational Linguistics. Ralph Allan Bradley and Milton E. Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324 345. Anthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Evaluating question answering evaluation. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 119124, Hong Kong, China. Association for Computational Linguistics. We are the first to demonstrate that fine-tuned evaluation language models can effectively leverage the capabilities of LLMs for evaluating and improving long-form, open-ended generations. However, this work has not fully explored the potential of GRPO and reward design for enabling self-improvement Anthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2020. Mocha: dataset for training and evaluating generative reading comprehension metrics. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics. Cheng-Han Chiang and Hung yi Lee. 2023a. Can large language models be an alternative to human evaluations? Preprint, arXiv:2305.01937. Cheng-Han Chiang and Hung yi Lee. 2023b. closer look into automatic evaluation using large language models. Preprint, arXiv:2310.05657. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, and Ion Stoica. 2024. Chatbot arena: an open platform for evaluating llms by human preference. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org. DeepSeek-AI, Daya Guo, and et al. 2025. Deepseekr1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting large webtext corpora: case study on the colossal clean crawled corpus. Preprint, arXiv:2104.08758. Aarohi Srivastava et al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Preprint, arXiv:2206.04615. Michael Krumdick, Charles Lovering, Varshini Reddy, Seth Ebner, and Chris Tanner. 2025. No free labels: Limitations of llm-as-a-judge without human grounding. Preprint, arXiv:2503.05061. S. Kullback and R. A. Leibler. 1951. On information and sufficiency. The Annals of Mathematical Statistics, 22(1):7986. Abdullatif KÃ¶ksal, Timo Schick, Anna Korhonen, and Hinrich SchÃ¼tze. 2023. Longform: Effective instruction tuning with reverse instructions. Preprint, arXiv:2304.08460. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. 2024. Rewardbench: Evaluating reward models for language modeling. Preprint, arXiv:2403.13787. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven C. H. Hoi. 2022. Coderl: Mastering code generation through pretrained modPreprint, els and deep reinforcement learning. arXiv:2207.01780. Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. 2016. Deep reinforcement learning for dialogue generation. Preprint, arXiv:1606.01541. Alexander R. Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Re-evaluating summarization evaluation. Preprint, arXiv:2007.12626. Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, and Jordan Lee Boyd-Graber. 2024. Pedants: Cheap but effective and interpretable answer equivalence. Preprint, arXiv:2402.11161. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. Eli5: Long form question answering. Preprint, arXiv:1907.09190. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, and Jian Guo. 2025. survey on llm-as-a-judge. Preprint, arXiv:2411.15594. Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. 2024. Openrlhf: An easyto-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024. Prometheus 2: An open source language model specialized in evaluating other language models. Preprint, arXiv:2405.01535. Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answering. Preprint, arXiv:2103.06332. Zongxia Li, Xiyang Wu, Hongyang Du, Fuxiao Liu, Huy Nghiem, and Guangyao Shi. 2025a. survey of state of the art large vision language models: Benchmark evaluations and challenges. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR) Workshops, pages 15871606. Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, and Jordan Lee Boyd-Graber. 2025b. Videohallu: Evaluating and mitigating multi-modal hallucinations on synthetic video understanding. Preprint, arXiv:2505.01481. Chin-Yew Lin. 2004. Rouge: package for automatic evaluation of summaries. In Proceedings of the Workshop on Text Summarization Branches Out. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. 2024. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025. Understanding r1-zero-like training: critical perspective. Preprint, arXiv:2503.20783. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. 2025. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. Notion Blog. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL 02, page 311318, USA. Association for Computational Linguistics. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Paul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu Geist, Sertan Girgin, LÃ©onard Hussenot, Orgad Keller, Nikola Momchev, Sabela Ramos, Piotr Stanczyk, Nino Vieillard, Olivier Bachem, Gal Elidan, Avinatan Hassidim, Olivier Pietquin, and Idan Szpektor. 2023. Factually consistent summarization via reinforcement learning with textual entailment feedback. Preprint, arXiv:2306.00186. Hamidreza Rouzegar and Masoud Makrehchi. 2024. Enhancing text classification through llm-driven active learning and human annotation. arXiv preprint arXiv:2406.12114. John Schulman, Filip Wolski, Prafulla Dhariwal, ProxPreprint, Alec Radford, and Oleg Klimov. 2017. imal policy optimization algorithms. arXiv:1707.06347. Kun Shao, Zhentao Tang, Yuanheng Zhu, Nannan Li, and Dongbin Zhao. 2019. survey of deep reinforcement learning in video games. Preprint, arXiv:1912.10944. Stack Exchange contributors. 2025. Stack Exhttps://{site}.stackexchange.com/ change. questions/{question_id}. [Online; accessed 5May-2025]. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2022. Learning to summarize from human feedback. Preprint, arXiv:2009.01325. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023a. Alpaca: strong, replicable instruction-following https://crfm.stanford.edu/2023/03/ model. 13/alpaca.html. [Online; accessed 5-May-2025]. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023b. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. Benjamin Warner, Antoine Chaffin, Benjamin ClaviÃ©, Orion Weller, Oskar HallstrÃ¶m, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, and Iacopo Poli. 2024. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. Preprint, arXiv:2412.13663. Wikipedia contributors. 2025. Wikipedia, the free encyclopedia. [Online; accessed 5-May-2025]. Tianhao Wu, Banghua Zhu, Ruoyu Zhang, Zhaojin Wen, Kannan Ramchandran, and Jiantao Jiao. 2023. Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment. Preprint, arXiv:2310.00212. Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. 2024. Regularizing hidden states enables learning generalizable reward model for llms. Preprint, arXiv:2406.10216. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. 2025. Dapo: An opensource llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Jixiao Zhang and Chunsheng Zuo. 2025. Grpo-lead: difficulty-aware reinforcement learning approach for concise mathematical reasoning in language models. arXiv preprint arXiv:2504.09696. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Preprint, Evaluating text generation with bert. arXiv:1904.09675. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Preprint, arXiv:2306.05685. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less is more for alignment. Preprint, arXiv:2305.11206. Yuhang Zhou and Wei Ai. 2024. Teaching-assistantin-the-loop: Improving knowledge distillation from imperfect teacher models in low-budget scenarios. arXiv preprint arXiv:2406.05322. Yuhang Zhou, Giannis Karamanolakis, Victor Soto, Anna Rumshisky, Mayank Kulkarni, Furong Huang, Wei Ai, and Jianhua Lu. 2025a. Mergeme: Model merging techniques for homogeneous and heterogeneous moes. arXiv preprint arXiv:2502.00997. Yuhang Zhou, Jing Zhu, Shengyi Qian, Zhuokai Zhao, Xiyao Wang, Xiaoyu Liu, Ming Li, Paiheng Xu, Wei Ai, and Furong Huang. 2025b. Disco balances the scales: Adaptive domain-and difficulty-aware reinforcement learning on imbalanced data. arXiv preprint arXiv:2505.15074."
        },
        {
            "title": "A Technical Details",
            "content": "In this section, we provide additional technical details for GRPO mentioned in Section 3, further illustrating the regularization terms used in GRPO and the specifics of the Bradley-Terry loss employed by GRM. A.1 GRPO Regularization Objective JGRPO(Ï) = Ex,{yi} [ 1 i=1 ( min Ïi(Ï)A(x, yi), )] clip (Ïi(Ï), 1 Ïµ, 1 + Ïµ) A(x, yi) Î² ExD[KL(ÏÏ(x)Ïref(x))], (5) where Ïi(Ï) = ÏÏ(yix) (yix) is the probability ratio for yi, Ïµ is the clipping hyperparameter and Î² is the KL penalty coefficient. ÏÏold A.2 GRM Bradley and Terry Loss The GRM-llama-3B is trained to minimize the Bradley-Terry Loss: Lreward(Î¸) = E(x,yc,yr) [log Ï (rÎ¸(x, yc) rÎ¸(x, yr))] (6) where rÎ¸(x, y) denotes the reward score predicted by the model and Ï() is the sigmoid function. Generally, the GRM rÎ¸(x, y) is used in RLHF training, either for selection in Best-of-n (BoN) decoding or as the optimization objective in reinforcement learning (e.g., PPO (Schulman et al., 2017)). We use GRM-llama-3B as GRM to provide reward signals for GRPO, rather than for ranking models or as PPO reward model. We use sigmoid function to normalize its real-valued outputs to the [0, 1]."
        },
        {
            "title": "B Dataset Details",
            "content": "Table 4 presents details of the datasets used in our work, including the sizes of the training and testing sets, as well as example inputs and reference responses. Explain Like Im 5 (ELI5) is dataset derived from Reddits r/explainlikeimfive community (Fan et al., 2019).8 It contains 270K threads where users 7We choose GRM-llama-3B for its best performance as the smallest model on RewardBench (Lambert et al., 2024), which offers good trade-off between quality and efficiency without the heavy GPU demands of larger models. 8https://www.reddit.com/r/explainlikeimfive/ ask open-ended questions and receive simple, easyto-understand explanationsframed as if explaining to five-year-old. The topics span wide range of domains, including chemistry, psychology, biology, and earth science. The dataset is intended to help models learn to explain complex topics in accessible ways. We sample 10,444 questions for training and 1,056 for testing. Alpaca is collection of 52K instructionresponse pairs generated by OpenAIs text-davinci003 to fine-tune the LLaMA 7B model (Taori et al., 2023a).9 It features diverse prompts and long-form responses in the style of Self-Instruct (Wang et al., 2022). We use cleaned version of Alpaca (Taori et al., 2023b) that removes instances with hallucinated answers, empty responses, or instructions to generate images. Additionally, we filter out examples with responses shorter than 50 words, resulting in final set of 10,444 training and 1,334 test examples. LongForm is constructed by applying reverse instruction generation to an English corpus, following the approach in (KÃ¶ksal et al., 2023). It includes diverse set of human-written documents sourced from Wikipedia (Wikipedia contributors, 2025), C4 (Dodge et al., 2021), Stack Exchange (Stack Exchange contributors, 2025), and BigBench (et al, 2023). Instructions are generated by LLMs, covering wide range of tasks such as question answering, email writing, story or poem generation, and text summarization. We exclude examples requiring code generation, as they fall outside our intended scope. The final dataset contains 8,648 training examples and 956 test examples."
        },
        {
            "title": "C Prompt Template",
            "content": "We show the prompt template used for training in Table 5, the template for point-wise evaluation in Table 7, and the template for pairwise preference evaluation in Table 8."
        },
        {
            "title": "D Markdown Expression Check",
            "content": "We provide Table 6 to exhibit the markdowns, which is regular expression that checks whether response follows particular structure We provide Table 6 to present the markdown patternsregular expressions used to check whether response follows specific structure and return boolean value, as described in the caption of Table 3. 9https://openai.com Dataset # Train # Test Example Input Example Reference Response ELI5 10,444 1,056 Could we theoretically create an infinite echo? LongForm 8,648 956 Explain how Venezuela raised its minimum wage. Alpaca 10,444 1, Develop customer service strategy to improve customer experience. The perfect conditions would be wall of atoms that will not move at all when bumped. Considering the fact that heat is defined by the movement of atoms... Venezuela raised its minimum wage to 1 million bolivars per month on Monday, the third increase this year that puts the figure at just $1.61 at the black market exchange rate. President Nicolas Maduro... Here is customer service strategy that can help in improving the customer experience: 1. Identify your customers needs... Table 4: Overview of the datasets used in our experiments. All datasets contain long-form, open-ended questions spanning diverse domains (e.g., science, instruction following), with responses averaging 185 words. lowing), Figure 7 (Tone and Fluency), and Figure 8 (Failure cases of SFT). Point-wise Evaluation Template You will be given user question, reference answer, and system answer. Your task is to provide an overall rating scoring how well the system answer addresses the Give your user question against the reference answer. answer as an integer on scale of 1 to 5, where 1 means that the system answer is not informative, and 5 means that the answer addresses the question according to the criteria below. Rubric: Factual Accuracy: The answer must be factually correct and does not contradict the reference answer. Relevance and Completeness: The answer should directly address the specific question, covering all essential aspects. Clarity well-structured, coherent, and easy to follow. Organization: answer The and should be Conciseness: repetition and be as clear and succinct as possible. should answer avoid The unnecessary Completeness: The answer is complete and not repetitive. Please base your overall rating on how well the system answer performs in these areas. Question: {question} Reference Answer: {reference_answer} System Answer: {answer} Please be as strict and as critical and harsh as possible. Provide your feedback as follows: Feedback::: Final rating: (your rating, as an integer between 1 and 5) Table 7: Prompt template for point-wise evaluation. Training Prompt Template The user asks question, and the Assistant answers it. The assistant provides the user with the answer that strictly follows the following guidelines. The answer should be enclosed within <answer> </answer> tags, respectively, i.e., <answer> ANSWER HERE </answer>. Your answer should follow these rubric criteria: Rubric: Factual Accuracy: The answer must be factually correct and does not contradict the reference answer. Relevance and Completeness: The answer should directly address the specific question, covering all essential aspects. Clarity and Organization: The answer should be well-structured, coherent, and easy to follow. Conciseness: The answer should avoid unnecessary repetition and be as clear and succinct as possible. Completeness: The answer is complete and not repetitive. Response Format rules: - Always start your response with <answer> tag and end with </answer>. - Do not include any text or commentary before the opening <answer> tag and after the closing </answer> tag. For example, your response should follow this format: <answer> [Your final detailed answer goes here] </answer> Question: {question} Table 5: Training prompt template for LLMs to generate responses. Table 6: Markdown Expression Check Pattern Description ^#{1,6}s ^[-*+]s ^d+.s ^>s ```[sS]+?``` `[^`n]+?` .+ ATX-style headings (e.g. # Heading) Unordered list items (e.g. - item) Ordered list items (e.g. 1. item) Blockquotes (e.g. > quote) Fenced code blocks Inline code spans Tables using pipe separators"
        },
        {
            "title": "E Annotation Tool",
            "content": "We present the annotation tool for response quality annotation in Figure 3. Annotators will be displayed with the question prompt, the answers for the seven models, where they need to slide due to limited screen width. Annotators can then put their Likert scores (1-5) and comments or notes for each response, and then finally rank the responses based on their preferences and ratings."
        },
        {
            "title": "F Showcases for Qualitative Analysis",
            "content": "We present qualitative examples to accompany our analysis in Section 6.2. See Figure 4 (Readability), Figure 5 (Content Logic), Figure 6 (Instruction FolFigure 3: Our annotation tool for response quality annotation. Annotators will be displayed with the question prompt, the answers for the seven models, where they need to slide due to limited screen width. Annotators can then put their Likert scores (1-5) and comments or notes for each response, and then finally rank the responses based on their preferences and ratings. Pairwise Preference Evaluation Template You are fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring each assessment reflects the absolute standards set for performance. Your task is to provide your preferred response as either or B. Please strictly follow the output format as: Feedback: Reason why you choose this answer [RESULT] or B</s> Rubric: Factual Accuracy: The answer must be factually correct and does not contradict the reference answer. Relevance and Completeness: The answer should directly address the specific question, covering all essential aspects. Clarity and Organization: The answer should be well-structured, coherent, and easy to follow. Conciseness: The answer should avoid unnecessary repetition and be as clear and succinct as possible. Completeness: The answer is complete and not repetitive. Write detailed feedback that assesses the quality of two responses strictly based on the given score rubric, not evaluating in general. After writing feedback, choose better response between Response and Response B. You should refer to the score rubric. Question: {question} Reference Answer: {reference_answer} Answer A: {answer_A} Answer B: {answer_B} Please be as strict and as critical and harsh as possible. Provide your feedback as follows: Feedback::: Final rating: (your rating, as an integer between 1 and 5) Table 8: Prompt template for pairwise evaluation. Qualitative Analysis: Readability Prompt: Describe the new functions of the Tesla Model 3, and how they improve the driving experience Dataset: Instruction Models Compared: Qwen2.5-3B-PrefBERT v.s. Qwen2.5-3B-Rouge-L & Qwen2.5-3B-BERTScore Reference Answer (Qwen2.5-72B-Instruct) : The Tesla Model 3 has several advanced features and functions that significantly enhance the driving experience. Here are some key highlights: 1. **Autopilot and Full Self-Driving (FSD) Capabilities**: ... Qwen2.5-3B-PrefBERT Certainly! Highlighting the new functions of the Tesla Model 3 and how they elevate the driving experience showcases Teslas commitment to innovation and user-centric design. Heres concise breakdown, seamlessly intertwining functionality, clarity, and depth: 1. **Autopilot Advanced Driver Assistance Systems**: ... Analysis: This answer uses more complex language, longer sentences, and promotional tone that may appeal to more technical or marketing-savvy reader but reduces overall clarity and ease of comprehension. Qwen2.5-3B-ROUGE-L The Tesla Model 3 has several new functions that enhance the driving experience. One of the most notable improvements is the Autopilot system, which includes features like automatic emergency braking, adaptive cruise control, and lane centering. These features help drivers maintain safe distance from other vehicles and stay within their lane, reducing the workload and stress of driving... Analysis: This answer presents information in dense, uniform paragraphs with minimal structural guidance, making it harder for readers to scan and retain key points. It also repeats ideas (e.g., mentioning safety features like automatic emergency braking twice) and lacks cohesion, which disrupts the logical flow. Qwen2.5-3B-BERTScore The Tesla Model 3 has several advanced features designed to enhance the driving experience. One of its notable improvements is the Autopilot system, which allows for semi-autonomous driving capabilities. This includes automatic steering, acceleration, and braking under certain conditions, reducing the drivers workload and enhancing safety... Analysis: This answer presents information in long, uniform paragraphs without visual structure, making it harder for readers to quickly locate and retain key points. The dense formatting in this answer lacks clear segmentation, which forces readers to parse multiple features at once without guidance or emphasis. Additionally, the transitions between topics are minimal, and the writing style is more monotonous, reducing engagement. Figure 4: Qualitative Analysis: Readability. We present showcases comparing the readability of PrefBERT against baseline models, accompanied by detailed analysis. Qualitative Analysis: Content Logic Prompt: Categorize the AI technologies mentioned below. Machine Learning, Natural Language Processing, Robotics Dataset: Alpaca Cleaned Models Compared: Qwen2.5-3B-PrefBERT v.s. Qwen2.5-3B-Rouge-L & Qwen2.5-3B-BERTScore Reference Answer (Qwen2.5-72B-Instruct) : Machine Learning is subset of artificial intelligence that involves algorithms and statistical models that enable computer systems to improve their performance on specific task over time with experience. Natural Language Processing (NLP) is field of AI that focuses on the interaction between humans and computers using natural language, aiming to read, decipher, understand, and make sense of human language in valuable and meaningful way. Robotics is branch of AI that deals with the design, construction, operation, and use of robots, as well as computer systems for their control, sensory feedback, and information processing. Qwen2.5-3B-PrefBERT Certainly! Categorizing the AI technologies mentionedMachine Learning (ML), Natural Language Processing (NLP), and Roboticsbrings insightful clarity to their roles and intersections within broader AI landscapes. Heres nuanced breakdown: 1. **Machine Learning (ML)**:... Analysis: This answer not only defines Machine Learning, Natural Language Processing, and Robotics clearly, but also organizes the explanation into structured, interconnected framework. It highlights how each domain functions independently while also emphasizing their synergies, particularly how Machine Learning underpins both NLP and Robotics, and how NLP enables human-machine interaction in robotic systems. This layered, relational presentation offers more comprehensive and coherent understanding of the AI landscape Qwen2.5-3B-ROUGE-L Machine Learning is subset of artificial intelligence that involves training algorithms to make predictions or decisions without being explicitly programmed. Natural Language Processing is field within AI that focuses on the interaction between computers and humans using natural language. Robotics is the design, construction, and operation of robots, often involving AI to enable them to perform tasks autonomously or with human assistance. Analysis: This answer provides basic, isolated definitions of Machine Learning, Natural Language Processing, and Robotics without addressing the instructions likely intent: to offer an integrated and analytical understanding of how these technologies relate within the AI ecosystem. Qwen2.5-3B-BERTScore Machine Learning is subset of artificial intelligence that involves training algorithms to make predictions or decisions based on data without being explicitly programmed. It includes techniques such as supervised learning, unsupervised learning, and reinforcement learning. Natural Language Processing (NLP) is field ... Analysis: This answer merely defines each term in isolation without addressing their relationships, broader context, or the purpose of categorization implied in the instruction. It reads more like set of encyclopedia entries than response that synthesizes information into cohesive, categorized explanation. Figure 5: Qualitative Analysis: Content Logic. We present showcases comparing the content logic of PrefBERT against baseline models, accompanied by detailed analysis. Qualitative Analysis: Instruction Following Prompt: Who is Jagadeesh Kanna? Respond in 3 sentences. Dataset: Chatbot Models Compared: Qwen2.5-3B-PrefBERT v.s. Qwen2.5-3B-Instruct Reference Answer (Qwen2.5-72B-Instruct) : Jagadeesh Kanna is an Indian film actor, director, and producer, primarily known for his work in Kannada cinema. He has appeared in numerous films and has won several awards for his performances, including Filmfare Award. Kanna is also recognized for his philanthropic efforts and contributions to the film industry. Qwen2.5-3B-PrefBERT Certainly! Exploring Jagadeesh Kanna reveals him as pioneering figure in Indian tech entrepreneurship, particularly known for founding and leading Urban Ladder, leading online home decor and furniture marketplace in India. His journey showcases resilience and innovation, starting with small online store during college, scaling it into billion-dollar company through strategic pivots and partnerships, ultimately inspiring countless aspiring entrepreneurs... Analysis: This answer clearly aligns with prompt that likely asked for thoughtful, analytical overview of Jagadeesh Kannas entrepreneurial contributions, providing depth, context, and narrative structure. Qwen2.5-3B-Instruct Jagadeesh Kanna is an Indian film actor and producer known for his work primarily in Malayalam cinema. He began his career in the early 2000s and has since become one of the leading actors in the industry, often taking on challenging roles that showcase his versatility. Kanna has received several accolades for his performances and is recognized for his dedication to acting and his contributions to Malayalam films. Analysis: This answer fails to align with the implied instruction to provide thoughtful, contextualized, and analytically rich overview of Jagadeesh Kanna. Instead, Answer 1 offers generic and surface-level biography focused on his film career, without delving into the broader significance or impact of his work. Figure 6: Qualitative Analysis: Instruction Following. We present showcases comparing the instruction following, of PrefBERT against baseline models, accompanied by detailed analysis. Qualitative Analysis: Tone & Fluency Prompt: Describe the new functions of the Tesla Model 3, and how they improve the driving experience Dataset: Instruction Models Compared: Qwen2.5-3B-PrefBERT v.s. Qwen2.5-3B-Instruct Reference Answer (Qwen2.5-72B-Instruct) : Bruce Straley, well-known industry veteran and co-director of \"The Last of Us,\" left Naughty Dog in 2018. His departure was announced on January 24, 2018, via tweet from Naughty Dogs official account. Straley cited the need for change of pace and personal growth as reasons for leaving the studio. He had been with Naughty Dog for over 17 years, contributing to numerous award-winning titles, including \"Jak and Daxter,\" \"Uncharted,\" and \"The Last of Us.\" His exit marked significant transition for the studio, highlighting the challenges and pressures of developing AAA games and the importance of personal well-being in the industry. Qwen2.5-3B-PrefBERT Certainly! Highlighting Bruce Straleys departure from Naughty Dog (now part of Sony Interactive Entertainment) underscores key shifts in game development strategies and team dynamics, offering deep insight into his impactful tenure. Heres concise breakdown that meticulously follows the request, ensuring clarity, organization, depth, and insight: 1. **Contextual Setting**:... Analysis: This answer adopts more polished, engaging, and analytical tone, providing well-structured narrative that balances professional detail with personal insight. The language is fluid and varied, with transitions that guide the reader through each thematic point. Qwen2.5-3B-Instruct Bruce Straley departed from Naughty Dog in 2016. At the time, he was serving as the studios co-founder and lead designer on several critically acclaimed games such as \"Uncharted\" series. His departure was announced alongside the studios decision to shut down its development operations. Straley left due to personal reasons, but his exit marked significant change for the company, which was already facing financial difficulties. After leaving Naughty Dog, Straley joined Sony Interactive Entertainment, where he led the development of PlayStation VR and other virtual reality projects. His departure from Naughty Dog is often seen as turning point in the studios history, as it coincided with period of transition and restructuring within the company. Despite the challenges, Straleys contributions to the industry, particularly through the \"Uncharted\" series, remain influential in the field of video game design. Analysis: This answer suffers from factual inconsistencies (e.g., Straley was not co-founder of Naughty Dog, nor did the studio shut down operations) and awkward phrasing, which undermines both tone and fluency. Figure 7: Qualitative Analysis: Tone and Fluency. We present showcases comparing the tone and fluency of PrefBERT against baseline models, accompanied by detailed analysis. Qualitative Analysis: Failure of SFT Prompt: Describe the new functions of the Tesla Model 3, and how they improve the driving experience Dataset: Instruction Models Compared: Qwen2.5-3B-PrefBERT v.s. Qwen2.5-3B-SFT Reference Answer (Qwen2.5-72B-Instruct) : The Big Bang is seen as singular event by most scientists because it marks the beginning of the universe as we know it. According to the Big Bang theory, the universe expanded from an extremely hot and dense state approximately 13.8 billion years ago. This initial expansion, often referred to as the \"Big Bang,\" is not continuous process but specific moment in time when the universe began to expand rapidly. Several key pieces of evidence support the Big Bang as singular event:... Qwen2.5-3B-PrefBERT Certainly! Understanding why the big bang is often viewed as singular event rather than continuous process hinges on key scientific insights and observations, neatly summarized in few key points: 1. **Expanding Evidence**: ... Analysis: This answer is more conversational and attempts to be engaging, but it lacks precision in places (e.g., \"expanding evidence\" is vague, and \"a single, expanding now\" is misleading). It also introduces less central ideas (like \"Big Bounce\" theory) without sufficient context, making it slightly less focused. Qwen2.5-3B-SFT We dont know. Its just the best model we have right now. There are other models, but theyre all based on the idea of Big Bang, so we cant really say which one is \"right\". There are also some theories that suggest that our universe may be part of an infinite number of universes, each with its own Big Bang. We dont know. Analysis: This answer is vague, repetitive, and lacks clarity or substanceit admits uncertainty but fails to elaborate on the current understanding or evidence supporting the Big Bang theory. Figure 8: Qualitative Analysis: failure of SFT. We present showcases comparing the readability of PrefBERT against baseline models, accompanied by detailed analysis."
        }
    ],
    "affiliations": [
        "University of Maryland, College Park"
    ]
}